{"custom_id": "keras#d92fab69a253db150af9d3fd638f2a01d53f4aa9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 90 | Lines Deleted: 26 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 116 | Churn Cumulative: 2275 | Contributors (this commit): 27 | Commits (past 90d): 7 | Contributors (cumulative): 27 | DMM Complexity: 0.1111111111111111\n\nDIFF:\n@@ -13,6 +13,8 @@ from six.moves import range\n import os\n import threading\n import warnings\n+import multiprocessing.pool\n+from functools import partial\n \n from .. import backend as K\n \n@@ -821,6 +823,73 @@ class NumpyArrayIterator(Iterator):\n         return batch_x, batch_y\n \n \n+def _count_valid_files_in_directory(directory, white_list_formats, follow_links):\n+    \"\"\"Count files with extension in `white_list_formats` contained in a directory.\n+\n+    # Arguments\n+        directory: absolute path to the directory containing files to be counted\n+        white_list_formats: set of strings containing allowed extensions for\n+            the files to be counted.\n+\n+    # Returns\n+        the count of files with extension in `white_list_formats` contained in\n+        the directory.\n+    \"\"\"\n+    def _recursive_list(subpath):\n+        return sorted(os.walk(subpath, followlinks=follow_links), key=lambda tpl: tpl[0])\n+\n+    samples = 0\n+    for root, _, files in _recursive_list(directory):\n+        for fname in files:\n+            is_valid = False\n+            for extension in white_list_formats:\n+                if fname.lower().endswith('.' + extension):\n+                    is_valid = True\n+                    break\n+            if is_valid:\n+                samples += 1\n+    return samples\n+\n+\n+def _list_valid_filenames_in_directory(directory, white_list_formats,\n+                                       class_indices, follow_links):\n+    \"\"\"List paths of files in `subdir` relative from `directory` whose extensions are in `white_list_formats`.\n+\n+    # Arguments\n+        directory: absolute path to a directory containing the files to list.\n+            The directory name is used as class label and must be a key of `class_indices`.\n+        white_list_formats: set of strings containing allowed extensions for\n+            the files to be counted.\n+        class_indices: dictionary mapping a class name to its index.\n+\n+    # Returns\n+        classes: a list of class indices\n+        filenames: the path of valid files in `directory`, relative from\n+            `directory`'s parent (e.g., if `directory` is \"dataset/class1\",\n+            the filenames will be [\"class1/file1.jpg\", \"class1/file2.jpg\", ...]).\n+    \"\"\"\n+    def _recursive_list(subpath):\n+        return sorted(os.walk(subpath, followlinks=follow_links), key=lambda tpl: tpl[0])\n+\n+    classes = []\n+    filenames = []\n+    subdir = os.path.basename(directory)\n+    basedir = os.path.dirname(directory)\n+    for root, _, files in _recursive_list(directory):\n+        for fname in files:\n+            is_valid = False\n+            for extension in white_list_formats:\n+                if fname.lower().endswith('.' + extension):\n+                    is_valid = True\n+                    break\n+            if is_valid:\n+                classes.append(class_indices[subdir])\n+                # add filename relative to directory\n+                absolute_path = os.path.join(root, fname)\n+                filenames.append(os.path.relpath(absolute_path, basedir))\n+    return classes, filenames\n+\n+\n class DirectoryIterator(Iterator):\n     \"\"\"Iterator capable of reading images from a directory on disk.\n \n@@ -913,38 +982,33 @@ class DirectoryIterator(Iterator):\n         def _recursive_list(subpath):\n             return sorted(os.walk(subpath, followlinks=follow_links), key=lambda tpl: tpl[0])\n \n-        for subdir in classes:\n-            subpath = os.path.join(directory, subdir)\n-            for root, _, files in _recursive_list(subpath):\n-                for fname in files:\n-                    is_valid = False\n-                    for extension in white_list_formats:\n-                        if fname.lower().endswith('.' + extension):\n-                            is_valid = True\n-                            break\n-                    if is_valid:\n-                        self.samples += 1\n+        pool = multiprocessing.pool.ThreadPool()\n+        function_partial = partial(_count_valid_files_in_directory,\n+                                   white_list_formats=white_list_formats,\n+                                   follow_links=follow_links)\n+        self.samples = sum(pool.map(function_partial,\n+                                    (os.path.join(directory, subdir)\n+                                     for subdir in classes)))\n+\n         print('Found %d images belonging to %d classes.' % (self.samples, self.num_class))\n \n         # second, build an index of the images in the different class subfolders\n+        results = []\n+\n         self.filenames = []\n         self.classes = np.zeros((self.samples,), dtype='int32')\n         i = 0\n-        for subdir in classes:\n-            subpath = os.path.join(directory, subdir)\n-            for root, _, files in _recursive_list(subpath):\n-                for fname in files:\n-                    is_valid = False\n-                    for extension in white_list_formats:\n-                        if fname.lower().endswith('.' + extension):\n-                            is_valid = True\n-                            break\n-                    if is_valid:\n-                        self.classes[i] = self.class_indices[subdir]\n-                        i += 1\n-                        # add filename relative to directory\n-                        absolute_path = os.path.join(root, fname)\n-                        self.filenames.append(os.path.relpath(absolute_path, directory))\n+        for dirpath in (os.path.join(directory, subdir) for subdir in classes):\n+            results.append(pool.apply_async(_list_valid_filenames_in_directory,\n+                                            (dirpath, white_list_formats,\n+                                             self.class_indices, follow_links)))\n+        for res in results:\n+            classes, filenames = res.get()\n+            self.classes[i:i + len(classes)] = classes\n+            self.filenames += filenames\n+            i += len(classes)\n+        pool.close()\n+        pool.join()\n         super(DirectoryIterator, self).__init__(self.samples, batch_size, shuffle, seed)\n \n     def next(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#11d9c995cc8744d6de04f89f705957369e2865ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 38 | Churn Cumulative: 2209 | Contributors (this commit): 23 | Commits (past 90d): 3 | Contributors (cumulative): 25 | DMM Complexity: 0.041666666666666664\n\nDIFF:\n@@ -1,5 +1,6 @@\n from __future__ import absolute_import\n import six\n+import copy\n from six.moves import zip\n \n from . import backend as K\n@@ -11,7 +12,30 @@ if K.backend() == 'tensorflow':\n \n \n def clip_norm(g, c, n):\n-    if c > 0:\n+    if c <= 0:  # if clipnorm == 0 no need to add ops to the graph\n+        return g\n+\n+    # tf require using a special op to multiply IndexedSliced by scalar\n+    if K.backend() == 'tensorflow':\n+        condition = n >= c\n+        then_expression = tf.scalar_mul(c / n, g)\n+        else_expression = g\n+\n+        # saving the shape to avoid converting sparse tensor to dense\n+        if isinstance(then_expression, tf.Tensor):\n+            g_shape = copy.copy(then_expression.get_shape())\n+        elif isinstance(then_expression, tf.IndexedSlices):\n+            g_shape = copy.copy(then_expression.dense_shape)\n+        if condition.dtype != tf.bool:\n+            condition = tf.cast(condition, 'bool')\n+        g = tf.cond(condition,\n+                    lambda: then_expression,\n+                    lambda: else_expression)\n+        if isinstance(then_expression, tf.Tensor):\n+            g.set_shape(g_shape)\n+        elif isinstance(then_expression, tf.IndexedSlices):\n+            g._dense_shape = g_shape\n+    else:\n         g = K.switch(K.greater_equal(n, c), g * c / n, g)\n     return g\n \n\n@@ -6,7 +6,8 @@ import string\n from keras.utils.test_utils import get_test_data, keras_test\n from keras.utils.np_utils import to_categorical\n from keras.models import Sequential\n-from keras import layers\n+from keras import layers, optimizers\n+import keras.backend as K\n import keras\n \n \n@@ -204,5 +205,14 @@ def test_masked_temporal():\n     ground_truth = -np.log(0.5)\n     assert(np.abs(history.history['loss'][-1] - ground_truth) < 0.06)\n \n+\n+@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TF backend')\n+@keras_test\n+def test_embedding_with_clipnorm():\n+    model = Sequential()\n+    model.add(layers.Embedding(input_dim=1, output_dim=1))\n+    model.compile(optimizer=optimizers.SGD(clipnorm=0.1), loss='mse')\n+    model.fit(np.array([[0]]), np.array([[[0.5]]]), epochs=1)\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5012678e172de7b956627461801d47372d2c8676", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1281 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -161,7 +161,7 @@ def recurrent_args_preprocessor(args, kwargs):\n             kwargs.pop('forget_bias_init')\n             warnings.warn('The `forget_bias_init` argument '\n                           'has been ignored. Use `unit_forget_bias=True` '\n-                          'instead to intialize with ones.', stacklevel=3)\n+                          'instead to initialize with ones.', stacklevel=3)\n     if 'input_dim' in kwargs:\n         input_length = kwargs.pop('input_length', None)\n         input_dim = kwargs.pop('input_dim')\n@@ -461,7 +461,7 @@ def convlstm2d_args_preprocessor(args, kwargs):\n         else:\n             warnings.warn('The `forget_bias_init` argument '\n                           'has been ignored. Use `unit_forget_bias=True` '\n-                          'instead to intialize with ones.', stacklevel=3)\n+                          'instead to initialize with ones.', stacklevel=3)\n     args, kwargs, _converted = conv2d_args_preprocessor(args, kwargs)\n     return args, kwargs, converted + _converted\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d4b618bf23d83b1a7306bc6816bd509c23bba15d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 327 | Contributors (this commit): 11 | Commits (past 90d): 13 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -20,4 +20,4 @@ from . import regularizers\n # Importable from root because it's technically not a layer\r\n from .layers import Input\r\n \r\n-__version__ = '2.0.4'\r\n+__version__ = '2.0.5'\r\n\n@@ -3,12 +3,12 @@ from setuptools import find_packages\n \n \n setup(name='Keras',\n-      version='2.0.4',\n+      version='2.0.5',\n       description='Deep Learning for Python',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n-      download_url='https://github.com/fchollet/keras/tarball/2.0.4',\n+      download_url='https://github.com/fchollet/keras/tarball/2.0.5',\n       license='MIT',\n       install_requires=['theano', 'pyyaml', 'six'],\n       extras_require={\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d63ab4251f2fb8034785d358ddcd655257d7b7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 2685 | Contributors (this commit): 50 | Commits (past 90d): 13 | Contributors (cumulative): 50 | DMM Complexity: 1.0\n\nDIFF:\n@@ -490,8 +490,11 @@ class EarlyStopping(Callback):\n     def on_epoch_end(self, epoch, logs=None):\n         current = logs.get(self.monitor)\n         if current is None:\n-            warnings.warn('Early stopping requires %s available!' %\n-                          (self.monitor), RuntimeWarning)\n+            warnings.warn(\n+                'Early stopping conditioned on metric `%s` '\n+                'which is not available. Available metrics are: %s' %\n+                (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n+            )\n \n         if self.monitor_op(current - self.min_delta, self.best):\n             self.best = current\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f7b925a8930ddfc63901fbcdb51be26956096d7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 100 | Lines Deleted: 48 | Files Changed: 7 | Hunks: 52 | Methods Changed: 39 | Complexity Δ (Sum/Max): 5/1 | Churn Δ: 148 | Churn Cumulative: 3309 | Contributors (this commit): 24 | Commits (past 90d): 20 | Contributors (cumulative): 35 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,6 +19,17 @@ batch_size = 32\n epochs = 1\n \n \n+@pytest.fixture\n+def in_tmpdir(tmpdir):\n+    \"\"\"Runs a function in a temporary directory.\n+\n+    Checks that the directory is empty afterwards.\n+    \"\"\"\n+    with tmpdir.as_cwd():\n+        yield None\n+    assert not tmpdir.listdir()\n+\n+\n def _get_test_data():\n     np.random.seed(1234)\n \n@@ -36,7 +47,7 @@ def _get_test_data():\n \n \n @keras_test\n-def test_merge_sum():\n+def test_merge_sum(in_tmpdir):\n     (x_train, y_train), (x_test, y_test) = _get_test_data()\n     left = Sequential()\n     left.add(Dense(num_hidden, input_shape=(input_dim,)))\n@@ -131,7 +142,7 @@ def test_merge_dot():\n \n \n @keras_test\n-def test_merge_concat():\n+def test_merge_concat(in_tmpdir):\n     (x_train, y_train), (x_test, y_test) = _get_test_data()\n \n     left = Sequential(name='branch_1')\n@@ -171,7 +182,7 @@ def test_merge_concat():\n \n \n @keras_test\n-def test_merge_recursivity():\n+def test_merge_recursivity(in_tmpdir):\n     (x_train, y_train), (x_test, y_test) = _get_test_data()\n     left = Sequential()\n     left.add(Dense(num_hidden, input_shape=(input_dim,)))\n@@ -228,7 +239,7 @@ def test_merge_recursivity():\n \n \n @keras_test\n-def test_merge_overlap():\n+def test_merge_overlap(in_tmpdir):\n     (x_train, y_train), (x_test, y_test) = _get_test_data()\n     left = Sequential()\n     left.add(Dense(num_hidden, input_shape=(input_dim,)))\n\n@@ -3,8 +3,6 @@ from keras.preprocessing import image\n from PIL import Image\n import numpy as np\n import os\n-import shutil\n-import tempfile\n \n \n class TestImage:\n@@ -29,7 +27,7 @@ class TestImage:\n     def teardown_class(cls):\n         del cls.all_test_images\n \n-    def test_image_data_generator(self):\n+    def test_image_data_generator(self, tmpdir):\n         for test_images in self.all_test_images:\n             img_list = []\n             for im in test_images:\n@@ -54,12 +52,10 @@ class TestImage:\n                 vertical_flip=True)\n             generator.fit(images, augment=True)\n \n-            tmp_folder = tempfile.mkdtemp(prefix='test_images')\n             for x, y in generator.flow(images, np.arange(images.shape[0]),\n-                                       shuffle=True, save_to_dir=tmp_folder):\n+                                       shuffle=True, save_to_dir=str(tmpdir)):\n                 assert x.shape[1:] == images.shape[1:]\n                 break\n-            shutil.rmtree(tmp_folder)\n \n     def test_image_data_generator_invalid_data(self):\n         generator = image.ImageDataGenerator(\n@@ -107,9 +103,8 @@ class TestImage:\n         x = np.random.random((32, 3, 10, 10))\n         generator.fit(x)\n \n-    def test_directory_iterator(self):\n+    def test_directory_iterator(self, tmpdir):\n         num_classes = 2\n-        tmp_folder = tempfile.mkdtemp(prefix='test_images')\n \n         # create folders and subfolders\n         paths = []\n@@ -122,7 +117,7 @@ class TestImage:\n                 os.path.join(class_directory, 'subfolder-1', 'sub-subfolder')\n             ]\n             for path in classpaths:\n-                os.mkdir(os.path.join(tmp_folder, path))\n+                tmpdir.join(path).mkdir()\n             paths.append(classpaths)\n \n         # save the images in the paths\n@@ -136,34 +131,32 @@ class TestImage:\n                 classpaths = paths[im_class]\n                 filename = os.path.join(classpaths[count % len(classpaths)], 'image-{}.jpg'.format(count))\n                 filenames.append(filename)\n-                im.save(os.path.join(tmp_folder, filename))\n+                im.save(str(tmpdir / filename))\n                 count += 1\n \n         # create iterator\n         generator = image.ImageDataGenerator()\n-        dir_iterator = generator.flow_from_directory(tmp_folder)\n+        dir_iterator = generator.flow_from_directory(str(tmpdir))\n \n         # check number of classes and images\n         assert(len(dir_iterator.class_indices) == num_classes)\n         assert(len(dir_iterator.classes) == count)\n         assert(sorted(dir_iterator.filenames) == sorted(filenames))\n-        shutil.rmtree(tmp_folder)\n \n-    def test_directory_iterator_class_mode_input(self):\n-        tmp_folder = tempfile.mkdtemp(prefix='test_images')\n-        os.mkdir(os.path.join(tmp_folder, 'class-1'))\n+    def test_directory_iterator_class_mode_input(self, tmpdir):\n+        tmpdir.join('class-1').mkdir()\n \n         # save the images in the paths\n         count = 0\n         for test_images in self.all_test_images:\n             for im in test_images:\n-                filename = os.path.join(tmp_folder, 'class-1', 'image-{}.jpg'.format(count))\n-                im.save(os.path.join(tmp_folder, filename))\n+                filename = str(tmpdir / 'class-1' / 'image-{}.jpg'.format(count))\n+                im.save(filename)\n                 count += 1\n \n         # create iterator\n         generator = image.ImageDataGenerator()\n-        dir_iterator = generator.flow_from_directory(tmp_folder, class_mode='input')\n+        dir_iterator = generator.flow_from_directory(str(tmpdir), class_mode='input')\n         batch = next(dir_iterator)\n \n         # check if input and output have the same shape\n@@ -173,7 +166,6 @@ class TestImage:\n         output_img = batch[1][0]\n         output_img[0][0][0] += 1\n         assert(input_img[0][0][0] != output_img[0][0][0])\n-        shutil.rmtree(tmp_folder)\n \n     def test_img_utils(self):\n         height, width = 10, 8\n\n@@ -54,9 +54,9 @@ def test_TerminateOnNaN():\n \n \n @keras_test\n-def test_ModelCheckpoint():\n+def test_ModelCheckpoint(tmpdir):\n     np.random.seed(1337)\n-    filepath = 'checkpoint.h5'\n+    filepath = str(tmpdir / 'checkpoint.h5')\n     (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n@@ -80,7 +80,7 @@ def test_ModelCheckpoint():\n                                       save_best_only=save_best_only, mode=mode)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n               validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n-    assert os.path.exists(filepath)\n+    assert os.path.isfile(filepath)\n     os.remove(filepath)\n \n     # case 2\n@@ -89,7 +89,7 @@ def test_ModelCheckpoint():\n                                       save_best_only=save_best_only, mode=mode)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n               validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n-    assert os.path.exists(filepath)\n+    assert os.path.isfile(filepath)\n     os.remove(filepath)\n \n     # case 3\n@@ -99,7 +99,7 @@ def test_ModelCheckpoint():\n                                       save_best_only=save_best_only, mode=mode)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n               validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n-    assert os.path.exists(filepath)\n+    assert os.path.isfile(filepath)\n     os.remove(filepath)\n \n     # case 4\n@@ -108,7 +108,7 @@ def test_ModelCheckpoint():\n                                       save_best_only=save_best_only, mode=mode)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n               validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n-    assert os.path.exists(filepath)\n+    assert os.path.isfile(filepath)\n     os.remove(filepath)\n \n     # case 5\n@@ -121,12 +121,13 @@ def test_ModelCheckpoint():\n                                       period=period)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n               validation_data=(X_test, y_test), callbacks=cbks, epochs=4)\n-    assert os.path.exists(filepath.format(epoch=1))\n-    assert os.path.exists(filepath.format(epoch=3))\n+    assert os.path.isfile(filepath.format(epoch=1))\n+    assert os.path.isfile(filepath.format(epoch=3))\n     assert not os.path.exists(filepath.format(epoch=0))\n     assert not os.path.exists(filepath.format(epoch=2))\n     os.remove(filepath.format(epoch=1))\n     os.remove(filepath.format(epoch=3))\n+    assert not tmpdir.listdir()\n \n \n @keras_test\n@@ -244,9 +245,9 @@ def test_ReduceLROnPlateau():\n \n \n @keras_test\n-def test_CSVLogger():\n+def test_CSVLogger(tmpdir):\n     np.random.seed(1337)\n-    filepath = 'log.tsv'\n+    filepath = str(tmpdir / 'log.tsv')\n     sep = '\\t'\n     (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                          num_test=test_samples,\n@@ -273,7 +274,7 @@ def test_CSVLogger():\n     model.fit(X_train, y_train, batch_size=batch_size,\n               validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n \n-    assert os.path.exists(filepath)\n+    assert os.path.isfile(filepath)\n     with open(filepath) as csvfile:\n         dialect = Sniffer().sniff(csvfile.read())\n     assert dialect.delimiter == sep\n@@ -296,14 +297,15 @@ def test_CSVLogger():\n         assert len(re.findall('epoch', output)) == 1\n \n     os.remove(filepath)\n+    assert not tmpdir.listdir()\n \n \n @keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n                     reason='Requires tensorflow backend')\n-def test_TensorBoard():\n+def test_TensorBoard(tmpdir):\n     np.random.seed(np.random.randint(1, 1e7))\n-    filepath = './logs_' + str(np.random.randint(1, 1e4))\n+    filepath = str(tmpdir / 'logs')\n \n     (X_train, y_train), (X_test, y_test) = get_test_data(\n         num_train=train_samples,\n@@ -379,16 +381,17 @@ def test_TensorBoard():\n     model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                         callbacks=cbks)\n \n-    assert os.path.exists(filepath)\n+    assert os.path.isdir(filepath)\n     shutil.rmtree(filepath)\n+    assert not tmpdir.listdir()\n \n \n @keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n                     reason='Requires tensorflow backend')\n-def test_TensorBoard_convnet():\n+def test_TensorBoard_convnet(tmpdir):\n     np.random.seed(np.random.randint(1, 1e7))\n-    filepath = './logs_' + str(np.random.randint(1, 1e4))\n+    filepath = str(tmpdir / 'logs')\n \n     input_shape = (16, 16, 3)\n     (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,\n@@ -421,8 +424,9 @@ def test_TensorBoard_convnet():\n                         validation_data=(x_test, y_test),\n                         callbacks=cbks,\n                         verbose=0)\n-    assert os.path.exists(filepath)\n+    assert os.path.isdir(filepath)\n     shutil.rmtree(filepath)\n+    assert not tmpdir.listdir()\n \n \n @keras_test\n@@ -510,10 +514,10 @@ def test_LambdaCallback():\n @keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n                     reason=\"Requires tensorflow backend\")\n-def test_TensorBoard_with_ReduceLROnPlateau():\n+def test_TensorBoard_with_ReduceLROnPlateau(tmpdir):\n     import shutil\n     np.random.seed(np.random.randint(1, 1e7))\n-    filepath = './logs_' + str(np.random.randint(1, 1e4))\n+    filepath = str(tmpdir / 'logs')\n \n     (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                          num_test=test_samples,\n@@ -542,8 +546,9 @@ def test_TensorBoard_with_ReduceLROnPlateau():\n     model.fit(X_train, y_train, batch_size=batch_size,\n               validation_data=(X_test, y_test), callbacks=cbks, epochs=2)\n \n-    assert os.path.exists(filepath)\n+    assert os.path.isdir(filepath)\n     shutil.rmtree(filepath)\n+    assert not tmpdir.listdir()\n \n \n if __name__ == '__main__':\n\n@@ -21,6 +21,17 @@ batch_size = 32\n epochs = 1\n \n \n+@pytest.fixture\n+def in_tmpdir(tmpdir):\n+    \"\"\"Runs a function in a temporary directory.\n+\n+    Checks that the directory is empty afterwards.\n+    \"\"\"\n+    with tmpdir.as_cwd():\n+        yield None\n+    assert not tmpdir.listdir()\n+\n+\n @keras_test\n def test_sequential_pop():\n     model = Sequential()\n@@ -92,7 +103,7 @@ def test_sequential_fit_generator():\n \n \n @keras_test\n-def test_sequential():\n+def test_sequential(in_tmpdir):\n     (x_train, y_train), (x_test, y_test) = _get_test_data()\n \n     # TODO: factor out\n@@ -160,7 +171,7 @@ def test_sequential():\n \n \n @keras_test\n-def test_nested_sequential():\n+def test_nested_sequential(in_tmpdir):\n     (x_train, y_train), (x_test, y_test) = _get_test_data()\n \n     inner = Sequential()\n\n@@ -11,7 +11,18 @@ from keras.utils.data_utils import validate_file\n from keras.utils.data_utils import _hash_file\n \n \n-def test_data_utils():\n+@pytest.fixture\n+def in_tmpdir(tmpdir):\n+    \"\"\"Runs a function in a temporary directory.\n+\n+    Checks that the directory is empty afterwards.\n+    \"\"\"\n+    with tmpdir.as_cwd():\n+        yield None\n+    assert not tmpdir.listdir()\n+\n+\n+def test_data_utils(in_tmpdir):\n     \"\"\"Tests get_file from a url, plus extraction and validation.\n     \"\"\"\n     dirname = 'data_utils'\n\n@@ -10,6 +10,17 @@ import warnings\n import h5py\n \n \n+@pytest.fixture\n+def in_tmpdir(tmpdir):\n+    \"\"\"Runs a function in a temporary directory.\n+\n+    Checks that the directory is empty afterwards.\n+    \"\"\"\n+    with tmpdir.as_cwd():\n+        yield None\n+    assert not tmpdir.listdir()\n+\n+\n def create_dataset(h5_path='test.h5'):\n     X = np.random.randn(200, 10).astype('float32')\n     y = np.random.randint(0, 2, size=(200, 1))\n@@ -23,7 +34,7 @@ def create_dataset(h5_path='test.h5'):\n     f.close()\n \n \n-def test_io_utils():\n+def test_io_utils(in_tmpdir):\n     '''Tests the HDF5Matrix code using the sample from @jfsantos at\n     https://gist.github.com/jfsantos/e2ef822c744357a4ed16ec0c885100a3\n     '''\n\n@@ -7,6 +7,17 @@ from keras.layers.core import Dense\n from keras.utils.test_utils import keras_test\n \n \n+@pytest.fixture\n+def in_tmpdir(tmpdir):\n+    \"\"\"Runs a function in a temporary directory.\n+\n+    Checks that the directory is empty afterwards.\n+    \"\"\"\n+    with tmpdir.as_cwd():\n+        yield None\n+    assert not tmpdir.listdir()\n+\n+\n @keras_test\n def test_multiprocessing_training():\n     arr_data = np.random.randint(0, 256, (50, 2))\n@@ -46,7 +57,7 @@ def test_multiprocessing_training():\n \n \n @keras_test\n-def test_multiprocessing_training_fromfile():\n+def test_multiprocessing_training_fromfile(in_tmpdir):\n     arr_data = np.random.randint(0, 256, (50, 2))\n     arr_labels = np.random.randint(0, 2, 50)\n     np.savez('data.npz', **{'data': arr_data, 'labels': arr_labels})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cd943231d1e8c7d691ff278d043bcf3bc5c563bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 4326 | Contributors (this commit): 49 | Commits (past 90d): 19 | Contributors (cumulative): 49 | DMM Complexity: 0.0\n\nDIFF:\n@@ -621,6 +621,10 @@ class GeneratorEnqueuer(object):\n             if self._pickle_safe:\n                 self.queue = multiprocessing.Queue(maxsize=max_q_size)\n                 self._stop_event = multiprocessing.Event()\n+                if hasattr(data_generator_task, 'lock'):\n+                    # We should replace the threading lock of the iterator\n+                    # with a process-safe lock.\n+                    data_generator_task.lock = multiprocessing.Lock()\n             else:\n                 self.queue = queue.Queue()\n                 self._stop_event = threading.Event()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#295e4f8064e4d75e8e56a5e0895909d48110c7aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 142 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -37,7 +37,7 @@ print('Loading data...')\n X_train = sequence.pad_sequences(X_train, max_length)\n X_test = sequence.pad_sequences(X_test, max_length)\n \n-# Compile and train different models while meauring performance.\n+# Compile and train different models while measuring performance.\n results = []\n for mode in modes:\n     print('Testing mode: implementation={}'.format(mode))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#21cf50734a6996da7023dc500bdcc8ac7d74ef48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 146 | Lines Deleted: 1 | Files Changed: 6 | Hunks: 8 | Methods Changed: 11 | Complexity Δ (Sum/Max): 13/7 | Churn Δ: 147 | Churn Cumulative: 1288 | Contributors (this commit): 19 | Commits (past 90d): 12 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,6 +34,20 @@ def elu(x, alpha=1.0):\n     return K.elu(x, alpha)\n \n \n+def selu(x):\n+    \"\"\"Scaled Exponential Linear Unit. (Klambauer et al., 2017)\n+\n+    # Arguments\n+        x: A tensor or variable to compute the activation function for.\n+\n+    # References\n+        - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n+    \"\"\"\n+    alpha = 1.6732632423543772848170429916717\n+    scale = 1.0507009873554804934193349852946\n+    return scale * K.elu(x, alpha)\n+\n+\n def softplus(x):\n     return K.softplus(x)\n \n\n@@ -371,6 +371,29 @@ def he_normal(seed=None):\n                            seed=seed)\n \n \n+def lecun_normal(seed=None):\n+    \"\"\"LeCun normal initializer.\n+\n+    It draws samples from a truncated normal distribution centered on 0\n+    with `stddev = sqrt(1 / fan_in)`\n+    where `fan_in` is the number of input units in the weight tensor.\n+\n+    # Arguments\n+        seed: A Python integer. Used to seed the random generator.\n+\n+    # Returns\n+        An initializer.\n+\n+    # References\n+        - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n+        - [Efficient Backprop](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n+    \"\"\"\n+    return VarianceScaling(scale=1.,\n+                           mode='fan_in',\n+                           distribution='normal',\n+                           seed=seed)\n+\n+\n def he_uniform(seed=None):\n     \"\"\"He uniform variance scaling initializer.\n \n\n@@ -5,6 +5,7 @@ from ..engine import Layer\n from .. import backend as K\n import numpy as np\n from ..legacy import interfaces\n+from ..engine import InputSpec\n \n \n class GaussianNoise(Layer):\n@@ -90,3 +91,70 @@ class GaussianDropout(Layer):\n         config = {'rate': self.rate}\n         base_config = super(GaussianDropout, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class AlphaDropout(Layer):\n+    \"\"\"Applies Alpha Dropout to the input.\n+\n+    Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n+    to their original values, in order to ensure the self-normalizing property\n+    even after this dropout.\n+    Alpha Dropout fits well to Scaled Exponential Linear Units\n+    by randomly setting activations to the negative saturation value.\n+\n+    # Arguments\n+        rate: float, drop probability (as with `Dropout`).\n+            The multiplicative noise will have\n+            standard deviation `sqrt(rate / (1 - rate))`.\n+        seed: A Python integer to use as random seed.\n+\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Same shape as input.\n+\n+    # References\n+        - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n+    \"\"\"\n+    def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n+        super(AlphaDropout, self).__init__(**kwargs)\n+        self.rate = rate\n+        self.noise_shape = noise_shape\n+        self.seed = seed\n+        self.supports_masking = True\n+\n+    def _get_noise_shape(self, inputs):\n+        return self.noise_shape if self.noise_shape else K.shape(inputs)\n+\n+    def call(self, inputs, training=None):\n+        if 0. < self.rate < 1.:\n+            noise_shape = self._get_noise_shape(inputs)\n+\n+            def dropped_inputs(inputs=inputs, rate=self.rate, seed=self.seed):\n+                alpha = 1.6732632423543772848170429916717\n+                scale = 1.0507009873554804934193349852946\n+                alpha_p = -alpha * scale\n+\n+                kept_idx = K.greater_equal(K.random_uniform(noise_shape, seed=seed), rate)\n+                kept_idx = K.cast(kept_idx, K.floatx())\n+\n+                # Get affine transformation params\n+                a = ((1 - rate) * (1 + rate * alpha_p ** 2)) ** -0.5\n+                b = -a * alpha_p * rate\n+\n+                # Apply mask\n+                x = inputs * kept_idx + alpha_p * (1 - kept_idx)\n+\n+                # Do affine transformation\n+                return a * x + b\n+\n+            return K.in_train_phase(dropped_inputs, inputs, training=training)\n+        return inputs\n+\n+    def get_config(self):\n+        config = {'rate': self.rate}\n+        base_config = super(AlphaDropout, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -15,7 +15,7 @@ def get_standard_values():\n def test_serialization():\n     all_activations = ['softmax', 'relu', 'elu', 'tanh',\n                        'sigmoid', 'hard_sigmoid', 'linear',\n-                       'softplus', 'softsign']\n+                       'softplus', 'softsign', 'selu']\n     for name in all_activations:\n         fn = activations.get(name)\n         ref_fn = getattr(activations, name)\n@@ -146,6 +146,29 @@ def test_elu():\n     assert_allclose(result, true_result)\n \n \n+def test_selu():\n+    x = K.placeholder(ndim=2)\n+    f = K.function([x], [activations.selu(x)])\n+    alpha = 1.6732632423543772848170429916717\n+    scale = 1.0507009873554804934193349852946\n+\n+    positive_values = get_standard_values()\n+    result = f([positive_values])[0]\n+    assert_allclose(result, positive_values * scale, rtol=1e-05)\n+\n+    negative_values = np.array([[-1, -2]], dtype=K.floatx())\n+\n+    # cntk can't rebind the input shape, so create the model again to test different batch size\n+    if (K.backend() == 'cntk'):\n+        x2 = K.placeholder(ndim=2)\n+        f = K.function([x2], [activations.selu(x2)])\n+\n+    result = f([negative_values])[0]\n+    true_result = (np.exp(negative_values) - 1) * scale * alpha\n+\n+    assert_allclose(result, true_result)\n+\n+\n def test_tanh():\n     test_values = get_standard_values()\n \n\n@@ -90,6 +90,14 @@ def test_he_normal(tensor_shape):\n             target_mean=0., target_std=None, target_max=2 * scale)\n \n \n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_lecun_normal(tensor_shape):\n+    fan_in, _ = initializers._compute_fans(tensor_shape)\n+    scale = np.sqrt(1. / fan_in)\n+    _runner(initializers.lecun_normal(), tensor_shape,\n+            target_mean=0., target_std=scale)\n+\n+\n @pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n def test_orthogonal(tensor_shape):\n     _runner(initializers.orthogonal(), tensor_shape,\n\n@@ -23,5 +23,14 @@ def test_GaussianDropout():\n                input_shape=(3, 2, 3))\n \n \n+@keras_test\n+@pytest.mark.skipif((K.backend() == 'cntk'),\n+                    reason=\"cntk does not support it yet\")\n+def test_AlphaDropout():\n+    layer_test(noise.AlphaDropout,\n+               kwargs={'rate': 0.1},\n+               input_shape=(3, 2, 3))\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a625fcde5cb6a1a170a51690d21b575c0e7884c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 33 | Churn Cumulative: 951 | Contributors (this commit): 13 | Commits (past 90d): 13 | Contributors (cumulative): 16 | DMM Complexity: 0.7777777777777778\n\nDIFF:\n@@ -84,12 +84,13 @@ class Wrapper(Layer):\n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n         from . import deserialize as deserialize_layer\n-        layer = deserialize_layer(config.pop('layer'), custom_objects=custom_objects)\n+        layer = deserialize_layer(config.pop('layer'),\n+                                  custom_objects=custom_objects)\n         return cls(layer, **config)\n \n \n class TimeDistributed(Wrapper):\n-    \"\"\"This wrapper allows to apply a layer to every temporal slice of an input.\n+    \"\"\"This wrapper applies a layer to every temporal slice of an input.\n \n     The input should be at least 3D, and the dimension of index one\n     will be considered to be the temporal dimension.\n@@ -157,12 +158,17 @@ class TimeDistributed(Wrapper):\n         func_args = inspect.getargspec(self.layer.call).args\n         if 'training' in func_args:\n             kwargs['training'] = training\n+        uses_learning_phase = False\n \n         input_shape = K.int_shape(inputs)\n         if input_shape[0]:\n             # batch size matters, use rnn-based implementation\n             def step(x, _):\n+                global uses_learning_phase\n                 output = self.layer.call(x, **kwargs)\n+                if hasattr(output, '_uses_learning_phase'):\n+                    uses_learning_phase = (output._uses_learning_phase or\n+                                           uses_learning_phase)\n                 return output, []\n \n             _, outputs, _ = K.rnn(step, inputs,\n@@ -179,7 +185,10 @@ class TimeDistributed(Wrapper):\n                 input_length = K.shape(inputs)[1]\n             # Shape: (num_samples * timesteps, ...)\n             inputs = K.reshape(inputs, (-1,) + input_shape[2:])\n-            y = self.layer.call(inputs, **kwargs)  # (num_samples * timesteps, ...)\n+            # (num_samples * timesteps, ...)\n+            y = self.layer.call(inputs, **kwargs)\n+            if hasattr(y, '_uses_learning_phase'):\n+                uses_learning_phase = y._uses_learning_phase\n             # Shape: (num_samples, timesteps, ...)\n             output_shape = self.compute_output_shape(input_shape)\n             y = K.reshape(y, (-1, input_length) + output_shape[2:])\n@@ -190,9 +199,8 @@ class TimeDistributed(Wrapper):\n             regularization_loss = self.layer.activity_regularizer(y)\n             self.add_loss(regularization_loss, inputs)\n \n-        if 'training' in kwargs:\n+        if uses_learning_phase:\n             y._uses_learning_phase = True\n-            pass\n         return y\n \n \n@@ -214,7 +222,8 @@ class Bidirectional(Wrapper):\n \n     ```python\n         model = Sequential()\n-        model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))\n+        model.add(Bidirectional(LSTM(10, return_sequences=True),\n+                                input_shape=(5, 10)))\n         model.add(Bidirectional(LSTM(10)))\n         model.add(Dense(5))\n         model.add(Activation('softmax'))\n\n@@ -87,11 +87,15 @@ def test_TimeDistributed():\n     outer_model.compile(optimizer='rmsprop', loss='mse')\n     outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)), epochs=1, batch_size=10)\n \n+\n+@keras_test\n+def test_TimeDistributed_learning_phase():\n     # test layers that need learning_phase to be set\n-    model = Sequential()\n-    model.add(wrappers.TimeDistributed(core.Dropout(.5), input_shape=(3, 2)))\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.predict(np.random.random((10, 3, 2)))\n+    x = Input(shape=(3, 2))\n+    y = wrappers.TimeDistributed(core.Dropout(.999))(x, training=True)\n+    model = Model(x, y)\n+    y = model.predict(np.random.random((10, 3, 2)))\n+    assert_allclose(0., y, atol=1e-2)\n \n \n @keras_test\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f65a56fb65062c8d14d215c9f4b1015b97cc5bf3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 109 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -89,6 +89,8 @@ def test_TimeDistributed():\n \n \n @keras_test\n+@pytest.mark.skipif((K.backend() == 'cntk'),\n+                    reason='cntk does not support dropout yet')\n def test_TimeDistributed_learning_phase():\n     # test layers that need learning_phase to be set\n     x = Input(shape=(3, 2))\n@@ -120,7 +122,7 @@ def test_regularizers():\n \n @keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason=\"cntk does not support reverse yet\")\n+                    reason='cntk does not support reverse yet')\n def test_Bidirectional():\n     rnn = recurrent.SimpleRNN\n     samples = 2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1c39881989826aec605f8a30dd11cbdf3872790", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 2692 | Contributors (this commit): 51 | Commits (past 90d): 14 | Contributors (cumulative): 51 | DMM Complexity: None\n\nDIFF:\n@@ -585,20 +585,19 @@ class LearningRateScheduler(Callback):\n class TensorBoard(Callback):\n     \"\"\"Tensorboard basic visualizations.\n \n+    [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)\n+    is a visualization tool provided with TensorFlow.\n+\n     This callback writes a log for TensorBoard, which allows\n     you to visualize dynamic graphs of your training and test\n     metrics, as well as activation histograms for the different\n     layers in your model.\n \n-    TensorBoard is a visualization tool provided with TensorFlow.\n-\n     If you have installed TensorFlow with pip, you should be able\n     to launch TensorBoard from the command line:\n     ```\n     tensorboard --logdir=/full_path_to_your_logs\n     ```\n-    You can find more information about TensorBoard\n-    [here](https://www.tensorflow.org/get_started/summaries_and_tensorboard).\n \n     # Arguments\n         log_dir: the path of the directory where to save the log\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c73ba916f6295586324a298316b5774a53505d8a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1317 | Contributors (this commit): 19 | Commits (past 90d): 3 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -41,7 +41,7 @@ class LeakyReLU(Layer):\n         return K.relu(inputs, alpha=self.alpha)\n \n     def get_config(self):\n-        config = {'alpha': self.alpha}\n+        config = {'alpha': float(self.alpha)}\n         base_config = super(LeakyReLU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#be6503a8a81873109cf830bef225bf06a13edda2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4328 | Contributors (this commit): 50 | Commits (past 90d): 19 | Contributors (cumulative): 50 | DMM Complexity: None\n\nDIFF:\n@@ -100,7 +100,7 @@ def _standardize_input_data(data, names, shapes=None,\n         if len(names) > 1:\n             # Case: model expects multiple inputs but only received\n             # a single Numpy array.\n-            raise ValueError('The model expects ' + str(len(names)) +\n+            raise ValueError('The model expects ' + str(len(names)) + ' ' +\n                              exception_prefix +\n                              ' arrays, but only received one array. '\n                              'Found: array with shape ' + str(data.shape))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5ca5699b00881f6995dda787efdb1df461014b67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 8528 | Contributors (this commit): 84 | Commits (past 90d): 30 | Contributors (cumulative): 93 | DMM Complexity: None\n\nDIFF:\n@@ -63,7 +63,7 @@ def set_floatx(floatx):\n     \"\"\"Sets the default float type.\n \n     # Arguments\n-        String: 'float16', 'float32', or 'float64'.\n+        floatx: String, 'float16', 'float32', or 'float64'.\n \n     # Example\n     ```python\n\n@@ -856,7 +856,7 @@ def update(x, new_x):\n     \"\"\"Update the value of `x` to `new_x`.\n \n     # Arguments\n-        x: A Variable.\n+        x: A `Variable`.\n         new_x: A tensor of same shape as `x`.\n \n     # Returns\n@@ -869,7 +869,7 @@ def update_add(x, increment):\n     \"\"\"Update the value of `x` by adding `increment`.\n \n     # Arguments\n-        x: A Variable.\n+        x: A `Variable`.\n         increment: A tensor of same shape as `x`.\n \n     # Returns\n@@ -882,7 +882,7 @@ def update_sub(x, decrement):\n     \"\"\"Update the value of `x` by subtracting `decrement`.\n \n     # Arguments\n-        x: A Variable.\n+        x: A `Variable`.\n         decrement: A tensor of same shape as `x`.\n \n     # Returns\n@@ -895,12 +895,13 @@ def moving_average_update(x, value, momentum):\n     \"\"\"Compute the moving average of a variable.\n \n     # Arguments\n-        x: A Variable.\n-        value: A tensor with the same shape as `variable`.\n+        x: A `Variable`.\n+        value: A tensor with the same shape as `x`.\n         momentum: The moving average momentum.\n \n     # Returns\n-        An Operation to update the variable.\"\"\"\n+        An operation to update the variable.\n+    \"\"\"\n     return moving_averages.assign_moving_average(\n         x, value, momentum, zero_debias=False)\n \n@@ -2088,9 +2089,6 @@ def one_hot(indices, num_classes):\n     # Returns\n         (n + 1)D one hot representation of the input\n         with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`\n-\n-    # Returns\n-        The one-hot tensor.\n     \"\"\"\n     return tf.one_hot(indices, depth=num_classes, axis=-1)\n \n@@ -3655,7 +3653,7 @@ def foldr(fn, elems, initializer=None, name=None):\n         name: A string name for the foldr node in the graph\n \n     # Returns\n-        Same type and shape as initializer\n+        Tensor with same type and shape as `initializer`.\n     \"\"\"\n     return tf.foldr(fn, elems, initializer=initializer, name=name)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
