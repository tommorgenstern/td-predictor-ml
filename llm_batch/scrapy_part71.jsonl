{"custom_id": "scrapy#41036af6437ff0b3a9cbea692d544bfbfea6b2a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 55 | Files Changed: 3 | Hunks: 18 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/3 | Churn Δ: 114 | Churn Cumulative: 171 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,13 +5,21 @@ from os.path import join, dirname, abspath, exists\n from scrapy.spider import spiders\n from scrapy.command import ScrapyCommand\n from scrapy.conf import settings\n-from scrapy.utils.misc import render_templatefile, string_camelcase\n+from scrapy.utils.template import render_templatefile, string_camelcase\n \n+def sanitize_module_name(module_name):\n+    \"\"\"Sanitize the given module name, by replacing dashes with underscores and\n+    prefixing it with a letter if it doesn't start with one\n+    \"\"\"\n+    module_name = module_name.replace('-', '_')\n+    if module_name[0] not in string.letters:\n+        module_name = \"a\" + module_name\n+    return module_name\n \n class Command(ScrapyCommand):\n \n     def syntax(self):\n-        return \"[options] <spider_name> <spider_domain_name>\"\n+        return \"[options] <spider_module_name> <spider_domain_name>\"\n \n     def short_desc(self):\n         return \"Generate new spider based on template passed with --template\"\n@@ -27,45 +35,39 @@ class Command(ScrapyCommand):\n         if len(args) < 2:\n             return False\n \n-        template_file = join(settings['TEMPLATES_DIR'], 'spider_%s.tmpl' % opts.template)\n+        template_file = join(settings['TEMPLATES_DIR'], 'spider_%s.tmpl' % \\\n+            opts.template)\n         if not exists(template_file):\n-            print \"Template '%s.tmpl' not found\" % opts.template\n+            print \"Unable to create spider: template %r not found.\" % opts.template\n+            print \"Use genspider --list to see all available templates.\"\n             return\n \n-        name = self.normalize_name(args[0])\n+        module = sanitize_module_name(args[0])\n         domain = args[1]\n-        spiders_dict = spiders.asdict()\n-        if domain in spiders_dict.keys():\n-            if opts.force:\n-                print \"Spider '%s' already exists. Overwriting it...\" % domain\n-            else:\n-                print \"Spider '%s' already exists\" % domain\n+        spider = spiders.fromdomain(domain)\n+        if spider and not opts.force:\n+            print \"Spider '%s' already exists in module:\" % domain\n+            print \"  %s\" % spider.__module__\n             return\n-        self._genspider(name, domain, template_file)\n+        self._genspider(module, domain, opts.template, template_file)\n \n-    def normalize_name(self, name):\n-        # - are replaced by _, for valid python modules\n-        name = name.replace('-', '_')\n-        # name must start with a letter, for valid python modules\n-        if name[0] not in string.letters:\n-            name = \"a\" + name\n-            print \"Spider names must start with a letter; converted to %s.\" % name\n-        return name\n-        \n-    def _genspider(self, name, domain, template_file):\n+    def _genspider(self, module, domain, template_name, template_file):\n         \"\"\"Generate the spider module, based on the given template\"\"\"\n         tvars = {\n             'project_name': settings.get('PROJECT_NAME'),\n             'ProjectName': string_camelcase(settings.get('PROJECT_NAME')),\n-            'name': name,\n+            'module': module,\n             'site': domain,\n-            'classname': '%sSpider' % ''.join([s.capitalize() for s in name.split('_')])\n+            'classname': '%sSpider' % ''.join([s.capitalize() \\\n+                for s in module.split('_')])\n         }\n \n         spiders_module = __import__(settings['NEWSPIDER_MODULE'], {}, {}, [''])\n         spiders_dir = abspath(dirname(spiders_module.__file__))\n-        spider_file = '%s/%s.py' % (spiders_dir, name)\n+        spider_file = \"%s.py\" % join(spiders_dir, module)\n \n         shutil.copyfile(template_file, spider_file)\n         render_templatefile(spider_file, **tvars)\n-\n+        print \"Created spider %r using template %r in module:\" % (domain, \\\n+            template_name)\n+        print \"  %s.%s\" % (spiders_module.__name__, module)\n\n@@ -7,7 +7,6 @@ from contextlib import closing\n import os\n import re\n import gzip\n-import string\n import hashlib\n import csv\n \n@@ -128,19 +127,6 @@ def hash_values(*values):\n         hash.update(value)\n     return hash.hexdigest()\n \n-\n-def render_templatefile(path, **kwargs):\n-    with open(path, 'rb') as file:\n-        raw = file.read()\n-\n-    content = string.Template(raw).substitute(**kwargs)\n-\n-    with open(path.rstrip('.tmpl'), 'wb') as file:\n-        file.write(content)\n-    if path.endswith('.tmpl'):\n-        os.remove(path)\n-\n-\n def items_to_csv(file, items, delimiter=';', headers=None):\n     \"\"\"\n     This function takes a list of items and stores their attributes\n@@ -168,20 +154,6 @@ def items_to_csv(file, items, delimiter=';', headers=None):\n         csv_file.writerow(row)\n \n \n-CAMELCASE_INVALID_CHARS = re.compile('[^a-zA-Z]')\n-def string_camelcase(string):\n-    \"\"\" Convert a word  to its CamelCase version and remove invalid chars\n-\n-    >>> string_camelcase('lost-pound')\n-    'LostPound'\n-\n-    >>> string_camelcase('missing_images')\n-    'MissingImages'\n-\n-    \"\"\"\n-    return CAMELCASE_INVALID_CHARS.sub('', string.title())\n-\n-\n def md5sum(buffer):\n     \"\"\"Calculate the md5 checksum of a file\n \n@@ -194,7 +166,8 @@ def md5sum(buffer):\n     buffer.seek(0)\n     while 1:\n         d = buffer.read(8096)\n-        if not d: break\n+        if not d:\n+            break\n         m.update(d)\n     return m.hexdigest()\n \n\n@@ -0,0 +1,29 @@\n+\"\"\"Helper functions for working with templates\"\"\"\n+\n+import os\n+import re\n+import string\n+\n+def render_templatefile(path, **kwargs):\n+    with open(path, 'rb') as file:\n+        raw = file.read()\n+\n+    content = string.Template(raw).substitute(**kwargs)\n+\n+    with open(path.rstrip('.tmpl'), 'wb') as file:\n+        file.write(content)\n+    if path.endswith('.tmpl'):\n+        os.remove(path)\n+\n+CAMELCASE_INVALID_CHARS = re.compile('[^a-zA-Z]')\n+def string_camelcase(string):\n+    \"\"\" Convert a word  to its CamelCase version and remove invalid chars\n+\n+    >>> string_camelcase('lost-pound')\n+    'LostPound'\n+\n+    >>> string_camelcase('missing_images')\n+    'MissingImages'\n+\n+    \"\"\"\n+    return CAMELCASE_INVALID_CHARS.sub('', string.title())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4f06f6ef41d90ebb4e9b2aa1caa874cb3885261e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 40 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import warnings\n+from functools import wraps\n \n \n def deprecated(use_instead=None):\n@@ -6,19 +7,20 @@ def deprecated(use_instead=None):\n     as deprecated. It will result in a warning being emitted\n     when the function is used.\"\"\"\n \n-    def wraps(func):\n+    def wrapped(func):\n+        @wraps(func)\n         def new_func(*args, **kwargs):\n             message = \"Call to deprecated function %s.\" % func.__name__\n             if use_instead:\n                 message += \" Use %s instead.\" % use_instead\n \n-            warnings.warn_explicit(\n-                message,\n+            warnings.warn_explicit(message,\n                 category=DeprecationWarning,\n                 filename=func.func_code.co_filename,\n-                lineno=func.func_code.co_firstlineno + 1\n-            )\n+                lineno=func.func_code.co_firstlineno + 1,\n+                registry=func.func_globals.setdefault('__warningregistry__', \\\n+                                                      {}),\n+                module_globals=func.func_globals)\n             return func(*args, **kwargs)\n         return new_func\n-    return wraps\n-\n+    return wrapped\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c81987f624ea486005c4a6eada60493561f75171", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 21 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/5 | Churn Δ: 22 | Churn Cumulative: 61 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,15 +1,10 @@\n import unittest\n from cStringIO import StringIO\n \n-from scrapy.utils.misc import hash_values, items_to_csv, load_object, arg_to_iter\n+from scrapy.utils.misc import items_to_csv, load_object, arg_to_iter\n from scrapy.item import ScrapedItem\n \n class UtilsMiscTestCase(unittest.TestCase):\n-    def test_hash_values(self):\n-        self.assertEqual(hash_values('some', 'values', 'to', 'hash'),\n-                         'f37f5dc65beaaea35af05e16e26d439fd150c576')\n-\n-        self.assertRaises(ValueError, hash_values, 'some', None, 'value')\n \n     def test_items_to_csv(self):\n         item_1 = ScrapedItem()\n\n@@ -112,21 +112,6 @@ def extract_regex(regex, text, encoding):\n     else:\n         return [remove_entities(unicode(s, encoding), keep=['lt', 'amp']) for s in strings]\n \n-def hash_values(*values):\n-    \"\"\"Hash a series of non-None values.\n-\n-    For example:\n-    >>> hash_values('some', 'values', 'to', 'hash')\n-    'f37f5dc65beaaea35af05e16e26d439fd150c576'\n-    \"\"\"\n-    hash = hashlib.sha1()\n-    for value in values:\n-        if value is None:\n-            message = \"hash_values was passed None at argument index %d\" % list(values).index(None)\n-            raise ValueError(message)\n-        hash.update(value)\n-    return hash.hexdigest()\n-\n def items_to_csv(file, items, delimiter=';', headers=None):\n     \"\"\"\n     This function takes a list of items and stores their attributes\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6dfda357ef034cfb3da8c3f76a4df7a4920e2b75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 18 | Churn Cumulative: 72 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -27,24 +27,6 @@ def arg_to_iter(arg):\n     else:\n         return [arg]\n \n-def dict_updatedefault(D, E, **F):\n-    \"\"\"\n-    updatedefault(D, E, **F) -> None.\n-\n-    Update D from E and F: for k in E: D.setdefault(k, E[k])\n-    (if E has keys else: for (k, v) in E: D.setdefault(k, v))\n-    then: for k in F: D.setdefault(k, F[k])\n-    \"\"\"\n-    for k in E:\n-        if isinstance(k, tuple):\n-            k, v = k\n-        else:\n-            v = E[k]\n-        D.setdefault(k, v)\n-\n-    for k in F:\n-        D.setdefault(k, F[k])\n-\n def memoize(cache, hash):\n     def decorator(func):\n         def wrapper(*args, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#50de128e0d7a13ce52dc04a9b0b00b3fe7b7fc84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 72 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 73 | Churn Cumulative: 152 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 0.2962962962962963\n\nDIFF:\n@@ -1,55 +1,11 @@\n import unittest\n from cStringIO import StringIO\n \n-from scrapy.utils.misc import items_to_csv, load_object, arg_to_iter\n+from scrapy.utils.misc import load_object, arg_to_iter\n from scrapy.item import ScrapedItem\n \n class UtilsMiscTestCase(unittest.TestCase):\n \n-    def test_items_to_csv(self):\n-        item_1 = ScrapedItem()\n-        item_1._hidden = '543565'\n-        item_1.id = '3213'\n-        item_1.name = 'Item 1'\n-        item_1.description = 'Really cute item'\n-        item_1.url = 'http://dummyurl.com'\n-\n-        item_2 = ScrapedItem()\n-        item_2._hidden = 'lala'\n-        item_2.id = '1234'\n-        item_2.name = 'Item 2'\n-        item_2.description = 'This item rocks!'\n-        item_2.url = 'http://dummyurl.com/2'\n-        item_2.supplier = 'A random supplier'\n-\n-        file = StringIO()\n-        items_to_csv(file, [item_1, item_2])\n-        file.reset()\n-        self.assertEqual(file.read(),\n-            '\"description\";\"id\";\"name\";\"url\"\\r\\n' +\n-            '\"Really cute item\";\"3213\";\"Item 1\";\"http://dummyurl.com\"\\r\\n' +\n-            '\"This item rocks!\";\"1234\";\"Item 2\";\"http://dummyurl.com/2\"\\r\\n')\n-\n-        file = StringIO()\n-        items_to_csv(file, [item_2, item_1])\n-        file.reset()\n-        self.assertEqual(file.read(),\n-            '\"description\";\"id\";\"name\";\"supplier\";\"url\"\\r\\n' +\n-            '\"This item rocks!\";\"1234\";\"Item 2\";\"A random supplier\";\"http://dummyurl.com/2\"\\r\\n' +\n-            '\"Really cute item\";\"3213\";\"Item 1\";\"\";\"http://dummyurl.com\"\\r\\n')\n-\n-        file = StringIO()\n-        items_to_csv(file, [item_1, item_2], headers=['id', 'name'], delimiter=',')\n-        file.reset()\n-        self.assertEqual(file.read(),\n-            '\"id\",\"name\"\\r\\n' +\n-            '\"3213\",\"Item 1\"\\r\\n' +\n-            '\"1234\",\"Item 2\"\\r\\n')\n-\n-        file = StringIO()\n-        items_to_csv(file, [])\n-        self.assertEqual(file.tell(), 0)\n-\n     def test_load_object(self):\n         obj = load_object('scrapy.utils.misc.load_object')\n         assert obj is load_object\n\n@@ -94,33 +94,6 @@ def extract_regex(regex, text, encoding):\n     else:\n         return [remove_entities(unicode(s, encoding), keep=['lt', 'amp']) for s in strings]\n \n-def items_to_csv(file, items, delimiter=';', headers=None):\n-    \"\"\"\n-    This function takes a list of items and stores their attributes\n-    in a csv file given in 'file' (which can be either a descriptor, or a filename).\n-    The saved attributes are either the ones found in the 'headers' parameter\n-    (if specified) or the first item's list of public attributes.\n-    The written file will be encoded as utf-8.\n-    \"\"\"\n-    if not items or not hasattr(items, '__iter__'):\n-        return\n-\n-    if isinstance(file, basestring):\n-        file = open(file, 'ab+')\n-    csv_file = csv.writer(file, delimiter=delimiter, quoting=csv.QUOTE_ALL)\n-    header = headers or sorted([key for key in items[0].__dict__.keys() if not key.startswith('_')])\n-    if not file.tell():\n-        csv_file.writerow(header)\n-\n-    for item in items:\n-        row = []\n-        for attrib in header:\n-            value = getattr(item, attrib, None)\n-            value = unicode_to_str(value) if isinstance(value, basestring) else value\n-            row.append(value)\n-        csv_file.writerow(row)\n-\n-\n def md5sum(buffer):\n     \"\"\"Calculate the md5 checksum of a file\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e71764e7072c8e2208690d269432f3ebec4f8474", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 53 | Files Changed: 1 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 55 | Churn Cumulative: 154 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,20 +1,10 @@\n-\"\"\"\n-Auxiliary functions which doesn't fit anywhere else\n-\"\"\"\n-from __future__ import with_statement\n-from contextlib import closing\n+\"\"\"Helper functions which doesn't fit anywhere else\"\"\"\n \n-import os\n import re\n-import gzip\n import hashlib\n-import csv\n \n-from twisted.internet import defer\n-\n-from scrapy.utils.python import flatten, unicode_to_str\n+from scrapy.utils.python import flatten\n from scrapy.utils.markup import remove_entities\n-from scrapy.utils.defer import defer_succeed\n \n def arg_to_iter(arg):\n     \"\"\"Convert an argument to an iterable. The argument can be a None, single\n@@ -27,25 +17,6 @@ def arg_to_iter(arg):\n     else:\n         return [arg]\n \n-def memoize(cache, hash):\n-    def decorator(func):\n-        def wrapper(*args, **kwargs):\n-            key = hash(*args, **kwargs)\n-            if key in cache:\n-                return defer_succeed(cache[key])\n-\n-            def _store(_):\n-                cache[key] = _\n-                return _\n-\n-            result = func(*args, **kwargs)\n-            if isinstance(result, defer.Deferred):\n-                return result.addBoth(_store)\n-            cache[key] = result\n-            return result\n-        return wrapper\n-    return decorator\n-\n def load_object(path):\n     \"\"\"Load an object given its absolute object path, and return it.\n \n@@ -111,25 +82,3 @@ def md5sum(buffer):\n         m.update(d)\n     return m.hexdigest()\n \n-\n-def gzip_file(logfile):\n-    \"\"\"Gzip a file in place, just like gzip unix command\n-\n-    >>> import gzip\n-    >>> import tempfile\n-    >>> logfile = tempfile.mktemp()\n-    >>> handle = open(logfile, 'wb')\n-    >>> handle.write('something to compress')\n-    >>> handle.close()\n-    >>> logfile_gz = gzip_file(logfile)\n-    >>> gzip.open(logfile_gz).read()\n-    'something to compress'\n-    \"\"\"\n-    logfile_gz = '%s.gz' % logfile\n-    with closing(gzip.open(logfile_gz, 'wb')) as f_out:\n-        with open(logfile) as f_in:\n-            f_out.writelines(f_in)\n-    os.remove(logfile)\n-    return logfile_gz\n-\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d6da7eb04fb9bcaf19f9f415a7c73f1bd7a76d84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 155 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -41,7 +41,6 @@ def load_object(path):\n         raise NameError, \"Module '%s' doesn't define any object named '%s'\" % (module, name)\n \n     return obj\n-load_class = load_object # backwards compatibility, but isnt going to be available for too long.\n \n def extract_regex(regex, text, encoding):\n     \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#29710461f62c39ad3b8bef5d65f6502a6d6179de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 29 | Files Changed: 1 | Hunks: 10 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 57 | Churn Cumulative: 67 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,17 +4,14 @@ enable this middleware and enable the ROBOTSTXT_OBEY setting.\n \n \"\"\"\n \n-import urlparse\n import robotparser\n \n from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy.core.engine import scrapyengine\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.spider import spiders\n+from scrapy.core.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n-from scrapy.core.exceptions import IgnoreRequest\n from scrapy.conf import settings\n \n class RobotsTxtMiddleware(object):\n@@ -25,39 +22,41 @@ class RobotsTxtMiddleware(object):\n             raise NotConfigured\n \n         self._parsers = {}\n-        self._spiderdomains = {}\n+        self._spider_netlocs = {}\n+        self._useragents = {}\n         self._pending = {}\n         dispatcher.connect(self.domain_open, signals.domain_open)\n         dispatcher.connect(self.domain_closed, signals.domain_closed)\n \n     def process_request(self, request, spider):\n-        agent = getattr(spider, 'user_agent', None) or settings['USER_AGENT']\n-        rp = self.robot_parser(request.url, spider.domain_name)\n-        if rp and not rp.can_fetch(agent, request.url):\n+        useragent = self._useragents[spider]\n+        rp = self.robot_parser(request.url, spider)\n+        if rp and not rp.can_fetch(useragent, request.url):\n             raise IgnoreRequest(\"URL forbidden by robots.txt: %s\" % request.url)\n \n-    def robot_parser(self, url, spiderdomain):\n-        parsedurl = urlparse.urlparse(url)\n-        urldomain = parsedurl.hostname\n-        if urldomain in self._parsers:\n-            return self._parsers[urldomain]\n-        else:\n-            self._parsers[urldomain] = None\n-            robotsurl = \"%s://%s/robots.txt\" % parsedurl[0:2]\n+    def robot_parser(self, url, spider):\n+        netloc = url.netloc\n+        if netloc not in self._parsers:\n+            self._parsers[netloc] = None\n+            robotsurl = \"%s://%s/robots.txt\" % (url.scheme, url.netloc)\n             robotsreq = Request(robotsurl, priority=self.DOWNLOAD_PRIORITY)\n-            dfd = scrapyengine.download(robotsreq, spiders.fromdomain(spiderdomain))\n-            dfd.addCallbacks(callback=self._parse_robots, callbackArgs=[urldomain])\n-            self._spiderdomains[spiderdomain].add(urldomain)\n+            dfd = scrapyengine.download(robotsreq, spider)\n+            dfd.addCallback(self._parse_robots)\n+            self._spider_netlocs[spider].add(netloc)\n+        return self._parsers[netloc]\n \n-    def _parse_robots(self, response, urldomain):\n-        rp = robotparser.RobotFileParser()\n-        rp.parse(response.to_string().splitlines())\n-        self._parsers[urldomain] = rp\n+    def _parse_robots(self, response):\n+        rp = robotparser.RobotFileParser(response.url)\n+        rp.parse(response.body.splitlines())\n+        self._parsers[response.url.netloc] = rp\n \n-    def domain_open(self, domain):\n-        self._spiderdomains[domain] = set()\n+    def domain_open(self, spider):\n+        self._spider_netlocs[spider] = set()\n+        self._useragents[spider] = getattr(spider, 'user_agent', None) \\\n+            or settings['USER_AGENT']\n \n-    def domain_closed(self, domain):\n-        for urldomain in self._spiderdomains[domain]:\n-            del self._parsers[urldomain]\n-        del self._spiderdomains[domain]\n+    def domain_closed(self, domain, spider):\n+        for netloc in self._spider_netlocs[domain]:\n+            del self._parsers[netloc]\n+        del self._spider_netlocs[domain]\n+        del self._useragents[spider]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9ae3a1946d414b1eb820fb46435fa5566dccf5d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 110 | Lines Deleted: 154 | Files Changed: 17 | Hunks: 57 | Methods Changed: 35 | Complexity Δ (Sum/Max): 83/28 | Churn Δ: 264 | Churn Cumulative: 718 | Contributors (this commit): 3 | Commits (past 90d): 38 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,6 +5,8 @@ import hmac\n import base64\n import hashlib\n from urlparse import urlsplit\n+\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.conf import settings\n \n \n@@ -82,7 +84,8 @@ class AWSMiddleware(object):\n         self.secret_key = settings['AWS_SECRET_ACCESS_KEY'] or os.environ.get('AWS_SECRET_ACCESS_KEY')\n \n     def process_request(self, request, spider):\n+        hostname = urlparse_cached(request).hostname\n         if spider.domain_name == 's3.amazonaws.com' \\\n-                or (request.url.hostname and request.url.hostname.endswith('s3.amazonaws.com')):\n+                or (hostname and hostname.endswith('s3.amazonaws.com')):\n             request.headers['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime())\n             sign_request(request, self.access_key, self.secret_key)\n\n@@ -9,11 +9,12 @@ from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy import log\n-from scrapy.http import Response, Headers\n+from scrapy.http import Headers\n from scrapy.core.exceptions import NotConfigured, IgnoreRequest\n from scrapy.core.downloader.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.http import headers_dict_to_raw, headers_raw_to_dict\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.conf import settings\n \n \n@@ -55,7 +56,7 @@ class HttpCacheMiddleware(object):\n \n \n def is_cacheable(request):\n-    return request.url.scheme in ['http', 'https']\n+    return urlparse_cached(request).scheme in ['http', 'https']\n \n \n class Cache(object):\n\n@@ -12,6 +12,7 @@ from scrapy.core import signals\n from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.conf import settings\n \n class RobotsTxtMiddleware(object):\n@@ -48,7 +49,7 @@ class RobotsTxtMiddleware(object):\n     def _parse_robots(self, response):\n         rp = robotparser.RobotFileParser(response.url)\n         rp.parse(response.body.splitlines())\n-        self._parsers[response.url.netloc] = rp\n+        self._parsers[urlparse_cached(response).netloc] = rp\n \n     def domain_open(self, spider):\n         self._spider_netlocs[spider] = set()\n\n@@ -6,8 +6,6 @@ from __future__ import with_statement\n import urlparse\n \n from twisted.internet import reactor\n-from twisted.web import error as web_error\n-\n try:\n     from twisted.internet import ssl\n except ImportError:\n@@ -18,11 +16,12 @@ from scrapy.core import signals\n from scrapy.http import Headers\n from scrapy.core.exceptions import NotSupported\n from scrapy.utils.defer import defer_succeed\n-from scrapy.conf import settings\n-\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.core.downloader.dnscache import DNSCache\n from scrapy.core.downloader.responsetypes import responsetypes\n-from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory as HTTPClientFactory\n+from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory\n+from scrapy.conf import settings\n+\n \n default_timeout = settings.getint('DOWNLOAD_TIMEOUT')\n ssl_supported = 'ssl' in optional_features\n@@ -31,7 +30,7 @@ ssl_supported = 'ssl' in optional_features\n dnscache = DNSCache()\n \n def download_any(request, spider):\n-    scheme = request.url.scheme\n+    scheme = urlparse_cached(request).scheme\n     if scheme == 'http':\n         return download_http(request, spider)\n     elif scheme == 'https':\n@@ -39,21 +38,16 @@ def download_any(request, spider):\n             return download_https(request, spider)\n         else:\n             raise NotSupported(\"HTTPS not supported: install pyopenssl library\")\n-    elif request.url.scheme == 'file':\n+    elif scheme == 'file':\n         return download_file(request, spider)\n     else:\n-        raise NotSupported(\"Unsupported URL scheme '%s' in: <%s>\" % (request.url.scheme, request.url))\n+        raise NotSupported(\"Unsupported URL scheme '%s' in: <%s>\" % (scheme, request.url))\n \n def create_factory(request, spider):\n     \"\"\"Return HTTPClientFactory for the given Request\"\"\"\n     url = urlparse.urldefrag(request.url)[0]\n     timeout = getattr(spider, \"download_timeout\", None) or default_timeout\n-\n-    factory = HTTPClientFactory(url=url, # never pass unicode urls to twisted\n-                                method=request.method,\n-                                body=request.body or None, # see http://dev.scrapy.org/ticket/60\n-                                headers=request.headers,\n-                                timeout=timeout)\n+    factory = ScrapyHTTPClientFactory.from_request(request, timeout)\n \n     def _create_response(body):\n         body = body or ''\n@@ -71,16 +65,18 @@ def create_factory(request, spider):\n def download_http(request, spider):\n     \"\"\"Return a deferred for the HTTP download\"\"\"\n     factory = create_factory(request, spider)\n-    ip = dnscache.get(request.url.hostname)\n-    port = request.url.port\n+    url = urlparse_cached(request)\n+    ip = dnscache.get(url.hostname)\n+    port = url.port\n     reactor.connectTCP(ip, port or 80, factory)\n     return factory.deferred\n \n def download_https(request, spider):\n     \"\"\"Return a deferred for the HTTPS download\"\"\"\n     factory = create_factory(request, spider)\n-    ip = dnscache.get(request.url.hostname)\n-    port = request.url.port\n+    url = urlparse_cached(request)\n+    ip = dnscache.get(url.hostname)\n+    port = url.port\n     contextFactory = ssl.ClientContextFactory()\n     reactor.connectSSL(ip, port or 443, factory, contextFactory)\n     return factory.deferred\n\n@@ -1,30 +1,26 @@\n-from urlparse import urlunparse\n+from urlparse import urlparse, urlunparse\n \n from twisted.python import failure\n from twisted.web.client import HTTPClientFactory, PartialDownloadError\n from twisted.web.http import HTTPClient\n from twisted.internet import defer\n \n-from scrapy.http import Url, Headers\n-\n-\n-def _parse(url):\n-    url = url.strip()\n-    try:\n-        parsed = url.parsedurl\n-    except AttributeError:\n-        parsed = Url(url.strip()).parsedurl\n+from scrapy.http import Headers\n+from scrapy.utils.httpobj import urlparse_cached\n \n+def _parsed_url_args(parsed):\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     host = parsed.hostname\n     port = parsed.port\n     scheme = parsed.scheme\n-\n     if port is None:\n         port = 443 if scheme == 'https' else 80\n-\n     return scheme, host, port, path\n \n+def _parse(url):\n+    url = url.strip()\n+    parsed = urlparse(url)\n+    return _parsed_url_args(parsed)\n \n class ScrapyHTTPPageGetter(HTTPClient):\n \n@@ -81,11 +77,15 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n     waiting = 1\n     noisy = False\n \n-    def __init__(self, url, method='GET', body=None, headers=None, timeout=0):\n+    def __init__(self, url, method='GET', body=None, headers=None, timeout=0, parsedurl=None):\n         self.url = url\n         self.method = method\n         self.body = body or None\n+        if parsedurl:\n+            self.scheme, self.host, self.port, self.path = _parsed_url_args(parsedurl)\n+        else:\n             self.scheme, self.host, self.port, self.path = _parse(url)\n+\n         self.timeout = timeout\n         self.headers = Headers(headers or {})\n         self.deferred = defer.Deferred()\n@@ -99,6 +99,16 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n             # just in case a broken http/1.1 decides to keep connection alive\n             self.headers.setdefault(\"Connection\", \"close\")\n \n+    @classmethod\n+    def from_request(cls, request, timeout):\n+        return cls(request.url,\n+            method=request.method,\n+            body=request.body or None, # see http://dev.scrapy.org/ticket/60\n+            headers=Headers(request.headers or {}),\n+            timeout=timeout,\n+            parsedurl=urlparse_cached(request),\n+            )\n+\n     def gotHeaders(self, headers):\n         self.response_headers = headers\n \n\n@@ -2,10 +2,9 @@\n Module containing all HTTP related classes\n \n Use this module (instead of the more specific ones) when importing Headers,\n-Request, Response and Url outside this module.\n+Request and Response outside this module.\n \"\"\"\n \n-from scrapy.http.url import Url\n from scrapy.http.headers import Headers\n \n from scrapy.http.request import Request\n\n@@ -1,5 +1,6 @@\n-from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy, Cookie\n+from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy\n \n+from scrapy.utils.httpobj import urlparse_cached\n \n class CookieJar(object):\n     def __init__(self, policy=None):\n@@ -67,10 +68,10 @@ class WrappedRequest(object):\n         return self.request.url\n \n     def get_host(self):\n-        return self.request.url.netloc\n+        return urlparse_cached(self.request).netloc\n \n     def get_type(self):\n-        return self.request.url.scheme\n+        return urlparse_cached(self.request).scheme\n \n     def is_unverifiable(self):\n         \"\"\"Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.\n\n@@ -9,7 +9,6 @@ import copy\n \n from twisted.internet import defer\n \n-from scrapy.http.url import Url\n from scrapy.http.headers import Headers\n from scrapy.utils.url import safe_url_string\n \n@@ -58,9 +57,7 @@ class Request(object):\n     def _set_url(self, url):\n         if isinstance(url, basestring):\n             decoded_url = url if isinstance(url, unicode) else url.decode(self.encoding)\n-            self._url = Url(safe_url_string(decoded_url, self.encoding))\n-        elif isinstance(url, Url):\n-            self._url = url\n+            self._url = safe_url_string(decoded_url, self.encoding)\n         else:\n             raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n \n\n@@ -7,7 +7,6 @@ See documentation in docs/ref/request-response.rst\n \n import copy\n \n-from scrapy.http.url import Url\n from scrapy.http.headers import Headers\n \n class Response(object):\n@@ -16,7 +15,7 @@ class Response(object):\n         'flags', '_cache', '__weakref__']\n \n     def __init__(self, url, status=200, headers=None, body='', meta=None, flags=None):\n-        self.url = Url(url)\n+        self.url = url\n         self.headers = Headers(headers or {})\n         self.status = int(status)\n         self._set_body(body)\n\n@@ -1,57 +0,0 @@\n-\"\"\"\n-The Url class is similar to the urlparse.ParseResult class (it has the same\n-attributes) with the following differences:\n-\n-- it inherits from str\n-- it's lazy, so it only parses the url when needed\n-\"\"\"\n-\n-import urlparse\n-\n-class Url(str):\n-\n-    @property\n-    def parsedurl(self):\n-        if not hasattr(self, '_parsedurl'):\n-            self._parsedurl = urlparse.urlparse(self)\n-        return self._parsedurl\n-\n-    @property\n-    def scheme(self):\n-        return self.parsedurl.scheme\n-\n-    @property\n-    def netloc(self):\n-        return self.parsedurl.netloc\n-\n-    @property\n-    def path(self):\n-        return self.parsedurl.path\n-\n-    @property\n-    def params(self):\n-        return self.parsedurl.params\n-\n-    @property\n-    def query(self):\n-        return self.parsedurl.query\n-\n-    @property\n-    def fragment(self):\n-        return self.parsedurl.fragment\n-\n-    @property\n-    def username(self):\n-        return self.parsedurl.username\n-\n-    @property\n-    def password(self):\n-        return self.parsedurl.password\n-\n-    @property\n-    def hostname(self):\n-        return self.parsedurl.hostname\n-\n-    @property\n-    def port(self):\n-        return self.parsedurl.port\n\n@@ -1,8 +1,9 @@\n import unittest\n-from cStringIO import StringIO\n import cgi\n+from cStringIO import StringIO\n+from urlparse import urlparse\n \n-from scrapy.http import Request, FormRequest, XmlRpcRequest, Headers, Url, Response\n+from scrapy.http import Request, FormRequest, XmlRpcRequest, Headers, Response\n \n class RequestTest(unittest.TestCase):\n \n@@ -10,17 +11,17 @@ class RequestTest(unittest.TestCase):\n         # Request requires url in the constructor\n         self.assertRaises(Exception, Request)\n \n-        # url argument must be basestring or Url\n+        # url argument must be basestring\n         self.assertRaises(TypeError, Request, 123)\n-        r = Request(Url('http://www.example.com'))\n+        r = Request('http://www.example.com')\n \n         r = Request(\"http://www.example.com\")\n-        assert isinstance(r.url, Url)\n+        assert isinstance(r.url, str)\n         self.assertEqual(r.url, \"http://www.example.com\")\n         self.assertEqual(r.method, \"GET\")\n \n         r.url = \"http://www.example.com/other\"\n-        assert isinstance(r.url, Url)\n+        assert isinstance(r.url, str)\n \n         assert isinstance(r.headers, Headers)\n         self.assertEqual(r.headers, {})\n@@ -28,7 +29,6 @@ class RequestTest(unittest.TestCase):\n \n         meta = {\"lala\": \"lolo\"}\n         headers = {\"caca\": \"coco\"}\n-        body = \"a body\"\n         r = Request(\"http://www.example.com\", meta=meta, headers=headers, body=\"a body\")\n \n         assert r.meta is not meta\n@@ -224,9 +224,9 @@ class FormRequestTest(unittest.TestCase):\n         response = Response(\"http://www.example.com/this/list.html\", body=respbody)\n         r1 = FormRequest.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})\n         self.assertEqual(r1.method, 'GET')\n-        self.assertEqual(r1.url.hostname, \"www.example.com\")\n-        self.assertEqual(r1.url.path, \"/this/get.php\")\n-        urlargs = cgi.parse_qs(r1.url.query)\n+        self.assertEqual(urlparse(r1.url).hostname, \"www.example.com\")\n+        self.assertEqual(urlparse(r1.url).path, \"/this/get.php\")\n+        urlargs = cgi.parse_qs(urlparse(r1.url).query)\n         self.assertEqual(set(urlargs['test']), set(['val1', 'val2']))\n         self.assertEqual(set(urlargs['one']), set(['two', 'three']))\n         self.assertEqual(urlargs['test2'], ['xxx'])\n\n@@ -1,5 +1,5 @@\n import unittest\n-from scrapy.http import Response, TextResponse, HtmlResponse, XmlResponse, Headers, Url\n+from scrapy.http import Response, TextResponse, HtmlResponse, XmlResponse, Headers\n \n class ResponseTest(unittest.TestCase):\n \n@@ -14,7 +14,7 @@ class ResponseTest(unittest.TestCase):\n         self.assertTrue(isinstance(Response('http://example.com/', headers={}, status=200, body=''), Response))\n \n         r = Response(\"http://www.example.com\")\n-        assert isinstance(r.url, Url)\n+        assert isinstance(r.url, str)\n         self.assertEqual(r.url, \"http://www.example.com\")\n         self.assertEqual(r.status, 200)\n \n\n@@ -1,35 +0,0 @@\n-import unittest\n-from scrapy.http import Url\n-\n-class UrlClassTest(unittest.TestCase):\n-\n-    def test_url_attributes(self):\n-        u = Url(\"http://scrapy.org/wiki/info\")\n-        self.assertEqual(\"scrapy.org\", u.hostname)\n-        self.assertEqual(\"/wiki/info\", u.path)\n-        self.assertEqual(None, u.username)\n-        self.assertEqual(None, u.password)\n-\n-        u = Url(\"http://scrapy.org:8080/wiki/info\")\n-        self.assertEqual(\"scrapy.org\", u.hostname)\n-        self.assertEqual(\"/wiki/info\", u.path)\n-        self.assertEqual(8080, u.port)\n-        self.assertEqual(None, u.username)\n-        self.assertEqual(None, u.password)\n-\n-        u = Url(\"http://someuser:somepass@example.com/ticket/query?owner=pablo\")\n-        self.assertEqual(\"someuser\", u.username)\n-        self.assertEqual(\"somepass\", u.password)\n-        self.assertEqual(\"example.com\", u.hostname)\n-        self.assertEqual(\"/ticket/query\", u.path)\n-        self.assertEqual(\"owner=pablo\", u.query)\n-\n-        u = Url(\"http://example.com/somepage.html#fragment-1\")\n-        self.assertEqual(\"fragment-1\", u.fragment)\n-\n-        u = Url(\"file:///home/pablo/file.txt\")\n-        self.assertEqual(\"/home/pablo/file.txt\", u.path)\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n-\n\n@@ -0,0 +1,26 @@\n+import unittest\n+import urlparse\n+\n+from scrapy.http import Request\n+from scrapy.utils.httpobj import urlparse_cached\n+\n+class HttpobjUtilsTest(unittest.TestCase):\n+\n+    def test_urlparse_cached(self):\n+        url = \"http://www.example.com/index.html\"\n+        request1 = Request(url)\n+        request2 = Request(url)\n+        req1a = urlparse_cached(request1)\n+        req1b = urlparse_cached(request1)\n+        req2 = urlparse_cached(request2)\n+        urlp = urlparse.urlparse(url)\n+\n+        assert req1a == req2\n+        assert req1a == urlp\n+        assert req1a is req1b\n+        assert req1a is not req2\n+        assert req1a is not req2\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n@@ -13,14 +13,14 @@ from twisted.python.filepath import FilePath\n from twisted.protocols.policies import WrappingFactory\n \n from scrapy.core.downloader import webclient as client\n-from scrapy.http import Url, Headers\n+from scrapy.http import Headers\n \n \n class ParseUrlTestCase(unittest.TestCase):\n     \"\"\"Test URL parsing facility and defaults values.\"\"\"\n \n     def _parse(self, url):\n-        f = client.ScrapyHTTPClientFactory(Url(url))\n+        f = client.ScrapyHTTPClientFactory(url)\n         return (f.scheme, f.host, f.port, f.path)\n \n     def testParse(self):\n\n@@ -0,0 +1,14 @@\n+\"\"\"Helper functions for scrapy.http objects (Request, Response)\"\"\"\n+\n+import weakref\n+\n+from urlparse import urlparse\n+\n+_urlparse_cache = weakref.WeakKeyDictionary()\n+def urlparse_cached(request_or_response):\n+    \"\"\"Return urlparse.urlparse caching the result, where the argument can be a\n+    Request or Response object\n+    \"\"\"\n+    if request_or_response not in _urlparse_cache:\n+        _urlparse_cache[request_or_response] = urlparse(request_or_response.url)\n+    return _urlparse_cache[request_or_response]\n\n@@ -7,6 +7,7 @@ import hashlib\n from base64 import urlsafe_b64encode\n \n from scrapy.utils.url import canonicalize_url\n+from scrapy.utils.httpobj import urlparse_cached\n \n def request_fingerprint(request, include_headers=()):\n     \"\"\"\n@@ -78,9 +79,9 @@ def request_httprepr(request):\n     bytes that will be send when performing the request (that's controlled\n     by Twisted).\n     \"\"\"\n-\n+    hostname = urlparse_cached(request).hostname\n     s  = \"%s %s HTTP/1.1\\r\\n\" % (request.method, request.url)\n-    s += \"Host: %s\\r\\n\" % request.url.hostname\n+    s += \"Host: %s\\r\\n\" % hostname\n     if request.headers:\n         s += request.headers.to_string() + \"\\r\\n\"\n     s += \"\\r\\n\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#49cc1b1e7ec521d68d875a4f360642d54cdf64c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 30 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n \"\"\"Helper functions for working with templates\"\"\"\n+from __future__ import with_statement\n \n import os\n import re\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4b910881daf63ec8078d712c787790e51d995608", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 53 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,9 +11,10 @@ class UtilsMemoryTestCase(unittest.TestCase):\n             raise unittest.SkipTest('/proc filesystem not supported')\n         vmsize = get_vmvalue_from_procfs('VmSize')\n         vmrss = get_vmvalue_from_procfs('VmRSS')\n-        assert vmsize > 0\n-        assert vmrss > 0\n-        assert vmsize > vmrss\n+        self.assert_(isinstance(vmsize, int))\n+        self.assert_(vmsize > 0)\n+        self.assert_(vmrss > 0)\n+        self.assert_(vmsize > vmrss)\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -21,5 +21,5 @@ def get_vmvalue_from_procfs(vmkey='VmSize', pid=None):\n     if len(v) < 3:\n         return 0  # invalid format?\n     # convert Vm value to bytes\n-    return float(v[1]) * _vmvalue_scale[v[2]]\n+    return int(v[1]) * _vmvalue_scale[v[2]]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ec8f934a8e5bba1e2bac3238ce3b450e46beb03a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 11 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,7 @@ from optparse import OptionParser\n import re\n \n import scrapy\n-from scrapy.utils.misc import render_templatefile, string_camelcase\n+from scrapy.utils.template import render_templatefile, string_camelcase\n from scrapy.utils.python import ignore_patterns, copytree\n \n usage = \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7f304614106398a0da0f9278fefc6b31115b304c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 38 | Files Changed: 8 | Hunks: 16 | Methods Changed: 8 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 83 | Churn Cumulative: 470 | Contributors (this commit): 4 | Commits (past 90d): 53 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -70,21 +70,23 @@ DOWNLOADER_STATS = True\n \n DUPEFILTER_CLASS = 'scrapy.contrib.dupefilter.RequestFingerprintDupeFilter'\n \n-EXTENSIONS = [\n-    'scrapy.stats.corestats.CoreStats',\n-    'scrapy.management.web.WebConsole',\n-    'scrapy.management.telnet.TelnetConsole',\n-    'scrapy.contrib.webconsole.scheduler.SchedulerQueue',\n-    'scrapy.contrib.webconsole.livestats.LiveStats',\n-    'scrapy.contrib.webconsole.spiderctl.Spiderctl',\n-    'scrapy.contrib.webconsole.enginestatus.EngineStatus',\n-    'scrapy.contrib.webconsole.stats.StatsDump',\n-    'scrapy.contrib.spider.reloader.SpiderReloader',\n-    'scrapy.contrib.memusage.MemoryUsage',\n-    'scrapy.contrib.memdebug.MemoryDebugger',\n-    'scrapy.contrib.closedomain.CloseDomain',\n-    'scrapy.contrib.debug.StackTraceDump',\n-]\n+EXTENSIONS = {}\n+\n+EXTENSIONS_BASE = {\n+    'scrapy.stats.corestats.CoreStats': 0,\n+    'scrapy.management.web.WebConsole': 0,\n+    'scrapy.management.telnet.TelnetConsole': 0,\n+    'scrapy.contrib.webconsole.scheduler.SchedulerQueue': 0,\n+    'scrapy.contrib.webconsole.livestats.LiveStats': 0,\n+    'scrapy.contrib.webconsole.spiderctl.Spiderctl': 0,\n+    'scrapy.contrib.webconsole.enginestatus.EngineStatus': 0,\n+    'scrapy.contrib.webconsole.stats.StatsDump': 0,\n+    'scrapy.contrib.spider.reloader.SpiderReloader': 0,\n+    'scrapy.contrib.memusage.MemoryUsage': 0,\n+    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n+    'scrapy.contrib.closedomain.CloseDomain': 0,\n+    'scrapy.contrib.debug.StackTraceDump': 0,\n+}\n \n GROUPSETTINGS_ENABLED = False\n GROUPSETTINGS_MODULE = ''\n\n@@ -12,7 +12,7 @@ from scrapy.http import Request, Response\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import mustbe_deferred\n-from scrapy.utils.middleware import build_middleware_list\n+from scrapy.utils.conf import build_component_list\n from scrapy.conf import settings\n \n class DownloaderMiddlewareManager(object):\n@@ -36,7 +36,7 @@ class DownloaderMiddlewareManager(object):\n \n     def load(self):\n         \"\"\"Load middleware defined in settings module\"\"\"\n-        mwlist = build_middleware_list(settings['DOWNLOADER_MIDDLEWARES_BASE'],\n+        mwlist = build_component_list(settings['DOWNLOADER_MIDDLEWARES_BASE'], \\\n             settings['DOWNLOADER_MIDDLEWARES'])\n         self.enabled.clear()\n         self.disabled.clear()\n\n@@ -13,7 +13,7 @@ from scrapy.http import Response\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import mustbe_deferred\n-from scrapy.utils.middleware import build_middleware_list\n+from scrapy.utils.conf import build_component_list\n from scrapy.conf import settings\n \n class SchedulerMiddlewareManager(object):\n@@ -27,7 +27,7 @@ class SchedulerMiddlewareManager(object):\n \n     def load(self):\n         \"\"\"Load middleware defined in settings module\"\"\"\n-        mwlist = build_middleware_list(settings['SCHEDULER_MIDDLEWARES_BASE'],\n+        mwlist = build_component_list(settings['SCHEDULER_MIDDLEWARES_BASE'], \\\n             settings['SCHEDULER_MIDDLEWARES'])\n         self.enabled.clear()\n         self.disabled.clear()\n\n@@ -5,6 +5,7 @@ ExtensionManager (extensions) to be used as singleton.\n \"\"\"\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n+from scrapy.utils.conf import build_component_list\n from scrapy import log\n from scrapy.conf import settings\n \n@@ -23,7 +24,9 @@ class ExtensionManager(object):\n         self.loaded = False\n         self.enabled.clear()\n         self.disabled.clear()\n-        for extension_path in settings.getlist('EXTENSIONS'):\n+        extlist = build_component_list(settings['EXTENSIONS_BASE'], \\\n+            settings['EXTENSIONS'])\n+        for extension_path in extlist:\n             try:\n                 cls = load_object(extension_path)\n                 self.enabled[cls.__name__] = cls()\n\n@@ -9,7 +9,7 @@ docs/topics/spider-middleware.rst\n from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n-from scrapy.utils.middleware import build_middleware_list\n+from scrapy.utils.conf import build_component_list\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.http import Request\n from scrapy.conf import settings\n@@ -37,7 +37,7 @@ class SpiderMiddlewareManager(object):\n \n     def load(self):\n         \"\"\"Load middleware defined in settings module\"\"\"\n-        mwlist = build_middleware_list(settings['SPIDER_MIDDLEWARES_BASE'],\n+        mwlist = build_component_list(settings['SPIDER_MIDDLEWARES_BASE'], \\\n             settings['SPIDER_MIDDLEWARES'])\n         self.enabled.clear()\n         self.disabled.clear()\n\n@@ -1,17 +1,17 @@\n import unittest\n \n-from scrapy.utils.middleware import build_middleware_list\n+from scrapy.utils.conf import build_component_list\n \n-class UtilsMiddlewareTestCase(unittest.TestCase):\n+class UtilsConfTestCase(unittest.TestCase):\n \n-    def test_build_middleware_list(self):\n+    def test_build_component_list(self):\n         base = {'one': 1, 'two': 2, 'three': 3, 'five': 5, 'six': None}\n         custom = {'two': None, 'three': 8, 'four': 4}\n-        self.assertEqual(build_middleware_list(base, custom),\n+        self.assertEqual(build_component_list(base, custom),\n                          ['one', 'four', 'five', 'three'])\n \n         custom = ['a', 'b', 'c']\n-        self.assertEqual(build_middleware_list(base, custom), custom)\n+        self.assertEqual(build_component_list(base, custom), custom)\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -0,0 +1,13 @@\n+from operator import itemgetter\n+\n+def build_component_list(base, custom):\n+    \"\"\"Compose a component list based on a custom and base dict of components\n+    (typically middlewares or extensions), unless custom is already a list, in\n+    which case it's returned.\n+    \"\"\"\n+    if isinstance(custom, (list, tuple)):\n+        return custom\n+    compdict = base.copy()\n+    compdict.update(custom)\n+    return [k for k, v in sorted(compdict.items(), key=itemgetter(1)) \\\n+        if v is not None]\n\n@@ -1,11 +0,0 @@\n-from operator import itemgetter\n-\n-def build_middleware_list(base, custom):\n-    \"\"\"Compose a middleware list based on a custom and base dict of\n-    middlewares, unless custom is already a list, in which case it's returned.\n-    \"\"\"\n-    if isinstance(custom, (list, tuple)):\n-        return custom\n-    mwdict = base.copy()\n-    mwdict.update(custom)\n-    return [k for k, v in sorted(mwdict.items(), key=itemgetter(1)) if v is not None]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
