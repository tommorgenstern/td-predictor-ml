{"custom_id": "scrapy#517ed0749b81b234dd99f3c2538dc64c7513a80f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1734 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -332,7 +332,7 @@ class Stream:\n \n     def receive_headers(self, headers: List[HeaderTuple]) -> None:\n         for name, value in headers:\n-            self._response['headers'][name] = value\n+            self._response['headers'].appendlist(name, value)\n \n         # Check if we exceed the allowed max data size which can be received\n         expected_size = int(self._response['headers'].get(b'Content-Length', -1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fd6742e8117eb76f4a717b33b24790c1419bd838", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 135 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -7,17 +7,27 @@ from scrapy.utils.display import pformat, pprint\n \n class TestDisplay(TestCase):\n     object = {'a': 1}\n-    colorized_string = (\n+    colorized_strings = {\n+        (\n+            (\n                 \"{\\x1b[33m'\\x1b[39;49;00m\\x1b[33ma\\x1b[39;49;00m\\x1b[33m'\"\n-        \"\\x1b[39;49;00m: \\x1b[34m1\\x1b[39;49;00m}\\n\"\n+                \"\\x1b[39;49;00m: \\x1b[34m1\\x1b[39;49;00m}\"\n             )\n+            + suffix\n+        )\n+        for suffix in (\n+            # https://github.com/pygments/pygments/issues/2313\n+            \"\\n\",  # pygments ≤ 2.13\n+            \"\\x1b[37m\\x1b[39;49;00m\\n\",  # pygments ≥ 2.14\n+        )\n+    }\n     plain_string = \"{'a': 1}\"\n \n     @mock.patch('sys.platform', 'linux')\n     @mock.patch(\"sys.stdout.isatty\")\n     def test_pformat(self, isatty):\n         isatty.return_value = True\n-        self.assertEqual(pformat(self.object), self.colorized_string)\n+        self.assertIn(pformat(self.object), self.colorized_strings)\n \n     @mock.patch(\"sys.stdout.isatty\")\n     def test_pformat_dont_colorize(self, isatty):\n@@ -33,7 +43,7 @@ class TestDisplay(TestCase):\n     def test_pformat_old_windows(self, isatty, version):\n         isatty.return_value = True\n         version.return_value = '10.0.14392'\n-        self.assertEqual(pformat(self.object), self.colorized_string)\n+        self.assertIn(pformat(self.object), self.colorized_strings)\n \n     @mock.patch('sys.platform', 'win32')\n     @mock.patch('scrapy.utils.display._enable_windows_terminal_processing')\n@@ -53,7 +63,7 @@ class TestDisplay(TestCase):\n         isatty.return_value = True\n         version.return_value = '10.0.14393'\n         terminal_processing.return_value = True\n-        self.assertEqual(pformat(self.object), self.colorized_string)\n+        self.assertIn(pformat(self.object), self.colorized_strings)\n \n     @mock.patch('sys.platform', 'linux')\n     @mock.patch(\"sys.stdout.isatty\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#724b0332878b5df502dc9504aec69c47fb1ea0bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 36 | Churn Cumulative: 1101 | Contributors (this commit): 28 | Commits (past 90d): 13 | Contributors (cumulative): 40 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,7 +26,7 @@ from twisted.python import failure\n from twisted.python.failure import Failure\n \n from scrapy.exceptions import IgnoreRequest\n-from scrapy.utils.reactor import is_asyncio_reactor_installed, get_asyncio_event_loop_policy\n+from scrapy.utils.reactor import is_asyncio_reactor_installed, _get_asyncio_event_loop\n \n \n def defer_fail(_failure: Failure) -> Deferred:\n@@ -267,7 +267,7 @@ def deferred_from_coro(o) -> Any:\n             # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n             return ensureDeferred(o)\n         # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n-        event_loop = get_asyncio_event_loop_policy().get_event_loop()\n+        event_loop = _get_asyncio_event_loop()\n         return Deferred.fromFuture(asyncio.ensure_future(o, loop=event_loop))\n     return o\n \n@@ -318,8 +318,7 @@ def deferred_to_future(d: Deferred) -> Future:\n                 d = treq.get('https://example.com/additional')\n                 additional_response = await deferred_to_future(d)\n     \"\"\"\n-    policy = get_asyncio_event_loop_policy()\n-    return d.asFuture(policy.get_event_loop())\n+    return d.asFuture(_get_asyncio_event_loop())\n \n \n def maybe_deferred_to_future(d: Deferred) -> Union[Deferred, Future]:\n\n@@ -1,6 +1,7 @@\n import asyncio\n import sys\n from contextlib import suppress\n+from warnings import catch_warnings, filterwarnings\n \n from twisted.internet import asyncioreactor, error\n \n@@ -81,6 +82,10 @@ def install_reactor(reactor_path, event_loop_path=None):\n             installer()\n \n \n+def _get_asyncio_event_loop():\n+    return set_asyncio_event_loop(None)\n+\n+\n def set_asyncio_event_loop(event_loop_path):\n     \"\"\"Sets and returns the event loop with specified import path.\"\"\"\n     policy = get_asyncio_event_loop_policy()\n@@ -90,11 +95,26 @@ def set_asyncio_event_loop(event_loop_path):\n         asyncio.set_event_loop(event_loop)\n     else:\n         try:\n+            with catch_warnings():\n+                # In Python 3.10.9, 3.11.1, 3.12 and 3.13, a DeprecationWarning\n+                # is emitted about the lack of a current event loop, because in\n+                # Python 3.14 and later `get_event_loop` will raise a\n+                # RuntimeError in that event. Because our code is already\n+                # prepared for that future behavior, we ignore the deprecation\n+                # warning.\n+                filterwarnings(\n+                    \"ignore\",\n+                    message=\"There is no current event loop\",\n+                    category=DeprecationWarning,\n+                )\n                 event_loop = policy.get_event_loop()\n         except RuntimeError:\n-            # `get_event_loop` is expected to fail when called from a new thread\n-            # with no asyncio event loop yet installed. Such is the case when\n-            # called from `scrapy shell`\n+            # `get_event_loop` raises RuntimeError when called with no asyncio\n+            # event loop yet installed in the following scenarios:\n+            # - From a thread other than the main thread. For example, when\n+            #   using ``scrapy shell``.\n+            # - Previsibly on Python 3.14 and later.\n+            #   https://github.com/python/cpython/issues/100160#issuecomment-1345581902\n             event_loop = policy.new_event_loop()\n             asyncio.set_event_loop(event_loop)\n     return event_loop\n\n@@ -14,6 +14,9 @@ class AsyncioTest(TestCase):\n         self.assertEqual(is_asyncio_reactor_installed(), self.reactor_pytest == 'asyncio')\n \n     def test_install_asyncio_reactor(self):\n+        from twisted.internet import reactor as original_reactor\n         with warnings.catch_warnings(record=True) as w:\n             install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n             self.assertEqual(len(w), 0)\n+        from twisted.internet import reactor\n+        assert original_reactor == reactor\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#16546564f673b0d58c55584870149424abc73fee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 775 | Contributors (this commit): 35 | Commits (past 90d): 7 | Contributors (cumulative): 35 | DMM Complexity: None\n\nDIFF:\n@@ -19,7 +19,7 @@ def has_environment_marker_platform_impl_support():\n \n install_requires = [\n     'Twisted>=18.9.0',\n-    'cryptography>=3.3',\n+    'cryptography>=3.4.6',\n     'cssselect>=0.9.1',\n     'itemloaders>=1.0.1',\n     'parsel>=1.5.0',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1ab900659e32b6792aabfb30a51ce117d2578cfc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6828 | Contributors (this commit): 36 | Commits (past 90d): 9 | Contributors (cumulative): 36 | DMM Complexity: None\n\nDIFF:\n@@ -700,7 +700,7 @@ class InstrumentedFeedSlot(_FeedSlot):\n class IsExportingListener:\n     \"\"\"When subscribed to InstrumentedFeedSlot, keeps track of when\n     a call to start_exporting has been made without a closing call to\n-    finish_exporting and when a call to finis_exporting has been made\n+    finish_exporting and when a call to finish_exporting has been made\n     before a call to start_exporting.\"\"\"\n     def __init__(self):\n         self.start_without_finish = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d9f6de6bf5d3b59e4230b7920a079db5c8cc5ffa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 17 | Churn Cumulative: 2896 | Contributors (this commit): 32 | Commits (past 90d): 7 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -214,6 +214,12 @@ class LargeChunkedFileResource(resource.Resource):\n         return server.NOT_DONE_YET\n \n \n+class DuplicateHeaderResource(resource.Resource):\n+    def render(self, request):\n+        request.responseHeaders.setRawHeaders(b\"Set-Cookie\", [b\"a=b\", b\"c=d\"])\n+        return b\"\"\n+\n+\n class HttpTestCase(unittest.TestCase):\n     scheme = 'http'\n     download_handler_cls: Type = HTTPDownloadHandler\n@@ -239,6 +245,7 @@ class HttpTestCase(unittest.TestCase):\n         r.putChild(b\"contentlength\", ContentLengthHeaderResource())\n         r.putChild(b\"nocontenttype\", EmptyContentTypeHeaderResource())\n         r.putChild(b\"largechunkedfile\", LargeChunkedFileResource())\n+        r.putChild(b\"duplicate-header\", DuplicateHeaderResource())\n         r.putChild(b\"echo\", Echo())\n         self.site = server.Site(r, timeout=None)\n         self.wrapper = WrappingFactory(self.site)\n@@ -407,6 +414,16 @@ class HttpTestCase(unittest.TestCase):\n             HtmlResponse,\n         )\n \n+    def test_get_duplicate_header(self):\n+        def _test(response):\n+            self.assertEqual(\n+                response.headers.getlist(b'Set-Cookie'),\n+                [b'a=b', b'c=d'],\n+            )\n+\n+        request = Request(self.getURL('duplicate-header'))\n+        return self.download_request(request, Spider('foo')).addCallback(_test)\n+\n \n class Http10TestCase(HttpTestCase):\n     \"\"\"HTTP 1.0 test case\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b0ec38876d52febb6811348961b75247ab4fe1c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 392 | Contributors (this commit): 17 | Commits (past 90d): 4 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -195,7 +195,8 @@ class LxmlLinkExtractor:\n         Only links that match the settings passed to the ``__init__`` method of\n         the link extractor are returned.\n \n-        Duplicate links are omitted.\n+        Duplicate links are omitted if the ``unique`` parameter is set to ``True``, \n+        otherwise they are returned.\n         \"\"\"\n         base_url = get_base_url(response)\n         if self.restrict_xpaths:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b386c648643529f3f2d9e4d1783632840e6fe270", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 3684 | Contributors (this commit): 31 | Commits (past 90d): 12 | Contributors (cumulative): 38 | DMM Complexity: 1.0\n\nDIFF:\n@@ -541,6 +541,7 @@ class TextResponseTest(BaseResponseTest):\n             'http://example.com/sample2.html',\n             'http://example.com/sample3.html',\n             'http://example.com/sample3.html',\n+            'http://example.com/sample3.html',\n             'http://example.com/sample3.html#foo',\n             'http://www.google.com/something',\n             'http://example.com/innertag.html'\n\n@@ -53,6 +53,7 @@ class Base:\n                 Link(url='http://example.com/sample2.html', text='sample 2'),\n                 Link(url='http://example.com/sample3.html', text='sample 3 text'),\n                 Link(url='http://example.com/sample3.html', text='sample 3 repetition'),\n+                Link(url='http://example.com/sample3.html', text='sample 3 repetition'),\n                 Link(url='http://example.com/sample3.html#foo', text='sample 3 repetition with fragment')\n             ])\n \n@@ -64,6 +65,7 @@ class Base:\n                 Link(url='http://example.com/sample2.html', text='sample 2'),\n                 Link(url='http://example.com/sample3.html', text='sample 3 text'),\n                 Link(url='http://example.com/sample3.html', text='sample 3 repetition'),\n+                Link(url='http://example.com/sample3.html', text='sample 3 repetition'),\n                 Link(url='http://example.com/sample3.html', text='sample 3 repetition with fragment')\n             ])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#faa5bd0f6b688eaef18a6245f2d703e8fc8ff684", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 394 | Contributors (this commit): 18 | Commits (past 90d): 5 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -195,7 +195,7 @@ class LxmlLinkExtractor:\n         Only links that match the settings passed to the ``__init__`` method of\n         the link extractor are returned.\n \n-        Duplicate links are omitted if the ``unique`` parameter is set to ``True``, \n+        Duplicate links are omitted if the ``unique`` attribute is set to ``True``, \n         otherwise they are returned.\n         \"\"\"\n         base_url = get_base_url(response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#93ad6a4bc2fd2d453a37961b817a8a2a85f589c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 16 | Files Changed: 3 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 21 | Churn Cumulative: 999 | Contributors (this commit): 21 | Commits (past 90d): 9 | Contributors (cumulative): 31 | DMM Complexity: 0.5\n\nDIFF:\n@@ -19,8 +19,8 @@ METHOD_TLSv12 = 'TLSv1.2'\n openssl_methods = {\n     METHOD_TLS: SSL.SSLv23_METHOD,      # protocol negotiation (recommended)\n     METHOD_TLSv10: SSL.TLSv1_METHOD,    # TLS 1.0 only\n-    METHOD_TLSv11: getattr(SSL, 'TLSv1_1_METHOD', 5),   # TLS 1.1 only\n-    METHOD_TLSv12: getattr(SSL, 'TLSv1_2_METHOD', 6),   # TLS 1.2 only\n+    METHOD_TLSv11: SSL.TLSv1_1_METHOD,  # TLS 1.1 only\n+    METHOD_TLSv12: SSL.TLSv1_2_METHOD,  # TLS 1.2 only\n }\n \n \n\n@@ -1,14 +1,9 @@\n-import OpenSSL\n+import OpenSSL.SSL\n import OpenSSL._util as pyOpenSSLutil\n \n from scrapy.utils.python import to_unicode\n \n \n-# The OpenSSL symbol is present since 1.1.1 but it's not currently supported in any version of pyOpenSSL.\n-# Using the binding directly, as this code does, requires cryptography 2.4.\n-SSL_OP_NO_TLSv1_3 = getattr(pyOpenSSLutil.lib, 'SSL_OP_NO_TLSv1_3', 0)\n-\n-\n def ffi_buf_to_string(buf):\n     return to_unicode(pyOpenSSLutil.ffi.string(buf))\n \n@@ -22,9 +17,6 @@ def x509name_to_string(x509name):\n \n \n def get_temp_key_info(ssl_object):\n-    if not hasattr(pyOpenSSLutil.lib, 'SSL_get_server_tmp_key'):  # requires OpenSSL 1.0.2\n-        return None\n-\n     # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()\n     temp_key_p = pyOpenSSLutil.ffi.new(\"EVP_PKEY **\")\n     if not pyOpenSSLutil.lib.SSL_get_server_tmp_key(ssl_object, temp_key_p):\n@@ -55,7 +47,5 @@ def get_temp_key_info(ssl_object):\n \n \n def get_openssl_version():\n-    system_openssl = OpenSSL.SSL.SSLeay_version(\n-        OpenSSL.SSL.SSLEAY_VERSION\n-    ).decode('ascii', errors='replace')\n+    system_openssl = OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION)\n     return f'{OpenSSL.version.__version__} ({system_openssl})'\n\n@@ -19,7 +19,6 @@ from twisted.web.static import File\n from twisted.web.util import redirectTo\n \n from scrapy.utils.python import to_bytes, to_unicode\n-from scrapy.utils.ssl import SSL_OP_NO_TLSv1_3\n from scrapy.utils.test import get_testenv\n \n \n@@ -350,7 +349,7 @@ def ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.c\n     if cipher_string:\n         ctx = factory.getContext()\n         # disabling TLS1.3 because it unconditionally enables some strong ciphers\n-        ctx.set_options(SSL.OP_CIPHER_SERVER_PREFERENCE | SSL_OP_NO_TLSv1_3)\n+        ctx.set_options(SSL.OP_CIPHER_SERVER_PREFERENCE | SSL.OP_NO_TLSv1_3)\n         ctx.set_cipher_list(to_bytes(cipher_string))\n     return factory\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fb52918d23b37cd2581aa39ebbd422b017ec8051", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 480 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -61,7 +61,9 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n     # kept for old-style HTTP/1.0 downloader context twisted calls,\n     # e.g. connectSSL()\n     def getContext(self, hostname=None, port=None):\n-        return self.getCertificateOptions().getContext()\n+        ctx = self.getCertificateOptions().getContext()\n+        ctx.set_options(0x4)  # OP_LEGACY_SERVER_CONNECT\n+        return ctx\n \n     def creatorForNetloc(self, hostname, port):\n         return ScrapyClientTLSOptions(hostname.decode(\"ascii\"), self.getContext(),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#43ab8bd16acdabc8be64b421b030fb6e1751e47b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 137 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -47,5 +47,7 @@ def get_temp_key_info(ssl_object):\n \n \n def get_openssl_version():\n-    system_openssl = OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION)\n+    system_openssl = OpenSSL.SSL.SSLeay_version(\n+        OpenSSL.SSL.SSLEAY_VERSION\n+    ).decode('ascii', errors='replace')\n     return f'{OpenSSL.version.__version__} ({system_openssl})'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#caaeb235a08a250a9438c97259f2a74ae2ca8bbd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 234 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,4 @@\n from functools import wraps\n-from collections import OrderedDict\n \n \n def _embed_ipython_shell(namespace={}, banner=''):\n@@ -63,12 +62,12 @@ def _embed_standard_shell(namespace={}, banner=''):\n     return wrapper\n \n \n-DEFAULT_PYTHON_SHELLS = OrderedDict([\n-    ('ptpython', _embed_ptpython_shell),\n-    ('ipython', _embed_ipython_shell),\n-    ('bpython', _embed_bpython_shell),\n-    ('python', _embed_standard_shell),\n-])\n+DEFAULT_PYTHON_SHELLS = {\n+    'ptpython': _embed_ptpython_shell,\n+    'ipython': _embed_ipython_shell,\n+    'bpython': _embed_bpython_shell,\n+    'python': _embed_standard_shell,\n+}\n \n \n def get_shell_embed_func(shells=None, known_shells=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#50500a6b2897418405b459b6a2a80a7a5b0a6f29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 18 | Files Changed: 8 | Hunks: 18 | Methods Changed: 12 | Complexity Δ (Sum/Max): -2/1 | Churn Δ: 66 | Churn Cumulative: 7830 | Contributors (this commit): 57 | Commits (past 90d): 29 | Contributors (cumulative): 147 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,6 +9,7 @@ import logging\n from twisted.internet.defer import Deferred, maybeDeferred\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n+from scrapy.http.request import NO_CALLBACK\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import load_object\n@@ -65,7 +66,8 @@ class RobotsTxtMiddleware:\n             robotsreq = Request(\n                 robotsurl,\n                 priority=self.DOWNLOAD_PRIORITY,\n-                meta={'dont_obey_robotstxt': True}\n+                meta={'dont_obey_robotstxt': True},\n+                callback=NO_CALLBACK,\n             )\n             dfd = self.crawler.engine.download(robotsreq)\n             dfd.addCallback(self._parse_robots, netloc, spider)\n\n@@ -20,6 +20,17 @@ from scrapy.utils.url import escape_ajax\n \n RequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n \n+#: When assigned to the ``callback`` parameter of\n+#: :class:`~scrapy.http.Request`, it indicates that the request it not meant to\n+#: have a spider callback at all.\n+#:\n+#: This value should be used by :ref:`components <topics-components>`\n+#: that create and handle their own requests, e.g. through\n+#: :meth:`scrapy.core.engine.ExecutionEngine.download`, so that download\n+#: middlewares handling such requests can treat them differently from requests\n+#: intended for the :meth:`~scrapy.Spider.parse` callback.\n+NO_CALLBACK = object()\n+\n \n class Request(object_ref):\n     \"\"\"Represents an HTTP request, which is usually generated in a Spider and\n@@ -63,12 +74,8 @@ class Request(object_ref):\n             raise TypeError(f\"Request priority not an integer: {priority!r}\")\n         self.priority = priority\n \n-        if callback is not None and not callable(callback):\n-            raise TypeError(f'callback must be a callable, got {type(callback).__name__}')\n-        if errback is not None and not callable(errback):\n-            raise TypeError(f'errback must be a callable, got {type(errback).__name__}')\n-        self.callback = callback\n-        self.errback = errback\n+        self._set_xback(\"callback\", callback)\n+        self._set_xback(\"errback\", errback)\n \n         self.cookies = cookies or {}\n         self.headers = Headers(headers or {}, encoding=encoding)\n@@ -78,6 +85,15 @@ class Request(object_ref):\n         self._cb_kwargs = dict(cb_kwargs) if cb_kwargs else None\n         self.flags = [] if flags is None else list(flags)\n \n+    def _set_xback(self, name: str, value: Optional[Callable]) -> None:\n+        if (\n+            value is not None\n+            and (name != \"callback\" or value is not NO_CALLBACK)\n+            and not callable(value)\n+        ):\n+            raise TypeError(f'{name} must be a callable, got {type(value).__name__}')\n+        setattr(self, name, value)\n+\n     @property\n     def cb_kwargs(self) -> dict:\n         if self._cb_kwargs is None:\n\n@@ -7,6 +7,7 @@ from warnings import warn\n from twisted.internet.defer import Deferred, DeferredList\n from twisted.python.failure import Failure\n \n+from scrapy.http.request import NO_CALLBACK\n from scrapy.settings import Settings\n from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.defer import mustbe_deferred, defer_result\n@@ -93,7 +94,7 @@ class MediaPipeline:\n         fp = self._fingerprinter.fingerprint(request)\n         cb = request.callback or (lambda _: _)\n         eb = request.errback\n-        request.callback = None\n+        request.callback = NO_CALLBACK\n         request.errback = None\n \n         # Return cached result if request was already seen\n\n@@ -8,6 +8,7 @@ from scrapy.downloadermiddlewares.robotstxt import (RobotsTxtMiddleware,\n                                                     logger as mw_module_logger)\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Request, Response, TextResponse\n+from scrapy.http.request import NO_CALLBACK\n from scrapy.settings import Settings\n from tests.test_robotstxt_interface import rerp_available, reppy_available\n \n@@ -53,6 +54,7 @@ Disallow: /some/randome/page.html\n         middleware = RobotsTxtMiddleware(self._get_successful_crawler())\n         return DeferredList([\n             self.assertNotIgnored(Request('http://site.local/allowed'), middleware),\n+            maybeDeferred(self.assertRobotsTxtRequested, \"http://site.local\"),\n             self.assertIgnored(Request('http://site.local/admin/main'), middleware),\n             self.assertIgnored(Request('http://site.local/static/'), middleware),\n             self.assertIgnored(Request('http://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:'), middleware),\n@@ -183,6 +185,12 @@ Disallow: /some/randome/page.html\n         return self.assertFailure(maybeDeferred(middleware.process_request, request, spider),\n                                   IgnoreRequest)\n \n+    def assertRobotsTxtRequested(self, base_url):\n+        calls = self.crawler.engine.download.call_args_list\n+        request = calls[0][0][0]\n+        self.assertEqual(request.url, f\"{base_url}/robots.txt\")\n+        self.assertEqual(request.callback, NO_CALLBACK)\n+\n \n class RobotsTxtMiddlewareWithRerpTest(RobotsTxtMiddlewareTest):\n     if not rerp_available():\n\n@@ -7,6 +7,7 @@ from unittest import mock\n from urllib.parse import parse_qs, unquote_to_bytes, urlparse\n \n from scrapy.http import Request, FormRequest, XmlRpcRequest, JsonRequest, Headers, HtmlResponse\n+from scrapy.http.request import NO_CALLBACK\n from scrapy.utils.python import to_bytes, to_unicode\n \n \n@@ -277,6 +278,12 @@ class RequestTest(unittest.TestCase):\n         self.assertIs(r4.callback, a_function)\n         self.assertIs(r4.errback, a_function)\n \n+        r5 = self.request_class(\n+            url='http://example.com',\n+            callback=NO_CALLBACK,\n+        )\n+        self.assertIs(r5.callback, NO_CALLBACK)\n+\n     def test_callback_and_errback_type(self):\n         with self.assertRaises(TypeError):\n             self.request_class('http://example.com', callback='a_function')\n@@ -288,6 +295,8 @@ class RequestTest(unittest.TestCase):\n                 callback='a_function',\n                 errback='a_function',\n             )\n+        with self.assertRaises(TypeError):\n+            self.request_class('http://example.com', errback=NO_CALLBACK)\n \n     def test_from_curl(self):\n         # Note: more curated tests regarding curl conversion are in\n\n@@ -33,10 +33,7 @@ from scrapy.utils.test import (\n     skip_if_no_boto,\n )\n \n-\n-def _mocked_download_func(request, info):\n-    response = request.meta.get('response')\n-    return response() if callable(response) else response\n+from .test_pipeline_media import _mocked_download_func\n \n \n class FilesPipelineTestCase(unittest.TestCase):\n\n@@ -31,18 +31,13 @@ else:\n         skip_pillow = None\n \n \n-def _mocked_download_func(request, info):\n-    response = request.meta.get('response')\n-    return response() if callable(response) else response\n-\n-\n class ImagesPipelineTestCase(unittest.TestCase):\n \n     skip = skip_pillow\n \n     def setUp(self):\n         self.tempdir = mkdtemp()\n-        self.pipeline = ImagesPipeline(self.tempdir, download_func=_mocked_download_func)\n+        self.pipeline = ImagesPipeline(self.tempdir)\n \n     def tearDown(self):\n         rmtree(self.tempdir)\n\n@@ -9,6 +9,7 @@ from twisted.internet.defer import Deferred, inlineCallbacks\n \n from scrapy import signals\n from scrapy.http import Request, Response\n+from scrapy.http.request import NO_CALLBACK\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.pipelines.files import FileException\n@@ -29,6 +30,7 @@ else:\n \n \n def _mocked_download_func(request, info):\n+    assert request.callback is NO_CALLBACK\n     response = request.meta.get('response')\n     return response() if callable(response) else response\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f449ee53778b038f9ecb1feccb2fa9baa40e1f56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 44 | Churn Cumulative: 753 | Contributors (this commit): 21 | Commits (past 90d): 4 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,7 +12,9 @@ from email.mime.text import MIMEText\n from email.utils import formatdate\n from io import BytesIO\n \n+from twisted.python.versions import Version\n from twisted.internet import defer, ssl\n+from twisted import version as twisted_version\n \n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import to_bytes\n@@ -126,16 +128,11 @@ class MailSender:\n                       'mailattachs': nattachs, 'mailerr': errstr})\n \n     def _sendmail(self, to_addrs, msg):\n-        # Import twisted.mail here because it is not available in python3\n         from twisted.internet import reactor\n-        from twisted.mail.smtp import ESMTPSenderFactory\n         msg = BytesIO(msg)\n         d = defer.Deferred()\n-        factory = ESMTPSenderFactory(\n-            self.smtpuser, self.smtppass, self.mailfrom, to_addrs, msg, d,\n-            heloFallback=True, requireAuthentication=False, requireTransportSecurity=self.smtptls,\n-        )\n-        factory.noisy = False\n+\n+        factory = self._create_sender_factory(to_addrs, msg, d)\n \n         if self.smtpssl:\n             reactor.connectSSL(self.smtphost, self.smtpport, factory, ssl.ClientContextFactory())\n@@ -143,3 +140,20 @@ class MailSender:\n             reactor.connectTCP(self.smtphost, self.smtpport, factory)\n \n         return d\n+\n+    def _create_sender_factory(self, to_addrs, msg, d):\n+        from twisted.mail.smtp import ESMTPSenderFactory\n+\n+        factory_keywords = {\n+            'heloFallback': True,\n+            'requireAuthentication': False,\n+            'requireTransportSecurity': self.smtptls\n+        }\n+\n+        # Newer versions of twisted require the hostname to use STARTTLS\n+        if twisted_version >= Version('twisted', 21, 2, 0):\n+            factory_keywords['hostname'] = self.smtphost\n+\n+        factory = ESMTPSenderFactory(self.smtpuser, self.smtppass, self.mailfrom, to_addrs, msg, d, **factory_keywords)\n+        factory.noisy = False\n+        return factory\n\n@@ -4,6 +4,11 @@ import unittest\n from io import BytesIO\n from email.charset import Charset\n \n+from twisted.internet._sslverify import ClientTLSOptions\n+from twisted.internet.ssl import ClientContextFactory\n+from twisted.python.versions import Version\n+from twisted.internet import defer\n+from twisted import version as twisted_version\n from scrapy.mail import MailSender\n \n \n@@ -121,6 +126,17 @@ class MailSenderTest(unittest.TestCase):\n         self.assertEqual(text.get_charset(), Charset('utf-8'))\n         self.assertEqual(attach.get_payload(decode=True).decode('utf-8'), body)\n \n+    def test_create_sender_factory_with_host(self):\n+        mailsender = MailSender(debug=False, smtphost='smtp.testhost.com')\n+\n+        factory = mailsender._create_sender_factory(to_addrs=['test@scrapy.org'], msg='test', d=defer.Deferred())\n+\n+        context = factory.buildProtocol('test@scrapy.org').context\n+        if twisted_version >= Version('twisted', 21, 2, 0):\n+            self.assertIsInstance(context, ClientTLSOptions)\n+        else:\n+            self.assertIsInstance(context, ClientContextFactory)\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5c1559f60e459a9678eecfe24c4fea06e272dbab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 577 | Contributors (this commit): 17 | Commits (past 90d): 5 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,8 @@ requests in Scrapy.\n See documentation in docs/topics/request-response.rst\n \"\"\"\n import inspect\n-from typing import Callable, List, Optional, Tuple, Type, TypeVar, Union\n+from enum import Enum\n+from typing import Any, Callable, Final, List, Optional, Tuple, Type, TypeVar, Union\n \n from w3lib.url import safe_url_string\n \n@@ -20,6 +21,11 @@ from scrapy.utils.url import escape_ajax\n \n RequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n \n+\n+# https://github.com/python/typing/issues/689#issuecomment-561425237\n+class _NoCallback(Enum):\n+    NO_CALLBACK = 0\n+\n #: When assigned to the ``callback`` parameter of\n #: :class:`~scrapy.http.Request`, it indicates that the request it not meant to\n #: have a spider callback at all.\n@@ -29,7 +35,7 @@ RequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n #: :meth:`scrapy.core.engine.ExecutionEngine.download`, so that download\n #: middlewares handling such requests can treat them differently from requests\n #: intended for the :meth:`~scrapy.Spider.parse` callback.\n-NO_CALLBACK = object()\n+NO_CALLBACK: Final = _NoCallback.NO_CALLBACK\n \n \n class Request(object_ref):\n@@ -49,6 +55,8 @@ class Request(object_ref):\n     Currently used by :meth:`Request.replace`, :meth:`Request.to_dict` and\n     :func:`~scrapy.utils.request.request_from_dict`.\n     \"\"\"\n+    callback: Union[None, _NoCallback, Callable]\n+    errback: Optional[Callable]\n \n     def __init__(\n         self,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4242ae405d9775098607a67612b5640a4daaf840", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1357 | Contributors (this commit): 42 | Commits (past 90d): 13 | Contributors (cumulative): 52 | DMM Complexity: None\n\nDIFF:\n@@ -6,8 +6,9 @@ See documentation in docs/topics/request-response.rst\n \"\"\"\n import inspect\n from enum import Enum\n-from typing import Any, Callable, Final, List, Optional, Tuple, Type, TypeVar, Union\n+from typing import Callable, List, Optional, Tuple, Type, TypeVar, Union\n \n+from typing_extensions import Final\n from w3lib.url import safe_url_string\n \n import scrapy\n@@ -26,6 +27,7 @@ RequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n class _NoCallback(Enum):\n     NO_CALLBACK = 0\n \n+\n #: When assigned to the ``callback`` parameter of\n #: :class:`~scrapy.http.Request`, it indicates that the request it not meant to\n #: have a spider callback at all.\n\n@@ -34,6 +34,7 @@ install_requires = [\n     'packaging',\n     'tldextract',\n     'lxml>=4.3.0',\n+    'typing-extensions>=3.10.0.0',\n ]\n extras_require = {}\n cpython_dependencies = [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b1dd893fbb4d2efed3342e6c98a6bbf4b393d48e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 26 | Churn Cumulative: 2296 | Contributors (this commit): 33 | Commits (past 90d): 6 | Contributors (cumulative): 53 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,7 +14,7 @@ from contextlib import suppress\n from ftplib import FTP\n from io import BytesIO\n from pathlib import Path\n-from typing import DefaultDict, Optional, Set\n+from typing import DefaultDict, Optional, Set, Union\n from urllib.parse import urlparse\n \n from itemadapter import ItemAdapter\n@@ -41,7 +41,8 @@ class FileException(Exception):\n \n \n class FSFilesStore:\n-    def __init__(self, basedir: str):\n+    def __init__(self, basedir: Union[str, os.PathLike]):\n+        basedir = str(basedir)  # support Path object\n         if '://' in basedir:\n             basedir = basedir.split('://', 1)[1]\n         self.basedir = basedir\n@@ -65,8 +66,8 @@ class FSFilesStore:\n \n         return {'last_modified': last_modified, 'checksum': checksum}\n \n-    def _get_filesystem_path(self, path: str) -> Path:\n-        path_comps = path.split('/')\n+    def _get_filesystem_path(self, path: Union[str, os.PathLike]) -> Path:\n+        path_comps = str(path).split('/')\n         return Path(self.basedir, *path_comps)\n \n     def _mkdir(self, dirname: Path, domain: Optional[str] = None):\n\n@@ -405,6 +405,23 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n             self.assertEqual(getattr(pipeline_cls, pipe_inst_attr),\n                              expected_value)\n \n+    def test_file_pipeline_using_pathlike_objects(self):\n+\n+        class CustomFilesPipelineWithPathLikeDir(FilesPipeline):\n+            def file_path(self, request, response=None, info=None, *, item=None):\n+                return Path('subdir') / Path(request.url).name\n+\n+        pipeline = CustomFilesPipelineWithPathLikeDir.from_settings(\n+            Settings({'FILES_STORE': Path('./Temp')})\n+        )\n+        request = Request(\"http://example.com/image01.jpg\")\n+        self.assertEqual(pipeline.file_path(request), Path('subdir/image01.jpg'))\n+\n+    def test_files_store_constructor_with_pathlike_object(self):\n+        path = Path('./FileDir')\n+        fs_store = FSFilesStore(path)\n+        self.assertEqual(fs_store.basedir, str(path))\n+\n \n class TestS3FilesStore(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#818d69fa003a14a2f2058e76258e6269d227efb4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 583 | Contributors (this commit): 17 | Commits (past 90d): 7 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -29,7 +29,7 @@ class _NoCallback(Enum):\n \n \n #: When assigned to the ``callback`` parameter of\n-#: :class:`~scrapy.http.Request`, it indicates that the request it not meant to\n+#: :class:`~scrapy.http.Request`, it indicates that the request is not meant to\n #: have a spider callback at all.\n #:\n #: This value should be used by :ref:`components <topics-components>`\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8ee48174717591bbf74497fded060d7ae83d67ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 267 | Lines Deleted: 91 | Files Changed: 37 | Hunks: 87 | Methods Changed: 63 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 358 | Churn Cumulative: 40567 | Contributors (this commit): 176 | Commits (past 90d): 175 | Contributors (cumulative): 674 | DMM Complexity: 0.905982905982906\n\nDIFF:\n@@ -1,9 +1,10 @@\n \"\"\"\n-A spider that generate light requests to meassure QPS throughput\n+A spider that generate light requests to measure QPS throughput\n \n usage:\n \n-    scrapy runspider qpsclient.py --loglevel=INFO --set RANDOMIZE_DOWNLOAD_DELAY=0 --set CONCURRENT_REQUESTS=50 -a qps=10 -a latency=0.3\n+    scrapy runspider qpsclient.py --loglevel=INFO --set RANDOMIZE_DOWNLOAD_DELAY=0\n+     --set CONCURRENT_REQUESTS=50 -a qps=10 -a latency=0.3\n \n \"\"\"\n \n\n@@ -24,7 +24,7 @@ class ScrapyArgumentParser(argparse.ArgumentParser):\n \n \n def _iter_command_classes(module_name):\n-    # TODO: add `name` attribute to commands and and merge this function with\n+    # TODO: add `name` attribute to commands and merge this function with\n     # scrapy.utils.spider.iter_spider_classes\n     for module in walk_modules(module_name):\n         for obj in vars(module).values():\n\n@@ -83,7 +83,9 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n     # kept for old-style HTTP/1.0 downloader context twisted calls,\n     # e.g. connectSSL()\n     def getContext(self, hostname=None, port=None):\n-        return self.getCertificateOptions().getContext()\n+        ctx = self.getCertificateOptions().getContext()\n+        ctx.set_options(0x4)  # OP_LEGACY_SERVER_CONNECT\n+        return ctx\n \n     def creatorForNetloc(self, hostname, port):\n         return ScrapyClientTLSOptions(\n\n@@ -23,8 +23,8 @@ METHOD_TLSv12 = \"TLSv1.2\"\n openssl_methods = {\n     METHOD_TLS: SSL.SSLv23_METHOD,  # protocol negotiation (recommended)\n     METHOD_TLSv10: SSL.TLSv1_METHOD,  # TLS 1.0 only\n-    METHOD_TLSv11: getattr(SSL, \"TLSv1_1_METHOD\", 5),  # TLS 1.1 only\n-    METHOD_TLSv12: getattr(SSL, \"TLSv1_2_METHOD\", 6),  # TLS 1.2 only\n+    METHOD_TLSv11: SSL.TLSv1_1_METHOD,  # TLS 1.1 only\n+    METHOD_TLSv12: SSL.TLSv1_2_METHOD,  # TLS 1.2 only\n }\n \n \n\n@@ -101,7 +101,7 @@ class ScrapyHTTPPageGetter(HTTPClient):\n # This class used to inherit from Twisted’s\n # twisted.web.client.HTTPClientFactory. When that class was deprecated in\n # Twisted (https://github.com/twisted/twisted/pull/643), we merged its\n-# non-overriden code into this class.\n+# non-overridden code into this class.\n class ScrapyHTTPClientFactory(ClientFactory):\n \n     protocol = ScrapyHTTPPageGetter\n\n@@ -348,7 +348,7 @@ class Stream:\n \n     def receive_headers(self, headers: List[HeaderTuple]) -> None:\n         for name, value in headers:\n-            self._response[\"headers\"][name] = value\n+            self._response[\"headers\"].appendlist(name, value)\n \n         # Check if we exceed the allowed max data size which can be received\n         expected_size = int(self._response[\"headers\"].get(b\"Content-Length\", -1))\n\n@@ -384,11 +384,11 @@ class FeedExporter:\n         return defer.DeferredList(deferred_list) if deferred_list else None\n \n     def _close_slot(self, slot, spider):\n+        slot.finish_exporting()\n         if not slot.itemcount and not slot.store_empty:\n             # We need to call slot.storage.store nonetheless to get the file\n             # properly closed.\n             return defer.maybeDeferred(slot.storage.store, slot.file)\n-        slot.finish_exporting()\n         logmsg = f\"{slot.format} feed ({slot.itemcount} items) in: {slot.uri}\"\n         d = defer.maybeDeferred(slot.storage.store, slot.file)\n \n\n@@ -196,7 +196,7 @@ class RFC2616Policy:\n         if response.status in (300, 301, 308):\n             return self.MAXAGE\n \n-        # Insufficient information to compute fresshness lifetime\n+        # Insufficient information to compute freshness lifetime\n         return 0\n \n     def _compute_current_age(self, response, request, now):\n\n@@ -200,7 +200,7 @@ def _select_value(ele: SelectElement, n: str, v: str):\n         o = ele.value_options\n         return (n, o[0]) if o else (None, None)\n     if v is not None and multiple:\n-        # This is a workround to bug in lxml fixed 2.3.1\n+        # This is a workaround to bug in lxml fixed 2.3.1\n         # fix https://github.com/lxml/lxml/commit/57f49eed82068a20da3db8f1b18ae00c1bab8b12#L1L1139\n         selected_options = ele.xpath(\".//option[@selected]\")\n         values = [(o.get(\"value\") or o.text or \"\").strip() for o in selected_options]\n\n@@ -226,7 +226,8 @@ class LxmlLinkExtractor:\n         Only links that match the settings passed to the ``__init__`` method of\n         the link extractor are returned.\n \n-        Duplicate links are omitted.\n+        Duplicate links are omitted if the ``unique`` attribute is set to ``True``,\n+        otherwise they are returned.\n         \"\"\"\n         base_url = get_base_url(response)\n         if self.restrict_xpaths:\n@@ -239,4 +240,6 @@ class LxmlLinkExtractor:\n         for doc in docs:\n             links = self._extract_links(doc, response.url, response.encoding, base_url)\n             all_links.extend(self._process_links(links))\n+        if self.link_extractor.unique:\n             return unique_list(all_links)\n+        return all_links\n\n@@ -151,8 +151,8 @@ class ImagesPipeline(FilesPipeline):\n             )\n             if self._deprecated_convert_image:\n                 warnings.warn(\n-                    f\"{self.__class__.__name__}.convert_image() method overriden in a deprecated way, \"\n-                    \"overriden method does not accept response_body argument.\",\n+                    f\"{self.__class__.__name__}.convert_image() method overridden in a deprecated way, \"\n+                    \"overridden method does not accept response_body argument.\",\n                     category=ScrapyDeprecationWarning,\n                 )\n \n\n@@ -177,7 +177,11 @@ class Shell:\n \n def inspect_response(response, spider):\n     \"\"\"Open a shell to inspect the given response\"\"\"\n+    # Shell.start removes the SIGINT handler, so save it and re-add it after\n+    # the shell has closed\n+    sigint_handler = signal.getsignal(signal.SIGINT)\n     Shell(spider.crawler).start(response=response, spider=spider)\n+    signal.signal(signal.SIGINT, sigint_handler)\n \n \n def _request_deferred(request):\n\n@@ -1,5 +1,4 @@\n from functools import wraps\n-from collections import OrderedDict\n \n \n def _embed_ipython_shell(namespace={}, banner=\"\"):\n@@ -70,14 +69,12 @@ def _embed_standard_shell(namespace={}, banner=\"\"):\n     return wrapper\n \n \n-DEFAULT_PYTHON_SHELLS = OrderedDict(\n-    [\n-        (\"ptpython\", _embed_ptpython_shell),\n-        (\"ipython\", _embed_ipython_shell),\n-        (\"bpython\", _embed_bpython_shell),\n-        (\"python\", _embed_standard_shell),\n-    ]\n-)\n+DEFAULT_PYTHON_SHELLS = {\n+    \"ptpython\": _embed_ptpython_shell,\n+    \"ipython\": _embed_ipython_shell,\n+    \"bpython\": _embed_bpython_shell,\n+    \"python\": _embed_standard_shell,\n+}\n \n \n def get_shell_embed_func(shells=None, known_shells=None):\n\n@@ -26,10 +26,7 @@ from twisted.python import failure\n from twisted.python.failure import Failure\n \n from scrapy.exceptions import IgnoreRequest\n-from scrapy.utils.reactor import (\n-    is_asyncio_reactor_installed,\n-    get_asyncio_event_loop_policy,\n-)\n+from scrapy.utils.reactor import is_asyncio_reactor_installed, _get_asyncio_event_loop\n \n \n def defer_fail(_failure: Failure) -> Deferred:\n@@ -290,7 +287,7 @@ def deferred_from_coro(o) -> Any:\n             # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n             return ensureDeferred(o)\n         # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n-        event_loop = get_asyncio_event_loop_policy().get_event_loop()\n+        event_loop = _get_asyncio_event_loop()\n         return Deferred.fromFuture(asyncio.ensure_future(o, loop=event_loop))\n     return o\n \n@@ -343,8 +340,7 @@ def deferred_to_future(d: Deferred) -> Future:\n                 d = treq.get('https://example.com/additional')\n                 additional_response = await deferred_to_future(d)\n     \"\"\"\n-    policy = get_asyncio_event_loop_policy()\n-    return d.asFuture(policy.get_event_loop())\n+    return d.asFuture(_get_asyncio_event_loop())\n \n \n def maybe_deferred_to_future(d: Deferred) -> Union[Deferred, Future]:\n\n@@ -1,6 +1,7 @@\n import asyncio\n import sys\n from contextlib import suppress\n+from warnings import catch_warnings, filterwarnings\n \n from twisted.internet import asyncioreactor, error\n \n@@ -83,6 +84,10 @@ def install_reactor(reactor_path, event_loop_path=None):\n             installer()\n \n \n+def _get_asyncio_event_loop():\n+    return set_asyncio_event_loop(None)\n+\n+\n def set_asyncio_event_loop(event_loop_path):\n     \"\"\"Sets and returns the event loop with specified import path.\"\"\"\n     policy = get_asyncio_event_loop_policy()\n@@ -92,11 +97,26 @@ def set_asyncio_event_loop(event_loop_path):\n         asyncio.set_event_loop(event_loop)\n     else:\n         try:\n+            with catch_warnings():\n+                # In Python 3.10.9, 3.11.1, 3.12 and 3.13, a DeprecationWarning\n+                # is emitted about the lack of a current event loop, because in\n+                # Python 3.14 and later `get_event_loop` will raise a\n+                # RuntimeError in that event. Because our code is already\n+                # prepared for that future behavior, we ignore the deprecation\n+                # warning.\n+                filterwarnings(\n+                    \"ignore\",\n+                    message=\"There is no current event loop\",\n+                    category=DeprecationWarning,\n+                )\n                 event_loop = policy.get_event_loop()\n         except RuntimeError:\n-            # `get_event_loop` is expected to fail when called from a new thread\n-            # with no asyncio event loop yet installed. Such is the case when\n-            # called from `scrapy shell`\n+            # `get_event_loop` raises RuntimeError when called with no asyncio\n+            # event loop yet installed in the following scenarios:\n+            # - From a thread other than the main thread. For example, when\n+            #   using ``scrapy shell``.\n+            # - Previsibly on Python 3.14 and later.\n+            #   https://github.com/python/cpython/issues/100160#issuecomment-1345581902\n             event_loop = policy.new_event_loop()\n             asyncio.set_event_loop(event_loop)\n     return event_loop\n\n@@ -40,7 +40,7 @@ def get_meta_refresh(\n     response: \"scrapy.http.response.text.TextResponse\",\n     ignore_tags: Optional[Iterable[str]] = (\"script\", \"noscript\"),\n ) -> Union[Tuple[None, None], Tuple[float, str]]:\n-    \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n+    \"\"\"Parse the http-equiv refresh parameter from the given response\"\"\"\n     if response not in _metaref_cache:\n         text = response.text[0:4096]\n         _metaref_cache[response] = html.get_meta_refresh(\n\n@@ -1,14 +1,9 @@\n-import OpenSSL\n+import OpenSSL.SSL\n import OpenSSL._util as pyOpenSSLutil\n \n from scrapy.utils.python import to_unicode\n \n \n-# The OpenSSL symbol is present since 1.1.1 but it's not currently supported in any version of pyOpenSSL.\n-# Using the binding directly, as this code does, requires cryptography 2.4.\n-SSL_OP_NO_TLSv1_3 = getattr(pyOpenSSLutil.lib, \"SSL_OP_NO_TLSv1_3\", 0)\n-\n-\n def ffi_buf_to_string(buf):\n     return to_unicode(pyOpenSSLutil.ffi.string(buf))\n \n@@ -24,11 +19,6 @@ def x509name_to_string(x509name):\n \n \n def get_temp_key_info(ssl_object):\n-    if not hasattr(\n-        pyOpenSSLutil.lib, \"SSL_get_server_tmp_key\"\n-    ):  # requires OpenSSL 1.0.2\n-        return None\n-\n     # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()\n     temp_key_p = pyOpenSSLutil.ffi.new(\"EVP_PKEY **\")\n     if not pyOpenSSLutil.lib.SSL_get_server_tmp_key(ssl_object, temp_key_p):\n\n@@ -48,7 +48,7 @@ def parse_url(url, encoding=None):\n \n def escape_ajax(url):\n     \"\"\"\n-    Return the crawleable url according to:\n+    Return the crawlable url according to:\n     https://developers.google.com/webmasters/ajax-crawling/docs/getting-started\n \n     >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n\n@@ -19,7 +19,7 @@ def has_environment_marker_platform_impl_support():\n \n install_requires = [\n     \"Twisted>=18.9.0\",\n-    \"cryptography>=3.3\",\n+    \"cryptography>=3.4.6\",\n     \"cssselect>=0.9.1\",\n     \"itemloaders>=1.0.1\",\n     \"parsel>=1.5.0\",\n\n@@ -19,7 +19,6 @@ from twisted.web.static import File\n from twisted.web.util import redirectTo\n \n from scrapy.utils.python import to_bytes, to_unicode\n-from scrapy.utils.ssl import SSL_OP_NO_TLSv1_3\n from scrapy.utils.test import get_testenv\n \n \n@@ -358,7 +357,7 @@ def ssl_context_factory(\n     if cipher_string:\n         ctx = factory.getContext()\n         # disabling TLS1.3 because it unconditionally enables some strong ciphers\n-        ctx.set_options(SSL.OP_CIPHER_SERVER_PREFERENCE | SSL_OP_NO_TLSv1_3)\n+        ctx.set_options(SSL.OP_CIPHER_SERVER_PREFERENCE | SSL.OP_NO_TLSv1_3)\n         ctx.set_cipher_list(to_bytes(cipher_string))\n     return factory\n \n\n@@ -11,6 +11,6 @@ class ZeroDivisionErrorPipeline:\n         return item\n \n \n-class ProcessWithZeroDivisionErrorPipiline:\n+class ProcessWithZeroDivisionErrorPipeline:\n     def process_item(self, item, spider):\n         1 / 0\n\n@@ -336,7 +336,7 @@ class StartprojectTemplatesTest(ProjectTest):\n         self.assertEqual(actual_permissions, expected_permissions)\n \n     def test_startproject_permissions_unchanged_in_destination(self):\n-        \"\"\"Check that pre-existing folders and files in the destination folder\n+        \"\"\"Check that preexisting folders and files in the destination folder\n         do not see their permissions modified.\"\"\"\n         scrapy_path = scrapy.__path__[0]\n         project_template = Path(scrapy_path, \"templates\", \"project\")\n\n@@ -154,7 +154,7 @@ class CrawlTestCase(TestCase):\n             raise unittest.SkipTest(\"Non-existing hosts are resolvable\")\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n-            # try to fetch the homepage of a non-existent domain\n+            # try to fetch the homepage of a nonexistent domain\n             yield crawler.crawl(\n                 \"http://dns.resolution.invalid./\", mockserver=self.mockserver\n             )\n@@ -183,7 +183,7 @@ class CrawlTestCase(TestCase):\n         self.assertIs(record.exc_info[0], ZeroDivisionError)\n \n     @defer.inlineCallbacks\n-    def test_start_requests_lazyness(self):\n+    def test_start_requests_laziness(self):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n         crawler = get_crawler(BrokenStartRequestsSpider, settings)\n         yield crawler.crawl(mockserver=self.mockserver)\n\n@@ -209,6 +209,12 @@ class LargeChunkedFileResource(resource.Resource):\n         return server.NOT_DONE_YET\n \n \n+class DuplicateHeaderResource(resource.Resource):\n+    def render(self, request):\n+        request.responseHeaders.setRawHeaders(b\"Set-Cookie\", [b\"a=b\", b\"c=d\"])\n+        return b\"\"\n+\n+\n class HttpTestCase(unittest.TestCase):\n     scheme = \"http\"\n     download_handler_cls: Type = HTTPDownloadHandler\n@@ -234,6 +240,7 @@ class HttpTestCase(unittest.TestCase):\n         r.putChild(b\"contentlength\", ContentLengthHeaderResource())\n         r.putChild(b\"nocontenttype\", EmptyContentTypeHeaderResource())\n         r.putChild(b\"largechunkedfile\", LargeChunkedFileResource())\n+        r.putChild(b\"duplicate-header\", DuplicateHeaderResource())\n         r.putChild(b\"echo\", Echo())\n         self.site = server.Site(r, timeout=None)\n         self.wrapper = WrappingFactory(self.site)\n@@ -407,6 +414,16 @@ class HttpTestCase(unittest.TestCase):\n             HtmlResponse,\n         )\n \n+    def test_get_duplicate_header(self):\n+        def _test(response):\n+            self.assertEqual(\n+                response.headers.getlist(b\"Set-Cookie\"),\n+                [b\"a=b\", b\"c=d\"],\n+            )\n+\n+        request = Request(self.getURL(\"duplicate-header\"))\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n+\n \n class Http10TestCase(HttpTestCase):\n     \"\"\"HTTP 1.0 test case\"\"\"\n@@ -1095,9 +1112,9 @@ class BaseFTPTestCase(unittest.TestCase):\n \n         return self._add_test_callbacks(d, _test)\n \n-    def test_ftp_download_notexist(self):\n+    def test_ftp_download_nonexistent(self):\n         request = Request(\n-            url=f\"ftp://127.0.0.1:{self.portNum}/notexist.txt\", meta=self.req_meta\n+            url=f\"ftp://127.0.0.1:{self.portNum}/nonexistent.txt\", meta=self.req_meta\n         )\n         d = self.download_handler.download_request(request, None)\n \n\n@@ -19,7 +19,7 @@ class UserAgentMiddlewareTest(TestCase):\n         self.assertEqual(req.headers[\"User-Agent\"], b\"default_useragent\")\n \n     def test_remove_agent(self):\n-        # settings UESR_AGENT to None should remove the user agent\n+        # settings USER_AGENT to None should remove the user agent\n         spider, mw = self.get_spider_and_mw(\"default_useragent\")\n         spider.user_agent = None\n         mw.spider_opened(spider)\n\n@@ -109,7 +109,7 @@ class DataClassItemsSpider(TestSpider):\n class ItemZeroDivisionErrorSpider(TestSpider):\n     custom_settings = {\n         \"ITEM_PIPELINES\": {\n-            \"tests.pipelines.ProcessWithZeroDivisionErrorPipiline\": 300,\n+            \"tests.pipelines.ProcessWithZeroDivisionErrorPipeline\": 300,\n         }\n     }\n \n\n@@ -33,8 +33,9 @@ from zope.interface.verify import verifyObject\n \n import scrapy\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n-from scrapy.exporters import CsvItemExporter\n+from scrapy.exporters import CsvItemExporter, JsonItemExporter\n from scrapy.extensions.feedexport import (\n+    _FeedSlot,\n     BlockingFeedStorage,\n     FeedExporter,\n     FileFeedStorage,\n@@ -664,6 +665,50 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n         return result\n \n \n+class InstrumentedFeedSlot(_FeedSlot):\n+    \"\"\"Instrumented _FeedSlot subclass for keeping track of calls to\n+    start_exporting and finish_exporting.\"\"\"\n+\n+    def start_exporting(self):\n+        self.update_listener(\"start\")\n+        super().start_exporting()\n+\n+    def finish_exporting(self):\n+        self.update_listener(\"finish\")\n+        super().finish_exporting()\n+\n+    @classmethod\n+    def subscribe__listener(cls, listener):\n+        cls.update_listener = listener.update\n+\n+\n+class IsExportingListener:\n+    \"\"\"When subscribed to InstrumentedFeedSlot, keeps track of when\n+    a call to start_exporting has been made without a closing call to\n+    finish_exporting and when a call to finish_exporting has been made\n+    before a call to start_exporting.\"\"\"\n+\n+    def __init__(self):\n+        self.start_without_finish = False\n+        self.finish_without_start = False\n+\n+    def update(self, method):\n+        if method == \"start\":\n+            self.start_without_finish = True\n+        elif method == \"finish\":\n+            if self.start_without_finish:\n+                self.start_without_finish = False\n+            else:\n+                self.finish_before_start = True\n+\n+\n+class ExceptionJsonItemExporter(JsonItemExporter):\n+    \"\"\"JsonItemExporter that throws an exception every time export_item is called.\"\"\"\n+\n+    def export_item(self, _):\n+        raise Exception(\"foo\")\n+\n+\n class FeedExportTest(FeedExportTestBase):\n     __test__ = True\n \n@@ -909,6 +954,84 @@ class FeedExportTest(FeedExportTestBase):\n             data = yield self.exported_no_data(settings)\n             self.assertEqual(b\"\", data[fmt])\n \n+    @defer.inlineCallbacks\n+    def test_start_finish_exporting_items(self):\n+        items = [\n+            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n+        ]\n+        settings = {\n+            \"FEEDS\": {\n+                self._random_temp_filename(): {\"format\": \"json\"},\n+            },\n+            \"FEED_EXPORT_INDENT\": None,\n+        }\n+\n+        listener = IsExportingListener()\n+        InstrumentedFeedSlot.subscribe__listener(listener)\n+\n+        with mock.patch(\"scrapy.extensions.feedexport._FeedSlot\", InstrumentedFeedSlot):\n+            _ = yield self.exported_data(items, settings)\n+            self.assertFalse(listener.start_without_finish)\n+            self.assertFalse(listener.finish_without_start)\n+\n+    @defer.inlineCallbacks\n+    def test_start_finish_exporting_no_items(self):\n+        items = []\n+        settings = {\n+            \"FEEDS\": {\n+                self._random_temp_filename(): {\"format\": \"json\"},\n+            },\n+            \"FEED_EXPORT_INDENT\": None,\n+        }\n+\n+        listener = IsExportingListener()\n+        InstrumentedFeedSlot.subscribe__listener(listener)\n+\n+        with mock.patch(\"scrapy.extensions.feedexport._FeedSlot\", InstrumentedFeedSlot):\n+            _ = yield self.exported_data(items, settings)\n+            self.assertFalse(listener.start_without_finish)\n+            self.assertFalse(listener.finish_without_start)\n+\n+    @defer.inlineCallbacks\n+    def test_start_finish_exporting_items_exception(self):\n+        items = [\n+            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n+        ]\n+        settings = {\n+            \"FEEDS\": {\n+                self._random_temp_filename(): {\"format\": \"json\"},\n+            },\n+            \"FEED_EXPORTERS\": {\"json\": ExceptionJsonItemExporter},\n+            \"FEED_EXPORT_INDENT\": None,\n+        }\n+\n+        listener = IsExportingListener()\n+        InstrumentedFeedSlot.subscribe__listener(listener)\n+\n+        with mock.patch(\"scrapy.extensions.feedexport._FeedSlot\", InstrumentedFeedSlot):\n+            _ = yield self.exported_data(items, settings)\n+            self.assertFalse(listener.start_without_finish)\n+            self.assertFalse(listener.finish_without_start)\n+\n+    @defer.inlineCallbacks\n+    def test_start_finish_exporting_no_items_exception(self):\n+        items = []\n+        settings = {\n+            \"FEEDS\": {\n+                self._random_temp_filename(): {\"format\": \"json\"},\n+            },\n+            \"FEED_EXPORTERS\": {\"json\": ExceptionJsonItemExporter},\n+            \"FEED_EXPORT_INDENT\": None,\n+        }\n+\n+        listener = IsExportingListener()\n+        InstrumentedFeedSlot.subscribe__listener(listener)\n+\n+        with mock.patch(\"scrapy.extensions.feedexport._FeedSlot\", InstrumentedFeedSlot):\n+            _ = yield self.exported_data(items, settings)\n+            self.assertFalse(listener.start_without_finish)\n+            self.assertFalse(listener.finish_without_start)\n+\n     @defer.inlineCallbacks\n     def test_export_no_items_store_empty(self):\n         formats = (\n\n@@ -399,7 +399,7 @@ class RequestTest(unittest.TestCase):\n             )\n             self.assertEqual(r.method, \"DELETE\")\n \n-        # If `ignore_unknon_options` is set to `False` it raises an error with\n+        # If `ignore_unknown_options` is set to `False` it raises an error with\n         # the unknown options: --foo and -z\n         self.assertRaises(\n             ValueError,\n@@ -997,7 +997,7 @@ class FormRequestTest(RequestTest):\n         fs = _qs(r1)\n         self.assertEqual(fs, {b\"four\": [b\"4\"], b\"three\": [b\"3\"]})\n \n-    def test_from_response_formname_notexist(self):\n+    def test_from_response_formname_nonexistent(self):\n         response = _buildresponse(\n             \"\"\"<form name=\"form1\" action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"one\" value=\"1\">\n@@ -1044,7 +1044,7 @@ class FormRequestTest(RequestTest):\n         fs = _qs(r1)\n         self.assertEqual(fs, {b\"four\": [b\"4\"], b\"three\": [b\"3\"]})\n \n-    def test_from_response_formname_notexists_fallback_formid(self):\n+    def test_from_response_formname_nonexistent_fallback_formid(self):\n         response = _buildresponse(\n             \"\"\"<form action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"one\" value=\"1\">\n@@ -1062,7 +1062,7 @@ class FormRequestTest(RequestTest):\n         fs = _qs(r1)\n         self.assertEqual(fs, {b\"four\": [b\"4\"], b\"three\": [b\"3\"]})\n \n-    def test_from_response_formid_notexist(self):\n+    def test_from_response_formid_nonexistent(self):\n         response = _buildresponse(\n             \"\"\"<form id=\"form1\" action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"one\" value=\"1\">\n\n@@ -518,7 +518,7 @@ class TextResponseTest(BaseResponseTest):\n     def test_bom_is_removed_from_body(self):\n         # Inferring encoding from body also cache decoded body as sideeffect,\n         # this test tries to ensure that calling response.encoding and\n-        # response.text in indistint order doesn't affect final\n+        # response.text in indistinct order doesn't affect final\n         # values for encoding and decoded body.\n         url = \"http://example.com\"\n         body = b\"\\xef\\xbb\\xbfWORD\"\n@@ -645,6 +645,7 @@ class TextResponseTest(BaseResponseTest):\n             \"http://example.com/sample2.html\",\n             \"http://example.com/sample3.html\",\n             \"http://example.com/sample3.html\",\n+            \"http://example.com/sample3.html\",\n             \"http://example.com/sample3.html#foo\",\n             \"http://www.google.com/something\",\n             \"http://example.com/innertag.html\",\n\n@@ -74,6 +74,10 @@ class Base:\n                         url=\"http://example.com/sample3.html\",\n                         text=\"sample 3 repetition\",\n                     ),\n+                    Link(\n+                        url=\"http://example.com/sample3.html\",\n+                        text=\"sample 3 repetition\",\n+                    ),\n                     Link(\n                         url=\"http://example.com/sample3.html#foo\",\n                         text=\"sample 3 repetition with fragment\",\n@@ -93,6 +97,10 @@ class Base:\n                         url=\"http://example.com/sample3.html\",\n                         text=\"sample 3 repetition\",\n                     ),\n+                    Link(\n+                        url=\"http://example.com/sample3.html\",\n+                        text=\"sample 3 repetition\",\n+                    ),\n                     Link(\n                         url=\"http://example.com/sample3.html\",\n                         text=\"sample 3 repetition with fragment\",\n\n@@ -225,8 +225,8 @@ class ImagesPipelineTestCase(unittest.TestCase):\n                 self.assertEqual(buf.getvalue(), thumb_buf.getvalue())\n \n                 expected_warning_msg = (\n-                    \".convert_image() method overriden in a deprecated way, \"\n-                    \"overriden method does not accept response_body argument.\"\n+                    \".convert_image() method overridden in a deprecated way, \"\n+                    \"overridden method does not accept response_body argument.\"\n                 )\n                 self.assertEqual(\n                     len(\n@@ -244,7 +244,7 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         with warnings.catch_warnings(record=True) as w:\n             warnings.simplefilter(\"always\")\n             SIZE = (100, 100)\n-            # straigh forward case: RGB and JPEG\n+            # straight forward case: RGB and JPEG\n             COLOUR = (0, 127, 255)\n             im, _ = _create_image(\"JPEG\", \"RGB\", SIZE, COLOUR)\n             converted, _ = self.pipeline.convert_image(im)\n@@ -271,7 +271,7 @@ class ImagesPipelineTestCase(unittest.TestCase):\n             self.assertEqual(converted.mode, \"RGB\")\n             self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n \n-            # ensure that we recieved deprecation warnings\n+            # ensure that we received deprecation warnings\n             expected_warning_msg = \".convert_image() method called in a deprecated way\"\n             self.assertTrue(\n                 len(\n@@ -287,7 +287,7 @@ class ImagesPipelineTestCase(unittest.TestCase):\n     def test_convert_image_new(self):\n         # tests for new API\n         SIZE = (100, 100)\n-        # straigh forward case: RGB and JPEG\n+        # straight forward case: RGB and JPEG\n         COLOUR = (0, 127, 255)\n         im, buf = _create_image(\"JPEG\", \"RGB\", SIZE, COLOUR)\n         converted, converted_buf = self.pipeline.convert_image(im, response_body=buf)\n\n@@ -11,12 +11,12 @@ from tests.mockserver import MockServer\n from tests.spiders import SingleRequestSpider\n \n \n-OVERRIDEN_URL = \"https://example.org\"\n+OVERRIDDEN_URL = \"https://example.org\"\n \n \n class ProcessResponseMiddleware:\n     def process_response(self, request, response, spider):\n-        return response.replace(request=Request(OVERRIDEN_URL))\n+        return response.replace(request=Request(OVERRIDDEN_URL))\n \n \n class RaiseExceptionRequestMiddleware:\n@@ -30,7 +30,7 @@ class CatchExceptionOverrideRequestMiddleware:\n         return Response(\n             url=\"http://localhost/\",\n             body=b\"Caught \" + exception.__class__.__name__.encode(\"utf-8\"),\n-            request=Request(OVERRIDEN_URL),\n+            request=Request(OVERRIDDEN_URL),\n         )\n \n \n@@ -52,7 +52,7 @@ class AlternativeCallbacksSpider(SingleRequestSpider):\n class AlternativeCallbacksMiddleware:\n     def process_response(self, request, response, spider):\n         new_request = request.replace(\n-            url=OVERRIDEN_URL,\n+            url=OVERRIDDEN_URL,\n             callback=spider.alt_callback,\n             cb_kwargs={\"foo\": \"bar\"},\n         )\n@@ -132,16 +132,16 @@ class CrawlTestCase(TestCase):\n             yield crawler.crawl(seed=url, mockserver=self.mockserver)\n \n         response = crawler.spider.meta[\"responses\"][0]\n-        self.assertEqual(response.request.url, OVERRIDEN_URL)\n+        self.assertEqual(response.request.url, OVERRIDDEN_URL)\n \n         self.assertEqual(signal_params[\"response\"].url, url)\n-        self.assertEqual(signal_params[\"request\"].url, OVERRIDEN_URL)\n+        self.assertEqual(signal_params[\"request\"].url, OVERRIDDEN_URL)\n \n         log.check_present(\n             (\n                 \"scrapy.core.engine\",\n                 \"DEBUG\",\n-                f\"Crawled (200) <GET {OVERRIDEN_URL}> (referer: None)\",\n+                f\"Crawled (200) <GET {OVERRIDDEN_URL}> (referer: None)\",\n             ),\n         )\n \n@@ -166,7 +166,7 @@ class CrawlTestCase(TestCase):\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         response = crawler.spider.meta[\"responses\"][0]\n         self.assertEqual(response.body, b\"Caught ZeroDivisionError\")\n-        self.assertEqual(response.request.url, OVERRIDEN_URL)\n+        self.assertEqual(response.request.url, OVERRIDDEN_URL)\n \n     @defer.inlineCallbacks\n     def test_downloader_middleware_do_not_override_in_process_exception(self):\n\n@@ -227,7 +227,7 @@ class MixinSameOrigin:\n         ),\n         (\"http://example.com:81/page.html\", \"http://example.com/not-page.html\", None),\n         (\"http://example.com/page.html\", \"http://example.com:81/not-page.html\", None),\n-        # Different protocols: do NOT send refferer\n+        # Different protocols: do NOT send referrer\n         (\"https://example.com/page.html\", \"http://example.com/not-page.html\", None),\n         (\"https://example.com/page.html\", \"http://not.example.com/\", None),\n         (\"ftps://example.com/urls.zip\", \"https://example.com/not-page.html\", None),\n@@ -750,19 +750,19 @@ class TestRequestMetaUnsafeUrl(MixinUnsafeUrl, TestRefererMiddleware):\n     req_meta = {\"referrer_policy\": POLICY_UNSAFE_URL}\n \n \n-class TestRequestMetaPredecence001(MixinUnsafeUrl, TestRefererMiddleware):\n+class TestRequestMetaPrecedence001(MixinUnsafeUrl, TestRefererMiddleware):\n     settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.SameOriginPolicy\"}\n     req_meta = {\"referrer_policy\": POLICY_UNSAFE_URL}\n \n \n-class TestRequestMetaPredecence002(MixinNoReferrer, TestRefererMiddleware):\n+class TestRequestMetaPrecedence002(MixinNoReferrer, TestRefererMiddleware):\n     settings = {\n         \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy\"\n     }\n     req_meta = {\"referrer_policy\": POLICY_NO_REFERRER}\n \n \n-class TestRequestMetaPredecence003(MixinUnsafeUrl, TestRefererMiddleware):\n+class TestRequestMetaPrecedence003(MixinUnsafeUrl, TestRefererMiddleware):\n     settings = {\n         \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy\"\n     }\n@@ -888,19 +888,19 @@ class TestSettingsPolicyByName(TestCase):\n             RefererMiddleware(settings)\n \n \n-class TestPolicyHeaderPredecence001(MixinUnsafeUrl, TestRefererMiddleware):\n+class TestPolicyHeaderPrecedence001(MixinUnsafeUrl, TestRefererMiddleware):\n     settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.SameOriginPolicy\"}\n     resp_headers = {\"Referrer-Policy\": POLICY_UNSAFE_URL.upper()}\n \n \n-class TestPolicyHeaderPredecence002(MixinNoReferrer, TestRefererMiddleware):\n+class TestPolicyHeaderPrecedence002(MixinNoReferrer, TestRefererMiddleware):\n     settings = {\n         \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy\"\n     }\n     resp_headers = {\"Referrer-Policy\": POLICY_NO_REFERRER.swapcase()}\n \n \n-class TestPolicyHeaderPredecence003(\n+class TestPolicyHeaderPrecedence003(\n     MixinNoReferrerWhenDowngrade, TestRefererMiddleware\n ):\n     settings = {\n@@ -909,7 +909,7 @@ class TestPolicyHeaderPredecence003(\n     resp_headers = {\"Referrer-Policy\": POLICY_NO_REFERRER_WHEN_DOWNGRADE.title()}\n \n \n-class TestPolicyHeaderPredecence004(\n+class TestPolicyHeaderPrecedence004(\n     MixinNoReferrerWhenDowngrade, TestRefererMiddleware\n ):\n     \"\"\"\n\n@@ -15,6 +15,11 @@ class AsyncioTest(TestCase):\n         )\n \n     def test_install_asyncio_reactor(self):\n+        from twisted.internet import reactor as original_reactor\n+\n         with warnings.catch_warnings(record=True) as w:\n             install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n             self.assertEqual(len(w), 0)\n+        from twisted.internet import reactor\n+\n+        assert original_reactor == reactor\n\n@@ -74,7 +74,7 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         self.assertIn(\"foo.NewClass\", str(w[1].message))\n         self.assertIn(\"bar.OldClass\", str(w[1].message))\n \n-    def test_subclassing_warns_only_on_direct_childs(self):\n+    def test_subclassing_warns_only_on_direct_children(self):\n         Deprecated = create_deprecated_class(\n             \"Deprecated\", NewName, warn_once=False, warn_category=MyWarning\n         )\n\n@@ -7,17 +7,27 @@ from scrapy.utils.display import pformat, pprint\n \n class TestDisplay(TestCase):\n     object = {\"a\": 1}\n-    colorized_string = (\n+    colorized_strings = {\n+        (\n+            (\n                 \"{\\x1b[33m'\\x1b[39;49;00m\\x1b[33ma\\x1b[39;49;00m\\x1b[33m'\"\n-        \"\\x1b[39;49;00m: \\x1b[34m1\\x1b[39;49;00m}\\n\"\n+                \"\\x1b[39;49;00m: \\x1b[34m1\\x1b[39;49;00m}\"\n             )\n+            + suffix\n+        )\n+        for suffix in (\n+            # https://github.com/pygments/pygments/issues/2313\n+            \"\\n\",  # pygments ≤ 2.13\n+            \"\\x1b[37m\\x1b[39;49;00m\\n\",  # pygments ≥ 2.14\n+        )\n+    }\n     plain_string = \"{'a': 1}\"\n \n     @mock.patch(\"sys.platform\", \"linux\")\n     @mock.patch(\"sys.stdout.isatty\")\n     def test_pformat(self, isatty):\n         isatty.return_value = True\n-        self.assertEqual(pformat(self.object), self.colorized_string)\n+        self.assertIn(pformat(self.object), self.colorized_strings)\n \n     @mock.patch(\"sys.stdout.isatty\")\n     def test_pformat_dont_colorize(self, isatty):\n@@ -33,7 +43,7 @@ class TestDisplay(TestCase):\n     def test_pformat_old_windows(self, isatty, version):\n         isatty.return_value = True\n         version.return_value = \"10.0.14392\"\n-        self.assertEqual(pformat(self.object), self.colorized_string)\n+        self.assertIn(pformat(self.object), self.colorized_strings)\n \n     @mock.patch(\"sys.platform\", \"win32\")\n     @mock.patch(\"scrapy.utils.display._enable_windows_terminal_processing\")\n@@ -55,7 +65,7 @@ class TestDisplay(TestCase):\n         isatty.return_value = True\n         version.return_value = \"10.0.14393\"\n         terminal_processing.return_value = True\n-        self.assertEqual(pformat(self.object), self.colorized_string)\n+        self.assertIn(pformat(self.object), self.colorized_strings)\n \n     @mock.patch(\"sys.platform\", \"linux\")\n     @mock.patch(\"sys.stdout.isatty\")\n\n@@ -159,7 +159,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n         b = Obj()\n         # no attributes given return False\n         self.assertFalse(equal_attributes(a, b, []))\n-        # not existent attributes\n+        # nonexistent attributes\n         self.assertFalse(equal_attributes(a, b, [\"x\", \"y\"]))\n \n         a.x = 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
