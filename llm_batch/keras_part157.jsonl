{"custom_id": "keras#b0500764a8a451882262328c0f1676402fb0a783", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 72 | Files Changed: 4 | Hunks: 25 | Methods Changed: 15 | Complexity Δ (Sum/Max): -3/2 | Churn Δ: 153 | Churn Cumulative: 992 | Contributors (this commit): 8 | Commits (past 90d): 16 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,13 +1,11 @@\n from __future__ import absolute_import\n-from six.moves import cPickle\n-import gzip\n from ..utils.data_utils import get_file\n from six.moves import zip\n import numpy as np\n-import sys\n+import json\n \n \n-def load_data(path='imdb_full.pkl', num_words=None, skip_top=0,\n+def load_data(path='imdb.npz', num_words=None, skip_top=0,\n               maxlen=None, seed=113,\n               start_char=1, oov_char=2, index_from=3):\n     \"\"\"Loads the IMDB dataset.\n@@ -41,15 +39,12 @@ def load_data(path='imdb_full.pkl', num_words=None, skip_top=0,\n     have simply been skipped.\n     \"\"\"\n     path = get_file(path,\n-                    origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n-                    md5_hash='d091312047c43cf9e4e38fef92437263')\n-\n-    if path.endswith('.gz'):\n-        f = gzip.open(path, 'rb')\n-    else:\n-        f = open(path, 'rb')\n-\n-    (x_train, labels_train), (x_test, labels_test) = cPickle.load(f)\n+                    origin='https://s3.amazonaws.com/text-datasets/imdb.npz')\n+    f = np.load(path)\n+    x_train = f['x_train']\n+    labels_train = f['y_train']\n+    x_test = f['x_test']\n+    labels_test = f['y_test']\n     f.close()\n \n     np.random.seed(seed)\n@@ -62,8 +57,8 @@ def load_data(path='imdb_full.pkl', num_words=None, skip_top=0,\n     np.random.seed(seed * 2)\n     np.random.shuffle(labels_test)\n \n-    xs = x_train + x_test\n-    labels = labels_train + labels_test\n+    xs = np.concatenate([x_train, x_test])\n+    labels = np.concatenate([labels_train, labels_test])\n \n     if start_char is not None:\n         xs = [[start_char] + [w + index_from for w in x] for x in xs]\n@@ -110,7 +105,7 @@ def load_data(path='imdb_full.pkl', num_words=None, skip_top=0,\n     return (x_train, y_train), (x_test, y_test)\n \n \n-def get_word_index(path='imdb_word_index.pkl'):\n+def get_word_index(path='imdb_word_index.json'):\n     \"\"\"Retrieves the dictionary mapping word indices back to words.\n \n     # Arguments\n@@ -120,14 +115,8 @@ def get_word_index(path='imdb_word_index.pkl'):\n         The word index dictionary.\n     \"\"\"\n     path = get_file(path,\n-                    origin='https://s3.amazonaws.com/text-datasets/imdb_word_index.pkl',\n-                    md5_hash='72d94b01291be4ff843198d3b0e1e4d7')\n-    f = open(path, 'rb')\n-\n-    if sys.version_info < (3,):\n-        data = cPickle.load(f)\n-    else:\n-        data = cPickle.load(f, encoding='latin1')\n-\n+                    origin='https://s3.amazonaws.com/text-datasets/imdb_word_index.json')\n+    f = open(path)\n+    data = json.load(f)\n     f.close()\n     return data\n\n@@ -1,10 +1,8 @@\n-import gzip\n from ..utils.data_utils import get_file\n-from six.moves import cPickle\n-import sys\n+import numpy as np\n \n \n-def load_data(path='mnist.pkl.gz'):\n+def load_data(path='mnist.npz'):\n     \"\"\"Loads the MNIST dataset.\n \n     # Arguments\n@@ -14,17 +12,11 @@ def load_data(path='mnist.pkl.gz'):\n     # Returns\n         Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n     \"\"\"\n-    path = get_file(path, origin='https://s3.amazonaws.com/img-datasets/mnist.pkl.gz')\n-\n-    if path.endswith('.gz'):\n-        f = gzip.open(path, 'rb')\n-    else:\n-        f = open(path, 'rb')\n-\n-    if sys.version_info < (3,):\n-        data = cPickle.load(f)\n-    else:\n-        data = cPickle.load(f, encoding='bytes')\n-\n+    path = get_file(path, origin='https://s3.amazonaws.com/img-datasets/mnist.npz')\n+    f = np.load(path)\n+    x_train = f['x_train']\n+    y_train = f['y_train']\n+    x_test = f['x_test']\n+    y_test = f['y_test']\n     f.close()\n-    return data  # (x_train, y_train), (x_test, y_test)\n+    return (x_train, y_train), (x_test, y_test)\n\n@@ -1,13 +1,12 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n from ..utils.data_utils import get_file\n-from six.moves import cPickle\n from six.moves import zip\n import numpy as np\n-import sys\n+import json\n \n \n-def load_data(path='reuters.pkl', num_words=None, skip_top=0,\n+def load_data(path='reuters.npz', num_words=None, skip_top=0,\n               maxlen=None, test_split=0.2, seed=113,\n               start_char=1, oov_char=2, index_from=3):\n     \"\"\"Loads the Reuters newswire classification dataset.\n@@ -37,11 +36,11 @@ def load_data(path='reuters.pkl', num_words=None, skip_top=0,\n     Words that were not seen in the trining set but are in the test set\n     have simply been skipped.\n     \"\"\"\n-\n-    path = get_file(path, origin='https://s3.amazonaws.com/text-datasets/reuters.pkl')\n-    f = open(path, 'rb')\n-    xs, labels = cPickle.load(f)\n-    f.close()\n+    path = get_file(path, origin='https://s3.amazonaws.com/text-datasets/reuters.npz')\n+    npzfile = np.load(path)\n+    xs = npzfile['x']\n+    labels = npzfile['y']\n+    npzfile.close()\n \n     np.random.seed(seed)\n     np.random.shuffle(xs)\n@@ -81,16 +80,16 @@ def load_data(path='reuters.pkl', num_words=None, skip_top=0,\n             new_xs.append(nx)\n         xs = new_xs\n \n-    x_train = xs[:int(len(xs) * (1 - test_split))]\n-    y_train = labels[:int(len(xs) * (1 - test_split))]\n+    x_train = np.array(xs[:int(len(xs) * (1 - test_split))])\n+    y_train = np.array(labels[:int(len(xs) * (1 - test_split))])\n \n-    x_test = xs[int(len(xs) * (1 - test_split)):]\n-    y_test = labels[int(len(xs) * (1 - test_split)):]\n+    x_test = np.array(xs[int(len(xs) * (1 - test_split)):])\n+    y_test = np.array(labels[int(len(xs) * (1 - test_split)):])\n \n     return (x_train, y_train), (x_test, y_test)\n \n \n-def get_word_index(path='reuters_word_index.pkl'):\n+def get_word_index(path='reuters_word_index.json'):\n     \"\"\"Retrieves the dictionary mapping word indices back to words.\n \n     # Arguments\n@@ -99,13 +98,8 @@ def get_word_index(path='reuters_word_index.pkl'):\n     # Returns\n         The word index dictionary.\n     \"\"\"\n-    path = get_file(path, origin='https://s3.amazonaws.com/text-datasets/reuters_word_index.pkl')\n-    f = open(path, 'rb')\n-\n-    if sys.version_info < (3,):\n-        data = cPickle.load(f)\n-    else:\n-        data = cPickle.load(f, encoding='latin1')\n-\n+    path = get_file(path, origin='https://s3.amazonaws.com/text-datasets/reuters_word_index.json')\n+    f = open(path)\n+    data = json.load(f)\n     f.close()\n     return data\n\n@@ -2,7 +2,12 @@ from __future__ import print_function\n import pytest\n import time\n import random\n-from keras.datasets import cifar10, cifar100, reuters, imdb, mnist\n+from keras.datasets import cifar10\n+from keras.datasets import cifar100\n+from keras.datasets import reuters\n+from keras.datasets import imdb\n+from keras.datasets import mnist\n+from keras.datasets import boston_housing\n \n \n def test_cifar():\n@@ -10,9 +15,15 @@ def test_cifar():\n     # to speed up frequent testing\n     random.seed(time.time())\n     if random.random() > 0.8:\n-        (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n-        (X_train, y_train), (X_test, y_test) = cifar100.load_data('fine')\n-        (X_train, y_train), (X_test, y_test) = cifar100.load_data('coarse')\n+        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n+        assert len(x_train) == len(y_train) == 50000\n+        assert len(x_test) == len(y_test) == 10000\n+        (x_train, y_train), (x_test, y_test) = cifar100.load_data('fine')\n+        assert len(x_train) == len(y_train) == 50000\n+        assert len(x_test) == len(y_test) == 10000\n+        (x_train, y_train), (x_test, y_test) = cifar100.load_data('coarse')\n+        assert len(x_train) == len(y_train) == 50000\n+        assert len(x_test) == len(y_test) == 10000\n \n \n def test_reuters():\n@@ -20,8 +31,15 @@ def test_reuters():\n     # to speed up frequent testing\n     random.seed(time.time())\n     if random.random() > 0.8:\n-        (X_train, y_train), (X_test, y_test) = reuters.load_data()\n-        (X_train, y_train), (X_test, y_test) = reuters.load_data(maxlen=10)\n+        (x_train, y_train), (x_test, y_test) = reuters.load_data()\n+        assert len(x_train) == len(y_train)\n+        assert len(x_test) == len(y_test)\n+        assert len(x_train) + len(x_test) == 11228\n+        (x_train, y_train), (x_test, y_test) = reuters.load_data(maxlen=10)\n+        assert len(x_train) == len(y_train)\n+        assert len(x_test) == len(y_test)\n+        word_index = reuters.get_word_index()\n+        assert isinstance(word_index, dict)\n \n \n def test_mnist():\n@@ -29,7 +47,9 @@ def test_mnist():\n     # to speed up frequent testing\n     random.seed(time.time())\n     if random.random() > 0.8:\n-        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n+        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n+        assert len(x_train) == len(y_train) == 60000\n+        assert len(x_test) == len(y_test) == 10000\n \n \n def test_imdb():\n@@ -37,8 +57,22 @@ def test_imdb():\n     # to speed up frequent testing\n     random.seed(time.time())\n     if random.random() > 0.8:\n-        (X_train, y_train), (X_test, y_test) = imdb.load_data()\n-        (X_train, y_train), (X_test, y_test) = imdb.load_data(maxlen=40)\n+        (x_train, y_train), (x_test, y_test) = imdb.load_data()\n+        (x_train, y_train), (x_test, y_test) = imdb.load_data(maxlen=40)\n+        assert len(x_train) == len(y_train)\n+        assert len(x_test) == len(y_test)\n+        word_index = imdb.get_word_index()\n+        assert isinstance(word_index, dict)\n+\n+\n+def test_boston_housing():\n+    # only run data download tests 20% of the time\n+    # to speed up frequent testing\n+    random.seed(time.time())\n+    if random.random() > 0.8:\n+        (x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n+        assert len(x_train) == len(y_train)\n+        assert len(x_test) == len(y_test)\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a60ee821a246dd3d1c7815ce70a473c48cb91965", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 38 | Churn Cumulative: 38 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,8 @@\n+from __future__ import absolute_import\n+\n+from . import mnist\n+from . import imdb\n+from . import reuters\n+from . import cifar10\n+from . import cifar100\n+from . import boston_housing\n\n@@ -0,0 +1,30 @@\n+from ..utils.data_utils import get_file\n+import numpy as np\n+\n+\n+def load_data(path='boston_housing.npz', seed=113, test_split=0.2):\n+    \"\"\"Loads the MNIST dataset.\n+\n+    # Arguments\n+        path: path where to cache the dataset locally\n+            (relative to ~/.keras/datasets).\n+\n+    # Returns\n+        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n+    \"\"\"\n+    path = get_file(path, origin='https://s3.amazonaws.com/keras-datasets/boston_housing.npz')\n+    f = np.load(path)\n+    x = f['x']\n+    y = f['y']\n+    f.close()\n+\n+    np.random.seed(seed)\n+    np.random.shuffle(x)\n+    np.random.seed(seed)\n+    np.random.shuffle(y)\n+\n+    x_train = np.array(x[:int(len(x) * (1 - test_split))])\n+    y_train = np.array(y[:int(len(x) * (1 - test_split))])\n+    x_test = np.array(x[int(len(x) * (1 - test_split)):])\n+    y_test = np.array(y[int(len(x) * (1 - test_split)):])\n+    return (x_train, y_train), (x_test, y_test)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1c4bc0c967234ea83fca77961504112eb3d85ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 39 | Churn Cumulative: 1971 | Contributors (this commit): 23 | Commits (past 90d): 14 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -497,9 +497,7 @@ class ImageDataGenerator(object):\n             theta = np.pi / 180 * np.random.uniform(-self.rotation_range, self.rotation_range)\n         else:\n             theta = 0\n-        rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n-                                    [np.sin(theta), np.cos(theta), 0],\n-                                    [0, 0, 1]])\n+\n         if self.height_shift_range:\n             tx = np.random.uniform(-self.height_shift_range, self.height_shift_range) * x.shape[img_row_axis]\n         else:\n@@ -510,34 +508,47 @@ class ImageDataGenerator(object):\n         else:\n             ty = 0\n \n-        translation_matrix = np.array([[1, 0, tx],\n-                                       [0, 1, ty],\n-                                       [0, 0, 1]])\n         if self.shear_range:\n             shear = np.random.uniform(-self.shear_range, self.shear_range)\n         else:\n             shear = 0\n-        shear_matrix = np.array([[1, -np.sin(shear), 0],\n-                                 [0, np.cos(shear), 0],\n-                                 [0, 0, 1]])\n \n         if self.zoom_range[0] == 1 and self.zoom_range[1] == 1:\n             zx, zy = 1, 1\n         else:\n             zx, zy = np.random.uniform(self.zoom_range[0], self.zoom_range[1], 2)\n+\n+        transform_matrix = None\n+        if theta != 0:\n+            rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n+                                        [np.sin(theta), np.cos(theta), 0],\n+                                        [0, 0, 1]])\n+            transform_matrix = rotation_matrix\n+\n+        if tx != 0 or ty != 0:\n+            shift_matrix = np.array([[1, 0, tx],\n+                                     [0, 1, ty],\n+                                     [0, 0, 1]])\n+            transform_matrix = shift_matrix if transform_matrix is None else np.dot(transform_matrix, shift_matrix)\n+\n+        if shear != 0:\n+            shear_matrix = np.array([[1, -np.sin(shear), 0],\n+                                    [0, np.cos(shear), 0],\n+                                    [0, 0, 1]])\n+            transform_matrix = shear_matrix if transform_matrix is None else np.dot(transform_matrix, shear_matrix)\n+\n+        if zx != 1 or zy != 1:\n             zoom_matrix = np.array([[zx, 0, 0],\n                                     [0, zy, 0],\n                                     [0, 0, 1]])\n+            transform_matrix = zoom_matrix if transform_matrix is None else np.dot(transform_matrix, zoom_matrix)\n \n-        transform_matrix = np.dot(np.dot(np.dot(rotation_matrix,\n-                                                translation_matrix),\n-                                         shear_matrix),\n-                                  zoom_matrix)\n-\n+        if transform_matrix is not None:\n             h, w = x.shape[img_row_axis], x.shape[img_col_axis]\n             transform_matrix = transform_matrix_offset_center(transform_matrix, h, w)\n             x = apply_transform(x, transform_matrix, img_channel_axis,\n                                 fill_mode=self.fill_mode, cval=self.cval)\n+\n         if self.channel_shift_range != 0:\n             x = random_channel_shift(x,\n                                      self.channel_shift_range,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f0e0527591ffd6c061c94d6027c29a31bfb23d78", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 210 | Lines Deleted: 142 | Files Changed: 11 | Hunks: 105 | Methods Changed: 18 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 352 | Churn Cumulative: 16996 | Contributors (this commit): 74 | Commits (past 90d): 103 | Contributors (cumulative): 94 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,3 +3,4 @@ from .vgg19 import VGG19\n from .resnet50 import ResNet50\n from .inception_v3 import InceptionV3\n from .xception import Xception\n+from .music_tagger_crnn import MusicTaggerCRNN\n\n@@ -21,8 +21,9 @@ from __future__ import absolute_import\n import warnings\n \n from ..models import Model\n-from ..layers import Flatten, Dense, Input, BatchNormalization, merge\n-from ..layers import Convolution2D, MaxPooling2D, AveragePooling2D\n+from .. import layers\n+from ..layers import Flatten, Dense, Input, BatchNormalization\n+from ..layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n from ..engine.topology import get_source_inputs\n from ..utils.layer_utils import convert_all_kernels_in_model\n from ..utils.data_utils import get_file\n@@ -37,7 +38,7 @@ TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/relea\n \n \n def conv2d_bn(x, filters, num_row, num_col,\n-              border_mode='same', subsample=(1, 1),\n+              padding='same', strides=(1, 1),\n               name=None):\n     \"\"\"Utility function to apply conv + BN.\n     \"\"\"\n@@ -51,10 +52,10 @@ def conv2d_bn(x, filters, num_row, num_col,\n         bn_axis = 1\n     else:\n         bn_axis = 3\n-    x = Convolution2D(filters, num_row, num_col,\n-                      subsample=subsample,\n+    x = Conv2D(filters, (num_row, num_col),\n+               strides=strides,\n                activation='relu',\n-                      border_mode=border_mode,\n+               padding=padding,\n                name=conv_name)(x)\n     x = BatchNormalization(axis=bn_axis, name=bn_name)(x)\n     return x\n@@ -62,6 +63,7 @@ def conv2d_bn(x, filters, num_row, num_col,\n \n def InceptionV3(include_top=True, weights='imagenet',\n                 input_tensor=None, input_shape=None,\n+                pooling=None,\n                 classes=1000):\n     \"\"\"Instantiate the Inception v3 architecture,\n     optionally loading weights pre-trained\n@@ -91,6 +93,17 @@ def InceptionV3(include_top=True, weights='imagenet',\n             It should have exactly 3 inputs channels,\n             and width and height should be no smaller than 139.\n             E.g. `(150, 150, 3)` would be one valid value.\n+        pooling: Optional pooling mode for feature extraction\n+            when `include_top` is `False`.\n+            - `None` means that the output of the model will be\n+                the 4D tensor output of the\n+                last convolutional layer.\n+            - `avg` means that global average pooling\n+                will be applied to the output of the\n+                last convolutional layer, and thus\n+                the output of the model will be a 2D tensor.\n+            - `max` means that global max pooling will\n+                be applied.\n         classes: optional number of classes to classify images\n             into, only to be specified if `include_top` is True, and\n             if no `weights` argument is specified.\n@@ -127,13 +140,13 @@ def InceptionV3(include_top=True, weights='imagenet',\n     else:\n         channel_axis = 3\n \n-    x = conv2d_bn(img_input, 32, 3, 3, subsample=(2, 2), border_mode='valid')\n-    x = conv2d_bn(x, 32, 3, 3, border_mode='valid')\n+    x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding='valid')\n+    x = conv2d_bn(x, 32, 3, 3, padding='valid')\n     x = conv2d_bn(x, 64, 3, 3)\n     x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n \n-    x = conv2d_bn(x, 80, 1, 1, border_mode='valid')\n-    x = conv2d_bn(x, 192, 3, 3, border_mode='valid')\n+    x = conv2d_bn(x, 80, 1, 1, padding='valid')\n+    x = conv2d_bn(x, 192, 3, 3, padding='valid')\n     x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n \n     # mixed 0, 1, 2: 35 x 35 x 256\n@@ -148,23 +161,23 @@ def InceptionV3(include_top=True, weights='imagenet',\n         branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n \n         branch_pool = AveragePooling2D(\n-            (3, 3), strides=(1, 1), border_mode='same')(x)\n+            (3, 3), strides=(1, 1), padding='same')(x)\n         branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n-        x = merge([branch1x1, branch5x5, branch3x3dbl, branch_pool],\n-                  mode='concat', concat_axis=channel_axis,\n+        x = layers.concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool],\n+                               axis=channel_axis,\n                                name='mixed' + str(i))\n \n     # mixed 3: 17 x 17 x 768\n-    branch3x3 = conv2d_bn(x, 384, 3, 3, subsample=(2, 2), border_mode='valid')\n+    branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding='valid')\n \n     branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3,\n-                             subsample=(2, 2), border_mode='valid')\n+                             strides=(2, 2), padding='valid')\n \n     branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n-    x = merge([branch3x3, branch3x3dbl, branch_pool],\n-              mode='concat', concat_axis=channel_axis,\n+    x = layers.concatenate([branch3x3, branch3x3dbl, branch_pool],\n+                           axis=channel_axis,\n                            name='mixed3')\n \n     # mixed 4: 17 x 17 x 768\n@@ -180,10 +193,10 @@ def InceptionV3(include_top=True, weights='imagenet',\n     branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n     branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n \n-    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), border_mode='same')(x)\n+    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n     branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n-    x = merge([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n-              mode='concat', concat_axis=channel_axis,\n+    x = layers.concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n+                           axis=channel_axis,\n                            name='mixed4')\n \n     # mixed 5, 6: 17 x 17 x 768\n@@ -201,10 +214,10 @@ def InceptionV3(include_top=True, weights='imagenet',\n         branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n \n         branch_pool = AveragePooling2D(\n-            (3, 3), strides=(1, 1), border_mode='same')(x)\n+            (3, 3), strides=(1, 1), padding='same')(x)\n         branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n-        x = merge([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n-                  mode='concat', concat_axis=channel_axis,\n+        x = layers.concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n+                               axis=channel_axis,\n                                name='mixed' + str(5 + i))\n \n     # mixed 7: 17 x 17 x 768\n@@ -220,26 +233,26 @@ def InceptionV3(include_top=True, weights='imagenet',\n     branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n     branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n \n-    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), border_mode='same')(x)\n+    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n     branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n-    x = merge([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n-              mode='concat', concat_axis=channel_axis,\n+    x = layers.concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n+                           axis=channel_axis,\n                            name='mixed7')\n \n     # mixed 8: 8 x 8 x 1280\n     branch3x3 = conv2d_bn(x, 192, 1, 1)\n     branch3x3 = conv2d_bn(branch3x3, 320, 3, 3,\n-                          subsample=(2, 2), border_mode='valid')\n+                          strides=(2, 2), padding='valid')\n \n     branch7x7x3 = conv2d_bn(x, 192, 1, 1)\n     branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)\n     branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)\n     branch7x7x3 = conv2d_bn(branch7x7x3, 192, 3, 3,\n-                            subsample=(2, 2), border_mode='valid')\n+                            strides=(2, 2), padding='valid')\n \n     branch_pool = AveragePooling2D((3, 3), strides=(2, 2))(x)\n-    x = merge([branch3x3, branch7x7x3, branch_pool],\n-              mode='concat', concat_axis=channel_axis,\n+    x = layers.concatenate([branch3x3, branch7x7x3, branch_pool],\n+                           axis=channel_axis,\n                            name='mixed8')\n \n     # mixed 9: 8 x 8 x 2048\n@@ -249,22 +262,22 @@ def InceptionV3(include_top=True, weights='imagenet',\n         branch3x3 = conv2d_bn(x, 384, 1, 1)\n         branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)\n         branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)\n-        branch3x3 = merge([branch3x3_1, branch3x3_2],\n-                          mode='concat', concat_axis=channel_axis,\n+        branch3x3 = layers.concatenate([branch3x3_1, branch3x3_2],\n+                                       axis=channel_axis,\n                                        name='mixed9_' + str(i))\n \n         branch3x3dbl = conv2d_bn(x, 448, 1, 1)\n         branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)\n         branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)\n         branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)\n-        branch3x3dbl = merge([branch3x3dbl_1, branch3x3dbl_2],\n-                             mode='concat', concat_axis=channel_axis)\n+        branch3x3dbl = layers.concatenate([branch3x3dbl_1, branch3x3dbl_2],\n+                                          axis=channel_axis)\n \n         branch_pool = AveragePooling2D(\n-            (3, 3), strides=(1, 1), border_mode='same')(x)\n+            (3, 3), strides=(1, 1), padding='same')(x)\n         branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n-        x = merge([branch1x1, branch3x3, branch3x3dbl, branch_pool],\n-                  mode='concat', concat_axis=channel_axis,\n+        x = layers.concatenate([branch1x1, branch3x3, branch3x3dbl, branch_pool],\n+                               axis=channel_axis,\n                                name='mixed' + str(9 + i))\n \n     if include_top:\n@@ -272,6 +285,11 @@ def InceptionV3(include_top=True, weights='imagenet',\n         x = AveragePooling2D((8, 8), strides=(8, 8), name='avg_pool')(x)\n         x = Flatten(name='flatten')(x)\n         x = Dense(classes, activation='softmax', name='predictions')(x)\n+    else:\n+        if pooling == 'avg':\n+            x = GlobalAveragePooling2D()(x)\n+        elif pooling == 'max':\n+            x = GlobalMaxPooling2D()(x)\n \n     # Ensure that the model takes into account\n     # any potential predecessors of `input_tensor`.\n\n@@ -12,8 +12,8 @@ from __future__ import absolute_import\n from .. import backend as K\n from ..layers import Input, Dense\n from ..models import Model\n-from ..layers import Dense, Dropout, Reshape, Permute\n-from ..layers.convolutional import Convolution2D\n+from ..layers import Reshape, Permute\n+from ..layers.convolutional import Conv2D\n from ..layers.convolutional import MaxPooling2D, ZeroPadding2D\n from ..layers.normalization import BatchNormalization\n from ..layers.advanced_activations import ELU\n@@ -28,7 +28,8 @@ TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/dow\n \n \n def MusicTaggerCRNN(weights='msd', input_tensor=None,\n-                    include_top=True, classes=50):\n+                    include_top=True,\n+                    classes=50):\n     \"\"\"Instantiate the MusicTaggerCRNN architecture,\n     optionally loading weights pre-trained\n     on Million Song Dataset. Note that when using TensorFlow,\n@@ -86,11 +87,9 @@ def MusicTaggerCRNN(weights='msd', input_tensor=None,\n     # Determine input axis\n     if K.image_data_format() == 'channels_first':\n         channel_axis = 1\n-        freq_axis = 2\n         time_axis = 3\n     else:\n         channel_axis = 3\n-        freq_axis = 1\n         time_axis = 2\n \n     # Input block\n@@ -98,26 +97,26 @@ def MusicTaggerCRNN(weights='msd', input_tensor=None,\n     x = BatchNormalization(axis=time_axis, name='bn_0_freq')(x)\n \n     # Conv block 1\n-    x = Convolution2D(64, 3, 3, border_mode='same', name='conv1')(x)\n-    x = BatchNormalization(axis=channel_axis, mode=0, name='bn1')(x)\n+    x = Conv2D(64, (3, 3), padding='same', name='conv1')(x)\n+    x = BatchNormalization(axis=channel_axis, name='bn1')(x)\n     x = ELU()(x)\n     x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool1')(x)\n \n     # Conv block 2\n-    x = Convolution2D(128, 3, 3, border_mode='same', name='conv2')(x)\n-    x = BatchNormalization(axis=channel_axis, mode=0, name='bn2')(x)\n+    x = Conv2D(128, (3, 3), padding='same', name='conv2')(x)\n+    x = BatchNormalization(axis=channel_axis, name='bn2')(x)\n     x = ELU()(x)\n     x = MaxPooling2D(pool_size=(3, 3), strides=(3, 3), name='pool2')(x)\n \n     # Conv block 3\n-    x = Convolution2D(128, 3, 3, border_mode='same', name='conv3')(x)\n-    x = BatchNormalization(axis=channel_axis, mode=0, name='bn3')(x)\n+    x = Conv2D(128, (3, 3), padding='same', name='conv3')(x)\n+    x = BatchNormalization(axis=channel_axis, name='bn3')(x)\n     x = ELU()(x)\n     x = MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='pool3')(x)\n \n     # Conv block 4\n-    x = Convolution2D(128, 3, 3, border_mode='same', name='conv4')(x)\n-    x = BatchNormalization(axis=channel_axis, mode=0, name='bn4')(x)\n+    x = Conv2D(128, (3, 3), padding='same', name='conv4')(x)\n+    x = BatchNormalization(axis=channel_axis, name='bn4')(x)\n     x = ELU()(x)\n     x = MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='pool4')(x)\n \n\n@@ -12,9 +12,10 @@ from __future__ import absolute_import\n \n import warnings\n \n-from ..layers import merge, Input\n+from ..layers import Input\n+from .. import layers\n from ..layers import Dense, Activation, Flatten\n-from ..layers import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n+from ..layers import Conv2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n from ..layers import BatchNormalization\n from ..models import Model\n from .. import backend as K\n@@ -48,19 +49,19 @@ def identity_block(input_tensor, kernel_size, filters, stage, block):\n     conv_name_base = 'res' + str(stage) + block + '_branch'\n     bn_name_base = 'bn' + str(stage) + block + '_branch'\n \n-    x = Convolution2D(filters1, 1, 1, name=conv_name_base + '2a')(input_tensor)\n+    x = Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n     x = Activation('relu')(x)\n \n-    x = Convolution2D(filters2, kernel_size, kernel_size,\n-                      border_mode='same', name=conv_name_base + '2b')(x)\n+    x = Conv2D(filters2, kernel_size, kernel_size,\n+               pading='same', name=conv_name_base + '2b')(x)\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n     x = Activation('relu')(x)\n \n-    x = Convolution2D(filters3, 1, 1, name=conv_name_base + '2c')(x)\n+    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n \n-    x = merge([x, input_tensor], mode='sum')\n+    x = layers.sum([x, input_tensor])\n     x = Activation('relu')(x)\n     return x\n \n@@ -75,8 +76,8 @@ def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2))\n         stage: integer, current stage label, used for generating layer names\n         block: 'a','b'..., current block label, used for generating layer names\n \n-    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n-    And the shortcut should have subsample=(2,2) as well\n+    Note that from stage 3, the first conv layer at main path is with strides=(2,2)\n+    And the shortcut should have strides=(2,2) as well\n     \"\"\"\n     filters1, filters2, filters3 = filters\n     if K.image_data_format() == 'channels_last':\n@@ -86,30 +87,31 @@ def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2))\n     conv_name_base = 'res' + str(stage) + block + '_branch'\n     bn_name_base = 'bn' + str(stage) + block + '_branch'\n \n-    x = Convolution2D(filters1, 1, 1, subsample=strides,\n+    x = Conv2D(filters1, (1, 1), strides=strides,\n                name=conv_name_base + '2a')(input_tensor)\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n     x = Activation('relu')(x)\n \n-    x = Convolution2D(filters2, kernel_size, kernel_size, border_mode='same',\n+    x = Conv2D(filters2, (kernel_size, kernel_size), pading='same',\n                name=conv_name_base + '2b')(x)\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n     x = Activation('relu')(x)\n \n-    x = Convolution2D(filters3, 1, 1, name=conv_name_base + '2c')(x)\n+    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n \n-    shortcut = Convolution2D(filters3, 1, 1, subsample=strides,\n+    shortcut = Conv2D(filters3, (1, 1), strides=strides,\n                       name=conv_name_base + '1')(input_tensor)\n     shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n \n-    x = merge([x, shortcut], mode='sum')\n+    x = layers.sum([x, shortcut])\n     x = Activation('relu')(x)\n     return x\n \n \n def ResNet50(include_top=True, weights='imagenet',\n              input_tensor=None, input_shape=None,\n+             pooling=None,\n              classes=1000):\n     \"\"\"Instantiate the ResNet50 architecture,\n     optionally loading weights pre-trained\n@@ -137,6 +139,17 @@ def ResNet50(include_top=True, weights='imagenet',\n             It should have exactly 3 inputs channels,\n             and width and height should be no smaller than 197.\n             E.g. `(200, 200, 3)` would be one valid value.\n+        pooling: Optional pooling mode for feature extraction\n+            when `include_top` is `False`.\n+            - `None` means that the output of the model will be\n+                the 4D tensor output of the\n+                last convolutional layer.\n+            - `avg` means that global average pooling\n+                will be applied to the output of the\n+                last convolutional layer, and thus\n+                the output of the model will be a 2D tensor.\n+            - `max` means that global max pooling will\n+                be applied.\n         classes: optional number of classes to classify images\n             into, only to be specified if `include_top` is True, and\n             if no `weights` argument is specified.\n@@ -173,7 +186,7 @@ def ResNet50(include_top=True, weights='imagenet',\n         bn_axis = 1\n \n     x = ZeroPadding2D((3, 3))(img_input)\n-    x = Convolution2D(64, 7, 7, subsample=(2, 2), name='conv1')(x)\n+    x = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(x)\n     x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n     x = Activation('relu')(x)\n     x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n@@ -203,6 +216,11 @@ def ResNet50(include_top=True, weights='imagenet',\n     if include_top:\n         x = Flatten()(x)\n         x = Dense(classes, activation='softmax', name='fc1000')(x)\n+    else:\n+        if pooling == 'avg':\n+            x = GlobalAveragePooling2D()(x)\n+        elif pooling == 'max':\n+            x = GlobalMaxPooling2D()(x)\n \n     # Ensure that the model takes into account\n     # any potential predecessors of `input_tensor`.\n\n@@ -13,7 +13,7 @@ import warnings\n \n from ..models import Model\n from ..layers import Flatten, Dense, Input\n-from ..layers import Convolution2D, MaxPooling2D\n+from ..layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n from ..engine.topology import get_source_inputs\n from ..utils.layer_utils import convert_all_kernels_in_model\n from ..utils.data_utils import get_file\n@@ -29,6 +29,7 @@ TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/relea\n \n def VGG16(include_top=True, weights='imagenet',\n           input_tensor=None, input_shape=None,\n+          pooling=None,\n           classes=1000):\n     \"\"\"Instantiate the VGG16 architecture,\n     optionally loading weights pre-trained\n@@ -56,6 +57,17 @@ def VGG16(include_top=True, weights='imagenet',\n             It should have exactly 3 inputs channels,\n             and width and height should be no smaller than 48.\n             E.g. `(200, 200, 3)` would be one valid value.\n+        pooling: Optional pooling mode for feature extraction\n+            when `include_top` is `False`.\n+            - `None` means that the output of the model will be\n+                the 4D tensor output of the\n+                last convolutional layer.\n+            - `avg` means that global average pooling\n+                will be applied to the output of the\n+                last convolutional layer, and thus\n+                the output of the model will be a 2D tensor.\n+            - `max` means that global max pooling will\n+                be applied.\n         classes: optional number of classes to classify images\n             into, only to be specified if `include_top` is True, and\n             if no `weights` argument is specified.\n@@ -86,31 +98,31 @@ def VGG16(include_top=True, weights='imagenet',\n         else:\n             img_input = input_tensor\n     # Block 1\n-    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv1')(img_input)\n-    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv2')(x)\n+    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n+    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n     x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n \n     # Block 2\n-    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv1')(x)\n-    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv2')(x)\n+    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n+    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n     x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n \n     # Block 3\n-    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv1')(x)\n-    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv2')(x)\n-    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv3')(x)\n+    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n+    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n+    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n     x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n \n     # Block 4\n-    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv1')(x)\n-    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv2')(x)\n-    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv3')(x)\n+    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n+    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n+    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n     x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n \n     # Block 5\n-    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv1')(x)\n-    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv2')(x)\n-    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv3')(x)\n+    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n+    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n+    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n     x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n \n     if include_top:\n@@ -119,6 +131,11 @@ def VGG16(include_top=True, weights='imagenet',\n         x = Dense(4096, activation='relu', name='fc1')(x)\n         x = Dense(4096, activation='relu', name='fc2')(x)\n         x = Dense(classes, activation='softmax', name='predictions')(x)\n+    else:\n+        if pooling == 'avg':\n+            x = GlobalAveragePooling2D()(x)\n+        elif pooling == 'max':\n+            x = GlobalMaxPooling2D()(x)\n \n     # Ensure that the model takes into account\n     # any potential predecessors of `input_tensor`.\n\n@@ -13,7 +13,7 @@ import warnings\n \n from ..models import Model\n from ..layers import Flatten, Dense, Input\n-from ..layers import Convolution2D, MaxPooling2D, GlobalAveragePooling, GlobalMaxPooling\n+from ..layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n from ..engine.topology import get_source_inputs\n from ..utils.layer_utils import convert_all_kernels_in_model\n from ..utils.data_utils import get_file\n@@ -98,34 +98,34 @@ def VGG19(include_top=True, weights='imagenet',\n         else:\n             img_input = input_tensor\n     # Block 1\n-    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv1')(img_input)\n-    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv2')(x)\n+    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n+    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n     x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n \n     # Block 2\n-    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv1')(x)\n-    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv2')(x)\n+    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n+    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n     x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n \n     # Block 3\n-    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv1')(x)\n-    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv2')(x)\n-    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv3')(x)\n-    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv4')(x)\n+    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n+    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n+    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n+    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n     x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n \n     # Block 4\n-    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv1')(x)\n-    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv2')(x)\n-    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv3')(x)\n-    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv4')(x)\n+    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n+    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n+    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n+    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n     x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n \n     # Block 5\n-    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv1')(x)\n-    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv2')(x)\n-    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv3')(x)\n-    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv4')(x)\n+    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n+    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n+    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n+    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv4')(x)\n     x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n \n     if include_top:\n@@ -136,9 +136,9 @@ def VGG19(include_top=True, weights='imagenet',\n         x = Dense(classes, activation='softmax', name='predictions')(x)\n     else:\n         if pooling == 'avg':\n-            x = GlobalAveragePooling()(x)\n+            x = GlobalAveragePooling2D()(x)\n         elif pooling == 'max':\n-            x = GlobalMaxPooling()(x)\n+            x = GlobalMaxPooling2D()(x)\n \n     # Ensure that the model takes into account\n     # any potential predecessors of `input_tensor`.\n\n@@ -23,8 +23,9 @@ from __future__ import absolute_import\n import warnings\n \n from ..models import Model\n-from ..layers import Dense, Input, BatchNormalization, Activation, merge\n-from ..layers import Conv2D, SeparableConv2D, MaxPooling2D, GlobalAveragePooling2D\n+from .. import layers\n+from ..layers import Dense, Input, BatchNormalization, Activation\n+from ..layers import Conv2D, SeparableConv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n from ..engine.topology import get_source_inputs\n from ..utils.data_utils import get_file\n from .. import backend as K\n@@ -37,6 +38,7 @@ TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/relea\n \n def Xception(include_top=True, weights='imagenet',\n              input_tensor=None, input_shape=None,\n+             pooling=None,\n              classes=1000):\n     \"\"\"Instantiate the Xception architecture,\n     optionally loading weights pre-trained\n@@ -61,6 +63,17 @@ def Xception(include_top=True, weights='imagenet',\n             It should have exactly 3 inputs channels,\n             and width and height should be no smaller than 71.\n             E.g. `(150, 150, 3)` would be one valid value.\n+        pooling: Optional pooling mode for feature extraction\n+            when `include_top` is `False`.\n+            - `None` means that the output of the model will be\n+                the 4D tensor output of the\n+                last convolutional layer.\n+            - `avg` means that global average pooling\n+                will be applied to the output of the\n+                last convolutional layer, and thus\n+                the output of the model will be a 2D tensor.\n+            - `max` means that global max pooling will\n+                be applied.\n         classes: optional number of classes to classify images\n             into, only to be specified if `include_top` is True, and\n             if no `weights` argument is specified.\n@@ -110,95 +123,100 @@ def Xception(include_top=True, weights='imagenet',\n         else:\n             img_input = input_tensor\n \n-    x = Conv2D(32, 3, 3, subsample=(2, 2), bias=False, name='block1_conv1')(img_input)\n+    x = Conv2D(32, (3, 3), strides=(2, 2), use_bias=False, name='block1_conv1')(img_input)\n     x = BatchNormalization(name='block1_conv1_bn')(x)\n     x = Activation('relu', name='block1_conv1_act')(x)\n-    x = Conv2D(64, 3, 3, bias=False, name='block1_conv2')(x)\n+    x = Conv2D(64, (3, 3), use_bias=False, name='block1_conv2')(x)\n     x = BatchNormalization(name='block1_conv2_bn')(x)\n     x = Activation('relu', name='block1_conv2_act')(x)\n \n-    residual = Conv2D(128, 1, 1, subsample=(2, 2),\n-                      border_mode='same', bias=False)(x)\n+    residual = Conv2D(128, (1, 1), strides=(2, 2),\n+                      padding='same', use_bias=False)(x)\n     residual = BatchNormalization()(residual)\n \n-    x = SeparableConv2D(128, 3, 3, border_mode='same', bias=False, name='block2_sepconv1')(x)\n+    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1')(x)\n     x = BatchNormalization(name='block2_sepconv1_bn')(x)\n     x = Activation('relu', name='block2_sepconv2_act')(x)\n-    x = SeparableConv2D(128, 3, 3, border_mode='same', bias=False, name='block2_sepconv2')(x)\n+    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv2')(x)\n     x = BatchNormalization(name='block2_sepconv2_bn')(x)\n \n-    x = MaxPooling2D((3, 3), strides=(2, 2), border_mode='same', name='block2_pool')(x)\n-    x = merge([x, residual], mode='sum')\n+    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block2_pool')(x)\n+    x = layers.sum([x, residual])\n \n-    residual = Conv2D(256, 1, 1, subsample=(2, 2),\n-                      border_mode='same', bias=False)(x)\n+    residual = Conv2D(256, (1, 1), strides=(2, 2),\n+                      padding='same', use_bias=False)(x)\n     residual = BatchNormalization()(residual)\n \n     x = Activation('relu', name='block3_sepconv1_act')(x)\n-    x = SeparableConv2D(256, 3, 3, border_mode='same', bias=False, name='block3_sepconv1')(x)\n+    x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv1')(x)\n     x = BatchNormalization(name='block3_sepconv1_bn')(x)\n     x = Activation('relu', name='block3_sepconv2_act')(x)\n-    x = SeparableConv2D(256, 3, 3, border_mode='same', bias=False, name='block3_sepconv2')(x)\n+    x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv2')(x)\n     x = BatchNormalization(name='block3_sepconv2_bn')(x)\n \n-    x = MaxPooling2D((3, 3), strides=(2, 2), border_mode='same', name='block3_pool')(x)\n-    x = merge([x, residual], mode='sum')\n+    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block3_pool')(x)\n+    x = layers.sum([x, residual])\n \n-    residual = Conv2D(728, 1, 1, subsample=(2, 2),\n-                      border_mode='same', bias=False)(x)\n+    residual = Conv2D(728, (1, 1), strides=(2, 2),\n+                      padding='same', use_bias=False)(x)\n     residual = BatchNormalization()(residual)\n \n     x = Activation('relu', name='block4_sepconv1_act')(x)\n-    x = SeparableConv2D(728, 3, 3, border_mode='same', bias=False, name='block4_sepconv1')(x)\n+    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv1')(x)\n     x = BatchNormalization(name='block4_sepconv1_bn')(x)\n     x = Activation('relu', name='block4_sepconv2_act')(x)\n-    x = SeparableConv2D(728, 3, 3, border_mode='same', bias=False, name='block4_sepconv2')(x)\n+    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv2')(x)\n     x = BatchNormalization(name='block4_sepconv2_bn')(x)\n \n-    x = MaxPooling2D((3, 3), strides=(2, 2), border_mode='same', name='block4_pool')(x)\n-    x = merge([x, residual], mode='sum')\n+    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block4_pool')(x)\n+    x = layers.sum([x, residual])\n \n     for i in range(8):\n         residual = x\n         prefix = 'block' + str(i + 5)\n \n         x = Activation('relu', name=prefix + '_sepconv1_act')(x)\n-        x = SeparableConv2D(728, 3, 3, border_mode='same', bias=False, name=prefix + '_sepconv1')(x)\n+        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv1')(x)\n         x = BatchNormalization(name=prefix + '_sepconv1_bn')(x)\n         x = Activation('relu', name=prefix + '_sepconv2_act')(x)\n-        x = SeparableConv2D(728, 3, 3, border_mode='same', bias=False, name=prefix + '_sepconv2')(x)\n+        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv2')(x)\n         x = BatchNormalization(name=prefix + '_sepconv2_bn')(x)\n         x = Activation('relu', name=prefix + '_sepconv3_act')(x)\n-        x = SeparableConv2D(728, 3, 3, border_mode='same', bias=False, name=prefix + '_sepconv3')(x)\n+        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv3')(x)\n         x = BatchNormalization(name=prefix + '_sepconv3_bn')(x)\n \n-        x = merge([x, residual], mode='sum')\n+        x = layers.sum([x, residual])\n \n-    residual = Conv2D(1024, 1, 1, subsample=(2, 2),\n-                      border_mode='same', bias=False)(x)\n+    residual = Conv2D(1024, (1, 1), strides=(2, 2),\n+                      padding='same', use_bias=False)(x)\n     residual = BatchNormalization()(residual)\n \n     x = Activation('relu', name='block13_sepconv1_act')(x)\n-    x = SeparableConv2D(728, 3, 3, border_mode='same', bias=False, name='block13_sepconv1')(x)\n+    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block13_sepconv1')(x)\n     x = BatchNormalization(name='block13_sepconv1_bn')(x)\n     x = Activation('relu', name='block13_sepconv2_act')(x)\n-    x = SeparableConv2D(1024, 3, 3, border_mode='same', bias=False, name='block13_sepconv2')(x)\n+    x = SeparableConv2D(1024, (3, 3), padding='same', use_bias=False, name='block13_sepconv2')(x)\n     x = BatchNormalization(name='block13_sepconv2_bn')(x)\n \n-    x = MaxPooling2D((3, 3), strides=(2, 2), border_mode='same', name='block13_pool')(x)\n-    x = merge([x, residual], mode='sum')\n+    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block13_pool')(x)\n+    x = layers.sum([x, residual])\n \n-    x = SeparableConv2D(1536, 3, 3, border_mode='same', bias=False, name='block14_sepconv1')(x)\n+    x = SeparableConv2D(1536, (3, 3), padding='same', use_bias=False, name='block14_sepconv1')(x)\n     x = BatchNormalization(name='block14_sepconv1_bn')(x)\n     x = Activation('relu', name='block14_sepconv1_act')(x)\n \n-    x = SeparableConv2D(2048, 3, 3, border_mode='same', bias=False, name='block14_sepconv2')(x)\n+    x = SeparableConv2D(2048, (3, 3), padding='same', use_bias=False, name='block14_sepconv2')(x)\n     x = BatchNormalization(name='block14_sepconv2_bn')(x)\n     x = Activation('relu', name='block14_sepconv2_act')(x)\n \n     if include_top:\n         x = GlobalAveragePooling2D(name='avg_pool')(x)\n         x = Dense(classes, activation='softmax', name='predictions')(x)\n+    else:\n+        if pooling == 'avg':\n+            x = GlobalAveragePooling2D()(x)\n+        elif pooling == 'max':\n+            x = GlobalMaxPooling2D()(x)\n \n     # Ensure that the model takes into account\n     # any potential predecessors of `input_tensor`.\n\n@@ -329,7 +329,6 @@ class Reshape(Layer):\n     def __init__(self, target_shape, **kwargs):\n         super(Reshape, self).__init__(**kwargs)\n         self.target_shape = tuple(target_shape)\n-        self.input_spec = InputSpec(ndim=len(self.target_shape) + 1)\n \n     def _fix_unknown_dimension(self, input_shape, output_shape):\n         \"\"\"Find and replace a missing dimension in an output shape.\n\n@@ -105,11 +105,9 @@ class Concatenate(_Merge):\n \n     def build(self, input_shape):\n         # Used purely for shape validation.\n-        if not isinstance(input_shape, list) or len(input_shape) != 2:\n+        if not isinstance(input_shape, list):\n             raise ValueError('`Concatenate` layer should be called '\n-                             'on a list of 2 inputs.')\n-        if input_shape[0] is None or input_shape[1] is None:\n-            return\n+                             'on a list of inputs')\n         reduced_inputs_shapes = [list(shape) for shape in input_shape]\n         shape_set = set()\n         for i in range(len(reduced_inputs_shapes)):\n@@ -124,13 +122,13 @@ class Concatenate(_Merge):\n     def call(self, inputs):\n         if not isinstance(inputs, list):\n             raise ValueError('A `Concatenate` layer should be called '\n-                             'on a list of 2 inputs.')\n+                             'on a list of inputs.')\n         return K.concatenate(inputs, axis=self.axis)\n \n     def compute_output_shape(self, input_shape):\n-        if not isinstance(input_shape, list) or len(input_shape) != 2:\n+        if not isinstance(input_shape, list):\n             raise ValueError('A `Concatenate` layer should be called '\n-                             'on a list of 2 inputs.')\n+                             'on a list of inputs.')\n         input_shapes = input_shape\n         output_shape = list(input_shapes[0])\n         for shape in input_shapes[1:]:\n\n@@ -118,7 +118,7 @@ class _Pooling2D(Layer):\n         if strides is None:\n             strides = pool_size\n         self.pool_size = conv_utils.normalize_tuple(pool_size, 2, 'pool_size')\n-        self.strides = conv_utils.normalize_tuple(pool_size, 2, 'strides')\n+        self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n         self.padding = conv_utils.normalize_padding(padding)\n         self.data_format = conv_utils.normalize_data_format(data_format)\n         self.input_spec = InputSpec(ndim=4)\n\n@@ -269,7 +269,7 @@ def test_CSVLogger():\n \n @keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason=\"Requires tensorflow backend\")\n+                    reason='Requires tensorflow backend')\n def test_TensorBoard():\n     np.random.seed(1337)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
