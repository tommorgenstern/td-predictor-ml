{"custom_id": "keras#16fc9cd173118e5405ad1f7ee9640dbf47fde14a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 755 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 11 | Methods Changed: 30 | Complexity Δ (Sum/Max): 24/24 | Churn Δ: 756 | Churn Cumulative: 2502 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,492 @@\n+\"\"\"\n+Title: Zero-DCE for low-light image enhancement\n+Author: [Soumik Rakshit](http://github.com/soumik12345)\n+Converted to Keras Core by: [Soumik Rakshit](http://github.com/soumik12345)\n+Date created: 2021/09/18\n+Last modified: 2023/07/15\n+Description: Implementing Zero-Reference Deep Curve Estimation for low-light image enhancement.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+**Zero-Reference Deep Curve Estimation** or **Zero-DCE** formulates low-light image\n+enhancement as the task of estimating an image-specific\n+[*tonal curve*](https://en.wikipedia.org/wiki/Curve_(tonality)) with a deep neural network.\n+In this example, we train a lightweight deep network, **DCE-Net**, to estimate\n+pixel-wise and high-order tonal curves for dynamic range adjustment of a given image.\n+\n+Zero-DCE takes a low-light image as input and produces high-order tonal curves as its output.\n+These curves are then used for pixel-wise adjustment on the dynamic range of the input to\n+obtain an enhanced image. The curve estimation process is done in such a way that it maintains\n+the range of the enhanced image and preserves the contrast of neighboring pixels. This\n+curve estimation is inspired by curves adjustment used in photo editing software such as\n+Adobe Photoshop where users can adjust points throughout an image’s tonal range.\n+\n+Zero-DCE is appealing because of its relaxed assumptions with regard to reference images:\n+it does not require any input/output image pairs during training.\n+This is achieved through a set of carefully formulated non-reference loss functions,\n+which implicitly measure the enhancement quality and guide the training of the network.\n+\n+### References\n+\n+- [Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement](https://arxiv.org/pdf/2001.06826.pdf)\n+- [Curves adjustment in Adobe Photoshop](https://helpx.adobe.com/photoshop/using/curves-adjustment.html)\n+\"\"\"\n+\n+\"\"\"\n+## Downloading LOLDataset\n+\n+The **LoL Dataset** has been created for low-light image enhancement. It provides 485\n+images for training and 15 for testing. Each image pair in the dataset consists of a\n+low-light input image and its corresponding well-exposed reference image.\n+\"\"\"\n+\n+import os\n+import random\n+import numpy as np\n+from glob import glob\n+from PIL import Image, ImageOps\n+import matplotlib.pyplot as plt\n+\n+import keras_core as keras\n+from keras_core import layers\n+\n+import tensorflow as tf\n+\n+\"\"\"shell\n+wget https://huggingface.co/datasets/geekyrakshit/LoL-Dataset/resolve/main/lol_dataset.zip\n+unzip -q lol_dataset.zip && rm lol_dataset.zip\n+\"\"\"\n+\n+\"\"\"\n+## Creating a TensorFlow Dataset\n+\n+We use 300 low-light images from the LoL Dataset training set for training, and we use\n+the remaining 185 low-light images for validation. We resize the images to size `256 x\n+256` to be used for both training and validation. Note that in order to train the DCE-Net,\n+we will not require the corresponding enhanced images.\n+\"\"\"\n+\n+IMAGE_SIZE = 256\n+BATCH_SIZE = 16\n+MAX_TRAIN_IMAGES = 400\n+\n+\n+def load_data(image_path):\n+    image = tf.io.read_file(image_path)\n+    image = tf.image.decode_png(image, channels=3)\n+    image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n+    image = image / 255.0\n+    return image\n+\n+\n+def data_generator(low_light_images):\n+    dataset = tf.data.Dataset.from_tensor_slices((low_light_images))\n+    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n+    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n+    return dataset\n+\n+\n+train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\n+val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\n+test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\"))\n+\n+\n+train_dataset = data_generator(train_low_light_images)\n+val_dataset = data_generator(val_low_light_images)\n+\n+print(\"Train Dataset:\", train_dataset)\n+print(\"Validation Dataset:\", val_dataset)\n+\n+\"\"\"\n+## The Zero-DCE Framework\n+\n+The goal of DCE-Net is to estimate a set of best-fitting light-enhancement curves\n+(LE-curves) given an input image. The framework then maps all pixels of the input’s RGB\n+channels by applying the curves iteratively to obtain the final enhanced image.\n+\n+### Understanding light-enhancement curves\n+\n+A ligh-enhancement curve is a kind of curve that can map a low-light image\n+to its enhanced version automatically,\n+where the self-adaptive curve parameters are solely dependent on the input image.\n+When designing such a curve, three objectives should be taken into account:\n+\n+- Each pixel value of the enhanced image should be in the normalized range `[0,1]`, in order to\n+avoid information loss induced by overflow truncation.\n+- It should be monotonous, to preserve the contrast between neighboring pixels.\n+- The shape of this curve should be as simple as possible,\n+and the curve should be differentiable to allow backpropagation.\n+\n+The light-enhancement curve is separately applied to three RGB channels instead of solely on the\n+illumination channel. The three-channel adjustment can better preserve the inherent color and reduce\n+the risk of over-saturation.\n+\n+![](https://li-chongyi.github.io/Zero-DCE_files/framework.png)\n+\n+### DCE-Net\n+\n+The DCE-Net is a lightweight deep neural network that learns the mapping between an input\n+image and its best-fitting curve parameter maps. The input to the DCE-Net is a low-light\n+image while the outputs are a set of pixel-wise curve parameter maps for corresponding\n+higher-order curves. It is a plain CNN of seven convolutional layers with symmetrical\n+concatenation. Each layer consists of 32 convolutional kernels of size 3×3 and stride 1\n+followed by the ReLU activation function. The last convolutional layer is followed by the\n+Tanh activation function, which produces 24 parameter maps for 8 iterations, where each\n+iteration requires three curve parameter maps for the three channels.\n+\n+![](https://i.imgur.com/HtIg34W.png)\n+\"\"\"\n+\n+\n+def build_dce_net():\n+    input_img = keras.Input(shape=[None, None, 3])\n+    conv1 = layers.Conv2D(\n+        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n+    )(input_img)\n+    conv2 = layers.Conv2D(\n+        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n+    )(conv1)\n+    conv3 = layers.Conv2D(\n+        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n+    )(conv2)\n+    conv4 = layers.Conv2D(\n+        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n+    )(conv3)\n+    int_con1 = layers.Concatenate(axis=-1)([conv4, conv3])\n+    conv5 = layers.Conv2D(\n+        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n+    )(int_con1)\n+    int_con2 = layers.Concatenate(axis=-1)([conv5, conv2])\n+    conv6 = layers.Conv2D(\n+        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n+    )(int_con2)\n+    int_con3 = layers.Concatenate(axis=-1)([conv6, conv1])\n+    x_r = layers.Conv2D(24, (3, 3), strides=(1, 1), activation=\"tanh\", padding=\"same\")(\n+        int_con3\n+    )\n+    return keras.Model(inputs=input_img, outputs=x_r)\n+\n+\n+\"\"\"\n+## Loss functions\n+\n+To enable zero-reference learning in DCE-Net, we use a set of differentiable\n+zero-reference losses that allow us to evaluate the quality of enhanced images.\n+\"\"\"\n+\n+\"\"\"\n+### Color constancy loss\n+\n+The *color constancy loss* is used to correct the potential color deviations in the\n+enhanced image.\n+\"\"\"\n+\n+\n+def color_constancy_loss(x):\n+    mean_rgb = tf.reduce_mean(x, axis=(1, 2), keepdims=True)\n+    mr, mg, mb = mean_rgb[:, :, :, 0], mean_rgb[:, :, :, 1], mean_rgb[:, :, :, 2]\n+    d_rg = tf.square(mr - mg)\n+    d_rb = tf.square(mr - mb)\n+    d_gb = tf.square(mb - mg)\n+    return tf.sqrt(tf.square(d_rg) + tf.square(d_rb) + tf.square(d_gb))\n+\n+\n+\"\"\"\n+### Exposure loss\n+\n+To restrain under-/over-exposed regions, we use the *exposure control loss*.\n+It measures the distance between the average intensity value of a local region\n+and a preset well-exposedness level (set to `0.6`).\n+\"\"\"\n+\n+\n+def exposure_loss(x, mean_val=0.6):\n+    x = tf.reduce_mean(x, axis=3, keepdims=True)\n+    mean = tf.nn.avg_pool2d(x, ksize=16, strides=16, padding=\"VALID\")\n+    return tf.reduce_mean(tf.square(mean - mean_val))\n+\n+\n+\"\"\"\n+### Illumination smoothness loss\n+\n+To preserve the monotonicity relations between neighboring pixels, the\n+*illumination smoothness loss* is added to each curve parameter map.\n+\"\"\"\n+\n+\n+def illumination_smoothness_loss(x):\n+    batch_size = tf.shape(x)[0]\n+    h_x = tf.shape(x)[1]\n+    w_x = tf.shape(x)[2]\n+    count_h = (tf.shape(x)[2] - 1) * tf.shape(x)[3]\n+    count_w = tf.shape(x)[2] * (tf.shape(x)[3] - 1)\n+    h_tv = tf.reduce_sum(tf.square((x[:, 1:, :, :] - x[:, : h_x - 1, :, :])))\n+    w_tv = tf.reduce_sum(tf.square((x[:, :, 1:, :] - x[:, :, : w_x - 1, :])))\n+    batch_size = tf.cast(batch_size, dtype=tf.float32)\n+    count_h = tf.cast(count_h, dtype=tf.float32)\n+    count_w = tf.cast(count_w, dtype=tf.float32)\n+    return 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n+\n+\n+\"\"\"\n+### Spatial consistency loss\n+\n+The *spatial consistency loss* encourages spatial coherence of the enhanced image by\n+preserving the contrast between neighboring regions across the input image and its enhanced version.\n+\"\"\"\n+\n+\n+class SpatialConsistencyLoss(keras.losses.Loss):\n+    def __init__(self, **kwargs):\n+        super().__init__(reduction=\"none\")\n+\n+        self.left_kernel = tf.constant(\n+            [[[[0, 0, 0]], [[-1, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n+        )\n+        self.right_kernel = tf.constant(\n+            [[[[0, 0, 0]], [[0, 1, -1]], [[0, 0, 0]]]], dtype=tf.float32\n+        )\n+        self.up_kernel = tf.constant(\n+            [[[[0, -1, 0]], [[0, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n+        )\n+        self.down_kernel = tf.constant(\n+            [[[[0, 0, 0]], [[0, 1, 0]], [[0, -1, 0]]]], dtype=tf.float32\n+        )\n+\n+    def call(self, y_true, y_pred):\n+        original_mean = tf.reduce_mean(y_true, 3, keepdims=True)\n+        enhanced_mean = tf.reduce_mean(y_pred, 3, keepdims=True)\n+        original_pool = tf.nn.avg_pool2d(\n+            original_mean, ksize=4, strides=4, padding=\"VALID\"\n+        )\n+        enhanced_pool = tf.nn.avg_pool2d(\n+            enhanced_mean, ksize=4, strides=4, padding=\"VALID\"\n+        )\n+\n+        d_original_left = tf.nn.conv2d(\n+            original_pool, self.left_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n+        )\n+        d_original_right = tf.nn.conv2d(\n+            original_pool, self.right_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n+        )\n+        d_original_up = tf.nn.conv2d(\n+            original_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n+        )\n+        d_original_down = tf.nn.conv2d(\n+            original_pool, self.down_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n+        )\n+\n+        d_enhanced_left = tf.nn.conv2d(\n+            enhanced_pool, self.left_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n+        )\n+        d_enhanced_right = tf.nn.conv2d(\n+            enhanced_pool, self.right_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n+        )\n+        d_enhanced_up = tf.nn.conv2d(\n+            enhanced_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n+        )\n+        d_enhanced_down = tf.nn.conv2d(\n+            enhanced_pool, self.down_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n+        )\n+\n+        d_left = tf.square(d_original_left - d_enhanced_left)\n+        d_right = tf.square(d_original_right - d_enhanced_right)\n+        d_up = tf.square(d_original_up - d_enhanced_up)\n+        d_down = tf.square(d_original_down - d_enhanced_down)\n+        return d_left + d_right + d_up + d_down\n+\n+\n+\"\"\"\n+### Deep curve estimation model\n+\n+We implement the Zero-DCE framework as a Keras subclassed model.\n+\"\"\"\n+\n+\n+class ZeroDCE(keras.Model):\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+        self.dce_model = build_dce_net()\n+\n+    def compile(self, learning_rate, **kwargs):\n+        super().compile(**kwargs)\n+        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n+        self.spatial_constancy_loss = SpatialConsistencyLoss(reduction=\"none\")\n+        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n+        self.illumination_smoothness_loss_tracker = keras.metrics.Mean(name=\"illumination_smoothness_loss\")\n+        self.spatial_constancy_loss_tracker = keras.metrics.Mean(name=\"spatial_constancy_loss\")\n+        self.color_constancy_loss_tracker = keras.metrics.Mean(name=\"color_constancy_loss\")\n+        self.exposure_loss_tracker = keras.metrics.Mean(name=\"exposure_loss\")\n+\n+    @property\n+    def metrics(self):\n+        return [\n+            self.total_loss_tracker,\n+            self.illumination_smoothness_loss_tracker,\n+            self.spatial_constancy_loss_tracker,\n+            self.color_constancy_loss_tracker,\n+            self.exposure_loss_tracker,\n+        ]\n+\n+    def get_enhanced_image(self, data, output):\n+        r1 = output[:, :, :, :3]\n+        r2 = output[:, :, :, 3:6]\n+        r3 = output[:, :, :, 6:9]\n+        r4 = output[:, :, :, 9:12]\n+        r5 = output[:, :, :, 12:15]\n+        r6 = output[:, :, :, 15:18]\n+        r7 = output[:, :, :, 18:21]\n+        r8 = output[:, :, :, 21:24]\n+        x = data + r1 * (tf.square(data) - data)\n+        x = x + r2 * (tf.square(x) - x)\n+        x = x + r3 * (tf.square(x) - x)\n+        enhanced_image = x + r4 * (tf.square(x) - x)\n+        x = enhanced_image + r5 * (tf.square(enhanced_image) - enhanced_image)\n+        x = x + r6 * (tf.square(x) - x)\n+        x = x + r7 * (tf.square(x) - x)\n+        enhanced_image = x + r8 * (tf.square(x) - x)\n+        return enhanced_image\n+\n+    def call(self, data):\n+        dce_net_output = self.dce_model(data)\n+        return self.get_enhanced_image(data, dce_net_output)\n+\n+    def compute_losses(self, data, output):\n+        enhanced_image = self.get_enhanced_image(data, output)\n+        loss_illumination = 200 * illumination_smoothness_loss(output)\n+        loss_spatial_constancy = tf.reduce_mean(\n+            self.spatial_constancy_loss(enhanced_image, data)\n+        )\n+        loss_color_constancy = 5 * tf.reduce_mean(color_constancy_loss(enhanced_image))\n+        loss_exposure = 10 * tf.reduce_mean(exposure_loss(enhanced_image))\n+        total_loss = (\n+            loss_illumination\n+            + loss_spatial_constancy\n+            + loss_color_constancy\n+            + loss_exposure\n+        )\n+\n+        return {\n+            \"total_loss\": total_loss,\n+            \"illumination_smoothness_loss\": loss_illumination,\n+            \"spatial_constancy_loss\": loss_spatial_constancy,\n+            \"color_constancy_loss\": loss_color_constancy,\n+            \"exposure_loss\": loss_exposure,\n+        }\n+\n+    def train_step(self, data):\n+        with tf.GradientTape() as tape:\n+            output = self.dce_model(data)\n+            losses = self.compute_losses(data, output)\n+\n+        gradients = tape.gradient(\n+            losses[\"total_loss\"], self.dce_model.trainable_weights\n+        )\n+        self.optimizer.apply_gradients(zip(gradients, self.dce_model.trainable_weights))\n+\n+        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n+        self.illumination_smoothness_loss_tracker.update_state(losses[\"illumination_smoothness_loss\"])\n+        self.spatial_constancy_loss_tracker.update_state(losses[\"spatial_constancy_loss\"])\n+        self.color_constancy_loss_tracker.update_state(losses[\"color_constancy_loss\"])\n+        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n+\n+        return {metric.name: metric.result() for metric in self.metrics}\n+\n+    def test_step(self, data):\n+        output = self.dce_model(data)\n+        losses = self.compute_losses(data, output)\n+\n+        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n+        self.illumination_smoothness_loss_tracker.update_state(losses[\"illumination_smoothness_loss\"])\n+        self.spatial_constancy_loss_tracker.update_state(losses[\"spatial_constancy_loss\"])\n+        self.color_constancy_loss_tracker.update_state(losses[\"color_constancy_loss\"])\n+        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n+\n+        return {metric.name: metric.result() for metric in self.metrics}\n+\n+    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n+        \"\"\"While saving the weights, we simply save the weights of the DCE-Net\"\"\"\n+        self.dce_model.save_weights(\n+            filepath, overwrite=overwrite, save_format=save_format, options=options\n+        )\n+\n+    def load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None):\n+        \"\"\"While loading the weights, we simply load the weights of the DCE-Net\"\"\"\n+        self.dce_model.load_weights(\n+            filepath=filepath,\n+            by_name=by_name,\n+            skip_mismatch=skip_mismatch,\n+            options=options,\n+        )\n+\n+\n+\"\"\"\n+## Training\n+\"\"\"\n+\n+zero_dce_model = ZeroDCE()\n+zero_dce_model.compile(learning_rate=1e-4)\n+history = zero_dce_model.fit(train_dataset, validation_data=val_dataset, epochs=100)\n+\n+\n+def plot_result(item):\n+    plt.plot(history.history[item], label=item)\n+    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n+    plt.xlabel(\"Epochs\")\n+    plt.ylabel(item)\n+    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n+    plt.legend()\n+    plt.grid()\n+    plt.show()\n+\n+\n+plot_result(\"total_loss\")\n+plot_result(\"illumination_smoothness_loss\")\n+plot_result(\"spatial_constancy_loss\")\n+plot_result(\"color_constancy_loss\")\n+plot_result(\"exposure_loss\")\n+\n+\"\"\"\n+## Inference\n+\"\"\"\n+\n+\n+def plot_results(images, titles, figure_size=(12, 12)):\n+    fig = plt.figure(figsize=figure_size)\n+    for i in range(len(images)):\n+        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n+        _ = plt.imshow(images[i])\n+        plt.axis(\"off\")\n+    plt.show()\n+\n+\n+def infer(original_image):\n+    image = keras.utils.img_to_array(original_image)\n+    image = image.astype(\"float32\") / 255.0\n+    image = np.expand_dims(image, axis=0)\n+    output_image = zero_dce_model(image)\n+    output_image = tf.cast((output_image[0, :, :, :] * 255), dtype=np.uint8)\n+    output_image = Image.fromarray(output_image.numpy())\n+    return output_image\n+\n+\n+\"\"\"\n+### Inference on test images\n+\n+We compare the test images from LOLDataset enhanced by MIRNet with images enhanced via\n+the `PIL.ImageOps.autocontrast()` function.\n+\n+You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/low-light-image-enhancement)\n+and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/low-light-image-enhancement).\n+\"\"\"\n+\n+for val_image_file in test_low_light_images:\n+    original_image = Image.open(val_image_file)\n+    enhanced_image = infer(original_image)\n+    plot_results(\n+        [original_image, ImageOps.autocontrast(original_image), enhanced_image],\n+        [\"Original\", \"PIL Autocontrast\", \"Enhanced\"],\n+        (20, 12),\n+    )\n\n@@ -35,6 +35,30 @@ class SegmentSum(Operation):\n \n @keras_core_export(\"keras_core.ops.segment_sum\")\n def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n+    \"\"\"Computes the sum of segments in a tensor.\n+\n+    Args:\n+        data: Input tensor.\n+        segment_ids: A 1-D tensor containing segment indices for each\n+            element in `data`.\n+        num_segments: An integer representing the total number of\n+            segments. If not specified, it is inferred from the maximum\n+            value in `segment_ids`.\n+        sorted: A boolean indicating whether `segment_ids` is sorted.\n+            Default is `False`.\n+\n+    Returns:\n+        A tensor containing the sum of segments, where each element\n+        represents the sum of the corresponding segment in `data`.\n+\n+    Example:\n+    ```python\n+    >>> data = keras_core.ops.convert_to_tensor([1, 2, 3, 4, 5, 6])\n+    >>> segment_ids = keras_core.ops.convert_to_tensor([0, 1, 0, 1, 0, 1])\n+    >>> segment_sum(data, segment_ids)\n+    array([9 12], shape=(2,), dtype=int32)\n+    ```\n+    \"\"\"\n     if any_symbolic_tensors((data,)):\n         return SegmentSum(num_segments, sorted).symbolic_call(data, segment_ids)\n     return backend.math.segment_sum(\n@@ -63,6 +87,29 @@ class TopK(Operation):\n \n @keras_core_export(\"keras_core.ops.top_k\")\n def top_k(x, k, sorted=True):\n+    \"\"\"Finds the top-k values and their indices in a tensor.\n+\n+    Args:\n+        x: Input tensor.\n+        k: An integer representing the number of top elements to retrieve.\n+        sorted: A boolean indicating whether to sort the output in\n+        descending order. Default is `True`.\n+\n+    Returns:\n+        A tuple containing two tensors. The first tensor contains the\n+        top-k values, and the second tensor contains the indices of the\n+        top-k values in the input tensor.\n+\n+    Example:\n+    ```python\n+    >>> x = keras_core.ops.convert_to_tensor([5, 2, 7, 1, 9, 3])\n+    >>> values, indices = top_k(x, k=3)\n+    >>> print(values)\n+    array([9 7 5], shape=(3,), dtype=int32)\n+    >>> print(indices)\n+    array([4 2 0], shape=(3,), dtype=int32)\n+    ```\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return TopK(k, sorted).symbolic_call(x)\n     return backend.math.top_k(x, k, sorted)\n@@ -82,6 +129,28 @@ class InTopK(Operation):\n \n @keras_core_export(\"keras_core.ops.in_top_k\")\n def in_top_k(targets, predictions, k):\n+    \"\"\"Checks if the targets are in the top-k predictions.\n+\n+    Args:\n+        targets: A tensor of true labels.\n+        predictions: A tensor of predicted labels.\n+        k: An integer representing the number of predictions to consider.\n+\n+    Returns:\n+        A boolean tensor of the same shape as `targets`, where each element\n+        indicates whether the corresponding target is in the top-k predictions.\n+        \n+    Example:\n+    ```python\n+    >>> targets = keras_core.ops.convert_to_tensor([2, 5, 3])\n+    >>> predictions = keras_core.ops.convert_to_tensor(\n+                                                    [[0.1, 0.4, 0.6, 0.9, 0.5],\n+                                                    [0.1, 0.7, 0.9, 0.8, 0.3],\n+                                                    [0.1, 0.6, 0.9, 0.9, 0.5]])\n+    >>> in_top_k(targets, predictions, k=3)\n+    array([ True False  True], shape=(3,), dtype=bool)\n+    ```\n+    \"\"\"\n     if any_symbolic_tensors((targets, predictions)):\n         return InTopK(k).symbolic_call(targets, predictions)\n     return backend.math.in_top_k(targets, predictions, k)\n@@ -103,6 +172,27 @@ class Logsumexp(Operation):\n \n @keras_core_export(\"keras_core.ops.logsumexp\")\n def logsumexp(x, axis=None, keepdims=False):\n+    \"\"\"Computes the logarithm of sum of exponentials of elements in a tensor.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: An integer or a tuple of integers specifying the axis/axes\n+            along which to compute the sum. If `None`, the sum is computed\n+            over all elements. Default is `None`.\n+        keepdims: A boolean indicating whether to keep the dimensions of\n+            the input tensor when computing the sum. Default is `False`.\n+\n+    Returns:\n+        A tensor containing the logarithm of the sum of exponentials of\n+        elements in `x`.\n+\n+    Example:\n+    ```python\n+    >>> x = keras_core.ops.convert_to_tensor([1., 2., 3.])\n+    >>> logsumexp(x)\n+    array(3.407606, shape=(), dtype=float32)\n+    ```\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Logsumexp(axis, keepdims).symbolic_call(x)\n     return backend.math.logsumexp(x, axis=axis, keepdims=keepdims)\n@@ -152,6 +242,30 @@ class Qr(Operation):\n \n @keras_core_export(\"keras_core.ops.qr\")\n def qr(x, mode=\"reduced\"):\n+    \"\"\"Computes the QR decomposition of a tensor.\n+\n+    Args:\n+        x: Input tensor.\n+        mode: A string specifying the mode of the QR decomposition.\n+            - 'reduced': Returns the reduced QR decomposition. (default)\n+            - 'complete': Returns the complete QR decomposition.\n+\n+    Returns:\n+        A tuple containing two tensors. The first tensor represents the\n+        orthogonal matrix Q, and the second tensor represents the upper\n+        triangular matrix R.\n+\n+    Example:\n+    ```python\n+    >>> x = keras_core.ops.convert_to_tensor([[1., 2.], [3., 4.], [5., 6.]])\n+    >>> q, r = qr(x)\n+    >>> print(q)\n+    array([[-0.16903079  0.897085]\n+     [-0.5070925   0.2760267 ]\n+     [-0.8451542  -0.34503305]], shape=(3, 2), dtype=float32)\n+    ```\n+    \"\"\"\n+\n     if any_symbolic_tensors((x,)):\n         return Qr(mode=mode).symbolic_call(x)\n     return backend.math.qr(x, mode=mode)\n\n@@ -895,7 +895,7 @@ def conv_transpose(\n             the output tensor. Can be a single integer to specify the same\n             value for all spatial dimensions. The amount of output padding\n             along a given dimension must be lower than the stride along that\n-            same dimension. If set to None (default), the output shape is\n+            same dimension. If set to `None` (default), the output shape is\n             inferred.\n         data_format: A string, either `\"channels_last\"` or `\"channels_first\"`.\n             `data_format` determines the ordering of the dimensions in the\n@@ -955,6 +955,38 @@ class OneHot(Operation):\n \n @keras_core_export([\"keras_core.ops.one_hot\", \"keras_core.ops.nn.one_hot\"])\n def one_hot(x, num_classes, axis=-1, dtype=None):\n+    \"\"\"Converts integer tensor `x` into a one-hot tensor.\n+\n+    The one-hot encoding is a representation where each integer value is\n+    converted into a binary vector with a length equal to `num_classes`,\n+    and the index corresponding to the integer value is marked as 1, while\n+    all other indices are marked as 0.\n+\n+    Args:\n+        x : Integer tensor to be encoded. The shape can be\n+            arbitrary, but the dtype should be integer.\n+        num_classes: Number of classes for the one-hot encoding.\n+        axis: Axis along which the encoding is performed. Default is\n+            -1, which represents the last axis.\n+        dtype: (Optional) Data type of the output tensor. If not\n+            provided, it defaults to the default data type of the backend.\n+\n+    Returns:\n+        Integer tensor: One-hot encoded tensor with the same shape as `x`\n+        except for the specified `axis` dimension, which will have\n+        a length of `num_classes`. The dtype of the output tensor\n+        is determined by `dtype` or the default data type of the backend.\n+\n+    Example:\n+    ```python\n+    >>> x = keras_core.ops.convert_to_tensor([1, 3, 2, 0])\n+    >>> one_hot(x, num_classes=4)\n+    array([[0. 1. 0. 0.]\n+     [0. 0. 0. 1.]\n+     [0. 0. 1. 0.]\n+     [1. 0. 0. 0.]], shape=(4, 4), dtype=float32)\n+    ```\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return OneHot(num_classes, axis=axis, dtype=dtype).symbolic_call(x)\n     return backend.nn.one_hot(\n@@ -989,6 +1021,40 @@ class BinaryCrossentropy(Operation):\n     ]\n )\n def binary_crossentropy(target, output, from_logits=False):\n+    \"\"\"Computes binary cross-entropy loss between target and output tensor.\n+\n+    The binary cross-entropy loss is commonly used in binary\n+    classification tasks where each input sample belongs to one\n+    of the two classes. It measures the dissimilarity between the\n+    target and output probabilities or logits.\n+\n+    Args:\n+        target: The target tensor representing the true binary labels.\n+            Its shape should match the shape of the `output` tensor.\n+        output: The output tensor representing the predicted probabilities\n+            or logits. Its shape should match the shape of the\n+            `target` tensor.\n+        from_logits: (optional) Whether `output` is a tensor of logits or\n+            probabilities.\n+            Set it to `True` if `output` represents logits; otherwise,\n+            set it to `False` if `output` represents probabilities.\n+            Default is `False`.\n+\n+    Returns:\n+        Integer tensor: The computed binary cross-entropy loss between\n+        `target` and `output`.\n+\n+    Example:\n+    ```python\n+    >>> target = keras_core.ops.convert_to_tensor([0, 1, 1, 0],\n+                                                  dtype=float32)\n+    >>> output = keras_core.ops.convert_to_tensor([0.1, 0.9, 0.8, 0.2],\n+                                                  dtype=float32)\n+    >>> binary_crossentropy(target, output)\n+    array([0.10536054 0.10536054 0.22314355 0.22314355], \n+        shape=(4,), dtype=float32)\n+    ```\n+    \"\"\"\n     if any_symbolic_tensors((target, output)):\n         return BinaryCrossentropy(from_logits=from_logits).symbolic_call(\n             target, output\n@@ -1032,6 +1098,48 @@ class CategoricalCrossentropy(Operation):\n     ]\n )\n def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n+    \"\"\"Computes categorical cross-entropy loss between target and output tensor.\n+\n+    The categorical cross-entropy loss is commonly used in multi-class\n+    classification tasks where each input sample can belong to one of\n+    multiple classes. It measures the dissimilarity\n+    between the target and output probabilities or logits.\n+\n+    Args:\n+        target: The target tensor representing the true categorical labels.\n+            Its shape should match the shape of the `output` tensor\n+            except for the last dimension.\n+        output: The output tensor representing the predicted probabilities\n+            or logits. Its shape should match the shape of the `target`\n+            tensor except for the last dimension.\n+        from_logits: (optional) Whether `output` is a tensor of logits or\n+            probabilities.\n+            Set it to `True` if `output` represents logits; otherwise,\n+            set it to `False` if `output` represents probabilities.\n+            Default is `False`.\n+        axis: (optional) The axis along which the categorical cross-entropy\n+            is computed.\n+            Default is -1, which corresponds to the last dimension of\n+            the tensors.\n+\n+    Returns:\n+        Integer tensor: The computed categorical cross-entropy loss between\n+        `target` and `output`.\n+\n+    Example:\n+    ```python\n+    >>> target = keras_core.ops.convert_to_tensor([[1, 0, 0],\n+                                                   [0, 1, 0],\n+                                                   [0, 0, 1]],\n+                                                    dtype=float32)\n+    >>> output = keras_core.ops.convert_to_tensor([[0.9, 0.05, 0.05],\n+                                                   [0.1, 0.8, 0.1],\n+                                                   [0.2, 0.3, 0.5]],\n+                                                   dtype=float32)\n+    >>> categorical_crossentropy(target, output)\n+    array([0.10536054 0.22314355 0.6931472 ], shape=(3,), dtype=float32)\n+    ```\n+    \"\"\"\n     if any_symbolic_tensors((target, output)):\n         return CategoricalCrossentropy(\n             from_logits=from_logits, axis=axis\n@@ -1078,6 +1186,46 @@ class SparseCategoricalCrossentropy(Operation):\n     ]\n )\n def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n+    \"\"\"Computes sparse categorical cross-entropy loss.\n+\n+    The sparse categorical cross-entropy loss is similar to categorical\n+    cross-entropy, but it is used when the target tensor contains integer\n+    class labels instead of one-hot encoded vectors. It measures the\n+    dissimilarity between the target and output probabilities or logits.\n+\n+    Args:\n+        target: The target tensor representing the true class labels as integers.\n+            Its shape should match the shape of the `output` tensor except\n+            for the last dimension.\n+        output: The output tensor representing the predicted probabilities\n+            or logits.\n+            Its shape should match the shape of the `target` tensor except \n+            for the last dimension.\n+        from_logits: (optional) Whether `output` is a tensor of logits\n+            or probabilities.\n+            Set it to `True` if `output` represents logits; otherwise,\n+            set it to `False` if `output` represents probabilities.\n+            Default is `False`.\n+        axis: (optional) The axis along which the sparse categorical\n+            cross-entropy is computed.\n+            Default is -1, which corresponds to the last dimension\n+            of the tensors.\n+\n+    Returns:\n+        Integer tensor: The computed sparse categorical cross-entropy\n+        loss between `target` and `output`.\n+        \n+    Example:\n+    ```python\n+    >>> target = keras_core.ops.convert_to_tensor([0, 1, 2], dtype=int32)\n+    >>> output = keras_core.ops.convert_to_tensor([[0.9, 0.05, 0.05], \n+                                                   [0.1, 0.8, 0.1], \n+                                                   [0.2, 0.3, 0.5]],\n+                                                   dtype=float32)\n+    >>> sparse_categorical_crossentropy(target, output)\n+    array([0.10536056 0.22314355 0.6931472 ], shape=(3,), dtype=float32)\n+    ```\n+    \"\"\"\n     if any_symbolic_tensors((target, output)):\n         return SparseCategoricalCrossentropy(\n             from_logits=from_logits, axis=axis\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1a68df990af14e3d3a7fec2909a746cf96887bdc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 77 | Files Changed: 2 | Hunks: 23 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 110 | Churn Cumulative: 1136 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,4 @@\n-\"\"\"\n-segment_sum\n-top_k\n-in_top_k\n-logsumexp\n-\"\"\"\n+\"\"\"Commonly used math operations not included in NumPy.\"\"\"\n \n from keras_core import backend\n from keras_core.api_export import keras_core_export\n@@ -52,12 +47,11 @@ def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n         represents the sum of the corresponding segment in `data`.\n \n     Example:\n-    ```python\n+\n     >>> data = keras_core.ops.convert_to_tensor([1, 2, 3, 4, 5, 6])\n     >>> segment_ids = keras_core.ops.convert_to_tensor([0, 1, 0, 1, 0, 1])\n     >>> segment_sum(data, segment_ids)\n     array([9 12], shape=(2,), dtype=int32)\n-    ```\n     \"\"\"\n     if any_symbolic_tensors((data,)):\n         return SegmentSum(num_segments, sorted).symbolic_call(data, segment_ids)\n@@ -101,14 +95,14 @@ def top_k(x, k, sorted=True):\n         top-k values in the input tensor.\n \n     Example:\n-    ```python\n+\n     >>> x = keras_core.ops.convert_to_tensor([5, 2, 7, 1, 9, 3])\n     >>> values, indices = top_k(x, k=3)\n     >>> print(values)\n     array([9 7 5], shape=(3,), dtype=int32)\n     >>> print(indices)\n     array([4 2 0], shape=(3,), dtype=int32)\n-    ```\n+\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return TopK(k, sorted).symbolic_call(x)\n@@ -141,15 +135,14 @@ def in_top_k(targets, predictions, k):\n         indicates whether the corresponding target is in the top-k predictions.\n \n     Example:\n-    ```python\n+\n     >>> targets = keras_core.ops.convert_to_tensor([2, 5, 3])\n     >>> predictions = keras_core.ops.convert_to_tensor(\n-                                                    [[0.1, 0.4, 0.6, 0.9, 0.5],\n-                                                    [0.1, 0.7, 0.9, 0.8, 0.3],\n-                                                    [0.1, 0.6, 0.9, 0.9, 0.5]])\n+    ... [[0.1, 0.4, 0.6, 0.9, 0.5],\n+    ...  [0.1, 0.7, 0.9, 0.8, 0.3],\n+    ...  [0.1, 0.6, 0.9, 0.9, 0.5]])\n     >>> in_top_k(targets, predictions, k=3)\n     array([ True False  True], shape=(3,), dtype=bool)\n-    ```\n     \"\"\"\n     if any_symbolic_tensors((targets, predictions)):\n         return InTopK(k).symbolic_call(targets, predictions)\n@@ -187,11 +180,10 @@ def logsumexp(x, axis=None, keepdims=False):\n         elements in `x`.\n \n     Example:\n-    ```python\n+\n     >>> x = keras_core.ops.convert_to_tensor([1., 2., 3.])\n     >>> logsumexp(x)\n-    array(3.407606, shape=(), dtype=float32)\n-    ```\n+    3.407606\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Logsumexp(axis, keepdims).symbolic_call(x)\n@@ -256,14 +248,13 @@ def qr(x, mode=\"reduced\"):\n         triangular matrix R.\n \n     Example:\n-    ```python\n+\n     >>> x = keras_core.ops.convert_to_tensor([[1., 2.], [3., 4.], [5., 6.]])\n     >>> q, r = qr(x)\n     >>> print(q)\n     array([[-0.16903079  0.897085]\n            [-0.5070925   0.2760267 ]\n            [-0.8451542  -0.34503305]], shape=(3, 2), dtype=float32)\n-    ```\n     \"\"\"\n \n     if any_symbolic_tensors((x,)):\n\n@@ -1,33 +1,4 @@\n-\"\"\"\n-relu\n-relu6\n-sigmoid\n-softplus\n-softsign\n-silu\n-swish\n-log_sigmoid\n-leaky_relu\n-hard_sigmoid\n-elu\n-selu\n-gelu\n-softmax\n-log_softmax\n-\n-max_pooling\n-average_pooling\n-conv\n-depthwise_conv\n-separable_conv\n-conv_transpose\n-\n-one_hot\n-top_k\n-in_top_k\n-\n-ctc ??\n-\"\"\"\n+\"\"\"Commonly-used neural network operations not included in NumPy.\"\"\"\n \n from keras_core import backend\n from keras_core.api_export import keras_core_export\n@@ -978,14 +949,13 @@ def one_hot(x, num_classes, axis=-1, dtype=None):\n         is determined by `dtype` or the default data type of the backend.\n \n     Example:\n-    ```python\n+\n     >>> x = keras_core.ops.convert_to_tensor([1, 3, 2, 0])\n     >>> one_hot(x, num_classes=4)\n     array([[0. 1. 0. 0.]\n            [0. 0. 0. 1.]\n            [0. 0. 1. 0.]\n            [1. 0. 0. 0.]], shape=(4, 4), dtype=float32)\n-    ```\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return OneHot(num_classes, axis=axis, dtype=dtype).symbolic_call(x)\n@@ -1045,15 +1015,12 @@ def binary_crossentropy(target, output, from_logits=False):\n         `target` and `output`.\n \n     Example:\n-    ```python\n-    >>> target = keras_core.ops.convert_to_tensor([0, 1, 1, 0],\n-                                                  dtype=float32)\n-    >>> output = keras_core.ops.convert_to_tensor([0.1, 0.9, 0.8, 0.2],\n-                                                  dtype=float32)\n+\n+    >>> target = keras_core.ops.convert_to_tensor([0, 1, 1, 0])\n+    >>> output = keras_core.ops.convert_to_tensor([0.1, 0.9, 0.8, 0.2])\n     >>> binary_crossentropy(target, output)\n     array([0.10536054 0.10536054 0.22314355 0.22314355],\n           shape=(4,), dtype=float32)\n-    ```\n     \"\"\"\n     if any_symbolic_tensors((target, output)):\n         return BinaryCrossentropy(from_logits=from_logits).symbolic_call(\n@@ -1127,18 +1094,17 @@ def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n         `target` and `output`.\n \n     Example:\n-    ```python\n-    >>> target = keras_core.ops.convert_to_tensor([[1, 0, 0],\n-                                                   [0, 1, 0],\n-                                                   [0, 0, 1]],\n-                                                    dtype=float32)\n-    >>> output = keras_core.ops.convert_to_tensor([[0.9, 0.05, 0.05],\n-                                                   [0.1, 0.8, 0.1],\n-                                                   [0.2, 0.3, 0.5]],\n-                                                   dtype=float32)\n+\n+    >>> target = keras_core.ops.convert_to_tensor(\n+    ... [[1, 0, 0],\n+    ...  [0, 1, 0],\n+    ...  [0, 0, 1]])\n+    >>> output = keras_core.ops.convert_to_tensor(\n+    ... [[0.9, 0.05, 0.05],\n+    ...  [0.1, 0.8, 0.1],\n+    ...  [0.2, 0.3, 0.5]])\n     >>> categorical_crossentropy(target, output)\n     array([0.10536054 0.22314355 0.6931472 ], shape=(3,), dtype=float32)\n-    ```\n     \"\"\"\n     if any_symbolic_tensors((target, output)):\n         return CategoricalCrossentropy(\n@@ -1194,9 +1160,9 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     dissimilarity between the target and output probabilities or logits.\n \n     Args:\n-        target: The target tensor representing the true class labels as integers.\n-            Its shape should match the shape of the `output` tensor except\n-            for the last dimension.\n+        target: The target tensor representing the true class labels as\n+            integers. Its shape should match the shape of the `output`\n+            tensor except for the last dimension.\n         output: The output tensor representing the predicted probabilities\n             or logits.\n             Its shape should match the shape of the `target` tensor except\n@@ -1216,15 +1182,14 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n         loss between `target` and `output`.\n \n     Example:\n-    ```python\n+\n     >>> target = keras_core.ops.convert_to_tensor([0, 1, 2], dtype=int32)\n-    >>> output = keras_core.ops.convert_to_tensor([[0.9, 0.05, 0.05], \n-                                                   [0.1, 0.8, 0.1], \n-                                                   [0.2, 0.3, 0.5]],\n-                                                   dtype=float32)\n+    >>> output = keras_core.ops.convert_to_tensor(\n+    ... [[0.9, 0.05, 0.05],\n+    ...  [0.1, 0.8, 0.1],\n+    ...  [0.2, 0.3, 0.5]])\n     >>> sparse_categorical_crossentropy(target, output)\n     array([0.10536056 0.22314355 0.6931472 ], shape=(3,), dtype=float32)\n-    ```\n     \"\"\"\n     if any_symbolic_tensors((target, output)):\n         return SparseCategoricalCrossentropy(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c8953e5a7d3742b77296a2e009214e4cceb417a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1175 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -60,7 +60,7 @@ def build():\n         namex.generate_api_files(package, code_directory=\"src\", verbose=True)\n \n         # Make sure to export the __version__ string\n-        from keras_core.src import __version__  # noqa: E402\n+        from keras_core.src.version import __version__  # noqa: E402\n \n         with open(os.path.join(package, \"__init__.py\")) as f:\n             init_contents = f.read()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aeccba7c7a435b7292034748028f64bf02dd19f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 430 | Contributors (this commit): 6 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -228,6 +228,13 @@ if \"KERAS_BACKEND\" in os.environ:\n         _BACKEND = _backend\n \n \n+if _BACKEND != \"tensorflow\":\n+    # If we are not running on the tensorflow backend, we should stop tensorflow\n+    # from using all available GPU memory. See\n+    # https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\n+    os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n+\n+\n @keras_core_export(\n     [\n         \"keras_core.config.backend\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#322b44d7619bf2664f89ce7a48f6a9e91507e6d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 987 | Contributors (this commit): 25 | Commits (past 90d): 19 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -228,13 +228,6 @@ if \"KERAS_BACKEND\" in os.environ:\n         _BACKEND = _backend\n \n \n-if _BACKEND != \"tensorflow\":\n-    # If we are not running on the tensorflow backend, we should stop tensorflow\n-    # from using all available GPU memory. See\n-    # https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\n-    os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n-\n-\n @keras_core_export(\n     [\n         \"keras_core.config.backend\",\n\n@@ -23,6 +23,9 @@ def get_version(rel_path):\n \n HERE = pathlib.Path(__file__).parent\n README = (HERE / \"README.md\").read_text()\n+if os.path.exists(\"keras_core/version.py\"):\n+    VERSION = get_version(\"keras_core/version.py\")\n+else:\n     VERSION = get_version(\"keras_core/__init__.py\")\n \n setup(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#96a5fa2b768e938851b345f5af0d23a6a3410df6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 444 | Contributors (this commit): 6 | Commits (past 90d): 14 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -228,6 +228,13 @@ if \"KERAS_BACKEND\" in os.environ:\n         _BACKEND = _backend\n \n \n+if _BACKEND != \"tensorflow\":\n+    # If we are not running on the tensorflow backend, we should stop tensorflow\n+    # from using all available GPU memory. See\n+    # https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\n+    os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n+\n+\n @keras_core_export(\n     [\n         \"keras_core.config.backend\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#37f4728267f538e82647a8fa54e046ec8f2002df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 549 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -146,8 +146,21 @@ from keras_core.ops.operation_utils import reduce_shape\n \n \n def broadcast_shapes(shape1, shape2):\n-    # Broadcast input shapes to a unified shape.\n-    # Convert to list for mutability.\n+    \"\"\"Broadcast input shapes to a unified shape.\n+    \n+    Convert to list for mutability.\n+\n+    Args:\n+        shape1: A tuple or list of integers.\n+        shape2: A tuple or list of integers.\n+\n+    Returns:\n+        output_shape (list of int or None): The broadcasted shape.\n+\n+    Example:\n+    >>> broadcast_shapes((5, 3), (1, 3))\n+    [5, 3]\n+    \"\"\"\n     shape1 = list(shape1)\n     shape2 = list(shape2)\n     origin_shape1 = shape1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bfc9feb17d0758619810ae26b50ace447e462a88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 24 | Churn Cumulative: 1826 | Contributors (this commit): 7 | Commits (past 90d): 17 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -146,21 +146,8 @@ from keras_core.ops.operation_utils import reduce_shape\n \n \n def broadcast_shapes(shape1, shape2):\n-    \"\"\"Broadcast input shapes to a unified shape.\n-    \n-    Convert to list for mutability.\n-\n-    Args:\n-        shape1: A tuple or list of integers.\n-        shape2: A tuple or list of integers.\n-\n-    Returns:\n-        output_shape (list of int or None): The broadcasted shape.\n-\n-    Example:\n-    >>> broadcast_shapes((5, 3), (1, 3))\n-    [5, 3]\n-    \"\"\"\n+    # Broadcast input shapes to a unified shape.\n+    # Convert to list for mutability.\n     shape1 = list(shape1)\n     shape2 = list(shape2)\n     origin_shape1 = shape1\n\n@@ -192,8 +192,8 @@ def print_summary(\n             relevant_nodes += v\n \n     if show_trainable:\n-        default_line_length += 8\n-        positions = [p * 0.88 for p in positions] + [1.0]\n+        default_line_length += 12\n+        positions = [p * 0.90 for p in positions] + [1.0]\n         header.append(\"Trainable\")\n         alignment.append(\"center\")\n \n@@ -262,11 +262,14 @@ def print_summary(\n         if not sequential_like:\n             fields.append(get_connections(layer))\n         if show_trainable:\n+            if layer.weights:\n                 fields.append(\n                     bold_text(\"Y\", color=34)\n                     if layer.trainable\n                     else bold_text(\"N\", color=9)\n                 )\n+            else:\n+                fields.append(bold_text(\"-\"))\n         return fields\n \n     def print_layer(layer, nested_level=0):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
