{"custom_id": "keras#f8add5472995229aaf2f21f8aa1644e72b6a0bda", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 18111 | Contributors (this commit): 69 | Commits (past 90d): 16 | Contributors (cumulative): 77 | DMM Complexity: 0.0\n\nDIFF:\n@@ -827,7 +827,8 @@ class RNN(Layer):\n \n     if self.stateful:\n       updates = [\n-          tf.compat.v1.assign(self_state, state) for self_state, state in zip(\n+          tf.compat.v1.assign(self_state, tf.cast(state, self_state.dtype))\n+          for self_state, state in zip(\n               tf.nest.flatten(self.states), tf.nest.flatten(states))\n       ]\n       self.add_update(updates)\n@@ -878,6 +879,9 @@ class RNN(Layer):\n                                           strict=True)\n       else:\n         initial_state = self.states\n+      initial_state = tf.nest.map_structure(\n+          lambda v: tf.cast(v, self.compute_dtype), initial_state\n+      )\n     elif initial_state is None:\n       initial_state = self.get_initial_state(inputs)\n \n@@ -949,10 +953,13 @@ class RNN(Layer):\n       if getattr(self.cell, 'get_initial_state', None):\n         flat_init_state_values = tf.nest.flatten(self.cell.get_initial_state(\n             inputs=None, batch_size=batch_size,\n-            dtype=self.dtype or backend.floatx()))\n+            # Use variable_dtype instead of compute_dtype, since the state is\n+            # stored in a variable\n+            dtype=self.variable_dtype or backend.floatx()))\n       else:\n         flat_init_state_values = tf.nest.flatten(_generate_zero_filled_state(\n-            batch_size, self.cell.state_size, self.dtype or backend.floatx()))\n+            batch_size, self.cell.state_size,\n+            self.variable_dtype or backend.floatx()))\n       flat_states_variables = tf.nest.map_structure(\n           backend.variable, flat_init_state_values)\n       self.states = tf.nest.pack_sequence_as(self.cell.state_size,\n\n@@ -444,7 +444,8 @@ class GRU(recurrent.DropoutRNNCellMixin, recurrent.GRU):\n           inputs, initial_state, training, mask, row_lengths)\n \n     if self.stateful:\n-      updates = [tf.compat.v1.assign(self.states[0], states[0])]\n+      updates = [tf.compat.v1.assign(self.states[0],\n+                                     tf.cast(states[0], self.states[0].dtype))]\n       self.add_update(updates)\n \n     if self.return_sequences:\n@@ -1256,7 +1257,7 @@ class LSTM(recurrent.DropoutRNNCellMixin, recurrent.LSTM):\n \n     if self.stateful:\n       updates = [\n-          tf.compat.v1.assign(self_state, state)\n+          tf.compat.v1.assign(self_state, tf.cast(state, self_state.dtype))\n           for self_state, state in zip(self.states, states)\n       ]\n       self.add_update(updates)\n\n@@ -140,10 +140,17 @@ class LayerCorrectnessTest(keras_parameterized.TestCase):\n       ('GlobalAveragePooling2D', pooling.GlobalAveragePooling2D, (2, 2, 2, 1)),\n       ('SimpleRNN', lambda: recurrent.SimpleRNN(units=4),\n        (4, 4, 4), 1e-2, 1e-2),\n+      ('SimpleRNN_stateful',\n+       lambda: recurrent.SimpleRNN(units=4, stateful=True),\n+       (4, 4, 4), 1e-2, 1e-2),\n       ('GRU', lambda: recurrent.GRU(units=4), (4, 4, 4)),\n       ('LSTM', lambda: recurrent.LSTM(units=4), (4, 4, 4)),\n       ('GRUV2', lambda: recurrent_v2.GRU(units=4), (4, 4, 4)),\n+      ('GRUV2_stateful', lambda: recurrent_v2.GRU(units=4, stateful=True),\n+       (4, 4, 4)),\n       ('LSTMV2', lambda: recurrent_v2.LSTM(units=4), (4, 4, 4)),\n+      ('LSTMV2_stateful', lambda: recurrent_v2.LSTM(units=4, stateful=True),\n+       (4, 4, 4)),\n       ('TimeDistributed', lambda: wrappers.TimeDistributed(core.Dense(2)),\n        (2, 2, 2)),\n       ('Bidirectional',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d347ae57ff963ab903934529089fba43240de57c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 4 | Churn Cumulative: 19151 | Contributors (this commit): 79 | Commits (past 90d): 18 | Contributors (cumulative): 79 | DMM Complexity: None\n\nDIFF:\n@@ -579,8 +579,8 @@ class Conv2D(Conv):\n       value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n     padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n       `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-      to the left/right or up/down of the input such that output has the same\n-      height/width dimension as the input.\n+      to the left/right or up/down of the input. The output has the same\n+      height/width dimension as the input when `\"same\"` and `strides=(1,1)`.\n     data_format: A string, one of `channels_last` (default) or `channels_first`.\n       The ordering of the dimensions in the inputs. `channels_last` corresponds\n       to inputs with shape `(batch_size, height, width, channels)` while\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#72c5ba7b90ada253c46599657b484caeec917e63", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 92 | Lines Deleted: 29 | Files Changed: 2 | Hunks: 20 | Methods Changed: 16 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 121 | Churn Cumulative: 4670 | Contributors (this commit): 39 | Commits (past 90d): 11 | Contributors (cumulative): 43 | DMM Complexity: 1.0\n\nDIFF:\n@@ -35,9 +35,6 @@ from keras.utils import tf_contextlib\n from keras.utils import tf_inspect\n from tensorflow.python.util.tf_export import keras_export\n \n-_GLOBAL_CUSTOM_OBJECTS = {}\n-_GLOBAL_CUSTOM_NAMES = {}\n-\n # Flag that determines whether to skip the NotImplementedError when calling\n # get_config in custom models and layers. This is only enabled when saving to\n # SavedModel, when the config isn't required.\n@@ -47,6 +44,17 @@ _SKIP_FAILED_SERIALIZATION = False\n _LAYER_UNDEFINED_CONFIG_KEY = 'layer was saved without config'\n \n \n+class CustomObjectsContext(threading.local):\n+\n+  def __init__(self):\n+    super(CustomObjectsContext, self).__init__()\n+    self.objects = {}\n+    self.names = {}\n+\n+\n+GLOBAL_CUSTOM_OBJECTS = CustomObjectsContext()\n+\n+\n @keras_export('keras.utils.custom_object_scope',  # pylint: disable=g-classes-have-attributes\n               'keras.utils.CustomObjectScope')\n class CustomObjectScope:\n@@ -79,14 +87,14 @@ class CustomObjectScope:\n     self.backup = None\n \n   def __enter__(self):\n-    self.backup = _GLOBAL_CUSTOM_OBJECTS.copy()\n+    self.backup = GLOBAL_CUSTOM_OBJECTS.objects.copy()\n     for objects in self.custom_objects:\n-      _GLOBAL_CUSTOM_OBJECTS.update(objects)\n+      GLOBAL_CUSTOM_OBJECTS.objects.update(objects)\n     return self\n \n   def __exit__(self, *args, **kwargs):\n-    _GLOBAL_CUSTOM_OBJECTS.clear()\n-    _GLOBAL_CUSTOM_OBJECTS.update(self.backup)\n+    GLOBAL_CUSTOM_OBJECTS.objects.clear()\n+    GLOBAL_CUSTOM_OBJECTS.objects.update(self.backup)\n \n \n @keras_export('keras.utils.get_custom_objects')\n@@ -105,9 +113,9 @@ def get_custom_objects():\n   ```\n \n   Returns:\n-      Global dictionary of names to classes (`_GLOBAL_CUSTOM_OBJECTS`).\n+      Global dictionary of names to classes (`GLOBAL_CUSTOM_OBJECTS.objects`).\n   \"\"\"\n-  return _GLOBAL_CUSTOM_OBJECTS\n+  return GLOBAL_CUSTOM_OBJECTS.objects\n \n \n # Store a unique, per-object ID for shared objects.\n@@ -374,16 +382,16 @@ def register_keras_serializable(package='Custom', name=None):\n       raise ValueError(\n           'Cannot register a class that does not have a get_config() method.')\n \n-    if registered_name in _GLOBAL_CUSTOM_OBJECTS:\n-      raise ValueError(\n-          f'{registered_name} has already been registered to '\n-          f'{_GLOBAL_CUSTOM_OBJECTS[registered_name]}')\n+    if registered_name in GLOBAL_CUSTOM_OBJECTS.objects:\n+      raise ValueError(f'{registered_name} has already been registered to '\n+                       f'{GLOBAL_CUSTOM_OBJECTS.objects[registered_name]}')\n \n-    if arg in _GLOBAL_CUSTOM_NAMES:\n+    if arg in GLOBAL_CUSTOM_OBJECTS.names:\n       raise ValueError(\n-          f'{arg} has already been registered to {_GLOBAL_CUSTOM_NAMES[arg]}')\n-    _GLOBAL_CUSTOM_OBJECTS[registered_name] = arg\n-    _GLOBAL_CUSTOM_NAMES[arg] = registered_name\n+          f'{arg} has already been registered to {GLOBAL_CUSTOM_OBJECTS.names[arg]}'\n+      )\n+    GLOBAL_CUSTOM_OBJECTS.objects[registered_name] = arg\n+    GLOBAL_CUSTOM_OBJECTS.names[arg] = registered_name\n \n     return arg\n \n@@ -405,8 +413,8 @@ def get_registered_name(obj):\n     The name associated with the object, or the default Python name if the\n       object is not registered.\n   \"\"\"\n-  if obj in _GLOBAL_CUSTOM_NAMES:\n-    return _GLOBAL_CUSTOM_NAMES[obj]\n+  if obj in GLOBAL_CUSTOM_OBJECTS.names:\n+    return GLOBAL_CUSTOM_OBJECTS.names[obj]\n   else:\n     return obj.__name__\n \n@@ -449,8 +457,8 @@ def get_registered_object(name, custom_objects=None, module_objects=None):\n     An instantiable class associated with 'name', or None if no such class\n       exists.\n   \"\"\"\n-  if name in _GLOBAL_CUSTOM_OBJECTS:\n-    return _GLOBAL_CUSTOM_OBJECTS[name]\n+  if name in GLOBAL_CUSTOM_OBJECTS.objects:\n+    return GLOBAL_CUSTOM_OBJECTS.objects[name]\n   elif custom_objects and name in custom_objects:\n     return custom_objects[name]\n   elif module_objects and name in module_objects:\n@@ -533,8 +541,8 @@ def serialize_keras_object(instance):\n \n def get_custom_objects_by_name(item, custom_objects=None):\n   \"\"\"Returns the item if it is in either local or global custom objects.\"\"\"\n-  if item in _GLOBAL_CUSTOM_OBJECTS:\n-    return _GLOBAL_CUSTOM_OBJECTS[item]\n+  if item in GLOBAL_CUSTOM_OBJECTS.objects:\n+    return GLOBAL_CUSTOM_OBJECTS.objects[item]\n   elif custom_objects and item in custom_objects:\n     return custom_objects[item]\n   return None\n@@ -676,7 +684,7 @@ def deserialize_keras_object(identifier,\n         deserialized_obj = cls.from_config(\n             cls_config,\n             custom_objects=dict(\n-                list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n+                list(GLOBAL_CUSTOM_OBJECTS.objects.items()) +\n                 list(custom_objects.items())))\n       else:\n         with CustomObjectScope(custom_objects):\n@@ -698,8 +706,8 @@ def deserialize_keras_object(identifier,\n     object_name = identifier\n     if custom_objects and object_name in custom_objects:\n       obj = custom_objects.get(object_name)\n-    elif object_name in _GLOBAL_CUSTOM_OBJECTS:\n-      obj = _GLOBAL_CUSTOM_OBJECTS[object_name]\n+    elif object_name in GLOBAL_CUSTOM_OBJECTS.objects:\n+      obj = GLOBAL_CUSTOM_OBJECTS.objects[object_name]\n     else:\n       obj = module_objects.get(object_name)\n       if obj is None:\n\n@@ -17,6 +17,7 @@\n import tensorflow.compat.v2 as tf\n \n from functools import partial\n+import threading\n \n import numpy as np\n \n@@ -87,6 +88,58 @@ class TestCustomObjectScope(tf.test.TestCase):\n       cl = keras.regularizers.get('CustomClass')\n       self.assertEqual(cl.__class__, CustomClass)\n \n+  def test_custom_object_scope_thread_safety(self):\n+\n+    class CustomClass:\n+      pass\n+\n+    def custom_object_scope_in_thread(event_start, event_entered, event_finish,\n+                                      event_exited):\n+\n+      def fn():\n+        event_start.wait()\n+        with keras.utils.generic_utils.custom_object_scope(\n+            {'CustomClass': CustomClass}):\n+          event_entered.set()\n+          event_finish.wait()\n+        event_exited.set()\n+\n+      return fn\n+\n+    event_start_1 = threading.Event()\n+    event_entered_1 = threading.Event()\n+    event_finish_1 = threading.Event()\n+    event_exited_1 = threading.Event()\n+    thread_1 = threading.Thread(\n+        target=custom_object_scope_in_thread(event_start_1, event_entered_1,\n+                                             event_finish_1, event_exited_1))\n+\n+    event_start_2 = threading.Event()\n+    event_entered_2 = threading.Event()\n+    event_finish_2 = threading.Event()\n+    event_exited_2 = threading.Event()\n+    thread_2 = threading.Thread(\n+        target=custom_object_scope_in_thread(event_start_2, event_entered_2,\n+                                             event_finish_2, event_exited_2))\n+\n+    keras.utils.generic_utils.get_custom_objects().clear()\n+    thread_1.start()\n+    thread_2.start()\n+\n+    event_start_1.set()\n+    event_entered_1.wait()\n+    event_start_2.set()\n+    event_entered_2.wait()\n+\n+    event_finish_1.set()\n+    event_exited_1.wait()\n+    event_finish_2.set()\n+    event_exited_2.wait()\n+\n+    thread_1.join()\n+    thread_2.join()\n+    self.assertEqual(keras.utils.generic_utils.get_custom_objects(), {})\n+\n \n class SerializeKerasObjectTest(tf.test.TestCase):\n \n@@ -110,7 +163,8 @@ class SerializeKerasObjectTest(tf.test.TestCase):\n \n     serialized_name = 'Custom>TestClass'\n     inst = TestClass(value=10)\n-    class_name = keras.utils.generic_utils._GLOBAL_CUSTOM_NAMES[TestClass]\n+    class_name = keras.utils.generic_utils.GLOBAL_CUSTOM_OBJECTS.names[\n+        TestClass]\n     self.assertEqual(serialized_name, class_name)\n     config = keras.utils.generic_utils.serialize_keras_object(inst)\n     self.assertEqual(class_name, config['class_name'])\n@@ -144,7 +198,8 @@ class SerializeKerasObjectTest(tf.test.TestCase):\n \n     serialized_name = 'TestPackage>CustomName'\n     inst = OtherTestClass(val=5)\n-    class_name = keras.utils.generic_utils._GLOBAL_CUSTOM_NAMES[OtherTestClass]\n+    class_name = keras.utils.generic_utils.GLOBAL_CUSTOM_OBJECTS.names[\n+        OtherTestClass]\n     self.assertEqual(serialized_name, class_name)\n     fn_class_name = keras.utils.generic_utils.get_registered_name(\n         OtherTestClass)\n@@ -167,7 +222,7 @@ class SerializeKerasObjectTest(tf.test.TestCase):\n       return 42\n \n     serialized_name = 'Custom>my_fn'\n-    class_name = keras.utils.generic_utils._GLOBAL_CUSTOM_NAMES[my_fn]\n+    class_name = keras.utils.generic_utils.GLOBAL_CUSTOM_OBJECTS.names[my_fn]\n     self.assertEqual(serialized_name, class_name)\n     fn_class_name = keras.utils.generic_utils.get_registered_name(my_fn)\n     self.assertEqual(fn_class_name, class_name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3856f4a78a30ef80458c52f76564d8e9b55dab0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 15810 | Contributors (this commit): 109 | Commits (past 90d): 35 | Contributors (cumulative): 109 | DMM Complexity: None\n\nDIFF:\n@@ -931,7 +931,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             (since they generate batches).\n         epochs: Integer. Number of epochs to train the model.\n             An epoch is an iteration over the entire `x` and `y`\n-            data provided.\n+            data provided \n+            (unless the steps_per_epoch flag is set to something other than None).\n             Note that in conjunction with `initial_epoch`,\n             `epochs` is to be understood as \"final epoch\".\n             The model is not trained for a number of iterations\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6129e9561ef7ebe11153244b7d5bdd676f746ffe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 149 | Lines Deleted: 81 | Files Changed: 9 | Hunks: 59 | Methods Changed: 29 | Complexity Δ (Sum/Max): -2/3 | Churn Δ: 230 | Churn Cumulative: 9295 | Contributors (this commit): 7 | Commits (past 90d): 71 | Contributors (cumulative): 38 | DMM Complexity: 0.8823529411764706\n\nDIFF:\n@@ -14,12 +14,14 @@\n # ==============================================================================\n \"\"\"Keras discretization preprocessing layer.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n # pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-direct-tensorflow-import\n \n-import numpy as np\n from keras.engine import base_preprocessing_layer\n+from keras.layers.preprocessing import preprocessing_utils as utils\n from keras.utils import tf_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -190,7 +192,7 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n       raise ValueError(\"Both `num_bins` and `bin_boundaries` should not be \"\n                        \"set. You passed `num_bins={}` and \"\n                        \"`bin_boundaries={}`\".format(num_bins, bin_boundaries))\n-    bin_boundaries = self._convert_to_list(bin_boundaries)\n+    bin_boundaries = utils.listify_tensors(bin_boundaries)\n     self.input_bin_boundaries = bin_boundaries\n     self.bin_boundaries = bin_boundaries if bin_boundaries is not None else []\n     self.num_bins = num_bins\n@@ -232,7 +234,7 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n       return\n \n     # The bucketize op only support list boundaries.\n-    self.bin_boundaries = self._convert_to_list(\n+    self.bin_boundaries = utils.listify_tensors(\n         get_bin_boundaries(self.summary, self.num_bins))\n \n   def reset_state(self):  # pylint: disable=method-hidden\n@@ -282,11 +284,3 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n           dense_shape=tf.identity(inputs.dense_shape))\n     else:\n       return bucketize(inputs)\n-\n-  def _convert_to_list(self, inputs):\n-    if tf.is_tensor(inputs):\n-      inputs = inputs.numpy()\n-    if isinstance(inputs, (np.ndarray)):\n-      inputs = inputs.tolist()\n-      inputs = list(inputs)\n-    return inputs\n\n@@ -15,6 +15,7 @@\n \"\"\"Keras index lookup preprocessing layer.\"\"\"\n \n # pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-direct-tensorflow-import\n \n import collections\n \n@@ -22,6 +23,7 @@ from keras import backend\n from keras.engine import base_layer_utils\n from keras.engine import base_preprocessing_layer\n from keras.layers.preprocessing import category_encoding\n+from keras.layers.preprocessing import preprocessing_utils as utils\n from keras.saving.saved_model import layer_serialization\n from keras.utils import layer_utils\n from keras.utils import tf_utils\n@@ -197,14 +199,15 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n     self.output_mode = output_mode\n     self.sparse = sparse\n     self.pad_to_max_tokens = pad_to_max_tokens\n-    self.input_vocabulary = None\n-    # IndexLookupLayerSavedModelSaver will clear the config config vocabulary to\n-    # restore the lookup table ops directly. We persist this hidden option to\n-    # persist the fact that we have have a non-adaptable layer with a manually\n-    # set vocabulary.\n-    self._has_input_vocabulary = kwargs.pop(\"has_input_vocabulary\", False)\n     self._frozen_vocab_size = None\n \n+    self.input_vocabulary = vocabulary\n+    # VocabularySavedModelSaver will clear the config vocabulary to restore the\n+    # lookup table ops directly. We persist this hidden option to persist the\n+    # fact that we have have a non-adaptable layer with a manually set vocab.\n+    self._has_input_vocabulary = kwargs.pop(\"has_input_vocabulary\",\n+                                            (vocabulary is not None))\n+\n     # Drop deprecated config options.\n     kwargs.pop(\"vocabulary_size\", None)\n     kwargs.pop(\"has_static_table\", None)\n@@ -257,6 +260,8 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n       # find and restore a lookup_table attribute on the layer. This table needs\n       # to be uninitialized as a StaticHashTable cannot be initialized twice.\n       self.lookup_table = self._uninitialized_lookup_table()\n+\n+    # Only set up adapt state if we did not recieve a vocab on construction.\n     if not self._has_input_vocabulary:\n       # Add a custom weight handler to return the layers vocab as it's weight.\n       self._add_trackable(VocabWeightHandler(self), False)\n@@ -330,7 +335,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         \"mask_token\": self.mask_token,\n         \"output_mode\": self.output_mode,\n         \"pad_to_max_tokens\": self.pad_to_max_tokens,\n-        \"vocabulary\": self._make_serializable(self.input_vocabulary),\n+        \"vocabulary\": utils.listify_tensors(self.input_vocabulary),\n     }\n \n     base_config = super().get_config()\n@@ -363,9 +368,6 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         been called.\n       RuntimeError: If a tensor vocabulary is passed outside of eager execution.\n     \"\"\"\n-    self.input_vocabulary = vocabulary\n-    self._has_input_vocabulary = True\n-\n     if self.output_mode != TF_IDF and idf_weights is not None:\n       raise ValueError(\"`idf_weights` should only be set if output_mode is \"\n                        \"TF_IDF. output_mode is {}.\".format(self.output_mode))\n@@ -753,14 +755,6 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n     else:\n       return tf.expand_dims(inputs, axis)\n \n-  def _make_serializable(self, x):\n-    if tf.is_tensor(x):\n-      x = x.numpy()\n-    if isinstance(x, (np.ndarray)):\n-      x = x.tolist()\n-      x = list(x)\n-    return x\n-\n   def _oov_start_index(self):\n     return 1 if self.mask_token is not None and self.output_mode == INT else 0\n \n@@ -831,7 +825,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n \n   @property\n   def _trackable_saved_model_saver(self):\n-    return layer_serialization.IndexLookupLayerSavedModelSaver(self)\n+    return layer_serialization.VocabularySavedModelSaver(self)\n \n   # Override points for IntegerLookup and StringLookup.\n   def _tensor_vocab_to_numpy(self, vocabulary):\n\n@@ -14,15 +14,16 @@\n # ==============================================================================\n \"\"\"Normalization preprocessing layer.\"\"\"\n \n+# pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-direct-tensorflow-import\n+\n from keras import backend\n from keras.engine import base_preprocessing_layer\n+from keras.layers.preprocessing import preprocessing_utils as utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n-\n from tensorflow.python.util.tf_export import keras_export\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n @keras_export('keras.layers.Normalization',\n               'keras.layers.experimental.preprocessing.Normalization')\n@@ -262,8 +263,8 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n     config = super().get_config()\n     config.update({\n         'axis': self.axis,\n-        'mean': self._convert_to_list(self.input_mean),\n-        'variance': self._convert_to_list(self.input_variance),\n+        'mean': utils.listify_tensors(self.input_mean),\n+        'variance': utils.listify_tensors(self.input_variance),\n     })\n     return config\n \n@@ -272,11 +273,3 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n     if inputs.dtype != self.dtype:\n       inputs = tf.cast(inputs, self.dtype)\n     return inputs\n-\n-  def _convert_to_list(self, inputs):\n-    if tf.is_tensor(inputs):\n-      inputs = inputs.numpy()\n-    if isinstance(inputs, (np.ndarray)):\n-      inputs = inputs.tolist()\n-      inputs = list(inputs)\n-    return inputs\n\n@@ -12,12 +12,11 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for Keras' base preprocessing layer.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n+\"\"\"Tests utils for preprocessing layers.\"\"\"\n \n import collections\n import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n class PreprocessingLayerTest(tf.test.TestCase):\n\n@@ -0,0 +1,27 @@\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Utils for preprocessing layers.\"\"\"\n+\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+\n+def listify_tensors(x):\n+  \"\"\"Convert any tensors or numpy arrays to lists for config serialization.\"\"\"\n+  if tf.is_tensor(x):\n+    x = x.numpy()\n+  if isinstance(x, np.ndarray):\n+    x = x.tolist()\n+  return x\n\n@@ -14,16 +14,19 @@\n # ==============================================================================\n \"\"\"Keras text vectorization preprocessing layer.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n # pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-direct-tensorflow-import\n \n-import numpy as np\n from keras import backend\n from keras.engine import base_preprocessing_layer\n from keras.layers.preprocessing import index_lookup\n+from keras.layers.preprocessing import preprocessing_utils as utils\n from keras.layers.preprocessing import string_lookup\n+from keras.saving.saved_model import layer_serialization\n from keras.utils import layer_utils\n from keras.utils import tf_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n LOWER_AND_STRIP_PUNCTUATION = \"lower_and_strip_punctuation\"\n@@ -307,6 +310,13 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n \n     self._output_mode = output_mode\n     self._output_sequence_length = output_sequence_length\n+\n+    # VocabularySavedModelSaver will clear the config vocabulary to restore the\n+    # lookup table ops directly. We persist this hidden option to persist the\n+    # fact that we have have a non-adaptable layer with a manually set vocab.\n+    self._has_input_vocabulary = kwargs.pop(\"has_input_vocabulary\",\n+                                            (vocabulary is not None))\n+\n     # Drop deprecated config options.\n     kwargs.pop(\"vocabulary_size\", None)\n \n@@ -314,12 +324,13 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell(\"TextVectorization\").set(\n         True)\n \n-    self._index_lookup_layer = string_lookup.StringLookup(\n+    self._lookup_layer = string_lookup.StringLookup(\n         max_tokens=max_tokens,\n         vocabulary=vocabulary,\n         pad_to_max_tokens=pad_to_max_tokens,\n         mask_token=\"\",\n-        output_mode=output_mode if output_mode is not None else INT)\n+        output_mode=output_mode if output_mode is not None else INT,\n+        has_input_vocabulary=self._has_input_vocabulary)\n \n   def compute_output_shape(self, input_shape):\n     if self._output_mode == INT:\n@@ -330,7 +341,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n         input_shape = tuple(input_shape) + (1,)\n     else:\n       input_shape = tuple(input_shape) + (None,)\n-    return self._index_lookup_layer.compute_output_shape(input_shape)\n+    return self._lookup_layer.compute_output_shape(input_shape)\n \n   def compute_output_signature(self, input_spec):\n     output_shape = self.compute_output_shape(input_spec.shape.as_list())\n@@ -339,13 +350,13 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     return tf.TensorSpec(shape=output_shape, dtype=output_dtype)\n \n   def update_state(self, data):\n-    self._index_lookup_layer.update_state(self._preprocess(data))\n+    self._lookup_layer.update_state(self._preprocess(data))\n \n   def finalize_state(self):\n-    self._index_lookup_layer.finalize_state()\n+    self._lookup_layer.finalize_state()\n \n   def reset_state(self):  # pylint: disable=method-hidden\n-    self._index_lookup_layer.reset_state()\n+    self._lookup_layer.reset_state()\n \n   def get_vocabulary(self, include_special_tokens=True):\n     \"\"\"Returns the current vocabulary of the layer.\n@@ -356,7 +367,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n         equal the term's index when calling the layer. If False, the returned\n         vocabulary will not include any padding or OOV tokens.\n     \"\"\"\n-    return self._index_lookup_layer.get_vocabulary(include_special_tokens)\n+    return self._lookup_layer.get_vocabulary(include_special_tokens)\n \n   def vocabulary_size(self):\n     \"\"\"Gets the current size of the layer's vocabulary.\n@@ -364,20 +375,19 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     Returns:\n       The integer size of the voculary, including optional mask and oov indices.\n     \"\"\"\n-    return self._index_lookup_layer.vocabulary_size()\n+    return self._lookup_layer.vocabulary_size()\n \n   def get_config(self):\n-    # This does not include the 'vocabulary' arg, since if the vocab was passed\n-    # at init time it's now stored in variable state - we don't need to\n-    # pull it off disk again.\n+    vocab = self._lookup_layer.input_vocabulary\n     config = {\n-        \"max_tokens\": self._index_lookup_layer.max_tokens,\n+        \"max_tokens\": self._lookup_layer.max_tokens,\n         \"standardize\": self._standardize,\n         \"split\": self._split,\n         \"ngrams\": self._ngrams_arg,\n         \"output_mode\": self._output_mode,\n         \"output_sequence_length\": self._output_sequence_length,\n-        \"pad_to_max_tokens\": self._index_lookup_layer.pad_to_max_tokens,\n+        \"pad_to_max_tokens\": self._lookup_layer.pad_to_max_tokens,\n+        \"vocabulary\": utils.listify_tensors(vocab),\n     }\n     base_config = super(TextVectorization, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n@@ -408,7 +418,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n         if `pad_to_max_tokens` is False and the layer itself has already been\n         called.\n     \"\"\"\n-    self._index_lookup_layer.set_vocabulary(vocabulary, idf_weights=idf_weights)\n+    self._lookup_layer.set_vocabulary(vocabulary, idf_weights=idf_weights)\n \n   def build(self, input_shape):\n     # We have to use 'and not ==' here, because input_shape[1] !/== 1 can result\n@@ -484,7 +494,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     if self._output_mode is None:\n       return inputs\n \n-    lookup_data = self._index_lookup_layer(inputs)\n+    lookup_data = self._lookup_layer(inputs)\n \n     # For any non-int output, we can return directly from the underlying layer.\n     if self._output_mode is not INT:\n@@ -511,3 +521,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n       return tf.pad(lookup_data, padding)\n \n     return lookup_data\n+\n+  @property\n+  def _trackable_saved_model_saver(self):\n+    return layer_serialization.VocabularySavedModelSaver(self)\n\n@@ -56,8 +56,8 @@ class TextVectorizationDistributionTest(\n           max_tokens=None,\n           standardize=None,\n           split=None,\n-          output_mode=text_vectorization.INT)\n-      layer.set_vocabulary(vocab_data)\n+          output_mode=text_vectorization.INT,\n+          vocabulary=vocab_data)\n       int_data = layer(input_data)\n       model = keras.Model(inputs=input_data, outputs=int_data)\n \n\n@@ -1554,7 +1554,11 @@ class TextVectorizationSavingTest(\n     gc.collect()\n     super(TextVectorizationSavingTest, self).tearDown()\n \n-  def test_saving(self):\n+  @parameterized.parameters(\n+      {\"init_vocab\": True},\n+      {\"init_vocab\": False},\n+  )\n+  def test_saving(self, init_vocab):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n                             [\"fire\", \"and\", \"earth\", \"michigan\"]])\n@@ -1562,11 +1566,14 @@ class TextVectorizationSavingTest(\n \n     # Build and validate a golden model.\n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n+    vocabulary = vocab_data if init_vocab else None\n     layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.INT)\n+        output_mode=text_vectorization.INT,\n+        vocabulary=vocabulary)\n+    if not init_vocab:\n       layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -1578,14 +1585,16 @@ class TextVectorizationSavingTest(\n \n     # Delete the session and graph to ensure that the loaded model is generated\n     # from scratch.\n-    # TODO(b/149526183): Can't clear session when TF2 is disabled.\n-    if tf.__internal__.tf2.enabled():\n     keras.backend.clear_session()\n \n     loaded_model = keras.models.load_model(output_path)\n     self.assertAllEqual(loaded_model.predict(input_array), expected_output)\n \n-  def test_saving_when_nested(self):\n+  @parameterized.parameters(\n+      {\"init_vocab\": True},\n+      {\"init_vocab\": False},\n+  )\n+  def test_saving_when_nested(self, init_vocab):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n                             [\"fire\", \"and\", \"earth\", \"michigan\"]])\n@@ -1593,11 +1602,14 @@ class TextVectorizationSavingTest(\n \n     # Build and validate a golden model.\n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n+    vocabulary = vocab_data if init_vocab else None\n     layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.INT)\n+        output_mode=text_vectorization.INT,\n+        vocabulary=vocabulary)\n+    if not init_vocab:\n       layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -1612,8 +1624,38 @@ class TextVectorizationSavingTest(\n \n     # Delete the session and graph to ensure that the loaded model is generated\n     # from scratch.\n-    # TODO(b/149526183): Can't clear session when TF2 is disabled.\n-    if tf.__internal__.tf2.enabled():\n+    keras.backend.clear_session()\n+\n+    loaded_model = keras.models.load_model(output_path)\n+    self.assertAllEqual(loaded_model.predict(input_array), expected_output)\n+\n+  def test_saving_when_adapted(self):\n+    adapt_data = [\n+        \"earth\", \"earth\", \"earth\", \"earth\", \"wind\", \"wind\", \"wind\", \"and\",\n+        \"and\", \"fire\"\n+    ]\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n+                            [\"fire\", \"and\", \"earth\", \"michigan\"]])\n+    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n+\n+    # Build and validate a golden model.\n+    input_data = keras.Input(shape=(None,), dtype=tf.string)\n+    layer = text_vectorization.TextVectorization(\n+        max_tokens=None,\n+        standardize=None,\n+        split=None,\n+        output_mode=text_vectorization.INT)\n+    layer.adapt(adapt_data)\n+    int_data = layer(input_data)\n+    model = keras.Model(inputs=input_data, outputs=int_data)\n+\n+    # Save the model to disk.\n+    output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n+\n+    model.save(output_path, save_format=\"tf\")\n+\n+    # Delete the session and graph to ensure that the loaded model is generated\n+    # from scratch.\n     keras.backend.clear_session()\n \n     loaded_model = keras.models.load_model(output_path)\n\n@@ -162,18 +162,23 @@ class RNNSavedModelSaver(LayerSavedModelSaver):\n     return objects, functions\n \n \n-class IndexLookupLayerSavedModelSaver(LayerSavedModelSaver):\n-  \"\"\"Index lookup layer serialization.\"\"\"\n+class VocabularySavedModelSaver(LayerSavedModelSaver):\n+  \"\"\"Handles vocabulary layer serialization.\n+\n+  This class is needed for StringLookup, IntegerLookup, and TextVectorization,\n+  which all have a vocabulary as part of the config. Currently, we keep this\n+  vocab as part of the config until saving, when we need to clear it to avoid\n+  initializing a StaticHashTable twice (once when restoring the config and once\n+  when restoring restoring module resources). After clearing the vocab, we\n+  presist a property to the layer indicating it was constructed with a vocab.\n+  \"\"\"\n \n   @property\n   def python_properties(self):\n     # TODO(kathywu): Add python property validator\n     metadata = self._python_properties_internal()\n-    # Clear the vocabulary from the config during saving. The vocab will be\n-    # saved as part of the lookup table directly, which correctly handle saving\n-    # vocabulary files as a SavedModel asset.\n+    # Clear the vocabulary from the config during saving.\n     metadata['config']['vocabulary'] = None\n-    # Keep a separate config property to track that a vocabulary was passed in\n-    # and not adapted.\n+    # Persist a property to track that a vocabulary was passed on construction.\n     metadata['config']['has_input_vocabulary'] = self.obj._has_input_vocabulary  # pylint: disable=protected-access\n     return metadata\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8dc3e566af36731571cb4f295e32d06b5cee6d10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 43 | Churn Cumulative: 2400 | Contributors (this commit): 27 | Commits (past 90d): 5 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,6 +17,7 @@\n \n import copy\n import types\n+import warnings\n \n import numpy as np\n \n@@ -25,6 +26,7 @@ from keras.models import Sequential\n from keras.utils.generic_utils import has_arg\n from keras.utils.np_utils import to_categorical\n from tensorflow.python.util.tf_export import keras_export\n+from tensorflow.tools.docs import doc_controls  # pylint: disable=g-direct-tensorflow-import\n \n \n class BaseWrapper:\n@@ -184,10 +186,21 @@ class BaseWrapper:\n \n \n @keras_export('keras.wrappers.scikit_learn.KerasClassifier')\n+@doc_controls.do_not_generate_docs\n class KerasClassifier(BaseWrapper):\n   \"\"\"Implementation of the scikit-learn classifier API for Keras.\n+\n+  DEPRECATED. Use [Sci-Keras](https://github.com/adriangb/scikeras) instead.\n   \"\"\"\n \n+  def __init__(self, build_fn=None, **sk_params):\n+    warnings.warn(\n+        'KerasClassifier is deprecated, '\n+        'use Sci-Keras (https://github.com/adriangb/scikeras) instead.',\n+        DeprecationWarning\n+    )\n+    super().__init__(build_fn, **sk_params)\n+\n   def fit(self, x, y, **kwargs):\n     \"\"\"Constructs a new model with `build_fn` & fit the model to `(x, y)`.\n \n@@ -309,10 +322,22 @@ class KerasClassifier(BaseWrapper):\n \n \n @keras_export('keras.wrappers.scikit_learn.KerasRegressor')\n+@doc_controls.do_not_generate_docs\n class KerasRegressor(BaseWrapper):\n   \"\"\"Implementation of the scikit-learn regressor API for Keras.\n+\n+  DEPRECATED. Use [Sci-Keras](https://github.com/adriangb/scikeras) instead.\n   \"\"\"\n \n+  @doc_controls.do_not_doc_inheritable\n+  def __init__(self, build_fn=None, **sk_params):\n+    warnings.warn(\n+        'KerasRegressor is deprecated, '\n+        'use Sci-Keras (https://github.com/adriangb/scikeras) instead.',\n+        DeprecationWarning\n+    )\n+    super().__init__(build_fn, **sk_params)\n+\n   def predict(self, x, **kwargs):\n     \"\"\"Returns predictions for the given test data.\n \n\n@@ -14,6 +14,8 @@\n # ==============================================================================\n \"\"\"Tests for Scikit-learn API wrapper.\"\"\"\n \n+import warnings\n+\n import tensorflow.compat.v2 as tf\n \n import numpy as np\n@@ -183,6 +185,22 @@ class ScikitLearnAPIWrapperTest(tf.test.TestCase):\n \n       assert_regression_works(reg)\n \n+  def test_regressor_deprecated(self):\n+    with warnings.catch_warnings(record=True) as w:\n+      warnings.simplefilter('always')\n+      scikit_learn.KerasRegressor(build_fn_reg)\n+      assert len(w) == 1\n+      assert issubclass(w[-1].category, DeprecationWarning)\n+      assert 'KerasRegressor is deprecated' in str(w[-1].message)\n+\n+  def test_classifier_deprecated(self):\n+    with warnings.catch_warnings(record=True) as w:\n+      warnings.simplefilter('always')\n+      scikit_learn.KerasClassifier(build_fn_clf)\n+      assert len(w) == 1\n+      assert issubclass(w[-1].category, DeprecationWarning)\n+      assert 'KerasClassifier is deprecated' in str(w[-1].message)\n+\n \n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a23d4ed1fea66c9cb4b6aeb272481c5482042b09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 336 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -179,7 +179,7 @@ def image_dataset_from_directory(directory,\n     num_channels = 1\n   else:\n     raise ValueError(\n-        '`color_mode` must be one of {\"rbg\", \"rgba\", \"grayscale\"}. '\n+        '`color_mode` must be one of {\"rgb\", \"rgba\", \"grayscale\"}. '\n         f'Received: color_mode={color_mode}')\n   interpolation = image_preprocessing.get_interpolation(interpolation)\n   dataset_utils.check_validation_split_arg(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b4d0fe887bcaffc36b7914e23670de8e92d02d6b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 122 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -48,12 +48,12 @@ if '--project_name' in sys.argv:\n setuptools.setup(\n     name=project_name,\n     version=_VERSION.replace('-', ''),\n-    description=DOCLINES[0],\n+    description='Deep learning for humans.',\n     long_description='\\n'.join(DOCLINES[2:]),\n     url='https://keras.io/',\n     download_url='https://github.com/keras-team/keras/tags',\n-    author='Google Inc.',\n-    author_email='packages@tensorflow.org',\n+    author='Keras team',\n+    author_email='keras-users@googlegroups.com',\n     packages=setuptools.find_packages(),\n     install_requires=REQUIRED_PACKAGES,\n     # PyPI package information.\n@@ -64,7 +64,6 @@ setuptools.setup(\n         'Intended Audience :: Science/Research',\n         'License :: OSI Approved :: Apache Software License',\n         'Programming Language :: Python :: 3',\n-        'Programming Language :: Python :: 3.6',\n         'Programming Language :: Python :: 3.7',\n         'Programming Language :: Python :: 3.8',\n         'Programming Language :: Python :: 3.9',\n@@ -77,5 +76,5 @@ setuptools.setup(\n         'Topic :: Software Development :: Libraries :: Python Modules',\n     ],\n     license='Apache 2.0',\n-    keywords='tensorflow keras tensor machine learning',\n+    keywords=['keras', 'tensorflow', 'machine learning', 'deep learning'],\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8c95f05bf913fe5bb7efd1cd85ef243bc003a9b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 19155 | Contributors (this commit): 79 | Commits (past 90d): 18 | Contributors (cumulative): 79 | DMM Complexity: None\n\nDIFF:\n@@ -579,8 +579,8 @@ class Conv2D(Conv):\n       value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n     padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n       `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-      to the left/right or up/down of the input. The output has the same\n-      height/width dimension as the input when `\"same\"` and `strides=(1,1)`.\n+      to the left/right or up/down of the input. When padding=\"same\" and \n+      strides=1, the output has the same size as the input.\n     data_format: A string, one of `channels_last` (default) or `channels_first`.\n       The ordering of the dimensions in the inputs. `channels_last` corresponds\n       to inputs with shape `(batch_size, height, width, channels)` while\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#97a8d25cd9bff86e32df52c9c7f840e11e272efe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 340 | Files Changed: 2 | Hunks: 2 | Methods Changed: 11 | Complexity Δ (Sum/Max): -21/0 | Churn Δ: 340 | Churn Cumulative: 706 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 0.3274336283185841\n\nDIFF:\n@@ -1,113 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Keras reduction layer.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-# pylint: disable=g-classes-have-attributes\n-\n-from keras.engine.base_layer import Layer\n-from tensorflow.python.platform import tf_logging as logging\n-\n-\n-def get_reduce_op(reduction_str):\n-  \"\"\"Translate a reduction string name to a reduction op.\"\"\"\n-  if reduction_str == \"max\":\n-    return tf.reduce_max\n-  elif reduction_str == \"mean\":\n-    return tf.reduce_mean\n-  elif reduction_str == \"min\":\n-    return tf.reduce_min\n-  elif reduction_str == \"prod\":\n-    return tf.reduce_prod\n-  elif reduction_str == \"sum\":\n-    return tf.reduce_sum\n-  else:\n-    raise ValueError(\"Reduction %s is not supported for unweighted inputs.\" %\n-                     reduction_str)\n-\n-\n-class Reduction(Layer):\n-  \"\"\"Performs an optionally-weighted reduction.\n-\n-  This layer performs a reduction across one axis of its input data. This\n-  data may optionally be weighted by passing in an identical float tensor.\n-\n-  Args:\n-    reduction: The type of reduction to perform. Can be one of the following:\n-      \"max\", \"mean\", \"min\", \"prod\", or \"sum\". This layer uses the Tensorflow\n-      reduce op which corresponds to that reduction (so, for \"mean\", we use\n-      \"reduce_mean\").\n-    axis: The axis to reduce along. Defaults to '-2', which is usually the axis\n-      that contains embeddings (but is not within the embedding itself).\n-\n-  Input shape:\n-    A tensor of 2 or more dimensions of any numeric dtype.\n-\n-  Output:\n-    A tensor of 1 less dimension than the input tensor, of the same dtype.\n-\n-  Call arguments:\n-    inputs: The data to reduce.\n-    weights: An optional tensor or constant of the same shape as inputs that\n-      will weight the input data before it is reduced.\n-  \"\"\"\n-  # TODO(momernick): Add example here.\n-\n-  def __init__(self, reduction, axis=-2, **kwargs):\n-    self.reduction = reduction\n-    self.axis = axis\n-    # We temporarily turn off autocasting, as it does not apply to named call\n-    # kwargs.\n-    super(Reduction, self).__init__(**kwargs)\n-\n-  def call(self, inputs, weights=None):\n-    # If we are not weighting the inputs we can immediately reduce the data\n-    # and return it.\n-    if weights is None:\n-      return get_reduce_op(self.reduction)(inputs, axis=self.axis)\n-\n-    # TODO(momernick): Add checks for this and a decent error message if the\n-    # weight shape isn't compatible.\n-    if weights.shape.rank + 1 == inputs.shape.rank:\n-      weights = tf.expand_dims(weights, -1)\n-\n-    weighted_inputs = tf.multiply(inputs, weights)\n-\n-    # Weighted sum and prod can be expressed as reductions over the weighted\n-    # values, as can min and max.\n-    if self.reduction in (\"sum\", \"prod\", \"min\", \"max\"):\n-      return get_reduce_op(self.reduction)(weighted_inputs, axis=self.axis)\n-\n-    # Weighted mean is a bit more complicated: we have to do a sum of the\n-    # weighted values and divide by the sum of the weights.\n-    if self.reduction == \"mean\":\n-      input_sum = tf.reduce_sum(weighted_inputs, axis=self.axis)\n-      weight_sum = tf.reduce_sum(weights, axis=self.axis)\n-      return tf.divide(input_sum, weight_sum)\n-\n-    # sqrtn is also more complicated: it's like mean but with a normalized\n-    # divisor.\n-    if self.reduction == \"sqrtn\":\n-      logging.warning(\"Reduction `sqrtn` is deprecated and will be removed \"\n-                      \"2021-01-01. Please use the `sum` reduction and divide \"\n-                      \"the output by the normalized weights instead.\")\n-      input_sum = tf.reduce_sum(weighted_inputs, axis=self.axis)\n-      squared_weights = tf.pow(weights, 2)\n-      squared_weights_sum = tf.reduce_sum(squared_weights, axis=self.axis)\n-      sqrt_weights_sum = tf.sqrt(squared_weights_sum)\n-      return tf.divide(input_sum, sqrt_weights_sum)\n-\n-    raise ValueError(\"%s is not a supported weighted reduction.\" %\n-                     self.reduction)\n\n@@ -1,227 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tests for keras.layers.preprocessing.reduction.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-from absl.testing import parameterized\n-import numpy as np\n-\n-import keras\n-\n-from keras import keras_parameterized\n-from keras.layers.preprocessing import reduction\n-\n-\n-@keras_parameterized.run_all_keras_modes\n-class ReductionTest(keras_parameterized.TestCase):\n-\n-  @parameterized.named_parameters(\n-      {\n-          \"testcase_name\": \"max\",\n-          \"reduction_str\": \"max\",\n-          \"expected_output\": [[3.0, 3.0], [3.0, 2.0]]\n-      }, {\n-          \"testcase_name\": \"mean\",\n-          \"reduction_str\": \"mean\",\n-          \"expected_output\": [[2.0, 2.0], [2.0, 1.5]]\n-      }, {\n-          \"testcase_name\": \"min\",\n-          \"reduction_str\": \"min\",\n-          \"expected_output\": [[1.0, 1.0], [1.0, 1.0]]\n-      }, {\n-          \"testcase_name\": \"prod\",\n-          \"reduction_str\": \"prod\",\n-          \"expected_output\": [[6.0, 6.0], [3.0, 2.0]]\n-      }, {\n-          \"testcase_name\": \"sum\",\n-          \"reduction_str\": \"sum\",\n-          \"expected_output\": [[6.0, 6.0], [4.0, 3.0]]\n-      })\n-  def test_unweighted_ragged_reduction(self, reduction_str, expected_output):\n-    data = tf.ragged.constant([[[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]],\n-                                        [[3.0, 1.0], [1.0, 2.0]]])\n-    input_tensor = keras.Input(shape=(None, None), ragged=True)\n-\n-    output_tensor = reduction.Reduction(reduction=reduction_str)(input_tensor)\n-    model = keras.Model(input_tensor, output_tensor)\n-\n-    output = model.predict(data)\n-\n-    self.assertAllClose(expected_output, output)\n-\n-  @parameterized.named_parameters(\n-      {\n-          \"testcase_name\": \"max\",\n-          \"reduction_str\": \"max\",\n-          \"expected_output\": [[4.0, 4.0], [1.5, 6.0]]\n-      }, {\n-          \"testcase_name\": \"mean\",\n-          \"reduction_str\": \"mean\",\n-          \"expected_output\": [[2.0, 2.0], [1.666667, 1.75]]\n-      }, {\n-          \"testcase_name\": \"min\",\n-          \"reduction_str\": \"min\",\n-          \"expected_output\": [[1.0, 1.0], [1.0, 1.0]]\n-      }, {\n-          \"testcase_name\": \"prod\",\n-          \"reduction_str\": \"prod\",\n-          \"expected_output\": [[12.0, 12.0], [1.5, 6.0]]\n-      }, {\n-          \"testcase_name\": \"sum\",\n-          \"reduction_str\": \"sum\",\n-          \"expected_output\": [[8.0, 8.0], [2.5, 7.0]]\n-      }, {\n-          \"testcase_name\": \"sqrtn\",\n-          \"reduction_str\": \"sqrtn\",\n-          \"expected_output\": [[3.265986, 3.265986], [2.236067, 2.213594]]\n-      })\n-  def test_weighted_ragged_reduction(self, reduction_str, expected_output):\n-    data = tf.ragged.constant([[[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]],\n-                                        [[3.0, 1.0], [1.0, 2.0]]])\n-    input_tensor = keras.Input(shape=(None, None), ragged=True)\n-\n-    weights = tf.ragged.constant([[[1.0, 1.0], [2.0, 2.0], [1.0, 1.0]],\n-                                           [[0.5, 1.0], [1.0, 3.0]]])\n-    weight_input_tensor = keras.Input(shape=(None, None), ragged=True)\n-\n-    output_tensor = reduction.Reduction(reduction=reduction_str)(\n-        input_tensor, weights=weight_input_tensor)\n-    model = keras.Model([input_tensor, weight_input_tensor], output_tensor)\n-\n-    output = model.predict([data, weights])\n-    self.assertAllClose(expected_output, output)\n-\n-  def test_weighted_ragged_reduction_with_different_dimensionality(self):\n-    data = tf.ragged.constant([[[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]],\n-                                        [[3.0, 1.0], [1.0, 2.0]]])\n-    input_tensor = keras.Input(shape=(None, None), ragged=True)\n-\n-    weights = tf.ragged.constant([[1.0, 2.0, 1.0], [1.0, 1.0]])\n-    weight_input_tensor = keras.Input(shape=(None,), ragged=True)\n-\n-    output_tensor = reduction.Reduction(reduction=\"mean\")(\n-        input_tensor, weights=weight_input_tensor)\n-    model = keras.Model([input_tensor, weight_input_tensor], output_tensor)\n-\n-    output = model.predict([data, weights])\n-    expected_output = [[2.0, 2.0], [2.0, 1.5]]\n-    self.assertAllClose(expected_output, output)\n-\n-  @parameterized.named_parameters(\n-      {\n-          \"testcase_name\": \"max\",\n-          \"reduction_str\": \"max\",\n-          \"expected_output\": [[3.0, 3.0], [3.0, 2.0]]\n-      }, {\n-          \"testcase_name\": \"mean\",\n-          \"reduction_str\": \"mean\",\n-          \"expected_output\": [[2.0, 2.0], [1.333333, 1.0]]\n-      }, {\n-          \"testcase_name\": \"min\",\n-          \"reduction_str\": \"min\",\n-          \"expected_output\": [[1.0, 1.0], [0.0, 0.0]]\n-      }, {\n-          \"testcase_name\": \"prod\",\n-          \"reduction_str\": \"prod\",\n-          \"expected_output\": [[6.0, 6.0], [0.0, 0.0]]\n-      }, {\n-          \"testcase_name\": \"sum\",\n-          \"reduction_str\": \"sum\",\n-          \"expected_output\": [[6.0, 6.0], [4.0, 3.0]]\n-      })\n-  def test_unweighted_dense_reduction(self, reduction_str, expected_output):\n-    data = np.array([[[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]],\n-                     [[3.0, 1.0], [1.0, 2.0], [0.0, 0.0]]])\n-    input_tensor = keras.Input(shape=(None, None))\n-\n-    output_tensor = reduction.Reduction(reduction=reduction_str)(input_tensor)\n-    model = keras.Model(input_tensor, output_tensor)\n-\n-    output = model.predict(data)\n-\n-    self.assertAllClose(expected_output, output)\n-\n-  @parameterized.named_parameters(\n-      {\n-          \"testcase_name\": \"max\",\n-          \"reduction_str\": \"max\",\n-          \"expected_output\": [[4.0, 4.0], [1.5, 6.0]]\n-      }, {\n-          \"testcase_name\": \"mean\",\n-          \"reduction_str\": \"mean\",\n-          \"expected_output\": [[2.0, 2.0], [1.666667, 1.75]]\n-      }, {\n-          \"testcase_name\": \"min\",\n-          \"reduction_str\": \"min\",\n-          \"expected_output\": [[1.0, 1.0], [0.0, 0.0]]\n-      }, {\n-          \"testcase_name\": \"prod\",\n-          \"reduction_str\": \"prod\",\n-          \"expected_output\": [[12.0, 12.0], [0.0, 0.0]]\n-      }, {\n-          \"testcase_name\": \"sum\",\n-          \"reduction_str\": \"sum\",\n-          \"expected_output\": [[8.0, 8.0], [2.5, 7.0]]\n-      }, {\n-          \"testcase_name\": \"sqrtn\",\n-          \"reduction_str\": \"sqrtn\",\n-          \"expected_output\": [[3.265986, 3.265986], [2.236067, 2.213594]]\n-      })\n-  def test_weighted_dense_reduction(self, reduction_str, expected_output):\n-    data = np.array([[[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]],\n-                     [[3.0, 1.0], [1.0, 2.0], [0.0, 0.0]]])\n-    input_tensor = keras.Input(shape=(None, None))\n-\n-    weights = np.array([[[1.0, 1.0], [2.0, 2.0], [1.0, 1.0]],\n-                        [[0.5, 1.0], [1.0, 3.0], [0.0, 0.0]]])\n-    weight_input_tensor = keras.Input(shape=(None, None))\n-\n-    output_tensor = reduction.Reduction(reduction=reduction_str)(\n-        input_tensor, weights=weight_input_tensor)\n-    model = keras.Model([input_tensor, weight_input_tensor], output_tensor)\n-\n-    output = model.predict([data, weights])\n-\n-    self.assertAllClose(expected_output, output)\n-\n-  def test_weighted_dense_reduction_with_different_dimensionality(self):\n-    data = np.array([[[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]],\n-                     [[3.0, 1.0], [1.0, 2.0], [0.0, 0.0]]])\n-    input_tensor = keras.Input(shape=(None, None))\n-\n-    weights = np.array([[1.0, 2.0, 1.0], [1.0, 1.0, 0.0]])\n-    weight_input_tensor = keras.Input(shape=(None,))\n-\n-    output_tensor = reduction.Reduction(reduction=\"mean\")(\n-        input_tensor, weights=weight_input_tensor)\n-    model = keras.Model([input_tensor, weight_input_tensor], output_tensor)\n-\n-    output = model.predict([data, weights])\n-    expected_output = [[2.0, 2.0], [2.0, 1.5]]\n-    self.assertAllClose(expected_output, output)\n-\n-  def test_sqrtn_fails_on_unweighted_ragged(self):\n-    input_tensor = keras.Input(shape=(None, None), ragged=True)\n-    with self.assertRaisesRegex(ValueError, \".*sqrtn.*\"):\n-      _ = reduction.Reduction(reduction=\"sqrtn\")(input_tensor)\n-\n-  def test_sqrtn_fails_on_unweighted_dense(self):\n-    input_tensor = keras.Input(shape=(None, None))\n-    with self.assertRaisesRegex(ValueError, \".*sqrtn.*\"):\n-      _ = reduction.Reduction(reduction=\"sqrtn\")(input_tensor)\n-\n-if __name__ == \"__main__\":\n-  tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
