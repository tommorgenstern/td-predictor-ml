{"custom_id": "keras#6bd6d0df856c2ebb386d37e953a2f2b2eb5abc3a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 168 | Files Changed: 4 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 168 | Churn Cumulative: 2161 | Contributors (this commit): 17 | Commits (past 90d): 19 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -200,7 +200,6 @@ from keras.layers.rnn.base_rnn import RNN\n from keras.layers.rnn.abstract_rnn_cell import AbstractRNNCell\n from keras.layers.rnn.stacked_rnn_cells import StackedRNNCells\n from keras.layers.rnn.simple_rnn import SimpleRNNCell\n-from keras.layers.rnn.lstm_v1 import PeepholeLSTMCell\n from keras.layers.rnn.simple_rnn import SimpleRNN\n \n if tf.__internal__.tf2.enabled():\n\n@@ -22,7 +22,6 @@ from keras.layers.rnn.base_rnn import RNN\n from keras.layers.rnn.abstract_rnn_cell import AbstractRNNCell\n from keras.layers.rnn.stacked_rnn_cells import StackedRNNCells\n from keras.layers.rnn.simple_rnn import SimpleRNNCell\n-from keras.layers.rnn.lstm_v1 import PeepholeLSTMCell\n from keras.layers.rnn.simple_rnn import SimpleRNN\n \n if tf.__internal__.tf2.enabled():\n\n@@ -25,7 +25,6 @@ import keras\n from keras.engine import base_layer_utils\n from keras.layers.rnn import gru\n from keras.layers.rnn import gru_v1\n-from keras.layers.rnn import legacy_cells\n from keras.layers.rnn import lstm\n from keras.layers.rnn import lstm_v1\n from keras.testing_infra import test_combinations\n@@ -1319,48 +1318,6 @@ class RNNTest(test_combinations.TestCase):\n          np.zeros((batch, t, o2, o3))])\n     self.assertEqual(model.output_shape, [(None, t, o1), (None, t, o2, o3)])\n \n-  def test_peephole_lstm_cell(self):\n-\n-    def _run_cell(cell_fn, **kwargs):\n-      inputs = tf.one_hot([1, 2, 3, 4], 4)\n-      cell = cell_fn(5, **kwargs)\n-      cell.build(inputs.shape)\n-      initial_state = cell.get_initial_state(\n-          inputs=inputs, batch_size=4, dtype=tf.float32)\n-      inputs, _ = cell(inputs, initial_state)\n-      output = inputs\n-      if not tf.executing_eagerly():\n-        self.evaluate(tf.compat.v1.global_variables_initializer())\n-        output = self.evaluate(output)\n-      return output\n-\n-    tf.compat.v1.set_random_seed(12345)\n-    # `recurrent_activation` kwarg is set to sigmoid as that is hardcoded into\n-    # rnn_cell.LSTMCell.\n-    no_peephole_output = _run_cell(\n-        keras.layers.LSTMCell,\n-        kernel_initializer='ones',\n-        recurrent_activation='sigmoid',\n-        implementation=1)\n-    first_implementation_output = _run_cell(\n-        keras.layers.PeepholeLSTMCell,\n-        kernel_initializer='ones',\n-        recurrent_activation='sigmoid',\n-        implementation=1)\n-    second_implementation_output = _run_cell(\n-        keras.layers.PeepholeLSTMCell,\n-        kernel_initializer='ones',\n-        recurrent_activation='sigmoid',\n-        implementation=2)\n-    tf_lstm_cell_output = _run_cell(\n-        legacy_cells.LSTMCell,\n-        use_peepholes=True,\n-        initializer=tf.compat.v1.ones_initializer)\n-    self.assertNotAllClose(first_implementation_output, no_peephole_output)\n-    self.assertAllClose(first_implementation_output,\n-                        second_implementation_output)\n-    self.assertAllClose(first_implementation_output, tf_lstm_cell_output)\n-\n   def test_masking_rnn_with_output_and_states(self):\n \n     class Cell(keras.layers.Layer):\n\n@@ -15,8 +15,6 @@\n \"\"\"Long Short-Term Memory V1 layer.\"\"\"\n # pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n-import warnings\n-\n from keras import activations\n from keras import backend\n from keras import constraints\n@@ -317,127 +315,6 @@ class LSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n         self, inputs, batch_size, dtype))\n \n \n-@keras_export('keras.experimental.PeepholeLSTMCell')\n-class PeepholeLSTMCell(LSTMCell):\n-  \"\"\"Equivalent to LSTMCell class but adds peephole connections.\n-\n-  Peephole connections allow the gates to utilize the previous internal state as\n-  well as the previous hidden state (which is what LSTMCell is limited to).\n-  This allows PeepholeLSTMCell to better learn precise timings over LSTMCell.\n-\n-  From [Gers et al., 2002](\n-    http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf):\n-\n-  \"We find that LSTM augmented by 'peephole connections' from its internal\n-  cells to its multiplicative gates can learn the fine distinction between\n-  sequences of spikes spaced either 50 or 49 time steps apart without the help\n-  of any short training exemplars.\"\n-\n-  The peephole implementation is based on:\n-\n-  [Sak et al., 2014](https://research.google.com/pubs/archive/43905.pdf)\n-\n-  Example:\n-\n-  ```python\n-  # Create 2 PeepholeLSTMCells\n-  peephole_lstm_cells = [PeepholeLSTMCell(size) for size in [128, 256]]\n-  # Create a layer composed sequentially of the peephole LSTM cells.\n-  layer = RNN(peephole_lstm_cells)\n-  input = keras.Input((timesteps, input_dim))\n-  output = layer(input)\n-  ```\n-  \"\"\"\n-\n-  def __init__(self,\n-               units,\n-               activation='tanh',\n-               recurrent_activation='hard_sigmoid',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               unit_forget_bias=True,\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               dropout=0.,\n-               recurrent_dropout=0.,\n-               **kwargs):\n-    warnings.warn(\n-        '`tf.keras.experimental.PeepholeLSTMCell` is deprecated '\n-        'and will be removed in a future version. '\n-        'Please use tensorflow_addons.rnn.PeepholeLSTMCell '\n-        'instead.',\n-        stacklevel=2)\n-    super(PeepholeLSTMCell, self).__init__(\n-        units=units,\n-        activation=activation,\n-        recurrent_activation=recurrent_activation,\n-        use_bias=use_bias,\n-        kernel_initializer=kernel_initializer,\n-        recurrent_initializer=recurrent_initializer,\n-        bias_initializer=bias_initializer,\n-        unit_forget_bias=unit_forget_bias,\n-        kernel_regularizer=kernel_regularizer,\n-        recurrent_regularizer=recurrent_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        kernel_constraint=kernel_constraint,\n-        recurrent_constraint=recurrent_constraint,\n-        bias_constraint=bias_constraint,\n-        dropout=dropout,\n-        recurrent_dropout=recurrent_dropout,\n-        implementation=kwargs.pop('implementation', 1),\n-        **kwargs)\n-\n-  def build(self, input_shape):\n-    super(PeepholeLSTMCell, self).build(input_shape)\n-    # The following are the weight matrices for the peephole connections. These\n-    # are multiplied with the previous internal state during the computation of\n-    # carry and output.\n-    self.input_gate_peephole_weights = self.add_weight(\n-        shape=(self.units,),\n-        name='input_gate_peephole_weights',\n-        initializer=self.kernel_initializer)\n-    self.forget_gate_peephole_weights = self.add_weight(\n-        shape=(self.units,),\n-        name='forget_gate_peephole_weights',\n-        initializer=self.kernel_initializer)\n-    self.output_gate_peephole_weights = self.add_weight(\n-        shape=(self.units,),\n-        name='output_gate_peephole_weights',\n-        initializer=self.kernel_initializer)\n-\n-  def _compute_carry_and_output(self, x, h_tm1, c_tm1):\n-    x_i, x_f, x_c, x_o = x\n-    h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1\n-    i = self.recurrent_activation(\n-        x_i + backend.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]) +\n-        self.input_gate_peephole_weights * c_tm1)\n-    f = self.recurrent_activation(x_f + backend.dot(\n-        h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]) +\n-                                  self.forget_gate_peephole_weights * c_tm1)\n-    c = f * c_tm1 + i * self.activation(x_c + backend.dot(\n-        h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))\n-    o = self.recurrent_activation(\n-        x_o + backend.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:]) +\n-        self.output_gate_peephole_weights * c)\n-    return c, o\n-\n-  def _compute_carry_and_output_fused(self, z, c_tm1):\n-    z0, z1, z2, z3 = z\n-    i = self.recurrent_activation(z0 +\n-                                  self.input_gate_peephole_weights * c_tm1)\n-    f = self.recurrent_activation(z1 +\n-                                  self.forget_gate_peephole_weights * c_tm1)\n-    c = f * c_tm1 + i * self.activation(z2)\n-    o = self.recurrent_activation(z3 + self.output_gate_peephole_weights * c)\n-    return c, o\n-\n-\n @keras_export(v1=['keras.layers.LSTM'])\n class LSTM(RNN):\n   \"\"\"Long Short-Term Memory layer - Hochreiter 1997.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#75d70a610dffe927d89ceb400d79bb7f9027b26e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 189 | Lines Deleted: 21 | Files Changed: 3 | Hunks: 21 | Methods Changed: 14 | Complexity Δ (Sum/Max): 24/14 | Churn Δ: 210 | Churn Cumulative: 811 | Contributors (this commit): 6 | Commits (past 90d): 7 | Contributors (cumulative): 9 | DMM Complexity: 0.6981132075471698\n\nDIFF:\n@@ -15,10 +15,10 @@\n # ==============================================================================\n \"\"\"Tests for ClusterCoordinator and Keras models.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n import keras\n from keras.distribute import multi_worker_testing_utils\n from keras.engine import base_layer\n+import tensorflow.compat.v2 as tf\n \n \n class ShardedVariableTest(tf.test.TestCase):\n@@ -28,7 +28,8 @@ class ShardedVariableTest(tf.test.TestCase):\n     super().setUpClass()\n     cls.strategy = tf.distribute.experimental.ParameterServerStrategy(\n         multi_worker_testing_utils.make_parameter_server_cluster(3, 2),\n-        variable_partitioner=tf.distribute.experimental.partitioners.FixedShardsPartitioner(2))\n+        variable_partitioner=tf.distribute.experimental.partitioners\n+        .FixedShardsPartitioner(2))\n \n   def assert_list_all_equal(self, list1, list2):\n     \"\"\"Used in lieu of `assertAllEqual`.\n@@ -157,6 +158,110 @@ class ShardedVariableTest(tf.test.TestCase):\n       with self.strategy.scope():\n         keras.models.load_model(saved_dir)\n \n+  def test_slot_variable_checkpointing(self):\n+\n+    with self.strategy.scope():\n+      # Set a name so the ShardedVariable is well-named for slot var keying\n+      var = tf.Variable([1., 2., 3., 4., 5., 6.], name='test')\n+\n+    opt = keras.optimizers.optimizer_v2.adam.Adam()\n+\n+    # Run once to trigger apply_gradients to populate optimizer slot variables.\n+    def train_step():\n+      with tf.GradientTape() as tape:\n+        loss = sum(var)\n+      opt.minimize(loss, var.variables, tape=tape)\n+\n+    self.strategy.run(train_step)\n+\n+    # Check that we can call get_slot using each slot, before and after\n+    # Checkpointing, and get the same results\n+    pre_ckpt_slots = []\n+    for slot in opt.get_slot_names():\n+      pre_ckpt_slots.extend([v.numpy() for v in opt.get_slot(var, slot)])\n+\n+    ckpt = tf.train.Checkpoint(var=var, opt=opt)\n+\n+    # Assert that checkpoint has slots for each shard and the ShardedVariable\n+    self.assertLen(ckpt.opt._slots, 3)\n+    for var_name in ckpt.opt._slots.keys():\n+      self.assertLen(ckpt.opt._slots[var_name], 2)\n+      self.assertEqual(ckpt.opt._slots[var_name].keys(), {'m', 'v'})\n+      if hasattr(ckpt.opt._slots[var_name]['m'], 'variables'):\n+        self.assertLen(ckpt.opt._slots[var_name]['m'].variables, 2)\n+        self.assertLen(ckpt.opt._slots[var_name]['v'].variables, 2)\n+\n+    saved_dir = self.get_temp_dir()\n+    ckpt_prefix = f'{saved_dir}/ckpt'\n+    ckpt.save(ckpt_prefix)\n+\n+    # Run once more to alter slot variables and ensure checkpoint restores\n+    # the earlier values.\n+    self.strategy.run(train_step)\n+\n+    changed_ckpt_slots = []\n+    for slot in opt.get_slot_names():\n+      changed_ckpt_slots.extend([v.numpy() for v in opt.get_slot(var, slot)])\n+    self.assertNotAllClose(pre_ckpt_slots, changed_ckpt_slots)\n+\n+    ckpt.restore(tf.train.latest_checkpoint(saved_dir))\n+\n+    post_ckpt_slots = []\n+    for slot in opt.get_slot_names():\n+      post_ckpt_slots.extend([v.numpy() for v in opt.get_slot(var, slot)])\n+\n+    self.assertAllClose(pre_ckpt_slots, post_ckpt_slots)\n+\n+  def test_slot_variable_checkpoint_load_with_diff_shards(self):\n+\n+    with self.strategy.scope():\n+      # Set a name so the ShardedVariable is well-named for slot var keying\n+      var = tf.Variable([1., 2., 3., 4., 5., 6.], name='test')\n+\n+    opt = keras.optimizers.optimizer_v2.adam.Adam()\n+\n+    # Run once to trigger apply_gradients to populate optimizer slot variables.\n+    def train_step():\n+      with tf.GradientTape() as tape:\n+        loss = sum(var)\n+      opt.minimize(loss, var.variables, tape=tape)\n+\n+    self.strategy.run(train_step)\n+\n+    # Check that we can call get_slot using each slot, before and after\n+    # Checkpointing, and get the same results\n+    pre_ckpt_slots = []\n+    for slot in opt.get_slot_names():\n+      pre_ckpt_slots.extend(\n+          tf.concat(list(opt.get_slot(var, slot)), axis=0).numpy())\n+\n+    ckpt = tf.train.Checkpoint(var=var, opt=opt)\n+    saved_dir = self.get_temp_dir()\n+    ckpt_prefix = f'{saved_dir}/ckpt'\n+    ckpt.save(ckpt_prefix)\n+\n+    # Create new strategy with different number of shards\n+    strategy2 = tf.distribute.experimental.ParameterServerStrategy(\n+        multi_worker_testing_utils.make_parameter_server_cluster(3, 2),\n+        variable_partitioner=tf.distribute.experimental.partitioners\n+        .FixedShardsPartitioner(3))\n+\n+    # Create new variable with different values, to be overwritten by ckpt.\n+    with strategy2.scope():\n+      var = tf.Variable([0., 1., 2., 3., 4., 5.], name='test')\n+\n+    opt = keras.optimizers.optimizer_v2.adam.Adam()\n+    # Run once to trigger apply_gradients to populate optimizer slot variables.\n+    strategy2.run(train_step)\n+\n+    new_ckpt = tf.train.Checkpoint(var=var, opt=opt)\n+    new_ckpt.restore(tf.train.latest_checkpoint(saved_dir))\n+    post_ckpt_slots = []\n+    for slot in new_ckpt.opt.get_slot_names():\n+      post_ckpt_slots.extend(\n+          tf.concat(list(new_ckpt.opt.get_slot(var, slot)),\n+                    axis=0).numpy())\n+    self.assertAllClose(pre_ckpt_slots, post_ckpt_slots)\n \n if __name__ == '__main__':\n   tf.compat.v1.enable_v2_behavior()\n\n@@ -30,7 +30,7 @@ from keras.utils import layer_utils\n from keras.utils import tf_inspect\n from keras.utils import tf_utils\n import tensorflow.compat.v2 as tf\n-from tensorflow.python.util.tf_export import keras_export\n+from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-direct-tensorflow-import\n \n \n keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(\n@@ -656,8 +656,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n             \"`tf.distribute.get_replica_context`.\")\n \n       strategy = tf.distribute.get_strategy()\n-      if (not experimental_aggregate_gradients and strategy and\n-          isinstance(strategy,\n+      if (not experimental_aggregate_gradients and strategy and isinstance(\n+          strategy,\n           (tf.compat.v1.distribute.experimental.ParameterServerStrategy,\n            tf.distribute.experimental.ParameterServerStrategy,\n            tf.distribute.experimental.CentralStorageStrategy,\n@@ -812,6 +812,34 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n   def _create_slots(self, var_list):\n     pass\n \n+  def _create_slots_for_sharded_variables(self, var_list):\n+    \"\"\"Add ShardedVariables to slots to later reconstruct for checkpointing.\n+\n+    ShardedVariables don't have slot variables created for them; their shards\n+    do. This function allows users to call get_slot with a ShardedVariable input\n+    and receive a ShardedVariable output containing the appropriate slot vars.\n+\n+    Iterate over the variables to find shards, and aggregate the sharded\n+    containers in a set. Add these ShardedVariables to _slots so that get_slot\n+    can retrieve the proper slot variables for their component shards, and\n+    reconstruct those into a ShardedVariable.\n+\n+    Args:\n+      var_list: list or tuple of `Variable` objects that will be minimized\n+        using this optimizer.\n+    \"\"\"\n+    sharded_vars = set()\n+    for var in var_list:\n+      if getattr(var, \"_sharded_container\", False):\n+        sharded_vars.add(var._sharded_container())  # pylint: disable=protected-access\n+\n+    for sharded_var in sharded_vars:\n+      sharded_key = _var_key(sharded_var)\n+      slot_dict = {}\n+      for slot in self.get_slot_names():\n+        slot_dict[slot] = sharded_var\n+      self._slots[sharded_key] = slot_dict\n+\n   def _create_all_weights(self, var_list):\n     \"\"\"Creates all weights, including iterations, hyperparameters and slot vars.\n \n@@ -828,6 +856,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     _ = self.iterations\n     self._create_hypers()\n     self._create_slots(var_list)\n+    self._create_slots_for_sharded_variables(var_list)\n \n   def __getattribute__(self, name):\n     \"\"\"Overridden to support hyperparameter access.\"\"\"\n@@ -890,9 +919,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     if weight is None:\n       if isinstance(initializer, str) or callable(initializer):\n         initializer = initializers.get(initializer)\n-        if isinstance(\n-            initializer,\n-            tf.__internal__.tracking.CheckpointInitialValueCallable) or (shape is not None):\n+        if isinstance(initializer, tf.__internal__.tracking\n+                      .CheckpointInitialValueCallable) or (shape is not None):\n           slot_shape = shape\n         else:\n           slot_shape = var.shape\n@@ -929,7 +957,20 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n   def get_slot(self, var, slot_name):\n     var_key = _var_key(var)\n     slot_dict = self._slots[var_key]\n-    return slot_dict[slot_name]\n+    slot_variable = slot_dict[slot_name]\n+    if isinstance(slot_variable,\n+                  tf.__internal__.distribute.ShardedVariable):\n+      # Construct a ShardedVariable that points to the input ShardedVariable's\n+      # component shard's slot variables.\n+      shard_vars = []\n+      for shard in slot_variable.variables:\n+        slot_shard = self.get_slot(shard, slot_name)\n+        shard_vars.append(slot_shard)\n+      slot_variable = (\n+          tf.__internal__.distribute.ShardedVariable(\n+              shard_vars, name=slot_variable.name)\n+          )\n+    return slot_variable\n \n   def _prepare(self, var_list):\n     keys = set()\n@@ -1407,6 +1448,11 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n       # instead special-case this dependency and otherwise pretend it's a normal\n       # graph.\n     if slot_variable is not None:\n+      # For sharded variables, we need the logic in get_slot to combine slot\n+      # variables for its shards\n+      if (slot_variable is variable) and (isinstance(\n+          variable, tf.__internal__.distribute.ShardedVariable)):\n+        return self.get_slot(variable, slot_name)\n       # If we've either made this slot variable, or if we've pulled out an\n       # existing slot variable, we should restore it.\n       return slot_variable\n@@ -1448,7 +1494,7 @@ def _var_key(var):\n   # Get the distributed variable if it exists.\n   if hasattr(var, \"_distributed_container\"):\n     var = var._distributed_container()\n-  if var._in_graph_mode:\n+  if getattr(var, \"_in_graph_mode\", False):\n     return var._shared_name\n   return var._unique_id\n \n\n@@ -14,37 +14,36 @@\n # ==============================================================================\n \"\"\"Functional test for OptimizerV2.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n import collections\n \n from absl.testing import parameterized\n-import numpy as np\n-\n import keras\n-from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n from keras import backend\n from keras import callbacks\n from keras import losses\n-from keras.optimizers import optimizer_v1\n from keras.engine import input_layer\n from keras.engine import sequential\n from keras.engine import training\n from keras.layers import core\n from keras.layers import regularization\n+from keras.optimizers import learning_rate_schedule\n+from keras.optimizers import optimizer_v1\n from keras.optimizers.optimizer_v2 import adadelta\n from keras.optimizers.optimizer_v2 import adagrad\n from keras.optimizers.optimizer_v2 import adam\n from keras.optimizers.optimizer_v2 import adamax\n from keras.optimizers.optimizer_v2 import ftrl\n from keras.optimizers.optimizer_v2 import gradient_descent\n-from keras.optimizers import learning_rate_schedule\n from keras.optimizers.optimizer_v2 import nadam\n from keras.optimizers.optimizer_v2 import optimizer_v2\n from keras.optimizers.optimizer_v2 import rmsprop\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import np_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n \n \n _DATA_TYPES = [tf.half, tf.float32, tf.float64]\n@@ -251,10 +250,8 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     constraint_01 = lambda x: tf.clip_by_value(x, -0.1, 0.)\n     constraint_0 = lambda x: tf.clip_by_value(x, 0., 1.)\n     with test_utils.use_gpu():\n-      var0 = tf.Variable([1.0, 2.0],\n-                                constraint=constraint_01)\n-      var1 = tf.Variable([3.0, 4.0],\n-                                constraint=constraint_0)\n+      var0 = tf.Variable([1.0, 2.0], constraint=constraint_01)\n+      var1 = tf.Variable([3.0, 4.0], constraint=constraint_0)\n       loss = lambda: 5 * var0 + 3 * var1\n       sgd = gradient_descent.SGD(3.0)\n \n@@ -770,6 +767,26 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.evaluate(opt_op)\n     self.assertEqual(self.evaluate(var), 1.0)\n \n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n+  def test_create_slots_for_sharded_variables(self):\n+    # set names so that ShardedVariable is well-named for slot variable keying.\n+    var_a = tf.Variable([1.0], name='part_0')\n+    var_b = tf.Variable([2.0], name='part_1')\n+    sharded_var = tf.__internal__.distribute.ShardedVariable([var_a, var_b])\n+\n+    opt = adagrad.Adagrad()\n+    opt._create_slots(sharded_var.variables)\n+    opt._create_slots_for_sharded_variables(sharded_var.variables)\n+\n+    sharded_slot = opt.get_slot(sharded_var, 'accumulator')\n+    self.assertIsInstance(\n+        sharded_slot, tf.__internal__.distribute.ShardedVariable)\n+\n+    slot_a = opt.get_slot(var_a, 'accumulator')\n+    self.assertAllClose(sharded_slot.variables[0], slot_a)\n+    slot_b = opt.get_slot(var_b, 'accumulator')\n+    self.assertAllClose(sharded_slot.variables[1], slot_b)\n+\n \n @test_combinations.run_all_keras_modes\n class OptimizersCompatibilityTest(test_combinations.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#25709a205dd97d1979b3949b81833d728d15b89d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 125 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 130 | Churn Cumulative: 10845 | Contributors (this commit): 26 | Commits (past 90d): 20 | Contributors (cumulative): 35 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,6 +20,7 @@\n import tensorflow.compat.v2 as tf\n \n import collections\n+import contextlib\n import copy\n import functools\n import itertools\n@@ -83,6 +84,42 @@ keras_premade_model_gauge = tf.__internal__.monitoring.BoolGauge(\n \n _is_name_scope_on_model_declaration_enabled = False\n \n+_name_scope_unnester_stack = threading.local()\n+\n+\n+@contextlib.contextmanager\n+def _name_scope_unnester(full_name_scope):\n+  \"\"\"Helper to get relative name scope from fully specified nested name scopes.\n+\n+  Args:\n+    full_name_scope: full(absolute) name scope path.\n+\n+  Yields:\n+    Relative name scope path from the parent `_name_scope_unnester` context\n+    manager.\n+\n+  Example:\n+  ```\n+  with _name_scope_unnester('a') as name1:  # name1 == 'a'\n+    with _name_scope_unnester('a/b') as name2:  # name2 == 'b'\n+      with _name_scope_unnester('a/b/c') as name3:  # name3 == 'c'\n+        pass\n+  ```\n+  \"\"\"\n+  if not getattr(_name_scope_unnester_stack, 'value', None):\n+    _name_scope_unnester_stack.value = ['']\n+\n+  _name_scope_unnester_stack.value.append(full_name_scope)\n+\n+  try:\n+    full_name_scope = _name_scope_unnester_stack.value[-1]\n+    outer_name_scope = _name_scope_unnester_stack.value[-2]\n+    relative_name_scope = full_name_scope.lstrip(outer_name_scope)\n+    relative_name_scope = relative_name_scope.lstrip('/')\n+    yield relative_name_scope\n+  finally:\n+    _name_scope_unnester_stack.value.pop()\n+\n \n @keras_export('keras.layers.Layer')\n class Layer(tf.Module, version_utils.LayerVersionSelector):\n@@ -413,7 +450,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n     # Save outer name scope at layer declaration so that it is preserved at\n     # the actual layer construction.\n-    self._outer_name_scope = tf.get_current_name_scope()\n+    self._name_scope_on_declaration = tf.get_current_name_scope()\n \n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n   @generic_utils.default\n@@ -928,17 +965,23 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         training=training_mode):\n \n       input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n+\n       if eager:\n         call_fn = self.call\n         name_scope = self._name\n       else:\n-        name_scope = self._name_scope()  # Avoid autoincrementing.  # pylint: disable=not-callable\n+        name_scope = self._get_unnested_name_scope()\n         call_fn = self._autographed_call()\n+\n       call_fn = traceback_utils.inject_argument_info_in_traceback(\n           call_fn,\n           object_name=f'layer \"{self.name}\" (type {self.__class__.__name__})')\n+      with contextlib.ExitStack() as namescope_stack:\n+        if _is_name_scope_on_model_declaration_enabled:\n+          namescope_stack.enter_context(_name_scope_unnester(\n+              self._name_scope_on_declaration))\n+        namescope_stack.enter_context(tf.name_scope(name_scope))\n \n-      with tf.name_scope(name_scope):\n         if not self.built:\n           self._maybe_build(inputs)\n \n@@ -958,6 +1001,24 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n         return outputs\n \n+  def _get_unnested_name_scope(self):\n+    if _is_name_scope_on_model_declaration_enabled:\n+      with _name_scope_unnester(self._name_scope_on_declaration\n+                               ) as relative_name_scope_on_declaration:\n+      # To avoid `tf.name_scope` autoincrement, use absolute path.\n+        relative_name_scope = filter(\n+            None,\n+            [tf.get_current_name_scope(), relative_name_scope_on_declaration])\n+        current_name_scope = '/'.join(relative_name_scope) + '/'\n+        if current_name_scope == '/':\n+          current_name_scope = self._name_scope_on_declaration\n+        with tf.name_scope(current_name_scope):\n+          name_scope = self._name_scope()  # Avoid autoincrementing.  # pylint: disable=not-callable\n+    else:\n+      name_scope = self._name_scope()\n+\n+    return name_scope\n+\n   @property\n   def dtype(self):\n     \"\"\"The dtype of the layer weights.\n@@ -2423,8 +2484,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     if not tf.__internal__.tf2.enabled():\n       return self.name\n     name_scope = self.name\n-    if _is_name_scope_on_model_declaration_enabled and self._outer_name_scope:\n-      name_scope = self._outer_name_scope + '/' + name_scope\n     current_name_scope = tf.__internal__.get_name_scope()\n     if current_name_scope:\n       name_scope = current_name_scope + '/' + name_scope\n\n@@ -1436,6 +1436,67 @@ class NameScopingTest(test_combinations.TestCase):\n     ])\n     base_layer._apply_name_scope_on_model_declaration(False)\n \n+  @test_utils.run_v2_only\n+  def test_apply_name_scope_on_nested_layer_model_declaration(self):\n+    if not tf.executing_eagerly():\n+      self.skipTest('`apply_name_scope_on_model_declaration` API is supported'\n+                    ' only for V2 eager')\n+\n+    base_layer._apply_name_scope_on_model_declaration(True)\n+\n+    class ThreeDenses(layers.Layer):\n+\n+      def __init__(self, name='ThreeDenses', **kwargs):\n+        super().__init__(name=name, **kwargs)\n+        self.inner_dense_1 = layers.Dense(10, name='NestedDense1')\n+        with tf.name_scope('inner1/inner2'):\n+          self.inner_dense_2 = layers.Dense(20, name='NestedDense2')\n+        self.inner_dense_3 = layers.Dense(30, name='NestedDense3')\n+\n+      def call(self, x):\n+        x = self.inner_dense_1(x)\n+        x = self.inner_dense_2(x)\n+        x = self.inner_dense_3(x)\n+        return x\n+\n+    inputs = input_layer.Input((3,))\n+    with tf.name_scope('outer'):\n+      x = ThreeDenses()(inputs)\n+    outputs = layers.Dense(10, name='OuterDense')(x)\n+\n+    model = training_lib.Model(inputs, outputs)\n+    node_names = self._get_model_node_names(model, np.random.random((1, 3)),\n+                                            'call_scope')\n+\n+    self.assertListEqual(node_names, [\n+        'call_scope/Const', 'call_scope/model/Cast',\n+        'call_scope/model/outer/ThreeDenses/NestedDense1/MatMul/ReadVariableOp/resource',\n+        'call_scope/model/outer/ThreeDenses/NestedDense1/MatMul/ReadVariableOp',\n+        'call_scope/model/outer/ThreeDenses/NestedDense1/MatMul',\n+        'call_scope/model/outer/ThreeDenses/NestedDense1/BiasAdd/ReadVariableOp/resource',\n+        'call_scope/model/outer/ThreeDenses/NestedDense1/BiasAdd/ReadVariableOp',\n+        'call_scope/model/outer/ThreeDenses/NestedDense1/BiasAdd',\n+        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/MatMul/ReadVariableOp/resource',\n+        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/MatMul/ReadVariableOp',\n+        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/MatMul',\n+        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/BiasAdd/ReadVariableOp/resource',\n+        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/BiasAdd/ReadVariableOp',\n+        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/BiasAdd',\n+        'call_scope/model/outer/ThreeDenses/NestedDense3/MatMul/ReadVariableOp/resource',\n+        'call_scope/model/outer/ThreeDenses/NestedDense3/MatMul/ReadVariableOp',\n+        'call_scope/model/outer/ThreeDenses/NestedDense3/MatMul',\n+        'call_scope/model/outer/ThreeDenses/NestedDense3/BiasAdd/ReadVariableOp/resource',\n+        'call_scope/model/outer/ThreeDenses/NestedDense3/BiasAdd/ReadVariableOp',\n+        'call_scope/model/outer/ThreeDenses/NestedDense3/BiasAdd',\n+        'call_scope/model/OuterDense/MatMul/ReadVariableOp/resource',\n+        'call_scope/model/OuterDense/MatMul/ReadVariableOp',\n+        'call_scope/model/OuterDense/MatMul',\n+        'call_scope/model/OuterDense/BiasAdd/ReadVariableOp/resource',\n+        'call_scope/model/OuterDense/BiasAdd/ReadVariableOp',\n+        'call_scope/model/OuterDense/BiasAdd', 'Identity', 'NoOp'\n+    ])\n+    base_layer._apply_name_scope_on_model_declaration(False)\n+\n   def _get_model_node_names(self, model, inputs, call_name_scope):\n     \"\"\"Returns a list of model's node names.\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca76d905659d31e002e04714a74f5bb4b156da72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1090 | Contributors (this commit): 26 | Commits (past 90d): 2 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -290,7 +290,7 @@ def model_to_dot(model,\n             [format_shape(ishape) for ishape in layer.input_shapes])\n       else:\n         inputlabels = '?'\n-      label = '{%s}|{input:|output:}|{{%s}}|{{%s}}' % (label, inputlabels,\n+      label = '{%s}|{input:|output:}|{{%s}|{%s}}' % (label, inputlabels,\n                                                        outputlabels)\n     if not expand_nested or not isinstance(\n         layer, functional.Functional):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#07de2fe6aa9b39d9e89ac96b408bfc52a761c7fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 24 | Files Changed: 2 | Hunks: 17 | Methods Changed: 7 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 43 | Churn Cumulative: 6274 | Contributors (this commit): 7 | Commits (past 90d): 17 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -26,6 +26,7 @@ from keras.utils import tf_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n+from tensorflow.tools.docs import doc_controls\n \n ResizeMethod = tf.image.ResizeMethod\n \n@@ -247,12 +248,12 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n \n   The `call()` method support two formats of inputs:\n   1. Single image tensor with 3D (HWC) or 4D (NHWC) format.\n-  2. A dict of tensors with stabel keys. The supported keys are:\n+  2. A dict of tensors with stable keys. The supported keys are:\n     `\"images\"`, `\"labels\"` and `\"bounding_boxes\"` at the moment. We might add\n     more keys in future when we support more types of augmentation.\n \n   The output of the `call()` will be in two formats, which will be the same\n-  structure as the inputs to the inputs.\n+  structure as the inputs.\n \n   The `call()` will handle the logic detecting the training/inference\n   mode, unpack the inputs, forward to the correct function, and pack the output\n@@ -282,6 +283,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     super().__init__(seed=seed, **kwargs)\n     self.rate = rate\n \n+  @doc_controls.for_subclass_implementers\n   def augment_image(self, image, transformation=None):\n     \"\"\"Augment a single image during training.\n \n@@ -296,6 +298,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     \"\"\"\n     raise NotImplementedError()\n \n+  @doc_controls.for_subclass_implementers\n   def augment_label(self, label, transformation=None):\n     \"\"\"Augment a single label during training.\n \n@@ -310,6 +313,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     \"\"\"\n     raise NotImplementedError()\n \n+  @doc_controls.for_subclass_implementers\n   def augment_bounding_box(self, bounding_box, transformation=None):\n     \"\"\"Augment bounding boxes for one image during training.\n \n@@ -324,6 +328,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     \"\"\"\n     raise NotImplementedError()\n \n+  @doc_controls.for_subclass_implementers\n   def get_random_tranformation(self):\n     \"\"\"Produce random transformation config.\n \n@@ -539,7 +544,7 @@ HORIZONTAL_AND_VERTICAL = 'horizontal_and_vertical'\n @keras_export('keras.layers.RandomFlip',\n               'keras.layers.experimental.preprocessing.RandomFlip',\n               v1=[])\n-class RandomFlip(base_layer.BaseRandomLayer):\n+class RandomFlip(BaseImageAugmentationLayer):\n   \"\"\"A preprocessing layer which randomly flips images during training.\n \n   This layer will flip the images horizontally and or vertically based on the\n@@ -589,11 +594,8 @@ class RandomFlip(base_layer.BaseRandomLayer):\n                        'argument {arg}'.format(name=self.name, arg=mode))\n     self.seed = seed\n \n-  def call(self, inputs, training=True):\n-    inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n-\n-    def random_flipped_inputs(inputs):\n-      flipped_outputs = inputs\n+  def augment_image(self, image, transformation=None):\n+    flipped_outputs = image\n     if self.horizontal:\n       seed = self._random_generator.make_seed_for_stateless_op()\n       if seed is not None:\n@@ -610,14 +612,9 @@ class RandomFlip(base_layer.BaseRandomLayer):\n       else:\n         flipped_outputs = tf.image.random_flip_up_down(\n             flipped_outputs, self._random_generator.make_legacy_seed())\n-      flipped_outputs.set_shape(inputs.shape)\n+    flipped_outputs.set_shape(image.shape)\n     return flipped_outputs\n \n-    if training:\n-      return random_flipped_inputs(inputs)\n-    else:\n-      return inputs\n-\n   def compute_output_shape(self, input_shape):\n     return input_shape\n \n\n@@ -496,8 +496,9 @@ class RandomFlipTest(test_combinations.TestCase):\n     orig_width = 8\n     channels = 3\n     if mock_random is None:\n-      mock_random = [1 for _ in range(num_samples)]\n-      mock_random = np.reshape(mock_random, [2, 1, 1, 1])\n+      mock_random = [0.0 for _ in range(num_samples)]\n+      if mode == 'horizontal_and_vertical':\n+        mock_random *= 2\n     inp = np.random.random((num_samples, orig_height, orig_width, channels))\n     if expected_output is None:\n       expected_output = inp\n@@ -508,7 +509,7 @@ class RandomFlipTest(test_combinations.TestCase):\n     with tf.compat.v1.test.mock.patch.object(\n         stateless_random_ops,\n         'stateless_random_uniform',\n-        return_value=mock_random,\n+        side_effect=mock_random,\n     ):\n       with test_utils.use_gpu():\n         layer = image_preprocessing.RandomFlip(mode)\n@@ -524,8 +525,7 @@ class RandomFlipTest(test_combinations.TestCase):\n \n   def test_random_flip_horizontal_half(self):\n     np.random.seed(1337)\n-    mock_random = [1, 0]\n-    mock_random = np.reshape(mock_random, [2, 1, 1, 1])\n+    mock_random = [0.0, 1.0]\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images.copy()\n     expected_output[0, :, :, :] = np.flip(input_images[0, :, :, :], axis=1)\n@@ -533,8 +533,7 @@ class RandomFlipTest(test_combinations.TestCase):\n \n   def test_random_flip_vertical_half(self):\n     np.random.seed(1337)\n-    mock_random = [1, 0]\n-    mock_random = np.reshape(mock_random, [2, 1, 1, 1])\n+    mock_random = [0.0, 1.0]\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images.copy()\n     expected_output[0, :, :, :] = np.flip(input_images[0, :, :, :], axis=0)\n@@ -551,12 +550,11 @@ class RandomFlipTest(test_combinations.TestCase):\n   def test_random_flip_default(self):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = np.flip(np.flip(input_images, axis=1), axis=2)\n-    mock_random = [1, 1]\n-    mock_random = np.reshape(mock_random, [2, 1, 1, 1])\n+    mock_random = [0.0, 0.0, 0.0, 0.0]\n     with tf.compat.v1.test.mock.patch.object(\n         stateless_random_ops,\n         'stateless_random_uniform',\n-        return_value=mock_random,\n+        side_effect=mock_random,\n     ):\n       with self.cached_session():\n         layer = image_preprocessing.RandomFlip()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c3463cb2404af170b3e27822ca25456bc233d42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1799 | Contributors (this commit): 9 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -845,13 +845,13 @@ def binary_matches(y_true, y_pred, threshold=0.5):\n   \"\"\"Creates int Tensor, 1 for label-prediction match, 0 for mismatch.\n \n   Args:\n-    y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n-    y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n+    y_true: Ground truth values, of shape (batch_size, d0, .. dN).\n+    y_pred: The predicted values, of shape (batch_size, d0, .. dN).\n     threshold: (Optional) Float representing the threshold for deciding whether\n       prediction values are 1 or 0.\n \n   Returns:\n-    Binary matches. shape = `[batch_size, d0, .. dN]`\n+    Binary matches, of shape = (batch_size, d0, .. dN).\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n   threshold = tf.cast(threshold, y_pred.dtype)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c46b3c7c2307f3fb0a19a78b666ba32439591feb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1154 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -3216,7 +3216,7 @@ def binary_accuracy(y_true, y_pred, threshold=0.5):\n     Binary accuracy values. shape = `[batch_size, d0, .. dN-1]`\n   \"\"\"\n \n-  return backend.mean(metrics_utils.binary_matches(y_true, y_pred, threshold), axis=-1)\n+  return tf.reduce_mean(metrics_utils.binary_matches(y_true, y_pred, threshold), axis=-1)\n \n \n @keras_export('keras.metrics.categorical_accuracy')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#74fcf5a22b841f26bdb8f1a188c33d78ac5ca1b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 21 | Files Changed: 1 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 41 | Churn Cumulative: 1195 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -3194,10 +3194,6 @@ def accuracy(y_true, y_pred):\n def binary_accuracy(y_true, y_pred, threshold=0.5):\n   \"\"\"Calculates how often predictions match binary labels.\n \n-  Note: calls metrics_utils.binary_matches with mean reduction. This maintains\n-  public facing binary_accuracy behavior and seperates it from the vital\n-  behavior of the binary_matches method needed in backend dependencies.\n-\n   Standalone usage:\n   >>> y_true = [[1], [1], [0], [0]]\n   >>> y_pred = [[1], [1], [0], [0]]\n@@ -3215,6 +3211,9 @@ def binary_accuracy(y_true, y_pred, threshold=0.5):\n   Returns:\n     Binary accuracy values. shape = `[batch_size, d0, .. dN-1]`\n   \"\"\"\n+  # Note: calls metrics_utils.binary_matches with mean reduction. This maintains\n+  # public facing binary_accuracy behavior and seperates it from the vital\n+  # behavior of the binary_matches method needed in backend dependencies.\n \n   return tf.reduce_mean(metrics_utils.binary_matches(y_true, y_pred, threshold), axis=-1)\n \n@@ -3224,10 +3223,6 @@ def binary_accuracy(y_true, y_pred, threshold=0.5):\n def categorical_accuracy(y_true, y_pred):\n   \"\"\"Calculates how often predictions match one-hot labels.\n \n-  Note: wraps metrics_utils.categorical_matches. This seperates public facing\n-  categorical_accuracy behavior from the vital behavior of the \n-  categorical_matches method needed in backend dependencies.\n-\n   Standalone usage:\n   >>> y_true = [[0, 0, 1], [0, 1, 0]]\n   >>> y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n@@ -3246,6 +3241,10 @@ def categorical_accuracy(y_true, y_pred):\n   Returns:\n     Categorical accuracy values.\n   \"\"\"\n+  # Note: wraps metrics_utils.categorical_matches. This seperates public facing\n+  # categorical_accuracy behavior from the vital behavior of the \n+  # categorical_matches method needed in backend dependencies.\n+\n   return metrics_utils.categorical_matches(y_true, y_pred)\n \n \n@@ -3254,10 +3253,6 @@ def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n   \"\"\"Calculates how often predictions match integer labels.\n \n-  Note: wraps metrics_utils.sparse_categorical_matches. This seperates\n-  public facing sparse_categorical_accuracy behavior from the vital behavior\n-  of the sparse_categorical_matches method needed in backend dependencies.\n-\n   Standalone usage:\n   >>> y_true = [2, 1]\n   >>> y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n@@ -3276,6 +3271,10 @@ def sparse_categorical_accuracy(y_true, y_pred):\n   Returns:\n     Sparse categorical accuracy values.\n   \"\"\"\n+  # Note: wraps metrics_utils.sparse_categorical_matches. This seperates\n+  # public facing sparse_categorical_accuracy behavior from the vital behavior\n+  # of the sparse_categorical_matches method needed in backend dependencies.\n+\n   return metrics_utils.sparse_categorical_matches(y_true, y_pred)\n \n \n@@ -3284,10 +3283,6 @@ def sparse_categorical_accuracy(y_true, y_pred):\n def top_k_categorical_accuracy(y_true, y_pred, k=5):\n   \"\"\"Computes how often targets are in the top `K` predictions.\n \n-  Note: wraps metrics_utils.top_k_categorical_matches. This seperates\n-  public facing top_k_categorical_accuracy behavior from the vital behavior\n-  of the top_k_categorical_matches method needed in backend dependencies.\n-\n   Standalone usage:\n   >>> y_true = [[0, 0, 1], [0, 1, 0]]\n   >>> y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n@@ -3305,6 +3300,10 @@ def top_k_categorical_accuracy(y_true, y_pred, k=5):\n   Returns:\n     Top K categorical accuracy value.\n   \"\"\"\n+  # Note: wraps metrics_utils.top_k_categorical_matches. This seperates\n+  # public facing top_k_categorical_accuracy behavior from the vital behavior\n+  # of the top_k_categorical_matches method needed in backend dependencies.\n+\n   return metrics_utils.top_k_categorical_matches(y_true, y_pred, k)\n \n \n@@ -3313,11 +3312,6 @@ def top_k_categorical_accuracy(y_true, y_pred, k=5):\n def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n   \"\"\"Computes how often integer targets are in the top `K` predictions.\n \n-  Note: wraps metrics_utils.sparse_top_k_categorical_matches. This seperates\n-  public facing sparse_top_k_categorical_accuracy behavior from the vital\n-  behavior of the sparse_top_k_categorical_matches method needed in backend\n-  dependencies.\n-\n   Standalone usage:\n   >>> y_true = [2, 1]\n   >>> y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n@@ -3336,6 +3330,11 @@ def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n   Returns:\n     Sparse top K categorical accuracy value.\n   \"\"\"\n+  # Note: wraps metrics_utils.sparse_top_k_categorical_matches. This seperates\n+  # public facing sparse_top_k_categorical_accuracy behavior from the vital\n+  # behavior of the sparse_top_k_categorical_matches method needed in backend\n+  # dependencies.\n+  \n   return metrics_utils.sparse_top_k_categorical_matches(y_true, y_pred, k)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#be6bdda0848109f8f81a3d3bf8e79f01e68008fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1801 | Contributors (this commit): 9 | Commits (past 90d): 7 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -851,7 +851,7 @@ def binary_matches(y_true, y_pred, threshold=0.5):\n       prediction values are 1 or 0.\n \n   Returns:\n-    Binary matches, of shape = (batch_size, d0, .. dN).\n+    Binary matches, of shape (batch_size, d0, .. dN).\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n   threshold = tf.cast(threshold, y_pred.dtype)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
