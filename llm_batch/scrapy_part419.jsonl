{"custom_id": "scrapy#ba10dcfd1a44fae0d48cba6bf6b49f2bd05c35a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 13 | Files Changed: 3 | Hunks: 12 | Methods Changed: 6 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 37 | Churn Cumulative: 3984 | Contributors (this commit): 48 | Commits (past 90d): 7 | Contributors (cumulative): 66 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,7 @@ import re\n from contextlib import suppress\n from io import BytesIO\n from time import time\n-from typing import TYPE_CHECKING, Any, TypedDict, TypeVar\n+from typing import TYPE_CHECKING, Any, TypedDict, TypeVar, cast\n from urllib.parse import urldefrag, urlparse\n \n from twisted.internet import ssl\n@@ -27,7 +27,7 @@ from twisted.web.client import (\n from twisted.web.client import Response as TxResponse\n from twisted.web.http import PotentialDataLoss, _DataLoss\n from twisted.web.http_headers import Headers as TxHeaders\n-from twisted.web.iweb import UNKNOWN_LENGTH, IBodyProducer, IPolicyForHTTPS\n+from twisted.web.iweb import UNKNOWN_LENGTH, IBodyProducer, IPolicyForHTTPS, IResponse\n from zope.interface import implementer\n \n from scrapy import Request, Spider, signals\n@@ -286,11 +286,11 @@ class TunnelingAgent(Agent):\n         key: Any,\n         endpoint: TCP4ClientEndpoint,\n         method: bytes,\n-        parsedURI: bytes,\n+        parsedURI: URI,\n         headers: TxHeaders | None,\n         bodyProducer: IBodyProducer | None,\n         requestPath: bytes,\n-    ) -> Deferred[TxResponse]:\n+    ) -> Deferred[IResponse]:\n         # proxy host and port are required for HTTP pool `key`\n         # otherwise, same remote host connection request could reuse\n         # a cached tunneled connection to a different proxy\n@@ -329,14 +329,14 @@ class ScrapyProxyAgent(Agent):\n         uri: bytes,\n         headers: TxHeaders | None = None,\n         bodyProducer: IBodyProducer | None = None,\n-    ) -> Deferred[TxResponse]:\n+    ) -> Deferred[IResponse]:\n         \"\"\"\n         Issue a new request via the configured proxy.\n         \"\"\"\n         # Cache *all* connections under the same key, since we are only\n         # connecting to a single destination, the proxy:\n         return self._requestWithEndpoint(\n-            key=(\"http-proxy\", self._proxyURI.host, self._proxyURI.port),\n+            key=(b\"http-proxy\", self._proxyURI.host, self._proxyURI.port),\n             endpoint=self._getEndpoint(self._proxyURI),\n             method=method,\n             parsedURI=URI.fromBytes(uri),\n@@ -426,8 +426,11 @@ class ScrapyAgent:\n             headers.removeHeader(b\"Proxy-Authorization\")\n         bodyproducer = _RequestBodyProducer(request.body) if request.body else None\n         start_time = time()\n-        d: Deferred[TxResponse] = agent.request(\n-            method, to_bytes(url, encoding=\"ascii\"), headers, bodyproducer\n+        d: Deferred[IResponse] = agent.request(\n+            method,\n+            to_bytes(url, encoding=\"ascii\"),\n+            headers,\n+            cast(IBodyProducer, bodyproducer),\n         )\n         # set download latency\n         d.addCallback(self._cb_latency, request, start_time)\n\n@@ -10,7 +10,7 @@ import warnings\n from importlib import import_module\n from pathlib import Path\n from posixpath import split\n-from typing import TYPE_CHECKING, Any, TypeVar\n+from typing import TYPE_CHECKING, Any, TypeVar, cast\n from unittest import TestCase, mock\n \n from twisted.trial.unittest import SkipTest\n@@ -211,4 +211,4 @@ def get_web_client_agent_req(url: str) -> Deferred[TxResponse]:\n     from twisted.web.client import Agent  # imports twisted.internet.reactor\n \n     agent = Agent(reactor)\n-    return agent.request(b\"GET\", url.encode(\"utf-8\"))\n+    return cast(\"Deferred[TxResponse]\", agent.request(b\"GET\", url.encode(\"utf-8\")))\n\n@@ -4,7 +4,7 @@ import shutil\n import warnings\n from pathlib import Path\n from tempfile import mkdtemp\n-from typing import Any\n+from typing import Any, cast\n \n import OpenSSL.SSL\n import pytest\n@@ -14,6 +14,7 @@ from twisted.trial import unittest\n from twisted.web import server, static\n from twisted.web.client import Agent, BrowserLikePolicyForHTTPS, readBody\n from twisted.web.client import Response as TxResponse\n+from twisted.web.iweb import IBodyProducer\n \n from scrapy.core.downloader import Slot\n from scrapy.core.downloader.contextfactory import (\n@@ -76,8 +77,15 @@ class TestContextFactoryBase(unittest.TestCase):\n \n         agent = Agent(reactor, contextFactory=client_context_factory)\n         body_producer = _RequestBodyProducer(body.encode()) if body else None\n-        response: TxResponse = await maybe_deferred_to_future(\n-            agent.request(b\"GET\", url.encode(), bodyProducer=body_producer)\n+        response: TxResponse = cast(\n+            TxResponse,\n+            await maybe_deferred_to_future(\n+                agent.request(\n+                    b\"GET\",\n+                    url.encode(),\n+                    bodyProducer=cast(IBodyProducer, body_producer),\n+                )\n+            ),\n         )\n         with warnings.catch_warnings():\n             # https://github.com/twisted/twisted/issues/8227\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#24a827c72e9f5b35ddcd12ccce2ce7c6611d2845", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 13 | Files Changed: 3 | Hunks: 12 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 37 | Churn Cumulative: 4021 | Contributors (this commit): 48 | Commits (past 90d): 10 | Contributors (cumulative): 66 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,7 @@ import re\n from contextlib import suppress\n from io import BytesIO\n from time import time\n-from typing import TYPE_CHECKING, Any, TypedDict, TypeVar\n+from typing import TYPE_CHECKING, Any, TypedDict, TypeVar, cast\n from urllib.parse import urldefrag, urlparse\n \n from twisted.internet import ssl\n@@ -27,7 +27,7 @@ from twisted.web.client import (\n from twisted.web.client import Response as TxResponse\n from twisted.web.http import PotentialDataLoss, _DataLoss\n from twisted.web.http_headers import Headers as TxHeaders\n-from twisted.web.iweb import UNKNOWN_LENGTH, IBodyProducer, IPolicyForHTTPS\n+from twisted.web.iweb import UNKNOWN_LENGTH, IBodyProducer, IPolicyForHTTPS, IResponse\n from zope.interface import implementer\n \n from scrapy import Request, Spider, signals\n@@ -286,11 +286,11 @@ class TunnelingAgent(Agent):\n         key: Any,\n         endpoint: TCP4ClientEndpoint,\n         method: bytes,\n-        parsedURI: bytes,\n+        parsedURI: URI,\n         headers: TxHeaders | None,\n         bodyProducer: IBodyProducer | None,\n         requestPath: bytes,\n-    ) -> Deferred[TxResponse]:\n+    ) -> Deferred[IResponse]:\n         # proxy host and port are required for HTTP pool `key`\n         # otherwise, same remote host connection request could reuse\n         # a cached tunneled connection to a different proxy\n@@ -329,14 +329,14 @@ class ScrapyProxyAgent(Agent):\n         uri: bytes,\n         headers: TxHeaders | None = None,\n         bodyProducer: IBodyProducer | None = None,\n-    ) -> Deferred[TxResponse]:\n+    ) -> Deferred[IResponse]:\n         \"\"\"\n         Issue a new request via the configured proxy.\n         \"\"\"\n         # Cache *all* connections under the same key, since we are only\n         # connecting to a single destination, the proxy:\n         return self._requestWithEndpoint(\n-            key=(\"http-proxy\", self._proxyURI.host, self._proxyURI.port),\n+            key=(b\"http-proxy\", self._proxyURI.host, self._proxyURI.port),\n             endpoint=self._getEndpoint(self._proxyURI),\n             method=method,\n             parsedURI=URI.fromBytes(uri),\n@@ -426,8 +426,11 @@ class ScrapyAgent:\n             headers.removeHeader(b\"Proxy-Authorization\")\n         bodyproducer = _RequestBodyProducer(request.body) if request.body else None\n         start_time = time()\n-        d: Deferred[TxResponse] = agent.request(\n-            method, to_bytes(url, encoding=\"ascii\"), headers, bodyproducer\n+        d: Deferred[IResponse] = agent.request(\n+            method,\n+            to_bytes(url, encoding=\"ascii\"),\n+            headers,\n+            cast(IBodyProducer, bodyproducer),\n         )\n         # set download latency\n         d.addCallback(self._cb_latency, request, start_time)\n\n@@ -10,7 +10,7 @@ import warnings\n from importlib import import_module\n from pathlib import Path\n from posixpath import split\n-from typing import TYPE_CHECKING, Any, TypeVar\n+from typing import TYPE_CHECKING, Any, TypeVar, cast\n from unittest import TestCase, mock\n \n from twisted.trial.unittest import SkipTest\n@@ -216,4 +216,4 @@ def get_web_client_agent_req(url: str) -> Deferred[TxResponse]:\n     from twisted.web.client import Agent  # imports twisted.internet.reactor\n \n     agent = Agent(reactor)\n-    return agent.request(b\"GET\", url.encode(\"utf-8\"))\n+    return cast(\"Deferred[TxResponse]\", agent.request(b\"GET\", url.encode(\"utf-8\")))\n\n@@ -4,7 +4,7 @@ import shutil\n import warnings\n from pathlib import Path\n from tempfile import mkdtemp\n-from typing import Any\n+from typing import Any, cast\n \n import OpenSSL.SSL\n import pytest\n@@ -15,6 +15,7 @@ from twisted.trial import unittest\n from twisted.web import server, static\n from twisted.web.client import Agent, BrowserLikePolicyForHTTPS, readBody\n from twisted.web.client import Response as TxResponse\n+from twisted.web.iweb import IBodyProducer\n \n from scrapy.core.downloader import Slot\n from scrapy.core.downloader.contextfactory import (\n@@ -73,8 +74,15 @@ class TestContextFactoryBase(unittest.TestCase):\n     ) -> bytes:\n         agent = Agent(reactor, contextFactory=client_context_factory)\n         body_producer = _RequestBodyProducer(body.encode()) if body else None\n-        response: TxResponse = await maybe_deferred_to_future(\n-            agent.request(b\"GET\", url.encode(), bodyProducer=body_producer)\n+        response: TxResponse = cast(\n+            TxResponse,\n+            await maybe_deferred_to_future(\n+                agent.request(\n+                    b\"GET\",\n+                    url.encode(),\n+                    bodyProducer=cast(IBodyProducer, body_producer),\n+                )\n+            ),\n         )\n         with warnings.catch_warnings():\n             # https://github.com/twisted/twisted/issues/8227\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6b5a4a64173fc051063f01c05925519e45dbbfdd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 961 | Contributors (this commit): 28 | Commits (past 90d): 3 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -32,8 +32,9 @@ class ScrapyArgumentParser(argparse.ArgumentParser):\n     def _parse_optional(\n         self, arg_string: str\n     ) -> tuple[argparse.Action | None, str, str | None] | None:\n-        # if starts with -: it means that is a parameter not a argument\n-        if arg_string[:2] == \"-:\":\n+        # Support something like ‘-o -:json’, where ‘-:json’ is a value for\n+        # ‘-o’, not another parameter.\n+        if arg_string.startswith(\"-:\"):\n             return None\n \n         return super()._parse_optional(arg_string)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b4d11b8b2565b5686c9f395ca1a8dd085609aafc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 279 | Lines Deleted: 312 | Files Changed: 20 | Hunks: 181 | Methods Changed: 106 | Complexity Δ (Sum/Max): -13/1 | Churn Δ: 591 | Churn Cumulative: 26233 | Contributors (this commit): 90 | Commits (past 90d): 58 | Contributors (cumulative): 271 | DMM Complexity: 0.0\n\nDIFF:\n@@ -39,23 +39,21 @@ class TestSimpleHttps(HTTP11DownloadHandlerMixin, TestSimpleHttpsBase):\n     pass\n \n \n-class Https11WrongHostnameTestCase(\n-    HTTP11DownloadHandlerMixin, TestHttpsWrongHostnameBase\n-):\n+class TestHttps11WrongHostname(HTTP11DownloadHandlerMixin, TestHttpsWrongHostnameBase):\n     pass\n \n \n-class Https11InvalidDNSId(HTTP11DownloadHandlerMixin, TestHttpsInvalidDNSIdBase):\n+class TestHttps11InvalidDNSId(HTTP11DownloadHandlerMixin, TestHttpsInvalidDNSIdBase):\n     pass\n \n \n-class Https11InvalidDNSPattern(\n+class TestHttps11InvalidDNSPattern(\n     HTTP11DownloadHandlerMixin, TestHttpsInvalidDNSPatternBase\n ):\n     pass\n \n \n-class Https11CustomCiphers(HTTP11DownloadHandlerMixin, TestHttpsCustomCiphersBase):\n+class TestHttps11CustomCiphers(HTTP11DownloadHandlerMixin, TestHttpsCustomCiphersBase):\n     pass\n \n \n\n@@ -163,23 +163,25 @@ class TestHttps2(H2DownloadHandlerMixin, TestHttps11Base):\n         assert json.loads(response.text)[\"headers\"][header] == [value1, value2]\n \n \n-class Https2WrongHostnameTestCase(H2DownloadHandlerMixin, TestHttpsWrongHostnameBase):\n+class TestHttps2WrongHostname(H2DownloadHandlerMixin, TestHttpsWrongHostnameBase):\n     pass\n \n \n-class Https2InvalidDNSId(H2DownloadHandlerMixin, TestHttpsInvalidDNSIdBase):\n+class TestHttps2InvalidDNSId(H2DownloadHandlerMixin, TestHttpsInvalidDNSIdBase):\n     pass\n \n \n-class Https2InvalidDNSPattern(H2DownloadHandlerMixin, TestHttpsInvalidDNSPatternBase):\n+class TestHttps2InvalidDNSPattern(\n+    H2DownloadHandlerMixin, TestHttpsInvalidDNSPatternBase\n+):\n     pass\n \n \n-class Https2CustomCiphers(H2DownloadHandlerMixin, TestHttpsCustomCiphersBase):\n+class TestHttps2CustomCiphers(H2DownloadHandlerMixin, TestHttpsCustomCiphersBase):\n     pass\n \n \n-class Http2MockServerTestCase(TestHttpMockServerBase):\n+class TestHttp2MockServer(TestHttpMockServerBase):\n     \"\"\"HTTP 2.0 test case with MockServer\"\"\"\n \n     @property\n@@ -193,7 +195,7 @@ class Http2MockServerTestCase(TestHttpMockServerBase):\n     is_secure = True\n \n \n-class Https2ProxyTestCase(H2DownloadHandlerMixin, TestHttpProxyBase):\n+class TestHttps2Proxy(H2DownloadHandlerMixin, TestHttpProxyBase):\n     # only used for HTTPS tests\n     keyfile = \"keys/localhost.key\"\n     certfile = \"keys/localhost.crt\"\n\n@@ -12,6 +12,7 @@ from unittest import mock\n \n import pytest\n from twisted.cred import checkers, credentials, portal\n+from twisted.internet.defer import inlineCallbacks\n from twisted.protocols.ftp import FTPFactory, FTPRealm\n from twisted.trial import unittest\n from w3lib.url import path_to_file_uri\n@@ -340,9 +341,10 @@ class TestFTPBase(unittest.TestCase):\n         self.portNum = self.port.getHost().port\n         crawler = get_crawler()\n         self.download_handler = build_from_crawler(FTPDownloadHandler, crawler)\n-        self.addCleanup(self.port.stopListening)\n \n+    @inlineCallbacks\n     def tearDown(self):\n+        yield self.port.stopListening()\n         shutil.rmtree(self.directory)\n \n     def _add_test_callbacks(self, deferred, callback=None, errback=None):\n@@ -478,9 +480,10 @@ class TestAnonymousFTP(TestFTPBase):\n         self.portNum = self.port.getHost().port\n         crawler = get_crawler()\n         self.download_handler = build_from_crawler(FTPDownloadHandler, crawler)\n-        self.addCleanup(self.port.stopListening)\n \n+    @inlineCallbacks\n     def tearDown(self):\n+        yield self.port.stopListening()\n         shutil.rmtree(self.directory)\n \n \n\n@@ -247,10 +247,8 @@ Disallow: /some/randome/page.html\n         assert request.callback == NO_CALLBACK\n \n \n+@pytest.mark.skipif(not rerp_available(), reason=\"Rerp parser is not installed\")\n class TestRobotsTxtMiddlewareWithRerp(TestRobotsTxtMiddleware):\n-    if not rerp_available():\n-        skip = \"Rerp parser is not installed\"\n-\n     def setUp(self):\n         super().setUp()\n         self.crawler.settings.set(\n\n@@ -49,7 +49,7 @@ class DownloaderSlotsSettingsTestSpider(MetaSpider):\n         self.times[slot].append(time.time())\n \n \n-class CrawlTestCase(TestCase):\n+class TestCrawl(TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.mockserver = MockServer()\n\n@@ -27,7 +27,7 @@ async def sleep(seconds: float = 0.001) -> None:\n     await maybe_deferred_to_future(deferred)\n \n \n-class MainTestCase(TestCase):\n+class TestMain(TestCase):\n     @deferred_f_from_coro_f\n     async def test_sleep(self):\n         \"\"\"Neither asynchronous sleeps on Spider.start() nor the equivalent on\n@@ -119,7 +119,7 @@ class MainTestCase(TestCase):\n         assert actual_urls == expected_urls, f\"{actual_urls=} != {expected_urls=}\"\n \n \n-class RequestSendOrderTestCase(TestCase):\n+class TestRequestSendOrder(TestCase):\n     seconds = 0.1  # increase if flaky\n \n     @classmethod\n\n@@ -8,7 +8,7 @@ from scrapy.extensions.telnet import TelnetConsole\n from scrapy.utils.test import get_crawler\n \n \n-class TelnetExtensionTest(unittest.TestCase):\n+class TestTelnetExtension(unittest.TestCase):\n     def _get_console_and_portal(self, settings=None):\n         crawler = get_crawler(settings_dict=settings)\n         console = TelnetConsole(crawler)\n\n@@ -91,52 +91,53 @@ def mock_google_cloud_storage() -> tuple[Any, Any, Any]:\n     return (client_mock, bucket_mock, blob_mock)\n \n \n-# TODO: replace self.mktemp() and drop the unittest.TestCase base\n-class TestFileFeedStorage(unittest.TestCase):\n-    def test_store_file_uri(self):\n-        path = Path(self.mktemp()).resolve()\n+class TestFileFeedStorage:\n+    def test_store_file_uri(self, tmp_path):\n+        path = tmp_path / \"file.txt\"\n         uri = path_to_file_uri(str(path))\n         self._assert_stores(FileFeedStorage(uri), path)\n \n-    def test_store_file_uri_makedirs(self):\n-        path = Path(self.mktemp()).resolve() / \"more\" / \"paths\" / \"file.txt\"\n+    def test_store_file_uri_makedirs(self, tmp_path):\n+        path = tmp_path / \"more\" / \"paths\" / \"file.txt\"\n         uri = path_to_file_uri(str(path))\n         self._assert_stores(FileFeedStorage(uri), path)\n \n-    def test_store_direct_path(self):\n-        path = Path(self.mktemp()).resolve()\n+    def test_store_direct_path(self, tmp_path):\n+        path = tmp_path / \"file.txt\"\n         self._assert_stores(FileFeedStorage(str(path)), path)\n \n-    def test_store_direct_path_relative(self):\n-        path = Path(self.mktemp())\n+    def test_store_direct_path_relative(self, tmp_path):\n+        path = (tmp_path / \"foo\" / \"bar\").relative_to(Path.cwd())\n         self._assert_stores(FileFeedStorage(str(path)), path)\n \n-    def test_interface(self):\n-        path = self.mktemp()\n-        st = FileFeedStorage(path)\n+    def test_interface(self, tmp_path):\n+        path = tmp_path / \"file.txt\"\n+        st = FileFeedStorage(str(path))\n         verifyObject(IFeedStorage, st)\n \n-    def _store(self, feed_options=None) -> Path:\n-        path = Path(self.mktemp()).resolve()\n+    @staticmethod\n+    def _store(path: Path, feed_options: dict[str, Any] | None = None) -> None:\n         storage = FileFeedStorage(str(path), feed_options=feed_options)\n         spider = scrapy.Spider(\"default\")\n         file = storage.open(spider)\n         file.write(b\"content\")\n         storage.store(file)\n-        return path\n \n-    def test_append(self):\n-        path = self._store()\n+    def test_append(self, tmp_path):\n+        path = tmp_path / \"file.txt\"\n+        self._store(path)\n         self._assert_stores(FileFeedStorage(str(path)), path, b\"contentcontent\")\n \n-    def test_overwrite(self):\n-        path = self._store({\"overwrite\": True})\n+    def test_overwrite(self, tmp_path):\n+        path = tmp_path / \"file.txt\"\n+        self._store(path, {\"overwrite\": True})\n         self._assert_stores(\n             FileFeedStorage(str(path), feed_options={\"overwrite\": True}), path\n         )\n \n+    @staticmethod\n     def _assert_stores(\n-        self, storage: FileFeedStorage, path: Path, expected_content: bytes = b\"content\"\n+        storage: FileFeedStorage, path: Path, expected_content: bytes = b\"content\"\n     ) -> None:\n         spider = scrapy.Spider(\"default\")\n         file = storage.open(spider)\n\n@@ -8,7 +8,7 @@ import string\n from ipaddress import IPv4Address\n from pathlib import Path\n from tempfile import mkdtemp\n-from typing import TYPE_CHECKING\n+from typing import TYPE_CHECKING, Any, Callable\n from unittest import mock\n from urllib.parse import urlencode\n \n@@ -32,17 +32,22 @@ from twisted.web.static import File\n from scrapy.http import JsonRequest, Request, Response\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n+from scrapy.utils.defer import (\n+    deferred_f_from_coro_f,\n+    deferred_from_coro,\n+    maybe_deferred_to_future,\n+)\n from tests.mockserver import LeafResource, Status, ssl_context_factory\n \n if TYPE_CHECKING:\n-    from twisted.python.failure import Failure\n+    from collections.abc import Coroutine\n \n \n-def generate_random_string(size):\n+def generate_random_string(size: int) -> str:\n     return \"\".join(random.choices(string.ascii_uppercase + string.digits, k=size))\n \n \n-def make_html_body(val):\n+def make_html_body(val: str) -> bytes:\n     response = f\"\"\"<html>\n <h1>Hello from HTTP2<h1>\n <p>{val}</p>\n@@ -92,7 +97,7 @@ class GetDataHtmlLarge(LeafResource):\n \n class PostDataJsonMixin:\n     @staticmethod\n-    def make_response(request: TxRequest, extra_data: str):\n+    def make_response(request: TxRequest, extra_data: str) -> bytes:\n         assert request.content is not None\n         response = {\n             \"request-headers\": {},\n@@ -179,7 +184,6 @@ def get_client_certificate(\n     pem = key_file.read_text(encoding=\"utf-8\") + certificate_file.read_text(\n         encoding=\"utf-8\"\n     )\n-\n     return PrivateCertificate.loadPEM(pem)\n \n \n@@ -238,6 +242,7 @@ class TestHttps2ClientProtocol(TestCase):\n         uri = URI.fromBytes(bytes(self.get_url(\"/\"), \"utf-8\"))\n \n         self.conn_closed_deferred = Deferred()\n+\n         from scrapy.core.http2.protocol import H2ClientFactory\n \n         h2_client_factory = H2ClientFactory(uri, Settings(), self.conn_closed_deferred)\n@@ -255,7 +260,7 @@ class TestHttps2ClientProtocol(TestCase):\n         shutil.rmtree(self.temp_directory)\n         self.conn_closed_deferred = None\n \n-    def get_url(self, path):\n+    def get_url(self, path: str) -> str:\n         \"\"\"\n         :param path: Should have / at the starting compulsorily if not empty\n         :return: Complete url\n@@ -264,20 +269,27 @@ class TestHttps2ClientProtocol(TestCase):\n         assert path[0] == \"/\" or path[0] == \"&\"\n         return f\"{self.scheme}://{self.hostname}:{self.port_number}{path}\"\n \n-    def make_request(self, request: Request) -> Deferred:\n+    async def make_request(self, request: Request) -> Response:\n+        return await maybe_deferred_to_future(self.make_request_dfd(request))\n+\n+    def make_request_dfd(self, request: Request) -> Deferred[Response]:\n         return self.client.request(request, DummySpider())\n \n     @staticmethod\n-    def _check_repeat(get_deferred, count):\n+    async def _check_repeat(\n+        get_coro: Callable[[], Coroutine[Any, Any, None]], count: int\n+    ) -> None:\n         d_list = []\n         for _ in range(count):\n-            d = get_deferred()\n+            d = deferred_from_coro(get_coro())\n             d_list.append(d)\n \n-        return DeferredList(d_list, fireOnOneErrback=True)\n+        await maybe_deferred_to_future(DeferredList(d_list, fireOnOneErrback=True))\n \n-    def _check_GET(self, request: Request, expected_body, expected_status):\n-        def check_response(response: Response):\n+    async def _check_GET(\n+        self, request: Request, expected_body: bytes, expected_status: int\n+    ) -> None:\n+        response = await self.make_request(request)\n         assert response.status == expected_status\n         assert response.body == expected_body\n         assert response.request == request\n@@ -287,45 +299,45 @@ class TestHttps2ClientProtocol(TestCase):\n         content_length = int(content_length_header)\n         assert len(response.body) == content_length\n \n-        d = self.make_request(request)\n-        d.addCallback(check_response)\n-        d.addErrback(self.fail)\n-        return d\n-\n-    def test_GET_small_body(self):\n+    @deferred_f_from_coro_f\n+    async def test_GET_small_body(self):\n         request = Request(self.get_url(\"/get-data-html-small\"))\n-        return self._check_GET(request, Data.HTML_SMALL, 200)\n+        await self._check_GET(request, Data.HTML_SMALL, 200)\n \n-    def test_GET_large_body(self):\n+    @deferred_f_from_coro_f\n+    async def test_GET_large_body(self):\n         request = Request(self.get_url(\"/get-data-html-large\"))\n-        return self._check_GET(request, Data.HTML_LARGE, 200)\n+        await self._check_GET(request, Data.HTML_LARGE, 200)\n \n-    def _check_GET_x10(self, *args, **kwargs):\n-        def get_deferred():\n-            return self._check_GET(*args, **kwargs)\n+    async def _check_GET_x10(\n+        self, request: Request, expected_body: bytes, expected_status: int\n+    ) -> None:\n+        async def get_coro() -> None:\n+            await self._check_GET(request, expected_body, expected_status)\n \n-        return self._check_repeat(get_deferred, 10)\n+        await self._check_repeat(get_coro, 10)\n \n-    def test_GET_small_body_x10(self):\n-        return self._check_GET_x10(\n+    @deferred_f_from_coro_f\n+    async def test_GET_small_body_x10(self):\n+        await self._check_GET_x10(\n             Request(self.get_url(\"/get-data-html-small\")), Data.HTML_SMALL, 200\n         )\n \n-    def test_GET_large_body_x10(self):\n-        return self._check_GET_x10(\n+    @deferred_f_from_coro_f\n+    async def test_GET_large_body_x10(self):\n+        await self._check_GET_x10(\n             Request(self.get_url(\"/get-data-html-large\")), Data.HTML_LARGE, 200\n         )\n \n-    def _check_POST_json(\n+    async def _check_POST_json(\n         self,\n         request: Request,\n-        expected_request_body,\n-        expected_extra_data,\n+        expected_request_body: dict[str, str],\n+        expected_extra_data: str,\n         expected_status: int,\n-    ):\n-        d = self.make_request(request)\n+    ) -> None:\n+        response = await self.make_request(request)\n \n-        def assert_response(response: Response):\n         assert response.status == expected_status\n         assert response.request == request\n \n@@ -356,51 +368,47 @@ class TestHttps2ClientProtocol(TestCase):\n             assert k_str in request_headers\n             assert request_headers[k_str] == str(v[0], \"utf-8\")\n \n-        d.addCallback(assert_response)\n-        d.addErrback(self.fail)\n-        return d\n-\n-    def test_POST_small_json(self):\n+    @deferred_f_from_coro_f\n+    async def test_POST_small_json(self):\n         request = JsonRequest(\n             url=self.get_url(\"/post-data-json-small\"),\n             method=\"POST\",\n             data=Data.JSON_SMALL,\n         )\n-        return self._check_POST_json(request, Data.JSON_SMALL, Data.EXTRA_SMALL, 200)\n+        await self._check_POST_json(request, Data.JSON_SMALL, Data.EXTRA_SMALL, 200)\n \n-    def test_POST_large_json(self):\n+    @deferred_f_from_coro_f\n+    async def test_POST_large_json(self):\n         request = JsonRequest(\n             url=self.get_url(\"/post-data-json-large\"),\n             method=\"POST\",\n             data=Data.JSON_LARGE,\n         )\n-        return self._check_POST_json(request, Data.JSON_LARGE, Data.EXTRA_LARGE, 200)\n+        await self._check_POST_json(request, Data.JSON_LARGE, Data.EXTRA_LARGE, 200)\n \n-    def _check_POST_json_x10(self, *args, **kwargs):\n-        def get_deferred():\n-            return self._check_POST_json(*args, **kwargs)\n+    async def _check_POST_json_x10(self, *args, **kwargs):\n+        async def get_coro() -> None:\n+            await self._check_POST_json(*args, **kwargs)\n \n-        return self._check_repeat(get_deferred, 10)\n+        await self._check_repeat(get_coro, 10)\n \n-    def test_POST_small_json_x10(self):\n+    @deferred_f_from_coro_f\n+    async def test_POST_small_json_x10(self):\n         request = JsonRequest(\n             url=self.get_url(\"/post-data-json-small\"),\n             method=\"POST\",\n             data=Data.JSON_SMALL,\n         )\n-        return self._check_POST_json_x10(\n-            request, Data.JSON_SMALL, Data.EXTRA_SMALL, 200\n-        )\n+        await self._check_POST_json_x10(request, Data.JSON_SMALL, Data.EXTRA_SMALL, 200)\n \n-    def test_POST_large_json_x10(self):\n+    @deferred_f_from_coro_f\n+    async def test_POST_large_json_x10(self):\n         request = JsonRequest(\n             url=self.get_url(\"/post-data-json-large\"),\n             method=\"POST\",\n             data=Data.JSON_LARGE,\n         )\n-        return self._check_POST_json_x10(\n-            request, Data.JSON_LARGE, Data.EXTRA_LARGE, 200\n-        )\n+        await self._check_POST_json_x10(request, Data.JSON_LARGE, Data.EXTRA_LARGE, 200)\n \n     @inlineCallbacks\n     def test_invalid_negotiated_protocol(self):\n@@ -409,77 +417,59 @@ class TestHttps2ClientProtocol(TestCase):\n         ):\n             request = Request(url=self.get_url(\"/status?n=200\"))\n             with pytest.raises(ResponseFailed):\n-                yield self.make_request(request)\n+                yield self.make_request_dfd(request)\n \n+    @inlineCallbacks\n     def test_cancel_request(self):\n         request = Request(url=self.get_url(\"/get-data-html-large\"))\n-\n-        def assert_response(response: Response):\n+        d = self.make_request_dfd(request)\n+        d.cancel()\n+        response = yield d\n         assert response.status == 499\n         assert response.request == request\n \n-        d = self.make_request(request)\n-        d.addCallback(assert_response)\n-        d.addErrback(self.fail)\n-        d.cancel()\n-\n-        return d\n-\n-    def test_download_maxsize_exceeded(self):\n+    @deferred_f_from_coro_f\n+    async def test_download_maxsize_exceeded(self):\n         request = Request(\n             url=self.get_url(\"/get-data-html-large\"), meta={\"download_maxsize\": 1000}\n         )\n-\n-        def assert_cancelled_error(failure):\n-            assert isinstance(failure.value, CancelledError)\n+        with pytest.raises(CancelledError) as exc_info:\n+            await self.make_request(request)\n         error_pattern = re.compile(\n             rf\"Cancelling download of {request.url}: received response \"\n             rf\"size \\(\\d*\\) larger than download max size \\(1000\\)\"\n         )\n-            assert len(re.findall(error_pattern, str(failure.value))) == 1\n-\n-        d = self.make_request(request)\n-        d.addCallback(self.fail)\n-        d.addErrback(assert_cancelled_error)\n-        return d\n+        assert len(re.findall(error_pattern, str(exc_info.value))) == 1\n \n+    @inlineCallbacks\n     def test_received_dataloss_response(self):\n         \"\"\"In case when value of Header Content-Length != len(Received Data)\n         ProtocolError is raised\"\"\"\n-        request = Request(url=self.get_url(\"/dataloss\"))\n-\n-        def assert_failure(failure: Failure):\n-            assert len(failure.value.reasons) > 0\n         from h2.exceptions import InvalidBodyLengthError\n \n+        request = Request(url=self.get_url(\"/dataloss\"))\n+        with pytest.raises(ResponseFailed) as exc_info:\n+            yield self.make_request_dfd(request)\n+        assert len(exc_info.value.reasons) > 0\n         assert any(\n             isinstance(error, InvalidBodyLengthError)\n-                for error in failure.value.reasons\n+            for error in exc_info.value.reasons\n         )\n \n-        d = self.make_request(request)\n-        d.addCallback(self.fail)\n-        d.addErrback(assert_failure)\n-        return d\n-\n-    def test_missing_content_length_header(self):\n+    @deferred_f_from_coro_f\n+    async def test_missing_content_length_header(self):\n         request = Request(url=self.get_url(\"/no-content-length-header\"))\n-\n-        def assert_content_length(response: Response):\n+        response = await self.make_request(request)\n         assert response.status == 200\n         assert response.body == Data.NO_CONTENT_LENGTH\n         assert response.request == request\n         assert \"Content-Length\" not in response.headers\n \n-        d = self.make_request(request)\n-        d.addCallback(assert_content_length)\n-        d.addErrback(self.fail)\n-        return d\n-\n-    @inlineCallbacks\n-    def _check_log_warnsize(self, request, warn_pattern, expected_body):\n+    async def _check_log_warnsize(\n+        self, request: Request, warn_pattern: re.Pattern[str], expected_body: bytes\n+    ) -> None:\n         with self.assertLogs(\"scrapy.core.http2.stream\", level=\"WARNING\") as cm:\n-            response = yield self.make_request(request)\n+            response = await self.make_request(request)\n             assert response.status == 200\n             assert response.request == request\n             assert response.body == expected_body\n@@ -487,8 +477,8 @@ class TestHttps2ClientProtocol(TestCase):\n             # Check the warning is raised only once for this request\n             assert sum(len(re.findall(warn_pattern, log)) for log in cm.output) == 1\n \n-    @inlineCallbacks\n-    def test_log_expected_warnsize(self):\n+    @deferred_f_from_coro_f\n+    async def test_log_expected_warnsize(self):\n         request = Request(\n             url=self.get_url(\"/get-data-html-large\"), meta={\"download_warnsize\": 1000}\n         )\n@@ -497,10 +487,10 @@ class TestHttps2ClientProtocol(TestCase):\n             rf\"download warn size \\(1000\\) in request {request}\"\n         )\n \n-        yield self._check_log_warnsize(request, warn_pattern, Data.HTML_LARGE)\n+        await self._check_log_warnsize(request, warn_pattern, Data.HTML_LARGE)\n \n-    @inlineCallbacks\n-    def test_log_received_warnsize(self):\n+    @deferred_f_from_coro_f\n+    async def test_log_received_warnsize(self):\n         request = Request(\n             url=self.get_url(\"/no-content-length-header\"),\n             meta={\"download_warnsize\": 10},\n@@ -510,20 +500,22 @@ class TestHttps2ClientProtocol(TestCase):\n             rf\"warn size \\(10\\) in request {request}\"\n         )\n \n-        yield self._check_log_warnsize(request, warn_pattern, Data.NO_CONTENT_LENGTH)\n+        await self._check_log_warnsize(request, warn_pattern, Data.NO_CONTENT_LENGTH)\n \n-    def test_max_concurrent_streams(self):\n+    @deferred_f_from_coro_f\n+    async def test_max_concurrent_streams(self):\n         \"\"\"Send 500 requests at one to check if we can handle\n         very large number of request.\n         \"\"\"\n \n-        def get_deferred():\n-            return self._check_GET(\n+        async def get_coro() -> None:\n+            await self._check_GET(\n                 Request(self.get_url(\"/get-data-html-small\")), Data.HTML_SMALL, 200\n             )\n \n-        return self._check_repeat(get_deferred, 500)\n+        await self._check_repeat(get_coro, 500)\n \n+    @inlineCallbacks\n     def test_inactive_stream(self):\n         \"\"\"Here we send 110 requests considering the MAX_CONCURRENT_STREAMS\n         by default is 100. After sending the first 100 requests we close the\n@@ -532,6 +524,7 @@ class TestHttps2ClientProtocol(TestCase):\n \n         def assert_inactive_stream(failure):\n             assert failure.check(ResponseFailed) is not None\n+\n             from scrapy.core.http2.stream import InactiveStreamClosed\n \n             assert any(\n@@ -540,14 +533,14 @@ class TestHttps2ClientProtocol(TestCase):\n \n         # Send 100 request (we do not check the result)\n         for _ in range(100):\n-            d = self.make_request(Request(self.get_url(\"/get-data-html-small\")))\n+            d = self.make_request_dfd(Request(self.get_url(\"/get-data-html-small\")))\n             d.addBoth(lambda _: None)\n             d_list.append(d)\n \n         # Now send 10 extra request and save the response deferred in a list\n         for _ in range(10):\n-            d = self.make_request(Request(self.get_url(\"/get-data-html-small\")))\n-            d.addCallback(self.fail)\n+            d = self.make_request_dfd(Request(self.get_url(\"/get-data-html-small\")))\n+            d.addCallback(lambda _: pytest.fail(\"This request should have failed\"))\n             d.addErrback(assert_inactive_stream)\n             d_list.append(d)\n \n@@ -555,13 +548,15 @@ class TestHttps2ClientProtocol(TestCase):\n         # with InactiveStreamClosed\n         self.client.transport.loseConnection()\n \n-        return DeferredList(d_list, consumeErrors=True, fireOnOneErrback=True)\n+        yield DeferredList(d_list, consumeErrors=True, fireOnOneErrback=True)\n \n-    def test_invalid_request_type(self):\n+    @deferred_f_from_coro_f\n+    async def test_invalid_request_type(self):\n         with pytest.raises(TypeError):\n-            self.make_request(\"https://InvalidDataTypePassed.com\")\n+            await self.make_request(\"https://InvalidDataTypePassed.com\")\n \n-    def test_query_parameters(self):\n+    @deferred_f_from_coro_f\n+    async def test_query_parameters(self):\n         params = {\n             \"a\": generate_random_string(20),\n             \"b\": generate_random_string(20),\n@@ -569,100 +564,72 @@ class TestHttps2ClientProtocol(TestCase):\n             \"d\": generate_random_string(20),\n         }\n         request = Request(self.get_url(f\"/query-params?{urlencode(params)}\"))\n-\n-        def assert_query_params(response: Response):\n+        response = await self.make_request(request)\n         content_encoding_header = response.headers[b\"Content-Encoding\"]\n         assert content_encoding_header is not None\n         content_encoding = str(content_encoding_header, \"utf-8\")\n         data = json.loads(str(response.body, content_encoding))\n         assert data == params\n \n-        d = self.make_request(request)\n-        d.addCallback(assert_query_params)\n-        d.addErrback(self.fail)\n-\n-        return d\n-\n-    def test_status_codes(self):\n-        def assert_response_status(response: Response, expected_status: int):\n-            assert response.status == expected_status\n-\n-        d_list = []\n+    @deferred_f_from_coro_f\n+    async def test_status_codes(self):\n         for status in [200, 404]:\n             request = Request(self.get_url(f\"/status?n={status}\"))\n-            d = self.make_request(request)\n-            d.addCallback(assert_response_status, status)\n-            d.addErrback(self.fail)\n-            d_list.append(d)\n+            response = await self.make_request(request)\n+            assert response.status == status\n \n-        return DeferredList(d_list, fireOnOneErrback=True)\n-\n-    def test_response_has_correct_certificate_ip_address(self):\n+    @deferred_f_from_coro_f\n+    async def test_response_has_correct_certificate_ip_address(self):\n         request = Request(self.get_url(\"/status?n=200\"))\n-\n-        def assert_metadata(response: Response):\n+        response = await self.make_request(request)\n         assert response.request == request\n         assert isinstance(response.certificate, Certificate)\n         assert response.certificate.original is not None\n-            assert (\n-                response.certificate.getIssuer() == self.client_certificate.getIssuer()\n-            )\n+        assert response.certificate.getIssuer() == self.client_certificate.getIssuer()\n         assert response.certificate.getPublicKey().matches(\n             self.client_certificate.getPublicKey()\n         )\n-\n         assert isinstance(response.ip_address, IPv4Address)\n         assert str(response.ip_address) == \"127.0.0.1\"\n \n-        d = self.make_request(request)\n-        d.addCallback(assert_metadata)\n-        d.addErrback(self.fail)\n-\n-        return d\n-\n-    def _check_invalid_netloc(self, url):\n-        request = Request(url)\n-\n-        def assert_invalid_hostname(failure: Failure):\n+    async def _check_invalid_netloc(self, url: str) -> None:\n         from scrapy.core.http2.stream import InvalidHostname\n \n-            assert failure.check(InvalidHostname) is not None\n-            error_msg = str(failure.value)\n+        request = Request(url)\n+        with pytest.raises(InvalidHostname) as exc_info:\n+            await self.make_request(request)\n+        error_msg = str(exc_info.value)\n         assert \"localhost\" in error_msg\n         assert \"127.0.0.1\" in error_msg\n         assert str(request) in error_msg\n \n-        d = self.make_request(request)\n-        d.addCallback(self.fail)\n-        d.addErrback(assert_invalid_hostname)\n-        return d\n+    @deferred_f_from_coro_f\n+    async def test_invalid_hostname(self):\n+        await self._check_invalid_netloc(\"https://notlocalhost.notlocalhostdomain\")\n \n-    def test_invalid_hostname(self):\n-        return self._check_invalid_netloc(\"https://notlocalhost.notlocalhostdomain\")\n-\n-    def test_invalid_host_port(self):\n+    @deferred_f_from_coro_f\n+    async def test_invalid_host_port(self):\n         port = self.port_number + 1\n-        return self._check_invalid_netloc(f\"https://127.0.0.1:{port}\")\n+        await self._check_invalid_netloc(f\"https://127.0.0.1:{port}\")\n \n-    def test_connection_stays_with_invalid_requests(self):\n-        d_list = [\n-            self.test_invalid_hostname(),\n-            self.test_invalid_host_port(),\n-            self.test_GET_small_body(),\n-            self.test_POST_small_json(),\n-        ]\n-\n-        return DeferredList(d_list, fireOnOneErrback=True)\n+    @deferred_f_from_coro_f\n+    async def test_connection_stays_with_invalid_requests(self):\n+        await maybe_deferred_to_future(self.test_invalid_hostname())\n+        await maybe_deferred_to_future(self.test_invalid_host_port())\n+        await maybe_deferred_to_future(self.test_GET_small_body())\n+        await maybe_deferred_to_future(self.test_POST_small_json())\n \n+    @inlineCallbacks\n     def test_connection_timeout(self):\n         request = Request(self.get_url(\"/timeout\"))\n-        d = self.make_request(request)\n \n         # Update the timer to 1s to test connection timeout\n         self.client.setTimeout(1)\n \n-        def assert_timeout_error(failure: Failure):\n-            for err in failure.value.reasons:\n+        with pytest.raises(ResponseFailed) as exc_info:\n+            yield self.make_request_dfd(request)\n+\n+        for err in exc_info.value.reasons:\n             from scrapy.core.http2.protocol import H2ClientProtocol\n \n             if isinstance(err, TimeoutError):\n@@ -674,18 +641,13 @@ class TestHttps2ClientProtocol(TestCase):\n         else:\n             pytest.fail(\"No TimeoutError raised.\")\n \n-        d.addCallback(self.fail)\n-        d.addErrback(assert_timeout_error)\n-        return d\n-\n-    def test_request_headers_received(self):\n+    @deferred_f_from_coro_f\n+    async def test_request_headers_received(self):\n         request = Request(\n             self.get_url(\"/request-headers\"),\n             headers={\"header-1\": \"header value 1\", \"header-2\": \"header value 2\"},\n         )\n-        d = self.make_request(request)\n-\n-        def assert_request_headers(response: Response):\n+        response = await self.make_request(request)\n         assert response.status == 200\n         assert response.request == request\n \n@@ -695,7 +657,3 @@ class TestHttps2ClientProtocol(TestCase):\n             k, v = str(k, \"utf-8\"), str(v[0], \"utf-8\")\n             assert k in response_headers\n             assert v == response_headers[k]\n-\n-        d.addErrback(self.fail)\n-        d.addCallback(assert_request_headers)\n-        return d\n\n@@ -5,6 +5,7 @@ from pathlib import Path\n from tempfile import mkdtemp\n from typing import TYPE_CHECKING, Any\n \n+import pytest\n from testfixtures import LogCapture\n from twisted.internet.defer import inlineCallbacks\n from twisted.trial.unittest import TestCase\n@@ -218,18 +219,20 @@ class TestFileDownloadCrawl(TestCase):\n         assert \"ZeroDivisionError\" in str(log)\n \n \n-skip_pillow: str | None\n+pillow_available: bool\n try:\n     from PIL import Image  # noqa: F401\n except ImportError:\n-    skip_pillow = \"Missing Python Imaging Library, install https://pypi.org/pypi/Pillow\"\n+    pillow_available = False\n else:\n-    skip_pillow = None\n+    pillow_available = True\n \n \n-class ImageDownloadCrawlTestCase(TestFileDownloadCrawl):\n-    skip = skip_pillow\n-\n+@pytest.mark.skipif(\n+    not pillow_available,\n+    reason=\"Missing Python Imaging Library, install https://pypi.org/pypi/Pillow\",\n+)\n+class TestImageDownloadCrawl(TestFileDownloadCrawl):\n     pipeline_class = \"scrapy.pipelines.images.ImagesPipeline\"\n     store_setting_key = \"IMAGES_STORE\"\n     media_key = \"images\"\n\n@@ -3,6 +3,7 @@ import os\n import random\n import time\n import warnings\n+from abc import ABC, abstractmethod\n from datetime import datetime\n from io import BytesIO\n from pathlib import Path\n@@ -265,7 +266,12 @@ class TestFilesPipeline(unittest.TestCase):\n         assert file_path(request, item=item) == \"full/path-to-store-file\"\n \n \n-class FilesPipelineTestCaseFieldsMixin:\n+class TestFilesPipelineFieldsMixin(ABC):\n+    @property\n+    @abstractmethod\n+    def item_class(self) -> Any:\n+        raise NotImplementedError\n+\n     def test_item_fields_default(self, tmp_path):\n         url = \"http://www.example.com/files/1.txt\"\n         item = self.item_class(name=\"item1\", file_urls=[url])\n@@ -302,7 +308,7 @@ class FilesPipelineTestCaseFieldsMixin:\n         assert isinstance(item, self.item_class)\n \n \n-class TestFilesPipelineFieldsDict(FilesPipelineTestCaseFieldsMixin):\n+class TestFilesPipelineFieldsDict(TestFilesPipelineFieldsMixin):\n     item_class = dict\n \n \n@@ -316,7 +322,7 @@ class FilesPipelineTestItem(Item):\n     custom_files = Field()\n \n \n-class TestFilesPipelineFieldsItem(FilesPipelineTestCaseFieldsMixin):\n+class TestFilesPipelineFieldsItem(TestFilesPipelineFieldsMixin):\n     item_class = FilesPipelineTestItem\n \n \n@@ -331,7 +337,7 @@ class FilesPipelineTestDataClass:\n     custom_files: list = dataclasses.field(default_factory=list)\n \n \n-class TestFilesPipelineFieldsDataClass(FilesPipelineTestCaseFieldsMixin):\n+class TestFilesPipelineFieldsDataClass(TestFilesPipelineFieldsMixin):\n     item_class = FilesPipelineTestDataClass\n \n \n@@ -346,7 +352,7 @@ class FilesPipelineTestAttrsItem:\n     custom_files: list[dict[str, str]] = attr.ib(default=list)\n \n \n-class TestFilesPipelineFieldsAttrsItem(FilesPipelineTestCaseFieldsMixin):\n+class TestFilesPipelineFieldsAttrsItem(TestFilesPipelineFieldsMixin):\n     item_class = FilesPipelineTestAttrsItem\n \n \n\n@@ -3,8 +3,10 @@ from __future__ import annotations\n import dataclasses\n import io\n import random\n+from abc import ABC, abstractmethod\n from shutil import rmtree\n from tempfile import mkdtemp\n+from typing import Any\n \n import attr\n import pytest\n@@ -208,7 +210,12 @@ class TestImagesPipeline:\n         assert converted.getcolors() == [(10000, (205, 230, 255))]\n \n \n-class ImagesPipelineTestCaseFieldsMixin:\n+class TestImagesPipelineFieldsMixin(ABC):\n+    @property\n+    @abstractmethod\n+    def item_class(self) -> Any:\n+        raise NotImplementedError\n+\n     def test_item_fields_default(self):\n         url = \"http://www.example.com/images/1.jpg\"\n         item = self.item_class(name=\"item1\", image_urls=[url])\n@@ -245,7 +252,7 @@ class ImagesPipelineTestCaseFieldsMixin:\n         assert isinstance(item, self.item_class)\n \n \n-class TestImagesPipelineFieldsDict(ImagesPipelineTestCaseFieldsMixin):\n+class TestImagesPipelineFieldsDict(TestImagesPipelineFieldsMixin):\n     item_class = dict\n \n \n@@ -259,7 +266,7 @@ class ImagesPipelineTestItem(Item):\n     custom_images = Field()\n \n \n-class TestImagesPipelineFieldsItem(ImagesPipelineTestCaseFieldsMixin):\n+class TestImagesPipelineFieldsItem(TestImagesPipelineFieldsMixin):\n     item_class = ImagesPipelineTestItem\n \n \n@@ -274,7 +281,7 @@ class ImagesPipelineTestDataClass:\n     custom_images: list = dataclasses.field(default_factory=list)\n \n \n-class TestImagesPipelineFieldsDataClass(ImagesPipelineTestCaseFieldsMixin):\n+class TestImagesPipelineFieldsDataClass(TestImagesPipelineFieldsMixin):\n     item_class = ImagesPipelineTestDataClass\n \n \n@@ -289,7 +296,7 @@ class ImagesPipelineTestAttrsItem:\n     custom_images: list[dict[str, str]] = attr.ib(default=list)\n \n \n-class TestImagesPipelineFieldsAttrsItem(ImagesPipelineTestCaseFieldsMixin):\n+class TestImagesPipelineFieldsAttrsItem(TestImagesPipelineFieldsMixin):\n     item_class = ImagesPipelineTestAttrsItem\n \n \n\n@@ -345,7 +345,7 @@ class TestMediaPipeline(TestBaseMediaPipeline):\n \n     @inlineCallbacks\n     def test_use_media_to_download_result(self):\n-        req = Request(\"http://url\", meta={\"result\": \"ITSME\", \"response\": self.fail})\n+        req = Request(\"http://url\", meta={\"result\": \"ITSME\"})\n         item = {\"requests\": req}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item[\"results\"] == [(True, \"ITSME\")]\n\n@@ -115,7 +115,7 @@ class TestMinimalScheduler(InterfaceCheckMixin):\n         assert not self.scheduler.has_pending_requests()\n \n \n-class SimpleSchedulerTest(TestCase, InterfaceCheckMixin):\n+class TestSimpleScheduler(TestCase, InterfaceCheckMixin):\n     def setUp(self):\n         self.scheduler = SimpleScheduler()\n \n@@ -145,7 +145,7 @@ class SimpleSchedulerTest(TestCase, InterfaceCheckMixin):\n         assert close_result == \"close\"\n \n \n-class MinimalSchedulerCrawlTest(TestCase):\n+class TestMinimalSchedulerCrawl(TestCase):\n     scheduler_cls = MinimalScheduler\n \n     @inlineCallbacks\n@@ -162,5 +162,5 @@ class MinimalSchedulerCrawlTest(TestCase):\n             assert f\"'item_scraped_count': {len(PATHS)}\" in str(log)\n \n \n-class SimpleSchedulerCrawlTest(MinimalSchedulerCrawlTest):\n+class TestSimpleSchedulerCrawl(TestMinimalSchedulerCrawl):\n     scheduler_cls = SimpleScheduler\n\n@@ -21,7 +21,7 @@ class ItemSpider(Spider):\n         return {\"index\": response.meta[\"index\"]}\n \n \n-class MainTestCase(TestCase):\n+class TestMain(TestCase):\n     @deferred_f_from_coro_f\n     async def test_scheduler_empty(self):\n         crawler = get_crawler()\n@@ -35,7 +35,7 @@ class MainTestCase(TestCase):\n         assert len(calls) >= 1\n \n \n-class MockServerTestCase(TestCase):\n+class TestMockServer(TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.mockserver = MockServer()\n\n@@ -18,7 +18,7 @@ ITEM_A = {\"id\": \"a\"}\n ITEM_B = {\"id\": \"b\"}\n \n \n-class MainTestCase(TestCase):\n+class TestMain(TestCase):\n     async def _test_spider(self, spider, expected_items=None):\n         actual_items = []\n         expected_items = [] if expected_items is None else expected_items\n\n@@ -47,7 +47,7 @@ class UniversalSpiderMiddleware:\n         raise NotImplementedError\n \n \n-# Spiders and spider middlewares for MainTestCase._test_wrap\n+# Spiders and spider middlewares for TestMain._test_wrap\n \n \n class ModernWrapSpider(Spider):\n@@ -106,7 +106,7 @@ class DeprecatedWrapSpiderMiddleware:\n         yield ITEM_C\n \n \n-class MainTestCase(TestCase):\n+class TestMain(TestCase):\n     async def _test(self, spider_middlewares, spider_cls, expected_items):\n         actual_items = []\n \n\n@@ -92,10 +92,10 @@ class TestDeferUtils(unittest.TestCase):\n         x = yield process_parallel([cb1, cb2, cb3], \"res\", \"v1\", \"v2\")\n         assert x == [\"(cb1 res v1 v2)\", \"(cb2 res v1 v2)\", \"(cb3 res v1 v2)\"]\n \n+    @inlineCallbacks\n     def test_process_parallel_failure(self):\n-        d = process_parallel([cb1, cb_fail, cb3], \"res\", \"v1\", \"v2\")\n-        self.failUnlessFailure(d, TypeError)\n-        return d\n+        with pytest.raises(TypeError):\n+            yield process_parallel([cb1, cb_fail, cb3], \"res\", \"v1\", \"v2\")\n \n \n class TestIterErrback:\n\n@@ -59,12 +59,12 @@ class TestSendCatchLog(unittest.TestCase):\n         return \"OK\"\n \n \n-class SendCatchLogDeferredTest(TestSendCatchLog):\n+class TestSendCatchLogDeferred(TestSendCatchLog):\n     def _get_result(self, signal, *a, **kw):\n         return send_catch_log_deferred(signal, *a, **kw)\n \n \n-class SendCatchLogDeferredTest2(SendCatchLogDeferredTest):\n+class TestSendCatchLogDeferred2(TestSendCatchLogDeferred):\n     def ok_handler(self, arg, handlers_called):\n         from twisted.internet import reactor\n \n@@ -76,7 +76,7 @@ class SendCatchLogDeferredTest2(SendCatchLogDeferredTest):\n \n \n @pytest.mark.usefixtures(\"reactor_pytest\")\n-class SendCatchLogDeferredAsyncDefTest(SendCatchLogDeferredTest):\n+class TestSendCatchLogDeferredAsyncDef(TestSendCatchLogDeferred):\n     async def ok_handler(self, arg, handlers_called):\n         handlers_called.add(self.ok_handler)\n         assert arg == \"test\"\n@@ -85,7 +85,7 @@ class SendCatchLogDeferredAsyncDefTest(SendCatchLogDeferredTest):\n \n \n @pytest.mark.only_asyncio\n-class SendCatchLogDeferredAsyncioTest(SendCatchLogDeferredTest):\n+class TestSendCatchLogDeferredAsyncio(TestSendCatchLogDeferred):\n     async def ok_handler(self, arg, handlers_called):\n         handlers_called.add(self.ok_handler)\n         assert arg == \"test\"\n@@ -93,12 +93,12 @@ class SendCatchLogDeferredAsyncioTest(SendCatchLogDeferredTest):\n         return await get_from_asyncio_queue(\"OK\")\n \n \n-class SendCatchLogAsyncTest(TestSendCatchLog):\n+class TestSendCatchLogAsync(TestSendCatchLog):\n     def _get_result(self, signal, *a, **kw):\n         return deferred_from_coro(send_catch_log_async(signal, *a, **kw))\n \n \n-class SendCatchLogAsyncTest2(SendCatchLogAsyncTest):\n+class TestSendCatchLogAsync2(TestSendCatchLogAsync):\n     def ok_handler(self, arg, handlers_called):\n         from twisted.internet import reactor\n \n@@ -110,7 +110,7 @@ class SendCatchLogAsyncTest2(SendCatchLogAsyncTest):\n \n \n @pytest.mark.usefixtures(\"reactor_pytest\")\n-class SendCatchLogAsyncAsyncDefTest(SendCatchLogAsyncTest):\n+class TestSendCatchLogAsyncAsyncDef(TestSendCatchLogAsync):\n     async def ok_handler(self, arg, handlers_called):\n         handlers_called.add(self.ok_handler)\n         assert arg == \"test\"\n@@ -119,7 +119,7 @@ class SendCatchLogAsyncAsyncDefTest(SendCatchLogAsyncTest):\n \n \n @pytest.mark.only_asyncio\n-class SendCatchLogAsyncAsyncioTest(SendCatchLogAsyncTest):\n+class TestSendCatchLogAsyncAsyncio(TestSendCatchLogAsync):\n     async def ok_handler(self, arg, handlers_called):\n         handlers_called.add(self.ok_handler)\n         assert arg == \"test\"\n\n@@ -234,35 +234,31 @@ class TestWebClient(unittest.TestCase):\n     def getURL(self, path):\n         return f\"http://127.0.0.1:{self.portno}/{path}\"\n \n+    @inlineCallbacks\n     def testPayload(self):\n         s = \"0123456789\" * 10\n-        return getPage(self.getURL(\"payload\"), body=s).addCallback(\n-            self.assertEqual, to_bytes(s)\n-        )\n+        body = yield getPage(self.getURL(\"payload\"), body=s)\n+        assert body == to_bytes(s)\n \n+    @inlineCallbacks\n     def testHostHeader(self):\n         # if we pass Host header explicitly, it should be used, otherwise\n         # it should extract from url\n-        return defer.gatherResults(\n-            [\n-                getPage(self.getURL(\"host\")).addCallback(\n-                    self.assertEqual, to_bytes(f\"127.0.0.1:{self.portno}\")\n-                ),\n-                getPage(\n-                    self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"}\n-                ).addCallback(self.assertEqual, to_bytes(\"www.example.com\")),\n-            ]\n-        )\n+        body = yield getPage(self.getURL(\"host\"))\n+        assert body == to_bytes(f\"127.0.0.1:{self.portno}\")\n+        body = yield getPage(self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"})\n+        assert body == to_bytes(\"www.example.com\")\n \n+    @inlineCallbacks\n     def test_getPage(self):\n         \"\"\"\n         L{client.getPage} returns a L{Deferred} which is called back with\n         the body of the response if the default method B{GET} is used.\n         \"\"\"\n-        d = getPage(self.getURL(\"file\"))\n-        d.addCallback(self.assertEqual, b\"0123456789\")\n-        return d\n+        body = yield getPage(self.getURL(\"file\"))\n+        assert body == b\"0123456789\"\n \n+    @inlineCallbacks\n     def test_getPageHead(self):\n         \"\"\"\n         L{client.getPage} returns a L{Deferred} which is called back with\n@@ -273,22 +269,20 @@ class TestWebClient(unittest.TestCase):\n         def _getPage(method):\n             return getPage(self.getURL(\"file\"), method=method)\n \n-        return defer.gatherResults(\n-            [\n-                _getPage(\"head\").addCallback(self.assertEqual, b\"\"),\n-                _getPage(\"HEAD\").addCallback(self.assertEqual, b\"\"),\n-            ]\n-        )\n+        body = yield _getPage(\"head\")\n+        assert body == b\"\"\n+        body = yield _getPage(\"HEAD\")\n+        assert body == b\"\"\n \n+    @inlineCallbacks\n     def test_timeoutNotTriggering(self):\n         \"\"\"\n         When a non-zero timeout is passed to L{getPage} and the page is\n         retrieved before the timeout period elapses, the L{Deferred} is\n         called back with the contents of the page.\n         \"\"\"\n-        d = getPage(self.getURL(\"host\"), timeout=100)\n-        d.addCallback(self.assertEqual, to_bytes(f\"127.0.0.1:{self.portno}\"))\n-        return d\n+        body = yield getPage(self.getURL(\"host\"), timeout=100)\n+        assert body == to_bytes(f\"127.0.0.1:{self.portno}\")\n \n     @inlineCallbacks\n     def test_timeoutTriggering(self):\n@@ -307,12 +301,12 @@ class TestWebClient(unittest.TestCase):\n         if connected:\n             connected[0].transport.loseConnection()\n \n+    @inlineCallbacks\n     def testNotFound(self):\n-        return getPage(self.getURL(\"notsuchfile\")).addCallback(self._cbNoSuchFile)\n-\n-    def _cbNoSuchFile(self, pageData):\n-        assert b\"404 - No Such Resource\" in pageData\n+        body = yield getPage(self.getURL(\"notsuchfile\"))\n+        assert b\"404 - No Such Resource\" in body\n \n+    @inlineCallbacks\n     def testFactoryInfo(self):\n         from twisted.internet import reactor\n \n@@ -320,63 +314,60 @@ class TestWebClient(unittest.TestCase):\n         parsed = urlparse(url)\n         factory = client.ScrapyHTTPClientFactory(Request(url))\n         reactor.connectTCP(parsed.hostname, parsed.port, factory)\n-        return factory.deferred.addCallback(self._cbFactoryInfo, factory)\n-\n-    def _cbFactoryInfo(self, ignoredResult, factory):\n+        yield factory.deferred\n         assert factory.status == b\"200\"\n         assert factory.version.startswith(b\"HTTP/\")\n         assert factory.message == b\"OK\"\n         assert factory.response_headers[b\"content-length\"] == b\"10\"\n \n+    @inlineCallbacks\n     def testRedirect(self):\n-        return getPage(self.getURL(\"redirect\")).addCallback(self._cbRedirect)\n-\n-    def _cbRedirect(self, pageData):\n+        body = yield getPage(self.getURL(\"redirect\"))\n         assert (\n-            pageData\n+            body\n             == b'\\n<html>\\n    <head>\\n        <meta http-equiv=\"refresh\" content=\"0;URL=/file\">\\n'\n             b'    </head>\\n    <body bgcolor=\"#FFFFFF\" text=\"#000000\">\\n    '\n             b'<a href=\"/file\">click here</a>\\n    </body>\\n</html>\\n'\n         )\n \n+    @inlineCallbacks\n     def test_encoding(self):\n         \"\"\"Test that non-standart body encoding matches\n         Content-Encoding header\"\"\"\n-        body = b\"\\xd0\\x81\\xd1\\x8e\\xd0\\xaf\"\n-        dfd = getPage(\n-            self.getURL(\"encoding\"), body=body, response_transform=lambda r: r\n+        original_body = b\"\\xd0\\x81\\xd1\\x8e\\xd0\\xaf\"\n+        response = yield getPage(\n+            self.getURL(\"encoding\"), body=original_body, response_transform=lambda r: r\n         )\n-        return dfd.addCallback(self._check_Encoding, body)\n-\n-    def _check_Encoding(self, response, original_body):\n         content_encoding = to_unicode(response.headers[b\"Content-Encoding\"])\n         assert content_encoding == EncodingResource.out_encoding\n         assert response.body.decode(content_encoding) == to_unicode(original_body)\n \n \n @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n-class WebClientSSLTestCase(TestContextFactoryBase):\n+class TestWebClientSSL(TestContextFactoryBase):\n+    @inlineCallbacks\n     def testPayload(self):\n         s = \"0123456789\" * 10\n-        return getPage(self.getURL(\"payload\"), body=s).addCallback(\n-            self.assertEqual, to_bytes(s)\n-        )\n+        body = yield getPage(self.getURL(\"payload\"), body=s)\n+        assert body == to_bytes(s)\n \n \n-class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n+class TestWebClientCustomCiphersSSL(TestWebClientSSL):\n     # we try to use a cipher that is not enabled by default in OpenSSL\n     custom_ciphers = \"CAMELLIA256-SHA\"\n     context_factory = ssl_context_factory(cipher_string=custom_ciphers)\n \n+    @inlineCallbacks\n     def testPayload(self):\n         s = \"0123456789\" * 10\n         crawler = get_crawler(\n             settings_dict={\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.custom_ciphers}\n         )\n         client_context_factory = build_from_crawler(ScrapyClientContextFactory, crawler)\n-        return getPage(\n+        body = yield getPage(\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n-        ).addCallback(self.assertEqual, to_bytes(s))\n+        )\n+        assert body == to_bytes(s)\n \n     @inlineCallbacks\n     def testPayloadDisabledCipher(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
