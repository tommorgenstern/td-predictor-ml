{"custom_id": "keras#e6ae72cad154f4a733cafa427495bf63e8e74861", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 96 | Churn Cumulative: 516 | Contributors (this commit): 7 | Commits (past 90d): 23 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -290,6 +290,102 @@ class TimeDistributedDense(Layer):\n             \"activation\":self.activation.__name__}\n \n \n+class AutoEncoder(Layer):\n+    '''\n+        A standard autoencoder model.\n+        Output dimensions are always the same size as the input\n+            encoder = f(W.x + b_encoder)\n+            decoder = f(W'.encoder + b_decoder)\n+    '''\n+    def __init__(self, input_dim, hidden_dim, init='glorot_normal',\n+                 encoder_activation='sigmoid', decoder_activation='sigmoid', weights=None,\n+                 W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n+\n+        super(AutoEncoder,self).__init__()\n+        self.init = initializations.get(init)\n+        self.encoder_activation = activations.get(encoder_activation)\n+        self.decoder_activation = activations.get(decoder_activation)\n+        self.input_dim = input_dim\n+        self.hidden_dim = hidden_dim\n+\n+        self.input = T.matrix()\n+        self.W = self.init((self.input_dim, self.hidden_dim))\n+        self.encoder_bias = shared_zeros((self.hidden_dim))\n+        self.decoder_bias = shared_zeros((self.input_dim))\n+\n+        self.params = [self.W, self.encoder_bias, self.decoder_bias]\n+\n+        self.regularizers = [W_regularizer, b_regularizer]\n+        self.constraints = [W_constraint, b_constraint]\n+\n+        if weights is not None:\n+            self.set_weights(weights)\n+\n+    def get_hidden(self, train):\n+        X = self.get_input(train)\n+        encode = self.encoder_activation(T.dot(X, self.W) + self.encoder_bias)\n+        return encode\n+\n+    def get_output(self, train):\n+        encode = self.get_hidden(train)\n+        decode = self.decoder_activation(T.dot(encode, self.W.T) + self.decoder_bias)\n+        return decode\n+\n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"input_dim\":self.input_dim,\n+            \"hidden_dim\":self.hidden_dim,\n+            \"output_dim\":self.input_dim,\n+            \"init\":self.init.__name__,\n+            \"encoder_activation\":self.encoder_activation.__name__,\n+            \"decoder_activation\":self.decoder_activation.__name__,}\n+\n+\n+class DenoisingAutoEncoder(AutoEncoder):\n+    '''\n+        A denoising autoencoder model.\n+        Output dimensions are always the same size as the input\n+            x_p = x.noise\n+            encoder = f(W.x_p + b_encoder)\n+            decoder = f(W'.encoder + b_decoder)\n+    '''\n+    def __init__(self, input_dim, hidden_dim, init='glorot_normal',\n+                 encoder_activation='sigmoid', decoder_activation='sigmoid', corruption_level=0.3,\n+                 weights=None, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n+\n+        AutoEncoder.__init__(self, input_dim, hidden_dim,\n+                             init, encoder_activation, decoder_activation,\n+                             weights, W_regularizer, b_regularizer,\n+                             W_constraint, b_constraint)\n+        self.corruption_level = corruption_level\n+\n+    def get_corrupted_input(self, input):\n+        \"\"\"\n+            http://deeplearning.net/tutorial/dA.html\n+        \"\"\"\n+        return srng.binomial(size=input.shape, n=1,\n+                             p=1 - self.corruption_level,\n+                             dtype=theano.config.floatX) * input\n+\n+    def get_hidden(self, train):\n+        X = self.get_corrupted_input(self.get_input(train))\n+        encode = self.encoder_activation(T.dot(X, self.W) + self.encoder_bias)\n+        return encode\n+\n+    def get_output(self, train):\n+        encode = self.get_hidden(train)\n+        decode = self.decoder_activation(T.dot(encode, self.W.T) + self.decoder_bias)\n+        return decode\n+\n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"input_dim\":self.input_dim,\n+            \"hidden_dim\":self.hidden_dim,\n+            \"output_dim\":self.input_dim,\n+            \"init\":self.init.__name__,\n+            \"corruption_level\":self.corruption_level,\n+            \"encoder_activation\":self.encoder_activation.__name__,\n+            \"decoder_activation\":self.decoder_activation.__name__,}\n \n \n class MaxoutDense(Layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#54b999e661d4eca342d270115f4ef740ce979923", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 105 | Lines Deleted: 77 | Files Changed: 4 | Hunks: 51 | Methods Changed: 16 | Complexity Δ (Sum/Max): 13/12 | Churn Δ: 182 | Churn Cumulative: 1723 | Contributors (this commit): 20 | Commits (past 90d): 76 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,7 +17,7 @@ from keras.preprocessing.text import Tokenizer\n         python examples/reuters_mlp.py\n '''\n \n-max_words = 10000\n+max_words = 1000\n batch_size = 32\n \n print(\"Loading data...\")\n@@ -51,8 +51,12 @@ model.add(Activation('softmax'))\n \n model.compile(loss='categorical_crossentropy', optimizer='adam')\n \n-history = model.fit(X_train, Y_train, nb_epoch=3, batch_size=batch_size, verbose=1, show_accuracy=True, validation_split=0.1)\n-print(history)\n-score = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=1, show_accuracy=True)\n+history = model.fit(X_train, Y_train, nb_epoch=3, batch_size=batch_size, verbose=1, show_accuracy=False, validation_split=0.1)\n+print(history.epoch)\n+print(history.loss)\n+print(history.accuracy)\n+print(history.validation_loss)\n+print(history.validation_accuracy)\n+score = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=1, show_accuracy=False)\n print('Test score:', score[0])\n print('Test accuracy:', score[1])\n\n@@ -26,21 +26,21 @@ class CallbackList(object):\n         for callback in self.callbacks:\n             callback._set_model(model)\n \n-    def on_epoch_begin(self, epoch):\n+    def on_epoch_begin(self, epoch, logs={}):\n         for callback in self.callbacks:\n-            callback.on_epoch_begin(epoch)\n+            callback.on_epoch_begin(epoch, logs)\n         self._delta_t_batch = 0.\n         self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)\n         self._delta_ts_batch_end = deque([], maxlen=self.queue_length)\n \n-    def on_epoch_end(self, epoch):\n+    def on_epoch_end(self, epoch, logs={}):\n         for callback in self.callbacks:\n-            callback.on_epoch_end(epoch)\n+            callback.on_epoch_end(epoch, logs)\n \n-    def on_batch_begin(self, batch):\n+    def on_batch_begin(self, batch, logs={}):\n         t_before_callbacks = time.time()\n         for callback in self.callbacks:\n-            callback.on_batch_begin(batch)\n+            callback.on_batch_begin(batch, logs)\n         self._delta_ts_batch_begin.append(time.time() - t_before_callbacks)\n         delta_t_median = np.median(self._delta_ts_batch_begin)\n         if self._delta_t_batch > 0. and delta_t_median > 0.95 * self._delta_t_batch \\\n@@ -49,11 +49,11 @@ class CallbackList(object):\n                 'to the batch update (%f). Check your callbacks.' % delta_t_median)\n         self._t_enter_batch = time.time()\n \n-    def on_batch_end(self, batch):\n+    def on_batch_end(self, batch, logs={}):\n         self._delta_t_batch = time.time() - self._t_enter_batch\n         t_before_callbacks = time.time()\n         for callback in self.callbacks:\n-            callback.on_batch_end(batch)\n+            callback.on_batch_end(batch, logs)\n         self._delta_ts_batch_end.append(time.time() - t_before_callbacks)\n         delta_t_median = np.median(self._delta_ts_batch_end)\n         if self._delta_t_batch > 0. and delta_t_median > 0.95 * self._delta_t_batch \\\n@@ -61,13 +61,13 @@ class CallbackList(object):\n             warnings.warn('Method on_batch_end() is slow compared '\n                 'to the batch update (%f). Check your callbacks.' % delta_t_median)\n \n-    def on_train_begin(self):\n+    def on_train_begin(self, logs={}):\n         for callback in self.callbacks:\n-            callback.on_train_begin()\n+            callback.on_train_begin(logs)\n \n-    def on_train_end(self):\n+    def on_train_end(self, logs={}):\n         for callback in self.callbacks:\n-            callback.on_train_end()\n+            callback.on_train_end(logs)\n \n \n class Callback(object):\n@@ -81,62 +81,102 @@ class Callback(object):\n     def _set_model(self, model):\n         self.model = model\n \n-    def on_epoch_begin(self, epoch):\n+    def on_epoch_begin(self, epoch, logs={}):\n         pass\n \n-    def on_epoch_end(self, epoch):\n+    def on_epoch_end(self, epoch, logs={}):\n         pass\n \n-    def on_batch_begin(self, batch):\n+    def on_batch_begin(self, batch, logs={}):\n         pass\n \n-    def on_batch_end(self, batch):\n+    def on_batch_end(self, batch, logs={}):\n         pass\n \n-    def on_train_begin(self):\n+    def on_train_begin(self, logs={}):\n         pass\n \n-    def on_train_end(self):\n+    def on_train_end(self, logs={}):\n         pass\n \n class BaseLogger(Callback):\n \n-    def on_train_begin(self):\n+    def on_train_begin(self, logs={}):\n         self.verbose = self.params['verbose']\n \n-    def on_epoch_begin(self, epoch):\n+    def on_epoch_begin(self, epoch, logs={}):\n         if self.verbose:\n             print('Epoch %d' % epoch)\n             self.progbar = Progbar(target=self.params['nb_sample'], \\\n                 verbose=self.verbose)\n         self.current = 0\n+        self.tot_loss = 0.\n+        self.tot_acc = 0.\n \n-    def on_batch_begin(self, batch):\n+    def on_batch_begin(self, batch, logs={}):\n+        if self.current < self.params['nb_sample']:\n             self.log_values = []\n \n-    def on_batch_end(self, batch_index):\n-        self.current += self.model.batch_history['batch_size'][-1]\n-        # skip progbar update for the last batch; will be handled by on_epoch_end\n-        if self.current < self.params['nb_sample']:\n-            loss = self.model.batch_history['loss'][-1]\n+    def on_batch_end(self, batch, logs={}):\n+        batch_size = logs.get('size', 0)\n+        self.current += batch_size\n+\n+        loss = logs.get('loss')\n         self.log_values.append(('loss', loss))\n+        self.tot_loss += loss * batch_size\n         if self.params['show_accuracy']:\n-                accuracy = self.model.batch_history['accuracy'][-1]\n+            accuracy = logs.get('accuracy')\n             self.log_values.append(('acc.', accuracy))\n-            if self.verbose:\n+            self.tot_acc += accuracy * batch_size\n+        # skip progbar update for the last batch; will be handled by on_epoch_end\n+        if self.verbose and self.current < self.params['nb_sample']:\n             self.progbar.update(self.current, self.log_values)\n \n-    def on_epoch_end(self, epoch):\n-        loss = self.model.batch_history['loss'][-1]\n-        self.log_values.append(('loss', loss))\n-\n+    def on_epoch_end(self, epoch, logs={}):\n+        self.log_values.append(('loss', self.tot_loss / self.current))\n         if self.params['show_accuracy']:\n-            accuracy = self.model.batch_history['accuracy'][-1]\n-            self.log_values.append(('acc.', accuracy))\n+            self.log_values.append(('acc.', self.tot_acc / self.current))\n         if self.params['do_validation']:\n-            val_loss = self.model.epoch_history['val_loss'][-1]\n+            val_loss = logs.get('val_loss')\n             self.log_values.append(('val. loss', val_loss))\n             if self.params['show_accuracy']:\n-                val_acc = self.model.epoch_history['val_accuracy'][-1]\n+                val_acc = logs.get('val_accuracy')\n                 self.log_values.append(('val. acc.', val_acc))\n         self.progbar.update(self.current, self.log_values)\n+\n+\n+class History(Callback):\n+\n+    def on_train_begin(self, logs={}):\n+        self.epoch = []\n+        self.loss = []\n+        if self.params['show_accuracy']:\n+            self.accuracy = []\n+        if self.params['do_validation']:\n+            self.validation_loss = []\n+            if self.params['show_accuracy']:\n+                self.validation_accuracy = []\n+\n+    def on_epoch_begin(self, epoch, logs={}):\n+        self.seen = 0\n+        self.tot_loss = 0.\n+        self.tot_accuracy = 0.\n+\n+    def on_batch_end(self, batch, logs={}):\n+        batch_size = logs.get('size', 0)\n+        self.seen += batch_size\n+        self.tot_loss += logs.get('loss', 0.) * batch_size\n+        if self.params['show_accuracy']:\n+            self.tot_accuracy += logs.get('accuracy', 0.) * batch_size\n+\n+    def on_epoch_end(self, epoch, logs={}):\n+        val_loss = logs.get('val_loss')\n+        val_acc = logs.get('val_accuracy')\n+        self.epoch.append(epoch)\n+        self.loss.append(self.tot_loss / self.seen)\n+        if self.params['show_accuracy']:\n+            self.accuracy.append(self.tot_accuracy / self.seen)\n+        if self.params['do_validation']:\n+            self.validation_loss.append(val_loss)\n+            if self.params['show_accuracy']:\n+                self.validation_accuracy.append(val_acc)\n\n@@ -218,6 +218,7 @@ class Sequential(Model):\n         callbacks = cbks.CallbackList(callbacks)\n         if verbose:\n             callbacks.append(cbks.BaseLogger())\n+            callbacks.append(cbks.History())\n \n         callbacks._set_model(self)\n         callbacks._set_params({\n@@ -228,26 +229,9 @@ class Sequential(Model):\n             'do_validation': do_validation,\n             'show_accuracy': show_accuracy\n         })\n-        self.batch_history = {\n-            'batch':[],\n-            'batch_size':[],\n-            'loss':[],\n-            'accuracy':[],\n-            'val_loss':[],\n-            'val_accuracy':[],\n-        }\n-        self.epoch_history = {\n-            'epoch':[],\n-            'epoch_size':[],\n-            'loss':[],\n-            'accuracy':[],\n-            'val_loss':[],\n-            'val_accuracy':[],\n-        }\n         callbacks.on_train_begin()\n \n         for epoch in range(nb_epoch):\n-            self.epoch_history['epoch'] = epoch\n             callbacks.on_epoch_begin(epoch)\n             if shuffle:\n                 np.random.shuffle(index_array)\n@@ -258,40 +242,38 @@ class Sequential(Model):\n                 X_batch = slice_X(X, batch_ids)\n                 y_batch = y[batch_ids]\n \n-                self.batch_history['batch'].append(batch_index)\n-                self.batch_history['batch_size'].append(len(batch_ids))\n-                callbacks.on_batch_begin(batch_index)\n+                batch_logs = {}\n+                batch_logs['batch'] = batch_index\n+                batch_logs['size'] = len(batch_ids)\n+                callbacks.on_batch_begin(batch_index, batch_logs)\n \n                 ins = X_batch + [y_batch]\n                 if show_accuracy:\n                     loss, acc = self._train_with_acc(*ins)\n-                    self.batch_history['accuracy'].append(acc)\n+                    batch_logs['accuracy'] = acc\n                 else:\n                     loss = self._train(*ins)\n-                self.batch_history['loss'].append(loss)\n+                batch_logs['loss'] = loss\n \n-                callbacks.on_batch_end(batch_index)\n+                callbacks.on_batch_end(batch_index, batch_logs)\n                 \n                 if batch_index == len(batches) - 1: # last batch\n                     # validation\n+                    epoch_logs = {}\n                     if do_validation:\n                         if show_accuracy:\n                             val_loss, val_acc = self.evaluate(X_val, y_val, batch_size=batch_size, \\\n                                 verbose=0, show_accuracy=True)\n-                            self.epoch_history['val_accuracy'].append(val_acc)\n+                            epoch_logs['val_accuracy'] = val_acc\n                         else:\n                             val_loss = self.evaluate(X_val, y_val, batch_size=batch_size, verbose=0)\n-                        self.epoch_history['val_loss'].append(val_loss)\n+                        epoch_logs['val_loss'] = val_loss\n \n-            epoch_loss = sum(map(lambda x: x[0]*x[1], zip(self.batch_history['batch_size'], self.batch_history['loss']))) / len(y)\n-            self.epoch_history['loss'].append(epoch_loss)\n-            if show_accuracy:\n-                epoch_acc = sum(map(lambda x: x[0]*x[1], zip(self.batch_history['batch_size'], self.batch_history['accuracy']))) / len(y)\n-                self.epoch_history['accuracy'].append(epoch_acc)\n-            callbacks.on_epoch_end(epoch)\n+            callbacks.on_epoch_end(epoch, epoch_logs)\n \n         callbacks.on_train_end()\n-        return self.epoch_history\n+        # return history\n+        return callbacks.callbacks[-1]\n \n     def predict(self, X, batch_size=128, verbose=1):\n         X = standardize_X(X)\n@@ -334,6 +316,7 @@ class Sequential(Model):\n         if show_accuracy:\n             tot_acc = 0.\n         tot_score = 0.\n+        seen = 0\n \n         batches = make_batches(len(y), batch_size)\n         if verbose:\n@@ -345,21 +328,22 @@ class Sequential(Model):\n             ins = X_batch + [y_batch]\n             if show_accuracy:\n                 loss, acc = self._test_with_acc(*ins)\n-                tot_acc += acc\n+                tot_acc += acc * len(y_batch)\n                 log_values = [('loss', loss), ('acc.', acc)]\n             else:\n                 loss = self._test(*ins)\n                 log_values = [('loss', loss)]\n-            tot_score += loss\n+            tot_score += loss * len(y_batch)\n+            seen += len(y_batch)\n \n             # logging\n             if verbose:\n                 progbar.update(batch_end, log_values)\n \n         if show_accuracy:\n-            return tot_score/len(batches), tot_acc/len(batches)\n+            return tot_score / seen, tot_acc / seen\n         else:\n-            return tot_score/len(batches)\n+            return tot_score / seen\n \n     def get_config(self, verbose=0):\n         layers = []\n\n@@ -60,7 +60,7 @@ class Progbar(object):\n         '''\n         for k, v in values:\n             if k not in self.sum_values:\n-                self.sum_values[k] = [v * max(1, current-self.seen_so_far), current-self.seen_so_far]\n+                self.sum_values[k] = [v * (current-self.seen_so_far), current-self.seen_so_far]\n                 self.unique_values.append(k)\n             else:\n                 self.sum_values[k][0] += v * (current-self.seen_so_far)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9ce20f0b7e0e99dae90d66bc9ddf076a6df22121", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 225 | Lines Deleted: 61 | Files Changed: 2 | Hunks: 17 | Methods Changed: 18 | Complexity Δ (Sum/Max): 22/19 | Churn Δ: 286 | Churn Cumulative: 864 | Contributors (this commit): 7 | Commits (past 90d): 26 | Contributors (cumulative): 9 | DMM Complexity: 0.14285714285714285\n\nDIFF:\n@@ -292,100 +292,125 @@ class TimeDistributedDense(Layer):\n \n class AutoEncoder(Layer):\n     '''\n-        A standard autoencoder model.\n-        Output dimensions are always the same size as the input\n-            encoder = f(W.x + b_encoder)\n-            decoder = f(W'.encoder + b_decoder)\n+        A customizable autoencoder model.\n+        If output_reconstruction then dim(input) = dim(output)\n+        else dim(output) = dim(hidden)\n     '''\n-    def __init__(self, input_dim, hidden_dim, init='glorot_normal',\n-                 encoder_activation='sigmoid', decoder_activation='sigmoid', weights=None,\n-                 W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n+    def __init__(self, encoder=None, decoder=None, output_reconstruction=True, tie_weights=False,\n+                 weights=None, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n \n         super(AutoEncoder,self).__init__()\n-        self.init = initializations.get(init)\n-        self.encoder_activation = activations.get(encoder_activation)\n-        self.decoder_activation = activations.get(decoder_activation)\n-        self.input_dim = input_dim\n-        self.hidden_dim = hidden_dim\n+        if encoder is None or decoder is None:\n+            raise Exception(\"Please specify the encoder/decoder layers\")\n \n-        self.input = T.matrix()\n-        self.W = self.init((self.input_dim, self.hidden_dim))\n-        self.encoder_bias = shared_zeros((self.hidden_dim))\n-        self.decoder_bias = shared_zeros((self.input_dim))\n+        if not isinstance(encoder, Layer) or not isinstance(decoder, Layer):\n+            raise Exception(\"Only Layer types are supported as inputs for autoencoders\")\n \n-        self.params = [self.W, self.encoder_bias, self.decoder_bias]\n+        self.input_dim = encoder.input_dim\n+        self.hidden_dim = decoder.input_dim\n+        self.output_reconstruction = output_reconstruction\n+        self.tie_weights = tie_weights\n+        self.encoder = encoder\n+        self.decoder = decoder\n \n-        self.regularizers = [W_regularizer, b_regularizer]\n-        self.constraints = [W_constraint, b_constraint]\n+        self.decoder.connect(self.encoder)\n+\n+        self.params = []\n+        self.regularizers = []\n+        self.constraints = []\n+        for m in [encoder, decoder]:\n+            self.params += m.params\n+            if hasattr(m, 'regularizers'):\n+                self.regularizers += m.regularizers\n+            if hasattr(m, 'constraints'):\n+                self.constraints += m.constraints\n \n         if weights is not None:\n             self.set_weights(weights)\n \n-    def get_hidden(self, train):\n-        X = self.get_input(train)\n-        encode = self.encoder_activation(T.dot(X, self.W) + self.encoder_bias)\n-        return encode\n+    def connect(self, node):\n+        self.encoder.previous = node\n+\n+    def get_weights(self):\n+        weights = []\n+        for m in [encoder, decoder]:\n+            weights += m.get_weights()\n+        return weights\n+\n+    def set_weights(self, weights):\n+        models = [encoder, decoder]\n+        for i in range(len(models)):\n+            nb_param = len(models[i].params)\n+            models[i].set_weights(weights[:nb_param])\n+            weights = weights[nb_param:]\n+\n+    def get_input(self, train=False):\n+        if hasattr(self.encoder, 'previous'):\n+            return  self.encoder.previous.get_output(train=train)\n+        else:\n+            return self.encoder.input\n+\n+    @property\n+    def input(self):\n+        return self.get_input()\n+\n+    def _get_hidden(self, train):\n+        return self.encoder.get_output(train)\n \n     def get_output(self, train):\n-        encode = self.get_hidden(train)\n-        decode = self.decoder_activation(T.dot(encode, self.W.T) + self.decoder_bias)\n+        if not train and not self.output_reconstruction:\n+            return self._get_hidden(train)\n+\n+        decode = self.decoder.get_output(train)\n+\n+        if self.tie_weights:\n+            encoder_params = self.encoder.get_weights()\n+            decoder_params = self.decoder.get_weights()\n+            for dec_param, enc_param in zip(decoder_params, encoder_params):\n+                if len(dec_param.shape) > 1:\n+                    enc_param = dec_param.T\n+\n         return decode\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n-            \"input_dim\":self.input_dim,\n-            \"hidden_dim\":self.hidden_dim,\n-            \"output_dim\":self.input_dim,\n-            \"init\":self.init.__name__,\n-            \"encoder_activation\":self.encoder_activation.__name__,\n-            \"decoder_activation\":self.decoder_activation.__name__,}\n+                \"encoder_config\":self.encoder.get_config(),\n+                \"decoder_config\":self.decoder.get_config(),\n+                \"output_reconstruction\":self.output_reconstruction,\n+                \"tie_weights\":self.tie_weights}\n \n \n class DenoisingAutoEncoder(AutoEncoder):\n     '''\n-        A denoising autoencoder model.\n-        Output dimensions are always the same size as the input\n-            x_p = x.noise\n-            encoder = f(W.x_p + b_encoder)\n-            decoder = f(W'.encoder + b_decoder)\n+        A denoising autoencoder model that inherits the base features from autoencoder\n+        Cannot be the first layer in a model: same reasoning as Dropout, etc\n     '''\n-    def __init__(self, input_dim, hidden_dim, init='glorot_normal',\n-                 encoder_activation='sigmoid', decoder_activation='sigmoid', corruption_level=0.3,\n-                 weights=None, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n-\n-        AutoEncoder.__init__(self, input_dim, hidden_dim,\n-                             init, encoder_activation, decoder_activation,\n-                             weights, W_regularizer, b_regularizer,\n-                             W_constraint, b_constraint)\n+    def __init__(self, encoder=None, decoder=None, output_reconstruction=True, tie_weights=False,\n+                 weights=None, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None,\n+                 corruption_level=0.3):\n+        super(DenoisingAutoEncoder, self).__init__(encoder, decoder, output_reconstruction, tie_weights,\n+                                                   weights, W_regularizer, b_regularizer, W_constraint, b_constraint)\n         self.corruption_level = corruption_level\n \n-    def get_corrupted_input(self, input):\n+    def _get_corrupted_input(self, input):\n         \"\"\"\n             http://deeplearning.net/tutorial/dA.html\n         \"\"\"\n-        return srng.binomial(size=input.shape, n=1,\n+        return srng.binomial(size=(self.input_dim, 1), n=1,\n                              p=1-self.corruption_level,\n                              dtype=theano.config.floatX) * input\n \n-    def get_hidden(self, train):\n-        X = self.get_corrupted_input(self.get_input(train))\n-        encode = self.encoder_activation(T.dot(X, self.W) + self.encoder_bias)\n-        return encode\n-\n-    def get_output(self, train):\n-        encode = self.get_hidden(train)\n-        decode = self.decoder_activation(T.dot(encode, self.W.T) + self.decoder_bias)\n-        return decode\n+    def get_input(self, train=False):\n+        uncorrupted_input = super(DenoisingAutoEncoder, self).get_input(train)\n+        return self._get_corrupted_input(uncorrupted_input)\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n-            \"input_dim\":self.input_dim,\n-            \"hidden_dim\":self.hidden_dim,\n-            \"output_dim\":self.input_dim,\n-            \"init\":self.init.__name__,\n+                \"encoder_config\":self.encoder.get_config(),\n+                \"decoder_config\":self.decoder.get_config(),\n                 \"corruption_level\":self.corruption_level,\n-            \"encoder_activation\":self.encoder_activation.__name__,\n-            \"decoder_activation\":self.decoder_activation.__name__,}\n+                \"output_reconstruction\":self.output_reconstruction,\n+                \"tie_weights\":self.tie_weights}\n \n \n class MaxoutDense(Layer):\n\n@@ -0,0 +1,139 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+from keras.datasets import mnist\n+from keras.models import Sequential\n+from keras.layers.core import DenoisingAutoEncoder, AutoEncoder, Dense, Activation, TimeDistributedDense, Flatten\n+from keras.layers.recurrent import LSTM\n+from keras.layers.embeddings import Embedding\n+from keras.layers.core import Layer\n+from keras.utils import np_utils\n+\n+import numpy as np\n+\n+# Try different things here: 'lstm' or 'classical' or 'denoising'\n+autoencoder_type = 'lstm'\n+\n+nb_classes = 10\n+batch_size = 128\n+nb_epoch = 10\n+activation = 'linear'\n+\n+input_dim = 784\n+hidden_dim = 392\n+\n+max_train_samples = 5000\n+max_test_samples = 1000\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+\n+X_train = X_train.reshape(60000,input_dim)[:max_train_samples]\n+X_test = X_test.reshape(10000,input_dim)[:max_test_samples]\n+X_train = X_train.astype(\"float32\")\n+X_test = X_test.astype(\"float32\")\n+X_train /= 255\n+X_test /= 255\n+\n+# convert class vectors to binary class matrices\n+Y_train = np_utils.to_categorical(y_train, nb_classes)[:max_train_samples]\n+Y_test = np_utils.to_categorical(y_test, nb_classes)[:max_test_samples]\n+\n+print(\"X_train: \", X_train.shape)\n+print(\"X_test: \", X_test.shape)\n+\n+##########################\n+# dense model test       #\n+##########################\n+\n+print(\"Training classical fully connected layer for classification\")\n+model_classical = Sequential()\n+model_classical.add(Dense(input_dim, 10, activation=activation))\n+model_classical.add(Activation('softmax'))\n+model_classical.get_config(verbose=1)\n+model_classical.compile(loss='categorical_crossentropy', optimizer='adam')\n+model_classical.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=(X_test, Y_test))\n+classical_score = model_classical.evaluate(X_test, Y_test, verbose=0)\n+print('\\nclassical_score:', classical_score)\n+print('\\n')\n+\n+##########################\n+# autoencoder model test #\n+##########################\n+\n+def build_lstm_autoencoder(autoencoder, X_train, X_test):\n+\tX_train = X_train[:, np.newaxis, :] \n+\tX_test = X_test[:, np.newaxis, :]\n+\tprint(\"Modified X_train: \", X_train.shape)\n+\tprint(\"Modified X_test: \", X_test.shape)\n+\n+\t# The TimeDistributedDense isn't really necessary, however you need a lot of GPU memory to do 784x394-394x784\n+\tautoencoder.add(TimeDistributedDense(input_dim, 16))\n+\tautoencoder.add(AutoEncoder(encoder=LSTM(16, 8, activation=activation, return_sequences=True)\n+\t\t\t\t\t\t\t\t, decoder=LSTM(8, input_dim, activation=activation, return_sequences=True)\n+\t\t\t\t\t\t\t\t, output_reconstruction=False))\n+\treturn autoencoder, X_train, X_test\n+\n+def build_classical_autoencoder(autoencoder):\n+\tautoencoder.add(AutoEncoder(encoder=Dense(input_dim, hidden_dim, activation=activation)\n+\t\t\t\t\t\t\t\t, decoder=Dense(hidden_dim, input_dim, activation=activation)\n+\t\t\t\t\t\t\t\t, output_reconstruction=False, tie_weights=True))\n+\treturn autoencoder\n+\n+def build_denoising_autoencoder(autoencoder):\n+\t# You need another layer before a denoising autoencoder\n+\t# This is similar to the dropout layers, etc..\n+\tautoencoder.add(Dense(input_dim, input_dim))\n+\tautoencoder.add(DenoisingAutoEncoder(encoder=Dense(input_dim, hidden_dim, activation=activation)\n+\t\t\t\t\t\t\t\t\t\t, decoder=Dense(hidden_dim, input_dim, activation=activation)\n+\t\t\t\t\t\t\t\t\t\t, output_reconstruction=False, tie_weights=True, corruption_level=0.3))\n+\treturn autoencoder\n+\n+# Build our autoencoder model\n+autoencoder = Sequential()\n+if autoencoder_type == 'lstm':\n+\tprint(\"Training LSTM AutoEncoder\")\n+\tautoencoder, X_train, X_test = build_lstm_autoencoder(autoencoder, X_train, X_test)\n+elif autoencoder_type == 'denoising':\n+\tprint(\"Training Denoising AutoEncoder\")\n+\tautoencoder = build_denoising_autoencoder(autoencoder)\n+elif autoencoder_type == 'classical':\n+\tprint(\"Training Classical AutoEncoder\")\n+\tautoencoder = build_classical_autoencoder(autoencoder)\n+else:\n+\tprint(\"Error: unknown autoencoder type!\")\n+\texit(-1)\n+\n+autoencoder.get_config(verbose=1)\n+autoencoder.compile(loss='mean_squared_error', optimizer='adam')\n+# Do NOT use validation data with return output_reconstruction=True\n+autoencoder.fit(X_train, X_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0)\n+\n+# Do an inference pass\n+prefilter_train = autoencoder.predict(X_train, verbose=0)\n+prefilter_test = autoencoder.predict(X_test, verbose=0)\n+print(\"prefilter_train: \", prefilter_train.shape)\n+print(\"prefilter_test: \", prefilter_test.shape)\n+\n+# Classify results from Autoencoder\n+print(\"Building classical fully connected layer for classification\")\n+model = Sequential()\n+if autoencoder_type == 'lstm':\n+\tmodel.add(TimeDistributedDense(8, 10, activation=activation))\n+\tmodel.add(Flatten())\n+else:\n+\tmodel.add(Dense(392, 10, activation=activation))\n+\n+model.add(Activation('softmax'))\n+\n+model.get_config(verbose=1)\n+model.compile(loss='categorical_crossentropy', optimizer='adam')\n+model.fit(prefilter_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=(prefilter_test, Y_test))\n+\n+score = model.evaluate(prefilter_test, Y_test, verbose=0, show_accuracy=False)\n+print('\\nscore:', score)\n+print('\\n')\n+\n+if score < classical_score:\n+\tprint(\"error: classical score > autoencoder score!\")\n+else:\n+\tprint(\"autoencoder improvement: \", 100.0*(float(score)-float(classical_score))/float(classical_score), \"%\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8824bdb910c162b23251401f55e5ac6888ef43d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 674 | Contributors (this commit): 7 | Commits (past 90d): 25 | Contributors (cumulative): 7 | DMM Complexity: 0.25\n\nDIFF:\n@@ -296,8 +296,7 @@ class AutoEncoder(Layer):\n         If output_reconstruction then dim(input) = dim(output)\n         else dim(output) = dim(hidden)\n     '''\n-    def __init__(self, encoder=None, decoder=None, output_reconstruction=True, tie_weights=False,\n-                 weights=None, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n+    def __init__(self, encoder=None, decoder=None, output_reconstruction=True, tie_weights=False, weights=None):\n \n         super(AutoEncoder,self).__init__()\n         if encoder is None or decoder is None:\n@@ -383,13 +382,9 @@ class AutoEncoder(Layer):\n class DenoisingAutoEncoder(AutoEncoder):\n     '''\n         A denoising autoencoder model that inherits the base features from autoencoder\n-        Cannot be the first layer in a model: same reasoning as Dropout, etc\n     '''\n-    def __init__(self, encoder=None, decoder=None, output_reconstruction=True, tie_weights=False,\n-                 weights=None, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None,\n-                 corruption_level=0.3):\n-        super(DenoisingAutoEncoder, self).__init__(encoder, decoder, output_reconstruction, tie_weights,\n-                                                   weights, W_regularizer, b_regularizer, W_constraint, b_constraint)\n+    def __init__(self, encoder=None, decoder=None, output_reconstruction=True, tie_weights=False, weights=None, corruption_level=0.3):\n+        super(DenoisingAutoEncoder, self).__init__(encoder, decoder, output_reconstruction, tie_weights, weights)\n         self.corruption_level = corruption_level\n \n     def _get_corrupted_input(self, input):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a4af21d588f75e244f90e4cfae95ecc47e1e113d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 43 | Churn Cumulative: 1094 | Contributors (this commit): 16 | Commits (past 90d): 51 | Contributors (cumulative): 19 | DMM Complexity: 0.9285714285714286\n\nDIFF:\n@@ -61,6 +61,16 @@ class Model(object):\n     def get_input(self, train):\n         raise NotImplementedError\n \n+    def calculate_class_weights(self, Y, class_weight):\n+        # this method expects a binary class indicator matrix\n+        # find class index in indicator matrix and map it to a weight vector w\n+        y_classes = Y.argmax(axis=1)\n+        if isinstance(class_weight, dict):\n+            w = np.array(map(lambda x: class_weight[x], y_classes))\n+        else:\n+            w = np.ones((Y.shape[0]))\n+        return w\n+\n     def compile(self, optimizer, loss, class_mode=\"categorical\", theano_mode=None):\n         self.optimizer = optimizers.get(optimizer)\n         self.loss = objectives.get(loss)\n@@ -75,7 +85,10 @@ class Model(object):\n         # target of model\n         self.y = T.zeros_like(self.y_train)\n \n-        train_loss = self.loss(self.y, self.y_train)\n+        # parameter for rescaling the objective function\n+        self.class_weights = T.vector()\n+\n+        train_loss = self.loss(self.y, self.y_train, self.class_weights)\n         test_score = self.loss(self.y, self.y_test)\n \n         if class_mode == \"categorical\":\n@@ -92,11 +105,11 @@ class Model(object):\n         updates = self.optimizer.get_updates(self.params, self.regularizers, self.constraints, train_loss)\n \n         if type(self.X_train) == list:\n-            train_ins = self.X_train + [self.y]\n+            train_ins = self.X_train + [self.y] + [self.class_weights]\n             test_ins = self.X_test + [self.y]\n             predict_ins = self.X_test\n         else:\n-            train_ins = [self.X_train, self.y]\n+            train_ins = [self.X_train, self.y, self.class_weights]\n             test_ins = [self.X_test, self.y]\n             predict_ins = [self.X_test]\n \n@@ -162,11 +175,14 @@ class Sequential(Model):\n             self.layers[0].input = ndim_tensor(ndim)\n         return self.layers[0].get_input(train)\n \n-\n-    def train(self, X, y, accuracy=False):\n+    def train(self, X, y, accuracy=False, class_weight=None):\n         X = standardize_X(X)\n         y = standardize_y(y)\n-        ins = X + [y]\n+\n+        # calculate the weight vector for the loss function\n+        w = self.calculate_class_weights(y, class_weight)\n+\n+        ins = X + [y] + [w]\n         if accuracy:\n             return self._train_with_acc(*ins)\n         else:\n@@ -184,7 +200,7 @@ class Sequential(Model):\n \n \n     def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n-            validation_split=0., validation_data=None, shuffle=True, show_accuracy=False):\n+            validation_split=0., validation_data=None, shuffle=True, show_accuracy=False, class_weight=None):\n         \n         X = standardize_X(X)\n         y = standardize_y(y)\n@@ -241,13 +257,15 @@ class Sequential(Model):\n                 batch_ids = index_array[batch_start:batch_end]\n                 X_batch = slice_X(X, batch_ids)\n                 y_batch = y[batch_ids]\n+                # calculate weight vector for current batch\n+                w = self.calculate_class_weights(y_batch, class_weight)\n \n                 batch_logs = {}\n                 batch_logs['batch'] = batch_index\n                 batch_logs['size'] = len(batch_ids)\n                 callbacks.on_batch_begin(batch_index, batch_logs)\n \n-                ins = X_batch + [y_batch]\n+                ins = X_batch + [y_batch] + [w]\n                 if show_accuracy:\n                     loss, acc = self._train_with_acc(*ins)\n                     batch_logs['accuracy'] = acc\n\n@@ -18,13 +18,18 @@ def squared_hinge(y_true, y_pred):\n def hinge(y_true, y_pred):\n     return T.maximum(1. - y_true * y_pred, 0.).mean()\n \n-def categorical_crossentropy(y_true, y_pred):\n+def categorical_crossentropy(y_true, y_pred, weight=None):\n     '''Expects a binary class matrix instead of a vector of scalar classes\n     '''\n     y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n     # scale preds so that the class probas of each sample sum to 1\n     y_pred /= y_pred.sum(axis=1, keepdims=True) \n-    return T.nnet.categorical_crossentropy(y_pred, y_true).mean()\n+    cce = T.nnet.categorical_crossentropy(y_pred, y_true)\n+    if weight is not None:\n+        # return avg. of scaled cat. crossentropy\n+        return (weight*cce).mean()\n+    else:\n+        return cce.mean()\n \n def binary_crossentropy(y_true, y_pred):\n     y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7bcc4cb257a904074f3581cb5f43ceb68e967a3f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 79 | Churn Cumulative: 79 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,79 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+from keras.datasets import mnist\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation, Merge\n+from keras.utils import np_utils\n+import numpy as np\n+\n+nb_classes = 10\n+batch_size = 128\n+nb_epoch = 15\n+\n+max_train_samples = 5000\n+max_test_samples = 1000\n+\n+np.random.seed(1337) # for reproducibility\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+\n+X_train = X_train.reshape(60000,784)[:max_train_samples]\n+X_test = X_test.reshape(10000,784)[:max_test_samples]\n+X_train = X_train.astype(\"float32\")\n+X_test = X_test.astype(\"float32\")\n+X_train /= 255\n+X_test /= 255\n+\n+# convert class vectors to binary class matrices\n+y_train = y_train[:max_train_samples]\n+y_test = y_test[:max_test_samples]\n+Y_train = np_utils.to_categorical(y_train, nb_classes)\n+Y_test = np_utils.to_categorical(y_test, nb_classes)\n+\n+def createMNISTModel():\n+    model = Sequential()\n+    model.add(Dense(784, 50))\n+    model.add(Activation('relu'))\n+    model.add(Dense(50, 10))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+    return model\n+\n+model_classweights_fit = createMNISTModel()\n+model_fit = createMNISTModel()\n+\n+model_classweights_train = createMNISTModel()\n+model_train = createMNISTModel()\n+\n+high_weight = 100\n+class_weight = {0:1,1:1,2:1,3:1,4:1,5:1,6:1,7:1,8:1,9:high_weight}\n+\n+########################\n+#test different methods#\n+########################\n+\n+# fit\n+model_classweights_fit.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=3, validation_data=(X_test, Y_test), class_weight=class_weight)\n+model_fit.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=3, validation_data=(X_test, Y_test))\n+# train\n+model_classweights_train.train(X_train, Y_train, class_weight=class_weight)\n+model_train.train(X_train, Y_train)\n+\n+print('Classification accuracies on test set:')\n+for nb in range(nb_classes):\n+    testIdcs = np.where(y_test == np.array(nb))[0]\n+    X_temp = X_test[testIdcs, :]\n+    Y_temp = Y_test[testIdcs,:]\n+    # eval model which was trained with fit()\n+    score_cw = model_classweights_fit.evaluate(X_temp, Y_temp, show_accuracy=True, verbose=0)\n+    score = model_fit.evaluate(X_temp, Y_temp, show_accuracy=True, verbose=0)\n+    # eval model which was trained with train()\n+    score_cw_train = model_classweights_train.evaluate(X_temp, Y_temp, show_accuracy=True, verbose=0)\n+    score_train = model_train.evaluate(X_temp, Y_temp, show_accuracy=True, verbose=0)\n+    # print test accuracies for class weighted model vs. uniform weights\n+    print(\"Digit %d: class_weight = %d -> %.3f \\t class_weight = %d -> %.3f\" % (nb, class_weight[nb], score_cw[1], 1, score[1]))\n+    if class_weight[nb] == high_weight and (score_cw[1] <= score[1] or score_cw_train[1] <= score_train[1]):\n+        raise Exception('Class weights are not implemented correctly')\n+\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#65e4f94e45f32d9cbe99337d74ed9c1ebad3412a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 44 | Files Changed: 2 | Hunks: 25 | Methods Changed: 16 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 95 | Churn Cumulative: 970 | Contributors (this commit): 7 | Commits (past 90d): 29 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -293,31 +293,35 @@ class TimeDistributedDense(Layer):\n class AutoEncoder(Layer):\n     '''\n         A customizable autoencoder model.\n-        If output_reconstruction then dim(input) = dim(output)\n-        else dim(output) = dim(hidden)\n+          - Supports deep architectures by passing appropriate encoders/decoders list\n+          - If output_reconstruction then dim(input) = dim(output) else dim(output) = dim(hidden)\n     '''\n-    def __init__(self, encoder=None, decoder=None, output_reconstruction=True, tie_weights=False, weights=None):\n+    def __init__(self, encoders=[], decoders=[], output_reconstruction=True, tie_weights=False, weights=None):\n \n         super(AutoEncoder,self).__init__()\n-        if encoder is None or decoder is None:\n+        if not encoders or not decoders:\n             raise Exception(\"Please specify the encoder/decoder layers\")\n \n-        if not isinstance(encoder, Layer) or not isinstance(decoder, Layer):\n-            raise Exception(\"Only Layer types are supported as inputs for autoencoders\")\n+        if not len(encoders) == len(decoders):\n+            raise Exception(\"There need to be an equal number of encoders and decoders\")\n \n-        self.input_dim = encoder.input_dim\n-        self.hidden_dim = decoder.input_dim\n+        # connect all encoders & decoders to their previous (respectively)\n+        for i in xrange(len(encoders)-1, 0, -1):\n+            encoders[i].connect(encoders[i-1])\n+            decoders[i].connect(decoders[i-1])\n+        decoders[0].connect(encoders[-1])  # connect the first to the last\n+\n+        self.input_dim = encoders[0].input_dim\n+        self.hidden_dim = reversed([d.input_dim for d in decoders])\n         self.output_reconstruction = output_reconstruction\n         self.tie_weights = tie_weights\n-        self.encoder = encoder\n-        self.decoder = decoder\n-\n-        self.decoder.connect(self.encoder)\n+        self.encoders = encoders\n+        self.decoders = decoders\n \n         self.params = []\n         self.regularizers = []\n         self.constraints = []\n-        for m in [encoder, decoder]:\n+        for m in encoders + decoders:\n             self.params += m.params\n             if hasattr(m, 'regularizers'):\n                 self.regularizers += m.regularizers\n@@ -328,53 +332,54 @@ class AutoEncoder(Layer):\n             self.set_weights(weights)\n \n     def connect(self, node):\n-        self.encoder.previous = node\n+        self.encoders[0].previous = node\n \n     def get_weights(self):\n         weights = []\n-        for m in [encoder, decoder]:\n+        for m in encoders + decoders:\n             weights += m.get_weights()\n         return weights\n \n     def set_weights(self, weights):\n-        models = [encoder, decoder]\n+        models = encoders + decoders\n         for i in range(len(models)):\n             nb_param = len(models[i].params)\n             models[i].set_weights(weights[:nb_param])\n             weights = weights[nb_param:]\n \n     def get_input(self, train=False):\n-        if hasattr(self.encoder, 'previous'):\n-            return  self.encoder.previous.get_output(train=train)\n+        if hasattr(self.encoders[0], 'previous'):\n+            return  self.encoders[0].previous.get_output(train=train)\n         else:\n-            return self.encoder.input\n+            return self.encoders[0].input\n \n     @property\n     def input(self):\n         return self.get_input()\n \n     def _get_hidden(self, train):\n-        return self.encoder.get_output(train)\n+        return self.encoders[-1].get_output(train)\n+\n+    def _tranpose_weights(self, src, dest):\n+        if len(dest.shape) > 1:\n+            dest = src.T\n \n     def get_output(self, train):\n         if not train and not self.output_reconstruction:\n             return self._get_hidden(train)\n \n-        decode = self.decoder.get_output(train)\n+        decode = self.decoders[-1].get_output(train)\n \n         if self.tie_weights:\n-            encoder_params = self.encoder.get_weights()\n-            decoder_params = self.decoder.get_weights()\n-            for dec_param, enc_param in zip(decoder_params, encoder_params):\n-                if len(dec_param.shape) > 1:\n-                    enc_param = dec_param.T\n+            for e,d in zip(self.encoders, self.decoders):\n+                map(self._tranpose_weights, e.get_weights(), d.get_weights())\n \n         return decode\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n-                \"encoder_config\":self.encoder.get_config(),\n-                \"decoder_config\":self.decoder.get_config(),\n+                \"encoder_config\":[e.get_config() for e in self.encoders],\n+                \"decoder_config\":[d.get_config() for d in self.decoders],\n                 \"output_reconstruction\":self.output_reconstruction,\n                 \"tie_weights\":self.tie_weights}\n \n@@ -383,8 +388,8 @@ class DenoisingAutoEncoder(AutoEncoder):\n     '''\n         A denoising autoencoder model that inherits the base features from autoencoder\n     '''\n-    def __init__(self, encoder=None, decoder=None, output_reconstruction=True, tie_weights=False, weights=None, corruption_level=0.3):\n-        super(DenoisingAutoEncoder, self).__init__(encoder, decoder, output_reconstruction, tie_weights, weights)\n+    def __init__(self, encoders=None, decoders=None, output_reconstruction=True, tie_weights=False, weights=None, corruption_level=0.3):\n+        super(DenoisingAutoEncoder, self).__init__(encoders, decoders, output_reconstruction, tie_weights, weights)\n         self.corruption_level = corruption_level\n \n     def _get_corrupted_input(self, input):\n@@ -401,8 +406,8 @@ class DenoisingAutoEncoder(AutoEncoder):\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n-                \"encoder_config\":self.encoder.get_config(),\n-                \"decoder_config\":self.decoder.get_config(),\n+                \"encoder_config\":[e.get_config() for e in self.encoders],\n+                \"decoder_config\":[d.get_config() for d in self.decoders],\n                 \"corruption_level\":self.corruption_level,\n                 \"output_reconstruction\":self.output_reconstruction,\n                 \"tie_weights\":self.tie_weights}\n\n@@ -11,7 +11,7 @@ from keras.utils import np_utils\n import numpy as np\n \n # Try different things here: 'lstm' or 'classical' or 'denoising'\n-autoencoder_type = 'lstm'\n+autoencoder_type = 'classical'\n \n nb_classes = 10\n batch_size = 128\n@@ -68,23 +68,23 @@ def build_lstm_autoencoder(autoencoder, X_train, X_test):\n \n \t# The TimeDistributedDense isn't really necessary, however you need a lot of GPU memory to do 784x394-394x784\n \tautoencoder.add(TimeDistributedDense(input_dim, 16))\n-\tautoencoder.add(AutoEncoder(encoder=LSTM(16, 8, activation=activation, return_sequences=True)\n-\t\t\t\t\t\t\t\t, decoder=LSTM(8, input_dim, activation=activation, return_sequences=True)\n-\t\t\t\t\t\t\t\t, output_reconstruction=False))\n+\tautoencoder.add(AutoEncoder(encoders=[LSTM(16, 8, activation=activation, return_sequences=True)]\n+\t\t\t\t\t\t\t\t, decoders=[LSTM(8, input_dim, activation=activation, return_sequences=True)]\n+\t\t\t\t\t\t\t\t, output_reconstruction=False, tie_weights=True))\n \treturn autoencoder, X_train, X_test\n \n-def build_classical_autoencoder(autoencoder):\n-\tautoencoder.add(AutoEncoder(encoder=Dense(input_dim, hidden_dim, activation=activation)\n-\t\t\t\t\t\t\t\t, decoder=Dense(hidden_dim, input_dim, activation=activation)\n-\t\t\t\t\t\t\t\t, output_reconstruction=False, tie_weights=True))\n+def build_deep_classical_autoencoder(autoencoder):\n+\tencoders = [Dense(input_dim, hidden_dim, activation=activation), Dense(hidden_dim, hidden_dim/2, activation=activation)]\n+\tdecoders = [Dense(hidden_dim/2, hidden_dim, activation=activation), Dense(hidden_dim, input_dim, activation=activation)]\n+\tautoencoder.add(AutoEncoder(encoders=encoders, decoders=decoders, output_reconstruction=False, tie_weights=True))\n \treturn autoencoder\n \n def build_denoising_autoencoder(autoencoder):\n \t# You need another layer before a denoising autoencoder\n \t# This is similar to the dropout layers, etc..\n \tautoencoder.add(Dense(input_dim, input_dim))\n-\tautoencoder.add(DenoisingAutoEncoder(encoder=Dense(input_dim, hidden_dim, activation=activation)\n-\t\t\t\t\t\t\t\t\t\t, decoder=Dense(hidden_dim, input_dim, activation=activation)\n+\tautoencoder.add(DenoisingAutoEncoder(encoders=[Dense(input_dim, hidden_dim, activation=activation)]\n+\t\t\t\t\t\t\t\t\t\t, decoders=[Dense(hidden_dim, input_dim, activation=activation)]\n \t\t\t\t\t\t\t\t\t\t, output_reconstruction=False, tie_weights=True, corruption_level=0.3))\n \treturn autoencoder\n \n@@ -98,7 +98,7 @@ elif autoencoder_type == 'denoising':\n \tautoencoder = build_denoising_autoencoder(autoencoder)\n elif autoencoder_type == 'classical':\n \tprint(\"Training Classical AutoEncoder\")\n-\tautoencoder = build_classical_autoencoder(autoencoder)\n+\tautoencoder = build_deep_classical_autoencoder(autoencoder)\n else:\n \tprint(\"Error: unknown autoencoder type!\")\n \texit(-1)\n@@ -106,7 +106,7 @@ else:\n autoencoder.get_config(verbose=1)\n autoencoder.compile(loss='mean_squared_error', optimizer='adam')\n # Do NOT use validation data with return output_reconstruction=True\n-autoencoder.fit(X_train, X_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0)\n+autoencoder.fit(X_train, X_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=1)\n \n # Do an inference pass\n prefilter_train = autoencoder.predict(X_train, verbose=0)\n@@ -120,6 +120,8 @@ model = Sequential()\n if autoencoder_type == 'lstm':\n \tmodel.add(TimeDistributedDense(8, 10, activation=activation))\n \tmodel.add(Flatten())\n+elif autoencoder_type == 'classical':\n+\tmodel.add(Dense(196, 10, activation=activation))\n else:\n \tmodel.add(Dense(392, 10, activation=activation))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
