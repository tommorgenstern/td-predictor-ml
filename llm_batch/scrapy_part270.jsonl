{"custom_id": "scrapy#084a1cda6dfd94a3671d49c362db2ee6ea88a10d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 1208 | Contributors (this commit): 19 | Commits (past 90d): 13 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -415,7 +415,7 @@ class FeedExportTest(unittest.TestCase):\n                 spider_cls.start_urls = [s.url('/')]\n                 yield runner.crawl(spider_cls)\n \n-            with open(res_path, 'rb') as f:\n+            with open(res_uri, 'rb') as f:\n                 content = f.read()\n \n         finally:\n@@ -850,12 +850,12 @@ class FeedExportTest(unittest.TestCase):\n     def test_pathlib_uri(self):\n         tmpdir = tempfile.mkdtemp()\n         feed_uri = Path(tmpdir) / 'res'\n-        feed_uri=str(feed_uri)\n         res_uri = urljoin('file:', pathname2url(feed_uri))\n         settings = {\n             'FEED_FORMAT': 'csv',\n             'FEED_STORE_EMPTY': True,\n-            'FEED_URI': res_uri,\n+            'FEED_URI': feed_uri,\n+            'FEED_URI_ISPATH' : True\n         }\n         \n         data = yield self.exported_no_data(settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0042c389eb2d44d017bc8af069c7dd7ebd9319bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1209 | Contributors (this commit): 19 | Commits (past 90d): 14 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -855,7 +855,6 @@ class FeedExportTest(unittest.TestCase):\n             'FEED_FORMAT': 'csv',\n             'FEED_STORE_EMPTY': True,\n             'FEED_URI': feed_uri,\n-            'FEED_URI_ISPATH' : True\n         }\n         \n         data = yield self.exported_no_data(settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9e6e2dde2b7736278075d6a8268511a3bc44b8b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1211 | Contributors (this commit): 19 | Commits (past 90d): 15 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -415,7 +415,7 @@ class FeedExportTest(unittest.TestCase):\n                 spider_cls.start_urls = [s.url('/')]\n                 yield runner.crawl(spider_cls)\n \n-            with open(res_uri, 'rb') as f:\n+            with open(defaults['FEED_URI'], 'rb') as f:\n                 content = f.read()\n \n         finally:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#970c3be1603483a61637b94afdb965eb24342744", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1216 | Contributors (this commit): 19 | Commits (past 90d): 16 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -407,6 +407,7 @@ class FeedExportTest(unittest.TestCase):\n         defaults = {\n             'FEED_URI': res_uri,\n             'FEED_FORMAT': 'csv',\n+            'FEED_PATH': res_path\n         }\n         defaults.update(settings or {})\n         try:\n@@ -415,7 +416,7 @@ class FeedExportTest(unittest.TestCase):\n                 spider_cls.start_urls = [s.url('/')]\n                 yield runner.crawl(spider_cls)\n \n-            with open(defaults['FEED_URI'], 'rb') as f:\n+            with open(defaults['FEED_PATH'], 'rb') as f:\n                 content = f.read()\n \n         finally:\n@@ -855,8 +856,8 @@ class FeedExportTest(unittest.TestCase):\n             'FEED_FORMAT': 'csv',\n             'FEED_STORE_EMPTY': True,\n             'FEED_URI': feed_uri,\n+            'FEED_PATH': feed_uri\n         }\n-        \n         data = yield self.exported_no_data(settings)\n         self.assertEqual(data, b'')\n         shutil.rmtree(tmpdir, ignore_errors=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f39ff4945854fb5d98389fabfa2d2e5a059c0643", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1217 | Contributors (this commit): 19 | Commits (past 90d): 17 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -851,7 +851,6 @@ class FeedExportTest(unittest.TestCase):\n     def test_pathlib_uri(self):\n         tmpdir = tempfile.mkdtemp()\n         feed_uri = Path(tmpdir) / 'res'\n-        res_uri = urljoin('file:', pathname2url(feed_uri))\n         settings = {\n             'FEED_FORMAT': 'csv',\n             'FEED_STORE_EMPTY': True,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#50eaabe1fc540218f9f04197810367154eb3e102", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1219 | Contributors (this commit): 19 | Commits (past 90d): 18 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -416,7 +416,7 @@ class FeedExportTest(unittest.TestCase):\n                 spider_cls.start_urls = [s.url('/')]\n                 yield runner.crawl(spider_cls)\n \n-            with open(defaults['FEED_PATH'], 'rb') as f:\n+            with open(str(defaults['FEED_PATH']), 'rb') as f:\n                 content = f.read()\n \n         finally:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#79d2f99995a12ffc19ab7cd2fda10112cf5e9b65", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 17 | Churn Cumulative: 33 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,16 +1,29 @@\n-from doctest import ELLIPSIS\n+import os\n+from doctest import ELLIPSIS, NORMALIZE_WHITESPACE\n \n+from scrapy.http.response.html import HtmlResponse\n from sybil import Sybil\n from sybil.parsers.codeblock import CodeBlockParser\n from sybil.parsers.doctest import DocTestParser\n from sybil.parsers.skip import skip\n \n \n+def load_response(url, filename):\n+    input_path = os.path.join(os.path.dirname(__file__), '_tests', filename)\n+    with open(input_path, 'rb') as input_file:\n+        return HtmlResponse(url, body=input_file.read())\n+\n+\n+def setup(namespace):\n+    namespace['load_response'] = load_response\n+\n+\n pytest_collect_file = Sybil(\n     parsers=[\n-        DocTestParser(optionflags=ELLIPSIS),\n+        DocTestParser(optionflags=ELLIPSIS | NORMALIZE_WHITESPACE),\n         CodeBlockParser(future_imports=['print_function']),\n         skip,\n     ],\n     pattern='*.rst',\n+    setup=setup,\n ).pytest()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7b7bb028f45a06b3444950521ea582d0e83691ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 167 | Contributors (this commit): 12 | Commits (past 90d): 6 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -273,4 +273,5 @@ coverage_ignore_pyobjects = [\n \n intersphinx_mapping = {\n     'python': ('https://docs.python.org/3', None),\n+    'sphinx': ('https://www.sphinx-doc.org/en/stable', None),\n }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8a6a063778d45e8ba68a59558d2930332c1e9a83", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 168 | Contributors (this commit): 12 | Commits (past 90d): 7 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -31,6 +31,7 @@ extensions = [\n     'sphinx.ext.autodoc',\n     'sphinx.ext.coverage',\n     'sphinx.ext.intersphinx',\n+    'sphinx.ext.viewcode',\n ]\n \n # Add any paths that contain templates here, relative to this directory.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4b8b0345e58ee5990bd0c28205b5eb0b892680d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1221 | Contributors (this commit): 19 | Commits (past 90d): 19 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -29,7 +29,7 @@ from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete, get\n from scrapy.utils.python import to_native_str\n from scrapy.utils.project import get_project_settings\n \n-from pathlib2 import Path\n+from pathlib import Path\n \n \n class FileFeedStorageTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#76c31094dff2920778b42ed746c6a9cd5b08f4e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 169 | Contributors (this commit): 12 | Commits (past 90d): 8 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -27,6 +27,7 @@ sys.path.insert(0, path.dirname(path.dirname(__file__)))\n # Add any Sphinx extension module names here, as strings. They can be extensions\n # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\n extensions = [\n+    'notfound.extension',\n     'scrapydocs',\n     'sphinx.ext.autodoc',\n     'sphinx.ext.coverage',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a3a3107bc45483e0d7c77e45412121fdb8d539b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 4 | Churn Cumulative: 731 | Contributors (this commit): 19 | Commits (past 90d): 8 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -388,9 +388,7 @@ class MutableChain(object):\n         self.data = chain(self.data, *iterables)\n \n     def __iter__(self):\n-        return self.data.__iter__()\n+        return self\n \n     def __next__(self):\n         return next(self.data)\n-\n-    next = __next__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#33ef24c757c797c804c8fd242b8ab5705219b452", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 147 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -79,7 +79,7 @@ class BuildComponentListTest(unittest.TestCase):\n         self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n         d = {'one': {'a': 'a', 'b': 2}}\n         self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {'one': 'lorem ipsum',}\n+        d = {'one': 'lorem ipsum'}\n         self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1d7c8cb0b1d3aa225fd396f263938fd2f171fb73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 171 | Files Changed: 28 | Hunks: 57 | Methods Changed: 23 | Complexity Δ (Sum/Max): 30/17 | Churn Δ: 190 | Churn Cumulative: 11923 | Contributors (this commit): 104 | Commits (past 90d): 200 | Contributors (cumulative): 345 | DMM Complexity: 0.639344262295082\n\nDIFF:\n@@ -1,16 +1,6 @@\n-import six\n from six.moves import copyreg\n \n \n-if six.PY2:\n-    from urlparse import urlparse\n-    \n-    # workaround for https://bugs.python.org/issue9374 - Python < 2.7.4\n-    if urlparse('s3://bucket/key?key=value').query != 'key=value':\n-        from urlparse import uses_query\n-        uses_query.append('s3')\n-\n-\n # Undo what Twisted's perspective broker adds to pickle register\n # to prevent bugs like Twisted#7989 while serializing requests\n import twisted.persisted.styles  # NOQA\n\n@@ -1,5 +1,5 @@\n from __future__ import print_function\n-import sys, six\n+import sys\n from w3lib.url import is_url\n \n from scrapy.commands import ScrapyCommand\n@@ -45,8 +45,7 @@ class Command(ScrapyCommand):\n             self._print_bytes(response.body)\n \n     def _print_bytes(self, bytes_):\n-        bytes_writer = sys.stdout if six.PY2 else sys.stdout.buffer\n-        bytes_writer.write(bytes_ + b'\\n')\n+        sys.stdout.buffer.write(bytes_ + b'\\n')\n \n     def run(self, args, opts):\n         if len(args) != 1 or not is_url(args[0]):\n\n@@ -88,20 +88,9 @@ class Crawler(object):\n             yield self.engine.open_spider(self.spider, start_requests)\n             yield defer.maybeDeferred(self.engine.start)\n         except Exception:\n-            # In Python 2 reraising an exception after yield discards\n-            # the original traceback (see https://bugs.python.org/issue7563),\n-            # so sys.exc_info() workaround is used.\n-            # This workaround also works in Python 3, but it is not needed,\n-            # and it is slower, so in Python 3 we use native `raise`.\n-            if six.PY2:\n-                exc_info = sys.exc_info()\n-\n             self.crawling = False\n             if self.engine is not None:\n                 yield self.engine.close()\n-\n-            if six.PY2:\n-                six.reraise(*exc_info)\n             raise\n \n     def _create_spider(self, *args, **kwargs):\n\n@@ -216,7 +216,7 @@ class CsvItemExporter(BaseItemExporter):\n             write_through=True,\n             encoding=self.encoding,\n             newline='' # Windows needs this https://github.com/scrapy/scrapy/issues/3034\n-        ) if six.PY3 else file\n+        )\n         self.csv_writer = csv.writer(self.stream, **kwargs)\n         self._headers_not_written = True\n         self._join_multivalued = join_multivalued\n\n@@ -10,7 +10,6 @@ import logging\n import posixpath\n from tempfile import NamedTemporaryFile\n from datetime import datetime\n-import six\n from six.moves.urllib.parse import urlparse, unquote\n from ftplib import FTP\n \n@@ -65,7 +64,7 @@ class StdoutFeedStorage(object):\n \n     def __init__(self, uri, _stdout=None):\n         if not _stdout:\n-            _stdout = sys.stdout if six.PY2 else sys.stdout.buffer\n+            _stdout = sys.stdout.buffer\n         self._stdout = _stdout\n \n     def open(self, spider):\n\n@@ -104,8 +104,7 @@ def _get_form(response, formname, formid, formnumber, formxpath):\n                 el = el.getparent()\n                 if el is None:\n                     break\n-        encoded = formxpath if six.PY3 else formxpath.encode('unicode_escape')\n-        raise ValueError('No <form> element found with %s' % encoded)\n+        raise ValueError('No <form> element found with %s' % formxpath)\n \n     # If we get here, it means that either formname was None\n     # or invalid\n\n@@ -32,9 +32,6 @@ class TextResponse(Response):\n \n     def _set_url(self, url):\n         if isinstance(url, six.text_type):\n-            if six.PY2 and self.encoding is None:\n-                raise TypeError(\"Cannot convert unicode url - %s \"\n-                                \"has no encoding\" % type(self).__name__)\n             self._url = to_native_str(url, self.encoding)\n         else:\n             super(TextResponse, self)._set_url(url)\n\n@@ -4,8 +4,8 @@ Scrapy Item\n See documentation in docs/topics/item.rst\n \"\"\"\n \n-import collections\n from abc import ABCMeta\n+from collections.abc import MutableMapping\n from copy import deepcopy\n from pprint import pformat\n from warnings import warn\n@@ -16,12 +16,6 @@ from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.trackref import object_ref\n \n \n-if six.PY2:\n-    MutableMapping = collections.MutableMapping\n-else:\n-    MutableMapping = collections.abc.MutableMapping\n-\n-\n class BaseItem(object_ref):\n     \"\"\"Base class for all scraped items.\n \n\n@@ -4,12 +4,6 @@ This module defines the Link object used in Link extractors.\n For actual link extractors implementation see scrapy.linkextractors, or\n its documentation in: docs/topics/link-extractors.rst\n \"\"\"\n-import warnings\n-import six\n-\n-from scrapy.utils.python import to_bytes\n-\n-\n class Link(object):\n     \"\"\"Link objects represent an extracted link by the LinkExtractor.\"\"\"\n \n@@ -17,11 +11,6 @@ class Link(object):\n \n     def __init__(self, url, text='', fragment='', nofollow=False):\n         if not isinstance(url, str):\n-            if six.PY2:\n-                warnings.warn(\"Link urls must be str objects. \"\n-                              \"Assuming utf-8 encoding (which could be wrong)\")\n-                url = to_bytes(url, encoding='utf8')\n-            else:\n             got = url.__class__.__name__\n             raise TypeError(\"Link urls must be str objects, got %s\" % got)\n         self.url = url\n\n@@ -9,16 +9,11 @@ try:\n     from cStringIO import StringIO as BytesIO\n except ImportError:\n     from io import BytesIO\n-import six\n \n from email.utils import COMMASPACE, formatdate\n from six.moves.email_mime_multipart import MIMEMultipart\n from six.moves.email_mime_text import MIMEText\n from six.moves.email_mime_base import MIMEBase\n-if six.PY2:\n-    from email.MIMENonMultipart import MIMENonMultipart\n-    from email import Encoders\n-else:\n from email.mime.nonmultipart import MIMENonMultipart\n from email import encoders as Encoders\n \n\n@@ -1,19 +1,13 @@\n import six\n import json\n import copy\n-import collections\n+from collections.abc import MutableMapping\n from importlib import import_module\n from pprint import pformat\n \n from scrapy.settings import default_settings\n \n \n-if six.PY2:\n-    MutableMapping = collections.MutableMapping\n-else:\n-    MutableMapping = collections.abc.MutableMapping\n-\n-\n SETTINGS_PRIORITIES = {\n     'default': 0,\n     'command': 10,\n\n@@ -17,8 +17,6 @@ import sys\n from importlib import import_module\n from os.path import join, abspath, dirname\n \n-import six\n-\n AJAXCRAWL_ENABLED = False\n \n AUTOTHROTTLE_ENABLED = False\n@@ -179,7 +177,7 @@ HTTPCACHE_ALWAYS_STORE = False\n HTTPCACHE_IGNORE_HTTP_CODES = []\n HTTPCACHE_IGNORE_SCHEMES = ['file']\n HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS = []\n-HTTPCACHE_DBM_MODULE = 'anydbm' if six.PY2 else 'dbm'\n+HTTPCACHE_DBM_MODULE = 'dbm'\n HTTPCACHE_POLICY = 'scrapy.extensions.httpcache.DummyPolicy'\n HTTPCACHE_GZIP = False\n \n\n@@ -1,7 +1,6 @@\n \"\"\"Boto/botocore helpers\"\"\"\n \n from __future__ import absolute_import\n-import six\n \n from scrapy.exceptions import NotConfigured\n \n@@ -11,11 +10,4 @@ def is_botocore():\n         import botocore\n         return True\n     except ImportError:\n-        if six.PY2:\n-            try:\n-                import boto\n-                return False\n-            except ImportError:\n-                raise NotConfigured('missing botocore or boto library')\n-        else:\n         raise NotConfigured('missing botocore library')\n\n@@ -1,13 +1,10 @@\n+from configparser import ConfigParser\n import os\n import sys\n import numbers\n from operator import itemgetter\n \n import six\n-if six.PY2:\n-    from ConfigParser import SafeConfigParser as ConfigParser\n-else:\n-    from configparser import ConfigParser\n \n from scrapy.settings import BaseSettings\n from scrapy.utils.deprecate import update_classpath\n\n@@ -7,6 +7,7 @@ This module must not depend on any module outside the Standard Library.\n \n import copy\n import collections\n+from collections.abc import Mapping\n import warnings\n \n import six\n@@ -14,12 +15,6 @@ import six\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n-if six.PY2:\n-    Mapping = collections.Mapping\n-else:\n-    Mapping = collections.abc.Mapping\n-\n-\n class MultiValueDictKeyError(KeyError):\n     def __init__(self, *args, **kwargs):\n         warnings.warn(\n@@ -252,7 +247,6 @@ class MergeDict(object):\n     first occurrence will be used.\n     \"\"\"\n     def __init__(self, *dicts):\n-        if not six.PY2:\n         warnings.warn(\n             \"scrapy.utils.datatypes.MergeDict is deprecated in favor \"\n             \"of collections.ChainMap (introduced in Python 3.3)\",\n\n@@ -6,7 +6,6 @@ except ImportError:\n     from io import BytesIO\n from gzip import GzipFile\n \n-import six\n import re\n \n from scrapy.utils.decorators import deprecated\n@@ -17,12 +16,6 @@ from scrapy.utils.decorators import deprecated\n #   (regression or bug-fix compared to Python 3.4)\n # - read1(), which fetches data before raising EOFError on next call\n #   works here but is only available from Python>=3.3\n-# - scrapy does not support Python 3.2\n-# - Python 2.7 GzipFile works fine with standard read() + extrabuf\n-if six.PY2:\n-    def read1(gzf, size=-1):\n-        return gzf.read(size)\n-else:\n def read1(gzf, size=-1):\n     return gzf.read1(size)\n \n@@ -37,7 +30,7 @@ def gunzip(data):\n     chunk = b'.'\n     while chunk:\n         try:\n-            chunk = read1(f, 8196)\n+            chunk = f.read1(8196)\n             output_list.append(chunk)\n         except (IOError, EOFError, struct.error):\n             # complete only if there is some data, otherwise re-raise\n\n@@ -102,11 +102,7 @@ def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n     def row_to_unicode(row_):\n         return [to_unicode(field, encoding) for field in row_]\n \n-    # Python 3 csv reader input object needs to return strings\n-    if six.PY3:\n     lines = StringIO(_body_or_str(obj, unicode=True))\n-    else:\n-        lines = BytesIO(_body_or_str(obj, unicode=False))\n \n     kwargs = {}\n     if delimiter: kwargs[\"delimiter\"] = delimiter\n\n@@ -113,9 +113,6 @@ def to_bytes(text, encoding=None, errors='strict'):\n def to_native_str(text, encoding=None, errors='strict'):\n     \"\"\" Return str representation of ``text``\n     (bytes in Python 2.x and unicode in Python 3.x). \"\"\"\n-    if six.PY2:\n-        return to_bytes(text, encoding, errors)\n-    else:\n     return to_unicode(text, encoding, errors)\n \n \n@@ -189,7 +186,7 @@ def _getargspec_py23(func):\n     \"\"\"_getargspec_py23(function) -> named tuple ArgSpec(args, varargs, keywords,\n                                                         defaults)\n \n-    Identical to inspect.getargspec() in python2, but uses\n+    Was identical to inspect.getargspec() in python2, but uses\n     inspect.getfullargspec() for python3 behind the scenes to avoid\n     DeprecationWarning.\n \n@@ -199,9 +196,6 @@ def _getargspec_py23(func):\n     >>> _getargspec_py23(f)\n     ArgSpec(args=['a', 'b'], varargs='ar', keywords='kw', defaults=(2,))\n     \"\"\"\n-    if six.PY2:\n-        return inspect.getargspec(func)\n-\n     return inspect.ArgSpec(*inspect.getfullargspec(func)[:4])\n \n \n\n@@ -35,18 +35,3 @@ def get_testdata(*paths):\n     path = os.path.join(tests_datadir, *paths)\n     with open(path, 'rb') as f:\n         return f.read()\n-\n-\n-# FIXME: delete after dropping py2 support\n-# Monkey patch the unittest module to prevent the\n-# DeprecationWarning about assertRaisesRegexp -> assertRaisesRegex\n-import six\n-if six.PY2:\n-    import unittest\n-    import twisted.trial.unittest\n-    if not getattr(unittest.TestCase, 'assertRegex', None):\n-        unittest.TestCase.assertRegex = unittest.TestCase.assertRegexpMatches\n-    if not getattr(unittest.TestCase, 'assertRaisesRegex', None):\n-        unittest.TestCase.assertRaisesRegex = unittest.TestCase.assertRaisesRegexp\n-    if not getattr(twisted.trial.unittest.TestCase, 'assertRaisesRegex', None):\n-        twisted.trial.unittest.TestCase.assertRaisesRegex = twisted.trial.unittest.TestCase.assertRaisesRegexp\n\n@@ -3,13 +3,12 @@ import cgi\n import unittest\n import re\n import json\n+from urllib.parse import unquote_to_bytes\n import warnings\n \n import six\n from six.moves import xmlrpc_client as xmlrpclib\n from six.moves.urllib.parse import urlparse, parse_qs, unquote\n-if six.PY3:\n-    from urllib.parse import unquote_to_bytes\n \n from scrapy.http import Request, FormRequest, XmlRpcRequest, JsonRequest, Headers, HtmlResponse\n from scrapy.utils.python import to_bytes, to_native_str\n@@ -1064,8 +1063,7 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(fs, {})\n \n         xpath = u\"//form[@name='\\u03b1']\"\n-        encoded = xpath if six.PY3 else xpath.encode('unicode_escape')\n-        self.assertRaisesRegex(ValueError, re.escape(encoded),\n+        self.assertRaisesRegex(ValueError, re.escape(xpath),\n                                self.request_class.from_response,\n                                response, formxpath=xpath)\n \n@@ -1208,9 +1206,6 @@ def _qs(req, encoding='utf-8', to_unicode=False):\n         qs = req.body\n     else:\n         qs = req.url.partition('?')[2]\n-    if six.PY2:\n-        uqs = unquote(to_native_str(qs, encoding))\n-    elif six.PY3:\n     uqs = unquote_to_bytes(qs)\n     if to_unicode:\n         uqs = uqs.decode(encoding)\n\n@@ -21,7 +21,6 @@ class BaseResponseTest(unittest.TestCase):\n         # Response requires url in the consturctor\n         self.assertRaises(Exception, self.response_class)\n         self.assertTrue(isinstance(self.response_class('http://example.com/'), self.response_class))\n-        if not six.PY2:\n         self.assertRaises(TypeError, self.response_class, b\"http://example.com\")\n         # body can be str or None\n         self.assertTrue(isinstance(self.response_class('http://example.com/', body=b''), self.response_class))\n\n@@ -62,10 +62,6 @@ class ItemTest(unittest.TestCase):\n         i['number'] = 123\n         itemrepr = repr(i)\n \n-        if six.PY2:\n-            self.assertEqual(itemrepr,\n-                             \"{'name': u'John Doe', 'number': 123}\")\n-        else:\n         self.assertEqual(itemrepr,\n                          \"{'name': 'John Doe', 'number': 123}\")\n \n\n@@ -1,6 +1,4 @@\n import unittest\n-import warnings\n-import six\n \n from scrapy.link import Link\n \n@@ -46,12 +44,5 @@ class LinkTest(unittest.TestCase):\n         self._assert_same_links(l1, l2)\n \n     def test_non_str_url_py2(self):\n-        if six.PY2:\n-            with warnings.catch_warnings(record=True) as w:\n-                link = Link(u\"http://www.example.com/\\xa3\")\n-                self.assertIsInstance(link.url, str)\n-                self.assertEqual(link.url, b'http://www.example.com/\\xc2\\xa3')\n-            assert len(w) == 1, \"warning not issued\"\n-        else:\n         with self.assertRaises(TypeError):\n             Link(b\"http://www.example.com/\\xc2\\xa3\")\n\n@@ -3,7 +3,6 @@ from twisted.trial import unittest\n from scrapy.settings import Settings\n from scrapy.exceptions import NotConfigured\n from scrapy.middleware import MiddlewareManager\n-import six\n \n class M1(object):\n \n@@ -66,14 +65,6 @@ class MiddlewareManagerTest(unittest.TestCase):\n \n     def test_methods(self):\n         mwman = TestMiddlewareManager(M1(), M2(), M3())\n-        if six.PY2:\n-            self.assertEqual([x.im_class for x in mwman.methods['open_spider']],\n-                [M1, M2])\n-            self.assertEqual([x.im_class for x in mwman.methods['close_spider']],\n-                [M2, M1])\n-            self.assertEqual([x.im_class for x in mwman.methods['process']],\n-                [M1, M3])\n-        else:\n         self.assertEqual([x.__self__.__class__ for x in mwman.methods['open_spider']],\n             [M1, M2])\n         self.assertEqual([x.__self__.__class__ for x in mwman.methods['close_spider']],\n\n@@ -1,7 +1,6 @@\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n-import six\n \n from scrapy.http import Request\n from scrapy.crawler import CrawlerRunner\n@@ -161,9 +160,4 @@ class CallbackKeywordArgumentsTestCase(TestCase):\n         self.assertEqual(exceptions['takes_less'].exc_info[0], TypeError)\n         self.assertEqual(str(exceptions['takes_less'].exc_info[1]), \"parse_takes_less() got an unexpected keyword argument 'number'\")\n         self.assertEqual(exceptions['takes_more'].exc_info[0], TypeError)\n-        # py2 and py3 messages are different\n-        exc_message = str(exceptions['takes_more'].exc_info[1])\n-        if six.PY2:\n-            self.assertEqual(exc_message, \"parse_takes_more() takes exactly 5 arguments (4 given)\")\n-        elif six.PY3:\n-            self.assertEqual(exc_message, \"parse_takes_more() missing 1 required positional argument: 'other'\")\n+        self.assertEqual(str(exceptions['takes_more'].exc_info[1]), \"parse_takes_more() missing 1 required positional argument: 'other'\")\n\n@@ -60,9 +60,6 @@ class SettingsAttributeTest(unittest.TestCase):\n \n class BaseSettingsTest(unittest.TestCase):\n \n-    if six.PY3:\n-        assertItemsEqual = unittest.TestCase.assertCountEqual\n-\n     def setUp(self):\n         self.settings = BaseSettings()\n \n@@ -152,7 +149,7 @@ class BaseSettingsTest(unittest.TestCase):\n         self.settings.setmodule(\n             'tests.test_settings.default_settings', 10)\n \n-        self.assertItemsEqual(six.iterkeys(self.settings.attributes),\n+        self.assertCountEqual(six.iterkeys(self.settings.attributes),\n                               six.iterkeys(ctrl_attributes))\n \n         for key in six.iterkeys(ctrl_attributes):\n@@ -343,9 +340,6 @@ class BaseSettingsTest(unittest.TestCase):\n \n class SettingsTest(unittest.TestCase):\n \n-    if six.PY3:\n-        assertItemsEqual = unittest.TestCase.assertCountEqual\n-\n     def setUp(self):\n         self.settings = Settings()\n \n\n@@ -1,10 +1,5 @@\n import copy\n import unittest\n-\n-import six\n-if six.PY2:\n-    from collections import Mapping, MutableMapping\n-else:\n from collections.abc import Mapping, MutableMapping\n \n from scrapy.utils.datatypes import CaselessDict, LocalCache, SequenceExclude\n\n@@ -231,12 +231,11 @@ class UtilsPythonTestCase(unittest.TestCase):\n             self.assertEqual(get_func_args(\" \".join), [])\n             self.assertEqual(get_func_args(operator.itemgetter(2)), [])\n         else:\n-            stripself = not six.PY2  # PyPy3 exposes them as methods\n             self.assertEqual(\n-                get_func_args(six.text_type.split, stripself), ['sep', 'maxsplit'])\n-            self.assertEqual(get_func_args(\" \".join, stripself), ['list'])\n+                get_func_args(six.text_type.split, True), ['sep', 'maxsplit'])\n+            self.assertEqual(get_func_args(\" \".join, True), ['list'])\n             self.assertEqual(\n-                get_func_args(operator.itemgetter(2), stripself), ['obj'])\n+                get_func_args(operator.itemgetter(2), True), ['obj'])\n \n \n     def test_without_none_values(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e696ed06d2975ee86e7ce3a2d3892588c420a04", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 22 | Churn Cumulative: 347 | Contributors (this commit): 12 | Commits (past 90d): 14 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -143,11 +143,11 @@ class XmlItemExporter(BaseItemExporter):\n \n     def _beautify_newline(self, new_item=False):\n         if self.indent is not None and (self.indent > 0 or new_item):\n-            self._xg_characters('\\n')\n+            self.xg.characters('\\n')\n \n     def _beautify_indent(self, depth=1):\n         if self.indent:\n-            self._xg_characters(' ' * self.indent * depth)\n+            self.xg.characters(' ' * self.indent * depth)\n \n     def start_exporting(self):\n         self.xg.startDocument()\n@@ -182,26 +182,12 @@ class XmlItemExporter(BaseItemExporter):\n                 self._export_xml_field('value', value, depth=depth+1)\n             self._beautify_indent(depth=depth)\n         elif isinstance(serialized_value, six.text_type):\n-            self._xg_characters(serialized_value)\n+            self.xg.characters(serialized_value)\n         else:\n-            self._xg_characters(str(serialized_value))\n+            self.xg.characters(str(serialized_value))\n         self.xg.endElement(name)\n         self._beautify_newline()\n \n-    # Workaround for https://bugs.python.org/issue17606\n-    # Before Python 2.7.4 xml.sax.saxutils required bytes;\n-    # since 2.7.4 it requires unicode. The bug is likely to be\n-    # fixed in 2.7.6, but 2.7.6 will still support unicode,\n-    # and Python 3.x will require unicode, so \">= 2.7.4\" should be fine.\n-    if sys.version_info[:3] >= (2, 7, 4):\n-        def _xg_characters(self, serialized_value):\n-            if not isinstance(serialized_value, six.text_type):\n-                serialized_value = serialized_value.decode(self.encoding)\n-            return self.xg.characters(serialized_value)\n-    else:  # pragma: no cover\n-        def _xg_characters(self, serialized_value):\n-            return self.xg.characters(serialized_value)\n-\n \n class CsvItemExporter(BaseItemExporter):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#065fe29d3cc6634893dfead320779498fb061cd4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 193 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -16,6 +16,7 @@ from scrapy.utils.decorators import deprecated\n #   (regression or bug-fix compared to Python 3.4)\n # - read1(), which fetches data before raising EOFError on next call\n #   works here but is only available from Python>=3.3\n+@deprecated('GzipFile.read1')\n def read1(gzf, size=-1):\n     return gzf.read1(size)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#85e79ae792752353ea60bff3d93f9e77bea300fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 40 | Files Changed: 8 | Hunks: 16 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 49 | Churn Cumulative: 2258 | Contributors (this commit): 42 | Commits (past 90d): 48 | Contributors (cumulative): 86 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,6 +4,7 @@ and extract the potentially compressed responses that may arrive.\n \n import bz2\n import gzip\n+from io import BytesIO\n import zipfile\n import tarfile\n import logging\n@@ -11,11 +12,6 @@ from tempfile import mktemp\n \n import six\n \n-try:\n-    from cStringIO import StringIO as BytesIO\n-except ImportError:\n-    from io import BytesIO\n-\n from scrapy.responsetypes import responsetypes\n \n logger = logging.getLogger(__name__)\n\n@@ -3,12 +3,8 @@ Mail sending helpers\n \n See documentation in docs/topics/email.rst\n \"\"\"\n-import logging\n-\n-try:\n-    from cStringIO import StringIO as BytesIO\n-except ImportError:\n from io import BytesIO\n+import logging\n \n from email.utils import COMMASPACE, formatdate\n from six.moves.email_mime_multipart import MIMEMultipart\n\n@@ -5,6 +5,7 @@ See documentation in topics/media-pipeline.rst\n \"\"\"\n import functools\n import hashlib\n+from io import BytesIO\n import mimetypes\n import os\n import os.path\n@@ -15,12 +16,6 @@ from six.moves.urllib.parse import urlparse\n from collections import defaultdict\n import six\n \n-\n-try:\n-    from cStringIO import StringIO as BytesIO\n-except ImportError:\n-    from io import BytesIO\n-\n from twisted.internet import defer, threads\n \n from scrapy.pipelines.media import MediaPipeline\n\n@@ -5,12 +5,8 @@ See documentation in topics/media-pipeline.rst\n \"\"\"\n import functools\n import hashlib\n-import six\n-\n-try:\n-    from cStringIO import StringIO as BytesIO\n-except ImportError:\n from io import BytesIO\n+import six\n \n from PIL import Image\n \n\n@@ -1,12 +1,7 @@\n-import struct\n-\n-try:\n-    from cStringIO import StringIO as BytesIO\n-except ImportError:\n-    from io import BytesIO\n from gzip import GzipFile\n-\n+from io import BytesIO\n import re\n+import struct\n \n from scrapy.utils.decorators import deprecated\n \n\n@@ -1,11 +1,7 @@\n import re\n import csv\n-import logging\n-try:\n-    from cStringIO import StringIO as BytesIO\n-except ImportError:\n-    from io import BytesIO\n from io import StringIO\n+import logging\n import six\n \n from scrapy.http import TextResponse, Response\n\n@@ -1,3 +1,4 @@\n+from io import StringIO\n import json\n import os\n import pstats\n@@ -7,10 +8,6 @@ from subprocess import Popen, PIPE\n import sys\n import tempfile\n import unittest\n-try:\n-    from cStringIO import StringIO\n-except ImportError:\n-    from io import StringIO\n \n from scrapy.utils.test import get_testenv\n \n\n@@ -144,9 +144,7 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n \n         # The Failure should encapsulate a FileException ...\n         self.assertEqual(failure.value, file_exc)\n-        # ... and if we're running on Python 3 ...\n-        if sys.version_info.major >= 3:\n-            # ... it should have the returnValue exception set as its context\n+        # ... and it should have the returnValue exception set as its context\n         self.assertEqual(failure.value.__context__, def_gen_return_exc)\n \n         # Let's calculate the request fingerprint and fake some runtime data...\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cfa633f5e865fa491257215cc912d7321cc6da78", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1095 | Contributors (this commit): 23 | Commits (past 90d): 17 | Contributors (cumulative): 30 | DMM Complexity: None\n\nDIFF:\n@@ -88,7 +88,7 @@ class Response(object_ref):\n     @property\n     def text(self):\n         \"\"\"For subclasses of TextResponse, this will return the body\n-        as text (unicode object in Python 2 and str in Python 3)\n+        as str\n         \"\"\"\n         raise AttributeError(\"Response content isn't text\")\n \n\n@@ -90,7 +90,7 @@ def to_unicode(text, encoding=None, errors='strict'):\n     if isinstance(text, six.text_type):\n         return text\n     if not isinstance(text, (bytes, six.text_type)):\n-        raise TypeError('to_unicode must receive a bytes, str or unicode '\n+        raise TypeError('to_unicode must receive a bytes or str '\n                         'object, got %s' % type(text).__name__)\n     if encoding is None:\n         encoding = 'utf-8'\n@@ -103,16 +103,16 @@ def to_bytes(text, encoding=None, errors='strict'):\n     if isinstance(text, bytes):\n         return text\n     if not isinstance(text, six.string_types):\n-        raise TypeError('to_bytes must receive a unicode, str or bytes '\n+        raise TypeError('to_bytes must receive a str or bytes '\n                         'object, got %s' % type(text).__name__)\n     if encoding is None:\n         encoding = 'utf-8'\n     return text.encode(encoding, errors)\n \n \n+@deprecated('to_unicode')\n def to_native_str(text, encoding=None, errors='strict'):\n-    \"\"\" Return str representation of ``text``\n-    (bytes in Python 2.x and unicode in Python 3.x). \"\"\"\n+    \"\"\" Return str representation of ``text``. \"\"\"\n     return to_unicode(text, encoding, errors)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#92ffd2f249ec276436277385e525397a2fb8b0a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 44 | Files Changed: 18 | Hunks: 35 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 61 | Churn Cumulative: 7863 | Contributors (this commit): 59 | Commits (past 90d): 111 | Contributors (cumulative): 165 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,6 @@\n import base64\n from six.moves.urllib.parse import unquote, urlunparse\n from six.moves.urllib.request import getproxies, proxy_bypass\n-try:\n-    from urllib2 import _parse_proxy\n-except ImportError:\n from urllib.request import _parse_proxy\n \n from scrapy.exceptions import NotConfigured\n\n@@ -3,10 +3,7 @@ This module provides some commonly used processors for Item Loaders.\n \n See documentation in docs/topics/loaders.rst\n \"\"\"\n-try:\n from collections import ChainMap\n-except ImportError:\n-    from scrapy.utils.datatypes import MergeDict as ChainMap\n \n from scrapy.utils.misc import arg_to_iter\n from scrapy.loader.common import wrap_loader_context\n\n@@ -21,11 +21,6 @@ if 'COV_CORE_CONFIG' in os.environ:\n     os.environ['COV_CORE_CONFIG'] = os.path.join(_sourceroot,\n                                                  os.environ['COV_CORE_CONFIG'])\n \n-try:\n-    import unittest.mock as mock\n-except ImportError:\n-    import mock\n-\n tests_datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n                              'sample_data')\n \n\n@@ -2,11 +2,8 @@ import os\n import six\n import shutil\n import tempfile\n-import contextlib\n-try:\n from unittest import mock\n-except ImportError:\n-    import mock\n+import contextlib\n \n from testfixtures import LogCapture\n from twisted.trial import unittest\n\n@@ -1,3 +1,5 @@\n+from unittest import mock\n+\n from twisted.trial.unittest import TestCase\n from twisted.python.failure import Failure\n \n@@ -7,7 +9,6 @@ from scrapy.exceptions import _InvalidOutput\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.utils.test import get_crawler\n from scrapy.utils.python import to_bytes\n-from tests import mock\n \n \n class ManagerTestCase(TestCase):\n\n@@ -1,5 +1,8 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n+\n+from unittest import mock\n+\n from twisted.internet import reactor, error\n from twisted.internet.defer import Deferred, DeferredList, maybeDeferred\n from twisted.python import failure\n@@ -9,7 +12,6 @@ from scrapy.downloadermiddlewares.robotstxt import (RobotsTxtMiddleware,\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Request, Response, TextResponse\n from scrapy.settings import Settings\n-from tests import mock\n from tests.test_robotstxt_interface import rerp_available, reppy_available\n \n \n\n@@ -1,8 +1,3 @@\n-try:\n-    import unittest.mock as mock\n-except ImportError:\n-    import mock\n-\n from twisted.trial import unittest\n from twisted.conch.telnet import ITelnetProtocol\n from twisted.cred import credentials\n\n@@ -7,6 +7,7 @@ from io import BytesIO\n import tempfile\n import shutil\n import string\n+from unittest import mock\n from six.moves.urllib.parse import urljoin, urlparse, quote\n from six.moves.urllib.request import pathname2url\n \n@@ -15,7 +16,6 @@ from twisted.trial import unittest\n from twisted.internet import defer\n from scrapy.crawler import CrawlerRunner\n from scrapy.settings import Settings\n-from tests import mock\n from tests.mockserver import MockServer\n from w3lib.url import path_to_file_uri\n \n\n@@ -3,6 +3,7 @@ import cgi\n import unittest\n import re\n import json\n+from unittest import mock\n from urllib.parse import unquote_to_bytes\n import warnings\n \n@@ -13,8 +14,6 @@ from six.moves.urllib.parse import urlparse, parse_qs, unquote\n from scrapy.http import Request, FormRequest, XmlRpcRequest, JsonRequest, Headers, HtmlResponse\n from scrapy.utils.python import to_bytes, to_native_str\n \n-from tests import mock\n-\n \n class RequestTest(unittest.TestCase):\n \n\n@@ -1,12 +1,12 @@\n import sys\n import unittest\n+from unittest import mock\n from warnings import catch_warnings\n \n import six\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.item import ABCMeta, DictItem, Field, Item, ItemMeta\n-from tests import mock\n \n \n PY36_PLUS = (sys.version_info.major >= 3) and (sys.version_info.minor >= 6)\n\n@@ -1,10 +1,9 @@\n import os\n import random\n import time\n-import hashlib\n-import warnings\n from tempfile import mkdtemp\n from shutil import rmtree\n+from unittest import mock\n from six.moves.urllib.parse import urlparse\n from six import BytesIO\n \n@@ -15,13 +14,10 @@ from scrapy.pipelines.files import FilesPipeline, FSFilesStore, S3FilesStore, GC\n from scrapy.item import Item, Field\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n-from scrapy.utils.python import to_bytes\n from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete\n from scrapy.utils.test import assert_gcs_environ, get_gcs_content_and_delete\n from scrapy.utils.boto import is_botocore\n \n-from tests import mock\n-\n \n def _mocked_download_func(request, info):\n     response = request.meta.get('response')\n\n@@ -1,10 +1,9 @@\n import six\n import unittest\n-import warnings\n+from unittest import mock\n \n from scrapy.settings import (BaseSettings, Settings, SettingsAttribute,\n                              SETTINGS_PRIORITIES, get_settings_priority)\n-from tests import mock\n from . import default_settings\n \n \n\n@@ -1,5 +1,6 @@\n import gzip\n import inspect\n+from unittest import mock\n import warnings\n from io import BytesIO\n \n@@ -17,8 +18,6 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.test import get_crawler\n \n-from tests import mock\n-\n \n class SpiderTest(unittest.TestCase):\n \n\n@@ -1,3 +1,5 @@\n+from unittest import mock\n+\n from twisted.trial.unittest import TestCase\n from twisted.python.failure import Failure\n \n@@ -6,7 +8,6 @@ from scrapy.http import Request, Response\n from scrapy.exceptions import _InvalidOutput\n from scrapy.utils.test import get_crawler\n from scrapy.core.spidermw import SpiderMiddlewareManager\n-from tests import mock\n \n \n class SpiderMiddlewareTestCase(TestCase):\n\n@@ -1,10 +1,6 @@\n from datetime import datetime\n import unittest\n-\n-try:\n from unittest import mock\n-except ImportError:\n-    import mock\n \n from scrapy.extensions.corestats import CoreStats\n from scrapy.spiders import Spider\n\n@@ -2,12 +2,11 @@\n from __future__ import absolute_import\n import inspect\n import unittest\n+from unittest import mock\n import warnings\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.deprecate import create_deprecated_class, update_classpath\n \n-from tests import mock\n-\n \n class MyWarning(UserWarning):\n     pass\n\n@@ -1,11 +1,11 @@\n import sys\n import os\n import unittest\n+from unittest import mock\n \n from scrapy.item import Item, Field\n from scrapy.utils.misc import arg_to_iter, create_instance, load_object, set_environ, walk_modules\n \n-from tests import mock\n \n __doctests__ = ['scrapy.utils.misc']\n \n\n@@ -1,7 +1,7 @@\n import six\n import unittest\n+from unittest import mock\n from scrapy.utils import trackref\n-from tests import mock\n \n \n class Foo(trackref.object_ref):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
