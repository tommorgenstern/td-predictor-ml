{"custom_id": "keras#435cf42e24419b60acda6f24e828d59cabe65a2e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 8670 | Contributors (this commit): 95 | Commits (past 90d): 6 | Contributors (cumulative): 95 | DMM Complexity: None\n\nDIFF:\n@@ -66,7 +66,7 @@ class Model(Network):\n                 will then be the *weighted sum* of all individual losses,\n                 weighted by the `loss_weights` coefficients.\n                 If a list, it is expected to have a 1:1 mapping\n-                to the model's outputs. If a tensor, it is expected to map\n+                to the model's outputs. If a dict, it is expected to map\n                 output names (strings) to scalar coefficients.\n             sample_weight_mode: If you need to do timestep-wise\n                 sample weighting (2D weights), set this to `\"temporal\"`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7dd8545a1c2c98119f0e9176ebfa7e49d4eb24dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 2043 | Contributors (this commit): 17 | Commits (past 90d): 4 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -829,7 +829,7 @@ def test_multi_output_mask():\n def test_constant_initializer_with_numpy():\n     model = Sequential()\n     model.add(Dense(2, input_shape=(3,),\n-                    kernel_initializer=Constant(np.ones((3, 2)))))\n+                    kernel_initializer=Constant(1.)))\n     model.add(Dense(3))\n     model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#029461c7d1fe7a6927a6f9d0fa13dbf49543b413", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 13 | Churn Cumulative: 3843 | Contributors (this commit): 33 | Commits (past 90d): 8 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2285,22 +2285,15 @@ def one_hot(indices, num_classes):\n def get_value(x):\n     if isinstance(\n             x,\n-            C.variables.Parameter) or isinstance(\n-            x,\n-            C.variables.Constant):\n+            (C.variables.Parameter, C.variables.Constant)):\n         return x.value\n     else:\n         return eval(x)\n \n \n def batch_get_value(xs):\n-    result = []\n-    for x in xs:\n-        if (isinstance(x, C.variables.Parameter) or\n-           isinstance(x, C.variables.Constant)):\n-            result.append(x.value)\n-        else:\n-            result.append(eval(x))\n+    result = [get_value(x) for x in xs]\n+\n     return result\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5b653e2577abf2691479bd2bfb69bad6bdf3382b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 10 | Files Changed: 5 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 19 | Churn Cumulative: 21872 | Contributors (this commit): 138 | Commits (past 90d): 50 | Contributors (cumulative): 182 | DMM Complexity: 0.0\n\nDIFF:\n@@ -366,7 +366,7 @@ def std(x, axis=None, keepdims=False):\n def logsumexp(x, axis=None, keepdims=False):\n     if isinstance(axis, list):\n         axis = tuple(axis)\n-    return sp.misc.logsumexp(x, axis=axis, keepdims=keepdims)\n+    return sp.special.logsumexp(x, axis=axis, keepdims=keepdims)\n \n \n def sum(x, axis=None, keepdims=False):\n\n@@ -719,6 +719,9 @@ def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n         dtype = floatx()\n     x = tf_keras_backend.placeholder(\n         shape=shape, ndim=ndim, dtype=dtype, sparse=sparse, name=name)\n+    if shape is None:\n+        if ndim is not None:\n+            shape = tuple(None for _ in range(ndim))\n     x._keras_shape = shape\n     x._uses_learning_phase = False\n     return x\n\n@@ -854,17 +854,15 @@ class TestBackend(object):\n             return simple_no_states\n \n         kwargs_list = [\n-            {'go_backwards': False, 'mask': None},\n-            {'go_backwards': True, 'mask': None},\n-            {'go_backwards': False, 'mask': mask},\n-            {'go_backwards': True, 'mask': mask},\n+            {'go_backwards': False},\n+            {'go_backwards': True},\n         ]\n         for kwargs in kwargs_list:\n             check_rnn_operation(step_function_k=get_step_function(K, wi_k),\n                                 step_function_np=get_step_function(KNP, wi),\n                                 inputs_np=x,\n                                 initial_states_np=[],\n-                                mask_np=kwargs.pop('mask', None),\n+                                mask_np=None,\n                                 **kwargs)\n \n     def test_rnn_constants(self):\n\n@@ -292,7 +292,7 @@ def test_TensorBoard_convnet(tmpdir):\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='rmsprop',\n                   metrics=['accuracy'])\n-    tsb = callbacks.TensorBoard(histogram_freq=1)\n+    tsb = callbacks.TensorBoard(filepath, histogram_freq=1)\n     cbks = [tsb]\n     model.summary()\n     history = model.fit(x_train, y_train, epochs=2, batch_size=16,\n\n@@ -21,9 +21,7 @@ sequence_len = 2\n @pytest.mark.parametrize('data_format', ['channels_first', 'channels_last'])\n @pytest.mark.parametrize('return_sequences', [True, False])\n @pytest.mark.parametrize('use_mask', [True, False])\n-def test_convolutional_recurrent(data_format='channels_last',\n-                                 return_sequences=False,\n-                                 use_mask=False):\n+def test_convolutional_recurrent(data_format, return_sequences, use_mask):\n \n     class Masking5D(Masking):\n         \"\"\"Regular masking layer returns wrong shape of mask for RNN\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#331d5b0102ab0cc79cece1f03cc551d8105db3c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 9396 | Contributors (this commit): 66 | Commits (past 90d): 26 | Contributors (cumulative): 75 | DMM Complexity: 0.0\n\nDIFF:\n@@ -939,7 +939,6 @@ class LearningRateScheduler(Callback):\n         logs['lr'] = K.get_value(self.model.optimizer.lr)\n \n \n-\n class ReduceLROnPlateau(Callback):\n     \"\"\"Reduce learning rate when a metric has stopped improving.\n \n\n@@ -512,7 +512,8 @@ class Adam(Optimizer):\n                      name='vhat_' + str(i))\n                      for (i, p) in enumerate(params)]\n         else:\n-            vhats = [K.zeros(1, name='vhat_' + str(i)) for i in range(len(params))]\n+            vhats = [K.zeros(1, name='vhat_' + str(i))\n+                     for i in range(len(params))]\n         self.weights = [self.iterations] + ms + vs + vhats\n \n         for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n\n@@ -659,7 +659,7 @@ class TestBackend(object):\n         new_val_list = [k.get_value(x) for x, k in zip(x_list, test_backend)]\n         assert_list_pairwise(new_val_list)\n \n-    @pytest.mark.skipif(K.backend() != 'tensorflow',\n+    @pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(),\n                         reason='Uses the `fetches` argument.')\n     def test_function_tf_fetches(self):\n         # Additional operations can be passed to tf.Session().run() via its\n@@ -681,7 +681,7 @@ class TestBackend(object):\n         assert output == [30.]\n         assert K.get_session().run(fetches=[x, y]) == [11., 5.]\n \n-    @pytest.mark.skipif(K.backend() != 'tensorflow',\n+    @pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(),\n                         reason='Uses the `feed_dict` argument.')\n     def test_function_tf_feed_dict(self):\n         # Additional substitutions can be passed to `tf.Session().run()` via its\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b4e01aee7f4a9b815542c77c3d537e7ba66dd8c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 14 | Churn Cumulative: 10473 | Contributors (this commit): 53 | Commits (past 90d): 25 | Contributors (cumulative): 71 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2735,7 +2735,14 @@ def to_dense(tensor):\n \n \n def cumsum(x, axis=0):\n-    raise NotImplementedError\n+    dim = x.shape[axis]\n+    U = C.constant(np.triu(np.ones((dim, dim))).astype(x.dtype))\n+    if axis != -1:\n+        x = C.swapaxes(x, -1, axis)\n+    out = C.times(x, U)\n+    if axis != -1:\n+        out = C.swapaxes(out, -1, axis)\n+    return out\n \n \n def cumprod(x, axis=0):\n\n@@ -562,10 +562,13 @@ class TestBackend(object):\n \n     @pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support '\n                                                       'cumsum and cumprod yet')\n-    def test_cumsum_cumprod(self):\n+    def test_cumsum(self):\n         check_single_tensor_operation('cumsum', (4, 2), WITH_NP)\n         check_single_tensor_operation('cumsum', (4, 2), WITH_NP, axis=1)\n \n+    @pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support '\n+                                                      'cumprod yet')\n+    def test_cumprod(self):\n         check_single_tensor_operation('cumprod', (4, 2), WITH_NP)\n         check_single_tensor_operation('cumprod', (4, 2), WITH_NP, axis=1)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f55055a9f053b35fa721d3eb75dd07ea5a5f1e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 3 | Churn Cumulative: 2432 | Contributors (this commit): 37 | Commits (past 90d): 10 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -224,9 +224,6 @@ class RMSprop(Optimizer):\n     at their default values\n     (except the learning rate, which can be freely tuned).\n \n-    This optimizer is usually a good choice for recurrent\n-    neural networks.\n-\n     # Arguments\n         lr: float >= 0. Learning rate.\n         rho: float >= 0.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c24d16af155e20976bdf61e468ba760408e676ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 6753 | Contributors (this commit): 38 | Commits (past 90d): 25 | Contributors (cumulative): 45 | DMM Complexity: 1.0\n\nDIFF:\n@@ -316,6 +316,12 @@ def l2_normalize(x, axis=-1):\n     return x / np.sqrt(y)\n \n \n+def in_top_k(predictions, targets, k):\n+    top_k = np.argsort(-predictions)[:, :k]\n+    targets = targets.reshape(-1, 1)\n+    return np.any(targets == top_k, axis=-1)\n+\n+\n def binary_crossentropy(target, output, from_logits=False):\n     if not from_logits:\n         output = np.clip(output, 1e-7, 1 - 1e-7)\n\n@@ -1156,6 +1156,7 @@ class TestBackend(object):\n         check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=-1)\n         check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=1)\n \n+    @pytest.mark.skipif(K.backend() == 'cntk', reason='Bug in CNTK')\n     def test_in_top_k(self):\n         batch_size = 20\n         num_classes = 10\n@@ -1166,10 +1167,10 @@ class TestBackend(object):\n \n         # (k == 0 or k > num_classes) does not raise an error\n         # but just return an unmeaningful tensor.\n-        for k in range(num_classes + 1):\n+        for k in range(1, num_classes + 1):\n             z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'),\n                                         b.variable(targets, dtype='int32'), k))\n-                      for b in [KTH, KTF]]\n+                      for b in WITH_NP]\n             assert_list_pairwise(z_list)\n \n         # Identical prediction test case:\n@@ -1184,7 +1185,7 @@ class TestBackend(object):\n         for k in range(1, num_classes + 1):\n             z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'),\n                                         b.variable(targets, dtype='int32'), k))\n-                      for b in [KTH, KTF]]\n+                      for b in WITH_NP]\n             assert_list_pairwise(z_list)\n \n     @pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dc83851aca230648c4f75ac1a0d4b9971b2d6aa0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 45 | Files Changed: 4 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 55/60 | Churn Δ: 90 | Churn Cumulative: 31645 | Contributors (this commit): 167 | Commits (past 90d): 55 | Contributors (cumulative): 292 | DMM Complexity: 0.0\n\nDIFF:\n@@ -843,11 +843,9 @@ def tile(x, n):\n \n     shape = int_shape(x)\n     num_dynamic_axis = _get_dynamic_axis_num(x)\n-    # Padding the axis\n-    if len(n) < len(shape):\n+    if len(n) < len(shape):  # Padding the axis\n         n = tuple([1 for _ in range(len(shape) - len(n))]) + n\n-\n-    if len(n) != len(shape):\n+    elif len(n) != len(shape):\n         raise NotImplementedError\n \n     i = num_dynamic_axis\n\n@@ -2385,9 +2385,31 @@ def tile(x, n):\n \n     # Returns\n         A tiled tensor.\n+\n+    # Example\n+    ```python\n+        >>> from keras import backend as K\n+        >>> kvar = K.variable(np.random.random((2, 3)))\n+        >>> kvar_tile = K.tile(K.eye(2), (2, 3))\n+        >>> K.eval(kvar_tile)\n+        array([[1., 0., 1., 0., 1., 0.],\n+               [0., 1., 0., 1., 0., 1.],\n+               [1., 0., 1., 0., 1., 0.],\n+               [0., 1., 0., 1., 0., 1.]], dtype=float32)\n+    ```\n+    {{np_implementation}}\n     \"\"\"\n     if isinstance(n, int):\n-        n = [n]\n+        n = (n,)\n+    elif isinstance(n, list):\n+        n = tuple(n)\n+\n+    shape = int_shape(x)\n+    if len(n) < len(shape):  # Padding the axis\n+        n = tuple([1 for _ in range(len(shape) - len(n))]) + n\n+    elif len(n) != len(shape):\n+        raise NotImplementedError\n+\n     return tf.tile(x, n)\n \n \n\n@@ -101,16 +101,6 @@ def to_dense(tensor):\n         return tensor\n \n \n-def _is_explicit_shape(shape):\n-    if hasattr(shape, '__iter__'):\n-        for x in shape:\n-            if x is not None:\n-                if not isinstance(x, int):\n-                    return False\n-        return True\n-    return False\n-\n-\n NAME_SCOPE_STACK = []\n \n \n@@ -1066,33 +1056,22 @@ def arange(start, stop=None, step=1, dtype='int32'):\n \n \n def tile(x, n):\n+    if isinstance(n, int):\n+        n = (n,)\n+    elif isinstance(n, list):\n+        n = tuple(n)\n+\n     y = T.tile(x, n)\n-    if hasattr(x, '_keras_shape'):\n-        if _is_explicit_shape(n):\n-            output_shape = x._keras_shape[:-len(n)]\n-            for i, j in zip(x._keras_shape, n):\n-                if i is None:\n-                    output_shape += (None,)\n-                else:\n-                    output_shape += (i * j,)\n-        elif isinstance(n, int):\n-            output_shape = x._keras_shape[:-1]\n-            if x._keras_shape[-1] is None:\n-                output_shape += (None,)\n-            else:\n-                output_shape += (x._keras_shape[-1] * n,)\n-        else:\n-            # symbolic n\n-            if n.ndim == 0:\n-                # n is a scalar\n-                output_shape = x._keras_shape[:-1] + (None,)\n-            elif hasattr(n, '_keras_shape'):\n-                # n is a vector\n-                n_size = n._keras_shape[0]\n-                output_shape = x._keras_shape[:-n_size] + (None,) * n_size\n-            else:\n-                output_shape = (None,) * x.ndim\n-        y._keras_shape = output_shape\n+    shape = int_shape(x)\n+    if shape is None:\n+        return y\n+    elif len(n) < len(shape):  # Padding the axis\n+        n = tuple([1 for _ in range(len(shape) - len(n))]) + n\n+    elif len(n) != len(shape):\n+        raise NotImplementedError\n+\n+    y._keras_shape = tuple([None if a is None else a * b\n+                            for (a, b) in zip(shape, n)])\n     return y\n \n \n\n@@ -428,10 +428,11 @@ class TestBackend(object):\n                     assert y._keras_shape == K.int_shape(y)\n \n     def test_tile(self):\n-        shape = (3, 4)\n-        arr = np.arange(np.prod(shape)).reshape(shape)\n-        check_single_tensor_operation('tile', arr, WITH_NP, n=[2, 1])\n-        check_single_tensor_operation('tile', (2, 5), WITH_NP, n=[5, 2])\n+        check_single_tensor_operation('tile', (3, 4), WITH_NP, n=2)\n+        check_single_tensor_operation('tile', (3, 4), WITH_NP, n=(2, 1))\n+        check_single_tensor_operation('tile', (3, 4, 5), WITH_NP, n=2)\n+        check_single_tensor_operation('tile', (3, 4, 5), WITH_NP, n=(1, 2))\n+        check_single_tensor_operation('tile', (3, 4, 5), WITH_NP, n=(3, 1, 2))\n \n         # test theano shape inference when\n         # input shape has None entries\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d7c0e952f7a3d0dbf7d9489a8b5c959e04bd9379", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 29 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 45 | Churn Cumulative: 6807 | Contributors (this commit): 38 | Commits (past 90d): 28 | Contributors (cumulative): 46 | DMM Complexity: 1.0\n\nDIFF:\n@@ -372,7 +372,7 @@ def std(x, axis=None, keepdims=False):\n def logsumexp(x, axis=None, keepdims=False):\n     if isinstance(axis, list):\n         axis = tuple(axis)\n-    return sp.misc.logsumexp(x, axis=axis, keepdims=keepdims)\n+    return sp.special.logsumexp(x, axis=axis, keepdims=keepdims)\n \n \n def sum(x, axis=None, keepdims=False):\n\n@@ -507,11 +507,6 @@ class TestBackend(object):\n         check_single_tensor_operation('std', (4, 2), WITH_NP, axis=1, keepdims=True)\n         check_single_tensor_operation('std', (4, 2, 3), WITH_NP, axis=[1, -1])\n \n-        check_single_tensor_operation('logsumexp', (4, 2), WITH_NP)\n-        check_single_tensor_operation('logsumexp', (4, 2),\n-                                      WITH_NP, axis=1, keepdims=True)\n-        check_single_tensor_operation('logsumexp', (4, 2, 3), WITH_NP, axis=[1, -1])\n-\n         check_single_tensor_operation('prod', (4, 2), WITH_NP)\n         check_single_tensor_operation('prod', (4, 2), WITH_NP, axis=1, keepdims=True)\n         check_single_tensor_operation('prod', (4, 2, 3), WITH_NP, axis=[1, -1])\n@@ -1023,29 +1018,21 @@ class TestBackend(object):\n             # not updated last timestep:\n             assert_allclose(K.eval(last_states[0]), expected_last_state)\n \n-    @pytest.mark.parametrize('x_np,axis,keepdims', [\n-        (np.array([1.1, 0.8, 0.9]), 0, False),\n-        (np.array([[1.1, 0.8, 0.9]]), 0, False),\n-        (np.array([[1.1, 0.8, 0.9]]), 1, False),\n-        (np.array([[1.1, 0.8, 0.9]]), -1, False),\n-        (np.array([[1.1, 0.8, 0.9]]), 1, True),\n-        (np.array([[1.1], [1.2]]), 0, False),\n-        (np.array([[1.1], [1.2]]), 1, False),\n-        (np.array([[1.1], [1.2]]), -1, False),\n-        (np.array([[1.1], [1.2]]), -1, True),\n-        (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), None, False),\n-        (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 0, False),\n-        (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 1, False),\n-        (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), -1, False),\n-    ])\n-    def test_logsumexp(self, x_np, axis, keepdims):\n-        '''\n-        Check if K.logsumexp works properly for values close to one.\n-        '''\n-        x = K.variable(x_np)\n-        assert_allclose(K.eval(K.logsumexp(x, axis=axis, keepdims=keepdims)),\n-                        np.log(np.sum(np.exp(x_np), axis=axis, keepdims=keepdims)),\n-                        rtol=1e-5)\n+    @pytest.mark.parametrize('shape', [(3, ), (1, 3), (2, 1), (4, 2), (4, 2, 3)])\n+    def test_logsumexp(self, shape):\n+        check_single_tensor_operation('logsumexp', shape, WITH_NP, axis=None)\n+        check_single_tensor_operation('logsumexp', shape, WITH_NP, axis=0)\n+        check_single_tensor_operation('logsumexp', shape, WITH_NP, axis=-1)\n+        check_single_tensor_operation('logsumexp', shape, WITH_NP, axis=-1,\n+                                      keepdims=True)\n+        if len(shape) > 1:\n+            check_single_tensor_operation('logsumexp', shape, WITH_NP, axis=1)\n+            check_single_tensor_operation('logsumexp', shape, WITH_NP, axis=1,\n+                                          keepdims=True)\n+        if len(shape) > 2:\n+            check_single_tensor_operation('logsumexp', shape, WITH_NP, axis=[1, -1])\n+            check_single_tensor_operation('logsumexp', shape, WITH_NP, axis=[1, -1],\n+                                          keepdims=True)\n \n     @pytest.mark.skipif(K.backend() != 'tensorflow',\n                         reason='The optimization is applied only with TensorFlow.')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ee3d6a6465df01d27cbf3bb3c5bace05298756e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 9 | Churn Cumulative: 1868 | Contributors (this commit): 14 | Commits (past 90d): 5 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -33,6 +33,11 @@ try:\n except ImportError:\n     tf_file_io = None\n \n+try:\n+    getargspec = inspect.getfullargspec\n+except AttributeError:  # getargspec() is deprecated since Python 3.0\n+    getargspec = inspect.getargspec\n+\n \n def _serialize_model(model, h5dict, include_optimizer=True):\n     \"\"\"Model serialization logic.\n@@ -422,7 +427,7 @@ def allow_read_from_gcs(load_function):\n         if name in kwargs:\n             arg = kwargs.pop(name)\n             return arg, args, kwargs\n-        argnames = inspect.getargspec(f)[0]\n+        argnames = getargspec(f)[0]\n         for i, (argname, arg) in enumerate(zip(argnames, args)):\n             if argname == name:\n                 return arg, args[:i] + args[i + 1:], kwargs\n@@ -587,7 +592,7 @@ def model_from_yaml(yaml_string, custom_objects=None):\n     # Returns\n         A Keras model instance (uncompiled).\n     \"\"\"\n-    config = yaml.load(yaml_string)\n+    config = yaml.load(yaml_string, Loader=yaml.FullLoader)\n     from ..layers import deserialize\n     return deserialize(config, custom_objects=custom_objects)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a11bf06aa756f43f740f5c51b8ffbf722ad8c1ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1307 | Contributors (this commit): 21 | Commits (past 90d): 2 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -586,6 +586,7 @@ def test_saving_model_with_long_weights_names():\n     f = x\n     for i in range(4):\n         f = Dense(2, name='nested_model_dense_%d' % (i,))(f)\n+    f = Dense(2, name='nested_model_dense_4', trainable=False)(f)\n     # This layer name will make the `weights_name`\n     # HDF5 attribute blow out of proportion.\n     f = Dense(2, name='nested_model_output' + ('x' * (2**15)))(f)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a374c93092341f24027f60ffe32d56d2bff29b83", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 373 | Contributors (this commit): 21 | Commits (past 90d): 2 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -130,12 +130,26 @@ def relu(x, alpha=0., max_value=None, threshold=0.):\n \n def tanh(x):\n     \"\"\"Hyperbolic tangent activation function.\n+\n+    # Arguments\n+        x: Input tensor.\n+\n+    # Returns\n+        The hyperbolic activation:\n+        `tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`\n+\n     \"\"\"\n     return K.tanh(x)\n \n \n def sigmoid(x):\n     \"\"\"Sigmoid activation function.\n+\n+    # Arguments\n+        x: Input tensor.\n+\n+    # Returns\n+        The sigmoid activation: `1 / (1 + exp(-x))`.\n     \"\"\"\n     return K.sigmoid(x)\n \n@@ -160,12 +174,24 @@ def hard_sigmoid(x):\n \n def exponential(x):\n     \"\"\"Exponential (base e) activation function.\n+\n+    # Arguments:\n+        x: Input tensor.\n+\n+    # Returns\n+        Exponential activation: `exp(x)`.\n     \"\"\"\n     return K.exp(x)\n \n \n def linear(x):\n     \"\"\"Linear (i.e. identity) activation function.\n+\n+    # Arguments\n+        x: Input tensor.\n+\n+    # Returns\n+        Input tensor, unchanged.\n     \"\"\"\n     return x\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f0eb8d538c82798944346b4b2df917a06bf5e9d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 58 | Files Changed: 1 | Hunks: 29 | Methods Changed: 6 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 127 | Churn Cumulative: 395 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,59 +1,68 @@\n \"\"\"\n-#Train a simple CNN-Capsule Network on the CIFAR10 small images dataset.\n+This example trains a simple CNN-Capsule Network on the CIFAR10 data set.\n \n Without Data Augmentation:\n-It gets to 75% validation accuracy in 10 epochs,\n-and 79% after 15 epochs, and overfitting after 20 epochs\n+It gets to 75% validation accuracy in 10 epochs, 79% after 15 epochs,\n+and overfitting after 20 epochs\n \n With Data Augmentation:\n-It gets to 75% validation accuracy in 10 epochs,\n-and 79% after 15 epochs, and 83% after 30 epochs.\n-In my test, highest validation accuracy is 83.79% after 50 epochs.\n+It gets to 75% validation accuracy in 10 epochs, 79% after 15 epochs,\n+and 83% after 30 epochs.\n \n-This is a fast Implement, just 20s/epoch with a gtx 1070 gpu.\n+The highest achieved validation accuracy is 83.79% after 50 epochs.\n+This is a fast implementation that takes just 20s/epoch on a GTX 1070 GPU.\n+\n+The paper \"Dynamic Routing Between Capsules\": https://arxiv.org/abs/1710.09829\n \"\"\"\n-\n from __future__ import print_function\n-from keras import backend as K\n-from keras.layers import Layer\n+\n from keras import activations\n+from keras import backend as K\n+from keras import layers\n from keras import utils\n from keras.datasets import cifar10\n from keras.models import Model\n-from keras.layers import *\n from keras.preprocessing.image import ImageDataGenerator\n \n \n-# the squashing function.\n-# we use 0.5 in stead of 1 in hinton's paper.\n-# if 1, the norm of vector will be zoomed out.\n-# if 0.5, the norm will be zoomed in while original norm is less than 0.5\n-# and be zoomed out while original norm is greater than 0.5.\n def squash(x, axis=-1):\n+    \"\"\"The Squashing Function.\n+    The nonlinear activation function used in Capsule Network\n+    # Arguments\n+        x: Input Tensor.\n+        axis: Integer axis along which the squashing function is to be applied.\n+\n+    # Returns\n+        Tensor with scaled value of the input tensor\n+    \"\"\"\n     s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n     scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n     return scale * x\n \n \n-# define our own softmax function instead of K.softmax\n-# because K.softmax can not specify axis.\n-def softmax(x, axis=-1):\n-    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n-    return ex / K.sum(ex, axis=axis, keepdims=True)\n-\n-\n-# define the margin loss like hinge loss\n def margin_loss(y_true, y_pred):\n+    \"\"\"Margin loss\n+\n+    # Arguments\n+        y_true: tensor of true targets.\n+        y_pred: tensor of predicted targets.\n+\n+    # Returns\n+        Tensor with one scalar loss entry per sample.\n+    \"\"\"\n     lamb, margin = 0.5, 0.1\n     return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n         1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n \n \n-class Capsule(Layer):\n-    \"\"\"A Capsule Implement with Pure Keras\n-    There are two vesions of Capsule.\n-    One is like dense layer (for the fixed-shape input),\n-    and the other is like timedistributed dense (for various length input).\n+class Capsule(layers.Layer):\n+    \"\"\"Capsule Network\n+\n+    A Capsule Network Layer implementation in Keras\n+    There are two versions of Capsule Networks.\n+    One is similar to dense layer (for the fixed-shape input),\n+    and the other is similar to time distributed dense layer\n+    (for inputs of varied length).\n \n     The input shape of Capsule must be (batch_size,\n                                         input_num_capsule,\n@@ -63,9 +72,15 @@ class Capsule(Layer):\n                              num_capsule,\n                              dim_capsule\n                             )\n+    The Capsule implementation is from https://github.com/bojone/Capsule/\n \n-    Capsule Implement is from https://github.com/bojone/Capsule/\n-    Capsule Paper: https://arxiv.org/abs/1710.09829\n+\n+    # Arguments\n+        num_capsule: An integer, the number of capsules.\n+        dim_capsule: An integer, the dimensions of the capsule.\n+        routings: An integer, the number of routings.\n+        share_weights: A boolean, sets weight sharing between layers.\n+        activation: A string, the activation function to be applied.\n     \"\"\"\n \n     def __init__(self,\n@@ -103,17 +118,17 @@ class Capsule(Layer):\n                 initializer='glorot_uniform',\n                 trainable=True)\n \n-    def call(self, inputs):\n+    def call(self, inputs, **kwargs):\n         \"\"\"Following the routing algorithm from Hinton's paper,\n         but replace b = b + <u,v> with b = <u,v>.\n \n-        This change can improve the feature representation of Capsule.\n+        This change can improve the feature representation of the capsule.\n \n         However, you can replace\n             b = K.batch_dot(outputs, hat_inputs, [2, 3])\n         with\n             b += K.batch_dot(outputs, hat_inputs, [2, 3])\n-        to realize a standard routing.\n+        to get standard routing.\n         \"\"\"\n \n         if self.share_weights:\n@@ -129,18 +144,18 @@ class Capsule(Layer):\n         hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n \n         b = K.zeros_like(hat_inputs[:, :, :, 0])\n+        print(self.routings)\n         for i in range(self.routings):\n-            c = softmax(b, 1)\n+            c = K.softmax(b, 1)\n             o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n             if i < self.routings - 1:\n                 b = K.batch_dot(o, hat_inputs, [2, 3])\n                 if K.backend() == 'theano':\n                     o = K.sum(o, axis=1)\n-\n         return o\n \n     def compute_output_shape(self, input_shape):\n-        return (None, self.num_capsule, self.dim_capsule)\n+        return None, self.num_capsule, self.dim_capsule\n \n \n batch_size = 128\n@@ -155,34 +170,30 @@ x_test /= 255\n y_train = utils.to_categorical(y_train, num_classes)\n y_test = utils.to_categorical(y_test, num_classes)\n \n-# A common Conv2D model\n-input_image = Input(shape=(None, None, 3))\n-x = Conv2D(64, (3, 3), activation='relu')(input_image)\n-x = Conv2D(64, (3, 3), activation='relu')(x)\n-x = AveragePooling2D((2, 2))(x)\n-x = Conv2D(128, (3, 3), activation='relu')(x)\n-x = Conv2D(128, (3, 3), activation='relu')(x)\n+# A simple Conv2D model\n+input_image = layers.Input(shape=(None, None, 3))\n+x = layers.Conv2D(64, (3, 3), activation='relu')(input_image)\n+x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n+x = layers.AveragePooling2D((2, 2))(x)\n+x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n+x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n \n+# Now, we reshape it to (batch_size, input_num_capsule, input_dim_capsule)\n+# then connect a capsule layer.\n+# The output of final model is the lengths of 10 capsules, which have 16 dimensions.\n+# The length of the output vector of the capsule expresses the probability of\n+# existence of the entity, so the problem becomes a 10 two-classification problem.\n \n-\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\n-then connect a Capsule layer.\n-\n-the output of final model is the lengths of 10 Capsule, whose dim=16.\n-\n-the length of Capsule is the proba,\n-so the problem becomes a 10 two-classification problem.\n-\"\"\"\n-\n-x = Reshape((-1, 128))(x)\n+x = layers.Reshape((-1, 128))(x)\n capsule = Capsule(10, 16, 3, True)(x)\n-output = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\n+output = layers.Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\n model = Model(inputs=input_image, outputs=output)\n \n-# we use a margin loss\n+# Margin loss is used\n model.compile(loss=margin_loss, optimizer='adam', metrics=['accuracy'])\n model.summary()\n \n-# we can compare the performance with or without data augmentation\n+# Compare the performance with and without data augmentation\n data_augmentation = True\n \n if not data_augmentation:\n@@ -196,7 +207,7 @@ if not data_augmentation:\n         shuffle=True)\n else:\n     print('Using real-time data augmentation.')\n-    # This will do preprocessing and realtime data augmentation:\n+    # This will do preprocessing and real-time data augmentation:\n     datagen = ImageDataGenerator(\n         featurewise_center=False,  # set input mean to 0 over the dataset\n         samplewise_center=False,  # set each sample mean to 0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#86cb6b6072261290d2fcb7cdb6580c5e63f8e5ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 3860 | Contributors (this commit): 34 | Commits (past 90d): 11 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -1879,8 +1879,6 @@ def pool2d(x, pool_size, strides=(1, 1),\n     data_format = normalize_data_format(data_format)\n \n     padding = _preprocess_border_mode(padding)\n-    strides = strides\n-    pool_size = pool_size\n     x = _preprocess_conv2d_input(x, data_format)\n     if pool_mode == 'max':\n         x = C.pooling(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9bd69784e31f17ee3501167c267dad5c64f25de8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 12065 | Contributors (this commit): 68 | Commits (past 90d): 2 | Contributors (cumulative): 68 | DMM Complexity: None\n\nDIFF:\n@@ -2240,8 +2240,8 @@ class ZeroPadding3D(_ZeroPadding):\n             - If int: the same symmetric padding\n                 is applied to height and width.\n             - If tuple of 3 ints:\n-                interpreted as two different\n-                symmetric padding values for height and width:\n+                interpreted as three different\n+                symmetric padding values for depth, height, and width:\n                 `(symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad)`.\n             - If tuple of 3 tuples of 2 ints:\n                 interpreted as\n@@ -2496,7 +2496,7 @@ class Cropping3D(_Cropping):\n             - If int: the same symmetric cropping\n                 is applied to depth, height, and width.\n             - If tuple of 3 ints:\n-                interpreted as two different\n+                interpreted as three different\n                 symmetric cropping values for depth, height, and width:\n                 `(symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop)`.\n             - If tuple of 3 tuples of 2 ints:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8e23a3ec47a2ccbf6cdd222a80886c6b9f17264f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 100 | Lines Deleted: 67 | Files Changed: 5 | Hunks: 38 | Methods Changed: 17 | Complexity Δ (Sum/Max): -54/5 | Churn Δ: 167 | Churn Cumulative: 28779 | Contributors (this commit): 165 | Commits (past 90d): 53 | Contributors (cumulative): 278 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,6 +14,7 @@ from tensorflow.python.ops import functional_ops\n from tensorflow.python.ops import ctc_ops as ctc\n from .common import floatx, epsilon, image_data_format\n \n+import sys\n import functools\n import threading\n \n@@ -1203,7 +1204,9 @@ def update(x, new_x):\n     # Returns\n         The variable `x` updated.\n     \"\"\"\n-    return tf_state_ops.assign(x, new_x)\n+    op = tf_state_ops.assign(x, new_x)\n+    with tf.control_dependencies([op]):\n+        return tf.identity(x)\n \n \n @symbolic\n@@ -1217,7 +1220,9 @@ def update_add(x, increment):\n     # Returns\n         The variable `x` updated.\n     \"\"\"\n-    return tf_state_ops.assign_add(x, increment)\n+    op = tf_state_ops.assign_add(x, increment)\n+    with tf.control_dependencies([op]):\n+        return tf.identity(x)\n \n \n @symbolic\n@@ -1231,7 +1236,9 @@ def update_sub(x, decrement):\n     # Returns\n         The variable `x` updated.\n     \"\"\"\n-    return tf_state_ops.assign_sub(x, decrement)\n+    op = tf_state_ops.assign_sub(x, decrement)\n+    with tf.control_dependencies([op]):\n+        return tf.identity(x)\n \n \n @symbolic\n@@ -2880,6 +2887,7 @@ def get_variable_shape(x):\n     return int_shape(x)\n \n \n+@symbolic\n def print_tensor(x, message=''):\n     \"\"\"Prints `message` and the tensor value when evaluated.\n \n@@ -2899,8 +2907,9 @@ def print_tensor(x, message=''):\n     # Returns\n         The same tensor `x`, unchanged.\n     \"\"\"\n-    # TODO\n-    return tf.Print(x, [x], message)\n+    op = tf.print(message, x, output_stream=sys.stdout)\n+    with tf.control_dependencies([op]):\n+        return tf.identity(x)\n \n \n # GRAPH MANIPULATION\n\n@@ -1378,8 +1378,7 @@ def get_variable_shape(x):\n \n \n def print_tensor(x, message=''):\n-    \"\"\"Print the message and the tensor when evaluated and return the same\n-    tensor.\n+    \"\"\"Print the message & the tensor when evaluated & return the same tensor.\n     \"\"\"\n     p_op = Print(message)\n     return p_op(x)\n\n@@ -80,8 +80,11 @@ class RandomNormal(Initializer):\n         self.seed = seed\n \n     def __call__(self, shape, dtype=None):\n-        return K.random_normal(shape, self.mean, self.stddev,\n+        x = K.random_normal(shape, self.mean, self.stddev,\n                             dtype=dtype, seed=self.seed)\n+        if self.seed is not None:\n+            self.seed += 1\n+        return x\n \n     def get_config(self):\n         return {\n@@ -108,8 +111,11 @@ class RandomUniform(Initializer):\n         self.seed = seed\n \n     def __call__(self, shape, dtype=None):\n-        return K.random_uniform(shape, self.minval, self.maxval,\n+        x = K.random_uniform(shape, self.minval, self.maxval,\n                              dtype=dtype, seed=self.seed)\n+        if self.seed is not None:\n+            self.seed += 1\n+        return x\n \n     def get_config(self):\n         return {\n@@ -141,8 +147,11 @@ class TruncatedNormal(Initializer):\n         self.seed = seed\n \n     def __call__(self, shape, dtype=None):\n-        return K.truncated_normal(shape, self.mean, self.stddev,\n+        x = K.truncated_normal(shape, self.mean, self.stddev,\n                                dtype=dtype, seed=self.seed)\n+        if self.seed is not None:\n+            self.seed += 1\n+        return x\n \n     def get_config(self):\n         return {\n@@ -210,12 +219,15 @@ class VarianceScaling(Initializer):\n         if self.distribution == 'normal':\n             # 0.879... = scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\n             stddev = np.sqrt(scale) / .87962566103423978\n-            return K.truncated_normal(shape, 0., stddev,\n+            x = K.truncated_normal(shape, 0., stddev,\n                                    dtype=dtype, seed=self.seed)\n         else:\n             limit = np.sqrt(3. * scale)\n-            return K.random_uniform(shape, -limit, limit,\n+            x = K.random_uniform(shape, -limit, limit,\n                                  dtype=dtype, seed=self.seed)\n+        if self.seed is not None:\n+            self.seed += 1\n+        return x\n \n     def get_config(self):\n         return {\n@@ -251,6 +263,7 @@ class Orthogonal(Initializer):\n         rng = np.random\n         if self.seed is not None:\n             rng = np.random.RandomState(self.seed)\n+            self.seed += 1\n         a = rng.normal(0.0, 1.0, flat_shape)\n         u, _, v = np.linalg.svd(a, full_matrices=False)\n         # Pick the one with the correct shape.\n\n@@ -480,11 +480,18 @@ class TestBackend(object):\n         else:\n             assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)\n \n-    def test_print_tensor(self):\n+    def test_print_tensor(self, capsys):\n+        for k in [KTH, KTF]:\n+            x = k.placeholder((1, 1))\n+            y = k.print_tensor(x, 'msg')\n+            fn = k.function([x], [y])\n+            _ = fn([np.ones((1, 1))])\n+            out, err = capsys.readouterr()\n+            # Theano inserts \"__str__ = \" for no good reason\n+            assert out.replace('__str__ = ', '') == 'msg [[1.]]\\n'\n+\n         check_single_tensor_operation('print_tensor', (), WITH_NP)\n         check_single_tensor_operation('print_tensor', (2,), WITH_NP)\n-        check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)\n-        check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)\n \n     def test_elementwise_operations(self):\n         check_single_tensor_operation('max', (4, 2), WITH_NP)\n@@ -581,27 +588,39 @@ class TestBackend(object):\n \n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n-    def test_update_add(self):\n-        x = np.random.randn(3, 4)\n+    def test_update(self):\n+        x = np.ones((3, 4))\n         x_var = K.variable(x)\n-        increment = np.random.randn(3, 4)\n+        new_x = np.random.random((3, 4))\n \n-        x += increment\n-        K.eval(K.update_add(x_var, increment))\n+        op = K.update(x_var, new_x)\n+        K.eval(op)\n \n-        assert_allclose(x, K.eval(x_var), atol=1e-05)\n+        assert_allclose(new_x, K.eval(x_var), atol=1e-05)\n+\n+    @pytest.mark.skipif(K.backend() == 'theano',\n+                        reason='theano returns tuples for update ops')\n+    def test_update_add(self):\n+        x = np.ones((3, 4))\n+        x_var = K.variable(x)\n+        increment = np.random.random((3, 4))\n+\n+        op = K.update_add(x_var, increment)\n+        K.eval(op)\n+\n+        assert_allclose(x + increment, K.eval(x_var), atol=1e-05)\n \n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_sub(self):\n-        x = np.random.randn(3, 4)\n+        x = np.ones((3, 4))\n         x_var = K.variable(x)\n-        decrement = np.random.randn(3, 4)\n+        decrement = np.random.random((3, 4))\n \n-        x -= decrement\n-        K.eval(K.update_sub(x_var, decrement))\n+        op = K.update_sub(x_var, decrement)\n+        K.eval(op)\n \n-        assert_allclose(x, K.eval(x_var), atol=1e-05)\n+        assert_allclose(x - decrement, K.eval(x_var), atol=1e-05)\n \n     @pytest.mark.skipif(K.backend() == 'cntk',\n                         reason='cntk doesn\\'t support gradient in this way.')\n@@ -712,7 +731,7 @@ class TestBackend(object):\n         assert output == [21.]\n         assert K.get_session().run(fetches=[x, y]) == [30., 40.]\n \n-    @pytest.mark.skipif(K.backend() != 'tensorflow',\n+    @pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(),\n                         reason='Uses the `options` and `run_metadata` arguments.')\n     def test_function_tf_run_options_with_run_metadata(self):\n         from tensorflow.core.protobuf import config_pb2\n@@ -1363,58 +1382,41 @@ class TestBackend(object):\n         assert_allclose(y1, y2, atol=1e-05)\n \n     def test_random_normal(self):\n-        # test standard normal as well as a normal with a different set of parameters\n+        # TODO: make this a parameterized test\n         for mean, std in [(0., 1.), (-10., 5.)]:\n-            rand = K.eval(K.random_normal((300, 200),\n-                                          mean=mean, stddev=std, seed=1337))\n-            assert rand.shape == (300, 200)\n+            rand = K.eval(K.random_normal((200, 200),\n+                                          mean=mean,\n+                                          stddev=std))\n+            assert rand.shape == (200, 200)\n             assert np.abs(np.mean(rand) - mean) < std * 0.015\n             assert np.abs(np.std(rand) - std) < std * 0.015\n \n-            # test that random_normal also generates different values when used\n-            # within a function\n-            r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)\n-            samples = np.array([K.eval(r) for _ in range(200)])\n-            assert np.abs(np.mean(samples) - mean) < std * 0.015\n-            assert np.abs(np.std(samples) - std) < std * 0.015\n-\n     def test_random_uniform(self):\n         min_val = -1.\n         max_val = 1.\n-        rand = K.eval(K.random_uniform((200, 100), min_val, max_val))\n-        assert rand.shape == (200, 100)\n+        rand = K.eval(K.random_uniform((200, 200), min_val, max_val))\n+        assert rand.shape == (200, 200)\n         assert np.abs(np.mean(rand)) < 0.015\n         assert max_val - 0.015 < np.max(rand) <= max_val\n         assert min_val + 0.015 > np.min(rand) >= min_val\n \n-        r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)\n-        samples = np.array([K.eval(r) for _ in range(200)])\n-        assert np.abs(np.mean(samples)) < 0.015\n-        assert max_val - 0.015 < np.max(samples) <= max_val\n-        assert min_val + 0.015 > np.min(samples) >= min_val\n-\n     def test_random_binomial(self):\n         p = 0.5\n-        rand = K.eval(K.random_binomial((200, 100), p))\n-        assert rand.shape == (200, 100)\n+        rand = K.eval(K.random_binomial((200, 200), p))\n+        assert rand.shape == (200, 200)\n         assert np.abs(np.mean(rand) - p) < 0.015\n         assert np.max(rand) == 1\n         assert np.min(rand) == 0\n \n-        r = K.random_binomial((10, 10), p)\n-        samples = np.array([K.eval(r) for _ in range(200)])\n-        assert np.abs(np.mean(samples) - p) < 0.015\n-        assert np.max(samples) == 1\n-        assert np.min(samples) == 0\n-\n     def test_truncated_normal(self):\n         mean = 0.\n         std = 1.\n         min_val = -2.\n         max_val = 2.\n-        rand = K.eval(K.truncated_normal((300, 200),\n-                                         mean=mean, stddev=std, seed=1337))\n-        assert rand.shape == (300, 200)\n+        rand = K.eval(K.truncated_normal((200, 200),\n+                                         mean=mean,\n+                                         stddev=std))\n+        assert rand.shape == (200, 200)\n         assert np.abs(np.mean(rand) - mean) < 0.015\n         assert np.max(rand) <= max_val\n         assert np.min(rand) >= min_val\n@@ -2122,16 +2124,6 @@ class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n \n-    @pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(),\n-                        reason='This test is for tensorflow parallelism.')\n-    def test_tensorflow_session_parallelism_settings(self, monkeypatch):\n-        for threads in [1, 2]:\n-            K.clear_session()\n-            monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n-            cfg = K.get_session()._config\n-            assert cfg.intra_op_parallelism_threads == threads\n-            assert cfg.inter_op_parallelism_threads == threads\n-\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -142,5 +142,25 @@ def test_one(tensor_shape):\n             target_mean=1., target_max=1.)\n \n \n+@pytest.mark.parametrize('initializer',\n+                         [initializers.orthogonal,\n+                          initializers.uniform,\n+                          initializers.normal,\n+                          initializers.truncated_normal,\n+                          initializers.VarianceScaling],\n+                         ids=['orthogonal',\n+                              'uniform',\n+                              'normal',\n+                              'truncated_normal',\n+                              'variance_scaling'])\n+def test_statefulness(initializer):\n+    # Test that calling a same seeded random initializer\n+    # in succession results in different values.\n+    init = initializer(seed=1337)\n+    samples = [init((2, 2)) for _ in range(2)]\n+    samples = [K.get_value(K.variable(x)) for x in samples]\n+    assert np.mean(np.abs(samples[0] - samples[1])) > 0.\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
