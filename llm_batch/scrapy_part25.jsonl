{"custom_id": "scrapy#eff09f2f7808a6887b60c8e9e383eb477d7b2ed6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 81 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,6 +20,10 @@ class Command(ScrapyCommand):\n         parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", help=\"avoid using pygments to colorize the output\")\n \n     def pipeline_process(self, item, opts):\n+        item.set_guid(spider)\n+        for key in item.__dict__.keys():\n+           if key.startswith('_'):\n+               item.__dict__.pop(key, None)\n         return item\n \n     def run_method(self, response, method, args, opts):\n@@ -39,10 +43,6 @@ class Command(ScrapyCommand):\n             result = method_fcn(response)\n             links = [i for i in result if isinstance(i, Request)]\n             items = [self.pipeline_process(i, opts) for i in result if isinstance(i, ScrapedItem)]\n-            for item in items:\n-                for key in item.__dict__.keys():\n-                    if key.startswith('_'):\n-                        item.__dict__.pop(key, None)\n \n         return items, links\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c75ac38b9288624bf29a8f306e7a02c70dc0cf05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 86 | Lines Deleted: 98 | Files Changed: 4 | Hunks: 14 | Methods Changed: 17 | Complexity Δ (Sum/Max): -2/14 | Churn Δ: 184 | Churn Cumulative: 613 | Contributors (this commit): 2 | Commits (past 90d): 19 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,46 +1,95 @@\n-from scrapy.command.commands.parse_method import Command as ScrapyCommand\n+from scrapy.command import ScrapyCommand\n from scrapy.fetcher import fetch\n+from scrapy.http import Request\n+from scrapy.item import ScrapedItem\n from scrapy.spider import spiders\n+from scrapy.utils import display\n from scrapy import log\n \n class Command(ScrapyCommand):\n     def syntax(self):\n-        return \"[options] <url>\"\n+        return \"[options] <url> <method>\"\n \n     def short_desc(self):\n-        return \"Parse the URL and print its results\"\n+        return \"Parse the URL with the given spider method and print the results\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--identify\", dest=\"identify\", action=\"store_true\", help=\"try to use identify instead of parse\")\n+        parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\", help=\"don't show extracted links\")\n+        parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", help=\"don't show scraped items\")\n+        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", help=\"avoid using pygments to colorize the output\")\n+        parser.add_option(\"--matches\", dest=\"matches\", action=\"store_true\", help=\"avoid using pygments to colorize the output\")\n+\n+    def pipeline_process(self, item, spider, opts):\n+        return item\n+\n+    def run_method(self, spider, response, method, args, opts):\n+        spider = spiders.fromurl(response.url)\n+        if not spider:\n+            log.msg('Couldnt find spider for url: %s' % response.url, level=log.ERROR)\n+            return (), ()\n+\n+        items = []\n+        links = []\n+        if method:\n+            method_fcn = method if callable(method) else getattr(spider, method, None)\n+            if not method_fcn:\n+                log.msg('Couldnt find method %s in %s spider' % (method, spider.domain_name))\n+                return (), ()\n+\n+            result = method_fcn(response)\n+            links = [i for i in result if isinstance(i, Request)]\n+            items = [self.pipeline_process(i, spider, opts) for i in result if isinstance(i, ScrapedItem)]\n+\n+        return items, links\n+\n+    def print_results(self, items, links, opts):\n+        display.nocolour = opts.nocolour\n+        if not opts.noitems:\n+            for item in items:\n+                for key in item.__dict__.keys():\n+                   if key.startswith('_'):\n+                       item.__dict__.pop(key, None)\n+            print \"# Scraped Items\", \"-\"*60\n+            display.pprint(list(items))\n+\n+        if not opts.nolinks:\n+            print \"# Links\", \"-\"*68\n+            display.pprint(list(links))\n \n     def run(self, args, opts):\n-        if not args:\n-            print \"A URL is required\"\n+        if opts.matches:\n+            url = args[0]\n+            method = None            \n+        else:\n+            if len(args) < 2:\n+                print \"A URL and method is required\"\n                 return\n+            else:\n+                url, method = args[:2]\n \n         items = set()\n         links = set()\n-        responses = fetch(args)\n-        for response in responses:\n+        for response in fetch([url]):\n             spider = spiders.fromurl(response.url)\n-            if spider:\n-                if opts.identify and hasattr(spider, 'identify_products'):\n-                    ret_items, ret_links = ScrapyCommand.run_method(self, response, 'identify_products', args, opts)\n-                else:\n-                    if hasattr(spider, 'rules'):\n-                        for rule in spider.rules:\n-                            if rule.link_extractor.match(response.url):\n-                                ret_items, ret_links = ScrapyCommand.run_method(self, response, rule.callback, args, opts)\n-                                break\n-                    else:\n-                        ret_items, ret_links = ScrapyCommand.run_method(self, response, 'parse', args, opts)\n-                    items = items.union(ret_items)\n-                    links = links.union(ret_links)\n-            else:\n+            if not spider:\n                 log.msg('Couldnt find spider for \"%s\"' % response.url)\n                 continue\n \n-        if items or links:\n+            if method:\n+                ret_items, ret_links = self.run_method(spider, response, method, args, opts)\n+                items = items.union(ret_items)\n+                links = links.union(ret_links)\n+            else:\n+                if hasattr(spider, 'rules'):\n+                    for rule in spider.rules:\n+                        if rule.link_extractor.matches(response.url):\n+                            ret_items, ret_links = self.run_method(spider, response, rule.callback, args, opts)\n+                            items = items.union(ret_items)\n+                            links = links.union(ret_links)\n+                else:\n+                    log.msg('No rules found for spider \"%s\", please specify a parsing method' % spider.domain_name)\n+                    continue\n+\n         self.print_results(items, links, opts)\n \n\n@@ -1,73 +0,0 @@\n-from scrapy.command import ScrapyCommand\n-from scrapy.fetcher import fetch\n-from scrapy.http import Request\n-from scrapy.item import ScrapedItem\n-from scrapy.spider import spiders\n-from scrapy.utils import display\n-from scrapy import log\n-\n-class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"[options] <url> <method>\"\n-\n-    def short_desc(self):\n-        return \"Parse the URL with the given spider method and print the results\"\n-\n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--links\", dest=\"links\", action=\"store_true\", help=\"show extracted links\")\n-        parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", help=\"don't show scraped items\")\n-        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", help=\"avoid using pygments to colorize the output\")\n-\n-    def pipeline_process(self, item, opts):\n-        item.set_guid(spider)\n-        for key in item.__dict__.keys():\n-           if key.startswith('_'):\n-               item.__dict__.pop(key, None)\n-        return item\n-\n-    def run_method(self, response, method, args, opts):\n-        spider = spiders.fromurl(response.url)\n-        if not spider:\n-            log.msg('Couldnt find spider for url: %s' % response.url, level=log.ERROR)\n-            return (), ()\n-\n-        items = []\n-        links = []\n-        if method:\n-            method_fcn = method if callable(method) else getattr(spider, method, None)\n-            if not method_fcn:\n-                log.msg('Couldnt find method %s in spider %s' % (method, spider.__name__))\n-                return (), ()\n-\n-            result = method_fcn(response)\n-            links = [i for i in result if isinstance(i, Request)]\n-            items = [self.pipeline_process(i, opts) for i in result if isinstance(i, ScrapedItem)]\n-\n-        return items, links\n-\n-    def print_results(self, items, links, opts):\n-        display.nocolour = opts.nocolour\n-        if not opts.noitems:\n-            print \"# Scraped Items\", \"-\"*60\n-            display.pprint(list(items))\n-\n-        if opts.links:\n-            print \"# Links\", \"-\"*68\n-            display.pprint(list(links))\n-\n-    def run(self, args, opts):\n-        if len(args) < 2:\n-            print \"A URL and method is required\"\n-            return\n-\n-        items = set()\n-        links = set()\n-        url, method = args[:2]\n-        for response in fetch([url]):\n-            ret_items, ret_links = self.run_method(response, method, args, opts)\n-            items = items.union(ret_items)\n-            links = links.union(ret_links)\n-\n-        self.print_results(items, links, opts)\n-\n\n@@ -71,6 +71,11 @@ class LinkExtractor(FixedSGMLParser):\n         if self.current_link and not self.current_link.text:\n             self.current_link.text = data\n \n+    def matches(self, url):\n+        \"\"\"This extractor matches with any url, since\n+        it doesn't contain any patterns\"\"\"\n+        return True\n+\n \n class Link(object):\n     \"\"\"\n\n@@ -72,5 +72,12 @@ class RegexLinkExtractor(LinkExtractor):\n \n         return links\n \n-    def match(self, url):\n-        return any(regex.search(url) for regex in self.allow_res) and not any(regex.search(url) for regex in self.deny_res)\n+    def matches(self, url):\n+        if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains):\n+            return False\n+        if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):\n+            return False\n+            \n+        allowed = [regex.search(url) for regex in self.allow_res]\n+        denied = [regex.search(url) for regex in self.deny_res]\n+        return any(allowed) and not any(denied)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#50379ee21e2ea7ecf4374ce1502a0c47d22bda52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 38 | Churn Cumulative: 200 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -78,6 +78,6 @@ class RegexLinkExtractor(LinkExtractor):\n         if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):\n             return False\n             \n-        allowed = [regex.search(url) for regex in self.allow_res]\n-        denied = [regex.search(url) for regex in self.deny_res]\n-        return any(allowed) and not any(denied)\n+        allowed = [regex.search(url) for regex in self.allow_res] if self.allow_res else [True]\n+        denied = [regex.search(url) for regex in self.deny_res] if self.deny_res else []\n+        return True if any(allowed) and not any(denied) else False\n\n@@ -2,6 +2,7 @@ import unittest\n \n from scrapy.http import Response\n from scrapy.link import LinkExtractor, Link\n+from scrapy.link.extractors import RegexLinkExtractor\n \n class LinkExtractorTestCase(unittest.TestCase):\n     def test_basic(self):\n@@ -31,5 +32,36 @@ class LinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.extract_urls(response), \n                          [Link(url='http://otherdomain.com/base/item/12.html', text='Item 12')])\n \n+    def test_matches(self):\n+        url1 = 'http://lotsofstuff.com/stuff1/index'\n+        url2 = 'http://evenmorestuff.com/uglystuff/index'\n+\n+        lx = LinkExtractor()\n+        self.assertEqual(lx.matches(url1), True)\n+        self.assertEqual(lx.matches(url2), True)\n+\n+        lx = RegexLinkExtractor(allow=(r'stuff1', ))\n+        self.assertEqual(lx.matches(url1), True)\n+        self.assertEqual(lx.matches(url2), False)\n+\n+        lx = RegexLinkExtractor(deny=(r'uglystuff', ))\n+        self.assertEqual(lx.matches(url1), True)\n+        self.assertEqual(lx.matches(url2), False)\n+\n+        lx = RegexLinkExtractor(allow_domains=('evenmorestuff.com', ))\n+        self.assertEqual(lx.matches(url1), False)\n+        self.assertEqual(lx.matches(url2), True)\n+\n+        lx = RegexLinkExtractor(deny_domains=('lotsofstuff.com', ))\n+        self.assertEqual(lx.matches(url1), False)\n+        self.assertEqual(lx.matches(url2), True)\n+\n+        lx = RegexLinkExtractor(allow=('blah1', ), deny=('blah2', ),\n+            allow_domains=('blah1.com', ), deny_domains=('blah2.com', ))\n+        self.assertEqual(lx.matches('http://blah1.com/blah1'), True)\n+        self.assertEqual(lx.matches('http://blah1.com/blah2'), False)\n+        self.assertEqual(lx.matches('http://blah2.com/blah1'), False)\n+        self.assertEqual(lx.matches('http://blah2.com/blah2'), False)\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c22223beb78027f1c716ca4a9743e55675b53610", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 35 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -28,3 +28,6 @@ EXTENSIONS = []\n # contrib.middleware.retry.RetryMiddleware default settings\n RETRY_TIMES = 3\n RETRY_HTTP_CODES = ['500', '503', '504', '400', '408', '200']\n+\n+# amazon s3 images backend sometimes fails to authenticate or validate http protocol\n+RETRY_HTTP_CODES = RETRY_HTTP_CODES + ['505', '403']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a4ba7b9d9af84c727e9a0aae15cb7603151d63e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 125 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -55,7 +55,7 @@ class RegexLinkExtractor(LinkExtractor):\n             response = new_response_from_xpaths(response, self.restrict_xpaths)\n \n         links = LinkExtractor.extract_urls(self, response, unique)\n-        links = [link for link in links if _is_valid_url(link.url)]\n+        links = [link.strip() for link in links if _is_valid_url(link.url)]\n \n         if self.allow_res:\n             links = [link for link in links if _matches(link.url, self.allow_res)]\n@@ -80,4 +80,4 @@ class RegexLinkExtractor(LinkExtractor):\n             \n         allowed = [regex.search(url) for regex in self.allow_res] if self.allow_res else [True]\n         denied = [regex.search(url) for regex in self.deny_res] if self.deny_res else []\n-        return True if any(allowed) and not any(denied) else False\n+        return any(allowed) and not any(denied)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#068d2278a37b6570bcf98a103bff312d122a6ea9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 246 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -44,7 +44,7 @@ class LinkExtractor(FixedSGMLParser):\n         base_url = self.base_url if self.base_url else response.url\n         ret = []\n         for link in self.links:\n-            link.url = urljoin(base_url, link.url)\n+            link.url = urljoin(base_url, link.url).strip()\n             ret.append(link)\n         return ret\n \n\n@@ -55,7 +55,7 @@ class RegexLinkExtractor(LinkExtractor):\n             response = new_response_from_xpaths(response, self.restrict_xpaths)\n \n         links = LinkExtractor.extract_urls(self, response, unique)\n-        links = [link.strip() for link in links if _is_valid_url(link.url)]\n+        links = [link for link in links if _is_valid_url(link.url)]\n \n         if self.allow_res:\n             links = [link for link in links if _matches(link.url, self.allow_res)]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7ac8ae993a30ce6ea221e2474df4a8eb7eada1ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 38 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -28,6 +28,3 @@ EXTENSIONS = []\n # contrib.middleware.retry.RetryMiddleware default settings\n RETRY_TIMES = 3\n RETRY_HTTP_CODES = ['500', '503', '504', '400', '408', '200']\n-\n-# amazon s3 images backend sometimes fails to authenticate or validate http protocol\n-RETRY_HTTP_CODES = RETRY_HTTP_CODES + ['505', '403']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ddda08ee513a1a61f86148fd10646b33094fdabf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 28/28 | Churn Δ: 16 | Churn Cumulative: 16 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ import copy\n from scrapy.http import Request\n from scrapy.spider import BaseSpider\n from scrapy.item import ScrapedItem\n+from scrapy.conf import settings\n \n class Rule(object):\n     \"\"\"\n@@ -60,12 +61,10 @@ class CrawlSpider(BaseSpider):\n         \"\"\"This function is called by the framework core for all the\n         start_urls. Do not override this function, override parse_start_url\n         instead.\"\"\"\n-        return self._response_downloaded(response, self.parse_start_url, cb_kwargs={}, follow=True)\n-\n-    def parse_start_url(self, response):\n-        \"\"\"Overrideable callback function for processing start_urls. It must\n-        return a list of ScrapedItems and/or Requests\"\"\"\n-        return []\n+        for rule in self._rules:\n+            if rule.callback and rule.link_extractor.matches(response.url):\n+                return self._response_downloaded(response, rule.callback, rule.cb_kwargs, follow=True)\n+        return self._response_downloaded(response, None, cb_kwargs={}, follow=True)\n \n     def process_results(self, results, response):\n         \"\"\"This overridable method is called for each result (item or request)\n@@ -86,7 +85,7 @@ class CrawlSpider(BaseSpider):\n         seen = set()\n         for rule in self._rules:\n             links = [l for l in rule.link_extractor.extract_urls(response) if l not in seen]\n-            if rule.process_links:\n+            if links and rule.process_links:\n                 links = rule.process_links(links)\n             seen = seen.union(links)\n             for link in links:\n@@ -102,7 +101,8 @@ class CrawlSpider(BaseSpider):\n         It returns a list of requests/items.\n         \"\"\"\n         res = []\n-        if follow:\n+\n+        if follow and settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True):\n             res.extend(self._requests_to_follow(response))\n         if callback:\n             cb_res = callback(response, **cb_kwargs) or ()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#977e13e3b961ecf145ff036679f97d9e0e2f7e22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 229 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -18,7 +18,7 @@ class Command(ScrapyCommand):\n         parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\", help=\"don't show extracted links\")\n         parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", help=\"don't show scraped items\")\n         parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", help=\"avoid using pygments to colorize the output\")\n-        parser.add_option(\"--matches\", dest=\"matches\", action=\"store_true\", help=\"avoid using pygments to colorize the output\")\n+        parser.add_option(\"--matches\", dest=\"matches\", action=\"store_true\", help=\"try to match and parse the url with the defined rules (if any)\")\n \n     def pipeline_process(self, item, spider, opts):\n         return item\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f371c952dd9114618546bf5f3a0d59b1d0cd11e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 375 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -131,6 +131,9 @@ class S3ImagesPipeline(BaseImagesPipeline):\n             self._s3_put_image(thumb, key, info)\n \n     def _s3_put_image(self, image, key, info):\n+        if image.mode != 'RGB':\n+            image = image.convert('RGB')\n+\n         buf = StringIO()\n         try:\n             image.save(buf, 'JPEG')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc2a699458285d01adb29fad903e11f8de175a3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 233 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -85,6 +85,10 @@ class Command(ScrapyCommand):\n                     for rule in spider.rules:\n                         if rule.link_extractor.matches(response.url):\n                             ret_items, ret_links = self.run_method(spider, response, rule.callback, args, opts)\n+                            extracted_links = rule.link_extractor.extract_urls(response)\n+                            for link in extracted_links:\n+                                links.add(Request(url=link.url, link_text=link.text))\n+\n                             items = items.union(ret_items)\n                             links = links.union(ret_links)\n                 else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#242bd38b3f37228537f4c075117dbed9502e5408", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 237 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -83,12 +83,12 @@ class Command(ScrapyCommand):\n             else:\n                 if hasattr(spider, 'rules'):\n                     for rule in spider.rules:\n-                        if rule.link_extractor.matches(response.url):\n-                            ret_items, ret_links = self.run_method(spider, response, rule.callback, args, opts)\n                         extracted_links = rule.link_extractor.extract_urls(response)\n                         for link in extracted_links:\n                            links.add(Request(url=link.url, link_text=link.text))\n \n+                        if rule.link_extractor.matches(response.url):\n+                            ret_items, ret_links = self.run_method(spider, response, rule.callback, args, opts)\n                             items = items.union(ret_items)\n                             links = links.union(ret_links)\n                 else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e030f38427a9c15c4d482140e5ec5a76ed0f6c35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 22 | Churn Cumulative: 259 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -68,8 +68,8 @@ class Command(ScrapyCommand):\n             else:\n                 url, method = args[:2]\n \n-        items = set()\n-        links = set()\n+        items = []\n+        links = []\n         for response in fetch([url]):\n             spider = spiders.fromurl(response.url)\n             if not spider:\n@@ -78,19 +78,19 @@ class Command(ScrapyCommand):\n \n             if method:\n                 ret_items, ret_links = self.run_method(spider, response, method, args, opts)\n-                items = items.union(ret_items)\n-                links = links.union(ret_links)\n+                items.extend(ret_items)\n+                links.extend(ret_links)\n             else:\n                 if hasattr(spider, 'rules'):\n-                    for rule in spider.rules:\n-                        extracted_links = rule.link_extractor.extract_urls(response)\n-                        for link in extracted_links:\n-                           links.add(Request(url=link.url, link_text=link.text))\n+                    already_parsed = False\n \n-                        if rule.link_extractor.matches(response.url):\n+                    for rule in spider.rules:\n+                        links.extend(Request(url=link.url, link_text=link.text) for link in rule.link_extractor.extract_urls(response))\n+                        if not already_parsed and rule.link_extractor.matches(response.url):\n+                            already_parsed = True\n                             ret_items, ret_links = self.run_method(spider, response, rule.callback, args, opts)\n-                            items = items.union(ret_items)\n-                            links = links.union(ret_links)\n+                            items.extend(ret_items)\n+                            links.extend(ret_links)\n                 else:\n                     log.msg('No rules found for spider \"%s\", please specify a parsing method' % spider.domain_name)\n                     continue\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c7cb6c4e446512b55aa82000a508a1501e69f8f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 35 | Files Changed: 3 | Hunks: 14 | Methods Changed: 8 | Complexity Δ (Sum/Max): 3/8 | Churn Δ: 91 | Churn Cumulative: 428 | Contributors (this commit): 2 | Commits (past 90d): 21 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,6 +2,10 @@ from scrapy.command import ScrapyCommand\n from scrapy.core.manager import scrapymanager\n from scrapy.replay import Replay\n from scrapy.conf import settings\n+from scrapy.utils.url import is_url\n+from scrapy.spider import spiders\n+from scrapy.http import Request\n+from scrapy import log\n \n \n class Command(ScrapyCommand):\n@@ -19,6 +23,7 @@ class Command(ScrapyCommand):\n         parser.add_option(\"--record\", dest=\"record\", help=\"use FILE for recording session (see replay command)\", metavar=\"FILE\")\n         parser.add_option(\"--record-dir\", dest=\"recorddir\", help=\"use DIR for recording (instead of file)\", metavar=\"DIR\")\n         parser.add_option(\"-n\", \"--nofollow\", dest=\"nofollow\", action=\"store_true\", help=\"don't follow links (for use with URLs only)\")\n+        parser.add_option(\"-c\", \"--callback\", dest=\"callback\", action=\"store\", help=\"use the provided callback for starting to crawl the given url\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -41,4 +46,28 @@ class Command(ScrapyCommand):\n             self.replay.record(args=args, opts=opts.__dict__)\n \n     def run(self, args, opts):\n+        if opts.callback:\n+            requests = []\n+            for a in args:\n+                if is_url(a):\n+                    spider = spiders.fromurl(a)\n+                    urls = [a]\n+                else:\n+                    spider = spiders.fromdomain(a)\n+                    urls = spider.start_urls if hasattr(spider.start_urls, '__iter__') else [spider.start_urls]\n+\n+                if spider:\n+                    if hasattr(spider, opts.callback):\n+                        requests.extend(Request(url=url, callback=getattr(spider, opts.callback)) for url in urls)\n+                    else:\n+                        log.msg('Callback %s doesnt exist in spider %s' % (opts.callback, spider.domain_name), log.ERROR)\n+                else:\n+                    log.msg('Could not found spider for %s' % a, log.ERROR)\n+\n+            if requests:\n+                args = requests\n+            else:\n+                log.msg('Couldnt create any requests from the provided arguments', log.ERROR)\n+                return\n+\n         scrapymanager.runonce(*args, **opts.__dict__)\n\n@@ -8,17 +8,18 @@ from scrapy import log\n \n class Command(ScrapyCommand):\n     def syntax(self):\n-        return \"[options] <url> <method>\"\n+        return \"[options] <url>\"\n \n     def short_desc(self):\n-        return \"Parse the URL with the given spider method and print the results\"\n+        return \"Parse the given URL and print the results\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\", help=\"don't show extracted links\")\n         parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", help=\"don't show scraped items\")\n         parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", help=\"avoid using pygments to colorize the output\")\n-        parser.add_option(\"--matches\", dest=\"matches\", action=\"store_true\", help=\"try to match and parse the url with the defined rules (if any)\")\n+        parser.add_option(\"-r\", \"--rules\", dest=\"rules\", action=\"store_true\", help=\"try to match and parse the url with the defined rules (if any)\")\n+        parser.add_option(\"-c\", \"--callback\", dest=\"callback\", action=\"store\", help=\"use the provided callback for parsing the url\")\n \n     def pipeline_process(self, item, spider, opts):\n         return item\n@@ -58,42 +59,32 @@ class Command(ScrapyCommand):\n             display.pprint(list(links))\n \n     def run(self, args, opts):\n-        if opts.matches:\n-            url = args[0]\n-            method = None\n-        else:\n-            if len(args) < 2:\n-                print \"A URL and method is required\"\n+        if not args:\n+            print \"An URL is required\"\n             return\n-            else:\n-                url, method = args[:2]\n \n-        items = []\n-        links = []\n-        for response in fetch([url]):\n+        ret_items, ret_links = [], []\n+        for response in fetch(args):\n             spider = spiders.fromurl(response.url)\n             if not spider:\n                 log.msg('Couldnt find spider for \"%s\"' % response.url)\n                 continue\n \n-            if method:\n-                ret_items, ret_links = self.run_method(spider, response, method, args, opts)\n-                items.extend(ret_items)\n-                links.extend(ret_links)\n-            else:\n-                if hasattr(spider, 'rules'):\n-                    already_parsed = False\n-\n-                    for rule in spider.rules:\n-                        links.extend(Request(url=link.url, link_text=link.text) for link in rule.link_extractor.extract_urls(response))\n-                        if not already_parsed and rule.link_extractor.matches(response.url):\n-                            already_parsed = True\n-                            ret_items, ret_links = self.run_method(spider, response, rule.callback, args, opts)\n-                            items.extend(ret_items)\n-                            links.extend(ret_links)\n+            if opts.callback:\n+                items, links = self.run_method(spider, response, opts.callback, args, opts)\n+            elif opts.rules:\n+                for rule in getattr(spider, 'rules', ()):\n+                    if rule.link_extractor.matches(response.url):\n+                        items, links = self.run_method(spider, response, rule.callback, args, opts)\n+                        break\n                 else:\n                     log.msg('No rules found for spider \"%s\", please specify a parsing method' % spider.domain_name)\n                     continue\n+            else:\n+                items, links = self.run_method(spider, response, 'parse', args, opts)\n \n-        self.print_results(items, links, opts)\n+            ret_items.extend(items)\n+            ret_links.extend(links)\n+\n+        self.print_results(ret_items, ret_links, opts)\n \n\n@@ -2,7 +2,6 @@ import copy\n \n from scrapy.http import Request\n from scrapy.spider import BaseSpider\n-from scrapy.item import ScrapedItem\n from scrapy.conf import settings\n \n class Rule(object):\n@@ -61,10 +60,12 @@ class CrawlSpider(BaseSpider):\n         \"\"\"This function is called by the framework core for all the\n         start_urls. Do not override this function, override parse_start_url\n         instead.\"\"\"\n-        for rule in self._rules:\n-            if rule.callback and rule.link_extractor.matches(response.url):\n-                return self._response_downloaded(response, rule.callback, rule.cb_kwargs, follow=True)\n-        return self._response_downloaded(response, None, cb_kwargs={}, follow=True)\n+        return self._response_downloaded(response, self.parse_start_url, cb_kwargs={}, follow=True)\n+\n+    def parse_start_url(self, response):\n+        \"\"\"Overrideable callback function for processing start_urls. It must\n+        return a list of ScrapedItems and/or Requests\"\"\"\n+        return []\n \n     def process_results(self, results, response):\n         \"\"\"This overridable method is called for each result (item or request)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#72c978d8a012a5f2f560f76564407a6b296abfab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 17 | Churn Cumulative: 528 | Contributors (this commit): 2 | Commits (past 90d): 16 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ import hmac\n import base64\n import hashlib\n from urlparse import urlsplit\n+from scrapy.conf import settings\n \n \n METADATA_PREFIX = 'x-amz-meta-'\n@@ -74,3 +75,12 @@ def sign_request(req, accesskey, secretkey):\n     req.headers['Authorization'] = \"AWS %s:%s\" % (accesskey, b64_hmac)\n \n \n+class AWSMiddleware(object):\n+    def __init__(self):\n+        self.access_key = settings['AWS_ACCESS_KEY_ID']\n+        self.secret_key = settings['AWS_SECRET_ACCESS_KEY']\n+\n+    def process_request(self, request, spider):\n+        if spider.domain_name in 's3.amazonaws.com':\n+            request.headers['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime())\n+            sign_request(request, self.access_key, self.secret_key)\n\n@@ -12,7 +12,7 @@ from scrapy.http import Request\n from scrapy.stats import stats\n from scrapy.core.exceptions import DropItem, NotConfigured, HttpException\n from scrapy.contrib.pipeline.media import MediaPipeline\n-from scrapy.contrib.aws import canonical_string, sign_request\n+from scrapy.contrib.aws import sign_request\n from scrapy.conf import settings\n \n from .images import BaseImagesPipeline, NoimagesDrop, ImageException\n@@ -36,6 +36,10 @@ class S3ImagesPipeline(BaseImagesPipeline):\n #             ('270', (270, 270))\n     )\n \n+    # Automatically sign requests with AWS authorization header,\n+    # alternative we can do this using scrapy.contrib.aws.AWSMiddleware\n+    sign_requests = True\n+\n     def __init__(self):\n         if not settings['S3_IMAGES']:\n             raise NotConfigured\n@@ -50,6 +54,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n     def s3request(self, key, method, body=None, headers=None):\n         url = 'http://%s.s3.amazonaws.com/%s%s' % (self.bucket_name, self.prefix, key)\n         req = Request(url, method=method, body=body, headers=headers)\n+        if self.sign_requests:\n             sign_request(req, self.access_key, self.secret_key)\n         return req\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f18bce7062159ae18ecf08ee363b2b1ef9ccfbae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 20 | Churn Cumulative: 822 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -35,6 +35,12 @@ class SiteDetails(object):\n     def capacity(self):\n         return self.max_concurrent_requests - len(self.downloading)\n \n+    def outstanding(self):\n+        return len(self.active) + len(self.queue)\n+\n+    def needs_backout(self):\n+        return self.outstanding() > (2 * self.max_concurrent_requests)\n+\n \n class Downloader(object):\n     \"\"\"Maintain many concurrent downloads and provide an HTTP abstraction\n@@ -155,6 +161,10 @@ class Downloader(object):\n         else:\n             log.msg('Domain %s already closed' % domain, log.TRACE, domain=domain)\n \n+    def needs_backout(self, domain):\n+        site = self.sites.get(domain)\n+        return (site.needs_backout() if site else True)\n+\n     # Most of the following functions must be reviewed to decide if are really needed\n     def domain_is_open(self, domain):\n         return domain in self.sites\n@@ -169,7 +179,7 @@ class Downloader(object):\n         \"\"\"\n         site = self.sites.get(domain)\n         if site:\n-            return len(site.active) + len(site.queue)\n+            return site.outstanding()\n \n     def domain_is_idle(self, domain):\n         return not self.outstanding(domain)\n\n@@ -38,9 +38,6 @@ class ExecutionEngine(object):\n     process for that domain.\n     \"\"\"\n \n-    # back off downloading more pages if we exceed this backlog size\n-    DOWNLOADER_BACKLOG = 10\n-\n     def __init__(self):\n         self.configured = False\n         self.keep_alive = False\n@@ -179,9 +176,8 @@ class ExecutionEngine(object):\n         if not self.running:\n             return\n \n-        # outstanding requests in downloader queue\n-        outstanding = self.downloader.outstanding(domain)\n-        if outstanding > self.DOWNLOADER_BACKLOG:\n+        # backout enqueing downloads if domain needs it\n+        if self.downloader.needs_backout(domain):\n             return\n \n         # Next pending request from scheduler\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0ce417eb0bfac85bec6ab6f805c9901e9da1d0b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 622 | Contributors (this commit): 2 | Commits (past 90d): 25 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -62,12 +62,12 @@ class Command(ScrapyCommand):\n                     else:\n                         log.msg('Callback %s doesnt exist in spider %s' % (opts.callback, spider.domain_name), log.ERROR)\n                 else:\n-                    log.msg('Could not found spider for %s' % a, log.ERROR)\n+                    log.msg('Cannot find spider for %s' % a, log.ERROR)\n \n             if requests:\n                 args = requests\n             else:\n-                log.msg('Couldnt create any requests from the provided arguments', log.ERROR)\n+                log.msg('Cannot create any requests with the provided arguments', log.ERROR)\n                 return\n \n         scrapymanager.runonce(*args, **opts.__dict__)\n\n@@ -27,7 +27,7 @@ class Command(ScrapyCommand):\n     def run_method(self, spider, response, method, args, opts):\n         spider = spiders.fromurl(response.url)\n         if not spider:\n-            log.msg('Couldnt find spider for url: %s' % response.url, level=log.ERROR)\n+            log.msg('Cannot find spider for url: %s' % response.url, level=log.ERROR)\n             return (), ()\n \n         items = []\n@@ -35,7 +35,7 @@ class Command(ScrapyCommand):\n         if method:\n             method_fcn = method if callable(method) else getattr(spider, method, None)\n             if not method_fcn:\n-                log.msg('Couldnt find method %s in %s spider' % (method, spider.domain_name))\n+                log.msg('Cannot find method %s in %s spider' % (method, spider.domain_name))\n                 return (), ()\n \n             result = method_fcn(response)\n@@ -67,7 +67,7 @@ class Command(ScrapyCommand):\n         for response in fetch(args):\n             spider = spiders.fromurl(response.url)\n             if not spider:\n-                log.msg('Couldnt find spider for \"%s\"' % response.url)\n+                log.msg('Cannot find spider for \"%s\"' % response.url)\n                 continue\n \n             if opts.callback:\n\n@@ -117,7 +117,7 @@ class ExecutionManager(object):\n         for domain in sites:\n             spider = spiders.fromdomain(domain)\n             if not spider:\n-                log.msg('Could not found spider for %s' % domain, log.ERROR)\n+                log.msg('Could not find spider for %s' % domain, log.ERROR)\n                 continue\n             for url in self._start_urls(spider):\n                 request = Request(url, callback=spider.parse, dont_filter=True)\n@@ -129,7 +129,7 @@ class ExecutionManager(object):\n                 request = Request(url=url, callback=spider.parse, dont_filter=True)\n                 _add(spider.domain_name, request)\n             else:\n-                log.msg('Could not found spider for <%s>' % url, log.ERROR)\n+                log.msg('Could not find spider for <%s>' % url, log.ERROR)\n \n         # requests\n         for request in requests:\n@@ -138,7 +138,7 @@ class ExecutionManager(object):\n             else:\n                 spider = spiders.fromurl(request.url)\n             if not spider:\n-                log.msg('Could not found spider for %s' % request, log.ERROR)\n+                log.msg('Could not find spider for %s' % request, log.ERROR)\n                 continue\n             _add(spider.domain_name, request)\n         return perdomain\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aad2eae55d0c822f460eefdc67e8791b74d91392", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 24 | Churn Cumulative: 406 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,7 @@ import Image\n from scrapy import log\n from scrapy.http import Request\n from scrapy.stats import stats\n+from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import DropItem, NotConfigured, HttpException\n from scrapy.contrib.pipeline.media import MediaPipeline\n from scrapy.contrib.aws import sign_request\n@@ -40,6 +41,8 @@ class S3ImagesPipeline(BaseImagesPipeline):\n     # alternative we can do this using scrapy.contrib.aws.AWSMiddleware\n     sign_requests = True\n \n+    s3_custom_spider = None\n+\n     def __init__(self):\n         if not settings['S3_IMAGES']:\n             raise NotConfigured\n@@ -51,7 +54,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n         self.image_refresh_days = settings.getint('IMAGES_REFRESH_DAYS', 90)\n         MediaPipeline.__init__(self)\n \n-    def s3request(self, key, method, body=None, headers=None):\n+    def s3_request(self, key, method, body=None, headers=None):\n         url = 'http://%s.s3.amazonaws.com/%s%s' % (self.bucket_name, self.prefix, key)\n         req = Request(url, method=method, body=body, headers=headers)\n         if self.sign_requests:\n@@ -112,8 +115,8 @@ class S3ImagesPipeline(BaseImagesPipeline):\n             return '%s#%s' % (key, etag)\n \n         key = self.s3_image_key(request.url)\n-        req = self.s3request(key, method='HEAD')\n-        dfd = self.download(req, info)\n+        req = self.s3_request(key, method='HEAD')\n+        dfd = self.s3_download(req, info)\n         dfd.addCallbacks(_onsuccess, lambda _:None)\n         dfd.addErrback(log.err, 'S3ImagesPipeline.media_to_download')\n         return dfd\n@@ -154,7 +157,18 @@ class S3ImagesPipeline(BaseImagesPipeline):\n                 }\n \n         buf.seek(0)\n-        req = self.s3request(key, method='PUT', body=buf.read(), headers=headers)\n-        return self.download(req, info), buf\n+        req = self.s3_request(key, method='PUT', body=buf.read(), headers=headers)\n+        return self.s3_download(req, info), buf\n+\n+    def s3_download(self, request, info):\n+        \"\"\"This method is used for HEAD and PUT requests sent to amazon S3\n+\n+        It tries to use a specific spider domain for uploads, or defaults\n+        to current domain spider.\n+\n+        \"\"\"\n+        if self.s3_custom_spider:\n+            return scrapyengine.schedule(request, self.s3_custom_spider)\n+        return self.download(request, info.spider)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
