{"custom_id": "keras#18e575c6d6e22d18194fc3d438a49fcd640f7aed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 163 | Lines Deleted: 13 | Files Changed: 5 | Hunks: 7 | Methods Changed: 10 | Complexity Δ (Sum/Max): 15/11 | Churn Δ: 176 | Churn Cumulative: 1002 | Contributors (this commit): 10 | Commits (past 90d): 69 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -66,4 +66,5 @@ from keras_core.layers.regularization.gaussian_noise import GaussianNoise\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout1D\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout2D\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout3D\n+from keras_core.layers.reshaping.flatten import Flatten\n from keras_core.layers.reshaping.reshape import Reshape\n\n@@ -0,0 +1,72 @@\n+import math\n+\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.layers.input_spec import InputSpec\n+from keras_core.layers.layer import Layer\n+\n+\n+class Flatten(Layer):\n+    \"\"\"Flattens the input. Does not affect the batch size.\n+\n+    Note: If inputs are shaped `(batch,)` without a feature axis, then\n+    flattening adds an extra channel dimension and output shape is `(batch, 1)`.\n+\n+    Args:\n+        data_format: A string, one of `'channels_last'` (default) or\n+            `'channels_first'`. The ordering of the dimensions in the inputs.\n+            '`channels_last`' corresponds to inputs with shape\n+            `(batch, ..., channels)` while `'channels_first'` corresponds to\n+            inputs with shape `(batch, channels, ...)`.\n+            When unspecified, uses `image_data_format` value found in your Keras\n+            config file at `~/.keras/keras.json` (if exists). Defaults to\n+            `'channels_last'`.\n+\n+    Example:\n+\n+    >>> x = keras_core.Input(shape=(3, 32, 32))\n+    >>> y = keras_core.layers.Conv2D(64, 3, 3)(x)\n+    >>> y.shape\n+    (None, 1, 10, 64)\n+\n+    >>> y = keras_core.layers.Flatten()(y)\n+    >>> y.shape\n+    (None, 640)\n+    \"\"\"\n+\n+    def __init__(self, data_format=None, name=None, dtype=None):\n+        super().__init__(name=name, dtype=dtype)\n+        self.data_format = (\n+            backend.image_data_format() if data_format is None else data_format\n+        )\n+        self.input_spec = InputSpec(min_ndim=1)\n+        self._channels_first = self.data_format == \"channels_first\"\n+\n+    def call(self, inputs):\n+        input_shape = inputs.shape\n+        rank = len(input_shape)\n+\n+        if self._channels_first and rank > 1:\n+            # Switch to channels-last format.\n+            inputs = ops.transpose(inputs, axes=(0, *range(2, rank), 1))\n+\n+        output_shape = tuple(\n+            dim if dim is not None else -1\n+            for dim in self.compute_output_shape(input_shape)\n+        )\n+        return ops.reshape(inputs, output_shape)\n+\n+    def compute_output_shape(self, input_shape):\n+        non_batch_dims = input_shape[1:]\n+        if len(non_batch_dims) == 0:\n+            flattened_dim = 1\n+        elif None in non_batch_dims:\n+            flattened_dim = None\n+        else:\n+            flattened_dim = math.prod(non_batch_dims)\n+        return (input_shape[0], flattened_dim)\n+\n+    def get_config(self):\n+        config = {\"data_format\": self.data_format}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,84 @@\n+import numpy as np\n+import pytest\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import operations as ops\n+from keras_core import testing\n+\n+\n+class FlattenTest(testing.TestCase):\n+    def test_flatten(self):\n+        inputs = np.random.random((10, 3, 5, 5)).astype(\"float32\")\n+\n+        # Test default data_format and channels_last\n+        expected_output = ops.convert_to_tensor(\n+            np.reshape(inputs, (-1, 5 * 5 * 3))\n+        )\n+        self.run_layer_test(\n+            layers.Flatten,\n+            init_kwargs={},\n+            input_data=inputs,\n+            expected_output=expected_output,\n+        )\n+        self.run_layer_test(\n+            layers.Flatten,\n+            init_kwargs={\"data_format\": \"channels_last\"},\n+            input_data=inputs,\n+            expected_output=expected_output,\n+        )\n+\n+        # Test channels_first\n+        expected_output = ops.convert_to_tensor(\n+            np.reshape(np.transpose(inputs, (0, 2, 3, 1)), (-1, 5 * 5 * 3))\n+        )\n+        self.run_layer_test(\n+            layers.Flatten,\n+            init_kwargs={\"data_format\": \"channels_first\"},\n+            input_data=inputs,\n+            expected_output=expected_output,\n+        )\n+\n+    def test_flatten_with_scalar_channels(self):\n+        inputs = np.random.random((10,)).astype(\"float32\")\n+        expected_output = ops.convert_to_tensor(np.expand_dims(inputs, -1))\n+\n+        # Test default data_format and channels_last\n+        self.run_layer_test(\n+            layers.Flatten,\n+            init_kwargs={},\n+            input_data=inputs,\n+            expected_output=expected_output,\n+        )\n+        self.run_layer_test(\n+            layers.Flatten,\n+            init_kwargs={\"data_format\": \"channels_last\"},\n+            input_data=inputs,\n+            expected_output=expected_output,\n+        )\n+\n+        # Test channels_first\n+        self.run_layer_test(\n+            layers.Flatten,\n+            init_kwargs={\"data_format\": \"channels_first\"},\n+            input_data=inputs,\n+            expected_output=expected_output,\n+        )\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_flatten_with_dynamic_batch_size(self):\n+        input_layer = layers.Input(batch_shape=(None, 2, 3))\n+        flattened = layers.Flatten()(input_layer)\n+        self.assertEqual(flattened.shape, (None, 2 * 3))\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_flatten_with_dynamic_dimension(self):\n+        input_layer = layers.Input(batch_shape=(5, 2, None))\n+        flattened = layers.Flatten()(input_layer)\n+        self.assertEqual(flattened.shape, (5, None))\n\n@@ -21,21 +21,14 @@ class Reshape(Layer):\n \n     Example:\n \n-    >>> # as first layer in a Sequential model\n-    >>> model = keras_core.Sequential()\n-    >>> model.add(keras_core.layers.Reshape((3, 4), input_shape=(12,)))\n-    >>> # model.output_shape == (None, 3, 4), `None` is the batch size.\n-    >>> model.output_shape\n+    >>> x = keras_core.Input(shape=(12,))\n+    >>> y = keras_core.layers.Reshape((3, 4))(x)\n+    >>> y.shape\n     (None, 3, 4)\n \n-    >>> # as intermediate layer in a Sequential model\n-    >>> model.add(keras_core.layers.Reshape((6, 2)))\n-    >>> model.output_shape\n-    (None, 6, 2)\n-\n     >>> # also supports shape inference using `-1` as dimension\n-    >>> model.add(keras_core.layers.Reshape((-1, 2, 2)))\n-    >>> model.output_shape\n+    >>> y = keras_core.layers.Reshape((-1, 2, 2))(x)\n+    >>> y.shape\n     (None, 3, 2, 2)\n     \"\"\"\n \n\n@@ -70,7 +70,7 @@ class TestCase(unittest.TestCase):\n         self,\n         layer_cls,\n         init_kwargs,\n-        input_shape,\n+        input_shape=None,\n         input_dtype=\"float32\",\n         input_data=None,\n         call_kwargs=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a11cd5b026f638d4f2a4f7faaf098f2671573eed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 284 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,9 +2,11 @@ import numpy as np\n import tensorflow as tf\n \n from keras_core import backend\n+from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n \n \n+@keras_core_export(\"keras_core.layers.Discretization\")\n class Discretization(Layer):\n     \"\"\"A preprocessing layer which buckets continuous features by ranges.\n \n@@ -94,10 +96,14 @@ class Discretization(Layer):\n         output_mode=\"int\",\n         sparse=False,\n         name=None,\n+        **kwargs,\n     ):\n         super().__init__(name=name)\n         if sparse and backend.backend() != \"tensorflow\":\n-            raise ValueError()\n+            raise ValueError(\n+                \"`sparse` can only be set to True with the \"\n+                \"TensorFlow backend.\"\n+            )\n         self.layer = tf.keras.layers.Discretization(\n             bin_boundaries=bin_boundaries,\n             num_bins=num_bins,\n@@ -105,6 +111,7 @@ class Discretization(Layer):\n             output_mode=output_mode,\n             sparse=sparse,\n             name=name,\n+            **kwargs,\n         )\n         self.bin_boundaries = (\n             bin_boundaries if bin_boundaries is not None else []\n\n@@ -13,23 +13,19 @@ class Flatten(Layer):\n     flattening adds an extra channel dimension and output shape is `(batch, 1)`.\n \n     Args:\n-        data_format: A string, one of `'channels_last'` (default) or\n-            `'channels_first'`. The ordering of the dimensions in the inputs.\n-            '`channels_last`' corresponds to inputs with shape\n-            `(batch, ..., channels)` while `'channels_first'` corresponds to\n+        data_format: A string, one of `\"channels_last\"` (default) or\n+            `\"channels_first\"`. The ordering of the dimensions in the inputs.\n+            `\"channels_last\"` corresponds to inputs with shape\n+            `(batch, ..., channels)` while `\"channels_first\"` corresponds to\n             inputs with shape `(batch, channels, ...)`.\n             When unspecified, uses `image_data_format` value found in your Keras\n             config file at `~/.keras/keras.json` (if exists). Defaults to\n-            `'channels_last'`.\n+            `\"channels_last\"`.\n \n     Example:\n \n-    >>> x = keras_core.Input(shape=(3, 32, 32))\n-    >>> y = keras_core.layers.Conv2D(64, 3, 3)(x)\n-    >>> y.shape\n-    (None, 1, 10, 64)\n-\n-    >>> y = keras_core.layers.Flatten()(y)\n+    >>> x = keras_core.Input(shape=(10, 64))\n+    >>> y = keras_core.layers.Flatten()(x)\n     >>> y.shape\n     (None, 640)\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2a864258aeaf6b1237b8b4cb17adee110aa63b9d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 126 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 8 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 126 | Churn Cumulative: 214 | Contributors (this commit): 7 | Commits (past 90d): 40 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -68,3 +68,4 @@ from keras_core.layers.regularization.spatial_dropout import SpatialDropout2D\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout3D\n from keras_core.layers.reshaping.flatten import Flatten\n from keras_core.layers.reshaping.reshape import Reshape\n+from keras_core.layers.reshaping.up_sampling1d import UpSampling1D\n\n@@ -0,0 +1,59 @@\n+from keras_core import operations as ops\n+from keras_core.layers.input_spec import InputSpec\n+from keras_core.layers.layer import Layer\n+\n+\n+class UpSampling1D(Layer):\n+    \"\"\"Upsampling layer for 1D inputs.\n+\n+    Repeats each temporal step `size` times along the time axis.\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 2, 3)\n+    >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+    >>> print(x)\n+    [[[ 0  1  2]\n+      [ 3  4  5]]\n+     [[ 6  7  8]\n+      [ 9 10 11]]]\n+    >>> y = keras_core.layers.UpSampling1D(size=2)(x)\n+    >>> print(y)\n+    [[[ 0.  1.  2.]\n+      [ 0.  1.  2.]\n+      [ 3.  4.  5.]\n+      [ 3.  4.  5.]]\n+\n+     [[ 6.  7.  8.]\n+      [ 6.  7.  8.]\n+      [ 9. 10. 11.]\n+      [ 9. 10. 11.]]]\n+\n+    Args:\n+        size: Integer. Upsampling factor.\n+\n+    Input shape:\n+        3D tensor with shape: `(batch_size, steps, features)`.\n+\n+    Output shape:\n+        3D tensor with shape: `(batch_size, upsampled_steps, features)`.\n+    \"\"\"\n+\n+    def __init__(self, size=2, **kwargs):\n+        super().__init__(**kwargs)\n+        self.size = int(size)\n+        self.input_spec = InputSpec(ndim=3)\n+\n+    def compute_output_shape(self, input_shape):\n+        size = (\n+            self.size * input_shape[1] if input_shape[1] is not None else None\n+        )\n+        return [input_shape[0], size, input_shape[2]]\n+\n+    def call(self, inputs):\n+        return ops.repeat(x=inputs, repeats=self.size, axis=1)\n+\n+    def get_config(self):\n+        config = {\"size\": self.size}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,66 @@\n+import numpy as np\n+import pytest\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import testing\n+from keras_core.backend.common.keras_tensor import KerasTensor\n+\n+\n+class UpSamplingTest(testing.TestCase):\n+    def test_upsampling_1d(self):\n+        self.run_layer_test(\n+            layers.UpSampling1D,\n+            init_kwargs={\"size\": 2},\n+            input_shape=(3, 5, 4),\n+            expected_output_shape=(3, 10, 4),\n+            expected_output_dtype=\"float32\",\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+    def test_upsampling_1d_correctness(self):\n+        np.testing.assert_array_equal(\n+            layers.UpSampling1D(size=2)(np.arange(12).reshape((2, 2, 3))),\n+            np.array(\n+                [\n+                    [\n+                        [0.0, 1.0, 2.0],\n+                        [0.0, 1.0, 2.0],\n+                        [3.0, 4.0, 5.0],\n+                        [3.0, 4.0, 5.0],\n+                    ],\n+                    [\n+                        [6.0, 7.0, 8.0],\n+                        [6.0, 7.0, 8.0],\n+                        [9.0, 10.0, 11.0],\n+                        [9.0, 10.0, 11.0],\n+                    ],\n+                ]\n+            ),\n+        )\n+\n+    def test_upsampling_1d_correctness_with_ones(self):\n+        np.testing.assert_array_equal(\n+            layers.UpSampling1D(size=3)(np.ones((2, 1, 5))), np.ones((2, 3, 5))\n+        )\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_upsampling_1d_with_dynamic_shape(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(layers.UpSampling1D(size=2)(x).shape, (None, 4, 3))\n+        self.assertEqual(layers.UpSampling1D(size=4)(x).shape, (None, 8, 3))\n+\n+        y = KerasTensor([2, None, 3])\n+        self.assertEqual(layers.UpSampling1D(size=2)(y).shape, (2, None, 3))\n+        self.assertEqual(layers.UpSampling1D(size=4)(y).shape, (2, None, 3))\n+\n+        z = KerasTensor([2, 3, None])\n+        self.assertEqual(layers.UpSampling1D(size=2)(z).shape, (2, 6, None))\n+        self.assertEqual(layers.UpSampling1D(size=4)(z).shape, (2, 12, None))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#56a43a0397c8ba10da30e69937792a589a07f184", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 143 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 4 | Methods Changed: 9 | Complexity Δ (Sum/Max): 14/12 | Churn Δ: 144 | Churn Cumulative: 260 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 6 | DMM Complexity: 0.7934782608695652\n\nDIFF:\n@@ -1,3 +1,4 @@\n from keras_core.layers.activations.elu import ELU\n from keras_core.layers.activations.leaky_relu import LeakyReLU\n+from keras_core.layers.activations.prelu import PReLU\n from keras_core.layers.activations.relu import ReLU\n\n@@ -5,7 +5,7 @@ from keras_core.layers.activations import leaky_relu\n \n \n class LeakyReLUTest(testing.TestCase):\n-    def test_relu(self):\n+    def test_leaky_relu(self):\n         self.run_layer_test(\n             leaky_relu.LeakyReLU,\n             init_kwargs={\n\n@@ -0,0 +1,108 @@\n+from keras_core import activations\n+from keras_core import constraints\n+from keras_core import initializers\n+from keras_core import regularizers\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.input_spec import InputSpec\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.PReLU\")\n+class PReLU(Layer):\n+    \"\"\"Parametric Rectified Linear Unit activation layer.\n+\n+    Formula:\n+    ``` python\n+    f(x) = negative_slope * x for x < 0\n+    f(x) = x for x >= 0\n+    ```\n+    where `negative_slope` is a learned array with the same shape as x.\n+\n+    Args:\n+        negative_slope_initializer: Initializer function for the weights.\n+        negative_slope_regularizer: Regularizer for the weights.\n+        negative_slope_constraint: Constraint for the weights.\n+        shared_axes: The axes along which to share learnable\n+            parameters for the activation function.\n+            For example, if the incoming feature maps\n+            are from a 2D convolution\n+            with output shape `(batch, height, width, channels)`,\n+            and you wish to share parameters across space\n+            so that each filter only has one set of parameters,\n+            set `shared_axes=[1, 2]`.\n+        **kwargs: Base layer keyword arguments, such as\n+            `name` and `dtype`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        negative_slope_initializer=\"Zeros\",\n+        negative_slope_regularizer=None,\n+        negative_slope_constraint=None,\n+        shared_axes=None,\n+        **kwargs\n+    ):\n+        super().__init__(**kwargs)\n+        self.supports_masking = True\n+        self.negative_slope_initializer = initializers.get(\n+            negative_slope_initializer\n+        )\n+        self.negative_slope_regularizer = regularizers.get(\n+            negative_slope_regularizer\n+        )\n+        self.negative_slope_constraint = constraints.get(\n+            negative_slope_constraint\n+        )\n+        if shared_axes is None:\n+            self.shared_axes = None\n+        elif not isinstance(shared_axes, (list, tuple)):\n+            self.shared_axes = [shared_axes]\n+        else:\n+            self.shared_axes = list(shared_axes)\n+\n+    def build(self, input_shape):\n+        param_shape = list(input_shape[1:])\n+        if self.shared_axes is not None:\n+            for i in self.shared_axes:\n+                param_shape[i - 1] = 1\n+        self.negative_slope = self.add_weight(\n+            shape=param_shape,\n+            name=\"negative_slope\",\n+            initializer=self.negative_slope_initializer,\n+            regularizer=self.negative_slope_regularizer,\n+            constraint=self.negative_slope_constraint,\n+        )\n+        # Set input spec\n+        axes = {}\n+        if self.shared_axes:\n+            for i in range(1, len(input_shape)):\n+                if i not in self.shared_axes:\n+                    axes[i] = input_shape[i]\n+        self.input_spec = InputSpec(ndim=len(input_shape), axes=axes)\n+        self.built = True\n+\n+    def call(self, inputs):\n+        pos = activations.relu(inputs)\n+        neg = -self.negative_slope * activations.relu(-inputs)\n+        return pos + neg\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"negative_slope_initializer\": initializers.serialize(\n+                    self.negative_slope_initializer\n+                ),\n+                \"negative_slope_regularizer\": regularizers.serialize(\n+                    self.negative_slope_regularizer\n+                ),\n+                \"negative_slope_constraint\": constraints.serialize(\n+                    self.negative_slope_constraint\n+                ),\n+                \"shared_axes\": self.shared_axes,\n+            }\n+        )\n+        return config\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n\n@@ -0,0 +1,33 @@\n+import numpy as np\n+\n+from keras_core import testing\n+from keras_core.layers.activations import prelu\n+\n+\n+class PReLUTest(testing.TestCase):\n+    def test_prelu(self):\n+        self.run_layer_test(\n+            prelu.PReLU,\n+            init_kwargs={\n+                \"negative_slope_initializer\": \"zeros\",\n+                \"negative_slope_regularizer\": \"L1\",\n+                \"negative_slope_constraint\": \"MaxNorm\",\n+                \"shared_axes\": 1,\n+            },\n+            input_shape=(2, 3, 4),\n+            supports_masking=True,\n+        )\n+\n+    def test_prelu_correctness(self):\n+        prelu_layer = prelu.PReLU(\n+            negative_slope_initializer=\"glorot_uniform\",\n+            negative_slope_regularizer=\"l1\",\n+            negative_slope_constraint=\"non_neg\",\n+            shared_axes=None,\n+        )\n+        test_input = np.random.randn(10, 5)\n+        result = prelu_layer(test_input)\n+        expected_output = np.maximum(\n+            0, test_input\n+        ) + prelu_layer.negative_slope.numpy() * np.minimum(0, test_input)\n+        self.assertAllClose(result, expected_output)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c24cf7394fa8b7424b45f119160589ef5ede8f15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 92 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 6 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 92 | Churn Cumulative: 523 | Contributors (this commit): 9 | Commits (past 90d): 27 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,81 @@\n+\"\"\"Tests for tf.distribute related functionality under tf implementation.\"\"\"\n+\n+import pytest\n+import tensorflow as tf\n+from tensorflow.python.eager import context\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import models\n+from keras_core import testing\n+\n+\n+@pytest.mark.skipif(\n+    backend.backend() != 'tensorflow',\n+    reason='The distribute test can only run with TF backend.',\n+)\n+class DistributeTest(testing.TestCase):\n+\n+    def setUp(self):\n+        super().setUp()\n+        # Need at least 2 devices for distribution related tests.\n+        cpus = tf.config.list_physical_devices(\"CPU\")\n+        context._reset_context()\n+        tf.config.set_logical_device_configuration(\n+            cpus[0],\n+            [\n+                tf.config.LogicalDeviceConfiguration(),\n+                tf.config.LogicalDeviceConfiguration(),\n+            ],\n+        )\n+\n+    def test_variable_creation(self):\n+        strategy = tf.distribute.MirroredStrategy(['CPU:0', 'CPU:1'])\n+        with strategy.scope():\n+            dense = layers.Dense(2)\n+            dense.build([4, 2])\n+\n+        self.assertIsInstance(dense.kernel, backend.KerasVariable)\n+        self.assertIsInstance(dense.kernel.value,\n+                              tf.distribute.DistributedValues)\n+        self.assertIn('MirroredVariable', dense.kernel.value.__class__.__name__)\n+\n+        self.assertIsInstance(dense.kernel, backend.KerasVariable)\n+        self.assertIsInstance(dense.bias.value, tf.distribute.DistributedValues)\n+        self.assertIn('MirroredVariable', dense.bias.value.__class__.__name__)\n+\n+    def test_strategy_run(self):\n+        strategy = tf.distribute.MirroredStrategy(['CPU:0', 'CPU:1'])\n+\n+        with strategy.scope():\n+            inputs = layers.Input(shape=[4])\n+            dense = layers.Dense(2)\n+            output = dense(inputs)\n+            model = models.Functional(inputs, output)\n+\n+        self.assertIsInstance(dense.kernel, backend.KerasVariable)\n+        self.assertIsInstance(dense.kernel.value,\n+                              tf.distribute.DistributedValues)\n+\n+        def input_fn(ctx):\n+            if ctx.replica_id_in_sync_group == 1:\n+                return tf.ones([8, 4])\n+            else:\n+                return tf.zeros([8, 4])\n+\n+        distributed_inputs = strategy.\\\n+            experimental_distribute_values_from_function(input_fn)\n+\n+        @tf.function\n+        def run_fn(data):\n+            return model(data)\n+\n+        result = strategy.run(run_fn, args=(distributed_inputs,))\n+\n+        self.assertIsInstance(result,\n+                              tf.types.experimental.distributed.PerReplica)\n+        self.assertLen(result.values, 2)\n+        self.assertEqual(result.values[0].shape, [8, 2])\n+        self.assertEqual(result.values[1].shape, [8, 2])\n+        self.assertNotAllClose(result.values[0], result.values[1])\n+        self.assertAllClose(result.values[0], tf.zeros([8, 2]))\n\n@@ -26,6 +26,17 @@ class TestCase(unittest.TestCase):\n     def assertAllClose(self, x1, x2, atol=1e-6, rtol=1e-6, msg=None):\n         np.testing.assert_allclose(x1, x2, atol=atol, rtol=rtol)\n \n+    def assertNotAllClose(self, x1, x2, atol=1e-6, rtol=1e-6, msg=None):\n+        try:\n+            self.assertAllClose(x1, x2, atol=atol, rtol=rtol, msg=msg)\n+        except AssertionError:\n+            return\n+        msg = msg or \"\"\n+        raise AssertionError(\n+            f\"The two values are close at all elements. \\n\"\n+            f\"{msg}.\\n\"\n+            f\"Values: {x1}\")\n+\n     def assertAlmostEqual(self, x1, x2, decimal=3, msg=None):\n         np.testing.assert_almost_equal(x1, x2, decimal=decimal)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a5b931963245464a4866efd0b80e023fd82c9e6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 432 | Lines Deleted: 9 | Files Changed: 6 | Hunks: 11 | Methods Changed: 23 | Complexity Δ (Sum/Max): 28/24 | Churn Δ: 441 | Churn Cumulative: 2183 | Contributors (this commit): 11 | Commits (past 90d): 124 | Contributors (cumulative): 25 | DMM Complexity: 0.9765625\n\nDIFF:\n@@ -57,6 +57,7 @@ from keras_core.layers.preprocessing.discretization import Discretization\n from keras_core.layers.preprocessing.normalization import Normalization\n from keras_core.layers.preprocessing.rescaling import Rescaling\n from keras_core.layers.preprocessing.resizing import Resizing\n+from keras_core.layers.preprocessing.text_vectorization import TextVectorization\n from keras_core.layers.regularization.activity_regularization import (\n     ActivityRegularization,\n )\n\n@@ -65,6 +65,7 @@ class Layer(Operation):\n         self._trainable_variables = []\n         self._non_trainable_variables = []\n         self._supports_masking = not utils.is_default(self.compute_mask)\n+        self._allow_non_tensor_positional_args = False\n         self._build_shapes_dict = None\n         self._call_signature_parameters = [\n             p.name for p in inspect.signature(self.call).parameters.values()\n@@ -333,8 +334,11 @@ class Layer(Operation):\n \n         ##########################################################\n         # 2. Enforce that only tensors can be passed positionally.\n+        if not self._allow_non_tensor_positional_args:\n             for arg in nest.flatten(args):\n-            if not isinstance(arg, KerasTensor) and not backend.is_tensor(arg):\n+                if not isinstance(arg, KerasTensor) and not backend.is_tensor(\n+                    arg\n+                ):\n                     raise ValueError(\n                         \"Only input tensors may be passed as \"\n                         \"positional arguments. The following argument value \"\n\n@@ -141,21 +141,22 @@ class Discretization(Layer):\n \n         Arguments:\n           data: The data to train on. It can be passed either as a\n-              `tf.data.Dataset`, or as a numpy array.\n+                batched `tf.data.Dataset`, or as a numpy array.\n             batch_size: Integer or `None`.\n                 Number of samples per state update.\n                 If unspecified, `batch_size` will default to 32.\n                 Do not specify the `batch_size` if your data is in the\n-              form of datasets, generators, or `keras.utils.Sequence` instances\n-              (since they generate batches).\n+                form of a `tf.data.Dataset`\n+                (it is expected to be already batched).\n             steps: Integer or `None`.\n                 Total number of steps (batches of samples)\n                 When training with input tensors such as\n-              TensorFlow data tensors, the default `None` is equal to\n+                the default `None` is equal to\n                 the number of samples in your dataset divided by\n-              the batch size, or 1 if that cannot be determined. If x is a\n-              `tf.data.Dataset`, and `steps` is `None`, the epoch will run until\n-              the input dataset is exhausted. When passing an infinitely\n+                the batch size, or 1 if that cannot be determined.\n+                If `data` is a `tf.data.Dataset`, and `steps` is `None`,\n+                the epoch will run until the input dataset is exhausted.\n+                When passing an infinitely\n                 repeating dataset, you must specify the `steps` argument. This\n                 argument is not supported with array inputs.\n         \"\"\"\n\n@@ -0,0 +1,373 @@\n+import numpy as np\n+import tensorflow as tf\n+\n+from keras_core import backend\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+from keras_core.saving import serialization_lib\n+\n+\n+@keras_core_export(\"keras_core.layers.TextVectorization\")\n+class TextVectorization(Layer):\n+    \"\"\"A preprocessing layer which maps text features to integer sequences.\n+\n+    This layer has basic options for managing text in a Keras model. It\n+    transforms a batch of strings (one example = one string) into either a list\n+    of token indices (one example = 1D tensor of integer token indices) or a\n+    dense representation (one example = 1D tensor of float values representing\n+    data about the example's tokens). This layer is meant to handle natural\n+    language inputs. To handle simple string inputs (categorical strings or\n+    pre-tokenized strings) see `kers_core.layers.StringLookup`.\n+\n+    The vocabulary for the layer must be either supplied on construction or\n+    learned via `adapt()`. When this layer is adapted, it will analyze the\n+    dataset, determine the frequency of individual string values, and create a\n+    vocabulary from them. This vocabulary can have unlimited size or be capped,\n+    depending on the configuration options for this layer; if there are more\n+    unique values in the input than the maximum vocabulary size, the most\n+    frequent terms will be used to create the vocabulary.\n+\n+    The processing of each example contains the following steps:\n+\n+    1. Standardize each example (usually lowercasing + punctuation stripping)\n+    2. Split each example into substrings (usually words)\n+    3. Recombine substrings into tokens (usually ngrams)\n+    4. Index tokens (associate a unique int value with each token)\n+    5. Transform each example using this index, either into a vector of ints or\n+       a dense float vector.\n+\n+    Some notes on passing callables to customize splitting and normalization for\n+    this layer:\n+\n+    1. Any callable can be passed to this Layer, but if you want to serialize\n+       this object you should only pass functions that are registered Keras\n+       serializables (see `keras_core.saving.register_keras_serializable`\n+       for more details).\n+    2. When using a custom callable for `standardize`, the data received\n+       by the callable will be exactly as passed to this layer. The callable\n+       should return a tensor of the same shape as the input.\n+    3. When using a custom callable for `split`, the data received by the\n+       callable will have the 1st dimension squeezed out - instead of\n+       `[[\"string to split\"], [\"another string to split\"]]`, the Callable will\n+       see `[\"string to split\", \"another string to split\"]`.\n+       The callable should return a `tf.Tensor` of dtype `string`\n+       with the first dimension containing the split tokens -\n+       in this example, we should see something like `[[\"string\", \"to\",\n+       \"split\"], [\"another\", \"string\", \"to\", \"split\"]]`.\n+\n+    **Note:** This layer wraps `tf.keras.layers.TextVectorization`. It cannot\n+    be used as part of the compiled computation graph of a model with\n+    any backend other than TensorFlow.\n+    It can however be used with any backend when running eagerly.\n+    It can also always be used as part of an input preprocessing pipeline\n+    with any backend (outside the model itself), which is how we recommend\n+    to use this layer.\n+\n+    Args:\n+        max_tokens: Maximum size of the vocabulary for this layer. This should\n+            only be specified when adapting a vocabulary or when setting\n+            `pad_to_max_tokens=True`. Note that this vocabulary\n+            contains 1 OOV token, so the effective number of tokens is\n+            `(max_tokens - 1 - (1 if output_mode == \"int\" else 0))`.\n+        standardize: Optional specification for standardization to apply to the\n+            input text. Values can be:\n+            - `None`: No standardization.\n+            - `\"lower_and_strip_punctuation\"`: Text will be lowercased and all\n+                punctuation removed.\n+            - `\"lower\"`: Text will be lowercased.\n+            - `\"strip_punctuation\"`: All punctuation will be removed.\n+            - Callable: Inputs will passed to the callable function,\n+                which should be standardized and returned.\n+        split: Optional specification for splitting the input text.\n+            Values can be:\n+            - `None`: No splitting.\n+            - `\"whitespace\"`: Split on whitespace.\n+            - `\"character\"`: Split on each unicode character.\n+            - Callable: Standardized inputs will passed to the callable\n+                function, which should be split and returned.\n+        ngrams: Optional specification for ngrams to create from the\n+            possibly-split input text. Values can be `None`, an integer\n+            or tuple of integers; passing an integer will create ngrams\n+            up to that integer, and passing a tuple of integers will\n+            create ngrams for the specified values in the tuple.\n+            Passing `None` means that no ngrams will be created.\n+        output_mode: Optional specification for the output of the layer.\n+            Values can be `\"int\"`, `\"multi_hot\"`, `\"count\"` or `\"tf_idf\"`,\n+            configuring the layer as follows:\n+            - `\"int\"`: Outputs integer indices, one integer index per split\n+                string token. When `output_mode == \"int\"`,\n+                0 is reserved for masked locations;\n+                this reduces the vocab size to `max_tokens - 2`\n+                instead of `max_tokens - 1`.\n+            - `\"multi_hot\"`: Outputs a single int array per batch, of either\n+                vocab_size or max_tokens size, containing 1s in all elements\n+                where the token mapped to that index exists at least\n+                once in the batch item.\n+            - `\"count\"`: Like `\"multi_hot\"`, but the int array contains\n+                a count of the number of times the token at that index\n+                appeared in the batch item.\n+            - `\"tf_idf\"`: Like `\"multi_hot\"`, but the TF-IDF algorithm\n+                is applied to find the value in each token slot.\n+            For `\"int\"` output, any shape of input and output is supported.\n+            For all other output modes, currently only rank 1 inputs\n+            (and rank 2 outputs after splitting) are supported.\n+        output_sequence_length: Only valid in INT mode. If set, the output will\n+            have its time dimension padded or truncated to exactly\n+            `output_sequence_length` values, resulting in a tensor of shape\n+            `(batch_size, output_sequence_length)` regardless of how many tokens\n+            resulted from the splitting step. Defaults to `None`.\n+        pad_to_max_tokens: Only valid in  `\"multi_hot\"`, `\"count\"`,\n+            and `\"tf_idf\"` modes. If `True`, the output will have\n+            its feature axis padded to `max_tokens` even if the number\n+            of unique tokens in the vocabulary is less than `max_tokens`,\n+            resulting in a tensor of shape `(batch_size, max_tokens)`\n+            regardless of vocabulary size. Defaults to `False`.\n+        vocabulary: Optional. Either an array of strings or a string path to a\n+            text file. If passing an array, can pass a tuple, list,\n+            1D NumPy array, or 1D tensor containing the string vocabulary terms.\n+            If passing a file path, the file should contain one line per term\n+            in the vocabulary. If this argument is set,\n+            there is no need to `adapt()` the layer.\n+        idf_weights: Only valid when `output_mode` is `\"tf_idf\"`. A tuple, list,\n+            1D NumPy array, or 1D tensor of the same length as the vocabulary,\n+            containing the floating point inverse document frequency weights,\n+            which will be multiplied by per sample term counts for\n+            the final `tf_idf` weight. If the `vocabulary` argument is set,\n+            and `output_mode` is `\"tf_idf\"`, this argument must be supplied.\n+        ragged: Boolean. Only applicable to `\"int\"` output mode.\n+            Only supported with TensorFlow backend.\n+            If `True`, returns a `RaggedTensor` instead of a dense `Tensor`,\n+            where each sequence may have a different length\n+            after string splitting. Defaults to `False`.\n+        sparse: Boolean. Only applicable to `\"multi_hot\"`, `\"count\"`, and\n+            `\"tf_idf\"` output modes. Only supported with TensorFlow\n+            backend. If `True`, returns a `SparseTensor`\n+            instead of a dense `Tensor`. Defaults to `False`.\n+        encoding: Optional. The text encoding to use to interpret the input\n+            strings. Defaults to `\"utf-8\"`.\n+\n+    Examples:\n+\n+    This example instantiates a `TextVectorization` layer that lowercases text,\n+    splits on whitespace, strips punctuation, and outputs integer vocab indices.\n+\n+    >>> max_tokens = 5000  # Maximum vocab size.\n+    >>> max_len = 4  # Sequence length to pad the outputs to.\n+    >>> # Create the layer.\n+    >>> vectorize_layer = TextVectorization(\n+    ...     max_tokens=max_tokens,\n+    ...     output_mode='int',\n+    ...     output_sequence_length=max_len)\n+\n+    >>> # Now that the vocab layer has been created, call `adapt` on the\n+    >>> # list of strings to create the vocabulary.\n+    >>> vectorize_layer.adapt([\"foo bar\", \"bar baz\", \"baz bada boom\"])\n+\n+    >>> # Now, the layer can map strings to integers -- you can use an\n+    >>> # embedding layer to map these integers to learned embeddings.\n+    >>> input_data = [[\"foo qux bar\"], [\"qux baz\"]]\n+    >>> vectorize_layer(input_data)\n+    array([[4, 1, 3, 0],\n+           [1, 2, 0, 0]])\n+\n+    This example instantiates a `TextVectorization` layer by passing a list\n+    of vocabulary terms to the layer's `__init__()` method.\n+\n+    >>> vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n+    >>> max_len = 4  # Sequence length to pad the outputs to.\n+    >>> # Create the layer, passing the vocab directly. You can also pass the\n+    >>> # vocabulary arg a path to a file containing one vocabulary word per\n+    >>> # line.\n+    >>> vectorize_layer = keras_core.layers.TextVectorization(\n+    ...     max_tokens=max_tokens,\n+    ...     output_mode='int',\n+    ...     output_sequence_length=max_len,\n+    ...     vocabulary=vocab_data)\n+\n+    >>> # Because we've passed the vocabulary directly, we don't need to adapt\n+    >>> # the layer - the vocabulary is already set. The vocabulary contains the\n+    >>> # padding token ('') and OOV token ('[UNK]')\n+    >>> # as well as the passed tokens.\n+    >>> vectorize_layer.get_vocabulary()\n+    ['', '[UNK]', 'earth', 'wind', 'and', 'fire']\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        max_tokens=None,\n+        standardize=\"lower_and_strip_punctuation\",\n+        split=\"whitespace\",\n+        ngrams=None,\n+        output_mode=\"int\",\n+        output_sequence_length=None,\n+        pad_to_max_tokens=False,\n+        vocabulary=None,\n+        idf_weights=None,\n+        sparse=False,\n+        ragged=False,\n+        encoding=\"utf-8\",\n+        name=None,\n+        **kwargs,\n+    ):\n+        super().__init__(name=name)\n+        if sparse and backend.backend() != \"tensorflow\":\n+            raise ValueError(\n+                \"`sparse` can only be set to True with the \"\n+                \"TensorFlow backend.\"\n+            )\n+        if ragged and backend.backend() != \"tensorflow\":\n+            raise ValueError(\n+                \"`ragged` can only be set to True with the \"\n+                \"TensorFlow backend.\"\n+            )\n+        self.layer = tf.keras.layers.TextVectorization(\n+            max_tokens=max_tokens,\n+            standardize=standardize,\n+            split=split,\n+            ngrams=ngrams,\n+            output_mode=output_mode,\n+            output_sequence_length=output_sequence_length,\n+            pad_to_max_tokens=pad_to_max_tokens,\n+            vocabulary=vocabulary,\n+            idf_weights=idf_weights,\n+            sparse=sparse,\n+            ragged=ragged,\n+            encoding=encoding,\n+            name=name,\n+            **kwargs,\n+        )\n+        self._allow_non_tensor_positional_args = True\n+\n+    def compute_output_shape(self, input_shape):\n+        return tuple(self.layer.compute_output_shape(input_shape))\n+\n+    def adapt(self, data, batch_size=None, steps=None):\n+        \"\"\"Computes a vocabulary of string terms from tokens in a dataset.\n+\n+        Calling `adapt()` on a `TextVectorization` layer is an alternative to\n+        passing in a precomputed vocabulary on construction via the `vocabulary`\n+        argument. A `TextVectorization` layer should always be either adapted\n+        over a dataset or supplied with a vocabulary.\n+\n+        During `adapt()`, the layer will build a vocabulary of all string tokens\n+        seen in the dataset, sorted by occurrence count, with ties broken by\n+        sort order of the tokens (high to low). At the end of `adapt()`, if\n+        `max_tokens` is set, the vocabulary wil be truncated to `max_tokens`\n+        size. For example, adapting a layer with `max_tokens=1000` will compute\n+        the 1000 most frequent tokens occurring in the input dataset. If\n+        `output_mode='tf-idf'`, `adapt()` will also learn the document\n+        frequencies of each token in the input dataset.\n+\n+        Arguments:\n+            data: The data to train on. It can be passed either as a\n+                batched `tf.data.Dataset`, as a list of strings,\n+                or as a NumPy array.\n+            batch_size: Integer or `None`.\n+                Number of samples per state update.\n+                If unspecified, `batch_size` will default to 32.\n+                Do not specify the `batch_size` if your data is in the\n+                form of a `tf.data.Dataset`\n+                (it is expected to be already batched).\n+            steps: Integer or `None`.\n+                Total number of steps (batches of samples)\n+                When training with input tensors such as\n+                the default `None` is equal to\n+                the number of samples in your dataset divided by\n+                the batch size, or 1 if that cannot be determined.\n+                If `data` is a `tf.data.Dataset`, and `steps` is `None`,\n+                the epoch will run until the input dataset is exhausted.\n+                When passing an infinitely\n+                repeating dataset, you must specify the `steps` argument. This\n+                argument is not supported with array inputs or list inputs.\n+        \"\"\"\n+        self.layer.adapt(data, batch_size=batch_size, steps=steps)\n+\n+    def update_state(self, data):\n+        self.layer.update_state(data)\n+\n+    def finalize_state(self):\n+        self.layer.finalize_state()\n+\n+    def reset_state(self):\n+        self.layer.reset_state()\n+\n+    def get_vocabulary(self, include_special_tokens=True):\n+        \"\"\"Returns the current vocabulary of the layer.\n+\n+        Args:\n+            include_special_tokens: If `True`, the returned vocabulary\n+                will include the padding and OOV tokens,\n+                and a term's index in the vocabulary will equal\n+                the term's index when calling the layer. If `False`, the\n+                returned vocabulary will not include any padding\n+                or OOV tokens.\n+        \"\"\"\n+        return self.layer.get_vocabulary(\n+            include_special_tokens=include_special_tokens\n+        )\n+\n+    def vocabulary_size(self):\n+        \"\"\"Gets the current size of the layer's vocabulary.\n+\n+        Returns:\n+            The integer size of the vocabulary, including optional\n+            mask and OOV indices.\n+        \"\"\"\n+        return self.layer.vocabulary_size()\n+\n+    def get_config(self):\n+        return self.layer.get_config()\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        if not isinstance(config[\"standardize\"], str):\n+            config[\"standardize\"] = serialization_lib.deserialize_keras_object(\n+                config[\"standardize\"]\n+            )\n+        if not isinstance(config[\"split\"], str):\n+            config[\"split\"] = serialization_lib.deserialize_keras_object(\n+                config[\"split\"]\n+            )\n+        return cls(**config)\n+\n+    def set_vocabulary(self, vocabulary, idf_weights=None):\n+        \"\"\"Sets vocabulary (and optionally document frequency) for this layer.\n+\n+        This method sets the vocabulary and idf weights for this layer directly,\n+        instead of analyzing a dataset through `adapt()`. It should be used\n+        whenever the vocab (and optionally document frequency) information is\n+        already known. If vocabulary data is already present in the layer, this\n+        method will replace it.\n+\n+        Args:\n+            vocabulary: Either an array or a string path to a text file.\n+                If passing an array, can pass a tuple, list, 1D NumPy array,\n+                or 1D tensor containing the vocbulary terms.\n+                If passing a file path, the file should contain one line\n+                per term in the vocabulary.\n+            idf_weights: A tuple, list, 1D NumPy array, or 1D tensor of inverse\n+                document frequency weights with equal length to vocabulary.\n+                Must be set if `output_mode` is `\"tf_idf\"`.\n+                Should not be set otherwise.\n+        \"\"\"\n+        self.layer.set_vocabulary(vocabulary, idf_weights=idf_weights)\n+\n+    def call(self, inputs):\n+        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n+            inputs = tf.convert_to_tensor(np.array(inputs))\n+        outputs = self.layer.call(inputs)\n+        if backend.backend() != \"tensorflow\":\n+            outputs = backend.convert_to_tensor(outputs)\n+        return outputs\n+\n+    def save_own_variables(self, store):\n+        self.layer.save_own_variables(store)\n+\n+    def load_own_variables(self, store):\n+        self.layer.load_own_variables(store)\n+\n+    def save_assets(self, dir_path):\n+        self.layer.save_assets(dir_path)\n+\n+    def load_assets(self, dir_path):\n+        self.layer.load_assets(dir_path)\n\n@@ -0,0 +1,43 @@\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class TextVectorizationTest(testing.TestCase):\n+    def test_config(self):\n+        layer = layers.TextVectorization(\n+            output_mode=\"int\",\n+            vocabulary=[\"one\", \"two\"],\n+            output_sequence_length=5,\n+        )\n+        self.run_class_serialization_test(layer)\n+\n+    def test_adapt_flow(self):\n+        max_tokens = 5000\n+        max_len = 4\n+        vectorize_layer = layers.TextVectorization(\n+            max_tokens=max_tokens,\n+            output_mode=\"int\",\n+            output_sequence_length=max_len,\n+        )\n+        vectorize_layer.adapt([\"foo bar\", \"bar baz\", \"baz bada boom\"])\n+        input_data = [[\"foo qux bar\"], [\"qux baz\"]]\n+        output = vectorize_layer(input_data)\n+        self.assertTrue(backend.is_tensor(output))\n+        self.assertAllClose(output, np.array([[4, 1, 3, 0], [1, 2, 0, 0]]))\n+\n+    def test_fixed_vocabulary(self):\n+        max_tokens = 5000\n+        max_len = 4\n+        vectorize_layer = layers.TextVectorization(\n+            max_tokens=max_tokens,\n+            output_mode=\"int\",\n+            output_sequence_length=max_len,\n+            vocabulary=[\"baz\", \"bar\", \"foo\"],\n+        )\n+        input_data = [[\"foo qux bar\"], [\"qux baz\"]]\n+        output = vectorize_layer(input_data)\n+        self.assertTrue(backend.is_tensor(output))\n+        self.assertAllClose(output, np.array([[4, 1, 3, 0], [1, 2, 0, 0]]))\n\n@@ -35,7 +35,8 @@ class TestCase(unittest.TestCase):\n         raise AssertionError(\n             f\"The two values are close at all elements. \\n\"\n             f\"{msg}.\\n\"\n-            f\"Values: {x1}\")\n+            f\"Values: {x1}\"\n+        )\n \n     def assertAlmostEqual(self, x1, x2, decimal=3, msg=None):\n         np.testing.assert_almost_equal(x1, x2, decimal=decimal)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
