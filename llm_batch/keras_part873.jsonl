{"custom_id": "keras#2db547ba789ad786314d92438bd5ff789e11179a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 12 | Files Changed: 4 | Hunks: 16 | Methods Changed: 13 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 94 | Churn Cumulative: 617 | Contributors (this commit): 5 | Commits (past 90d): 14 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -40,14 +40,19 @@ class RandomContrast(BaseImagePreprocessingLayer):\n             `[1.0 - lower, 1.0 + upper]`. For any pixel x in the channel,\n             the output will be `(x - mean) * factor + mean`\n             where `mean` is the mean value of the channel.\n+        value_range: the range of values the incoming images will have.\n+            Represented as a two-number tuple written `[low, high]`. This is\n+            typically either `[0, 1]` or `[0, 255]` depending on how your\n+            preprocessing pipeline is set up.\n         seed: Integer. Used to create a random seed.\n     \"\"\"\n \n     _FACTOR_BOUNDS = (0, 1)\n \n-    def __init__(self, factor, seed=None, **kwargs):\n+    def __init__(self, factor, value_range=(0, 255), seed=None, **kwargs):\n         super().__init__(**kwargs)\n         self._set_factor(factor)\n+        self.value_range = value_range\n         self.seed = seed\n         self.generator = SeedGenerator(seed)\n \n@@ -89,7 +94,9 @@ class RandomContrast(BaseImagePreprocessingLayer):\n         if training:\n             constrast_factor = transformation[\"contrast_factor\"]\n             outputs = self._adjust_constrast(images, constrast_factor)\n-            outputs = self.backend.numpy.clip(outputs, 0, 255)\n+            outputs = self.backend.numpy.clip(\n+                outputs, self.value_range[0], self.value_range[1]\n+            )\n             self.backend.numpy.reshape(outputs, self.backend.shape(images))\n             return outputs\n         return images\n@@ -135,6 +142,7 @@ class RandomContrast(BaseImagePreprocessingLayer):\n     def get_config(self):\n         config = {\n             \"factor\": self.factor,\n+            \"value_range\": self.value_range,\n             \"seed\": self.seed,\n         }\n         base_config = super().get_config()\n\n@@ -14,6 +14,7 @@ class RandomContrastTest(testing.TestCase):\n             layers.RandomContrast,\n             init_kwargs={\n                 \"factor\": 0.75,\n+                \"value_range\": (0, 255),\n                 \"seed\": 1,\n             },\n             input_shape=(8, 3, 4, 3),\n@@ -24,6 +25,7 @@ class RandomContrastTest(testing.TestCase):\n             layers.RandomContrast,\n             init_kwargs={\n                 \"factor\": 0.75,\n+                \"value_range\": (0, 255),\n                 \"seed\": 1,\n                 \"data_format\": \"channels_first\",\n             },\n@@ -32,21 +34,67 @@ class RandomContrastTest(testing.TestCase):\n             expected_output_shape=(8, 3, 4, 4),\n         )\n \n-    def test_random_contrast(self):\n+    def test_random_contrast_with_value_range_0_to_255(self):\n         seed = 9809\n         np.random.seed(seed)\n+\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n             inputs = np.random.random((12, 8, 16, 3))\n-        layer = layers.RandomContrast(factor=0.5, seed=seed)\n-        outputs = layer(inputs)\n+            height_axis = -3\n+            width_axis = -2\n+        else:\n+            inputs = np.random.random((12, 3, 8, 16))\n+            height_axis = -2\n+            width_axis = -1\n+\n+        inputs = backend.convert_to_tensor(inputs, dtype=\"float32\")\n+        layer = layers.RandomContrast(\n+            factor=0.5, value_range=(0, 255), seed=seed\n+        )\n+        transformation = layer.get_random_transformation(inputs, training=True)\n+        outputs = layer.transform_images(inputs, transformation, training=True)\n \n         # Actual contrast arithmetic\n         np.random.seed(seed)\n-        factor = np.random.uniform(0.5, 1.5)\n-        inp_mean = np.mean(inputs, axis=-3, keepdims=True)\n-        inp_mean = np.mean(inp_mean, axis=-2, keepdims=True)\n+        factor = backend.convert_to_numpy(transformation[\"contrast_factor\"])\n+        inputs = backend.convert_to_numpy(inputs)\n+        inp_mean = np.mean(inputs, axis=height_axis, keepdims=True)\n+        inp_mean = np.mean(inp_mean, axis=width_axis, keepdims=True)\n         actual_outputs = (inputs - inp_mean) * factor + inp_mean\n         outputs = backend.convert_to_numpy(outputs)\n-        actual_outputs = np.clip(outputs, 0, 255)\n+        actual_outputs = np.clip(actual_outputs, 0, 255)\n+\n+        self.assertAllClose(outputs, actual_outputs)\n+\n+    def test_random_contrast_with_value_range_0_to_1(self):\n+        seed = 9809\n+        np.random.seed(seed)\n+\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            inputs = np.random.random((12, 8, 16, 3))\n+            height_axis = -3\n+            width_axis = -2\n+        else:\n+            inputs = np.random.random((12, 3, 8, 16))\n+            height_axis = -2\n+            width_axis = -1\n+\n+        inputs = backend.convert_to_tensor(inputs, dtype=\"float32\")\n+        layer = layers.RandomContrast(factor=0.5, value_range=(0, 1), seed=seed)\n+        transformation = layer.get_random_transformation(inputs, training=True)\n+        outputs = layer.transform_images(inputs, transformation, training=True)\n+\n+        # Actual contrast arithmetic\n+        np.random.seed(seed)\n+        factor = backend.convert_to_numpy(transformation[\"contrast_factor\"])\n+        inputs = backend.convert_to_numpy(inputs)\n+        inp_mean = np.mean(inputs, axis=height_axis, keepdims=True)\n+        inp_mean = np.mean(inp_mean, axis=width_axis, keepdims=True)\n+        actual_outputs = (inputs - inp_mean) * factor + inp_mean\n+        outputs = backend.convert_to_numpy(outputs)\n+        actual_outputs = np.clip(actual_outputs, 0, 1)\n \n         self.assertAllClose(outputs, actual_outputs)\n \n\n@@ -44,7 +44,12 @@ class RandomHue(BaseImagePreprocessingLayer):\n     _FACTOR_BOUNDS = (0, 1)\n \n     def __init__(\n-        self, factor, value_range, data_format=None, seed=None, **kwargs\n+        self,\n+        factor,\n+        value_range=(0, 255),\n+        data_format=None,\n+        seed=None,\n+        **kwargs,\n     ):\n         super().__init__(data_format=data_format, **kwargs)\n         self._set_factor(factor)\n\n@@ -31,14 +31,23 @@ class RandomHueTest(testing.TestCase):\n         output = layer(inputs, training=False)\n         self.assertAllClose(inputs, output)\n \n-    def test_random_hue_value_range(self):\n+    def test_random_hue_value_range_0_to_1(self):\n         image = keras.random.uniform(shape=(3, 3, 3), minval=0, maxval=1)\n \n+        layer = layers.RandomHue(0.2, (0, 1))\n+        adjusted_image = layer(image)\n+\n+        self.assertTrue(keras.ops.numpy.all(adjusted_image >= 0))\n+        self.assertTrue(keras.ops.numpy.all(adjusted_image <= 1))\n+\n+    def test_random_hue_value_range_0_to_255(self):\n+        image = keras.random.uniform(shape=(3, 3, 3), minval=0, maxval=255)\n+\n         layer = layers.RandomHue(0.2, (0, 255))\n         adjusted_image = layer(image)\n \n         self.assertTrue(keras.ops.numpy.all(adjusted_image >= 0))\n-        self.assertTrue(keras.ops.numpy.all(adjusted_image <= 1))\n+        self.assertTrue(keras.ops.numpy.all(adjusted_image <= 255))\n \n     def test_random_hue_no_change_with_zero_factor(self):\n         data_format = backend.config.image_data_format()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7c491bd160a3d3eb5896a6316fdd7c997a1013c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 41 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -702,7 +702,6 @@ class OneHotMeanIoU(MeanIoU):\n             associated label.\n         axis: (Optional) The dimension containing the logits. Defaults to `-1`.\n \n-    Example:\n \n     Example:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0d3ba37b19af86c704ea7624828490b9a3a732c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 341 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 5 | Methods Changed: 14 | Complexity Δ (Sum/Max): 34/21 | Churn Δ: 341 | Churn Cumulative: 1148 | Contributors (this commit): 7 | Commits (past 90d): 31 | Contributors (cumulative): 21 | DMM Complexity: 0.7688888888888888\n\nDIFF:\n@@ -155,6 +155,9 @@ from keras.src.layers.preprocessing.image_preprocessing.mix_up import MixUp\n from keras.src.layers.preprocessing.image_preprocessing.random_brightness import (\n     RandomBrightness,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.random_color_jitter import (\n+    RandomColorJitter,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.random_contrast import (\n     RandomContrast,\n )\n\n@@ -155,6 +155,9 @@ from keras.src.layers.preprocessing.image_preprocessing.mix_up import MixUp\n from keras.src.layers.preprocessing.image_preprocessing.random_brightness import (\n     RandomBrightness,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.random_color_jitter import (\n+    RandomColorJitter,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.random_contrast import (\n     RandomContrast,\n )\n\n@@ -99,6 +99,9 @@ from keras.src.layers.preprocessing.image_preprocessing.mix_up import MixUp\n from keras.src.layers.preprocessing.image_preprocessing.random_brightness import (\n     RandomBrightness,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.random_color_jitter import (\n+    RandomColorJitter,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.random_contrast import (\n     RandomContrast,\n )\n\n@@ -0,0 +1,197 @@\n+import keras.src.layers.preprocessing.image_preprocessing.random_brightness as random_brightness  # noqa: E501\n+import keras.src.layers.preprocessing.image_preprocessing.random_contrast as random_contrast  # noqa: E501\n+import keras.src.layers.preprocessing.image_preprocessing.random_hue as random_hue  # noqa: E501\n+import keras.src.layers.preprocessing.image_preprocessing.random_saturation as random_saturation  # noqa: E501\n+from keras.src.api_export import keras_export\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n+from keras.src.random.seed_generator import SeedGenerator\n+from keras.src.utils import backend_utils\n+\n+\n+@keras_export(\"keras.layers.RandomColorJitter\")\n+class RandomColorJitter(BaseImagePreprocessingLayer):\n+    \"\"\"RandomColorJitter class randomly apply brightness, contrast, saturation\n+    and hue image processing operation sequentially and randomly on the\n+    input.\n+\n+    Args:\n+        value_range: the range of values the incoming images will have.\n+            Represented as a two number tuple written [low, high].\n+            This is typically either `[0, 1]` or `[0, 255]` depending\n+            on how your preprocessing pipeline is set up.\n+        brightness_factor: Float or a list/tuple of 2 floats between -1.0\n+            and 1.0. The factor is used to determine the lower bound and\n+            upper bound of the brightness adjustment. A float value will\n+            be chosen randomly between the limits. When -1.0 is chosen,\n+            the output image will be black, and when 1.0 is chosen, the\n+            image will be fully white. When only one float is provided,\n+            eg, 0.2, then -0.2 will be used for lower bound and 0.2 will\n+            be used for upper bound.\n+        contrast_factor: a positive float represented as fraction of value,\n+            or a tuple of size 2 representing lower and upper bound. When\n+            represented as a single float, lower = upper. The contrast\n+            factor will be randomly picked between `[1.0 - lower, 1.0 +\n+            upper]`. For any pixel x in the channel, the output will be\n+            `(x - mean) * factor + mean` where `mean` is the mean value\n+            of the channel.\n+        saturation_factor: A tuple of two floats or a single float. `factor`\n+            controls the extent to which the image saturation is impacted.\n+            `factor=0.5` makes this layer perform a no-op operation.\n+            `factor=0.0` makes the image fully grayscale. `factor=1.0`\n+            makes the image fully saturated. Values should be between\n+            `0.0` and `1.0`. If a tuple is used, a `factor` is sampled\n+            between the two values for every image augmented. If a single\n+            float is used, a value between `0.0` and the passed float is\n+            sampled. To ensure the value is always the same, pass a tuple\n+            with two identical floats: `(0.5, 0.5)`.\n+        hue_factor: A single float or a tuple of two floats. `factor`\n+            controls the extent to which the image hue is impacted.\n+            `factor=0.0` makes this layer perform a no-op operation,\n+            while a value of `1.0` performs the most aggressive contrast\n+            adjustment available. If a tuple is used, a `factor` is\n+            sampled between the two values for every image augmented.\n+            If a single float is used, a value between `0.0` and the\n+            passed float is sampled. In order to ensure the value is\n+            always the same, please pass a tuple with two identical\n+            floats: `(0.5, 0.5)`.\n+        seed: Integer. Used to create a random seed.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        value_range=(0, 255),\n+        brightness_factor=None,\n+        contrast_factor=None,\n+        saturation_factor=None,\n+        hue_factor=None,\n+        seed=None,\n+        data_format=None,\n+        **kwargs,\n+    ):\n+        super().__init__(data_format=data_format, **kwargs)\n+        self.value_range = value_range\n+        self.brightness_factor = brightness_factor\n+        self.contrast_factor = contrast_factor\n+        self.saturation_factor = saturation_factor\n+        self.hue_factor = hue_factor\n+        self.seed = seed\n+        self.generator = SeedGenerator(seed)\n+\n+        self.random_brightness = None\n+        self.random_contrast = None\n+        self.random_saturation = None\n+        self.random_hue = None\n+\n+        if self.brightness_factor is not None:\n+            self.random_brightness = random_brightness.RandomBrightness(\n+                factor=self.brightness_factor,\n+                value_range=self.value_range,\n+                seed=self.seed,\n+            )\n+\n+        if self.contrast_factor is not None:\n+            self.random_contrast = random_contrast.RandomContrast(\n+                factor=self.contrast_factor,\n+                value_range=self.value_range,\n+                seed=self.seed,\n+            )\n+\n+        if self.saturation_factor is not None:\n+            self.random_saturation = random_saturation.RandomSaturation(\n+                factor=self.saturation_factor,\n+                value_range=self.value_range,\n+                seed=self.seed,\n+            )\n+\n+        if self.hue_factor is not None:\n+            self.random_hue = random_hue.RandomHue(\n+                factor=self.hue_factor,\n+                value_range=self.value_range,\n+                seed=self.seed,\n+            )\n+\n+    def transform_images(self, images, transformation, training=True):\n+        if training:\n+            if backend_utils.in_tf_graph():\n+                self.backend.set_backend(\"tensorflow\")\n+            images = self.backend.cast(images, self.compute_dtype)\n+            if self.brightness_factor is not None:\n+                if backend_utils.in_tf_graph():\n+                    self.random_brightness.backend.set_backend(\"tensorflow\")\n+                transformation = (\n+                    self.random_brightness.get_random_transformation(\n+                        images,\n+                        seed=self._get_seed_generator(self.backend._backend),\n+                    )\n+                )\n+                images = self.random_brightness.transform_images(\n+                    images, transformation\n+                )\n+            if self.contrast_factor is not None:\n+                if backend_utils.in_tf_graph():\n+                    self.random_contrast.backend.set_backend(\"tensorflow\")\n+                transformation = self.random_contrast.get_random_transformation(\n+                    images, seed=self._get_seed_generator(self.backend._backend)\n+                )\n+                transformation[\"contrast_factor\"] = self.backend.cast(\n+                    transformation[\"contrast_factor\"], dtype=self.compute_dtype\n+                )\n+                images = self.random_contrast.transform_images(\n+                    images, transformation\n+                )\n+            if self.saturation_factor is not None:\n+                if backend_utils.in_tf_graph():\n+                    self.random_saturation.backend.set_backend(\"tensorflow\")\n+                transformation = (\n+                    self.random_saturation.get_random_transformation(\n+                        images,\n+                        seed=self._get_seed_generator(self.backend._backend),\n+                    )\n+                )\n+                images = self.random_saturation.transform_images(\n+                    images, transformation\n+                )\n+            if self.hue_factor is not None:\n+                if backend_utils.in_tf_graph():\n+                    self.random_hue.backend.set_backend(\"tensorflow\")\n+                transformation = self.random_hue.get_random_transformation(\n+                    images, seed=self._get_seed_generator(self.backend._backend)\n+                )\n+                images = self.random_hue.transform_images(\n+                    images, transformation\n+                )\n+            images = self.backend.cast(images, self.compute_dtype)\n+        return images\n+\n+    def transform_labels(self, labels, transformation, training=True):\n+        return labels\n+\n+    def transform_bounding_boxes(\n+        self,\n+        bounding_boxes,\n+        transformation,\n+        training=True,\n+    ):\n+        return bounding_boxes\n+\n+    def transform_segmentation_masks(\n+        self, segmentation_masks, transformation, training=True\n+    ):\n+        return segmentation_masks\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def get_config(self):\n+        config = {\n+            \"value_range\": self.value_range,\n+            \"brightness_factor\": self.brightness_factor,\n+            \"contrast_factor\": self.contrast_factor,\n+            \"saturation_factor\": self.saturation_factor,\n+            \"hue_factor\": self.hue_factor,\n+            \"seed\": self.seed,\n+        }\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,135 @@\n+import numpy as np\n+import pytest\n+from tensorflow import data as tf_data\n+\n+from keras.src import backend\n+from keras.src import layers\n+from keras.src import testing\n+\n+\n+class RandomColorJitterTest(testing.TestCase):\n+    @pytest.mark.requires_trainable_backend\n+    def test_layer(self):\n+        self.run_layer_test(\n+            layers.RandomColorJitter,\n+            init_kwargs={\n+                \"value_range\": (20, 200),\n+                \"brightness_factor\": 0.2,\n+                \"contrast_factor\": 0.2,\n+                \"saturation_factor\": 0.2,\n+                \"hue_factor\": 0.2,\n+                \"seed\": 1,\n+            },\n+            input_shape=(8, 3, 4, 3),\n+            supports_masking=False,\n+            expected_output_shape=(8, 3, 4, 3),\n+        )\n+\n+    def test_random_color_jitter_inference(self):\n+        seed = 3481\n+        layer = layers.RandomColorJitter(\n+            value_range=(0, 1),\n+            brightness_factor=0.1,\n+            contrast_factor=0.2,\n+            saturation_factor=0.9,\n+            hue_factor=0.1,\n+        )\n+\n+        np.random.seed(seed)\n+        inputs = np.random.randint(0, 255, size=(224, 224, 3))\n+        output = layer(inputs, training=False)\n+        self.assertAllClose(inputs, output)\n+\n+    def test_brightness_only(self):\n+        seed = 2390\n+        np.random.seed(seed)\n+\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            inputs = np.random.random((12, 8, 16, 3))\n+        else:\n+            inputs = np.random.random((12, 3, 8, 16))\n+\n+        layer = layers.RandomColorJitter(\n+            brightness_factor=[0.5, 0.5], seed=seed\n+        )\n+        output = backend.convert_to_numpy(layer(inputs))\n+\n+        layer = layers.RandomBrightness(factor=[0.5, 0.5], seed=seed)\n+        sub_output = backend.convert_to_numpy(layer(inputs))\n+\n+        self.assertAllClose(output, sub_output)\n+\n+    def test_saturation_only(self):\n+        seed = 2390\n+        np.random.seed(seed)\n+\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            inputs = np.random.random((12, 8, 16, 3))\n+        else:\n+            inputs = np.random.random((12, 3, 8, 16))\n+\n+        layer = layers.RandomColorJitter(\n+            saturation_factor=[0.5, 0.5], seed=seed\n+        )\n+        output = layer(inputs)\n+\n+        layer = layers.RandomSaturation(factor=[0.5, 0.5], seed=seed)\n+        sub_output = layer(inputs)\n+\n+        self.assertAllClose(output, sub_output)\n+\n+    def test_hue_only(self):\n+        seed = 2390\n+        np.random.seed(seed)\n+\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            inputs = np.random.random((12, 8, 16, 3))\n+        else:\n+            inputs = np.random.random((12, 3, 8, 16))\n+\n+        layer = layers.RandomColorJitter(hue_factor=[0.5, 0.5], seed=seed)\n+        output = layer(inputs)\n+\n+        layer = layers.RandomHue(factor=[0.5, 0.5], seed=seed)\n+        sub_output = layer(inputs)\n+\n+        self.assertAllClose(output, sub_output)\n+\n+    def test_contrast_only(self):\n+        seed = 2390\n+        np.random.seed(seed)\n+\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            inputs = np.random.random((12, 8, 16, 3))\n+        else:\n+            inputs = np.random.random((12, 3, 8, 16))\n+\n+        layer = layers.RandomColorJitter(contrast_factor=[0.5, 0.5], seed=seed)\n+        output = layer(inputs)\n+\n+        layer = layers.RandomContrast(factor=[0.5, 0.5], seed=seed)\n+        sub_output = layer(inputs)\n+\n+        self.assertAllClose(output, sub_output)\n+\n+    def test_tf_data_compatibility(self):\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            input_data = np.random.random((2, 8, 8, 3))\n+        else:\n+            input_data = np.random.random((2, 3, 8, 8))\n+        layer = layers.RandomColorJitter(\n+            value_range=(0, 1),\n+            brightness_factor=0.1,\n+            contrast_factor=0.2,\n+            saturation_factor=0.9,\n+            hue_factor=0.1,\n+        )\n+\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        for output in ds.take(1):\n+            output.numpy()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3dd958bcd6f7e9d5f7e007919205716ea1215df7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 14 | Files Changed: 8 | Hunks: 21 | Methods Changed: 17 | Complexity Δ (Sum/Max): 9/2 | Churn Δ: 56 | Churn Cumulative: 1800 | Contributors (this commit): 5 | Commits (past 90d): 29 | Contributors (cumulative): 24 | DMM Complexity: 0.9615384615384616\n\nDIFF:\n@@ -170,25 +170,31 @@ class Equalization(BaseImagePreprocessingLayer):\n         )\n         return self.backend.numpy.take(lookup_table, indices)\n \n-    def transform_images(self, images, transformations=None, **kwargs):\n+    def transform_images(self, images, transformation, training=True):\n+        if training:\n             images = self.backend.cast(images, self.compute_dtype)\n \n             if self.data_format == \"channels_first\":\n                 channels = []\n                 for i in range(self.backend.core.shape(images)[-3]):\n                     channel = images[..., i, :, :]\n-                equalized = self._equalize_channel(channel, self.value_range)\n+                    equalized = self._equalize_channel(\n+                        channel, self.value_range\n+                    )\n                     channels.append(equalized)\n                 equalized_images = self.backend.numpy.stack(channels, axis=-3)\n             else:\n                 channels = []\n                 for i in range(self.backend.core.shape(images)[-1]):\n                     channel = images[..., i]\n-                equalized = self._equalize_channel(channel, self.value_range)\n+                    equalized = self._equalize_channel(\n+                        channel, self.value_range\n+                    )\n                     channels.append(equalized)\n                 equalized_images = self.backend.numpy.stack(channels, axis=-1)\n \n             return self.backend.cast(equalized_images, self.compute_dtype)\n+        return images\n \n     def compute_output_shape(self, input_shape):\n         return input_shape\n@@ -196,14 +202,19 @@ class Equalization(BaseImagePreprocessingLayer):\n     def compute_output_spec(self, inputs, **kwargs):\n         return inputs\n \n-    def transform_bounding_boxes(self, bounding_boxes, **kwargs):\n+    def transform_bounding_boxes(\n+        self,\n+        bounding_boxes,\n+        transformation,\n+        training=True,\n+    ):\n         return bounding_boxes\n \n-    def transform_labels(self, labels, transformations=None, **kwargs):\n+    def transform_labels(self, labels, transformation, training=True):\n         return labels\n \n     def transform_segmentation_masks(\n-        self, segmentation_masks, transformations, **kwargs\n+        self, segmentation_masks, transformation, training=True\n     ):\n         return segmentation_masks\n \n\n@@ -122,6 +122,7 @@ class RandomCrop(BaseImagePreprocessingLayer):\n         return h_start, w_start\n \n     def transform_images(self, images, transformation, training=True):\n+        if training:\n             images = self.backend.cast(images, self.compute_dtype)\n             crop_box_hstart, crop_box_wstart = transformation\n             crop_height = self.height\n@@ -197,6 +198,8 @@ class RandomCrop(BaseImagePreprocessingLayer):\n             \"labels\": (num_boxes, num_classes),\n         }\n         \"\"\"\n+\n+        if training:\n             h_start, w_start = transformation\n             if not self.backend.is_tensor(bounding_boxes[\"boxes\"]):\n                 bounding_boxes = densify_bounding_boxes(\n@@ -247,6 +250,7 @@ class RandomCrop(BaseImagePreprocessingLayer):\n                 \"boxes\": boxes,\n                 \"labels\": bounding_boxes[\"labels\"],\n             }\n+        return bounding_boxes\n \n     def transform_segmentation_masks(\n         self, segmentation_masks, transformation, training=True\n\n@@ -101,9 +101,6 @@ class RandomFlip(BaseImagePreprocessingLayer):\n         transformation,\n         training=True,\n     ):\n-        if backend_utils.in_tf_graph():\n-            self.backend.set_backend(\"tensorflow\")\n-\n         def _flip_boxes_horizontal(boxes):\n             x1, x2, x3, x4 = self.backend.numpy.split(boxes, 4, axis=-1)\n             outputs = self.backend.numpy.concatenate(\n@@ -134,6 +131,10 @@ class RandomFlip(BaseImagePreprocessingLayer):\n                 )\n             return bboxes\n \n+        if training:\n+            if backend_utils.in_tf_graph():\n+                self.backend.set_backend(\"tensorflow\")\n+\n             flips = self.backend.numpy.squeeze(transformation[\"flips\"], axis=-1)\n \n             if self.data_format == \"channels_first\":\n\n@@ -71,17 +71,21 @@ class RandomGrayscale(BaseImagePreprocessingLayer):\n         )\n         return should_apply\n \n-    def transform_images(self, images, transformations=None, **kwargs):\n+    def transform_images(self, images, transformation, training=True):\n+        if training:\n             should_apply = (\n-            transformations\n-            if transformations is not None\n+                transformation\n+                if transformation is not None\n                 else self.get_random_transformation(images)\n             )\n \n             grayscale_images = self.backend.image.rgb_to_grayscale(\n                 images, data_format=self.data_format\n             )\n-        return self.backend.numpy.where(should_apply, grayscale_images, images)\n+            return self.backend.numpy.where(\n+                should_apply, grayscale_images, images\n+            )\n+        return images\n \n     def compute_output_shape(self, input_shape):\n         return input_shape\n\n@@ -131,6 +131,7 @@ class RandomRotation(BaseImagePreprocessingLayer):\n         transformation,\n         training=True,\n     ):\n+        if training:\n             ops = self.backend\n             boxes = bounding_boxes[\"boxes\"]\n             height = transformation[\"image_height\"]\n\n@@ -215,6 +215,7 @@ class RandomTranslation(BaseImagePreprocessingLayer):\n         transformation,\n         training=True,\n     ):\n+        if training:\n             if backend_utils.in_tf_graph():\n                 self.backend.set_backend(\"tensorflow\")\n \n\n@@ -217,6 +217,7 @@ class RandomZoom(BaseImagePreprocessingLayer):\n         transformation,\n         training=True,\n     ):\n+        if training:\n             if backend_utils.in_tf_graph():\n                 self.backend.set_backend(\"tensorflow\")\n \n@@ -240,7 +241,9 @@ class RandomZoom(BaseImagePreprocessingLayer):\n             )\n \n             zooms = self.backend.cast(\n-            self.backend.numpy.concatenate([width_zoom, height_zoom], axis=1),\n+                self.backend.numpy.concatenate(\n+                    [width_zoom, height_zoom], axis=1\n+                ),\n                 dtype=\"float32\",\n             )\n             transform = self._get_zoom_matrix(zooms, height, width)\n\n@@ -156,6 +156,8 @@ class Solarization(BaseImagePreprocessingLayer):\n \n     def transform_images(self, images, transformation, training=True):\n         images = self.backend.cast(images, self.compute_dtype)\n+\n+        if training:\n             if transformation is None:\n                 return images\n \n@@ -183,6 +185,7 @@ class Solarization(BaseImagePreprocessingLayer):\n             if backend.is_int_dtype(images.dtype):\n                 results = self.backend.numpy.round(results)\n             return _saturate_cast(results, images.dtype, self.backend)\n+        return images\n \n     def transform_labels(self, labels, transformation, training=True):\n         return labels\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c1316e5f1219aeff063ec904c37ebd78e3de8ff2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 78 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -8,16 +8,15 @@ from keras.src.api_export import keras_export\n from keras.src.utils import io_utils\n \n try:\n-    # pydot-ng is a fork of pydot that is better maintained.\n+    import pydot\n+except ImportError:\n+    # pydot_ng and pydotplus are older forks of pydot\n+    # which may still be used by some users\n+    try:\n         import pydot_ng as pydot\n     except ImportError:\n-    # pydotplus is an improved version of pydot\n         try:\n             import pydotplus as pydot\n-    except ImportError:\n-        # Fall back on pydot if necessary.\n-        try:\n-            import pydot\n         except ImportError:\n             pydot = None\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#df002a9942a99624bda25cd296e86b4d0a2e5ac8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 111 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 8/5 | Churn Δ: 117 | Churn Cumulative: 255 | Contributors (this commit): 6 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: 0.9696969696969697\n\nDIFF:\n@@ -1,6 +1,7 @@\n from keras.src import backend\n from keras.src import ops\n from keras.src.api_export import keras_export\n+from keras.src.backend import KerasTensor\n from keras.src.layers.layer import Layer\n \n \n@@ -84,6 +85,8 @@ class Attention(Layer):\n                 f\"Received: score_mode={score_mode}\"\n             )\n \n+        self._return_attention_scores = False\n+\n     def build(self, input_shape):\n         self._validate_inputs(input_shape)\n         self.scale = None\n@@ -217,6 +220,7 @@ class Attention(Layer):\n         use_causal_mask=False,\n     ):\n         self._validate_inputs(inputs=inputs, mask=mask)\n+        self._return_attention_scores = return_attention_scores\n         q = inputs[0]\n         v = inputs[1]\n         k = inputs[2] if len(inputs) > 2 else v\n@@ -226,16 +230,17 @@ class Attention(Layer):\n         scores_mask = self._calculate_score_mask(\n             scores, v_mask, use_causal_mask\n         )\n-        result, attention_scores = self._apply_scores(\n+        attention_output, attention_scores = self._apply_scores(\n             scores=scores, value=v, scores_mask=scores_mask, training=training\n         )\n         if q_mask is not None:\n             # Mask of shape [batch_size, Tq, 1].\n             q_mask = ops.expand_dims(q_mask, axis=-1)\n-            result *= ops.cast(q_mask, dtype=result.dtype)\n+            attention_output *= ops.cast(q_mask, dtype=attention_output.dtype)\n         if return_attention_scores:\n-            return result, attention_scores\n-        return result\n+            return (attention_output, attention_scores)\n+        else:\n+            return attention_output\n \n     def compute_mask(self, inputs, mask=None):\n         self._validate_inputs(inputs=inputs, mask=mask)\n@@ -244,8 +249,49 @@ class Attention(Layer):\n         return ops.convert_to_tensor(mask[0])\n \n     def compute_output_shape(self, input_shape):\n-        \"\"\"Returns shape of value tensor dim, but for query tensor length\"\"\"\n-        return (*input_shape[0][:-1], input_shape[1][-1])\n+        query_shape, value_shape, key_shape = input_shape\n+        if key_shape is None:\n+            key_shape = value_shape\n+\n+        output_shape = (*query_shape[:-1], value_shape[-1])\n+        if self._return_attention_scores:\n+            scores_shape = (query_shape[0], query_shape[1], key_shape[1])\n+            return output_shape, scores_shape\n+        return output_shape\n+\n+    def compute_output_spec(\n+        self,\n+        inputs,\n+        mask=None,\n+        return_attention_scores=False,\n+        training=None,\n+        use_causal_mask=False,\n+    ):\n+        # Validate and unpack inputs\n+        self._validate_inputs(inputs, mask)\n+        query = inputs[0]\n+        value = inputs[1]\n+        key = inputs[2] if len(inputs) > 2 else value\n+\n+        # Compute primary output shape\n+        output_shape = self.compute_output_shape(\n+            [query.shape, value.shape, key.shape]\n+        )\n+        output_spec = KerasTensor(output_shape, dtype=self.compute_dtype)\n+\n+        # Handle attention scores if requested\n+        if self._return_attention_scores:\n+            scores_shape = (\n+                query.shape[0],\n+                query.shape[1],\n+                key.shape[1],\n+            )  # (batch_size, Tq, Tv)\n+            attention_scores_spec = KerasTensor(\n+                scores_shape, dtype=self.compute_dtype\n+            )\n+            return (output_spec, attention_scores_spec)\n+\n+        return output_spec\n \n     def _validate_inputs(self, inputs, mask=None):\n         \"\"\"Validates arguments of the call method.\"\"\"\n\n@@ -358,3 +358,62 @@ class AttentionTest(testing.TestCase):\n             ),\n             output.shape,\n         )\n+\n+    def test_return_attention_scores_true(self):\n+        \"\"\"Test that the layer returns attention scores along with outputs.\"\"\"\n+        # Generate dummy input data\n+        query = np.random.random((2, 8, 16)).astype(np.float32)\n+        value = np.random.random((2, 4, 16)).astype(np.float32)\n+\n+        # Initialize the Attention layer\n+        layer = layers.Attention()\n+\n+        # Call the layer with return_attention_scores=True\n+        output, attention_scores = layer(\n+            [query, value], return_attention_scores=True\n+        )\n+\n+        # Check the shape of the outputs\n+        self.assertEqual(output.shape, (2, 8, 16))  # Output shape\n+        self.assertEqual(\n+            attention_scores.shape, (2, 8, 4)\n+        )  # Attention scores shape\n+\n+    def test_return_attention_scores_true_and_tuple(self):\n+        \"\"\"Test that the layer outputs are a tuple when\n+        return_attention_scores=True.\"\"\"\n+        # Generate dummy input data\n+        query = np.random.random((2, 8, 16)).astype(np.float32)\n+        value = np.random.random((2, 4, 16)).astype(np.float32)\n+\n+        # Initialize the Attention layer\n+        layer = layers.Attention()\n+\n+        # Call the layer with return_attention_scores=True\n+        outputs = layer([query, value], return_attention_scores=True)\n+\n+        # Check that outputs is a tuple\n+        self.assertIsInstance(\n+            outputs, tuple, \"Expected the outputs to be a tuple\"\n+        )\n+\n+    def test_return_attention_scores_true_tuple_then_unpack(self):\n+        \"\"\"Test that outputs can be unpacked correctly.\"\"\"\n+        # Generate dummy input data\n+        query = np.random.random((2, 8, 16)).astype(np.float32)\n+        value = np.random.random((2, 4, 16)).astype(np.float32)\n+\n+        # Initialize the Attention layer\n+        layer = layers.Attention()\n+\n+        # Call the layer with return_attention_scores=True\n+        outputs = layer([query, value], return_attention_scores=True)\n+\n+        # Unpack the outputs\n+        output, attention_scores = outputs\n+\n+        # Check the shape of the unpacked outputs\n+        self.assertEqual(output.shape, (2, 8, 16))  # Output shape\n+        self.assertEqual(\n+            attention_scores.shape, (2, 8, 4)\n+        )  # Attention scores shape\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f54c127a5da4c905588813e79e219c780cebfb4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 218 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 5 | Methods Changed: 15 | Complexity Δ (Sum/Max): 26/16 | Churn Δ: 218 | Churn Cumulative: 1034 | Contributors (this commit): 7 | Commits (past 90d): 34 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -155,6 +155,9 @@ from keras.src.layers.preprocessing.image_preprocessing.mix_up import MixUp\n from keras.src.layers.preprocessing.image_preprocessing.random_brightness import (\n     RandomBrightness,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration import (\n+    RandomColorDegeneration,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.random_color_jitter import (\n     RandomColorJitter,\n )\n\n@@ -155,6 +155,9 @@ from keras.src.layers.preprocessing.image_preprocessing.mix_up import MixUp\n from keras.src.layers.preprocessing.image_preprocessing.random_brightness import (\n     RandomBrightness,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration import (\n+    RandomColorDegeneration,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.random_color_jitter import (\n     RandomColorJitter,\n )\n\n@@ -99,6 +99,9 @@ from keras.src.layers.preprocessing.image_preprocessing.mix_up import MixUp\n from keras.src.layers.preprocessing.image_preprocessing.random_brightness import (\n     RandomBrightness,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration import (\n+    RandomColorDegeneration,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.random_color_jitter import (\n     RandomColorJitter,\n )\n\n@@ -0,0 +1,132 @@\n+from keras.src.api_export import keras_export\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n+from keras.src.random import SeedGenerator\n+\n+\n+@keras_export(\"keras.layers.RandomColorDegeneration\")\n+class RandomColorDegeneration(BaseImagePreprocessingLayer):\n+    \"\"\"Randomly performs the color degeneration operation on given images.\n+\n+    The sharpness operation first converts an image to gray scale, then back to\n+    color. It then takes a weighted average between original image and the\n+    degenerated image. This makes colors appear more dull.\n+\n+    Args:\n+        factor: A tuple of two floats or a single float.\n+            `factor` controls the extent to which the\n+            image sharpness is impacted. `factor=0.0` makes this layer perform a\n+            no-op operation, while a value of 1.0 uses the degenerated result\n+            entirely. Values between 0 and 1 result in linear interpolation\n+            between the original image and the sharpened image.\n+            Values should be between `0.0` and `1.0`. If a tuple is used, a\n+            `factor` is sampled between the two values for every image\n+            augmented. If a single float is used, a value between `0.0` and the\n+            passed float is sampled. In order to ensure the value is always the\n+            same, please pass a tuple with two identical floats: `(0.5, 0.5)`.\n+        seed: Integer. Used to create a random seed.\n+    \"\"\"\n+\n+    _VALUE_RANGE_VALIDATION_ERROR = (\n+        \"The `value_range` argument should be a list of two numbers. \"\n+    )\n+\n+    def __init__(\n+        self,\n+        factor,\n+        value_range=(0, 255),\n+        data_format=None,\n+        seed=None,\n+        **kwargs,\n+    ):\n+        super().__init__(data_format=data_format, **kwargs)\n+        self._set_factor(factor)\n+        self._set_value_range(value_range)\n+        self.seed = seed\n+        self.generator = SeedGenerator(seed)\n+\n+    def _set_value_range(self, value_range):\n+        if not isinstance(value_range, (tuple, list)):\n+            raise ValueError(\n+                self._VALUE_RANGE_VALIDATION_ERROR\n+                + f\"Received: value_range={value_range}\"\n+            )\n+        if len(value_range) != 2:\n+            raise ValueError(\n+                self._VALUE_RANGE_VALIDATION_ERROR\n+                + f\"Received: value_range={value_range}\"\n+            )\n+        self.value_range = sorted(value_range)\n+\n+    def get_random_transformation(self, data, training=True, seed=None):\n+        if isinstance(data, dict):\n+            images = data[\"images\"]\n+        else:\n+            images = data\n+        images_shape = self.backend.shape(images)\n+        rank = len(images_shape)\n+        if rank == 3:\n+            batch_size = 1\n+        elif rank == 4:\n+            batch_size = images_shape[0]\n+        else:\n+            raise ValueError(\n+                \"Expected the input image to be rank 3 or 4. Received: \"\n+                f\"inputs.shape={images_shape}\"\n+            )\n+\n+        if seed is None:\n+            seed = self._get_seed_generator(self.backend._backend)\n+\n+        factor = self.backend.random.uniform(\n+            (batch_size, 1, 1, 1),\n+            minval=self.factor[0],\n+            maxval=self.factor[1],\n+            seed=seed,\n+        )\n+        factor = factor\n+        return {\"factor\": factor}\n+\n+    def transform_images(self, images, transformation=None, training=True):\n+        if training:\n+            images = self.backend.cast(images, self.compute_dtype)\n+            factor = self.backend.cast(\n+                transformation[\"factor\"], self.compute_dtype\n+            )\n+            degenerates = self.backend.image.rgb_to_grayscale(\n+                images, data_format=self.data_format\n+            )\n+            images = images + factor * (degenerates - images)\n+            images = self.backend.numpy.clip(\n+                images, self.value_range[0], self.value_range[1]\n+            )\n+            images = self.backend.cast(images, self.compute_dtype)\n+        return images\n+\n+    def transform_labels(self, labels, transformation, training=True):\n+        return labels\n+\n+    def transform_segmentation_masks(\n+        self, segmentation_masks, transformation, training=True\n+    ):\n+        return segmentation_masks\n+\n+    def transform_bounding_boxes(\n+        self, bounding_boxes, transformation, training=True\n+    ):\n+        return bounding_boxes\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"factor\": self.factor,\n+                \"value_range\": self.value_range,\n+                \"seed\": self.seed,\n+            }\n+        )\n+        return config\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n\n@@ -0,0 +1,77 @@\n+import numpy as np\n+import pytest\n+from tensorflow import data as tf_data\n+\n+import keras\n+from keras.src import backend\n+from keras.src import layers\n+from keras.src import testing\n+\n+\n+class RandomColorDegenerationTest(testing.TestCase):\n+    @pytest.mark.requires_trainable_backend\n+    def test_layer(self):\n+        self.run_layer_test(\n+            layers.RandomColorDegeneration,\n+            init_kwargs={\n+                \"factor\": 0.75,\n+                \"value_range\": (0, 1),\n+                \"seed\": 1,\n+            },\n+            input_shape=(8, 3, 4, 3),\n+            supports_masking=False,\n+            expected_output_shape=(8, 3, 4, 3),\n+        )\n+\n+    def test_random_color_degeneration_value_range(self):\n+        image = keras.random.uniform(shape=(3, 3, 3), minval=0, maxval=1)\n+\n+        layer = layers.RandomColorDegeneration(0.2, value_range=(0, 1))\n+        adjusted_image = layer(image)\n+\n+        self.assertTrue(keras.ops.numpy.all(adjusted_image >= 0))\n+        self.assertTrue(keras.ops.numpy.all(adjusted_image <= 1))\n+\n+    def test_random_color_degeneration_no_op(self):\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            inputs = np.random.random((2, 8, 8, 3))\n+        else:\n+            inputs = np.random.random((2, 3, 8, 8))\n+\n+        layer = layers.RandomColorDegeneration((0.5, 0.5))\n+        output = layer(inputs, training=False)\n+        self.assertAllClose(inputs, output, atol=1e-3, rtol=1e-5)\n+\n+    def test_random_color_degeneration_factor_zero(self):\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            inputs = np.random.random((2, 8, 8, 3))\n+        else:\n+            inputs = np.random.random((2, 3, 8, 8))\n+        layer = layers.RandomColorDegeneration(factor=(0.0, 0.0))\n+        result = layer(inputs)\n+\n+        self.assertAllClose(inputs, result, atol=1e-3, rtol=1e-5)\n+\n+    def test_random_color_degeneration_randomness(self):\n+        image = keras.random.uniform(shape=(3, 3, 3), minval=0, maxval=1)[:5]\n+\n+        layer = layers.RandomColorDegeneration(0.2)\n+        adjusted_images = layer(image)\n+\n+        self.assertNotAllClose(adjusted_images, image)\n+\n+    def test_tf_data_compatibility(self):\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            input_data = np.random.random((2, 8, 8, 3))\n+        else:\n+            input_data = np.random.random((2, 3, 8, 8))\n+        layer = layers.RandomColorDegeneration(\n+            factor=0.5, data_format=data_format, seed=1337\n+        )\n+\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        for output in ds.take(1):\n+            output.numpy()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
