{"custom_id": "scrapy#7b0877c50e1dd8d31ddceb1859a4bac712dc3cc5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 152 | Files Changed: 7 | Hunks: 12 | Methods Changed: 16 | Complexity Δ (Sum/Max): -19/0 | Churn Δ: 175 | Churn Cumulative: 264 | Contributors (this commit): 2 | Commits (past 90d): 30 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,79 +0,0 @@\n-from datetime import datetime\n-\n-from django.db import models\n-from django.template.defaultfilters import slugify\n-from django.utils.translation import ugettext_lazy as _\n-\n-\n-REST_HELP_TEXT = _(\"\"\"ReST markup language allowed\n-                      <a href='http://en.wikipedia.org/wiki/ReStructuredText'>\n-                      Read more</a>\"\"\")\n-\n-MAIN_HELP_TEXT = _(\"Useful to filter articles, like those public on homepage\")\n-\n-\n-class Article(models.Model):\n-    title = models.CharField(_(\"title\"), max_length=256, core=True,\n-                             blank=False)\n-    slug = models.SlugField(_(\"slug\"), prepopulate_from=(\"title\",),\n-                            editable=False)\n-    text = models.TextField(_(\"text\"), core=True, help_text=REST_HELP_TEXT)\n-    main = models.BooleanField(_(\"main\"), core=True, blank=False,\n-                               default=False, help_text=MAIN_HELP_TEXT)\n-    position = models.IntegerField(_(\"position\"), core=True, blank=False,\n-                                   default=0)\n-    publish = models.BooleanField(_(\"publish\"), core=True, default=False)\n-\n-    # automatic dates\n-    created = models.DateTimeField(core=True, editable=False)\n-    updated = models.DateTimeField(core=True, editable=False)\n-\n-    def toggle_publish(self):\n-        self.publish = not self.publish\n-        self.save()\n-\n-    def position_up(self):\n-        self.position += 1\n-        self.save()\n-\n-    def position_down(self):\n-        self.position -= 1\n-        self.save()\n-\n-    def save(self):\n-        if not self.id:\n-            self.created = datetime.now()\n-        self.updated = datetime.now()\n-        self.slug = slugify(self.title)\n-        super(Article, self).save()\n-\n-    def __unicode__(self):\n-        return self.title\n-\n-    # ugly, but django-admin isn't very versatile right now\n-    def position_link(self):\n-        return _(\"%(position)s (<a href='/admin/article/article/%(id)s/position/up/'>Up</a>\" \\\n-                 \" | <a href='/admin/article/article/%(id)s/position/down/'>Down</a>)\") % \\\n-               { \"position\": self.position, \"id\": self.id }\n-    position_link.short_description = u\"position\"\n-    position_link.allow_tags = True\n-\n-    def publish_link(self):\n-        img_url = \"/media/img/admin/icon-%s.gif\" % \\\n-                  (self.publish and \"yes\" or \"no\")\n-        html = _('<img alt=\"%s\" src=\"' + img_url + '\"/> ' \\\n-                 '(<a href=\"%s/publish/toggle/\">Toggle</a>)')\n-        return html % (_(str(self.publish)), self.id)\n-    publish_link.short_description = u\"publish\"\n-    publish_link.allow_tags = True\n-\n-    class Admin:\n-        list_display = (\"title\", \"main\", \"position_link\", \"publish_link\",\n-                        \"updated\")\n-        list_filter = (\"main\", \"created\", \"publish\")\n-        save_on_top = True\n-\n-    class Meta:\n-        verbose_name = _(\"article\")\n-        verbose_name_plural = _(\"articles\")\n-        ordering = [ \"-position\", ]\n\n\n@@ -1,47 +0,0 @@\n-from django import template\n-\n-from scrapyorg.lib.templatetags import *\n-\n-from scrapyorg.article.models import Article\n-\n-\n-register = template.Library()\n-\n-\n-@register.tag(name=\"load_main_articles\")\n-def do_load_main_articles(parser, token):\n-    return do_load(parser, token, True)\n-\n-\n-@register.tag(name=\"load_last_articles\")\n-def do_load_last_articles(parser, token):\n-    return do_load(parser, token)\n-\n-\n-def do_load(parser, token, only_main=False):\n-    syntax_msg = 'Syntax: %s \"COUNT\" as \"VAR_NAME\"'\n-\n-    try:\n-        tag, count, _as, var_name = token.split_contents()\n-\n-        if not is_string(count) or not is_string(var_name):\n-            raise_syntax(syntax_msg % tag)\n-        count = int(unquoute(count))\n-    except:\n-        raise_syntax(syntax_msg % token.split_contents()[0])\n-    return LoadArticlesNode(count, unquoute(var_name), only_main)\n-\n-\n-class LoadArticlesNode(template.Node):\n-    def __init__(self, count, var_name, only_main=False):\n-        self.only_main = only_main\n-        self.count = count\n-        self.var_name = var_name\n-\n-    def render(self, context):\n-        articles = Article.objects.filter(publish=True)\n-        if self.only_main:\n-            articles = articles.filter(main=True)\n-\n-        context[self.var_name] = articles[:self.count]\n-        return ''\n\n@@ -4,7 +4,6 @@ from scrapyorg.article.views import *\n \n \n urlpatterns = patterns('',\n-    (r\"^(?P<article_id>\\d+)/position/up/$\", position_up),\n-    (r\"^(?P<article_id>\\d+)/position/down/$\", position_down),\n-    (r\"^(?P<article_id>\\d+)/publish/toggle/$\", publish_toggle),\n+    (r\"^$\", render_template, { \"path\": \"home\" }),\n+    (r\"(?P<path>.*)/\", render_template),\n )\n\n@@ -1,24 +1,21 @@\n-from django.shortcuts import get_object_or_404\n-from django.http import HttpResponseRedirect\n-from django.contrib.admin.views.decorators import staff_member_required\n+from os.path import join\n \n-from scrapyorg.article.models import Article\n+from django.template import TemplateDoesNotExist\n+from django.template.context import RequestContext\n+from django.shortcuts import render_to_response\n+from django.http import Http404\n \n \n-@staff_member_required\n-def position_up(request, article_id):\n-    article = get_object_or_404(Article, pk=article_id)\n-    article.position_up()\n-    return HttpResponseRedirect(\"/admin/article/article/\")\n+ARTICLES_TEMPLATES_DIR = \"articles\"\n \n-@staff_member_required\n-def position_down(request, article_id):\n-    article = get_object_or_404(Article, pk=article_id)\n-    article.position_down()\n-    return HttpResponseRedirect(\"/admin/article/article/\")\n \n-@staff_member_required\n-def publish_toggle(request, article_id):\n-    article = get_object_or_404(Article, pk=article_id)\n-    article.toggle_publish()\n-    return HttpResponseRedirect(\"/admin/article/article/\")\n+def render_template(request, path):\n+    if not path.endswith(\".html\"):\n+        path = path + \".html\"\n+    path = join(ARTICLES_TEMPLATES_DIR, path)\n+\n+    try:\n+        c = RequestContext(request)\n+        return render_to_response(path, context_instance=c)\n+    except TemplateDoesNotExist, e:\n+        raise Http404(\"Article does not exists\")\n\n@@ -62,7 +62,6 @@ MIDDLEWARE_CLASSES = (\n     'django.contrib.sessions.middleware.SessionMiddleware',\n     'django.contrib.auth.middleware.AuthenticationMiddleware',\n     'django.middleware.doc.XViewMiddleware',\n-    'django.contrib.flatpages.middleware.FlatpageFallbackMiddleware',\n )\n \n ROOT_URLCONF = 'scrapyorg.urls'\n\n@@ -4,12 +4,9 @@ from django.conf import settings\n \n \n urlpatterns = patterns('',\n-    (r\"^$\", direct_to_template, { \"template\": \"home.html\" }),\n-    (r\"^article/\", include(\"scrapyorg.article.urls\")),\n     (r\"^weblog/\", include(\"scrapyorg.blog.urls\")),\n \n     # admin\n-    (r\"^admin/article/article/\", include(\"scrapyorg.article.urls\")),\n     (r\"^admin/download/downloadlink/\", include(\"scrapyorg.download.urls\")),\n     (r\"^admin/\", include(\"django.contrib.admin.urls\")),\n )\n@@ -19,3 +16,8 @@ if settings.DEBUG: # devel\n     urlpatterns += patterns('',         \n         (r'^%s/(?P<path>.*)$' % settings.MEDIA_URL[1:], 'django.views.static.serve', {'document_root': settings.MEDIA_ROOT}),\n     )\n+    \n+# last resort, it's an article\n+urlpatterns += patterns('',         \n+    (r\"\", include(\"scrapyorg.article.urls\")),\n+)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c76200561d81036de6b2f3d9593587a22aaa24fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 2 | Churn Cumulative: 78 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -57,8 +57,10 @@ class Report(object):\n \n     def engine_stopped(self):\n         if self.passed:\n+            if self.passed_file:\n                 self.passed_file.write('\\n--- Total scraped products: %d\\n' % self.total['passed'])\n                 self.passed_file.close()\n         if self.dropped:\n+            if self.dropped_file:\n                 self.dropped_file.write('\\n--- Total dropped products: %d\\n' % self.total['dropped'])\n                 self.dropped_file.close()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f955c9693b080281cba16ece40080f398bc512d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 80 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ class Report(object):\n     def get_product_text(self, product, dropped=False):\n         product_text = '###Product\\n%s' % self.get_product_attribs(product)\n         if product.variants:\n-            product_text = '%s\\n##Variants\\n%s' % (product_text, ''.join([self.get_product_attribs(variant) for variant in product.variants]))\n+            product_text = '%s\\n##Variants\\n%s' % (product_text, '\\n'.join([self.get_product_attribs(variant) for variant in product.variants]))\n         if dropped:\n             product_text = '%s--- Dropping reason: %s ---\\n' % (product_text, dropped)\n         product_text = product_text + '\\n\\n'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0f65d2f2081040d858f374dae18a2166dc3139e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 85 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,7 +10,7 @@ class Report(object):\n         self.dropped_file = None\n         self.passed = passed\n         self.dropped = dropped\n-        self.total = { 'passed': 0, 'dropped': 0 }\n+        self.total = { 'passed': 0, 'dropped': 0, 'variants': 0 }\n \n         dispatcher.connect(self.domain_open, signal=signals.domain_open)\n         dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n@@ -49,6 +49,7 @@ class Report(object):\n \n     def item_passed(self, item, spider):\n         self.total['passed'] += 1\n+        self.total['variants'] += len(item.variants)\n         self.passed_file.write(self.get_product_text(item))\n \n     def item_dropped(self, item, spider, response, exception):\n@@ -58,7 +59,7 @@ class Report(object):\n     def engine_stopped(self):\n         if self.passed:\n             if self.passed_file:\n-                self.passed_file.write('\\n--- Total scraped products: %d\\n' % self.total['passed'])\n+                self.passed_file.write('\\n--- Total scraped: %d products + %d variants\\n' % (self.total['passed'], self.total['variants']))\n                 self.passed_file.close()\n         if self.dropped:\n             if self.dropped_file:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#380a65e72104bff579038aecc4fc9caaaa419d50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 237 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -48,6 +48,7 @@ class Replay(object):\n         self._load(repfile, usedir)\n \n         settings.overrides['CACHE2_DIR'] = self.cache2dir\n+        settings.overrides['CACHE2_EXPIRATION_SECS'] = -1\n         settings.overrides['CACHE2_IGNORE_MISSING'] = self.playing or self.updating\n         settings.overrides['CACHE2_SECTORIZE'] = False\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3badb39ed719cc48d9713887dad75ee321286071", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 72 | Files Changed: 2 | Hunks: 4 | Methods Changed: 9 | Complexity Δ (Sum/Max): -21/0 | Churn Δ: 72 | Churn Cumulative: 208 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 3 | DMM Complexity: 0.034482758620689655\n\nDIFF:\n@@ -1,7 +1,6 @@\n from scrapy.command import ScrapyCommand\n from scrapy.core.manager import scrapymanager\n from scrapy.replay import Replay\n-from scrapy.report import Report\n from scrapy.conf import settings\n \n \n@@ -19,8 +18,6 @@ class Command(ScrapyCommand):\n         parser.add_option(\"--restrict\", dest=\"restrict\", action=\"store_true\", help=\"restrict crawling only to the given urls\")\n         parser.add_option(\"--record\", dest=\"record\", help=\"use FILE for recording session (see replay command)\", metavar=\"FILE\")\n         parser.add_option(\"--record-dir\", dest=\"recorddir\", help=\"use DIR for recording (instead of file)\", metavar=\"DIR\")\n-        parser.add_option(\"--report\", dest=\"doreport\", action='store_true', help=\"generate a report of the scraped products in a text file\")\n-        parser.add_option(\"--report-dropped\", dest=\"doreport_dropped\", action=\"store_true\", help=\"generate a report of the dropped products in a text file\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -38,8 +35,6 @@ class Command(ScrapyCommand):\n             # disconnecting since pydispatcher uses weak references \n             self.replay = Replay(opts.record or opts.recorddir, mode='record', usedir=bool(opts.recorddir))\n             self.replay.record(args=args, opts=opts.__dict__)\n-        if opts.doreport or opts.doreport_dropped:\n-            self.report = Report(passed=opts.doreport, dropped=opts.doreport_dropped)\n \n     def run(self, args, opts):\n         scrapymanager.runonce(*args, **opts.__dict__)\n\n@@ -1,67 +0,0 @@\n-# -*- coding: utf8 -*-\n-from datetime import datetime\n-from pydispatch import dispatcher\n-from scrapy.core import signals\n-\n-class Report(object):\n-    def __init__(self, passed, dropped):\n-        self.domain = ''\n-        self.passed_file = None\n-        self.dropped_file = None\n-        self.passed = passed\n-        self.dropped = dropped\n-        self.total = { 'passed': 0, 'dropped': 0, 'variants': 0 }\n-\n-        dispatcher.connect(self.domain_open, signal=signals.domain_open)\n-        dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n-        if self.passed:\n-            dispatcher.connect(self.item_passed, signal=signals.item_passed)\n-        if self.dropped:\n-            dispatcher.connect(self.item_dropped, signal=signals.item_dropped)\n-\n-    def get_product_attribs(self, product):\n-        product_attribs = ''\n-        for attrib, value in product.iteritems():\n-            product_attribs = '%s%s: %s\\n' % (product_attribs, attrib, value)\n-        return product_attribs\n-\n-    def get_product_text(self, product, dropped=False):\n-        product_text = '###Product\\n%s' % self.get_product_attribs(product)\n-        if product.variants:\n-            product_text = '%s\\n##Variants\\n%s' % (product_text, '\\n'.join([self.get_product_attribs(variant) for variant in product.variants]))\n-        if dropped:\n-            product_text = '%s--- Dropping reason: %s ---\\n' % (product_text, dropped)\n-        product_text = product_text + '\\n\\n'\n-        return product_text\n-\n-    def domain_open(self, domain, spider):\n-        self.domain = domain\n-        now = datetime.now()\n-        filename = '%s_%s_%s.report' % (self.domain, now.strftime('%Y%m%d'), now.strftime('%H%M'))\n-        if self.passed:\n-            self.passed_file = open(filename, 'w')\n-            self.passed_file.write('Scraping results for domain \"%s\"\\n\\n%s%s%s' % (self.domain, '##################################\\n',\n-              '### Products scraped correctly ###\\n', '##################################\\n'))\n-        if self.dropped:\n-            self.dropped_file = open(filename + '.dropped', 'w')\n-            self.dropped_file.write('Scraping results for domain \"%s\"\\n\\n%s%s%s' % (self.domain,\n-              '########################\\n', '### Dropped products ###\\n', '########################\\n'))\n-\n-    def item_passed(self, item, spider):\n-        self.total['passed'] += 1\n-        self.total['variants'] += len(item.variants)\n-        self.passed_file.write(self.get_product_text(item))\n-\n-    def item_dropped(self, item, spider, response, exception):\n-        self.total['dropped'] += 1\n-        self.dropped_file.write(self.get_product_text(item, exception))\n-\n-    def engine_stopped(self):\n-        if self.passed:\n-            if self.passed_file:\n-                self.passed_file.write('\\n--- Total scraped: %d products + %d variants\\n' % (self.total['passed'], self.total['variants']))\n-                self.passed_file.close()\n-        if self.dropped:\n-            if self.dropped_file:\n-                self.dropped_file.write('\\n--- Total dropped products: %d\\n' % self.total['dropped'])\n-                self.dropped_file.close()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7a195a97949ea88a0d2e8227789b603786181ca2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 58 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -25,6 +25,9 @@ class RedirectMiddleware(object):\n                     redirected.body = None\n                     # This is needed to avoid redirection loops with requests that contain dont_filter = True\n                     # Example (9 May 2008): http://www.55max.com/product/001_photography.asp?3233,0,0,0,Michael+Banks\n+                    if isinstance(redirected.dont_filter, int):\n+                        redirected.dont_filter -= 1\n+                    else:\n                         redirected.dont_filter = False\n                     log.msg(\"Redirecting (%s) to %s from %s\" % (status, redirected, request), level=log.DEBUG, domain=spider.domain_name)\n                     return redirected\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7fc59232495a433379cfa11c22b709d2954ff305", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 23 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,5 +18,6 @@ def mysql_connect(db_uri, **kwargs):\n             del(d['passwd'])\n \n         d['charset'] = \"utf8\"\n+        d['reconnect'] = 1\n         d.update(kwargs)\n         return MySQLdb.connect(**d)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#388f7641cf0ccf57744e8616f24efd794055eae2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 25 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -18,6 +18,6 @@ def mysql_connect(db_uri, **kwargs):\n             del(d['passwd'])\n \n         d['charset'] = \"utf8\"\n-        d['reconnect'] = 1\n+        #d['reconnect'] = 1\n         d.update(kwargs)\n         return MySQLdb.connect(**d)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#22eb48dae55bf3a322693c5dfdaae7821216d5e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 29 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,6 +2,7 @@\n Function for dealing with databases\n \"\"\"\n import re\n+from scrapy.conf import settings\n \n def mysql_connect(db_uri, **kwargs):\n     \"\"\"\n@@ -17,7 +18,6 @@ def mysql_connect(db_uri, **kwargs):\n         if d['passwd'] is None:\n             del(d['passwd'])\n \n-        d['charset'] = \"utf8\"\n-        #d['reconnect'] = 1\n+        d.update(settings.get(\"MYSQL_CONNECTION_SETTINGS\"))\n         d.update(kwargs)\n         return MySQLdb.connect(**d)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#246bdedcfb6a622b7b58723437548b1609649aca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 96 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -81,7 +81,7 @@ urlmapping = (\n \n class UrlToGuidService(WebSite):\n     def __init__(self):\n-        if not settings['WS_ENABLED']:\n+        if not settings.getbool('WS_ENABLED'):\n             raise NotConfigured\n \n         port = settings.getint('WS_PORT') or 8088\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6b890c1b2875687b90bce9cacd5b823ad633d5f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 31 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ Function for dealing with databases\n \"\"\"\n import re\n from scrapy.conf import settings\n+from scrapy.core import log\n \n def mysql_connect(db_uri, **kwargs):\n     \"\"\"\n@@ -20,4 +21,5 @@ def mysql_connect(db_uri, **kwargs):\n \n         d.update(settings.get(\"MYSQL_CONNECTION_SETTINGS\"))\n         d.update(kwargs)\n+        log.msg(\"Connecting db with settings %s\" % d )\n         return MySQLdb.connect(**d)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3f7100fe1ec0392c05a7812426c1a5dc1d73beff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 39 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -21,5 +21,11 @@ def mysql_connect(db_uri, **kwargs):\n \n         d.update(settings.get(\"MYSQL_CONNECTION_SETTINGS\"))\n         d.update(kwargs)\n-        log.msg(\"Connecting db with settings %s\" % d )\n+        \n+        dcopy = d.copy()\n+        print dcopy\n+        if dcopy.get(\"passwd\"):\n+            dcopy[\"passwd\"] = \"********\"\n+        log.msg(\"Connecting db with settings %s\" % dcopy )\n+        \n         return MySQLdb.connect(**d)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#59d6e925823cb9c8b3ee7862a3e781e48d6f321a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 40 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,7 +23,6 @@ def mysql_connect(db_uri, **kwargs):\n         d.update(kwargs)\n         \n         dcopy = d.copy()\n-        print dcopy\n         if dcopy.get(\"passwd\"):\n             dcopy[\"passwd\"] = \"********\"\n         log.msg(\"Connecting db with settings %s\" % dcopy )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4606b7f9c5eb09c7b3666544baa82cc4ba775aef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 583 | Files Changed: 7 | Hunks: 7 | Methods Changed: 43 | Complexity Δ (Sum/Max): -129/0 | Churn Δ: 583 | Churn Cumulative: 1166 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 14 | DMM Complexity: 0.3622448979591837\n\nDIFF:\n@@ -1,2 +0,0 @@\n-from scrapy.contrib.cluster.master import ClusterMaster, ClusterMasterWeb\n-from scrapy.contrib.cluster.worker import ClusterWorker, ClusterWorkerWeb\n\n@@ -1,2 +0,0 @@\n-from scrapy.contrib.cluster.master.manager import ClusterMaster\n-from scrapy.contrib.cluster.master.web import ClusterMasterWeb\n\n@@ -1,189 +0,0 @@\n-import urlparse\n-import urllib\n-import bisect\n-\n-from pydispatch import dispatcher\n-from twisted.web.client import getPage\n-\n-from scrapy.core import log, signals\n-from scrapy.core.engine import scrapyengine\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.serialization import unserialize\n-from scrapy.conf import settings\n-\n-class ClusterNode(object):\n-    def __init__(self, name, url):\n-        self.name = name\n-        self.url = url\n-        self.maxproc = 0\n-        self.running = {}\n-        self.pending = []\n-        self.loadavg = (0.0, 0.0, 0.0)\n-        self.status = \"down\"  # down/crawling/idle/error\n-        self.available = False\n-\n-        self.wsurl = urlparse.urljoin(self.url, \"cluster_worker/ws/?format=json\")\n-\n-    def update(self):\n-        d = getPage(self.wsurl)\n-        d.addCallbacks(self._cbUpdate, self._ebUpdate)\n-\n-    def schedule(self, domains):\n-        args = [(\"schedule\", domain) for domain in domains]\n-        self._wsRequest(args)\n-\n-    def stop(self, domains):\n-        args = [(\"stop\", domain) for domain in domains]\n-        self._wsRequest(args)\n-\n-    def remove(self, domains):\n-        args = [(\"remove\", domain) for domain in domains]\n-        self._wsRequest(args)\n-\n-    def _wsRequest(self, args):\n-        d = getPage(\"%s?%s\" % (self.wsurl, urllib.urlencode(args)))\n-        d.addCallbacks(self._cbUpdate, self._ebUpdate)\n-        \n-    def _cbUpdate(self, jsonstatus):\n-        try:\n-            pmstatus = unserialize(jsonstatus, 'json')\n-            self.maxproc = int(pmstatus.get('maxproc', None))\n-            self.running = pmstatus.get('running') or {}\n-            self.pending = pmstatus.get('pending') or []\n-            self.loadavg = tuple(pmstatus.get('loadavg', (0.0, 0.0, 0.0)))\n-            self.status = \"crawling\" if self.running else \"idle\"\n-            self.available = True\n-        except ValueError:\n-            self.status = \"error\"\n-            self.available = False\n-\n-    def _ebUpdate(self, err):\n-        self.status = \"down\"\n-        self.available = False\n-\n-\n-class ClusterMaster(object):\n-\n-    def __init__(self):\n-        if not settings.getbool('CLUSTER_MASTER_ENABLED'):\n-            raise NotConfigured\n-        self.nodes = {}\n-\n-        dispatcher.connect(self._engine_started, signal=signals.engine_started)\n-\n-    def load_nodes(self):\n-        \"\"\"Loads nodes from the CLUSTER_MASTER_NODES setting\"\"\"\n-        for name, url in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():\n-            self.add_node(name, url)\n-\n-    def update_nodes(self):\n-        for node in self.nodes.itervalues():\n-            node.update()\n-\n-    def add_node(self, name, url):\n-        \"\"\"Add node given its node\"\"\"\n-        if name not in self.nodes:\n-            node = ClusterNode(name, url)\n-            self.nodes[name] = node\n-            node.update()\n-\n-    def remove_node(self, nodename):\n-        raise NotImplemented\n-\n-    def schedule(self, domains, nodename=None):\n-        if nodename:\n-            node = self.nodes[nodename]\n-            node.schedule(domains)\n-        else:\n-            self._dispatch_domains(domains)\n-\n-    def stop(self, domains):\n-        to_stop = {}\n-        for domain in domains:\n-            node = self.running.get(domain, None)\n-            if node:\n-                if node.name not in to_stop:\n-                    to_stop[node.name] = []\n-                to_stop[node.name].append(domain)\n-\n-        for nodename, domains in to_stop.iteritems():\n-            self.nodes[nodename].stop(domains)\n-\n-    def remove(self, domains):\n-        to_remove = {}\n-        for domain in domains:\n-            node = self.pending.get(domain, None)\n-            if node:\n-                if node.name not in to_remove:\n-                    to_remove[node.name] = []\n-                to_remove[node.name].append(domain)\n-\n-        for nodename, domains in to_remove.iteritems():\n-            self.nodes[nodename].remove(domains)\n-\n-    def discard(self, domains):\n-        \"\"\"Stop and remove all running and pending instances of the given\n-        domains\"\"\"\n-        self.remove(domains)\n-        self.stop(domains)\n-\n-    @property\n-    def running(self):\n-        \"\"\"Return dict of running domains as domain -> node\"\"\"\n-        d = {}\n-        for node in self.nodes.itervalues():\n-            for domain in node.running.iterkeys():\n-                d[domain] = node\n-        return d\n-\n-    @property\n-    def pending(self):\n-        \"\"\"Return dict of pending domains as domain -> node\"\"\"\n-        d = {}\n-        for node in self.nodes.itervalues():\n-            for domain in node.pending:\n-                d[domain] = node\n-        return d\n-\n-    @property\n-    def available_nodes(self):\n-        return (node for node in self.nodes.itervalues() if node.available)\n-\n-    def _dispatch_domains(self, domains):\n-        \"\"\"Schedule the given domains in the availables nodes as good as\n-        possible. The algorithm follows the next rules (in order):\n-        \n-        1. search for nodes with available capacity(running < maxproc) and (if\n-        any) schedules the domains there\n-\n-        2. if there isn't any node with available capacity it schedules the\n-        domain in the node with the smallest number of pending spiders\n-        \"\"\"\n-\n-        to_schedule = {}  # domains to schedule per node\n-        pending_node = [] # list of #pending, node\n-\n-        for node in self.available_nodes:\n-            capacity = node.maxproc - len(node.running)\n-            bisect.insort(pending_node, (len(node.pending), node))\n-\n-            to_schedule[node.name] = []\n-            while domains and capacity > 0:\n-                to_schedule[node.name].append(domains.pop(0))\n-                capacity -= 1\n-            if not domains:\n-                break\n-\n-        for domain in domains:\n-            pending, node = pending_node.pop(0)\n-            to_schedule[node.name].append(domain)\n-            bisect.insort(pending_node, (pending+1, node))\n-            \n-        for nodename, domains in to_schedule.iteritems():\n-            if domains:\n-                self.nodes[nodename].schedule(domains)\n-\n-    def _engine_started(self):\n-        self.load_nodes()\n-        scrapyengine.addtask(self.update_nodes, settings.getint('CLUSTER_MASTER_POLL_INTERVAL'))\n-\n\n@@ -1,165 +0,0 @@\n-import datetime\n-\n-from pydispatch import dispatcher\n-\n-from scrapy.spider import spiders \n-from scrapy.management.web import banner, webconsole_discover_module\n-from scrapy.utils.serialization import parse_jsondatetime\n-from scrapy.contrib.cluster.master import ClusterMaster\n-\n-class ClusterMasterWeb(ClusterMaster):\n-    webconsole_id = 'cluster_master'\n-    webconsole_name = 'Cluster master'\n-\n-    def __init__(self):\n-        ClusterMaster.__init__(self)\n-\n-        dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n-\n-    def webconsole_render(self, wc_request):\n-        changes = \"\"\n-        if wc_request.path == '/cluster/nodes/':\n-            return self.render_nodes(wc_request)\n-        elif wc_request.path == '/cluster/domains/':\n-            return self.render_domains(wc_request)\n-        elif wc_request.args:\n-            changes = self.webconsole_control(wc_request)\n-\n-        s = self.render_header()\n-\n-        s += \"<h2>Home</h2>\\n\"\n-\n-        s += \"<table border='1'>\\n\"\n-        s += \"<tr><th>&nbsp;</th><th>Name</th><th>Status</th><th>Running</th><th>Pending</th><th>Load.avg</th></tr>\\n\"\n-        for node in self.nodes.itervalues():\n-            #chkbox = \"<input type='checkbox' name='shutdown' value='%s' />\" % domain if node.status in [\"up\", \"idle\"] else \"&nbsp;\"\n-            nodelink = \"<a href='nodes/#%s'>%s</a>\" % (node.name, node.name)\n-            chkbox = \"&nbsp;\"\n-            loadavg = \"%.2f %.2f %.2f\" % node.loadavg\n-            s += \"<tr><td>%s</td><td>%s</td><td>%s</td><td>%d/%d</td><td>%d</td><td>%s</td></tr>\\n\" % \\\n-                 (chkbox, nodelink, node.status, len(node.running), node.maxproc, len(node.pending), loadavg)\n-        s += \"</table>\\n\"\n-\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-\n-        return str(s)\n-\n-    def webconsole_control(self, wc_request):\n-        args = wc_request.args\n-\n-        if \"updatenodes\" in args:\n-            self.update_nodes()\n-\n-        if \"schedule\" in args:\n-            node = args[\"node\"][0] if \"node\" in args else None\n-            self.schedule(args[\"schedule\"], nodename=node)\n-\n-        if \"stop\" in args:\n-            self.stop(args[\"stop\"])\n-\n-        if \"remove\" in args:\n-            self.remove(args[\"remove\"])\n-\n-        return \"\"\n-\n-    def render_nodes(self, wc_request):\n-        if wc_request.args:\n-            self.webconsole_control(wc_request)\n-\n-        now = datetime.datetime.utcnow()\n-\n-        s = self.render_header()\n-        for node in self.nodes.itervalues():\n-            s += \"<h2><a name='%s'>%s</h2>\\n\" % (node.name, node.name)\n-\n-            s += \"<h3>Running domains</h3>\\n\"\n-            if node.running:\n-                s += \"<form method='post' action='.'>\\n\"\n-                s += \"<table border='1'>\\n\"\n-                s += \"<tr><th>&nbsp;</th><th>PID</th><th>Domain</th><th>Status</th><th>Running time</th><th>Log file</th></tr>\\n\"\n-                for domain, proc in node.running.iteritems():\n-                    chkbox = \"<input type='checkbox' name='stop' value='%s' />\" % domain if proc['status'] == \"running\" else \"&nbsp;\"\n-                    start_time = parse_jsondatetime(proc.get('start_time', None))\n-                    elapsed = now - start_time if start_time else None\n-                    s += \"<tr><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>\\n\" % \\\n-                         (chkbox, proc['pid'], domain, proc['status'], elapsed, proc['logfile'])\n-                s += \"</table>\\n\"\n-                s += \"<input type='hidden' name='node' value='%s'>\\n\" % node.name\n-                s += \"<p><input type='submit' value='Stop selected domains on %s'></p>\\n\" % node.name\n-                s += \"</form>\\n\"\n-            else:\n-                s += \"<p>No running domains on %s</p>\\n\" % node.name\n-\n-            # pending domains\n-            s += \"<h3>Pending domains</h3>\\n\"\n-            if node.pending:\n-                s += \"<form method='post' action='.'>\\n\"\n-                s += \"<select name='remove' multiple='multiple' size='10'>\\n\"\n-                for domain in node.pending:\n-                    s += \"<option>%s</option>\\n\" % domain\n-                s += \"</select>\\n\"\n-                s += \"<input type='hidden' name='node' value='%s'>\\n\" % node.name\n-                s += \"<p><input type='submit' value='Remove selected pending domains on %s'></p>\\n\" % node.name\n-                s += \"</form>\\n\"\n-            else:\n-                s += \"<p>No pending domains on %s</p>\\n\" % node.name\n-\n-        return str(s)\n-\n-    def render_domains(self, wc_request):\n-        if wc_request.args:\n-            self.webconsole_control(wc_request)\n-\n-        enabled_domains = set(spiders.asdict(include_disabled=False).keys())\n-        inactive_domains = enabled_domains - set(self.running.keys() + self.pending.keys())\n-\n-        s = self.render_header()\n-\n-        s += \"<h2>Schedule domains</h2>\\n\"\n-\n-        s += \"Inactive domains (not running or pending)<br />\"\n-        s += \"<form method='post' action='.'>\\n\"\n-        s += \"<select name='schedule' multiple='multiple' size='10'>\\n\"\n-        for domain in sorted(inactive_domains):\n-            s += \"<option>%s</option>\\n\" % domain\n-        s += \"</select>\\n\"\n-        s += \"</br>\\n\"\n-        s += \"Node (only available nodes shown):<br />\\n\"\n-        s += \"<select name='node'>\\n\"\n-        s += \"<option value='' selected='selected'>any</option>\"\n-        for node in self.available_nodes:\n-            domcount = \"%d/%d/%d\" % (len(node.running), node.maxproc, len(node.pending))\n-            loadavg = \"%.2f %.2f %.2f\" % node.loadavg\n-            s += \"<option value='%s'>%s [D: %s | LA: %s]</option>\" % (node.name, node.name, domcount, loadavg)\n-        s += \"</select>\\n\"\n-        s += \"<p><input type='submit' value='Schedule selected domains'></p>\\n\"\n-        s += \"</form>\\n\"\n-\n-        s += \"<h2>Domains</h2>\\n\"\n-\n-        s += \"<table border='1'>\\n\"\n-        s += \"<tr><th>Domain</th><th>Status</th><th>Node</th></tr>\\n\"\n-        s += self._domains_table(self.running, '<b>running</b>')\n-        s += self._domains_table(self.pending, 'pending')\n-        s += \"</table>\\n\"\n-\n-        return str(s)\n-\n-    def render_header(self):\n-        s = banner(self)\n-        s += \"<p>Nav: \"\n-        s += \"<a href='/cluster/'>Home</a> | \"\n-        s += \"<a href='/cluster/domains/'>Domains</a> | \"\n-        s += \"<a href='/cluster/nodes/'>Nodes</a> (<a href='/cluster/nodes/?updatenodes=1'>update</a>)\"\n-        s += \"</p>\"\n-        return s\n-\n-    def _domains_table(self, dict_, status):\n-        s = \"\"\n-        for domain, node in dict_.iteritems():\n-            s += \"<tr><td>%s</td><td>%s</td><td>%s</td></tr>\\n\" % (domain, status, node.name)\n-        return s\n-\n-    def webconsole_discover_module(self):\n-        return self\n\n@@ -1,2 +0,0 @@\n-from scrapy.contrib.cluster.worker.manager import ClusterWorker\n-from scrapy.contrib.cluster.worker.web import ClusterWorkerWeb\n\n@@ -1,96 +0,0 @@\n-import sys\n-import os\n-import time\n-import datetime\n-\n-from twisted.internet import protocol, reactor\n-\n-from scrapy.core import log\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.conf import settings\n-\n-class ScrapyProcessProtocol(protocol.ProcessProtocol):\n-    def __init__(self, procman, domain, logfile=None):\n-        self.procman = procman\n-        self.domain = domain\n-        self.logfile = logfile\n-        self.start_time = datetime.datetime.utcnow()\n-        self.status = \"starting\"\n-        self.pid = -1\n-\n-    def __str__(self):\n-        return \"<ScrapyProcess domain=%s, pid=%s, status=%s>\" % (self.domain, self.pid, self.status)\n-\n-    def connectionMade(self):\n-        self.pid = self.transport.pid\n-        log.msg(\"ClusterWorker: started domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n-        self.transport.closeStdin()\n-        self.status = \"running\"\n-\n-    def processEnded(self, status_object):\n-        log.msg(\"ClusterWorker: finished domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n-        del self.procman.running[self.domain]\n-        self.procman.next_pending()\n-    \n-\n-class ClusterWorker(object):\n-\n-    def __init__(self):\n-        if not settings.getbool('CLUSTER_WORKER_ENABLED'):\n-            raise NotConfigured\n-\n-        self.maxproc = settings.getint('CLUSTER_WORKER_MAXPROC')\n-        self.logdir = settings['CLUSTER_WORKER_LOGDIR']\n-        self.running = {}\n-        self.pending = []\n-\n-    def schedule(self, domain):\n-        \"\"\"Schedule new domain to be crawled in a separate processes\"\"\"\n-\n-        if len(self.running) < self.maxproc and domain not in self.running:\n-            self._run(domain)\n-        else:\n-            self.pending.append(domain)\n-\n-    def stop(self, domain):\n-        \"\"\"Stop running domain. For removing pending (not yet started) domains\n-        use remove() instead\"\"\"\n-\n-        if domain in self.running:\n-            proc = self.running[domain]\n-            log.msg(\"ClusterWorker: Sending shutdown signal to domain=%s, pid=%d\" % (domain, proc.pid))\n-            proc.transport.signalProcess('INT')\n-            proc.status = \"closing\"\n-\n-    def remove(self, domain):\n-        \"\"\"Remove all scheduled instances of the given domain (if it hasn't\n-        started yet). Otherwise use stop()\"\"\"\n-\n-        while domain in self.pending:\n-            self.pending.remove(domain)\n-\n-    def next_pending(self):\n-        \"\"\"Run the next domain in the pending list, which is not already running\"\"\"\n-\n-        if len(self.running) >= self.maxproc:\n-            return\n-        for domain in self.pending:\n-            if domain not in self.running:\n-                self._run(domain)\n-                self.pending.remove(domain)\n-                return\n-\n-    def _run(self, domain):\n-        \"\"\"Spawn process to run the given domain. Don't call this method\n-        directly. Instead use schedule().\"\"\"\n-\n-        logfile = os.path.join(self.logdir, domain, time.strftime(\"%FT%T.log\"))\n-        if not os.path.exists(os.path.dirname(logfile)):\n-            os.makedirs(os.path.dirname(logfile))\n-        scrapy_proc = ScrapyProcessProtocol(self, domain, logfile)\n-        env = {'SCRAPY_LOGFILE': logfile}\n-        args = [sys.executable, sys.argv[0], 'crawl', domain]\n-        proc = reactor.spawnProcess(scrapy_proc, sys.executable, args=args, env=env)\n-        self.running[domain] = scrapy_proc\n-\n-\n\n@@ -1,127 +0,0 @@\n-import os\n-import datetime\n-\n-from pydispatch import dispatcher\n-\n-from scrapy.spider import spiders\n-from scrapy.management.web import banner, webconsole_discover_module\n-from scrapy.utils.serialization import serialize\n-from scrapy.contrib.cluster.worker import ClusterWorker\n-\n-class ClusterWorkerWeb(ClusterWorker):\n-    webconsole_id = 'cluster_worker'\n-    webconsole_name = 'Cluster worker'\n-\n-    def __init__(self):\n-        ClusterWorker.__init__(self)\n-\n-        dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n-\n-    def webconsole_render(self, wc_request):\n-        changes = \"\"\n-        if wc_request.path == '/cluster_worker/ws/':\n-            return self.webconsole_control(wc_request, ws=True)\n-        elif wc_request.args:\n-            changes = self.webconsole_control(wc_request)\n-\n-        now = datetime.datetime.utcnow()\n-\n-        s = banner(self)\n-\n-        # running processes\n-        s += \"<h2>Running processes</h2>\\n\"\n-        if self.running:\n-            s += \"<form method='post' action='.'>\\n\"\n-            s += \"<table border='1'>\\n\"\n-            s += \"<tr><th>&nbsp;</th><th>PID</th><th>Domain</th><th>Status</th><th>Log file</th><th>Running time</th></tr>\\n\"\n-            for domain, proc in self.running.iteritems():\n-                chkbox = \"<input type='checkbox' name='stop' value='%s' />\" % domain if proc.status == \"running\" else \"&nbsp;\"\n-                elapsed = now - proc.start_time\n-                s += \"<tr><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>\\n\" % \\\n-                     (chkbox, proc.pid, domain, proc.status, proc.logfile, elapsed)\n-            s += \"</table>\\n\"\n-            s += \"<p><input type='submit' value='Stop selected domains'></p>\\n\"\n-            s += \"</form>\\n\"\n-        else:\n-            s += \"<p>No running processes</p>\\n\"\n-\n-        # pending domains\n-        s += \"<h2>Pending domains</h2>\\n\"\n-        if self.pending:\n-            s += \"<form method='post' action='.'>\\n\"\n-            s += \"<select name='remove' multiple='multiple' size='10'>\\n\"\n-            for domain in self.pending:\n-                s += \"<option>%s</option>\\n\" % domain\n-            s += \"</select>\\n\"\n-            s += \"<p><input type='submit' value='Remove selected pending domains'></p>\\n\"\n-            s += \"</form>\\n\"\n-        else:\n-            s += \"<p>No pending domains</p>\\n\"\n-\n-        # schedule domains\n-        enabled_domains = spiders.asdict(include_disabled=False).keys()\n-        s += \"<h2>Schedule domains</h2>\\n\"\n-        s += \"<form method='post' action='.'>\\n\"\n-        s += \"<select name='schedule' multiple='multiple' size='10'>\\n\"\n-        for domain in sorted(enabled_domains):\n-            s += \"<option>%s</option>\\n\" % domain\n-        s += \"</select>\\n\"\n-        s += \"<p><input type='submit' value='Schedule selected domains'></p>\\n\"\n-        s += \"</form>\\n\"\n-\n-        s += changes\n-\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-\n-        return s\n-\n-    def webconsole_control(self, wc_request, ws=False):\n-        args = wc_request.args\n-\n-        if \"schedule\" in args:\n-            for domain in args[\"schedule\"]:\n-                self.schedule(domain)\n-            if ws:\n-                return self.ws_status(wc_request)\n-            else:\n-                return \"<p>Scheduled domains: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"schedule\"]) + \"</p>\\n\"\n-\n-        if \"stop\" in args:\n-            for domain in args[\"stop\"]:\n-                self.stop(domain)\n-            if ws:\n-                return self.ws_status(wc_request)\n-            else:\n-                return \"<p>Stopped running processes: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"stop\"]) + \"</p>\\n\"\n-\n-        if \"remove\" in args:\n-            for domain in args[\"remove\"]:\n-                self.remove(domain)\n-            if ws:\n-                return self.ws_status(wc_request)\n-            else:\n-                return \"<p>Removed pending domains: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"remove\"]) + \"</p>\\n\"\n-        \n-        if ws:\n-            return self.ws_status(wc_request)\n-        else:\n-            return \"\"\n-\n-    def ws_status(self, wc_request):\n-        format = wc_request.args['format'][0] if 'format' in wc_request.args else 'json'\n-        wc_request.setHeader('content-type', 'text/plain')\n-        exported_proc_attrs = ['pid', 'status', 'start_time', 'logfile']\n-        d = {'maxproc': self.maxproc, 'running': {}, 'pending': []}\n-        for domain, proc in self.running.iteritems():\n-            d2 = {}\n-            for a in exported_proc_attrs:\n-                d2[a] = getattr(proc, a)\n-            d['running'][domain] = d2\n-        d['pending'] = self.pending\n-        d['loadavg'] = os.getloadavg()\n-        content = serialize(d, format)\n-        return content\n-\n-    def webconsole_discover_module(self):\n-        return self\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
