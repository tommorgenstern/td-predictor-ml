{"custom_id": "keras#0c8e6319cff0b0b723aa1a7b8d8a0b3628558848", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 48 | Files Changed: 4 | Hunks: 19 | Methods Changed: 7 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 53 | Churn Cumulative: 15259 | Contributors (this commit): 81 | Commits (past 90d): 114 | Contributors (cumulative): 102 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,8 +11,6 @@ from .common import cast_to_floatx\n from .common import image_data_format\n from .common import set_image_data_format\n from .common import is_keras_tensor\n-from .common import legacy_weight_ordering\n-from .common import set_legacy_weight_ordering\n \n _keras_base_dir = os.path.expanduser('~')\n if not os.access(_keras_base_dir, os.W_OK):\n\n@@ -4,7 +4,6 @@ import numpy as np\n _FLOATX = 'float32'\n _EPSILON = 10e-8\n _IMAGE_DATA_FORMAT = 'channels_last'\n-_LEGACY_WEIGHT_ORDERING = False\n \n \n def epsilon():\n@@ -175,13 +174,3 @@ def is_keras_tensor(x):\n         return True\n     else:\n         return False\n-\n-\n-def set_legacy_weight_ordering(value):\n-    global _LEGACY_WEIGHT_ORDERING\n-    assert value in {True, False}\n-    _LEGACY_WEIGHT_ORDERING = value\n-\n-\n-def legacy_weight_ordering():\n-    return _LEGACY_WEIGHT_ORDERING\n\n@@ -1,19 +1,18 @@\n-from collections import defaultdict\n import tensorflow as tf\n-\n from tensorflow.python.training import moving_averages\n from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import control_flow_ops\n from tensorflow.python.ops import functional_ops\n-try:\n from tensorflow.python.ops import ctc_ops as ctc\n-except ImportError:\n-    import tensorflow.contrib.ctc as ctc\n \n+from collections import defaultdict\n import numpy as np\n import os\n import warnings\n-from .common import floatx, _EPSILON, image_data_format\n+\n+from .common import floatx\n+from .common import _EPSILON\n+from .common import image_data_format\n py_all = all\n \n # INTERNAL UTILS\n@@ -38,10 +37,6 @@ _GRAPH_UID_DICTS = {}\n # Change its value via `manual_variable_initialization(value)`.\n _MANUAL_VAR_INIT = False\n \n-# These two integers contain the tensorflow version for coping with API breaks.\n-tf_major_version = int(tf.__version__.split('.')[0])\n-tf_minor_version = int(tf.__version__.split('.')[1])\n-\n \n def get_uid(prefix=''):\n     global _GRAPH_UID_DICTS\n@@ -278,14 +273,9 @@ def variable(value, dtype=None, name=None):\n         sparse_coo = value.tocoo()\n         indices = np.concatenate((np.expand_dims(sparse_coo.row, 1),\n                                   np.expand_dims(sparse_coo.col, 1)), 1)\n-        if tf_major_version >= 1:\n         v = tf.SparseTensor(indices=indices,\n                             values=sparse_coo.data,\n                             dense_shape=sparse_coo.shape)\n-        else:\n-            v = tf.SparseTensor(indices=indices,\n-                                values=sparse_coo.data,\n-                                shape=sparse_coo.shape)\n         v._keras_shape = sparse_coo.shape\n         v._uses_learning_phase = False\n         return v\n@@ -914,16 +904,10 @@ def batch_dot(x, y, axes=None):\n     if isinstance(axes, int):\n         axes = (axes, axes)\n     if ndim(x) == 2 and ndim(y) == 2:\n-        if tf_major_version >= 1:\n         if axes[0] == axes[1]:\n             out = tf.reduce_sum(tf.multiply(x, y), axes[0])\n         else:\n             out = tf.reduce_sum(tf.multiply(tf.transpose(x, [1, 0]), y), axes[1])\n-        else:\n-            if axes[0] == axes[1]:\n-                out = tf.reduce_sum(tf.mul(x, y), axes[0])\n-            else:\n-                out = tf.reduce_sum(tf.mul(tf.transpose(x, [1, 0]), y), axes[1])\n     else:\n         if axes is not None:\n             adj_x = None if axes[0] == ndim(x) - 1 else True\n@@ -1537,13 +1521,7 @@ def concatenate(tensors, axis=-1):\n     if py_all([is_sparse(x) for x in tensors]):\n         return tf.sparse_concat(axis, tensors)\n     else:\n-        if tf_major_version >= 1:\n         return tf.concat([to_dense(x) for x in tensors], axis)\n-        else:\n-            try:\n-                return tf.concat_v2([to_dense(x) for x in tensors], axis)\n-            except AttributeError:\n-                return tf.concat(axis, [to_dense(x) for x in tensors])\n \n \n def reshape(x, shape):\n@@ -3336,13 +3314,8 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n             sequence_length=input_length, beam_width=beam_width,\n             top_paths=top_paths)\n \n-    if tf_major_version >= 1:\n     decoded_dense = [tf.sparse_to_dense(st.indices, st.dense_shape, st.values, default_value=-1)\n                      for st in decoded]\n-    else:\n-        decoded_dense = [tf.sparse_to_dense(st.indices, st.shape, st.values, default_value=-1)\n-                         for st in decoded]\n-\n     return (decoded_dense, log_prob)\n \n \n\n@@ -1654,9 +1654,6 @@ class Container(Layer):\n             layers_for_depth = layers_by_depth[depth]\n             # Container.layers needs to have a deterministic order:\n             # here we order them by traversal order.\n-            if K.legacy_weight_ordering():\n-                layers_for_depth.sort(key=lambda x: x.name)\n-            else:\n             layers_for_depth.sort(key=lambda x: layer_indices[x])\n             for layer in layers_for_depth:\n                 layers.append(layer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c81713367e3e97f13dd584ae8e48b79615951437", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 30 | Churn Cumulative: 1320 | Contributors (this commit): 3 | Commits (past 90d): 15 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -111,10 +111,13 @@ class LocallyConnected1D(Layer):\n         if input_dim is None:\n             raise ValueError('Axis 2 of input should be fully-defined. '\n                              'Found shape:', input_shape)\n-        _, output_length, filters = self.compute_output_shape(input_shape)\n+        output_length = conv_utils.conv_output_length(input_shape[1],\n+                                                      self.kernel_size[0],\n+                                                      self.padding,\n+                                                      self.strides[0])\n         self.kernel_shape = (output_length,\n                              self.kernel_size[0] * input_dim,\n-                             filters)\n+                             self.filters)\n         self.kernel = self.add_weight(\n             self.kernel_shape,\n             initializer=self.kernel_initializer,\n@@ -300,23 +303,20 @@ class LocallyConnected2D(Layer):\n \n     def build(self, input_shape):\n         if self.data_format == 'channels_last':\n-            space = input_shape[1:-1]\n+            input_row, input_col = input_shape[1:-1]\n+            input_filter = input_shape[3]\n         else:\n-            space = input_shape[2:]\n-        if space[0] is None or space[1] is None:\n+            input_row, input_col = input_shape[2:]\n+            input_filter = input_shape[1]\n+        if input_row is None or input_col is None:\n             raise ValueError('The spatial dimensions of the inputs to '\n                              ' a LocallyConnected2D layer '\n                              'should be fully-defined, but layer received '\n                              'the inputs shape ' + str(input_shape))\n-\n-        output_shape = self.compute_output_shape(input_shape)\n-        if self.data_format == 'channels_first':\n-            _, filters, output_row, output_col = output_shape\n-            input_filter = input_shape[1]\n-        elif self.data_format == 'channels_last':\n-            _, output_row, output_col, filters = output_shape\n-            input_filter = input_shape[3]\n-\n+        output_row = conv_utils.conv_output_length(input_row, self.kernel_size[0],\n+                                                   self.padding, self.strides[0])\n+        output_col = conv_utils.conv_output_length(input_col, self.kernel_size[1],\n+                                                   self.padding, self.strides[1])\n         self.output_row = output_row\n         self.output_col = output_col\n         self.kernel_shape = (output_row * output_col,\n@@ -328,7 +328,7 @@ class LocallyConnected2D(Layer):\n                                       regularizer=self.kernel_regularizer,\n                                       constraint=self.kernel_constraint)\n         if self.use_bias:\n-            self.bias = self.add_weight((output_row, output_col, filters),\n+            self.bias = self.add_weight((output_row, output_col, self.filters),\n                                         initializer=self.bias_initializer,\n                                         name='bias',\n                                         regularizer=self.bias_regularizer,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#825d58e2bbbe234019898e414fb9129af1f17014", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 544 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -229,7 +229,11 @@ class Orthogonal(Initializer):\n         self.seed = seed\n \n     def __call__(self, shape, dtype=None):\n-        flat_shape = (shape[0], np.prod(shape[1:]))\n+        num_rows = 1\n+        for dim in shape[:-1]:\n+            num_rows *= dim\n+        num_cols = shape[-1]\n+        flat_shape = (num_rows, num_cols)\n         if self.seed is not None:\n             np.random.seed(self.seed)\n         a = np.random.normal(0.0, 1.0, flat_shape)\n@@ -419,8 +423,8 @@ def _compute_fans(shape, data_format='channels_last'):\n     if len(shape) == 2:\n         fan_in = shape[0]\n         fan_out = shape[1]\n-    elif len(shape) == 4 or len(shape) == 5:\n-        # Assuming convolution kernels (2D or 3D).\n+    elif len(shape) in {3, 4, 5}:\n+        # Assuming convolution kernels (1D, 2D or 3D).\n         # TH kernel shape: (depth, input_depth, ...)\n         # TF kernel shape: (..., input_depth, depth)\n         if data_format == 'channels_first':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6cb1fe9490bd2050ffedb6dd3d87d17145b10a60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 9563 | Contributors (this commit): 33 | Commits (past 90d): 33 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -1093,18 +1093,18 @@ class UpSampling2D(Layer):\n \n     def compute_output_shape(self, input_shape):\n         if self.data_format == 'channels_first':\n-            width = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n-            height = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n+            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n+            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n             return (input_shape[0],\n                     input_shape[1],\n-                    width,\n-                    height)\n-        elif self.data_format == 'channels_last':\n-            width = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n-            height = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n-            return (input_shape[0],\n-                    width,\n                     height,\n+                    width)\n+        elif self.data_format == 'channels_last':\n+            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n+            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n+            return (input_shape[0],\n+                    height,\n+                    width,\n                     input_shape[3])\n \n     def call(self, inputs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3cfd306c76b4428237fc0fc09f06a4f179324d3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 9573 | Contributors (this commit): 33 | Commits (past 90d): 34 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1178,8 +1178,6 @@ class UpSampling3D(Layer):\n                     dim2,\n                     dim3,\n                     input_shape[4])\n-        else:\n-            raise ValueError('Invalid data_format:', self.data_format)\n \n     def call(self, inputs):\n         return K.resize_volumes(inputs,\n@@ -1316,8 +1314,6 @@ class ZeroPadding2D(Layer):\n                     rows,\n                     cols,\n                     input_shape[3])\n-        else:\n-            raise ValueError('Invalid data_format:', self.data_format)\n \n     def call(self, inputs):\n         return K.spatial_2d_padding(inputs,\n@@ -1417,8 +1413,6 @@ class ZeroPadding3D(Layer):\n                     dim2,\n                     dim3,\n                     input_shape[4])\n-        else:\n-            raise ValueError('Invalid data_format:', self.data_format)\n \n     def call(self, inputs):\n         return K.spatial_3d_padding(inputs,\n@@ -1569,8 +1563,6 @@ class Cropping2D(Layer):\n                     input_shape[1] - self.cropping[0][0] - self.cropping[0][1] if input_shape[1] else None,\n                     input_shape[2] - self.cropping[1][0] - self.cropping[1][1] if input_shape[2] else None,\n                     input_shape[3])\n-        else:\n-            raise ValueError('Invalid data_format:', self.data_format)\n \n     def call(self, inputs):\n         if self.data_format == 'channels_first':\n@@ -1710,8 +1702,6 @@ class Cropping3D(Layer):\n                     dim2,\n                     dim3,\n                     input_shape[4])\n-        else:\n-            raise ValueError('Invalid data_format:', self.data_format)\n \n     def call(self, inputs):\n         if self.data_format == 'channels_first':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#53b9f9c44093ade92dc1cca3e3bda9f7bfed9936", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4576 | Contributors (this commit): 55 | Commits (past 90d): 49 | Contributors (cumulative): 55 | DMM Complexity: None\n\nDIFF:\n@@ -1287,7 +1287,7 @@ def categorical_crossentropy(output, target, from_logits=False):\n \n def sparse_categorical_crossentropy(output, target, from_logits=False):\n     target = T.cast(T.flatten(target), 'int32')\n-    target = T.extra_ops.to_one_hot(target, num_classes=output.shape[-1])\n+    target = T.extra_ops.to_one_hot(target, nb_class=output.shape[-1])\n     target = reshape(target, shape(output))\n     return categorical_crossentropy(output, target, from_logits)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#99fa71a91656f8b0e1988de105ca4159bab17180", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 265 | Files Changed: 4 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): -26/0 | Churn Δ: 265 | Churn Cumulative: 847 | Contributors (this commit): 5 | Commits (past 90d): 21 | Contributors (cumulative): 9 | DMM Complexity: 0.9035087719298246\n\nDIFF:\n@@ -3,4 +3,3 @@ from .vgg19 import VGG19\n from .resnet50 import ResNet50\n from .inception_v3 import InceptionV3\n from .xception import Xception\n-from .music_tagger_crnn import MusicTaggerCRNN\n\n@@ -1,92 +0,0 @@\n-import numpy as np\n-from .. import backend as K\n-\n-try:\n-    import librosa\n-except ImportError:\n-    librosa = None\n-\n-\n-TAGS = ['rock', 'pop', 'alternative', 'indie', 'electronic',\n-        'female vocalists', 'dance', '00s', 'alternative rock', 'jazz',\n-        'beautiful', 'metal', 'chillout', 'male vocalists',\n-        'classic rock', 'soul', 'indie rock', 'Mellow', 'electronica',\n-        '80s', 'folk', '90s', 'chill', 'instrumental', 'punk',\n-        'oldies', 'blues', 'hard rock', 'ambient', 'acoustic',\n-        'experimental', 'female vocalist', 'guitar', 'Hip-Hop',\n-        '70s', 'party', 'country', 'easy listening',\n-        'sexy', 'catchy', 'funk', 'electro', 'heavy metal',\n-        'Progressive rock', '60s', 'rnb', 'indie pop',\n-        'sad', 'House', 'happy']\n-\n-\n-def preprocess_input(audio_path, data_format=None):\n-    \"\"\"Reads an audio file and outputs a Mel-spectrogram.\n-\n-    # Arguments\n-        audio_path: path to the target audio file.\n-        data_format: data format for the output spectrogram image.\n-\n-    # Returns\n-        3D Numpy tensor encoding the Mel-spectrogram.\n-\n-    # Raises\n-        ImportError: if librosa is not available.\n-    \"\"\"\n-    if data_format is None:\n-        data_format = K.image_data_format()\n-    assert data_format in {'channels_last', 'channels_first'}\n-\n-    if librosa is None:\n-        raise ImportError('Librosa is required to process audio files. '\n-                          'Install it via `pip install librosa` or visit '\n-                          'http://librosa.github.io/librosa/ for details.')\n-\n-    # mel-spectrogram parameters\n-    sr = 12000\n-    n_fft = 512\n-    n_mels = 96\n-    hop_length = 256\n-    duration = 29.12\n-\n-    src, sr = librosa.load(audio_path, sr=sr)\n-    n_sample = src.shape[0]\n-    n_sample_wanted = int(duration * sr)\n-\n-    # trim the signal at the center\n-    if n_sample < n_sample_wanted:  # if too short\n-        src = np.hstack((src, np.zeros((int(duration * sr) - n_sample,))))\n-    elif n_sample > n_sample_wanted:  # if too long\n-        src = src[(n_sample - n_sample_wanted) // 2:\n-                  (n_sample + n_sample_wanted) // 2]\n-\n-    logam = librosa.logamplitude\n-    melgram = librosa.feature.melspectrogram\n-    x = logam(melgram(y=src, sr=sr, hop_length=hop_length,\n-                      n_fft=n_fft, n_mels=n_mels) ** 2,\n-              ref_power=1.0)\n-\n-    if data_format == 'channels_first':\n-        x = np.expand_dims(x, axis=0)\n-    elif data_format == 'channels_last':\n-        x = np.expand_dims(x, axis=3)\n-    return x\n-\n-\n-def decode_predictions(preds, top_n=5):\n-    \"\"\"Decode the output of a music tagger model.\n-\n-    # Arguments\n-        preds: 2-dimensional numpy array\n-        top_n: integer in [0, 50], number of items to show.\n-\n-    # Returns\n-        Decoded output.\n-    \"\"\"\n-    assert len(preds.shape) == 2 and preds.shape[1] == 50\n-    results = []\n-    for pred in preds:\n-        result = zip(TAGS, pred)\n-        result = sorted(result, key=lambda x: x[1], reverse=True)\n-        results.append(result[:top_n])\n-    return results\n\n@@ -1,166 +0,0 @@\n-# -*- coding: utf-8 -*-\n-\"\"\"MusicTaggerCRNN model for Keras.\n-\n-# Reference:\n-\n-- [Music-auto_tagging-keras](https://github.com/keunwoochoi/music-auto_tagging-keras)\n-\n-\"\"\"\n-from __future__ import print_function\n-from __future__ import absolute_import\n-\n-from .. import backend as K\n-from ..layers import Input, Dense\n-from ..models import Model\n-from ..layers import Reshape, Permute\n-from ..layers.convolutional import Conv2D\n-from ..layers.convolutional import MaxPooling2D\n-from ..layers.convolutional import ZeroPadding2D\n-from ..layers.normalization import BatchNormalization\n-from ..layers.advanced_activations import ELU\n-from ..layers.recurrent import GRU\n-from ..engine.topology import get_source_inputs\n-from ..utils.data_utils import get_file\n-from ..utils.layer_utils import convert_all_kernels_in_model\n-from .audio_conv_utils import decode_predictions\n-from .audio_conv_utils import preprocess_input\n-\n-TH_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.3/music_tagger_crnn_weights_tf_kernels_th_data_format.h5'\n-TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.3/music_tagger_crnn_weights_tf_kernels_tf_data_format.h5'\n-\n-\n-def MusicTaggerCRNN(weights='msd', input_tensor=None,\n-                    include_top=True,\n-                    classes=50):\n-    \"\"\"Instantiates the MusicTaggerCRNN architecture.\n-\n-    Optionally loads weights pre-trained\n-    on Million Song Dataset. Note that when using TensorFlow,\n-    for best performance you should set\n-    `image_data_format=\"channels_last\"` in your Keras config\n-    at ~/.keras/keras.json.\n-\n-    The model and the weights are compatible with both\n-    TensorFlow and Theano. The data format\n-    convention used by the model is the one\n-    specified in your Keras config file.\n-\n-    For preparing mel-spectrogram input, see\n-    `audio_conv_utils.py` in [applications](https://github.com/fchollet/keras/tree/master/keras/applications).\n-    You will need to install [Librosa](http://librosa.github.io/librosa/)\n-    to use it.\n-\n-    # Arguments\n-        weights: one of `None` (random initialization)\n-            or \"msd\" (pre-training on ImageNet).\n-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n-            to use as image input for the model.\n-        include_top: whether to include the 1 fully-connected\n-            layer (output layer) at the top of the network.\n-            If False, the network outputs 32-dim features.\n-        classes: optional number of classes to classify images\n-            into, only to be specified if `include_top` is True, and\n-            if no `weights` argument is specified.\n-\n-    # Returns\n-        A Keras model instance.\n-\n-    # Raises\n-        ValueError: in case of invalid argument for `weights`,\n-            or invalid input shape.\n-    \"\"\"\n-    if weights not in {'msd', None}:\n-        raise ValueError('The `weights` argument should be either '\n-                         '`None` (random initialization) or `msd` '\n-                         '(pre-training on Million Song Dataset).')\n-\n-    if weights == 'msd' and include_top and classes != 50:\n-        raise ValueError('If using `weights` as msd with `include_top`'\n-                         ' as true, `classes` should be 50')\n-    # Determine proper input shape\n-    if K.image_data_format() == 'channels_first':\n-        input_shape = (1, 96, 1366)\n-    else:\n-        input_shape = (96, 1366, 1)\n-\n-    if input_tensor is None:\n-        melgram_input = Input(shape=input_shape)\n-    else:\n-        if not K.is_keras_tensor(input_tensor):\n-            melgram_input = Input(tensor=input_tensor, shape=input_shape)\n-        else:\n-            melgram_input = input_tensor\n-\n-    # Determine input axis\n-    if K.image_data_format() == 'channels_first':\n-        channel_axis = 1\n-        time_axis = 3\n-    else:\n-        channel_axis = 3\n-        time_axis = 2\n-\n-    # Input block\n-    x = ZeroPadding2D(padding=(0, 37))(melgram_input)\n-    x = BatchNormalization(axis=time_axis, name='bn_0_freq')(x)\n-\n-    # Conv block 1\n-    x = Conv2D(64, (3, 3), padding='same', name='conv1')(x)\n-    x = BatchNormalization(axis=channel_axis, name='bn1')(x)\n-    x = ELU()(x)\n-    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool1')(x)\n-\n-    # Conv block 2\n-    x = Conv2D(128, (3, 3), padding='same', name='conv2')(x)\n-    x = BatchNormalization(axis=channel_axis, name='bn2')(x)\n-    x = ELU()(x)\n-    x = MaxPooling2D(pool_size=(3, 3), strides=(3, 3), name='pool2')(x)\n-\n-    # Conv block 3\n-    x = Conv2D(128, (3, 3), padding='same', name='conv3')(x)\n-    x = BatchNormalization(axis=channel_axis, name='bn3')(x)\n-    x = ELU()(x)\n-    x = MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='pool3')(x)\n-\n-    # Conv block 4\n-    x = Conv2D(128, (3, 3), padding='same', name='conv4')(x)\n-    x = BatchNormalization(axis=channel_axis, name='bn4')(x)\n-    x = ELU()(x)\n-    x = MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='pool4')(x)\n-\n-    # reshaping\n-    if K.image_data_format() == 'channels_first':\n-        x = Permute((3, 1, 2))(x)\n-    x = Reshape((15, 128))(x)\n-\n-    # GRU block 1, 2, output\n-    x = GRU(32, return_sequences=True, name='gru1')(x)\n-    x = GRU(32, return_sequences=False, name='gru2')(x)\n-\n-    if include_top:\n-        x = Dense(classes, activation='sigmoid', name='output')(x)\n-\n-    # Ensure that the model takes into account\n-    # any potential predecessors of `input_tensor`.\n-    if input_tensor is not None:\n-        inputs = get_source_inputs(input_tensor)\n-    else:\n-        inputs = melgram_input\n-    # Create model.\n-    model = Model(inputs, x, name='music_tagger_crnn')\n-\n-    if weights is None:\n-        return model\n-    else:\n-        # Load weights\n-        if K.image_data_format() == 'channels_last':\n-            weights_path = get_file('music_tagger_crnn_weights_tf_kernels_tf_data_format.h5',\n-                                    TF_WEIGHTS_PATH,\n-                                    cache_subdir='models')\n-        else:\n-            weights_path = get_file('music_tagger_crnn_weights_tf_kernels_th_data_format.h5',\n-                                    TH_WEIGHTS_PATH,\n-                                    cache_subdir='models')\n-        model.load_weights(weights_path, by_name=True)\n-        if K.backend() == 'theano':\n-            convert_all_kernels_in_model(model)\n-        return model\n\n@@ -82,11 +82,5 @@ def test_inceptionv3_pooling():\n     assert model.output_shape == (None, 2048)\n \n \n-@keras_test\n-def test_music_tagger_rcnn():\n-    model = applications.MusicTaggerCRNN(weights=None)\n-    assert model.output_shape == (None, 50)\n-\n-\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1a283e9d1aa327a25719003c99dbd282943bfe8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 16406 | Contributors (this commit): 75 | Commits (past 90d): 70 | Contributors (cumulative): 81 | DMM Complexity: 0.2\n\nDIFF:\n@@ -2492,7 +2492,8 @@ class Container(Layer):\n         model_config = {\n             'class_name': self.__class__.__name__,\n             'config': config,\n-            'keras_version': keras_version\n+            'keras_version': keras_version,\n+            'backend': K.backend()\n         }\n         return model_config\n \n@@ -2681,7 +2682,11 @@ def _collect_input_shape(input_tensors):\n \n \n def save_weights_to_hdf5_group(f, layers):\n+    from . import __version__ as keras_version\n+\n     f.attrs['layer_names'] = [layer.name.encode('utf8') for layer in layers]\n+    f.attrs['backend'] = K.backend().encode('utf8')\n+    f.attrs['keras_version'] = str(keras_version).encode('utf8')\n \n     for layer in layers:\n         g = f.create_group(layer.name)\n\n@@ -94,6 +94,7 @@ def save_model(model, filepath, overwrite=True):\n \n     f = h5py.File(filepath, 'w')\n     f.attrs['keras_version'] = str(keras_version).encode('utf8')\n+    f.attrs['backend'] = K.backend().encode('utf8')\n     f.attrs['model_config'] = json.dumps({\n         'class_name': model.__class__.__name__,\n         'config': model.get_config()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f516cc6ddefeeecec0e42e1f25deee24737af2bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 111 | Lines Deleted: 108 | Files Changed: 6 | Hunks: 30 | Methods Changed: 8 | Complexity Δ (Sum/Max): 2/3 | Churn Δ: 219 | Churn Cumulative: 3817 | Contributors (this commit): 35 | Commits (past 90d): 70 | Contributors (cumulative): 48 | DMM Complexity: 1.0\n\nDIFF:\n@@ -39,10 +39,8 @@ from .imagenet_utils import decode_predictions\n from .imagenet_utils import _obtain_input_shape\n \n \n-TH_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/inception_v3_weights_th_dim_ordering_th_kernels.h5'\n-TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/inception_v3_weights_tf_dim_ordering_tf_kernels.h5'\n-TH_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/inception_v3_weights_th_dim_ordering_th_kernels_notop.h5'\n-TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n+WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/inception_v3_weights_tf_dim_ordering_tf_kernels.h5'\n+WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n \n \n def conv2d_bn(x, filters, num_row, num_col,\n@@ -309,8 +307,7 @@ def InceptionV3(include_top=True, weights='imagenet',\n \n     if include_top:\n         # Classification block\n-        x = AveragePooling2D((8, 8), strides=(8, 8), name='avg_pool')(x)\n-        x = Flatten(name='flatten')(x)\n+        x = GlobalAveragePooling2D(name='avg_pool')(x)\n         x = Dense(classes, activation='softmax', name='predictions')(x)\n     else:\n         if pooling == 'avg':\n@@ -330,17 +327,6 @@ def InceptionV3(include_top=True, weights='imagenet',\n     # load weights\n     if weights == 'imagenet':\n         if K.image_data_format() == 'channels_first':\n-            if include_top:\n-                weights_path = get_file('inception_v3_weights_th_dim_ordering_th_kernels.h5',\n-                                        TH_WEIGHTS_PATH,\n-                                        cache_subdir='models',\n-                                        md5_hash='b3baf3070cc4bf476d43a2ea61b0ca5f')\n-            else:\n-                weights_path = get_file('inception_v3_weights_th_dim_ordering_th_kernels_notop.h5',\n-                                        TH_WEIGHTS_PATH_NO_TOP,\n-                                        cache_subdir='models',\n-                                        md5_hash='79aaa90ab4372b4593ba3df64e142f05')\n-            model.load_weights(weights_path)\n             if K.backend() == 'tensorflow':\n                 warnings.warn('You are using the TensorFlow backend, yet you '\n                               'are using the Theano '\n@@ -350,16 +336,14 @@ def InceptionV3(include_top=True, weights='imagenet',\n                               '`image_data_format=\"channels_last\"` in '\n                               'your Keras config '\n                               'at ~/.keras/keras.json.')\n-                convert_all_kernels_in_model(model)\n-        else:\n         if include_top:\n             weights_path = get_file('inception_v3_weights_tf_dim_ordering_tf_kernels.h5',\n-                                        TF_WEIGHTS_PATH,\n+                                    WEIGHTS_PATH,\n                                     cache_subdir='models',\n                                     md5_hash='fe114b3ff2ea4bf891e9353d1bbfb32f')\n         else:\n             weights_path = get_file('inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n-                                        TF_WEIGHTS_PATH_NO_TOP,\n+                                    WEIGHTS_PATH_NO_TOP,\n                                     cache_subdir='models',\n                                     md5_hash='2f3609166de1d967d1a481094754f691')\n         model.load_weights(weights_path)\n\n@@ -27,17 +27,15 @@ from ..layers import BatchNormalization\n from ..models import Model\n from .. import backend as K\n from ..engine.topology import get_source_inputs\n-from ..utils.layer_utils import convert_all_kernels_in_model\n+from ..utils import layer_utils\n from ..utils.data_utils import get_file\n from .imagenet_utils import decode_predictions\n from .imagenet_utils import preprocess_input\n from .imagenet_utils import _obtain_input_shape\n \n \n-TH_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_th_dim_ordering_th_kernels.h5'\n-TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n-TH_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_th_dim_ordering_th_kernels_notop.h5'\n-TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n+WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n+WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n \n \n def identity_block(input_tensor, kernel_size, filters, stage, block):\n@@ -65,8 +63,8 @@ def identity_block(input_tensor, kernel_size, filters, stage, block):\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n     x = Activation('relu')(x)\n \n-    x = Conv2D(filters2, kernel_size, kernel_size,\n-               pading='same', name=conv_name_base + '2b')(x)\n+    x = Conv2D(filters2, kernel_size,\n+               padding='same', name=conv_name_base + '2b')(x)\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n     x = Activation('relu')(x)\n \n@@ -107,7 +105,7 @@ def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2))\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n     x = Activation('relu')(x)\n \n-    x = Conv2D(filters2, (kernel_size, kernel_size), pading='same',\n+    x = Conv2D(filters2, kernel_size, padding='same',\n                name=conv_name_base + '2b')(x)\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n     x = Activation('relu')(x)\n@@ -253,18 +251,27 @@ def ResNet50(include_top=True, weights='imagenet',\n \n     # load weights\n     if weights == 'imagenet':\n+        if include_top:\n+            weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels.h5',\n+                                    WEIGHTS_PATH,\n+                                    cache_subdir='models',\n+                                    md5_hash='a7b3fe01876f51b976af0dea6bc144eb')\n+        else:\n+            weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n+                                    WEIGHTS_PATH_NO_TOP,\n+                                    cache_subdir='models',\n+                                    md5_hash='a268eb855778b3df3c7506639542a6af')\n+        model.load_weights(weights_path)\n+        if K.backend() == 'theano':\n+            layer_utils.convert_all_kernels_in_model(model)\n+\n         if K.image_data_format() == 'channels_first':\n             if include_top:\n-                weights_path = get_file('resnet50_weights_th_dim_ordering_th_kernels.h5',\n-                                        TH_WEIGHTS_PATH,\n-                                        cache_subdir='models',\n-                                        md5_hash='1c1f8f5b0c8ee28fe9d950625a230e1c')\n-            else:\n-                weights_path = get_file('resnet50_weights_th_dim_ordering_th_kernels_notop.h5',\n-                                        TH_WEIGHTS_PATH_NO_TOP,\n-                                        cache_subdir='models',\n-                                        md5_hash='f64f049c92468c9affcd44b0976cdafe')\n-            model.load_weights(weights_path)\n+                maxpool = model.get_layer(name='avg_pool')\n+                shape = maxpool.output_shape[1:]\n+                dense = model.get_layer(name='fc1000')\n+                layer_utils.convert_dense_weights_data_format(dense, shape, 'channels_first')\n+\n             if K.backend() == 'tensorflow':\n                 warnings.warn('You are using the TensorFlow backend, yet you '\n                               'are using the Theano '\n@@ -274,19 +281,4 @@ def ResNet50(include_top=True, weights='imagenet',\n                               '`image_data_format=\"channels_last\"` in '\n                               'your Keras config '\n                               'at ~/.keras/keras.json.')\n-                convert_all_kernels_in_model(model)\n-        else:\n-            if include_top:\n-                weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels.h5',\n-                                        TF_WEIGHTS_PATH,\n-                                        cache_subdir='models',\n-                                        md5_hash='a7b3fe01876f51b976af0dea6bc144eb')\n-            else:\n-                weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n-                                        TF_WEIGHTS_PATH_NO_TOP,\n-                                        cache_subdir='models',\n-                                        md5_hash='a268eb855778b3df3c7506639542a6af')\n-            model.load_weights(weights_path)\n-            if K.backend() == 'theano':\n-                convert_all_kernels_in_model(model)\n     return model\n\n@@ -20,7 +20,7 @@ from ..layers import MaxPooling2D\n from ..layers import GlobalAveragePooling2D\n from ..layers import GlobalMaxPooling2D\n from ..engine.topology import get_source_inputs\n-from ..utils.layer_utils import convert_all_kernels_in_model\n+from ..utils import layer_utils\n from ..utils.data_utils import get_file\n from .. import backend as K\n from .imagenet_utils import decode_predictions\n@@ -28,10 +28,8 @@ from .imagenet_utils import preprocess_input\n from .imagenet_utils import _obtain_input_shape\n \n \n-TH_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels.h5'\n-TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n-TH_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels_notop.h5'\n-TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n+WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n+WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n \n \n def VGG16(include_top=True, weights='imagenet',\n@@ -160,16 +158,25 @@ def VGG16(include_top=True, weights='imagenet',\n \n     # load weights\n     if weights == 'imagenet':\n-        if K.image_data_format() == 'channels_first':\n         if include_top:\n-                weights_path = get_file('vgg16_weights_th_dim_ordering_th_kernels.h5',\n-                                        TH_WEIGHTS_PATH,\n+            weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n+                                    WEIGHTS_PATH,\n                                     cache_subdir='models')\n         else:\n-                weights_path = get_file('vgg16_weights_th_dim_ordering_th_kernels_notop.h5',\n-                                        TH_WEIGHTS_PATH_NO_TOP,\n+            weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n+                                    WEIGHTS_PATH_NO_TOP,\n                                     cache_subdir='models')\n         model.load_weights(weights_path)\n+        if K.backend() == 'theano':\n+            layer_utils.convert_all_kernels_in_model(model)\n+\n+        if K.image_data_format() == 'channels_first':\n+            if include_top:\n+                maxpool = model.get_layer(name='block5_pool')\n+                shape = maxpool.output_shape[1:]\n+                dense = model.get_layer(name='fc1')\n+                layer_utils.convert_dense_weights_data_format(dense, shape, 'channels_first')\n+\n             if K.backend() == 'tensorflow':\n                 warnings.warn('You are using the TensorFlow backend, yet you '\n                               'are using the Theano '\n@@ -179,17 +186,4 @@ def VGG16(include_top=True, weights='imagenet',\n                               '`image_data_format=\"channels_last\"` in '\n                               'your Keras config '\n                               'at ~/.keras/keras.json.')\n-                convert_all_kernels_in_model(model)\n-        else:\n-            if include_top:\n-                weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n-                                        TF_WEIGHTS_PATH,\n-                                        cache_subdir='models')\n-            else:\n-                weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n-                                        TF_WEIGHTS_PATH_NO_TOP,\n-                                        cache_subdir='models')\n-            model.load_weights(weights_path)\n-            if K.backend() == 'theano':\n-                convert_all_kernels_in_model(model)\n     return model\n\n@@ -20,7 +20,7 @@ from ..layers import MaxPooling2D\n from ..layers import GlobalAveragePooling2D\n from ..layers import GlobalMaxPooling2D\n from ..engine.topology import get_source_inputs\n-from ..utils.layer_utils import convert_all_kernels_in_model\n+from ..utils import layer_utils\n from ..utils.data_utils import get_file\n from .. import backend as K\n from .imagenet_utils import decode_predictions\n@@ -28,10 +28,8 @@ from .imagenet_utils import preprocess_input\n from .imagenet_utils import _obtain_input_shape\n \n \n-TH_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_th_dim_ordering_th_kernels.h5'\n-TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5'\n-TH_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_th_dim_ordering_th_kernels_notop.h5'\n-TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n+WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5'\n+WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n \n \n def VGG19(include_top=True, weights='imagenet',\n@@ -163,16 +161,25 @@ def VGG19(include_top=True, weights='imagenet',\n \n     # load weights\n     if weights == 'imagenet':\n-        if K.image_data_format() == 'channels_first':\n         if include_top:\n-                weights_path = get_file('vgg19_weights_th_dim_ordering_th_kernels.h5',\n-                                        TH_WEIGHTS_PATH,\n+            weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels.h5',\n+                                    WEIGHTS_PATH,\n                                     cache_subdir='models')\n         else:\n-                weights_path = get_file('vgg19_weights_th_dim_ordering_th_kernels_notop.h5',\n-                                        TH_WEIGHTS_PATH_NO_TOP,\n+            weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n+                                    WEIGHTS_PATH_NO_TOP,\n                                     cache_subdir='models')\n         model.load_weights(weights_path)\n+        if K.backend() == 'theano':\n+            layer_utils.convert_all_kernels_in_model(model)\n+\n+        if K.image_data_format() == 'channels_first':\n+            if include_top:\n+                maxpool = model.get_layer(name='block5_pool')\n+                shape = maxpool.output_shape[1:]\n+                dense = model.get_layer(name='fc1')\n+                layer_utils.convert_dense_weights_data_format(dense, shape, 'channels_first')\n+\n             if K.backend() == 'tensorflow':\n                 warnings.warn('You are using the TensorFlow backend, yet you '\n                               'are using the Theano '\n@@ -182,17 +189,4 @@ def VGG19(include_top=True, weights='imagenet',\n                               '`image_data_format=\"channels_last\"` in '\n                               'your Keras config '\n                               'at ~/.keras/keras.json.')\n-                convert_all_kernels_in_model(model)\n-        else:\n-            if include_top:\n-                weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels.h5',\n-                                        TF_WEIGHTS_PATH,\n-                                        cache_subdir='models')\n-            else:\n-                weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n-                                        TF_WEIGHTS_PATH_NO_TOP,\n-                                        cache_subdir='models')\n-            model.load_weights(weights_path)\n-            if K.backend() == 'theano':\n-                convert_all_kernels_in_model(model)\n     return model\n\n@@ -93,14 +93,6 @@ class BatchNormalization(Layer):\n                                     axes={self.axis: dim})\n         shape = (dim,)\n \n-        if self.center:\n-            self.beta = self.add_weight(shape,\n-                                        name='beta',\n-                                        initializer=self.beta_initializer,\n-                                        regularizer=self.beta_regularizer,\n-                                        constraint=self.beta_constraint)\n-        else:\n-            self.beta = None\n         if self.scale:\n             self.gamma = self.add_weight(shape,\n                                          name='gamma',\n@@ -109,6 +101,14 @@ class BatchNormalization(Layer):\n                                          constraint=self.gamma_constraint)\n         else:\n             self.gamma = None\n+        if self.center:\n+            self.beta = self.add_weight(shape,\n+                                        name='beta',\n+                                        initializer=self.beta_initializer,\n+                                        regularizer=self.beta_regularizer,\n+                                        constraint=self.beta_constraint)\n+        else:\n+            self.beta = None\n         self.moving_mean = self.add_weight(\n             shape,\n             name='moving_mean',\n\n@@ -174,3 +174,42 @@ def convert_all_kernels_in_model(model):\n             converted_w = convert_kernel(original_w)\n             to_assign.append((layer.W, converted_w))\n     K.batch_set_value(to_assign)\n+\n+\n+def convert_dense_weights_data_format(dense,\n+                                      previous_feature_map_shape,\n+                                      target_data_format='channels_first'):\n+    \"\"\"Utility useful when changing a convnet's `data_format`.\n+\n+    When porting the weights of a convnet from one data format to the other,\n+    if the convnet includes a `Flatten` layer\n+    (applied to the last convolutional feature map)\n+    followed by a `Dense` layer, the weights of that `Dense` layer\n+    should be updated to reflect the new dimension ordering.\n+\n+    # Arguments\n+        dense: The target `Dense` layer.\n+        previous_feature_map_shape: A shape tuple of 3 integers,\n+            e.g. `(512, 7, 7)`. The shape of the convolutional\n+            feature map right before the `Flatten` layer that\n+            came before the target `Dense` layer.\n+        target_data_format: One of \"channels_last\", \"channels_first\".\n+            Set it \"channels_last\"\n+            if converting a \"chnnels_first\" model to \"channels_last\",\n+            or reciprocally.\n+    \"\"\"\n+    assert target_data_format in {'channels_last', 'channels_first'}\n+    kernel, bias = dense.get_weights()\n+    for i in range(kernel.shape[1]):\n+        if target_data_format == 'channels_first':\n+            c, h, w = previous_feature_map_shape\n+            original_fm_shape = (h, w, c)\n+            ki = kernel[:, i].reshape(original_fm_shape)\n+            ki = np.transpose(ki, (2, 0, 1))  # last -> first\n+        else:\n+            h, w, c = previous_feature_map_shape\n+            original_fm_shape = (c, h, w)\n+            ki = kernel[:, i].reshape(original_fm_shape)\n+            ki = np.transpose(ki, (1, 2, 0))  # first -> last\n+        kernel[:, i] = np.reshape(ki, (np.prod(previous_feature_map_shape),))\n+    dense.set_weights([kernel, bias])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
