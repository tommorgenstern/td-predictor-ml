{"custom_id": "scrapy#203fa9667fb69f6251c0a44b14cf0450ce769a32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 15 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 36 | Churn Cumulative: 2328 | Contributors (this commit): 24 | Commits (past 90d): 5 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,13 +4,15 @@ import json\n import logging\n from abc import abstractmethod\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Optional, Type, TypeVar, cast\n+from typing import TYPE_CHECKING, Any, Optional, Type, cast\n \n+from queuelib.queue import BaseQueue\n from twisted.internet.defer import Deferred\n \n from scrapy.crawler import Crawler\n from scrapy.dupefilters import BaseDupeFilter\n from scrapy.http.request import Request\n+from scrapy.pqueues import ScrapyPriorityQueue\n from scrapy.spiders import Spider\n from scrapy.statscollectors import StatsCollector\n from scrapy.utils.job import job_dir\n@@ -121,9 +123,6 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):\n         raise NotImplementedError()\n \n \n-SchedulerTV = TypeVar(\"SchedulerTV\", bound=\"Scheduler\")\n-\n-\n class Scheduler(BaseScheduler):\n     \"\"\"\n     Default Scrapy scheduler. This implementation also handles duplication\n@@ -179,24 +178,24 @@ class Scheduler(BaseScheduler):\n         self,\n         dupefilter: BaseDupeFilter,\n         jobdir: Optional[str] = None,\n-        dqclass=None,\n-        mqclass=None,\n+        dqclass: Optional[Type[BaseQueue]] = None,\n+        mqclass: Optional[Type[BaseQueue]] = None,\n         logunser: bool = False,\n         stats: Optional[StatsCollector] = None,\n-        pqclass=None,\n+        pqclass: Optional[Type[ScrapyPriorityQueue]] = None,\n         crawler: Optional[Crawler] = None,\n     ):\n         self.df: BaseDupeFilter = dupefilter\n         self.dqdir: Optional[str] = self._dqdir(jobdir)\n-        self.pqclass = pqclass\n-        self.dqclass = dqclass\n-        self.mqclass = mqclass\n+        self.pqclass: Optional[Type[ScrapyPriorityQueue]] = pqclass\n+        self.dqclass: Optional[Type[BaseQueue]] = dqclass\n+        self.mqclass: Optional[Type[BaseQueue]] = mqclass\n         self.logunser: bool = logunser\n         self.stats: Optional[StatsCollector] = stats\n         self.crawler: Optional[Crawler] = crawler\n \n     @classmethod\n-    def from_crawler(cls: Type[SchedulerTV], crawler: Crawler) -> SchedulerTV:\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         \"\"\"\n         Factory method, initializes the scheduler with arguments taken from the crawl settings\n         \"\"\"\n@@ -221,9 +220,9 @@ class Scheduler(BaseScheduler):\n         (2) initialize the disk queue if the ``jobdir`` attribute is a valid directory\n         (3) return the result of the dupefilter's ``open`` method\n         \"\"\"\n-        self.spider = spider\n-        self.mqs = self._mq()\n-        self.dqs = self._dq() if self.dqdir else None\n+        self.spider: Spider = spider\n+        self.mqs: ScrapyPriorityQueue = self._mq()\n+        self.dqs: Optional[ScrapyPriorityQueue] = self._dq() if self.dqdir else None\n         return self.df.open()\n \n     def close(self, reason: str) -> Optional[Deferred]:\n@@ -320,9 +319,10 @@ class Scheduler(BaseScheduler):\n             return self.dqs.pop()\n         return None\n \n-    def _mq(self):\n+    def _mq(self) -> ScrapyPriorityQueue:\n         \"\"\"Create a new priority queue instance, with in-memory storage\"\"\"\n         assert self.crawler\n+        assert self.pqclass\n         return build_from_crawler(\n             self.pqclass,\n             self.crawler,\n@@ -330,10 +330,11 @@ class Scheduler(BaseScheduler):\n             key=\"\",\n         )\n \n-    def _dq(self):\n+    def _dq(self) -> ScrapyPriorityQueue:\n         \"\"\"Create a new priority queue instance, with disk storage\"\"\"\n         assert self.crawler\n         assert self.dqdir\n+        assert self.pqclass\n         state = self._read_dqs_state(self.dqdir)\n         q = build_from_crawler(\n             self.pqclass,\n\n@@ -16,13 +16,14 @@ from typing import (\n \n from scrapy import Request\n from scrapy.core.downloader import Downloader\n-from scrapy.crawler import Crawler\n from scrapy.utils.misc import build_from_crawler\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n+\n logger = logging.getLogger(__name__)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bd0d4cee885744c7ea38185ec42f0137e7632b79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 12 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 1500 | Contributors (this commit): 24 | Commits (past 90d): 6 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,6 @@ from abc import abstractmethod\n from pathlib import Path\n from typing import TYPE_CHECKING, Any, Optional, Type, cast\n \n-from queuelib.queue import BaseQueue\n from twisted.internet.defer import Deferred\n \n from scrapy.crawler import Crawler\n@@ -19,6 +18,9 @@ from scrapy.utils.job import job_dir\n from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n+    # requires queuelib >= 1.6.2\n+    from queuelib.queue import BaseQueue\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n\n@@ -22,7 +22,7 @@ if TYPE_CHECKING:\n \n \n def _with_mkdir(queue_class: Type[queue.BaseQueue]) -> Type[queue.BaseQueue]:\n-    class DirectoriesCreated(queue_class):\n+    class DirectoriesCreated(queue_class):  # type: ignore[valid-type,misc]\n         def __init__(self, path: Union[str, PathLike], *args: Any, **kwargs: Any):\n             dirname = Path(path).parent\n             if not dirname.exists():\n@@ -37,7 +37,7 @@ def _serializable_queue(\n     serialize: Callable[[Any], bytes],\n     deserialize: Callable[[bytes], Any],\n ) -> Type[queue.BaseQueue]:\n-    class SerializableQueue(queue_class):\n+    class SerializableQueue(queue_class):  # type: ignore[valid-type,misc]\n         def push(self, obj: Any) -> None:\n             s = serialize(obj)\n             super().push(s)\n@@ -71,7 +71,7 @@ def _serializable_queue(\n def _scrapy_serialization_queue(\n     queue_class: Type[queue.BaseQueue],\n ) -> Type[queue.BaseQueue]:\n-    class ScrapyRequestQueue(queue_class):\n+    class ScrapyRequestQueue(queue_class):  # type: ignore[valid-type,misc]\n         def __init__(self, crawler: Crawler, key: str):\n             self.spider = crawler.spider\n             super().__init__(key)\n@@ -110,7 +110,7 @@ def _scrapy_serialization_queue(\n def _scrapy_non_serialization_queue(\n     queue_class: Type[queue.BaseQueue],\n ) -> Type[queue.BaseQueue]:\n-    class ScrapyRequestQueue(queue_class):\n+    class ScrapyRequestQueue(queue_class):  # type: ignore[valid-type,misc]\n         @classmethod\n         def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n             return cls()\n@@ -142,17 +142,18 @@ def _pickle_serialize(obj: Any) -> bytes:\n         raise ValueError(str(e)) from e\n \n \n+# queue.*Queue aren't subclasses of queue.BaseQueue\n _PickleFifoSerializationDiskQueue = _serializable_queue(\n-    _with_mkdir(queue.FifoDiskQueue), _pickle_serialize, pickle.loads\n+    _with_mkdir(queue.FifoDiskQueue), _pickle_serialize, pickle.loads  # type: ignore[arg-type]\n )\n _PickleLifoSerializationDiskQueue = _serializable_queue(\n-    _with_mkdir(queue.LifoDiskQueue), _pickle_serialize, pickle.loads\n+    _with_mkdir(queue.LifoDiskQueue), _pickle_serialize, pickle.loads  # type: ignore[arg-type]\n )\n _MarshalFifoSerializationDiskQueue = _serializable_queue(\n-    _with_mkdir(queue.FifoDiskQueue), marshal.dumps, marshal.loads\n+    _with_mkdir(queue.FifoDiskQueue), marshal.dumps, marshal.loads  # type: ignore[arg-type]\n )\n _MarshalLifoSerializationDiskQueue = _serializable_queue(\n-    _with_mkdir(queue.LifoDiskQueue), marshal.dumps, marshal.loads\n+    _with_mkdir(queue.LifoDiskQueue), marshal.dumps, marshal.loads  # type: ignore[arg-type]\n )\n \n # public queue classes\n@@ -160,5 +161,5 @@ PickleFifoDiskQueue = _scrapy_serialization_queue(_PickleFifoSerializationDiskQu\n PickleLifoDiskQueue = _scrapy_serialization_queue(_PickleLifoSerializationDiskQueue)\n MarshalFifoDiskQueue = _scrapy_serialization_queue(_MarshalFifoSerializationDiskQueue)\n MarshalLifoDiskQueue = _scrapy_serialization_queue(_MarshalLifoSerializationDiskQueue)\n-FifoMemoryQueue = _scrapy_non_serialization_queue(queue.FifoMemoryQueue)\n-LifoMemoryQueue = _scrapy_non_serialization_queue(queue.LifoMemoryQueue)\n+FifoMemoryQueue = _scrapy_non_serialization_queue(queue.FifoMemoryQueue)  # type: ignore[arg-type]\n+LifoMemoryQueue = _scrapy_non_serialization_queue(queue.LifoMemoryQueue)  # type: ignore[arg-type]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1f394306e14ccab9d14ffb9adc64a7c5c08d9af6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 124 | Lines Deleted: 36 | Files Changed: 13 | Hunks: 57 | Methods Changed: 19 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 160 | Churn Cumulative: 9095 | Contributors (this commit): 87 | Commits (past 90d): 22 | Contributors (cumulative): 240 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import warnings\n from typing import TYPE_CHECKING, Any, List, Optional\n \n@@ -25,6 +27,9 @@ from scrapy.utils.misc import build_from_crawler, load_object\n if TYPE_CHECKING:\n     from twisted.internet._sslverify import ClientTLSOptions\n \n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n \n @implementer(IPolicyForHTTPS)\n class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n@@ -62,7 +67,7 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n         method: int = SSL.SSLv23_METHOD,\n         *args: Any,\n         **kwargs: Any,\n-    ):\n+    ) -> Self:\n         tls_verbose_logging: bool = settings.getbool(\n             \"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\"\n         )\n\n@@ -28,18 +28,26 @@ In case of status 200 request, response.headers will come with two keys:\n     'Size' - with size of the downloaded data\n \"\"\"\n \n+from __future__ import annotations\n+\n import re\n from io import BytesIO\n+from typing import TYPE_CHECKING\n from urllib.parse import unquote\n \n from twisted.internet.protocol import ClientCreator, Protocol\n from twisted.protocols.ftp import CommandFailed, FTPClient\n \n+from scrapy.crawler import Crawler\n from scrapy.http import Response\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n \n class ReceivedDataProtocol(Protocol):\n     def __init__(self, filename=None):\n@@ -76,7 +84,7 @@ class FTPDownloadHandler:\n         self.passive_mode = settings[\"FTP_PASSIVE_MODE\"]\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler.settings)\n \n     def download_request(self, request, spider):\n\n@@ -1,9 +1,17 @@\n \"\"\"Download handlers for http and https schemes\n \"\"\"\n \n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING\n+\n from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import to_unicode\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n \n class HTTP10DownloadHandler:\n     lazy = False\n@@ -17,7 +25,7 @@ class HTTP10DownloadHandler:\n         self._crawler = crawler\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler) -> Self:\n         return cls(crawler.settings, crawler)\n \n     def download_request(self, request, spider):\n\n@@ -1,11 +1,14 @@\n \"\"\"Download handlers for http and https schemes\"\"\"\n \n+from __future__ import annotations\n+\n import ipaddress\n import logging\n import re\n from contextlib import suppress\n from io import BytesIO\n from time import time\n+from typing import TYPE_CHECKING\n from urllib.parse import urldefrag, urlunparse\n \n from twisted.internet import defer, protocol, ssl\n@@ -32,6 +35,11 @@ from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.python import to_bytes, to_unicode\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -56,7 +64,7 @@ class HTTP11DownloadHandler:\n         self._disconnect_timeout = 1\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler) -> Self:\n         return cls(crawler.settings, crawler)\n \n     def download_request(self, request, spider):\n\n@@ -1,5 +1,7 @@\n+from __future__ import annotations\n+\n from time import time\n-from typing import Optional, Type, TypeVar\n+from typing import TYPE_CHECKING, Optional\n from urllib.parse import urldefrag\n \n from twisted.internet.base import DelayedCall\n@@ -16,9 +18,9 @@ from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.utils.python import to_bytes\n \n-H2DownloadHandlerOrSubclass = TypeVar(\n-    \"H2DownloadHandlerOrSubclass\", bound=\"H2DownloadHandler\"\n-)\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n \n class H2DownloadHandler:\n@@ -31,9 +33,7 @@ class H2DownloadHandler:\n         self._context_factory = load_context_factory_from_settings(settings, crawler)\n \n     @classmethod\n-    def from_crawler(\n-        cls: Type[H2DownloadHandlerOrSubclass], crawler: Crawler\n-    ) -> H2DownloadHandlerOrSubclass:\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler.settings, crawler)\n \n     def download_request(self, request: Request, spider: Spider) -> Deferred:\n\n@@ -1,9 +1,17 @@\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING\n+\n from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import build_from_crawler\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n \n class S3DownloadHandler:\n     def __init__(\n@@ -57,7 +65,7 @@ class S3DownloadHandler:\n         self._download_http = _http_handler.download_request\n \n     @classmethod\n-    def from_crawler(cls, crawler, **kwargs):\n+    def from_crawler(cls, crawler, **kwargs) -> Self:\n         return cls(crawler.settings, crawler=crawler, **kwargs)\n \n     def download_request(self, request, spider):\n\n@@ -5,8 +5,11 @@ requests in Scrapy.\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n import inspect\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     AnyStr,\n     Callable,\n@@ -17,8 +20,6 @@ from typing import (\n     NoReturn,\n     Optional,\n     Tuple,\n-    Type,\n-    TypeVar,\n     Union,\n     cast,\n )\n@@ -32,7 +33,9 @@ from scrapy.utils.python import to_bytes\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import escape_ajax\n \n-RequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n \n def NO_CALLBACK(*args: Any, **kwargs: Any) -> NoReturn:\n@@ -186,11 +189,11 @@ class Request(object_ref):\n \n     @classmethod\n     def from_curl(\n-        cls: Type[RequestTypeVar],\n+        cls,\n         curl_command: str,\n         ignore_unknown_options: bool = True,\n         **kwargs: Any,\n-    ) -> RequestTypeVar:\n+    ) -> Self:\n         \"\"\"Create a Request object from a string containing a `cURL\n         <https://curl.haxx.se/>`_ command. It populates the HTTP method, the\n         URL, the headers, the cookies and the body. It accepts the same\n\n@@ -4,6 +4,8 @@ Files Pipeline\n See documentation in topics/media-pipeline.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n import base64\n import functools\n import hashlib\n@@ -16,7 +18,7 @@ from ftplib import FTP\n from io import BytesIO\n from os import PathLike\n from pathlib import Path\n-from typing import IO, DefaultDict, Optional, Set, Union\n+from typing import IO, TYPE_CHECKING, DefaultDict, Optional, Set, Type, Union, cast\n from urllib.parse import urlparse\n \n from itemadapter import ItemAdapter\n@@ -34,6 +36,10 @@ from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.python import to_bytes\n from scrapy.utils.request import referer_str\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -385,8 +391,8 @@ class FilesPipeline(MediaPipeline):\n         super().__init__(download_func=download_func, settings=settings)\n \n     @classmethod\n-    def from_settings(cls, settings):\n-        s3store = cls.STORE_SCHEMES[\"s3\"]\n+    def from_settings(cls, settings) -> Self:\n+        s3store: Type[S3FilesStore] = cast(Type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n         s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n         s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n         s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n@@ -396,11 +402,15 @@ class FilesPipeline(MediaPipeline):\n         s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n         s3store.POLICY = settings[\"FILES_STORE_S3_ACL\"]\n \n-        gcs_store = cls.STORE_SCHEMES[\"gs\"]\n+        gcs_store: Type[GCSFilesStore] = cast(\n+            Type[GCSFilesStore], cls.STORE_SCHEMES[\"gs\"]\n+        )\n         gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n         gcs_store.POLICY = settings[\"FILES_STORE_GCS_ACL\"] or None\n \n-        ftp_store = cls.STORE_SCHEMES[\"ftp\"]\n+        ftp_store: Type[FTPFilesStore] = cast(\n+            Type[FTPFilesStore], cls.STORE_SCHEMES[\"ftp\"]\n+        )\n         ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n         ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n         ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n\n@@ -4,25 +4,38 @@ Images Pipeline\n See documentation in topics/media-pipeline.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n import functools\n import hashlib\n import warnings\n from contextlib import suppress\n from io import BytesIO\n from os import PathLike\n-from typing import Dict, Tuple, Union\n+from typing import TYPE_CHECKING, Dict, Tuple, Type, Union, cast\n \n from itemadapter import ItemAdapter\n \n from scrapy.exceptions import DropItem, NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.http.request import NO_CALLBACK\n-from scrapy.pipelines.files import FileException, FilesPipeline, _md5sum\n+from scrapy.pipelines.files import (\n+    FileException,\n+    FilesPipeline,\n+    FTPFilesStore,\n+    GCSFilesStore,\n+    S3FilesStore,\n+    _md5sum,\n+)\n \n # TODO: from scrapy.pipelines.media import MediaPipeline\n from scrapy.settings import Settings\n from scrapy.utils.python import get_func_args, to_bytes\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n \n class NoimagesDrop(DropItem):\n     \"\"\"Product with no images exception\"\"\"\n@@ -96,8 +109,8 @@ class ImagesPipeline(FilesPipeline):\n         self._deprecated_convert_image = None\n \n     @classmethod\n-    def from_settings(cls, settings):\n-        s3store = cls.STORE_SCHEMES[\"s3\"]\n+    def from_settings(cls, settings) -> Self:\n+        s3store: Type[S3FilesStore] = cast(Type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n         s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n         s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n         s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n@@ -107,11 +120,15 @@ class ImagesPipeline(FilesPipeline):\n         s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n         s3store.POLICY = settings[\"IMAGES_STORE_S3_ACL\"]\n \n-        gcs_store = cls.STORE_SCHEMES[\"gs\"]\n+        gcs_store: Type[GCSFilesStore] = cast(\n+            Type[GCSFilesStore], cls.STORE_SCHEMES[\"gs\"]\n+        )\n         gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n         gcs_store.POLICY = settings[\"IMAGES_STORE_GCS_ACL\"] or None\n \n-        ftp_store = cls.STORE_SCHEMES[\"ftp\"]\n+        ftp_store: Type[FTPFilesStore] = cast(\n+            Type[FTPFilesStore], cls.STORE_SCHEMES[\"ftp\"]\n+        )\n         ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n         ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n         ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n\n@@ -1,6 +1,9 @@\n+from __future__ import annotations\n+\n import functools\n import logging\n from collections import defaultdict\n+from typing import TYPE_CHECKING\n \n from twisted.internet.defer import Deferred, DeferredList\n from twisted.python.failure import Failure\n@@ -12,6 +15,11 @@ from scrapy.utils.defer import defer_result, mustbe_deferred\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import arg_to_iter\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -67,9 +75,9 @@ class MediaPipeline:\n         return formatted_key\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler) -> Self:\n         try:\n-            pipe = cls.from_settings(crawler.settings)\n+            pipe = cls.from_settings(crawler.settings)  # type: ignore[attr-defined]\n         except AttributeError:\n             pipe = cls()\n         pipe.crawler = crawler\n\n@@ -5,8 +5,10 @@ for scraping typical web sites that requires crawling pages.\n See documentation in docs/topics/spiders.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n import copy\n-from typing import AsyncIterable, Awaitable, Sequence\n+from typing import TYPE_CHECKING, AsyncIterable, Awaitable, Sequence\n \n from scrapy.http import HtmlResponse, Request, Response\n from scrapy.linkextractors import LinkExtractor\n@@ -14,6 +16,10 @@ from scrapy.spiders import Spider\n from scrapy.utils.asyncgen import collect_asyncgen\n from scrapy.utils.spider import iterate_spider_output\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n \n def _identity(x):\n     return x\n@@ -140,9 +146,9 @@ class CrawlSpider(Spider):\n             self._rules[-1]._compile(self)\n \n     @classmethod\n-    def from_crawler(cls, crawler, *args, **kwargs):\n+    def from_crawler(cls, crawler, *args, **kwargs) -> Self:\n         spider = super().from_crawler(crawler, *args, **kwargs)\n-        spider._follow_links = crawler.settings.getbool(\n+        spider._follow_links = crawler.settings.getbool(  # type: ignore[attr-defined]\n             \"CRAWLSPIDER_FOLLOW_LINKS\", True\n         )\n         return spider\n\n@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import logging\n import re\n from typing import TYPE_CHECKING, Any\n@@ -26,7 +28,7 @@ class SitemapSpider(Spider):\n     _warn_size: int\n \n     @classmethod\n-    def from_crawler(cls, crawler: \"Crawler\", *args: Any, **kwargs: Any) -> \"Self\":\n+    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n         spider = super().from_crawler(crawler, *args, **kwargs)\n         spider._max_size = getattr(\n             spider, \"download_maxsize\", spider.settings.getint(\"DOWNLOAD_MAXSIZE\")\n\n@@ -3,6 +3,8 @@ This module provides some useful functions for working with\n scrapy.http.Request objects\n \"\"\"\n \n+from __future__ import annotations\n+\n import hashlib\n import json\n import warnings\n@@ -32,6 +34,9 @@ from scrapy.utils.misc import load_object\n from scrapy.utils.python import to_bytes, to_unicode\n \n if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n     from scrapy.crawler import Crawler\n \n \n@@ -133,10 +138,10 @@ class RequestFingerprinter:\n     \"\"\"\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler) -> Self:\n         return cls(crawler)\n \n-    def __init__(self, crawler: Optional[\"Crawler\"] = None):\n+    def __init__(self, crawler: Optional[Crawler] = None):\n         if crawler:\n             implementation = crawler.settings.get(\n                 \"REQUEST_FINGERPRINTER_IMPLEMENTATION\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8a08283580176049cb539423795830b9faea91a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 51 | Files Changed: 2 | Hunks: 41 | Methods Changed: 46 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 127 | Churn Cumulative: 1443 | Contributors (this commit): 25 | Commits (past 90d): 4 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,36 +1,56 @@\n+from __future__ import annotations\n+\n import re\n import time\n from http.cookiejar import Cookie\n from http.cookiejar import CookieJar as _CookieJar\n-from http.cookiejar import DefaultCookiePolicy\n-from typing import Sequence\n+from http.cookiejar import CookiePolicy, DefaultCookiePolicy\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    Dict,\n+    Iterator,\n+    List,\n+    Optional,\n+    Sequence,\n+    Tuple,\n+    cast,\n+)\n \n from scrapy import Request\n from scrapy.http import Response\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_unicode\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n # Defined in the http.cookiejar module, but undocumented:\n # https://github.com/python/cpython/blob/v3.9.0/Lib/http/cookiejar.py#L527\n IPV4_RE = re.compile(r\"\\.\\d+$\", re.ASCII)\n \n \n class CookieJar:\n-    def __init__(self, policy=None, check_expired_frequency=10000):\n-        self.policy = policy or DefaultCookiePolicy()\n-        self.jar = _CookieJar(self.policy)\n-        self.jar._cookies_lock = _DummyLock()\n-        self.check_expired_frequency = check_expired_frequency\n-        self.processed = 0\n+    def __init__(\n+        self,\n+        policy: Optional[CookiePolicy] = None,\n+        check_expired_frequency: int = 10000,\n+    ):\n+        self.policy: CookiePolicy = policy or DefaultCookiePolicy()\n+        self.jar: _CookieJar = _CookieJar(self.policy)\n+        self.jar._cookies_lock = _DummyLock()  # type: ignore[attr-defined]\n+        self.check_expired_frequency: int = check_expired_frequency\n+        self.processed: int = 0\n \n-    def extract_cookies(self, response, request):\n+    def extract_cookies(self, response: Response, request: Request) -> None:\n         wreq = WrappedRequest(request)\n         wrsp = WrappedResponse(response)\n-        return self.jar.extract_cookies(wrsp, wreq)\n+        self.jar.extract_cookies(wrsp, wreq)  # type: ignore[arg-type]\n \n     def add_cookie_header(self, request: Request) -> None:\n         wreq = WrappedRequest(request)\n-        self.policy._now = self.jar._now = int(time.time())\n+        self.policy._now = self.jar._now = int(time.time())  # type: ignore[attr-defined]\n \n         # the cookiejar implementation iterates through all domains\n         # instead we restrict to potential matches on the domain\n@@ -47,10 +67,10 @@ class CookieJar:\n \n         cookies = []\n         for host in hosts:\n-            if host in self.jar._cookies:\n-                cookies += self.jar._cookies_for_domain(host, wreq)\n+            if host in self.jar._cookies:  # type: ignore[attr-defined]\n+                cookies += self.jar._cookies_for_domain(host, wreq)  # type: ignore[attr-defined]\n \n-        attrs = self.jar._cookie_attrs(cookies)\n+        attrs = self.jar._cookie_attrs(cookies)  # type: ignore[attr-defined]\n         if attrs:\n             if not wreq.has_header(\"Cookie\"):\n                 wreq.add_unredirected_header(\"Cookie\", \"; \".join(attrs))\n@@ -61,37 +81,42 @@ class CookieJar:\n             self.jar.clear_expired_cookies()\n \n     @property\n-    def _cookies(self):\n-        return self.jar._cookies\n+    def _cookies(self) -> Dict[str, Dict[str, Dict[str, Cookie]]]:\n+        return self.jar._cookies  # type: ignore[attr-defined,no-any-return]\n \n-    def clear_session_cookies(self, *args, **kwargs):\n-        return self.jar.clear_session_cookies(*args, **kwargs)\n+    def clear_session_cookies(self) -> None:\n+        return self.jar.clear_session_cookies()\n \n-    def clear(self, domain=None, path=None, name=None):\n-        return self.jar.clear(domain, path, name)\n+    def clear(\n+        self,\n+        domain: Optional[str] = None,\n+        path: Optional[str] = None,\n+        name: Optional[str] = None,\n+    ) -> None:\n+        self.jar.clear(domain, path, name)\n \n-    def __iter__(self):\n+    def __iter__(self) -> Iterator[Cookie]:\n         return iter(self.jar)\n \n-    def __len__(self):\n+    def __len__(self) -> int:\n         return len(self.jar)\n \n-    def set_policy(self, pol):\n-        return self.jar.set_policy(pol)\n+    def set_policy(self, pol: CookiePolicy) -> None:\n+        self.jar.set_policy(pol)\n \n     def make_cookies(self, response: Response, request: Request) -> Sequence[Cookie]:\n         wreq = WrappedRequest(request)\n         wrsp = WrappedResponse(response)\n-        return self.jar.make_cookies(wrsp, wreq)\n+        return self.jar.make_cookies(wrsp, wreq)  # type: ignore[arg-type]\n \n-    def set_cookie(self, cookie):\n+    def set_cookie(self, cookie: Cookie) -> None:\n         self.jar.set_cookie(cookie)\n \n     def set_cookie_if_ok(self, cookie: Cookie, request: Request) -> None:\n-        self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))\n+        self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))  # type: ignore[arg-type]\n \n \n-def potential_domain_matches(domain):\n+def potential_domain_matches(domain: str) -> List[str]:\n     \"\"\"Potential domain matches for a cookie\n \n     >>> potential_domain_matches('www.example.com')\n@@ -111,10 +136,10 @@ def potential_domain_matches(domain):\n \n \n class _DummyLock:\n-    def acquire(self):\n+    def acquire(self) -> None:\n         pass\n \n-    def release(self):\n+    def release(self) -> None:\n         pass\n \n \n@@ -124,19 +149,19 @@ class WrappedRequest:\n     see http://docs.python.org/library/urllib2.html#urllib2.Request\n     \"\"\"\n \n-    def __init__(self, request):\n+    def __init__(self, request: Request):\n         self.request = request\n \n-    def get_full_url(self):\n+    def get_full_url(self) -> str:\n         return self.request.url\n \n-    def get_host(self):\n+    def get_host(self) -> str:\n         return urlparse_cached(self.request).netloc\n \n-    def get_type(self):\n+    def get_type(self) -> str:\n         return urlparse_cached(self.request).scheme\n \n-    def is_unverifiable(self):\n+    def is_unverifiable(self) -> bool:\n         \"\"\"Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.\n \n         It defaults to False. An unverifiable request is one whose URL the user did not have the\n@@ -144,36 +169,36 @@ class WrappedRequest:\n         HTML document, and the user had no option to approve the automatic\n         fetching of the image, this should be true.\n         \"\"\"\n-        return self.request.meta.get(\"is_unverifiable\", False)\n+        return cast(bool, self.request.meta.get(\"is_unverifiable\", False))\n \n     @property\n-    def full_url(self):\n+    def full_url(self) -> str:\n         return self.get_full_url()\n \n     @property\n-    def host(self):\n+    def host(self) -> str:\n         return self.get_host()\n \n     @property\n-    def type(self):\n+    def type(self) -> str:\n         return self.get_type()\n \n     @property\n-    def unverifiable(self):\n+    def unverifiable(self) -> bool:\n         return self.is_unverifiable()\n \n     @property\n-    def origin_req_host(self):\n-        return urlparse_cached(self.request).hostname\n+    def origin_req_host(self) -> str:\n+        return cast(str, urlparse_cached(self.request).hostname)\n \n-    def has_header(self, name):\n+    def has_header(self, name: str) -> bool:\n         return name in self.request.headers\n \n-    def get_header(self, name, default=None):\n+    def get_header(self, name: str, default: Optional[str] = None) -> Optional[str]:\n         value = self.request.headers.get(name, default)\n         return to_unicode(value, errors=\"replace\") if value is not None else None\n \n-    def header_items(self):\n+    def header_items(self) -> List[Tuple[str, List[str]]]:\n         return [\n             (\n                 to_unicode(k, errors=\"replace\"),\n@@ -182,18 +207,18 @@ class WrappedRequest:\n             for k, v in self.request.headers.items()\n         ]\n \n-    def add_unredirected_header(self, name, value):\n+    def add_unredirected_header(self, name: str, value: str) -> None:\n         self.request.headers.appendlist(name, value)\n \n \n class WrappedResponse:\n-    def __init__(self, response):\n+    def __init__(self, response: Response):\n         self.response = response\n \n-    def info(self):\n+    def info(self) -> Self:\n         return self\n \n-    def get_all(self, name, default=None):\n+    def get_all(self, name: str, default: Any = None) -> List[str]:\n         return [\n             to_unicode(v, errors=\"replace\") for v in self.response.headers.getlist(name)\n         ]\n\n@@ -159,12 +159,12 @@ class TextResponse(Response):\n     def jmespath(self, query: str, **kwargs: Any) -> SelectorList:\n         from scrapy.selector import SelectorList\n \n-        if not hasattr(self.selector, \"jmespath\"):  # type: ignore[attr-defined]\n+        if not hasattr(self.selector, \"jmespath\"):\n             raise AttributeError(\n                 \"Please install parsel >= 1.8.1 to get jmespath support\"\n             )\n \n-        return cast(SelectorList, self.selector.jmespath(query, **kwargs))  # type: ignore[attr-defined]\n+        return cast(SelectorList, self.selector.jmespath(query, **kwargs))\n \n     def xpath(self, query: str, **kwargs: Any) -> SelectorList:\n         from scrapy.selector import SelectorList\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c76dfc383f34514a5a2841d2e32ac4e0c8c751a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 91 | Lines Deleted: 69 | Files Changed: 3 | Hunks: 31 | Methods Changed: 32 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 160 | Churn Cumulative: 1051 | Contributors (this commit): 34 | Commits (past 90d): 10 | Contributors (cumulative): 52 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,7 @@ from urllib.parse import urlencode\n import scrapy\n from scrapy import Request\n from scrapy.commands import ScrapyCommand\n-from scrapy.http import Response\n+from scrapy.http import Response, TextResponse\n from scrapy.linkextractors import LinkExtractor\n \n \n@@ -61,5 +61,6 @@ class _BenchSpider(scrapy.Spider):\n         return [scrapy.Request(url, dont_filter=True)]\n \n     def parse(self, response: Response) -> Any:  # type: ignore[override]\n+        assert isinstance(Response, TextResponse)\n         for link in self.link_extractor.extract_links(response):\n             yield scrapy.Request(link.url, callback=self.parse)\n\n@@ -7,6 +7,7 @@ For more info see docs/topics/link-extractors.rst\n \"\"\"\n \n import re\n+from typing import Iterable\n \n # common file extensions that are not followed if they occur in links\n IGNORED_EXTENSIONS = [\n@@ -110,14 +111,11 @@ IGNORED_EXTENSIONS = [\n ]\n \n \n-_re_type = type(re.compile(\"\", 0))\n-\n-\n-def _matches(url, regexs):\n+def _matches(url: str, regexs: Iterable[re.Pattern[str]]) -> bool:\n     return any(r.search(url) for r in regexs)\n \n \n-def _is_valid_url(url):\n+def _is_valid_url(url: str) -> bool:\n     return url.split(\"://\", 1)[0] in {\"http\", \"https\", \"file\", \"ftp\"}\n \n \n\n@@ -5,21 +5,19 @@ Link extractor based on lxml.html\n import logging\n import operator\n from functools import partial\n+from typing import Any, Callable, Iterable, List, Optional, Set, Tuple, Union, cast\n from urllib.parse import urljoin, urlparse\n \n from lxml import etree  # nosec\n+from lxml.html import HtmlElement  # nosec\n from parsel.csstranslator import HTMLTranslator\n from w3lib.html import strip_html5_whitespace\n from w3lib.url import canonicalize_url, safe_url_string\n \n+from scrapy import Selector\n+from scrapy.http import TextResponse\n from scrapy.link import Link\n-from scrapy.linkextractors import (\n-    IGNORED_EXTENSIONS,\n-    _is_valid_url,\n-    _matches,\n-    _re_type,\n-    re,\n-)\n+from scrapy.linkextractors import IGNORED_EXTENSIONS, _is_valid_url, _matches, re\n from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n from scrapy.utils.python import unique as unique_list\n from scrapy.utils.response import get_base_url\n@@ -33,41 +31,56 @@ XHTML_NAMESPACE = \"http://www.w3.org/1999/xhtml\"\n _collect_string_content = etree.XPath(\"string()\")\n \n \n-def _nons(tag):\n+def _nons(tag: Any) -> Any:\n     if isinstance(tag, str):\n         if tag[0] == \"{\" and tag[1 : len(XHTML_NAMESPACE) + 1] == XHTML_NAMESPACE:\n             return tag.split(\"}\")[-1]\n     return tag\n \n \n-def _identity(x):\n+def _identity(x: Any) -> Any:\n     return x\n \n \n-def _canonicalize_link_url(link):\n+def _canonicalize_link_url(link: Link) -> str:\n     return canonicalize_url(link.url, keep_fragments=True)\n \n \n class LxmlParserLinkExtractor:\n     def __init__(\n         self,\n-        tag=\"a\",\n-        attr=\"href\",\n-        process=None,\n-        unique=False,\n-        strip=True,\n-        canonicalized=False,\n+        tag: Union[str, Callable[[str], bool]] = \"a\",\n+        attr: Union[str, Callable[[str], bool]] = \"href\",\n+        process: Optional[Callable[[Any], Any]] = None,\n+        unique: bool = False,\n+        strip: bool = True,\n+        canonicalized: bool = False,\n     ):\n-        self.scan_tag = tag if callable(tag) else partial(operator.eq, tag)\n-        self.scan_attr = attr if callable(attr) else partial(operator.eq, attr)\n-        self.process_attr = process if callable(process) else _identity\n-        self.unique = unique\n-        self.strip = strip\n-        self.link_key = (\n-            operator.attrgetter(\"url\") if canonicalized else _canonicalize_link_url\n+        # mypy doesn't infer types for operator.* and also for partial()\n+        self.scan_tag: Callable[[str], bool] = (\n+            tag\n+            if callable(tag)\n+            else cast(Callable[[str], bool], partial(operator.eq, tag))\n+        )\n+        self.scan_attr: Callable[[str], bool] = (\n+            attr\n+            if callable(attr)\n+            else cast(Callable[[str], bool], partial(operator.eq, attr))\n+        )\n+        self.process_attr: Callable[[Any], Any] = (\n+            process if callable(process) else _identity\n+        )\n+        self.unique: bool = unique\n+        self.strip: bool = strip\n+        self.link_key: Callable[[Link], str] = (\n+            cast(Callable[[Link], str], operator.attrgetter(\"url\"))\n+            if canonicalized\n+            else _canonicalize_link_url\n         )\n \n-    def _iter_links(self, document):\n+    def _iter_links(\n+        self, document: HtmlElement\n+    ) -> Iterable[Tuple[HtmlElement, str, str]]:\n         for el in document.iter(etree.Element):\n             if not self.scan_tag(_nons(el.tag)):\n                 continue\n@@ -75,10 +88,16 @@ class LxmlParserLinkExtractor:\n             for attrib in attribs:\n                 if not self.scan_attr(attrib):\n                     continue\n-                yield (el, attrib, attribs[attrib])\n+                yield el, attrib, attribs[attrib]\n \n-    def _extract_links(self, selector, response_url, response_encoding, base_url):\n-        links = []\n+    def _extract_links(\n+        self,\n+        selector: Selector,\n+        response_url: str,\n+        response_encoding: str,\n+        base_url: str,\n+    ) -> List[Link]:\n+        links: List[Link] = []\n         # hacky way to get the underlying lxml parsed document\n         for el, attr, attr_val in self._iter_links(selector.root):\n             # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)\n@@ -108,44 +127,48 @@ class LxmlParserLinkExtractor:\n             links.append(link)\n         return self._deduplicate_if_needed(links)\n \n-    def extract_links(self, response):\n+    def extract_links(self, response: TextResponse) -> List[Link]:\n         base_url = get_base_url(response)\n         return self._extract_links(\n             response.selector, response.url, response.encoding, base_url\n         )\n \n-    def _process_links(self, links):\n+    def _process_links(self, links: List[Link]) -> List[Link]:\n         \"\"\"Normalize and filter extracted links\n \n         The subclass should override it if necessary\n         \"\"\"\n         return self._deduplicate_if_needed(links)\n \n-    def _deduplicate_if_needed(self, links):\n+    def _deduplicate_if_needed(self, links: List[Link]) -> List[Link]:\n         if self.unique:\n             return unique_list(links, key=self.link_key)\n         return links\n \n \n+_RegexT = Union[str, re.Pattern[str]]\n+_RegexOrSeveralT = Union[_RegexT, Iterable[_RegexT]]\n+\n+\n class LxmlLinkExtractor:\n     _csstranslator = HTMLTranslator()\n \n     def __init__(\n         self,\n-        allow=(),\n-        deny=(),\n-        allow_domains=(),\n-        deny_domains=(),\n-        restrict_xpaths=(),\n-        tags=(\"a\", \"area\"),\n-        attrs=(\"href\",),\n-        canonicalize=False,\n-        unique=True,\n-        process_value=None,\n-        deny_extensions=None,\n-        restrict_css=(),\n-        strip=True,\n-        restrict_text=None,\n+        allow: _RegexOrSeveralT = (),\n+        deny: _RegexOrSeveralT = (),\n+        allow_domains: Union[str, Iterable[str]] = (),\n+        deny_domains: Union[str, Iterable[str]] = (),\n+        restrict_xpaths: Union[str, Iterable[str]] = (),\n+        tags: Union[str, Iterable[str]] = (\"a\", \"area\"),\n+        attrs: Union[str, Iterable[str]] = (\"href\",),\n+        canonicalize: bool = False,\n+        unique: bool = True,\n+        process_value: Optional[Callable[[Any], Any]] = None,\n+        deny_extensions: Union[str, Iterable[str], None] = None,\n+        restrict_css: Union[str, Iterable[str]] = (),\n+        strip: bool = True,\n+        restrict_text: Optional[_RegexOrSeveralT] = None,\n     ):\n         tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n         self.link_extractor = LxmlParserLinkExtractor(\n@@ -156,31 +179,31 @@ class LxmlLinkExtractor:\n             strip=strip,\n             canonicalized=not canonicalize,\n         )\n-        self.allow_res = [\n-            x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(allow)\n-        ]\n-        self.deny_res = [\n-            x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(deny)\n-        ]\n+        self.allow_res: List[re.Pattern[str]] = self._compile_regexes(allow)\n+        self.deny_res: List[re.Pattern[str]] = self._compile_regexes(deny)\n \n-        self.allow_domains = set(arg_to_iter(allow_domains))\n-        self.deny_domains = set(arg_to_iter(deny_domains))\n+        self.allow_domains: Set[str] = set(arg_to_iter(allow_domains))\n+        self.deny_domains: Set[str] = set(arg_to_iter(deny_domains))\n \n-        self.restrict_xpaths = tuple(arg_to_iter(restrict_xpaths))\n+        self.restrict_xpaths: Tuple[str, ...] = tuple(arg_to_iter(restrict_xpaths))\n         self.restrict_xpaths += tuple(\n             map(self._csstranslator.css_to_xpath, arg_to_iter(restrict_css))\n         )\n \n         if deny_extensions is None:\n             deny_extensions = IGNORED_EXTENSIONS\n-        self.canonicalize = canonicalize\n-        self.deny_extensions = {\".\" + e for e in arg_to_iter(deny_extensions)}\n-        self.restrict_text = [\n-            x if isinstance(x, _re_type) else re.compile(x)\n-            for x in arg_to_iter(restrict_text)\n+        self.canonicalize: bool = canonicalize\n+        self.deny_extensions: Set[str] = {\".\" + e for e in arg_to_iter(deny_extensions)}\n+        self.restrict_text: List[re.Pattern[str]] = self._compile_regexes(restrict_text)\n+\n+    @staticmethod\n+    def _compile_regexes(value: Optional[_RegexOrSeveralT]) -> List[re.Pattern[str]]:\n+        return [\n+            x if isinstance(x, re.Pattern) else re.compile(x)\n+            for x in arg_to_iter(value)\n         ]\n \n-    def _link_allowed(self, link):\n+    def _link_allowed(self, link: Link) -> bool:\n         if not _is_valid_url(link.url):\n             return False\n         if self.allow_res and not _matches(link.url, self.allow_res):\n@@ -202,7 +225,7 @@ class LxmlLinkExtractor:\n             return False\n         return True\n \n-    def matches(self, url):\n+    def matches(self, url: str) -> bool:\n         if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains):\n             return False\n         if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):\n@@ -216,7 +239,7 @@ class LxmlLinkExtractor:\n         denied = (regex.search(url) for regex in self.deny_res) if self.deny_res else []\n         return any(allowed) and not any(denied)\n \n-    def _process_links(self, links):\n+    def _process_links(self, links: List[Link]) -> List[Link]:\n         links = [x for x in links if self._link_allowed(x)]\n         if self.canonicalize:\n             for link in links:\n@@ -224,10 +247,10 @@ class LxmlLinkExtractor:\n         links = self.link_extractor._process_links(links)\n         return links\n \n-    def _extract_links(self, *args, **kwargs):\n+    def _extract_links(self, *args: Any, **kwargs: Any) -> List[Link]:\n         return self.link_extractor._extract_links(*args, **kwargs)\n \n-    def extract_links(self, response):\n+    def extract_links(self, response: TextResponse) -> List[Link]:\n         \"\"\"Returns a list of :class:`~scrapy.link.Link` objects from the\n         specified :class:`response <scrapy.http.Response>`.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#40e4a5960477299e36a0f7363db4a3c03576f64e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 932 | Contributors (this commit): 31 | Commits (past 90d): 9 | Contributors (cumulative): 41 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,7 @@ For more info see docs/topics/link-extractors.rst\n \"\"\"\n \n import re\n-from typing import Iterable\n+from typing import Iterable, Pattern\n \n # common file extensions that are not followed if they occur in links\n IGNORED_EXTENSIONS = [\n@@ -111,7 +111,7 @@ IGNORED_EXTENSIONS = [\n ]\n \n \n-def _matches(url: str, regexs: Iterable[re.Pattern[str]]) -> bool:\n+def _matches(url: str, regexs: Iterable[Pattern[str]]) -> bool:\n     return any(r.search(url) for r in regexs)\n \n \n\n@@ -5,7 +5,18 @@ Link extractor based on lxml.html\n import logging\n import operator\n from functools import partial\n-from typing import Any, Callable, Iterable, List, Optional, Set, Tuple, Union, cast\n+from typing import (\n+    Any,\n+    Callable,\n+    Iterable,\n+    List,\n+    Optional,\n+    Pattern,\n+    Set,\n+    Tuple,\n+    Union,\n+    cast,\n+)\n from urllib.parse import urljoin, urlparse\n \n from lxml import etree  # nosec\n@@ -146,7 +157,7 @@ class LxmlParserLinkExtractor:\n         return links\n \n \n-_RegexT = Union[str, re.Pattern[str]]\n+_RegexT = Union[str, Pattern[str]]\n _RegexOrSeveralT = Union[_RegexT, Iterable[_RegexT]]\n \n \n@@ -179,8 +190,8 @@ class LxmlLinkExtractor:\n             strip=strip,\n             canonicalized=not canonicalize,\n         )\n-        self.allow_res: List[re.Pattern[str]] = self._compile_regexes(allow)\n-        self.deny_res: List[re.Pattern[str]] = self._compile_regexes(deny)\n+        self.allow_res: List[Pattern[str]] = self._compile_regexes(allow)\n+        self.deny_res: List[Pattern[str]] = self._compile_regexes(deny)\n \n         self.allow_domains: Set[str] = set(arg_to_iter(allow_domains))\n         self.deny_domains: Set[str] = set(arg_to_iter(deny_domains))\n@@ -194,10 +205,10 @@ class LxmlLinkExtractor:\n             deny_extensions = IGNORED_EXTENSIONS\n         self.canonicalize: bool = canonicalize\n         self.deny_extensions: Set[str] = {\".\" + e for e in arg_to_iter(deny_extensions)}\n-        self.restrict_text: List[re.Pattern[str]] = self._compile_regexes(restrict_text)\n+        self.restrict_text: List[Pattern[str]] = self._compile_regexes(restrict_text)\n \n     @staticmethod\n-    def _compile_regexes(value: Optional[_RegexOrSeveralT]) -> List[re.Pattern[str]]:\n+    def _compile_regexes(value: Optional[_RegexOrSeveralT]) -> List[Pattern[str]]:\n         return [\n             x if isinstance(x, re.Pattern) else re.compile(x)\n             for x in arg_to_iter(value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
