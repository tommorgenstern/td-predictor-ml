{"custom_id": "scrapy#cb49567ca671647882408763f309cb3f37b9734e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 43 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -18,7 +18,6 @@ from scrapy.xlib import twisted_250_monkeypatches\n from scrapy.utils.encoding import add_encoding_alias\n add_encoding_alias('gb2312', 'zh-cn')\n add_encoding_alias('cp1251', 'win-1251')\n-add_encoding_alias('cp1252', 'iso8859-1', overwrite=True)\n \n # optional_features is a set containing Scrapy optional features\n optional_features = set()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#173e94386b6afe715cefeb9724da0f1e8c152770", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 16 | Files Changed: 8 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 65 | Churn Cumulative: 769 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,7 +26,7 @@ class HtmlParserLinkExtractor(HTMLParser):\n         links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links\n \n         ret = []\n-        base_url = self.base_url if self.base_url else response_url\n+        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url\n         for link in links:\n             link.url = urljoin_rfc(base_url, link.url, response_encoding)\n             link.url = safe_url_string(link.url, response_encoding)\n\n@@ -51,8 +51,7 @@ class HTMLImageLinkExtractor(object):\n     def extract_links(self, response):\n         xs = HtmlXPathSelector(response)\n         base_url = xs.select('//base/@href').extract()\n-        base_url = unicode_to_str(base_url[0], response.encoding) if base_url \\\n-            else unicode_to_str(response.url, response.encoding)\n+        base_url = urljoin_rfc(response.url, base_url[0]) if base_url else response.url\n \n         links = []\n         for location in self.locations:\n\n@@ -29,7 +29,7 @@ class LxmlLinkExtractor(object):\n         links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links\n \n         ret = []\n-        base_url = self.base_url if self.base_url else response_url\n+        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url\n         for link in links:\n             link.url = urljoin_rfc(base_url, link.url, response_encoding)\n             link.url = safe_url_string(link.url, response_encoding)\n\n@@ -16,8 +16,9 @@ def clean_link(link_text):\n \n class RegexLinkExtractor(SgmlLinkExtractor):\n     \"\"\"High performant link extractor\"\"\"\n+\n     def _extract_links(self, response_text, response_url, response_encoding):\n-        base_url = self.base_url if self.base_url else response_url\n+        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url\n \n         clean_url = lambda u: urljoin_rfc(base_url, remove_entities(clean_link(u.decode(response_encoding))))\n         clean_text = lambda t: replace_escape_chars(remove_tags(t.decode(response_encoding))).strip()\n\n@@ -28,7 +28,7 @@ class BaseSgmlLinkExtractor(FixedSGMLParser):\n         links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links\n \n         ret = []\n-        base_url = self.base_url if self.base_url else response_url\n+        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url\n         for link in links:\n             link.url = urljoin_rfc(base_url, link.url, response_encoding)\n             link.url = safe_url_string(link.url, response_encoding)\n\n@@ -30,7 +30,7 @@ class BaseSgmlRequestExtractor(FixedSGMLParser):\n         self.feed(response_text)\n         self.close()\n \n-        base_url = self.base_url if self.base_url else response_url\n+        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url\n         self._make_absolute_urls(base_url, response_encoding)\n         self._fix_link_text_encoding(response_encoding)\n \n\n@@ -50,20 +50,39 @@ class RequestExtractorTest(AbstractRequestExtractorTest):\n             )\n \n     def test_base_url(self):\n+        reqx = BaseSgmlRequestExtractor()\n+\n         html = \"\"\"<html><head><title>Page title<title>\n         <base href=\"http://otherdomain.com/base/\" />\n         <body><p><a href=\"item/12.html\">Item 12</a></p>\n         </body></html>\"\"\"\n-        response = HtmlResponse(\"http://example.org/somepage/index.html\",\n-                    body=html)\n-        reqx = BaseSgmlRequestExtractor()\n+        response = HtmlResponse(\"https://example.org/p/index.html\", body=html)\n+        reqs = reqx.extract_requests(response)\n+        self.failUnless(self._requests_equals( \\\n+            [Request('http://otherdomain.com/base/item/12.html', \\\n+                    meta={'link_text': 'Item 12'})], reqs), reqs)\n \n-        self.failUnless(\n-            self._requests_equals(reqx.extract_requests(response),\n-                    [ Request('http://otherdomain.com/base/item/12.html',\n-                             meta={'link_text': 'Item 12'}) ]\n-                    )\n-            )\n+        # base url is an absolute path and relative to host\n+        html = \"\"\"<html><head><title>Page title<title>\n+        <base href=\"/\" />\n+        <body><p><a href=\"item/12.html\">Item 12</a></p>\n+        </body></html>\"\"\"\n+        response = HtmlResponse(\"https://example.org/p/index.html\", body=html)\n+        reqs = reqx.extract_requests(response)\n+        self.failUnless(self._requests_equals( \\\n+            [Request('https://example.org/item/12.html', \\\n+                    meta={'link_text': 'Item 12'})], reqs), reqs)\n+\n+        # base url has no scheme\n+        html = \"\"\"<html><head><title>Page title<title>\n+        <base href=\"//noscheme.com/base/\" />\n+        <body><p><a href=\"item/12.html\">Item 12</a></p>\n+        </body></html>\"\"\"\n+        response = HtmlResponse(\"https://example.org/p/index.html\", body=html)\n+        reqs = reqx.extract_requests(response)\n+        self.failUnless(self._requests_equals( \\\n+            [Request('https://noscheme.com/base/item/12.html', \\\n+                    meta={'link_text': 'Item 12'})], reqs), reqs)\n \n     def test_extraction_encoding(self):\n         #TODO: use own fixtures\n\n@@ -35,6 +35,20 @@ class LinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.extract_links(response),\n                          [Link(url='http://otherdomain.com/base/item/12.html', text='Item 12')])\n \n+        # base url is an absolute path and relative to host\n+        html = \"\"\"<html><head><title>Page title<title><base href=\"/\" />\n+        <body><p><a href=\"item/12.html\">Item 12</a></p></body></html>\"\"\"\n+        response = HtmlResponse(\"https://example.org/somepage/index.html\", body=html)\n+        self.assertEqual(lx.extract_links(response),\n+                         [Link(url='https://example.org/item/12.html', text='Item 12')])\n+\n+        # base url has no scheme\n+        html = \"\"\"<html><head><title>Page title<title><base href=\"//noschemedomain.com/path/to/\" />\n+        <body><p><a href=\"item/12.html\">Item 12</a></p></body></html>\"\"\"\n+        response = HtmlResponse(\"https://example.org/somepage/index.html\", body=html)\n+        self.assertEqual(lx.extract_links(response),\n+                         [Link(url='https://noschemedomain.com/path/to/item/12.html', text='Item 12')])\n+\n     def test_extraction_encoding(self):\n         body = get_testdata('link_extractor', 'linkextractor_noenc.html')\n         response_utf8 = HtmlResponse(url='http://example.com/utf8', body=body, headers={'Content-Type': ['text/html; charset=utf-8']})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1330697c3dc9cc64189b0da9506767bbbe6d70f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 128 | Lines Deleted: 105 | Files Changed: 13 | Hunks: 47 | Methods Changed: 30 | Complexity Δ (Sum/Max): 2/6 | Churn Δ: 233 | Churn Cumulative: 1301 | Contributors (this commit): 3 | Commits (past 90d): 27 | Contributors (cumulative): 26 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,11 +14,6 @@ if sys.version_info < (2,5):\n # monkey patches to fix external library issues\n from scrapy.xlib import twisted_250_monkeypatches\n \n-# add some common encoding aliases not included by default in Python\n-from scrapy.utils.encoding import add_encoding_alias\n-add_encoding_alias('gb2312', 'zh-cn')\n-add_encoding_alias('cp1251', 'win-1251')\n-\n # optional_features is a set containing Scrapy optional features\n optional_features = set()\n \n\n@@ -71,6 +71,23 @@ DOWNLOADER_STATS = True\n \n DUPEFILTER_CLASS = 'scrapy.contrib.dupefilter.RequestFingerprintDupeFilter'\n \n+ENCODING_ALIASES = {}\n+\n+ENCODING_ALIASES_BASE = {\n+    'zh-cn': 'gb2312',\n+    'win-1251': 'cp1251',\n+    'macintosh' : 'mac-roman',\n+    'x-sjis': 'shift-jis',\n+    'iso-8859-1': 'cp1252',\n+    'iso8859-1': 'cp1252',\n+    '8859': 'cp1252',\n+    'cp819': 'cp1252',\n+    'latin': 'cp1252',\n+    'latin1': 'cp1252',\n+    'latin_1': 'cp1252',\n+    'l1': 'cp1252',\n+}\n+\n EXTENSIONS = {}\n \n EXTENSIONS_BASE = {\n\n@@ -1,4 +1,5 @@\n from scrapy import log\n+from scrapy.http import HtmlResponse\n from scrapy.utils.url import urljoin_rfc\n from scrapy.utils.response import get_meta_refresh\n from scrapy.core.exceptions import IgnoreRequest\n@@ -24,6 +25,7 @@ class RedirectMiddleware(object):\n             redirected = request.replace(url=redirected_url)\n             return self._redirect(redirected, request, spider, response.status)\n \n+        if isinstance(response, HtmlResponse):\n             interval, url = get_meta_refresh(response)\n             if url and interval < self.max_metarefresh_delay:\n                 redirected = self._redirect_request_using_get(request, url)\n\n@@ -23,9 +23,6 @@ class HtmlResponse(TextResponse):\n     METATAG_RE  = re.compile(r'<meta\\s+%s\\s+%s' % (_httpequiv_re, _content_re), re.I)\n     METATAG_RE2 = re.compile(r'<meta\\s+%s\\s+%s' % (_content_re, _httpequiv_re), re.I)\n \n-    def body_encoding(self):\n-        return self._body_declared_encoding() or super(HtmlResponse, self).body_encoding()\n-\n     @memoizemethod_noargs\n     def _body_declared_encoding(self):\n         chunk = self.body[:5000]\n\n@@ -5,13 +5,13 @@ discovering (through HTTP headers) to base Response class.\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n-import codecs\n import re\n \n from scrapy.xlib.BeautifulSoup import UnicodeDammit\n \n from scrapy.http.response import Response\n from scrapy.utils.python import memoizemethod_noargs\n+from scrapy.utils.encoding import encoding_exists, resolve_encoding\n from scrapy.conf import settings\n \n class TextResponse(Response):\n@@ -19,12 +19,12 @@ class TextResponse(Response):\n     _DEFAULT_ENCODING = settings['DEFAULT_RESPONSE_ENCODING']\n     _ENCODING_RE = re.compile(r'charset=([\\w-]+)', re.I)\n \n-    __slots__ = ['_encoding', '_body_inferred_encoding']\n+    __slots__ = ['_encoding', '_cached_benc']\n \n     def __init__(self, url, status=200, headers=None, body=None, meta=None, \\\n             flags=None, encoding=None):\n         self._encoding = encoding\n-        self._body_inferred_encoding = None\n+        self._cached_benc = None\n         super(TextResponse, self).__init__(url, status, headers, body, meta, flags)\n \n     def _get_url(self):\n@@ -57,36 +57,39 @@ class TextResponse(Response):\n \n     @property\n     def encoding(self):\n-        return self._encoding or self.headers_encoding() or self.body_encoding()\n+        enc = self._declared_encoding()\n+        if not (enc and encoding_exists(enc)):\n+            enc = self._body_inferred_encoding() or self._DEFAULT_ENCODING\n+        return resolve_encoding(enc)\n \n-    @memoizemethod_noargs\n-    def headers_encoding(self):\n-        content_type = self.headers.get('Content-Type')\n-        if content_type:\n-            encoding = self._ENCODING_RE.search(content_type)\n-            if encoding:\n-                enc = encoding.group(1)\n-                try:\n-                    codecs.lookup(enc) # check if the encoding is valid\n-                    return enc\n-                except LookupError:\n-                    pass\n+    def _declared_encoding(self):\n+        return self._encoding or self._headers_encoding() \\\n+            or self._body_declared_encoding()\n \n     @memoizemethod_noargs\n     def body_as_unicode(self):\n         \"\"\"Return body as unicode\"\"\"\n-        possible_encodings = (self._encoding, self.headers_encoding(), \\\n-            self._body_declared_encoding())\n-        dammit = UnicodeDammit(self.body, possible_encodings)\n-        self._body_inferred_encoding = dammit.originalEncoding\n-        if self._body_inferred_encoding in ('ascii', None):\n-            self._body_inferred_encoding = self._DEFAULT_ENCODING\n-        return dammit.unicode\n+        denc = self._declared_encoding()\n+        dencs = [resolve_encoding(denc)] if denc else []\n+        dammit = UnicodeDammit(self.body, dencs)\n+        benc = dammit.originalEncoding\n+        self._cached_benc = benc if benc != 'ascii' else None\n+        return self.body.decode(benc) if benc == 'utf-16' else dammit.unicode\n \n-    def body_encoding(self):\n-        if self._body_inferred_encoding is None:\n+    @memoizemethod_noargs\n+    def _headers_encoding(self):\n+        content_type = self.headers.get('Content-Type')\n+        if content_type:\n+            m = self._ENCODING_RE.search(content_type)\n+            if m:\n+                encoding = m.group(1)\n+                if encoding_exists(encoding):\n+                    return encoding\n+\n+    def _body_inferred_encoding(self):\n+        if self._cached_benc is None:\n             self.body_as_unicode()\n-        return self._body_inferred_encoding\n+        return self._cached_benc\n \n     def _body_declared_encoding(self):\n         # implemented in subclasses (XmlResponse, HtmlResponse)\n\n@@ -18,9 +18,6 @@ class XmlResponse(TextResponse):\n     _encoding_re  = _template % ('encoding', r'(?P<charset>[\\w-]+)')\n     XMLDECL_RE  = re.compile(r'<\\?xml\\s.*?%s' % _encoding_re, re.I)\n \n-    def body_encoding(self):\n-        return self._body_declared_encoding() or super(XmlResponse, self).body_encoding()\n-\n     @memoizemethod_noargs\n     def _body_declared_encoding(self):\n         chunk = self.body[:5000]\n\n@@ -3,7 +3,7 @@ import unittest\n from scrapy.contrib.downloadermiddleware.redirect import RedirectMiddleware\n from scrapy.spider import BaseSpider\n from scrapy.core.exceptions import IgnoreRequest\n-from scrapy.http import Request, Response, Headers\n+from scrapy.http import Request, Response, HtmlResponse, Headers\n \n class RedirectMiddlewareTest(unittest.TestCase):\n \n@@ -58,7 +58,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n             <head><meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n             </html>\"\"\"\n         req = Request(url='http://example.org')\n-        rsp = Response(url='http://example.org', body=body)\n+        rsp = HtmlResponse(url='http://example.org', body=body)\n         req2 = self.mw.process_response(req, rsp, self.spider)\n \n         assert isinstance(req2, Request)\n@@ -70,7 +70,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n             <head><meta http-equiv=\"refresh\" content=\"1000;url=http://example.org/newpage\" /></head>\n             </html>\"\"\"\n         req = Request(url='http://example.org')\n-        rsp = Response(url='http://example.org', body=body)\n+        rsp = HtmlResponse(url='http://example.org', body=body)\n         rsp2 = self.mw.process_response(req, rsp, self.spider)\n \n         assert rsp is rsp2\n@@ -81,7 +81,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n             </html>\"\"\"\n         req = Request(url='http://example.org', method='POST', body='test',\n             headers={'Content-Type': 'text/plain', 'Content-length': '4'})\n-        rsp = Response(url='http://example.org', body=body)\n+        rsp = HtmlResponse(url='http://example.org', body=body)\n         req2 = self.mw.process_response(req, rsp, self.spider)\n \n         assert isinstance(req2, Request)\n\n@@ -1,21 +0,0 @@\n-import unittest\n-\n-import scrapy # adds encoding aliases (if not added before)\n-\n-class EncodingAliasesTestCase(unittest.TestCase):\n-\n-    def test_encoding_aliases(self):\n-        \"\"\"Test common encdoing aliases not included in Python\"\"\"\n-\n-        uni = u'\\u041c\\u041e\\u0421K\\u0412\\u0410'\n-        str = uni.encode('windows-1251')\n-        self.assertEqual(uni.encode('windows-1251'), uni.encode('win-1251'))\n-        self.assertEqual(str.decode('windows-1251'), str.decode('win-1251'))\n-\n-        text = u'\\u8f6f\\u4ef6\\u540d\\u79f0'\n-        str = uni.encode('gb2312')\n-        self.assertEqual(uni.encode('gb2312'), uni.encode('zh-cn'))\n-        self.assertEqual(str.decode('gb2312'), str.decode('zh-cn'))\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -2,6 +2,7 @@ import unittest\n import weakref\n \n from scrapy.http import Response, TextResponse, HtmlResponse, XmlResponse, Headers\n+from scrapy.utils.encoding import resolve_encoding\n from scrapy.conf import settings\n \n \n@@ -112,10 +113,13 @@ class BaseResponseTest(unittest.TestCase):\n             body_str = body\n \n         assert isinstance(response.body, str)\n-        self.assertEqual(response.encoding, encoding)\n+        self._assert_response_encoding(response, encoding)\n         self.assertEqual(response.body, body_str)\n         self.assertEqual(response.body_as_unicode(), body_unicode)\n \n+    def _assert_response_encoding(self, response, encoding):\n+        self.assertEqual(response.encoding, resolve_encoding(encoding))\n+\n class ResponseText(BaseResponseTest):\n \n     def test_no_unicode_url(self):\n@@ -134,14 +138,14 @@ class TextResponseTest(BaseResponseTest):\n \n         assert isinstance(r2, self.response_class)\n         self.assertEqual(r2.url, \"http://www.example.com/other\")\n-        self.assertEqual(r2.encoding, \"cp852\")\n+        self._assert_response_encoding(r2, \"cp852\")\n         self.assertEqual(r3.url, \"http://www.example.com/other\")\n-        self.assertEqual(r3.encoding, \"latin1\")\n+        self.assertEqual(r3._declared_encoding(), \"latin1\")\n \n     def test_unicode_url(self):\n         # instantiate with unicode url without encoding (should set default encoding)\n         resp = self.response_class(u\"http://www.example.com/\")\n-        self.assertEqual(resp.encoding, settings['DEFAULT_RESPONSE_ENCODING'])\n+        self._assert_response_encoding(resp, settings['DEFAULT_RESPONSE_ENCODING'])\n \n         # make sure urls are converted to str\n         resp = self.response_class(url=u\"http://www.example.com/\", encoding='utf-8')\n@@ -175,18 +179,18 @@ class TextResponseTest(BaseResponseTest):\n         r2 = self.response_class(\"http://www.example.com\", encoding='utf-8', body=u\"\\xa3\")\n         r3 = self.response_class(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]}, body=\"\\xa3\")\n         r4 = self.response_class(\"http://www.example.com\", body=\"\\xa2\\xa3\")\n-        r5 = self.response_class(\"http://www.example.com\",\n-        headers={\"Content-type\": [\"text/html; charset=None\"]}, body=\"\\xc2\\xa3\")\n+        r5 = self.response_class(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=None\"]}, body=\"\\xc2\\xa3\")\n \n-        self.assertEqual(r1.headers_encoding(), \"utf-8\")\n-        self.assertEqual(r2.headers_encoding(), None)\n-        self.assertEqual(r2.encoding, 'utf-8')\n-        self.assertEqual(r3.headers_encoding(), \"iso-8859-1\")\n-        self.assertEqual(r3.encoding, 'iso-8859-1')\n-        self.assertEqual(r4.headers_encoding(), None)\n-        self.assertEqual(r5.headers_encoding(), None)\n-        self.assertEqual(r5.encoding, \"utf-8\")\n-        assert r4.body_encoding() is not None and r4.body_encoding() != 'ascii'\n+        self.assertEqual(r1._headers_encoding(), \"utf-8\")\n+        self.assertEqual(r2._headers_encoding(), None)\n+        self.assertEqual(r2._declared_encoding(), 'utf-8')\n+        self._assert_response_encoding(r2, 'utf-8')\n+        self.assertEqual(r3._headers_encoding(), \"iso-8859-1\")\n+        self.assertEqual(r3._declared_encoding(), \"iso-8859-1\")\n+        self.assertEqual(r4._headers_encoding(), None)\n+        self.assertEqual(r5._headers_encoding(), None)\n+        self._assert_response_encoding(r5, \"utf-8\")\n+        assert r4._body_inferred_encoding() is not None and r4._body_inferred_encoding() != 'ascii'\n         self._assert_response_values(r1, 'utf-8', u\"\\xa3\")\n         self._assert_response_values(r2, 'utf-8', u\"\\xa3\")\n         self._assert_response_values(r3, 'iso-8859-1', u\"\\xa3\")\n\n@@ -0,0 +1,25 @@\n+import unittest\n+\n+from scrapy.utils.encoding import encoding_exists, resolve_encoding\n+\n+class UtilsEncodingTestCase(unittest.TestCase):\n+\n+    _ENCODING_ALIASES = {\n+        'foo': 'cp1252',\n+        'bar': 'none',\n+    }\n+\n+    def test_resolve_encoding(self):\n+        self.assertEqual(resolve_encoding('latin1', self._ENCODING_ALIASES),\n+                         'latin1')\n+        self.assertEqual(resolve_encoding('foo', self._ENCODING_ALIASES),\n+                         'cp1252')\n+\n+    def test_encoding_exists(self):\n+        assert encoding_exists('latin1', self._ENCODING_ALIASES)\n+        assert encoding_exists('foo', self._ENCODING_ALIASES)\n+        assert not encoding_exists('bar', self._ENCODING_ALIASES)\n+        assert not encoding_exists('none', self._ENCODING_ALIASES)\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n@@ -29,7 +29,7 @@ class ResponseUtilsTest(unittest.TestCase):\n         self.assertTrue(isinstance(body_or_str(u'text', unicode=True), unicode))\n \n     def test_get_base_url(self):\n-        response = Response(url='http://example.org', body=\"\"\"\\\n+        response = HtmlResponse(url='http://example.org', body=\"\"\"\\\n             <html>\\\n             <head><title>Dummy</title><base href='http://example.org/something' /></head>\\\n             <body>blahablsdfsal&amp;</body>\\\n@@ -42,17 +42,17 @@ class ResponseUtilsTest(unittest.TestCase):\n             <head><title>Dummy</title><meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n             <body>blahablsdfsal&amp;</body>\n             </html>\"\"\"\n-        response = Response(url='http://example.org', body=body)\n+        response = TextResponse(url='http://example.org', body=body)\n         self.assertEqual(get_meta_refresh(response), (5, 'http://example.org/newpage'))\n \n         # refresh without url should return (None, None)\n         body = \"\"\"<meta http-equiv=\"refresh\" content=\"5\" />\"\"\"\n-        response = Response(url='http://example.org', body=body)\n+        response = TextResponse(url='http://example.org', body=body)\n         self.assertEqual(get_meta_refresh(response), (None, None))\n \n         body = \"\"\"<meta http-equiv=\"refresh\" content=\"5;\n             url=http://example.org/newpage\" /></head>\"\"\"\n-        response = Response(url='http://example.org', body=body)\n+        response = TextResponse(url='http://example.org', body=body)\n         self.assertEqual(get_meta_refresh(response), (5, 'http://example.org/newpage'))\n \n         # meta refresh in multiple lines\n@@ -60,17 +60,17 @@ class ResponseUtilsTest(unittest.TestCase):\n                <META\n                HTTP-EQUIV=\"Refresh\"\n                CONTENT=\"1; URL=http://example.org/newpage\">\"\"\"\n-        response = Response(url='http://example.org', body=body)\n+        response = TextResponse(url='http://example.org', body=body)\n         self.assertEqual(get_meta_refresh(response), (1, 'http://example.org/newpage'))\n \n         # entities in the redirect url\n         body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=&#39;http://www.example.com/other&#39;\">\"\"\"\n-        response = Response(url='http://example.com', body=body)\n+        response = TextResponse(url='http://example.com', body=body)\n         self.assertEqual(get_meta_refresh(response), (3, 'http://www.example.com/other'))\n \n         # relative redirects\n         body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=other.html\">\"\"\"\n-        response = Response(url='http://example.com/page/this.html', body=body)\n+        response = TextResponse(url='http://example.com/page/this.html', body=body)\n         self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/page/other.html'))\n \n         # non-standard encodings (utf-16)\n@@ -81,7 +81,7 @@ class ResponseUtilsTest(unittest.TestCase):\n \n         # non-ascii chars in the url (default encoding - utf8)\n         body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=http://example.com/to\\xc2\\xa3\">\"\"\"\n-        response = Response(url='http://example.com', body=body)\n+        response = TextResponse(url='http://example.com', body=body)\n         self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/to%C2%A3'))\n \n         # non-ascii chars in the url (custom encoding - latin1)\n@@ -89,13 +89,8 @@ class ResponseUtilsTest(unittest.TestCase):\n         response = TextResponse(url='http://example.com', body=body, encoding='latin1')\n         self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/to%C2%A3'))\n \n-        # wrong encodings (possibly caused by truncated chunks)\n-        body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=http://example.com/this\\xc2_THAT\">\"\"\"\n-        response = Response(url='http://example.com', body=body)\n-        self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/thisTHAT'))\n-\n         # responses without refresh tag should return None None\n-        response = Response(url='http://example.org')\n+        response = TextResponse(url='http://example.org')\n         self.assertEqual(get_meta_refresh(response), (None, None))\n         response = TextResponse(url='http://example.org')\n         self.assertEqual(get_meta_refresh(response), (None, None))\n\n@@ -1,11 +1,20 @@\n import codecs\n \n-def add_encoding_alias(encoding, alias, overwrite=False):\n+from scrapy.conf import settings\n+\n+_ENCODING_ALIASES = dict(settings['ENCODING_ALIASES_BASE'])\n+_ENCODING_ALIASES.update(settings['ENCODING_ALIASES'])\n+\n+def encoding_exists(encoding, _aliases=_ENCODING_ALIASES):\n+    \"\"\"Returns ``True`` if encoding is valid, otherwise returns ``False``\"\"\"\n     try:\n-        codecs.lookup(alias)\n-        alias_exists = True\n+        codecs.lookup(resolve_encoding(encoding, _aliases))\n     except LookupError:\n-        alias_exists = False\n-    if overwrite or not alias_exists:\n-        codec = codecs.lookup(encoding)\n-        codecs.register(lambda x: codec if x == alias else None)\n+        return False\n+    return True\n+\n+def resolve_encoding(alias, _aliases=_ENCODING_ALIASES):\n+    \"\"\"Return the encoding the given alias maps to, or the alias as passed if\n+    no mapping is found.\n+    \"\"\"\n+    return _aliases.get(alias.lower(), alias)\n\n@@ -18,7 +18,8 @@ from scrapy.xlib.BeautifulSoup import BeautifulSoup\n from scrapy.http import Response, HtmlResponse\n \n def body_or_str(obj, unicode=True):\n-    assert isinstance(obj, (Response, basestring)), \"obj must be Response or basestring, not %s\" % type(obj).__name__\n+    assert isinstance(obj, (Response, basestring)), \\\n+        \"obj must be Response or basestring, not %s\" % type(obj).__name__\n     if isinstance(obj, Response):\n         return obj.body_as_unicode() if unicode else obj.body\n     elif isinstance(obj, str):\n@@ -26,16 +27,17 @@ def body_or_str(obj, unicode=True):\n     else:\n         return obj if unicode else obj.encode('utf-8')\n \n-BASEURL_RE = re.compile(r'<base\\s+href\\s*=\\s*[\\\"\\']\\s*([^\\\"\\'\\s]+)\\s*[\\\"\\']', re.I)\n+BASEURL_RE = re.compile(ur'<base\\s+href\\s*=\\s*[\\\"\\']\\s*([^\\\"\\'\\s]+)\\s*[\\\"\\']', re.I)\n _baseurl_cache = weakref.WeakKeyDictionary()\n def get_base_url(response):\n     \"\"\" Return the base url of the given response used to resolve relative links. \"\"\"\n     if response not in _baseurl_cache:\n-        match = BASEURL_RE.search(response.body[0:4096])\n+        match = BASEURL_RE.search(response.body_as_unicode()[0:4096])\n         _baseurl_cache[response] = match.group(1) if match else response.url\n     return _baseurl_cache[response]\n \n-META_REFRESH_RE = re.compile(ur'<meta[^>]*http-equiv[^>]*refresh[^>]*content\\s*=\\s*(?P<quote>[\"\\'])(?P<int>\\d+)\\s*;\\s*url=(?P<url>.*?)(?P=quote)', re.DOTALL | re.IGNORECASE)\n+META_REFRESH_RE = re.compile(ur'<meta[^>]*http-equiv[^>]*refresh[^>]*content\\s*=\\s*(?P<quote>[\"\\'])(?P<int>\\d+)\\s*;\\s*url=(?P<url>.*?)(?P=quote)', \\\n+    re.DOTALL | re.IGNORECASE)\n _metaref_cache = weakref.WeakKeyDictionary()\n def get_meta_refresh(response):\n     \"\"\"Parse the http-equiv parameter of the HTML meta element from the given\n@@ -46,9 +48,7 @@ def get_meta_refresh(response):\n     If no meta redirect is found, (None, None) is returned.\n     \"\"\"\n     if response not in _metaref_cache:\n-        encoding = getattr(response, 'encoding', None) or 'utf-8'\n-        body_chunk = remove_entities(unicode(response.body[0:4096], encoding, \\\n-            errors='ignore'))\n+        body_chunk = remove_entities(response.body_as_unicode()[0:4096])\n         match = META_REFRESH_RE.search(body_chunk)\n         if match:\n             interval = int(match.group('int'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#996a1b3574cde64823408a42ae27e5f27186ac26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 296 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,13 +29,29 @@ class ResponseUtilsTest(unittest.TestCase):\n         self.assertTrue(isinstance(body_or_str(u'text', unicode=True), unicode))\n \n     def test_get_base_url(self):\n-        response = HtmlResponse(url='http://example.org', body=\"\"\"\\\n+        response = HtmlResponse(url='https://example.org', body=\"\"\"\\\n             <html>\\\n             <head><title>Dummy</title><base href='http://example.org/something' /></head>\\\n             <body>blahablsdfsal&amp;</body>\\\n             </html>\"\"\")\n         self.assertEqual(get_base_url(response), 'http://example.org/something')\n \n+        # relative url with absolute path\n+        response = HtmlResponse(url='https://example.org', body=\"\"\"\\\n+            <html>\\\n+            <head><title>Dummy</title><base href='/absolutepath' /></head>\\\n+            <body>blahablsdfsal&amp;</body>\\\n+            </html>\"\"\")\n+        self.assertEqual(get_base_url(response), 'https://example.org/absolutepath')\n+\n+        # no scheme url\n+        response = HtmlResponse(url='https://example.org', body=\"\"\"\\\n+            <html>\\\n+            <head><title>Dummy</title><base href='//noscheme.com/path' /></head>\\\n+            <body>blahablsdfsal&amp;</body>\\\n+            </html>\"\"\")\n+        self.assertEqual(get_base_url(response), 'https://noscheme.com/path')\n+\n     def test_get_meta_refresh(self):\n         body = \"\"\"\n             <html>\n\n@@ -33,7 +33,7 @@ def get_base_url(response):\n     \"\"\" Return the base url of the given response used to resolve relative links. \"\"\"\n     if response not in _baseurl_cache:\n         match = BASEURL_RE.search(response.body_as_unicode()[0:4096])\n-        _baseurl_cache[response] = match.group(1) if match else response.url\n+        _baseurl_cache[response] = urljoin_rfc(response.url, match.group(1)) if match else response.url\n     return _baseurl_cache[response]\n \n META_REFRESH_RE = re.compile(ur'<meta[^>]*http-equiv[^>]*refresh[^>]*content\\s*=\\s*(?P<quote>[\"\\'])(?P<int>\\d+)\\s*;\\s*url=(?P<url>.*?)(?P=quote)', \\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#de896fa62decef1010db863390895ad372017035", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 26 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): -14/0 | Churn Δ: 37 | Churn Cumulative: 279 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -96,20 +96,12 @@ class Request(object_ref):\n         \"\"\"Return a copy of this Request\"\"\"\n         return self.replace()\n \n-    def replace(self, url=None, callback=None, method=None, headers=None, body=None, \\\n-                cookies=None, meta=None, encoding=None, priority=None, \\\n-                dont_filter=None, errback=None):\n+    def replace(self, *args, **kwargs):\n         \"\"\"Create a new Request with the same attributes except for those\n         given new values.\n         \"\"\"\n-        return self.__class__(url=self.url if url is None else url,\n-                              callback=callback,\n-                              method=self.method if method is None else method,\n-                              headers=copy.deepcopy(self.headers) if headers is None else headers,\n-                              body=self.body if body is None else body,\n-                              cookies=self.cookies if cookies is None else cookies,\n-                              meta=self.meta if meta is None else meta,\n-                              encoding=self.encoding if encoding is None else encoding,\n-                              priority=self.priority if priority is None else priority,\n-                              dont_filter=self.dont_filter if dont_filter is None else dont_filter,\n-                              errback=errback)\n+        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta', \\\n+                'encoding', 'priority', 'dont_filter']:\n+            kwargs.setdefault(x, getattr(self, x))\n+        cls = kwargs.pop('cls', self.__class__)\n+        return cls(*args, **kwargs)\n\n@@ -71,18 +71,11 @@ class Response(object_ref):\n         \"\"\"Return a copy of this Response\"\"\"\n         return self.replace()\n \n-    def replace(self, url=None, status=None, headers=None, body=None, meta=None, \\\n-            flags=None, cls=None, **kwargs):\n+    def replace(self, *args, **kwargs):\n         \"\"\"Create a new Response with the same attributes except for those\n         given new values.\n         \"\"\"\n-        if cls is None:\n-            cls = self.__class__\n-        new = cls(url=self.url if url is None else url,\n-                  status=self.status if status is None else status,\n-                  headers=copy.deepcopy(self.headers) if headers is None else headers,\n-                  body=self.body if body is None else body,\n-                  meta=self.meta if meta is None else meta,\n-                  flags=self.flags if flags is None else flags,\n-                  **kwargs)\n-        return new\n+        for x in ['url', 'status', 'headers', 'body', 'meta', 'flags']:\n+            kwargs.setdefault(x, getattr(self, x))\n+        cls = kwargs.pop('cls', self.__class__)\n+        return cls(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4dc886e3192095ccdbd6ec11c81dc676757308f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 285 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -215,9 +215,11 @@ class TextResponseTest(BaseResponseTest):\n \n     def test_replace_wrong_encoding(self):\n         \"\"\"Test invalid chars are replaced properly\"\"\"\n-        # XXX: Policy for replacing invalid chars may change without prior notice\n         r = self.response_class(\"http://www.example.com\", encoding='utf-8', body='PREFIX\\xe3\\xabSUFFIX')\n+        # XXX: Policy for replacing invalid chars may suffer minor variations\n+        # but it should always contain the unicode replacement char (u'\\ufffd')\n         assert u'\\ufffd' in r.body_as_unicode(), repr(r.body_as_unicode())\n+\n         # FIXME: This test should pass once we stop using BeautifulSoup's UnicodeDammit in TextResponse\n         #r = self.response_class(\"http://www.example.com\", body='PREFIX\\xe3\\xabSUFFIX')\n         #assert u'\\ufffd' in r.body_as_unicode(), repr(r.body_as_unicode())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#32f9c5fe68800376e730ad1cecd5ea4537d573c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 31 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 31 | Churn Cumulative: 80 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,31 +0,0 @@\n-\"\"\"\n-A Stats collector for persisting stats (pickled) to a MySQL db\n-\"\"\"\n-\n-import cPickle as pickle\n-from datetime import datetime\n-\n-from scrapy.stats.collector import StatsCollector\n-from scrapy.utils.mysql import mysql_connect\n-from scrapy.conf import settings\n-\n-class MysqlStatsCollector(StatsCollector):\n-\n-    def __init__(self):\n-        super(MysqlStatsCollector, self).__init__()\n-        mysqluri = settings['STATS_MYSQL_URI']\n-        self._mysql_conn = mysql_connect(mysqluri, use_unicode=False) if mysqluri else None\n-        \n-    def _persist_stats(self, stats, spider=None):\n-        if spider is None: # only store spider-specific stats\n-            return\n-        if self._mysql_conn is None:\n-            return\n-        stored = datetime.utcnow()\n-        datas = pickle.dumps(stats)\n-        table = 'domain_data_history'\n-\n-        c = self._mysql_conn.cursor()\n-        c.execute(\"INSERT INTO %s (domain,stored,data) VALUES (%%s,%%s,%%s)\" % table, \\\n-            (spider.domain_name, stored, datas))\n-        self._mysql_conn.commit()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8db67b17a372fee109a6b2872b35742bf6f70740", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 58 | Files Changed: 8 | Hunks: 17 | Methods Changed: 17 | Complexity Δ (Sum/Max): -5/2 | Churn Δ: 118 | Churn Cumulative: 1101 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n from scrapy.command import ScrapyCommand\n from scrapy.core.manager import scrapymanager\n from scrapy.conf import settings\n+from scrapy.utils.url import is_url\n \n \n class Command(ScrapyCommand):\n@@ -24,4 +25,12 @@ class Command(ScrapyCommand):\n             settings.overrides['CRAWLSPIDER_FOLLOW_LINKS'] = False\n \n     def run(self, args, opts):\n-        scrapymanager.runonce(*args)\n+        for arg in args:\n+            # schedule arg as url or domain\n+            if is_url(arg):\n+                scrapymanager.crawl_url(arg)\n+            else:\n+                scrapymanager.crawl_domain(arg)\n+\n+        # crawl just scheduled arguments without keeping idle\n+        scrapymanager.start()\n\n@@ -52,6 +52,10 @@ class Command(ScrapyCommand):\n             dispatcher.connect(exporter.export_item, signal=signals.item_passed)\n             exporter.start_exporting()\n         module = _import_file(args[0])\n-        scrapymanager.runonce(module.SPIDER)\n+\n+        # schedule spider and start engine\n+        scrapymanager.crawl_spider(module.SPIDER)\n+        scrapymanager.start()\n+\n         if opts.output:\n             exporter.finish_exporting()\n\n@@ -9,4 +9,4 @@ class Command(ScrapyCommand):\n         return \"Start the Scrapy manager but don't run any spider (idle mode)\"\n \n     def run(self, args, opts):\n-        scrapymanager.start(*args)\n+        scrapymanager.start(keep_alive=True)\n\n@@ -135,14 +135,14 @@ class Spiderctl(object):\n         if \"add_pending_domains\" in args:\n             for domain in args[\"add_pending_domains\"]:\n                 if domain not in scrapyengine.scheduler.pending_requests:\n-                    scrapymanager.crawl(domain)\n+                    scrapymanager.crawl_domain(domain)\n             s += \"<p>\"\n             s += \"Scheduled spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"add_pending_domains\"])\n             s += \"</p>\"\n         if \"rerun_finished_domains\" in args:\n             for domain in args[\"rerun_finished_domains\"]:\n                 if domain not in scrapyengine.scheduler.pending_requests:\n-                    scrapymanager.crawl(domain)\n+                    scrapymanager.crawl_domain(domain)\n                 self.finished.remove(domain)\n             s += \"<p>\"\n             s += \"Re-scheduled finished spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"rerun_finished_domains\"])\n\n@@ -1,5 +1,4 @@\n import signal\n-from collections import defaultdict\n \n from twisted.internet import reactor\n \n@@ -12,40 +11,6 @@ from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.url import is_url\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n \n-def _get_spider_requests(*args):\n-    \"\"\"Collect requests and spiders from the given arguments. Returns a dict of\n-    spider -> list of requests\n-    \"\"\"\n-    spider_requests = defaultdict(list)\n-    for arg in args:\n-        if isinstance(arg, tuple):\n-            request, spider = arg\n-            spider_requests[spider] = request\n-        elif isinstance(arg, Request):\n-            spider = spiders.fromurl(arg.url) or BaseSpider('default')\n-            if spider:\n-                spider_requests[spider] += [arg]\n-            else:\n-                log.msg('Could not find spider for request: %s' % arg, log.ERROR)\n-        elif isinstance(arg, BaseSpider):\n-            spider_requests[arg] += arg.start_requests()\n-        elif is_url(arg):\n-            spider = spiders.fromurl(arg) or BaseSpider('default')\n-            if spider:\n-                for req in arg_to_iter(spider.make_requests_from_url(arg)):\n-                    spider_requests[spider] += [req]\n-            else:\n-                log.msg('Could not find spider for url: %s' % arg, log.ERROR)\n-        elif isinstance(arg, basestring):\n-            spider = spiders.fromdomain(arg)\n-            if spider:\n-                spider_requests[spider] += spider.start_requests()\n-            else:\n-                log.msg('Could not find spider for domain: %s' % arg, log.ERROR)\n-        else:\n-            raise TypeError(\"Unsupported argument: %r\" % arg)\n-    return spider_requests\n-\n \n class ExecutionManager(object):\n     \"\"\"Process a list of sites or urls.\n@@ -78,24 +43,44 @@ class ExecutionManager(object):\n         scrapyengine.configure()\n         self.configured = True\n         \n-    def crawl(self, *args):\n-        \"\"\"Schedule the given args for crawling. args is a list of urls or domains\"\"\"\n+    def crawl_url(self, url, spider=None):\n+        \"\"\"Schedule given url for crawling.\"\"\"\n+        spider = spider or spiders.fromurl(url)\n+        if spider:\n+            requests = arg_to_iter(spider.make_requests_from_url(url))\n+            self._crawl_requests(requests, spider)\n+        else:\n+            log.msg('Could not find spider for url: %s' % url, log.ERROR)\n+\n+    def crawl_request(self, request, spider=None):\n+        \"\"\"Schedule request for crawling.\"\"\"\n         assert self.configured, \"Scrapy Manager not yet configured\"\n-        spider_requests = _get_spider_requests(*args)\n-        for spider, requests in spider_requests.iteritems():\n-            for request in requests:\n+        spider = spider or spiders.fromurl(request.url)\n+        if spider:\n             scrapyengine.crawl(request, spider)\n+        else:\n+            log.msg('Could not find spider for request: %s' % url, log.ERROR)\n \n-    def runonce(self, *args):\n-        \"\"\"Run the engine until it finishes scraping all domains and then exit\"\"\"\n-        self.crawl(*args)\n-        scrapyengine.start()\n-        if self.control_reactor:\n-            reactor.run(installSignalHandlers=False)\n+    def crawl_domain(self, domain):\n+        \"\"\"Schedule given domain for crawling.\"\"\"\n+        spider = spiders.fromdomain(domain)\n+        if spider:\n+            self.crawl_spider(spider)\n+        else:\n+            log.msg('Could not find spider for domain: %s' % domain, log.ERROR)\n \n-    def start(self):\n+    def crawl_spider(self, spider):\n+        \"\"\"Schedule spider for crawling.\"\"\"\n+        requests = spider.start_requests()\n+        self._crawl_requests(requests, spider)\n+\n+    def _crawl_requests(self, requests, spider):\n+        for req in requests:\n+            self.crawl_request(req, spider)\n+\n+    def start(self, keep_alive=False):\n         \"\"\"Start the scrapy server, without scheduling any domains\"\"\"\n-        scrapyengine.keep_alive = True\n+        scrapyengine.keep_alive = keep_alive\n         scrapyengine.start()\n         if self.control_reactor:\n             reactor.run(installSignalHandlers=False)\n\n@@ -104,7 +104,7 @@ class Shell(object):\n         signal.signal(signal.SIGINT, signal.SIG_IGN)\n \n         reactor.callInThread(self._console_thread, url)\n-        scrapymanager.start()\n+        scrapymanager.start(keep_alive=True)\n \n     def inspect_response(self, response):\n         print\n\n@@ -97,7 +97,8 @@ class CrawlingSession(object):\n             dispatcher.connect(self.response_downloaded, signals.response_downloaded)\n \n             scrapymanager.configure()\n-            scrapymanager.runonce(self.spider)\n+            scrapymanager.crawl_spider(self.spider)\n+            scrapymanager.start()\n             self.port.stopListening()\n             self.wasrun = True\n \n\n@@ -10,8 +10,11 @@ def fetch(urls):\n     commands or standalone scripts.\n     \"\"\"\n     responses = []\n-    requests = [Request(url, callback=responses.append, dont_filter=True) \\\n-        for url in urls]\n-    scrapymanager.runonce(*requests)\n+    for url in urls:\n+        req = Request(url, callback=responses.append, dont_filter=True)\n+        # @@@ request will require a suitable spider.\n+        #     If not will not be schedule\n+        scrapymanager.crawl_request(req)\n+    scrapymanager.start()\n     return responses\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dd477914db1c4713dd1e73f4f5a8e9fed524361c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 113 | Lines Deleted: 54 | Files Changed: 7 | Hunks: 32 | Methods Changed: 19 | Complexity Δ (Sum/Max): 11/9 | Churn Δ: 167 | Churn Cumulative: 1408 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,8 +1,12 @@\n+from scrapy import log\n from scrapy.command import ScrapyCommand\n from scrapy.core.manager import scrapymanager\n from scrapy.conf import settings\n+from scrapy.http import Request\n+from scrapy.spider import spiders\n from scrapy.utils.url import is_url\n \n+from collections import defaultdict\n \n class Command(ScrapyCommand):\n \n@@ -25,12 +29,49 @@ class Command(ScrapyCommand):\n             settings.overrides['CRAWLSPIDER_FOLLOW_LINKS'] = False\n \n     def run(self, args, opts):\n-        for arg in args:\n-            # schedule arg as url or domain\n-            if is_url(arg):\n-                scrapymanager.crawl_url(arg)\n+        if opts.spider:\n+            spider = spiders.create(opts.spider)\n         else:\n-                scrapymanager.crawl_domain(arg)\n+            spider = None\n+\n+        # aggregate urls and domains\n+        urls = []\n+        domains = []\n+        for arg in args:\n+            if is_url(arg):\n+                urls.append(arg)\n+            else:\n+                domains.append(arg)\n+\n+        # schedule first domains\n+        for dom in domains:\n+            scrapymanager.crawl_domain(dom)\n+\n+        # if forced spider schedule urls directly\n+        if spider:\n+            for url in urls:\n+                scrapymanager.crawl_url(url, spider)\n+        else:\n+            # group urls by spider\n+            spider_urls = defaultdict(list)\n+            find_by_url = lambda url: spiders.find_by_request(Request(url))\n+            for url in urls:\n+                spider_names = find_by_url(url)\n+                if not spider_names:\n+                    log.msg('Could not find spider for url: %s' % url,\n+                            log.ERROR)\n+                elif len(spider_names) > 1:\n+                    log.msg('More than one spider found for url: %s' % url,\n+                            log.ERROR)\n+                else:\n+                    spider_urls[spider_names[0]].append(url)\n+\n+            # schedule grouped urls with same spider\n+            for name, urls in spider_urls.iteritems():\n+                # instance spider for each url-list\n+                spider = spiders.create(name)\n+                for url in urls:\n+                    scrapymanager.crawl_url(url, spider)\n \n         # crawl just scheduled arguments without keeping idle\n         scrapymanager.start()\n\n@@ -59,8 +59,14 @@ class Command(ScrapyCommand):\n \n         module = sanitize_module_name(args[0])\n         domain = args[1]\n-        spider = spiders.fromdomain(domain)\n-        if spider and not opts.force:\n+\n+        # if spider already exists and not force option then halt\n+        try:\n+            spider = spiders.create(domain)\n+        except KeyError:\n+            pass\n+        else:\n+            if not opts.force:\n                 print \"Spider '%s' already exists in module:\" % domain\n                 print \"  %s\" % spider.__module__\n                 sys.exit(1)\n\n@@ -37,10 +37,17 @@ class Command(ScrapyCommand):\n         return item\n \n     def run_callback(self, spider, response, callback, args, opts):\n-        spider = spiders.fromurl(response.url)\n-        if not spider:\n-            log.msg('Cannot find spider for url: %s' % response.url, level=log.ERROR)\n+        spider_names = spiders.find_by_request(response.request)\n+        if not spider_names:\n+            log.msg('Cannot find spider for url: %s' % response.url,\n+                    level=log.ERROR)\n             return (), ()\n+        elif len(spider_names) > 1:\n+            log.msg('More than one spider found for url: %s' % response.url,\n+                    level=log.ERROR)\n+            return (), ()\n+        else:\n+            spider = spiders.create(spider_names[0])\n \n         if callback:\n             callback_fcn = callback if callable(callback) else getattr(spider, callback, None)\n\n@@ -99,10 +99,6 @@ class ScrapyCommand(object):\n         if opts.nolog:\n             settings.overrides['LOG_ENABLED'] = False\n \n-        if opts.spider:\n-            from scrapy.spider import spiders\n-            spiders.force_domain = opts.spider\n-\n         if opts.pidfile:\n             with open(opts.pidfile, \"w\") as f:\n                 f.write(str(os.getpid()))\n\n@@ -19,35 +19,34 @@ class TwistedPluginSpiderManager(object):\n \n     def __init__(self):\n         self.loaded = False\n-        self.force_domain = None\n-        self._invaliddict = {}\n         self._spiders = {}\n \n-    def fromdomain(self, domain):\n-        return self._spiders.get(domain)\n+    def create(self, spider_id):\n+        \"\"\"\n+        Returns Spider instance by given identifier.\n+        If not exists raises KeyError.\n+        \"\"\"\n+        #@@@ currently spider_id = domain\n+        # if lookup fails let dict's KeyError exception propagate\n+        return self._spiders[spider_id]\n \n-    def fromurl(self, url):\n-        if self.force_domain:\n-            return self._spiders.get(self.force_domain)\n-        domain = urlparse.urlparse(url).hostname\n-        domain = str(domain).replace('www.', '')\n-        if domain:\n-            if domain in self._spiders:         # try first locating by domain\n-                return self._spiders[domain]\n-            else:                               # else search spider by spider\n-                plist = self._spiders.values()\n-                for p in plist:\n-                    if url_is_from_spider(url, p):\n-                        return p\n+    def find_by_request(self, request):\n+        \"\"\"\n+        Returns list of spiders ids that match given Request.\n+        \"\"\"\n+        # just find by request.url\n+        return [domain for domain, spider in self._spiders.iteritems()\n+                if url_is_from_spider(request.url, spider)]\n \n     def list(self):\n+        \"\"\"Returns list of spiders available.\"\"\"\n         return self._spiders.keys()\n \n     def load(self, spider_modules=None):\n+        \"\"\"Load spiders from module directory.\"\"\"\n         if spider_modules is None:\n             spider_modules = settings.getlist('SPIDER_MODULES')\n         self.spider_modules = spider_modules\n-        self._invaliddict = {}\n         self._spiders = {}\n \n         modules = [__import__(m, {}, {}, ['']) for m in self.spider_modules]\n\n@@ -6,20 +6,13 @@ from scrapy.extension import extensions\n from scrapy import log\n from scrapy.http import Request\n from scrapy.core.engine import scrapyengine\n-from scrapy.spider import BaseSpider, spiders\n+from scrapy.spider import spiders\n from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.url import is_url\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n \n \n class ExecutionManager(object):\n-    \"\"\"Process a list of sites or urls.\n \n-    This class should be used in a main for process a list of sites/urls.\n-\n-    It extracts products and could be used to store results in a database or\n-    just for testing spiders.\n-    \"\"\"\n     def __init__(self):\n         self.interrupted = False\n         self.configured = False\n@@ -45,29 +38,30 @@ class ExecutionManager(object):\n         \n     def crawl_url(self, url, spider=None):\n         \"\"\"Schedule given url for crawling.\"\"\"\n-        spider = spider or spiders.fromurl(url)\n+        if spider is None:\n+            spider = self._create_spider_for_request(Request(url), log_none=True, \\\n+                log_multiple=True)\n         if spider:\n             requests = arg_to_iter(spider.make_requests_from_url(url))\n             self._crawl_requests(requests, spider)\n-        else:\n-            log.msg('Could not find spider for url: %s' % url, log.ERROR)\n \n     def crawl_request(self, request, spider=None):\n         \"\"\"Schedule request for crawling.\"\"\"\n         assert self.configured, \"Scrapy Manager not yet configured\"\n-        spider = spider or spiders.fromurl(request.url)\n+        if spider is None:\n+            spider = self._create_spider_for_request(request, log_none=True, \\\n+                log_multiple=True)\n         if spider:\n             scrapyengine.crawl(request, spider)\n-        else:\n-            log.msg('Could not find spider for request: %s' % url, log.ERROR)\n \n     def crawl_domain(self, domain):\n         \"\"\"Schedule given domain for crawling.\"\"\"\n-        spider = spiders.fromdomain(domain)\n-        if spider:\n-            self.crawl_spider(spider)\n-        else:\n+        try:\n+            spider = spiders.create(domain)\n+        except KeyError:\n             log.msg('Could not find spider for domain: %s' % domain, log.ERROR)\n+        else:\n+            self.crawl_spider(spider)\n \n     def crawl_spider(self, spider):\n         \"\"\"Schedule spider for crawling.\"\"\"\n@@ -75,6 +69,7 @@ class ExecutionManager(object):\n         self._crawl_requests(requests, spider)\n \n     def _crawl_requests(self, requests, spider):\n+        \"\"\"Shortcut to schedule a list of requests\"\"\"\n         for req in requests:\n             self.crawl_request(req, spider)\n \n@@ -90,6 +85,17 @@ class ExecutionManager(object):\n         self.interrupted = True\n         scrapyengine.stop()\n \n+    def _create_spider_for_request(self, request, default=None, log_none=False, \\\n+            log_multiple=False):\n+        spider_names = spiders.find_by_request(request)\n+        if len(spider_names) == 1:\n+            return spiders.create(spider_names[0])\n+        if len(spider_names) > 1 and log_multiple:\n+            log.msg('More than one spider found for: %s' % request, log.ERROR)\n+        if len(spider_names) == 0 and log_none:\n+            log.msg('Could not find spider for: %s' % request, log.ERROR)\n+        return default\n+\n     def _signal_shutdown(self, signum, _):\n         signame = signal_names[signum]\n         log.msg(\"Received %s, shutting down gracefully. Send again to force \" \\\n\n@@ -35,6 +35,7 @@ def parse_url(url):\n             u = urlparse.urlparse(url)\n     return url\n \n+\n class Shell(object):\n \n     requires_project = False\n@@ -52,18 +53,21 @@ class Shell(object):\n         else:\n             url = parse_url(request_or_url)\n             request = Request(url)\n-        spider = spiders.fromurl(url) or BaseSpider('default')\n+\n+        spider = scrapymanager._create_spider_for_request(request, \\\n+            BaseSpider('default'), log_multiple=True)\n+\n         print \"Fetching %s...\" % request\n         response = threads.blockingCallFromThread(reactor, scrapyengine.schedule, \\\n             request, spider)\n         if response:\n-            self.populate_vars(url, response, request)\n+            self.populate_vars(url, response, request, spider)\n             if print_help:\n                 self.print_help()\n             else:\n                 print \"Done - use shelp() to see available objects\"\n \n-    def populate_vars(self, url=None, response=None, request=None):\n+    def populate_vars(self, url=None, response=None, request=None, spider=None):\n         item = self.item_class()\n         self.vars['item'] = item\n         if url:\n@@ -73,7 +77,7 @@ class Shell(object):\n             self.vars['url'] = url\n             self.vars['response'] = response\n             self.vars['request'] = request\n-            self.vars['spider'] = spiders.fromurl(url)\n+            self.vars['spider'] = spider\n         if not self.nofetch:\n             self.vars['fetch'] = self.fetch\n         self.vars['view'] = open_in_browser\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
