{"custom_id": "scrapy#071aa71b68f4424495e9d281fa7b437bc0450cca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 127 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 6 | Complexity Δ (Sum/Max): 16/14 | Churn Δ: 127 | Churn Cumulative: 127 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,86 @@\n+from pydispatch import dispatcher\n+\n+from scrapy.core import signals\n+from scrapy.http import Request\n+from scrapy.core.exceptions import NotConfigured, IgnoreRequest\n+from scrapy.utils.request import request_fingerprint\n+from scrapy.utils.misc import load_object\n+from scrapy.conf import settings\n+from scrapy import log\n+\n+\n+class BaseFilter(dict):\n+    \"\"\"Base class defining the duplicates requests filtering api\"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super(BaseFilter, self).__init__(*args, **kwargs)\n+        dispatcher.connect(self.open, signals.domain_open)\n+        dispatcher.connect(self.close, signals.domain_closed)\n+\n+    def open(self, domain):\n+        \"\"\"Called when a domain starts\"\"\"\n+        raise NotImplementedError()\n+\n+    def close(self, domain):\n+        \"\"\"Called when a domain is closed\"\"\"\n+        raise NotImplementedError()\n+\n+    def add(self, domain, request):\n+        \"\"\"Called to check if a request was already seen, and adds it to seen set.\n+\n+        returns True if not seen before, or False otherwise.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def has(self, domain, request):\n+        \"\"\"Called to check if a request was seen but doesnt add request to seen set.\"\"\"\n+        raise NotImplementedError()\n+\n+\n+class SimplePerDomainFilter(BaseFilter):\n+    \"\"\"Filter out a request if already seen for same domain\"\"\"\n+\n+    def open(self, domain):\n+        \"\"\"Initialize the resources needed for filtering for this domain\"\"\"\n+        self[domain] = set()\n+\n+    def close(self, domain):\n+        \"\"\"Remove the resources reserved for filtering for this domain\"\"\"\n+        del self[domain]\n+\n+    def add(self, domain, request):\n+        \"\"\"Add the fingerprint of a request to the domain set if a equivalent fingerprint has not been added.\n+        This method will return true if the fingerprint was added and false otherwise.\n+        \"\"\"\n+        fp = request_fingerprint(request)\n+        if fp not in self[domain]:\n+            self[domain].add(fp)\n+            return True\n+        return False\n+\n+    def has(self, domain, request):\n+        fp = request_fingerprint(request)\n+        return fp in self[domain]\n+\n+\n+class NullFilter(dict):\n+    def open(self, domain):\n+        pass\n+\n+    def close(self, domain):\n+        pass\n+\n+    def add(self, domain, request):\n+        return True\n+\n+    def has(self, domain, request):\n+        return None\n+\n+\n+try:\n+    duplicatesfilter\n+except NameError:\n+    clspath = settings.get('DUPLICATESFILTER_FILTERCLASS')\n+    cls = load_object(clspath) if clspath else NullFilter\n+    duplicatesfilter = cls()\n+\n\n@@ -0,0 +1,41 @@\n+import unittest\n+\n+from scrapy.http import Request\n+from scrapy.core.filters import SimplePerDomainFilter, NullFilter\n+\n+\n+class SimplePerDomainFilterTest(unittest.TestCase):\n+\n+    def test_filter(self):\n+        domain = 'scrapytest.org'\n+        filter = SimplePerDomainFilter()\n+        filter.open(domain)\n+        assert domain in filter\n+\n+        r1 = Request('http://scrapytest.org/1')\n+        r2 = Request('http://scrapytest.org/2')\n+        r3 = Request('http://scrapytest.org/2')\n+\n+        assert not filter.has(domain, r1)\n+        assert filter.add(domain, r1)\n+        assert filter.has(domain, r1)\n+\n+        assert filter.add(domain, r2)\n+        assert not filter.add(domain, r3)\n+\n+        filter.close(domain)\n+        assert domain not in filter\n+\n+\n+class NullFilterTest(unittest.TestCase):\n+\n+    def test_filter(self):\n+        domain = 'scrapytest.org'\n+        filter = NullFilter()\n+        filter.open(domain)\n+\n+        r1 = Request('http://scrapytest.org/1')\n+        assert not filter.has(domain, r1)\n+        assert filter.add(domain, r1)\n+        assert not filter.has(domain, r1)\n+        filter.close(domain)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#549c30d8d53d3aae6a69fcded1970eeaa8e25799", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 154 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ class RedirectMiddleware(object):\n         status = exception.status\n         response = exception.response\n \n-        if status in set([302, 303]):\n+        if status in [302, 303]:\n             redirected_url = urljoin(request.url, response.headers['location'][0])\n             redirected = request.replace(url=redirected_url, method='GET', body=None)\n             return self._redirect(redirected, request, spider, status)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4bf196fc993755ca45ecb870b9f8d58a01a6b4b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 11 | Files Changed: 6 | Hunks: 12 | Methods Changed: 8 | Complexity Δ (Sum/Max): -3/1 | Churn Δ: 30 | Churn Cumulative: 1328 | Contributors (this commit): 3 | Commits (past 90d): 51 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -148,6 +148,11 @@ RETRY_HTTP_CODES = ['500', '503', '504', '400', '408', '200']\n ROBOTSTXT_OBEY = False\n \n SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n+\n+SCHEDULER_MIDDLEWARES = [\n+        'scrapy.contrib.schedulermiddleware.duplicatesfilter.DuplicatesFilterMiddleware',\n+        ]\n+\n SCHEDULER_ORDER = 'BFO'   # available orders: BFO (default), DFO\n \n SPIDER_MODULES = []\n\n@@ -42,13 +42,12 @@ class RedirectMiddleware(object):\n         interval, url = get_meta_refresh(response)\n         if url and int(interval) < META_REFRESH_MAXSEC:\n             redirected = request.replace(url=urljoin(request.url, url))\n-            return self._redirect(redirected, request, spider, 'meta refresh') or response\n+            return self._redirect(redirected, request, spider, 'meta refresh')\n \n         return response\n \n     def _redirect(self, redirected, request, spider, reason):\n         domain = spider.domain_name\n-        if duplicatesfilter.add(domain, redirected) or redirected.dont_filter:\n         log.msg(\"Redirecting (%s) to %s from %s\" % (reason, redirected, request), level=log.DEBUG, domain=domain)\n         return redirected\n \n\n@@ -11,15 +11,12 @@ from scrapy import log\n class DuplicatesFilterMiddleware(object):\n     \"\"\"Filter out already seen requests to avoid visiting pages more than once.\"\"\"\n \n-    def process_spider_input(self, response, spider):\n-        duplicatesfilter.add(spider.domain_name, response.request)\n-\n     def process_spider_output(self, response, result, spider):\n         domain = spider.domain_name\n         for req in result:\n             if isinstance(req, Request):\n-                added = duplicatesfilter.add(domain, req)\n-                if not (added or req.dont_filter):\n+                has = duplicatesfilter.has(domain, req)\n+                if has and not req.dont_filter:\n                     log.msg('Skipped (already processed): %s' % req, log.TRACE, domain=domain)\n                     continue\n             yield req\n\n@@ -14,7 +14,7 @@ from scrapy import log\n from scrapy.stats import stats\n from scrapy.conf import settings\n from scrapy.core import signals\n-from scrapy.core.scheduler import Scheduler\n+from scrapy.core.scheduler import Scheduler, SchedulerMiddlewareManager\n from scrapy.core.downloader import Downloader\n from scrapy.core.exceptions import IgnoreRequest, HttpException, DontCloseDomain\n from scrapy.http import Response, Request\n@@ -59,6 +59,7 @@ class ExecutionEngine(object):\n         Configure execution engine with the given scheduling policy and downloader.\n         \"\"\"\n         self.scheduler = scheduler or Scheduler()\n+        self.schedulermiddleware = SchedulerMiddlewareManager(self.scheduler)\n         self.downloader = downloader or Downloader(self)\n         self.spidermiddleware = SpiderMiddlewareManager()\n         self._scraping = {}\n@@ -299,7 +300,7 @@ class ExecutionEngine(object):\n             return self._add_starter(request, spider, domain_priority)\n         if self.debug_mode: \n             log.msg('Scheduling %s (now)' % request_info(request), log.DEBUG)\n-        schd = self.scheduler.enqueue_request(domain, request, priority)\n+        schd = self.schedulermiddleware.enqueue_request(domain, request, priority)\n         self.next_request(spider)\n         return schd\n \n\n@@ -1,2 +1,3 @@\n from scrapy.core.scheduler.schedulers import Scheduler\n from scrapy.core.scheduler.store import MemoryStore\n+from scrapy.core.scheduler.middleware import SchedulerMiddlewareManager\n\n@@ -14,6 +14,9 @@ class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n         self.spider = spiders.fromdomain('scrapytest.org')\n         duplicatesfilter.open('scrapytest.org')\n \n+    def tearDown(self):\n+        duplicatesfilter.close('scrapytest.org')\n+\n     def test_process_spider_output(self):\n         mw = DuplicatesFilterMiddleware()\n \n@@ -25,10 +28,12 @@ class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n         r2 = Request('http://scrapytest.org/2')\n         r3 = Request('http://scrapytest.org/2')\n \n-        mw.process_spider_input(response, self.spider)\n+        duplicatesfilter.add('scrapytest.org', r0)\n+        duplicatesfilter.add('scrapytest.org', r2)\n+\n         filtered = list(mw.process_spider_output(response, [r0, r1, r2, r3], self.spider))\n \n         assert r0 not in filtered\n         assert r1 in filtered\n-        assert r2 in filtered\n+        assert r2 not in filtered\n         assert r3 not in filtered\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8d5222f1db5ceef0ef33b6fe5a6490d929246665", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 105 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 3 | Methods Changed: 9 | Complexity Δ (Sum/Max): 20/14 | Churn Δ: 105 | Churn Cumulative: 105 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.7547169811320755\n\nDIFF:\n\n@@ -0,0 +1,18 @@\n+\"\"\"\n+DuplicatesFilterMiddleware: Filter out already visited urls\n+\"\"\"\n+\n+from scrapy.http import Request\n+from scrapy.conf import settings\n+from scrapy.core.exceptions import IgnoreRequest\n+from scrapy.core.filters import duplicatesfilter\n+from scrapy import log\n+\n+\n+class DuplicatesFilterMiddleware(object):\n+    \"\"\"Filter out already seen requests to avoid visiting pages more than once.\"\"\"\n+\n+    def enqueue_request(self, domain, request, priority):\n+        added = duplicatesfilter.add(domain, request)\n+        if not (added or request.dont_filter):\n+            raise IgnoreRequest('Skipped (already seen request')\n\n@@ -0,0 +1,56 @@\n+\"\"\"\n+This module implements the Scheduler Middleware manager.\n+\n+For more information see the Scheduler Middleware doc in:\n+docs/topics/scheduler-middleware.rst\n+\n+\"\"\"\n+from twisted.internet.defer import Deferred\n+\n+from scrapy import log\n+from scrapy.http import Request, Response\n+from scrapy.core.exceptions import NotConfigured\n+from scrapy.utils.misc import load_object\n+from scrapy.utils.defer import mustbe_deferred\n+from scrapy.conf import settings\n+\n+class SchedulerMiddlewareManager(object):\n+\n+    def __init__(self, scheduler):\n+        self.loaded = False\n+        self.scheduler = scheduler\n+        self.mw_enqueue_request = []\n+        self.load()\n+\n+    def _add_middleware(self, mw):\n+        if hasattr(mw, 'enqueue_request'):\n+            self.mw_enqueue_request.append(mw.enqueue_request)\n+\n+    def load(self):\n+        \"\"\"Load middleware defined in settings module\"\"\"\n+        mws = []\n+        for mwpath in settings.getlist('SCHEDULER_MIDDLEWARES') or ():\n+            cls = load_object(mwpath)\n+            if cls:\n+                try:\n+                    mw = cls()\n+                    self._add_middleware(mw)\n+                    mws.append(mw)\n+                except NotConfigured:\n+                    pass\n+        log.msg(\"Enabled scheduler middlewares: %s\" % \", \".join([type(m).__name__ for m in mws]))\n+        self.loaded = True\n+\n+    def enqueue_request(self, domain, request, priority):\n+        def _enqueue_request(request):\n+            for method in self.mw_enqueue_request:\n+                result = method(domain=domain, request=request, priority=priority)\n+                assert result is None or isinstance(result, (Response, Deferred)), \\\n+                        'Middleware %s.enqueue_request must return None, Response or Deferred, got %s' % \\\n+                        (method.im_self.__class__.__name__, result.__class__.__name__)\n+                if result:\n+                    return result\n+            return self.scheduler.enqueue_request(domain=domain, request=request, priority=priority)\n+\n+        deferred = mustbe_deferred(_enqueue_request, request)\n+        return deferred\n\n@@ -0,0 +1,31 @@\n+import unittest\n+\n+from scrapy.spider import spiders\n+from scrapy.http import Request, Response\n+from scrapy.core.exceptions import IgnoreRequest\n+from scrapy.contrib.schedulermiddleware.duplicatesfilter import DuplicatesFilterMiddleware\n+from scrapy.core.filters import duplicatesfilter\n+\n+\n+class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        duplicatesfilter.open('scrapytest.org')\n+\n+    def tearDown(self):\n+        duplicatesfilter.close('scrapytest.org')\n+\n+    def test_process_spider_output(self):\n+        domain = 'scrapytest.org'\n+\n+        mw = DuplicatesFilterMiddleware()\n+\n+        r1 = Request('http://scrapytest.org/1')\n+        r2 = Request('http://scrapytest.org/2')\n+        r3 = Request('http://scrapytest.org/2')\n+        r4 = Request('http://scrapytest.org/1')\n+\n+        assert not mw.enqueue_request(domain, r1, 1)\n+        assert not mw.enqueue_request(domain, r2, 1)\n+        self.assertRaises(IgnoreRequest, mw.enqueue_request, domain, r3, 1)\n+        self.assertRaises(IgnoreRequest, mw.enqueue_request, domain, r4, 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5a5aef66293f7029642a8d8db17b97e6661a6d8b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 28 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,8 @@\n+from scrapy.item import ScrapedItem\n from scrapy.contrib_exp.newitem.fields import Field\n \n \n-class Item(object):\n+class Item(ScrapedItem):\n     \"\"\"\n     This is the base class for all scraped items.\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#059bf613fab0f3e138caeb1273e132194504cb8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 172 | Lines Deleted: 0 | Files Changed: 7 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 172 | Churn Cumulative: 172 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n\n@@ -0,0 +1,20 @@\n+# Define here the models for your scraped items\n+\n+from scrapy.contrib_exp import newitem \n+\n+from scrapy.contrib_exp.newitem import extractors\n+from scrapy.contrib_exp import adaptors\n+\n+\n+class GoogledirItem(newitem.Item):\n+    name = newitem.StringField()\n+    url = newitem.StringField()\n+    description = newitem.StringField()\n+\n+\n+class GoogledirItemExtractor(extractors.ItemExtractor):\n+    item_class = GoogledirItem\n+\n+    name = extractors.ExtractorField([adaptors.extract, adaptors.strip])\n+    url = extractors.ExtractorField([adaptors.extract, adaptors.strip])\n+    description = extractors.ExtractorField([adaptors.extract, adaptors.strip])\n\n@@ -0,0 +1,5 @@\n+# Define yours item pipelines here\n+\n+class GoogledirPipeline(object):\n+    def process_item(self, domain, item):\n+        return item\n\n@@ -0,0 +1,106 @@\n+import googledir\n+\n+# ---------------------------------------------------------------------------\n+# - Scrapy settings for googledir                                    -\n+# ---------------------------------------------------------------------------\n+PROJECT_NAME = 'googledir'\n+\n+BOT_NAME = PROJECT_NAME\n+BOT_VERSION = '1.0'\n+\n+SPIDER_MODULES = ['googledir.spiders']\n+NEWSPIDER_MODULE = 'googledir.spiders'\n+TEMPLATES_DIR = '%s/templates' % googledir.__path__[0]\n+ENABLED_SPIDERS_FILE = '%s/conf/enabled_spiders.list' % googledir.__path__[0]\n+DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n+USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n+\n+# The amount of time (in secs) that the downloader should wait before\n+# downloading consecutive pages from the same spider. This can be used\n+# to throttle the crawling speed to avoid hitting servers too\n+# hard. Decimal numbers are supported. Example:\n+# DOWNLOAD_DELAY = 2.5\n+DOWNLOAD_TIMEOUT = 600\n+\n+# use this spider class as default when no spider was found for a given url\n+#DEFAULT_SPIDER = 'scrapy.contrib.spiders.generic.GenericSpider'\n+\n+# uncomment if you want to add your own custom scrapy commands\n+#COMMANDS_MODULE = 'googledir.commands'\n+#COMMANDS_SETTINGS_MODULE = 'googledir.conf.commands'\n+\n+#Global timeout between sucessive downloads (can be overrided by spider\n+#attribute download_timeout\n+#DOWNLOAD_TIMEOUT = 0\n+\n+MYSQL_CONNECTION_SETTINGS = {\"charset\": \"utf8\" }\n+MYSQL_CONNECTION_PING_PERIOD = 600\n+\n+SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n+SCHEDULER_ORDER = 'BFO'   # available orders: BFO (default), DFO\n+\n+#CACHE2_DIR = '/tmp/cache2'  # if set, enables HTTP cache\n+#CACHE2_IGNORE_MISSING = 0   # ignore requests not in cache\n+#CACHE2_SECTORIZE = 1         # sectorize domains to distribute storage among servers\n+\n+#STATS_ENABLED = 1   # enable stats\n+#STATS_CLEANUP = 0   # cleanup domain stats when a domain is closed (saves memory)\n+#STATS_DEBUG = 0     # log stats on domain closed\n+\n+EXTENSIONS = (\n+    'scrapy.management.web.WebConsole',\n+    'scrapy.management.telnet.TelnetConsole',\n+)\n+\n+DOWNLOADER_MIDDLEWARES = (\n+    # Engine side\n+    'scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware',\n+    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware',\n+    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware',\n+    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware',\n+    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n+    'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n+    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n+    'scrapy.contrib.downloadermiddleware.compression.CompressionMiddleware',\n+    'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n+    'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n+    # Downloader side\n+)\n+\n+SPIDER_MIDDLEWARES = (\n+    # Engine side\n+    'scrapy.contrib.spidermiddleware.limit.RequestLimitMiddleware',\n+    'scrapy.contrib.spidermiddleware.restrict.RestrictMiddleware',\n+    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware',\n+    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware',\n+    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware',\n+    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware',\n+    'scrapy.contrib.spidermiddleware.urlfilter.UrlFilterMiddleware',\n+    'scrapy.contrib.spidermiddleware.duplicatesfilter.DuplicatesFilterMiddleware',\n+    # Spider side\n+)\n+\n+ITEM_PIPELINES = (\n+    'googledir.pipelines.GoogledirPipeline',\n+)\n+\n+#DEPTH_LIMIT = 10  # limit the maximum link depth to follow\n+#DEPTH_STATS = 1    # enable depth stats\n+\n+# Limit URL length. See: http://www.boutell.com/newfaq/misc/urllength.html\n+URLLENGTH_LIMIT = 2083\n+\n+#WEBCONSOLE_ENABLED = 1\n+#WEBCONSOLE_PORT = 8060  # if not set uses a dynamic port\n+\n+#TELNETCONSOLE_ENABLED = 1\n+#TELNETCONSOLE_PORT = 2020  # if not set uses a dynamic port\n+\n+# global mail sending settings\n+#MAIL_HOST = 'localhost'\n+#MAIL_FROM = 'scrapybot@localhost'\n+\n+# scrapy webservice\n+WS_ENABLED = 0\n+\n+SPIDERPROFILER_ENABLED = 0\n\n\n@@ -0,0 +1,34 @@\n+# -*- coding: utf8 -*-\n+import re\n+\n+from scrapy.xpath import HtmlXPathSelector\n+from scrapy.contrib.spiders import CrawlSpider, rule\n+from scrapy.utils.misc import items_to_csv\n+\n+from googledir.items import GoogledirItem, GoogledirItemExtractor\n+\n+class GoogleDirectorySpider(CrawlSpider):\n+    domain_name = 'google.com'\n+    start_urls = ['http://www.google.com/dirhp']\n+\n+    rules = (\n+        rule('google.com/[A-Z][a-zA-Z_/]+$', 'parse_category', follow=True),\n+    )\n+    csv_file = open('scraped_items.csv', 'ab+')\n+    \n+    def parse_category(self, response):\n+        # The selector we're going to use in order to extract data from the page\n+        hxs = HtmlXPathSelector(response)\n+\n+        # The path to website links in directory page\n+        links = hxs.x('//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td/font')\n+\n+        for link in links:\n+            extractor = GoogledirItemExtractor()\n+            extractor.name =  link.x('a/text()')\n+            extractor.url = link.x('a/@href')\n+            extractor.description = link.x('font[2]/text()')\n+            item = extractor.item_instance\n+            yield item\n+\n+SPIDER = GoogleDirectorySpider()\n\n@@ -0,0 +1,7 @@\n+#!/usr/bin/env python\n+\n+import os\n+os.environ.setdefault('SCRAPYSETTINGS_MODULE', 'googledir.settings')\n+\n+from scrapy.command.cmdline import execute\n+execute()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0c2e019f66e45c6df63434fd43d78559503dc030", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 90 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n import unittest\n from cStringIO import StringIO\n \n-from scrapy.utils.misc import hash_values, items_to_csv\n+from scrapy.utils.misc import hash_values, items_to_csv, load_object\n from scrapy.core.exceptions import UsageError\n from scrapy.item import ScrapedItem\n \n@@ -56,5 +56,9 @@ class UtilsMiscTestCase(unittest.TestCase):\n         items_to_csv(file, [])\n         self.assertEqual(file.tell(), 0)\n \n+    def test_load_object(self):\n+        obj = load_object('scrapy.utils.misc.load_object')\n+        assert obj is load_object\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8d8a5c5caae9c9429c29143b99439d86d6dde74d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): 11/7 | Churn Δ: 77 | Churn Cumulative: 412 | Contributors (this commit): 4 | Commits (past 90d): 13 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,4 @@\n-import inspect\n+from scrapy.utils.python import get_func_args\n \n \n class ItemExtractor(object):\n@@ -19,14 +19,17 @@ class ItemExtractor(object):\n         return fe\n \n     def __setattr__(self, name, value):\n-        if not (name.startswith('_') or name == 'item_instance'):\n-            if name in self._field_extractors.keys():\n-                setattr(self.item_instance, name, \n-                        self._field_extractors[name](value, self._response))\n-            else:\n+        if (name.startswith('_') or name == 'item_instance'):\n+            return object.__setattr__(self, name, value)\n+\n+        try:\n+            fieldextractor = self._field_extractors[name]\n+        except KeyError:\n             raise AttributeError(name)\n-        else:\n-            object.__setattr__(self, name, value)\n+\n+        adaptor_args = {'response': self._response}\n+        final = fieldextractor(value, adaptor_args=adaptor_args)\n+        setattr(self.item_instance, name, final)\n \n     def __getattribute__(self, name):\n         if not (name.startswith('_') or name.startswith('item_')):\n@@ -44,25 +47,18 @@ class ExtractorField(object):\n         self._funcs = []\n \n         for func in funcs:\n-            if inspect.isfunction(func):\n-                func_args, _, _, _ = inspect.getargspec(func)\n-            elif hasattr(func, '__call__'):\n-                try:\n-                    func_args, _, _, _ = inspect.getargspec(func.__call__)\n-                except Exception:\n-                    func_args = []\n-\n+            func_args = get_func_args(func)\n             takes_args = True if 'adaptor_args' in func_args else False\n             self._funcs.append((func, takes_args))\n \n-    def __call__(self, value, kwargs=None):\n+    def __call__(self, value, adaptor_args=None):\n         values = [value]\n \n         for func, takes_args in self._funcs:\n             next_round = []\n \n             for val in values:\n-                val = func(val, kwargs) if takes_args else func(val)\n+                val = func(val, adaptor_args) if takes_args else func(val)\n \n                 if isinstance(val, tuple):\n                     next_round.extend(val)\n@@ -73,3 +69,34 @@ class ExtractorField(object):\n \n         return list(values)\n \n+\n+def treeadapt(*funcs, **adaptor_args):\n+    pipe_adaptor_args = adaptor_args\n+    _funcs = []\n+    for func in funcs:\n+        takes_args = 'adaptor_args' in get_func_args(func)\n+        _funcs.append((func, takes_args))\n+\n+    def _adaptor(value, adaptor_args=None):\n+        values = value if isinstance(value, (list, tuple)) else [value]\n+        pipe_adaptor_args.update(adaptor_args or {})\n+        pipe_kwargs = {'adaptor_args': pipe_adaptor_args}\n+\n+        for func, takes_args in _funcs:\n+            next = []\n+            kwargs = pipe_kwargs if takes_args else {}\n+\n+            for val in values:\n+                val = func(val, **kwargs)\n+\n+                if isinstance(val, (list, tuple)):\n+                    next.extend(val)\n+                elif val is not None:\n+                    next.append(val)\n+\n+            values = next\n+        return list(values)\n+\n+    return _adaptor\n+\n+\n\n@@ -6,8 +6,9 @@ higher than 2.5 which is the lowest version supported by Scrapy.\n \n \"\"\"\n import re\n-import fnmatch\n import os\n+import fnmatch\n+import inspect\n from shutil import copy2, copystat\n from sgmllib import SGMLParser\n \n@@ -234,3 +235,14 @@ def copytree(src, dst, symlinks=False, ignore=None):\n \n # ----- end of shutil.copytree function from Python 2.6 ---- #\n \n+\n+def get_func_args(func):\n+    \"\"\"Return the argument name list of a callable\"\"\"\n+    if inspect.isfunction(func):\n+        func_args, _, _, _ = inspect.getargspec(func)\n+    elif hasattr(func, '__call__'):\n+        try:\n+            func_args, _, _, _ = inspect.getargspec(func.__call__)\n+        except Exception:\n+            func_args = []\n+    return func_args\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d5a1d77b67068a943e62c13414710f256b57461", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 15 | Churn Cumulative: 730 | Contributors (this commit): 4 | Commits (past 90d): 16 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n import unittest\n from cStringIO import StringIO\n \n-from scrapy.utils.misc import hash_values, items_to_csv, load_object\n+from scrapy.utils.misc import hash_values, items_to_csv, load_object, to_list\n from scrapy.core.exceptions import UsageError\n from scrapy.item import ScrapedItem\n \n@@ -60,5 +60,10 @@ class UtilsMiscTestCase(unittest.TestCase):\n         obj = load_object('scrapy.utils.misc.load_object')\n         assert obj is load_object\n \n+    def test_to_list(self):\n+        self.assertEqual(to_list(None), [])\n+        self.assertEqual(to_list('lala'), ['lala'])\n+        self.assertEqual(to_list(('lala', 'poo')), ['lala', 'poo'])\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -15,6 +15,14 @@ from scrapy.utils.python import flatten, unicode_to_str\n from scrapy.utils.markup import remove_entities\n from scrapy.utils.defer import defer_succeed\n \n+def to_list(obj):\n+    if obj is None:\n+        return []\n+    elif hasattr(obj, '__iter__'):\n+        return list(obj)\n+    else:\n+        return [obj]\n+\n def dict_updatedefault(D, E, **F):\n     \"\"\"\n     updatedefault(D, E, **F) -> None.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c6326a0425588cd4180e68c3c1d2b6199d1e3b12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 187 | Lines Deleted: 33 | Files Changed: 4 | Hunks: 6 | Methods Changed: 16 | Complexity Δ (Sum/Max): 17/14 | Churn Δ: 220 | Churn Cumulative: 494 | Contributors (this commit): 4 | Commits (past 90d): 16 | Contributors (cumulative): 8 | DMM Complexity: 0.8076923076923077\n\nDIFF:\n@@ -6,7 +6,6 @@ from __future__ import with_statement\n import os\n import urlparse\n \n-from twisted.web.client import HTTPClientFactory\n from twisted.internet import defer, reactor\n from twisted.web import error as web_error\n \n@@ -24,6 +23,7 @@ from scrapy.conf import settings\n \n from scrapy.core.downloader.dnscache import DNSCache\n from scrapy.core.downloader.responsetypes import responsetypes\n+from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory as HTTPClientFactory\n \n default_timeout = settings.getint('DOWNLOAD_TIMEOUT')\n default_agent = settings.get('USER_AGENT')\n\n@@ -0,0 +1,73 @@\n+from urlparse import urlunparse\n+\n+from twisted.web.client import HTTPClientFactory\n+from twisted.internet import reactor\n+\n+from scrapy.http import Url\n+\n+def _parse(url, defaultPort=None):\n+    url = url.strip()\n+    try:\n+        parsed = url.parsedurl\n+    except AttributeError:\n+        parsed = Url(url).parsedurl\n+\n+    scheme = parsed[0]\n+    path = urlunparse(('','')+parsed[2:])\n+    if defaultPort is None:\n+        if scheme == 'https':\n+            defaultPort = 443\n+        else:\n+            defaultPort = 80\n+    host, port = parsed[1], defaultPort\n+    if ':' in host:\n+        host, port = host.split(':')\n+        port = int(port)\n+    if path == \"\":\n+        path = \"/\"\n+    return scheme, host, port, path\n+\n+\n+class ScrapyHTTPClientFactory(HTTPClientFactory):\n+    \"\"\"Scrapy implementation of the HTTPClientFactory overwriting the\n+    serUrl method to make use of our Url object that cache the parse \n+    result. Also we override gotHeaders that dies when parsing malformed \n+    cookies.\n+    \"\"\"\n+\n+    def setURL(self, url):\n+        self.url = url\n+        scheme, host, port, path = _parse(url)\n+        if scheme and host:\n+            self.scheme = scheme\n+            self.host = host\n+            self.port = port\n+        self.path = path\n+\n+    def gotHeaders(self, headers):\n+        \"\"\"\n+        HTTPClientFactory.gotHeaders dies when parsing malformed cookies,\n+        and the crawler is getting malformed cookies from this site.\n+\n+        Cookies format: \n+            http://www.ietf.org/rfc/rfc2109.txt\n+        \n+        I have choosen not to filter based on this, so we don't filter invalid\n+        values that could be managed correctly by twisted.\n+        \"\"\"\n+        self.response_headers = headers\n+        if 'set-cookie' in headers:\n+            goodcookies = []\n+            for cookie in headers['set-cookie']:\n+                cookparts = cookie.split(';')\n+                cook = cookparts[0].lstrip()\n+                t = cook.split('=', 1)\n+                if len(t) == 2: #Good cookie\n+                    goodcookies.append(cookie)\n+                    k, v = t                \n+                    self.cookies[k.lstrip()] = v.lstrip()\n+            if goodcookies:\n+                self.response_headers['set-cookie'] = goodcookies\n+            else:\n+                del self.response_headers['set-cookie']\n+\n\n@@ -12,42 +12,11 @@ from twisted.web.client import HTTPClientFactory\n #sys.setrecursionlimit(7400)\n \n def apply_patches():\n-    patch_HTTPClientFactory_gotHeaders()\n-\n     if twisted.__version__ < '8.0.0':\n         patch_HTTPPageGetter_handleResponse()\n \n \n-# XXX: Monkeypatch for twisted.web.client-HTTPClientFactory\n-# HTTPClientFactory.gotHeaders dies when parsing malformed cookies,\n-# and the crawler is getting malformed cookies from this site.\n-_old_gotHeaders = HTTPClientFactory.gotHeaders\n-# Cookies format: http://www.ietf.org/rfc/rfc2109.txt\n-# I have choosen not to filter based on this, so we don't filter invalid\n-# values that could be managed correctly by twisted.\n-#_COOKIES = re.compile(r'^[^=;]+=[^=;]*(;[^=;]+=[^=;])*$')\n-\n-def _is_good_cookie(cookie):\n-    \"\"\" Check if a given cookie would make gotHeaders raise an exception \"\"\"\n-    cookparts = cookie.split(';')\n-    cook = cookparts[0]\n-    return len(cook.split('=', 1)) == 2\n-\n-def _new_gotHeaders(self, headers):\n-    \"\"\" Remove cookies that would make twisted raise an exception \"\"\"\n-    if headers.has_key('set-cookie'):\n-        cookies = [cookie for cookie in headers['set-cookie']\n-                   if _is_good_cookie(cookie)]\n-        if cookies:\n-            headers['set-cookie'] = cookies\n-        else:\n-            del headers['set-cookie']\n-\n-    return _old_gotHeaders(self, headers)\n-\n-def patch_HTTPClientFactory_gotHeaders():\n-    setattr(HTTPClientFactory, 'gotHeaders', _new_gotHeaders)\n-\n+# bugfix not present in twisted 2.5 for handling empty response of HEAD requests\n def patch_HTTPPageGetter_handleResponse():\n     from twisted.web.client import PartialDownloadError, HTTPPageGetter\n     from twisted.python import failure\n\n@@ -0,0 +1,112 @@\n+\"\"\"\n+Tests borrowed from the twisted.web.client tests.\n+\"\"\"\n+from urlparse import urlparse\n+\n+from twisted.trial import unittest\n+from twisted.web import server, static\n+from twisted.internet import reactor, defer\n+\n+from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory\n+from scrapy.http import Url\n+\n+class ParseUrlTestCase(unittest.TestCase):\n+    \"\"\"Test URL parsing facility and defaults values.\"\"\"\n+\n+    def _parse(self, url):\n+        f = ScrapyHTTPClientFactory(Url(url))\n+        return (f.scheme, f.host, f.port, f.path)\n+\n+    def testParse(self):\n+        scheme, host, port, path = self._parse(\"http://127.0.0.1/?param=value\")\n+        self.assertEquals(path, \"/?param=value\")\n+        self.assertEquals(port, 80)\n+        scheme, host, port, path = self._parse(\"http://127.0.0.1/\")\n+        self.assertEquals(path, \"/\")\n+        self.assertEquals(port, 80)\n+        scheme, host, port, path = self._parse(\"https://127.0.0.1/\")\n+        self.assertEquals(path, \"/\")\n+        self.assertEquals(port, 443)\n+        scheme, host, port, path = self._parse(\"http://spam:12345/\")\n+        self.assertEquals(port, 12345)\n+        scheme, host, port, path = self._parse(\"http://foo \")\n+        self.assertEquals(host, \"foo\")\n+        self.assertEquals(path, \"/\")\n+        scheme, host, port, path = self._parse(\"http://egg:7890\")\n+        self.assertEquals(port, 7890)\n+        self.assertEquals(host, \"egg\")\n+        self.assertEquals(path, \"/\")\n+\n+    def test_externalUnicodeInterference(self):\n+        \"\"\"\n+        L{client._parse} should return C{str} for the scheme, host, and path\n+        elements of its return tuple, even when passed an URL which has\n+        previously been passed to L{urlparse} as a C{unicode} string.\n+        \"\"\"\n+        badInput = u'http://example.com/path'\n+        goodInput = badInput.encode('ascii')\n+        urlparse(badInput)\n+        scheme, host, port, path = self._parse(goodInput)\n+        self.assertTrue(isinstance(scheme, str))\n+        self.assertTrue(isinstance(host, str))\n+        self.assertTrue(isinstance(path, str))\n+\n+\n+class FakeTransport:\n+    disconnecting = False\n+\n+    def __init__(self):\n+        self.data = []\n+\n+    def write(self, stuff):\n+        self.data.append(stuff)\n+\n+\n+class CookieTestCase(unittest.TestCase):\n+\n+    def _listen(self, site):\n+        return reactor.listenTCP(0, site, interface=\"127.0.0.1\")\n+\n+    def setUp(self):\n+        root = static.Data('El toro!', 'text/plain')\n+        site = server.Site(root, timeout=None)\n+        self.port = self._listen(site)\n+        self.portno = self.port.getHost().port\n+\n+    def tearDown(self):\n+        return self.port.stopListening()\n+\n+    def testMalformedCookieHeaderParsing(self):\n+        d = defer.Deferred()\n+        factory = ScrapyHTTPClientFactory('http://foo.example.com/')\n+        proto = factory.buildProtocol('127.42.42.42')\n+        proto.transport = FakeTransport()\n+        proto.connectionMade()\n+        for line in [\n+            '200 Ok',\n+            'Squash: yes',\n+            'Hands: stolen',\n+            'Set-Cookie: CUSTOMER=WILE_E_COYOTE; path=/; expires=Wednesday, 09-Nov-99 23:12:40 GMT',\n+            'Set-Cookie: PART_NUMBER=ROCKET_LAUNCHER_0001; path=/',\n+            'Set-Cookie: SHIPPING=FEDEX; path=/foo',\n+            'Set-Cookie: COUNTRY=UY; path=/foo',\n+            'Set-Cookie: GOOD_CUSTOMER;',\n+            'Set-Cookie: NO_A_BOT;',\n+            '',\n+            'body',\n+            'more body',\n+            ]:\n+            proto.dataReceived(line + '\\r\\n')\n+        self.assertEquals(proto.transport.data,\n+                          ['GET / HTTP/1.0\\r\\n',\n+                           'Host: foo.example.com\\r\\n',\n+                           'User-Agent: Twisted PageGetter\\r\\n',\n+                           '\\r\\n'])\n+        self.assertEquals(factory.cookies,\n+                          {\n+            'CUSTOMER': 'WILE_E_COYOTE',\n+            'PART_NUMBER': 'ROCKET_LAUNCHER_0001',\n+            'SHIPPING': 'FEDEX',\n+            'COUNTRY': 'UY',\n+            })\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d1265a4199f4ff22c2dd1805a7ab30bef03fba7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 34 | Files Changed: 9 | Hunks: 25 | Methods Changed: 9 | Complexity Δ (Sum/Max): 15/14 | Churn Δ: 56 | Churn Cumulative: 903 | Contributors (this commit): 3 | Commits (past 90d): 54 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -76,7 +76,7 @@ DOWNLOADER_MIDDLEWARES = [\n \n DOWNLOADER_STATS = True\n \n-DUPLICATESFILTER_FILTERCLASS = 'scrapy.core.filters.SimplePerDomainFilter'\n+DUPEFILTER_FILTERCLASS = 'scrapy.dupefilter.SimplePerDomainFilter'\n \n ENABLED_SPIDERS_FILE = ''\n \n\n@@ -1,15 +1,7 @@\n-import re\n-from pydispatch import dispatcher\n-\n-from scrapy.core import signals\n from scrapy import log\n-from scrapy.http import Request, Response\n from scrapy.core.exceptions import HttpException\n-from scrapy.core.filters import duplicatesfilter\n-from scrapy.utils.misc import load_object\n from scrapy.utils.url import urljoin_rfc as urljoin\n from scrapy.utils.response import get_meta_refresh\n-from scrapy.conf import settings\n \n class RedirectLoop(Exception):\n     pass\n\n@@ -2,17 +2,14 @@\n DuplicatesFilterMiddleware: Filter out already visited urls\n \"\"\"\n \n-from scrapy.http import Request\n-from scrapy.conf import settings\n from scrapy.core.exceptions import IgnoreRequest\n-from scrapy.core.filters import duplicatesfilter\n-from scrapy import log\n+from scrapy.dupefilter import dupefilter\n \n \n class DuplicatesFilterMiddleware(object):\n     \"\"\"Filter out already seen requests to avoid visiting pages more than once.\"\"\"\n \n     def enqueue_request(self, domain, request, priority):\n-        added = duplicatesfilter.add(domain, request)\n+        added = dupefilter.add(domain, request)\n         if not (added or request.dont_filter):\n             raise IgnoreRequest('Skipped (already seen request')\n\n@@ -3,8 +3,7 @@ DuplicatesFilterMiddleware: Filter out already visited urls\n \"\"\"\n \n from scrapy.http import Request\n-from scrapy.conf import settings\n-from scrapy.core.filters import duplicatesfilter\n+from scrapy.dupefilter import dupefilter\n from scrapy import log\n \n \n@@ -15,7 +14,7 @@ class DuplicatesFilterMiddleware(object):\n         domain = spider.domain_name\n         for req in result:\n             if isinstance(req, Request):\n-                has = duplicatesfilter.has(domain, req)\n+                has = dupefilter.has(domain, req)\n                 if has and not req.dont_filter:\n                     log.msg('Skipped (already processed): %s' % req, log.TRACE, domain=domain)\n                     continue\n\n@@ -1,12 +1,9 @@\n from pydispatch import dispatcher\n \n from scrapy.core import signals\n-from scrapy.http import Request\n-from scrapy.core.exceptions import NotConfigured, IgnoreRequest\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n-from scrapy import log\n \n \n class BaseFilter(dict):\n@@ -78,9 +75,9 @@ class NullFilter(dict):\n \n \n try:\n-    duplicatesfilter\n+    dupefilter\n except NameError:\n-    clspath = settings.get('DUPLICATESFILTER_FILTERCLASS')\n+    clspath = settings.get('DUPEFILTER_FILTERCLASS')\n     cls = load_object(clspath) if clspath else NullFilter\n-    duplicatesfilter = cls()\n+    dupefilter = cls()\n \n\n@@ -2,7 +2,7 @@ import unittest\n \n from scrapy.contrib.downloadermiddleware.redirect import RedirectMiddleware\n from scrapy.core.exceptions import HttpException\n-from scrapy.core.filters import duplicatesfilter\n+from scrapy.dupefilter import dupefilter\n from scrapy.spider import spiders\n from scrapy.http import Request, Response, Headers\n \n@@ -12,7 +12,10 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         spiders.spider_modules = ['scrapy.tests.test_spiders']\n         spiders.reload()\n         self.spider = spiders.fromdomain('scrapytest.org')\n-        duplicatesfilter.open('scrapytest.org')\n+        dupefilter.open('scrapytest.org')\n+\n+    def tearDown(self):\n+        dupefilter.close('scrapytest.org')\n \n     def test_process_exception(self):\n \n\n@@ -1,7 +1,7 @@\n import unittest\n \n from scrapy.http import Request\n-from scrapy.core.filters import SimplePerDomainFilter, NullFilter\n+from scrapy.dupefilter import SimplePerDomainFilter, NullFilter\n \n \n class SimplePerDomainFilterTest(unittest.TestCase):\n\n@@ -4,16 +4,16 @@ from scrapy.spider import spiders\n from scrapy.http import Request, Response\n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.contrib.schedulermiddleware.duplicatesfilter import DuplicatesFilterMiddleware\n-from scrapy.core.filters import duplicatesfilter\n+from scrapy.dupefilter import dupefilter\n \n \n class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n \n     def setUp(self):\n-        duplicatesfilter.open('scrapytest.org')\n+        dupefilter.open('scrapytest.org')\n \n     def tearDown(self):\n-        duplicatesfilter.close('scrapytest.org')\n+        dupefilter.close('scrapytest.org')\n \n     def test_process_spider_output(self):\n         domain = 'scrapytest.org'\n\n@@ -4,7 +4,7 @@ from scrapy.spider import spiders\n from scrapy.http import Request, Response\n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.contrib.spidermiddleware.duplicatesfilter import DuplicatesFilterMiddleware\n-from scrapy.core.filters import duplicatesfilter\n+from scrapy.dupefilter import dupefilter\n \n class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n \n@@ -12,10 +12,10 @@ class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n         spiders.spider_modules = ['scrapy.tests.test_spiders']\n         spiders.reload()\n         self.spider = spiders.fromdomain('scrapytest.org')\n-        duplicatesfilter.open('scrapytest.org')\n+        dupefilter.open('scrapytest.org')\n \n     def tearDown(self):\n-        duplicatesfilter.close('scrapytest.org')\n+        dupefilter.close('scrapytest.org')\n \n     def test_process_spider_output(self):\n         mw = DuplicatesFilterMiddleware()\n@@ -28,8 +28,8 @@ class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n         r2 = Request('http://scrapytest.org/2')\n         r3 = Request('http://scrapytest.org/2')\n \n-        duplicatesfilter.add('scrapytest.org', r0)\n-        duplicatesfilter.add('scrapytest.org', r2)\n+        dupefilter.add('scrapytest.org', r0)\n+        dupefilter.add('scrapytest.org', r2)\n \n         filtered = list(mw.process_spider_output(response, [r0, r1, r2, r3], self.spider))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b0624e23edd55452703e3d1cff97d778f24f30f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ebe106b7a20705d1d7669fdca04343b16f41cc1c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 1 | Churn Cumulative: 28 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,6 +13,7 @@ class CookiesMiddleware(object):\n         dispatcher.connect(self.domain_closed, signals.domain_closed)\n \n     def process_request(self, request, spider):\n+        if not request.meta.get('dont_merge_cookies', False):\n             dict_updatedefault(request.cookies, self.cookies[spider.domain_name])\n \n     def process_response(self, request, response, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#15909ef62ffadca8600013e5c10e0d908d5a51a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 4 | Methods Changed: 9 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 62 | Churn Cumulative: 474 | Contributors (this commit): 4 | Commits (past 90d): 16 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -70,7 +70,18 @@ class ExtractorField(object):\n         return list(values)\n \n \n-def treeadapt(*funcs, **adaptor_args):\n+def adaptor(*funcs, **adaptor_args):\n+    \"\"\"A pipe adaptor implementing the tree adaption logic\n+    \n+    It takes multiples unnamed arguments used as functions of the pipe, and a\n+    keywords used as adaptor_args to be passed to functions that supports it\n+\n+    If an adaptor function returns a list of values, each value is used as\n+    input for next adaptor function\n+\n+    Always returns a list of values\n+    \"\"\"\n+\n     pipe_adaptor_args = adaptor_args\n     _funcs = []\n     for func in funcs:\n@@ -79,8 +90,8 @@ def treeadapt(*funcs, **adaptor_args):\n \n     def _adaptor(value, adaptor_args=None):\n         values = value if isinstance(value, (list, tuple)) else [value]\n-        pipe_adaptor_args.update(adaptor_args or {})\n-        pipe_kwargs = {'adaptor_args': pipe_adaptor_args}\n+        aargs = dict(t for d in [pipe_adaptor_args, adaptor_args or {}] for t in d.iteritems())\n+        pipe_kwargs = { 'adaptor_args': aargs }\n \n         for func, takes_args in _funcs:\n             next = []\n\n@@ -0,0 +1,43 @@\n+\n+import unittest\n+from scrapy.contrib_exp.newitem.extractors import adaptor, ItemExtractor\n+\n+\n+class TreeadaptTest(unittest.TestCase):\n+\n+    def test_1_passtrough(self):\n+        ad = adaptor()\n+        self.assertEqual(ad('string'), ['string'])\n+\n+    def test_2_composing(self):\n+        addone = lambda v: v+1\n+        ad = adaptor(addone)\n+        self.assertEqual(ad(0), [1])\n+\n+        addtwo = lambda v: v+2\n+        ad2 = adaptor(addone, addtwo)\n+        self.assertEqual(ad2(0), [3])\n+\n+        ad3 = adaptor(ad, ad2)\n+        self.assertEqual(ad3(0), [4])\n+\n+    def test_3_adaptor_args(self):\n+        addn = lambda v, adaptor_args: v + adaptor_args.get('add_value', 0)\n+\n+        ad = adaptor(addn)\n+        self.assertEqual(ad(0), [0])\n+        self.assertEqual(ad(0, {'add_value': 3}), [3])\n+        self.assertEqual(ad(0), [0])\n+\n+        ad = adaptor(addn, add_value=5)\n+        self.assertEqual(ad(0), [5])\n+        self.assertEqual(ad(0, {'add_value': 3}), [3])\n+        self.assertEqual(ad(0), [5])\n+\n+\n+    def test_4_treelogic(self):\n+        split1 = lambda v: v.split('&')\n+        split2 = lambda v: v.split('=')\n+        ad = adaptor(split1, split2)\n+        self.assertEqual(ad('name=joe&job=joker'), ['name', 'joe', 'job', 'joker'])\n+\n\n@@ -245,4 +245,6 @@ def get_func_args(func):\n             func_args, _, _, _ = inspect.getargspec(func.__call__)\n         except Exception:\n             func_args = []\n+    else:\n+        raise TypeError('%s is not callable' % type(func))\n     return func_args\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
