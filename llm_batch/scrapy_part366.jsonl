{"custom_id": "scrapy#0bf29a7b1b9b6a641c486780b7b0fa455577bf39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 20 | Churn Cumulative: 987 | Contributors (this commit): 21 | Commits (past 90d): 11 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -30,7 +30,9 @@ class HttpCompressionMiddleware:\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n     sent/received from web sites\"\"\"\n \n-    def __init__(self, crawler=None):\n+    def __init__(self, *, crawler=None):\n+        if not crawler:\n+            return\n         self.stats = crawler.stats\n         self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n \n\n@@ -127,18 +127,6 @@ class HttpCompressionTest(TestCase):\n         self.assertStatsEqual(\"httpcompression/response_count\", 1)\n         self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n \n-    def test_process_response_gzip_no_stats(self):\n-        mw = HttpCompressionMiddleware()\n-        response = self._getresponse(\"gzip\")\n-        request = response.request\n-\n-        self.assertEqual(response.headers[\"Content-Encoding\"], b\"gzip\")\n-        newresponse = mw.process_response(request, response, self.spider)\n-        self.assertEqual(mw.stats, None)\n-        assert newresponse is not response\n-        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n-        assert \"Content-Encoding\" not in newresponse.headers\n-\n     def test_process_response_br(self):\n         try:\n             import brotli  # noqa: F401\n@@ -418,8 +406,8 @@ class HttpCompressionSubclassTest(TestCase):\n             (\n                 (\n                     \"HttpCompressionMiddleware subclasses must either modify \"\n-                    \"their '__init__' method to support a 'stats' parameter \"\n-                    \"or reimplement the 'from_crawler' method.\"\n+                    \"their '__init__' method to support a 'crawler' parameter \"\n+                    \"or reimplement their 'from_crawler' method.\"\n                 ),\n             ),\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0c4a98f8e072ccc26a393e51066b52f1f2f6b4d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 17 | Files Changed: 4 | Hunks: 12 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 66 | Churn Cumulative: 1337 | Contributors (this commit): 30 | Commits (past 90d): 17 | Contributors (cumulative): 49 | DMM Complexity: 0.07692307692307693\n\nDIFF:\n@@ -1,11 +1,10 @@\n import io\n import warnings\n-import zlib\n \n from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n-from scrapy.utils._compression import _DecompressionMaxSizeExceeded\n+from scrapy.utils._compression import _DecompressionMaxSizeExceeded, _inflate\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.gz import gunzip\n \n@@ -97,23 +96,14 @@ class HttpCompressionMiddleware:\n \n     def _decode(self, body, encoding):\n         if encoding == b\"gzip\" or encoding == b\"x-gzip\":\n-            body = gunzip(body, max_size=self._max_size)\n-\n+            return gunzip(body, max_size=self._max_size)\n         if encoding == b\"deflate\":\n-            try:\n-                body = zlib.decompress(body)\n-            except zlib.error:\n-                # ugly hack to work with raw deflate content that may\n-                # be sent by microsoft servers. For more information, see:\n-                # http://carsten.codimi.de/gzip.yaws/\n-                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\n-                # http://www.gzip.org/zlib/zlib_faq.html#faq38\n-                body = zlib.decompress(body, -15)\n+            return _inflate(body, max_size=self._max_size)\n         if encoding == b\"br\" and b\"br\" in ACCEPTED_ENCODINGS:\n-            body = brotli.decompress(body)\n+            return brotli.decompress(body)\n         if encoding == b\"zstd\" and b\"zstd\" in ACCEPTED_ENCODINGS:\n             # Using its streaming API since its simple API could handle only cases\n             # where there is content size data embedded in the frame\n             reader = zstandard.ZstdDecompressor().stream_reader(io.BytesIO(body))\n-            body = reader.read()\n+            return reader.read()\n         return body\n\n@@ -1,2 +1,41 @@\n+import zlib\n+from io import BytesIO\n+from typing import List\n+\n+\n class _DecompressionMaxSizeExceeded(ValueError):\n     pass\n+\n+\n+def _inflate(data: bytes, *, max_size: int = 0) -> bytes:\n+    decompressor = zlib.decompressobj()\n+    raw_decompressor = zlib.decompressobj(wbits=-15)\n+    input_stream = BytesIO(data)\n+    output_list: List[bytes] = []\n+    output_chunk = b\".\"\n+    decompressed_size = 0\n+    CHUNK_SIZE = 8196\n+    while output_chunk:\n+        input_chunk = input_stream.read(CHUNK_SIZE)\n+        try:\n+            output_chunk = decompressor.decompress(input_chunk)\n+        except zlib.error:\n+            if decompressor != raw_decompressor:\n+                # ugly hack to work with raw deflate content that may\n+                # be sent by microsoft servers. For more information, see:\n+                # http://carsten.codimi.de/gzip.yaws/\n+                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\n+                # http://www.gzip.org/zlib/zlib_faq.html#faq38\n+                decompressor = raw_decompressor\n+                output_chunk = decompressor.decompress(input_chunk)\n+            else:\n+                raise\n+        decompressed_size += len(output_chunk)\n+        if max_size and decompressed_size > max_size:\n+            raise _DecompressionMaxSizeExceeded(\n+                f\"The number of bytes decompressed so far \"\n+                f\"({decompressed_size}B) exceed the specified maximum \"\n+                f\"({max_size}B).\"\n+            )\n+        output_list.append(output_chunk)\n+    return b\"\".join(output_list)\n\n@@ -8,7 +8,7 @@ from scrapy.http import Response\n from ._compression import _DecompressionMaxSizeExceeded\n \n \n-def gunzip(data: bytes, max_size: int = 0) -> bytes:\n+def gunzip(data: bytes, *, max_size: int = 0) -> bytes:\n     \"\"\"Gunzip the given data and return as much data as possible.\n \n     This is resilient to CRC checksum errors.\n\n@@ -39,8 +39,8 @@ FORMAT = {\n         f\"bomb-{format_id}\": (f\"bomb-{format_id}.bin\", format_id)\n         for format_id in (\n             # \"br\",\n+            \"deflate\",  # 27 968 → 11 511 612\n             \"gzip\",  # 27 988 → 11 511 612\n-            # \"deflate\",\n             # \"zstd\",\n         )\n     },\n@@ -383,6 +383,9 @@ class HttpCompressionTest(TestCase):\n             self.spider,\n         )\n \n+    def test_compression_bomb_deflate(self):\n+        self._test_compression_bomb(\"deflate\")\n+\n     def test_compression_bomb_gzip(self):\n         self._test_compression_bomb(\"gzip\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fba167c5e1f356bcc452e95e92199f1a15135c60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 49 | Churn Cumulative: 1102 | Contributors (this commit): 21 | Commits (past 90d): 18 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,25 +4,25 @@ import warnings\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n-from scrapy.utils._compression import _DecompressionMaxSizeExceeded, _inflate\n+from scrapy.utils._compression import _DecompressionMaxSizeExceeded, _inflate, _unbrotli\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.gz import gunzip\n \n ACCEPTED_ENCODINGS = [b\"gzip\", b\"deflate\"]\n \n try:\n-    import brotli\n-\n-    ACCEPTED_ENCODINGS.append(b\"br\")\n+    import brotli  # noqa: F401\n except ImportError:\n     pass\n+else:\n+    ACCEPTED_ENCODINGS.append(b\"br\")\n \n try:\n     import zstandard\n-\n-    ACCEPTED_ENCODINGS.append(b\"zstd\")\n except ImportError:\n     pass\n+else:\n+    ACCEPTED_ENCODINGS.append(b\"zstd\")\n \n \n class HttpCompressionMiddleware:\n@@ -100,7 +100,7 @@ class HttpCompressionMiddleware:\n         if encoding == b\"deflate\":\n             return _inflate(body, max_size=self._max_size)\n         if encoding == b\"br\" and b\"br\" in ACCEPTED_ENCODINGS:\n-            return brotli.decompress(body)\n+            return _unbrotli(body, max_size=self._max_size)\n         if encoding == b\"zstd\" and b\"zstd\" in ACCEPTED_ENCODINGS:\n             # Using its streaming API since its simple API could handle only cases\n             # where there is content size data embedded in the frame\n\n@@ -2,6 +2,11 @@ import zlib\n from io import BytesIO\n from typing import List\n \n+try:\n+    import brotli\n+except ImportError:\n+    pass\n+\n \n class _DecompressionMaxSizeExceeded(ValueError):\n     pass\n@@ -39,3 +44,24 @@ def _inflate(data: bytes, *, max_size: int = 0) -> bytes:\n             )\n         output_list.append(output_chunk)\n     return b\"\".join(output_list)\n+\n+\n+def _unbrotli(data: bytes, *, max_size: int = 0) -> bytes:\n+    decompressor = brotli.Decompressor()\n+    input_stream = BytesIO(data)\n+    output_list: List[bytes] = []\n+    output_chunk = b\".\"\n+    decompressed_size = 0\n+    CHUNK_SIZE = 8196\n+    while output_chunk:\n+        input_chunk = input_stream.read(CHUNK_SIZE)\n+        output_chunk = decompressor.process(input_chunk)\n+        decompressed_size += len(output_chunk)\n+        if max_size and decompressed_size > max_size:\n+            raise _DecompressionMaxSizeExceeded(\n+                f\"The number of bytes decompressed so far \"\n+                f\"({decompressed_size}B) exceed the specified maximum \"\n+                f\"({max_size}B).\"\n+            )\n+        output_list.append(output_chunk)\n+    return b\"\".join(output_list)\n\n@@ -38,7 +38,7 @@ FORMAT = {\n     **{\n         f\"bomb-{format_id}\": (f\"bomb-{format_id}.bin\", format_id)\n         for format_id in (\n-            # \"br\",\n+            \"br\",  # 34 → 11 511 612\n             \"deflate\",  # 27 968 → 11 511 612\n             \"gzip\",  # 27 988 → 11 511 612\n             # \"zstd\",\n@@ -383,6 +383,13 @@ class HttpCompressionTest(TestCase):\n             self.spider,\n         )\n \n+    def test_compression_bomb_br(self):\n+        try:\n+            import brotli  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no brotli\")\n+        self._test_compression_bomb(\"br\")\n+\n     def test_compression_bomb_deflate(self):\n         self._test_compression_bomb(\"deflate\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9cc870387745f45f744ceef6ed226eefcce0e066", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 45 | Churn Cumulative: 1147 | Contributors (this commit): 21 | Commits (past 90d): 21 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,10 +1,14 @@\n-import io\n import warnings\n \n from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n-from scrapy.utils._compression import _DecompressionMaxSizeExceeded, _inflate, _unbrotli\n+from scrapy.utils._compression import (\n+    _DecompressionMaxSizeExceeded,\n+    _inflate,\n+    _unbrotli,\n+    _unzstd,\n+)\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.gz import gunzip\n \n@@ -18,7 +22,7 @@ else:\n     ACCEPTED_ENCODINGS.append(b\"br\")\n \n try:\n-    import zstandard\n+    import zstandard  # noqa: F401\n except ImportError:\n     pass\n else:\n@@ -102,8 +106,5 @@ class HttpCompressionMiddleware:\n         if encoding == b\"br\" and b\"br\" in ACCEPTED_ENCODINGS:\n             return _unbrotli(body, max_size=self._max_size)\n         if encoding == b\"zstd\" and b\"zstd\" in ACCEPTED_ENCODINGS:\n-            # Using its streaming API since its simple API could handle only cases\n-            # where there is content size data embedded in the frame\n-            reader = zstandard.ZstdDecompressor().stream_reader(io.BytesIO(body))\n-            return reader.read()\n+            return _unzstd(body, max_size=self._max_size)\n         return body\n\n@@ -7,6 +7,11 @@ try:\n except ImportError:\n     pass\n \n+try:\n+    import zstandard\n+except ImportError:\n+    pass\n+\n \n class _DecompressionMaxSizeExceeded(ValueError):\n     pass\n@@ -65,3 +70,23 @@ def _unbrotli(data: bytes, *, max_size: int = 0) -> bytes:\n             )\n         output_list.append(output_chunk)\n     return b\"\".join(output_list)\n+\n+\n+def _unzstd(data: bytes, *, max_size: int = 0) -> bytes:\n+    decompressor = zstandard.ZstdDecompressor()\n+    stream_reader = decompressor.stream_reader(BytesIO(data))\n+    output_list: List[bytes] = []\n+    output_chunk = b\".\"\n+    decompressed_size = 0\n+    CHUNK_SIZE = 8196\n+    while output_chunk:\n+        output_chunk = stream_reader.read(CHUNK_SIZE)\n+        decompressed_size += len(output_chunk)\n+        if max_size and decompressed_size > max_size:\n+            raise _DecompressionMaxSizeExceeded(\n+                f\"The number of bytes decompressed so far \"\n+                f\"({decompressed_size}B) exceed the specified maximum \"\n+                f\"({max_size}B).\"\n+            )\n+        output_list.append(output_chunk)\n+    return b\"\".join(output_list)\n\n@@ -41,7 +41,7 @@ FORMAT = {\n             \"br\",  # 34 → 11 511 612\n             \"deflate\",  # 27 968 → 11 511 612\n             \"gzip\",  # 27 988 → 11 511 612\n-            # \"zstd\",\n+            \"zstd\",  # 1 096 → 11 511 612\n         )\n     },\n }\n@@ -396,6 +396,9 @@ class HttpCompressionTest(TestCase):\n     def test_compression_bomb_gzip(self):\n         self._test_compression_bomb(\"gzip\")\n \n+    def test_compression_bomb_zstd(self):\n+        self._test_compression_bomb(\"zstd\")\n+\n \n class HttpCompressionSubclassTest(TestCase):\n     def test_init_missing_stats(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3fda2fe103dafa8d4d48b21c63b2321ccec9c378", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 23 | Churn Cumulative: 1373 | Contributors (this commit): 24 | Commits (past 90d): 4 | Contributors (cumulative): 34 | DMM Complexity: 0.14285714285714285\n\nDIFF:\n@@ -3,6 +3,7 @@ import re\n \n from scrapy.http import Request, XmlResponse\n from scrapy.spiders import Spider\n+from scrapy.utils._compression import _DecompressionMaxSizeExceeded\n from scrapy.utils.gz import gunzip, gzip_magic_number\n from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n \n@@ -71,7 +72,12 @@ class SitemapSpider(Spider):\n         if isinstance(response, XmlResponse):\n             return response.body\n         if gzip_magic_number(response):\n-            return gunzip(response.body)\n+            try:\n+                return gunzip(\n+                    response.body, max_size=self.settings.getint(\"DOWNLOAD_MAXSIZE\")\n+                )\n+            except _DecompressionMaxSizeExceeded:\n+                return None\n         # actual gzipped sitemap files are decompressed above ;\n         # if we are here (response body is not gzipped)\n         # and have a response for .xml.gz,\n\n@@ -2,6 +2,7 @@ import gzip\n import inspect\n import warnings\n from io import BytesIO\n+from pathlib import Path\n from typing import Any\n from unittest import mock\n \n@@ -25,7 +26,7 @@ from scrapy.spiders import (\n )\n from scrapy.spiders.init import InitSpider\n from scrapy.utils.test import get_crawler\n-from tests import get_testdata\n+from tests import get_testdata, tests_datadir\n \n \n class SpiderTest(unittest.TestCase):\n@@ -489,7 +490,8 @@ class SitemapSpiderTest(SpiderTest):\n     GZBODY = f.getvalue()\n \n     def assertSitemapBody(self, response, body):\n-        spider = self.spider_class(\"example.com\")\n+        crawler = get_crawler()\n+        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n         self.assertEqual(spider._get_sitemap_body(response), body)\n \n     def test_get_sitemap_body(self):\n@@ -692,6 +694,15 @@ Sitemap: /sitemap-relative-url.xml\n             [\"http://www.example.com/sitemap2.xml\"],\n         )\n \n+    def test_compression_bomb(self):\n+        settings = {\"DOWNLOAD_MAXSIZE\": 10_000_000}\n+        crawler = get_crawler(settings_dict=settings)\n+        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n+        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n+        body = body_path.read_bytes()\n+        response = Response(url=\"https://example.com\", body=body)\n+        self.assertIsNone(spider._get_sitemap_body(response))\n+\n \n class DeprecationTest(unittest.TestCase):\n     def test_crawl_spider(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e0b66c021ae20cdcc24e4bb02ffae56005d3a073", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 143 | Lines Deleted: 31 | Files Changed: 4 | Hunks: 25 | Methods Changed: 34 | Complexity Δ (Sum/Max): 16/12 | Churn Δ: 174 | Churn Cumulative: 2602 | Contributors (this commit): 33 | Commits (past 90d): 25 | Contributors (cumulative): 67 | DMM Complexity: 0.9247311827956989\n\nDIFF:\n@@ -1,5 +1,6 @@\n import warnings\n \n+from scrapy import signals\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n@@ -38,6 +39,7 @@ class HttpCompressionMiddleware:\n             return\n         self.stats = crawler.stats\n         self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n+        crawler.signals.connect(self.open_spider, signals.spider_opened)\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -52,10 +54,15 @@ class HttpCompressionMiddleware:\n                 \"reimplement their 'from_crawler' method.\",\n                 ScrapyDeprecationWarning,\n             )\n-            result = cls()\n-            result.stats = crawler.stats\n-            result._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n-            return result\n+            spider = cls()\n+            spider.stats = crawler.stats\n+            spider._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n+            crawler.signals.connect(spider.open_spider, signals.spider_opened)\n+            return spider\n+\n+    def open_spider(self, spider):\n+        if hasattr(spider, \"download_maxsize\"):\n+            self._max_size = spider.download_maxsize\n \n     def process_request(self, request, spider):\n         request.headers.setdefault(\"Accept-Encoding\", b\", \".join(ACCEPTED_ENCODINGS))\n@@ -67,8 +74,11 @@ class HttpCompressionMiddleware:\n             content_encoding = response.headers.getlist(\"Content-Encoding\")\n             if content_encoding:\n                 encoding = content_encoding.pop()\n+                max_size = request.meta.get(\"download_maxsize\", self._max_size)\n                 try:\n-                    decoded_body = self._decode(response.body, encoding.lower())\n+                    decoded_body = self._decode(\n+                        response.body, encoding.lower(), max_size\n+                    )\n                 except _DecompressionMaxSizeExceeded:\n                     raise IgnoreRequest(\n                         f\"Ignored response {response} because its body \"\n@@ -98,13 +108,13 @@ class HttpCompressionMiddleware:\n \n         return response\n \n-    def _decode(self, body, encoding):\n+    def _decode(self, body, encoding, max_size):\n         if encoding == b\"gzip\" or encoding == b\"x-gzip\":\n-            return gunzip(body, max_size=self._max_size)\n+            return gunzip(body, max_size=max_size)\n         if encoding == b\"deflate\":\n-            return _inflate(body, max_size=self._max_size)\n+            return _inflate(body, max_size=max_size)\n         if encoding == b\"br\" and b\"br\" in ACCEPTED_ENCODINGS:\n-            return _unbrotli(body, max_size=self._max_size)\n+            return _unbrotli(body, max_size=max_size)\n         if encoding == b\"zstd\" and b\"zstd\" in ACCEPTED_ENCODINGS:\n-            return _unzstd(body, max_size=self._max_size)\n+            return _unzstd(body, max_size=max_size)\n         return body\n\n@@ -72,10 +72,14 @@ class SitemapSpider(Spider):\n         if isinstance(response, XmlResponse):\n             return response.body\n         if gzip_magic_number(response):\n-            try:\n-                return gunzip(\n-                    response.body, max_size=self.settings.getint(\"DOWNLOAD_MAXSIZE\")\n+            max_size = response.meta.get(\n+                \"download_maxsize\",\n+                getattr(\n+                    self, \"download_maxsize\", self.settings.getint(\"DOWNLOAD_MAXSIZE\")\n+                ),\n             )\n+            try:\n+                return gunzip(response.body, max_size=max_size)\n             except _DecompressionMaxSizeExceeded:\n                 return None\n         # actual gzipped sitemap files are decompressed above ;\n\n@@ -49,10 +49,7 @@ FORMAT = {\n \n class HttpCompressionTest(TestCase):\n     def setUp(self):\n-        settings = {\n-            \"DOWNLOAD_MAXSIZE\": 10_000_000,  # For compression bomb tests.\n-        }\n-        self.crawler = get_crawler(Spider, settings_dict=settings)\n+        self.crawler = get_crawler(Spider)\n         self.spider = self.crawler._create_spider(\"scrapytest.org\")\n         self.mw = HttpCompressionMiddleware.from_crawler(self.crawler)\n         self.crawler.stats.open_spider(self.spider)\n@@ -373,31 +370,103 @@ class HttpCompressionTest(TestCase):\n         self.assertStatsEqual(\"httpcompression/response_count\", None)\n         self.assertStatsEqual(\"httpcompression/response_bytes\", None)\n \n-    def _test_compression_bomb(self, compression_id):\n+    def _test_compression_bomb_setting(self, compression_id):\n+        settings = {\"DOWNLOAD_MAXSIZE\": 10_000_000}\n+        crawler = get_crawler(Spider, settings_dict=settings)\n+        spider = crawler._create_spider(\"scrapytest.org\")\n+        mw = HttpCompressionMiddleware.from_crawler(crawler)\n+        mw.open_spider(spider)\n+\n         response = self._getresponse(f\"bomb-{compression_id}\")\n         self.assertRaises(\n             IgnoreRequest,\n-            self.mw.process_response,\n+            mw.process_response,\n             response.request,\n             response,\n-            self.spider,\n+            spider,\n         )\n \n-    def test_compression_bomb_br(self):\n+    def test_compression_bomb_setting_br(self):\n         try:\n             import brotli  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n-        self._test_compression_bomb(\"br\")\n+        self._test_compression_bomb_setting(\"br\")\n \n-    def test_compression_bomb_deflate(self):\n-        self._test_compression_bomb(\"deflate\")\n+    def test_compression_bomb_setting_deflate(self):\n+        self._test_compression_bomb_setting(\"deflate\")\n \n-    def test_compression_bomb_gzip(self):\n-        self._test_compression_bomb(\"gzip\")\n+    def test_compression_bomb_setting_gzip(self):\n+        self._test_compression_bomb_setting(\"gzip\")\n \n-    def test_compression_bomb_zstd(self):\n-        self._test_compression_bomb(\"zstd\")\n+    def test_compression_bomb_setting_zstd(self):\n+        self._test_compression_bomb_setting(\"zstd\")\n+\n+    def _test_compression_bomb_spider_attr(self, compression_id):\n+        class DownloadMaxSizeSpider(Spider):\n+            download_maxsize = 10_000_000\n+\n+        crawler = get_crawler(DownloadMaxSizeSpider)\n+        spider = crawler._create_spider(\"scrapytest.org\")\n+        mw = HttpCompressionMiddleware.from_crawler(crawler)\n+        mw.open_spider(spider)\n+\n+        response = self._getresponse(f\"bomb-{compression_id}\")\n+        self.assertRaises(\n+            IgnoreRequest,\n+            mw.process_response,\n+            response.request,\n+            response,\n+            spider,\n+        )\n+\n+    def test_compression_bomb_spider_attr_br(self):\n+        try:\n+            import brotli  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no brotli\")\n+        self._test_compression_bomb_spider_attr(\"br\")\n+\n+    def test_compression_bomb_spider_attr_deflate(self):\n+        self._test_compression_bomb_spider_attr(\"deflate\")\n+\n+    def test_compression_bomb_spider_attr_gzip(self):\n+        self._test_compression_bomb_spider_attr(\"gzip\")\n+\n+    def test_compression_bomb_spider_attr_zstd(self):\n+        self._test_compression_bomb_spider_attr(\"zstd\")\n+\n+    def _test_compression_bomb_request_meta(self, compression_id):\n+        crawler = get_crawler(Spider)\n+        spider = crawler._create_spider(\"scrapytest.org\")\n+        mw = HttpCompressionMiddleware.from_crawler(crawler)\n+        mw.open_spider(spider)\n+\n+        response = self._getresponse(f\"bomb-{compression_id}\")\n+        response.meta[\"download_maxsize\"] = 10_000_000\n+        self.assertRaises(\n+            IgnoreRequest,\n+            mw.process_response,\n+            response.request,\n+            response,\n+            spider,\n+        )\n+\n+    def test_compression_bomb_request_meta_br(self):\n+        try:\n+            import brotli  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no brotli\")\n+        self._test_compression_bomb_request_meta(\"br\")\n+\n+    def test_compression_bomb_request_meta_deflate(self):\n+        self._test_compression_bomb_request_meta(\"deflate\")\n+\n+    def test_compression_bomb_request_meta_gzip(self):\n+        self._test_compression_bomb_request_meta(\"gzip\")\n+\n+    def test_compression_bomb_request_meta_zstd(self):\n+        self._test_compression_bomb_request_meta(\"zstd\")\n \n \n class HttpCompressionSubclassTest(TestCase):\n\n@@ -509,6 +509,7 @@ class SitemapSpiderTest(SpiderTest):\n             url=\"http://www.example.com/sitemap\",\n             body=self.GZBODY,\n             headers={\"content-type\": \"application/gzip\"},\n+            request=Request(\"http://www.example.com/sitemap\"),\n         )\n         self.assertSitemapBody(r, self.BODY)\n \n@@ -517,7 +518,11 @@ class SitemapSpiderTest(SpiderTest):\n         self.assertSitemapBody(r, self.BODY)\n \n     def test_get_sitemap_body_xml_url_compressed(self):\n-        r = Response(url=\"http://www.example.com/sitemap.xml.gz\", body=self.GZBODY)\n+        r = Response(\n+            url=\"http://www.example.com/sitemap.xml.gz\",\n+            body=self.GZBODY,\n+            request=Request(\"http://www.example.com/sitemap\"),\n+        )\n         self.assertSitemapBody(r, self.BODY)\n \n         # .xml.gz but body decoded by HttpCompression middleware already\n@@ -694,13 +699,37 @@ Sitemap: /sitemap-relative-url.xml\n             [\"http://www.example.com/sitemap2.xml\"],\n         )\n \n-    def test_compression_bomb(self):\n+    def test_compression_bomb_setting(self):\n         settings = {\"DOWNLOAD_MAXSIZE\": 10_000_000}\n         crawler = get_crawler(settings_dict=settings)\n         spider = self.spider_class.from_crawler(crawler, \"example.com\")\n         body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n         body = body_path.read_bytes()\n-        response = Response(url=\"https://example.com\", body=body)\n+        request = Request(url=\"https://example.com\")\n+        response = Response(url=\"https://example.com\", body=body, request=request)\n+        self.assertIsNone(spider._get_sitemap_body(response))\n+\n+    def test_compression_bomb_spider_attr(self):\n+        class DownloadMaxSizeSpider(self.spider_class):\n+            download_maxsize = 10_000_000\n+\n+        crawler = get_crawler()\n+        spider = DownloadMaxSizeSpider.from_crawler(crawler, \"example.com\")\n+        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n+        body = body_path.read_bytes()\n+        request = Request(url=\"https://example.com\")\n+        response = Response(url=\"https://example.com\", body=body, request=request)\n+        self.assertIsNone(spider._get_sitemap_body(response))\n+\n+    def test_compression_bomb_request_meta(self):\n+        crawler = get_crawler()\n+        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n+        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n+        body = body_path.read_bytes()\n+        request = Request(\n+            url=\"https://example.com\", meta={\"download_maxsize\": 10_000_000}\n+        )\n+        response = Response(url=\"https://example.com\", body=body, request=request)\n         self.assertIsNone(spider._get_sitemap_body(response))\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#03d9866518ab43844ba0309394529240f4cf115e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 250 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 18 | Methods Changed: 24 | Complexity Δ (Sum/Max): 27/18 | Churn Δ: 257 | Churn Cumulative: 2859 | Contributors (this commit): 33 | Commits (past 90d): 29 | Contributors (cumulative): 67 | DMM Complexity: 0.9512195121951219\n\nDIFF:\n@@ -1,4 +1,5 @@\n import warnings\n+from logging import getLogger\n \n from scrapy import signals\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n@@ -13,6 +14,8 @@ from scrapy.utils._compression import (\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.gz import gunzip\n \n+logger = getLogger(__name__)\n+\n ACCEPTED_ENCODINGS = [b\"gzip\", b\"deflate\"]\n \n try:\n@@ -39,6 +42,7 @@ class HttpCompressionMiddleware:\n             return\n         self.stats = crawler.stats\n         self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n+        self._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\n         crawler.signals.connect(self.open_spider, signals.spider_opened)\n \n     @classmethod\n@@ -57,12 +61,15 @@ class HttpCompressionMiddleware:\n             spider = cls()\n             spider.stats = crawler.stats\n             spider._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n+            spider._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\n             crawler.signals.connect(spider.open_spider, signals.spider_opened)\n             return spider\n \n     def open_spider(self, spider):\n         if hasattr(spider, \"download_maxsize\"):\n             self._max_size = spider.download_maxsize\n+        if hasattr(spider, \"download_warnsize\"):\n+            self._warn_size = spider.download_warnsize\n \n     def process_request(self, request, spider):\n         request.headers.setdefault(\"Accept-Encoding\", b\", \".join(ACCEPTED_ENCODINGS))\n@@ -75,6 +82,7 @@ class HttpCompressionMiddleware:\n             if content_encoding:\n                 encoding = content_encoding.pop()\n                 max_size = request.meta.get(\"download_maxsize\", self._max_size)\n+                warn_size = request.meta.get(\"download_warnsize\", self._warn_size)\n                 try:\n                     decoded_body = self._decode(\n                         response.body, encoding.lower(), max_size\n@@ -85,6 +93,12 @@ class HttpCompressionMiddleware:\n                         f\"({len(response.body)} B) exceeded DOWNLOAD_MAXSIZE \"\n                         f\"({self._max_size} B) during decompression.\"\n                     )\n+                if len(response.body) < warn_size and len(decoded_body) >= warn_size:\n+                    logger.warning(\n+                        f\"{response} body size after decompression \"\n+                        f\"({len(decoded_body)} B) is larger than the \"\n+                        f\"download warning size ({warn_size} B).\"\n+                    )\n                 if self.stats:\n                     self.stats.inc_value(\n                         \"httpcompression/response_bytes\",\n\n@@ -1,5 +1,6 @@\n import logging\n import re\n+from typing import TYPE_CHECKING, Any\n \n from scrapy.http import Request, XmlResponse\n from scrapy.spiders import Spider\n@@ -7,6 +8,12 @@ from scrapy.utils._compression import _DecompressionMaxSizeExceeded\n from scrapy.utils.gz import gunzip, gzip_magic_number\n from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+    from scrapy.crawler import Crawler\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -16,6 +23,17 @@ class SitemapSpider(Spider):\n     sitemap_follow = [\"\"]\n     sitemap_alternate_links = False\n \n+    @classmethod\n+    def from_crawler(cls, crawler: \"Crawler\", *args: Any, **kwargs: Any) -> \"Self\":\n+        spider = super().from_crawler(crawler, *args, **kwargs)\n+        spider._max_size = getattr(\n+            spider, \"download_maxsize\", spider.settings.getint(\"DOWNLOAD_MAXSIZE\")\n+        )\n+        spider._warn_size = getattr(\n+            spider, \"download_warnsize\", spider.settings.getint(\"DOWNLOAD_WARNSIZE\")\n+        )\n+        return spider\n+\n     def __init__(self, *a, **kw):\n         super().__init__(*a, **kw)\n         self._cbs = []\n@@ -72,16 +90,19 @@ class SitemapSpider(Spider):\n         if isinstance(response, XmlResponse):\n             return response.body\n         if gzip_magic_number(response):\n-            max_size = response.meta.get(\n-                \"download_maxsize\",\n-                getattr(\n-                    self, \"download_maxsize\", self.settings.getint(\"DOWNLOAD_MAXSIZE\")\n-                ),\n-            )\n+            uncompressed_size = len(response.body)\n+            max_size = response.meta.get(\"download_maxsize\", self._max_size)\n+            warn_size = response.meta.get(\"download_warnsize\", self._warn_size)\n             try:\n-                return gunzip(response.body, max_size=max_size)\n+                body = gunzip(response.body, max_size=max_size)\n             except _DecompressionMaxSizeExceeded:\n                 return None\n+            if uncompressed_size < warn_size and len(body) >= warn_size:\n+                logger.warning(\n+                    f\"{response} body size after decompression ({len(body)} B) \"\n+                    f\"is larger than the download warning size ({warn_size} B).\"\n+                )\n+            return body\n         # actual gzipped sitemap files are decompressed above ;\n         # if we are here (response body is not gzipped)\n         # and have a response for .xml.gz,\n\n@@ -1,9 +1,11 @@\n from gzip import GzipFile\n from io import BytesIO\n+from logging import WARNING\n from pathlib import Path\n from unittest import SkipTest, TestCase\n from warnings import catch_warnings\n \n+from testfixtures import LogCapture\n from w3lib.encoding import resolve_encoding\n \n from scrapy.downloadermiddlewares.httpcompression import (\n@@ -468,6 +470,134 @@ class HttpCompressionTest(TestCase):\n     def test_compression_bomb_request_meta_zstd(self):\n         self._test_compression_bomb_request_meta(\"zstd\")\n \n+    def _test_download_warnsize_setting(self, compression_id):\n+        settings = {\"DOWNLOAD_WARNSIZE\": 10_000_000}\n+        crawler = get_crawler(Spider, settings_dict=settings)\n+        spider = crawler._create_spider(\"scrapytest.org\")\n+        mw = HttpCompressionMiddleware.from_crawler(crawler)\n+        mw.open_spider(spider)\n+        response = self._getresponse(f\"bomb-{compression_id}\")\n+\n+        with LogCapture(\n+            \"scrapy.downloadermiddlewares.httpcompression\",\n+            propagate=False,\n+            level=WARNING,\n+        ) as log:\n+            mw.process_response(response.request, response, spider)\n+        log.check(\n+            (\n+                \"scrapy.downloadermiddlewares.httpcompression\",\n+                \"WARNING\",\n+                (\n+                    \"<200 http://scrapytest.org/> body size after \"\n+                    \"decompression (11511612 B) is larger than the download \"\n+                    \"warning size (10000000 B).\"\n+                ),\n+            ),\n+        )\n+\n+    def test_download_warnsize_setting_br(self):\n+        try:\n+            import brotli  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no brotli\")\n+        self._test_download_warnsize_setting(\"br\")\n+\n+    def test_download_warnsize_setting_deflate(self):\n+        self._test_download_warnsize_setting(\"deflate\")\n+\n+    def test_download_warnsize_setting_gzip(self):\n+        self._test_download_warnsize_setting(\"gzip\")\n+\n+    def test_download_warnsize_setting_zstd(self):\n+        self._test_download_warnsize_setting(\"zstd\")\n+\n+    def _test_download_warnsize_spider_attr(self, compression_id):\n+        class DownloadWarnSizeSpider(Spider):\n+            download_warnsize = 10_000_000\n+\n+        crawler = get_crawler(DownloadWarnSizeSpider)\n+        spider = crawler._create_spider(\"scrapytest.org\")\n+        mw = HttpCompressionMiddleware.from_crawler(crawler)\n+        mw.open_spider(spider)\n+        response = self._getresponse(f\"bomb-{compression_id}\")\n+\n+        with LogCapture(\n+            \"scrapy.downloadermiddlewares.httpcompression\",\n+            propagate=False,\n+            level=WARNING,\n+        ) as log:\n+            mw.process_response(response.request, response, spider)\n+        log.check(\n+            (\n+                \"scrapy.downloadermiddlewares.httpcompression\",\n+                \"WARNING\",\n+                (\n+                    \"<200 http://scrapytest.org/> body size after \"\n+                    \"decompression (11511612 B) is larger than the download \"\n+                    \"warning size (10000000 B).\"\n+                ),\n+            ),\n+        )\n+\n+    def test_download_warnsize_spider_attr_br(self):\n+        try:\n+            import brotli  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no brotli\")\n+        self._test_download_warnsize_spider_attr(\"br\")\n+\n+    def test_download_warnsize_spider_attr_deflate(self):\n+        self._test_download_warnsize_spider_attr(\"deflate\")\n+\n+    def test_download_warnsize_spider_attr_gzip(self):\n+        self._test_download_warnsize_spider_attr(\"gzip\")\n+\n+    def test_download_warnsize_spider_attr_zstd(self):\n+        self._test_download_warnsize_spider_attr(\"zstd\")\n+\n+    def _test_download_warnsize_request_meta(self, compression_id):\n+        crawler = get_crawler(Spider)\n+        spider = crawler._create_spider(\"scrapytest.org\")\n+        mw = HttpCompressionMiddleware.from_crawler(crawler)\n+        mw.open_spider(spider)\n+        response = self._getresponse(f\"bomb-{compression_id}\")\n+        response.meta[\"download_warnsize\"] = 10_000_000\n+\n+        with LogCapture(\n+            \"scrapy.downloadermiddlewares.httpcompression\",\n+            propagate=False,\n+            level=WARNING,\n+        ) as log:\n+            mw.process_response(response.request, response, spider)\n+        log.check(\n+            (\n+                \"scrapy.downloadermiddlewares.httpcompression\",\n+                \"WARNING\",\n+                (\n+                    \"<200 http://scrapytest.org/> body size after \"\n+                    \"decompression (11511612 B) is larger than the download \"\n+                    \"warning size (10000000 B).\"\n+                ),\n+            ),\n+        )\n+\n+    def test_download_warnsize_request_meta_br(self):\n+        try:\n+            import brotli  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no brotli\")\n+        self._test_download_warnsize_request_meta(\"br\")\n+\n+    def test_download_warnsize_request_meta_deflate(self):\n+        self._test_download_warnsize_request_meta(\"deflate\")\n+\n+    def test_download_warnsize_request_meta_gzip(self):\n+        self._test_download_warnsize_request_meta(\"gzip\")\n+\n+    def test_download_warnsize_request_meta_zstd(self):\n+        self._test_download_warnsize_request_meta(\"zstd\")\n+\n \n class HttpCompressionSubclassTest(TestCase):\n     def test_init_missing_stats(self):\n\n@@ -2,6 +2,7 @@ import gzip\n import inspect\n import warnings\n from io import BytesIO\n+from logging import WARNING\n from pathlib import Path\n from typing import Any\n from unittest import mock\n@@ -732,6 +733,83 @@ Sitemap: /sitemap-relative-url.xml\n         response = Response(url=\"https://example.com\", body=body, request=request)\n         self.assertIsNone(spider._get_sitemap_body(response))\n \n+    def test_download_warnsize_setting(self):\n+        settings = {\"DOWNLOAD_WARNSIZE\": 10_000_000}\n+        crawler = get_crawler(settings_dict=settings)\n+        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n+        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n+        body = body_path.read_bytes()\n+        request = Request(url=\"https://example.com\")\n+        response = Response(url=\"https://example.com\", body=body, request=request)\n+        with LogCapture(\n+            \"scrapy.spiders.sitemap\", propagate=False, level=WARNING\n+        ) as log:\n+            spider._get_sitemap_body(response)\n+        log.check(\n+            (\n+                \"scrapy.spiders.sitemap\",\n+                \"WARNING\",\n+                (\n+                    \"<200 https://example.com> body size after decompression \"\n+                    \"(11511612 B) is larger than the download warning size \"\n+                    \"(10000000 B).\"\n+                ),\n+            ),\n+        )\n+\n+    def test_download_warnsize_spider_attr(self):\n+        class DownloadWarnSizeSpider(self.spider_class):\n+            download_warnsize = 10_000_000\n+\n+        crawler = get_crawler()\n+        spider = DownloadWarnSizeSpider.from_crawler(crawler, \"example.com\")\n+        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n+        body = body_path.read_bytes()\n+        request = Request(\n+            url=\"https://example.com\", meta={\"download_warnsize\": 10_000_000}\n+        )\n+        response = Response(url=\"https://example.com\", body=body, request=request)\n+        with LogCapture(\n+            \"scrapy.spiders.sitemap\", propagate=False, level=WARNING\n+        ) as log:\n+            spider._get_sitemap_body(response)\n+        log.check(\n+            (\n+                \"scrapy.spiders.sitemap\",\n+                \"WARNING\",\n+                (\n+                    \"<200 https://example.com> body size after decompression \"\n+                    \"(11511612 B) is larger than the download warning size \"\n+                    \"(10000000 B).\"\n+                ),\n+            ),\n+        )\n+\n+    def test_download_warnsize_request_meta(self):\n+        crawler = get_crawler()\n+        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n+        body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")\n+        body = body_path.read_bytes()\n+        request = Request(\n+            url=\"https://example.com\", meta={\"download_warnsize\": 10_000_000}\n+        )\n+        response = Response(url=\"https://example.com\", body=body, request=request)\n+        with LogCapture(\n+            \"scrapy.spiders.sitemap\", propagate=False, level=WARNING\n+        ) as log:\n+            spider._get_sitemap_body(response)\n+        log.check(\n+            (\n+                \"scrapy.spiders.sitemap\",\n+                \"WARNING\",\n+                (\n+                    \"<200 https://example.com> body size after decompression \"\n+                    \"(11511612 B) is larger than the download warning size \"\n+                    \"(10000000 B).\"\n+                ),\n+            ),\n+        )\n+\n \n class DeprecationTest(unittest.TestCase):\n     def test_crawl_spider(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cf80e5670e8317858b9f60008a5d2d97b4988da0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 6 | Churn Cumulative: 486 | Contributors (this commit): 21 | Commits (past 90d): 15 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -93,7 +93,7 @@ class HttpCompressionMiddleware:\n                         f\"({len(response.body)} B) exceeded DOWNLOAD_MAXSIZE \"\n                         f\"({self._max_size} B) during decompression.\"\n                     )\n-                if len(response.body) < warn_size and len(decoded_body) >= warn_size:\n+                if len(response.body) < warn_size <= len(decoded_body):\n                     logger.warning(\n                         f\"{response} body size after decompression \"\n                         f\"({len(decoded_body)} B) is larger than the \"\n\n@@ -22,6 +22,8 @@ class SitemapSpider(Spider):\n     sitemap_rules = [(\"\", \"parse\")]\n     sitemap_follow = [\"\"]\n     sitemap_alternate_links = False\n+    _max_size: int\n+    _warn_size: int\n \n     @classmethod\n     def from_crawler(cls, crawler: \"Crawler\", *args: Any, **kwargs: Any) -> \"Self\":\n@@ -97,7 +99,7 @@ class SitemapSpider(Spider):\n                 body = gunzip(response.body, max_size=max_size)\n             except _DecompressionMaxSizeExceeded:\n                 return None\n-            if uncompressed_size < warn_size and len(body) >= warn_size:\n+            if uncompressed_size < warn_size <= len(body):\n                 logger.warning(\n                     f\"{response} body size after decompression ({len(body)} B) \"\n                     f\"is larger than the download warning size ({warn_size} B).\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8e25f8c157e53c9f5df51e950fed18becfe7797d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 310 | Contributors (this commit): 15 | Commits (past 90d): 12 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -91,7 +91,7 @@ class HttpCompressionMiddleware:\n                     raise IgnoreRequest(\n                         f\"Ignored response {response} because its body \"\n                         f\"({len(response.body)} B) exceeded DOWNLOAD_MAXSIZE \"\n-                        f\"({self._max_size} B) during decompression.\"\n+                        f\"({max_size} B) during decompression.\"\n                     )\n                 if len(response.body) < warn_size <= len(decoded_body):\n                     logger.warning(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#09ce0ef52681e2e8344b6848b28bc222016362ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 27 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 27 | Churn Cumulative: 2961 | Contributors (this commit): 34 | Commits (past 90d): 4 | Contributors (cumulative): 34 | DMM Complexity: 0.0\n\nDIFF:\n@@ -726,33 +726,6 @@ class Http11MockServerTestCase(unittest.TestCase):\n         reason = crawler.spider.meta[\"close_reason\"]\n         self.assertTrue(reason, \"finished\")\n \n-    @defer.inlineCallbacks\n-    def test_download_gzip_response(self):\n-        crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n-        body = b\"1\" * 100  # PayloadResource requires body length to be 100\n-        request = Request(\n-            self.mockserver.url(\"/payload\"),\n-            method=\"POST\",\n-            body=body,\n-            meta={\"download_maxsize\": 50},\n-        )\n-        yield crawler.crawl(seed=request)\n-        failure = crawler.spider.meta[\"failure\"]\n-        # download_maxsize < 100, hence the CancelledError\n-        self.assertIsInstance(failure.value, defer.CancelledError)\n-\n-        # See issue https://twistedmatrix.com/trac/ticket/8175\n-        raise unittest.SkipTest(\"xpayload fails on PY3\")\n-        crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n-        request.headers.setdefault(b\"Accept-Encoding\", b\"gzip,deflate\")\n-        request = request.replace(url=self.mockserver.url(\"/xpayload\"))\n-        yield crawler.crawl(seed=request)\n-        # download_maxsize = 50 is enough for the gzipped response\n-        failure = crawler.spider.meta.get(\"failure\")\n-        self.assertIsNone(failure)\n-        reason = crawler.spider.meta[\"close_reason\"]\n-        self.assertTrue(reason, \"finished\")\n-\n \n class UriResource(resource.Resource):\n     \"\"\"Return the full uri that was requested\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
