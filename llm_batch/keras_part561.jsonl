{"custom_id": "keras#9389d82af4238f2b02da6c55f938ae5b583b6c3a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 23 | Churn Cumulative: 1458 | Contributors (this commit): 7 | Commits (past 90d): 80 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,6 +3,7 @@ import os\n \n from keras_core.backend.common.variables import AutocastScope\n from keras_core.backend.common.variables import get_autocast_scope\n+from keras_core.backend.common.variables import is_float_dtype\n from keras_core.backend.common.variables import standardize_dtype\n from keras_core.backend.common.variables import standardize_shape\n from keras_core.backend.config import backend\n\n@@ -3,5 +3,6 @@ from keras_core.backend.common import random\n from keras_core.backend.common.variables import AutocastScope\n from keras_core.backend.common.variables import KerasVariable\n from keras_core.backend.common.variables import get_autocast_scope\n+from keras_core.backend.common.variables import is_float_dtype\n from keras_core.backend.common.variables import standardize_dtype\n from keras_core.backend.common.variables import standardize_shape\n\n@@ -284,10 +284,22 @@ class Layer(Operation):\n         # Argument validation and conversion. #\n         # 1. Convert any array arguments to tensors of correct dtype.\n         def maybe_convert(x):\n-            if isinstance(x, np.ndarray) or backend.is_tensor(x):\n-                # TODO: only cast when float!\n+            if isinstance(x, np.ndarray):\n                 return backend.convert_to_tensor(x, dtype=self.compute_dtype)\n-            # TODO: cast KerasTensor too?\n+            if backend.is_tensor(x):\n+                if (\n+                    self.autocast\n+                    and backend.is_float_dtype(x.dtype)\n+                    and x.dtype != self.compute_dtype\n+                ):\n+                    return backend.cast(x, dtype=self.compute_dtype)\n+            elif isinstance(x, backend.KerasTensor):\n+                if (\n+                    self.autocast\n+                    and backend.is_float_dtype(x.dtype)\n+                    and x.dtype != self.compute_dtype\n+                ):\n+                    x.dtype = self.compute_dtype\n             return x\n \n         args = nest.map_structure(maybe_convert, args)\n\n@@ -264,8 +264,7 @@ class LayerTest(testing.TestCase):\n         self.assertEqual(y.dtype.name, \"float16\")\n         self.assertEqual(layer.kernel.dtype, \"float32\")\n \n-    # TODO\n-    def DISABLED_test_masking(self):\n+    def test_masking(self):\n         class BasicMaskedLayer(layers.Layer):\n             def __init__(self):\n                 super().__init__()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#861ad749eeff1f222f941c1f96f0f56c58a97742", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 35/18 | Churn Δ: 14 | Churn Cumulative: 1572 | Contributors (this commit): 15 | Commits (past 90d): 6 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -50,6 +50,7 @@ from keras.layers.preprocessing import text_vectorization\n from keras.layers.rnn import cell_wrappers\n from keras.layers.rnn import gru\n from keras.layers.rnn import lstm\n+from keras.metrics import base_metric\n from keras.saving import serialization_lib\n from keras.saving.legacy import serialization as legacy_serialization\n from keras.saving.legacy.saved_model import json_utils\n@@ -208,6 +209,13 @@ def serialize(layer, use_legacy_format=False):\n     pprint(tf.keras.layers.serialize(model))\n     # prints the configuration of the model, as a dict.\n     \"\"\"\n+    if isinstance(layer, base_metric.Metric):\n+        raise ValueError(\n+            f\"Cannot serialize {layer} since it is a metric. \"\n+            \"Please use the `keras.metrics.serialize()` and \"\n+            \"`keras.metrics.deserialize()` APIs to serialize \"\n+            \"and deserialize metrics.\"\n+        )\n     if use_legacy_format:\n         return legacy_serialization.serialize_keras_object(layer)\n \n\n@@ -24,6 +24,7 @@ from keras.layers.rnn import gru\n from keras.layers.rnn import gru_v1\n from keras.layers.rnn import lstm\n from keras.layers.rnn import lstm_v1\n+from keras.metrics import Mean\n from keras.testing_infra import test_combinations\n \n \n@@ -191,6 +192,11 @@ class LayerSerializationTest(parameterized.TestCase, tf.test.TestCase):\n             self.assertIsInstance(new_layer, gru_v1.GRU)\n             self.assertNotIsInstance(new_layer, gru.GRU)\n \n+    def test_serialize_metric_throws_error(self):\n+        metric = Mean()\n+        with self.assertRaisesRegex(ValueError, \"since it is a metric.\"):\n+            _ = keras.layers.serialize(metric)\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#846390b904867398ca605431d6587f31fd6f6b55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 405 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 21/18 | Churn Δ: 409 | Churn Cumulative: 801 | Contributors (this commit): 4 | Commits (past 90d): 20 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,8 @@\n from keras_core.api_export import keras_core_export\n+from keras_core.metrics.confusion_metrics import FalseNegatives\n from keras_core.metrics.confusion_metrics import FalsePositives\n+from keras_core.metrics.confusion_metrics import TrueNegatives\n+from keras_core.metrics.confusion_metrics import TruePositives\n from keras_core.metrics.hinge_metrics import CategoricalHinge\n from keras_core.metrics.hinge_metrics import Hinge\n from keras_core.metrics.hinge_metrics import SquaredHinge\n\n@@ -112,3 +112,135 @@ class FalsePositives(_ConfusionMatrixConditionCount):\n             name=name,\n             dtype=dtype,\n         )\n+\n+\n+@keras_core_export(\"keras_core.metrics.FalseNegatives\")\n+class FalseNegatives(_ConfusionMatrixConditionCount):\n+    \"\"\"Calculates the number of false negatives.\n+\n+    If `sample_weight` is given, calculates the sum of the weights of\n+    false negatives. This metric creates one local variable, `accumulator`\n+    that is used to keep track of the number of false negatives.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+        thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n+            list/tuple of float threshold values in [0, 1]. A threshold is\n+            compared with prediction values to determine the truth value of\n+            predictions (i.e., above the threshold is `true`, below is `false`).\n+            If used with a loss function that sets `from_logits=True` (i.e. no\n+            sigmoid applied to predictions), `thresholds` should be set to 0.\n+            One metric value is generated for each threshold value.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.FalseNegatives()\n+    >>> m.update_state([0, 1, 1, 1], [0, 1, 0, 0])\n+    >>> m.result()\n+    2.0\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 1, 1, 1], [0, 1, 0, 0], sample_weight=[0, 0, 1, 0])\n+    >>> m.result()\n+    1.0\n+    \"\"\"\n+\n+    def __init__(self, thresholds=None, name=None, dtype=None):\n+        super().__init__(\n+            confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_NEGATIVES,\n+            thresholds=thresholds,\n+            name=name,\n+            dtype=dtype,\n+        )\n+\n+\n+@keras_core_export(\"keras_core.metrics.TrueNegatives\")\n+class TrueNegatives(_ConfusionMatrixConditionCount):\n+    \"\"\"Calculates the number of true negatives.\n+\n+    If `sample_weight` is given, calculates the sum of the weights of\n+    true negatives. This metric creates one local variable, `accumulator`\n+    that is used to keep track of the number of true negatives.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+        thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n+            list/tuple of float threshold values in [0, 1]. A threshold is\n+            compared with prediction values to determine the truth value of\n+            predictions (i.e., above the threshold is `true`, below is `false`).\n+            If used with a loss function that sets `from_logits=True` (i.e. no\n+            sigmoid applied to predictions), `thresholds` should be set to 0.\n+            One metric value is generated for each threshold value.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.TrueNegatives()\n+    >>> m.update_state([0, 1, 0, 0], [1, 1, 0, 0])\n+    >>> m.result()\n+    2.0\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 1, 0, 0], [1, 1, 0, 0], sample_weight=[0, 0, 1, 0])\n+    >>> m.result()\n+    1.0\n+    \"\"\"\n+\n+    def __init__(self, thresholds=None, name=None, dtype=None):\n+        super().__init__(\n+            confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_NEGATIVES,\n+            thresholds=thresholds,\n+            name=name,\n+            dtype=dtype,\n+        )\n+\n+\n+@keras_core_export(\"keras_core.metrics.TruePositives\")\n+class TruePositives(_ConfusionMatrixConditionCount):\n+    \"\"\"Calculates the number of true positives.\n+\n+    If `sample_weight` is given, calculates the sum of the weights of\n+    true positives. This metric creates one local variable, `true_positives`\n+    that is used to keep track of the number of true positives.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+        thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n+            list/tuple of float threshold values in [0, 1]. A threshold is\n+            compared with prediction values to determine the truth value of\n+            predictions (i.e., above the threshold is `true`, below is `false`).\n+            If used with a loss function that sets `from_logits=True` (i.e. no\n+            sigmoid applied to predictions), `thresholds` should be set to 0.\n+            One metric value is generated for each threshold value.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.TruePositives()\n+    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n+    >>> m.result()\n+    2.0\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n+    >>> m.result()\n+    1.0\n+    \"\"\"\n+\n+    def __init__(self, thresholds=None, name=None, dtype=None):\n+        super().__init__(\n+            confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_POSITIVES,\n+            thresholds=thresholds,\n+            name=name,\n+            dtype=dtype,\n+        )\n\n@@ -5,6 +5,7 @@ from keras_core import metrics\n from keras_core import testing\n \n # TODO: remove reliance on this (or alternatively, turn it on by default).\n+# This is no longer needed with tf-nightly.\n np_config.enable_numpy_behavior()\n \n \n@@ -32,8 +33,7 @@ class FalsePositivesTest(testing.TestCase):\n         )\n \n         fp_obj.update_state(y_true, y_pred)\n-        result = fp_obj.result()\n-        self.assertAllClose(7.0, result)\n+        self.assertAllClose(7.0, fp_obj.result())\n \n     def test_weighted(self):\n         fp_obj = metrics.FalsePositives()\n@@ -63,8 +63,7 @@ class FalsePositivesTest(testing.TestCase):\n         )\n \n         fp_obj.update_state(y_true, y_pred)\n-        result = fp_obj.result()\n-        self.assertAllClose([7.0, 4.0, 2.0], result)\n+        self.assertAllClose([7.0, 4.0, 2.0], fp_obj.result())\n \n     def test_weighted_with_thresholds(self):\n         fp_obj = metrics.FalsePositives(thresholds=[0.15, 0.5, 0.85])\n@@ -102,3 +101,270 @@ class FalsePositivesTest(testing.TestCase):\n             r\"Threshold values must be in \\[0, 1\\]. Received: \\[None\\]\",\n         ):\n             metrics.FalsePositives(thresholds=[None])\n+\n+\n+class FalseNegativesTest(testing.TestCase):\n+    def test_config(self):\n+        fn_obj = metrics.FalseNegatives(name=\"my_fn\", thresholds=[0.4, 0.9])\n+        self.assertEqual(fn_obj.name, \"my_fn\")\n+        self.assertLen(fn_obj.variables, 1)\n+        self.assertEqual(fn_obj.thresholds, [0.4, 0.9])\n+\n+        # Check save and restore config\n+        fn_obj2 = metrics.FalseNegatives.from_config(fn_obj.get_config())\n+        self.assertEqual(fn_obj2.name, \"my_fn\")\n+        self.assertLen(fn_obj2.variables, 1)\n+        self.assertEqual(fn_obj2.thresholds, [0.4, 0.9])\n+\n+    def test_unweighted(self):\n+        fn_obj = metrics.FalseNegatives()\n+\n+        y_true = np.array(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = np.array(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+\n+        fn_obj.update_state(y_true, y_pred)\n+        self.assertAllClose(3.0, fn_obj.result())\n+\n+    def test_weighted(self):\n+        fn_obj = metrics.FalseNegatives()\n+        y_true = np.array(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = np.array(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+        sample_weight = np.array((1.0, 1.5, 2.0, 2.5))\n+        result = fn_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(5.0, result)\n+\n+    def test_unweighted_with_thresholds(self):\n+        fn_obj = metrics.FalseNegatives(thresholds=[0.15, 0.5, 0.85])\n+\n+        y_pred = np.array(\n+            (\n+                (0.9, 0.2, 0.8, 0.1),\n+                (0.2, 0.9, 0.7, 0.6),\n+                (0.1, 0.2, 0.4, 0.3),\n+                (0, 1, 0.7, 0.3),\n+            )\n+        )\n+        y_true = np.array(\n+            ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n+        )\n+\n+        fn_obj.update_state(y_true, y_pred)\n+        self.assertAllClose([1.0, 4.0, 6.0], fn_obj.result())\n+\n+    def test_weighted_with_thresholds(self):\n+        fn_obj = metrics.FalseNegatives(thresholds=[0.15, 0.5, 0.85])\n+\n+        y_pred = np.array(\n+            (\n+                (0.9, 0.2, 0.8, 0.1),\n+                (0.2, 0.9, 0.7, 0.6),\n+                (0.1, 0.2, 0.4, 0.3),\n+                (0, 1, 0.7, 0.3),\n+            )\n+        )\n+        y_true = np.array(\n+            ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n+        )\n+        sample_weight = ((3.0,), (5.0,), (7.0,), (4.0,))\n+\n+        result = fn_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose([4.0, 16.0, 23.0], result)\n+\n+    def test_threshold_limit(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"Threshold values must be in \\[0, 1\\]. Received: \\[-1, 2\\]\",\n+        ):\n+            metrics.FalseNegatives(thresholds=[-1, 0.5, 2])\n+\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"Threshold values must be in \\[0, 1\\]. Received: \\[None\\]\",\n+        ):\n+            metrics.FalseNegatives(thresholds=[None])\n+\n+\n+class TrueNegativesTest(testing.TestCase):\n+    def test_config(self):\n+        tn_obj = metrics.TrueNegatives(name=\"my_tn\", thresholds=[0.4, 0.9])\n+        self.assertEqual(tn_obj.name, \"my_tn\")\n+        self.assertLen(tn_obj.variables, 1)\n+        self.assertEqual(tn_obj.thresholds, [0.4, 0.9])\n+\n+        # Check save and restore config\n+        tn_obj2 = metrics.TrueNegatives.from_config(tn_obj.get_config())\n+        self.assertEqual(tn_obj2.name, \"my_tn\")\n+        self.assertLen(tn_obj2.variables, 1)\n+        self.assertEqual(tn_obj2.thresholds, [0.4, 0.9])\n+\n+    def test_unweighted(self):\n+        tn_obj = metrics.TrueNegatives()\n+\n+        y_true = np.array(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = np.array(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+\n+        tn_obj.update_state(y_true, y_pred)\n+        self.assertAllClose(3.0, tn_obj.result())\n+\n+    def test_weighted(self):\n+        tn_obj = metrics.TrueNegatives()\n+        y_true = np.array(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = np.array(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+        sample_weight = np.array((1.0, 1.5, 2.0, 2.5))\n+        result = tn_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(4.0, result)\n+\n+    def test_unweighted_with_thresholds(self):\n+        tn_obj = metrics.TrueNegatives(thresholds=[0.15, 0.5, 0.85])\n+\n+        y_pred = np.array(\n+            (\n+                (0.9, 0.2, 0.8, 0.1),\n+                (0.2, 0.9, 0.7, 0.6),\n+                (0.1, 0.2, 0.4, 0.3),\n+                (0, 1, 0.7, 0.3),\n+            )\n+        )\n+        y_true = np.array(\n+            ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n+        )\n+\n+        tn_obj.update_state(y_true, y_pred)\n+        self.assertAllClose([2.0, 5.0, 7.0], tn_obj.result())\n+\n+    def test_weighted_with_thresholds(self):\n+        tn_obj = metrics.TrueNegatives(thresholds=[0.15, 0.5, 0.85])\n+\n+        y_pred = np.array(\n+            (\n+                (0.9, 0.2, 0.8, 0.1),\n+                (0.2, 0.9, 0.7, 0.6),\n+                (0.1, 0.2, 0.4, 0.3),\n+                (0, 1, 0.7, 0.3),\n+            )\n+        )\n+        y_true = np.array(\n+            ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n+        )\n+        sample_weight = ((0.0, 2.0, 3.0, 5.0),)\n+\n+        result = tn_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose([5.0, 15.0, 23.0], result)\n+\n+    def test_threshold_limit(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"Threshold values must be in \\[0, 1\\]. Received: \\[-1, 2\\]\",\n+        ):\n+            metrics.TrueNegatives(thresholds=[-1, 0.5, 2])\n+\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"Threshold values must be in \\[0, 1\\]. Received: \\[None\\]\",\n+        ):\n+            metrics.TrueNegatives(thresholds=[None])\n+\n+\n+class TruePositiveTest(testing.TestCase):\n+    def test_config(self):\n+        tp_obj = metrics.TruePositives(name=\"my_tp\", thresholds=[0.4, 0.9])\n+        self.assertEqual(tp_obj.name, \"my_tp\")\n+        self.assertLen(tp_obj.variables, 1)\n+        self.assertEqual(tp_obj.thresholds, [0.4, 0.9])\n+\n+        # Check save and restore config\n+        tp_obj2 = metrics.TruePositives.from_config(tp_obj.get_config())\n+        self.assertEqual(tp_obj2.name, \"my_tp\")\n+        self.assertLen(tp_obj2.variables, 1)\n+        self.assertEqual(tp_obj2.thresholds, [0.4, 0.9])\n+\n+    def test_unweighted(self):\n+        tp_obj = metrics.TruePositives()\n+\n+        y_true = np.array(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = np.array(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+\n+        tp_obj.update_state(y_true, y_pred)\n+        self.assertAllClose(7.0, tp_obj.result())\n+\n+    def test_weighted(self):\n+        tp_obj = metrics.TruePositives()\n+        y_true = np.array(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = np.array(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+        sample_weight = np.array((1.0, 1.5, 2.0, 2.5))\n+        result = tp_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(12.0, result)\n+\n+    def test_unweighted_with_thresholds(self):\n+        tp_obj = metrics.TruePositives(thresholds=[0.15, 0.5, 0.85])\n+\n+        y_pred = np.array(\n+            (\n+                (0.9, 0.2, 0.8, 0.1),\n+                (0.2, 0.9, 0.7, 0.6),\n+                (0.1, 0.2, 0.4, 0.3),\n+                (0, 1, 0.7, 0.3),\n+            )\n+        )\n+        y_true = np.array(\n+            ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n+        )\n+\n+        tp_obj.update_state(y_true, y_pred)\n+        self.assertAllClose([6.0, 3.0, 1.0], tp_obj.result())\n+\n+    def test_weighted_with_thresholds(self):\n+        tp_obj = metrics.TruePositives(thresholds=[0.15, 0.5, 0.85])\n+\n+        y_pred = np.array(\n+            (\n+                (0.9, 0.2, 0.8, 0.1),\n+                (0.2, 0.9, 0.7, 0.6),\n+                (0.1, 0.2, 0.4, 0.3),\n+                (0, 1, 0.7, 0.3),\n+            )\n+        )\n+        y_true = np.array(\n+            ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n+        )\n+        sample_weight = 37.0\n+\n+        result = tp_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose([222.0, 111.0, 37.0], result)\n+\n+    def test_threshold_limit(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"Threshold values must be in \\[0, 1\\]. Received: \\[-1, 2\\]\",\n+        ):\n+            metrics.TruePositives(thresholds=[-1, 0.5, 2])\n+\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"Threshold values must be in \\[0, 1\\]. Received: \\[None\\]\",\n+        ):\n+            metrics.TruePositives(thresholds=[None])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3e62155446b6478a78865e2070929323dd066006", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 305 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 14 | Complexity Δ (Sum/Max): 30/21 | Churn Δ: 305 | Churn Cumulative: 323 | Contributors (this commit): 1 | Commits (past 90d): 14 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,3 +10,6 @@ from keras_core.layers.regularization.activity_regularization import (\n from keras_core.layers.regularization.dropout import Dropout\n from keras_core.layers.regularization.gaussian_dropout import GaussianDropout\n from keras_core.layers.regularization.gaussian_noise import GaussianNoise\n+from keras_core.layers.regularization.spatial_dropout import SpatialDropout1D\n+from keras_core.layers.regularization.spatial_dropout import SpatialDropout2D\n+from keras_core.layers.regularization.spatial_dropout import SpatialDropout3D\n\n@@ -0,0 +1,204 @@\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.input_spec import InputSpec\n+from keras_core.layers.regularization.dropout import Dropout\n+\n+\n+class BaseSpatialDropout(Dropout):\n+    def __init__(self, rate, seed=None, name=None, dtype=None):\n+        super().__init__(rate, seed=seed, name=name, dtype=dtype)\n+\n+    def call(self, inputs, training=False):\n+        if training and self.rate > 0:\n+            return backend.random.dropout(\n+                inputs,\n+                self.rate,\n+                noise_shape=self._get_noise_shape(inputs),\n+                seed=self.seed_generator,\n+            )\n+        return inputs\n+\n+    def get_config(self):\n+        return {\n+            \"rate\": self.rate,\n+            \"seed\": self.seed,\n+            \"name\": self.name,\n+            \"dtype\": self.dtype,\n+        }\n+\n+\n+@keras_core_export(\"keras_core.layers.SpatialDropout1D\")\n+class SpatialDropout1D(BaseSpatialDropout):\n+    \"\"\"Spatial 1D version of Dropout.\n+\n+    This layer performs the same function as Dropout, however, it drops\n+    entire 1D feature maps instead of individual elements. If adjacent frames\n+    within feature maps are strongly correlated (as is normally the case in\n+    early convolution layers) then regular dropout will not regularize the\n+    activations and will otherwise just result in an effective learning rate\n+    decrease. In this case, `SpatialDropout1D` will help promote independence\n+    between feature maps and should be used instead.\n+\n+    Args:\n+        rate: Float between 0 and 1. Fraction of the input units to drop.\n+\n+    Call arguments:\n+        inputs: A 3D tensor.\n+        training: Python boolean indicating whether the layer\n+            should behave in training mode (applying dropout)\n+            or in inference mode (pass-through).\n+\n+    Input shape:\n+        3D tensor with shape: `(samples, timesteps, channels)`\n+\n+    Output shape: Same as input.\n+\n+    Reference:\n+\n+    - [Tompson et al., 2014](https://arxiv.org/abs/1411.4280)\n+    \"\"\"\n+\n+    def __init__(self, rate, seed=None, name=None, dtype=None):\n+        super().__init__(rate, seed=seed, name=name, dtype=dtype)\n+        self.input_spec = InputSpec(ndim=3)\n+\n+    def _get_noise_shape(self, inputs):\n+        input_shape = ops.shape(inputs)\n+        return (input_shape[0], 1, input_shape[2])\n+\n+\n+@keras_core_export(\"keras_core.layers.SpatialDropout2D\")\n+class SpatialDropout2D(BaseSpatialDropout):\n+    \"\"\"Spatial 2D version of Dropout.\n+\n+    This version performs the same function as Dropout, however, it drops\n+    entire 2D feature maps instead of individual elements. If adjacent pixels\n+    within feature maps are strongly correlated (as is normally the case in\n+    early convolution layers) then regular dropout will not regularize the\n+    activations and will otherwise just result in an effective learning rate\n+    decrease. In this case, `SpatialDropout2D` will help promote independence\n+    between feature maps and should be used instead.\n+\n+    Args:\n+        rate: Float between 0 and 1. Fraction of the input units to drop.\n+        data_format: `\"channels_first\"` or `\"channels_last\"`.\n+            In `\"channels_first\"` mode, the channels dimension (the depth)\n+            is at index 1, in `\"channels_last\"` mode is it at index 3.\n+            It defaults to the `image_data_format` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be `\"channels_last\"`.\n+\n+    Call arguments:\n+        inputs: A 4D tensor.\n+        training: Python boolean indicating whether the layer\n+            should behave in training mode (applying dropout)\n+            or in inference mode (pass-through).\n+\n+    Input shape:\n+        4D tensor with shape: `(samples, channels, rows, cols)` if\n+            data_format='channels_first'\n+        or 4D tensor with shape: `(samples, rows, cols, channels)` if\n+            data_format='channels_last'.\n+\n+    Output shape: Same as input.\n+\n+    Reference:\n+\n+    - [Tompson et al., 2014](https://arxiv.org/abs/1411.4280)\n+    \"\"\"\n+\n+    def __init__(\n+        self, rate, data_format=None, seed=None, name=None, dtype=None\n+    ):\n+        super().__init__(rate, seed=seed, name=name, dtype=dtype)\n+        data_format = data_format or backend.image_data_format()\n+        if data_format not in {\"channels_last\", \"channels_first\"}:\n+            raise ValueError(\n+                '`data_format` must be \"channels_last\" or \"channels_first\". '\n+                f\"Received: data_format={data_format}.\"\n+            )\n+        self.data_format = data_format\n+        self.input_spec = InputSpec(ndim=4)\n+\n+    def _get_noise_shape(self, inputs):\n+        input_shape = ops.shape(inputs)\n+        if self.data_format == \"channels_first\":\n+            return (input_shape[0], input_shape[1], 1, 1)\n+        elif self.data_format == \"channels_last\":\n+            return (input_shape[0], 1, 1, input_shape[3])\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        config = {\n+            \"data_format\": self.data_format,\n+        }\n+        return {**base_config, **config}\n+\n+\n+@keras_core_export(\"keras_core.layers.SpatialDropout3D\")\n+class SpatialDropout3D(BaseSpatialDropout):\n+    \"\"\"Spatial 3D version of Dropout.\n+\n+    This version performs the same function as Dropout, however, it drops\n+    entire 3D feature maps instead of individual elements. If adjacent voxels\n+    within feature maps are strongly correlated (as is normally the case in\n+    early convolution layers) then regular dropout will not regularize the\n+    activations and will otherwise just result in an effective learning rate\n+    decrease. In this case, SpatialDropout3D will help promote independence\n+    between feature maps and should be used instead.\n+\n+    Args:\n+        rate: Float between 0 and 1. Fraction of the input units to drop.\n+        data_format: `\"channels_first\"` or `\"channels_last\"`.\n+            In `\"channels_first\"` mode, the channels dimension (the depth)\n+            is at index 1, in `\"channels_last\"` mode is it at index 4.\n+            It defaults to the `image_data_format` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be `\"channels_last\"`.\n+\n+    Call arguments:\n+        inputs: A 5D tensor.\n+        training: Python boolean indicating whether the layer\n+                should behave in training mode (applying dropout)\n+                or in inference mode (pass-through).\n+\n+    Input shape:\n+        5D tensor with shape: `(samples, channels, dim1, dim2, dim3)` if\n+            data_format='channels_first'\n+        or 5D tensor with shape: `(samples, dim1, dim2, dim3, channels)` if\n+            data_format='channels_last'.\n+\n+    Output shape: Same as input.\n+\n+    Reference:\n+\n+    - [Tompson et al., 2014](https://arxiv.org/abs/1411.4280)\n+    \"\"\"\n+\n+    def __init__(\n+        self, rate, data_format=None, seed=None, name=None, dtype=None\n+    ):\n+        super().__init__(rate, seed=seed, name=name, dtype=dtype)\n+        data_format = data_format or backend.image_data_format()\n+        if data_format not in {\"channels_last\", \"channels_first\"}:\n+            raise ValueError(\n+                '`data_format` must be \"channels_last\" or \"channels_first\". '\n+                f\"Received: data_format={data_format}.\"\n+            )\n+        self.data_format = data_format\n+        self.input_spec = InputSpec(ndim=5)\n+\n+    def _get_noise_shape(self, inputs):\n+        input_shape = ops.shape(inputs)\n+        if self.data_format == \"channels_first\":\n+            return (input_shape[0], input_shape[1], 1, 1, 1)\n+        elif self.data_format == \"channels_last\":\n+            return (input_shape[0], 1, 1, 1, input_shape[4])\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        config = {\n+            \"data_format\": self.data_format,\n+        }\n+        return {**base_config, **config}\n\n@@ -0,0 +1,98 @@\n+import numpy as np\n+import pytest\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core.testing import test_case\n+\n+\n+class SpatialDropoutTest(test_case.TestCase):\n+    def test_spatial_dropout_1d(self):\n+        self.run_layer_test(\n+            layers.SpatialDropout1D,\n+            init_kwargs={\"rate\": 0.5},\n+            call_kwargs={\"training\": True},\n+            input_shape=(2, 3, 4),\n+        )\n+\n+        self.run_layer_test(\n+            layers.SpatialDropout1D,\n+            init_kwargs={\"rate\": 0.5},\n+            call_kwargs={\"training\": False},\n+            input_shape=(2, 3, 4),\n+        )\n+\n+    def test_spatial_dropout_2d(self):\n+        self.run_layer_test(\n+            layers.SpatialDropout2D,\n+            init_kwargs={\"rate\": 0.5},\n+            call_kwargs={\"training\": True},\n+            input_shape=(2, 3, 4, 5),\n+        )\n+\n+        self.run_layer_test(\n+            layers.SpatialDropout2D,\n+            init_kwargs={\"rate\": 0.5, \"data_format\": \"channels_first\"},\n+            call_kwargs={\"training\": True},\n+            input_shape=(2, 3, 4, 5),\n+        )\n+\n+    def test_spatial_dropout_3d(self):\n+        self.run_layer_test(\n+            layers.SpatialDropout3D,\n+            init_kwargs={\"rate\": 0.5},\n+            call_kwargs={\"training\": True},\n+            input_shape=(2, 3, 4, 4, 5),\n+        )\n+\n+        self.run_layer_test(\n+            layers.SpatialDropout3D,\n+            init_kwargs={\"rate\": 0.5, \"data_format\": \"channels_first\"},\n+            call_kwargs={\"training\": True},\n+            input_shape=(2, 3, 4, 4, 5),\n+        )\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_spatial_dropout_1D_dynamic(self):\n+        inputs = layers.Input((3, 2))\n+        layer = layers.SpatialDropout1D(0.5)\n+        layer(inputs, training=True)\n+\n+    def test_spatial_dropout_1D_correctness(self):\n+        inputs = np.ones((10, 3, 10))\n+        layer = layers.SpatialDropout1D(0.5)\n+        outputs = layer(inputs, training=True)\n+        self.assertAllClose(outputs[:, 0, :], outputs[:, 1, :])\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_spatial_dropout_2D_dynamic(self):\n+        inputs = layers.Input((3, 2, 4))\n+        layer = layers.SpatialDropout2D(0.5)\n+        layer(inputs, training=True)\n+\n+    def test_spatial_dropout_2D_correctness(self):\n+        inputs = np.ones((10, 3, 3, 10))\n+        layer = layers.SpatialDropout2D(0.5)\n+        outputs = layer(inputs, training=True)\n+        self.assertAllClose(outputs[:, 0, 0, :], outputs[:, 1, 1, :])\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_spatial_dropout_3D_dynamic(self):\n+        inputs = layers.Input((3, 2, 4, 2))\n+        layer = layers.SpatialDropout3D(0.5)\n+        layer(inputs, training=True)\n+\n+    def test_spatial_dropout_3D_correctness(self):\n+        inputs = np.ones((10, 3, 3, 3, 10))\n+        layer = layers.SpatialDropout3D(0.5)\n+        outputs = layer(inputs, training=True)\n+        self.assertAllClose(outputs[:, 0, 0, 0, :], outputs[:, 1, 1, 1, :])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a2eddbaab2f145645af16111a779f47fe59c3e97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 335 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -23,7 +23,7 @@ class AdamW(optimizer.Optimizer):\n     Args:\n         learning_rate: A floating point value or a callable that takes no\n             arguments and returns the actual value to use. The learning rate.\n-            Defaults to `0.001`.\n+            Defaults to 0.001.\n         beta_1: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n             exponential decay rate for the 1st moment estimates.\n@@ -41,7 +41,8 @@ class AdamW(optimizer.Optimizer):\n             Defaults to `False`.\n         {{base_optimizer_keyword_args}}\n \n-    Reference:\n+    References:\n+\n     - [Loshchilov et al., 2019](https://arxiv.org/abs/1711.05101)\n     - [Kingma et al., 2014](http://arxiv.org/abs/1412.6980) for `adam`\n     - [Reddi et al., 2018](\n@@ -84,8 +85,8 @@ class AdamW(optimizer.Optimizer):\n \n         if self.weight_decay is None:\n             raise ValueError(\n-                \"Missing value of `weight_decay` which is required and\"\n-                \" must be a float value.\"\n+                \"Argument `weight_decay` must be a float. Received: \"\n+                \"weight_decay=None\"\n             )\n \n     def build(self, var_list):\n\n@@ -43,6 +43,7 @@ class RMSprop(optimizer.Optimizer):\n     9.683772\n \n     Reference:\n+\n     - [Hinton, 2012](\n         http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#125ef355e75603568cd1d773f49fcc106821c253", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 72 | Churn Cumulative: 72 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,45 @@\n+import random\n+\n+import numpy as np\n+import tensorflow as tf\n+\n+from keras_core.api_export import keras_core_export\n+\n+\n+@keras_core_export(\"keras_core.utils.set_random_seed\")\n+def set_random_seed(seed):\n+    \"\"\"Sets all random seeds (Python, NumPy, and backend framework, e.g. TF).\n+\n+    You can use this utility to make almost any Keras program fully\n+    deterministic. Some limitations apply in cases where network communications\n+    are involved (e.g. parameter server distribution), which creates additional\n+    sources of randomness, or when certain non-deterministic cuDNN ops are\n+    involved.\n+\n+    Calling this utility is equivalent to the following:\n+\n+    ```python\n+    import random\n+    import numpy as np\n+    import tensorflow as tf\n+    random.seed(seed)\n+    np.random.seed(seed)\n+    tf.random.set_seed(seed)\n+    ```\n+\n+    Note that the TensorFlow seed is set even if you're not using TensorFlow\n+    as your backend framework, since many workflows leverage `tf.data`\n+    pipelines (which feature random shuffling). Likewise many workflows\n+    might leverage NumPy APIs.\n+\n+    Arguments:\n+        seed: Integer, the random seed to use.\n+    \"\"\"\n+    if not isinstance(seed, int):\n+        raise ValueError(\n+            \"Expected `seed` argument to be an integer. \"\n+            f\"Received: seed={seed} (of type {type(seed)})\"\n+        )\n+    random.seed(seed)\n+    np.random.seed(seed)\n+    tf.random.set_seed(seed)\n\n@@ -0,0 +1,27 @@\n+import numpy as np\n+import tensorflow as tf\n+\n+import keras_core\n+from keras_core.testing import test_case\n+from keras_core.utils import rng_utils\n+\n+\n+class TestRandomSeedSetting(test_case.TestCase):\n+    def test_set_random_seed(self):\n+        def get_model_output():\n+            model = keras_core.Sequential(\n+                [\n+                    keras_core.layers.Dense(10),\n+                    keras_core.layers.Dropout(0.5),\n+                    keras_core.layers.Dense(10),\n+                ]\n+            )\n+            x = np.random.random((32, 10)).astype(\"float32\")\n+            ds = tf.data.Dataset.from_tensor_slices(x).shuffle(32).batch(16)\n+            return model.predict(ds)\n+\n+        rng_utils.set_random_seed(42)\n+        y1 = get_model_output()\n+        rng_utils.set_random_seed(42)\n+        y2 = get_model_output()\n+        self.assertAllClose(y1, y2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#38fda5a175f191fcfff7b8101367e3aa3c6ae4a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 10 | Files Changed: 4 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 74 | Churn Cumulative: 1783 | Contributors (this commit): 7 | Commits (past 90d): 36 | Contributors (cumulative): 15 | DMM Complexity: 0.3\n\nDIFF:\n@@ -4,7 +4,7 @@ from keras_core.api_export import keras_core_export\n from keras_core.losses.loss import Loss\n from keras_core.losses.loss import squeeze_to_same_rank\n from keras_core.saving import serialization_lib\n-from keras_core.utils.numerical_utils import l2_normalize\n+from keras_core.utils.numerical_utils import normalize\n \n \n class LossFunctionWrapper(Loss):\n@@ -579,6 +579,6 @@ def cosine_similarity(y_true, y_pred, axis=-1):\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n     y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n-    y_pred = l2_normalize(y_pred, axis=axis)\n-    y_true = l2_normalize(y_true, axis=axis)\n+    y_pred = normalize(y_pred, axis=axis)\n+    y_true = normalize(y_true, axis=axis)\n     return -ops.sum(y_true * y_pred, axis=axis)\n\n@@ -2,7 +2,7 @@ import math\n \n from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n-from keras_core.utils.numerical_utils import l2_normalize\n+from keras_core.utils.numerical_utils import normalize\n \n \n @keras_core_export(\n@@ -313,11 +313,11 @@ class OrthogonalRegularizer(Regularizer):\n                 f\"inputs.shape={inputs.shape}\"\n             )\n         if self.mode == \"rows\":\n-            inputs = l2_normalize(inputs, axis=1)\n+            inputs = normalize(inputs, axis=1)\n             product = ops.matmul(inputs, ops.transpose(inputs))\n             size = inputs.shape[0]\n         else:\n-            inputs = l2_normalize(inputs, axis=0)\n+            inputs = normalize(inputs, axis=0)\n             product = ops.matmul(ops.transpose(inputs), inputs)\n             size = inputs.shape[1]\n         product_no_diagonal = product * (\n\n@@ -5,11 +5,44 @@ from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n \n \n-def l2_normalize(x, axis=0):\n+@keras_core_export(\"keras_core.utils.normalize\")\n+def normalize(x, axis=-1, order=2):\n+    \"\"\"Normalizes an array.\n+\n+    If the input is a NumPy array, a NumPy array will be returned.\n+    If it's a backend tensor, a backend tensor will be returned.\n+\n+    Args:\n+        x: Array to normalize.\n+        axis: axis along which to normalize.\n+        order: Normalization order (e.g. `order=2` for L2 norm).\n+\n+    Returns:\n+        A normalized copy of the array.\n+    \"\"\"\n+    if not isinstance(order, int) or not order >= 1:\n+        raise ValueError(\n+            \"Argument `order` must be an int >= 1. \" f\"Received: order={order}\"\n+        )\n+    if isinstance(x, np.ndarray):\n+        # NumPy input\n+        norm = np.atleast_1d(np.linalg.norm(x, order, axis))\n+        norm[norm == 0] = 1\n+        return x / np.expand_dims(norm, axis)\n+\n+    # Backend tensor input\n+    if len(x.shape) == 0:\n+        x = ops.expand_dims(x, axis=0)\n     epsilon = backend.epsilon()\n-    square_sum = ops.sum(ops.square(x), axis=axis, keepdims=True)\n-    l2_norm = ops.reciprocal(ops.sqrt(ops.maximum(square_sum, epsilon)))\n-    return ops.multiply(x, l2_norm)\n+    if order == 2:\n+        power_sum = ops.sum(ops.square(x), axis=axis, keepdims=True)\n+        norm = ops.reciprocal(ops.sqrt(ops.maximum(power_sum, epsilon)))\n+    else:\n+        power_sum = ops.sum(ops.power(x, order), axis=axis, keepdims=True)\n+        norm = ops.reciprocal(\n+            ops.power(ops.maximum(power_sum, epsilon), 1.0 / order)\n+        )\n+    return ops.multiply(x, norm)\n \n \n @keras_core_export(\"keras_core.utils.to_categorical\")\n\n@@ -1,4 +1,5 @@\n import numpy as np\n+import tensorflow as tf\n from absl.testing import parameterized\n \n from keras_core import backend\n@@ -52,3 +53,23 @@ class TestNumericalUtils(testing.TestCase, parameterized.TestCase):\n         one_hot = numerical_utils.to_categorical(label, NUM_CLASSES)\n         assert backend.is_tensor(one_hot)\n         self.assertAllClose(one_hot, expected)\n+\n+    @parameterized.parameters([1, 2, 3])\n+    def test_normalize(self, order):\n+        xnp = np.random.random((3, 3))\n+        xb = backend.random.uniform((3, 3))\n+\n+        # Test NumPy\n+        out = numerical_utils.normalize(xnp, axis=-1, order=order)\n+        self.assertTrue(isinstance(out, np.ndarray))\n+        self.assertAllClose(\n+            tf.keras.utils.normalize(xnp, axis=-1, order=order), out\n+        )\n+\n+        # Test backend\n+        out = numerical_utils.normalize(xb, axis=-1, order=order)\n+        self.assertTrue(backend.is_tensor(out))\n+        self.assertAllClose(\n+            tf.keras.utils.normalize(np.array(xb), axis=-1, order=order),\n+            np.array(out),\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e2f6a35151a24cc86ccb17a5d78265a4cd379cf6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 120 | Lines Deleted: 117 | Files Changed: 11 | Hunks: 71 | Methods Changed: 39 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 237 | Churn Cumulative: 3574 | Contributors (this commit): 9 | Commits (past 90d): 115 | Contributors (cumulative): 33 | DMM Complexity: 0.8636363636363636\n\nDIFF:\n@@ -1,5 +1,4 @@\n-import threading\n-\n+from keras_core.backend import global_state\n from keras_core.backend.config import floatx\n from keras_core.utils.naming import auto_name\n \n@@ -301,22 +300,19 @@ class KerasVariable:\n         return value.__round__(ndigits)\n \n \n-GLOBAL_VARIABLE_TRACKER = threading.local()\n-\n-\n def register_uninitialized_variable(variable):\n-    global GLOBAL_VARIABLE_TRACKER\n-    if not hasattr(GLOBAL_VARIABLE_TRACKER, \"uninitialized_variables\"):\n-        GLOBAL_VARIABLE_TRACKER.uninitialized_variables = []\n-    GLOBAL_VARIABLE_TRACKER.uninitialized_variables.append(variable)\n+    uninitialized_variables = global_state.get_global_attribute(\n+        \"uninitialized_variables\", [], set_to_default=True\n+    )\n+    uninitialized_variables.append(variable)\n \n \n def initialize_all_variables():\n-    global GLOBAL_VARIABLE_TRACKER\n-    collection = getattr(GLOBAL_VARIABLE_TRACKER, \"uninitialized_variables\", [])\n+    collection = global_state.get_global_attribute(\"uninitialized_variables\")\n+    if collection:\n         for v in collection:\n             v._deferred_initialize()\n-    GLOBAL_VARIABLE_TRACKER.uninitialized_variables = []\n+    global_state.set_global_attribute(\"uninitialized_variables\", [])\n \n \n ALLOWED_DTYPES = {\n@@ -375,11 +371,8 @@ def is_float_dtype(dtype):\n     return dtype.startswith(\"float\") or dtype.startswith(\"bfloat\")\n \n \n-GLOBAL_AUTOCAST_TRACKER = threading.local()\n-\n-\n def get_autocast_scope():\n-    return getattr(GLOBAL_AUTOCAST_TRACKER, \"autocast_scope\", None)\n+    return global_state.get_global_attribute(\"autocast_scope\")\n \n \n class AutocastScope:\n@@ -409,7 +402,7 @@ class AutocastScope:\n \n     def __enter__(self):\n         self.original_scope = get_autocast_scope()\n-        GLOBAL_AUTOCAST_TRACKER.autocast_scope = self\n+        global_state.set_global_attribute(\"autocast_scope\", self)\n \n     def __exit__(self, *args, **kwargs):\n-        GLOBAL_AUTOCAST_TRACKER.autocast_scope = self.original_scope\n+        global_state.set_global_attribute(\"autocast_scope\", self.original_scope)\n\n@@ -1,5 +1,6 @@\n from tensorflow import nest\n \n+from keras_core.api_export import keras_core_export\n from keras_core.utils.naming import auto_name\n \n \n@@ -214,5 +215,15 @@ def any_symbolic_tensors(args=None, kwargs=None):\n     return False\n \n \n+@keras_core_export(\"keras_core.utils.is_keras_tensor\")\n def is_keras_tensor(x):\n+    \"\"\"Returns whether `x` is a Keras tensor.\n+\n+    A \"Keras tensor\" is a *symbolic tensor*, such as a tensor\n+    that was created via `Input()`. A \"symbolic tensor\"\n+    can be understood as a placeholder -- it does not\n+    contain any actual numerical data, only a shape and dtype.\n+    It can be used for building Functional models, but it\n+    cannot be used in actual computations.\n+    \"\"\"\n     return isinstance(x, KerasTensor)\n\n@@ -1,12 +1,7 @@\n-import threading\n-\n+from keras_core.backend import global_state\n from keras_core.backend.common.variables import KerasVariable\n from keras_core.backend.common.variables import initialize_all_variables\n \n-### Stateless context manager\n-\n-GLOBAL_SCOPE_TRACKER = threading.local()\n-\n \n class StatelessScope:\n     def __init__(self, state_mapping=None, collect_losses=False):\n@@ -36,7 +31,7 @@ class StatelessScope:\n \n     def __enter__(self):\n         self.original_scope = get_stateless_scope()\n-        GLOBAL_SCOPE_TRACKER.stateless_scope = self\n+        global_state.set_global_attribute(\"stateless_scope\", self)\n         return self\n \n     def add_loss(self, loss):\n@@ -50,7 +45,9 @@ class StatelessScope:\n         return self.state_mapping.get(id(variable), None)\n \n     def __exit__(self, *args, **kwargs):\n-        GLOBAL_SCOPE_TRACKER.stateless_scope = self.original_scope\n+        global_state.set_global_attribute(\n+            \"stateless_scope\", self.original_scope\n+        )\n         if self.original_scope is None:\n             # We're back in eager scope;\n             # if any variables were created within the stateless\n@@ -59,8 +56,8 @@ class StatelessScope:\n \n \n def in_stateless_scope():\n-    return getattr(GLOBAL_SCOPE_TRACKER, \"stateless_scope\", None) is not None\n+    return global_state.get_global_attribute(\"stateless_scope\") is not None\n \n \n def get_stateless_scope():\n-    return getattr(GLOBAL_SCOPE_TRACKER, \"stateless_scope\", None)\n+    return global_state.get_global_attribute(\"stateless_scope\")\n\n@@ -17,7 +17,6 @@ And some more magic:\n \"\"\"\n import collections\n import inspect\n-import threading\n import warnings\n \n import numpy as np\n@@ -30,6 +29,7 @@ from keras_core import regularizers\n from keras_core import utils\n from keras_core.api_export import keras_core_export\n from keras_core.backend import KerasTensor\n+from keras_core.backend import global_state\n from keras_core.layers import input_spec\n from keras_core.metrics.metric import Metric\n from keras_core.operations.operation import Operation\n@@ -757,20 +757,20 @@ class Layer(Operation):\n \n     def _get_call_context(self):\n         \"\"\"Returns currently active `CallContext`.\"\"\"\n-        global CALL_CTX\n-        call_ctx = getattr(CALL_CTX, \"current\", None)\n-        if call_ctx is None:\n+        layer_call_ctx = global_state.get_global_attribute(\"current_call_ctx\")\n+        if layer_call_ctx is None:\n             # Enter new call context.\n-            call_ctx = CallContext(entry_layer=self)\n-            CALL_CTX.current = call_ctx\n+            layer_call_ctx = CallContext(entry_layer=self)\n+            global_state.set_global_attribute(\n+                \"current_call_ctx\", layer_call_ctx\n+            )\n             self._clear_losses()\n-        return call_ctx\n+        return layer_call_ctx\n \n     def _maybe_reset_call_context(self):\n-        global CALL_CTX\n-        call_ctx = getattr(CALL_CTX, \"current\", None)\n-        if call_ctx is None or call_ctx.entry_layer == self:\n-            CALL_CTX.current = None\n+        layer_call_ctx = global_state.get_global_attribute(\"current_call_ctx\")\n+        if layer_call_ctx is None or layer_call_ctx.entry_layer == self:\n+            global_state.set_global_attribute(\"current_call_ctx\", None)\n \n     def _flatten_layers(self, include_self=True, recursive=True):\n         layers = []\n@@ -920,9 +920,6 @@ def check_build_signature(build_fn, shapes_dict):\n         )\n \n \n-CALL_CTX = threading.local()\n-\n-\n class CallContext:\n     def __init__(self, entry_layer):\n         self.entry_layer = entry_layer\n\n@@ -1,9 +1,6 @@\n-import threading\n-\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n-\n-GLOBAL_DTYPE_POLICY_TRACKER = threading.local()\n+from keras_core.backend import global_state\n \n \n @keras_core_export(\"keras_core.mixed_precision.DTypePolicy\")\n@@ -98,7 +95,6 @@ class DTypePolicy:\n \n @keras_core_export(\"keras_core.mixed_precision.set_dtype_policy\")\n def set_dtype_policy(policy):\n-    global GLOBAL_DTYPE_POLICY_TRACKER\n     if not isinstance(policy, DTypePolicy):\n         if isinstance(policy, str):\n             policy = DTypePolicy(policy)\n@@ -110,12 +106,12 @@ def set_dtype_policy(policy):\n                 f\"instance. Received: policy={policy} \"\n                 f\"(of type {type(policy)})\"\n             )\n-    GLOBAL_DTYPE_POLICY_TRACKER.dtype_policy = policy\n+    global_state.set_global_setting(\"dtype_policy\", policy)\n \n \n @keras_core_export(\"keras_core.mixed_precision.dtype_policy\")\n def dtype_policy():\n-    policy = getattr(GLOBAL_DTYPE_POLICY_TRACKER, \"dtype_policy\", None)\n+    policy = global_state.get_global_setting(\"dtype_policy\", None)\n     if policy is None:\n         policy = DTypePolicy(backend.floatx())\n         set_dtype_policy(policy)\n\n@@ -1,12 +1,10 @@\n import inspect\n-import threading\n \n from keras_core.api_export import keras_core_export\n+from keras_core.backend import global_state\n \n-_GLOBAL_CUSTOM_OBJECTS = {}\n-_GLOBAL_CUSTOM_NAMES = {}\n-# Thread-local custom objects set by custom_object_scope.\n-_THREAD_LOCAL_CUSTOM_OBJECTS = threading.local()\n+GLOBAL_CUSTOM_OBJECTS = {}\n+GLOBAL_CUSTOM_NAMES = {}\n \n \n @keras_core_export(\n@@ -46,13 +44,18 @@ class CustomObjectScope:\n         self.backup = None\n \n     def __enter__(self):\n-        self.backup = _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.copy()\n-        _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.update(self.custom_objects)\n+        self.backup = global_state.get_global_attribute(\n+            \"custom_objects_scope_dict\", {}\n+        ).copy()\n+        global_state.set_global_attribute(\n+            \"custom_objects_scope_dict\", self.custom_objects.copy()\n+        )\n         return self\n \n     def __exit__(self, *args, **kwargs):\n-        _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.clear()\n-        _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.update(self.backup)\n+        global_state.set_global_attribute(\n+            \"custom_objects_scope_dict\", self.backup.copy()\n+        )\n \n \n # Alias.\n@@ -79,7 +82,7 @@ def get_custom_objects():\n     Returns:\n         Global dictionary mapping registered class names to classes.\n     \"\"\"\n-    return _GLOBAL_CUSTOM_OBJECTS\n+    return GLOBAL_CUSTOM_OBJECTS\n \n \n @keras_core_export(\n@@ -108,10 +111,8 @@ def register_keras_serializable(package=\"Custom\", name=None):\n     class MyDense(keras_core.layers.Dense):\n         pass\n \n-    assert keras_core.saving.get_registered_object(\n-        'my_package>MyDense') == MyDense\n-    assert keras_core.saving.get_registered_name(\n-        MyDense) == 'my_package>MyDense'\n+    assert get_registered_object('my_package>MyDense') == MyDense\n+    assert get_registered_name(MyDense) == 'my_package>MyDense'\n     ```\n \n     Args:\n@@ -138,8 +139,8 @@ def register_keras_serializable(package=\"Custom\", name=None):\n                 \"get_config() method.\"\n             )\n \n-        _GLOBAL_CUSTOM_OBJECTS[registered_name] = arg\n-        _GLOBAL_CUSTOM_NAMES[arg] = registered_name\n+        GLOBAL_CUSTOM_OBJECTS[registered_name] = arg\n+        GLOBAL_CUSTOM_NAMES[arg] = registered_name\n \n         return arg\n \n@@ -161,8 +162,8 @@ def get_registered_name(obj):\n         The name associated with the object, or the default Python name if the\n             object is not registered.\n     \"\"\"\n-    if obj in _GLOBAL_CUSTOM_NAMES:\n-        return _GLOBAL_CUSTOM_NAMES[obj]\n+    if obj in GLOBAL_CUSTOM_NAMES:\n+        return GLOBAL_CUSTOM_NAMES[obj]\n     else:\n         return obj.__name__\n \n@@ -196,10 +197,13 @@ def get_registered_object(name, custom_objects=None, module_objects=None):\n         An instantiable class associated with `name`, or `None` if no such class\n             exists.\n     \"\"\"\n-    if name in _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__:\n-        return _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__[name]\n-    elif name in _GLOBAL_CUSTOM_OBJECTS:\n-        return _GLOBAL_CUSTOM_OBJECTS[name]\n+    custom_objects_scope_dict = global_state.get_global_attribute(\n+        \"custom_objects_scope_dict\", {}\n+    )\n+    if name in custom_objects_scope_dict:\n+        return custom_objects_scope_dict[name]\n+    elif name in GLOBAL_CUSTOM_OBJECTS:\n+        return GLOBAL_CUSTOM_OBJECTS[name]\n     elif custom_objects and name in custom_objects:\n         return custom_objects[name]\n     elif module_objects and name in module_objects:\n\n@@ -47,7 +47,7 @@ class TestObjectRegistration(testing.TestCase):\n \n         serialized_name = \"Custom>TestClass\"\n         inst = TestClass(value=10)\n-        class_name = object_registration._GLOBAL_CUSTOM_NAMES[TestClass]\n+        class_name = object_registration.GLOBAL_CUSTOM_NAMES[TestClass]\n         self.assertEqual(serialized_name, class_name)\n \n         config = serialization_lib.serialize_keras_object(inst)\n@@ -74,7 +74,7 @@ class TestObjectRegistration(testing.TestCase):\n \n         serialized_name = \"TestPackage>CustomName\"\n         inst = OtherTestClass(val=5)\n-        class_name = object_registration._GLOBAL_CUSTOM_NAMES[OtherTestClass]\n+        class_name = object_registration.GLOBAL_CUSTOM_NAMES[OtherTestClass]\n         self.assertEqual(serialized_name, class_name)\n         fn_class_name = object_registration.get_registered_name(OtherTestClass)\n         self.assertEqual(fn_class_name, class_name)\n@@ -95,7 +95,7 @@ class TestObjectRegistration(testing.TestCase):\n             return 42\n \n         serialized_name = \"Custom>my_fn\"\n-        class_name = object_registration._GLOBAL_CUSTOM_NAMES[my_fn]\n+        class_name = object_registration.GLOBAL_CUSTOM_NAMES[my_fn]\n         self.assertEqual(serialized_name, class_name)\n         fn_class_name = object_registration.get_registered_name(my_fn)\n         self.assertEqual(fn_class_name, class_name)\n\n@@ -2,7 +2,6 @@\n \n import importlib\n import inspect\n-import threading\n import types\n import warnings\n \n@@ -13,12 +12,11 @@ import tensorflow as tf\n from keras_core import api_export\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n+from keras_core.backend import global_state\n from keras_core.saving import object_registration\n from keras_core.utils import python_utils\n \n PLAIN_TYPES = (str, int, float, bool)\n-SHARED_OBJECTS = threading.local()\n-SAFE_MODE = threading.local()\n \n # List of Keras modules with built-in string representations for Keras defaults\n BUILTIN_MODULES = (\n@@ -48,61 +46,70 @@ class SafeModeScope:\n \n     def __enter__(self):\n         self.original_value = in_safe_mode()\n-        SAFE_MODE.safe_mode = self.safe_mode\n+        global_state.set_global_setting(\"safe_mode_saving\", self.safe_mode)\n \n     def __exit__(self, *args, **kwargs):\n-        SAFE_MODE.safe_mode = self.original_value\n+        global_state.set_global_setting(\"safe_mode_saving\", self.original_value)\n \n \n def enable_unsafe_deserialization():\n     \"\"\"Disables safe mode globally, allowing deserialization of lambdas.\"\"\"\n-    SAFE_MODE.safe_mode = False\n+    global_state.set_global_setting(\"safe_mode_saving\", False)\n \n \n def in_safe_mode():\n-    return getattr(SAFE_MODE, \"safe_mode\", None)\n+    return global_state.get_global_setting(\"safe_mode_saving\")\n \n \n class ObjectSharingScope:\n     \"\"\"Scope to enable detection and reuse of previously seen objects.\"\"\"\n \n     def __enter__(self):\n-        SHARED_OBJECTS.enabled = True\n-        SHARED_OBJECTS.id_to_obj_map = {}\n-        SHARED_OBJECTS.id_to_config_map = {}\n+        global_state.set_global_attribute(\"shared_objects/id_to_obj_map\", {})\n+        global_state.set_global_attribute(\"shared_objects/id_to_config_map\", {})\n \n     def __exit__(self, *args, **kwargs):\n-        SHARED_OBJECTS.enabled = False\n-        SHARED_OBJECTS.id_to_obj_map = {}\n-        SHARED_OBJECTS.id_to_config_map = {}\n+        global_state.set_global_attribute(\"shared_objects/id_to_obj_map\", None)\n+        global_state.set_global_attribute(\n+            \"shared_objects/id_to_config_map\", None\n+        )\n \n \n def get_shared_object(obj_id):\n     \"\"\"Retrieve an object previously seen during deserialization.\"\"\"\n-    if getattr(SHARED_OBJECTS, \"enabled\", False):\n-        return SHARED_OBJECTS.id_to_obj_map.get(obj_id, None)\n+    id_to_obj_map = global_state.get_global_attribute(\n+        \"shared_objects/id_to_obj_map\"\n+    )\n+    if id_to_obj_map is not None:\n+        return id_to_obj_map.get(obj_id, None)\n \n \n def record_object_after_serialization(obj, config):\n     \"\"\"Call after serializing an object, to keep track of its config.\"\"\"\n     if config[\"module\"] == \"__main__\":\n         config[\"module\"] = None  # Ensures module is None when no module found\n-    if not getattr(SHARED_OBJECTS, \"enabled\", False):\n+    id_to_config_map = global_state.get_global_attribute(\n+        \"shared_objects/id_to_config_map\"\n+    )\n+    if id_to_config_map is None:\n         return  # Not in a sharing scope\n     obj_id = int(id(obj))\n-    if obj_id not in SHARED_OBJECTS.id_to_config_map:\n-        SHARED_OBJECTS.id_to_config_map[obj_id] = config\n+    if obj_id not in id_to_config_map:\n+        id_to_config_map[obj_id] = config\n     else:\n         config[\"shared_object_id\"] = obj_id\n-        prev_config = SHARED_OBJECTS.id_to_config_map[obj_id]\n+        prev_config = id_to_config_map[obj_id]\n         prev_config[\"shared_object_id\"] = obj_id\n \n \n def record_object_after_deserialization(obj, obj_id):\n     \"\"\"Call after deserializing an object, to keep track of it in the future.\"\"\"\n-    if not getattr(SHARED_OBJECTS, \"enabled\", False):\n+    id_to_obj_map = global_state.get_global_attribute(\n+        \"shared_objects/id_to_obj_map\"\n+    )\n+    if id_to_obj_map is None:\n         return  # Not in a sharing scope\n-    SHARED_OBJECTS.id_to_obj_map[obj_id] = obj\n+    id_to_obj_map[obj_id] = obj\n \n \n @keras_core_export(\"keras_core.saving.serialize_keras_object\")\n@@ -467,8 +474,8 @@ def deserialize_keras_object(\n \n     module_objects = kwargs.pop(\"module_objects\", None)\n     custom_objects = custom_objects or {}\n-    tlco = object_registration._THREAD_LOCAL_CUSTOM_OBJECTS.__dict__\n-    gco = object_registration._GLOBAL_CUSTOM_OBJECTS\n+    tlco = global_state.get_global_attribute(\"custom_objects_scope_dict\", {})\n+    gco = object_registration.GLOBAL_CUSTOM_OBJECTS\n     custom_objects = {**custom_objects, **tlco, **gco}\n \n     if config is None:\n\n@@ -1,12 +1,9 @@\n import sys\n-import threading\n \n from absl import logging\n \n from keras_core.api_export import keras_core_export\n-\n-INTERACTIVE_LOGGING = threading.local()\n-INTERACTIVE_LOGGING.enable = True\n+from keras_core.backend import global_state\n \n \n @keras_core_export(\"keras_core.utils.enable_interactive_logging\")\n@@ -17,7 +14,7 @@ def enable_interactive_logging():\n     This provides the best experience when using Keras in an interactive\n     environment such as a shell or a notebook.\n     \"\"\"\n-    INTERACTIVE_LOGGING.enable = True\n+    global_state.set_global_setting(\"interactive_logging\", True)\n \n \n @keras_core_export(\"keras_core.utils.disable_interactive_logging\")\n@@ -28,7 +25,7 @@ def disable_interactive_logging():\n     This is the best option when using Keras in a non-interactive\n     way, such as running a training or inference job on a server.\n     \"\"\"\n-    INTERACTIVE_LOGGING.enable = False\n+    global_state.set_global_setting(\"interactive_logging\", False)\n \n \n @keras_core_export(\"keras_core.utils.is_interactive_logging_enabled\")\n@@ -42,9 +39,7 @@ def is_interactive_logging_enabled():\n     Returns:\n         Boolean (True if interactive logging is enabled and False otherwise).\n     \"\"\"\n-    # Use `getattr` in case `INTERACTIVE_LOGGING`\n-    # does not have the `enable` attribute.\n-    return getattr(INTERACTIVE_LOGGING, \"enable\", True)\n+    return global_state.get_global_setting(\"interactive_logging\", True)\n \n \n def print_msg(message, line_break=True):\n\n@@ -1,7 +1,7 @@\n import collections\n import re\n \n-OBJECT_NAME_UIDS = collections.defaultdict(int)\n+from keras_core.backend.global_state import get_global_attribute\n \n \n def auto_name(prefix):\n@@ -10,12 +10,16 @@ def auto_name(prefix):\n \n \n def uniquify(name):\n-    global OBJECT_NAME_UIDS\n-    if name in OBJECT_NAME_UIDS:\n-        unique_name = f\"{name}_{OBJECT_NAME_UIDS[name]}\"\n+    object_name_uids = get_global_attribute(\n+        \"object_name_uids\",\n+        default=collections.defaultdict(int),\n+        set_to_default=True,\n+    )\n+    if name in object_name_uids:\n+        unique_name = f\"{name}_{object_name_uids[name]}\"\n     else:\n         unique_name = name\n-    OBJECT_NAME_UIDS[name] += 1\n+    object_name_uids[name] += 1\n     return unique_name\n \n \n\n@@ -1,19 +1,18 @@\n-import threading\n-\n-GLOBAL_SCOPE_TRACKER = threading.local()\n+from keras_core.backend.global_state import get_global_attribute\n+from keras_core.backend.global_state import set_global_attribute\n \n \n class DotNotTrackScope:\n     def __enter__(self):\n         self.original_value = is_tracking_enabled()\n-        GLOBAL_SCOPE_TRACKER.tracking_on = False\n+        set_global_attribute(\"tracking_on\", False)\n \n     def __exit__(self, *args, **kwargs):\n-        GLOBAL_SCOPE_TRACKER.tracking_on = self.original_value\n+        set_global_attribute(\"tracking_on\", self.original_value)\n \n \n def is_tracking_enabled():\n-    return getattr(GLOBAL_SCOPE_TRACKER, \"tracking_on\", True)\n+    return get_global_attribute(\"tracking_on\", True)\n \n \n def no_automatic_dependency_tracking(fn):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
