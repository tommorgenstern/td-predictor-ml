{"custom_id": "keras#49e415f0ff0d6a6309281710f650457ce8e65207", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 20 | Files Changed: 5 | Hunks: 32 | Methods Changed: 13 | Complexity Δ (Sum/Max): 14/10 | Churn Δ: 93 | Churn Cumulative: 2777 | Contributors (this commit): 7 | Commits (past 90d): 92 | Contributors (cumulative): 23 | DMM Complexity: 0.603448275862069\n\nDIFF:\n@@ -74,6 +74,7 @@ class JAXTrainer(base_trainer.Trainer):\n             steps_per_epoch=steps_per_epoch,\n             shuffle=shuffle,\n             class_weight=class_weight,\n+            steps_per_execution=self.steps_per_execution,\n         )\n \n         compile_metrics_unbuilt = (\n@@ -83,6 +84,7 @@ class JAXTrainer(base_trainer.Trainer):\n         if not self.built or compile_metrics_unbuilt:\n             # Build the model on one batch of data.\n             for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+                data = data[0]\n                 (\n                     x,\n                     y,\n@@ -168,14 +170,21 @@ class JAXTrainer(base_trainer.Trainer):\n             )\n             return logs, state\n \n+        def _train_multi_step(state, data):\n+            for single_step_data in data:\n+                logs, state = _train_step(state, single_step_data)\n+            return logs, state\n+\n         if not self.run_eagerly and self.jit_compile:\n \n             @jax.jit\n             def train_step(state, data):\n-                return _train_step(state, data)\n+                if self.steps_per_execution > 1:\n+                    return _train_multi_step(state, data)\n+                return _train_step(state, data[0])\n \n         else:\n-            train_step = _train_step\n+            train_step = _train_multi_step\n \n         self.stop_training = False\n         callbacks.on_train_begin()\n@@ -239,6 +248,7 @@ class JAXTrainer(base_trainer.Trainer):\n                         y=val_y,\n                         sample_weight=val_sample_weight,\n                         batch_size=validation_batch_size or batch_size,\n+                        steps_per_execution=self.steps_per_execution,\n                     )\n                 val_logs = self.evaluate(\n                     x=val_x,\n@@ -300,11 +310,13 @@ class JAXTrainer(base_trainer.Trainer):\n                 batch_size=batch_size,\n                 steps_per_epoch=steps,\n                 shuffle=False,\n+                steps_per_execution=self.steps_per_execution,\n             )\n \n         if not self.built:\n             # Build the model on one batch of data.\n             for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+                data = data[0]\n                 (\n                     x,\n                     y,\n@@ -374,14 +386,21 @@ class JAXTrainer(base_trainer.Trainer):\n             )\n             return logs, state\n \n+        def _test_multi_step(state, data):\n+            for single_step_data in data:\n+                logs, state = _test_step(state, single_step_data)\n+            return logs, state\n+\n         if not self.run_eagerly and self.jit_compile:\n \n             @jax.jit\n             def test_step(state, data):\n-                return _test_step(state, data)\n+                if self.steps_per_execution > 1:\n+                    return _test_multi_step(state, data)\n+                return _test_step(state, data[0])\n \n         else:\n-            test_step = _test_step\n+            test_step = _test_multi_step\n \n         callbacks.on_test_begin()\n         logs = None\n@@ -430,13 +449,14 @@ class JAXTrainer(base_trainer.Trainer):\n             batch_size=batch_size,\n             steps_per_epoch=steps,\n             shuffle=False,\n+            steps_per_execution=self.steps_per_execution,\n         )\n \n         if not self.built:\n             # Build the model on one batch of data.\n             for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n                 # Build model\n-                self(data)\n+                self(data[0])\n                 break\n \n         # Container that configures and calls callbacks.\n@@ -451,18 +471,36 @@ class JAXTrainer(base_trainer.Trainer):\n                 model=self,\n             )\n \n+        def _predict_multi_step(\n+            trainable_variables, non_trainable_variables, data\n+        ):\n+            return [\n+                self.stateless_call(\n+                    trainable_variables,\n+                    non_trainable_variables,\n+                    single_step_data,\n+                )\n+                for single_step_data in data\n+            ]\n+\n         if not self.run_eagerly and self.jit_compile:\n \n             @jax.jit\n             def predict_step(\n                 trainable_variables, non_trainable_variables, data\n             ):\n-                return self.stateless_call(\n+                if self.steps_per_execution > 1:\n+                    return _predict_multi_step(\n                         trainable_variables, non_trainable_variables, data\n                     )\n+                return [\n+                    self.stateless_call(\n+                        trainable_variables, non_trainable_variables, data[0]\n+                    )\n+                ]\n \n         else:\n-            predict_step = self.stateless_call\n+            predict_step = _predict_multi_step\n \n         callbacks.on_predict_begin()\n \n@@ -471,9 +509,10 @@ class JAXTrainer(base_trainer.Trainer):\n         outputs = None\n         for step, x in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n             callbacks.on_predict_batch_begin(step)\n-            batch_outputs, non_trainable_variables = predict_step(\n+            multi_step_return_values = predict_step(\n                 trainable_variables, non_trainable_variables, x\n             )\n+            for batch_outputs, _ in multi_step_return_values:\n                 if outputs is None:\n                     outputs = tf.nest.map_structure(\n                         lambda batch_output: [batch_output],\n@@ -482,7 +521,9 @@ class JAXTrainer(base_trainer.Trainer):\n                 else:\n                     tf.__internal__.nest.map_structure_up_to(\n                         batch_outputs,\n-                    lambda output, batch_output: output.append(batch_output),\n+                        lambda output, batch_output: output.append(\n+                            batch_output\n+                        ),\n                         outputs,\n                         batch_outputs,\n                     )\n\n@@ -142,16 +142,21 @@ class EpochIterator:\n         return iterator\n \n     def enumerate_epoch(self, return_type=\"np\"):\n+        buffer = []\n         if self.steps_per_epoch:\n             if not self._current_iterator:\n                 self._current_iterator = self._get_iterator(return_type)\n                 self._insufficient_data = False\n+\n             for step in range(self.steps_per_epoch):\n                 if self._insufficient_data:\n                     break\n                 try:\n                     data = next(self._current_iterator)\n-                    yield step, data\n+                    buffer.append(data)\n+                    if len(buffer) == self.steps_per_execution:\n+                        yield step - len(buffer) + 1, buffer\n+                        buffer = []\n                 except (StopIteration, tf.errors.OutOfRangeError):\n                     warnings.warn(\n                         \"Your input ran out of data; interrupting epoch. \"\n@@ -163,9 +168,16 @@ class EpochIterator:\n                     )\n                     self._current_iterator = None\n                     self._insufficient_data = True\n+            if buffer:\n+                yield step - len(buffer) + 1, buffer\n         else:\n             for step, data in enumerate(self._get_iterator(return_type)):\n-                yield step, data\n+                buffer.append(data)\n+                if len(buffer) == self.steps_per_execution:\n+                    yield step - len(buffer) + 1, buffer\n+                    buffer = []\n+            if buffer:\n+                yield step - len(buffer) + 1, buffer\n             if not self._num_batches:\n                 # Infer the number of batches returned by the data_adater.\n                 # Assumed static.\n\n@@ -22,6 +22,7 @@ class TestEpochIterator(testing.TestCase):\n         )\n         steps_seen = []\n         for step, batch in iterator.enumerate_epoch(return_type=return_type):\n+            batch = batch[0]\n             steps_seen.append(step)\n             self.assertEqual(len(batch), 3)\n             if return_type == \"np\":\n\n@@ -16,6 +16,7 @@ class Trainer:\n         self._run_eagerly = False\n         self._jit_compile = True\n         self.compiled = False\n+        self.steps_per_execution = 1\n \n     @tracking.no_automatic_dependency_tracking\n     def compile(\n\n@@ -1,5 +1,4 @@\n import numpy as np\n-import pytest\n \n from keras_core import backend\n from keras_core import initializers\n@@ -216,11 +215,6 @@ class TestTrainer(testing.TestCase):\n     def test_predict_flow_jit(self):\n         self._test_predict_flow(run_eagerly=False, jit_compile=True)\n \n-    # TODO: Remove the skipif when implemented steps_per_execution for JAX.\n-    @pytest.mark.skipif(\n-        backend.backend() != \"tensorflow\",\n-        reason=\"JAX does not support steps_per_execution yet\",\n-    )\n     def test_steps_per_execution_steps_count(self):\n         class StepCount(Callback):\n             def __init__(self):\n@@ -235,12 +229,16 @@ class TestTrainer(testing.TestCase):\n         x = np.ones((100, 4))\n         y = np.ones((100, 1))\n         model = ExampleModel(units=1)\n-        model.compile(loss=\"mse\", optimizer=\"adam\", steps_per_execution=3)\n+        model.compile(\n+            loss=\"mse\",\n+            optimizer=\"adam\",\n+            steps_per_execution=3,\n+        )\n         step_count = StepCount()\n-        model.fit(x=x, y=y, batch_size=16, callbacks=[step_count])\n+        model.fit(x=x, y=y, batch_size=16, callbacks=[step_count], verbose=0)\n         self.assertEqual(step_count.count, 3)\n \n         model_2 = ExampleModel(units=1)\n         model_2.compile(loss=\"mse\", optimizer=\"adam\", steps_per_execution=1)\n-        model_2.fit(x=x, y=y, batch_size=16)\n+        model_2.fit(x=x, y=y, batch_size=16, verbose=0)\n         self.assertAllClose(model.get_weights(), model_2.get_weights())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f687b0b1f712d27972f56cccb8bfd3f612324847", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 19 | Files Changed: 8 | Hunks: 16 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 54 | Churn Cumulative: 2807 | Contributors (this commit): 5 | Commits (past 90d): 47 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,18 @@\n+from keras_core import activations\n from keras_core import backend\n+from keras_core import constraints\n+from keras_core import datasets\n+from keras_core import initializers\n from keras_core import layers\n from keras_core import models\n from keras_core import operations\n+from keras_core import optimizers\n+from keras_core import regularizers\n+from keras_core import utils\n from keras_core.layers import Input\n from keras_core.layers import Layer\n from keras_core.models import Functional\n from keras_core.models import Model\n from keras_core.models import Sequential\n+\n+__version__ = \"0.0.1\"\n\n@@ -1 +1,10 @@\n \"\"\"Small NumPy datasets for debugging/testing.\"\"\"\n+\n+from keras_core.datasets import boston_housing\n+from keras_core.datasets import california_housing\n+from keras_core.datasets import cifar10\n+from keras_core.datasets import cifar100\n+from keras_core.datasets import fashion_mnist\n+from keras_core.datasets import imdb\n+from keras_core.datasets import mnist\n+from keras_core.datasets import reuters\n\n@@ -9,7 +9,7 @@ from keras_core.backend import standardize_data_format\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n from keras_core.operations.operation_utils import compute_conv_output_shape\n-from keras_core.utils.argument_validation import normalize_tuple\n+from keras_core.utils.argument_validation import standardize_tuple\n \n \n class BaseConv(Layer):\n@@ -104,9 +104,9 @@ class BaseConv(Layer):\n         self.rank = rank\n         self.filters = filters\n         self.groups = groups or 1\n-        self.kernel_size = normalize_tuple(kernel_size, rank, \"kernel_size\")\n-        self.strides = normalize_tuple(strides, rank, \"strides\")\n-        self.dilation_rate = normalize_tuple(\n+        self.kernel_size = standardize_tuple(kernel_size, rank, \"kernel_size\")\n+        self.strides = standardize_tuple(strides, rank, \"strides\")\n+        self.dilation_rate = standardize_tuple(\n             dilation_rate, rank, \"dilation_rate\"\n         )\n         self.padding = padding\n\n@@ -11,7 +11,7 @@ from keras_core.backend.common.backend_utils import (\n )\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n-from keras_core.utils.argument_validation import normalize_tuple\n+from keras_core.utils.argument_validation import standardize_tuple\n \n \n class BaseConvTranspose(Layer):\n@@ -97,9 +97,9 @@ class BaseConvTranspose(Layer):\n         )\n         self.rank = rank\n         self.filters = filters\n-        self.kernel_size = normalize_tuple(kernel_size, rank, \"kernel_size\")\n-        self.strides = normalize_tuple(strides, rank, \"strides\")\n-        self.dilation_rate = normalize_tuple(\n+        self.kernel_size = standardize_tuple(kernel_size, rank, \"kernel_size\")\n+        self.strides = standardize_tuple(strides, rank, \"strides\")\n+        self.dilation_rate = standardize_tuple(\n             dilation_rate, rank, \"dilation_rate\"\n         )\n         self.padding = padding\n\n@@ -9,7 +9,7 @@ from keras_core.backend import standardize_data_format\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n from keras_core.operations.operation_utils import compute_conv_output_shape\n-from keras_core.utils.argument_validation import normalize_tuple\n+from keras_core.utils.argument_validation import standardize_tuple\n \n \n class BaseDepthwiseConv(Layer):\n@@ -109,9 +109,9 @@ class BaseDepthwiseConv(Layer):\n         )\n         self.rank = rank\n         self.depth_multiplier = depth_multiplier\n-        self.kernel_size = normalize_tuple(kernel_size, rank, \"kernel_size\")\n-        self.strides = normalize_tuple(strides, rank, \"strides\")\n-        self.dilation_rate = normalize_tuple(\n+        self.kernel_size = standardize_tuple(kernel_size, rank, \"kernel_size\")\n+        self.strides = standardize_tuple(strides, rank, \"strides\")\n+        self.dilation_rate = standardize_tuple(\n             dilation_rate, rank, \"dilation_rate\"\n         )\n         self.padding = padding\n\n@@ -10,7 +10,7 @@ from keras_core.backend import standardize_data_format\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n from keras_core.operations.operation_utils import compute_conv_output_shape\n-from keras_core.utils.argument_validation import normalize_tuple\n+from keras_core.utils.argument_validation import standardize_tuple\n \n \n class BaseSeparableConv(Layer):\n@@ -110,9 +110,9 @@ class BaseSeparableConv(Layer):\n         self.rank = rank\n         self.depth_multiplier = depth_multiplier\n         self.filters = filters\n-        self.kernel_size = normalize_tuple(kernel_size, rank, \"kernel_size\")\n-        self.strides = normalize_tuple(strides, rank, \"strides\")\n-        self.dilation_rate = normalize_tuple(\n+        self.kernel_size = standardize_tuple(kernel_size, rank, \"kernel_size\")\n+        self.strides = standardize_tuple(strides, rank, \"strides\")\n+        self.dilation_rate = standardize_tuple(\n             dilation_rate, rank, \"dilation_rate\"\n         )\n         self.padding = padding\n\n@@ -126,7 +126,6 @@ class CategoryEncoding(Layer):\n         return {**base_config, **config}\n \n     def call(self, inputs, count_weights=None):\n-\n         if count_weights is not None:\n             if self.output_mode != \"count\":\n                 raise ValueError(\n@@ -163,7 +162,6 @@ class CategoryEncoding(Layer):\n         depth,\n         count_weights=None,\n     ):\n-\n         # In all cases, we should uprank scalar input to a single sample.\n         if len(inputs.shape) == 0:\n             inputs = ops.expand_dims(inputs, -1)\n\n@@ -1,4 +1,4 @@\n-def normalize_tuple(value, n, name, allow_zero=False):\n+def standardize_tuple(value, n, name, allow_zero=False):\n     \"\"\"Transforms non-negative/positive integer/integers into an integer tuple.\n \n     Args:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c8113e8730d4c32e927bd6baabe6fd91ad17a115", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 320 | Lines Deleted: 292 | Files Changed: 6 | Hunks: 12 | Methods Changed: 75 | Complexity Δ (Sum/Max): 3/35 | Churn Δ: 612 | Churn Cumulative: 2205 | Contributors (this commit): 10 | Commits (past 90d): 93 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,162 +1,20 @@\n-import jax\n-import numpy as np\n-from jax import numpy as jnp\n-from tensorflow import nest\n-\n-from keras_core.backend.common import KerasVariable\n-from keras_core.backend.common import get_autocast_scope\n-from keras_core.backend.common import standardize_dtype\n-from keras_core.backend.common import standardize_shape\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.backend.common.stateless_scope import StatelessScope\n-from keras_core.backend.common.stateless_scope import get_stateless_scope\n-from keras_core.backend.common.stateless_scope import in_stateless_scope\n from keras_core.backend.jax import core\n from keras_core.backend.jax import image\n from keras_core.backend.jax import math\n from keras_core.backend.jax import nn\n from keras_core.backend.jax import numpy\n from keras_core.backend.jax import random\n-from keras_core.utils.naming import auto_name\n-\n-DYNAMIC_SHAPES_OK = False  # Dynamic shapes NG\n-\n-\n-def convert_to_tensor(x, dtype=None):\n-    if dtype is not None:\n-        dtype = standardize_dtype(dtype)\n-    if isinstance(x, Variable):\n-        if dtype and dtype != x.dtype:\n-            return x.value.astype(dtype)\n-        return x.value\n-    return jnp.array(x, dtype=dtype)\n-\n-\n-def is_tensor(x):\n-    if isinstance(x, jnp.ndarray):\n-        return True\n-    return False\n-\n-\n-def shape(x):\n-    # This will work as long as we disallow\n-    # dynamic shapes in JAX.\n-    return x.shape\n-\n-\n-def cast(x, dtype):\n-    return convert_to_tensor(x, dtype=dtype)\n-\n-\n-def cond(pred, true_fn, false_fn):\n-    return jax.lax.cond(pred, true_fun=true_fn, false_fun=false_fn)\n-\n-\n-def name_scope(name):\n-    return jax.named_scope(name)\n-\n-\n-def vectorized_map(function, elements):\n-    return jax.vmap(function)(elements)\n-\n-\n-class Variable(KerasVariable):\n-    def _initialize(self, value):\n-        self._value = jnp.array(value, dtype=self._dtype)\n-\n-    def assign(self, value):\n-        value = convert_to_tensor(value, dtype=self.dtype)\n-        if value.shape != self.shape:\n-            raise ValueError(\n-                \"The shape of the target variable and \"\n-                \"the shape of the target value in \"\n-                \"`variable.assign(value)` must match. \"\n-                f\"Received: value.shape={value.shape}; \"\n-                f\"variable.shape={self.value.shape}\"\n-            )\n-        if in_stateless_scope():\n-            scope = get_stateless_scope()\n-            scope.add_update((self, value))\n-        else:\n-            if isinstance(value, jnp.ndarray) and value.dtype == self.dtype:\n-                # Avoid a memory copy\n-                self._value = value\n-            else:\n-                self._value = jnp.array(value, dtype=self.dtype)\n-\n-    @property\n-    def value(self):\n-        if in_stateless_scope():\n-            scope = get_stateless_scope()\n-            value = scope.get_current_value(self)\n-            if value is not None:\n-                return self._maybe_autocast(value)\n-        if self._value is None:\n-            # Unitialized variable. Return a placeholder.\n-            # This is fine because it's only ever used\n-            # in during shape inference with JAX tracer objects\n-            # (anything else would be a bug, to be fixed.)\n-            return self._maybe_autocast(\n-                self._initializer(self._shape, dtype=self._dtype)\n-            )\n-        return self._maybe_autocast(self._value)\n-\n-    def numpy(self):\n-        return np.array(self.value)\n-\n-    # Overload native accessor.\n-    def __jax_array__(self):\n-        return self.value\n-\n-    def _convert_to_tensor(self, value, dtype=None):\n-        return convert_to_tensor(value, dtype=dtype)\n-\n-\n-# Shape / dtype inference util\n-def compute_output_spec(fn, *args, **kwargs):\n-    with StatelessScope():\n-\n-        def convert_keras_tensor_to_jax(x):\n-            if isinstance(x, KerasTensor):\n-                return jax.ShapeDtypeStruct(x.shape, dtype=x.dtype)\n-            return x\n-\n-        built_in_types = (type(None), int, float, str, bool, complex, bytes)\n-        static_args = []\n-        maybe_symbolic_args = []\n-        for arg in args:\n-            if isinstance(arg, built_in_types):\n-                static_args.append(arg)\n-            else:\n-                maybe_symbolic_args.append(arg)\n-        static_kwargs = {}\n-        maybe_symbolic_kwargs = {}\n-        for (\n-            k,\n-            arg,\n-        ) in kwargs.items():\n-            if isinstance(arg, built_in_types):\n-                static_kwargs[k] = arg\n-            else:\n-                maybe_symbolic_kwargs[k] = arg\n-\n-        def wrapped_fn(*args, **kwargs):\n-            return fn(*args, *static_args, **kwargs, **static_kwargs)\n-\n-        maybe_symbolic_args, maybe_symbolic_kwargs = nest.map_structure(\n-            convert_keras_tensor_to_jax,\n-            (maybe_symbolic_args, maybe_symbolic_kwargs),\n-        )\n-        _, jax_out = jax.make_jaxpr(wrapped_fn, return_shape=True)(\n-            *maybe_symbolic_args, **maybe_symbolic_kwargs\n-        )\n-\n-        def convert_jax_spec_to_keras_tensor(x):\n-            if isinstance(x, jax.ShapeDtypeStruct):\n-                return KerasTensor(x.shape, x.dtype)\n-            return x\n-\n-        output_shape = nest.map_structure(\n-            convert_jax_spec_to_keras_tensor, jax_out\n-        )\n-    return output_shape\n+from keras_core.backend.jax.core import DYNAMIC_SHAPES_OK\n+from keras_core.backend.jax.core import Variable\n+from keras_core.backend.jax.core import cast\n+from keras_core.backend.jax.core import compute_output_spec\n+from keras_core.backend.jax.core import cond\n+from keras_core.backend.jax.core import convert_to_tensor\n+from keras_core.backend.jax.core import is_tensor\n+from keras_core.backend.jax.core import name_scope\n+from keras_core.backend.jax.core import scatter\n+from keras_core.backend.jax.core import shape\n+from keras_core.backend.jax.core import vectorized_map\n+from keras_core.backend.jax.rnn import gru\n+from keras_core.backend.jax.rnn import lstm\n+from keras_core.backend.jax.rnn import rnn\n\n@@ -1,4 +1,156 @@\n+import jax\n import jax.numpy as jnp\n+import numpy as np\n+from tensorflow import nest\n+\n+from keras_core.backend.common import KerasVariable\n+from keras_core.backend.common import standardize_dtype\n+from keras_core.backend.common.keras_tensor import KerasTensor\n+from keras_core.backend.common.stateless_scope import StatelessScope\n+from keras_core.backend.common.stateless_scope import get_stateless_scope\n+from keras_core.backend.common.stateless_scope import in_stateless_scope\n+\n+DYNAMIC_SHAPES_OK = False  # Dynamic shapes NG\n+\n+\n+class Variable(KerasVariable):\n+    def _initialize(self, value):\n+        self._value = jnp.array(value, dtype=self._dtype)\n+\n+    def assign(self, value):\n+        value = convert_to_tensor(value, dtype=self.dtype)\n+        if value.shape != self.shape:\n+            raise ValueError(\n+                \"The shape of the target variable and \"\n+                \"the shape of the target value in \"\n+                \"`variable.assign(value)` must match. \"\n+                f\"Received: value.shape={value.shape}; \"\n+                f\"variable.shape={self.value.shape}\"\n+            )\n+        if in_stateless_scope():\n+            scope = get_stateless_scope()\n+            scope.add_update((self, value))\n+        else:\n+            if isinstance(value, jnp.ndarray) and value.dtype == self.dtype:\n+                # Avoid a memory copy\n+                self._value = value\n+            else:\n+                self._value = jnp.array(value, dtype=self.dtype)\n+\n+    @property\n+    def value(self):\n+        if in_stateless_scope():\n+            scope = get_stateless_scope()\n+            value = scope.get_current_value(self)\n+            if value is not None:\n+                return self._maybe_autocast(value)\n+        if self._value is None:\n+            # Unitialized variable. Return a placeholder.\n+            # This is fine because it's only ever used\n+            # in during shape inference with JAX tracer objects\n+            # (anything else would be a bug, to be fixed.)\n+            return self._maybe_autocast(\n+                self._initializer(self._shape, dtype=self._dtype)\n+            )\n+        return self._maybe_autocast(self._value)\n+\n+    def numpy(self):\n+        return np.array(self.value)\n+\n+    # Overload native accessor.\n+    def __jax_array__(self):\n+        return self.value\n+\n+    def _convert_to_tensor(self, value, dtype=None):\n+        return convert_to_tensor(value, dtype=dtype)\n+\n+\n+def convert_to_tensor(x, dtype=None):\n+    if dtype is not None:\n+        dtype = standardize_dtype(dtype)\n+    if isinstance(x, Variable):\n+        if dtype and dtype != x.dtype:\n+            return x.value.astype(dtype)\n+        return x.value\n+    return jnp.array(x, dtype=dtype)\n+\n+\n+def is_tensor(x):\n+    if isinstance(x, jnp.ndarray):\n+        return True\n+    return False\n+\n+\n+def shape(x):\n+    # This will work as long as we disallow\n+    # dynamic shapes in JAX.\n+    return x.shape\n+\n+\n+def cast(x, dtype):\n+    return convert_to_tensor(x, dtype=dtype)\n+\n+\n+def name_scope(name):\n+    return jax.named_scope(name)\n+\n+\n+# Shape / dtype inference util\n+def compute_output_spec(fn, *args, **kwargs):\n+    with StatelessScope():\n+\n+        def convert_keras_tensor_to_jax(x):\n+            if isinstance(x, KerasTensor):\n+                return jax.ShapeDtypeStruct(x.shape, dtype=x.dtype)\n+            return x\n+\n+        built_in_types = (type(None), int, float, str, bool, complex, bytes)\n+        static_args = []\n+        maybe_symbolic_args = []\n+        for arg in args:\n+            if isinstance(arg, built_in_types):\n+                static_args.append(arg)\n+            else:\n+                maybe_symbolic_args.append(arg)\n+        static_kwargs = {}\n+        maybe_symbolic_kwargs = {}\n+        for (\n+            k,\n+            arg,\n+        ) in kwargs.items():\n+            if isinstance(arg, built_in_types):\n+                static_kwargs[k] = arg\n+            else:\n+                maybe_symbolic_kwargs[k] = arg\n+\n+        def wrapped_fn(*args, **kwargs):\n+            return fn(*args, *static_args, **kwargs, **static_kwargs)\n+\n+        maybe_symbolic_args, maybe_symbolic_kwargs = nest.map_structure(\n+            convert_keras_tensor_to_jax,\n+            (maybe_symbolic_args, maybe_symbolic_kwargs),\n+        )\n+        _, jax_out = jax.make_jaxpr(wrapped_fn, return_shape=True)(\n+            *maybe_symbolic_args, **maybe_symbolic_kwargs\n+        )\n+\n+        def convert_jax_spec_to_keras_tensor(x):\n+            if isinstance(x, jax.ShapeDtypeStruct):\n+                return KerasTensor(x.shape, x.dtype)\n+            return x\n+\n+        output_shape = nest.map_structure(\n+            convert_jax_spec_to_keras_tensor, jax_out\n+        )\n+    return output_shape\n+\n+\n+def cond(pred, true_fn, false_fn):\n+    return jax.lax.cond(pred, true_fun=true_fn, false_fun=false_fn)\n+\n+\n+def vectorized_map(function, elements):\n+    return jax.vmap(function)(elements)\n \n \n def scatter(indices, values, shape):\n\n@@ -0,0 +1,10 @@\n+def rnn(*args, **kwargs):\n+    raise NotImplementedError\n+\n+\n+def lstm(*args, **kwargs):\n+    raise NotImplementedError\n+\n+\n+def gru(*args, **kwargs):\n+    raise NotImplementedError\n\n@@ -1,142 +1,20 @@\n-import numpy as np\n-import tensorflow as tf\n-from tensorflow.experimental import numpy as tfnp\n-\n-from keras_core.backend.common import KerasVariable\n-from keras_core.backend.common import standardize_dtype\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.backend.common.stateless_scope import StatelessScope\n-from keras_core.backend.common.stateless_scope import get_stateless_scope\n-from keras_core.backend.common.stateless_scope import in_stateless_scope\n from keras_core.backend.tensorflow import core\n from keras_core.backend.tensorflow import image\n from keras_core.backend.tensorflow import math\n from keras_core.backend.tensorflow import nn\n from keras_core.backend.tensorflow import numpy\n from keras_core.backend.tensorflow import random\n+from keras_core.backend.tensorflow.core import DYNAMIC_SHAPES_OK\n+from keras_core.backend.tensorflow.core import Variable\n+from keras_core.backend.tensorflow.core import cast\n+from keras_core.backend.tensorflow.core import compute_output_spec\n+from keras_core.backend.tensorflow.core import cond\n+from keras_core.backend.tensorflow.core import convert_to_tensor\n+from keras_core.backend.tensorflow.core import is_tensor\n+from keras_core.backend.tensorflow.core import name_scope\n+from keras_core.backend.tensorflow.core import scatter\n+from keras_core.backend.tensorflow.core import shape\n+from keras_core.backend.tensorflow.core import vectorized_map\n from keras_core.backend.tensorflow.rnn import gru\n from keras_core.backend.tensorflow.rnn import lstm\n from keras_core.backend.tensorflow.rnn import rnn\n-from keras_core.utils.naming import auto_name\n-\n-DYNAMIC_SHAPES_OK = True\n-\n-\n-class Variable(KerasVariable, tf.__internal__.types.Tensor):\n-    _should_act_as_resource_variable = True\n-\n-    @property\n-    def handle(self):\n-        return self.value.handle\n-\n-    def _initialize(self, value):\n-        self._value = tf.Variable(\n-            value, dtype=self._dtype, trainable=self.trainable, name=self.name\n-        )\n-\n-    def assign(self, value):\n-        value = convert_to_tensor(value, dtype=self.dtype)\n-        if value.shape != self.value.shape:\n-            raise ValueError(\n-                \"The shape of the target variable and \"\n-                \"the shape of the target value in \"\n-                \"`variable.assign(value)` must match. \"\n-                f\"Received: value.shape={value.shape}; \"\n-                f\"variable.shape={self.value.shape}\"\n-            )\n-        if in_stateless_scope():\n-            scope = get_stateless_scope()\n-            scope.add_update((self, value))\n-        else:\n-            self.value.assign(value)\n-\n-    @property\n-    def value(self):\n-        if in_stateless_scope():\n-            scope = get_stateless_scope()\n-            value = scope.get_current_value(self)\n-            if value is not None:\n-                return self._maybe_autocast(value)\n-        if self._value is None:\n-            # Unitialized variable. Return a placeholder.\n-            # This is fine because it's only ever used\n-            # during shape inference in a scratch graph\n-            # (anything else would be a bug, to be fixed.)\n-            init_val = self._initializer(self._shape, dtype=self._dtype)\n-            return self._maybe_autocast(init_val)\n-        return self._maybe_autocast(self._value)\n-\n-    def numpy(self):  # noqa: F811\n-        return self.value.numpy()\n-\n-    @property\n-    def shape(self):\n-        return tf.TensorShape(super().shape)\n-\n-    # Overload native accessor.\n-    def __tf_tensor__(self, dtype=None, name=None):\n-        return tf.convert_to_tensor(self.value, dtype=dtype, name=name)\n-\n-    def _convert_to_tensor(self, value, dtype=None):\n-        return convert_to_tensor(value, dtype=dtype)\n-\n-\n-def convert_to_tensor(x, dtype=None):\n-    if dtype is not None:\n-        dtype = standardize_dtype(dtype)\n-        if tf.is_tensor(x):\n-            return tf.cast(x, dtype=dtype)\n-    return tf.convert_to_tensor(x, dtype=dtype)\n-\n-\n-def is_tensor(x):\n-    return tf.is_tensor(x)\n-\n-\n-def shape(x):\n-    return tf.shape(x)\n-\n-\n-def cast(x, dtype):\n-    dtype = standardize_dtype(dtype)\n-    return tf.cast(x, dtype=dtype)\n-\n-\n-def cond(pred, true_fn, false_fn):\n-    return tf.cond(pred, true_fn=true_fn, false_fn=false_fn)\n-\n-\n-def name_scope(name):\n-    return tf.name_scope(name)\n-\n-\n-def vectorized_map(function, elements):\n-    return tf.vectorized_map(function, elements)\n-\n-\n-def compute_output_spec(fn, *args, **kwargs):\n-    with StatelessScope():\n-        graph_name = auto_name(\"scratch_graph\")\n-        with tf.__internal__.FuncGraph(graph_name).as_default():\n-\n-            def convert_keras_tensor_to_tf(x):\n-                if isinstance(x, KerasTensor):\n-                    return tf.compat.v1.placeholder(\n-                        shape=x.shape, dtype=x.dtype\n-                    )\n-                return x\n-\n-            args, kwargs = tf.nest.map_structure(\n-                convert_keras_tensor_to_tf, (args, kwargs)\n-            )\n-            tf_out = fn(*args, **kwargs)\n-\n-            def convert_tf_to_keras_tensor(x):\n-                if tf.is_tensor(x):\n-                    return KerasTensor(x.shape, x.dtype)\n-                return x\n-\n-            output_shape = tf.nest.map_structure(\n-                convert_tf_to_keras_tensor, tf_out\n-            )\n-    return output_shape\n\n@@ -1,5 +1,135 @@\n import tensorflow as tf\n \n+from keras_core.backend.common import KerasVariable\n+from keras_core.backend.common import standardize_dtype\n+from keras_core.backend.common.keras_tensor import KerasTensor\n+from keras_core.backend.common.stateless_scope import StatelessScope\n+from keras_core.backend.common.stateless_scope import get_stateless_scope\n+from keras_core.backend.common.stateless_scope import in_stateless_scope\n+from keras_core.utils.naming import auto_name\n+\n+DYNAMIC_SHAPES_OK = True\n+\n+\n+class Variable(KerasVariable, tf.__internal__.types.Tensor):\n+    _should_act_as_resource_variable = True\n+\n+    @property\n+    def handle(self):\n+        return self.value.handle\n+\n+    def _initialize(self, value):\n+        self._value = tf.Variable(\n+            value, dtype=self._dtype, trainable=self.trainable, name=self.name\n+        )\n+\n+    def assign(self, value):\n+        value = convert_to_tensor(value, dtype=self.dtype)\n+        if value.shape != self.value.shape:\n+            raise ValueError(\n+                \"The shape of the target variable and \"\n+                \"the shape of the target value in \"\n+                \"`variable.assign(value)` must match. \"\n+                f\"Received: value.shape={value.shape}; \"\n+                f\"variable.shape={self.value.shape}\"\n+            )\n+        if in_stateless_scope():\n+            scope = get_stateless_scope()\n+            scope.add_update((self, value))\n+        else:\n+            self.value.assign(value)\n+\n+    @property\n+    def value(self):\n+        if in_stateless_scope():\n+            scope = get_stateless_scope()\n+            value = scope.get_current_value(self)\n+            if value is not None:\n+                return self._maybe_autocast(value)\n+        if self._value is None:\n+            # Unitialized variable. Return a placeholder.\n+            # This is fine because it's only ever used\n+            # during shape inference in a scratch graph\n+            # (anything else would be a bug, to be fixed.)\n+            init_val = self._initializer(self._shape, dtype=self._dtype)\n+            return self._maybe_autocast(init_val)\n+        return self._maybe_autocast(self._value)\n+\n+    def numpy(self):  # noqa: F811\n+        return self.value.numpy()\n+\n+    @property\n+    def shape(self):\n+        return tf.TensorShape(super().shape)\n+\n+    # Overload native accessor.\n+    def __tf_tensor__(self, dtype=None, name=None):\n+        return tf.convert_to_tensor(self.value, dtype=dtype, name=name)\n+\n+    def _convert_to_tensor(self, value, dtype=None):\n+        return convert_to_tensor(value, dtype=dtype)\n+\n+\n+def convert_to_tensor(x, dtype=None):\n+    if dtype is not None:\n+        dtype = standardize_dtype(dtype)\n+        if tf.is_tensor(x):\n+            return tf.cast(x, dtype=dtype)\n+    return tf.convert_to_tensor(x, dtype=dtype)\n+\n+\n+def is_tensor(x):\n+    return tf.is_tensor(x)\n+\n+\n+def shape(x):\n+    return tf.shape(x)\n+\n+\n+def cast(x, dtype):\n+    dtype = standardize_dtype(dtype)\n+    return tf.cast(x, dtype=dtype)\n+\n+\n+def name_scope(name):\n+    return tf.name_scope(name)\n+\n+\n+def compute_output_spec(fn, *args, **kwargs):\n+    with StatelessScope():\n+        graph_name = auto_name(\"scratch_graph\")\n+        with tf.__internal__.FuncGraph(graph_name).as_default():\n+\n+            def convert_keras_tensor_to_tf(x):\n+                if isinstance(x, KerasTensor):\n+                    return tf.compat.v1.placeholder(\n+                        shape=x.shape, dtype=x.dtype\n+                    )\n+                return x\n+\n+            args, kwargs = tf.nest.map_structure(\n+                convert_keras_tensor_to_tf, (args, kwargs)\n+            )\n+            tf_out = fn(*args, **kwargs)\n+\n+            def convert_tf_to_keras_tensor(x):\n+                if tf.is_tensor(x):\n+                    return KerasTensor(x.shape, x.dtype)\n+                return x\n+\n+            output_shape = tf.nest.map_structure(\n+                convert_tf_to_keras_tensor, tf_out\n+            )\n+    return output_shape\n+\n+\n+def cond(pred, true_fn, false_fn):\n+    return tf.cond(pred, true_fn=true_fn, false_fn=false_fn)\n+\n+\n+def vectorized_map(function, elements):\n+    return tf.vectorized_map(function, elements)\n+\n \n def scatter(indices, values, shape):\n     return tf.scatter_nd(indices, values, shape)\n\n@@ -36,13 +36,13 @@ class DistributeTest(testing.TestCase):\n             dense = layers.Dense(2)\n             dense.build([4, 2])\n \n-        self.assertIsInstance(dense.kernel, backend.KerasVariable)\n+        self.assertIsInstance(dense.kernel, backend.Variable)\n         self.assertIsInstance(\n             dense.kernel.value, tf.distribute.DistributedValues\n         )\n         self.assertIn(\"MirroredVariable\", dense.kernel.value.__class__.__name__)\n \n-        self.assertIsInstance(dense.kernel, backend.KerasVariable)\n+        self.assertIsInstance(dense.kernel, backend.Variable)\n         self.assertIsInstance(dense.bias.value, tf.distribute.DistributedValues)\n         self.assertIn(\"MirroredVariable\", dense.bias.value.__class__.__name__)\n \n@@ -55,7 +55,7 @@ class DistributeTest(testing.TestCase):\n             output = dense(inputs)\n             model = models.Functional(inputs, output)\n \n-        self.assertIsInstance(dense.kernel, backend.KerasVariable)\n+        self.assertIsInstance(dense.kernel, backend.Variable)\n         self.assertIsInstance(\n             dense.kernel.value, tf.distribute.DistributedValues\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ecb5b64e1dbe729b2b216be54235bfaa75c81d33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 111 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -64/0 | Churn Δ: 124 | Churn Cumulative: 1199 | Contributors (this commit): 4 | Commits (past 90d): 13 | Contributors (cumulative): 5 | DMM Complexity: 0.12195121951219512\n\nDIFF:\n@@ -1,14 +1,6 @@\n-\"\"\"\n+from keras_core import backend\n \n-MANIFEST:\n-\n-normal\n-uniform\n-truncated_normal\n-dropout\n-\"\"\"\n-\n-from keras_core.backend.random import dropout\n-from keras_core.backend.random import normal\n-from keras_core.backend.random import truncated_normal\n-from keras_core.backend.random import uniform\n+dropout = backend.random.dropout\n+normal = backend.random.normal\n+truncated_normal = backend.random.truncated_normal\n+uniform = backend.random.uniform\n\n@@ -0,0 +1,106 @@\n+\"\"\"Script to create (and optionally install) a `.whl` archive for Keras Core.\n+\n+Usage:\n+\n+1. Create a `.whl` file in `dist/`:\n+\n+```\n+python3 pip_build.py\n+```\n+\n+2. Also install the new package immediately after:\n+\n+```\n+python3 pip_build.py --install\n+```\n+\"\"\"\n+import argparse\n+import glob\n+import os\n+import pathlib\n+import shutil\n+\n+import namex\n+\n+package = \"keras_core\"\n+build_directory = \"tmp_build_dir\"\n+dist_directory = \"dist\"\n+to_copy = [\n+    \"setup.py\",\n+]\n+\n+\n+def ignore_files(_, filenames):\n+    return [f for f in filenames if f.endswith(\"_test.py\")]\n+\n+\n+def build():\n+    if os.path.exists(build_directory):\n+        raise ValueError(f\"Directory already exists: {build_directory}\")\n+\n+    whl_path = None\n+    try:\n+        # Copy sources (`keras_core/` directory and setup files) to build directory\n+        root_path = pathlib.Path(__file__).parent.resolve()\n+        os.chdir(root_path)\n+        os.mkdir(build_directory)\n+        shutil.copytree(\n+            package, os.path.join(build_directory, package), ignore=ignore_files\n+        )\n+        for fname in to_copy:\n+            shutil.copy(fname, os.path.join(f\"{build_directory}\", fname))\n+        os.chdir(build_directory)\n+\n+        # Restructure the codebase so that source files live in `keras_core/src`\n+        namex.convert_codebase(package, code_directory=\"src\")\n+\n+        # Generate API __init__.py files in `keras_core/`\n+        namex.generate_api_files(package, code_directory=\"src\", verbose=True)\n+\n+        # Make sure to export the __version__ string\n+        from keras_core.src import __version__  # noqa: E402\n+\n+        with open(os.path.join(package, \"__init__.py\")) as f:\n+            init_contents = f.read()\n+        with open(os.path.join(package, \"__init__.py\"), \"w\") as f:\n+            f.write(init_contents + \"\\n\\n\" + f'__version__ = \"{__version__}\"\\n')\n+\n+        # Build the package\n+        os.system(\"python3 -m build\")\n+\n+        # Save the dist files generated by the build process\n+        os.chdir(root_path)\n+        if not os.path.exists(dist_directory):\n+            os.mkdir(dist_directory)\n+        for fpath in glob.glob(\n+            os.path.join(build_directory, dist_directory, \"*.*\")\n+        ):\n+            shutil.copy(fpath, dist_directory)\n+\n+        # Find the .whl file path\n+        for fname in os.listdir(dist_directory):\n+            if fname.endswith(\".whl\"):\n+                whl_path = os.path.abspath(os.path.join(dist_directory, fname))\n+        print(f\"Build successful. Wheel file available at {whl_path}\")\n+    finally:\n+        # Clean up: remove the build directory (no longer needed)\n+        shutil.rmtree(build_directory)\n+    return whl_path\n+\n+\n+def install_whl(whl_fpath):\n+    print(f\"Installing wheel file: {whl_fpath}\")\n+    os.system(f\"pip3 install {whl_fpath} --force-reinstall --no-dependencies\")\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--install\",\n+        action=\"store_true\",\n+        help=\"Whether to install the generated wheel file.\",\n+    )\n+    args = parser.parse_args()\n+    whl_path = build()\n+    if whl_path and args.install:\n+        install_whl(whl_path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b11400f2fb5d45ba6ca69bf48a00d321b88f52e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 7 | Files Changed: 5 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 869 | Contributors (this commit): 3 | Commits (past 90d): 24 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -360,7 +360,9 @@ def standardize_shape(shape, fully_defined=False):\n         if not isinstance(e, int):\n             raise ValueError(\n                 f\"Cannot convert '{shape}' to a shape. \"\n-                f\"Found invalid entry '{e}'\"\n+                f\"Found invalid entry '{e}'. Only \"\n+                \"fully-defined shapes are allowed with the \"\n+                f\"{config.backend()} backend.\"\n             )\n         if e < 0:\n             raise ValueError(\n\n@@ -1,8 +1,10 @@\n from keras_core import backend\n+from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n from keras_core.operations.node import Node\n \n \n+@keras_core_export(\"keras_core.layers.InputLayer\")\n class InputLayer(Layer):\n     def __init__(\n         self,\n@@ -64,6 +66,7 @@ class InputLayer(Layer):\n         }\n \n \n+@keras_core_export([\"keras_core.layers.Input\", \"keras_core.Input\"])\n def Input(shape=None, batch_size=None, dtype=None, batch_shape=None, name=None):\n     layer = InputLayer(\n         shape=shape,\n\n@@ -74,8 +74,8 @@ class MaxPooling1D(BasePooling):\n \n     def __init__(\n         self,\n-        pool_size,\n-        strides,\n+        pool_size=2,\n+        strides=None,\n         padding=\"valid\",\n         data_format=None,\n         name=None,\n\n@@ -90,8 +90,8 @@ class MaxPooling2D(BasePooling):\n \n     def __init__(\n         self,\n-        pool_size,\n-        strides,\n+        pool_size=(2, 2),\n+        strides=None,\n         padding=\"valid\",\n         data_format=None,\n         name=None,\n\n@@ -66,8 +66,8 @@ class MaxPooling3D(BasePooling):\n \n     def __init__(\n         self,\n-        pool_size,\n-        strides,\n+        pool_size=(2, 2, 2),\n+        strides=None,\n         padding=\"valid\",\n         data_format=None,\n         name=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1f850da825c2565e557644857fbb23f2f0510b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 86 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 86 | Churn Cumulative: 86 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n\n\n\n@@ -0,0 +1,33 @@\n+import numpy as np\n+\n+from keras_core import Model\n+from keras_core import layers\n+from keras_core import losses\n+from keras_core import metrics\n+from keras_core import optimizers\n+\n+inputs = layers.Input((100,), batch_size=32)\n+x = layers.Dense(256, activation=\"relu\")(inputs)\n+x = layers.Dense(256, activation=\"relu\")(x)\n+x = layers.Dense(256, activation=\"relu\")(x)\n+outputs = layers.Dense(16)(x)\n+model = Model(inputs, outputs)\n+\n+model.summary()\n+\n+x = np.random.random((50000, 100))\n+y = np.random.random((50000, 16))\n+batch_size = 32\n+epochs = 5\n+\n+model.compile(\n+    optimizer=optimizers.SGD(learning_rate=0.001),\n+    loss=losses.MeanSquaredError(),\n+    metrics=[metrics.MeanSquaredError()],\n+)\n+history = model.fit(\n+    x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n+)\n+\n+print(\"History:\")\n+print(history.history)\n\n@@ -0,0 +1,53 @@\n+import numpy as np\n+import keras_core\n+from keras_core import layers\n+from keras_core.utils import to_categorical\n+\n+# Model / data parameters\n+num_classes = 10\n+input_shape = (28, 28, 1)\n+\n+# Load the data and split it between train and test sets\n+(x_train, y_train), (x_test, y_test) = keras_core.datasets.mnist.load_data()\n+\n+# Scale images to the [0, 1] range\n+x_train = x_train.astype(\"float32\") / 255\n+x_test = x_test.astype(\"float32\") / 255\n+# Make sure images have shape (28, 28, 1)\n+x_train = np.expand_dims(x_train, -1)\n+x_test = np.expand_dims(x_test, -1)\n+print(\"x_train shape:\", x_train.shape)\n+print(x_train.shape[0], \"train samples\")\n+print(x_test.shape[0], \"test samples\")\n+\n+\n+# convert class vectors to binary class matrices\n+y_train = to_categorical(y_train, num_classes)\n+y_test = to_categorical(y_test, num_classes)\n+\n+batch_size = 128\n+epochs = 15\n+\n+model = keras_core.Sequential(\n+    [\n+        layers.Input(shape=input_shape, batch_size=batch_size),\n+        # layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n+        # layers.MaxPooling2D(pool_size=(2, 2)),\n+        # layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n+        # layers.MaxPooling2D(pool_size=(2, 2)),\n+        layers.Flatten(),\n+        layers.Dense(32, activation=\"relu\"),\n+        layers.Dropout(0.5),\n+        layers.Dense(num_classes, activation=\"softmax\"),\n+    ]\n+)\n+\n+model.summary()\n+\n+model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n+\n+model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n+\n+score = model.evaluate(x_test, y_test, verbose=0)\n+print(\"Test loss:\", score[0])\n+print(\"Test accuracy:\", score[1])\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
