{"custom_id": "scrapy#9514393bd1c9137c6a3b9964679de04bccbd0570", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2118 | Contributors (this commit): 23 | Commits (past 90d): 10 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -523,10 +523,8 @@ class _ResponseReader(protocol.Protocol):\n                 logger.debug(\"Download stopped for %(request)s from signal handler %(handler)s\",\n                              {\"request\": self._request, \"handler\": handler.__qualname__})\n                 self.transport._producer.loseConnection()\n-                self._finish_response(\n-                    flags=[\"download_stopped\"],\n-                    failure=result if result.value.fail else None,\n-                )\n+                failure = result if result.value.fail else None\n+                self._finish_response(flags=[\"download_stopped\"], failure=failure)\n \n         if self._maxsize and self._bytes_received > self._maxsize:\n             logger.error(\"Received (%(bytes)s) bytes larger than download \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fbe915d9cbd8ab801a679b05265cff416764d558", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1205 | Contributors (this commit): 14 | Commits (past 90d): 11 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -293,10 +293,12 @@ class BytesReceivedCallbackSpider(MetaSpider):\n         self.meta[\"failure\"] = failure\n \n     def bytes_received(self, data, request, spider):\n+        self.meta[\"bytes_received\"] = data\n         raise StopDownload(fail=False)\n \n \n class BytesReceivedErrbackSpider(BytesReceivedCallbackSpider):\n \n     def bytes_received(self, data, request, spider):\n+        self.meta[\"bytes_received\"] = data\n         raise StopDownload(fail=True)\n\n@@ -478,7 +478,8 @@ with multiples lines\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertIsNone(crawler.spider.meta.get(\"failure\"))\n         self.assertIsInstance(crawler.spider.meta[\"response\"], Response)\n-        self.assertLess(len(crawler.spider.meta[\"response\"].text), crawler.spider.full_response_length)\n+        self.assertEqual(crawler.spider.meta[\"response\"].body, crawler.spider.meta.get(\"bytes_received\"))\n+        self.assertLess(len(crawler.spider.meta[\"response\"].body), crawler.spider.full_response_length)\n \n     @defer.inlineCallbacks\n     def test_stop_download_errback(self):\n@@ -488,4 +489,5 @@ with multiples lines\n         self.assertIsInstance(crawler.spider.meta[\"failure\"], Failure)\n         self.assertIsInstance(crawler.spider.meta[\"failure\"].value, StopDownload)\n         self.assertIsInstance(crawler.spider.meta[\"failure\"].response, Response)\n-        self.assertLess(len(crawler.spider.meta[\"failure\"].response.text), crawler.spider.full_response_length)\n+        self.assertEqual(crawler.spider.meta[\"failure\"].response.body, crawler.spider.meta.get(\"bytes_received\"))\n+        self.assertLess(len(crawler.spider.meta[\"failure\"].response.body), crawler.spider.full_response_length)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#677e619d3761e6669c247786bb95822ce38c8080", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 3052 | Contributors (this commit): 26 | Commits (past 90d): 29 | Contributors (cumulative): 43 | DMM Complexity: 1.0\n\nDIFF:\n@@ -364,7 +364,9 @@ class FeedExporter:\n         \"\"\"\n         if not self.storage_batch_item_count or '%(batch_time)s' in uri or '%(batch_id)s' in uri:\n             return True\n-        logger.error('%(batch_time)s or %(batch_id)s must be in uri if FEED_STORAGE_BATCH_ITEM_COUNT setting is specified')\n+        logger.error(\n+            '%(batch_time)s or %(batch_id)s must be in uri if FEED_STORAGE_BATCH_ITEM_COUNT setting is specified'\n+        )\n         return False\n \n     def _storage_supported(self, uri):\n\n@@ -1204,12 +1204,20 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         items = [dict({'foo': u'FOO', 'bar': u'BAR'}), dict({'foo': u'FOO1', 'bar': u'BAR1'})]\n \n         formats = {\n-            'json': [u'[\\n{\"bar\": \"BAR\"}\\n]'.encode('utf-8'),\n-                     u'[\\n{\"bar\": \"BAR1\"}\\n]'.encode('utf-8')],\n-            'xml': [u'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>'.encode('latin-1'),\n-                    u'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n<items>\\n  <item>\\n    <foo>FOO1</foo>\\n  </item>\\n</items>'.encode('latin-1')],\n-            'csv': [u'bar,foo\\r\\nBAR,FOO\\r\\n'.encode('utf-8'),\n-                    u'bar,foo\\r\\nBAR1,FOO1\\r\\n'.encode('utf-8')],\n+            'json': ['[\\n{\"bar\": \"BAR\"}\\n]'.encode('utf-8'),\n+                     '[\\n{\"bar\": \"BAR1\"}\\n]'.encode('utf-8')],\n+            'xml': [\n+                (\n+                    '<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n+                    '<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>'\n+                ).encode('latin-1'),\n+                (\n+                    '<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n+                    '<items>\\n  <item>\\n    <foo>FOO1</foo>\\n  </item>\\n</items>'\n+                ).encode('latin-1')\n+            ],\n+            'csv': ['bar,foo\\r\\nBAR,FOO\\r\\n'.encode('utf-8'),\n+                    'bar,foo\\r\\nBAR1,FOO1\\r\\n'.encode('utf-8')],\n         }\n \n         settings = {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f6ed5edc31e7cc66225c0860e1534a6230511954", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 181 | Lines Deleted: 40 | Files Changed: 2 | Hunks: 23 | Methods Changed: 18 | Complexity Δ (Sum/Max): 17/11 | Churn Δ: 221 | Churn Cumulative: 450 | Contributors (this commit): 11 | Commits (past 90d): 13 | Contributors (cumulative): 14 | DMM Complexity: 0.8053097345132744\n\nDIFF:\n@@ -29,8 +29,7 @@ class CookiesMiddleware:\n \n         cookiejarkey = request.meta.get(\"cookiejar\")\n         jar = self.jars[cookiejarkey]\n-        cookies = self._get_request_cookies(jar, request)\n-        for cookie in cookies:\n+        for cookie in self._get_request_cookies(jar, request):\n             jar.set_cookie_if_ok(cookie, request)\n \n         # set Cookie header\n@@ -68,28 +67,65 @@ class CookiesMiddleware:\n                 msg = \"Received cookies from: {}\\n{}\".format(response, cookies)\n                 logger.debug(msg, extra={'spider': spider})\n \n-    def _format_cookie(self, cookie):\n-        # build cookie string\n-        cookie_str = '%s=%s' % (cookie['name'], cookie['value'])\n-\n-        if cookie.get('path', None):\n-            cookie_str += '; Path=%s' % cookie['path']\n-        if cookie.get('domain', None):\n-            cookie_str += '; Domain=%s' % cookie['domain']\n+    def _format_cookie(self, cookie, request):\n+        \"\"\"\n+        Given a dict consisting of cookie components, return its string representation.\n+        Decode from bytes if necessary.\n+        \"\"\"\n+        decoded = {}\n+        for key in (\"name\", \"value\", \"path\", \"domain\"):\n+            if not cookie.get(key):\n+                if key in (\"name\", \"value\"):\n+                    msg = \"Invalid cookie found in request {}: {} ('{}' is missing)\"\n+                    logger.warning(msg.format(request, cookie, key))\n+                    return\n+                continue\n+            if isinstance(cookie[key], str):\n+                decoded[key] = cookie[key]\n+            else:\n+                try:\n+                    decoded[key] = cookie[key].decode(\"utf8\")\n+                except UnicodeDecodeError:\n+                    logger.warning(\"Non UTF-8 encoded cookie found in request %s: %s\",\n+                                   request, cookie)\n+                    decoded[key] = cookie[key].decode(\"latin1\", errors=\"replace\")\n \n+        cookie_str = \"{}={}\".format(decoded.pop(\"name\"), decoded.pop(\"value\"))\n+        for key, value in decoded.items():  # path, domain\n+            cookie_str += \"; {}={}\".format(key.capitalize(), value)\n         return cookie_str\n \n     def _get_request_cookies(self, jar, request):\n-        if isinstance(request.cookies, dict):\n-            cookie_list = [\n-                {'name': k, 'value': v}\n-                for k, v in request.cookies.items()\n-            ]\n-        else:\n-            cookie_list = request.cookies\n-\n-        cookies = [self._format_cookie(x) for x in cookie_list]\n-        headers = {'Set-Cookie': cookies}\n-        response = Response(request.url, headers=headers)\n-\n+        \"\"\"\n+        Extract cookies from a Request. Values from the `Request.cookies` attribute\n+        take precedence over values from the `Cookie` request header.\n+        \"\"\"\n+        def get_cookies_from_header(jar, request):\n+            cookie_header = request.headers.get(\"Cookie\")\n+            if not cookie_header:\n+                return []\n+            cookie_gen_bytes = (s.strip() for s in cookie_header.split(b\";\"))\n+            cookie_list_unicode = []\n+            for cookie_bytes in cookie_gen_bytes:\n+                try:\n+                    cookie_unicode = cookie_bytes.decode(\"utf8\")\n+                except UnicodeDecodeError:\n+                    logger.warning(\"Non UTF-8 encoded cookie found in request %s: %s\",\n+                                   request, cookie_bytes)\n+                    cookie_unicode = cookie_bytes.decode(\"latin1\", errors=\"replace\")\n+                cookie_list_unicode.append(cookie_unicode)\n+            response = Response(request.url, headers={\"Set-Cookie\": cookie_list_unicode})\n             return jar.make_cookies(response, request)\n+\n+        def get_cookies_from_attribute(jar, request):\n+            if not request.cookies:\n+                return []\n+            elif isinstance(request.cookies, dict):\n+                cookies = ({\"name\": k, \"value\": v} for k, v in request.cookies.items())\n+            else:\n+                cookies = request.cookies\n+            formatted = filter(None, (self._format_cookie(c, request) for c in cookies))\n+            response = Response(request.url, headers={\"Set-Cookie\": formatted})\n+            return jar.make_cookies(response, request)\n+\n+        return get_cookies_from_header(jar, request) + get_cookies_from_attribute(jar, request)\n\n@@ -1,20 +1,21 @@\n-import re\n import logging\n-from unittest import TestCase\n from testfixtures import LogCapture\n+from unittest import TestCase\n \n+from scrapy.downloadermiddlewares.cookies import CookiesMiddleware\n+from scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware\n+from scrapy.exceptions import NotConfigured\n from scrapy.http import Response, Request\n from scrapy.spiders import Spider\n+from scrapy.utils.python import to_bytes\n from scrapy.utils.test import get_crawler\n-from scrapy.exceptions import NotConfigured\n-from scrapy.downloadermiddlewares.cookies import CookiesMiddleware\n \n \n class CookiesMiddlewareTest(TestCase):\n \n     def assertCookieValEqual(self, first, second, msg=None):\n         def split_cookies(cookies):\n-            return sorted(re.split(r\";\\s*\", cookies.decode(\"latin1\")))\n+            return sorted([s.strip() for s in to_bytes(cookies).split(b\";\")])\n         return self.assertEqual(split_cookies(first), split_cookies(second), msg=msg)\n \n     def setUp(self):\n@@ -61,12 +62,13 @@ class CookiesMiddlewareTest(TestCase):\n     def test_setting_enabled_cookies_debug(self):\n         crawler = get_crawler(settings_dict={'COOKIES_DEBUG': True})\n         mw = CookiesMiddleware.from_crawler(crawler)\n-        with LogCapture('scrapy.downloadermiddlewares.cookies',\n+        with LogCapture(\n+            'scrapy.downloadermiddlewares.cookies',\n             propagate=False,\n-                        level=logging.DEBUG) as log:\n+            level=logging.DEBUG,\n+        ) as log:\n             req = Request('http://scrapytest.org/')\n-            res = Response('http://scrapytest.org/',\n-                           headers={'Set-Cookie': 'C1=value1; path=/'})\n+            res = Response('http://scrapytest.org/', headers={'Set-Cookie': 'C1=value1; path=/'})\n             mw.process_response(req, res, crawler.spider)\n             req2 = Request('http://scrapytest.org/sub1/')\n             mw.process_request(req2, crawler.spider)\n@@ -85,12 +87,13 @@ class CookiesMiddlewareTest(TestCase):\n     def test_setting_disabled_cookies_debug(self):\n         crawler = get_crawler(settings_dict={'COOKIES_DEBUG': False})\n         mw = CookiesMiddleware.from_crawler(crawler)\n-        with LogCapture('scrapy.downloadermiddlewares.cookies',\n+        with LogCapture(\n+            'scrapy.downloadermiddlewares.cookies',\n             propagate=False,\n-                        level=logging.DEBUG) as log:\n+            level=logging.DEBUG,\n+        ) as log:\n             req = Request('http://scrapytest.org/')\n-            res = Response('http://scrapytest.org/',\n-                           headers={'Set-Cookie': 'C1=value1; path=/'})\n+            res = Response('http://scrapytest.org/', headers={'Set-Cookie': 'C1=value1; path=/'})\n             mw.process_response(req, res, crawler.spider)\n             req2 = Request('http://scrapytest.org/sub1/')\n             mw.process_request(req2, crawler.spider)\n@@ -102,8 +105,7 @@ class CookiesMiddlewareTest(TestCase):\n         assert self.mw.process_request(req, self.spider) is None\n         assert 'Cookie' not in req.headers\n \n-        headers = {'Set-Cookie': b'C1=in\\xa3valid; path=/',\n-                   'Other': b'ignore\\xa3me'}\n+        headers = {'Set-Cookie': b'C1=in\\xa3valid; path=/', 'Other': b'ignore\\xa3me'}\n         res = Response('http://scrapytest.org/', headers=headers)\n         assert self.mw.process_response(req, res, self.spider) is res\n \n@@ -124,7 +126,10 @@ class CookiesMiddlewareTest(TestCase):\n         assert 'Cookie' not in req.headers\n \n         # check that returned cookies are not merged back to jar\n-        res = Response('http://scrapytest.org/dontmerge', headers={'Set-Cookie': 'dont=mergeme; path=/'})\n+        res = Response(\n+            'http://scrapytest.org/dontmerge',\n+            headers={'Set-Cookie': 'dont=mergeme; path=/'},\n+        )\n         assert self.mw.process_response(req, res, self.spider) is res\n \n         # check that cookies are merged back\n@@ -179,7 +184,11 @@ class CookiesMiddlewareTest(TestCase):\n         self.assertCookieValEqual(req2.headers.get('Cookie'), b\"C1=value1; galleta=salada\")\n \n     def test_cookiejar_key(self):\n-        req = Request('http://scrapytest.org/', cookies={'galleta': 'salada'}, meta={'cookiejar': \"store1\"})\n+        req = Request(\n+            'http://scrapytest.org/',\n+            cookies={'galleta': 'salada'},\n+            meta={'cookiejar': \"store1\"},\n+        )\n         assert self.mw.process_request(req, self.spider) is None\n         self.assertEqual(req.headers.get('Cookie'), b'galleta=salada')\n \n@@ -191,7 +200,11 @@ class CookiesMiddlewareTest(TestCase):\n         assert self.mw.process_request(req2, self.spider) is None\n         self.assertCookieValEqual(req2.headers.get('Cookie'), b'C1=value1; galleta=salada')\n \n-        req3 = Request('http://scrapytest.org/', cookies={'galleta': 'dulce'}, meta={'cookiejar': \"store2\"})\n+        req3 = Request(\n+            'http://scrapytest.org/',\n+            cookies={'galleta': 'dulce'},\n+            meta={'cookiejar': \"store2\"},\n+        )\n         assert self.mw.process_request(req3, self.spider) is None\n         self.assertEqual(req3.headers.get('Cookie'), b'galleta=dulce')\n \n@@ -229,3 +242,95 @@ class CookiesMiddlewareTest(TestCase):\n         assert self.mw.process_request(request, self.spider) is None\n         self.assertIn('Cookie', request.headers)\n         self.assertEqual(b'currencyCookie=USD', request.headers['Cookie'])\n+\n+    def test_keep_cookie_from_default_request_headers_middleware(self):\n+        DEFAULT_REQUEST_HEADERS = dict(Cookie='default=value; asdf=qwerty')\n+        mw_default_headers = DefaultHeadersMiddleware(DEFAULT_REQUEST_HEADERS.items())\n+        # overwrite with values from 'cookies' request argument\n+        req1 = Request('http://example.org', cookies={'default': 'something'})\n+        assert mw_default_headers.process_request(req1, self.spider) is None\n+        assert self.mw.process_request(req1, self.spider) is None\n+        self.assertCookieValEqual(req1.headers['Cookie'], b'default=something; asdf=qwerty')\n+        # keep both\n+        req2 = Request('http://example.com', cookies={'a': 'b'})\n+        assert mw_default_headers.process_request(req2, self.spider) is None\n+        assert self.mw.process_request(req2, self.spider) is None\n+        self.assertCookieValEqual(req2.headers['Cookie'], b'default=value; a=b; asdf=qwerty')\n+\n+    def test_keep_cookie_header(self):\n+        # keep only cookies from 'Cookie' request header\n+        req1 = Request('http://scrapytest.org', headers={'Cookie': 'a=b; c=d'})\n+        assert self.mw.process_request(req1, self.spider) is None\n+        self.assertCookieValEqual(req1.headers['Cookie'], 'a=b; c=d')\n+        # keep cookies from both 'Cookie' request header and 'cookies' keyword\n+        req2 = Request('http://scrapytest.org', headers={'Cookie': 'a=b; c=d'}, cookies={'e': 'f'})\n+        assert self.mw.process_request(req2, self.spider) is None\n+        self.assertCookieValEqual(req2.headers['Cookie'], 'a=b; c=d; e=f')\n+        # overwrite values from 'Cookie' request header with 'cookies' keyword\n+        req3 = Request(\n+            'http://scrapytest.org',\n+            headers={'Cookie': 'a=b; c=d'},\n+            cookies={'a': 'new', 'e': 'f'},\n+        )\n+        assert self.mw.process_request(req3, self.spider) is None\n+        self.assertCookieValEqual(req3.headers['Cookie'], 'a=new; c=d; e=f')\n+\n+    def test_request_cookies_encoding(self):\n+        # 1) UTF8-encoded bytes\n+        req1 = Request('http://example.org', cookies={'a': u'á'.encode('utf8')})\n+        assert self.mw.process_request(req1, self.spider) is None\n+        self.assertCookieValEqual(req1.headers['Cookie'], b'a=\\xc3\\xa1')\n+\n+        # 2) Non UTF8-encoded bytes\n+        req2 = Request('http://example.org', cookies={'a': u'á'.encode('latin1')})\n+        assert self.mw.process_request(req2, self.spider) is None\n+        self.assertCookieValEqual(req2.headers['Cookie'], b'a=\\xc3\\xa1')\n+\n+        # 3) Unicode string\n+        req3 = Request('http://example.org', cookies={'a': u'á'})\n+        assert self.mw.process_request(req3, self.spider) is None\n+        self.assertCookieValEqual(req3.headers['Cookie'], b'a=\\xc3\\xa1')\n+\n+    def test_request_headers_cookie_encoding(self):\n+        # 1) UTF8-encoded bytes\n+        req1 = Request('http://example.org', headers={'Cookie': u'a=á'.encode('utf8')})\n+        assert self.mw.process_request(req1, self.spider) is None\n+        self.assertCookieValEqual(req1.headers['Cookie'], b'a=\\xc3\\xa1')\n+\n+        # 2) Non UTF8-encoded bytes\n+        req2 = Request('http://example.org', headers={'Cookie': u'a=á'.encode('latin1')})\n+        assert self.mw.process_request(req2, self.spider) is None\n+        self.assertCookieValEqual(req2.headers['Cookie'], b'a=\\xc3\\xa1')\n+\n+        # 3) Unicode string\n+        req3 = Request('http://example.org', headers={'Cookie': u'a=á'})\n+        assert self.mw.process_request(req3, self.spider) is None\n+        self.assertCookieValEqual(req3.headers['Cookie'], b'a=\\xc3\\xa1')\n+\n+    def test_invalid_cookies(self):\n+        \"\"\"\n+        Invalid cookies are logged as warnings and discarded\n+        \"\"\"\n+        with LogCapture(\n+            'scrapy.downloadermiddlewares.cookies',\n+            propagate=False,\n+            level=logging.INFO,\n+        ) as lc:\n+            cookies1 = [{'value': 'bar'}, {'name': 'key', 'value': 'value1'}]\n+            req1 = Request('http://example.org/1', cookies=cookies1)\n+            assert self.mw.process_request(req1, self.spider) is None\n+            cookies2 = [{'name': 'foo'}, {'name': 'key', 'value': 'value2'}]\n+            req2 = Request('http://example.org/2', cookies=cookies2)\n+            assert self.mw.process_request(req2, self.spider) is None\n+            lc.check(\n+                (\"scrapy.downloadermiddlewares.cookies\",\n+                 \"WARNING\",\n+                 \"Invalid cookie found in request <GET http://example.org/1>:\"\n+                 \" {'value': 'bar'} ('name' is missing)\"),\n+                (\"scrapy.downloadermiddlewares.cookies\",\n+                 \"WARNING\",\n+                 \"Invalid cookie found in request <GET http://example.org/2>:\"\n+                 \" {'name': 'foo'} ('value' is missing)\"),\n+            )\n+        self.assertCookieValEqual(req1.headers['Cookie'], 'key=value1')\n+        self.assertCookieValEqual(req2.headers['Cookie'], 'key=value2')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dd96f94e8cc1517b7021e35e46cbdc92580c6333", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 783 | Contributors (this commit): 22 | Commits (past 90d): 16 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -398,8 +398,9 @@ class FeedExporter:\n         params = {}\n         for k in dir(spider):\n             params[k] = getattr(spider, k)\n-        params['time'] = datetime.utcnow().replace(microsecond=0).isoformat().replace(':', '-')\n-        params['batch_time'] = datetime.utcnow().isoformat().replace(':', '-')\n+        utc_now = datetime.utcnow()\n+        params['time'] = utc_now.replace(microsecond=0).isoformat().replace(':', '-')\n+        params['batch_time'] = utc_now.isoformat().replace(':', '-')\n         params['batch_id'] = slot.batch_id + 1 if slot is not None else 1\n         uripar_function = load_object(uri_params) if uri_params else lambda x, y: None\n         uripar_function(params, spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#492197e4773798fcb19ba934c03d5b48e0b7c1f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 18 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 28 | Churn Cumulative: 4081 | Contributors (this commit): 42 | Commits (past 90d): 21 | Contributors (cumulative): 59 | DMM Complexity: 0.0\n\nDIFF:\n@@ -442,12 +442,8 @@ class ScrapyAgent:\n             ip_address=result[\"ip_address\"],\n         )\n         if result.get(\"failure\"):\n-            # This failure is not the same object that will reach the errback,\n-            # so we need to temporarily store the response in the exception.\n-            # It will be moved to the failure in core/scraper.py\n-            failure = result[\"failure\"]\n-            failure.value.response = response\n-            return failure\n+            result[\"failure\"].value.response = response\n+            return result[\"failure\"]\n         return response\n \n \n\n@@ -11,7 +11,7 @@ from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errba\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.misc import load_object, warn_on_generator_with_return_value\n from scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n-from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest, StopDownload\n+from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy import signals\n from scrapy.http import Request, Response\n from scrapy.item import _BaseItem\n@@ -147,14 +147,6 @@ class Scraper:\n \n     def call_spider(self, result, request, spider):\n         result.request = request\n-        # StopDownload exceptions: make the partial response an attribute of the failure\n-        if (\n-            isinstance(result, Failure)\n-            and isinstance(result.value, StopDownload)\n-            and hasattr(result.value, \"response\")\n-        ):\n-            result.response = result.value.response\n-            delattr(result.value, \"response\")\n         dfd = defer_result(result)\n         callback = request.callback or spider.parse\n         warn_on_generator_with_return_value(spider, callback)\n\n@@ -488,6 +488,10 @@ with multiples lines\n         self.assertIsNone(crawler.spider.meta.get(\"response\"))\n         self.assertIsInstance(crawler.spider.meta[\"failure\"], Failure)\n         self.assertIsInstance(crawler.spider.meta[\"failure\"].value, StopDownload)\n-        self.assertIsInstance(crawler.spider.meta[\"failure\"].response, Response)\n-        self.assertEqual(crawler.spider.meta[\"failure\"].response.body, crawler.spider.meta.get(\"bytes_received\"))\n-        self.assertLess(len(crawler.spider.meta[\"failure\"].response.body), crawler.spider.full_response_length)\n+        self.assertIsInstance(crawler.spider.meta[\"failure\"].value.response, Response)\n+        self.assertEqual(\n+            crawler.spider.meta[\"failure\"].value.response.body,\n+            crawler.spider.meta.get(\"bytes_received\"))\n+        self.assertLess(\n+            len(crawler.spider.meta[\"failure\"].value.response.body),\n+            crawler.spider.full_response_length)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#44d5801b05280ad633e63a6b25db264d6586d882", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 835 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -105,8 +105,8 @@ class LocalWeakReferencedCache(weakref.WeakKeyDictionary):\n     def __getitem__(self, key):\n         try:\n             return super(LocalWeakReferencedCache, self).__getitem__(key)\n-        except TypeError:\n-            return None  # key is not weak-referenceable, it's not cached\n+        except (TypeError, KeyError):\n+            return None  # key is either not weak-referenceable or not cached\n \n \n class SequenceExclude:\n\n@@ -271,6 +271,7 @@ class LocalWeakReferencedCacheTest(unittest.TestCase):\n         self.assertNotIn(r1, cache)\n         self.assertIn(r2, cache)\n         self.assertIn(r3, cache)\n+        self.assertEqual(cache[r1], None)\n         self.assertEqual(cache[r2], 2)\n         self.assertEqual(cache[r3], 3)\n         del r2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5cef927944152f11d296566767c1d794b34b38e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 33 | Churn Cumulative: 1645 | Contributors (this commit): 19 | Commits (past 90d): 10 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ discovering (through HTTP headers) to base Response class.\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n+import json\n import warnings\n from contextlib import suppress\n from typing import Generator\n@@ -21,10 +22,13 @@ from scrapy.http.response import Response\n from scrapy.utils.python import memoizemethod_noargs, to_unicode\n from scrapy.utils.response import get_base_url\n \n+_NONE = object()\n+\n \n class TextResponse(Response):\n \n     _DEFAULT_ENCODING = 'ascii'\n+    _cached_decoded_json = _NONE\n \n     def __init__(self, *args, **kwargs):\n         self._encoding = kwargs.pop('encoding', None)\n@@ -68,6 +72,14 @@ class TextResponse(Response):\n                       ScrapyDeprecationWarning, stacklevel=2)\n         return self.text\n \n+    def json(self):\n+        \"\"\"\n+        Deserialize a JSON document to a Python object.\n+        \"\"\"\n+        if self._cached_decoded_json is _NONE:\n+            self._cached_decoded_json = json.loads(self.text)\n+        return self._cached_decoded_json\n+\n     @property\n     def text(self):\n         \"\"\" Body as unicode \"\"\"\n\n@@ -1,4 +1,5 @@\n import unittest\n+from unittest import mock\n from warnings import catch_warnings\n \n from w3lib.encoding import resolve_encoding\n@@ -685,6 +686,26 @@ class TextResponseTest(BaseResponseTest):\n             self.assertEqual(len(warnings), 1)\n             self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n \n+    def test_json_response(self):\n+        json_body = b\"\"\"{\"ip\": \"109.187.217.200\"}\"\"\"\n+        json_response = self.response_class(\"http://www.example.com\", body=json_body)\n+        self.assertEqual(json_response.json(), {'ip': '109.187.217.200'})\n+\n+        text_body = b\"\"\"<html><body>text</body></html>\"\"\"\n+        text_response = self.response_class(\"http://www.example.com\", body=text_body)\n+        with self.assertRaises(ValueError):\n+            text_response.json()\n+\n+    def test_cache_json_response(self):\n+        json_valid_bodies = [b\"\"\"{\"ip\": \"109.187.217.200\"}\"\"\", b\"\"\"null\"\"\"]\n+        for json_body in json_valid_bodies:\n+            json_response = self.response_class(\"http://www.example.com\", body=json_body)\n+\n+            with mock.patch('json.loads') as mock_json:\n+                for _ in range(2):\n+                    json_response.json()\n+                mock_json.assert_called_once_with(json_body.decode())\n+\n \n class HtmlResponseTest(TextResponseTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#91e505edfe811c8faa6e457ecdae42c3644e6537", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 35 | Churn Cumulative: 1390 | Contributors (this commit): 18 | Commits (past 90d): 11 | Contributors (cumulative): 35 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,5 @@\n-import logging\n import inspect\n+import logging\n \n from scrapy.spiders import Spider\n from scrapy.utils.defer import deferred_from_coro\n@@ -18,7 +18,11 @@ def iterate_spider_output(result):\n         d = deferred_from_coro(collect_asyncgen(result))\n         d.addCallback(iterate_spider_output)\n         return d\n-    return arg_to_iter(deferred_from_coro(result))\n+    elif inspect.iscoroutine(result):\n+        d = deferred_from_coro(result)\n+        d.addCallback(iterate_spider_output)\n+        return d\n+    return arg_to_iter(result)\n \n \n def iter_spider_classes(module):\n\n@@ -119,6 +119,17 @@ class AsyncDefAsyncioReturnSpider(SimpleSpider):\n         return [{'id': 1}, {'id': 2}]\n \n \n+class AsyncDefAsyncioReturnSingleElementSpider(SimpleSpider):\n+\n+    name = \"asyncdef_asyncio_return_single_element\"\n+\n+    async def parse(self, response):\n+        await asyncio.sleep(0.1)\n+        status = await get_from_asyncio_queue(response.status)\n+        self.logger.info(\"Got response %d\" % status)\n+        return {\"foo\": 42}\n+\n+\n class AsyncDefAsyncioReqsReturnSpider(SimpleSpider):\n \n     name = 'asyncdef_asyncio_reqs_return'\n\n@@ -21,6 +21,7 @@ from scrapy.utils.python import to_unicode\n from tests.mockserver import MockServer\n from tests.spiders import (\n     AsyncDefAsyncioReqsReturnSpider,\n+    AsyncDefAsyncioReturnSingleElementSpider,\n     AsyncDefAsyncioReturnSpider,\n     AsyncDefAsyncioSpider,\n     AsyncDefSpider,\n@@ -364,6 +365,21 @@ with multiples lines\n         self.assertIn({'id': 1}, items)\n         self.assertIn({'id': 2}, items)\n \n+    @mark.only_asyncio()\n+    @defer.inlineCallbacks\n+    def test_async_def_asyncio_parse_items_single_element(self):\n+        items = []\n+\n+        def _on_item_scraped(item):\n+            items.append(item)\n+\n+        crawler = self.runner.create_crawler(AsyncDefAsyncioReturnSingleElementSpider)\n+        crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n+        with LogCapture() as log:\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+        self.assertIn(\"Got response 200\", str(log))\n+        self.assertIn({\"foo\": 42}, items)\n+\n     @mark.skipif(sys.version_info < (3, 6), reason=\"Async generators require Python 3.6 or higher\")\n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0cabf406d5c50060ac2da4e5c76d704606bee990", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 26 | Churn Cumulative: 263 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: 0.9333333333333333\n\nDIFF:\n@@ -4,6 +4,7 @@ import string\n from importlib import import_module\n from os.path import join, exists, abspath\n from shutil import ignore_patterns, move, copy2, copystat\n+import stat\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n@@ -79,6 +80,28 @@ class Command(ScrapyCommand):\n                 copy2(srcname, dstname)\n         copystat(src, dst)\n \n+    def _set_rw_permissions(self, path):\n+        \"\"\"\n+        Sets permissions of a directory tree to +rw and +rwx for folders.\n+        This is necessary if the start template files come without write\n+        permissions.\n+        \"\"\"\n+        mode_rw = (stat.S_IRUSR\n+                   | stat.S_IWUSR\n+                   | stat.S_IRGRP\n+                   | stat.S_IROTH)\n+\n+        mode_x = (stat.S_IXUSR\n+                  | stat.S_IXGRP\n+                  | stat.S_IXOTH)\n+\n+        os.chmod(path, mode_rw | mode_x)\n+        for root, dirs, files in os.walk(path):\n+            for dir in dirs:\n+                os.chmod(join(root, dir), mode_rw | mode_x)\n+            for file in files:\n+                os.chmod(join(root, file), mode_rw)\n+\n     def run(self, args, opts):\n         if len(args) not in (1, 2):\n             raise UsageError()\n@@ -99,6 +122,9 @@ class Command(ScrapyCommand):\n             return\n \n         self._copytree(self.templates_dir, abspath(project_dir))\n+\n+        self._set_rw_permissions(abspath(project_dir))\n+\n         move(join(project_dir, 'module'), join(project_dir, project_name))\n         for paths in TEMPLATES_TO_RENDER:\n             path = join(*paths)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2df3b54c7d2666a95793ea6788e88d1ae945e0f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 266 | Contributors (this commit): 13 | Commits (past 90d): 3 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -79,6 +79,7 @@ class Command(ScrapyCommand):\n             else:\n                 copy2(srcname, dstname)\n         copystat(src, dst)\n+        self._set_rw_permissions(dst)\n \n     def _set_rw_permissions(self, path):\n         \"\"\"\n@@ -123,8 +124,6 @@ class Command(ScrapyCommand):\n \n         self._copytree(self.templates_dir, abspath(project_dir))\n \n-        self._set_rw_permissions(abspath(project_dir))\n-\n         move(join(project_dir, 'module'), join(project_dir, project_name))\n         for paths in TEMPLATES_TO_RENDER:\n             path = join(*paths)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#76a2cbf0ff7a060833812704a0416ba617ddc8b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 269 | Contributors (this commit): 14 | Commits (past 90d): 4 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -1,10 +1,10 @@\n import re\n import os\n+import stat\n import string\n from importlib import import_module\n from os.path import join, exists, abspath\n from shutil import ignore_patterns, move, copy2, copystat\n-import stat\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n@@ -123,7 +123,6 @@ class Command(ScrapyCommand):\n             return\n \n         self._copytree(self.templates_dir, abspath(project_dir))\n-\n         move(join(project_dir, 'module'), join(project_dir, project_name))\n         for paths in TEMPLATES_TO_RENDER:\n             path = join(*paths)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8b549392f924ddad9536e55c6120638daf688dfd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 907 | Contributors (this commit): 25 | Commits (past 90d): 4 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -28,8 +28,8 @@ twisted_version = (_txv.major, _txv.minor, _txv.micro)\n \n \n # Check minimum required Python version\n-if sys.version_info < (3, 5):\n-    print(\"Scrapy %s requires Python 3.5\" % __version__)\n+if sys.version_info < (3, 5, 2):\n+    print(\"Scrapy %s requires Python 3.5.2\" % __version__)\n     sys.exit(1)\n \n \n\n@@ -66,7 +66,7 @@ setup(\n         'Topic :: Software Development :: Libraries :: Application Frameworks',\n         'Topic :: Software Development :: Libraries :: Python Modules',\n     ],\n-    python_requires='>=3.5',\n+    python_requires='>=3.5.2',\n     install_requires=[\n         'Twisted>=17.9.0',\n         'cryptography>=2.0',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d09ccf8d3b2932d9393e6ce4a20c46befdd43acf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 126 | Lines Deleted: 59 | Files Changed: 2 | Hunks: 27 | Methods Changed: 25 | Complexity Δ (Sum/Max): 2/7 | Churn Δ: 185 | Churn Cumulative: 633 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 0.9333333333333333\n\nDIFF:\n@@ -1,4 +1,5 @@\n import logging\n+from typing import Dict, List\n \n from h2.config import H2Configuration\n from h2.connection import H2Connection\n@@ -7,45 +8,19 @@ from h2.events import (\n     StreamEnded, StreamReset, TrailersReceived, WindowUpdated\n )\n from twisted.internet.protocol import connectionDone, Protocol\n-from zope.interface import implementer, Interface\n \n from scrapy.core.http2.stream import Stream\n from scrapy.http import Request\n \n LOGGER = logging.getLogger(__name__)\n+LOGGER.debug = print\n \n \n-class IH2EventsHandler(Interface):\n-    def connection_terminated(event: ConnectionTerminated):\n-        pass\n-\n-    def data_received(event: DataReceived):\n-        pass\n-\n-    def response_received(event: ResponseReceived):\n-        pass\n-\n-    def remote_settings_changed(event: RemoteSettingsChanged):\n-        pass\n-\n-    def stream_ended(event: StreamEnded):\n-        pass\n-\n-    def stream_reset(event: StreamReset):\n-        pass\n-\n-    def trailers_received(event: TrailersReceived):\n-        pass\n-\n-    def window_updated(event: WindowUpdated):\n-        pass\n-\n-\n-@implementer(IH2EventsHandler)\n class H2ClientProtocol(Protocol):\n-    # TODO: Check for user-agent while testing\n-    # TODO: Add support for cookies\n-    # TODO: Handle priority updates\n+    # TODO:\n+    #  1. Check for user-agent while testing\n+    #  2. Add support for cookies\n+    #  3. Handle priority updates\n \n     def __init__(self):\n         config = H2Configuration(client_side=True, header_encoding='utf-8')\n@@ -57,14 +32,14 @@ class H2ClientProtocol(Protocol):\n         self.next_stream_id = 1\n \n         # Streams are stored in a dictionary keyed off their stream IDs\n-        self.streams = {}\n+        self.streams: Dict[int, Stream] = {}\n \n         # Boolean to keep track the connection is made\n         # If requests are received before connection is made\n         # we keep all requests in a pool and send them as the connection\n         # is made\n         self.is_connection_made = False\n-        self._pending_request_stream_pool = []\n+        self._pending_request_stream_pool: List[Stream] = []\n \n     def _new_stream(self, request: Request):\n         \"\"\"Instantiates a new Stream object\n@@ -100,29 +75,27 @@ class H2ClientProtocol(Protocol):\n         \"\"\"Called by Twisted when the connection is established. We can start\n         sending some data now: we should open with the connection preamble.\n         \"\"\"\n-        LOGGER.info(\"Connection made to {}\".format(self.transport))\n+        LOGGER.debug(\"Connection made to {}\".format(self.transport))\n         self.conn.initiate_connection()\n         self._write_to_transport()\n \n         self.is_connection_made = True\n \n-        # Initiate all pending requests\n-        for stream in self._pending_request_stream_pool:\n-            assert isinstance(stream, Stream)\n-            stream.initiate_request()\n-\n-        self._pending_request_stream_pool.clear()\n-\n     def dataReceived(self, data):\n         events = self.conn.receive_data(data)\n         self._handle_events(events)\n         self._write_to_transport()\n \n     def connectionLost(self, reason=connectionDone):\n-\n         \"\"\"Called by Twisted when the transport connection is lost.\n         \"\"\"\n-        for stream_id in self.streams.keys():\n+        LOGGER.debug(f\"connectionLost {reason}\")\n+        stream_ids = list(self.streams.keys())\n+\n+        for stream in self._pending_request_stream_pool:\n+            stream_ids.remove(stream.stream_id)\n+\n+        for stream_id in stream_ids:\n             # TODO: Close each Stream instance in a clean manner\n             self.conn.end_stream(stream_id)\n \n@@ -135,6 +108,7 @@ class H2ClientProtocol(Protocol):\n                 triggered by sending data\n         \"\"\"\n         for event in events:\n+            LOGGER.debug(event)\n             if isinstance(event, ConnectionTerminated):\n                 self.connection_terminated(event)\n             elif isinstance(event, DataReceived):\n@@ -155,36 +129,60 @@ class H2ClientProtocol(Protocol):\n     def send_headers(self, stream_id, headers):\n         \"\"\"Send the headers for a given stream to the resource\n         Initiates a new connection hence.\n+        This function is wrapper for :func:`~h2.connection.H2Connection.send_headers`\n \n         Arguments:\n             stream_id {int} -- Valid stream id\n             headers {List[Tuple[str, str]]} -- Headers of the request\n         \"\"\"\n-        if stream_id in self.streams:\n-            self.conn.send_headers(stream_id, headers, end_stream=True)\n-            self._write_to_transport()\n-        else:\n-            pass\n+        LOGGER.debug(f'Send Headers: stream_id={stream_id} headers={headers}')\n+        self.conn.send_headers(stream_id, headers, end_stream=False)\n \n+    def send_data(self, stream_id, data):\n+        \"\"\"Send the data for a given stream to the resource.\n+        Requires request headers to be sent at least once before this\n+        function is called.\n+        This function is wrapper for :func:`~h2.connection.H2Connection.send_data`\n+\n+        Arguments:\n+            stream_id {int} -- Valid stream id\n+            data {bytes} -- The data to send on the stream.\n+        \"\"\"\n+        LOGGER.debug(f\"Send Data: stream_id={stream_id} data={data}\")\n+        self.conn.send_data(stream_id, data, end_stream=False)\n+\n+    def end_stream(self, stream_id):\n+        \"\"\"End the given stream.\n+        This function is wrapper for :func:`~h2.connection.H2Connection.end_stream`\n+\n+        Arguments:\n+             stream_id {int} - Valid stream id\n+        \"\"\"\n+        LOGGER.debug(f\"End Stream: stream_id={stream_id}\")\n+        self.conn.end_stream(stream_id)\n+\n+    # Event handler functions starts here\n     def connection_terminated(self, event: ConnectionTerminated):\n         pass\n \n     def data_received(self, event: DataReceived):\n         stream_id = event.stream_id\n-        # TODO: Stream do not exist in self.streams dict\n         self.streams[stream_id].receive_data(event.data)\n \n     def response_received(self, event: ResponseReceived):\n         stream_id = event.stream_id\n-        # TODO: Stream do not exist in self.streams dict\n         self.streams[stream_id].receive_headers(event.headers)\n \n     def remote_settings_changed(self, event: RemoteSettingsChanged):\n-        pass\n+        # TODO: handle MAX_CONCURRENT_STREAMS\n+        # Initiate all pending requests\n+        for stream in self._pending_request_stream_pool:\n+            stream.initiate_request()\n+\n+        self._pending_request_stream_pool.clear()\n \n     def stream_ended(self, event: StreamEnded):\n         stream_id = event.stream_id\n-        # TODO: Stream do not exist in self.streams dict\n         self.streams[stream_id].end_stream()\n \n     def stream_reset(self, event: StreamReset):\n@@ -194,4 +192,6 @@ class H2ClientProtocol(Protocol):\n         pass\n \n     def window_updated(self, event: WindowUpdated):\n-        pass\n+        stream_id = event.stream_id\n+        if stream_id != 0:\n+            self.streams[stream_id].window_updated()\n\n@@ -23,13 +23,23 @@ class Stream:\n             stream_id {int} -- For one HTTP/2 connection each stream is\n                 uniquely identified by a single integer\n             request {Request} -- HTTP request\n-            connection {H2ClientProtocol} -- HTTP/2 connection this stream belongs to\n+            connection {H2Connection} -- HTTP/2 connection this stream belongs to.\n         \"\"\"\n-\n         self.stream_id = stream_id\n         self._request = request\n-        self._conn = connection\n+        self._client_protocol = connection\n \n+        self._request_body = self._request.body\n+        self.content_length = 0 if self._request_body is None else len(self._request_body)\n+\n+        # Each time we send a data frame, we will decrease value by the amount send.\n+        self.remaining_content_length = self.content_length\n+\n+        # Flag to keep track whether we have ended this stream\n+        self.stream_ended = True\n+\n+        # Data received frame by frame from the server is appended\n+        # and passed to the response Deferred when completely received.\n         self._response_data = b\"\"\n \n         # Headers received after sending the request\n@@ -59,10 +69,67 @@ class Stream:\n \n             # TODO: Check if scheme can be \"http\" for HTTP/2 ?\n             (\":scheme\", \"https\"),\n-            (\":path\", url.path)\n+            (\":path\", url.path),\n+            # (\"Content-Length\", str(self.content_length))\n+\n+            # TODO: Make sure 'Content-Type' and 'Content-Encoding' headers\n+            #  are sent for request having body\n         ]\n \n-        self._conn.send_headers(self.stream_id, http2_request_headers)\n+        self._client_protocol.send_headers(self.stream_id, http2_request_headers)\n+        self.send_data()\n+\n+    def send_data(self):\n+        \"\"\"Called immediately after the headers are sent. Here we send all the\n+         data as part of the request.\n+\n+         If the content length is 0 initially then we end the stream immediately and\n+         wait for response data.\n+         \"\"\"\n+\n+        # TODO:\n+        #  1. Add test for sending very large data\n+        #  2. Add test for small data\n+        #  3. Both (1) and (2) should be tested for\n+        #    3.1 Large number of request\n+        #    3.2 Small number of requests\n+\n+        # Firstly, check what the flow control window is for current stream.\n+        window_size = self._client_protocol.conn.local_flow_control_window(stream_id=self.stream_id)\n+\n+        # Next, check what the maximum frame size is.\n+        max_frame_size = self._client_protocol.conn.max_outbound_frame_size\n+\n+        # We will send no more than the window size or the remaining file size\n+        # of data in this call, whichever is smaller.\n+        bytes_to_send = min(window_size, self.remaining_content_length)\n+\n+        # We now need to send a number of data frames.\n+        while bytes_to_send > 0:\n+            chunk_size = min(bytes_to_send, max_frame_size)\n+\n+            data_chunk_start = self.content_length - self.remaining_content_length\n+            data_chunk = self._request_body[data_chunk_start:data_chunk_start + chunk_size]\n+\n+            self._client_protocol.send_data(self.stream_id, data_chunk, end_stream=False)\n+\n+            bytes_to_send = max(0, bytes_to_send - chunk_size)\n+            self.remaining_content_length = max(0, self.remaining_content_length - chunk_size)\n+\n+        # End the stream if no more data has to be send\n+        if self.remaining_content_length == 0:\n+            self._client_protocol.end_stream(self.stream_id)\n+        else:\n+            # TODO: Continue from here :)\n+            pass\n+\n+    def window_updated(self):\n+        \"\"\"Flow control window size was changed.\n+        Send data that earlier could not be sent as we were\n+        blocked behind the flow control.\n+        \"\"\"\n+        if self.remaining_content_length > 0 and not self.stream_ended:\n+            self.send_data()\n \n     def receive_data(self, data: bytes):\n         self._response_data += data\n@@ -72,7 +139,7 @@ class Stream:\n             self._response_headers[name] = value\n \n     def end_stream(self):\n-        \"\"\"Stream is ended by the resource hence no further\n+        \"\"\"Stream is ended by the server hence no further\n         data or headers should be expected on this stream.\n \n         We will call the response deferred callback passing\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
