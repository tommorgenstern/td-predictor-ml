{"custom_id": "keras#fea95703f49b617269ce3af5bcb2d1b9527d9a96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 15 | Churn Cumulative: 1599 | Contributors (this commit): 14 | Commits (past 90d): 68 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -249,6 +249,21 @@ class Reshape(Layer):\n         return {\"name\":self.__class__.__name__,\n             \"dims\":self.dims}\n \n+class Permute(Layer):\n+    '''\n+        Permute the dimensions of the data according to the given tuple\n+    '''\n+    def __init__(self, dims):\n+        super(Permute,self).__init__()\n+        self.dims = dims\n+\n+    def get_output(self, train):\n+        X = self.get_input(train)\n+        return X.dimshuffle(self.dims)\n+\n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"dims\":self.dims}\n \n class Flatten(Layer):\n     '''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#91a15fdf43d17d005c82c3dc649baa8f6ff06c36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 79 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -2,12 +2,12 @@ from setuptools import setup\n from setuptools import find_packages\n \n setup(name = 'Keras',\n-      version = '0.1.1',\n+      version = '0.1.2',\n       description = 'Theano-based Deep Learning library',\n       author = 'Francois Chollet',\n       author_email = 'francois.chollet@gmail.com',\n       url = 'https://github.com/fchollet/keras',\n-      download_url = 'https://github.com/fchollet/keras/tarball/0.1.1',\n+      download_url = 'https://github.com/fchollet/keras/tarball/0.1.2',\n       license = 'MIT',\n       install_requires = ['theano', 'pyyaml'],\n       packages = find_packages(),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4b6bf1dbfe0d0b49f876c701b95a6ff66e9f206d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1601 | Contributors (this commit): 14 | Commits (past 90d): 67 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -259,7 +259,7 @@ class Permute(Layer):\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        return X.dimshuffle(self.dims)\n+        return X.dimshuffle((0,) + self.dims)\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#62a4f29a712ef5b898d4c7793a7125fa991ca05c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 20 | Churn Cumulative: 20 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,20 @@\n+from __future__ import print_function\n+import numpy as np\n+import theano\n+\n+\n+def print_layer_shapes(model, input_shape):\n+    \"\"\"\n+    Utility function that prints the shape of the output at each layer.\n+\n+    Arguments:\n+        model: An instance of models.Model\n+        input_shape: The shape of the input you will provide to the model.\n+    \"\"\"\n+    input_var = model.get_input(train=False)\n+    input_tmp = np.zeros(input_shape, dtype=np.float32)\n+    print(\"input shape : \", input_shape)\n+    for l in model.layers:\n+        shape_f = theano.function([input_var], l.get_output(train=False).shape)\n+        out_shape = shape_f(input_tmp)\n+        print('shape after', l.get_config()['name'], \":\", out_shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b4b5a654b52c5963ce3c3834444eb30f57986e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 6 | Churn Cumulative: 2325 | Contributors (this commit): 19 | Commits (past 90d): 83 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -139,7 +139,13 @@ class Model(object):\n             batches = make_batches(nb_train_sample, batch_size)\n             for batch_index, (batch_start, batch_end) in enumerate(batches):\n                 batch_ids = index_array[batch_start:batch_end]\n+                try:\n                     ins_batch = slice_X(ins, batch_ids)\n+                except TypeError as err:\n+                    print(\n+                        '\\n!! TypeError while preparing batch.',\n+                        'If using HDF5 input data, pass shuffle=\\'batch\\'.\\n')\n+                    raise\n \n                 batch_logs = {}\n                 batch_logs['batch'] = batch_index\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#98d49754edd5b5757fb7c37c8a3cdc9a65d6f238", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 13 | Churn Cumulative: 151 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -60,13 +60,10 @@ model.add(Activation('softmax'))\n model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n # helper function to sample an index from a probability array\n-def sample(a, diversity=0.75):\n-    if random.random() > diversity:\n-        return np.argmax(a)\n-    while 1:\n-        i = random.randint(0, len(a)-1)\n-        if a[i] > random.random():\n-            return i\n+def sample(a, temperature=1.0):\n+    a = np.log(a)/temperature\n+    a = np.exp(a)/np.sum(np.exp(a))\n+    return np.argmax(np.random.multinomial(1,a,1))\n \n # train the model, output generated text after each iteration\n for iteration in range(1, 60):\n@@ -77,7 +74,7 @@ for iteration in range(1, 60):\n \n     start_index = random.randint(0, len(text) - maxlen - 1)\n \n-    for diversity in [0.2, 0.4, 0.6, 0.8]:\n+    for diversity in [0.2, 0.5, 1.0, 1.2]:\n         print()\n         print('----- diversity:', diversity)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#36a9a3947368bc6763ba2c733d501c89ad1b2f67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 26 | Churn Cumulative: 60 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,5 @@\n from ..layers.core import Layer\n-from ..utils.theano_utils import shared_zeros\n+from ..utils.theano_utils import shared_zeros, shared_ones\n \n class LeakyReLU(Layer):\n     def __init__(self, alpha=0.3):\n@@ -36,3 +36,27 @@ class PReLU(Layer):\n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n         \"input_shape\":self.input_shape}\n+\n+\n+class Psoftplus(Layer):\n+    '''\n+        Parametric softplus of the form: alpha * (1 + exp(beta * X))\n+\n+        Reference:\n+            Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs\n+            http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003143\n+    '''\n+    def __init__(self, input_shape):\n+        super(Psoftplus,self).__init__()\n+        self.alphas = shared_ones(input_shape)\n+        self.betas = shared_ones(input_shape)\n+        self.params = [self.alphas, self.betas]\n+        self.input_shape = input_shape\n+\n+    def get_output(self, train):\n+        X = self.get_input(train)\n+        return T.nnet.softplus(self.betas * X) * self.alphas\n+\n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"input_shape\":self.input_shape}\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dc50928c1852561e8763e2d6ea94ccd8c930ebe1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 61 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,6 @@\n from ..layers.core import Layer\n from ..utils.theano_utils import shared_zeros, shared_ones\n+import theano.tensor as T\n \n class LeakyReLU(Layer):\n     def __init__(self, alpha=0.3):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2ada7d16cb90e6b8239a8d863c3294f03f372bbc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 126 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,5 @@\n from ..layers.core import Layer\n-from ..utils.theano_utils import shared_zeros, shared_ones\n+from ..utils.theano_utils import shared_zeros, shared_ones, shared_scalars\n import theano.tensor as T\n \n class LeakyReLU(Layer):\n@@ -49,8 +49,8 @@ class Psoftplus(Layer):\n     '''\n     def __init__(self, input_shape):\n         super(Psoftplus,self).__init__()\n-        self.alphas = shared_ones(input_shape)\n-        self.betas = shared_ones(input_shape)\n+        self.alphas = shared_scalars(input_shape, 0.2)\n+        self.betas = shared_scalars(input_shape, 5.0)\n         self.params = [self.alphas, self.betas]\n         self.input_shape = input_shape\n \n\n@@ -18,6 +18,9 @@ def shared_scalar(val=0., dtype=theano.config.floatX, name=None):\n def shared_ones(shape, dtype=theano.config.floatX, name=None):\n     return sharedX(np.ones(shape), dtype=dtype, name=name)\n \n+def shared_scalars(shape, val, dtype=theano.config.floatX, name=None):\n+    return sharedX(np.ones(shape)*val, dtype=dtype, name=name)\n+\n def alloc_zeros_matrix(*dims):\n     return T.alloc(np.cast[theano.config.floatX](0.), *dims)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2fbfbdddf6639d71dc990bdb1ccc603d3c7038d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 5 | Churn Cumulative: 2330 | Contributors (this commit): 19 | Commits (past 90d): 83 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -175,9 +175,8 @@ class Model(object):\n                 try:\n                     ins_batch = slice_X(ins, batch_ids)\n                 except TypeError as err:\n-                    print(\n-                        '\\n!! TypeError while preparing batch.',\n-                        'If using HDF5 input data, pass shuffle=\\'batch\\'.\\n')\n+                    print('TypeError while preparing batch. \\\n+                        If using HDF5 input data, pass shuffle=\"batch\".\\n')\n                     raise\n \n                 batch_logs = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ec8f7f00173599016e0fea01f081f465d91b1813", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 169 | Lines Deleted: 114 | Files Changed: 34 | Hunks: 192 | Methods Changed: 25 | Complexity Δ (Sum/Max): 3/6 | Churn Δ: 283 | Churn Cumulative: 10272 | Contributors (this commit): 43 | Commits (past 90d): 430 | Contributors (cumulative): 133 | DMM Complexity: 0.0\n\nDIFF:\n@@ -114,11 +114,3 @@ else:\n         for X_batch, Y_batch in datagen.flow(X_test, Y_test):\n             score = model.test_on_batch(X_batch, Y_batch)\n             progbar.add(X_batch.shape[0], values=[(\"test loss\", score)])\n-\n-            \n-\n-\n-\n-\n-\n-\n\n@@ -36,6 +36,7 @@ from sklearn.preprocessing import StandardScaler\n     Get the data from Kaggle: https://www.kaggle.com/c/otto-group-product-classification-challenge/data\n '''\n \n+\n def load_data(path, train=True):\n     df = pd.read_csv(path)\n     X = df.values.copy()\n@@ -47,6 +48,7 @@ def load_data(path, train=True):\n         X, ids = X[:, 1:].astype(np.float32), X[:, 0].astype(str)\n         return X, ids\n \n+\n def preprocess_data(X, scaler=None):\n     if not scaler:\n         scaler = StandardScaler()\n@@ -54,6 +56,7 @@ def preprocess_data(X, scaler=None):\n     X = scaler.transform(X)\n     return X, scaler\n \n+\n def preprocess_labels(labels, encoder=None, categorical=True):\n     if not encoder:\n         encoder = LabelEncoder()\n@@ -63,6 +66,7 @@ def preprocess_labels(labels, encoder=None, categorical=True):\n         y = np_utils.to_categorical(y)\n     return y, encoder\n \n+\n def make_submission(y_prob, ids, encoder, fname):\n     with open(fname, 'w') as f:\n         f.write('id,')\n\n@@ -20,8 +20,6 @@ batch_size = 128\n nb_classes = 10\n nb_epoch = 20\n \n-\n-\n # the data, shuffled and split between tran and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n \n\n@@ -66,6 +66,7 @@ html_tags = re.compile(r'<.*?>')\n to_replace = [('&#x27;', \"'\")]\n hex_tags = re.compile(r'&.*?;')\n \n+\n def clean_comment(comment):\n     c = str(comment.encode(\"utf-8\"))\n     c = html_tags.sub(' ', c)\n@@ -74,6 +75,7 @@ def clean_comment(comment):\n     c = hex_tags.sub(' ', c)\n     return c\n \n+\n def text_generator(path=data_path):\n     f = open(path)\n     for i, l in enumerate(f):\n@@ -158,18 +160,21 @@ word_index = tokenizer.word_index\n reverse_word_index = dict([(v, k) for k, v in list(word_index.items())])\n word_index = tokenizer.word_index\n \n+\n def embed_word(w):\n     i = word_index.get(w)\n     if (not i) or (i < skip_top) or (i >= max_features):\n         return None\n     return norm_weights[i]\n \n+\n def closest_to_point(point, nb_closest=10):\n     proximities = np.dot(norm_weights, point)\n     tups = list(zip(list(range(len(proximities))), proximities))\n     tups.sort(key=lambda x: x[1], reverse=True)\n     return [(reverse_word_index.get(t[0]), t[1]) for t in tups[:nb_closest]]\n \n+\n def closest_to_word(w, nb_closest=10):\n     i = word_index.get(w)\n     if (not i) or (i < skip_top) or (i >= max_features):\n@@ -190,7 +195,8 @@ def closest_to_word(w, nb_closest=10):\n     and frequency subsampling of factor 10e-5.\n '''\n \n-words = [\"article\", # post, story, hn, read, comments\n+words = [\n+    \"article\", # post, story, hn, read, comments\n     \"3\", # 6, 4, 5, 2\n     \"two\", # three, few, several, each\n     \"great\", # love, nice, working, looking\n@@ -214,4 +220,3 @@ for w in words:\n     print('====', w)\n     for r in res:\n         print(r)\n-\n\n@@ -1,35 +1,44 @@\n from __future__ import absolute_import\n import theano.tensor as T\n \n+\n def softmax(x):\n     return T.nnet.softmax(x.reshape((-1, x.shape[-1]))).reshape(x.shape)\n \n+\n def time_distributed_softmax(x):\n     import warnings\n     warnings.warn(\"time_distributed_softmax is deprecated. Just use softmax!\", DeprecationWarning)\n     return softmax(x)\n \n+\n def softplus(x):\n     return T.nnet.softplus(x)\n \n+\n def relu(x):\n     return (x + abs(x)) / 2.0\n \n+\n def tanh(x):\n     return T.tanh(x)\n \n+\n def sigmoid(x):\n     return T.nnet.sigmoid(x)\n \n+\n def hard_sigmoid(x):\n     return T.nnet.hard_sigmoid(x)\n \n+\n def linear(x):\n     '''\n     The function returns the variable that is passed in, so all types work\n     '''\n     return x\n \n+\n from .utils.generic_utils import get_from_module\n def get(identifier):\n     return get_from_module(identifier, globals(), 'activation function')\n\n@@ -1,16 +1,17 @@\n from __future__ import absolute_import\n from __future__ import print_function\n+\n import theano\n import theano.tensor as T\n import numpy as np\n-import warnings\n-import time, json\n+\n+import time, json, warnings\n from collections import deque\n \n from .utils.generic_utils import Progbar\n \n-class CallbackList(object):\n \n+class CallbackList(object):\n     def __init__(self, callbacks=[], queue_length=10):\n         self.callbacks = [c for c in callbacks]\n         self.queue_length = queue_length\n@@ -43,8 +44,7 @@ class CallbackList(object):\n             callback.on_batch_begin(batch, logs)\n         self._delta_ts_batch_begin.append(time.time() - t_before_callbacks)\n         delta_t_median = np.median(self._delta_ts_batch_begin)\n-        if self._delta_t_batch > 0. and delta_t_median > 0.95 * self._delta_t_batch \\\n-            and delta_t_median > 0.1:\n+        if self._delta_t_batch > 0. and delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1:\n             warnings.warn('Method on_batch_begin() is slow compared '\n                           'to the batch update (%f). Check your callbacks.' % delta_t_median)\n         self._t_enter_batch = time.time()\n@@ -56,8 +56,7 @@ class CallbackList(object):\n             callback.on_batch_end(batch, logs)\n         self._delta_ts_batch_end.append(time.time() - t_before_callbacks)\n         delta_t_median = np.median(self._delta_ts_batch_end)\n-        if self._delta_t_batch > 0. and delta_t_median > 0.95 * self._delta_t_batch \\\n-            and delta_t_median > 0.1:\n+        if self._delta_t_batch > 0. and delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1:\n             warnings.warn('Method on_batch_end() is slow compared '\n                           'to the batch update (%f). Check your callbacks.' % delta_t_median)\n \n@@ -99,8 +98,8 @@ class Callback(object):\n     def on_train_end(self, logs={}):\n         pass\n \n-class BaseLogger(Callback):\n \n+class BaseLogger(Callback):\n     def on_train_begin(self, logs={}):\n         self.verbose = self.params['verbose']\n \n@@ -177,8 +176,8 @@ class History(Callback):\n \n class ModelCheckpoint(Callback):\n     def __init__(self, filepath, monitor='val_loss', verbose=0, save_best_only=False):\n-        super(Callback, self).__init__()\n \n+        super(Callback, self).__init__()\n         self.monitor = monitor\n         self.verbose = verbose\n         self.filepath = filepath\n@@ -240,7 +239,6 @@ class RemoteMonitor(Callback):\n         self.seen = 0\n         self.totals = {}\n \n-\n     def on_batch_end(self, batch, logs={}):\n         batch_size = logs.get('size', 0)\n         self.seen += batch_size\n\n@@ -3,6 +3,7 @@ import theano\n import theano.tensor as T\n import numpy as np\n \n+\n class Constraint(object):\n     def __call__(self, p):\n         return p\n@@ -10,6 +11,7 @@ class Constraint(object):\n     def get_config(self):\n         return {\"name\": self.__class__.__name__}\n \n+\n class MaxNorm(Constraint):\n     def __init__(self, m=2):\n         self.m = m\n@@ -24,11 +26,13 @@ class MaxNorm(Constraint):\n         return {\"name\": self.__class__.__name__,\n                 \"m\": self.m}\n \n+\n class NonNeg(Constraint):\n     def __call__(self, p):\n         p *= T.ge(p, 0)\n         return p\n \n+\n class UnitNorm(Constraint):\n     def __call__(self, p):\n         return p / T.sqrt(T.sum(p**2, axis=-1, keepdims=True))\n\n@@ -1,9 +1,12 @@\n from __future__ import absolute_import\n from __future__ import print_function\n+\n import tarfile, inspect, os\n from six.moves.urllib.request import urlretrieve\n+\n from ..utils.generic_utils import Progbar\n \n+\n def get_file(fname, origin, untar=False):\n     datadir = os.path.expanduser(os.path.join('~', '.keras', 'datasets'))\n     if not os.path.exists(datadir):\n@@ -22,6 +25,7 @@ def get_file(fname, origin, untar=False):\n \n         global progbar\n         progbar = None\n+\n         def dl_progress(count, block_size, total_size):\n             global progbar\n             if progbar is None:\n@@ -41,4 +45,3 @@ def get_file(fname, origin, untar=False):\n         return untar_fpath\n \n     return fpath\n-\n\n@@ -5,6 +5,7 @@ import numpy as np\n \n from .utils.theano_utils import sharedX, shared_zeros, shared_ones\n \n+\n def get_fans(shape):\n     fan_in = shape[0] if len(shape) == 2 else np.prod(shape[1:])\n     fan_out = shape[1] if len(shape) == 2 else shape[0]\n@@ -14,9 +15,11 @@ def get_fans(shape):\n def uniform(shape, scale=0.05):\n     return sharedX(np.random.uniform(low=-scale, high=scale, size=shape))\n \n+\n def normal(shape, scale=0.05):\n     return sharedX(np.random.randn(*shape) * scale)\n \n+\n def lecun_uniform(shape):\n     ''' Reference: LeCun 98, Efficient Backprop\n         http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n@@ -25,6 +28,7 @@ def lecun_uniform(shape):\n     scale = np.sqrt(3. / fan_in)\n     return uniform(shape, scale)\n \n+\n def glorot_normal(shape):\n     ''' Reference: Glorot & Bengio, AISTATS 2010\n     '''\n@@ -32,11 +36,13 @@ def glorot_normal(shape):\n     s = np.sqrt(2. / (fan_in + fan_out))\n     return normal(shape, s)\n \n+\n def glorot_uniform(shape):\n     fan_in, fan_out = get_fans(shape)\n     s = np.sqrt(6. / (fan_in + fan_out))\n     return uniform(shape, s)\n \n+\n def he_normal(shape):\n     ''' Reference:  He et al., http://arxiv.org/abs/1502.01852\n     '''\n@@ -44,30 +50,36 @@ def he_normal(shape):\n     s = np.sqrt(2. / fan_in)\n     return normal(shape, s)\n \n+\n def he_uniform(shape):\n     fan_in, fan_out = get_fans(shape)\n     s = np.sqrt(6. / fan_in)\n     return uniform(shape, s)\n \n+\n def orthogonal(shape, scale=1.1):\n     ''' From Lasagne\n     '''\n     flat_shape = (shape[0], np.prod(shape[1:]))\n     a = np.random.normal(0.0, 1.0, flat_shape)\n     u, _, v = np.linalg.svd(a, full_matrices=False)\n-    q = u if u.shape == flat_shape else v # pick the one with the correct shape\n+    # pick the one with the correct shape\n+    q = u if u.shape == flat_shape else v\n     q = q.reshape(shape)\n     return sharedX(scale * q[:shape[0], :shape[1]])\n \n+\n def identity(shape, scale=1):\n     if len(shape) != 2 or shape[0] != shape[1]:\n         raise Exception(\"Identity matrix initialization can only be used for 2D square matrices\")\n     else:\n         return sharedX(scale * np.identity(shape[0]))\n \n+\n def zero(shape):\n     return shared_zeros(shape)\n \n+\n def one(shape):\n     return shared_ones(shape)\n \n\n@@ -1,6 +1,7 @@\n from ..layers.core import Layer\n from ..utils.theano_utils import shared_zeros\n \n+\n class LeakyReLU(Layer):\n     def __init__(self, alpha=0.3):\n         super(LeakyReLU, self).__init__()\n\n@@ -6,6 +6,7 @@ import theano.tensor as T\n from ..layers.core import Layer, Merge\n from six.moves import range\n \n+\n def ndim_tensor(ndim):\n     if ndim == 2:\n         return T.matrix()\n@@ -15,6 +16,7 @@ def ndim_tensor(ndim):\n         return T.tensor4()\n     return T.matrix()\n \n+\n class Sequential(Layer):\n     '''\n         Simple linear stack of layers.\n@@ -179,7 +181,10 @@ class Graph(Layer):\n \n         self.namespace.add(name)\n         self.nodes[name] = layer\n-        self.node_config.append({'name':name, 'input':input, 'inputs':inputs, 'merge_mode':merge_mode})\n+        self.node_config.append({'name': name,\n+                                 'input': input,\n+                                 'inputs': inputs,\n+                                 'merge_mode': merge_mode})\n         params, regularizers, constraints = layer.get_params()\n         self.params += params\n         self.regularizers += regularizers\n@@ -205,7 +210,10 @@ class Graph(Layer):\n             self.outputs[name] = merge\n         self.namespace.add(name)\n         self.output_order.append(name)\n-        self.output_config.append({'name':name, 'input':input, 'inputs':inputs, 'merge_mode':merge_mode})\n+        self.output_config.append({'name': name,\n+                                   'input': input,\n+                                   'inputs': inputs,\n+                                   'merge_mode': merge_mode})\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n\n@@ -9,6 +9,7 @@ from .. import activations, initializations, regularizers, constraints\n from ..utils.theano_utils import shared_zeros\n from ..layers.core import Layer\n \n+\n class Convolution1D(Layer):\n     def __init__(self, input_dim, nb_filter, filter_length,\n         init='uniform', activation='linear', weights=None,\n@@ -179,7 +180,6 @@ class Convolution2D(Layer):\n \n         return self.activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n \n-\n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n                 \"nb_filter\": self.nb_filter,\n@@ -205,7 +205,6 @@ class MaxPooling2D(Layer):\n         self.stride = stride\n         self.ignore_border = ignore_border\n \n-\n     def get_output(self, train):\n         X = self.get_input(train)\n         output = downsample.max_pool_2d(X, ds=self.poolsize, st=self.stride, ignore_border=self.ignore_border)\n\n@@ -14,6 +14,7 @@ from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n from six.moves import zip\n srng = RandomStreams(seed=np.random.randint(10e6))\n \n+\n class Layer(object):\n     def __init__(self):\n         self.params = []\n@@ -90,6 +91,10 @@ class Layer(object):\n \n         return self.params, regularizers, consts\n \n+    def set_name(self, name):\n+        for i in range(len(self.params)):\n+            self.params[i].name = '%s_p%d' % (name, i)\n+\n \n class MaskedLayer(Layer):\n     '''\n@@ -128,7 +133,7 @@ class Merge(object):\n             self.regularizers += regs\n             # params and constraints have the same size\n             for p, c in zip(params, consts):\n-                if not p in self.params:\n+                if p not in self.params:\n                     self.params.append(p)\n                     self.constraints.append(c)\n \n@@ -249,6 +254,7 @@ class Reshape(Layer):\n         return {\"name\": self.__class__.__name__,\n                 \"dims\": self.dims}\n \n+\n class Permute(Layer):\n     '''\n         Permute the dimensions of the data according to the given tuple\n@@ -265,6 +271,7 @@ class Permute(Layer):\n         return {\"name\": self.__class__.__name__,\n                 \"dims\": self.dims}\n \n+\n class Flatten(Layer):\n     '''\n         Reshape input to flat shape.\n@@ -307,7 +314,8 @@ class Dense(Layer):\n         Just your regular fully connected NN layer.\n     '''\n     def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, name=None,\n-        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n+                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n+                 W_constraint=None, b_constraint=None):\n \n         super(Dense, self).__init__()\n         self.init = initializations.get(init)\n@@ -322,7 +330,6 @@ class Dense(Layer):\n         self.params = [self.W, self.b]\n \n         self.regularizers = []\n-        \n         self.W_regularizer = regularizers.get(W_regularizer)\n         if self.W_regularizer:\n             self.W_regularizer.set_param(self.W)\n@@ -402,7 +409,8 @@ class TimeDistributedDense(MaskedLayer):\n \n     '''\n     def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None,\n-        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n+                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n+                 W_constraint=None, b_constraint=None):\n \n         super(TimeDistributedDense, self).__init__()\n         self.init = initializations.get(init)\n@@ -445,7 +453,6 @@ class TimeDistributedDense(MaskedLayer):\n         output = self.activation(T.dot(X.dimshuffle(1, 0, 2), self.W) + self.b)\n         return output.dimshuffle(1, 0, 2)\n \n-\n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n                 \"input_dim\": self.input_dim,\n@@ -526,14 +533,14 @@ class AutoEncoder(Layer):\n                 \"output_reconstruction\": self.output_reconstruction}\n \n \n-\n class MaxoutDense(Layer):\n     '''\n         Max-out layer, nb_feature is the number of pieces in the piecewise linear approx.\n         Refer to http://arxiv.org/pdf/1302.4389.pdf\n     '''\n     def __init__(self, input_dim, output_dim, nb_feature=4, init='glorot_uniform', weights=None,\n-        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n+                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n+                 W_constraint=None, b_constraint=None):\n \n         super(MaxoutDense, self).__init__()\n         self.init = initializations.get(init)\n\n@@ -98,6 +98,7 @@ class WordContextProduct(Layer):\n     '''\n     def __init__(self, input_dim, proj_dim=128,\n                  init='uniform', activation='sigmoid', weights=None):\n+\n         super(WordContextProduct, self).__init__()\n         self.input_dim = input_dim\n         self.proj_dim = proj_dim\n@@ -115,7 +116,6 @@ class WordContextProduct(Layer):\n         if weights is not None:\n             self.set_weights(weights)\n \n-\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         w = self.W_w[X[:, 0]] # nb_samples, proj_dim\n@@ -131,4 +131,3 @@ class WordContextProduct(Layer):\n                 \"proj_dim\": self.proj_dim,\n                 \"init\": self.init.__name__,\n                 \"activation\": self.activation.__name__}\n-\n\n@@ -3,6 +3,7 @@ from .core import srng, MaskedLayer\n import theano\n import theano.tensor as T\n \n+\n class GaussianNoise(MaskedLayer):\n     '''\n         Corruption process with GaussianNoise\n@@ -23,6 +24,7 @@ class GaussianNoise(MaskedLayer):\n         return {\"name\": self.__class__.__name__,\n                 \"sigma\": self.sigma}\n \n+\n class GaussianDropout(MaskedLayer):\n     '''\n         Multiplicative Gaussian Noise\n\n@@ -9,6 +9,7 @@ from ..utils.theano_utils import shared_scalar, shared_zeros, alloc_zeros_matrix\n from ..layers.core import Layer, MaskedLayer\n from six.moves import range\n \n+\n class Recurrent(MaskedLayer):\n     def get_output_mask(self, train=None):\n         if self.return_sequences:\n@@ -87,8 +88,7 @@ class SimpleRNN(Recurrent):\n             # initialization of the output. Input to _step with default tap=-1.\n             outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n             non_sequences=self.U, # static inputs to _step\n-            truncate_gradient=self.truncate_gradient\n-        )\n+            truncate_gradient=self.truncate_gradient)\n \n         if self.return_sequences:\n             return outputs.dimshuffle((1, 0, 2))\n@@ -276,8 +276,7 @@ class GRU(Recurrent):\n             sequences=[x_z, x_r, x_h, padded_mask],\n             outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n             non_sequences=[self.U_z, self.U_r, self.U_h],\n-            truncate_gradient=self.truncate_gradient\n-        )\n+            truncate_gradient=self.truncate_gradient)\n \n         if self.return_sequences:\n             return outputs.dimshuffle((1, 0, 2))\n@@ -295,7 +294,6 @@ class GRU(Recurrent):\n                 \"return_sequences\": self.return_sequences}\n \n \n-\n class LSTM(Recurrent):\n     '''\n         Acts as a spatiotemporal projection,\n@@ -397,8 +395,7 @@ class LSTM(Recurrent):\n                 T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1)\n             ],\n             non_sequences=[self.U_i, self.U_f, self.U_o, self.U_c],\n-            truncate_gradient=self.truncate_gradient \n-        )\n+            truncate_gradient=self.truncate_gradient)\n \n         if self.return_sequences:\n             return outputs.dimshuffle((1, 0, 2))\n@@ -417,7 +414,6 @@ class LSTM(Recurrent):\n                 \"return_sequences\": self.return_sequences}\n \n \n-\n class JZS1(Recurrent):\n     '''\n         Evolved recurrent neural network architectures from the evaluation of thousands\n@@ -506,8 +502,7 @@ class JZS1(Recurrent):\n             sequences=[x_z, x_r, x_h, padded_mask],\n             outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n             non_sequences=[self.U_r, self.U_h],\n-            truncate_gradient=self.truncate_gradient\n-        )\n+            truncate_gradient=self.truncate_gradient)\n         if self.return_sequences:\n             return outputs.dimshuffle((1, 0, 2))\n         return outputs[-1]\n@@ -524,7 +519,6 @@ class JZS1(Recurrent):\n                 \"return_sequences\": self.return_sequences}\n \n \n-\n class JZS2(Recurrent):\n     '''\n         Evolved recurrent neural network architectures from the evaluation of thousands\n@@ -614,8 +608,7 @@ class JZS2(Recurrent):\n             sequences=[x_z, x_r, x_h, padded_mask],\n             outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n             non_sequences=[self.U_z, self.U_r, self.U_h],\n-            truncate_gradient=self.truncate_gradient\n-        )\n+            truncate_gradient=self.truncate_gradient)\n         if self.return_sequences:\n             return outputs.dimshuffle((1, 0, 2))\n         return outputs[-1]\n@@ -632,7 +625,6 @@ class JZS2(Recurrent):\n                 \"return_sequences\": self.return_sequences}\n \n \n-\n class JZS3(Recurrent):\n     '''\n         Evolved recurrent neural network architectures from the evaluation of thousands\n@@ -731,7 +723,3 @@ class JZS3(Recurrent):\n                 \"inner_activation\": self.inner_activation.__name__,\n                 \"truncate_gradient\": self.truncate_gradient,\n                 \"return_sequences\": self.return_sequences}\n-\n-\n-\n-\n\n@@ -3,7 +3,7 @@ from __future__ import print_function\n import theano\n import theano.tensor as T\n import numpy as np\n-import warnings, time, copy, yaml\n+import warnings, time, copy, pprint\n \n from . import optimizers\n from . import objectives\n@@ -11,7 +11,6 @@ from . import regularizers\n from . import constraints\n from . import callbacks as cbks\n \n-import time, copy, pprint\n from .utils.layer_utils import container_from_config\n from .utils.generic_utils import Progbar, printv\n from .layers import containers\n@@ -25,6 +24,7 @@ def standardize_y(y):\n         y = np.expand_dims(y, 1)\n     return y\n \n+\n def batch_shuffle(index_array, batch_size):\n     batch_count = int(len(index_array)/batch_size)\n     # to reshape we need to be cleanly divisible by batch size\n@@ -95,16 +95,21 @@ def model_from_yaml(yaml_string):\n         Returns a model generated from a local yaml file,\n         which is either created by hand or from to_yaml method of Sequential or Graph\n     '''\n+    import yaml\n     model_params = yaml.load(yaml_string)\n     model_name = model_params.get('name')\n-    if not model_name in {'Graph', 'Sequential'}:\n+    if model_name not in {'Graph', 'Sequential'}:\n         raise Exception('Unrecognized model:', model_name)\n \n     # Create a container then set class to appropriate model\n     model = container_from_config(model_params)\n-    model.__class__ = get(model_name)\n+    if model_name == 'Graph':\n+        model.__class__ = Graph\n+    elif model_name == 'Sequential':\n+        model.__class__ = Sequential\n \n-    if 'optimizer' in model_params: # if it has an optimizer, the model is assumed to be compiled\n+    if 'optimizer' in model_params:\n+        # if it has an optimizer, the model is assumed to be compiled\n         loss = objectives.get(model_params.get('loss'))\n         class_mode = model_params.get('class_mode')\n         theano_mode = model_params.get('theano_mode')\n@@ -122,7 +127,7 @@ def model_from_yaml(yaml_string):\n \n \n class Model(object):\n-    def _fit(self, f, ins, out_labels=[], batch_size=128, nb_epoch=100, verbose=1, callbacks=[], \\\n+    def _fit(self, f, ins, out_labels=[], batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n              validation_split=0., val_f=None, val_ins=None, shuffle=True, metrics=[]):\n         '''\n             Abstract fit function for f(*ins). Assume that f returns a list, labelled by out_labels.\n@@ -277,6 +282,7 @@ class Model(object):\n             pp.pprint(config)\n         return config\n \n+\n class Sequential(Model, containers.Sequential):\n     '''\n         Inherits from Model the following methods:\n@@ -376,7 +382,6 @@ class Sequential(Model, containers.Sequential):\n         else:\n             return self._train(*ins)\n \n-\n     def test_on_batch(self, X, y, accuracy=False):\n         X = standardize_X(X)\n         y = standardize_y(y)\n@@ -387,12 +392,10 @@ class Sequential(Model, containers.Sequential):\n         else:\n             return self._test(*ins)\n \n-\n     def predict_on_batch(self, X):\n         ins = standardize_X(X)\n         return self._predict(*ins)\n \n-\n     def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n             validation_split=0., validation_data=None, shuffle=True, show_accuracy=False,\n             class_weight=None, sample_weight=None):\n@@ -427,22 +430,21 @@ class Sequential(Model, containers.Sequential):\n \n         ins = X + [y, sample_weight]\n         metrics = ['loss', 'acc', 'val_loss', 'val_acc']\n-        return self._fit(f, ins, out_labels=out_labels, batch_size=batch_size, nb_epoch=nb_epoch, verbose=verbose, callbacks=callbacks, \\\n-            validation_split=validation_split, val_f=val_f, val_ins=val_ins, shuffle=shuffle, metrics=metrics)\n-\n+        return self._fit(f, ins, out_labels=out_labels, batch_size=batch_size, nb_epoch=nb_epoch,\n+                         verbose=verbose, callbacks=callbacks,\n+                         validation_split=validation_split, val_f=val_f, val_ins=val_ins,\n+                         shuffle=shuffle, metrics=metrics)\n \n     def predict(self, X, batch_size=128, verbose=0):\n         X = standardize_X(X)\n         return self._predict_loop(self._predict, X, batch_size, verbose)[0]\n \n-\n     def predict_proba(self, X, batch_size=128, verbose=1):\n         preds = self.predict(X, batch_size, verbose)\n         if preds.min() < 0 or preds.max() > 1:\n             warnings.warn(\"Network returning invalid probability values.\")\n         return preds\n \n-\n     def predict_classes(self, X, batch_size=128, verbose=1):\n         proba = self.predict(X, batch_size=batch_size, verbose=verbose)\n         if self.class_mode == \"categorical\":\n@@ -450,7 +452,6 @@ class Sequential(Model, containers.Sequential):\n         else:\n             return (proba > 0.5).astype('int32')\n \n-\n     def evaluate(self, X, y, batch_size=128, show_accuracy=False, verbose=1, sample_weight=None):\n         X = standardize_X(X)\n         y = standardize_y(y)\n@@ -467,7 +468,6 @@ class Sequential(Model, containers.Sequential):\n         else:\n             return outs[0]\n \n-\n     def save_weights(self, filepath, overwrite=False):\n         # Save weights from all layers to HDF5\n         import h5py\n@@ -498,7 +498,6 @@ class Sequential(Model, containers.Sequential):\n         f.flush()\n         f.close()\n \n-\n     def load_weights(self, filepath):\n         '''\n             This method does not make use of Sequential.set_weights()\n@@ -513,12 +512,9 @@ class Sequential(Model, containers.Sequential):\n             self.layers[k].set_weights(weights)\n         f.close()\n \n-\n     def to_yaml(self):\n-        '''\n-            Stores a model to yaml string, optionally with all learnable parameters\n-            If the model is compiled, it will also serialize the necessary components\n-        '''\n+        # dump model configuration to yaml string\n+        import yaml\n         model_params = self.get_config()\n         if hasattr(self, 'optimizer'):\n             model_params['class_mode'] = self.class_mode\n@@ -600,8 +596,10 @@ class Graph(Model, containers.Graph):\n         f = self._train\n         out_labels = self.output_order\n         metrics = self.output_order + ['val_' + m for m in self.output_order]\n-        history = self._fit(f, ins, out_labels=out_labels, batch_size=batch_size, nb_epoch=nb_epoch, verbose=verbose, callbacks=callbacks, \\\n-            validation_split=validation_split, val_f=val_f, val_ins=val_ins, shuffle=shuffle, metrics=metrics)\n+        history = self._fit(f, ins, out_labels=out_labels, batch_size=batch_size, nb_epoch=nb_epoch,\n+                            verbose=verbose, callbacks=callbacks,\n+                            validation_split=validation_split, val_f=val_f, val_ins=val_ins,\n+                            shuffle=shuffle, metrics=metrics)\n         return history\n \n     def evaluate(self, data, batch_size=128, verbose=0):\n@@ -652,17 +650,11 @@ class Graph(Model, containers.Graph):\n         f.close()\n \n     def to_yaml(self):\n-        '''\n-            Stores a model to yaml string, optionally with all learnable parameters\n-            If the model is compiled, it will also serialize the necessary components\n-        '''\n+        # dump model configuration to yaml string\n+        import yaml\n         model_params = self.get_config()\n         if hasattr(self, 'optimizer'):\n             model_params['theano_mode'] = self.theano_mode\n             model_params['loss'] = self.loss\n             model_params['optimizer'] = self.optimizer.get_config()\n         return yaml.dump(model_params)\n-\n-from .utils.generic_utils import get_from_module\n-def get(identifier):\n-    return get_from_module(identifier, globals(), 'model')\n\n@@ -9,24 +9,31 @@ if theano.config.floatX == 'float64':\n else:\n     epsilon = 1.0e-7\n \n+\n def mean_squared_error(y_true, y_pred):\n     return T.sqr(y_pred - y_true).mean(axis=-1)\n \n+\n def mean_absolute_error(y_true, y_pred):\n     return T.abs_(y_pred - y_true).mean(axis=-1)\n \n+\n def mean_absolute_percentage_error(y_true, y_pred):\n     return T.abs_((y_true - y_pred) / T.clip(T.abs_(y_true), epsilon, np.inf)).mean(axis=-1) * 100.\n \n+\n def mean_squared_logarithmic_error(y_true, y_pred):\n     return T.sqr(T.log(T.clip(y_pred, epsilon, np.inf) + 1.) - T.log(T.clip(y_true, epsilon, np.inf) + 1.)).mean(axis=-1)\n \n+\n def squared_hinge(y_true, y_pred):\n     return T.sqr(T.maximum(1. - y_true * y_pred, 0.)).mean(axis=-1)\n \n+\n def hinge(y_true, y_pred):\n     return T.maximum(1. - y_true * y_pred, 0.).mean(axis=-1)\n \n+\n def categorical_crossentropy(y_true, y_pred):\n     '''Expects a binary class matrix instead of a vector of scalar classes\n     '''\n@@ -36,6 +43,7 @@ def categorical_crossentropy(y_true, y_pred):\n     cce = T.nnet.categorical_crossentropy(y_pred, y_true)\n     return cce\n \n+\n def binary_crossentropy(y_true, y_pred):\n     y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n     bce = T.nnet.binary_crossentropy(y_pred, y_true).mean(axis=-1)\n@@ -50,4 +58,3 @@ msle = MSLE = mean_squared_logarithmic_error\n from .utils.generic_utils import get_from_module\n def get(identifier):\n     return get_from_module(identifier, globals(), 'objective')\n-\n\n@@ -6,16 +6,18 @@ import numpy as np\n from .utils.theano_utils import shared_zeros, shared_scalar\n from six.moves import zip\n \n+\n def clip_norm(g, c, n):\n     if c > 0:\n         g = T.switch(T.ge(n, c), g * c / n, g)\n     return g\n \n+\n def kl_divergence(p, p_hat):\n     return p_hat - p + p * T.log(p / p_hat)\n \n-class Optimizer(object):\n \n+class Optimizer(object):\n     def get_updates(self, params, constraints, loss):\n         raise NotImplementedError\n \n@@ -32,6 +34,7 @@ class Optimizer(object):\n     def get_config(self):\n         return {\"name\": self.__class__.__name__}\n \n+\n class SGD(Optimizer):\n \n     def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False, *args, **kwargs):\n@@ -66,7 +69,6 @@ class SGD(Optimizer):\n \n \n class RMSprop(Optimizer):\n-\n     def __init__(self, lr=0.001, rho=0.9, epsilon=1e-6, *args, **kwargs):\n         self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n@@ -82,7 +84,6 @@ class RMSprop(Optimizer):\n \n             new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n             updates.append((p, c(new_p))) # apply constraints\n-            \n         return updates\n \n     def get_config(self):\n@@ -91,8 +92,8 @@ class RMSprop(Optimizer):\n                 \"rho\": self.rho,\n                 \"epsilon\": self.epsilon}\n \n-class Adagrad(Optimizer):\n \n+class Adagrad(Optimizer):\n     def __init__(self, lr=0.01, epsilon=1e-6, *args, **kwargs):\n         self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n@@ -105,7 +106,6 @@ class Adagrad(Optimizer):\n         for p, g, a, c in zip(params, grads, accumulators, constraints):\n             new_a = a + g ** 2 # update accumulator\n             updates.append((a, new_a))\n-\n             new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n             updates.append((p, c(new_p))) # apply constraints\n         return updates\n@@ -115,6 +115,7 @@ class Adagrad(Optimizer):\n                 \"lr\": self.lr,\n                 \"epsilon\": self.epsilon}\n \n+\n class Adadelta(Optimizer):\n     '''\n         Reference: http://arxiv.org/abs/1212.5701\n@@ -150,6 +151,7 @@ class Adadelta(Optimizer):\n                 \"rho\": self.rho,\n                 \"epsilon\": self.epsilon}\n \n+\n class Adam(Optimizer):\n     '''\n         Reference: http://arxiv.org/abs/1412.6980\n\n@@ -1,6 +1,7 @@\n from __future__ import absolute_import\n import theano.tensor as T\n \n+\n class Regularizer(object):\n     def set_param(self, p):\n         self.p = p\n@@ -14,6 +15,7 @@ class Regularizer(object):\n     def get_config(self):\n         return {\"name\": self.__class__.__name__}\n \n+\n class WeightRegularizer(Regularizer):\n     def __init__(self, l1=0., l2=0.):\n         self.l1 = l1\n@@ -32,6 +34,7 @@ class WeightRegularizer(Regularizer):\n                 \"l1\": self.l1,\n                 \"l2\": self.l2}\n \n+\n class ActivityRegularizer(Regularizer):\n     def __init__(self, l1=0., l2=0.):\n         self.l1 = l1\n@@ -50,21 +53,27 @@ class ActivityRegularizer(Regularizer):\n                 \"l1\": self.l1,\n                 \"l2\": self.l2}\n \n+\n def l1(l=0.01):\n     return WeightRegularizer(l1=l)\n \n+\n def l2(l=0.01):\n     return WeightRegularizer(l2=l)\n \n+\n def l1l2(l1=0.01, l2=0.01):\n     return WeightRegularizer(l1=l1, l2=l2)\n \n+\n def activity_l1(l=0.01):\n     return ActivityRegularizer(l1=l)\n \n+\n def activity_l2(l=0.01):\n     return ActivityRegularizer(l2=l)\n \n+\n def activity_l1l2(l1=0.01, l2=0.01):\n     return ActivityRegularizer(l1=l1, l2=l2)\n \n\n@@ -4,6 +4,7 @@ import time\n import sys\n import six\n \n+\n def get_from_module(identifier, module_params, module_name, instantiate=False, kwargs=None):\n     if isinstance(identifier, six.string_types):\n         res = module_params.get(identifier)\n@@ -17,9 +18,11 @@ def get_from_module(identifier, module_params, module_name, instantiate=False, k\n             return res\n     return identifier\n \n+\n def make_tuple(*args):\n     return args\n \n+\n def printv(v, prefix=''):\n     if type(v) == dict:\n         if 'name' in v:\n@@ -41,6 +44,7 @@ def printv(v, prefix=''):\n         prefix += '...'\n         print(prefix + str(v))\n \n+\n class Progbar(object):\n     def __init__(self, target, width=30, verbose=1):\n         '''\n@@ -122,6 +126,5 @@ class Progbar(object):\n                     info += ' - %s: %.4f' % (k, self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n                 sys.stdout.write(info + \"\\n\")\n \n-\n     def add(self, n, values=[]):\n         self.update(self.seen_so_far+n, values)\n\n@@ -3,9 +3,10 @@ import h5py\n import numpy as np\n from collections import defaultdict\n \n-class HDF5Matrix:\n \n-    refs = defaultdict(int)\n+class HDF5Matrix():\n+    def __init__(self):\n+        self.refs = defaultdict(int)\n \n     def __init__(self, datapath, dataset, start, end, normalizer=None):\n         if datapath not in list(self.refs.keys()):\n@@ -60,6 +61,7 @@ def save_array(array, name):\n     ds[:] = array\n     f.close()\n \n+\n def load_array(name):\n     import tables\n     f = tables.open_file(name)\n\n@@ -9,7 +9,6 @@ from ..layers.noise import GaussianNoise, GaussianDropout\n from ..layers.normalization import BatchNormalization\n from ..layers.recurrent import SimpleRNN, SimpleDeepRNN, GRU, LSTM, JZS1, JZS2, JZS3\n from ..layers import containers\n-\n from .. import regularizers\n from .. import constraints\n \n@@ -55,7 +54,8 @@ def container_from_config(layer_dict):\n             graph_layer.add_output(**output)\n         return graph_layer\n \n-    else: # The case in which layer_dict represents an \"atomic\" layer\n+    else:\n+        # The case in which layer_dict represents an \"atomic\" layer\n         layer_dict.pop('name')\n         if 'parameters' in layer_dict:\n             params = layer_dict.get('parameters')\n\n@@ -4,6 +4,7 @@ import scipy as sp\n from six.moves import range\n from six.moves import zip\n \n+\n def to_categorical(y, nb_classes=None):\n     '''Convert class vector (integers from 0 to nb_classes)\n     to binary class matrix, for use with categorical_crossentropy\n@@ -16,11 +17,13 @@ def to_categorical(y, nb_classes=None):\n         Y[i, y[i]] = 1.\n     return Y\n \n+\n def normalize(a, axis=-1, order=2):\n     l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n     l2[l2 == 0] = 1\n     return a / np.expand_dims(l2, axis)\n \n+\n def binary_logloss(p, y):\n     epsilon = 1e-15\n     p = sp.maximum(epsilon, p)\n@@ -29,19 +32,23 @@ def binary_logloss(p, y):\n     res *= -1.0/len(y)\n     return res\n \n+\n def multiclass_logloss(P, Y):\n     score = 0.\n     npreds = [P[i][Y[i]-1] for i in range(len(Y))]\n     score = -(1. / len(Y)) * np.sum(np.log(npreds))\n     return score\n \n+\n def accuracy(p, y):\n     return np.mean([a == b for a, b in zip(p, y)])\n \n+\n def probas_to_classes(y_pred):\n     if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n         return categorical_probas_to_classes(y_pred)\n     return np.array([1 if p > 0.5 else 0 for p in y_pred])\n \n+\n def categorical_probas_to_classes(p):\n     return np.argmax(p, axis=1)\n\n@@ -1,5 +1,6 @@\n import numpy as np\n \n+\n def get_test_data(nb_train=1000, nb_test=500, input_shape=(10,), output_shape=(2,),\n                   classification=True, nb_class=2):\n     '''\n\n@@ -3,21 +3,26 @@ import numpy as np\n import theano\n import theano.tensor as T\n \n+\n def floatX(X):\n     return np.asarray(X, dtype=theano.config.floatX)\n \n+\n def sharedX(X, dtype=theano.config.floatX, name=None):\n     return theano.shared(np.asarray(X, dtype=dtype), name=name)\n \n+\n def shared_zeros(shape, dtype=theano.config.floatX, name=None):\n     return sharedX(np.zeros(shape), dtype=dtype, name=name)\n \n+\n def shared_scalar(val=0., dtype=theano.config.floatX, name=None):\n     return theano.shared(np.cast[dtype](val))\n \n+\n def shared_ones(shape, dtype=theano.config.floatX, name=None):\n     return sharedX(np.ones(shape), dtype=dtype, name=name)\n \n+\n def alloc_zeros_matrix(*dims):\n     return T.alloc(np.cast[theano.config.floatX](0.), *dims)\n-\n\n@@ -10,5 +10,4 @@ setup(name = 'Keras',\n       download_url='https://github.com/fchollet/keras/tarball/0.1.2',\n       license='MIT',\n       install_requires=['theano', 'pyyaml'],\n-      packages = find_packages(),\n-)\n\\ No newline at end of file\n+      packages=find_packages())\n\n@@ -6,8 +6,8 @@ from keras.layers.embeddings import Embedding\n from theano import function\n from keras.constraints import unitnorm\n \n-class TestConcatenation(unittest.TestCase):\n \n+class TestEmbedding(unittest.TestCase):\n     def setUp(self):\n         self.X1 = np.array([[1], [2]], dtype='int32')\n         self.W1 = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]], dtype='float32')\n\n@@ -2,6 +2,7 @@ from __future__ import print_function\n import unittest\n import numpy as np\n np.random.seed(1337)\n+\n from keras.models import Graph, Sequential\n from keras.layers import containers\n from keras.layers.core import Dense, Activation\n@@ -17,6 +18,7 @@ y2 = np.random.random((100,))\n (X2_train, y2_train), (X2_test, y2_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(32,),\n                                                          classification=False, output_shape=(1,))\n \n+\n class TestGraph(unittest.TestCase):\n     def test_1o_1i(self):\n         print('test a non-sequential graph with 1 input and 1 output')\n\n@@ -32,6 +32,7 @@ Y_train = np_utils.to_categorical(y_train, nb_classes)\n Y_test = np_utils.to_categorical(y_test, nb_classes)\n test_ids = np.where(y_test == np.array(weighted_class))[0]\n \n+\n def create_model():\n     model = Sequential()\n     model.add(Dense(784, 50))\n@@ -40,14 +41,15 @@ def create_model():\n     model.add(Activation('softmax'))\n     return model\n \n+\n def _test_weights(model, class_weight=None, sample_weight=None):\n-    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, \\\n+    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0,\n               class_weight=class_weight, sample_weight=sample_weight)\n     score = model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n     return score\n \n-class TestConcatenation(unittest.TestCase):\n \n+class TestConcatenation(unittest.TestCase):\n     def test_loss_weighting(self):\n         class_weight = dict([(i, standard_weight) for i in range(nb_classes)])\n         class_weight[weighted_class] = high_weight\n\n@@ -32,6 +32,7 @@ Y_train = np_utils.to_categorical(y_train, nb_classes)\n Y_test = np_utils.to_categorical(y_test, nb_classes)\n test_ids = np.where(y_test == np.array(weighted_class))[0]\n \n+\n def create_model(weight_reg=None, activity_reg=None):\n     model = Sequential()\n     model.add(Dense(784, 50))\n\n@@ -25,6 +25,7 @@ y_train = np_utils.to_categorical(y_train)\n print(X_train.shape)\n print(y_train.shape)\n \n+\n class TestSequential(unittest.TestCase):\n     def test_sequential(self):\n         print('Test sequential')\n@@ -67,7 +68,6 @@ class TestSequential(unittest.TestCase):\n         print(nloss)\n         assert(loss == nloss)\n \n-\n     def test_merge_sum(self):\n         print('Test merge: sum')\n         left = Sequential()\n@@ -121,7 +121,6 @@ class TestSequential(unittest.TestCase):\n         print(nloss)\n         assert(loss == nloss)\n \n-\n     def test_merge_concat(self):\n         print('Test merge: concat')\n         left = Sequential()\n@@ -179,7 +178,6 @@ class TestSequential(unittest.TestCase):\n         print(nloss)\n         assert(loss == nloss)\n \n-\n     def test_merge_recursivity(self):\n         print('Test merge recursivity')\n \n@@ -231,7 +229,6 @@ class TestSequential(unittest.TestCase):\n         print(nloss)\n         assert(loss == nloss)\n \n-\n     def test_merge_overlap(self):\n         print('Test merge overlap')\n         left = Sequential()\n@@ -272,7 +269,6 @@ class TestSequential(unittest.TestCase):\n         assert(loss == nloss)\n \n \n-\n if __name__ == '__main__':\n     print('Test Sequential model')\n     unittest.main()\n\n@@ -1,6 +1,7 @@\n from __future__ import print_function\n import numpy as np\n np.random.seed(1337)\n+\n from keras.utils.test_utils import get_test_data\n from keras.models import Sequential\n from keras.layers.core import Dense, Activation, TimeDistributedDense, Flatten\n@@ -9,6 +10,7 @@ from keras.layers.convolutional import Convolution2D\n from keras.utils.np_utils import to_categorical\n import unittest\n \n+\n class TestRegularizers(unittest.TestCase):\n     def test_vector_clf(self):\n         nb_hidden = 10\n@@ -86,7 +88,6 @@ class TestRegularizers(unittest.TestCase):\n         history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n         self.assertTrue(history.history['val_loss'][-1] < 0.75)\n \n-\n     def test_seq_to_seq(self):\n         print('sequence to sequence data:')\n         (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(5, 10), output_shape=(5, 10),\n@@ -102,7 +103,6 @@ class TestRegularizers(unittest.TestCase):\n         history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n         self.assertTrue(history.history['val_loss'][-1] < 0.75)\n \n-\n     def test_img_clf(self):\n         print('image classification data:')\n         (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(3, 32, 32),\n\n@@ -70,14 +70,13 @@ def build_lstm_autoencoder(autoencoder, X_train, X_test):\n                                 output_reconstruction=False))\n     return autoencoder, X_train, X_test\n \n+\n def build_deep_classical_autoencoder(autoencoder):\n     encoder = containers.Sequential([Dense(input_dim, hidden_dim, activation=activation), Dense(hidden_dim, hidden_dim/2, activation=activation)])\n     decoder = containers.Sequential([Dense(hidden_dim/2, hidden_dim, activation=activation), Dense(hidden_dim, input_dim, activation=activation)])\n     autoencoder.add(AutoEncoder(encoder=encoder, decoder=decoder, output_reconstruction=False))\n     return autoencoder\n \n-\n-\n # Try different things here: 'lstm' or 'classical' or 'denoising'\n # or 'deep_denoising'\n \n@@ -129,4 +128,3 @@ for autoencoder_type in ['classical', 'lstm']:\n \n     print('Loss change:', (score[0] - classical_score[0])/classical_score[0], '%')\n     print('Accuracy change:', (score[1] - classical_score[1])/classical_score[1], '%')\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
