{"custom_id": "scrapy#20719bac5cdc4898e9f01fcf4f92aa781a9d0ad1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 2 | Churn Cumulative: 957 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -224,7 +224,7 @@ class BaseSettings(MutableMapping):\n         value = self.get(name, default)\n         if value is None:\n             return {}\n-        if isinstance(value, six.string_types):\n+        if isinstance(value, str):\n             try:\n                 return json.loads(value, object_pairs_hook=OrderedDict)\n             except ValueError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2d92a39003a5a0b01b265b87cf722a61f121ca60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 1350 | Contributors (this commit): 20 | Commits (past 90d): 9 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,7 +33,7 @@ from scrapy.responsetypes import responsetypes\n from scrapy.settings import Settings\n from scrapy.utils.test import get_crawler, skip_if_no_boto\n from scrapy.utils.python import to_bytes\n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n \n from tests.mockserver import MockServer, ssl_context_factory, Echo\n from tests.spiders import SingleRequestSpider\n@@ -687,6 +687,18 @@ class HttpProxyTestCase(unittest.TestCase):\n         request = Request('http://example.com', meta={'proxy': http_proxy})\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n \n+    def test_download_with_proxy_https_noconnect(self):\n+        def _test(response):\n+            self.assertEqual(response.status, 200)\n+            self.assertEqual(response.url, request.url)\n+            self.assertEqual(response.body, b'https://example.com')\n+\n+        http_proxy = '%s?noconnect' % self.getURL('')\n+        request = Request('https://example.com', meta={'proxy': http_proxy})\n+        with self.assertWarnsRegex(ScrapyDeprecationWarning,\n+                                   r'Using HTTPS proxies in the noconnect mode is deprecated'):\n+            return self.download_request(request, Spider('foo')).addCallback(_test)\n+\n     def test_download_without_proxy(self):\n         def _test(response):\n             self.assertEqual(response.status, 200)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bb2ff13e4c7c80cfc7925f60eadc97dec0d69026", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 1352 | Contributors (this commit): 20 | Commits (past 90d): 10 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -712,6 +712,8 @@ class HttpProxyTestCase(unittest.TestCase):\n class Http10ProxyTestCase(HttpProxyTestCase):\n     download_handler_cls = HTTP10DownloadHandler\n \n+    def test_download_with_proxy_https_noconnect(self):\n+        raise unittest.SkipTest('noconnect is not supported in HTTP10DownloadHandler')\n \n class Http11ProxyTestCase(HttpProxyTestCase):\n     download_handler_cls = HTTP11DownloadHandler\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ac302c3f615d339ed36000231ca3e1e1c347478a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1353 | Contributors (this commit): 20 | Commits (past 90d): 11 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -715,6 +715,7 @@ class Http10ProxyTestCase(HttpProxyTestCase):\n     def test_download_with_proxy_https_noconnect(self):\n         raise unittest.SkipTest('noconnect is not supported in HTTP10DownloadHandler')\n \n+\n class Http11ProxyTestCase(HttpProxyTestCase):\n     download_handler_cls = HTTP11DownloadHandler\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ee9881d2704798c9cd61b6da503bb0694227c58c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 2 | Churn Cumulative: 107 | Contributors (this commit): 13 | Commits (past 90d): 11 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -61,7 +61,7 @@ class FilteringLinkExtractor(object):\n             warn('scrapy.linkextractors.FilteringLinkExtractor is deprecated, '\n                  'please use scrapy.linkextractors.LinkExtractor instead',\n                  ScrapyDeprecationWarning, stacklevel=2)\n-        return super(FilteringLinkExtractor, cls).__new__(cls)\n+        return super().__new__(cls, *args, **kwargs)\n \n     def __init__(self, link_extractor, allow, deny, allow_domains, deny_domains,\n                  restrict_xpaths, canonicalize, deny_extensions, restrict_css, restrict_text):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#174769a3f08fcd84eaec8a88217a05f8ebc3f2cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 525 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -517,10 +517,10 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n             LxmlLinkExtractor()\n             self.assertEqual(len(warnings), 0)\n \n-            class SubclassedItem(LxmlLinkExtractor):\n+            class SubclassedLxmlLinkExtractor(LxmlLinkExtractor):\n                 pass\n \n-            SubclassedItem()\n+            SubclassedLxmlLinkExtractor()\n             self.assertEqual(len(warnings), 0)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7ccb169a27ccbedf3145b824e1bd655cb6902f32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 5 | Churn Cumulative: 188 | Contributors (this commit): 12 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -179,7 +179,6 @@ class EngineTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_crawler(self):\n-\n         for spider in TestSpider, DictItemsSpider:\n             self.run = CrawlerRun(spider)\n             yield self.run.run()\n@@ -189,11 +188,15 @@ class EngineTest(unittest.TestCase):\n             self._assert_scraped_items()\n             self._assert_signals_catched()\n \n+    @defer.inlineCallbacks\n+    def test_crawler_dupefilter(self):\n         self.run = CrawlerRun(TestDupeFilterSpider)\n         yield self.run.run()\n         self._assert_scheduled_requests(urls_to_visit=7)\n         self._assert_dropped_requests()\n \n+    @defer.inlineCallbacks\n+    def test_crawler_itemerror(self):\n         self.run = CrawlerRun(ItemZeroDivisionErrorSpider)\n         yield self.run.run()\n         self._assert_items_error()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#916382e109dc06a57f4631448555953d1fa540b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 95 | Lines Deleted: 26 | Files Changed: 4 | Hunks: 26 | Methods Changed: 22 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 121 | Churn Cumulative: 1240 | Contributors (this commit): 16 | Commits (past 90d): 14 | Contributors (cumulative): 38 | DMM Complexity: 0.8181818181818182\n\nDIFF:\n@@ -16,7 +16,11 @@ from scrapy.utils.python import get_func_args\n from scrapy.utils.spider import iterate_spider_output\n \n \n-def _identity(request, response):\n+def _identity(x):\n+    return x\n+\n+\n+def _identity_process_request(request, response):\n     return request\n \n \n@@ -32,17 +36,20 @@ _default_link_extractor = LinkExtractor()\n \n class Rule(object):\n \n-    def __init__(self, link_extractor=None, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None):\n+    def __init__(self, link_extractor=None, callback=None, cb_kwargs=None, follow=None,\n+                 process_links=None, process_request=None, errback=None):\n         self.link_extractor = link_extractor or _default_link_extractor\n         self.callback = callback\n+        self.errback = errback\n         self.cb_kwargs = cb_kwargs or {}\n-        self.process_links = process_links\n-        self.process_request = process_request or _identity\n+        self.process_links = process_links or _identity\n+        self.process_request = process_request or _identity_process_request\n         self.process_request_argcount = None\n         self.follow = follow if follow is not None else not callback\n \n     def _compile(self, spider):\n         self.callback = _get_method(self.callback, spider)\n+        self.errback = _get_method(self.errback, spider)\n         self.process_links = _get_method(self.process_links, spider)\n         self.process_request = _get_method(self.process_request, spider)\n         self.process_request_argcount = len(get_func_args(self.process_request))\n@@ -76,48 +83,59 @@ class CrawlSpider(Spider):\n     def process_results(self, response, results):\n         return results\n \n-    def _build_request(self, rule, link):\n-        r = Request(url=link.url, callback=self._response_downloaded)\n-        r.meta.update(rule=rule, link_text=link.text)\n-        return r\n+    def _build_request(self, rule_index, link):\n+        return Request(\n+            url=link.url,\n+            callback=self._callback,\n+            errback=self._errback,\n+            meta=dict(rule=rule_index, link_text=link.text),\n+        )\n \n     def _requests_to_follow(self, response):\n         if not isinstance(response, HtmlResponse):\n             return\n         seen = set()\n-        for n, rule in enumerate(self._rules):\n+        for rule_index, rule in enumerate(self._rules):\n             links = [lnk for lnk in rule.link_extractor.extract_links(response)\n                      if lnk not in seen]\n-            if links and rule.process_links:\n-                links = rule.process_links(links)\n-            for link in links:\n+            for link in rule.process_links(links):\n                 seen.add(link)\n-                request = self._build_request(n, link)\n+                request = self._build_request(rule_index, link)\n                 yield rule._process_request(request, response)\n \n-    def _response_downloaded(self, response):\n+    def _callback(self, response):\n         rule = self._rules[response.meta['rule']]\n         return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)\n \n+    def _errback(self, failure):\n+        rule = self._rules[failure.request.meta['rule']]\n+        return self._handle_failure(failure, rule.errback)\n+\n     def _parse_response(self, response, callback, cb_kwargs, follow=True):\n         if callback:\n             cb_res = callback(response, **cb_kwargs) or ()\n             cb_res = self.process_results(response, cb_res)\n-            for requests_or_item in iterate_spider_output(cb_res):\n-                yield requests_or_item\n+            for request_or_item in iterate_spider_output(cb_res):\n+                yield request_or_item\n \n         if follow and self._follow_links:\n             for request_or_item in self._requests_to_follow(response):\n                 yield request_or_item\n \n+    def _handle_failure(self, failure, errback):\n+        if errback:\n+            results = errback(failure) or ()\n+            for request_or_item in iterate_spider_output(results):\n+                yield request_or_item\n+\n     def _compile_rules(self):\n-        self._rules = [copy.copy(r) for r in self.rules]\n-        for rule in self._rules:\n-            rule._compile(self)\n+        self._rules = []\n+        for rule in self.rules:\n+            self._rules.append(copy.copy(rule))\n+            self._rules[-1]._compile(self)\n \n     @classmethod\n     def from_crawler(cls, crawler, *args, **kwargs):\n         spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)\n-        spider._follow_links = crawler.settings.getbool(\n-            'CRAWLSPIDER_FOLLOW_LINKS', True)\n+        spider._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True)\n         return spider\n\n@@ -164,6 +164,12 @@ class Drop(Partial):\n             request.finish()\n \n \n+class ArbitraryLengthPayloadResource(LeafResource):\n+\n+    def render(self, request):\n+        return request.content.read()\n+\n+\n class Root(Resource):\n \n     def __init__(self):\n@@ -177,6 +183,7 @@ class Root(Resource):\n         self.putChild(b\"echo\", Echo())\n         self.putChild(b\"payload\", PayloadResource())\n         self.putChild(b\"xpayload\", EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]))\n+        self.putChild(b\"alpayload\", ArbitraryLengthPayloadResource())\n         try:\n             from tests import tests_datadir\n             self.putChild(b\"files\", File(os.path.join(tests_datadir, 'test_site/files/')))\n\n@@ -1,14 +1,14 @@\n \"\"\"\n Some spiders used for testing and benchmarking\n \"\"\"\n-\n import time\n from urllib.parse import urlencode\n \n-from scrapy.spiders import Spider\n from scrapy.http import Request\n from scrapy.item import Item\n from scrapy.linkextractors import LinkExtractor\n+from scrapy.spiders import Spider\n+from scrapy.spiders.crawl import CrawlSpider, Rule\n \n \n class MockServerSpider(Spider):\n@@ -184,3 +184,35 @@ class DuplicateStartRequestsSpider(MockServerSpider):\n \n     def parse(self, response):\n         self.visited += 1\n+\n+\n+class CrawlSpiderWithErrback(MockServerSpider, CrawlSpider):\n+    name = 'crawl_spider_with_errback'\n+    custom_settings = {\n+        'RETRY_HTTP_CODES': [],  # no need to retry\n+    }\n+    rules = (\n+        Rule(LinkExtractor(), callback='callback', errback='errback', follow=True),\n+    )\n+\n+    def start_requests(self):\n+        test_body = b\"\"\"\n+        <html>\n+            <head><title>Page title<title></head>\n+            <body>\n+                <p><a href=\"/status?n=200\">Item 200</a></p>  <!-- callback -->\n+                <p><a href=\"/status?n=201\">Item 201</a></p>  <!-- callback -->\n+                <p><a href=\"/status?n=404\">Item 404</a></p>  <!-- errback -->\n+                <p><a href=\"/status?n=500\">Item 500</a></p>  <!-- errback -->\n+                <p><a href=\"/status?n=501\">Item 501</a></p>  <!-- errback -->\n+            </body>\n+        </html>\n+        \"\"\"\n+        url = self.mockserver.url(\"/alpayload\")\n+        yield Request(url, method=\"POST\", body=test_body)\n+\n+    def callback(self, response):\n+        self.logger.info('[callback] status %i', response.status)\n+\n+    def errback(self, failure):\n+        self.logger.info('[errback] status %i', failure.value.response.status)\n\n@@ -5,12 +5,12 @@ from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n \n-from scrapy.http import Request\n from scrapy.crawler import CrawlerRunner\n+from scrapy.http import Request\n from scrapy.utils.python import to_unicode\n-from tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \\\n-    BrokenStartRequestsSpider, SingleRequestSpider, DuplicateStartRequestsSpider\n from tests.mockserver import MockServer\n+from tests.spiders import (FollowAllSpider, DelaySpider, SimpleSpider, BrokenStartRequestsSpider,\n+                           SingleRequestSpider, DuplicateStartRequestsSpider, CrawlSpiderWithErrback)\n \n \n class CrawlTestCase(TestCase):\n@@ -277,3 +277,15 @@ with multiples lines\n \n         self._assert_retried(log)\n         self.assertIn(\"Got response 200\", str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_crawlspider_with_errback(self):\n+        self.runner.crawl(CrawlSpiderWithErrback, mockserver=self.mockserver)\n+\n+        with LogCapture() as log:\n+            yield self.runner.join()\n+\n+        self.assertIn(\"[callback] status 200\", str(log))\n+        self.assertIn(\"[callback] status 201\", str(log))\n+        self.assertIn(\"[errback] status 404\", str(log))\n+        self.assertIn(\"[errback] status 500\", str(log))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a5de2c64e6f1233a98b1b180606fca0ae4dd0871", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 430 | Contributors (this commit): 14 | Commits (past 90d): 3 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -59,7 +59,6 @@ class FilesPipelineTestCase(unittest.TestCase):\n                                     //+F0tzCwMK76ZKQ21AMqr7oAAC96JvD5aWM2kvZ78J0N7fmAAC46Y4Ap7y\")),\n                          'full/178059cbeba2e34120a67f2dc1afc3ecc09b61cb.png')\n \n-\n     def test_fs_store(self):\n         assert isinstance(self.pipeline.store, FSFilesStore)\n         self.assertEqual(self.pipeline.store.basedir, self.tempdir)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e22c0c27d9d33383f4ac18a4142ccffe1d9a0d3b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 109 | Contributors (this commit): 13 | Commits (past 90d): 12 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -60,7 +60,7 @@ class FilteringLinkExtractor(object):\n             warn('scrapy.linkextractors.FilteringLinkExtractor is deprecated, '\n                  'please use scrapy.linkextractors.LinkExtractor instead',\n                  ScrapyDeprecationWarning, stacklevel=2)\n-        return super().__new__(cls, *args, **kwargs)\n+        return super(FilteringLinkExtractor, cls).__new__(cls)\n \n     def __init__(self, link_extractor, allow, deny, allow_domains, deny_domains,\n                  restrict_xpaths, canonicalize, deny_extensions, restrict_css, restrict_text):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#40697dcbfa17dccf81adce7d033bc466ba6e98a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 28 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 28 | Churn Cumulative: 281 | Contributors (this commit): 14 | Commits (past 90d): 6 | Contributors (cumulative): 14 | DMM Complexity: 0.7333333333333333\n\nDIFF:\n@@ -1,14 +1,10 @@\n \"\"\"\n Helper functions for dealing with Twisted deferreds\n \"\"\"\n-import asyncio\n-import inspect\n-\n from twisted.internet import defer, task\n from twisted.python import failure\n \n from scrapy.exceptions import IgnoreRequest\n-from scrapy.utils.asyncio import is_asyncio_reactor_installed\n \n \n def defer_fail(_failure):\n@@ -118,27 +114,3 @@ def iter_errback(iterable, errback, *a, **kw):\n             break\n         except Exception:\n             errback(failure.Failure(), *a, **kw)\n-\n-\n-def _isfuture(o):\n-    # workaround for Python before 3.5.3 not having asyncio.isfuture\n-    if hasattr(asyncio, 'isfuture'):\n-        return asyncio.isfuture(o)\n-    return isinstance(o, asyncio.Future)\n-\n-\n-def deferred_from_coro(o, asyncio_enabled=False):\n-    \"\"\"Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine\"\"\"\n-    if isinstance(o, defer.Deferred):\n-        return o\n-    if _isfuture(o) or inspect.isawaitable(o):\n-        if not asyncio_enabled:\n-            # wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines\n-            # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n-            return defer.ensureDeferred(o)\n-        else:\n-            # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n-            if not is_asyncio_reactor_installed():\n-                raise TypeError('Using coroutines requires installing AsyncioSelectorReactor')\n-            return defer.Deferred.fromFuture(asyncio.ensure_future(o))\n-    return o\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e342de5038e3757660f947fe1fadf35b54cd7113", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 277 | Contributors (this commit): 12 | Commits (past 90d): 6 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,6 @@ from subprocess import Popen, PIPE\n from urllib.parse import urlencode\n \n from OpenSSL import SSL\n-\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n from twisted.web.static import File\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8de80f59db19d739a056a7e58662f90544fece16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 19 | Churn Cumulative: 2599 | Contributors (this commit): 23 | Commits (past 90d): 18 | Contributors (cumulative): 40 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,7 +14,7 @@ from scrapy.extension import ExtensionManager\n from scrapy.settings import overridden_settings, Settings\n from scrapy.signalmanager import SignalManager\n from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.utils.asyncio import install_asyncio_reactor\n+from scrapy.utils.asyncio import install_asyncio_reactor, is_asyncio_reactor_installed\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n from scrapy.utils.misc import load_object\n from scrapy.utils.log import (\n@@ -259,6 +259,10 @@ class CrawlerProcess(CrawlerRunner):\n         super(CrawlerProcess, self).__init__(settings)\n         if self.settings.getbool('ASYNCIO_ENABLED'):\n             install_asyncio_reactor()\n+            if not is_asyncio_reactor_installed():\n+                raise Exception(\"ASYNCIO_ENABLED is on but the Twisted asyncio \"\n+                                \"reactor is not installed, this is not supported.\")\n+\n         install_shutdown_handlers(self._signal_shutdown)\n         configure_logging(self.settings, install_root_handler)\n         log_scrapy_info(self.settings)\n\n@@ -11,7 +11,6 @@ from twisted.python import log as twisted_log\n import scrapy\n from scrapy.settings import Settings\n from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.utils.asyncio import is_asyncio_reactor_installed\n from scrapy.utils.versions import scrapy_components_versions\n \n \n@@ -150,12 +149,7 @@ def log_scrapy_info(settings):\n                     for name, version in scrapy_components_versions()\n                     if name != \"Scrapy\")})\n     if settings.getbool('ASYNCIO_ENABLED'):\n-        if is_asyncio_reactor_installed():\n         logger.debug(\"Asyncio support enabled\")\n-        else:\n-            logger.error(\"ASYNCIO_ENABLED is on but the Twisted asyncio \"\n-                         \"reactor is not installed, this is not supported \"\n-                         \"and asyncio coroutines will not work.\")\n \n \n class StreamLogger(object):\n\n@@ -264,13 +264,14 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_crawler_process_asyncio_enabled_true(self):\n         with LogCapture(level=logging.DEBUG) as log:\n+            if self.reactor_pytest == 'asyncio':\n                 runner = CrawlerProcess(settings={'ASYNCIO_ENABLED': True})\n                 yield runner.crawl(NoRequestsSpider)\n-            if self.reactor_pytest == 'asyncio':\n                 self.assertIn(\"Asyncio support enabled\", str(log))\n             else:\n-                self.assertNotIn(\"Asyncio support enabled\", str(log))\n-                self.assertIn(\"ASYNCIO_ENABLED is on but the Twisted asyncio reactor is not installed\", str(log))\n+                msg = \"ASYNCIO_ENABLED is on but the Twisted asyncio reactor is not installed\"\n+                with self.assertRaisesRegex(Exception, msg):\n+                    runner = CrawlerProcess(settings={'ASYNCIO_ENABLED': True})\n \n     @defer.inlineCallbacks\n     def test_crawler_process_asyncio_enabled_false(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#931b7e68d33e06b624b49a7abeababb4e3540474", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 10 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 28 | Churn Cumulative: 1453 | Contributors (this commit): 21 | Commits (past 90d): 14 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ from scrapy.responsetypes import responsetypes\n from scrapy.utils.decorators import defers\n \n \n-class FileDownloadHandler(object):\n+class FileDownloadHandler:\n     lazy = False\n \n     @defers\n\n@@ -1,21 +1,20 @@\n+import contextlib\n import os\n import shutil\n import tempfile\n from unittest import mock\n-import contextlib\n \n from testfixtures import LogCapture\n-from twisted.trial import unittest\n+from twisted.cred import checkers, credentials, portal\n+from twisted.internet import defer, error, reactor\n from twisted.protocols.policies import WrappingFactory\n from twisted.python.filepath import FilePath\n-from twisted.internet import reactor, defer, error\n-from twisted.web import server, static, util, resource\n+from twisted.trial import unittest\n+from twisted.web import resource, server, static, util\n from twisted.web._newclient import ResponseFailed\n from twisted.web.http import _DataLoss\n-from twisted.web.test.test_webclient import ForeverTakingResource, \\\n-        NoLengthResource, HostHeaderResource, \\\n-        PayloadResource\n-from twisted.cred import portal, checkers, credentials\n+from twisted.web.test.test_webclient import (ForeverTakingResource, HostHeaderResource,\n+                                             NoLengthResource, PayloadResource)\n from w3lib.url import path_to_file_uri\n \n from scrapy.core.downloader.handlers import DownloadHandlers\n@@ -26,13 +25,14 @@ from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n from scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n \n-from scrapy.spiders import Spider\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Headers, Request\n from scrapy.http.response.text import TextResponse\n from scrapy.responsetypes import responsetypes\n-from scrapy.utils.test import get_crawler, skip_if_no_boto\n+from scrapy.spiders import Spider\n+from scrapy.utils.misc import create_instance\n from scrapy.utils.python import to_bytes\n-from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n+from scrapy.utils.test import get_crawler, skip_if_no_boto\n \n from tests.mockserver import MockServer, ssl_context_factory, Echo\n from tests.spiders import SingleRequestSpider\n@@ -117,7 +117,9 @@ class FileTestCase(unittest.TestCase):\n         self.tmpname = self.mktemp()\n         with open(self.tmpname + '^', 'w') as f:\n             f.write('0123456789')\n-        self.download_request = FileDownloadHandler().download_request\n+        crawler = get_crawler()\n+        handler = create_instance(FileDownloadHandler, crawler.settings, crawler)\n+        self.download_request = handler.download_request\n \n     def tearDown(self):\n         os.unlink(self.tmpname + '^')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#342bf3cd35df856f4f2eafbf2717e9244c9deaf8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 240 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -50,9 +50,9 @@ class DownloadHandlers(object):\n             if skip_lazy and getattr(dhcls, 'lazy', True):\n                 return None\n             dh = create_instance(\n-                dhcls,\n-                self._crawler.settings,\n-                self._crawler,\n+                objcls=dhcls,\n+                settings=self._crawler.settings,\n+                crawler=self._crawler,\n             )\n         except NotConfigured as ex:\n             self._notconfigured[scheme] = str(ex)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9e5d945ef27ff9efee54b2245e833fefc3df72d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 35 | Churn Cumulative: 1414 | Contributors (this commit): 20 | Commits (past 90d): 13 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -252,7 +252,12 @@ class HttpTestCase(unittest.TestCase):\n         else:\n             self.port = reactor.listenTCP(0, self.wrapper, interface=self.host)\n         self.portno = self.port.getHost().port\n-        self.download_handler = self.download_handler_cls.from_crawler(get_crawler())\n+        crawler = get_crawler()\n+        self.download_handler = create_instance(\n+            objcls=self.download_handler_cls,\n+            settings=crawler.settings,\n+            crawler=crawler\n+        )\n         self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n@@ -492,8 +497,11 @@ class Http11TestCase(HttpTestCase):\n         return self.test_download_broken_content_allow_data_loss('broken-chunked')\n \n     def test_download_broken_content_allow_data_loss_via_setting(self, url='broken'):\n-        download_handler = self.download_handler_cls.from_crawler(\n-            get_crawler(settings_dict={'DOWNLOAD_FAIL_ON_DATALOSS': False})\n+        crawler = get_crawler(settings_dict={'DOWNLOAD_FAIL_ON_DATALOSS': False})\n+        download_handler = create_instance(\n+            objcls=self.download_handler_cls,\n+            settings=crawler.settings,\n+            crawler=crawler\n         )\n         request = Request(self.getURL(url))\n         d = download_handler.download_request(request, Spider('foo'))\n@@ -512,8 +520,11 @@ class Https11TestCase(Http11TestCase):\n \n     @defer.inlineCallbacks\n     def test_tls_logging(self):\n-        download_handler = self.download_handler_cls.from_crawler(\n-            get_crawler(settings_dict={'DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING': True})\n+        crawler = get_crawler(settings_dict={'DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING': True})\n+        download_handler = create_instance(\n+            objcls=self.download_handler_cls,\n+            settings=crawler.settings,\n+            crawler=crawler\n         )\n         try:\n             with LogCapture() as log_capture:\n@@ -581,8 +592,11 @@ class Https11CustomCiphers(unittest.TestCase):\n             0, self.wrapper, ssl_context_factory(self.keyfile, self.certfile, cipher_string='CAMELLIA256-SHA'),\n             interface=self.host)\n         self.portno = self.port.getHost().port\n-        self.download_handler = self.download_handler_cls.from_crawler(\n-            get_crawler(settings_dict={'DOWNLOADER_CLIENT_TLS_CIPHERS': 'CAMELLIA256-SHA'})\n+        crawler = get_crawler(settings_dict={'DOWNLOADER_CLIENT_TLS_CIPHERS': 'CAMELLIA256-SHA'})\n+        self.download_handler = create_instance(\n+            objcls=self.download_handler_cls,\n+            settings=crawler.settings,\n+            crawler=crawler\n         )\n         self.download_request = self.download_handler.download_request\n \n@@ -679,7 +693,12 @@ class HttpProxyTestCase(unittest.TestCase):\n         wrapper = WrappingFactory(site)\n         self.port = reactor.listenTCP(0, wrapper, interface='127.0.0.1')\n         self.portno = self.port.getHost().port\n-        self.download_handler = self.download_handler_cls.from_crawler(get_crawler())\n+        crawler = get_crawler()\n+        self.download_handler = create_instance(\n+            objcls=self.download_handler_cls,\n+            settings=crawler.settings,\n+            crawler=crawler\n+        )\n         self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fa21d8687a0f94e5d79cb28af11312ad58629954", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 1438 | Contributors (this commit): 20 | Commits (past 90d): 14 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -776,10 +776,13 @@ class S3AnonTestCase(unittest.TestCase):\n \n     def setUp(self):\n         skip_if_no_boto()\n-        self.s3reqh = S3DownloadHandler.from_crawler(\n-            crawler=get_crawler(),\n+        crawler = get_crawler()\n+        self.s3reqh = create_instance(\n+            objcls=S3DownloadHandler,\n+            settings=crawler.settings,\n+            crawler=crawler,\n             httpdownloadhandler=HttpDownloadHandlerMock,\n-            #anon=True, # is implicit\n+            # anon=True, # implicit\n         )\n         self.download_request = self.s3reqh.download_request\n         self.spider = Spider('foo')\n@@ -805,8 +808,11 @@ class S3TestCase(unittest.TestCase):\n \n     def setUp(self):\n         skip_if_no_boto()\n-        s3reqh = S3DownloadHandler.from_crawler(\n-            crawler=get_crawler(),\n+        crawler = get_crawler()\n+        s3reqh = create_instance(\n+            objcls=S3DownloadHandler,\n+            settings=crawler.settings,\n+            crawler=crawler,\n             aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n             aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n             httpdownloadhandler=HttpDownloadHandlerMock,\n@@ -830,7 +836,13 @@ class S3TestCase(unittest.TestCase):\n \n     def test_extra_kw(self):\n         try:\n-            S3DownloadHandler.from_crawler(get_crawler(), extra_kw=True)\n+            crawler = get_crawler()\n+            create_instance(\n+                objcls=S3DownloadHandler,\n+                settings=crawler.settings,\n+                crawler=crawler,\n+                extra_kw=True,\n+            )\n         except Exception as e:\n             self.assertIsInstance(e, (TypeError, NotConfigured))\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e6387de407297a36dd0e9b8e2058ae809cf3123", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 1447 | Contributors (this commit): 20 | Commits (past 90d): 15 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -984,7 +984,8 @@ class BaseFTPTestCase(unittest.TestCase):\n         self.factory = FTPFactory(portal=p)\n         self.port = reactor.listenTCP(0, self.factory, interface=\"127.0.0.1\")\n         self.portNum = self.port.getHost().port\n-        self.download_handler = FTPDownloadHandler(get_crawler())\n+        crawler = get_crawler()\n+        self.download_handler = create_instance(FTPDownloadHandler, crawler.settings, crawler)\n         self.addCleanup(self.port.stopListening)\n \n     def tearDown(self):\n@@ -1098,7 +1099,8 @@ class AnonymousFTPTestCase(BaseFTPTestCase):\n                                   userAnonymous=self.username)\n         self.port = reactor.listenTCP(0, self.factory, interface=\"127.0.0.1\")\n         self.portNum = self.port.getHost().port\n-        self.download_handler = FTPDownloadHandler(get_crawler())\n+        crawler = get_crawler()\n+        self.download_handler = create_instance(FTPDownloadHandler, crawler.settings, crawler)\n         self.addCleanup(self.port.stopListening)\n \n     def tearDown(self):\n@@ -1108,7 +1110,8 @@ class AnonymousFTPTestCase(BaseFTPTestCase):\n class DataURITestCase(unittest.TestCase):\n \n     def setUp(self):\n-        self.download_handler = DataURIDownloadHandler()\n+        crawler = get_crawler()\n+        self.download_handler = create_instance(DataURIDownloadHandler, crawler.settings, crawler)\n         self.download_request = self.download_handler.download_request\n         self.spider = Spider('foo')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8a567e98bbb1c7f917462d0376061303baef5883", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 21 | Churn Cumulative: 1468 | Contributors (this commit): 20 | Commits (past 90d): 16 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -38,29 +38,16 @@ from tests.mockserver import MockServer, ssl_context_factory, Echo\n from tests.spiders import SingleRequestSpider\n \n \n-class DummyDH(object):\n+class DummyDH:\n     lazy = False\n \n-    def __init__(self, crawler):\n-        pass\n \n-    @classmethod\n-    def from_crawler(cls, crawler):\n-        return cls(crawler)\n-\n-\n-class DummyLazyDH(object):\n+class DummyLazyDH:\n     # Default is lazy for backward compatibility\n-\n-    def __init__(self, crawler):\n     pass\n \n-    @classmethod\n-    def from_crawler(cls, crawler):\n-        return cls(crawler)\n \n-\n-class OffDH(object):\n+class OffDH:\n     lazy = False\n \n     def __init__(self, crawler):\n@@ -765,7 +752,7 @@ class Http11ProxyTestCase(HttpProxyTestCase):\n \n \n class HttpDownloadHandlerMock(object):\n-    def __init__(self, settings):\n+    def __init__(self, settings, crawler):\n         pass\n \n     def download_request(self, request, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a6ec89251eca6977f898e8341634bbe7288fd966", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 24 | Files Changed: 5 | Hunks: 19 | Methods Changed: 13 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 47 | Churn Cumulative: 3820 | Contributors (this commit): 32 | Commits (past 90d): 34 | Contributors (cumulative): 64 | DMM Complexity: 0.0\n\nDIFF:\n@@ -63,7 +63,7 @@ class ReceivedDataProtocol(Protocol):\n _CODE_RE = re.compile(r\"\\d+\")\n \n \n-class FTPDownloadHandler(object):\n+class FTPDownloadHandler:\n     lazy = False\n \n     CODE_MAPPING = {\n@@ -71,14 +71,14 @@ class FTPDownloadHandler(object):\n         \"default\": 503,\n     }\n \n-    def __init__(self, crawler):\n-        self.default_user = crawler.settings['FTP_USER']\n-        self.default_password = crawler.settings['FTP_PASSWORD']\n-        self.passive_mode = crawler.settings['FTP_PASSIVE_MODE']\n+    def __init__(self, settings, crawler=None):\n+        self.default_user = settings['FTP_USER']\n+        self.default_password = settings['FTP_PASSWORD']\n+        self.passive_mode = settings['FTP_PASSIVE_MODE']\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        return cls(crawler)\n+        return cls(crawler.settings, crawler)\n \n     def download_request(self, request, spider):\n         parsed_url = urlparse_cached(request)\n\n@@ -6,18 +6,18 @@ from scrapy.utils.misc import load_object, create_instance\n from scrapy.utils.python import to_unicode\n \n \n-class HTTP10DownloadHandler(object):\n+class HTTP10DownloadHandler:\n     lazy = False\n \n-    def __init__(self, crawler):\n+    def __init__(self, settings, crawler=None):\n         self.HTTPClientFactory = load_object(crawler.settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n         self.ClientContextFactory = load_object(crawler.settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n+        self._settings = settings\n         self._crawler = crawler\n-        self._settings = crawler.settings\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        return cls(crawler)\n+        return cls(crawler.settings, crawler)\n \n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n\n@@ -28,12 +28,10 @@ from scrapy.utils.python import to_bytes, to_unicode\n logger = logging.getLogger(__name__)\n \n \n-class HTTP11DownloadHandler(object):\n+class HTTP11DownloadHandler:\n     lazy = False\n \n-    def __init__(self, crawler):\n-        settings = crawler.settings\n-\n+    def __init__(self, settings, crawler=None):\n         self._pool = HTTPConnectionPool(reactor, persistent=True)\n         self._pool.maxPersistentPerHost = settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n         self._pool._factory.noisy = False\n@@ -68,7 +66,7 @@ class HTTP11DownloadHandler(object):\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        return cls(crawler)\n+        return cls(crawler.settings, crawler)\n \n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n\n@@ -4,6 +4,7 @@ from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.boto import is_botocore\n from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.utils.misc import create_instance\n \n \n def _get_boto_connection():\n@@ -30,14 +31,15 @@ def _get_boto_connection():\n     return _S3Connection\n \n \n-class S3DownloadHandler(object):\n+class S3DownloadHandler:\n \n-    def __init__(self, crawler, aws_access_key_id=None, aws_secret_access_key=None,\n+    def __init__(self, settings, crawler=None,\n+                 aws_access_key_id=None, aws_secret_access_key=None,\n                  httpdownloadhandler=HTTPDownloadHandler, **kw):\n         if not aws_access_key_id:\n-            aws_access_key_id = crawler.settings['AWS_ACCESS_KEY_ID']\n+            aws_access_key_id = settings['AWS_ACCESS_KEY_ID']\n         if not aws_secret_access_key:\n-            aws_secret_access_key = crawler.settings['AWS_SECRET_ACCESS_KEY']\n+            aws_secret_access_key = settings['AWS_SECRET_ACCESS_KEY']\n \n         # If no credentials could be found anywhere,\n         # consider this an anonymous connection request by default;\n@@ -66,11 +68,12 @@ class S3DownloadHandler(object):\n             except Exception as ex:\n                 raise NotConfigured(str(ex))\n \n-        self._download_http = httpdownloadhandler(crawler).download_request\n+        _http_handler = create_instance(httpdownloadhandler, settings, crawler)\n+        self._download_http = _http_handler.download_request\n \n     @classmethod\n     def from_crawler(cls, crawler, *args, **kwargs):\n-        return cls(crawler, *args, **kwargs)\n+        return cls(crawler.settings, crawler, *args, **kwargs)\n \n     def download_request(self, request, spider):\n         p = urlparse_cached(request)\n\n@@ -751,9 +751,7 @@ class Http11ProxyTestCase(HttpProxyTestCase):\n         self.assertIn(domain, timeout.osError)\n \n \n-class HttpDownloadHandlerMock(object):\n-    def __init__(self, settings, crawler):\n-        pass\n+class HttpDownloadHandlerMock:\n \n     def download_request(self, request, spider):\n         return request\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e2e15d66510c3040234ef9222ff2a17961c50eff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 17 | Files Changed: 5 | Hunks: 12 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 2362 | Contributors (this commit): 28 | Commits (past 90d): 20 | Contributors (cumulative): 47 | DMM Complexity: None\n\nDIFF:\n@@ -4,17 +4,17 @@ import logging\n \n from twisted.internet import defer\n \n-from scrapy.exceptions import NotSupported, NotConfigured\n+from scrapy import signals\n+from scrapy.exceptions import NotConfigured, NotSupported\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.python import without_none_values\n-from scrapy import signals\n \n \n logger = logging.getLogger(__name__)\n \n \n-class DownloadHandlers(object):\n+class DownloadHandlers:\n \n     def __init__(self, crawler):\n         self._crawler = crawler\n\n@@ -5,7 +5,7 @@ from scrapy.responsetypes import responsetypes\n from scrapy.utils.decorators import defers\n \n \n-class DataURIDownloadHandler(object):\n+class DataURIDownloadHandler:\n     lazy = False\n \n     @defers\n\n@@ -33,8 +33,8 @@ from io import BytesIO\n from urllib.parse import unquote\n \n from twisted.internet import reactor\n-from twisted.protocols.ftp import FTPClient, CommandFailed\n-from twisted.internet.protocol import Protocol, ClientCreator\n+from twisted.internet.protocol import ClientCreator, Protocol\n+from twisted.protocols.ftp import CommandFailed, FTPClient\n \n from scrapy.http import Response\n from scrapy.responsetypes import responsetypes\n\n@@ -2,7 +2,7 @@\n \"\"\"\n from twisted.internet import reactor\n \n-from scrapy.utils.misc import load_object, create_instance\n+from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.python import to_unicode\n \n \n\n@@ -1,27 +1,27 @@\n \"\"\"Download handlers for http and https schemes\"\"\"\n \n-import re\n import logging\n+import re\n import warnings\n from io import BytesIO\n from time import time\n from urllib.parse import urldefrag\n \n-from zope.interface import implementer\n-from twisted.internet import defer, reactor, protocol\n+from twisted.internet import defer, protocol, reactor\n+from twisted.internet.endpoints import TCP4ClientEndpoint\n+from twisted.internet.error import TimeoutError\n+from twisted.web.client import Agent, HTTPConnectionPool, ResponseDone, ResponseFailed, URI\n+from twisted.web.http import _DataLoss, PotentialDataLoss\n from twisted.web.http_headers import Headers as TxHeaders\n from twisted.web.iweb import IBodyProducer, UNKNOWN_LENGTH\n-from twisted.internet.error import TimeoutError\n-from twisted.web.http import _DataLoss, PotentialDataLoss\n-from twisted.web.client import Agent, ResponseDone, HTTPConnectionPool, ResponseFailed, URI\n-from twisted.internet.endpoints import TCP4ClientEndpoint\n+from zope.interface import implementer\n \n+from scrapy.core.downloader.tls import openssl_methods\n+from scrapy.core.downloader.webclient import _parse\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n-from scrapy.core.downloader.webclient import _parse\n-from scrapy.core.downloader.tls import openssl_methods\n-from scrapy.utils.misc import load_object, create_instance\n+from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.python import to_bytes, to_unicode\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
