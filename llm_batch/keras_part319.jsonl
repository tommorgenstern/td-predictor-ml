{"custom_id": "keras#da55ccb820650298004f2791ddd0dc712561d6c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 493 | Lines Deleted: 202 | Files Changed: 16 | Hunks: 74 | Methods Changed: 83 | Complexity Δ (Sum/Max): 39/24 | Churn Δ: 695 | Churn Cumulative: 42871 | Contributors (this commit): 153 | Commits (past 90d): 66 | Contributors (cumulative): 201 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,6 +20,7 @@ from __future__ import print_function\n \n import tensorflow as tf\n \n+from absl.testing import parameterized\n import numpy as np\n \n import keras\n@@ -53,7 +54,7 @@ class MiniModel(keras_training.Model):\n             tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n         ],\n         mode=[\"eager\"]))\n-class MirroredStrategyDefunTest(tf.test.TestCase):\n+class MirroredStrategyDefunTest(tf.test.TestCase, parameterized.TestCase):\n \n   def testTrain(self, distribution):\n     with distribution.scope():\n@@ -99,7 +100,7 @@ class MirroredStrategyDefunTest(tf.test.TestCase):\n         \"\"\"The step function for one training step.\"\"\"\n \n         def step_fn(inputs):\n-          \"\"\"The computation to run on each TPU device.\"\"\"\n+          \"\"\"The computation to run on each replica(GPU).\"\"\"\n           features, labels = inputs\n           with tf.GradientTape() as tape:\n             pred = model(features, training=True)\n@@ -126,6 +127,10 @@ class MirroredStrategyDefunTest(tf.test.TestCase):\n       self.assertGreater(accuracy.result().numpy(), 0.5)\n       self.assertEqual(optimizer.iterations.numpy(), num_epochs * num_steps)\n \n+    # Test save/load/serving the trained model.\n+    test_utils_obj.test_save_load_serving_model(\n+        model, feature_mapper, test_utils_obj.define_reverse_lookup_layer())\n+\n \n if __name__ == \"__main__\":\n   tf.test.main()\n\n@@ -23,13 +23,20 @@ import tensorflow as tf\n \n import random\n import tempfile\n+from absl import logging\n from absl.testing import parameterized\n+import numpy as np\n \n import keras\n from tensorflow.python.distribute import multi_worker_test_base\n from tensorflow.python.distribute.cluster_resolver import SimpleClusterResolver\n+from keras import callbacks as callbacks_lib\n+from keras.engine import sequential\n+from keras.layers import core as core_layers\n from keras.layers.preprocessing import string_lookup\n+from keras.optimizer_v2 import gradient_descent\n from keras.optimizer_v2 import rmsprop\n+from keras.utils import dataset_creator\n from keras.utils import losses_utils\n from tensorflow.python.training.server_lib import ClusterSpec\n \n@@ -42,16 +49,19 @@ FEATURE_VOCAB = [\n LABEL_VOCAB = [\"yes\", \"no\"]\n \n \n-def make_coordinator(num_workers, num_ps):\n+def make_cluster(num_workers, num_ps):\n   cluster_def = multi_worker_test_base.create_in_process_cluster(\n       num_workers=num_workers, num_ps=num_ps, rpc_layer=\"grpc\")\n   cluster_def[\"chief\"] = [\n       \"localhost:%d\" % multi_worker_test_base.pick_unused_port()\n   ]\n-  cluster_resolver = SimpleClusterResolver(\n-      ClusterSpec(cluster_def), rpc_layer=\"grpc\")\n+  return SimpleClusterResolver(ClusterSpec(cluster_def), rpc_layer=\"grpc\")\n+\n+\n+def make_coordinator(num_workers, num_ps):\n   return tf.distribute.experimental.coordinator.ClusterCoordinator(\n-      tf.distribute.experimental.ParameterServerStrategy(cluster_resolver))\n+      tf.distribute.experimental.ParameterServerStrategy(\n+          make_cluster(num_workers, num_ps)))\n \n \n # TODO(yuefengz): move this to keras/integration_tests.\n@@ -166,7 +176,7 @@ class KPLTest(tf.test.TestCase, parameterized.TestCase):\n         actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\n         accuracy.update_state(labels, actual_pred)\n \n-      self.coordinator._strategy.run(replica_fn, args=(iterator,))\n+      self.coordinator.strategy.run(replica_fn, args=(iterator,))\n \n     distributed_dataset = self.coordinator.create_per_worker_dataset(dataset_fn)\n     distributed_iterator = iter(distributed_dataset)\n@@ -218,6 +228,124 @@ class KPLTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertIn(prediction1, (\"yes\", \"no\"))\n \n \n+class ModelFitTest(tf.test.TestCase, parameterized.TestCase):\n+\n+  def _model_compile(self, steps_per_execution=1, run_eagerly=False):\n+\n+    class ResultAssertingCallback(callbacks_lib.Callback):\n+\n+      def __init__(self):\n+        self._prev_epoch = -1\n+\n+      def on_epoch_end(self, epoch, logs=None):\n+        logging.info(\"testModelFit: epoch=%r, logs=%r\", epoch, logs)\n+        if epoch <= self._prev_epoch:\n+          raise RuntimeError(\"Epoch is supposed to be larger than previous.\")\n+        self._prev_epoch = epoch\n+        if (logs.get(\"loss\", None) is None or\n+            not isinstance(logs[\"loss\"], np.floating)):\n+          raise RuntimeError(\"loss is supposed to be in the logs and float.\")\n+\n+    strategy = tf.distribute.experimental.ParameterServerStrategy(\n+        make_cluster(3, 2))\n+    with strategy.scope():\n+      model = sequential.Sequential([core_layers.Dense(10)])\n+    model.compile(\n+        gradient_descent.SGD(),\n+        loss=\"mse\",\n+        steps_per_execution=steps_per_execution,\n+        run_eagerly=run_eagerly)\n+    return model, [ResultAssertingCallback()]\n+\n+  def _model_fit(self,\n+                 steps_per_execution=1,\n+                 validation_data=None,\n+                 x=None,\n+                 steps_per_epoch=10,\n+                 run_eagerly=False):\n+    model, callbacks = self._model_compile(steps_per_execution, run_eagerly)\n+\n+    def dataset_fn(input_context):\n+      del input_context\n+      x = tf.random.uniform((10, 10))\n+      y = tf.random.uniform((10,))\n+      return tf.data.Dataset.from_tensor_slices(\n+          (x, y)).shuffle(10).repeat().batch(2)\n+\n+    x = x or dataset_creator.DatasetCreator(dataset_fn)\n+\n+    model.fit(\n+        x,\n+        epochs=10,\n+        steps_per_epoch=steps_per_epoch,\n+        verbose=0,\n+        callbacks=callbacks,\n+        validation_data=validation_data)\n+    return model\n+\n+  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n+  def testModelFit(self):\n+    model = self._model_fit()\n+    self.assertEqual(model.optimizer.iterations, 100)\n+\n+  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n+  def testModelFitWithStepsPerExecution(self):\n+    model = self._model_fit(steps_per_execution=10)\n+    self.assertEqual(model.optimizer.iterations, 100)\n+\n+  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n+  def testModelFitWithNoStepsPerEpoch(self):\n+    with self.assertRaisesRegex(\n+        ValueError, \"`steps_per_epoch` must be specified with \"\n+        \"`ParameterServerStrategy`.\"):\n+      self._model_fit(steps_per_epoch=None)\n+\n+  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n+  def testModelFitWithRunEagerly(self):\n+    with self.assertRaisesRegex(\n+        ValueError, \"When using `Model` with `ParameterServerStrategy`, \"\n+        \"`run_eagerly` is not supported.\"):\n+      self._model_fit(run_eagerly=True)\n+\n+  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n+  def testModelFitWithValidationData(self):\n+    with self.assertRaisesRegex(\n+        NotImplementedError, \"Evaluation in `model.fit` with \"\n+        \"`ParameterServerStrategy` is not yet supported.\"):\n+      self._model_fit(\n+          validation_data=tf.data.Dataset.from_tensor_slices([1, 1]))\n+\n+  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n+  def testModelFitWithDatasetInstance(self):\n+    with self.assertRaisesRegex(\n+        NotImplementedError, \"Only `DatasetCreator` input is supported in \"\n+        \"`ParameterServerStrategy` at this time.\"):\n+      self._model_fit(x=tf.data.Dataset.from_tensor_slices([1, 1]))\n+\n+  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n+  def testModelEvaluate(self):\n+    model, _ = self._model_compile()\n+    with self.assertRaisesRegex(\n+        NotImplementedError, \"`model.evaluate` is not yet supported with \"\n+        \"`ParameterServerStrategy`.\"):\n+      model.evaluate(x=tf.data.Dataset.from_tensor_slices([1, 1]))\n+\n+  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n+  def testModelPredict(self):\n+    model, _ = self._model_compile()\n+    with self.assertRaisesRegex(\n+        NotImplementedError, \"`model.predict` is not yet supported with \"\n+        \"`ParameterServerStrategy`.\"):\n+      model.predict(x=tf.data.Dataset.from_tensor_slices([1, 1]))\n+\n+  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n+  def testClusterCoordinatorSingleInstance(self):\n+    model = self._model_fit()\n+    strategy = model.distribute_strategy\n+    self.assertIs(strategy._cluster_coordinator,\n+                  tf.distribute.experimental.coordinator.ClusterCoordinator(strategy))\n+\n+\n if __name__ == \"__main__\":\n   tf.compat.v1.enable_v2_behavior()\n   tf.test.main()\n\n@@ -247,6 +247,15 @@ class LossesContainer(Container):\n       # Ok for a model to have no compiled loss.\n       return tf.zeros(shape=())\n \n+  def reset_states(self):\n+    \"\"\"Resets the state of loss metrics.\"\"\"\n+    if not self._built:\n+      return\n+    metrics = [self._loss_metric] + tf.nest.flatten(self._per_output_metrics)\n+    for metric_obj in metrics:\n+      if metric_obj is not None:\n+        metric_obj.reset_states()\n+\n   def _get_loss_object(self, loss):\n     \"\"\"Returns a `Loss` object.\n \n@@ -431,6 +440,21 @@ class MetricsContainer(Container):\n           continue\n         weighted_metric_obj.update_state(y_t, y_p, sample_weight=sw)\n \n+  def reset_states(self):\n+    \"\"\"Resets the state of all `Metric`s in this container.\"\"\"\n+    if self._built:\n+      metrics = self._metrics_in_order\n+    else:\n+      # If the user supplied `Metric` objects directly, we should\n+      # reset those. This could also contain `str`s or `function`s\n+      # though.\n+      metrics = tf.nest.flatten(self._user_metrics) + tf.nest.flatten(\n+          self._user_weighted_metrics)\n+\n+    for metric_obj in metrics:\n+      if isinstance(metric_obj, metrics_mod.Metric):\n+        metric_obj.reset_states()\n+\n   def _get_metric_objects(self, metrics, y_t, y_p):\n     \"\"\"Convert user-supplied metrics to `Metric` objects.\"\"\"\n     metrics = tf.nest.flatten(metrics)\n\n@@ -42,6 +42,9 @@ class LossesContainerTest(keras_parameterized.TestCase):\n     self.assertEqual(loss_metric.name, 'loss')\n     self.assertEqual(loss_metric.result().numpy(), 1.)\n \n+    loss_container.reset_states()\n+    self.assertEqual(loss_metric.result().numpy(), 0.)\n+\n   def test_loss_list(self):\n     loss_container = compile_utils.LossesContainer(['mse', 'mae'], [1, 0.5])\n \n@@ -68,6 +71,11 @@ class LossesContainerTest(keras_parameterized.TestCase):\n     self.assertEqual(output_2_metric.name, 'output_2_loss')\n     self.assertEqual(output_2_metric.result().numpy(), 0.5)\n \n+    loss_container.reset_states()\n+    self.assertEqual(loss_metric.result().numpy(), 0)\n+    self.assertEqual(output_1_metric.result().numpy(), 0)\n+    self.assertEqual(output_2_metric.result().numpy(), 0)\n+\n   def test_loss_dict(self):\n     loss_container = compile_utils.LossesContainer(\n         {\n@@ -100,6 +108,11 @@ class LossesContainerTest(keras_parameterized.TestCase):\n     self.assertEqual(out2_metric.name, 'out2_loss')\n     self.assertEqual(out2_metric.result().numpy(), 0.5)\n \n+    loss_container.reset_states()\n+    self.assertEqual(loss_metric.result().numpy(), 0)\n+    self.assertEqual(out1_metric.result().numpy(), 0)\n+    self.assertEqual(out2_metric.result().numpy(), 0)\n+\n   def test_loss_partial_dict_with_output_names(self):\n     loss_container = compile_utils.LossesContainer(\n         {'out2': 'mae'}, {'out2': 1.}, output_names=['out1', 'out2'])\n@@ -392,6 +405,9 @@ class MetricsContainerTest(keras_parameterized.TestCase):\n     self.assertEqual(metric.name, 'mse')\n     self.assertEqual(metric.result().numpy(), 1.)\n \n+    metric_container.reset_states()\n+    self.assertEqual(metric.result().numpy(), 0.)\n+\n   def test_list_of_metrics_one_output(self):\n     metric_container = compile_utils.MetricsContainer(['mse', 'mae'])\n     y_t, y_p = 2 * tf.ones((10, 5)), tf.zeros((10, 5))\n@@ -406,6 +422,10 @@ class MetricsContainerTest(keras_parameterized.TestCase):\n     self.assertEqual(mae_metric.name, 'mae')\n     self.assertEqual(mae_metric.result().numpy(), 2.)\n \n+    metric_container.reset_states()\n+    self.assertEqual(mse_metric.result().numpy(), 0.)\n+    self.assertEqual(mae_metric.result().numpy(), 0.)\n+\n   def test_list_of_metrics_list_of_outputs(self):\n     metric_container = compile_utils.MetricsContainer(\n         metrics=['mse', 'mae'],  # Should broadcast to both outputs.\n@@ -487,6 +507,12 @@ class MetricsContainerTest(keras_parameterized.TestCase):\n     self.assertEqual(weighted_mae_metric.name, 'out2_weighted_mae')\n     self.assertEqual(weighted_mae_metric.result().numpy(), 2.)\n \n+    metric_container.reset_states()\n+    self.assertEqual(mse_metric.result().numpy(), 0.)\n+    self.assertEqual(weighted_mse_metric.result().numpy(), 0.)\n+    self.assertEqual(mae_metric.result().numpy(), 0.)\n+    self.assertEqual(weighted_mae_metric.result().numpy(), 0.)\n+\n   def test_metric_partial_dict_with_output_names(self):\n     metric_container = compile_utils.MetricsContainer(\n         {'out2': 'mae'}, output_names=['out1', 'out2'])\n@@ -756,6 +782,15 @@ class MetricsContainerTest(keras_parameterized.TestCase):\n     self.assertEqual(metric_container.metrics[0].name, 'custom_metric_fn')\n     self.assertEqual(metric_container.metrics[1].name, 'custom_metric_class')\n \n+  def test_reset_states_existing_metric_before_built(self):\n+    metric = metrics_mod.Mean()\n+    metric.update_state([2.0, 4.0])\n+    self.assertEqual(metric.result().numpy(), 3.0)\n+\n+    metric_container = compile_utils.MetricsContainer(metric)\n+    metric_container.reset_states()\n+    self.assertEqual(metric.result().numpy(), 0.0)\n+\n \n if __name__ == '__main__':\n   tf.compat.v1.enable_eager_execution()\n\n@@ -34,6 +34,7 @@ from tensorflow.python.framework import smart_cond\n from keras import backend\n from keras.engine import training_utils\n from keras.utils import data_utils\n+from keras.utils import dataset_creator\n from keras.utils import tf_utils\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n@@ -498,6 +499,40 @@ class GenericArrayLikeDataAdapter(TensorLikeDataAdapter):\n     return dataset\n \n \n+class DatasetCreatorAdapter(DataAdapter):\n+  \"\"\"Adapter that handles dataset functions.\"\"\"\n+\n+  def __init__(self, *args, **kwargs):\n+    super(DatasetCreatorAdapter, self).__init__(*args, **kwargs)\n+\n+  @staticmethod\n+  def can_handle(x, y=None):\n+    if isinstance(x, dataset_creator.DatasetCreator):\n+      assert y is None\n+      return True\n+\n+  def should_recreate_iterator(self):\n+    # We expect users to shuffle the dataset in their `dataset_fn` supplied to\n+    # `DatasetCreator`. Since that is a buffered shuffle, we intend to not reset\n+    # the dataset so the batches that are not shuffled can still be pulled.\n+    return False\n+\n+  def get_size(self):\n+    raise NotImplementedError()\n+\n+  def get_dataset(self):\n+    raise NotImplementedError()\n+\n+  def batch_size(self):\n+    raise NotImplementedError()\n+\n+  def has_partial_batch(self):\n+    raise NotImplementedError()\n+\n+  def partial_batch_size(self):\n+    raise NotImplementedError()\n+\n+\n class CompositeTensorDataAdapter(DataAdapter):\n   \"\"\"Adapter that handles composite tensor.\"\"\"\n \n@@ -932,8 +967,8 @@ class KerasSequenceAdapter(GeneratorDataAdapter):\n \n ALL_ADAPTER_CLS = [\n     ListsOfScalarsDataAdapter, TensorLikeDataAdapter,\n-    GenericArrayLikeDataAdapter, DatasetAdapter,\n-    GeneratorDataAdapter, KerasSequenceAdapter, CompositeTensorDataAdapter,\n+    GenericArrayLikeDataAdapter, DatasetAdapter, GeneratorDataAdapter,\n+    KerasSequenceAdapter, CompositeTensorDataAdapter, DatasetCreatorAdapter\n ]\n \n \n@@ -1104,6 +1139,7 @@ class DataHandler(object):\n       self._steps_per_execution_value = steps_per_execution.numpy().item()\n \n     adapter_cls = select_data_adapter(x, y)\n+    self._verify_data_adapter_compatibility(adapter_cls)\n     self._adapter = adapter_cls(\n         x,\n         y,\n@@ -1119,6 +1155,23 @@ class DataHandler(object):\n         model=model)\n \n     strategy = tf.distribute.get_strategy()\n+\n+    self._current_step = 0\n+    self._step_increment = self._steps_per_execution_value - 1\n+    self._insufficient_data = False\n+\n+    self._configure_dataset_and_inferred_steps(strategy, x, steps_per_epoch,\n+                                               class_weight, distribute)\n+\n+  def _verify_data_adapter_compatibility(self, adapter_cls):\n+    if adapter_cls == DatasetCreatorAdapter:\n+      raise NotImplementedError(\"`DatasetCreator` input is only supported in \"\n+                                \"`ParameterServerStrategy` at this time.\")\n+\n+  def _configure_dataset_and_inferred_steps(self, strategy, x, steps_per_epoch,\n+                                            class_weight, distribute):\n+    \"\"\"Configure the `_dataset` and `_inferred_steps` attributes.\"\"\"\n+    del x\n     dataset = self._adapter.get_dataset()\n     if class_weight:\n       dataset = dataset.map(_make_class_weight_map_fn(class_weight))\n@@ -1129,11 +1182,6 @@ class DataHandler(object):\n     if distribute and not _is_distributed_dataset(dataset):\n       dataset = strategy.experimental_distribute_dataset(dataset)\n     self._dataset = dataset\n-\n-    self._current_step = 0\n-    self._step_increment = self._steps_per_execution_value - 1\n-    self._insufficient_data = False\n-\n     self._validate_data_handler()\n \n   def enumerate_epochs(self):\n@@ -1165,12 +1213,15 @@ class DataHandler(object):\n         self._steps_per_execution.assign(original_value)\n         self._steps_per_execution_value = original_value\n \n+  def sync(self):\n+    context.async_wait()\n+\n   @contextlib.contextmanager\n   def catch_stop_iteration(self):\n     \"\"\"Catches errors when an iterator runs out of data.\"\"\"\n     try:\n       yield\n-      context.async_wait()\n+      self.sync()\n     except (StopIteration, tf.errors.OutOfRangeError):\n       if self._inferred_steps is None:\n         self._inferred_steps = self._current_step\n@@ -1269,6 +1320,46 @@ class DataHandler(object):\n           \"`steps_per_execution > 1`, you must specify the number of steps \"\n           \"to run.\")\n \n+  def resolve_logs(self, logs):\n+    return logs\n+\n+\n+class _ClusterCoordinatorDataHandler(DataHandler):\n+  \"\"\"A `DataHandler` that is compatible with `ClusterCoordinator`.\"\"\"\n+\n+  def _verify_data_adapter_compatibility(self, adapter_cls):\n+    if adapter_cls != DatasetCreatorAdapter:\n+      raise NotImplementedError(\"Only `DatasetCreator` input is supported in \"\n+                                \"`ParameterServerStrategy` at this time.\")\n+\n+  def _configure_dataset_and_inferred_steps(self, strategy, x, steps_per_epoch,\n+                                            class_weight, distribute):\n+    if not isinstance(x, dataset_creator.DatasetCreator):\n+      raise TypeError(\"When using `ParameterServerStrategy`, `x` must be a \"\n+                      \"`DatasetCreator`.\")\n+\n+    def per_worker_dataset_fn():\n+      return strategy.distribute_datasets_from_function(x)\n+\n+    self._dataset = self._model._cluster_coordinator.create_per_worker_dataset(  # pylint: disable=protected-access\n+        per_worker_dataset_fn)\n+    if steps_per_epoch is None:\n+      raise ValueError(\n+          \"`steps_per_epoch` must be specified with `ParameterServerStrategy`.\")\n+    self._inferred_steps = steps_per_epoch\n+\n+  def sync(self):\n+    self._model._cluster_coordinator.join()  # pylint: disable=protected-access\n+\n+  def resolve_logs(self, logs):\n+    return logs.fetch()\n+\n+\n+def get_data_handler(*args, **kwargs):\n+  if getattr(kwargs[\"model\"], \"_cluster_coordinator\", None):\n+    return _ClusterCoordinatorDataHandler(*args, **kwargs)\n+  return DataHandler(*args, **kwargs)\n+\n \n def _make_class_weight_map_fn(class_weight):\n   \"\"\"Applies class weighting to a `Dataset`.\n\n@@ -282,6 +282,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       self._distribution_strategy = tf.distribute.get_strategy()\n     else:\n       self._distribution_strategy = None\n+\n+    self._cluster_coordinator = None\n+\n     # Defaults to value of `tf.config.experimental_functions_run_eagerly`.\n     self._run_eagerly = None\n     # Initialize cache attrs.\n@@ -720,6 +723,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                        'constructed with `dynamic=True`). '\n                        'You cannot set `run_eagerly=False`.')\n \n+    if self._cluster_coordinator and self._run_eagerly:\n+      raise ValueError('When using `Model` with `ParameterServerStrategy`, '\n+                       '`run_eagerly` is not supported.')\n+\n     # Run eagerly logic, by priority:\n     # (1) Dynamic models must be run eagerly.\n     # (2) Explicitly setting run_eagerly causes a Model to be run eagerly.\n@@ -828,6 +835,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           train_function, experimental_relax_shapes=True)\n \n     self.train_function = train_function\n+\n+    if self._cluster_coordinator:\n+      self.train_function = lambda iterator: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n+          train_function, args=(iterator,))\n+\n     return self.train_function\n \n   def fit(self,\n@@ -1056,10 +1068,14 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       val_x, val_y, val_sample_weight = (\n           data_adapter.unpack_x_y_sample_weight(validation_data))\n \n+    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n+      self._cluster_coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(\n+          self.distribute_strategy)\n+\n     with self.distribute_strategy.scope(), \\\n          training_utils.RespectCompiledTrainableState(self):\n       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n-      data_handler = data_adapter.DataHandler(\n+      data_handler = data_adapter.get_data_handler(\n           x=x,\n           y=y,\n           sample_weight=sample_weight,\n@@ -1118,6 +1134,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               if self.stop_training:\n                 break\n \n+        logs = data_handler.resolve_logs(logs)\n         if logs is None:\n           raise ValueError('Expect x to be a non-empty array or dataset.')\n         epoch_logs = copy.copy(logs)\n@@ -1127,7 +1144,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           # Create data_handler for evaluation and cache it.\n           if getattr(self, '_eval_data_handler', None) is None:\n             self._fit_frame = tf_inspect.currentframe()\n-            self._eval_data_handler = data_adapter.DataHandler(\n+            self._eval_data_handler = data_adapter.get_data_handler(\n                 x=val_x,\n                 y=val_y,\n                 sample_weight=val_sample_weight,\n@@ -1355,6 +1372,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     self._check_call_args('evaluate')\n     _disallow_inside_tf_function('evaluate')\n \n+    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n+      raise NotImplementedError('`model.evaluate` is not yet supported with '\n+                                '`ParameterServerStrategy`.')\n+\n     with self.distribute_strategy.scope():\n       # Use cached evaluation data only when it's called in `Model.fit`\n       if (getattr(self, '_fit_frame', None) is not None\n@@ -1363,7 +1384,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         data_handler = self._eval_data_handler\n       else:\n         # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n-        data_handler = data_adapter.DataHandler(\n+        data_handler = data_adapter.get_data_handler(\n             x=x,\n             y=y,\n             sample_weight=sample_weight,\n@@ -1590,6 +1611,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     self._check_call_args('predict')\n     _disallow_inside_tf_function('predict')\n \n+    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n+      raise NotImplementedError('`model.predict` is not yet supported with '\n+                                '`ParameterServerStrategy`.')\n+\n     outputs = None\n     with self.distribute_strategy.scope():\n       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n@@ -1607,7 +1632,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                         'AutoShardPolicy.FILE might lead to out-of-order result'\n                         '. Consider setting it to AutoShardPolicy.DATA.')\n \n-      data_handler = data_adapter.DataHandler(\n+      data_handler = data_adapter.get_data_handler(\n           x=x,\n           batch_size=batch_size,\n           steps_per_epoch=steps,\n@@ -2625,6 +2650,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     return functions\n \n   def _should_eval(self, epoch, validation_freq):\n+    if self._cluster_coordinator:\n+      raise NotImplementedError(\n+          'Evaluation in `model.fit` with '\n+          '`ParameterServerStrategy` is not yet supported.')\n     epoch = epoch + 1  # one-index the user-facing epoch.\n     if isinstance(validation_freq, int):\n       return epoch % validation_freq == 0\n\n@@ -73,20 +73,6 @@ class Hashing(base_preprocessing_layer.PreprocessingLayer):\n            [2],\n            [2]])>\n \n-\n-  Example (FarmHash64) with list of inputs:\n-  >>> layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3)\n-  >>> inp_1 = [['A'], ['B'], ['C'], ['D'], ['E']]\n-  >>> inp_2 = np.asarray([[5], [4], [3], [2], [1]])\n-  >>> layer([inp_1, inp_2])\n-  <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n-    array([[1],\n-           [1],\n-           [0],\n-           [2],\n-           [0]])>\n-\n-\n   Example (SipHash64):\n \n   >>> layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3,\n@@ -176,47 +162,13 @@ class Hashing(base_preprocessing_layer.PreprocessingLayer):\n \n   def call(self, inputs):\n     inputs = self._preprocess_inputs(inputs)\n-    if isinstance(inputs, (tuple, list)):\n-      return self._process_input_list(inputs)\n-    elif isinstance(inputs, tf.SparseTensor):\n+    if isinstance(inputs, tf.SparseTensor):\n       return tf.SparseTensor(\n           indices=inputs.indices,\n           values=self._hash_values_to_bins(inputs.values),\n           dense_shape=inputs.dense_shape)\n     return self._hash_values_to_bins(inputs)\n \n-  def _process_input_list(self, inputs):\n-    # TODO(momernick): support ragged_cross_hashed with corrected fingerprint\n-    # and siphash.\n-    if any(isinstance(inp, tf.RaggedTensor) for inp in inputs):\n-      raise ValueError('Hashing with ragged input is not supported yet.')\n-    if self.mask_value is not None:\n-      raise ValueError(\n-          'Cross hashing with a mask_value is not supported yet, mask_value is '\n-          '{}.'.format(self.mask_value))\n-    sparse_inputs = [\n-        inp for inp in inputs if isinstance(inp, tf.SparseTensor)\n-    ]\n-    dense_inputs = [\n-        inp for inp in inputs if not isinstance(inp, tf.SparseTensor)\n-    ]\n-    all_dense = True if not sparse_inputs else False\n-    indices = [sp_inp.indices for sp_inp in sparse_inputs]\n-    values = [sp_inp.values for sp_inp in sparse_inputs]\n-    shapes = [sp_inp.dense_shape for sp_inp in sparse_inputs]\n-    indices_out, values_out, shapes_out = tf.raw_ops.SparseCrossHashed(\n-        indices=indices,\n-        values=values,\n-        shapes=shapes,\n-        dense_inputs=dense_inputs,\n-        num_buckets=self.num_bins,\n-        strong_hash=self.strong_hash,\n-        salt=self.salt)\n-    sparse_out = tf.SparseTensor(indices_out, values_out, shapes_out)\n-    if all_dense:\n-      return tf.sparse.to_dense(sparse_out)\n-    return sparse_out\n-\n   def _hash_values_to_bins(self, values):\n     \"\"\"Converts a non-sparse tensor of values to bin indices.\"\"\"\n     str_to_hash_bucket = self._get_string_to_hash_bucket_fn()\n@@ -246,22 +198,9 @@ class Hashing(base_preprocessing_layer.PreprocessingLayer):\n           tf.strings.to_hash_bucket_strong, key=self.salt)\n \n   def compute_output_shape(self, input_shape):\n-    if not isinstance(input_shape, (tuple, list)):\n     return input_shape\n-    input_shapes = input_shape\n-    batch_size = None\n-    for inp_shape in input_shapes:\n-      inp_tensor_shape = tf.TensorShape(inp_shape).as_list()\n-      if len(inp_tensor_shape) != 2:\n-        raise ValueError('Inputs must be rank 2, get {}'.format(input_shapes))\n-      if batch_size is None:\n-        batch_size = inp_tensor_shape[0]\n-    # The second dimension is dynamic based on inputs.\n-    output_shape = [batch_size, None]\n-    return tf.TensorShape(output_shape)\n \n   def compute_output_signature(self, input_spec):\n-    if not isinstance(input_spec, (tuple, list)):\n     output_shape = self.compute_output_shape(input_spec.shape)\n     output_dtype = tf.int64\n     if isinstance(input_spec, tf.SparseTensorSpec):\n@@ -269,18 +208,6 @@ class Hashing(base_preprocessing_layer.PreprocessingLayer):\n           shape=output_shape, dtype=output_dtype)\n     else:\n       return tf.TensorSpec(shape=output_shape, dtype=output_dtype)\n-    input_shapes = [x.shape for x in input_spec]\n-    output_shape = self.compute_output_shape(input_shapes)\n-    if any(\n-        isinstance(inp_spec, tf.RaggedTensorSpec)\n-        for inp_spec in input_spec):\n-      return tf.TensorSpec(shape=output_shape, dtype=tf.int64)\n-    elif any(\n-        isinstance(inp_spec, tf.SparseTensorSpec)\n-        for inp_spec in input_spec):\n-      return tf.SparseTensorSpec(\n-          shape=output_shape, dtype=tf.int64)\n-    return tf.TensorSpec(shape=output_shape, dtype=tf.int64)\n \n   def get_config(self):\n     config = {\n\n@@ -58,23 +58,6 @@ class HashingTest(keras_parameterized.TestCase):\n     # 'omar' should map to 0.\n     self.assertAllClose([[0], [1], [2], [1], [1]], omar_mask_output)\n \n-  def test_hash_dense_multi_inputs_mask_value_farmhash(self):\n-    layer = hashing.Hashing(num_bins=3, mask_value='omar')\n-    inp_1 = np.asarray([['omar'], ['stringer'], ['marlo'], ['wire'],\n-                        ['skywalker']])\n-    inp_2 = np.asarray([['A'], ['B'], ['C'], ['D'], ['E']])\n-    with self.assertRaisesRegex(ValueError, 'not supported yet'):\n-      _ = layer([inp_1, inp_2])\n-\n-  def test_hash_dense_multi_inputs_farmhash(self):\n-    layer = hashing.Hashing(num_bins=2)\n-    inp_1 = np.asarray([['omar'], ['stringer'], ['marlo'], ['wire'],\n-                        ['skywalker']])\n-    inp_2 = np.asarray([['A'], ['B'], ['C'], ['D'], ['E']])\n-    output = layer([inp_1, inp_2])\n-    # Assert equal for hashed output that should be true on all platforms.\n-    self.assertAllClose([[0], [0], [1], [1], [0]], output)\n-\n   def test_hash_dense_list_input_farmhash(self):\n     layer = hashing.Hashing(num_bins=2)\n     inp = [['omar'], ['stringer'], ['marlo'], ['wire'], ['skywalker']]\n@@ -87,15 +70,6 @@ class HashingTest(keras_parameterized.TestCase):\n     # Assert equal for hashed output that should be true on all platforms.\n     self.assertAllClose([0, 0, 1, 0, 0], output)\n \n-  def test_hash_dense_list_inputs_mixed_int_string_farmhash(self):\n-    layer = hashing.Hashing(num_bins=2)\n-    inp_1 = np.asarray([['omar'], ['stringer'], ['marlo'], ['wire'],\n-                        ['skywalker']])\n-    inp_2 = np.asarray([[1], [2], [3], [4], [5]]).astype(np.int64)\n-    output = layer([inp_1, inp_2])\n-    # Assert equal for hashed output that should be true on all platforms.\n-    self.assertAllClose([[0], [1], [1], [1], [0]], output)\n-\n   def test_hash_dense_int_input_farmhash(self):\n     layer = hashing.Hashing(num_bins=3)\n     inp = np.asarray([[0], [1], [2], [3], [4]])\n@@ -117,21 +91,6 @@ class HashingTest(keras_parameterized.TestCase):\n     # Note the result is different from (133, 137).\n     self.assertAllClose([[1], [0], [1], [0], [1]], output_2)\n \n-  def test_hash_dense_multi_inputs_siphash(self):\n-    layer = hashing.Hashing(num_bins=2, salt=[133, 137])\n-    inp_1 = np.asarray([['omar'], ['stringer'], ['marlo'], ['wire'],\n-                        ['skywalker']])\n-    inp_2 = np.asarray([['A'], ['B'], ['C'], ['D'], ['E']])\n-    output = layer([inp_1, inp_2])\n-    # Assert equal for hashed output that should be true on all platforms.\n-    # Note the result is different from FarmHash.\n-    self.assertAllClose([[0], [1], [0], [0], [1]], output)\n-\n-    layer_2 = hashing.Hashing(num_bins=2, salt=[211, 137])\n-    output_2 = layer_2([inp_1, inp_2])\n-    # Note the result is different from (133, 137).\n-    self.assertAllClose([[1], [1], [1], [0], [1]], output_2)\n-\n   def test_hash_dense_int_input_siphash(self):\n     layer = hashing.Hashing(num_bins=3, salt=[133, 137])\n     inp = np.asarray([[0], [1], [2], [3], [4]])\n@@ -168,19 +127,6 @@ class HashingTest(keras_parameterized.TestCase):\n     # 'omar' should map to 0.\n     self.assertAllClose([0, 1, 2, 1, 1], omar_mask_output.values)\n \n-  def test_hash_sparse_multi_inputs_farmhash(self):\n-    layer = hashing.Hashing(num_bins=2)\n-    indices = [[0, 0], [1, 0], [2, 0]]\n-    inp_1 = tf.SparseTensor(\n-        indices=indices,\n-        values=['omar', 'stringer', 'marlo'],\n-        dense_shape=[3, 1])\n-    inp_2 = tf.SparseTensor(\n-        indices=indices, values=['A', 'B', 'C'], dense_shape=[3, 1])\n-    output = layer([inp_1, inp_2])\n-    self.assertAllClose(indices, output.indices)\n-    self.assertAllClose([0, 0, 1], output.values)\n-\n   def test_hash_sparse_int_input_farmhash(self):\n     layer = hashing.Hashing(num_bins=3)\n     indices = [[0, 0], [1, 0], [1, 1], [2, 0], [2, 1]]\n@@ -207,25 +153,6 @@ class HashingTest(keras_parameterized.TestCase):\n     # The result should be same with test_hash_dense_input_siphash.\n     self.assertAllClose([1, 0, 1, 0, 1], output.values)\n \n-  def test_hash_sparse_multi_inputs_siphash(self):\n-    layer = hashing.Hashing(num_bins=2, salt=[133, 137])\n-    indices = [[0, 0], [1, 0], [2, 0]]\n-    inp_1 = tf.SparseTensor(\n-        indices=indices,\n-        values=['omar', 'stringer', 'marlo'],\n-        dense_shape=[3, 1])\n-    inp_2 = tf.SparseTensor(\n-        indices=indices, values=['A', 'B', 'C'], dense_shape=[3, 1])\n-    output = layer([inp_1, inp_2])\n-    # The result should be same with test_hash_dense_input_siphash.\n-    self.assertAllClose(indices, output.indices)\n-    self.assertAllClose([0, 1, 0], output.values)\n-\n-    layer_2 = hashing.Hashing(num_bins=2, salt=[211, 137])\n-    output = layer_2([inp_1, inp_2])\n-    # The result should be same with test_hash_dense_input_siphash.\n-    self.assertAllClose([1, 1, 1], output.values)\n-\n   def test_hash_sparse_int_input_siphash(self):\n     layer = hashing.Hashing(num_bins=3, salt=[133, 137])\n     indices = [[0, 0], [1, 0], [1, 1], [2, 0], [2, 1]]\n@@ -266,17 +193,6 @@ class HashingTest(keras_parameterized.TestCase):\n     expected_output = [[0, 1, 2, 1], [2, 1, 1]]\n     self.assertAllClose(expected_output, omar_mask_output)\n \n-  def test_hash_ragged_string_multi_inputs_farmhash(self):\n-    layer = hashing.Hashing(num_bins=2)\n-    inp_data_1 = tf.ragged.constant(\n-        [['omar', 'stringer', 'marlo', 'wire'], ['marlo', 'skywalker', 'wire']],\n-        dtype=tf.string)\n-    inp_data_2 = tf.ragged.constant(\n-        [['omar', 'stringer', 'marlo', 'wire'], ['marlo', 'skywalker', 'wire']],\n-        dtype=tf.string)\n-    with self.assertRaisesRegex(ValueError, 'not supported yet'):\n-      _ = layer([inp_data_1, inp_data_2])\n-\n   def test_hash_ragged_int_input_farmhash(self):\n     layer = hashing.Hashing(num_bins=3)\n     inp_data = tf.ragged.constant([[0, 1, 3, 4], [2, 1, 0]],\n@@ -315,17 +231,6 @@ class HashingTest(keras_parameterized.TestCase):\n     model = training.Model(inputs=inp_t, outputs=out_t)\n     self.assertAllClose(out_data, model.predict(inp_data))\n \n-  def test_hash_ragged_string_multi_inputs_siphash(self):\n-    layer = hashing.Hashing(num_bins=2, salt=[133, 137])\n-    inp_data_1 = tf.ragged.constant(\n-        [['omar', 'stringer', 'marlo', 'wire'], ['marlo', 'skywalker', 'wire']],\n-        dtype=tf.string)\n-    inp_data_2 = tf.ragged.constant(\n-        [['omar', 'stringer', 'marlo', 'wire'], ['marlo', 'skywalker', 'wire']],\n-        dtype=tf.string)\n-    with self.assertRaisesRegex(ValueError, 'not supported yet'):\n-      _ = layer([inp_data_1, inp_data_2])\n-\n   def test_hash_ragged_int_input_siphash(self):\n     layer = hashing.Hashing(num_bins=3, salt=[133, 137])\n     inp_data = tf.ragged.constant([[0, 1, 3, 4], [2, 1, 0]],\n\n@@ -228,7 +228,7 @@ class RNN(Layer):\n         `batch_size` is a scalar tensor that represents the batch size\n         of the inputs. `dtype` is `tf.DType` that represents the dtype of\n         the inputs.\n-        For backward compatible reason, if this method is not implemented\n+        For backward compatibility, if this method is not implemented\n         by the cell, the RNN layer will create a zero filled tensor with the\n         size of [batch_size, cell.state_size].\n       In the case that `cell` is a list of RNN cell instances, the cells\n\n@@ -1716,6 +1716,28 @@ def binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0):\n       K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n \n \n+@dispatch.dispatch_for_types(binary_crossentropy, tf.RaggedTensor)\n+def _ragged_tensor_binary_crossentropy(y_true,\n+                                       y_pred,\n+                                       from_logits=False,\n+                                       label_smoothing=0):\n+  \"\"\" Implements support for handling RaggedTensors.\n+\n+      Expected shape: (batch, sequence_len) with sequence_len being variable\n+      per batch.\n+      Return shape: (batch,); returns the per batch mean of the loss values.\n+\n+      When used by BinaryCrossentropy() with the default reduction\n+      (SUM_OVER_BATCH_SIZE), the reduction averages the per batch losses over\n+      the number of batches.\n+  \"\"\"\n+  fn = functools.partial(\n+      binary_crossentropy,\n+      from_logits=from_logits,\n+      label_smoothing=label_smoothing)\n+  return _ragged_tensor_apply_loss(fn, y_true, y_pred)\n+\n+\n @keras_export('keras.metrics.kl_divergence',\n               'keras.metrics.kullback_leibler_divergence', 'keras.metrics.kld',\n               'keras.metrics.KLD', 'keras.losses.kl_divergence',\n\n@@ -888,6 +888,35 @@ class BinaryCrossentropyTest(tf.test.TestCase):\n     expected_value = (100.0 + 50.0 * label_smoothing) / 3.0\n     self.assertAlmostEqual(self.evaluate(loss), expected_value, 3)\n \n+  def test_ragged_tensors(self):\n+    bce_obj = losses.BinaryCrossentropy()\n+    y_true = tf.ragged.constant([[1, 0, 1], [0]])\n+    y_pred = tf.ragged.constant([[1, 1, 1], [0]], dtype=tf.float32)\n+    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n+    loss = bce_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+    # per batch loss = [ sum([0, 15.33, 0]) / 3, 0. ]\n+    #                = [ 5.11, 0]\n+    # Reduced loss = 5.11 * 1.2 / 2\n+\n+    self.assertAlmostEqual(self.evaluate(loss), 3.0666, 3)\n+\n+    # Test with logits.\n+    y_true = tf.ragged.constant([[1, 0, 1], [0, 1]])\n+    logits = tf.ragged.constant([[100.0, -100.0, 100.0],\n+                                          [100.0, 100.0]])\n+    weights = tf.constant([4, 3])\n+    bce_obj = losses.BinaryCrossentropy(from_logits=True)\n+    loss = bce_obj(y_true, logits, sample_weight=weights)\n+\n+    # Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+    #            (where x = logits and z = y_true)\n+    # Loss = [(0 + 0 + 0)/3, 100 / 2]\n+    # Weighted loss = [0 * 4, 50 * 3]\n+    # Reduced loss = (0 + 50 * 3) / 2\n+\n+    self.assertAlmostEqual(self.evaluate(loss), 75., 3)\n+\n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n class CategoricalCrossentropyTest(tf.test.TestCase):\n\n@@ -37,6 +37,7 @@ from keras.saving.saved_model import load as keras_load\n from keras.saving.saved_model import serialized_attributes\n from keras.saving.saved_model import utils\n from keras.utils import tf_inspect\n+from keras.utils import tf_utils\n from keras.utils import version_utils\n from keras.utils.generic_utils import LazyLoader\n from tensorflow.python.platform import tf_logging as logging\n@@ -503,13 +504,17 @@ class LayerCallCollection(object):\n     return fn\n \n \n+def _filtered_inputs(inputs):\n+  return list(filter(tf_utils.is_tensor_or_variable, tf.nest.flatten(inputs)))\n+\n+\n def layer_call_wrapper(call_collection, method):\n   \"\"\"Ensures layer losses are kept the same, and runs method in call context.\"\"\"\n   def wrapper(*args, **kwargs):\n     \"\"\"Calls method within call context.\"\"\"\n     layer = call_collection.layer\n     training = None\n-    inputs = call_collection.get_input_arg_value(args, kwargs)\n+    inputs = _filtered_inputs([args, kwargs])\n     # pylint: disable=protected-access\n     if (args or kwargs) and call_collection.training_arg_was_passed(\n         args, kwargs):\n@@ -562,11 +567,12 @@ def _wrap_call_and_conditional_losses(layer):\n   \"\"\"\n   # Create function that generates both outputs and losses\n   layer_call = _get_layer_call_method(layer)\n-  def call_and_return_conditional_losses(inputs, *args, **kwargs):\n+  def call_and_return_conditional_losses(*args, **kwargs):\n     \"\"\"Returns layer (call_output, conditional losses) tuple.\"\"\"\n-    call_output = layer_call(inputs, *args, **kwargs)\n+    call_output = layer_call(*args, **kwargs)\n     if version_utils.is_v1_layer_or_model(layer):\n-      conditional_losses = layer.get_losses_for(inputs)\n+      conditional_losses = layer.get_losses_for(\n+          _filtered_inputs([args, kwargs]))\n     else:\n       conditional_losses = [\n           l for l in layer.losses if not hasattr(l, '_unconditional_loss')\n\n@@ -820,6 +820,41 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n     self.evaluate(tf.compat.v1.variables_initializer(loaded.variables))\n     self.assertAllClose(model.predict(f), loaded.predict(f))\n \n+  def testSaveLayerMultipleInputs(self):\n+    class CustomLayer(keras.layers.Layer):\n+\n+      def call(self, *input_list):\n+        self.add_loss(input_list[-2] * 2, inputs=True)\n+        return sum(input_list[:-1])  # The test's last input is a non-tensor arg\n+\n+    # TODO(b/175902133): Models only support one input argument. Also, create a\n+    # subclassed model because functional/sequential models still have funky\n+    # behavior when calling with multiple non-nested arguments.\n+    class CustomModel(keras.Model):\n+\n+      def build(self, _):\n+        self.layer = CustomLayer()\n+\n+      def call(self, inputs):\n+        inputs = inputs[:]\n+        inputs.append(object())  # Test that the layer handles non-tensor inputs\n+        return self.layer(*inputs)\n+\n+    model = CustomModel()\n+    inp = [tf.constant(i, shape=[1, 1], dtype=tf.float32)\n+           for i in range(1, 5)]\n+    expected = model(inp)\n+    expected_loss = model.get_losses_for(inp)\n+    saved_model_dir = self._save_model_dir()\n+    model.save(saved_model_dir, save_format='tf')\n+    loaded = keras_load.load(saved_model_dir)\n+    actual = loaded(inp)\n+    actual_loss = loaded.get_losses_for(inp)\n+    self.assertAllEqual(self.evaluate(expected),\n+                        self.evaluate(actual))\n+    self.assertAllEqual(self.evaluate(expected_loss),\n+                        self.evaluate(actual_loss))\n+\n \n class TestSavedModelFormat(tf.test.TestCase):\n \n\n@@ -64,12 +64,12 @@ def use_wrapped_call(layer, call_fn, default_training_value=None,\n       original_call, call_fn, expects_training_arg, default_training_value)\n \n   def return_outputs_and_add_losses(*args, **kwargs):\n-    \"\"\"Returns the outputs from the call_fn, and adds the losses.\"\"\"\n-    inputs_arg_index = 1 if return_method else 0\n-    inputs = args[inputs_arg_index]\n-    args = args[inputs_arg_index + 1:]\n-    outputs, losses = fn(inputs, *args, **kwargs)\n-    layer.add_loss(losses, inputs=inputs)\n+    \"\"\"Returns the outputs from the layer call function, and adds the losses.\"\"\"\n+    if return_method:\n+      args = args[1:]\n+\n+    outputs, losses = fn(*args, **kwargs)\n+    layer.add_loss(losses, inputs=True)\n \n     # TODO(kathywu): This is a temporary hack. When a network of layers is\n     # revived from SavedModel, only the top-level layer will have losses. This\n@@ -270,3 +270,4 @@ def keras_option_scope(save_traces):\n \n def should_save_traces():\n   return _save_options_context.save_traces\n+\n\n@@ -48,8 +48,6 @@ class DatasetCreatorTest(tf.test.TestCase):\n         next(iter(tf.data.Dataset.from_tensor_slices([1, 1]))))\n \n   def test_dataset_creator_usage_in_parameter_server_model_fit(self):\n-    self.skipTest(\"TODO(rchao): Enable this test once training API changes for \"\n-                  \"DatasetFactory is submitted.\")\n     cluster_def = multi_worker_test_base.create_in_process_cluster(\n         num_workers=2, num_ps=1, rpc_layer=\"grpc\")\n     cluster_def[\"chief\"] = [\n\n@@ -20,12 +20,13 @@ from __future__ import print_function\n import tensorflow as tf\n \n import random\n+import tempfile\n \n import keras\n from keras.layers.preprocessing import string_lookup\n \n \n-class DistributeKplTestUtils:\n+class DistributeKplTestUtils(tf.test.TestCase):\n   \"\"\"Utils for test of tf.distribute + KPL.\"\"\"\n   FEATURE_VOCAB = [\n       \"avenger\", \"ironman\", \"batman\", \"hulk\", \"spiderman\", \"kingkong\",\n@@ -125,3 +126,58 @@ class DistributeKplTestUtils:\n             emb_output)\n     model = keras.Model({\"features\": model_input}, dense_output)\n     return model\n+\n+  def define_reverse_lookup_layer(self):\n+    \"\"\"Create string reverse lookup layer for serving.\"\"\"\n+\n+    label_inverse_lookup_layer = string_lookup.StringLookup(\n+        num_oov_indices=1,\n+        mask_token=None,\n+        vocabulary=self.LABEL_VOCAB,\n+        invert=True)\n+    return label_inverse_lookup_layer\n+\n+  def create_serving_signature(self, model, feature_mapper,\n+                               label_inverse_lookup_layer):\n+    \"\"\"Create serving signature for the given model.\"\"\"\n+\n+    @tf.function\n+    def serve_fn(raw_features):\n+      raw_features = tf.compat.v1.expand_dims(raw_features, axis=0)\n+      transformed_features = model.feature_mapper(raw_features)\n+      outputs = model(transformed_features)\n+      outputs = tf.compat.v1.squeeze(outputs, axis=0)\n+      outputs = tf.cast(tf.greater(outputs, 0.5), tf.int64)\n+      decoded_outputs = model.label_inverse_lookup_layer(outputs)\n+      return tf.compat.v1.squeeze(decoded_outputs, axis=0)\n+\n+    model.feature_mapper = feature_mapper\n+    model.label_inverse_lookup_layer = label_inverse_lookup_layer\n+    # serving does NOT have batch dimension\n+    return serve_fn.get_concrete_function(\n+        tf.TensorSpec(\n+            shape=(3), dtype=tf.string, name=\"example\"))\n+\n+  def test_save_load_serving_model(self, model, feature_mapper,\n+                                   label_inverse_lookup_layer):\n+    \"\"\"Test save/load/serving model.\"\"\"\n+\n+    serving_fn = self.create_serving_signature(model, feature_mapper,\n+                                               label_inverse_lookup_layer)\n+\n+    saved_model_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n+    tf.saved_model.save(\n+        model, saved_model_dir, signatures={\"serving_default\": serving_fn})\n+\n+    # Test the saved_model.\n+    loaded_serving_fn = keras.saving.save.load_model(\n+        saved_model_dir).signatures[\"serving_default\"]\n+\n+    # check the result w/ and w/o avenger.\n+    prediction0 = loaded_serving_fn(\n+        tf.constant([\"avenger\", \"ironman\", \"avenger\"]))[\"output_0\"]\n+    self.assertIn(prediction0.numpy().decode(\"UTF-8\"), (\"yes\", \"no\"))\n+\n+    prediction1 = loaded_serving_fn(\n+        tf.constant([\"ironman\", \"ironman\", \"unkonwn\"]))[\"output_0\"]\n+    self.assertIn(prediction1.numpy().decode(\"UTF-8\"), (\"yes\", \"no\"))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
