{"custom_id": "keras#946058ec050395005d671de4355ebc149c2d26e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 20 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 78 | Churn Cumulative: 1401 | Contributors (this commit): 4 | Commits (past 90d): 24 | Contributors (cumulative): 6 | DMM Complexity: 0.9791666666666666\n\nDIFF:\n@@ -7,6 +7,7 @@ from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.backend import random\n from keras_core.initializers.initializer import Initializer\n+from keras_core.saving import serialization_lib\n \n \n @keras_core_export(\"keras_core.initializers.RandomNormal\")\n@@ -43,6 +44,7 @@ class RandomNormal(Initializer):\n     def __init__(self, mean=0.0, stddev=1.0, seed=None):\n         self.mean = mean\n         self.stddev = stddev\n+        self._init_seed = seed\n         self.seed = seed or random.make_default_seed()\n         super().__init__()\n \n@@ -56,7 +58,8 @@ class RandomNormal(Initializer):\n         )\n \n     def get_config(self):\n-        return {\"mean\": self.mean, \"stddev\": self.stddev, \"seed\": self.seed}\n+        seed_config = serialization_lib.serialize_keras_object(self._init_seed)\n+        return {\"mean\": self.mean, \"stddev\": self.stddev, \"seed\": seed_config}\n \n \n @keras_core_export(\"keras_core.initializers.TruncatedNormal\")\n@@ -96,6 +99,7 @@ class TruncatedNormal(Initializer):\n     def __init__(self, mean=0.0, stddev=1.0, seed=None):\n         self.mean = mean\n         self.stddev = stddev\n+        self._init_seed = seed\n         self.seed = seed or random.make_default_seed()\n         super().__init__()\n \n@@ -109,7 +113,8 @@ class TruncatedNormal(Initializer):\n         )\n \n     def get_config(self):\n-        return {\"mean\": self.mean, \"stddev\": self.stddev, \"seed\": self.seed}\n+        seed_config = serialization_lib.serialize_keras_object(self._init_seed)\n+        return {\"mean\": self.mean, \"stddev\": self.stddev, \"seed\": seed_config}\n \n \n @keras_core_export(\"keras_core.initializers.RandomUniform\")\n@@ -146,6 +151,7 @@ class RandomUniform(Initializer):\n     def __init__(self, minval=0.0, maxval=1.0, seed=None):\n         self.minval = minval\n         self.maxval = maxval\n+        self._init_seed = seed\n         self.seed = seed or random.make_default_seed()\n         super().__init__()\n \n@@ -159,7 +165,12 @@ class RandomUniform(Initializer):\n         )\n \n     def get_config(self):\n-        return {\"minval\": self.minval, \"maxval\": self.maxval, \"seed\": self.seed}\n+        seed_config = serialization_lib.serialize_keras_object(self._init_seed)\n+        return {\n+            \"minval\": self.minval,\n+            \"maxval\": self.maxval,\n+            \"seed\": seed_config,\n+        }\n \n \n @keras_core_export(\"keras_core.initializers.VarianceScaling\")\n@@ -239,6 +250,7 @@ class VarianceScaling(Initializer):\n         self.scale = scale\n         self.mode = mode\n         self.distribution = distribution\n+        self._init_seed = seed\n         self.seed = seed or random.make_default_seed()\n \n     def __call__(self, shape, dtype=None):\n@@ -267,11 +279,12 @@ class VarianceScaling(Initializer):\n             )\n \n     def get_config(self):\n+        seed_config = serialization_lib.serialize_keras_object(self._init_seed)\n         return {\n             \"scale\": self.scale,\n             \"mode\": self.mode,\n             \"distribution\": self.distribution,\n-            \"seed\": self.seed,\n+            \"seed\": seed_config,\n         }\n \n \n@@ -314,7 +327,9 @@ class GlorotUniform(VarianceScaling):\n         )\n \n     def get_config(self):\n-        return {\"seed\": self.seed}\n+        return {\n+            \"seed\": serialization_lib.serialize_keras_object(self._init_seed)\n+        }\n \n \n @keras_core_export(\"keras_core.initializers.GlorotNormal\")\n@@ -360,7 +375,9 @@ class GlorotNormal(VarianceScaling):\n         )\n \n     def get_config(self):\n-        return {\"seed\": self.seed}\n+        return {\n+            \"seed\": serialization_lib.serialize_keras_object(self._init_seed)\n+        }\n \n \n @keras_core_export(\"keras_core.initializers.LecunNormal\")\n@@ -406,7 +423,9 @@ class LecunNormal(VarianceScaling):\n         )\n \n     def get_config(self):\n-        return {\"seed\": self.seed}\n+        return {\n+            \"seed\": serialization_lib.serialize_keras_object(self._init_seed)\n+        }\n \n \n @keras_core_export(\"keras_core.initializers.LecunUniform\")\n@@ -448,7 +467,9 @@ class LecunUniform(VarianceScaling):\n         )\n \n     def get_config(self):\n-        return {\"seed\": self.seed}\n+        return {\n+            \"seed\": serialization_lib.serialize_keras_object(self._init_seed)\n+        }\n \n \n @keras_core_export(\"keras_core.initializers.HeNormal\")\n@@ -490,7 +511,9 @@ class HeNormal(VarianceScaling):\n         )\n \n     def get_config(self):\n-        return {\"seed\": self.seed}\n+        return {\n+            \"seed\": serialization_lib.serialize_keras_object(self._init_seed)\n+        }\n \n \n @keras_core_export(\"keras_core.initializers.HeUniform\")\n@@ -532,7 +555,9 @@ class HeUniform(VarianceScaling):\n         )\n \n     def get_config(self):\n-        return {\"seed\": self.seed}\n+        return {\n+            \"seed\": serialization_lib.serialize_keras_object(self._init_seed)\n+        }\n \n \n def compute_fans(shape):\n@@ -605,6 +630,7 @@ class OrthogonalInitializer(Initializer):\n \n     def __init__(self, gain=1.0, seed=None):\n         self.gain = gain\n+        self._init_seed = seed\n         self.seed = seed or random.make_default_seed()\n \n     def __call__(self, shape, dtype=None):\n@@ -636,4 +662,5 @@ class OrthogonalInitializer(Initializer):\n         return self.gain * ops.reshape(q, shape)\n \n     def get_config(self):\n-        return {\"gain\": self.gain, \"seed\": self.seed}\n+        seed_config = serialization_lib.serialize_keras_object(self._init_seed)\n+        return {\"gain\": self.gain, \"seed\": seed_config}\n\n@@ -1,5 +1,6 @@\n import numpy as np\n \n+from keras_core import backend\n from keras_core import initializers\n from keras_core import testing\n from keras_core import utils\n@@ -24,6 +25,34 @@ class InitializersTest(testing.TestCase):\n \n         self.run_class_serialization_test(initializer)\n \n+        # Test serialization with SeedGenerator\n+        initializer = initializers.RandomNormal(\n+            mean=mean, stddev=stddev, seed=backend.random.SeedGenerator(1337)\n+        )\n+        values = initializer(shape=shape)\n+\n+        # Test that unseeded generator gets different results after cloning\n+        initializer = initializers.RandomNormal(\n+            mean=mean, stddev=stddev, seed=None\n+        )\n+        values = initializer(shape=shape)\n+        cloned_initializer = initializers.RandomNormal.from_config(\n+            initializer.get_config()\n+        )\n+        new_values = cloned_initializer(shape=shape)\n+        self.assertNotAllClose(values, new_values)\n+\n+        # Test that seeded generator gets same results after cloning\n+        initializer = initializers.RandomNormal(\n+            mean=mean, stddev=stddev, seed=1337\n+        )\n+        values = initializer(shape=shape)\n+        cloned_initializer = initializers.RandomNormal.from_config(\n+            initializer.get_config()\n+        )\n+        new_values = cloned_initializer(shape=shape)\n+        self.assertAllClose(values, new_values)\n+\n     def test_random_uniform(self):\n         shape = (5, 5)\n         minval = -1.0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2723ab5504f87018269937450d664386e32e18ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 518 | Lines Deleted: 101 | Files Changed: 6 | Hunks: 13 | Methods Changed: 22 | Complexity Δ (Sum/Max): 46/47 | Churn Δ: 619 | Churn Cumulative: 897 | Contributors (this commit): 10 | Commits (past 90d): 84 | Contributors (cumulative): 18 | DMM Complexity: 0.603399433427762\n\nDIFF:\n@@ -14,7 +14,6 @@ from keras_core.backend.torch.core import is_tensor\n from keras_core.backend.torch.core import name_scope\n from keras_core.backend.torch.core import scatter\n from keras_core.backend.torch.core import shape\n-from keras_core.backend.torch.core import to_torch_dtype\n from keras_core.backend.torch.core import vectorized_map\n from keras_core.backend.torch.rnn import gru\n from keras_core.backend.torch.rnn import lstm\n\n@@ -13,7 +13,6 @@ TORCH_DTYPES = {\n     \"float32\": torch.float32,\n     \"float64\": torch.float64,\n     \"uint8\": torch.uint8,\n-    \"uint32\": torch.int64,  # TODO: Torch doesn't have `uint32` dtype.\n     \"int8\": torch.int8,\n     \"int16\": torch.int16,\n     \"int32\": torch.int32,\n@@ -24,8 +23,6 @@ TORCH_DTYPES = {\n \n \n def to_torch_dtype(dtype):\n-    if isinstance(dtype, torch.dtype):\n-        return dtype\n     dtype = standardize_dtype(dtype)\n     dtype = TORCH_DTYPES.get(dtype, None)\n     if dtype is None:\n\n@@ -1,113 +1,20 @@\n-import torch\n-import torch.nn.functional as tnn\n-\n from keras_core.backend.config import floatx\n-from keras_core.backend.torch.core import to_torch_dtype\n from keras_core.random.seed_generator import SeedGenerator\n from keras_core.random.seed_generator import draw_seed\n from keras_core.random.seed_generator import make_default_seed\n \n \n-def torch_seed_generator(seed):\n-    seed_val, _ = draw_seed(seed)\n-    generator = torch.Generator()\n-    generator.manual_seed(int(seed_val))\n-    return generator\n-\n-\n def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n-    \"\"\"Produce random number based on the normal distribution.\n-\n-    Args:\n-        shape: The shape of the random values to generate.\n-        mean: Floats, defaults to 0. Mean of the random values to generate.\n-        stddev: Floats, defaults to 1. Standard deviation of the random values\n-            to generate.\n-        dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `keras.backend.floatx()` is used,\n-            which defaults to `float32` unless you configured it otherwise (via\n-            `keras.backend.set_floatx(float_dtype)`).\n-        seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n-    \"\"\"\n-    dtype = dtype or floatx()\n-    dtype = to_torch_dtype(dtype)\n-    generator = torch_seed_generator(seed)\n-    return torch.normal(\n-        mean, stddev, size=shape, generator=generator, dtype=dtype\n-    )\n+    raise NotImplementedError\n \n \n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n-    \"\"\"Produce random number based on the uniform distribution.\n-\n-    Args:\n-        shape: The shape of the random values to generate.\n-        minval: Floats, defaults to 0. Lower bound of the range of\n-            random values to generate (inclusive).\n-        maxval: Floats, defaults to 1. Upper bound of the range of\n-            random values to generate (exclusive).\n-        dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `keras.backend.floatx()` is used,\n-            which defaults to `float32` unless you configured it otherwise (via\n-            `keras.backend.set_floatx(float_dtype)`)\n-        seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n-    \"\"\"\n-    dtype = dtype or floatx()\n-    dtype = to_torch_dtype(dtype)\n-    generator = torch_seed_generator(seed)\n-    return (maxval - minval) * torch.rand(\n-        *shape, generator=generator, dtype=dtype\n-    ) + minval\n+    raise NotImplementedError\n \n \n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n-    \"\"\"Produce random number based on the truncated normal distribution.\n-\n-    Args:\n-        shape: The shape of the random values to generate.\n-        mean: Floats, defaults to 0. Mean of the random values to generate.\n-        stddev: Floats, defaults to 1. Standard deviation of the random values\n-            to generate.\n-        dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `keras.backend.floatx()` is used,\n-            which defaults to `float32` unless you configured it otherwise (via\n-            `keras.backend.set_floatx(float_dtype)`)\n-        seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n-    \"\"\"\n-    x = torch.empty(shape)\n-    # TODO: setting seed globally via `manual_seed` might create side effects.\n-    if seed is not None:\n-        seed_val, _ = draw_seed(seed)\n-        torch.manual_seed(int(seed_val))\n-    return torch.nn.init.trunc_normal_(\n-        x, mean=mean, std=stddev, a=-stddev * 2, b=stddev * 2\n-    )\n+    raise NotImplementedError\n \n \n def dropout(inputs, rate, noise_shape=None, seed=None):\n-    # TODO: setting seed globally via `manual_seed` might create side effects.\n-    if seed is not None:\n-        seed_val, _ = draw_seed(seed)\n-        torch.manual_seed(int(seed_val))\n-    return tnn.dropout(inputs, p=rate)\n+    raise NotImplementedError\n\n@@ -102,6 +102,7 @@ from keras_core.layers.reshaping.permute import Permute\n from keras_core.layers.reshaping.repeat_vector import RepeatVector\n from keras_core.layers.reshaping.reshape import Reshape\n from keras_core.layers.reshaping.up_sampling1d import UpSampling1D\n+from keras_core.layers.rnn.bidirectional import Bidirectional\n from keras_core.layers.rnn.gru import GRU\n from keras_core.layers.rnn.lstm import LSTM\n from keras_core.layers.rnn.rnn import RNN\n\n@@ -0,0 +1,316 @@\n+import copy\n+\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.core.wrapper import Wrapper\n+from keras_core.layers.layer import Layer\n+from keras_core.saving import serialization_lib\n+\n+\n+@keras_core_export(\"keras_core.layers.Bidirectional\")\n+class Bidirectional(Wrapper):\n+    \"\"\"Bidirectional wrapper for RNNs.\n+\n+    Args:\n+        layer: `keras_core.layers.RNN` instance, such as\n+            `keras_core.layers.LSTM` or `keras_core.layers.GRU`.\n+            It could also be a `keras_core.layers.Layer` instance\n+            that meets the following criteria:\n+            1. Be a sequence-processing layer (accepts 3D+ inputs).\n+            2. Have a `go_backwards`, `return_sequences` and `return_state`\n+            attribute (with the same semantics as for the `RNN` class).\n+            3. Have an `input_spec` attribute.\n+            4. Implement serialization via `get_config()` and `from_config()`.\n+            Note that the recommended way to create new RNN layers is to write a\n+            custom RNN cell and use it with `keras_core.layers.RNN`, instead of\n+            subclassing `keras_core.layers.Layer` directly.\n+            When `return_sequences` is `True`, the output of the masked\n+            timestep will be zero regardless of the layer's original\n+            `zero_output_for_mask` value.\n+        merge_mode: Mode by which outputs of the forward and backward RNNs\n+            will be combined. One of `{\"sum\", \"mul\", \"concat\", \"ave\", None}`.\n+            If `None`, the outputs will not be combined,\n+            they will be returned as a list. Defaults to `\"concat\"`.\n+        backward_layer: Optional `keras_core.layers.RNN`,\n+            or `keras_core.layers.Layer` instance to be used to handle\n+            backwards input processing.\n+            If `backward_layer` is not provided, the layer instance passed\n+            as the `layer` argument will be used to generate the backward layer\n+            automatically.\n+            Note that the provided `backward_layer` layer should have properties\n+            matching those of the `layer` argument, in particular\n+            it should have the same values for `stateful`, `return_states`,\n+            `return_sequences`, etc. In addition, `backward_layer`\n+            and `layer` should have different `go_backwards` argument values.\n+            A `ValueError` will be raised if these requirements are not met.\n+\n+    Call arguments:\n+        The call arguments for this layer are the same as those of the\n+        wrapped RNN layer. Beware that when passing the `initial_state`\n+        argument during the call of this layer, the first half in the\n+        list of elements in the `initial_state` list will be passed to\n+        the forward RNN call and the last half in the list of elements\n+        will be passed to the backward RNN call.\n+\n+    Note: instantiating a `Bidirectional` layer from an existing RNN layer\n+    instance will not reuse the weights state of the RNN layer instance -- the\n+    `Bidirectional` layer will have freshly initialized weights.\n+\n+    Examples:\n+\n+    ```python\n+    model = Sequential([\n+        Input(shape=(5, 10)),\n+        Bidirectional(LSTM(10, return_sequences=True),\n+        Bidirectional(LSTM(10)),\n+        Dense(5, activation=\"softmax\"),\n+    ])\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    # With custom backward layer\n+    forward_layer = LSTM(10, return_sequences=True)\n+    backward_layer = LSTM(10, activation='relu', return_sequences=True,\n+                          go_backwards=True)\n+    model = Sequential([\n+        Input(shape=(5, 10)),\n+        Bidirectional(forward_layer, backward_layer=backward_layer),\n+        Dense(5, activation=\"softmax\"),\n+    ])\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        layer,\n+        merge_mode=\"concat\",\n+        weights=None,\n+        backward_layer=None,\n+        **kwargs,\n+    ):\n+        if not isinstance(layer, Layer):\n+            raise ValueError(\n+                \"Please initialize `Bidirectional` layer with a \"\n+                f\"`keras_core.layers.Layer` instance. Received: {layer}\"\n+            )\n+        if backward_layer is not None and not isinstance(backward_layer, Layer):\n+            raise ValueError(\n+                \"`backward_layer` need to be a `keras_core.layers.Layer` \"\n+                f\"instance. Received: {backward_layer}\"\n+            )\n+        if merge_mode not in [\"sum\", \"mul\", \"ave\", \"concat\", None]:\n+            raise ValueError(\n+                f\"Invalid merge mode. Received: {merge_mode}. \"\n+                \"Merge mode should be one of \"\n+                '{\"sum\", \"mul\", \"ave\", \"concat\", None}'\n+            )\n+        super().__init__(layer, **kwargs)\n+\n+        # Recreate the forward layer from the original layer config, so that it\n+        # will not carry over any state from the layer.\n+        config = serialization_lib.serialize_keras_object(layer)\n+        config[\"config\"][\"name\"] = \"forward_\" + layer.name.removeprefix(\n+            \"forward_\"\n+        )\n+        self.forward_layer = serialization_lib.deserialize_keras_object(config)\n+\n+        if backward_layer is None:\n+            config = serialization_lib.serialize_keras_object(layer)\n+            config[\"config\"][\"go_backwards\"] = True\n+            config[\"config\"][\"name\"] = \"backward_\" + layer.name.removeprefix(\n+                \"backward_\"\n+            )\n+            self.backward_layer = serialization_lib.deserialize_keras_object(\n+                config\n+            )\n+        else:\n+            self.backward_layer = backward_layer\n+        self._verify_layer_config()\n+\n+        def force_zero_output_for_mask(layer):\n+            # Force the zero_output_for_mask to be True if returning sequences.\n+            if getattr(layer, \"zero_output_for_mask\", None) is not None:\n+                layer.zero_output_for_mask = layer.return_sequences\n+\n+        force_zero_output_for_mask(self.forward_layer)\n+        force_zero_output_for_mask(self.backward_layer)\n+\n+        self.merge_mode = merge_mode\n+        if weights:\n+            nw = len(weights)\n+            self.forward_layer.initial_weights = weights[: nw // 2]\n+            self.backward_layer.initial_weights = weights[nw // 2 :]\n+        self.stateful = layer.stateful\n+        self.return_sequences = layer.return_sequences\n+        self.return_state = layer.return_state\n+        self.supports_masking = True\n+        self.input_spec = layer.input_spec\n+\n+    def _verify_layer_config(self):\n+        \"\"\"Ensure the forward and backward layers have valid common property.\"\"\"\n+        if self.forward_layer.go_backwards == self.backward_layer.go_backwards:\n+            raise ValueError(\n+                \"Forward layer and backward layer should have different \"\n+                \"`go_backwards` value. Received: \"\n+                \"forward_layer.go_backwards \"\n+                f\"{self.forward_layer.go_backwards}, \"\n+                \"backward_layer.go_backwards=\"\n+                f\"{self.backward_layer.go_backwards}\"\n+            )\n+\n+        common_attributes = (\"stateful\", \"return_sequences\", \"return_state\")\n+        for a in common_attributes:\n+            forward_value = getattr(self.forward_layer, a)\n+            backward_value = getattr(self.backward_layer, a)\n+            if forward_value != backward_value:\n+                raise ValueError(\n+                    \"Forward layer and backward layer are expected to have \"\n+                    f'the same value for attribute \"{a}\", got '\n+                    f'\"{forward_value}\" for forward layer and '\n+                    f'\"{backward_value}\" for backward layer'\n+                )\n+\n+    def compute_output_shape(self, sequence_shape, initial_state_shape=None):\n+        output_shape = self.forward_layer.compute_output_shape(sequence_shape)\n+\n+        if self.return_state:\n+            output_shape, state_shape = output_shape[0], output_shape[1:]\n+\n+        if self.merge_mode == \"concat\":\n+            output_shape = list(output_shape)\n+            output_shape[-1] *= 2\n+            output_shape = tuple(output_shape)\n+        elif self.merge_mode is None:\n+            output_shape = [output_shape, copy.copy(output_shape)]\n+\n+        if self.return_state:\n+            if self.merge_mode is None:\n+                return output_shape + state_shape + copy.copy(state_shape)\n+            return [output_shape] + state_shape + copy.copy(state_shape)\n+        return output_shape\n+\n+    def call(\n+        self,\n+        sequence,\n+        initial_state=None,\n+        mask=None,\n+        training=None,\n+    ):\n+        kwargs = {}\n+        if self.forward_layer._call_has_training_arg():\n+            kwargs[\"training\"] = training\n+        if self.forward_layer._call_has_mask_arg():\n+            kwargs[\"mask\"] = mask\n+\n+        if initial_state is not None:\n+            # initial_states are not keras tensors, eg eager tensor from np\n+            # array.  They are only passed in from kwarg initial_state, and\n+            # should be passed to forward/backward layer via kwarg\n+            # initial_state as well.\n+            forward_inputs, backward_inputs = sequence, sequence\n+            half = len(initial_state) // 2\n+            forward_state = initial_state[:half]\n+            backward_state = initial_state[half:]\n+        else:\n+            forward_inputs, backward_inputs = sequence, sequence\n+            forward_state, backward_state = None, None\n+\n+        y = self.forward_layer(\n+            forward_inputs, initial_state=forward_state, **kwargs\n+        )\n+        y_rev = self.backward_layer(\n+            backward_inputs, initial_state=backward_state, **kwargs\n+        )\n+\n+        if self.return_state:\n+            states = y[1:] + y_rev[1:]\n+            y = y[0]\n+            y_rev = y_rev[0]\n+\n+        if self.return_sequences:\n+            y_rev = ops.flip(y_rev, axis=1)\n+        if self.merge_mode == \"concat\":\n+            output = ops.concatenate([y, y_rev], axis=-1)\n+        elif self.merge_mode == \"sum\":\n+            output = y + y_rev\n+        elif self.merge_mode == \"ave\":\n+            output = (y + y_rev) / 2\n+        elif self.merge_mode == \"mul\":\n+            output = y * y_rev\n+        elif self.merge_mode is None:\n+            output = [y, y_rev]\n+        else:\n+            raise ValueError(\n+                \"Unrecognized value for `merge_mode`. \"\n+                f\"Received: {self.merge_mode}\"\n+                'Expected one of {\"concat\", \"sum\", \"ave\", \"mul\"}.'\n+            )\n+        if self.return_state:\n+            if self.merge_mode is None:\n+                return output + states\n+            return [output] + states\n+        return output\n+\n+    def reset_states(self):\n+        # Compatibility alias.\n+        self.reset_state()\n+\n+    def reset_state(self):\n+        if not self.stateful:\n+            raise AttributeError(\"Layer must be stateful.\")\n+        self.forward_layer.reset_state()\n+        self.backward_layer.reset_state()\n+\n+    def build(self, sequence_shape, initial_state_shape=None):\n+        self.forward_layer.build(sequence_shape)\n+        self.backward_layer.build(sequence_shape)\n+        self.built = True\n+\n+    def compute_mask(self, sequence, mask):\n+        if isinstance(mask, list):\n+            mask = mask[0]\n+        if self.return_sequences:\n+            if not self.merge_mode:\n+                output_mask = [mask, mask]\n+            else:\n+                output_mask = mask\n+        else:\n+            output_mask = [None, None] if not self.merge_mode else None\n+\n+        if self.return_state:\n+            states = self.forward_layer.states\n+            state_mask = [None for _ in states]\n+            if isinstance(output_mask, list):\n+                return output_mask + state_mask * 2\n+            return [output_mask] + state_mask * 2\n+        return output_mask\n+\n+    def get_config(self):\n+        config = {\"merge_mode\": self.merge_mode}\n+        config[\"layer\"] = serialization_lib.serialize_keras_object(\n+            self.forward_layer\n+        )\n+        config[\"backward_layer\"] = serialization_lib.serialize_keras_object(\n+            self.backward_layer\n+        )\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n+\n+    @classmethod\n+    def from_config(cls, config, custom_objects=None):\n+        # Instead of updating the input, create a copy and use that.\n+        config = copy.deepcopy(config)\n+\n+        config[\"layer\"] = serialization_lib.deserialize_keras_object(\n+            config[\"layer\"], custom_objects=custom_objects\n+        )\n+        # Handle (optional) backward layer instantiation.\n+        backward_layer_config = config.pop(\"backward_layer\", None)\n+        if backward_layer_config is not None:\n+            backward_layer = serialization_lib.deserialize_keras_object(\n+                backward_layer_config, custom_objects=custom_objects\n+            )\n+            config[\"backward_layer\"] = backward_layer\n+        # Instantiate the wrapper, adjust it and return it.\n+        layer = cls(**config)\n+        return layer\n\n@@ -0,0 +1,197 @@\n+import numpy as np\n+\n+from keras_core import initializers\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class SimpleRNNTest(testing.TestCase):\n+    def test_basics(self):\n+        self.run_layer_test(\n+            layers.Bidirectional,\n+            init_kwargs={\"layer\": layers.SimpleRNN(4)},\n+            input_shape=(3, 2, 4),\n+            expected_output_shape=(3, 8),\n+            expected_num_trainable_weights=6,\n+            expected_num_non_trainable_weights=0,\n+            supports_masking=True,\n+        )\n+        self.run_layer_test(\n+            layers.Bidirectional,\n+            init_kwargs={\n+                \"layer\": layers.SimpleRNN(4),\n+                \"backward_layer\": layers.SimpleRNN(4, go_backwards=True),\n+                \"merge_mode\": \"sum\",\n+            },\n+            input_shape=(3, 2, 4),\n+            expected_output_shape=(3, 4),\n+            expected_num_trainable_weights=6,\n+            expected_num_non_trainable_weights=0,\n+            supports_masking=True,\n+        )\n+\n+    def test_correctness(self):\n+        sequence = np.arange(24).reshape((2, 3, 4)).astype(\"float32\")\n+        forward_layer = layers.SimpleRNN(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+        )\n+        layer = layers.Bidirectional(\n+            layer=forward_layer,\n+        )\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.39687276, 0.39687276, 0.10004295, 0.10004295],\n+                    [0.7237238, 0.7237238, 0.53391594, 0.53391594],\n+                ]\n+            ),\n+            output,\n+        )\n+\n+        layer = layers.Bidirectional(layer=forward_layer, merge_mode=\"ave\")\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array([[0.24845785, 0.24845785], [0.6288199, 0.6288199]]),\n+            output,\n+        )\n+\n+        layer = layers.Bidirectional(layer=forward_layer, merge_mode=None)\n+        output1, output2 = layer(sequence)\n+        self.assertAllClose(\n+            np.array([[0.39687276, 0.39687276], [0.7237238, 0.7237238]]),\n+            output1,\n+        )\n+        self.assertAllClose(\n+            np.array([[0.10004295, 0.10004295], [0.53391594, 0.53391594]]),\n+            output2,\n+        )\n+\n+        backward_layer = layers.SimpleRNN(\n+            2,\n+            kernel_initializer=initializers.Constant(0.03),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.01),\n+            go_backwards=True,\n+        )\n+        layer = layers.Bidirectional(\n+            layer=forward_layer, backward_layer=backward_layer, merge_mode=\"mul\"\n+        )\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array([[0.08374989, 0.08374989], [0.6740834, 0.6740834]]),\n+            output,\n+        )\n+\n+        forward_layer = layers.GRU(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            return_sequences=True,\n+        )\n+        layer = layers.Bidirectional(layer=forward_layer, merge_mode=\"sum\")\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [\n+                        [0.20937867, 0.20937867],\n+                        [0.34462988, 0.34462988],\n+                        [0.40290534, 0.40290534],\n+                    ],\n+                    [\n+                        [0.59829646, 0.59829646],\n+                        [0.6734641, 0.6734641],\n+                        [0.6479671, 0.6479671],\n+                    ],\n+                ]\n+            ),\n+            output,\n+        )\n+\n+    def test_statefulness(self):\n+        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n+        forward_layer = layers.LSTM(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            stateful=True,\n+        )\n+        layer = layers.Bidirectional(layer=forward_layer)\n+        layer(sequence)\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.26234663, 0.26234663, 0.16959146, 0.16959146],\n+                    [0.6137073, 0.6137073, 0.5381646, 0.5381646],\n+                ]\n+            ),\n+            output,\n+        )\n+        layer.reset_state()\n+        layer(sequence)\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.26234663, 0.26234663, 0.16959146, 0.16959146],\n+                    [0.6137073, 0.6137073, 0.5381646, 0.5381646],\n+                ]\n+            ),\n+            output,\n+        )\n+\n+    def test_pass_initial_state(self):\n+        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n+        initial_state = [\n+            np.arange(4).reshape((2, 2)).astype(\"float32\") * 1,\n+            np.arange(4).reshape((2, 2)).astype(\"float32\") * 2,\n+            np.arange(4).reshape((2, 2)).astype(\"float32\") * 3,\n+            np.arange(4).reshape((2, 2)).astype(\"float32\") * 4,\n+        ]\n+        forward_layer = layers.LSTM(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+        )\n+        layer = layers.Bidirectional(\n+            layer=forward_layer,\n+        )\n+        output = layer(sequence, initial_state=initial_state)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.20794602, 0.4577124, 0.14046375, 0.48191673],\n+                    [0.6682636, 0.6711909, 0.60943645, 0.60950446],\n+                ]\n+            ),\n+            output,\n+        )\n+\n+    def test_masking(self):\n+        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n+        forward_layer = layers.GRU(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+        )\n+        layer = layers.Bidirectional(layer=forward_layer)\n+        mask = np.array([[True, True, False, True], [True, False, False, True]])\n+        output = layer(sequence, mask=mask)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.19393763, 0.19393763, 0.11669192, 0.11669192],\n+                    [0.30818558, 0.30818558, 0.28380975, 0.28380975],\n+                ]\n+            ),\n+            output,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cc053ac3099bb2b5f51001671cbfee41564fd14d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 997 | Lines Deleted: 60 | Files Changed: 8 | Hunks: 33 | Methods Changed: 41 | Complexity Δ (Sum/Max): 78/56 | Churn Δ: 1057 | Churn Cumulative: 4051 | Contributors (this commit): 15 | Commits (past 90d): 211 | Contributors (cumulative): 42 | DMM Complexity: 0.8783068783068783\n\nDIFF:\n@@ -1,6 +1,7 @@\n from keras_core.layers.activations.activation import Activation\n from keras_core.layers.attention.additive_attention import AdditiveAttention\n from keras_core.layers.attention.attention import Attention\n+from keras_core.layers.attention.multi_head_attention import MultiHeadAttention\n from keras_core.layers.convolutional.conv1d import Conv1D\n from keras_core.layers.convolutional.conv1d_transpose import Conv1DTranspose\n from keras_core.layers.convolutional.conv2d import Conv2D\n\n@@ -0,0 +1,649 @@\n+import collections\n+import math\n+import string\n+\n+import numpy as np\n+\n+from keras_core import constraints\n+from keras_core import initializers\n+from keras_core import operations as ops\n+from keras_core import regularizers\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.activations.softmax import Softmax\n+from keras_core.layers.core.einsum_dense import EinsumDense\n+from keras_core.layers.layer import Layer\n+from keras_core.layers.regularization.dropout import Dropout\n+\n+\n+@keras_core_export(\"keras_core.layers.Attention\")\n+class MultiHeadAttention(Layer):\n+    \"\"\"MultiHeadAttention layer.\n+\n+    This is an implementation of multi-headed attention as described in the\n+    paper \"Attention is all you Need\"\n+    [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762).\n+    If `query`, `key,` `value` are the same, then\n+    this is self-attention. Each timestep in `query` attends to the\n+    corresponding sequence in `key`, and returns a fixed-width vector.\n+\n+    This layer first projects `query`, `key` and `value`. These are\n+    (effectively) a list of tensors of length `num_attention_heads`, where the\n+    corresponding shapes are `(batch_size, <query dimensions>, key_dim)`,\n+    `(batch_size, <key/value dimensions>, key_dim)`,\n+    `(batch_size, <key/value dimensions>, value_dim)`.\n+\n+    Then, the query and key tensors are dot-producted and scaled. These are\n+    softmaxed to obtain attention probabilities. The value tensors are then\n+    interpolated by these probabilities, then concatenated back to a single\n+    tensor.\n+\n+    Finally, the result tensor with the last dimension as `value_dim` can take\n+    a linear projection and return.\n+\n+    Args:\n+        num_heads: Number of attention heads.\n+        key_dim: Size of each attention head for query and key.\n+        value_dim: Size of each attention head for value.\n+        dropout: Dropout probability.\n+        use_bias: Boolean, whether the dense layers use bias vectors/matrices.\n+        output_shape: The expected shape of an output tensor, besides the batch\n+            and sequence dims. If not specified, projects back to the query\n+            feature dim (the query input's last dimension).\n+        attention_axes: axes over which the attention is applied. `None` means\n+            attention over all axes, but batch, heads, and features.\n+        kernel_initializer: Initializer for dense layer kernels.\n+        bias_initializer: Initializer for dense layer biases.\n+        kernel_regularizer: Regularizer for dense layer kernels.\n+        bias_regularizer: Regularizer for dense layer biases.\n+        activity_regularizer: Regularizer for dense layer activity.\n+        kernel_constraint: Constraint for dense layer kernels.\n+        bias_constraint: Constraint for dense layer kernels.\n+\n+    Call arguments:\n+        query: Query tensor of shape `(B, T, dim)`, where `B` is the batch size,\n+            `T` is the target sequence length, and dim is the feature dimension.\n+        value: Value tensor of shape `(B, S, dim)`, where `B` is the batch size,\n+            `S` is the source sequence length, and dim is the feature dimension.\n+        key: Optional key tensor of shape `(B, S, dim)`. If not given, will\n+            use `value` for both `key` and `value`, which is the most common\n+            case.\n+        attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n+            attention to certain positions. The boolean mask specifies which\n+            query elements can attend to which key elements, 1 indicates\n+            attention and 0 indicates no attention. Broadcasting can happen for\n+            the missing batch dimensions and the head dimension.\n+        return_attention_scores: A boolean to indicate whether the output should\n+            be `(attention_output, attention_scores)` if `True`, or\n+            `attention_output` if `False`. Defaults to `False`.\n+        training: Python boolean indicating whether the layer should behave in\n+            training mode (adding dropout) or in inference mode (no dropout).\n+            Will go with either using the training mode of the parent\n+            layer/model, or `False` (inference) if there is no parent layer.\n+        use_causal_mask: A boolean to indicate whether to apply a causal mask to\n+            prevent tokens from attending to future tokens (e.g., used in a\n+            decoder Transformer).\n+\n+    Returns:\n+        attention_output: The result of the computation, of shape `(B, T, E)`,\n+            where `T` is for target sequence shapes and `E` is the query input\n+            last dimension if `output_shape` is `None`. Otherwise, the\n+            multi-head outputs are projected to the shape specified by\n+            `output_shape`.\n+        attention_scores: (Optional) multi-head attention coefficients over\n+            attention axes.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        num_heads,\n+        key_dim,\n+        value_dim=None,\n+        dropout=0.0,\n+        use_bias=True,\n+        output_shape=None,\n+        attention_axes=None,\n+        kernel_initializer=\"glorot_uniform\",\n+        bias_initializer=\"zeros\",\n+        kernel_regularizer=None,\n+        bias_regularizer=None,\n+        activity_regularizer=None,\n+        kernel_constraint=None,\n+        bias_constraint=None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.supports_masking = True\n+        self._num_heads = num_heads\n+        self._key_dim = key_dim\n+        self._value_dim = value_dim if value_dim else key_dim\n+        self._dropout = dropout\n+        self._use_bias = use_bias\n+        self._output_shape = output_shape\n+        self._kernel_initializer = initializers.get(kernel_initializer)\n+        self._bias_initializer = initializers.get(bias_initializer)\n+        self._kernel_regularizer = regularizers.get(kernel_regularizer)\n+        self._bias_regularizer = regularizers.get(bias_regularizer)\n+        self._activity_regularizer = regularizers.get(activity_regularizer)\n+        self._kernel_constraint = constraints.get(kernel_constraint)\n+        self._bias_constraint = constraints.get(bias_constraint)\n+        if isinstance(attention_axes, int):\n+            attention_axes = (attention_axes,)\n+        elif attention_axes and not isinstance(attention_axes, (list, tuple)):\n+            raise ValueError(\n+                \"`attention_axes` must be an int, list, or tuple.\"\n+                f\"Received: attention_axes={attention_axes}\"\n+            )\n+        self._attention_axes = attention_axes\n+        self._built_from_signature = False\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        config = {\n+            \"num_heads\": self._num_heads,\n+            \"key_dim\": self._key_dim,\n+            \"value_dim\": self._value_dim,\n+            \"dropout\": self._dropout,\n+            \"use_bias\": self._use_bias,\n+            \"output_shape\": self._output_shape,\n+            \"attention_axes\": self._attention_axes,\n+            \"kernel_initializer\": initializers.serialize(\n+                self._kernel_initializer\n+            ),\n+            \"bias_initializer\": initializers.serialize(self._bias_initializer),\n+            \"kernel_regularizer\": regularizers.serialize(\n+                self._kernel_regularizer\n+            ),\n+            \"bias_regularizer\": regularizers.serialize(self._bias_regularizer),\n+            \"activity_regularizer\": regularizers.serialize(\n+                self._activity_regularizer\n+            ),\n+            \"kernel_constraint\": constraints.serialize(self._kernel_constraint),\n+            \"bias_constraint\": constraints.serialize(self._bias_constraint),\n+        }\n+        return {**base_config, **config}\n+\n+    def build(\n+        self,\n+        query_shape,\n+        value_shape,\n+        key_shape=None,\n+    ):\n+        \"\"\"Builds layers and variables.\n+\n+        Args:\n+            query_shape: Shape of the `query` tensor.\n+            value_shape: Shape of the `value` tensor.\n+            key: Optional shape of the `key` tensor.\n+        \"\"\"\n+        key_shape = value_shape if key_shape is None else key_shape\n+        query_rank = len(query_shape)\n+        value_rank = len(value_shape)\n+        key_rank = len(key_shape)\n+        einsum_equation, bias_axes, output_rank = _build_proj_equation(\n+            query_rank - 1, bound_dims=1, output_dims=2\n+        )\n+        self._query_dense = EinsumDense(\n+            einsum_equation,\n+            output_shape=_get_output_shape(\n+                output_rank - 1, [self._num_heads, self._key_dim]\n+            ),\n+            bias_axes=bias_axes if self._use_bias else None,\n+            name=\"query\",\n+            **self._get_common_kwargs_for_sublayer(),\n+        )\n+        self._query_dense.build(query_shape)\n+        einsum_equation, bias_axes, output_rank = _build_proj_equation(\n+            key_rank - 1, bound_dims=1, output_dims=2\n+        )\n+        self._key_dense = EinsumDense(\n+            einsum_equation,\n+            output_shape=_get_output_shape(\n+                output_rank - 1, [self._num_heads, self._key_dim]\n+            ),\n+            bias_axes=bias_axes if self._use_bias else None,\n+            name=\"key\",\n+            **self._get_common_kwargs_for_sublayer(),\n+        )\n+        self._key_dense.build(key_shape)\n+        einsum_equation, bias_axes, output_rank = _build_proj_equation(\n+            value_rank - 1, bound_dims=1, output_dims=2\n+        )\n+        self._value_dense = EinsumDense(\n+            einsum_equation,\n+            output_shape=_get_output_shape(\n+                output_rank - 1, [self._num_heads, self._value_dim]\n+            ),\n+            bias_axes=bias_axes if self._use_bias else None,\n+            name=\"value\",\n+            **self._get_common_kwargs_for_sublayer(),\n+        )\n+        self._value_dense.build(value_shape)\n+\n+        # Builds the attention computations for multi-head dot product\n+        # attention.  These computations could be wrapped into the keras\n+        # attention layer once it supports multi-head einsum computations.\n+        self._build_attention(output_rank)\n+        self._output_dense = self._make_output_dense(\n+            query_shape,\n+            self._get_common_kwargs_for_sublayer(),\n+            \"attention_output\",\n+        )\n+        output_dense_input_shape = list(\n+            self._query_dense.compute_output_shape(query_shape)\n+        )\n+        output_dense_input_shape[-1] = self._value_dim\n+        self._output_dense.build(tuple(output_dense_input_shape))\n+        self.built = True\n+\n+    def _get_common_kwargs_for_sublayer(self):\n+        common_kwargs = dict(\n+            kernel_regularizer=self._kernel_regularizer,\n+            bias_regularizer=self._bias_regularizer,\n+            activity_regularizer=self._activity_regularizer,\n+            kernel_constraint=self._kernel_constraint,\n+            bias_constraint=self._bias_constraint,\n+        )\n+        # Create new clone of kernel/bias initializer, so that we don't reuse\n+        # the initializer instance, which could lead to same init value since\n+        # initializer is stateless.\n+        kernel_initializer = self._kernel_initializer.__class__.from_config(\n+            self._kernel_initializer.get_config()\n+        )\n+        bias_initializer = self._bias_initializer.__class__.from_config(\n+            self._bias_initializer.get_config()\n+        )\n+        common_kwargs[\"kernel_initializer\"] = kernel_initializer\n+        common_kwargs[\"bias_initializer\"] = bias_initializer\n+        return common_kwargs\n+\n+    def _make_output_dense(self, query_shape, common_kwargs, name=None):\n+        \"\"\"Builds the output projection matrix.\n+\n+        Args:\n+            free_dims: Number of free dimensions for einsum equation building.\n+            common_kwargs: Common keyword arguments for einsum layer.\n+            name: Name for the projection layer.\n+\n+        Returns:\n+            Projection layer.\n+        \"\"\"\n+        query_rank = len(query_shape)\n+        if self._output_shape:\n+            if not isinstance(self._output_shape, collections.abc.Sized):\n+                output_shape = [self._output_shape]\n+            else:\n+                output_shape = self._output_shape\n+        else:\n+            output_shape = [query_shape[-1]]\n+        einsum_equation, bias_axes, output_rank = _build_proj_equation(\n+            query_rank - 1, bound_dims=2, output_dims=len(output_shape)\n+        )\n+        return EinsumDense(\n+            einsum_equation,\n+            output_shape=_get_output_shape(output_rank - 1, output_shape),\n+            bias_axes=bias_axes if self._use_bias else None,\n+            name=name,\n+            **common_kwargs,\n+        )\n+\n+    def _build_attention(self, rank):\n+        \"\"\"Builds multi-head dot-product attention computations.\n+\n+        This function builds attributes necessary for `_compute_attention` to\n+        customize attention computation to replace the default dot-product\n+        attention.\n+\n+        Args:\n+            rank: the rank of query, key, value tensors.\n+        \"\"\"\n+        if self._attention_axes is None:\n+            self._attention_axes = tuple(range(1, rank - 2))\n+        else:\n+            self._attention_axes = tuple(self._attention_axes)\n+        (\n+            self._dot_product_equation,\n+            self._combine_equation,\n+            attn_scores_rank,\n+        ) = _build_attention_equation(rank, attn_axes=self._attention_axes)\n+        norm_axes = tuple(\n+            range(\n+                attn_scores_rank - len(self._attention_axes), attn_scores_rank\n+            )\n+        )\n+        self._softmax = Softmax(axis=norm_axes)\n+        self._dropout_layer = Dropout(rate=self._dropout)\n+\n+    def _masked_softmax(self, attention_scores, attention_mask=None):\n+        # Normalize the attention scores to probabilities.\n+        # attention_scores = [B, N, T, S]\n+        if attention_mask is not None:\n+            # The expand dim happens starting from the `num_heads` dimension,\n+            # (<batch_dims>, num_heads, <query_attention_dims,\n+            # key_attention_dims>)\n+            mask_expansion_axis = -len(self._attention_axes) * 2 - 1\n+            for _ in range(\n+                len(attention_scores.shape) - len(attention_mask.shape)\n+            ):\n+                attention_mask = ops.expand_dims(\n+                    attention_mask, axis=mask_expansion_axis\n+                )\n+        return self._softmax(attention_scores, mask=attention_mask)\n+\n+    def _compute_attention(\n+        self, query, key, value, attention_mask=None, training=None\n+    ):\n+        \"\"\"Applies Dot-product attention with query, key, value tensors.\n+\n+        This function defines the computation inside `call` with projected\n+        multi-head Q, K, V inputs. Users can override this function for\n+        customized attention implementation.\n+\n+        Args:\n+            query: Projected query tensor of shape `(B, T, N, key_dim)`.\n+            key: Projected key tensor of shape `(B, S, N, key_dim)`.\n+            value: Projected value tensor of shape `(B, S, N, value_dim)`.\n+            attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n+                attention to certain positions. It is generally not needed if\n+                the `query` and `value` (and/or `key`) are masked.\n+            training: Python boolean indicating whether the layer should behave\n+                in training mode (adding dropout) or in inference mode (doing\n+                nothing).\n+\n+        Returns:\n+          attention_output: Multi-headed outputs of attention computation.\n+          attention_scores: Multi-headed attention weights.\n+        \"\"\"\n+        # Note: Applying scalar multiply at the smaller end of einsum improves\n+        # XLA performance, but may introduce slight numeric differences in\n+        # the Transformer attention head.\n+        query = ops.multiply(query, 1.0 / math.sqrt(float(self._key_dim)))\n+\n+        # Take the dot product between \"query\" and \"key\" to get the raw\n+        # attention scores.\n+        attention_scores = ops.einsum(self._dot_product_equation, key, query)\n+\n+        attention_scores = self._masked_softmax(\n+            attention_scores, attention_mask\n+        )\n+\n+        # This is actually dropping out entire tokens to attend to, which might\n+        # seem a bit unusual, but is taken from the original Transformer paper.\n+        attention_scores_dropout = self._dropout_layer(\n+            attention_scores, training=training\n+        )\n+\n+        # `context_layer` = [B, T, N, H]\n+        attention_output = ops.einsum(\n+            self._combine_equation, attention_scores_dropout, value\n+        )\n+        return attention_output, attention_scores\n+\n+    def call(\n+        self,\n+        query,\n+        value,\n+        key=None,\n+        query_mask=None,\n+        value_mask=None,\n+        key_mask=None,\n+        attention_mask=None,\n+        return_attention_scores=False,\n+        training=None,\n+        use_causal_mask=False,\n+    ):\n+        if key is None:\n+            key = value\n+\n+        attention_mask = self._compute_attention_mask(\n+            query,\n+            value,\n+            query_mask=query_mask,\n+            value_mask=value_mask,\n+            key_mask=key_mask,\n+            attention_mask=attention_mask,\n+            use_causal_mask=use_causal_mask,\n+        )\n+\n+        #   N = `num_attention_heads`\n+        #   H = `size_per_head`\n+        # `query` = [B, T, N ,H]\n+        query = self._query_dense(query)\n+\n+        # `key` = [B, S, N, H]\n+        key = self._key_dense(key)\n+\n+        # `value` = [B, S, N, H]\n+        value = self._value_dense(value)\n+\n+        attention_output, attention_scores = self._compute_attention(\n+            query, key, value, attention_mask, training\n+        )\n+        attention_output = self._output_dense(attention_output)\n+\n+        if return_attention_scores:\n+            return attention_output, attention_scores\n+        return attention_output\n+\n+    def _compute_attention_mask(\n+        self,\n+        query,\n+        value,\n+        query_mask=None,\n+        value_mask=None,\n+        key_mask=None,\n+        attention_mask=None,\n+        use_causal_mask=False,\n+    ):\n+        \"\"\"Computes the attention mask, using the Keras masks of the inputs.\n+\n+        * The `query`'s mask is reshaped from [B, T] to [B, T, 1].\n+        * The `value`'s mask is reshaped from [B, S] to [B, 1, S].\n+        * The `key`'s mask is reshaped from [B, S] to [B, 1, S]. The `key`'s\n+          mask is ignored if `key` is `None` or if `key is value`.\n+        * If `use_causal_mask=True`, then the causal mask is computed. Its shape\n+          is [1, T, S].\n+\n+        All defined masks are merged using a logical AND operation (`&`).\n+\n+        In general, if the `query` and `value` are masked, then there is no need\n+        to define the `attention_mask`.\n+\n+        Args:\n+            query: Projected query tensor of shape `(B, T, N, key_dim)`.\n+            key: Projected key tensor of shape `(B, T, N, key_dim)`.\n+            value: Projected value tensor of shape `(B, T, N, value_dim)`.\n+            attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n+                attention to certain positions.\n+            use_causal_mask: A boolean to indicate whether to apply a causal\n+                mask to prevent tokens from attending to future tokens (e.g.,\n+                used in a decoder Transformer).\n+\n+        Returns:\n+            attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n+                attention to certain positions, based on the Keras masks of the\n+                `query`, `key`, `value`, and `attention_mask` tensors, and the\n+                causal mask if `use_causal_mask=True`.\n+        \"\"\"\n+        auto_mask = None\n+        if query_mask is not None:\n+            query_mask = ops.cast(query_mask, \"bool\")  # defensive casting\n+            # B = batch size, T = max query length\n+            auto_mask = ops.expand_dims(query_mask, -1)  # shape is [B, T, 1]\n+        if value_mask is not None:\n+            value_mask = ops.cast(value_mask, \"bool\")  # defensive casting\n+            # B = batch size, S == max value length\n+            mask = ops.expand_dims(value_mask, -2)  # shape is [B, 1, S]\n+            auto_mask = mask if auto_mask is None else auto_mask & mask\n+        if key_mask is not None:\n+            key_mask = ops.cast(key_mask, \"bool\")  # defensive casting\n+            # B == batch size, S == max key length == max value length\n+            mask = ops.expand_dims(key_mask, -2)  # shape is [B, 1, S]\n+            auto_mask = mask if auto_mask is None else auto_mask & mask\n+        if use_causal_mask:\n+            # the shape of the causal mask is [1, T, S]\n+            mask = self._compute_causal_mask(query, value)\n+            auto_mask = mask if auto_mask is None else auto_mask & mask\n+        if auto_mask is not None:\n+            # merge attention_mask & automatic mask, to shape [B, T, S]\n+            attention_mask = (\n+                auto_mask\n+                if attention_mask is None\n+                else ops.cast(attention_mask, bool) & auto_mask\n+            )\n+        return attention_mask\n+\n+    def _compute_causal_mask(self, query, value=None):\n+        \"\"\"Computes a causal mask (e.g., for masked self-attention layers).\n+\n+        For example, if query and value both contain sequences of length 4,\n+        this function returns a boolean tensor equal to:\n+\n+        ```\n+        [[[True,  False, False, False],\n+          [True,  True,  False, False],\n+          [True,  True,  True,  False],\n+          [True,  True,  True,  True]]]\n+        ```\n+\n+        Args:\n+            query: query tensor of shape `(B, T, ...)`.\n+            value: value tensor of shape `(B, S, ...)` (optional, defaults to\n+                query).\n+\n+        Returns:\n+            mask: a boolean tensor of shape `(1, T, S)` containing a lower\n+                triangular matrix of shape `(T, S)`.\n+        \"\"\"\n+        q_seq_length = ops.shape(query)[1]\n+        v_seq_length = q_seq_length if value is None else ops.shape(value)[1]\n+        ones_mask = ops.ones((1, q_seq_length, v_seq_length), dtype=\"int32\")\n+        row_index = ops.cumsum(ones_mask, axis=-2)\n+        col_index = ops.cumsum(ones_mask, axis=-1)\n+        return ops.greater_equal(row_index, col_index)\n+\n+    def compute_output_shape(\n+        self,\n+        query_shape,\n+        value_shape,\n+        key_shape=None,\n+    ):\n+        if key_shape is None:\n+            key_shape = value_shape\n+\n+        if query_shape[-1] != value_shape[-1]:\n+            raise ValueError(\n+                \"The last dimension of `query_shape` and `value_shape` \"\n+                f\"must be equal, but are {query_shape[-1]}, {value_shape[-1]}. \"\n+                \"Received: query_shape={query_shape}, value_shape={value_shape}\"\n+            )\n+\n+        if value_shape[1:-1] != key_shape[1:-1]:\n+            raise ValueError(\n+                \"All dimensions of `value` and `key`, except the last one, \"\n+                f\"must be equal. Received: value_shape={value_shape} and \"\n+                f\"key_shape={key_shape}\"\n+            )\n+\n+        if self._output_shape:\n+            return query_shape[:-1] + self._output_shape\n+\n+        return query_shape\n+\n+\n+def _index_to_einsum_variable(i):\n+    \"\"\"Coverts an index to a einsum variable name.\n+\n+    We simply map indices to lowercase characters, e.g. 0 -> 'a', 1 -> 'b'.\n+    \"\"\"\n+    return string.ascii_lowercase[i]\n+\n+\n+def _build_attention_equation(rank, attn_axes):\n+    \"\"\"Builds einsum equations for the attention computation.\n+\n+    Query, key, value inputs after projection are expected to have the shape as:\n+    `(bs, <non-attention dims>, <attention dims>, num_heads, channels)`.\n+    `bs` and `<non-attention dims>` are treated as `<batch dims>`.\n+\n+    The attention operations can be generalized:\n+    1. Query-key dot product:\n+        (<batch dims>, <query attention dims>, num_heads, channels),\n+        (<batch dims>, <key attention dims>, num_heads, channels) ->\n+        (<batch dims>, num_heads, <query attention dims>, <key attention dims>)\n+    2. Combination:\n+        (<batch dims>, num_heads, <query attention dims>, <key attention dims>),\n+        (<batch dims>, <value attention dims>, num_heads, channels) -> (<batch\n+        dims>, <query attention dims>, num_heads, channels)\n+\n+    Args:\n+        rank: Rank of query, key, value tensors.\n+        attn_axes: List/tuple of axes, `[-1, rank)`,\n+            that attention will be applied to.\n+\n+    Returns:\n+        Einsum equations.\n+    \"\"\"\n+    target_notation = \"\"\n+    for i in range(rank):\n+        target_notation += _index_to_einsum_variable(i)\n+    # `batch_dims` includes the head dim.\n+    batch_dims = tuple(np.delete(range(rank), attn_axes + (rank - 1,)))\n+    letter_offset = rank\n+    source_notation = \"\"\n+    for i in range(rank):\n+        if i in batch_dims or i == rank - 1:\n+            source_notation += target_notation[i]\n+        else:\n+            source_notation += _index_to_einsum_variable(letter_offset)\n+            letter_offset += 1\n+\n+    product_notation = \"\".join(\n+        [target_notation[i] for i in batch_dims]\n+        + [target_notation[i] for i in attn_axes]\n+        + [source_notation[i] for i in attn_axes]\n+    )\n+    dot_product_equation = \"%s,%s->%s\" % (\n+        source_notation,\n+        target_notation,\n+        product_notation,\n+    )\n+    attn_scores_rank = len(product_notation)\n+    combine_equation = \"%s,%s->%s\" % (\n+        product_notation,\n+        source_notation,\n+        target_notation,\n+    )\n+    return dot_product_equation, combine_equation, attn_scores_rank\n+\n+\n+def _build_proj_equation(free_dims, bound_dims, output_dims):\n+    \"\"\"Builds an einsum equation for projections inside multi-head attention.\"\"\"\n+    input_str = \"\"\n+    kernel_str = \"\"\n+    output_str = \"\"\n+    bias_axes = \"\"\n+    letter_offset = 0\n+    for i in range(free_dims):\n+        char = _index_to_einsum_variable(i + letter_offset)\n+        input_str += char\n+        output_str += char\n+\n+    letter_offset += free_dims\n+    for i in range(bound_dims):\n+        char = _index_to_einsum_variable(i + letter_offset)\n+        input_str += char\n+        kernel_str += char\n+\n+    letter_offset += bound_dims\n+    for i in range(output_dims):\n+        char = _index_to_einsum_variable(i + letter_offset)\n+        kernel_str += char\n+        output_str += char\n+        bias_axes += char\n+    equation = f\"{input_str},{kernel_str}->{output_str}\"\n+\n+    return equation, bias_axes, len(output_str)\n+\n+\n+def _get_output_shape(output_rank, known_last_dims):\n+    return [None] * (output_rank - len(known_last_dims)) + list(known_last_dims)\n\n@@ -0,0 +1,238 @@\n+import numpy as np\n+from absl.testing import parameterized\n+\n+from keras_core import initializers\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class MultiHeadAttentionTest(testing.TestCase, parameterized.TestCase):\n+    def test_basics(self):\n+        self.run_layer_test(\n+            layers.MultiHeadAttention,\n+            init_kwargs={\n+                \"num_heads\": 2,\n+                \"key_dim\": 2,\n+            },\n+            input_shape={\"query_shape\": (2, 8, 16), \"value_shape\": (2, 4, 16)},\n+            expected_output_shape=(2, 8, 16),\n+            expected_num_trainable_weights=8,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+        self.run_layer_test(\n+            layers.MultiHeadAttention,\n+            init_kwargs={\n+                \"num_heads\": 2,\n+                \"key_dim\": 2,\n+                \"value_dim\": 4,\n+                \"use_bias\": False,\n+                \"dropout\": 0.5,\n+            },\n+            input_shape={\"query_shape\": (2, 8, 16), \"value_shape\": (2, 4, 16)},\n+            expected_output_shape=(2, 8, 16),\n+            expected_num_trainable_weights=4,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    @parameterized.named_parameters(\n+        (\"4d_inputs_1freebatch_mask2\", (3, 4), (3, 2), (4, 2), (2,)),\n+        (\"4d_inputs_1freebatch_mask3\", (3, 4), (3, 2), (3, 4, 2), (2,)),\n+        (\"4d_inputs_1freebatch_mask4\", (3, 4), (3, 2), (3, 2, 4, 2), (2,)),\n+        (\"4d_inputs_2d_attention\", (3, 4), (3, 2), (3, 4, 3, 2), (1, 2)),\n+        (\"5d_inputs_2d_attention\", (5, 3, 4), (5, 3, 2), (3, 4, 3, 2), (2, 3)),\n+        (\n+            \"5d_inputs_2d_attention_fullmask\",\n+            (5, 3, 4),\n+            (5, 3, 2),\n+            (5, 3, 4, 3, 2),\n+            (2, 3),\n+        ),\n+    )\n+    def test_high_dim_attention(\n+        self, q_dims, v_dims, mask_dims, attention_axes\n+    ):\n+        batch_size, hidden_size = 3, 8\n+        query_shape = (batch_size,) + q_dims + (hidden_size,)\n+        value_shape = (batch_size,) + v_dims + (hidden_size,)\n+        self.run_layer_test(\n+            layers.MultiHeadAttention,\n+            init_kwargs={\n+                \"num_heads\": 2,\n+                \"key_dim\": 2,\n+                \"attention_axes\": attention_axes,\n+            },\n+            input_shape={\n+                \"query_shape\": query_shape,\n+                \"value_shape\": value_shape,\n+            },\n+            expected_output_shape=query_shape,\n+            expected_num_trainable_weights=8,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    @parameterized.named_parameters(\n+        (\"without_key_same_proj\", (4, 8), (2, 8), None, None),\n+        (\"with_key_same_proj\", (4, 8), (2, 8), (2, 3), None),\n+        (\"wihtout_key_different_proj\", (4, 8), (2, 8), None, (3, 4)),\n+        (\"with_key_different_proj\", (4, 8), (2, 8), (2, 3), (1, 5)),\n+        (\"high_dim_same_proj\", (4, 2, 3, 8), (1, 1, 5, 8), (1, 1, 5, 2), None),\n+        (\n+            \"high_dim_different_proj\",\n+            (4, 2, 3, 8),\n+            (1, 1, 5, 8),\n+            (1, 1, 5, 2),\n+            (3, 2),\n+        ),\n+    )\n+    def test_compute_output_shape(\n+        self, query_dims, value_dims, key_dims, output_shape\n+    ):\n+        \"\"\"Test computed shape is equal to the layer output's shape.\"\"\"\n+        layer = layers.MultiHeadAttention(\n+            num_heads=2,\n+            key_dim=2,\n+            value_dim=2,\n+            output_shape=output_shape,\n+        )\n+        batch_size = 7\n+        query_shape = (batch_size,) + query_dims\n+        value_shape = (batch_size,) + value_dims\n+        key_shape = (batch_size,) + key_dims if key_dims else None\n+\n+        query = np.ones(query_shape)\n+        value = np.ones(value_shape)\n+        key = np.ones(key_shape) if key_shape else None\n+        output = layer(query=query, value=value, key=key)\n+        comp_output_shape = layer.compute_output_shape(\n+            query_shape, value_shape, key_shape\n+        )\n+        self.assertEqual(output.shape, comp_output_shape)\n+\n+    @parameterized.named_parameters(\n+        (\"query_value_dim_mismatch\", (2, 4, 8), (2, 2, 7), 2),\n+        (\"key_value_dim_mismatch\", (2, 4, 8), (2, 2, 8), (2, 1, 7)),\n+        (\n+            \"key_value_dim_mismatch_high_dim\",\n+            (2, 4, 2, 3, 8),\n+            (2, 1, 1, 5, 8),\n+            (2, 1, 15, 5, 2),\n+        ),\n+    )\n+    def test_shape_mismatch_error(self, query_shape, value_shape, key_shape):\n+        \"\"\"Test dimension mismatches\"\"\"\n+        layer = layers.MultiHeadAttention(\n+            num_heads=4,\n+            key_dim=2,\n+            value_dim=2,\n+        )\n+        with self.assertRaisesRegex(ValueError, r\"must be equal\"):\n+            layer.compute_output_shape(query_shape, value_shape, key_shape)\n+\n+    def test_initializer(self):\n+        # Test with a specified initializer.\n+        layer = layers.MultiHeadAttention(\n+            num_heads=12,\n+            key_dim=64,\n+            kernel_initializer=initializers.TruncatedNormal(stddev=0.02),\n+        )\n+        layer.build((2, 4, 8), (2, 4, 8))\n+\n+        # Make sure the sub layers have different kernel init value.\n+        self.assertNotAllClose(\n+            layer._query_dense.kernel,\n+            layer._key_dense.kernel,\n+        )\n+        self.assertNotAllClose(\n+            layer._query_dense.kernel,\n+            layer._value_dense.kernel,\n+        )\n+        self.assertNotAllClose(\n+            layer._query_dense.kernel,\n+            layer._output_dense.kernel,\n+        )\n+\n+    def test_query_mask_progagation(self):\n+        \"\"\"Test automatic propagation of the query's mask.\"\"\"\n+        layer = layers.MultiHeadAttention(num_heads=2, key_dim=2)\n+        self.assertTrue(layer.supports_masking)\n+        query = np.array([[1, 2, 3, 0, 0], [3, 3, 1, 1, 2], [1, 0, 0, 0, 0]])\n+        masked_query = layers.Embedding(4, 8, mask_zero=True)(query)\n+        value = np.random.normal(size=(3, 3, 8))\n+        output = layer(query=masked_query, value=value)\n+        self.assertAllClose(masked_query._keras_mask, output._keras_mask)\n+\n+    @parameterized.named_parameters((\"causal\", True), (\"not_causal\", 0))\n+    def test_masking(self, use_causal_mask):\n+        \"\"\"Test that the value and causal masks are taken into account.\"\"\"\n+        layer = layers.MultiHeadAttention(num_heads=2, key_dim=2)\n+        query = np.array([[1, 2, 3, 0, 0], [3, 3, 1, 1, 2], [1, 0, 0, 0, 0]])\n+        masked_query = layers.Embedding(4, 8, mask_zero=True)(query)\n+        value = np.array([[5, 4, 0], [3, 0, 0], [2, 1, 1]])\n+        masked_value = layers.Embedding(6, 8, mask_zero=True)(value)\n+        output = layer(\n+            query=masked_query,\n+            value=masked_value,\n+            use_causal_mask=use_causal_mask,\n+        )\n+        mask = np.array(\n+            [[[1, 1, 0]] * 3 + [[0, 0, 0]] * 2]\n+            + [[[1, 0, 0]] * 5]\n+            + [[[1, 1, 1]] + [[0, 0, 0]] * 4]\n+        ).astype(bool)\n+        if use_causal_mask:\n+            mask = mask & np.array(\n+                [[[1, 0, 0], [1, 1, 0]] + [[1, 1, 1]] * 3]\n+            ).astype(bool)\n+        del masked_query._keras_mask\n+        del masked_value._keras_mask\n+        output_with_manual_mask = layer(\n+            query=masked_query, value=masked_value, attention_mask=mask\n+        )\n+        self.assertAllClose(output, output_with_manual_mask)\n+\n+    def test_correctness(self):\n+        query = np.array([[[1.0, 0.0], [0.0, 1.0]]])\n+        key = np.array([[[0.0, 1.0], [1.0, 0.0]]])\n+        value = np.array([[[1.0, 2.0], [3.0, 4.0]]])\n+\n+        # Setup layer.\n+        num_heads = 2\n+        key_dim = 2\n+        layer = layers.MultiHeadAttention(\n+            num_heads=num_heads,\n+            key_dim=key_dim,\n+        )\n+        layer.build(query.shape, key.shape, value.shape)\n+\n+        # Set layer weights.\n+        kernel = np.identity(key_dim)\n+        # To get an identity kernel we need to add a head dim and repeat on it.\n+        kernel = np.repeat(kernel[:, np.newaxis, :], num_heads, axis=1)\n+        # Zeros for all biases.\n+        bias = np.zeros((2, 2))\n+        output_bias = np.zeros((2,))\n+        layer.set_weights([kernel, bias] * 3 + [kernel, output_bias])\n+\n+        # Call layer and assert output.\n+        output, scores = layer(\n+            query=query,\n+            value=value,\n+            key=key,\n+            return_attention_scores=True,\n+        )\n+        self.assertAllClose(output, [[[5.679, 5.679], [4.32, 4.32]]], atol=1e-3)\n+        self.assertAllClose(\n+            scores,\n+            [[[[0.33, 0.67], [0.67, 0.33]], [[0.33, 0.67], [0.67, 0.33]]]],\n+            atol=1e-3,\n+        )\n\n@@ -571,7 +571,7 @@ class Layer(Operation):\n         else:\n             # Use compute_output_shape() to return the right output spec\n             call_spec = CallSpec(self.call, args, kwargs)\n-            shapes_dict = get_shapes_dict(call_spec)\n+            shapes_dict = get_shapes_dict(self.compute_output_shape, call_spec)\n             if len(shapes_dict) == 1:\n                 # Single arg: pass it positionally\n                 input_shape = tuple(shapes_dict.values())[0]\n@@ -723,7 +723,7 @@ class Layer(Operation):\n \n     def _maybe_build(self, call_spec):\n         if not self.built:\n-            shapes_dict = get_shapes_dict(call_spec)\n+            shapes_dict = get_shapes_dict(self.build, call_spec)\n             self._build_shapes_dict = shapes_dict\n             failure = False\n             if len(shapes_dict) == 1:\n@@ -741,9 +741,6 @@ class Layer(Operation):\n                     else:\n                         self.build(input_shape)\n             else:\n-                # More than one shape: pass them by name,\n-                # and check that build() expects the right args.\n-                check_build_signature(self.build, shapes_dict)\n                 with backend.name_scope(self.name):\n                     if utils.is_default(self.build):\n                         if might_have_unbuilt_state(self):\n@@ -751,29 +748,7 @@ class Layer(Operation):\n                             if not status:\n                                 failure = True\n                     else:\n-                        run_build = True\n-                        build_args = set(\n-                            inspect.signature(self.build).parameters.keys()\n-                        )\n-                        for key in shapes_dict.keys():\n-                            if key not in build_args:\n-                                run_build = False\n-                        if run_build:\n                         self.build(**shapes_dict)\n-                        else:\n-                            raise ValueError(\n-                                \"In a layer with multiple tensor arguments \"\n-                                \"in call(), the build() method should accept \"\n-                                \"corresponding `*_shape` arguments, e.g. \"\n-                                \"if the call signature is \"\n-                                \"`def call(self, x1, x2)` \"\n-                                \"then the build signature should be \"\n-                                \"`def build(self, x1_shape, x2_shape)`. \"\n-                                \"Keras will not build this layer automatically \"\n-                                \"since it does not conform to this. \"\n-                                \"Expected the following build keys: \"\n-                                f\"{list(shapes_dict.keys())}\"\n-                            )\n             if failure:\n                 if call_spec.eager:\n                     # Will let the actual eager call do the state-building\n@@ -821,7 +796,7 @@ class Layer(Operation):\n             # Case: all input keyword arguments were plain tensors.\n             input_tensors = {\n                 # We strip the `_shape` suffix to recover kwarg names.\n-                k[:-6]: backend.KerasTensor(shape)\n+                k.removesuffix(\"_shape\"): backend.KerasTensor(shape)\n                 for k, shape in shapes_dict.items()\n             }\n             try:\n@@ -992,16 +967,17 @@ def get_arguments_dict(fn, args, kwargs):\n     return arg_dict\n \n \n-def get_shapes_dict(call_spec):\n+def get_shapes_dict(target_fn, call_spec):\n     \"\"\"Convert the call() arguments dict into a dict of input shape arguments.\n \n     Example:\n \n     ```\n-    >>> get_shapes_dict(call_spec)\n+    >>> get_shapes_dict(self.build, call_spec)\n     {\"input_a_shape\": (2, 3)}\n     ```\n     \"\"\"\n+    expected_names = check_shapes_signature(target_fn, call_spec)\n     shapes_dict = {}\n     for k, v in call_spec.tensor_arguments_dict.items():\n         if k == \"mask\" or k.startswith(\"mask_\"):\n@@ -1010,6 +986,8 @@ def get_shapes_dict(call_spec):\n         if k == \"kwargs\" or k == \"args\":\n             # Do not include catch-alls in shapes dict\n             continue\n+        if expected_names is not None and f\"{k}_shape\" not in expected_names:\n+            continue\n         if k in call_spec.nested_tensor_argument_names:\n             shapes_dict[f\"{k}_shape\"] = nest.map_structure(\n                 lambda x: backend.standardize_shape(x.shape), v\n@@ -1019,22 +997,26 @@ def get_shapes_dict(call_spec):\n     return shapes_dict\n \n \n-def check_build_signature(build_fn, shapes_dict):\n-    \"\"\"Asserts that the argument names in build_fn match entries in shapes_dict.\n+def check_shapes_signature(target_fn, call_spec):\n+    \"\"\"Asserts that the argument names in `target_fn` match arguments in `call`.\n \n-    For instance if call() has the signature `def call(self, a, b)`\n-    then we'll see `shapes_dict == {\"a_shape\": (...), \"b_shape\": (...)}\n-    and we expect build() to have signature `def build(self, a_shape, b_shape)`.\n+    We use this to check that `build()` and `compute_output_shape()` arguments\n+    align with `call()` arguments.\n \n-    When there is a single tensor argument, we pass it positionally and thus\n-    don't check names (if we did, it would force call() to always take\n-    `input` as its first argument, which is usually not the case).\n+    For instance if `build()` has the signature\n+    `def build(self, a_shape, b_shape)` we expect `call()` to accept the\n+    arguments `a` and `b`.\n+\n+    When there is a single argument accepted by `target_fn`, we do allow any\n+    name and do not check the call signature.\n+\n+    Returns:\n+        The list of arguments names expected by the `target_fn` or\n+        `None` if any passed name is acceptable.\n     \"\"\"\n-    if len(shapes_dict) == 1:\n-        return\n-    if utils.is_default(build_fn):\n-        return\n-    sig = inspect.signature(build_fn)\n+    if utils.is_default(target_fn):\n+        return None\n+    sig = inspect.signature(target_fn)\n     expected_names = []\n     for name, param in sig.parameters.items():\n         if param.kind in (\n@@ -1043,14 +1025,29 @@ def check_build_signature(build_fn, shapes_dict):\n             param.KEYWORD_ONLY,\n         ):\n             expected_names.append(name)\n-    if set(expected_names) != set(shapes_dict.keys()):\n-        comma_separated = \", \".join(shapes_dict.keys())\n-        raise ValueError(\n-            \"For a `call()` method with more than one tensor argument, \"\n-            \"the arguments of the `build()` method should match the \"\n-            \"tensor arguments of `call()` method. Here we expect the signature \"\n-            f\"`build(self, {comma_separated})`.\"\n+    if len(expected_names) == 1:\n+        return None\n+    for name in expected_names:\n+        method_name = target_fn.__name__\n+        error_preamble = (\n+            f\"For a `{method_name}()` method with more than one argument, all \"\n+            \"arguments should have a `_shape` suffix and match an argument \"\n+            f\"from `call()`. E.g. `{method_name}(self, foo_shape, bar_shape)` \"\n+            \"would match `call(self, foo, bar)`.\"\n         )\n+        if not name.endswith(\"_shape\"):\n+            raise ValueError(\n+                f\"{error_preamble} Received `{method_name}()` argument \"\n+                f\"`{name}`, which does not end in `_shape`.\"\n+            )\n+        expected_call_arg = name.removesuffix(\"_shape\")\n+        if expected_call_arg not in call_spec.arguments_dict:\n+            raise ValueError(\n+                f\"{error_preamble} Received `{method_name}()` argument \"\n+                f\"`{name}`, but `call()` does not have argument \"\n+                f\"`{expected_call_arg}`.\"\n+            )\n+    return expected_names\n \n \n class CallContext:\n\n@@ -588,3 +588,41 @@ class LayerTest(testing.TestCase):\n         self.assertEqual(len(layer.trainable_variables), 2)\n         self.assertEqual(len(layer.non_trainable_weights), 2)\n         self.assertEqual(len(layer.non_trainable_variables), 3)\n+\n+    def test_build_signature_errors(self):\n+        class NoShapeSuffix(layers.Layer):\n+            def build(self, foo_shape, bar):\n+                self._built = True\n+\n+            def call(self, foo, bar):\n+                return foo + bar\n+\n+        class NonMatchingArgument(layers.Layer):\n+            def build(self, foo_shape, baz_shape):\n+                self._built = True\n+\n+            def call(self, foo, bar):\n+                return foo + bar\n+\n+        class MatchingArguments(layers.Layer):\n+            def build(self, foo_shape, bar_shape):\n+                self._built = True\n+\n+            def call(self, foo, bar):\n+                return foo + bar\n+\n+        foo = backend.numpy.ones((4, 4))\n+        bar = backend.numpy.ones((4, 4))\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"argument `bar`, which does not end in `_shape`\",\n+        ):\n+            NoShapeSuffix()(foo, bar)\n+\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`baz_shape`, but `call\\(\\)` does not have argument `baz`\",\n+        ):\n+            NonMatchingArgument()(foo, bar)\n+\n+        MatchingArguments()(foo, bar)\n\n@@ -1,10 +1,10 @@\n from keras_core import backend\n-from keras_core import layers\n from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n \n \n @keras_core_export(\"keras_core.layers.Dropout\")\n-class Dropout(layers.Layer):\n+class Dropout(Layer):\n     \"\"\"Applies dropout to the input.\n \n     The `Dropout` layer randomly sets input units to 0 with a frequency of\n\n@@ -51,7 +51,7 @@ class SimpleRNNCell(Layer, DropoutRNNCell):\n             for the linear transformation of the recurrent state. Default: 0.\n \n     Call arguments:\n-        inputs: A 2D tensor, with shape `(batch, features)`.\n+        sequence: A 2D tensor, with shape `(batch, features)`.\n         states: A 2D tensor with shape `(batch, units)`, which is the state\n             from the previous time step.\n         training: Python boolean indicating whether the layer should behave in\n@@ -151,14 +151,14 @@ class SimpleRNNCell(Layer, DropoutRNNCell):\n             self.bias = None\n         self.built = True\n \n-    def call(self, inputs, states, training=False):\n+    def call(self, sequence, states, training=False):\n         prev_output = states[0] if isinstance(states, (list, tuple)) else states\n-        dp_mask = self.get_dropout_mask(inputs)\n+        dp_mask = self.get_dropout_mask(sequence)\n         rec_dp_mask = self.get_recurrent_dropout_mask(prev_output)\n \n         if training and dp_mask is not None:\n-            inputs *= dp_mask\n-        h = ops.matmul(inputs, self.kernel)\n+            sequence *= dp_mask\n+        h = ops.matmul(sequence, self.kernel)\n         if self.bias is not None:\n             h += self.bias\n \n@@ -265,7 +265,7 @@ class SimpleRNN(RNN):\n             Unrolling is only suitable for short sequences.\n \n     Call arguments:\n-        inputs: A 3D tensor, with shape `[batch, timesteps, feature]`.\n+        sequence: A 3D tensor, with shape `[batch, timesteps, feature]`.\n         mask: Binary tensor of shape `[batch, timesteps]` indicating whether\n             a given timestep should be masked. An individual `True` entry\n             indicates that the corresponding timestep should be utilized,\n\n@@ -255,12 +255,20 @@ class TestCase(unittest.TestCase):\n         # Build test.\n         if input_shape is not None:\n             layer = layer_cls(**init_kwargs)\n+            if isinstance(input_shape, dict):\n+                layer.build(**input_shape)\n+            else:\n                 layer.build(input_shape)\n             run_build_asserts(layer)\n \n             # Symbolic call test.\n             keras_tensor_inputs = create_keras_tensors(input_shape, input_dtype)\n             layer = layer_cls(**init_kwargs)\n+            if isinstance(keras_tensor_inputs, dict):\n+                keras_tensor_outputs = layer(\n+                    **keras_tensor_inputs, **call_kwargs\n+                )\n+            else:\n                 keras_tensor_outputs = layer(keras_tensor_inputs, **call_kwargs)\n             run_build_asserts(layer)\n             run_output_asserts(layer, keras_tensor_outputs, eager=False)\n@@ -274,6 +282,9 @@ class TestCase(unittest.TestCase):\n             if input_data is None:\n                 input_data = create_eager_tensors(input_shape, input_dtype)\n             layer = layer_cls(**init_kwargs)\n+            if isinstance(input_data, dict):\n+                output_data = layer(**input_data, **call_kwargs)\n+            else:\n                 output_data = layer(input_data, **call_kwargs)\n             run_output_asserts(layer, output_data, eager=True)\n \n@@ -287,7 +298,7 @@ def create_keras_tensors(input_shape, dtype):\n         return [keras_tensor.KerasTensor(s, dtype=dtype) for s in input_shape]\n     if isinstance(input_shape, dict):\n         return {\n-            k: keras_tensor.KerasTensor(v, dtype=dtype)\n+            k.removesuffix(\"_shape\"): keras_tensor.KerasTensor(v, dtype=dtype)\n             for k, v in input_shape.items()\n         }\n \n@@ -320,4 +331,7 @@ def create_eager_tensors(input_shape, dtype):\n     if isinstance(input_shape, list):\n         return [create_fn(s, dtype=dtype) for s in input_shape]\n     if isinstance(input_shape, dict):\n-        return {k: create_fn(v, dtype=dtype) for k, v in input_shape.items()}\n+        return {\n+            k.removesuffix(\"_shape\"): create_fn(v, dtype=dtype)\n+            for k, v in input_shape.items()\n+        }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
