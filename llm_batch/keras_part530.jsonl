{"custom_id": "keras#a90cb90da4dc560d6607f5ac05df7cb96fa5a44b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 205 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 12 | Complexity Δ (Sum/Max): 14/7 | Churn Δ: 205 | Churn Cumulative: 521 | Contributors (this commit): 19 | Commits (past 90d): 10 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -66,6 +66,9 @@ from keras.utils.data_utils import pad_sequences\n from keras.utils.sidecar_evaluator import SidecarEvaluator\n from keras.utils.sidecar_evaluator import SidecarEvaluatorModelExport\n \n+# Timed Thread\n+from keras.utils.timed_threads import TimedThread\n+\n # Visualization related\n from keras.utils.vis_utils import model_to_dot\n from keras.utils.vis_utils import plot_model\n\n@@ -0,0 +1,124 @@\n+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Thread utilities.\"\"\"\n+\n+import abc\n+import threading\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export(\"keras.utils.TimedThread\", v1=[])\n+class TimedThread(threading.Thread):\n+    \"\"\"Time-based interval Threads.\n+\n+    Runs a timed thread every x seconds. It can be used to run a threaded\n+    function alongside model training or any other snippet of code.\n+\n+    Args:\n+        interval: The interval, in seconds, to wait between calls to the\n+            `on_interval` function.\n+        **kwargs: additional args that are passed to `threading.Thread`.\n+\n+    Examples:\n+\n+    ```python\n+    class TimedLogIterations(keras.utils.TimedThread):\n+        def __init__(self, model, interval, *args, **kwargs):\n+            self.model = model\n+            super().__init__(interval, *args, **kwargs)\n+\n+        def on_interval(self):\n+            # Logs Optimizer iterations every x seconds\n+            try:\n+                opt_iterations = self.model.optimizer.iterations.numpy()\n+                print(f\"Epoch: {epoch}, Optimizer Iterations: {opt_iterations}\")\n+            except Exception as e:\n+                print(str(e))  # To prevent thread from getting killed\n+\n+    # `start` and `stop` the `TimerThread` manually. If the `on_interval` call\n+    # requires access to `model` or other objects, override `__init__` method.\n+    # Wrap it in a `try-except` to handle exceptions and `stop` the thread run.\n+    timed_logs = TimedLogIterations(model=model, interval=5)\n+    timed_logs.start()\n+    try:\n+        model.fit(...)\n+    finally:\n+        timed_logs.stop()\n+\n+    # Alternatively, run the `TimedThread` in a context manager\n+    with TimedLogIterations(model=model, interval=5):\n+        model.fit(...)\n+\n+    # If the timed thread instance needs access to callback events,\n+    # subclass both `TimedThread` and `Callback`.  Note that when calling\n+    # `super`, they will have to called for each parent class if both of them\n+    # have the method that needs to be run. Also, note that `Callback` has\n+    # access to `model` as an attribute and need not be explictly provided.\n+    class LogThreadCallback(\n+        keras.utils.TimedThread, keras.callbacks.Callback\n+    ):\n+        def __init__(self, interval):\n+            self._epoch = 0\n+            keras.utils.TimedThread.__init__(self, interval)\n+            keras.callbacks.Callback.__init__(self)\n+\n+        def on_interval(self):\n+            if self.epoch:\n+                opt_iter = self.model.optimizer.iterations.numpy()\n+                logging.info(f\"Epoch: {self._epoch}, Opt Iteration: {opt_iter}\")\n+\n+        def on_epoch_begin(self, epoch, logs=None):\n+            self._epoch = epoch\n+\n+    with LogThreadCallback(interval=5) as thread_callback:\n+        # It's required to pass `thread_callback` to also `callbacks` arg of\n+        # `model.fit` to be triggered on callback events.\n+        model.fit(..., callbacks=[thread_callback])\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, interval, **kwargs):\n+        self.interval = interval\n+        daemon = kwargs.pop(\"daemon\", True)\n+        self.thread_stop_event = threading.Event()\n+        super().__init__(target=self._call_on_interval, daemon=daemon, **kwargs)\n+\n+    def _call_on_interval(self):\n+        # Runs indefinitely once thread is started\n+        while not self.thread_stop_event.is_set():\n+            self.on_interval()\n+            self.thread_stop_event.wait(self.interval)\n+\n+    def stop(self):\n+        \"\"\"Stops the thread run.\"\"\"\n+        self.thread_stop_event.set()\n+\n+    def __enter__(self):\n+        # Starts the thread in context manager\n+        self.start()\n+        return self\n+\n+    def __exit__(self, *args, **kwargs):\n+        # Stops the thread run.\n+        self.stop()\n+\n+    @abc.abstractmethod\n+    def on_interval(self):\n+        \"\"\"User-defined behavior that is called in the thread.\"\"\"\n+        raise NotImplementedError(\n+            \"Runs every x interval seconds. Needs to be \"\n+            \"implemented in subclasses of `TimedThread`\"\n+        )\n\n@@ -0,0 +1,78 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for timed_threads.\"\"\"\n+\n+import time\n+\n+import tensorflow.compat.v2 as tf\n+from absl import logging\n+\n+import keras\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n+from keras.utils import timed_threads\n+\n+\n+@test_utils.run_v2_only\n+class TimedThreadTest(test_combinations.TestCase):\n+    def test_timed_thread_run(self):\n+        class LogThread(timed_threads.TimedThread):\n+            def on_interval(self):\n+                logging.info(\"Thread Run\")\n+\n+        log_thread = LogThread(interval=0.1)\n+        with self.assertLogs(level=\"INFO\") as logs:\n+            log_thread.start()\n+            time.sleep(1)\n+            self.assertTrue(log_thread.is_alive())\n+            log_thread.stop()\n+        self.assertIn(\"INFO:absl:Thread Run\", logs.output)\n+        time.sleep(0.1)\n+        self.assertFalse(log_thread.is_alive())\n+\n+    def test_timed_thread_callback_model_fit(self):\n+        class LogThreadCallback(\n+            timed_threads.TimedThread, keras.callbacks.Callback\n+        ):\n+            def __init__(self, interval):\n+                self._epoch = 0\n+                timed_threads.TimedThread.__init__(self, interval=interval)\n+                keras.callbacks.Callback.__init__(self)\n+\n+            def on_interval(self):\n+                if self._epoch:\n+                    # Verify that `model` is accessible.\n+                    _ = self.model.optimizer.iterations.numpy()\n+                    logging.info(f\"Thread Run Epoch: {self._epoch}\")\n+\n+            def on_epoch_begin(self, epoch, logs=None):\n+                self._epoch = epoch\n+                time.sleep(1)\n+\n+        x = tf.random.normal((32, 2))\n+        y = tf.ones((32, 1), dtype=tf.float32)\n+        model = keras.Sequential([keras.layers.Dense(1)])\n+        model.compile(loss=\"mse\")\n+        with self.assertLogs(level=\"INFO\") as logs, LogThreadCallback(\n+            interval=0.1\n+        ) as log_thread_callback:\n+            self.assertIsNone(log_thread_callback.model)\n+            model.fit(x, y, epochs=2, callbacks=[log_thread_callback])\n+            self.assertIsNotNone(log_thread_callback.model)\n+            self.assertIn(\"INFO:absl:Thread Run Epoch: 1\", logs.output)\n+\n+\n+if __name__ == \"__main__\":\n+    tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6544d6b85048f7cac598ee75c018515edc88db93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 253 | Lines Deleted: 86 | Files Changed: 30 | Hunks: 82 | Methods Changed: 69 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 339 | Churn Cumulative: 4708 | Contributors (this commit): 2 | Commits (past 90d): 86 | Contributors (cumulative): 31 | DMM Complexity: 0.7870967741935484\n\nDIFF:\n@@ -32,7 +32,9 @@ class MiniDropout(Layer):\n         self.seed_generator = backend.random.RandomSeedGenerator(1337)\n \n     def call(self, inputs):\n-        return backend.random.dropout(inputs, self.rate, seed=self.seed_generator)\n+        return backend.random.dropout(\n+            inputs, self.rate, seed=self.seed_generator\n+        )\n \n \n class MiniBatchNorm(Layer):\n@@ -43,7 +45,9 @@ class MiniBatchNorm(Layer):\n \n     def build(self, input_shape):\n         shape = (input_shape[-1],)\n-        self.mean = backend.Variable(initializers.Zeros()(shape), trainable=False)\n+        self.mean = backend.Variable(\n+            initializers.Zeros()(shape), trainable=False\n+        )\n         self.variance = backend.Variable(\n             initializers.GlorotUniform()(shape), trainable=False\n         )\n@@ -58,7 +62,9 @@ class MiniBatchNorm(Layer):\n             self.variance.assign(\n                 self.variance * self.momentum + variance * (1.0 - self.momentum)\n             )\n-            self.mean.assign(self.mean * self.momentum + mean * (1.0 - self.momentum))\n+            self.mean.assign(\n+                self.mean * self.momentum + mean * (1.0 - self.momentum)\n+            )\n         else:\n             outputs = (inputs - self.mean) / (self.variance + self.epsilon)\n         outputs *= self.gamma\n\n@@ -34,7 +34,9 @@ class MiniDropout(Layer):\n         self.seed_generator = backend.random.RandomSeedGenerator(1337)\n \n     def call(self, inputs):\n-        return backend.random.dropout(inputs, self.rate, seed=self.seed_generator)\n+        return backend.random.dropout(\n+            inputs, self.rate, seed=self.seed_generator\n+        )\n \n \n class MiniBatchNorm(Layer):\n@@ -45,7 +47,9 @@ class MiniBatchNorm(Layer):\n \n     def build(self, input_shape):\n         shape = (input_shape[-1],)\n-        self.mean = backend.Variable(initializers.Zeros()(shape), trainable=False)\n+        self.mean = backend.Variable(\n+            initializers.Zeros()(shape), trainable=False\n+        )\n         self.variance = backend.Variable(\n             initializers.GlorotUniform()(shape), trainable=False\n         )\n@@ -60,7 +64,9 @@ class MiniBatchNorm(Layer):\n             self.variance.assign(\n                 self.variance * self.momentum + variance * (1.0 - self.momentum)\n             )\n-            self.mean.assign(self.mean * self.momentum + mean * (1.0 - self.momentum))\n+            self.mean.assign(\n+                self.mean * self.momentum + mean * (1.0 - self.momentum)\n+            )\n         else:\n             outputs = (inputs - self.mean) / (self.variance + self.epsilon)\n         outputs *= self.gamma\n@@ -107,7 +113,9 @@ optimizer.build(model.trainable_variables)\n ## Currently operational workflow\n \n \n-def compute_loss_and_updates(trainable_variables, non_trainable_variables, x, y):\n+def compute_loss_and_updates(\n+    trainable_variables, non_trainable_variables, x, y\n+):\n     y_pred, non_trainable_variables = model.stateless_call(\n         trainable_variables, non_trainable_variables, x\n     )\n@@ -119,7 +127,9 @@ grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True)\n \n \n @jax.jit\n-def train_step(trainable_variables, non_trainable_variables, optimizer_variables, x, y):\n+def train_step(\n+    trainable_variables, non_trainable_variables, optimizer_variables, x, y\n+):\n     (loss, non_trainable_variables), grads = grad_fn(\n         trainable_variables, non_trainable_variables, x, y\n     )\n@@ -134,14 +144,20 @@ trainable_variables = model.trainable_variables\n non_trainable_variables = model.non_trainable_variables\n optimizer_variables = optimizer.variables\n for x, y in dataset:\n-    trainable_variables, non_trainable_variables, optimizer_variables = train_step(\n+    (\n+        trainable_variables,\n+        non_trainable_variables,\n+        optimizer_variables,\n+    ) = train_step(\n         trainable_variables, non_trainable_variables, optimizer_variables, x, y\n     )\n \n # Post-processing model state update\n for variable, value in zip(model.trainable_variables, trainable_variables):\n     variable.assign(value)\n-for variable, value in zip(model.non_trainable_variables, non_trainable_variables):\n+for variable, value in zip(\n+    model.non_trainable_variables, non_trainable_variables\n+):\n     variable.assign(value)\n \n print(\"Updated values\")\n\n@@ -30,9 +30,7 @@ class KerasVariable:\n         raise NotImplementedError\n \n     def __repr__(self):\n-        return (\n-            f\"<KerasVariable shape={self.shape}, dtype={self.dtype}, name={self.name}>\"\n-        )\n+        return f\"<KerasVariable shape={self.shape}, dtype={self.dtype}, name={self.name}>\"\n \n \n ALLOWED_DTYPES = {\n\n@@ -82,7 +82,8 @@ def set_floatx(value):\n     accepted_dtypes = {\"float16\", \"float32\", \"float64\"}\n     if value not in accepted_dtypes:\n         raise ValueError(\n-            f\"Unknown `floatx` value: {value}. \" f\"Expected one of {accepted_dtypes}\"\n+            f\"Unknown `floatx` value: {value}. \"\n+            f\"Expected one of {accepted_dtypes}\"\n         )\n     _FLOATX = str(value)\n \n\n@@ -264,7 +264,8 @@ def compute_output_spec(fn, *args, **kwargs):\n             return fn(*args, *static_args, **kwargs, **static_kwargs)\n \n         maybe_symbolic_args, maybe_symbolic_kwargs = nest.map_structure(\n-            convert_keras_tensor_to_jax, (maybe_symbolic_args, maybe_symbolic_kwargs)\n+            convert_keras_tensor_to_jax,\n+            (maybe_symbolic_args, maybe_symbolic_kwargs),\n         )\n         _, jax_out = jax.make_jaxpr(wrapped_fn, return_shape=True)(\n             *maybe_symbolic_args, **maybe_symbolic_kwargs\n\n@@ -72,4 +72,6 @@ def dropout(inputs, rate, noise_shape=None, seed=None):\n     keep_prob = 1.0 - rate\n     mask = jax.random.bernoulli(seed, p=keep_prob, shape=noise_shape)\n     mask = jax.numpy.broadcast_to(mask, inputs.shape)\n-    return jax.lax.select(mask, inputs / keep_prob, jax.numpy.zeros_like(inputs))\n+    return jax.lax.select(\n+        mask, inputs / keep_prob, jax.numpy.zeros_like(inputs)\n+    )\n\n@@ -22,7 +22,9 @@ def draw_seed(seed):\n \n     if isinstance(seed, RandomSeedGenerator):\n         new_seed_value = seed.state.value\n-        seed.state.assign(seed.state + convert_to_tensor([0, 1], dtype=\"uint32\"))\n+        seed.state.assign(\n+            seed.state + convert_to_tensor([0, 1], dtype=\"uint32\")\n+        )\n         return new_seed_value\n     elif isinstance(seed, int):\n         return convert_to_tensor([seed, 0], dtype=\"uint32\")\n\n@@ -128,22 +128,32 @@ class Variable(KerasVariable):\n         return self.value.__rdiv__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __truediv__(self, other):\n-        return self.value.__truediv__(convert_to_tensor(other, dtype=self.dtype))\n+        return self.value.__truediv__(\n+            convert_to_tensor(other, dtype=self.dtype)\n+        )\n \n     def __rtruediv__(self, other):\n-        return self.value.__rtruediv__(convert_to_tensor(other, dtype=self.dtype))\n+        return self.value.__rtruediv__(\n+            convert_to_tensor(other, dtype=self.dtype)\n+        )\n \n     def __floordiv__(self, other):\n-        return self.value.__floordiv__(convert_to_tensor(other, dtype=self.dtype))\n+        return self.value.__floordiv__(\n+            convert_to_tensor(other, dtype=self.dtype)\n+        )\n \n     def __rfloordiv__(self, other):\n-        return self.value.__rfloordiv__(convert_to_tensor(other, dtype=self.dtype))\n+        return self.value.__rfloordiv__(\n+            convert_to_tensor(other, dtype=self.dtype)\n+        )\n \n     def __divmod__(self, other):\n         return self.value.__divmod__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __rdivmod__(self, other):\n-        return self.value.__rdivmod__(convert_to_tensor(other, dtype=self.dtype))\n+        return self.value.__rdivmod__(\n+            convert_to_tensor(other, dtype=self.dtype)\n+        )\n \n     def __mod__(self, other):\n         return self.value.__mod__(convert_to_tensor(other, dtype=self.dtype))\n@@ -161,7 +171,9 @@ class Variable(KerasVariable):\n         return self.value.__matmul__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __rmatmul__(self, other):\n-        return self.value.__rmatmul__(convert_to_tensor(other, dtype=self.dtype))\n+        return self.value.__rmatmul__(\n+            convert_to_tensor(other, dtype=self.dtype)\n+        )\n \n     def __and__(self, other):\n         return self.value.__and__(convert_to_tensor(other, dtype=self.dtype))\n@@ -185,13 +197,17 @@ class Variable(KerasVariable):\n         return self.value.__lshift__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __rlshift__(self, other):\n-        return self.value.__rlshift__(convert_to_tensor(other, dtype=self.dtype))\n+        return self.value.__rlshift__(\n+            convert_to_tensor(other, dtype=self.dtype)\n+        )\n \n     def __rshift__(self, other):\n         return self.value.__rshift__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __rrshift__(self, other):\n-        return self.value.__rrshift__(convert_to_tensor(other, dtype=self.dtype))\n+        return self.value.__rrshift__(\n+            convert_to_tensor(other, dtype=self.dtype)\n+        )\n \n     def __round__(self, ndigits=None):\n         return self.value.__round__(ndigits)\n@@ -232,7 +248,9 @@ def compute_output_spec(fn, *args, **kwargs):\n                 return tf.compat.v1.placeholder(shape=x.shape, dtype=x.dtype)\n             return x\n \n-        args, kwargs = tf.nest.map_structure(convert_keras_tensor_to_tf, (args, kwargs))\n+        args, kwargs = tf.nest.map_structure(\n+            convert_keras_tensor_to_tf, (args, kwargs)\n+        )\n         tf_out = fn(*args, **kwargs)\n \n         def convert_tf_to_keras_tensor(x):\n@@ -247,4 +265,6 @@ def execute(op_name, *args, **kwargs):\n     if hasattr(tfnp, op_name):\n         op = getattr(tfnp, op_name)\n         return op(*args, **kwargs)\n-    raise AttributeError(f\"The TensorFlow backend does not support op '{op_name}'\")\n+    raise AttributeError(\n+        f\"The TensorFlow backend does not support op '{op_name}'\"\n+    )\n\n@@ -2,7 +2,9 @@ import warnings\n from keras_core.api_export import keras_core_export\n \n \n-@keras_core_export([\"keras_core.Initializer\", \"keras_core.initializers.Initializer\"])\n+@keras_core_export(\n+    [\"keras_core.Initializer\", \"keras_core.initializers.Initializer\"]\n+)\n class Initializer:\n     \"\"\"Initializer base class: all Keras initializers inherit from this class.\n \n\n@@ -52,7 +52,8 @@ class VarianceScaling(Initializer):\n     ):\n         if scale <= 0.0:\n             raise ValueError(\n-                \"Argument `scale` must be positive float. \" f\"Received: scale={scale}\"\n+                \"Argument `scale` must be positive float. \"\n+                f\"Received: scale={scale}\"\n             )\n         allowed_modes = {\"fan_in\", \"fan_out\", \"fan_avg\"}\n         if mode not in allowed_modes:\n@@ -155,7 +156,9 @@ class GlorotUniform(VarianceScaling):\n     \"\"\"\n \n     def __init__(self, seed=None):\n-        super().__init__(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed=seed)\n+        super().__init__(\n+            scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed=seed\n+        )\n \n     def get_config(self):\n         return {\"seed\": self.seed}\n@@ -281,7 +284,9 @@ class LecunUniform(VarianceScaling):\n     \"\"\"\n \n     def __init__(self, seed=None):\n-        super().__init__(scale=1.0, mode=\"fan_in\", distribution=\"uniform\", seed=seed)\n+        super().__init__(\n+            scale=1.0, mode=\"fan_in\", distribution=\"uniform\", seed=seed\n+        )\n \n     def get_config(self):\n         return {\"seed\": self.seed}\n@@ -359,7 +364,9 @@ class HeUniform(VarianceScaling):\n     \"\"\"\n \n     def __init__(self, seed=None):\n-        super().__init__(scale=2.0, mode=\"fan_in\", distribution=\"uniform\", seed=seed)\n+        super().__init__(\n+            scale=2.0, mode=\"fan_in\", distribution=\"uniform\", seed=seed\n+        )\n \n     def get_config(self):\n         return {\"seed\": self.seed}\n\n@@ -56,7 +56,9 @@ class InputSpec:\n         allow_last_axis_squeeze=False,\n         name=None,\n     ):\n-        self.dtype = backend.standardize_dtype(dtype) if dtype is not None else None\n+        self.dtype = (\n+            backend.standardize_dtype(dtype) if dtype is not None else None\n+        )\n         if shape is not None:\n             self.shape = backend.standardize_shape(shape)\n             self.ndim = len(shape)\n\n@@ -62,7 +62,8 @@ class Layer(Operation):\n                 ),\n                 \"metrics\": (lambda x: isinstance(x, Metric), self._metrics),\n                 \"layers\": (\n-                    lambda x: isinstance(x, Layer) and not isinstance(x, Metric),\n+                    lambda x: isinstance(x, Layer)\n+                    and not isinstance(x, Metric),\n                     self._layers,\n                 ),\n                 # TODO: RandomSeedGenerator tracking\n@@ -251,7 +252,9 @@ class Layer(Operation):\n         # Argument validation and conversion. #\n         # 1. Convert first positional argument to tensor of correct dtype.\n         if args and not isinstance(args[0], KerasTensor):\n-            args = (nest.map_structure(backend.convert_to_tensor, args[0]),) + args[1:]\n+            args = (\n+                nest.map_structure(backend.convert_to_tensor, args[0]),\n+            ) + args[1:]\n \n         # 2. Convert any other array arguments to tensors of correct dtype.\n         def maybe_convert(x):\n@@ -578,9 +581,12 @@ def get_shapes_dict(arguments_dict):\n             shapes_dict[f\"{k}_shape\"] = backend.standardize_shape(v.shape)\n         elif nest.is_nested(v):\n             flat = nest.flatten(v)\n-            if any(isinstance(x, KerasTensor) or backend.is_tensor(x) for x in flat):\n-                if not all(\n+            if any(\n                 isinstance(x, KerasTensor) or backend.is_tensor(x) for x in flat\n+            ):\n+                if not all(\n+                    isinstance(x, KerasTensor) or backend.is_tensor(x)\n+                    for x in flat\n                 ):\n                     raise ValueError(\n                         \"You cannot mix tensors and non-tensors in a nested argument. \"\n\n@@ -13,7 +13,9 @@ class FunctionTest(testing.TestCase):\n                 return x + 1\n \n         x = keras_tensor.KerasTensor(shape=(2, 3), name=\"x\")\n-        with self.assertRaisesRegex(ValueError, \"Only input tensors may be passed as\"):\n+        with self.assertRaisesRegex(\n+            ValueError, \"Only input tensors may be passed as\"\n+        ):\n             SomeLayer()(x, True)\n \n         # This works\n\n@@ -31,7 +31,10 @@ class Loss:\n                 mask = None\n \n             return reduce_weighted_loss(\n-                losses, sample_weight=sample_weight, mask=mask, reduction=self.reduction\n+                losses,\n+                sample_weight=sample_weight,\n+                mask=mask,\n+                reduction=self.reduction,\n             )\n \n     def call(self, y_true, y_pred):\n@@ -72,7 +75,11 @@ def squeeze_to_same_rank(x1, x2):\n \n \n def reduce_loss(losses, reduction=\"sum_over_batch_size\"):\n-    if reduction is None or tuple(losses.shape) == () or tuple(losses.shape) == (0,):\n+    if (\n+        reduction is None\n+        or tuple(losses.shape) == ()\n+        or tuple(losses.shape) == (0,)\n+    ):\n         return losses\n     loss = ops.sum(losses)\n     if reduction == \"sum_over_batch_size\":\n\n@@ -52,7 +52,9 @@ class LossTest(testing.TestCase):\n         loss_fn = ExampleLoss()\n         loss = loss_fn(y_true, y_pred)\n         self.assertEqual(loss.dtype.name, \"float32\")\n-        self.assertAllClose(np.sum((masked_y_true - masked_y_pred) ** 2) / 3, loss)\n+        self.assertAllClose(\n+            np.sum((masked_y_true - masked_y_pred) ** 2) / 3, loss\n+        )\n \n         # Test edge case where everything is masked.\n         mask = np.array([False, False, False, False])\n@@ -69,7 +71,9 @@ class LossTest(testing.TestCase):\n         loss_fn = ExampleLoss()\n         loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n         self.assertEqual(loss.dtype.name, \"float32\")\n-        self.assertAllClose(np.sum(sample_weight * (y_true - y_pred) ** 2) / 4, loss)\n+        self.assertAllClose(\n+            np.sum(sample_weight * (y_true - y_pred) ** 2) / 4, loss\n+        )\n \n         # Test edge case where every weight is 0.\n         sample_weight = np.array([0.0, 0.0, 0.0, 0.0])\n@@ -96,7 +100,8 @@ class LossTest(testing.TestCase):\n         loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n         self.assertEqual(loss.dtype.name, \"float32\")\n         self.assertAllClose(\n-            np.sum(masked_sample_weight * (masked_y_true - masked_y_pred) ** 2) / 3,\n+            np.sum(masked_sample_weight * (masked_y_true - masked_y_pred) ** 2)\n+            / 3,\n             loss,\n         )\n \n@@ -130,7 +135,10 @@ class LossTest(testing.TestCase):\n             loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n             self.assertEqual(loss.dtype.name, \"float32\")\n             self.assertAllClose(\n-                np.sum(masked_sample_weight * (masked_y_true - masked_y_pred) ** 2) / 3,\n+                np.sum(\n+                    masked_sample_weight * (masked_y_true - masked_y_pred) ** 2\n+                )\n+                / 3,\n                 loss,\n             )\n \n\n@@ -4,7 +4,9 @@ from keras_core.losses.loss import squeeze_to_same_rank\n \n \n class LossFunctionWrapper(Loss):\n-    def __init__(self, fn, reduction=\"sum_over_batch_size\", name=None, **kwargs):\n+    def __init__(\n+        self, fn, reduction=\"sum_over_batch_size\", name=None, **kwargs\n+    ):\n         super().__init__(reduction=reduction, name=name)\n         self.fn = fn\n         self._fn_kwargs = kwargs\n@@ -29,5 +31,7 @@ def mean_squared_error(y_true, y_pred):\n \n \n class MeanSquaredError(LossFunctionWrapper):\n-    def __init__(self, reduction=\"sum_over_batch_size\", name=\"mean_squared_error\"):\n+    def __init__(\n+        self, reduction=\"sum_over_batch_size\", name=\"mean_squared_error\"\n+    ):\n         super().__init__(mean_squared_error, reduction=reduction, name=name)\n\n@@ -9,7 +9,9 @@ import numpy as np\n class ExampleMetric(Metric):\n     def __init__(self, name=\"mean_square_error\", dtype=None):\n         super().__init__(name=name, dtype=dtype)\n-        self.sum = self.add_variable(name=\"sum\", initializer=initializers.Zeros())\n+        self.sum = self.add_variable(\n+            name=\"sum\", initializer=initializers.Zeros()\n+        )\n         self.total = self.add_variable(\n             name=\"total\", initializer=initializers.Zeros(), dtype=\"int32\"\n         )\n@@ -48,7 +50,9 @@ class MetricTest(testing.TestCase):\n \n         self.assertAllClose(metric.total, 20)\n         result = metric.result()\n-        self.assertAllClose(result, np.sum((y_true - y_pred) ** 2) / num_samples)\n+        self.assertAllClose(\n+            result, np.sum((y_true - y_pred) ** 2) / num_samples\n+        )\n         metric.reset_state()\n         self.assertEqual(metric.result(), 0.0)\n \n@@ -60,7 +64,10 @@ class MetricTest(testing.TestCase):\n \n         # In dict\n         metric = ExampleMetric(name=\"mse\")\n-        metric.more_vars = {\"a\": backend.Variable(0.0), \"b\": backend.Variable(1.0)}\n+        metric.more_vars = {\n+            \"a\": backend.Variable(0.0),\n+            \"b\": backend.Variable(1.0),\n+        }\n         self.assertEqual(len(metric.variables), 4)\n \n         # In nested structured\n\n@@ -6,8 +6,12 @@ from keras_core import initializers\n class MeanSquareError(Metric):\n     def __init__(self, name=\"mean_square_error\", dtype=None):\n         super().__init__(name=name, dtype=dtype)\n-        self.sum = self.add_variable(name=\"sum\", initializer=initializers.Zeros())\n-        self.total = self.add_variable(name=\"total\", initializer=initializers.Zeros())\n+        self.sum = self.add_variable(\n+            name=\"sum\", initializer=initializers.Zeros()\n+        )\n+        self.total = self.add_variable(\n+            name=\"total\", initializer=initializers.Zeros()\n+        )\n \n     def update_state(self, y_true, y_pred):\n         # TODO: add support for sample_weight\n\n@@ -80,7 +80,9 @@ class Model(Layer, Trainer):\n                 f\"No such layer: {name}. Existing layers are: \"\n                 f\"{list(layer.name for layer in self.layers)}.\"\n             )\n-        raise ValueError(\"Provide either a layer name or layer index at `get_layer`.\")\n+        raise ValueError(\n+            \"Provide either a layer name or layer index at `get_layer`.\"\n+        )\n \n     def summary(\n         self,\n\n@@ -107,7 +107,9 @@ class Function(Operation):\n \n     def _assert_input_compatibility(self, inputs):\n         try:\n-            nest.assert_same_structure(inputs, self._inputs_struct, check_types=False)\n+            nest.assert_same_structure(\n+                inputs, self._inputs_struct, check_types=False\n+            )\n         except ValueError:\n             raise ValueError(\n                 \"Function was called with an invalid input structure. \"\n\n@@ -14,7 +14,9 @@ class FunctionTest(testing.TestCase):\n         x = knp.add(x1, x2)\n         y1 = x * 3\n         y2 = x**2\n-        fn = function.Function(inputs=[x1, x2], outputs=[y1, y2], name=\"test_function\")\n+        fn = function.Function(\n+            inputs=[x1, x2], outputs=[y1, y2], name=\"test_function\"\n+        )\n         self.assertEqual(fn.name, \"test_function\")\n \n         # Eager call\n@@ -88,7 +90,9 @@ class FunctionTest(testing.TestCase):\n         x = knp.add(x1, x2)\n         y1 = x * 3\n         y2 = x**2\n-        fn = function.Function(inputs=[x1, x2], outputs=[y1, y2], name=\"test_function\")\n+        fn = function.Function(\n+            inputs=[x1, x2], outputs=[y1, y2], name=\"test_function\"\n+        )\n         self.assertEqual(fn.name, \"test_function\")\n \n         # Bad structure\n\n@@ -36,7 +36,9 @@ class Node:\n         outputs: The output tensors of the `op.__call__()` call.\n     \"\"\"\n \n-    def __init__(self, operation, call_args=None, call_kwargs=None, outputs=None):\n+    def __init__(\n+        self, operation, call_args=None, call_kwargs=None, outputs=None\n+    ):\n         self.operation = operation\n         self.arguments = SymbolicArguments(*call_args, **call_kwargs)\n         self.outputs = [] if outputs is None else nest.flatten(outputs)\n@@ -99,7 +101,9 @@ class Node:\n \n \n class KerasHistory(\n-    collections.namedtuple(\"KerasHistory\", [\"operation\", \"node_index\", \"tensor_index\"])\n+    collections.namedtuple(\n+        \"KerasHistory\", [\"operation\", \"node_index\", \"tensor_index\"]\n+    )\n ):\n     \"\"\"Tracks the Operation call that created a Tensor.\n \n\n@@ -285,10 +285,14 @@ class Mean(Operation):\n         self.keepdims = keepdims\n \n     def call(self, x):\n-        return backend.execute(\"mean\", x, axis=self.axis, keepdims=self.keepdims)\n+        return backend.execute(\n+            \"mean\", x, axis=self.axis, keepdims=self.keepdims\n+        )\n \n     def compute_output_spec(self, x):\n-        return compute_np_output_spec(\"mean\", x, axis=self.axis, keepdims=self.keepdims)\n+        return compute_np_output_spec(\n+            \"mean\", x, axis=self.axis, keepdims=self.keepdims\n+        )\n \n \n def mean(x, axis=None, keepdims=False):\n@@ -306,7 +310,9 @@ class Var(Operation):\n         return backend.execute(\"var\", x, axis=self.axis, keepdims=self.keepdims)\n \n     def compute_output_spec(self, x):\n-        return compute_np_output_spec(\"var\", x, axis=self.axis, keepdims=self.keepdims)\n+        return compute_np_output_spec(\n+            \"var\", x, axis=self.axis, keepdims=self.keepdims\n+        )\n \n \n def var(x, axis=None, keepdims=False):\n@@ -324,7 +330,9 @@ class Sum(Operation):\n         return backend.execute(\"sum\", x, axis=self.axis, keepdims=self.keepdims)\n \n     def compute_output_spec(self, x):\n-        return compute_np_output_spec(\"sum\", x, axis=self.axis, keepdims=self.keepdims)\n+        return compute_np_output_spec(\n+            \"sum\", x, axis=self.axis, keepdims=self.keepdims\n+        )\n \n \n def sum(x, axis=None, keepdims=False):\n\n@@ -24,7 +24,9 @@ class Operation:\n         # sets _keras_history on the outputs, and adds itself to the\n         # `_outbound_nodes` of the ops that produced the inputs to this\n         # call.\n-        Node(operation=self, call_args=args, call_kwargs=kwargs, outputs=outputs)\n+        Node(\n+            operation=self, call_args=args, call_kwargs=kwargs, outputs=outputs\n+        )\n         return outputs\n \n     def call(self, *args, **kwargs):\n\n@@ -61,7 +61,9 @@ class Optimizer:\n         self.iterations = backend.Variable(\n             0, name=\"iteration\", dtype=\"int64\", trainable=False\n         )\n-        if isinstance(learning_rate, learning_rate_schedule.LearningRateSchedule):\n+        if isinstance(\n+            learning_rate, learning_rate_schedule.LearningRateSchedule\n+        ):\n             self._learning_rate = learning_rate\n         elif callable(learning_rate):\n             self._learning_rate = learning_rate\n@@ -262,7 +264,9 @@ class Optimizer:\n         return self._get_current_learning_rate()\n \n     def _get_current_learning_rate(self):\n-        if isinstance(self._learning_rate, learning_rate_schedule.LearningRateSchedule):\n+        if isinstance(\n+            self._learning_rate, learning_rate_schedule.LearningRateSchedule\n+        ):\n             return self._learning_rate(self.iterations)\n         elif callable(self._learning_rate):\n             return self._learning_rate(self.iterations)\n@@ -336,13 +340,17 @@ class Optimizer:\n             )\n \n         if var_list:\n-            self._exclude_from_weight_decay = [id(variable) for variable in var_list]\n+            self._exclude_from_weight_decay = [\n+                id(variable) for variable in var_list\n+            ]\n         else:\n             self._exclude_from_weight_decay = []\n         self._exclude_from_weight_decay_names = var_names or []\n \n     def _use_weight_decay(self, variable):\n-        exclude_from_weight_decay = getattr(self, \"_exclude_from_weight_decay\", [])\n+        exclude_from_weight_decay = getattr(\n+            self, \"_exclude_from_weight_decay\", []\n+        )\n         exclude_from_weight_decay_names = getattr(\n             self, \"_exclude_from_weight_decay_names\", []\n         )\n@@ -375,7 +383,9 @@ class Optimizer:\n     def _update_model_variables_moving_average(self, var_list):\n         \"\"\"Update the stored moving average using the latest value.\"\"\"\n         if self.use_ema:\n-            for var, average in zip(var_list, self._model_variables_moving_average):\n+            for var, average in zip(\n+                var_list, self._model_variables_moving_average\n+            ):\n                 average.assign(\n                     self.ema_momentum * average + (1 - self.ema_momentum) * var\n                 )\n@@ -394,7 +404,9 @@ class Optimizer:\n \n     def _overwrite_model_variables_with_average_value_helper(self, var_list):\n         \"\"\"Helper function that overwrites model variables.\"\"\"\n-        for var, average_var in zip(var_list, self._model_variables_moving_average):\n+        for var, average_var in zip(\n+            var_list, self._model_variables_moving_average\n+        ):\n             var.assign(average_var)\n \n     def finalize_variable_values(self, var_list):\n@@ -427,8 +439,12 @@ class Optimizer:\n             Python dictionary.\n         \"\"\"\n \n-        if isinstance(self._learning_rate, learning_rate_schedule.LearningRateSchedule):\n-            learning_rate = learning_rate_schedule.serialize(self._learning_rate)\n+        if isinstance(\n+            self._learning_rate, learning_rate_schedule.LearningRateSchedule\n+        ):\n+            learning_rate = learning_rate_schedule.serialize(\n+                self._learning_rate\n+            )\n         elif isinstance(self._learning_rate, backend.Variable):\n             learning_rate = float(self._learning_rate.numpy())\n         elif ops.is_tensor(self._learning_rate):\n\n@@ -85,7 +85,9 @@ class SGD(optimizer.Optimizer):\n         self.momentums = []\n         for variable in variables:\n             self.momentums.append(\n-                self.add_variable_from_reference(reference_variable=variable, name=\"m\")\n+                self.add_variable_from_reference(\n+                    reference_variable=variable, name=\"m\"\n+                )\n             )\n \n     def update_step(self, gradient, variable, learning_rate):\n@@ -98,7 +100,9 @@ class SGD(optimizer.Optimizer):\n         if m is not None:\n             m.assign(-gradient * learning_rate + m * momentum)\n             if self.nesterov:\n-                variable.assign(variable - gradient * learning_rate + m * momentum)\n+                variable.assign(\n+                    variable - gradient * learning_rate + m * momentum\n+                )\n             else:\n                 variable.assign(variable + m)\n         else:\n\n@@ -3,7 +3,9 @@ from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n \n \n-@keras_core_export([\"keras_core.Regularizer\", \"keras_core.regularizers.Regularizer\"])\n+@keras_core_export(\n+    [\"keras_core.Regularizer\", \"keras_core.regularizers.Regularizer\"]\n+)\n class Regularizer:\n     \"\"\"Regularizer base class.\n \n@@ -315,10 +317,15 @@ class OrthogonalRegularizer(Regularizer):\n             inputs = l2_normalize(inputs, axis=0)\n             product = ops.matmul(ops.transpose(inputs), inputs)\n             size = inputs.shape[1]\n-        product_no_diagonal = product * (1.0 - ops.eye(size, dtype=inputs.dtype))\n+        product_no_diagonal = product * (\n+            1.0 - ops.eye(size, dtype=inputs.dtype)\n+        )\n         num_pairs = size * (size - 1.0) / 2.0\n         return (\n-            self.factor * 0.5 * ops.sum(ops.absolute(product_no_diagonal)) / num_pairs\n+            self.factor\n+            * 0.5\n+            * ops.sum(ops.absolute(product_no_diagonal))\n+            / num_pairs\n         )\n \n     def get_config(self):\n@@ -327,7 +334,9 @@ class OrthogonalRegularizer(Regularizer):\n \n def validate_float_arg(value, name):\n     \"\"\"check penalty number availability, raise ValueError if failed.\"\"\"\n-    if not isinstance(value, (float, int)) or (math.isinf(value) or math.isnan(value)):\n+    if not isinstance(value, (float, int)) or (\n+        math.isinf(value) or math.isnan(value)\n+    ):\n         raise ValueError(\n             f\"Invalid value for argument {name}: expected a float. \"\n             f\"Received: {name}={value}\"\n\n@@ -212,7 +212,9 @@ def print_summary(\n         if sequential_like:\n             print_layer_summary(layer, prefix=\">>>\" * nested_level + \" \")\n         else:\n-            print_layer_summary_with_connections(layer, prefix=\">>>\" * nested_level + \" \")\n+            print_layer_summary_with_connections(\n+                layer, prefix=\">>>\" * nested_level + \" \"\n+            )\n \n         if expand_nested and hasattr(layer, \"layers\") and layer.layers:\n             nested_layers = layer.layers\n@@ -229,14 +231,16 @@ def print_summary(\n         rows=rows,\n         positions=positions,\n         # Left align layer name, center-align everything else\n-        alignments=[\"left\"] + [\"center\" for _ in range(len(header) - 1)]\n+        alignments=[\"left\"] + [\"center\" for _ in range(len(header) - 1)],\n     )\n     print_fn(table.make())\n \n     # After the table, append information about parameter count and  size.\n     if hasattr(model, \"_collected_trainable_weights\"):\n         trainable_count = count_params(model._collected_trainable_weights)\n-        trainable_memory_size = weight_memory_size(model._collected_trainable_weights)\n+        trainable_memory_size = weight_memory_size(\n+            model._collected_trainable_weights\n+        )\n     else:\n         trainable_count = count_params(model.trainable_weights)\n         trainable_memory_size = weight_memory_size(model.trainable_weights)\n@@ -282,7 +286,9 @@ def get_layer_index_bound_by_layer_name(model, layer_range=None):\n                 \"layer_range must be a list or tuple of length 2. Received: \"\n                 f\"layer_range = {layer_range} of length {len(layer_range)}\"\n             )\n-        if not isinstance(layer_range[0], str) or not isinstance(layer_range[1], str):\n+        if not isinstance(layer_range[0], str) or not isinstance(\n+            layer_range[1], str\n+        ):\n             raise ValueError(\n                 \"layer_range should contain string type only. \"\n                 f\"Received: {layer_range}\"\n\n@@ -2,7 +2,9 @@ import shutil\n \n \n class Table:\n-    def __init__(self, header, rows, positions, alignments=None, max_line_length=80):\n+    def __init__(\n+        self, header, rows, positions, alignments=None, max_line_length=80\n+    ):\n         if len(header) != len(positions):\n             raise ValueError(\"header and positions should be the same length.\")\n         if not all(p <= 1.0 for p in positions):\n@@ -21,7 +23,9 @@ class Table:\n         self.rows = rows\n \n         # Compute columns widths\n-        line_length = min(max_line_length, shutil.get_terminal_size().columns - 4)\n+        line_length = min(\n+            max_line_length, shutil.get_terminal_size().columns - 4\n+        )\n         column_widths = []\n         current = 0\n         for pos in positions:\n@@ -86,7 +90,12 @@ class Table:\n                 else:\n                     field = subfields[i]\n                 fields.append(field)\n-            line = vertical_separator.join(self.align_field(field, width, alignment) for field, width, alignment in zip(fields, self.column_widths, alignments))\n+            line = vertical_separator.join(\n+                self.align_field(field, width, alignment)\n+                for field, width, alignment in zip(\n+                    fields, self.column_widths, alignments\n+                )\n+            )\n             line = f\"{vertical_separator}{line}{vertical_separator}\"\n             rendered_lines.append(line)\n         return \"\\n\".join(rendered_lines)\n\n@@ -32,7 +32,9 @@ class MiniDropout(Layer):\n         self.seed_generator = backend.random.RandomSeedGenerator(1337)\n \n     def call(self, inputs):\n-        return backend.random.dropout(inputs, self.rate, seed=self.seed_generator)\n+        return backend.random.dropout(\n+            inputs, self.rate, seed=self.seed_generator\n+        )\n \n \n class MiniBatchNorm(Layer):\n@@ -43,7 +45,9 @@ class MiniBatchNorm(Layer):\n \n     def build(self, input_shape):\n         shape = (input_shape[-1],)\n-        self.mean = backend.Variable(initializers.Zeros()(shape), trainable=False)\n+        self.mean = backend.Variable(\n+            initializers.Zeros()(shape), trainable=False\n+        )\n         self.variance = backend.Variable(\n             initializers.GlorotUniform()(shape), trainable=False\n         )\n@@ -58,7 +62,9 @@ class MiniBatchNorm(Layer):\n             self.variance.assign(\n                 self.variance * self.momentum + variance * (1.0 - self.momentum)\n             )\n-            self.mean.assign(self.mean * self.momentum + mean * (1.0 - self.momentum))\n+            self.mean.assign(\n+                self.mean * self.momentum + mean * (1.0 - self.momentum)\n+            )\n         else:\n             outputs = (inputs - self.mean) / (self.variance + self.epsilon)\n         outputs *= self.gamma\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4b018c056080afebbb65718b76e771b85c59e95e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 378 | Lines Deleted: 527 | Files Changed: 33 | Hunks: 111 | Methods Changed: 97 | Complexity Δ (Sum/Max): -22/14 | Churn Δ: 905 | Churn Cumulative: 5790 | Contributors (this commit): 2 | Commits (past 90d): 124 | Contributors (cumulative): 65 | DMM Complexity: 0.47058823529411764\n\nDIFF:\n@@ -32,9 +32,7 @@ class MiniDropout(Layer):\n         self.seed_generator = backend.random.RandomSeedGenerator(1337)\n \n     def call(self, inputs):\n-        return backend.random.dropout(\n-            inputs, self.rate, seed=self.seed_generator\n-        )\n+        return backend.random.dropout(inputs, self.rate, seed=self.seed_generator)\n \n \n class MiniBatchNorm(Layer):\n@@ -45,9 +43,7 @@ class MiniBatchNorm(Layer):\n \n     def build(self, input_shape):\n         shape = (input_shape[-1],)\n-        self.mean = backend.Variable(\n-            initializers.Zeros()(shape), trainable=False\n-        )\n+        self.mean = backend.Variable(initializers.Zeros()(shape), trainable=False)\n         self.variance = backend.Variable(\n             initializers.GlorotUniform()(shape), trainable=False\n         )\n@@ -62,9 +58,7 @@ class MiniBatchNorm(Layer):\n             self.variance.assign(\n                 self.variance * self.momentum + variance * (1.0 - self.momentum)\n             )\n-            self.mean.assign(\n-                self.mean * self.momentum + mean * (1.0 - self.momentum)\n-            )\n+            self.mean.assign(self.mean * self.momentum + mean * (1.0 - self.momentum))\n         else:\n             outputs = (inputs - self.mean) / (self.variance + self.epsilon)\n         outputs *= self.gamma\n\n@@ -34,9 +34,7 @@ class MiniDropout(Layer):\n         self.seed_generator = backend.random.RandomSeedGenerator(1337)\n \n     def call(self, inputs):\n-        return backend.random.dropout(\n-            inputs, self.rate, seed=self.seed_generator\n-        )\n+        return backend.random.dropout(inputs, self.rate, seed=self.seed_generator)\n \n \n class MiniBatchNorm(Layer):\n@@ -47,9 +45,7 @@ class MiniBatchNorm(Layer):\n \n     def build(self, input_shape):\n         shape = (input_shape[-1],)\n-        self.mean = backend.Variable(\n-            initializers.Zeros()(shape), trainable=False\n-        )\n+        self.mean = backend.Variable(initializers.Zeros()(shape), trainable=False)\n         self.variance = backend.Variable(\n             initializers.GlorotUniform()(shape), trainable=False\n         )\n@@ -64,9 +60,7 @@ class MiniBatchNorm(Layer):\n             self.variance.assign(\n                 self.variance * self.momentum + variance * (1.0 - self.momentum)\n             )\n-            self.mean.assign(\n-                self.mean * self.momentum + mean * (1.0 - self.momentum)\n-            )\n+            self.mean.assign(self.mean * self.momentum + mean * (1.0 - self.momentum))\n         else:\n             outputs = (inputs - self.mean) / (self.variance + self.epsilon)\n         outputs *= self.gamma\n@@ -113,9 +107,7 @@ optimizer.build(model.trainable_variables)\n ## Currently operational workflow\n \n \n-def compute_loss_and_updates(\n-    trainable_variables, non_trainable_variables, x, y\n-):\n+def compute_loss_and_updates(trainable_variables, non_trainable_variables, x, y):\n     y_pred, non_trainable_variables = model.stateless_call(\n         trainable_variables, non_trainable_variables, x\n     )\n@@ -127,9 +119,7 @@ grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True)\n \n \n @jax.jit\n-def train_step(\n-    trainable_variables, non_trainable_variables, optimizer_variables, x, y\n-):\n+def train_step(trainable_variables, non_trainable_variables, optimizer_variables, x, y):\n     (loss, non_trainable_variables), grads = grad_fn(\n         trainable_variables, non_trainable_variables, x, y\n     )\n@@ -144,20 +134,14 @@ trainable_variables = model.trainable_variables\n non_trainable_variables = model.non_trainable_variables\n optimizer_variables = optimizer.variables\n for x, y in dataset:\n-    (\n-        trainable_variables,\n-        non_trainable_variables,\n-        optimizer_variables,\n-    ) = train_step(\n+    trainable_variables, non_trainable_variables, optimizer_variables = train_step(\n         trainable_variables, non_trainable_variables, optimizer_variables, x, y\n     )\n \n # Post-processing model state update\n for variable, value in zip(model.trainable_variables, trainable_variables):\n     variable.assign(value)\n-for variable, value in zip(\n-    model.non_trainable_variables, non_trainable_variables\n-):\n+for variable, value in zip(model.non_trainable_variables, non_trainable_variables):\n     variable.assign(value)\n \n print(\"Updated values\")\n\n@@ -30,7 +30,9 @@ class KerasVariable:\n         raise NotImplementedError\n \n     def __repr__(self):\n-        return f\"<KerasVariable shape={self.shape}, dtype={self.dtype}, name={self.name}>\"\n+        return (\n+            f\"<KerasVariable shape={self.shape}, dtype={self.dtype}, name={self.name}>\"\n+        )\n \n \n ALLOWED_DTYPES = {\n\n@@ -82,8 +82,7 @@ def set_floatx(value):\n     accepted_dtypes = {\"float16\", \"float32\", \"float64\"}\n     if value not in accepted_dtypes:\n         raise ValueError(\n-            f\"Unknown `floatx` value: {value}. \"\n-            f\"Expected one of {accepted_dtypes}\"\n+            f\"Unknown `floatx` value: {value}. \" f\"Expected one of {accepted_dtypes}\"\n         )\n     _FLOATX = str(value)\n \n\n@@ -264,8 +264,7 @@ def compute_output_spec(fn, *args, **kwargs):\n             return fn(*args, *static_args, **kwargs, **static_kwargs)\n \n         maybe_symbolic_args, maybe_symbolic_kwargs = nest.map_structure(\n-            convert_keras_tensor_to_jax,\n-            (maybe_symbolic_args, maybe_symbolic_kwargs),\n+            convert_keras_tensor_to_jax, (maybe_symbolic_args, maybe_symbolic_kwargs)\n         )\n         _, jax_out = jax.make_jaxpr(wrapped_fn, return_shape=True)(\n             *maybe_symbolic_args, **maybe_symbolic_kwargs\n\n@@ -72,6 +72,4 @@ def dropout(inputs, rate, noise_shape=None, seed=None):\n     keep_prob = 1.0 - rate\n     mask = jax.random.bernoulli(seed, p=keep_prob, shape=noise_shape)\n     mask = jax.numpy.broadcast_to(mask, inputs.shape)\n-    return jax.lax.select(\n-        mask, inputs / keep_prob, jax.numpy.zeros_like(inputs)\n-    )\n+    return jax.lax.select(mask, inputs / keep_prob, jax.numpy.zeros_like(inputs))\n\n@@ -22,9 +22,7 @@ def draw_seed(seed):\n \n     if isinstance(seed, RandomSeedGenerator):\n         new_seed_value = seed.state.value\n-        seed.state.assign(\n-            seed.state + convert_to_tensor([0, 1], dtype=\"uint32\")\n-        )\n+        seed.state.assign(seed.state + convert_to_tensor([0, 1], dtype=\"uint32\"))\n         return new_seed_value\n     elif isinstance(seed, int):\n         return convert_to_tensor([seed, 0], dtype=\"uint32\")\n\n@@ -128,32 +128,22 @@ class Variable(KerasVariable):\n         return self.value.__rdiv__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __truediv__(self, other):\n-        return self.value.__truediv__(\n-            convert_to_tensor(other, dtype=self.dtype)\n-        )\n+        return self.value.__truediv__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __rtruediv__(self, other):\n-        return self.value.__rtruediv__(\n-            convert_to_tensor(other, dtype=self.dtype)\n-        )\n+        return self.value.__rtruediv__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __floordiv__(self, other):\n-        return self.value.__floordiv__(\n-            convert_to_tensor(other, dtype=self.dtype)\n-        )\n+        return self.value.__floordiv__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __rfloordiv__(self, other):\n-        return self.value.__rfloordiv__(\n-            convert_to_tensor(other, dtype=self.dtype)\n-        )\n+        return self.value.__rfloordiv__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __divmod__(self, other):\n         return self.value.__divmod__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __rdivmod__(self, other):\n-        return self.value.__rdivmod__(\n-            convert_to_tensor(other, dtype=self.dtype)\n-        )\n+        return self.value.__rdivmod__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __mod__(self, other):\n         return self.value.__mod__(convert_to_tensor(other, dtype=self.dtype))\n@@ -171,9 +161,7 @@ class Variable(KerasVariable):\n         return self.value.__matmul__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __rmatmul__(self, other):\n-        return self.value.__rmatmul__(\n-            convert_to_tensor(other, dtype=self.dtype)\n-        )\n+        return self.value.__rmatmul__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __and__(self, other):\n         return self.value.__and__(convert_to_tensor(other, dtype=self.dtype))\n@@ -197,17 +185,13 @@ class Variable(KerasVariable):\n         return self.value.__lshift__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __rlshift__(self, other):\n-        return self.value.__rlshift__(\n-            convert_to_tensor(other, dtype=self.dtype)\n-        )\n+        return self.value.__rlshift__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __rshift__(self, other):\n         return self.value.__rshift__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __rrshift__(self, other):\n-        return self.value.__rrshift__(\n-            convert_to_tensor(other, dtype=self.dtype)\n-        )\n+        return self.value.__rrshift__(convert_to_tensor(other, dtype=self.dtype))\n \n     def __round__(self, ndigits=None):\n         return self.value.__round__(ndigits)\n@@ -248,9 +232,7 @@ def compute_output_spec(fn, *args, **kwargs):\n                 return tf.compat.v1.placeholder(shape=x.shape, dtype=x.dtype)\n             return x\n \n-        args, kwargs = tf.nest.map_structure(\n-            convert_keras_tensor_to_tf, (args, kwargs)\n-        )\n+        args, kwargs = tf.nest.map_structure(convert_keras_tensor_to_tf, (args, kwargs))\n         tf_out = fn(*args, **kwargs)\n \n         def convert_tf_to_keras_tensor(x):\n@@ -265,6 +247,4 @@ def execute(op_name, *args, **kwargs):\n     if hasattr(tfnp, op_name):\n         op = getattr(tfnp, op_name)\n         return op(*args, **kwargs)\n-    raise AttributeError(\n-        f\"The TensorFlow backend does not support op '{op_name}'\"\n-    )\n+    raise AttributeError(f\"The TensorFlow backend does not support op '{op_name}'\")\n\n@@ -2,9 +2,7 @@ import warnings\n from keras_core.api_export import keras_core_export\n \n \n-@keras_core_export(\n-    [\"keras_core.Initializer\", \"keras_core.initializers.Initializer\"]\n-)\n+@keras_core_export([\"keras_core.Initializer\", \"keras_core.initializers.Initializer\"])\n class Initializer:\n     \"\"\"Initializer base class: all Keras initializers inherit from this class.\n \n\n@@ -52,8 +52,7 @@ class VarianceScaling(Initializer):\n     ):\n         if scale <= 0.0:\n             raise ValueError(\n-                \"Argument `scale` must be positive float. \"\n-                f\"Received: scale={scale}\"\n+                \"Argument `scale` must be positive float. \" f\"Received: scale={scale}\"\n             )\n         allowed_modes = {\"fan_in\", \"fan_out\", \"fan_avg\"}\n         if mode not in allowed_modes:\n@@ -156,9 +155,7 @@ class GlorotUniform(VarianceScaling):\n     \"\"\"\n \n     def __init__(self, seed=None):\n-        super().__init__(\n-            scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed=seed\n-        )\n+        super().__init__(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed=seed)\n \n     def get_config(self):\n         return {\"seed\": self.seed}\n@@ -284,9 +281,7 @@ class LecunUniform(VarianceScaling):\n     \"\"\"\n \n     def __init__(self, seed=None):\n-        super().__init__(\n-            scale=1.0, mode=\"fan_in\", distribution=\"uniform\", seed=seed\n-        )\n+        super().__init__(scale=1.0, mode=\"fan_in\", distribution=\"uniform\", seed=seed)\n \n     def get_config(self):\n         return {\"seed\": self.seed}\n@@ -364,9 +359,7 @@ class HeUniform(VarianceScaling):\n     \"\"\"\n \n     def __init__(self, seed=None):\n-        super().__init__(\n-            scale=2.0, mode=\"fan_in\", distribution=\"uniform\", seed=seed\n-        )\n+        super().__init__(scale=2.0, mode=\"fan_in\", distribution=\"uniform\", seed=seed)\n \n     def get_config(self):\n         return {\"seed\": self.seed}\n@@ -398,3 +391,101 @@ def compute_fans(shape):\n         fan_in = shape[-2] * receptive_field_size\n         fan_out = shape[-1] * receptive_field_size\n     return int(fan_in), int(fan_out)\n+\n+\n+class RandomNormal(Initializer):\n+    \"\"\"Random normal initializer.\n+\n+    Draws samples from a normal distribution for given parameters.\n+\n+    Examples:\n+\n+    >>> # Standalone usage:\n+    >>> initializer = RandomNormal(mean=0.0, stddev=1.0)\n+    >>> values = initializer(shape=(2, 2))\n+\n+    >>> # Usage in a Keras layer:\n+    >>> initializer = RandomNormal(mean=0.0, stddev=1.0)\n+    >>> layer = Dense(3, kernel_initializer=initializer)\n+\n+    Args:\n+        mean: A python scalar or a scalar keras tensor. Mean of the random values to\n+            generate.\n+        stddev: A python scalar or a scalar keras tensor. Standard deviation of the\n+           random values to generate.\n+        seed: A Python integer or instance of\n+            `keras_core.backend.RandomSeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.RandomSeedGenerator`.\n+    \"\"\"\n+\n+    def __init__(self, mean=0.0, stddev=1.0, seed=None):\n+        self.mean = mean\n+        self.stddev = stddev\n+        self.seed = seed or random.make_default_seed()\n+        super().__init__()\n+\n+    def __call__(self, shape, dtype=None, **kwargs):\n+        return random.normal(\n+            shape=shape,\n+            mean=self.mean,\n+            stddev=self.stddev,\n+            seed=self.seed,\n+            dtype=dtype\n+        )\n+\n+    def get_config(self):\n+        return {\"mean\": self.mean, \"stddev\":self.stddev, \"seed\":self.seed}\n+\n+\n+class RandomUniform(Initializer):\n+    \"\"\"Random uniform initializer.\n+\n+    Draws samples from a uniform distribution for given parameters.\n+\n+    Examples:\n+\n+    >>> # Standalone usage:\n+    >>> initializer = RandomUniform(minval=0.0, maxval=1.0)\n+    >>> values = initializer(shape=(2, 2))\n+\n+    >>> # Usage in a Keras layer:\n+    >>> initializer = RandomUniform(minval=0.0, maxval=1.0)\n+    >>> layer = Dense(3, kernel_initializer=initializer)\n+\n+    Args:\n+        minval: A python scalar or a scalar keras tensor. Lower bound of the range of\n+            random values to generate (inclusive).\n+        maxval: A python scalar or a scalar keras tensor. Upper bound of the range of\n+            random values to generate (exclusive).\n+        seed: A Python integer or instance of\n+            `keras_core.backend.RandomSeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.RandomSeedGenerator`.\n+    \"\"\"\n+\n+    def __init__(self, minval=0.0, maxval=1.0, seed=None):\n+        self.minval = minval\n+        self.maxval = maxval\n+        self.seed = seed or random.make_default_seed()\n+        super().__init__()\n+\n+    def __call__(self, shape, dtype=None, **kwargs):\n+        return random.uniform(\n+            shape=shape,\n+            minval=self.minval,\n+            maxval=self.maxval,\n+            seed=self.seed,\n+            dtype=dtype\n+        )\n+\n+    def get_config(self):\n+        return {\"minval\": self.minval, \"maxval\":self.maxval, \"seed\":self.seed}\n\n@@ -0,0 +1,49 @@\n+from keras_core import testing\n+from keras_core import initializers\n+import numpy as np\n+\n+\n+class InitializersTest(testing.TestCase):\n+    def test_random_normal(self):\n+        shape = (5, 5)\n+        mean = 0.0\n+        stddev = 1.0\n+        seed = 1234\n+        external_config = {\"mean\": 1.0, \"stddev\": 0.5, \"seed\": 42}\n+        initializer = initializers.RandomNormal(\n+            mean=mean,\n+            stddev=stddev,\n+            seed=seed\n+        )\n+        values = initializer(shape=shape)\n+        self.assertEqual(initializer.mean, mean)\n+        self.assertEqual(initializer.stddev, stddev)\n+        self.assertEqual(initializer.seed, seed)\n+        self.assertEqual(values.shape, shape)\n+        self.test_load_external_config(initializer, external_config)\n+\n+    def test_random_uniform(self):\n+        shape = (5, 5)\n+        minval = -1.0\n+        maxval = 1.0\n+        seed = 1234\n+        external_config = {\"minval\": 0.0, \"maxval\": 1.0, \"seed\": 42}\n+        initializer = initializers.RandomUniform(\n+            minval=minval,\n+            maxval=maxval,\n+            seed=seed\n+        )\n+        values = initializer(shape=shape)\n+        self.assertEqual(initializer.minval, minval)\n+        self.assertEqual(initializer.maxval, maxval)\n+        self.assertEqual(initializer.seed, seed)\n+        self.assertEqual(values.shape, shape)\n+        self.test_load_external_config(initializer, external_config)\n+        values = values.numpy()\n+        self.assertGreaterEqual(np.min(values), minval)\n+        self.assertLess(np.max(values), maxval)\n+\n+    def test_load_external_config(self, initializer, config):\n+        initializer = initializer.from_config(config)\n+        self.assertEqual(initializer.get_config(), config)\n+\n\n@@ -56,9 +56,7 @@ class InputSpec:\n         allow_last_axis_squeeze=False,\n         name=None,\n     ):\n-        self.dtype = (\n-            backend.standardize_dtype(dtype) if dtype is not None else None\n-        )\n+        self.dtype = backend.standardize_dtype(dtype) if dtype is not None else None\n         if shape is not None:\n             self.shape = backend.standardize_shape(shape)\n             self.ndim = len(shape)\n\n@@ -62,8 +62,7 @@ class Layer(Operation):\n                 ),\n                 \"metrics\": (lambda x: isinstance(x, Metric), self._metrics),\n                 \"layers\": (\n-                    lambda x: isinstance(x, Layer)\n-                    and not isinstance(x, Metric),\n+                    lambda x: isinstance(x, Layer) and not isinstance(x, Metric),\n                     self._layers,\n                 ),\n                 # TODO: RandomSeedGenerator tracking\n@@ -252,9 +251,7 @@ class Layer(Operation):\n         # Argument validation and conversion. #\n         # 1. Convert first positional argument to tensor of correct dtype.\n         if args and not isinstance(args[0], KerasTensor):\n-            args = (\n-                nest.map_structure(backend.convert_to_tensor, args[0]),\n-            ) + args[1:]\n+            args = (nest.map_structure(backend.convert_to_tensor, args[0]),) + args[1:]\n \n         # 2. Convert any other array arguments to tensors of correct dtype.\n         def maybe_convert(x):\n@@ -581,12 +578,9 @@ def get_shapes_dict(arguments_dict):\n             shapes_dict[f\"{k}_shape\"] = backend.standardize_shape(v.shape)\n         elif nest.is_nested(v):\n             flat = nest.flatten(v)\n-            if any(\n-                isinstance(x, KerasTensor) or backend.is_tensor(x) for x in flat\n-            ):\n+            if any(isinstance(x, KerasTensor) or backend.is_tensor(x) for x in flat):\n                 if not all(\n-                    isinstance(x, KerasTensor) or backend.is_tensor(x)\n-                    for x in flat\n+                    isinstance(x, KerasTensor) or backend.is_tensor(x) for x in flat\n                 ):\n                     raise ValueError(\n                         \"You cannot mix tensors and non-tensors in a nested argument. \"\n\n@@ -13,9 +13,7 @@ class FunctionTest(testing.TestCase):\n                 return x + 1\n \n         x = keras_tensor.KerasTensor(shape=(2, 3), name=\"x\")\n-        with self.assertRaisesRegex(\n-            ValueError, \"Only input tensors may be passed as\"\n-        ):\n+        with self.assertRaisesRegex(ValueError, \"Only input tensors may be passed as\"):\n             SomeLayer()(x, True)\n \n         # This works\n\n@@ -1,7 +1,6 @@\n from keras_core import operations as ops\n from keras_core import backend\n from keras_core.utils.naming import auto_name\n-from keras_core.utils import dtype_utils\n from keras_core.api_export import keras_core_export\n \n \n@@ -31,10 +30,7 @@ class Loss:\n                 mask = None\n \n             return reduce_weighted_loss(\n-                losses,\n-                sample_weight=sample_weight,\n-                mask=mask,\n-                reduction=self.reduction,\n+                losses, sample_weight=sample_weight, mask=mask, reduction=self.reduction\n             )\n \n     def call(self, y_true, y_pred):\n@@ -75,11 +71,7 @@ def squeeze_to_same_rank(x1, x2):\n \n \n def reduce_loss(losses, reduction=\"sum_over_batch_size\"):\n-    if (\n-        reduction is None\n-        or tuple(losses.shape) == ()\n-        or tuple(losses.shape) == (0,)\n-    ):\n+    if reduction is None or tuple(losses.shape) == () or tuple(losses.shape) == (0,):\n         return losses\n     loss = ops.sum(losses)\n     if reduction == \"sum_over_batch_size\":\n@@ -109,7 +101,7 @@ def reduce_weighted_loss(\n     # Convert any non float dtypes to floats, to avoid loss of precision\n     # for dtype like int or bool.\n     dtype = backend.standardize_dtype(losses.dtype)\n-    if not dtype_utils.is_float(dtype):\n+    if not is_float(dtype):\n         input_dtype = losses.dtype\n         losses = ops.cast(losses, \"float32\")\n         input_casted = True\n@@ -131,6 +123,45 @@ def reduce_weighted_loss(\n     return loss\n \n \n+def float_dtype_size(dtype):\n+    if dtype in (\"bfloat16\", \"float16\"):\n+        return 16\n+    if dtype == \"float32\":\n+        return 32\n+    if dtype == \"float64\":\n+        return 64\n+    raise ValueError(f\"Invalid dtype: {dtype}\")\n+\n+\n+def is_float(dtype):\n+    return \"float\" in dtype\n+\n+\n+def cast_to_common_dtype(tensors):\n+    \"\"\"Cast a list of tensors to a common dtype.\n+\n+    If any tensor is floating-point, they will all be casted to the most-precise\n+    floating-point dtype. Otherwise the tensors are not casted.\n+\n+    Args:\n+        tensors: A list of tensors.\n+\n+    Returns:\n+        Same list, casted to a common dtype.\n+    \"\"\"\n+    highest_float = None\n+    for x in tensors:\n+        dtype = backend.standardize_dtype(x.dtype)\n+        if is_float(dtype):\n+            if highest_float is None or float_dtype_size(dtype) > highest_float:\n+                highest_float = dtype\n+            elif dtype == \"float16\" and highest_float == \"bfloat16\":\n+                highest_float = \"float32\"\n+    if highest_float:\n+        tensors = [ops.cast(x, highest_float) for x in tensors]\n+    return tensors\n+\n+\n def apply_mask(sample_weight, mask, dtype, reduction):\n     \"\"\"Applies any mask on predictions to sample weights.\"\"\"\n     if mask is not None:\n\n@@ -52,9 +52,7 @@ class LossTest(testing.TestCase):\n         loss_fn = ExampleLoss()\n         loss = loss_fn(y_true, y_pred)\n         self.assertEqual(loss.dtype.name, \"float32\")\n-        self.assertAllClose(\n-            np.sum((masked_y_true - masked_y_pred) ** 2) / 3, loss\n-        )\n+        self.assertAllClose(np.sum((masked_y_true - masked_y_pred) ** 2) / 3, loss)\n \n         # Test edge case where everything is masked.\n         mask = np.array([False, False, False, False])\n@@ -71,9 +69,7 @@ class LossTest(testing.TestCase):\n         loss_fn = ExampleLoss()\n         loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n         self.assertEqual(loss.dtype.name, \"float32\")\n-        self.assertAllClose(\n-            np.sum(sample_weight * (y_true - y_pred) ** 2) / 4, loss\n-        )\n+        self.assertAllClose(np.sum(sample_weight * (y_true - y_pred) ** 2) / 4, loss)\n \n         # Test edge case where every weight is 0.\n         sample_weight = np.array([0.0, 0.0, 0.0, 0.0])\n@@ -100,8 +96,7 @@ class LossTest(testing.TestCase):\n         loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n         self.assertEqual(loss.dtype.name, \"float32\")\n         self.assertAllClose(\n-            np.sum(masked_sample_weight * (masked_y_true - masked_y_pred) ** 2)\n-            / 3,\n+            np.sum(masked_sample_weight * (masked_y_true - masked_y_pred) ** 2) / 3,\n             loss,\n         )\n \n@@ -135,10 +130,7 @@ class LossTest(testing.TestCase):\n             loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n             self.assertEqual(loss.dtype.name, \"float32\")\n             self.assertAllClose(\n-                np.sum(\n-                    masked_sample_weight * (masked_y_true - masked_y_pred) ** 2\n-                )\n-                / 3,\n+                np.sum(masked_sample_weight * (masked_y_true - masked_y_pred) ** 2) / 3,\n                 loss,\n             )\n \n\n@@ -4,9 +4,7 @@ from keras_core.losses.loss import squeeze_to_same_rank\n \n \n class LossFunctionWrapper(Loss):\n-    def __init__(\n-        self, fn, reduction=\"sum_over_batch_size\", name=None, **kwargs\n-    ):\n+    def __init__(self, fn, reduction=\"sum_over_batch_size\", name=None, **kwargs):\n         super().__init__(reduction=reduction, name=name)\n         self.fn = fn\n         self._fn_kwargs = kwargs\n@@ -31,7 +29,5 @@ def mean_squared_error(y_true, y_pred):\n \n \n class MeanSquaredError(LossFunctionWrapper):\n-    def __init__(\n-        self, reduction=\"sum_over_batch_size\", name=\"mean_squared_error\"\n-    ):\n+    def __init__(self, reduction=\"sum_over_batch_size\", name=\"mean_squared_error\"):\n         super().__init__(mean_squared_error, reduction=reduction, name=name)\n\n@@ -9,9 +9,7 @@ import numpy as np\n class ExampleMetric(Metric):\n     def __init__(self, name=\"mean_square_error\", dtype=None):\n         super().__init__(name=name, dtype=dtype)\n-        self.sum = self.add_variable(\n-            name=\"sum\", initializer=initializers.Zeros()\n-        )\n+        self.sum = self.add_variable(name=\"sum\", initializer=initializers.Zeros())\n         self.total = self.add_variable(\n             name=\"total\", initializer=initializers.Zeros(), dtype=\"int32\"\n         )\n@@ -50,9 +48,7 @@ class MetricTest(testing.TestCase):\n \n         self.assertAllClose(metric.total, 20)\n         result = metric.result()\n-        self.assertAllClose(\n-            result, np.sum((y_true - y_pred) ** 2) / num_samples\n-        )\n+        self.assertAllClose(result, np.sum((y_true - y_pred) ** 2) / num_samples)\n         metric.reset_state()\n         self.assertEqual(metric.result(), 0.0)\n \n@@ -64,10 +60,7 @@ class MetricTest(testing.TestCase):\n \n         # In dict\n         metric = ExampleMetric(name=\"mse\")\n-        metric.more_vars = {\n-            \"a\": backend.Variable(0.0),\n-            \"b\": backend.Variable(1.0),\n-        }\n+        metric.more_vars = {\"a\": backend.Variable(0.0), \"b\": backend.Variable(1.0)}\n         self.assertEqual(len(metric.variables), 4)\n \n         # In nested structured\n\n@@ -6,12 +6,8 @@ from keras_core import initializers\n class MeanSquareError(Metric):\n     def __init__(self, name=\"mean_square_error\", dtype=None):\n         super().__init__(name=name, dtype=dtype)\n-        self.sum = self.add_variable(\n-            name=\"sum\", initializer=initializers.Zeros()\n-        )\n-        self.total = self.add_variable(\n-            name=\"total\", initializer=initializers.Zeros()\n-        )\n+        self.sum = self.add_variable(name=\"sum\", initializer=initializers.Zeros())\n+        self.total = self.add_variable(name=\"total\", initializer=initializers.Zeros())\n \n     def update_state(self, y_true, y_pred):\n         # TODO: add support for sample_weight\n\n@@ -27,7 +27,6 @@ class Functional(Function, Model):\n             return\n         super().__init__(inputs, outputs, name=name)\n         self._layers = self.layers\n-        self.built = True\n \n     @property\n     def layers(self):\n@@ -60,7 +59,7 @@ class Functional(Function, Model):\n         pass\n \n     def add_loss(self, loss):\n-        # Symbolic only. TODO\n+        # Symbolic only.\n         raise NotImplementedError\n \n \n\n@@ -107,9 +107,7 @@ class Function(Operation):\n \n     def _assert_input_compatibility(self, inputs):\n         try:\n-            nest.assert_same_structure(\n-                inputs, self._inputs_struct, check_types=False\n-            )\n+            nest.assert_same_structure(inputs, self._inputs_struct, check_types=False)\n         except ValueError:\n             raise ValueError(\n                 \"Function was called with an invalid input structure. \"\n\n@@ -14,9 +14,7 @@ class FunctionTest(testing.TestCase):\n         x = knp.add(x1, x2)\n         y1 = x * 3\n         y2 = x**2\n-        fn = function.Function(\n-            inputs=[x1, x2], outputs=[y1, y2], name=\"test_function\"\n-        )\n+        fn = function.Function(inputs=[x1, x2], outputs=[y1, y2], name=\"test_function\")\n         self.assertEqual(fn.name, \"test_function\")\n \n         # Eager call\n@@ -90,9 +88,7 @@ class FunctionTest(testing.TestCase):\n         x = knp.add(x1, x2)\n         y1 = x * 3\n         y2 = x**2\n-        fn = function.Function(\n-            inputs=[x1, x2], outputs=[y1, y2], name=\"test_function\"\n-        )\n+        fn = function.Function(inputs=[x1, x2], outputs=[y1, y2], name=\"test_function\")\n         self.assertEqual(fn.name, \"test_function\")\n \n         # Bad structure\n\n@@ -36,9 +36,7 @@ class Node:\n         outputs: The output tensors of the `op.__call__()` call.\n     \"\"\"\n \n-    def __init__(\n-        self, operation, call_args=None, call_kwargs=None, outputs=None\n-    ):\n+    def __init__(self, operation, call_args=None, call_kwargs=None, outputs=None):\n         self.operation = operation\n         self.arguments = SymbolicArguments(*call_args, **call_kwargs)\n         self.outputs = [] if outputs is None else nest.flatten(outputs)\n@@ -101,9 +99,7 @@ class Node:\n \n \n class KerasHistory(\n-    collections.namedtuple(\n-        \"KerasHistory\", [\"operation\", \"node_index\", \"tensor_index\"]\n-    )\n+    collections.namedtuple(\"KerasHistory\", [\"operation\", \"node_index\", \"tensor_index\"])\n ):\n     \"\"\"Tracks the Operation call that created a Tensor.\n \n\n@@ -285,14 +285,10 @@ class Mean(Operation):\n         self.keepdims = keepdims\n \n     def call(self, x):\n-        return backend.execute(\n-            \"mean\", x, axis=self.axis, keepdims=self.keepdims\n-        )\n+        return backend.execute(\"mean\", x, axis=self.axis, keepdims=self.keepdims)\n \n     def compute_output_spec(self, x):\n-        return compute_np_output_spec(\n-            \"mean\", x, axis=self.axis, keepdims=self.keepdims\n-        )\n+        return compute_np_output_spec(\"mean\", x, axis=self.axis, keepdims=self.keepdims)\n \n \n def mean(x, axis=None, keepdims=False):\n@@ -310,9 +306,7 @@ class Var(Operation):\n         return backend.execute(\"var\", x, axis=self.axis, keepdims=self.keepdims)\n \n     def compute_output_spec(self, x):\n-        return compute_np_output_spec(\n-            \"var\", x, axis=self.axis, keepdims=self.keepdims\n-        )\n+        return compute_np_output_spec(\"var\", x, axis=self.axis, keepdims=self.keepdims)\n \n \n def var(x, axis=None, keepdims=False):\n@@ -330,9 +324,7 @@ class Sum(Operation):\n         return backend.execute(\"sum\", x, axis=self.axis, keepdims=self.keepdims)\n \n     def compute_output_spec(self, x):\n-        return compute_np_output_spec(\n-            \"sum\", x, axis=self.axis, keepdims=self.keepdims\n-        )\n+        return compute_np_output_spec(\"sum\", x, axis=self.axis, keepdims=self.keepdims)\n \n \n def sum(x, axis=None, keepdims=False):\n\n@@ -24,9 +24,7 @@ class Operation:\n         # sets _keras_history on the outputs, and adds itself to the\n         # `_outbound_nodes` of the ops that produced the inputs to this\n         # call.\n-        Node(\n-            operation=self, call_args=args, call_kwargs=kwargs, outputs=outputs\n-        )\n+        Node(operation=self, call_args=args, call_kwargs=kwargs, outputs=outputs)\n         return outputs\n \n     def call(self, *args, **kwargs):\n\n@@ -61,9 +61,7 @@ class Optimizer:\n         self.iterations = backend.Variable(\n             0, name=\"iteration\", dtype=\"int64\", trainable=False\n         )\n-        if isinstance(\n-            learning_rate, learning_rate_schedule.LearningRateSchedule\n-        ):\n+        if isinstance(learning_rate, learning_rate_schedule.LearningRateSchedule):\n             self._learning_rate = learning_rate\n         elif callable(learning_rate):\n             self._learning_rate = learning_rate\n@@ -264,9 +262,7 @@ class Optimizer:\n         return self._get_current_learning_rate()\n \n     def _get_current_learning_rate(self):\n-        if isinstance(\n-            self._learning_rate, learning_rate_schedule.LearningRateSchedule\n-        ):\n+        if isinstance(self._learning_rate, learning_rate_schedule.LearningRateSchedule):\n             return self._learning_rate(self.iterations)\n         elif callable(self._learning_rate):\n             return self._learning_rate(self.iterations)\n@@ -340,17 +336,13 @@ class Optimizer:\n             )\n \n         if var_list:\n-            self._exclude_from_weight_decay = [\n-                id(variable) for variable in var_list\n-            ]\n+            self._exclude_from_weight_decay = [id(variable) for variable in var_list]\n         else:\n             self._exclude_from_weight_decay = []\n         self._exclude_from_weight_decay_names = var_names or []\n \n     def _use_weight_decay(self, variable):\n-        exclude_from_weight_decay = getattr(\n-            self, \"_exclude_from_weight_decay\", []\n-        )\n+        exclude_from_weight_decay = getattr(self, \"_exclude_from_weight_decay\", [])\n         exclude_from_weight_decay_names = getattr(\n             self, \"_exclude_from_weight_decay_names\", []\n         )\n@@ -383,9 +375,7 @@ class Optimizer:\n     def _update_model_variables_moving_average(self, var_list):\n         \"\"\"Update the stored moving average using the latest value.\"\"\"\n         if self.use_ema:\n-            for var, average in zip(\n-                var_list, self._model_variables_moving_average\n-            ):\n+            for var, average in zip(var_list, self._model_variables_moving_average):\n                 average.assign(\n                     self.ema_momentum * average + (1 - self.ema_momentum) * var\n                 )\n@@ -404,9 +394,7 @@ class Optimizer:\n \n     def _overwrite_model_variables_with_average_value_helper(self, var_list):\n         \"\"\"Helper function that overwrites model variables.\"\"\"\n-        for var, average_var in zip(\n-            var_list, self._model_variables_moving_average\n-        ):\n+        for var, average_var in zip(var_list, self._model_variables_moving_average):\n             var.assign(average_var)\n \n     def finalize_variable_values(self, var_list):\n@@ -439,12 +427,8 @@ class Optimizer:\n             Python dictionary.\n         \"\"\"\n \n-        if isinstance(\n-            self._learning_rate, learning_rate_schedule.LearningRateSchedule\n-        ):\n-            learning_rate = learning_rate_schedule.serialize(\n-                self._learning_rate\n-            )\n+        if isinstance(self._learning_rate, learning_rate_schedule.LearningRateSchedule):\n+            learning_rate = learning_rate_schedule.serialize(self._learning_rate)\n         elif isinstance(self._learning_rate, backend.Variable):\n             learning_rate = float(self._learning_rate.numpy())\n         elif ops.is_tensor(self._learning_rate):\n\n@@ -85,9 +85,7 @@ class SGD(optimizer.Optimizer):\n         self.momentums = []\n         for variable in variables:\n             self.momentums.append(\n-                self.add_variable_from_reference(\n-                    reference_variable=variable, name=\"m\"\n-                )\n+                self.add_variable_from_reference(reference_variable=variable, name=\"m\")\n             )\n \n     def update_step(self, gradient, variable, learning_rate):\n@@ -100,9 +98,7 @@ class SGD(optimizer.Optimizer):\n         if m is not None:\n             m.assign(-gradient * learning_rate + m * momentum)\n             if self.nesterov:\n-                variable.assign(\n-                    variable - gradient * learning_rate + m * momentum\n-                )\n+                variable.assign(variable - gradient * learning_rate + m * momentum)\n             else:\n                 variable.assign(variable + m)\n         else:\n\n@@ -3,9 +3,7 @@ from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n \n \n-@keras_core_export(\n-    [\"keras_core.Regularizer\", \"keras_core.regularizers.Regularizer\"]\n-)\n+@keras_core_export([\"keras_core.Regularizer\", \"keras_core.regularizers.Regularizer\"])\n class Regularizer:\n     \"\"\"Regularizer base class.\n \n@@ -317,15 +315,10 @@ class OrthogonalRegularizer(Regularizer):\n             inputs = l2_normalize(inputs, axis=0)\n             product = ops.matmul(ops.transpose(inputs), inputs)\n             size = inputs.shape[1]\n-        product_no_diagonal = product * (\n-            1.0 - ops.eye(size, dtype=inputs.dtype)\n-        )\n+        product_no_diagonal = product * (1.0 - ops.eye(size, dtype=inputs.dtype))\n         num_pairs = size * (size - 1.0) / 2.0\n         return (\n-            self.factor\n-            * 0.5\n-            * ops.sum(ops.absolute(product_no_diagonal))\n-            / num_pairs\n+            self.factor * 0.5 * ops.sum(ops.absolute(product_no_diagonal)) / num_pairs\n         )\n \n     def get_config(self):\n@@ -334,9 +327,7 @@ class OrthogonalRegularizer(Regularizer):\n \n def validate_float_arg(value, name):\n     \"\"\"check penalty number availability, raise ValueError if failed.\"\"\"\n-    if not isinstance(value, (float, int)) or (\n-        math.isinf(value) or math.isnan(value)\n-    ):\n+    if not isinstance(value, (float, int)) or (math.isinf(value) or math.isnan(value)):\n         raise ValueError(\n             f\"Invalid value for argument {name}: expected a float. \"\n             f\"Received: {name}={value}\"\n\n@@ -1,41 +0,0 @@\n-from keras_core import backend\n-from keras_core import operations as ops\n-\n-\n-def float_dtype_size(dtype):\n-    if dtype in (\"bfloat16\", \"float16\"):\n-        return 16\n-    if dtype == \"float32\":\n-        return 32\n-    if dtype == \"float64\":\n-        return 64\n-    raise ValueError(f\"Invalid dtype: {dtype}\")\n-\n-\n-def is_float(dtype):\n-    return \"float\" in dtype\n-\n-\n-def cast_to_common_dtype(tensors):\n-    \"\"\"Cast a list of tensors to a common dtype.\n-\n-    If any tensor is floating-point, they will all be casted to the most-precise\n-    floating-point dtype. Otherwise the tensors are not casted.\n-\n-    Args:\n-        tensors: A list of tensors.\n-\n-    Returns:\n-        Same list, casted to a common dtype.\n-    \"\"\"\n-    highest_float = None\n-    for x in tensors:\n-        dtype = backend.standardize_dtype(x.dtype)\n-        if is_float(dtype):\n-            if highest_float is None or float_dtype_size(dtype) > highest_float:\n-                highest_float = dtype\n-            elif dtype == \"float16\" and highest_float == \"bfloat16\":\n-                highest_float = \"float32\"\n-    if highest_float:\n-        tensors = [ops.cast(x, highest_float) for x in tensors]\n-    return tensors\n\n@@ -42,7 +42,9 @@ def is_interactive_logging_enabled():\n     \"\"\"\n     # Use `getattr` in case `INTERACTIVE_LOGGING`\n     # does not have the `enable` attribute.\n-    return getattr(INTERACTIVE_LOGGING, \"enable\", True)\n+    return getattr(\n+        INTERACTIVE_LOGGING, \"enable\", True\n+    )\n \n \n def print_msg(message, line_break=True):\n\n@@ -1,48 +1,13 @@\n from tensorflow import nest\n-from keras_core import backend\n-from keras_core.utils import io_utils\n-from keras_core.utils import dtype_utils\n-from keras_core.utils import text_rendering\n+from keras_core.backend import Variable\n import math\n-import re\n \n \n def count_params(weights):\n-    shapes = [v.shape for v in weights]\n+    shapes = [v.shape for v in weights if isinstance(v, Variable)]\n     return int(sum(math.prod(p) for p in shapes))\n \n \n-def weight_memory_size(weights):\n-    \"\"\"Compute the memory footprint for weights based on their dtypes.\n-\n-    Args:\n-        weights: An iterable contains the weights to compute weight size.\n-\n-    Returns:\n-        The total memory size (in Bytes) of the weights.\n-    \"\"\"\n-    unique_weights = set(weights)\n-    total_memory_size = 0\n-    for w in unique_weights:\n-        weight_shape = math.prod(w.shape)\n-        dtype = backend.standardize_dtype(w.dtype)\n-        per_param_size = dtype_utils.float_dtype_size(dtype)\n-        total_memory_size += weight_shape * per_param_size\n-    return total_memory_size\n-\n-\n-def readable_memory_size(weight_memory_size):\n-    \"\"\"Convert the weight memory size (Bytes) to a readable string.\"\"\"\n-    units = [\"Byte\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n-    scale = 1024\n-    for unit in units:\n-        if weight_memory_size / scale < 1:\n-            return \"{:.2f} {}\".format(weight_memory_size, unit)\n-        else:\n-            weight_memory_size /= scale\n-    return \"{:.2f} {}\".format(weight_memory_size, units[-1])\n-\n-\n def print_summary(\n     model,\n     line_length=None,\n@@ -121,13 +86,17 @@ def print_summary(\n     if sequential_like:\n         line_length = line_length or 65\n         positions = positions or [0.45, 0.85, 1.0]\n+        if positions[-1] <= 1:\n+            positions = [int(line_length * p) for p in positions]\n         # header names for the different log elements\n-        header = [\"Layer (type)\", \"Output Shape\", \"Param #\"]\n+        to_display = [\"Layer (type)\", \"Output Shape\", \"Param #\"]\n     else:\n         line_length = line_length or 98\n         positions = positions or [0.3, 0.6, 0.70, 1.0]\n+        if positions[-1] <= 1:\n+            positions = [int(line_length * p) for p in positions]\n         # header names for the different log elements\n-        header = [\"Layer (type)\", \"Output Shape\", \"Param #\", \"Connected to\"]\n+        to_display = [\"Layer (type)\", \"Output Shape\", \"Param #\", \"Connected to\"]\n         relevant_nodes = []\n         for v in model._nodes_by_depth.values():\n             relevant_nodes += v\n@@ -135,14 +104,63 @@ def print_summary(\n     if show_trainable:\n         line_length += 11\n         positions.append(line_length)\n-        header.append(\"Trainable\")\n+        to_display.append(\"Trainable\")\n \n     layer_range = get_layer_index_bound_by_layer_name(model, layer_range)\n \n-    print_fn(f'Model: \"{model.name}\"')\n-    rows = []\n+    def print_row(fields, positions, nested_level=0):\n+        left_to_print = [str(x) for x in fields]\n+        while any(left_to_print):\n+            line = \"\"\n+            for col in range(len(left_to_print)):\n+                if col > 0:\n+                    start_pos = positions[col - 1]\n+                else:\n+                    start_pos = 0\n+                end_pos = positions[col]\n+                # Leave room for 2 spaces to delineate columns\n+                # we don't need any if we are printing the last column\n+                space = 2 if col != len(positions) - 1 else 0\n+                cutoff = end_pos - start_pos - space\n+                # Except for last col, offset by one to align the start of col\n+                if col != len(positions) - 1:\n+                    cutoff -= 1\n+                if col == 0:\n+                    cutoff -= nested_level\n+                fit_into_line = left_to_print[col][:cutoff]\n+                # For nicer formatting we line-break on seeing end of\n+                # tuple/dict etc.\n+                line_break_conditions = (\"),\", \"},\", \"],\", \"',\")\n+                candidate_cutoffs = [\n+                    fit_into_line.find(x) + len(x)\n+                    for x in line_break_conditions\n+                    if fit_into_line.find(x) >= 0\n+                ]\n+                if candidate_cutoffs:\n+                    cutoff = min(candidate_cutoffs)\n+                    fit_into_line = fit_into_line[:cutoff]\n \n-    def print_layer_summary(layer, prefix=\" \"):\n+                if col == 0:\n+                    line += \"|\" * nested_level + \" \"\n+                line += fit_into_line\n+                line += \" \" * space if space else \"\"\n+                left_to_print[col] = left_to_print[col][cutoff:]\n+\n+                # Pad out to the next position\n+                # Make space for nested_level for last column\n+                if nested_level and col == len(positions) - 1:\n+                    line += \" \" * (positions[col] - len(line) - nested_level)\n+                else:\n+                    line += \" \" * (positions[col] - len(line))\n+            line += \"|\" * nested_level\n+            print_fn(line)\n+\n+    print_fn(f'Model: \"{model.name}\"')\n+    print_fn(\"_\" * line_length)\n+    print_row(to_display, positions)\n+    print_fn(\"=\" * line_length)\n+\n+    def print_layer_summary(layer, nested_level=0):\n         \"\"\"Prints a summary for a single layer.\n \n         Args:\n@@ -156,9 +174,9 @@ def print_summary(\n             output_shape = \"multiple\"\n         except RuntimeError:  # output_shape unknown in Eager mode.\n             output_shape = \"?\"\n-        name = prefix + layer.name\n+        name = layer.name\n         cls_name = layer.__class__.__name__\n-        if not layer.built:\n+        if not layer.built and not getattr(layer, \"_is_graph_network\", False):\n             # If a subclassed model has a layer that is not called in\n             # Model.call, the layer will not be built and we cannot call\n             # layer.count_params().\n@@ -169,9 +187,10 @@ def print_summary(\n \n         if show_trainable:\n             fields.append(\"Y\" if layer.trainable else \"N\")\n-        rows.append(fields)\n \n-    def print_layer_summary_with_connections(layer, prefix=\"\"):\n+        print_row(fields, positions, nested_level)\n+\n+    def print_layer_summary_with_connections(layer, nested_level=0):\n         \"\"\"Prints a summary for a single layer (including its connections).\n \n         Args:\n@@ -188,15 +207,18 @@ def print_summary(\n             if relevant_nodes and node not in relevant_nodes:\n                 # node is not part of the current network\n                 continue\n-            for kt in node.keras_inputs:\n-                keras_history = kt._keras_history\n-                inbound_layer = keras_history.layer\n-                node_index = keras_history.node_index\n-                tensor_index = keras_history.tensor_index\n+\n+            for (\n+                inbound_layer,\n+                node_index,\n+                tensor_index,\n+                _,\n+            ) in node.iterate_inbound():\n                 connections.append(\n                     f\"{inbound_layer.name}[{node_index}][{tensor_index}]\"\n                 )\n-        name = prefix + layer.name\n+\n+        name = layer.name\n         cls_name = layer.__class__.__name__\n         fields = [\n             name + \" (\" + cls_name + \")\",\n@@ -204,38 +226,49 @@ def print_summary(\n             layer.count_params(),\n             connections,\n         ]\n+\n         if show_trainable:\n             fields.append(\"Y\" if layer.trainable else \"N\")\n-        rows.append(fields)\n \n-    def print_layer(layer, nested_level=0):\n+        print_row(fields, positions, nested_level)\n+\n+    def print_layer(layer, nested_level=0, is_nested_last=False):\n         if sequential_like:\n-            print_layer_summary(layer, prefix=\">>>\" * nested_level + \" \")\n+            print_layer_summary(layer, nested_level)\n         else:\n-            print_layer_summary_with_connections(\n-                layer, prefix=\">>>\" * nested_level + \" \"\n-            )\n+            print_layer_summary_with_connections(layer, nested_level)\n \n         if expand_nested and hasattr(layer, \"layers\") and layer.layers:\n-            nested_layers = layer.layers\n-            nested_level += 1\n-            for i in range(len(nested_layers)):\n-                print_layer(nested_layers[i], nested_level=nested_level)\n+            print_fn(\n+                \"|\" * (nested_level + 1)\n+                + \"¯\" * (line_length - 2 * nested_level - 2)\n+                + \"|\" * (nested_level + 1)\n+            )\n+\n+            nested_layer = layer.layers\n+            is_nested_last = False\n+            for i in range(len(nested_layer)):\n+                if i == len(nested_layer) - 1:\n+                    is_nested_last = True\n+                print_layer(nested_layer[i], nested_level + 1, is_nested_last)\n+\n+            print_fn(\n+                \"|\" * nested_level\n+                + \"¯\" * (line_length - 2 * nested_level)\n+                + \"|\" * nested_level\n+            )\n+\n+        if not is_nested_last:\n+            print_fn(\n+                \"|\" * nested_level\n+                + \" \" * (line_length - 2 * nested_level)\n+                + \"|\" * nested_level\n+            )\n \n     for layer in model.layers[layer_range[0] : layer_range[1]]:\n         print_layer(layer)\n+    print_fn(\"=\" * line_length)\n \n-    # Render summary as a table.\n-    table = text_rendering.Table(\n-        header=header,\n-        rows=rows,\n-        positions=positions,\n-        # Left align layer name, center-align everything else\n-        alignments=[\"left\"] + [\"center\" for _ in range(len(header) - 1)],\n-    )\n-    print_fn(table.make())\n-\n-    # After the table, append information about parameter count and  size.\n     if hasattr(model, \"_collected_trainable_weights\"):\n         trainable_count = count_params(model._collected_trainable_weights)\n         trainable_memory_size = weight_memory_size(\n@@ -262,57 +295,6 @@ def print_summary(\n         f\"Non-trainable params: {non_trainable_count} \"\n         f\"({readable_memory_size(non_trainable_memory_size)})\"\n     )\n+    print_fn(\"_\" * line_length)\n \n-\n-def get_layer_index_bound_by_layer_name(model, layer_range=None):\n-    \"\"\"Get the layer indexes from the model based on layer names.\n-\n-    The layer indexes can be used to slice the model into sub models for\n-    display.\n-\n-    Args:\n-        model: `Model` instance.\n-        layer_names: a list or tuple of 2 strings, the starting layer name and\n-            ending layer name (both inclusive) for the result. All layers will\n-            be included when `None` is provided.\n-\n-    Returns:\n-        The index value of layer based on its unique name (layer_names).\n-        Output will be [first_layer_index, last_layer_index + 1].\n-    \"\"\"\n-    if layer_range is not None:\n-        if len(layer_range) != 2:\n-            raise ValueError(\n-                \"layer_range must be a list or tuple of length 2. Received: \"\n-                f\"layer_range = {layer_range} of length {len(layer_range)}\"\n-            )\n-        if not isinstance(layer_range[0], str) or not isinstance(\n-            layer_range[1], str\n-        ):\n-            raise ValueError(\n-                \"layer_range should contain string type only. \"\n-                f\"Received: {layer_range}\"\n-            )\n-    else:\n-        return [0, len(model.layers)]\n-\n-    lower_index = [\n-        idx\n-        for idx, layer in enumerate(model.layers)\n-        if re.match(layer_range[0], layer.name)\n-    ]\n-    upper_index = [\n-        idx\n-        for idx, layer in enumerate(model.layers)\n-        if re.match(layer_range[1], layer.name)\n-    ]\n-\n-    if not lower_index or not upper_index:\n-        raise ValueError(\n-            \"Passed layer_names do not match the layer names in the model. \"\n-            f\"Received: {layer_range}\"\n-        )\n-\n-    if min(lower_index) > max(upper_index):\n-        return [min(upper_index), max(lower_index) + 1]\n-    return [min(lower_index), max(upper_index) + 1]\n+    print_dtensor_variable_summary(model, print_fn, line_length)\n\n@@ -1,126 +0,0 @@\n-import shutil\n-\n-\n-class Table:\n-    def __init__(\n-        self, header, rows, positions, alignments=None, max_line_length=80\n-    ):\n-        if len(header) != len(positions):\n-            raise ValueError(\"header and positions should be the same length.\")\n-        if not all(p <= 1.0 for p in positions):\n-            raise ValueError(\"All positions should be <= 1.\")\n-        self.alignments = alignments or [\"center\" for _ in header]\n-        if len(self.alignments) != len(header):\n-            raise ValueError(\"header and alignments should be the same length.\")\n-        last_p = 0.0\n-        for p in positions:\n-            if p <= last_p:\n-                raise ValueError(\n-                    \"All consecutive positions should be greater than the last.\"\n-                )\n-            last_p = p\n-        self.header = header\n-        self.rows = rows\n-\n-        # Compute columns widths\n-        line_length = min(\n-            max_line_length, shutil.get_terminal_size().columns - 4\n-        )\n-        column_widths = []\n-        current = 0\n-        for pos in positions:\n-            width = int(pos * line_length) - current\n-            if width < 4:\n-                raise ValueError(\"Insufficient console width to print summary.\")\n-            column_widths.append(width)\n-            current += width\n-        self.column_widths = column_widths\n-\n-    def make_separator(self, left, mid, right, horizontal):\n-        line = mid.join(horizontal * width for width in self.column_widths)\n-        return f\"{left}{line}{right}\"\n-\n-    def print_row(\n-        self,\n-        fields,\n-        vertical_separator=\"│\",\n-        alignments=None,\n-    ):\n-        alignments = alignments or [\"center\" for _ in fields]\n-        lines = []\n-        line_break_conditions = (\"),\", \"},\", \"],\", \"',\", \") \")\n-        for field, width in zip(fields, self.column_widths):\n-            buffered_width = width - 1\n-            if len(field) < buffered_width and not \"\\n\" in field:\n-                lines.append([field])\n-                continue\n-            subfields = []\n-            while len(field) >= buffered_width or \"\\n\" in field:\n-                if \"\\n\" in field[:buffered_width]:\n-                    # priority: break on line break\n-                    cutoff = field.find(\"\\n\")\n-                    subfield = field[:cutoff]\n-                    field = field[cutoff + 1 :]\n-                    subfields.append(subfield)\n-                    continue\n-                # secondary: break on certain characters\n-                candidate_cutoffs = [\n-                    field.find(x) + len(x)\n-                    for x in line_break_conditions\n-                    if 0 < field.find(x) < buffered_width\n-                ]\n-                if candidate_cutoffs:\n-                    cutoff = min(buffered_width - 1, *candidate_cutoffs)\n-                else:\n-                    cutoff = buffered_width - 1\n-                subfield = field[:cutoff]\n-                field = field[cutoff:]\n-                subfields.append(subfield)\n-            if field:\n-                subfields.append(field)\n-            lines.append(subfields)\n-\n-        max_subfield_count = max(len(subs) for subs in lines)\n-        rendered_lines = []\n-        for i in range(max_subfield_count):\n-            fields = []\n-            for subfields in lines:\n-                if len(subfields) < i + 1:\n-                    field = \"\"\n-                else:\n-                    field = subfields[i]\n-                fields.append(field)\n-            line = vertical_separator.join(\n-                self.align_field(field, width, alignment)\n-                for field, width, alignment in zip(\n-                    fields, self.column_widths, alignments\n-                )\n-            )\n-            line = f\"{vertical_separator}{line}{vertical_separator}\"\n-            rendered_lines.append(line)\n-        return \"\\n\".join(rendered_lines)\n-\n-    @staticmethod\n-    def align_field(field, width, alignment):\n-        if alignment == \"center\":\n-            return field.center(width)\n-        if alignment == \"left\":\n-            return field.ljust(width)\n-        if alignment == \"right\":\n-            return field.rjust(width)\n-\n-    def make(self):\n-        lines = []\n-        # Print header\n-        lines.append(self.make_separator(*\"┏┳┓━\"))\n-        lines.append(self.print_row(self.header, vertical_separator=\"┃\"))\n-        lines.append(self.make_separator(*\"┡╇┩━\"))\n-\n-        # Print rows\n-        for i, row in enumerate(self.rows):\n-            lines.append(self.print_row(row, alignments=self.alignments))\n-            if i < len(self.rows) - 1:\n-                lines.append(self.make_separator(*\"├┼┤─\"))\n-\n-        lines.append(self.make_separator(*\"└┴┘─\"))\n-        return \"\\n\".join(lines)\n\n@@ -32,9 +32,7 @@ class MiniDropout(Layer):\n         self.seed_generator = backend.random.RandomSeedGenerator(1337)\n \n     def call(self, inputs):\n-        return backend.random.dropout(\n-            inputs, self.rate, seed=self.seed_generator\n-        )\n+        return backend.random.dropout(inputs, self.rate, seed=self.seed_generator)\n \n \n class MiniBatchNorm(Layer):\n@@ -45,9 +43,7 @@ class MiniBatchNorm(Layer):\n \n     def build(self, input_shape):\n         shape = (input_shape[-1],)\n-        self.mean = backend.Variable(\n-            initializers.Zeros()(shape), trainable=False\n-        )\n+        self.mean = backend.Variable(initializers.Zeros()(shape), trainable=False)\n         self.variance = backend.Variable(\n             initializers.GlorotUniform()(shape), trainable=False\n         )\n@@ -62,9 +58,7 @@ class MiniBatchNorm(Layer):\n             self.variance.assign(\n                 self.variance * self.momentum + variance * (1.0 - self.momentum)\n             )\n-            self.mean.assign(\n-                self.mean * self.momentum + mean * (1.0 - self.momentum)\n-            )\n+            self.mean.assign(self.mean * self.momentum + mean * (1.0 - self.momentum))\n         else:\n             outputs = (inputs - self.mean) / (self.variance + self.epsilon)\n         outputs *= self.gamma\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
