{"custom_id": "keras#7597b7707169d0aac4b9c220ff4342858f744fd9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 133 | Files Changed: 22 | Hunks: 49 | Methods Changed: 3 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 138 | Churn Cumulative: 8706 | Contributors (this commit): 21 | Commits (past 90d): 244 | Contributors (cumulative): 120 | DMM Complexity: 0.7333333333333333\n\nDIFF:\n@@ -31,11 +31,7 @@ class KerasTensor:\n     def __init__(self, shape, dtype=\"float32\", record_history=True, name=None):\n         from keras_core import backend\n \n-        shape = backend.standardize_shape(\n-            shape,\n-            allow_all_dynamic=backend.DYNAMIC_SHAPES_OK,\n-        )\n-        self.shape = shape\n+        self.shape = backend.standardize_shape(shape)\n         self.dtype = backend.standardize_dtype(dtype)\n         self.name = name or auto_name(self.__class__.__name__)\n         self.record_history = record_history\n\n@@ -416,9 +416,7 @@ def standardize_dtype(dtype):\n     return dtype\n \n \n-def standardize_shape(\n-    shape, allow_dynamic_batch_size=True, allow_all_dynamic=True\n-):\n+def standardize_shape(shape):\n     if not isinstance(shape, tuple):\n         if shape is None:\n             raise ValueError(\"Undefined shapes are not supported.\")\n@@ -431,23 +429,14 @@ def standardize_shape(\n         # either int or `None`\n         shape = tuple(map(lambda x: int(x) if x is not None else None, shape))\n \n-    for i, e in enumerate(shape):\n-        if i == 0 and allow_dynamic_batch_size and e is None:\n-            continue\n-        if allow_all_dynamic and e is None:\n+    for e in shape:\n+        if e is None:\n             continue\n         if not isinstance(e, int):\n-            msg = (\n+            raise ValueError(\n                 f\"Cannot convert '{shape}' to a shape. \"\n                 f\"Found invalid entry '{e}'. \"\n             )\n-            if not allow_dynamic_batch_size and e is None:\n-                msg += (\n-                    \"Dynamic shapes (shapes with `None` entries) \"\n-                    f\"are not allowed with the {config.backend()} \"\n-                    \"backend.\"\n-                )\n-            raise ValueError(msg)\n         if e < 0:\n             raise ValueError(\n                 f\"Cannot convert '{shape}' to a shape. \"\n\n@@ -4,7 +4,6 @@ from keras_core.backend.jax import math\n from keras_core.backend.jax import nn\n from keras_core.backend.jax import numpy\n from keras_core.backend.jax import random\n-from keras_core.backend.jax.core import DYNAMIC_SHAPES_OK\n from keras_core.backend.jax.core import Variable\n from keras_core.backend.jax.core import cast\n from keras_core.backend.jax.core import compute_output_spec\n\n@@ -13,8 +13,6 @@ from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.backend.jax import distribution\n from keras_core.utils.nest import pack_sequence_as\n \n-DYNAMIC_SHAPES_OK = True\n-\n \n class Variable(KerasVariable):\n     def _initialize(self, value):\n\n@@ -4,7 +4,6 @@ from keras_core.backend.numpy import math\n from keras_core.backend.numpy import nn\n from keras_core.backend.numpy import numpy\n from keras_core.backend.numpy import random\n-from keras_core.backend.numpy.core import DYNAMIC_SHAPES_OK\n from keras_core.backend.numpy.core import Variable\n from keras_core.backend.numpy.core import cast\n from keras_core.backend.numpy.core import compute_output_spec\n\n@@ -6,8 +6,6 @@ from keras_core.backend.common import standardize_dtype\n from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.backend.common.stateless_scope import StatelessScope\n \n-DYNAMIC_SHAPES_OK = True\n-\n \n class Variable(KerasVariable):\n     def _initialize(self, value):\n\n@@ -5,7 +5,6 @@ from keras_core.backend.tensorflow import nn\n from keras_core.backend.tensorflow import numpy\n from keras_core.backend.tensorflow import random\n from keras_core.backend.tensorflow import tensorboard\n-from keras_core.backend.tensorflow.core import DYNAMIC_SHAPES_OK\n from keras_core.backend.tensorflow.core import Variable\n from keras_core.backend.tensorflow.core import cast\n from keras_core.backend.tensorflow.core import compute_output_spec\n\n@@ -12,8 +12,6 @@ from keras_core.backend.common.name_scope import name_scope as base_name_scope\n from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.utils.naming import auto_name\n \n-DYNAMIC_SHAPES_OK = True\n-\n \n class Variable(\n     KerasVariable,\n\n@@ -20,7 +20,6 @@ from keras_core.backend.torch import math\n from keras_core.backend.torch import nn\n from keras_core.backend.torch import numpy\n from keras_core.backend.torch import random\n-from keras_core.backend.torch.core import DYNAMIC_SHAPES_OK\n from keras_core.backend.torch.core import Variable\n from keras_core.backend.torch.core import cast\n from keras_core.backend.torch.core import compute_output_spec\n\n@@ -12,7 +12,6 @@ from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.utils.nest import pack_sequence_as\n \n-DYNAMIC_SHAPES_OK = True\n # Some operators such as 'aten::_foreach_mul_.Scalar'\n # are not currently implemented for the MPS device.\n # check https://github.com/pytorch/pytorch/issues/77764.\n\n@@ -23,10 +23,6 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes.\",\n-    )\n     def test_add_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -118,10 +114,6 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes.\",\n-    )\n     def test_subtract_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -222,10 +214,6 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes.\",\n-    )\n     def test_minimum_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -317,10 +305,6 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes.\",\n-    )\n     def test_maximum_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -412,10 +396,6 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes.\",\n-    )\n     def test_multiply_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -507,10 +487,6 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes.\",\n-    )\n     def test_average_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -602,10 +578,6 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes.\",\n-    )\n     def test_concatenate_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -701,10 +673,6 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=None,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes.\",\n-    )\n     def test_dot_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4)\n         x2 = np.random.rand(2, 4)\n\n@@ -2,7 +2,6 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -63,10 +62,6 @@ class Cropping2DTest(testing.TestCase, parameterized.TestCase):\n             expected_output=expected_output,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_cropping_2d_with_dynamic_spatial_dim(self):\n         input_layer = layers.Input(batch_shape=(1, 7, None, 5))\n         cropped = layers.Cropping2D(((1, 2), (3, 4)))(input_layer)\n\n@@ -2,7 +2,6 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -123,10 +122,6 @@ class Cropping3DTest(testing.TestCase, parameterized.TestCase):\n             expected_output=expected_output,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_cropping_3d_with_dynamic_spatial_dim(self):\n         input_layer = layers.Input(batch_shape=(1, 7, None, 13, 5))\n         cropped = layers.Cropping3D(((1, 2), (3, 4), (5, 6)))(input_layer)\n\n@@ -1,7 +1,6 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -67,19 +66,11 @@ class FlattenTest(testing.TestCase):\n             expected_output=expected_output,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_flatten_with_dynamic_batch_size(self):\n         input_layer = layers.Input(batch_shape=(None, 2, 3))\n         flattened = layers.Flatten()(input_layer)\n         self.assertEqual(flattened.shape, (None, 2 * 3))\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_flatten_with_dynamic_dimension(self):\n         input_layer = layers.Input(batch_shape=(5, 2, None))\n         flattened = layers.Flatten()(input_layer)\n\n@@ -1,7 +1,6 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -21,10 +20,6 @@ class PermuteTest(testing.TestCase):\n             expected_output=expected_output,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_permute_with_dynamic_batch_size(self):\n         input_layer = layers.Input(batch_shape=(None, 3, 5))\n         permuted = layers.Permute((2, 1))(input_layer)\n\n@@ -1,7 +1,6 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -21,19 +20,11 @@ class FlattenTest(testing.TestCase):\n             expected_output=expected_output,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_repeat_vector_with_dynamic_batch_size(self):\n         input_layer = layers.Input(batch_shape=(None, 5))\n         repeated = layers.RepeatVector(n=3)(input_layer)\n         self.assertEqual(repeated.shape, (None, 3, 5))\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_repeat_vector_with_dynamic_dimension(self):\n         input_layer = layers.Input(batch_shape=(2, None))\n         repeated = layers.RepeatVector(n=3)(input_layer)\n\n@@ -1,6 +1,5 @@\n import pytest\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -56,19 +55,11 @@ class ReshapeTest(testing.TestCase):\n             expected_output_shape=(3, 2, 4),\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_reshape_with_dynamic_batch_size(self):\n         input_layer = layers.Input(shape=(2, 4))\n         reshaped = layers.Reshape((8,))(input_layer)\n         self.assertEqual(reshaped.shape, (None, 8))\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_reshape_sets_static_shape(self):\n         input_layer = layers.Input(batch_shape=(2, None))\n         reshaped = layers.Reshape((3, 5))(input_layer)\n\n@@ -1,7 +1,6 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n from keras_core.backend.common.keras_tensor import KerasTensor\n@@ -54,10 +53,6 @@ class UpSamplingTest(testing.TestCase):\n         self.assertEqual(layers.UpSampling1D(size=2)(x).shape, (None, 4, 3))\n         self.assertEqual(layers.UpSampling1D(size=4)(x).shape, (None, 8, 3))\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_upsampling_1d_with_dynamic_shape(self):\n         y = KerasTensor([2, None, 3])\n         self.assertEqual(layers.UpSampling1D(size=2)(y).shape, (2, None, 3))\n\n@@ -1,8 +1,6 @@\n import numpy as np\n-import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -25,10 +23,6 @@ class ZeroPadding1DTest(testing.TestCase, parameterized.TestCase):\n             self.assertAllClose(outputs[:, index, :], 0.0)\n         self.assertAllClose(outputs[:, 2:-2, :], inputs)\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_zero_padding_1d_with_dynamic_spatial_dim(self):\n         input_layer = layers.Input(batch_shape=(1, None, 3))\n         padded = layers.ZeroPadding1D((1, 2))(input_layer)\n\n@@ -1,8 +1,6 @@\n import numpy as np\n-import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -58,10 +56,6 @@ class ZeroPadding2DTest(testing.TestCase, parameterized.TestCase):\n                 self.assertAllClose(outputs[:, :, index, :], 0.0)\n             self.assertAllClose(outputs[:, 2:-2, 2:-2, :], inputs)\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_zero_padding_2d_with_dynamic_spatial_dim(self):\n         input_layer = layers.Input(batch_shape=(1, 2, None, 4))\n         padded = layers.ZeroPadding2D(((1, 2), (3, 4)))(input_layer)\n\n@@ -1,8 +1,6 @@\n import numpy as np\n-import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -64,10 +62,6 @@ class ZeroPadding3DTest(testing.TestCase, parameterized.TestCase):\n                 self.assertAllClose(outputs[:, :, :, index, :], 0.0)\n             self.assertAllClose(outputs[:, 2:-2, 2:-2, 2:-2, :], inputs)\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic batch sizes\",\n-    )\n     def test_zero_padding_3d_with_dynamic_spatial_dim(self):\n         input_layer = layers.Input(batch_shape=(1, 2, None, 4, 5))\n         padded = layers.ZeroPadding3D(((1, 2), (3, 4), (5, 6)))(input_layer)\n\n@@ -1,7 +1,6 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -206,10 +205,6 @@ class RNNTest(testing.TestCase):\n         self.assertEqual(output_shape[1], (3, 8))\n         self.assertEqual(output_shape[2], (3, 8))\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_dynamic_shapes(self):\n         sequence_shape = (None, None, 3)\n         layer = layers.RNN(OneStateRNNCell(8), return_sequences=False)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#07d3a0d86007117c1616597cd0c1abc05c290fbe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 459 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -219,7 +219,10 @@ class HashedCrossing(Layer):\n                 \"All `HashedCrossing` inputs should be dense tensors. \"\n                 f\"Received: inputs={inputs}\"\n             )\n-        if not all(tf.as_dtype(x.dtype).is_integer or x.dtype == tf.string for x in inputs):\n+        if not all(\n+            tf.as_dtype(x.dtype).is_integer or x.dtype == tf.string\n+            for x in inputs\n+        ):\n             raise ValueError(\n                 \"All `HashedCrossing` inputs should have an integer or \"\n                 f\"string dtype. Received: inputs={inputs}\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#399753e167a817c7bdde186503fb620cbb229056", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 150 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,3 @@\n-import numpy as np\n-\n from keras_core.utils.module_utils import tensorflow as tf\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b1bf505985989e0691348fd62e303a9b9f133a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 389 | Contributors (this commit): 8 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -220,7 +220,7 @@ class EpochIterator:\n def raise_unsupported_arg(arg_name, arg_description, input_type):\n     raise ValueError(\n         f\"When providing `x` as a {input_type}, `{arg_name}` \"\n-        f\"should not be passed. Instead, the {arg_description} should \"\n+        f\"should not be passed. Instead, {arg_description} should \"\n         f\"be included as part of the {input_type}.\"\n     )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#80d22c768d3ed0cbfd9027e54429df7b7125ac4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 422 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 422 | Churn Cumulative: 422 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,422 @@\n+\"\"\"\n+Title: GPT text generation from scratch with KerasNLP\n+Author: [Jesse Chan](https://github.com/jessechancy)\n+Date created: 2022/07/25\n+Last modified: 2022/07/25\n+Description: Using KerasNLP to train a mini-GPT model for text generation.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+In this example, we will use KerasNLP to build a scaled down Generative\n+Pre-Trained (GPT) model. GPT is a Transformer-based model that allows you to generate\n+sophisticated text from a prompt.\n+\n+We will train the model on the [simplebooks-92](https://arxiv.org/abs/1911.12391) corpus,\n+which is a dataset made from several novels. It is a good dataset for this example since\n+it has a small vocabulary and high word frequency, which is beneficial when training a\n+model with few parameters.\n+\n+This example combines concepts from\n+[Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/)\n+with KerasNLP abstractions. We will demonstrate how KerasNLP tokenization, layers and\n+metrics simplify the training\n+process, and then show how to generate output text using the KerasNLP sampling utilities.\n+\n+Note: If you are running this example on a Colab,\n+make sure to enable GPU runtime for faster training.\n+\n+This example requires KerasNLP. You can install it via the following command:\n+`pip install keras-nlp`\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import os\n+\n+os.environ[\"KERAS_BACKEND\"] = \"jax\"\n+\n+import keras_nlp\n+import keras_core as keras\n+\n+import tensorflow.data as tf_data\n+import tensorflow.strings as tf_strings\n+\n+\"\"\"\n+## Settings & hyperparameters\n+\"\"\"\n+\n+# Data\n+BATCH_SIZE = 64\n+SEQ_LEN = 128\n+MIN_TRAINING_SEQ_LEN = 450\n+\n+# Model\n+EMBED_DIM = 256\n+FEED_FORWARD_DIM = 256\n+NUM_HEADS = 3\n+NUM_LAYERS = 2\n+VOCAB_SIZE = 5000  # Limits parameters in model.\n+\n+# Training\n+EPOCHS = 6\n+\n+# Inference\n+NUM_TOKENS_TO_GENERATE = 80\n+\n+\"\"\"\n+## Load the data\n+\n+Now, let's download the dataset! The SimpleBooks dataset consists of 1,573 Gutenberg books, and has\n+one of the smallest vocabulary size to word-level tokens ratio. It has a vocabulary size of ~98k,\n+a third of WikiText-103's, with around the same number of tokens (~100M). This makes it easy to fit a small model.\n+\"\"\"\n+\n+keras.utils.get_file(\n+    origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\",\n+    extract=True,\n+)\n+dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n+\n+# Load simplebooks-92 train set and filter out short lines.\n+raw_train_ds = (\n+    tf_data.TextLineDataset(dir + \"simplebooks-92-raw/train.txt\")\n+    .filter(lambda x: tf_strings.length(x) > MIN_TRAINING_SEQ_LEN)\n+    .batch(BATCH_SIZE)\n+    .shuffle(buffer_size=256)\n+)\n+\n+# Load simplebooks-92 validation set and filter out short lines.\n+raw_val_ds = (\n+    tf_data.TextLineDataset(dir + \"simplebooks-92-raw/valid.txt\")\n+    .filter(lambda x: tf_strings.length(x) > MIN_TRAINING_SEQ_LEN)\n+    .batch(BATCH_SIZE)\n+)\n+\n+\"\"\"\n+## Train the tokenizer\n+\n+We train the tokenizer from the training dataset for a vocabulary size of `VOCAB_SIZE`,\n+which is a tuned hyperparameter. We want to limit the vocabulary as much as possible, as\n+we will see later on\n+that it has a large effect on the number of model parameters. We also don't want to include\n+*too few* vocabulary terms, or there would be too many out-of-vocabulary (OOV) sub-words. In\n+addition, three tokens are reserved in the vocabulary:\n+\n+- `\"[PAD]\"` for padding sequences to `SEQ_LEN`. This token has index 0 in both\n+`reserved_tokens` and `vocab`, since `WordPieceTokenizer` (and other layers) consider\n+`0`/`vocab[0]` as the default padding.\n+- `\"[UNK]\"` for OOV sub-words, which should match the default `oov_token=\"[UNK]\"` in\n+`WordPieceTokenizer`.\n+- `\"[BOS]\"` stands for beginning of sentence, but here technically it is a token\n+representing the beginning of each line of training data.\n+\"\"\"\n+\n+# Train tokenizer vocabulary\n+vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n+    raw_train_ds,\n+    vocabulary_size=VOCAB_SIZE,\n+    lowercase=True,\n+    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n+)\n+\n+\"\"\"\n+## Load tokenizer\n+\n+We use the vocabulary data to initialize\n+`keras_nlp.tokenizers.WordPieceTokenizer`. WordPieceTokenizer is an efficient\n+implementation of the WordPiece algorithm used by BERT and other models. It will strip,\n+lower-case and do other irreversible preprocessing operations.\n+\"\"\"\n+\n+tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n+    vocabulary=vocab,\n+    sequence_length=SEQ_LEN,\n+    lowercase=True,\n+)\n+\n+\"\"\"\n+## Tokenize data\n+\n+We preprocess the dataset by tokenizing and splitting it into `features` and `labels`.\n+\"\"\"\n+\n+# packer adds a start token\n+start_packer = keras_nlp.layers.StartEndPacker(\n+    sequence_length=SEQ_LEN,\n+    start_value=tokenizer.token_to_id(\"[BOS]\"),\n+)\n+\n+\n+def preprocess(inputs):\n+    outputs = tokenizer(inputs)\n+    features = start_packer(outputs)\n+    labels = outputs\n+    return features, labels\n+\n+\n+# Tokenize and split into train and label sequences.\n+train_ds = raw_train_ds.map(\n+    preprocess, num_parallel_calls=tf_data.AUTOTUNE\n+).prefetch(tf_data.AUTOTUNE)\n+val_ds = raw_val_ds.map(\n+    preprocess, num_parallel_calls=tf_data.AUTOTUNE\n+).prefetch(tf_data.AUTOTUNE)\n+\n+\"\"\"\n+## Build the model\n+\n+We create our scaled down GPT model with the following layers:\n+\n+- One `keras_nlp.layers.TokenAndPositionEmbedding` layer, which combines the embedding\n+for the token and its position.\n+- Multiple `keras_nlp.layers.TransformerDecoder` layers, with the default causal masking.\n+The layer has no cross-attention when run with decoder sequence only.\n+- One final dense linear layer\n+\"\"\"\n+\n+inputs = keras.layers.Input(shape=(None,), dtype=\"int32\")\n+# Embedding.\n+embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n+    vocabulary_size=VOCAB_SIZE,\n+    sequence_length=SEQ_LEN,\n+    embedding_dim=EMBED_DIM,\n+    mask_zero=True,\n+)\n+x = embedding_layer(inputs)\n+# Transformer decoders.\n+for _ in range(NUM_LAYERS):\n+    decoder_layer = keras_nlp.layers.TransformerDecoder(\n+        num_heads=NUM_HEADS,\n+        intermediate_dim=FEED_FORWARD_DIM,\n+    )\n+    x = decoder_layer(x)  # Giving one argument only skips cross-attention.\n+# Output.\n+outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n+model = keras.Model(inputs=inputs, outputs=outputs)\n+loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n+perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n+model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])\n+\n+\"\"\"\n+Let's take a look at our model summary - a large majority of the\n+parameters are in the `token_and_position_embedding` and the output `dense` layer!\n+This means that the vocabulary size (`VOCAB_SIZE`) has a large effect on the size of the model,\n+while the number of Transformer decoder layers (`NUM_LAYERS`) doesn't affect it as much.\n+\"\"\"\n+\n+model.summary()\n+\n+\"\"\"\n+## Training\n+\n+Now that we have our model, let's train it with the `fit()` method.\n+\"\"\"\n+\n+model.fit(train_ds, validation_data=val_ds, verbose=2, epochs=EPOCHS)\n+\n+\"\"\"\n+## Inference\n+\n+With our trained model, we can test it out to gauge its performance. To do this\n+we can seed our model with an input sequence starting with the `\"[BOS]\"` token,\n+and progressively sample the model by making predictions for each subsequent\n+token in a loop.\n+\n+To start lets build a prompt with the same shape as our model inputs, containing\n+only the `\"[BOS]\"` token.\n+\"\"\"\n+\n+# The \"packer\" layers adds the [BOS] token for us.\n+prompt_tokens = start_packer(tokenizer([\"\"]))\n+prompt_tokens\n+\n+\"\"\"\n+We will use the `keras_nlp.samplers` module for inference, which requires a\n+callback function wrapping the model we just trained. This wrapper calls\n+the model and returns the logit predictions for the current token we are\n+generating.\n+\n+Note: There are two pieces of more advanced functionality available when\n+defining your callback. The first is the ability to take in a `cache` of states\n+computed in previous generation steps, which can be used to speed up generation.\n+The second is the ability to output the final dense \"hidden state\" of each\n+generated token. This is used by `keras_nlp.samplers.ContrastiveSampler`, which\n+avoids repetition by penalizing repeated hidden states. Both are optional, and\n+we will ignore them for now.\n+\"\"\"\n+\n+\n+def next(prompt, cache, index):\n+    logits = model(prompt)[:, index - 1, :]\n+    # Ignore hidden states for now; only needed for contrastive search.\n+    hidden_states = None\n+    return logits, hidden_states, cache\n+\n+\n+\"\"\"\n+Creating the wrapper function is the most complex part of using these functions. Now that\n+it's done, let's test out the different utilities, starting with greedy search.\n+\"\"\"\n+\n+\"\"\"\n+### Greedy search\n+\n+We greedily pick the most probable token at each timestep. In other words, we get the\n+argmax of the model output.\n+\"\"\"\n+\n+sampler = keras_nlp.samplers.GreedySampler()\n+output_tokens = sampler(\n+    next=next,\n+    prompt=prompt_tokens,\n+    index=1,  # Start sampling immediately after the [BOS] token.\n+)\n+txt = tokenizer.detokenize(output_tokens)\n+print(f\"Greedy search generated text: \\n{txt}\\n\")\n+\n+\"\"\"\n+As you can see, greedy search starts out making some sense, but quickly starts repeating\n+itself. This is a common problem with text generation that can be fixed by some of the\n+probabilistic text generation utilities shown later on!\n+\"\"\"\n+\n+\"\"\"\n+### Beam search\n+\n+At a high-level, beam search keeps track of the `num_beams` most probable sequences at\n+each timestep, and predicts the best next token from all sequences. It is an improvement\n+over greedy search since it stores more possibilities. However, it is less efficient than\n+greedy search since it has to compute and store multiple potential sequences.\n+\n+**Note:** beam search with `num_beams=1` is identical to greedy search.\n+\"\"\"\n+\n+sampler = keras_nlp.samplers.BeamSampler(num_beams=10)\n+output_tokens = sampler(\n+    next=next,\n+    prompt=prompt_tokens,\n+    index=1,\n+)\n+txt = tokenizer.detokenize(output_tokens)\n+print(f\"Beam search generated text: \\n{txt}\\n\")\n+\n+\"\"\"\n+Similar to greedy search, beam search quickly starts repeating itself, since it is still\n+a deterministic method.\n+\"\"\"\n+\n+\"\"\"\n+### Random search\n+\n+Random search is our first probabilistic method. At each time step, it samples the next\n+token using the softmax probabilities provided by the model.\n+\"\"\"\n+\n+sampler = keras_nlp.samplers.RandomSampler()\n+output_tokens = sampler(\n+    next=next,\n+    prompt=prompt_tokens,\n+    index=1,\n+)\n+txt = tokenizer.detokenize(output_tokens)\n+print(f\"Random search generated text: \\n{txt}\\n\")\n+\n+\"\"\"\n+Voilà, no repetitions! However, with random search, we may see some nonsensical words\n+appearing since any word in the vocabulary has a chance of appearing with this sampling\n+method. This is fixed by our next search utility, top-k search.\n+\"\"\"\n+\n+\"\"\"\n+### Top-K search\n+\n+Similar to random search, we sample the next token from the probability distribution\n+provided by the model. The only difference is that here, we select out the top `k` most\n+probable tokens, and distribute the probability mass over them before sampling. This way,\n+we won't be sampling from low probability tokens, and hence we would have less\n+nonsensical words!\n+\"\"\"\n+\n+sampler = keras_nlp.samplers.TopKSampler(k=10)\n+output_tokens = sampler(\n+    next=next,\n+    prompt=prompt_tokens,\n+    index=1,\n+)\n+txt = tokenizer.detokenize(output_tokens)\n+print(f\"Top-K search generated text: \\n{txt}\\n\")\n+\n+\"\"\"\n+### Top-P search\n+\n+Even with the top-k search, there is something to improve upon. With top-k search, the\n+number `k` is fixed, which means it selects the same number of tokens for any probability\n+distribution. Consider two scenarios, one where the probability mass is concentrated over\n+2 words and another where the probability mass is evenly concentrated across 10. Should\n+we choose `k=2` or `k=10`? There is no one size that fits all `k` here.\n+\n+This is where top-p search comes in! Instead of choosing a `k`, we choose a probability\n+`p` that we want the probabilities of the top tokens to sum up to. This way, we can\n+dynamically adjust the `k` based on the probability distribution. By setting `p=0.9`, if\n+90% of the probability mass is concentrated on the top 2 tokens, we can filter out the\n+top 2 tokens to sample from. If instead the 90% is distributed over 10 tokens, it will\n+similarly filter out the top 10 tokens to sample from.\n+\"\"\"\n+\n+sampler = keras_nlp.samplers.TopPSampler(p=0.5)\n+output_tokens = sampler(\n+    next=next,\n+    prompt=prompt_tokens,\n+    index=1,\n+)\n+txt = tokenizer.detokenize(output_tokens)\n+print(f\"Top-P search generated text: \\n{txt}\\n\")\n+\n+\"\"\"\n+### Using callbacks for text generation\n+\n+We can also wrap the utilities in a callback, which allows you to print out a prediction\n+sequence for every epoch of the model! Here is an example of a callback for top-k search:\n+\"\"\"\n+\n+\n+class TopKTextGenerator(keras.callbacks.Callback):\n+    \"\"\"A callback to generate text from a trained model using top-k.\"\"\"\n+\n+    def __init__(self, k):\n+        self.sampler = keras_nlp.samplers.TopKSampler(k)\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        output_tokens = self.sampler(\n+            next=next,\n+            prompt=prompt_tokens,\n+            index=1,\n+        )\n+        txt = tokenizer.detokenize(output_tokens)\n+        print(f\"Top-K search generated text: \\n{txt}\\n\")\n+\n+\n+text_generation_callback = TopKTextGenerator(k=10)\n+# Dummy training loop to demonstrate callback.\n+model.fit(\n+    train_ds.take(1), verbose=2, epochs=2, callbacks=[text_generation_callback]\n+)\n+\n+\"\"\"\n+## Conclusion\n+\n+To recap, in this example, we use KerasNLP layers to train a sub-word vocabulary,\n+tokenize training data, create a miniature GPT model, and perform inference with the\n+text generation library.\n+\n+If you would like to understand how Transformers work, or learn more about training the\n+full GPT model, here are some further readings:\n+\n+- Attention Is All You Need [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)\n+- GPT-3 Paper [Brown et al., 2020](https://arxiv.org/abs/2005.14165)\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f51b5fae1b6c5be4d0b9acde2d93ba69ed02ec3a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 416 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -145,9 +145,7 @@ class HashedCrossingTest(testing.TestCase):\n         outputs = layer((feat1, feat2))\n         self.assertAllClose(outputs, 1)\n \n-        layer = tf.keras.layers.HashedCrossing(\n-            num_bins=5, output_mode=\"one_hot\"\n-        )\n+        layer = layers.HashedCrossing(num_bins=5, output_mode=\"one_hot\")\n         feat1 = tf.constant([\"A\", \"B\", \"A\", \"B\", \"A\"])\n         feat2 = tf.constant([101, 101, 101, 102, 102])\n         self.assertAllClose(\n@@ -163,7 +161,7 @@ class HashedCrossingTest(testing.TestCase):\n             layer((feat1, feat2)),\n         )\n \n-        layer = tf.keras.layers.HashedCrossing(num_bins=5)\n+        layer = layers.HashedCrossing(num_bins=5)\n         feat1 = tf.constant([\"A\", \"B\", \"A\", \"B\", \"A\"])\n         feat2 = tf.constant([101, 101, 101, 102, 102])\n         self.assertAllClose(tf.constant([1, 4, 1, 1, 3]), layer((feat1, feat2)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#271f84d0e07e0a1b717e466d39569de609e59b5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 25 | Churn Cumulative: 7525 | Contributors (this commit): 18 | Commits (past 90d): 99 | Contributors (cumulative): 40 | DMM Complexity: 0.47058823529411764\n\nDIFF:\n@@ -548,7 +548,10 @@ def vstack(xs):\n \n \n def where(condition, x1, x2):\n+    if x1 is not None and x2 is not None:\n         return np.where(condition, x1, x2)\n+    else:\n+        return np.where(condition)\n \n \n def divide(x1, x2):\n\n@@ -896,8 +896,12 @@ def vstack(xs):\n \n def where(condition, x1, x2):\n     condition = convert_to_tensor(condition, dtype=bool)\n-    x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n+    if x1 is not None and x2 is not None:\n+        x1 = convert_to_tensor(x1)\n+        x2 = convert_to_tensor(x2)\n         return torch.where(condition, x1, x2)\n+    else:\n+        return torch.where(condition)\n \n \n def divide(x1, x2):\n\n@@ -5033,7 +5033,7 @@ def vstack(xs):\n \n \n class Where(Operation):\n-    def call(self, condition, x1, x2):\n+    def call(self, condition, x1=None, x2=None):\n         return backend.numpy.where(condition, x1, x2)\n \n     def compute_output_spec(self, condition, x1, x2):\n@@ -5042,11 +5042,12 @@ class Where(Operation):\n         x2_shape = getattr(x2, \"shape\", [])\n         output_shape = broadcast_shapes(condition_shape, x1_shape)\n         output_shape = broadcast_shapes(output_shape, x2_shape)\n-        return KerasTensor(output_shape, dtype=x1.dtype)\n+        output_dtype = getattr(x1, \"dtype\", \"int\")\n+        return KerasTensor(output_shape, dtype=output_dtype)\n \n \n @keras_core_export([\"keras_core.ops.where\", \"keras_core.ops.numpy.where\"])\n-def where(condition, x1, x2):\n+def where(condition, x1=None, x2=None):\n     \"\"\"Return elements chosen from `x1` or `x2` depending on `condition`.\n \n     Args:\n@@ -5058,6 +5059,11 @@ def where(condition, x1, x2):\n         A tensor with elements from `x1` where `condition` is `True`, and\n         elements from `x2` where `condition` is `False`.\n     \"\"\"\n+    if (x1 is None and x2 is not None) or (x1 is not None and x2 is None):\n+        raise ValueError(\n+            \"`x1` and `x2` either both should be `None`\"\n+            \" or both should have non-None value.\"\n+        )\n     if any_symbolic_tensors((condition, x1, x2)):\n         return Where().symbolic_call(condition, x1, x2)\n     return backend.numpy.where(condition, x1, x2)\n\n@@ -246,6 +246,7 @@ class NumpyTwoInputOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 1])\n         y = KerasTensor([None, 3])\n         self.assertEqual(knp.where(condition, x, y).shape, (2, None, 3))\n+        self.assertEqual(knp.where(condition).shape, (2, None, 1))\n \n     def test_floordiv(self):\n         x = KerasTensor((None, 3))\n@@ -666,6 +667,7 @@ class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([2, 3])\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.where(condition, x, y).shape, (2, 3))\n+        self.assertAllEqual(knp.where(condition).shape, (2, 3))\n \n     def test_floordiv(self):\n         x = KerasTensor((2, 3))\n@@ -2177,6 +2179,8 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase):\n         y = np.array([4, 5, 6])\n         self.assertAllClose(knp.where(x > 1, x, y), np.where(x > 1, x, y))\n         self.assertAllClose(knp.Where()(x > 1, x, y), np.where(x > 1, x, y))\n+        self.assertAllClose(knp.where(x > 1), np.where(x > 1))\n+        self.assertAllClose(knp.Where()(x > 1), np.where(x > 1))\n \n     def test_digitize(self):\n         x = np.array([0.0, 1.0, 3.0, 1.6])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#25b138cec0e54b06407b81b9979650edc945adec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 150 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 9 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 150 | Churn Cumulative: 150 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,150 @@\n+\"\"\"Unified high level distribution APIs across backends.\n+\n+!!!DO NOT USE!!! Currently under development and APIs are not final.\n+\n+Currently only the JAX backend has been implemented, and the Tensorflow backend\n+will be implemented in future (via tf.dtensor API).\n+\"\"\"\n+\n+import contextlib\n+\n+from keras_core.backend.common import global_state\n+\n+GLOBAL_ATTRIBUTE_NAME = \"distribution\"\n+\n+\n+def list_devices(device_type=None):\n+    \"\"\"Return all the available devices based on the device type.\n+\n+    Note that this should return the global devices in a distributed setting.\n+\n+    Args:\n+        device_type: string of 'CPU', 'GPU' or 'TPU'. Default to GPU or TPU if\n+            available when device_type is not provided. Otherwise will return\n+            the CPU devices.\n+\n+    Return:\n+        List of devices that are available for distribute computation.\n+    \"\"\"\n+    pass\n+\n+\n+class DeviceMesh:\n+    \"\"\"The cluster of computation devices for distributed computation.\n+\n+    This is aligned with `jax.sharding.Mesh` and `tf.dtensor.Mesh`, which\n+    represents the computation devices in the global context.\n+\n+    See more details in [jax.sharding.Mesh](\n+        https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh)\n+    and [tf.dtensor.Mesh](\n+        https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Mesh).\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        shape,\n+        axis_names,\n+        devices=None,\n+    ):\n+        \"\"\"Initialize the DeviceMesh for the given topology.\n+\n+        Args:\n+            shape: tuple of list of integers. The shape of the overall\n+                DeviceMesh, e.g. `(8,)` for a data parallel only distribution,\n+                or `(4, 2)` for a model+data parallel distribution.\n+            axis_names: List of string. The logical name of the each axis for\n+                the DeviceMesh. The length of the `axis_names` should match to\n+                the rank of the `shape`. The `axis_names` will be used to\n+                match/create the `TensorLayout` when distribute the data and\n+                weights.\n+            devices: Optional list of devices. Default to all the available\n+                devices locally from `list_devices()`.\n+        \"\"\"\n+        pass\n+\n+\n+class TensorLayout:\n+    \"\"\"The layout of a Tensor.\n+\n+    This is aligned with `jax.sharding.NamedSharding` and `tf.dtensor.Layout`,\n+    which allocate the tensor to its logic axis based on the `DeviceMesh`. With\n+    `DeviceMesh` and `TensorLayout`, the actual mapping between a Tensor to the\n+    physical devices can be determined.\n+\n+    See more details in [jax.sharding.NamedSharding](\n+        https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding)\n+    and [tf.dtensor.Layout](\n+        https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Layout).\n+    \"\"\"\n+\n+    def __init__(self, axes):\n+        \"\"\"Initialize the TensorLayout with axis names.\n+\n+        Args:\n+            axes: list of strings that should map to the `axis_names` in\n+                `DeviceMesh`. For any dimentions that doesn't need any sharding,\n+                A `None` can be used a placeholder.\n+        \"\"\"\n+        pass\n+\n+\n+class Distribution:\n+    \"\"\"Base class for the distribution.\n+\n+    The `Distribution` has following key functionalities.\n+\n+    1. Distribute the model variables to the `DeviceMesh`.\n+    2. Distribute the input data to the `DeviceMesh`.\n+\n+    It can create a context scope so that the framework to properly detect the\n+    `Distribution` and distribute the variable/data accordingly.\n+    \"\"\"\n+\n+    def __init__(self, device_mesh):\n+        pass\n+\n+    def get_data_layout(self, data_shape):\n+        \"\"\"Retrieve the `TensorLayout` for the input data.\n+\n+        Args:\n+            data_shape: shape for the input data in list or tuple format.\n+\n+        Returns:\n+            The `TensorLayout` for the data, which can be used by\n+            `backend.distribute_tensor()` to redistribute a input data.\n+        \"\"\"\n+        pass\n+\n+    def get_variable_layout(self, variable_path):\n+        \"\"\"Retrieve the `TensorLayout` for the variable based on the path.\n+\n+        The path of the variable is available by `variable.path`.\n+\n+        Args:\n+            variable_path: string, the path for the variable to be distributed.\n+\n+        return:\n+            The `TensorLayout` for the variable, which can be used by\n+            `backend.distribute_tensor()` to redistribute a variable.\n+        \"\"\"\n+        pass\n+\n+    @contextlib.contextmanager\n+    def scope(self):\n+        \"\"\"Context manager to make the `Distribution` current.\"\"\"\n+        pass\n+\n+\n+def distribution():\n+    \"\"\"Retrieve the current distribution from global context.\"\"\"\n+    return global_state.get_global_attribute(GLOBAL_ATTRIBUTE_NAME)\n+\n+\n+def set_distribution(value):\n+    \"\"\"Set the distribution as the global distribution setting.\n+\n+    Args:\n+        value: a `Distribution` instance.\n+    \"\"\"\n+    global_state.set_global_attribute(GLOBAL_ATTRIBUTE_NAME, value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eeb3899fdc877d7b8a5f15dd431920944014f476", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 202 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 12 | Complexity Δ (Sum/Max): 24/21 | Churn Δ: 204 | Churn Cumulative: 967 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 0.937888198757764\n\nDIFF:\n@@ -171,8 +171,7 @@ class LegacyH5WholeModelTest(testing.TestCase):\n             def __init__(self, units, **kwargs):\n                 super().__init__(units, **kwargs)\n \n-        input_shape = [1]\n-        inputs = layers.Input(shape=input_shape)\n+        inputs = layers.Input(shape=[1])\n         custom_layer = MyDense(1)\n         model = models.Sequential(layers=[inputs, custom_layer])\n \n@@ -301,3 +300,184 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n \n         ref_input = np.random.random((1, 3))\n         self._check_reloading_model(ref_input, model, tf_keras_model)\n+\n+    def test_saving_lambda(self):\n+        mean = np.random.random((4, 2, 3))\n+        std = np.abs(np.random.random((4, 2, 3))) + 1e-5\n+        inputs = tf.keras.layers.Input(shape=(4, 2, 3))\n+        output = tf.keras.layers.Lambda(\n+            lambda image, mu, std: (image - mu) / std,\n+            arguments={\"mu\": mean, \"std\": std},\n+            output_shape=inputs.shape,\n+        )(inputs)\n+        tf_keras_model = tf.keras.Model(inputs, output)\n+        tf_keras_model.compile(loss=\"mse\", optimizer=\"sgd\", metrics=[\"acc\"])\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"lambda_model.h5\")\n+        tf_keras_model.save(temp_filepath)\n+        loaded = legacy_h5_format.load_model_from_hdf5(temp_filepath)\n+\n+        self.assertAllClose(mean, loaded.layers[1].arguments[\"mu\"])\n+        self.assertAllClose(std, loaded.layers[1].arguments[\"std\"])\n+\n+    def test_saving_include_optimizer_false(self):\n+        tf_keras_model = tf.keras.Sequential()\n+        tf_keras_model.add(tf.keras.layers.Dense(1))\n+        tf_keras_model.compile(\"adam\", loss=\"mse\")\n+        x, y = np.ones((10, 10)), np.ones((10, 1))\n+        tf_keras_model.fit(x, y)\n+        ref_output = tf_keras_model(x)\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"model.h5\")\n+        tf_keras_model.save(temp_filepath, include_optimizer=False)\n+        loaded = legacy_h5_format.load_model_from_hdf5(temp_filepath)\n+        output = loaded(x)\n+\n+        # Assert that optimizer does not exist in loaded model\n+        with self.assertRaises(AttributeError):\n+            _ = loaded.optimizer\n+\n+        # Compare output\n+        self.assertAllClose(ref_output, output, atol=1e-5)\n+\n+    def test_custom_sequential_registered_no_scope(self):\n+        @tf.keras.saving.register_keras_serializable(package=\"my_package\")\n+        class MyDense(tf.keras.layers.Dense):\n+            def __init__(self, units, **kwargs):\n+                super().__init__(units, **kwargs)\n+\n+        inputs = tf.keras.layers.Input(shape=[1])\n+        custom_layer = MyDense(1)\n+        tf_keras_model = tf.keras.Sequential(layers=[inputs, custom_layer])\n+\n+        # Re-implement and re-register in Keras Core\n+        @object_registration.register_keras_serializable(package=\"my_package\")\n+        class MyDense(layers.Dense):\n+            def __init__(self, units, **kwargs):\n+                super().__init__(units, **kwargs)\n+\n+        inputs = layers.Input(shape=[1])\n+        custom_layer = MyDense(1)\n+        model = models.Sequential(layers=[inputs, custom_layer])\n+\n+        ref_input = np.array([5])\n+        self._check_reloading_model(ref_input, model, tf_keras_model)\n+\n+    def test_custom_functional_registered_no_scope(self):\n+        @tf.keras.saving.register_keras_serializable(package=\"my_package\")\n+        class MyDense(tf.keras.layers.Dense):\n+            def __init__(self, units, **kwargs):\n+                super().__init__(units, **kwargs)\n+\n+        inputs = tf.keras.layers.Input(shape=[1])\n+        outputs = MyDense(1)(inputs)\n+        tf_keras_model = tf.keras.Model(inputs, outputs)\n+\n+        # Re-implement and re-register in Keras Core\n+        @object_registration.register_keras_serializable(package=\"my_package\")\n+        class MyDense(layers.Dense):\n+            def __init__(self, units, **kwargs):\n+                super().__init__(units, **kwargs)\n+\n+        inputs = layers.Input(shape=[1])\n+        outputs = MyDense(1)(inputs)\n+        model = models.Model(inputs, outputs)\n+\n+        ref_input = np.array([5])\n+        self._check_reloading_model(ref_input, model, tf_keras_model)\n+\n+    def test_nested_layers(self):\n+        class MyLayer(tf.keras.layers.Layer):\n+            def __init__(self, sublayers, **kwargs):\n+                super().__init__(**kwargs)\n+                self.sublayers = sublayers\n+\n+            def call(self, x):\n+                prev_input = x\n+                for layer in self.sublayers:\n+                    prev_input = layer(prev_input)\n+                return prev_input\n+\n+            def get_config(self):\n+                config = super().get_config()\n+                config[\"sublayers\"] = tf.keras.saving.serialize_keras_object(\n+                    self.sublayers\n+                )\n+                return config\n+\n+            @classmethod\n+            def from_config(cls, config):\n+                config[\"sublayers\"] = tf.keras.saving.deserialize_keras_object(\n+                    config[\"sublayers\"]\n+                )\n+                return cls(**config)\n+\n+        @tf.keras.saving.register_keras_serializable(package=\"Foo\")\n+        class RegisteredSubLayer(layers.Layer):\n+            def call(self, x):\n+                return x\n+\n+        layer = MyLayer(\n+            [\n+                tf.keras.layers.Dense(2, name=\"MyDense\"),\n+                RegisteredSubLayer(name=\"MySubLayer\"),\n+            ]\n+        )\n+        tf_keras_model = tf.keras.Sequential([layer])\n+\n+        x = np.random.random((4, 2))\n+        ref_output = tf_keras_model(x)\n+\n+        # Save TF Keras model to H5 file\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"model.h5\")\n+        tf_keras_model.save(temp_filepath)\n+\n+        # Re-implement in Keras Core\n+        class MyLayer(layers.Layer):\n+            def __init__(self, sublayers, **kwargs):\n+                super().__init__(**kwargs)\n+                self.sublayers = sublayers\n+\n+            def call(self, x):\n+                prev_input = x\n+                for layer in self.sublayers:\n+                    prev_input = layer(prev_input)\n+                return prev_input\n+\n+            def get_config(self):\n+                config = super().get_config()\n+                config[\"sublayers\"] = serialization_lib.serialize_keras_object(\n+                    self.sublayers\n+                )\n+                return config\n+\n+            @classmethod\n+            def from_config(cls, config):\n+                config[\n+                    \"sublayers\"\n+                ] = serialization_lib.deserialize_keras_object(\n+                    config[\"sublayers\"]\n+                )\n+                return cls(**config)\n+\n+        # Re-implement and re-register in Keras Core\n+        @object_registration.register_keras_serializable(package=\"Foo\")\n+        class RegisteredSubLayer(layers.Layer):\n+            def call(self, x):\n+                return x\n+\n+        # Load in Keras Core\n+        loaded_model = legacy_h5_format.load_model_from_hdf5(\n+            temp_filepath, custom_objects={\"MyLayer\": MyLayer}\n+        )\n+        loaded_layer = loaded_model.layers[0]\n+        output = loaded_model(x)\n+\n+        # Ensure nested layer structure\n+        self.assertIsInstance(loaded_layer.sublayers[0], layers.Dense)\n+        self.assertEqual(loaded_layer.sublayers[0].name, \"MyDense\")\n+        self.assertIsInstance(loaded_layer.sublayers[1], RegisteredSubLayer)\n+        self.assertEqual(loaded_layer.sublayers[1].name, \"MySubLayer\")\n+\n+        # Compare output\n+        self.assertAllClose(ref_output, output, atol=1e-5)\n\n@@ -15,6 +15,14 @@ from keras_core.saving import object_registration\n \n MODULE_OBJECTS = threading.local()\n \n+# Legacy lambda arguments not found in Keras Core\n+LAMBDA_DEP_ARGS = (\n+    \"module\",\n+    \"function_type\",\n+    \"output_shape_type\",\n+    \"output_shape_module\",\n+)\n+\n \n def model_from_config(config, custom_objects=None):\n     \"\"\"Instantiates a Keras model from its config.\n@@ -58,6 +66,18 @@ def model_from_config(config, custom_objects=None):\n     if axis is not None and isinstance(axis, list) and len(axis) == 1:\n         config[\"config\"][\"axis\"] = int(axis[0])\n \n+    # Handle backwards compatibility for Keras lambdas\n+    if config[\"class_name\"] == \"Lambda\":\n+        for dep_arg in LAMBDA_DEP_ARGS:\n+            _ = config[\"config\"].pop(dep_arg, None)\n+        function_config = config[\"config\"][\"function\"]\n+        if isinstance(function_config, list):\n+            function_dict = {\"class_name\": \"__lambda__\", \"config\": {}}\n+            function_dict[\"config\"][\"code\"] = function_config[0]\n+            function_dict[\"config\"][\"defaults\"] = function_config[1]\n+            function_dict[\"config\"][\"closure\"] = function_config[2]\n+            config[\"config\"][\"function\"] = function_dict\n+\n     # TODO(nkovela): Swap find and replace args during Keras 3.0 release\n     # Replace keras refs with keras_core\n     config = _find_replace_nested_dict(config, \"keras.\", \"keras_core.\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
