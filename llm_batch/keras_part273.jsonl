{"custom_id": "keras#2fdbaa10ab0fd72f962923c4cf73e024424a3022", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 17 | Churn Cumulative: 6338 | Contributors (this commit): 32 | Commits (past 90d): 62 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -986,7 +986,7 @@ class TestBackend(object):\n     def test_dropout(self):\n         val = np.random.random((100, 100))\n         z_list = [k.eval(k.dropout(k.variable(val), level=0.2))\n-                  for k in BACKENDS]\n+                  for k in WITH_NP]\n         assert_list_pairwise(z_list, allclose=False)\n         # dropout patterns are different, only check mean\n         for i in range(len(z_list) - 1):\n@@ -994,7 +994,7 @@ class TestBackend(object):\n \n         z_list = [k.eval(k.dropout(k.variable(val), level=0.2,\n                                    noise_shape=list(val.shape)))\n-                  for k in BACKENDS]\n+                  for k in WITH_NP]\n         assert_list_pairwise(z_list, allclose=False)\n         # dropout patterns are different, only check mean\n         for i in range(len(z_list) - 1):\n\n@@ -570,6 +570,19 @@ def variable(value, dtype=None, name=None, constraint=None):\n     return np.array(value, dtype)\n \n \n+def dropout(x, level, noise_shape=None, seed=None):\n+    if noise_shape is None:\n+        noise_shape = x.shape\n+    if learning_phase():\n+        noise = np.random.choice([0, 1],\n+                                 noise_shape,\n+                                 replace=True,\n+                                 p=[level, 1 - level])\n+        return x * noise / (1 - level)\n+    else:\n+        return x\n+\n+\n def equal(x, y):\n     return x == y\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8fc8a1681332cc4c8b05260b20ee54fa1589e3d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 116 | Lines Deleted: 54 | Files Changed: 1 | Hunks: 45 | Methods Changed: 18 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 170 | Churn Cumulative: 6465 | Contributors (this commit): 89 | Commits (past 90d): 11 | Contributors (cumulative): 89 | DMM Complexity: 0.4444444444444444\n\nDIFF:\n@@ -36,7 +36,8 @@ py_slice = slice\n \n # INTERNAL UTILS\n theano.config.floatX = floatx()\n-_LEARNING_PHASE = T.scalar(dtype='uint8', name='keras_learning_phase')  # 0 = test, 1 = train\n+# 0 = test, 1 = train\n+_LEARNING_PHASE = T.scalar(dtype='uint8', name='keras_learning_phase')\n _UID_PREFIXES = defaultdict(int)\n \n \n@@ -200,19 +201,23 @@ def is_keras_tensor(x):\n         >>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.\n         ValueError\n         >>> k_var = tf.placeholder('float32', shape=(1,1))\n-        >>> K.is_keras_tensor(k_var) # A variable indirectly created outside of keras is not a Keras tensor.\n+        >>> # A variable indirectly created outside of keras is not a Keras tensor.\n+        >>> K.is_keras_tensor(k_var)\n         False\n         >>> keras_var = K.variable(np_var)\n-        >>> K.is_keras_tensor(keras_var)  # A variable created with the keras backend is not a Keras tensor.\n+        >>> # A variable created with the keras backend is not a Keras tensor.\n+        >>> K.is_keras_tensor(keras_var)\n         False\n         >>> keras_placeholder = K.placeholder(shape=(2, 4, 5))\n-        >>> K.is_keras_tensor(keras_placeholder)  # A placeholder is not a Keras tensor.\n+        >>> # A placeholder is not a Keras tensor.\n+        >>> K.is_keras_tensor(keras_placeholder)\n         False\n         >>> keras_input = Input([10])\n         >>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.\n         True\n         >>> keras_layer_output = Dense(10)(keras_input)\n-        >>> K.is_keras_tensor(keras_layer_output) # Any Keras layer output is a Keras tensor.\n+        >>> # Any Keras layer output is a Keras tensor.\n+        >>> K.is_keras_tensor(keras_layer_output)\n         True\n     ```\n     \"\"\"\n@@ -670,7 +675,8 @@ def logsumexp(x, axis=None, keepdims=False):\n     # Returns\n         The reduced tensor.\n     \"\"\"\n-    # Theano has a built-in optimization for logsumexp (see https://github.com/Theano/Theano/pull/4736)\n+    # Theano has a built-in optimization for logsumexp\n+    # (see https://github.com/Theano/Theano/pull/4736)\n     # so we can just write the expression directly:\n     return T.log(T.sum(T.exp(x), axis=axis, keepdims=keepdims))\n \n@@ -747,7 +753,8 @@ def normalize_batch_in_training(x, gamma, beta,\n     # TODO remove this if statement when Theano without\n     # T.nnet.bn.batch_normalization_train is deprecated\n     if not hasattr(T.nnet.bn, 'batch_normalization_train'):\n-        return _old_normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon)\n+        return _old_normalize_batch_in_training(\n+            x, gamma, beta, reduction_axes, epsilon)\n \n     if gamma is None:\n         if beta is None:\n@@ -800,13 +807,16 @@ def _old_normalize_batch_in_training(x, gamma, beta, reduction_axes,\n         beta = zeros_like(x)\n \n     dev = theano.config.device\n-    use_cudnn = ndim(x) < 5 and reduction_axes == [0, 2, 3] and (dev.startswith('cuda') or dev.startswith('gpu'))\n+    use_cudnn = (ndim(x) < 5 and\n+                 reduction_axes == [0, 2, 3] and\n+                 (dev.startswith('cuda') or dev.startswith('gpu')))\n     if use_cudnn:\n         broadcast_beta = beta.dimshuffle('x', 0, 'x', 'x')\n         broadcast_gamma = gamma.dimshuffle('x', 0, 'x', 'x')\n         try:\n-            normed, mean, stdinv = theano.sandbox.cuda.dnn.dnn_batch_normalization_train(\n+            trained = theano.sandbox.cuda.dnn.dnn_batch_normalization_train(\n                 x, broadcast_gamma, broadcast_beta, 'spatial', epsilon)\n+            normed, mean, stdinv = trained\n             normed = theano.tensor.as_tensor_variable(normed)\n             mean = theano.tensor.as_tensor_variable(mean)\n             stdinv = theano.tensor.as_tensor_variable(stdinv)\n@@ -1317,7 +1327,12 @@ def reverse(x, axes):\n     \"\"\"\n     if isinstance(axes, int):\n         axes = [axes]\n-    slices = [py_slice(None, None, -1) if i in axes else py_slice(None, None, None) for i in range(x.ndim)]\n+    slices = []\n+    for i in range(x.ndim):\n+        if i in axes:\n+            slices.append(py_slice(None, None, -1))\n+        else:\n+            slices.append(py_slice(None, None, None))\n     return x[slices]\n \n \n@@ -1388,12 +1403,16 @@ class Function(object):\n         return self.function(*inputs)\n \n \n+def _raise_invalid_arg(key):\n+    msg = 'Invalid argument \"%s\" passed to K.function with Theano backend' % key\n+    raise ValueError(msg)\n+\n+\n def function(inputs, outputs, updates=[], **kwargs):\n     if len(kwargs) > 0:\n         for key in kwargs.keys():\n             if not has_arg(theano.function, key, True):\n-                msg = 'Invalid argument \"%s\" passed to K.function with Theano backend' % key\n-                raise ValueError(msg)\n+                _raise_invalid_arg(key)\n     return Function(inputs, outputs, updates=updates, **kwargs)\n \n \n@@ -1515,10 +1534,14 @@ def rnn(step_function, inputs, initial_states,\n             outputs = T.stack(*successive_outputs)\n             states = []\n             for i in range(len(successive_states[-1])):\n-                states.append(T.stack(*[states_at_step[i] for states_at_step in successive_states]))\n+                new_states = []\n+                for states_at_step in successive_states:\n+                    new_states.append(states_at_step[i])\n+                states.append(T.stack(*new_states))\n         else:\n             # build an all-zero tensor of shape (samples, output_dim)\n-            initial_output = step_function(inputs[0], initial_states + constants)[0] * 0\n+            initial_output = step_function(inputs[0], initial_states + constants)\n+            initial_output = initial_output[0] * 0\n             # Theano gets confused by broadcasting patterns in the scan op\n             initial_output = T.unbroadcast(initial_output, 0, 1)\n             if len(initial_states) > 0:\n@@ -1568,7 +1591,8 @@ def rnn(step_function, inputs, initial_states,\n             outputs = T.stack(*successive_outputs)\n             states = []\n             for i in range(len(successive_states[-1])):\n-                states.append(T.stack(*[states_at_step[i] for states_at_step in successive_states]))\n+                states.append(T.stack(\n+                    *[states_at_step[i] for states_at_step in successive_states]))\n \n         else:\n             def _step(inputs, *states):\n@@ -1916,7 +1940,8 @@ def _preprocess_conv2d_depthwise_kernel(kernel, kernel_shape, data_format):\n     # As of Keras 2.0.0, all kernels are normalized\n     # on the format `(rows, cols, input_depth, depth)`,\n     # independently of `data_format`.\n-    # Theano expects `(input_depth * depth, 1, rows, cols)` for depthwise convolution.\n+    # Theano expects `(input_depth * depth, 1, rows, cols)`\n+    # for depthwise convolution.\n     kernel = kernel[::-1, ::-1, :, :]\n     kernel = kernel.dimshuffle((2, 3, 0, 1))\n     kernel = reshape(kernel, kernel_shape)\n@@ -2026,9 +2051,11 @@ def _postprocess_conv2d_output(conv_out, x,\n                                strides, data_format):\n     if padding == 'same':\n         if kernel_shape[2] % 2 == 0:\n-            conv_out = conv_out[:, :, :(x.shape[2] + strides[0] - 1) // strides[0], :]\n+            i = (x.shape[2] + strides[0] - 1) // strides[0]\n+            conv_out = conv_out[:, :, :i, :]\n         if kernel_shape[3] % 2 == 0:\n-            conv_out = conv_out[:, :, :, :(x.shape[3] + strides[1] - 1) // strides[1]]\n+            i = (x.shape[3] + strides[1] - 1) // strides[1]\n+            conv_out = conv_out[:, :, :, :i]\n     if data_format == 'channels_last':\n         conv_out = conv_out.dimshuffle((0, 2, 3, 1))\n     return conv_out\n@@ -2039,11 +2066,14 @@ def _postprocess_conv3d_output(conv_out, x,\n                                strides, data_format):\n     if padding == 'same':\n         if kernel_shape[2] % 2 == 0:\n-            conv_out = conv_out[:, :, :(x.shape[2] + strides[0] - 1) // strides[0], :, :]\n+            i = (x.shape[2] + strides[0] - 1) // strides[0]\n+            conv_out = conv_out[:, :, :i, :, :]\n         if kernel_shape[3] % 2 == 0:\n-            conv_out = conv_out[:, :, :, :(x.shape[3] + strides[1] - 1) // strides[1], :]\n+            i = (x.shape[3] + strides[1] - 1) // strides[1]\n+            conv_out = conv_out[:, :, :, :i, :]\n         if kernel_shape[4] % 2 == 0:\n-            conv_out = conv_out[:, :, :, :, :(x.shape[4] + strides[2] - 1) // strides[2]]\n+            i = (x.shape[4] + strides[2] - 1) // strides[2]\n+            conv_out = conv_out[:, :, :, :, :i]\n     if data_format == 'channels_last':\n         conv_out = conv_out.dimshuffle((0, 2, 3, 4, 1))\n     return conv_out\n@@ -2178,7 +2208,8 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n     kernel = _preprocess_conv2d_kernel(kernel, data_format)\n \n     th_padding = _preprocess_padding(padding)\n-    op = T.nnet.abstract_conv.AbstractConv2d_gradInputs(imshp=None,\n+    op = T.nnet.abstract_conv.AbstractConv2d_gradInputs(\n+        imshp=None,\n         kshp=kernel_shape,\n         subsample=strides,\n         border_mode=th_padding,\n@@ -2207,7 +2238,8 @@ def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n         Output tensor.\n \n     # Raises\n-        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n+        ValueError: if `data_format` is neither `\"channels_last\"` or\n+        `\"channels_first\"`.\n     \"\"\"\n     data_format = normalize_data_format(data_format)\n     if isinstance(strides, int):\n@@ -2228,15 +2260,20 @@ def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n     image_shape = _preprocess_conv2d_image_shape(int_shape(x), data_format)\n     depthwise_kernel_shape = int_shape(depthwise_kernel)\n     if depthwise_kernel_shape is None:\n-        depthwise_kernel_shape = depthwise_kernel.eval().shape  # in case of a shared variable\n-    depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape(depthwise_kernel_shape, data_format)\n+        # in case of a shared variable\n+        depthwise_kernel_shape = depthwise_kernel.eval().shape\n+    depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape(\n+        depthwise_kernel_shape, data_format)\n     pointwise_kernel_shape = int_shape(pointwise_kernel)\n     if pointwise_kernel_shape is None:\n-        pointwise_kernel_shape = pointwise_kernel.eval().shape  # in case of a shared variable\n-    pointwise_kernel_shape = _preprocess_conv2d_filter_shape(pointwise_kernel_shape, data_format)\n+        # in case of a shared variable\n+        pointwise_kernel_shape = pointwise_kernel.eval().shape\n+    pointwise_kernel_shape = _preprocess_conv2d_filter_shape(\n+        pointwise_kernel_shape, data_format)\n \n     x = _preprocess_conv2d_input(x, data_format)\n-    depthwise_kernel = _preprocess_conv2d_depthwise_kernel(depthwise_kernel, depthwise_kernel_shape, data_format)\n+    depthwise_kernel = _preprocess_conv2d_depthwise_kernel(\n+        depthwise_kernel, depthwise_kernel_shape, data_format)\n     pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n     th_padding = _preprocess_padding(padding)\n \n@@ -2278,22 +2315,28 @@ def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n         Output tensor.\n \n     # Raises\n-        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n+        ValueError: if `data_format` is neither `\"channels_last\"` or\n+        `\"channels_first\"`.\n     \"\"\"\n     data_format = normalize_data_format(data_format)\n \n     image_shape = _preprocess_conv2d_image_shape(int_shape(x), data_format)\n     depthwise_kernel_shape = int_shape(depthwise_kernel)\n     if depthwise_kernel_shape is None:\n-        depthwise_kernel_shape = depthwise_kernel.eval().shape  # in case of a shared variable\n-    depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape(depthwise_kernel_shape, data_format)\n+        # in case of a shared variable\n+        depthwise_kernel_shape = depthwise_kernel.eval().shape\n+    depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape(\n+        depthwise_kernel_shape, data_format)\n     pointwise_kernel_shape = int_shape(pointwise_kernel)\n     if pointwise_kernel_shape is None:\n-        pointwise_kernel_shape = pointwise_kernel.eval().shape  # in case of a shared variable\n-    pointwise_kernel_shape = _preprocess_conv2d_filter_shape(pointwise_kernel_shape, data_format)\n+        # in case of a shared variable\n+        pointwise_kernel_shape = pointwise_kernel.eval().shape\n+    pointwise_kernel_shape = _preprocess_conv2d_filter_shape(\n+        pointwise_kernel_shape, data_format)\n \n     x = _preprocess_conv2d_input(x, data_format)\n-    depthwise_kernel = _preprocess_conv2d_depthwise_kernel(depthwise_kernel, depthwise_kernel_shape, data_format)\n+    depthwise_kernel = _preprocess_conv2d_depthwise_kernel(\n+        depthwise_kernel, depthwise_kernel_shape, data_format)\n     pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)\n     th_padding = _preprocess_padding(padding)\n \n@@ -2333,18 +2376,22 @@ def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n         Output tensor.\n \n     # Raises\n-        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n+        ValueError: if `data_format` is neither `\"channels_last\"` or\n+        `\"channels_first\"`.\n     \"\"\"\n     data_format = normalize_data_format(data_format)\n \n     image_shape = _preprocess_conv2d_image_shape(int_shape(x), data_format)\n     depthwise_kernel_shape = int_shape(depthwise_kernel)\n     if depthwise_kernel_shape is None:\n-        depthwise_kernel_shape = depthwise_kernel.eval().shape  # in case of a shared variable\n-    depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape(depthwise_kernel_shape, data_format)\n+        # in case of a shared variable\n+        depthwise_kernel_shape = depthwise_kernel.eval().shape\n+    depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape(\n+        depthwise_kernel_shape, data_format)\n \n     x = _preprocess_conv2d_input(x, data_format)\n-    depthwise_kernel = _preprocess_conv2d_depthwise_kernel(depthwise_kernel, depthwise_kernel_shape, data_format)\n+    depthwise_kernel = _preprocess_conv2d_depthwise_kernel(\n+        depthwise_kernel, depthwise_kernel_shape, data_format)\n     th_padding = _preprocess_padding(padding)\n \n     conv_out = T.nnet.conv2d(x, depthwise_kernel,\n@@ -2354,8 +2401,8 @@ def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n                              filter_shape=depthwise_kernel_shape,\n                              filter_dilation=dilation_rate,\n                              num_groups=image_shape[1])\n-    conv_out = _postprocess_conv2d_output(conv_out, x, padding,\n-                                          depthwise_kernel_shape, strides, data_format)\n+    conv_out = _postprocess_conv2d_output(\n+        conv_out, x, padding, depthwise_kernel_shape, strides, data_format)\n     return conv_out\n \n \n@@ -2454,8 +2501,10 @@ def pool2d(x, pool_size, strides=(1, 1), padding='valid',\n     assert pool_size[0] >= 1 and pool_size[1] >= 1\n \n     if padding == 'same':\n-        w_pad = pool_size[0] - 2 if pool_size[0] > 2 and pool_size[0] % 2 == 1 else pool_size[0] - 1\n-        h_pad = pool_size[1] - 2 if pool_size[1] > 2 and pool_size[1] % 2 == 1 else pool_size[1] - 1\n+        odd_pad_w = pool_size[0] > 2 and pool_size[0] % 2 == 1\n+        w_pad = pool_size[0] - 2 if odd_pad_w else pool_size[0] - 1\n+        odd_pad_h = pool_size[1] > 2 and pool_size[1] % 2 == 1\n+        h_pad = pool_size[1] - 2 if odd_pad_h else pool_size[1] - 1\n         pad = (w_pad, h_pad)\n     elif padding == 'valid':\n         pad = (0, 0)\n@@ -2660,7 +2709,8 @@ def ctc_update_log_p(skip_idxs, zeros, active, log_p_curr, log_p_prev):\n     # previous transitions\n     _p_prev = T.inc_subtensor(_p_prev[1:], _p_prev[:-1])\n     # skip transitions\n-    _p_prev = T.inc_subtensor(_p_prev[active_skip_idxs + 2], p_prev[active_skip_idxs])\n+    _p_prev = T.inc_subtensor(\n+        _p_prev[active_skip_idxs + 2], p_prev[active_skip_idxs])\n     updated_log_p_prev = T.log(_p_prev) + common_factor\n \n     log_p_next = T.set_subtensor(\n@@ -2671,24 +2721,30 @@ def ctc_update_log_p(skip_idxs, zeros, active, log_p_curr, log_p_prev):\n \n \n def ctc_path_probs(predict, Y, alpha=1e-4):\n-    smoothed_predict = (1 - alpha) * predict[:, Y] + alpha * np.float32(1.) / Y.shape[0]\n-    L = T.log(smoothed_predict)\n+    smoothed = (1 - alpha) * predict[:, Y] + alpha * np.float32(1.) / Y.shape[0]\n+    L = T.log(smoothed)\n     zeros = T.zeros_like(L[0])\n     log_first = zeros\n \n     f_skip_idxs = ctc_create_skip_idxs(Y)\n-    b_skip_idxs = ctc_create_skip_idxs(Y[::-1])  # there should be a shortcut to calculating this\n+    # there should be a shortcut to calculating this\n+    b_skip_idxs = ctc_create_skip_idxs(Y[::-1])\n \n     def step(log_f_curr, log_b_curr, f_active, log_f_prev, b_active, log_b_prev):\n-        f_active_next, log_f_next = ctc_update_log_p(f_skip_idxs, zeros, f_active, log_f_curr, log_f_prev)\n-        b_active_next, log_b_next = ctc_update_log_p(b_skip_idxs, zeros, b_active, log_b_curr, log_b_prev)\n+        f_active_next, log_f_next = ctc_update_log_p(\n+            f_skip_idxs, zeros, f_active, log_f_curr, log_f_prev)\n+        b_active_next, log_b_next = ctc_update_log_p(\n+            b_skip_idxs, zeros, b_active, log_b_curr, log_b_prev)\n         return f_active_next, log_f_next, b_active_next, log_b_next\n \n     [f_active, log_f_probs, b_active, log_b_probs], _ = theano.scan(\n-        step, sequences=[L, L[::-1, ::-1]], outputs_info=[np.int32(1), log_first, np.int32(1), log_first])\n+        step,\n+        sequences=[L, L[::-1, ::-1]],\n+        outputs_info=[np.int32(1), log_first, np.int32(1), log_first])\n \n     idxs = T.arange(L.shape[1]).dimshuffle('x', 0)\n-    mask = (idxs < f_active.dimshuffle(0, 'x')) & (idxs < b_active.dimshuffle(0, 'x'))[::-1, ::-1]\n+    mask = ((idxs < f_active.dimshuffle(0, 'x')) &\n+            (idxs < b_active.dimshuffle(0, 'x'))[::-1, ::-1])\n     log_probs = log_f_probs + log_b_probs[::-1, ::-1] - L\n     return log_probs, mask\n \n@@ -2696,7 +2752,8 @@ def ctc_path_probs(predict, Y, alpha=1e-4):\n def ctc_cost(predict, Y):\n     log_probs, mask = ctc_path_probs(predict, ctc_interleave_blanks(Y))\n     common_factor = T.max(log_probs)\n-    total_log_prob = T.log(T.sum(T.exp(log_probs - common_factor)[mask.nonzero()])) + common_factor\n+    total_log_prob = T.log(T.sum(T.exp(log_probs - common_factor)[mask.nonzero()]))\n+    total_log_prob = total_log_prob + common_factor\n     return -total_log_prob\n \n \n@@ -2706,8 +2763,8 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n \n     # Arguments\n         y_true: tensor (samples, max_string_length) containing the truth labels\n-        y_pred: tensor (samples, time_steps, num_categories) containing the prediction,\n-                or output of the softmax\n+        y_pred: tensor (samples, time_steps, num_categories) containing the\n+                prediction, or output of the softmax\n         input_length: tensor (samples,1) containing the sequence length for\n                 each batch item in y_pred\n         label_length: tensor (samples,1) containing the sequence length for\n@@ -2815,7 +2872,12 @@ def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n     return permute_dimensions(output, (1, 0, 2))\n \n \n-def local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format=None):\n+def local_conv2d(inputs,\n+                 kernel,\n+                 kernel_size,\n+                 strides,\n+                 output_shape,\n+                 data_format=None):\n     data_format = normalize_data_format(data_format)\n \n     stride_row, stride_col = strides\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5ef0fc3215fb43f5772673b31c51ad8a6582bc39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 86 | Lines Deleted: 26 | Files Changed: 1 | Hunks: 25 | Methods Changed: 17 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 112 | Churn Cumulative: 3554 | Contributors (this commit): 28 | Commits (past 90d): 10 | Contributors (cumulative): 28 | DMM Complexity: 0.34782608695652173\n\nDIFF:\n@@ -30,8 +30,12 @@ if dev.type() == 0:\n # A learning phase is a bool tensor used to run Keras models in\n # either train mode (learning_phase == 1) or test mode (learning_phase == 0).\n # LEARNING_PHASE_PLACEHOLDER is the placeholder for dynamic learning phase\n-_LEARNING_PHASE_PLACEHOLDER = C.constant(shape=(), dtype=np.float32, value=1.0, name='_keras_learning_phase')\n-# static learning phase flag, if it is not 0 or 1, we will go with dynamic learning phase tensor.\n+_LEARNING_PHASE_PLACEHOLDER = C.constant(\n+    shape=(), dtype=np.float32,\n+    value=1.0,\n+    name='_keras_learning_phase')\n+# static learning phase flag, if it is not 0 or 1, we will go with dynamic\n+# learning phase tensor.\n _LEARNING_PHASE = -1\n _UID_PREFIXES = defaultdict(int)\n \n@@ -58,7 +62,10 @@ def get_uid(prefix=''):\n \n def learning_phase():\n     # If _LEARNING_PHASE is not 0 or 1, return dynamic learning phase tensor\n-    return _LEARNING_PHASE if _LEARNING_PHASE in {0, 1} else _LEARNING_PHASE_PLACEHOLDER\n+    if _LEARNING_PHASE in {0, 1}:\n+        return _LEARNING_PHASE\n+    else:\n+        return _LEARNING_PHASE_PLACEHOLDER\n \n \n def set_learning_phase(value):\n@@ -240,7 +247,8 @@ def bias_add(x, bias, data_format=None):\n def eval(x):\n     if isinstance(x, C.cntk_py.Function):\n         return x.eval()\n-    elif isinstance(x, C.variables.Constant) or isinstance(x, C.variables.Parameter):\n+    elif (isinstance(x, C.variables.Constant) or isinstance(\n+            x, C.variables.Parameter)):\n         return x.value\n     else:\n         raise ValueError('CNTK Backend: `eval` method on '\n@@ -263,7 +271,11 @@ def placeholder(\n         if ndim:\n             shape = tuple([None for _ in range(ndim)])\n \n-    dynamic_dimension = C.FreeDimension if _get_cntk_version() >= 2.2 else C.InferredDimension\n+    if _get_cntk_version() >= 2.2:\n+        dynamic_dimension = C.FreeDimension\n+    else:\n+        dynamic_dimension = C.InferredDimension\n+\n     cntk_shape = [dynamic_dimension if s is None else s for s in shape]\n     cntk_shape = tuple(cntk_shape)\n \n@@ -398,7 +410,12 @@ def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     if seed is None:\n         # ensure that randomness is conditioned by the Numpy RNG\n         seed = np.random.randint(10e3)\n-    return C.random.uniform(shape=shape, dtype=dtype, low=minval, high=maxval, seed=seed)\n+    return C.random.uniform(\n+        shape=shape,\n+        dtype=dtype,\n+        low=minval,\n+        high=maxval,\n+        seed=seed)\n \n \n def random_uniform_variable(shape, low, high,\n@@ -470,7 +487,10 @@ def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     if seed is None:\n         # ensure that randomness is conditioned by the Numpy RNG\n         seed = np.random.randint(10e3)\n-    return C.random.normal(shape=shape, mean=mean, scale=stddev, seed=seed, dtype=dtype)\n+    return C.random.normal(\n+        shape=shape, mean=mean,\n+        scale=stddev, seed=seed,\n+        dtype=dtype)\n \n \n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n@@ -602,7 +622,9 @@ def gather(reference, indices):\n     else:\n         num_classes = reference.shape[0]\n         one_hot_matrix = C.ops.one_hot(indices, num_classes)\n-        return C.times(one_hot_matrix, reference, output_rank=len(reference.shape) - 1)\n+        return C.times(\n+            one_hot_matrix, reference,\n+            output_rank=len(reference.shape) - 1)\n \n \n def _remove_dims(x, axis, keepdims=False):\n@@ -703,7 +725,16 @@ def squeeze(x, axis):\n         del shape[_]\n \n     new_shape = shape[nones:]\n-    new_shape = tuple([C.InferredDimension if _ == C.FreeDimension else _ for _ in new_shape])\n+\n+    new_shape_temp = []\n+    for _ in new_shape:\n+        if _ == C.FreeDimension:\n+            new_shape_temp.append(C.InferredDimension)\n+        else:\n+            new_shape_temp.append(_)\n+\n+    new_shape = tuple(new_shape_temp)\n+\n     return C.reshape(x, new_shape)\n \n \n@@ -739,9 +770,11 @@ def _normalize_axis(axis, x):\n     nones = _get_dynamic_axis_num(x)\n \n     if nones > ndim:\n-        raise ValueError('CNTK Backend: tensor with keras shape: `%s` has '\n+        raise ValueError(\n+            'CNTK Backend: tensor with keras shape: `%s` has '\n             '%d cntk dynamic axis, this is not expected, please '\n-                         'double check the keras shape history.' % (str(shape), nones))\n+            'double check the keras shape history.'\n+            % (str(shape), nones))\n \n     # Current cntk does not support shape like (1, batch). so using the workaround\n     # here to mapping the correct axis. Will remove this tricky after we add support\n@@ -1103,7 +1136,15 @@ def flatten(x):\n \n \n def reshape(x, shape):\n-    shape = tuple([C.InferredDimension if _ == C.FreeDimension else _ for _ in shape])\n+    shape_temp = []\n+    for _ in shape:\n+        if _ == C.FreeDimension:\n+            shape_temp.append(C.InferredDimension)\n+        else:\n+            shape_temp.append(_)\n+\n+    shape = tuple(shape_temp)\n+\n     if isinstance(x, C.variables.Parameter):\n         return C.reshape(x, shape)\n     else:\n@@ -1144,7 +1185,8 @@ def permute_dimensions(x, pattern):\n     else:\n         current_layout = tuple([i for i in range(dims)])\n \n-    if num_dynamic_axis > 0 and pattern[:num_dynamic_axis] != current_layout[:num_dynamic_axis]:\n+    if (num_dynamic_axis > 0 and\n+            pattern[:num_dynamic_axis] != current_layout[:num_dynamic_axis]):\n                 raise ValueError('CNTK backend: the permute pattern %s '\n                                  'requested permute on dynamic axis, '\n                                  'which is not supported. Please do permute '\n@@ -1156,7 +1198,8 @@ def permute_dimensions(x, pattern):\n     return C.transpose(x, axis)\n \n \n-def resize_images(x, height_factor, width_factor, data_format, interpolation='nearest'):\n+def resize_images(x, height_factor, width_factor, data_format,\n+                  interpolation='nearest'):\n     if interpolation == 'nearest':\n         if data_format == 'channels_first':\n             output = repeat_elements(x, height_factor, axis=2)\n@@ -1381,7 +1424,8 @@ def rnn(step_function, inputs, initial_states,\n \n     need_convert = not has_seq_axis(inputs)\n     if go_backwards and need_convert is False:\n-        raise NotImplementedError('CNTK Backend: `go_backwards` is not supported with '\n+        raise NotImplementedError(\n+            'CNTK Backend: `go_backwards` is not supported with '\n             'variable-length sequences. Please specify a '\n             'static length for your sequences.')\n \n@@ -1404,7 +1448,9 @@ def rnn(step_function, inputs, initial_states,\n                 rnn_constants.append(new_c)\n             else:\n                 if _get_dynamic_axis_num(constant) == 1:\n-                    rnn_constants.append(C.sequence.broadcast_as(constant, rnn_inputs))\n+                    rnn_constants.append(C.sequence.broadcast_as(\n+                        constant,\n+                        rnn_inputs))\n                 else:\n                     rnn_constants.append(constant)\n     else:\n@@ -1422,7 +1468,8 @@ def rnn(step_function, inputs, initial_states,\n     with C.default_options(axis_offset=1):\n         def _recurrence(x, states, m):\n             # create place holder\n-            place_holders = [C.placeholder(dynamic_axes=x.dynamic_axes) for _ in states]\n+            place_holders = [C.placeholder(\n+                dynamic_axes=x.dynamic_axes) for _ in states]\n             past_values = []\n             for s, p in zip(states, place_holders):\n                 past_values.append(C.sequence.past_value(p, s))\n@@ -1434,7 +1481,12 @@ def rnn(step_function, inputs, initial_states,\n                 uses_learning_phase = True\n \n             if m is not None:\n-                new_states = [C.element_select(m, n, s) for n, s in zip(new_states, past_values)]\n+                new_states_temp = []\n+                for n, s in zip(new_states, past_values):\n+                    new_states_temp.append(C.element_select(m, n, s))\n+\n+                new_states = new_states_temp\n+\n             n_s = []\n             for o, p in zip(new_states, place_holders):\n                 n_s.append(o.replace_placeholders({p: o.output}))\n@@ -1457,7 +1509,8 @@ def rnn(step_function, inputs, initial_states,\n             if hasattr(C, 'unpack_batch'):\n                 f_stats.append(C.unpack_batch(l_s))\n             else:\n-                f_stats.append(C.user_function(ConvertToStatic(l_s, batch_size=i_s.shape[0])))\n+                f_stats.append(\n+                    C.user_function(ConvertToStatic(l_s, batch_size=i_s.shape[0])))\n         else:\n             f_stats.append(l_s)\n \n@@ -1504,7 +1557,8 @@ def conv1d(x, kernel, strides=1, padding='valid',\n     padding = _preprocess_border_mode(padding)\n \n     if dev.type() == 0 and dilation_rate != 1:\n-        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n+        raise ValueError(\n+            'Dilated convolution on CPU is not supported by CNTK backend. '\n             'Please set `dilation_rate` to 1. You passed: %s' % (dilation_rate,))\n \n     dilation_rate = (1, dilation_rate)\n@@ -1530,7 +1584,8 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',\n     padding = _preprocess_border_mode(padding)\n \n     if dev.type() == 0 and dilation_rate != (1, 1):\n-        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n+        raise ValueError(\n+            'Dilated convolution on CPU is not supported by CNTK backend. '\n             'Please set `dilation_rate` to (1, 1). '\n             'You passed: %s' % (dilation_rate,))\n \n@@ -1667,7 +1722,8 @@ def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n     padding = _preprocess_border_mode(padding)\n \n     if dev.type() == 0 and dilation_rate != (1, 1, 1):\n-        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n+        raise ValueError(\n+            'Dilated convolution on CPU is not supported by CNTK backend. '\n             'Please set `dilation_rate` to (1, 1, 1). '\n             'You passed: %s' % (dilation_rate,))\n \n@@ -1922,7 +1978,8 @@ class Function(object):\n                 unrelated_updates.extend(u_ops)\n \n             if len(unrelated_updates) > 0:\n-                self.unrelated_updates = C.combine([_.output for _ in unrelated_updates])\n+                self.unrelated_updates = C.combine(\n+                    [_.output for _ in unrelated_updates])\n \n         if self.trainer is None:\n             self.metrics_outputs = [f.output for f in outputs]\n@@ -1964,7 +2021,8 @@ class Function(object):\n                 # in current version cntk can't support input with variable\n                 # length. Will support it in next release.\n                 if not self._is_input_shape_compatible(value, tensor):\n-                    raise ValueError('CNTK backend: The placeholder has been resolved '\n+                    raise ValueError(\n+                        'CNTK backend: The placeholder has been resolved '\n                         'to shape `%s`, but input shape is `%s`. Currently '\n                         'CNTK can not take variable length inputs. Please '\n                         'pass inputs that have a static shape.'\n@@ -2006,7 +2064,8 @@ class Function(object):\n             # But the assign ops won't be executed under this mode, that's why\n             # we need this check.\n             if (self.unrelated_updates is None and\n-                    (_LEARNING_PHASE_PLACEHOLDER.value == 1.0 or _LEARNING_PHASE == 1)):\n+                    (_LEARNING_PHASE_PLACEHOLDER.value == 1.0 or\n+                        _LEARNING_PHASE == 1)):\n                 _, output_values = self.metrics_func.forward(\n                     input_dict,\n                     self.metrics_func.outputs,\n@@ -2469,7 +2528,8 @@ class ReshapeBatch(C.ops.functions.UserFunction):\n \n     def backward(self, state, root_gradients):\n         grad_array_view = root_gradients.data()\n-        num_element = root_gradients.shape()[0] * np.prod(np.asarray(self.target_shape))\n+        num_element = root_gradients.shape()[0] * np.prod(\n+            np.asarray(self.target_shape))\n         num_static_element = np.prod(np.asarray(self.from_shape))\n         num_old_batch = int(num_element / num_static_element)\n         return C.cntk_py.Value(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#37c0c1e1975e23bc96d31ed2c990412d0239a406", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 18 | Churn Cumulative: 168 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 13 | DMM Complexity: 0.8\n\nDIFF:\n@@ -61,7 +61,7 @@ def model_to_dot(model,\n     if isinstance(model, Sequential):\n         if not model.built:\n             model.build()\n-    layers = model.layers\n+    layers = model._layers\n \n     # Create graph nodes.\n     for layer in layers:\n@@ -108,6 +108,10 @@ def model_to_dot(model,\n             if node_key in model._network_nodes:\n                 for inbound_layer in node.inbound_layers:\n                     inbound_layer_id = str(id(inbound_layer))\n+                    # Make sure that both nodes exist before connecting them with\n+                    # an edge, as add_edge would otherwise create any missing node.\n+                    assert dot.get_node(inbound_layer_id)\n+                    assert dot.get_node(layer_id)\n                     dot.add_edge(pydot.Edge(inbound_layer_id, layer_id))\n     return dot\n \n\n@@ -4,6 +4,7 @@ import sys\n import numpy as np\n from keras.layers import Conv2D\n from keras.layers import Dense\n+from keras.layers import Embedding\n from keras.layers import Flatten\n from keras.layers import LSTM\n from keras.layers import TimeDistributed\n@@ -26,5 +27,16 @@ def test_plot_model():\n     os.remove('model2.png')\n \n \n+def test_plot_sequential_embedding():\n+    \"\"\"Fixes #11376\"\"\"\n+    model = Sequential()\n+    model.add(Embedding(10000, 256, input_length=400, name='embed'))\n+    vis_utils.plot_model(model,\n+                         to_file='model1.png',\n+                         show_shapes=True,\n+                         show_layer_names=True)\n+    os.remove('model1.png')\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c2244d2a4cb5f86968fb117f75469283a19b8a24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 31 | Files Changed: 1 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 52 | Churn Cumulative: 5629 | Contributors (this commit): 32 | Commits (past 90d): 41 | Contributors (cumulative): 32 | DMM Complexity: 0.1\n\nDIFF:\n@@ -1827,8 +1827,10 @@ class TestBackend(object):\n         koh = K.eval(K.one_hot(K.variable(indices, dtype='int32'), num_classes))\n         assert np.all(koh == oh)\n \n-    @pytest.mark.skipif(K.backend() == 'cntk',\n-                        reason='Sparse tensors are not supported in cntk.')\n+    @pytest.mark.skipif((K.backend() == 'cntk'\n+                         or (K.backend() == 'theano' and not K.th_sparse_module)),\n+                        reason='Sparse tensors are not supported in cntk '\n+                               'and Theano has some dependency issues for sparse.')\n     def test_sparse_dot(self):\n         x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n         x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n@@ -1838,20 +1840,17 @@ class TestBackend(object):\n         x_dense = x_sparse.toarray()\n \n         W = np.random.random((5, 4))\n-        # cntk not support it yet\n-        backends = [KTF]\n-        if KTH.th_sparse_module:\n-            # Theano has some dependency issues for sparse\n-            backends.append(KTH)\n-\n-        for k in backends:\n-            t_W = k.variable(W)\n-            k_s = k.eval(k.dot(k.variable(x_sparse), t_W))\n-            k_d = k.eval(k.dot(k.variable(x_dense), t_W))\n+        t_W = K.variable(W)\n+        k_s = K.eval(K.dot(K.variable(x_sparse), t_W))\n+        k_d = K.eval(K.dot(K.variable(x_dense), t_W))\n \n         assert k_s.shape == k_d.shape\n         assert_allclose(k_s, k_d, atol=1e-05)\n \n+    @pytest.mark.skipif((K.backend() == 'cntk'\n+                         or (K.backend() == 'theano' and not K.th_sparse_module)),\n+                        reason='Sparse tensors are not supported in cntk '\n+                               'and Theano has some dependency issues for sparse.')\n     def test_sparse_concat(self):\n         x_d = np.array([0, 7, 2, 3], dtype=np.float32)\n         x_r = np.array([0, 2, 2, 3], dtype=np.int64)\n@@ -1868,20 +1867,12 @@ class TestBackend(object):\n         x_dense_1 = x_sparse_1.toarray()\n         x_dense_2 = x_sparse_2.toarray()\n \n-        # cntk not support it yet\n-        backends = [KTF]\n-        if KTH.th_sparse_module:\n-            # Theano has some dependency issues for sparse\n-            backends.append(KTH)\n+        k_s = K.concatenate([K.variable(x_sparse_1), K.variable(x_sparse_2)])\n+        assert K.is_sparse(k_s)\n \n-        for k in backends:\n-            k_s = k.concatenate([k.variable(x_sparse_1), k.variable(x_sparse_2)])\n-            assert k.is_sparse(k_s)\n+        k_s_d = K.eval(k_s)\n \n-            k_s_d = k.eval(k_s)\n-\n-            k_d = k.eval(k.concatenate([k.variable(x_dense_1),\n-                                        k.variable(x_dense_2)]))\n+        k_d = K.eval(K.concatenate([K.variable(x_dense_1), K.variable(x_dense_2)]))\n \n         assert k_s_d.shape == k_d.shape\n         assert_allclose(k_s_d, k_d, atol=1e-05)\n@@ -1956,14 +1947,13 @@ class TestBackend(object):\n                 t = k.arange(10, dtype=dtype)\n                 assert k.dtype(t) == dtype\n \n-        for k in WITH_NP:\n-            start = k.constant(1, dtype='int32')\n-            t = k.arange(start)\n-            assert len(k.eval(t)) == 1\n+        start = K.constant(1, dtype='int32')\n+        t = K.arange(start)\n+        assert len(K.eval(t)) == 1\n \n-            start = k.constant(-1, dtype='int32')\n-            t = k.arange(start)\n-            assert len(k.eval(t)) == 0\n+        start = K.constant(-1, dtype='int32')\n+        t = K.arange(start)\n+        assert len(K.eval(t)) == 0\n \n     @pytest.mark.parametrize('training', [True, False])\n     def test_in_train_phase(self, training):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0a639230018d0c9e9f12476dc8b1c6e40c2dd0d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 9 | Files Changed: 4 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/2 | Churn Δ: 35 | Churn Cumulative: 25812 | Contributors (this commit): 159 | Commits (past 90d): 80 | Contributors (cumulative): 273 | DMM Complexity: 0.0\n\nDIFF:\n@@ -936,12 +936,14 @@ def pow(x, a):\n \n \n def clip(x, min_value, max_value):\n-    if max_value is not None and max_value < min_value:\n+    if (isinstance(min_value, (int, float)) and\n+            isinstance(max_value, (int, float))):\n+        if max_value < min_value:\n             max_value = min_value\n-    if max_value is None:\n-        max_value = np.inf\n     if min_value is None:\n         min_value = -np.inf\n+    if max_value is None:\n+        max_value = np.inf\n     return C.clip(x, min_value, max_value)\n \n \n\n@@ -1588,18 +1588,20 @@ def clip(x, min_value, max_value):\n \n     # Arguments\n         x: Tensor or variable.\n-        min_value: Python float or integer.\n-        max_value: Python float or integer.\n+        min_value: Python float, integer or tensor.\n+        max_value: Python float, integer or tensor.\n \n     # Returns\n         A tensor.\n     \"\"\"\n-    if max_value is not None and max_value < min_value:\n+    if (isinstance(min_value, (int, float)) and\n+            isinstance(max_value, (int, float))):\n+        if max_value < min_value:\n             max_value = min_value\n+    if min_value is None:\n+        min_value = -np.inf\n     if max_value is None:\n         max_value = np.inf\n-    min_value = _to_tensor(min_value, x.dtype.base_dtype)\n-    max_value = _to_tensor(max_value, x.dtype.base_dtype)\n     return tf.clip_by_value(x, min_value, max_value)\n \n \n\n@@ -694,8 +694,12 @@ def pow(x, a):\n \n \n def clip(x, min_value, max_value):\n-    if max_value is not None and max_value < min_value:\n+    if (isinstance(min_value, (int, float)) and\n+            isinstance(max_value, (int, float))):\n+        if max_value < min_value:\n             max_value = min_value\n+    if min_value is None:\n+        min_value = -np.inf\n     if max_value is None:\n         max_value = np.inf\n     return T.clip(x, min_value, max_value)\n\n@@ -2025,6 +2025,15 @@ class TestBackend(object):\n             with pytest.raises(TypeError):\n                 K.variable('', dtype='unsupported')\n \n+    def test_clip_supports_tensor_arguments(self):\n+        # GitHub issue: 11435\n+        x = K.variable([-10., -5., 0., 5., 10.])\n+        min_value = K.variable([-5., -4., 0., 3., 5.])\n+        max_value = K.variable([5., 4., 1., 4., 9.])\n+\n+        assert np.allclose(K.eval(K.clip(x, min_value, max_value)),\n+                           np.asarray([-5., -4., 0., 4., 9.], dtype=np.float32))\n+\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b9a74813081303399e8fce0f9f5f37f6a8343b03", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 12055 | Contributors (this commit): 66 | Commits (past 90d): 13 | Contributors (cumulative): 66 | DMM Complexity: None\n\nDIFF:\n@@ -249,10 +249,11 @@ class Conv1D(_Conv):\n     it is applied to the outputs as well.\n \n     When using this layer as the first layer in a model,\n-    provide an `input_shape` argument\n-    (tuple of integers or `None`, e.g.\n-    `(10, 128)` for sequences of 10 vectors of 128-dimensional vectors,\n-    or `(None, 128)` for variable-length sequences of 128-dimensional vectors.\n+    provide an `input_shape` argument (tuple of integers or `None`, does not\n+    include the batch axis), e.g. `input_shape=(10, 128)` for time series\n+    sequences of 10 time steps with 128 features per step in\n+    `data_format=\"channels_last\"`, or `(None, 128)` for variable-length\n+    sequences with 128 features per step.\n \n     # Arguments\n         filters: Integer, the dimensionality of the output space\n@@ -375,7 +376,7 @@ class Conv2D(_Conv):\n \n     When using this layer as the first layer in a model,\n     provide the keyword argument `input_shape`\n-    (tuple of integers, does not include the sample axis),\n+    (tuple of integers, does not include the batch axis),\n     e.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\n     in `data_format=\"channels_last\"`.\n \n@@ -506,7 +507,7 @@ class Conv3D(_Conv):\n \n     When using this layer as the first layer in a model,\n     provide the keyword argument `input_shape`\n-    (tuple of integers, does not include the sample axis),\n+    (tuple of integers, does not include the batch axis),\n     e.g. `input_shape=(128, 128, 128, 1)` for 128x128x128 volumes\n     with a single channel,\n     in `data_format=\"channels_last\"`.\n@@ -636,7 +637,7 @@ class Conv2DTranspose(Conv2D):\n \n     When using this layer as the first layer in a model,\n     provide the keyword argument `input_shape`\n-    (tuple of integers, does not include the sample axis),\n+    (tuple of integers, does not include the batch axis),\n     e.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\n     in `data_format=\"channels_last\"`.\n \n@@ -909,7 +910,7 @@ class Conv3DTranspose(Conv3D):\n \n     When using this layer as the first layer in a model,\n     provide the keyword argument `input_shape`\n-    (tuple of integers, does not include the sample axis),\n+    (tuple of integers, does not include the batch axis),\n     e.g. `input_shape=(128, 128, 128, 3)` for a 128x128x128 volume with 3 channels\n     if `data_format=\"channels_last\"`.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#36b9e4c055f32718a036cabaf767325b010c7485", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1784 | Contributors (this commit): 31 | Commits (past 90d): 3 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -80,6 +80,7 @@ from keras import models\n from keras import losses\n from keras import metrics\n from keras import backend\n+from keras import constraints\n from keras import activations\n from keras import preprocessing\n \n@@ -327,6 +328,10 @@ PAGES = [\n         'page': 'backend.md',\n         'all_module_functions': [backend],\n     },\n+    {\n+        'page': 'constraints.md',\n+        'all_module_classes': [constraints],\n+    },\n     {\n         'page': 'utils.md',\n         'functions': [utils.to_categorical,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
