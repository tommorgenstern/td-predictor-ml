{"custom_id": "keras#b4e2dd8d9e3222b55706ac14459d0d24ed430f5f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 749 | Contributors (this commit): 7 | Commits (past 90d): 27 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -361,20 +361,18 @@ class AutoEncoder(Layer):\n         return self.encoders[-1].get_output(train)\n \n     def _tranpose_weights(self, src, dest):\n-        if len(dest.shape) > 1:\n+        if len(dest.shape) > 1 and len(src.shape) > 1:\n             dest = src.T\n \n     def get_output(self, train):\n         if not train and not self.output_reconstruction:\n             return self._get_hidden(train)\n \n-        decode = self.decoders[-1].get_output(train)\n-\n         if self.tie_weights:\n             for e,d in zip(self.encoders, self.decoders):\n                 map(self._tranpose_weights, e.get_weights(), d.get_weights())\n \n-        return decode\n+        return self.decoders[-1].get_output(train)\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2ee57938b881e1f47430ed26a7be22f72ef75426", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 994 | Contributors (this commit): 7 | Commits (past 90d): 32 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -306,7 +306,7 @@ class AutoEncoder(Layer):\n             raise Exception(\"There need to be an equal number of encoders and decoders\")\n \n         # connect all encoders & decoders to their previous (respectively)\n-        for i in xrange(len(encoders)-1, 0, -1):\n+        for i in range(len(encoders)-1, 0, -1):\n             encoders[i].connect(encoders[i-1])\n             decoders[i].connect(decoders[i-1])\n         decoders[0].connect(encoders[-1])  # connect the first to the last\n@@ -386,7 +386,7 @@ class DenoisingAutoEncoder(AutoEncoder):\n     '''\n         A denoising autoencoder model that inherits the base features from autoencoder\n     '''\n-    def __init__(self, encoders=None, decoders=None, output_reconstruction=True, tie_weights=False, weights=None, corruption_level=0.3):\n+    def __init__(self, encoders=[], decoders=[], output_reconstruction=True, tie_weights=False, weights=None, corruption_level=0.3):\n         super(DenoisingAutoEncoder, self).__init__(encoders, decoders, output_reconstruction, tie_weights, weights)\n         self.corruption_level = corruption_level\n \n\n@@ -54,7 +54,6 @@ model_classical.compile(loss='categorical_crossentropy', optimizer='adam')\n model_classical.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=(X_test, Y_test))\n classical_score = model_classical.evaluate(X_test, Y_test, verbose=0)\n print('\\nclassical_score:', classical_score)\n-print('\\n')\n \n ##########################\n # autoencoder model test #\n@@ -68,9 +67,9 @@ def build_lstm_autoencoder(autoencoder, X_train, X_test):\n \n \t# The TimeDistributedDense isn't really necessary, however you need a lot of GPU memory to do 784x394-394x784\n \tautoencoder.add(TimeDistributedDense(input_dim, 16))\n-\tautoencoder.add(AutoEncoder(encoders=[LSTM(16, 8, activation=activation, return_sequences=True)]\n-\t\t\t\t\t\t\t\t, decoders=[LSTM(8, input_dim, activation=activation, return_sequences=True)]\n-\t\t\t\t\t\t\t\t, output_reconstruction=False, tie_weights=True))\n+\tautoencoder.add(AutoEncoder(encoders=[LSTM(16, 8, activation=activation, return_sequences=True)],\n+\t\t\t\t\t\t\t\tdecoders=[LSTM(8, input_dim, activation=activation, return_sequences=True)],\n+\t\t\t\t\t\t\t\toutput_reconstruction=False, tie_weights=True))\n \treturn autoencoder, X_train, X_test\n \n def build_deep_classical_autoencoder(autoencoder):\n@@ -83,9 +82,9 @@ def build_denoising_autoencoder(autoencoder):\n \t# You need another layer before a denoising autoencoder\n \t# This is similar to the dropout layers, etc..\n \tautoencoder.add(Dense(input_dim, input_dim))\n-\tautoencoder.add(DenoisingAutoEncoder(encoders=[Dense(input_dim, hidden_dim, activation=activation)]\n-\t\t\t\t\t\t\t\t\t\t, decoders=[Dense(hidden_dim, input_dim, activation=activation)]\n-\t\t\t\t\t\t\t\t\t\t, output_reconstruction=False, tie_weights=True, corruption_level=0.3))\n+\tautoencoder.add(DenoisingAutoEncoder(encoders=[Dense(input_dim, hidden_dim, activation=activation)],\n+\t\t\t\t\t\t\t\t\t\t decoders=[Dense(hidden_dim, input_dim, activation=activation)],\n+\t\t\t\t\t\t\t\t\t\t output_reconstruction=False, tie_weights=True, corruption_level=0.3))\n \treturn autoencoder\n \n # Build our autoencoder model\n@@ -133,7 +132,6 @@ model.fit(prefilter_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, sh\n \n score = model.evaluate(prefilter_test, Y_test, verbose=0, show_accuracy=False)\n print('\\nscore:', score)\n-print('\\n')\n \n if score < classical_score:\n \tprint(\"error: classical score > autoencoder score!\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8c1fe9193f55fd758af74d3270e5630db630fa2c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 29 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 158 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -59,35 +59,6 @@ if np.min(b)==0 and np.min(a)!=0:\n else:\n \traise ValueError('Nonneg test failed!')\n \t\n-model = Sequential()\n-model.add(Dense(784, 20))\n-model.add(Activation('relu', target=.4))\n-model.add(Dropout(0.1))\n-model.add(Dense(20, 20))\n-model.add(Activation('relu', target=.3))\n-model.add(Dropout(0.1))\n-model.add(Dense(20, 10))\n-model.add(Activation('softmax'))\n-\n-\n-rms = RMSprop()\n-model.compile(loss='categorical_crossentropy', optimizer=rms)\n-\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=20, show_accuracy=True, verbose=0)\n-\n-\n-\n-get_activations1 = theano.function([model.layers[0].input], model.layers[1].output(train=False), allow_input_downcast=True)\n-activations1 = get_activations1(X_train)\n-get_activations2 = theano.function([model.layers[0].input], model.layers[4].output(train=False), allow_input_downcast=True)\n-activations2 = get_activations2(X_train)\n-\n-if np.isclose(np.mean(activations1), .4, atol=.02) and np.isclose(np.mean(activations2), .3, atol=.02):\n-\tprint('KL penalty test passed')\n-else:\n-\traise ValueError('KL penalty test failed!')\n-\t\n-\n \n model = Sequential()\n model.add(Dense(784, 20))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#983c68bf4738f54e74921a52eb2b717b8b9ada41", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 757 | Contributors (this commit): 7 | Commits (past 90d): 29 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -336,12 +336,12 @@ class AutoEncoder(Layer):\n \n     def get_weights(self):\n         weights = []\n-        for m in encoders + decoders:\n+        for m in self.encoders + self.decoders:\n             weights += m.get_weights()\n         return weights\n \n     def set_weights(self, weights):\n-        models = encoders + decoders\n+        models = self.encoders + self.decoders\n         for i in range(len(models)):\n             nb_param = len(models[i].params)\n             models[i].set_weights(weights[:nb_param])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c4a84987c7f8732de44c87cb47d2fc16804cd9a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 759 | Contributors (this commit): 8 | Commits (past 90d): 30 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,6 +30,8 @@ class Layer(object):\n \n     def set_weights(self, weights):\n         for p, w in zip(self.params, weights):\n+            if p.eval().shape != w.shape:\n+                raise Exception(\"Layer shape %s not compatible with weight shape %s.\" % (p.eval().shape, w.shape))\n             p.set_value(floatX(w))\n \n     def get_weights(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ef01bacf434006eb09162a11fbc0362cb2994c21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 109 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 109 | Churn Cumulative: 549 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 4 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -180,3 +180,40 @@ class History(Callback):\n             self.validation_loss.append(val_loss)\n             if self.params['show_accuracy']:\n                 self.validation_accuracy.append(val_acc)\n+\n+class ModelCheckpoint(Callback):\n+    def __init__(self, filename=\"model_weights.hdf5\", path=\".\", verbose=0, save_best_only=False):\n+        super(Callback, self).__init__()\n+        import os\n+        self.store_filename = os.path.join(path, filename)\n+        self.verbose = verbose\n+\n+        self.save_best_only = save_best_only\n+        self.loss = []\n+        self.best_loss = np.Inf\n+        self.val_loss = []\n+        self.best_val_loss = np.Inf\n+\n+    def on_epoch_end(self, epoch, logs={}):\n+        '''currently, on_epoch_end receives epoch_logs from keras.models.Sequential.fit\n+        which does only contain, if at all, the validation loss and validation accuracy'''\n+        if self.save_best_only and self.params['do_validation']:\n+            cur_val_loss = logs.get('val_loss')\n+            self.val_loss.append(cur_val_loss)\n+            if cur_val_loss < self.best_val_loss:\n+                if self.verbose > 0:\n+                    print(\"Epoch %05d: valdidation loss improved from %0.5f to %0.5f, saving model to %s\"\n+                        % (epoch, self.best_val_loss, cur_val_loss, self.store_filename))\n+                self.best_val_loss = cur_val_loss\n+                self.model.save_weights(self.store_filename, overwrite=True)\n+            else:\n+                if self.verbose > 0:\n+                    print(\"Epoch %05d: validation loss did not improve\" % (epoch))\n+        elif self.save_best_only and not self.params['do_validation']:\n+            import warnings\n+            warnings.warn(\"Can save best model only with validation data, skipping\", RuntimeWarning)\n+        elif not self.save_best_only:\n+            if self.verbose > 0:\n+                print(\"Epoch %05d: saving model to %s\" % (epoch, self.store_filename))\n+            self.model.save_weights(self.store_filename, overwrite=True)\n+\n\n@@ -0,0 +1,72 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+from keras.datasets import mnist\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation, Merge\n+from keras.utils import np_utils\n+import keras.callbacks as cbks\n+import numpy as np\n+\n+nb_classes = 10\n+batch_size = 128\n+nb_epoch = 20\n+\n+# small sample size to overfit on training data\n+max_train_samples = 50\n+max_test_samples = 1000\n+\n+np.random.seed(1337) # for reproducibility\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+\n+X_train = X_train.reshape(60000,784)[:max_train_samples]\n+X_test = X_test.reshape(10000,784)[:max_test_samples]\n+X_train = X_train.astype(\"float32\")\n+X_test = X_test.astype(\"float32\")\n+X_train /= 255\n+X_test /= 255\n+\n+# convert class vectors to binary class matrices\n+Y_train = np_utils.to_categorical(y_train, nb_classes)[:max_train_samples]\n+Y_test = np_utils.to_categorical(y_test, nb_classes)[:max_test_samples]\n+\n+##########################\n+# model checkpoint tests #\n+##########################\n+\n+# Create a slightly larger network than required to test best validation save only \n+model = Sequential()\n+model.add(Dense(784, 500))\n+model.add(Activation('relu'))\n+model.add(Dense(500, 10))\n+model.add(Activation('softmax'))\n+model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+# test file location\n+path = \"/tmp\"\n+filename = \"model_weights.hdf5\"\n+import os\n+f = os.path.join(path, filename)\n+\n+print(\"Test model checkpointer\")\n+# only store best validation model in checkpointer\n+checkpointer = cbks.ModelCheckpoint(filename=filename, path=path, verbose=1, save_best_only=True)\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=(X_test, Y_test), callbacks =[checkpointer])\n+\n+if not os.path.isfile(f):\n+    raise Exception(\"Model weights were not saved to %s\" % (f))\n+\n+print(\"Test model checkpointer without validation data\")\n+import warnings\n+warnings.filterwarnings('error')\n+try:\n+    # this should issue a warning\n+    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, callbacks =[checkpointer])\n+except:\n+    print(\"Tests passed\")\n+    import sys\n+    sys.exit(0)\n+\n+raise Exception(\"Tests did not pass\")\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a6824931e46c461ae7e62d55de8361f64b0550ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 30 | Churn Cumulative: 1105 | Contributors (this commit): 16 | Commits (past 90d): 47 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -52,6 +52,16 @@ def slice_X(X, start=None, stop=None):\n         else:\n             return X[start:stop]\n \n+def calculate_class_weights(Y, class_weight):\n+    if isinstance(class_weight, dict):\n+        if Y.shape[1] > 1:\n+            y_classes = Y.argmax(axis=1)\n+        else:\n+            y_classes = Y\n+        w = np.array(map(lambda x: class_weight[x], y_classes))\n+    else:\n+        w = np.ones((Y.shape[0]))\n+    return w\n \n class Model(object):\n \n@@ -61,16 +71,6 @@ class Model(object):\n     def get_input(self, train):\n         raise NotImplementedError\n \n-    def calculate_class_weights(self, Y, class_weight):\n-        # this method expects a binary class indicator matrix\n-        # find class index in indicator matrix and map it to a weight vector w\n-        y_classes = Y.argmax(axis=1)\n-        if isinstance(class_weight, dict):\n-            w = np.array(map(lambda x: class_weight[x], y_classes))\n-        else:\n-            w = np.ones((Y.shape[0]))\n-        return w\n-\n     def compile(self, optimizer, loss, class_mode=\"categorical\", theano_mode=None):\n         self.optimizer = optimizers.get(optimizer)\n         self.loss = objectives.get(loss)\n@@ -105,7 +105,7 @@ class Model(object):\n         updates = self.optimizer.get_updates(self.params, self.regularizers, self.constraints, train_loss)\n \n         if type(self.X_train) == list:\n-            train_ins = self.X_train + [self.y] + [self.class_weights]\n+            train_ins = self.X_train + [self.y, self.class_weights]\n             test_ins = self.X_test + [self.y]\n             predict_ins = self.X_test\n         else:\n@@ -180,9 +180,9 @@ class Sequential(Model):\n         y = standardize_y(y)\n \n         # calculate the weight vector for the loss function\n-        w = self.calculate_class_weights(y, class_weight)\n+        w = calculate_class_weights(y, class_weight)\n \n-        ins = X + [y] + [w]\n+        ins = X + [y, w]\n         if accuracy:\n             return self._train_with_acc(*ins)\n         else:\n@@ -258,14 +258,14 @@ class Sequential(Model):\n                 X_batch = slice_X(X, batch_ids)\n                 y_batch = y[batch_ids]\n                 # calculate weight vector for current batch\n-                w = self.calculate_class_weights(y_batch, class_weight)\n+                w = calculate_class_weights(y_batch, class_weight)\n \n                 batch_logs = {}\n                 batch_logs['batch'] = batch_index\n                 batch_logs['size'] = len(batch_ids)\n                 callbacks.on_batch_begin(batch_index, batch_logs)\n \n-                ins = X_batch + [y_batch] + [w]\n+                ins = X_batch + [y_batch, w]\n                 if show_accuracy:\n                     loss, acc = self._train_with_acc(*ins)\n                     batch_logs['accuracy'] = acc\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#206f29ea6e4ef5bf6d389f711bef23dc09f9367d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 10 | Churn Cumulative: 1134 | Contributors (this commit): 16 | Commits (past 90d): 54 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -56,6 +56,8 @@ def calculate_class_weights(Y, class_weight):\n     if isinstance(class_weight, dict):\n         if Y.shape[1] > 1:\n             y_classes = Y.argmax(axis=1)\n+        elif Y.shape[1] == 1:\n+            y_classes = np.reshape(Y, Y.shape[0])\n         else:\n             y_classes = Y\n         w = np.array(map(lambda x: class_weight[x], y_classes))\n\n@@ -31,9 +31,13 @@ def categorical_crossentropy(y_true, y_pred, weight=None):\n     else:\n         return cce.mean()\n \n-def binary_crossentropy(y_true, y_pred):\n+def binary_crossentropy(y_true, y_pred, weight=None):\n     y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n-    return T.nnet.binary_crossentropy(y_pred, y_true).mean()\n+    bce = T.nnet.binary_crossentropy(y_pred, y_true).mean()\n+    if weight is not None:\n+        return (weight*bce).mean()\n+    else:\n+        return bce.mean()\n \n # aliases\n mse = MSE = mean_squared_error\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2084c1604dcf8850d3f35052a78b2288c268432a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 126 | Lines Deleted: 81 | Files Changed: 3 | Hunks: 13 | Methods Changed: 20 | Complexity Δ (Sum/Max): -2/19 | Churn Δ: 207 | Churn Cumulative: 2073 | Contributors (this commit): 18 | Commits (past 90d): 81 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,62 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import absolute_import\n+\n+import theano.tensor as T\n+from ..layers.core import Layer\n+from six.moves import range\n+\n+def ndim_tensor(ndim):\n+    if ndim == 2:\n+        return T.matrix()\n+    elif ndim == 3:\n+        return T.tensor3()\n+    elif ndim == 4:\n+        return T.tensor4()\n+    return T.matrix()\n+\n+class Sequential(Layer):\n+    def __init__(self, layers=[]):\n+        self.layers = []\n+        self.params = []\n+        self.regularizers = []\n+        self.constraints = []\n+        for layer in layers:\n+            self.add(layer)\n+\n+    def add(self, layer):\n+        self.layers.append(layer)\n+        if len(self.layers) > 1:\n+            self.layers[-1].connect(self.layers[-2])\n+        \n+        params, regularizers, constraints = layer.get_params()\n+        self.params += params\n+        self.regularizers += regularizers\n+        self.constraints += constraints\n+\n+    def get_output(self, train=False):\n+        return self.layers[-1].get_output(train)\n+\n+    def get_input(self, train=False):\n+        if not hasattr(self.layers[0], 'input'):\n+            for l in self.layers:\n+                if hasattr(l, 'input'):\n+                    break\n+            ndim = l.input.ndim \n+            self.layers[0].input = ndim_tensor(ndim)\n+        return self.layers[0].get_input(train)\n+\n+    def get_weights(self):\n+        weights = []\n+        for layer in self.layers:\n+            weights += layer.get_weights()\n+        return weights\n+\n+    def set_weights(self, weights):\n+        for i in range(len(self.layers)):\n+            nb_param = len(self.layers[i].params)\n+            self.layers[i].set_weights(weights[:nb_param])\n+            weights = weights[nb_param:]\n+\n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"layers\":[layer.get_config() for layer in self.layers]}\n\\ No newline at end of file\n\n@@ -7,6 +7,8 @@ import theano.tensor as T\n from .. import activations, initializations\n from ..utils.theano_utils import shared_zeros, floatX\n from ..utils.generic_utils import make_tuple\n+from .. import regularizers\n+from .. import constraints\n \n from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n from six.moves import zip\n@@ -16,8 +18,8 @@ class Layer(object):\n     def __init__(self):\n         self.params = []\n \n-    def connect(self, node):\n-        self.previous = node\n+    def connect(self, layer):\n+        self.previous = layer\n \n     def get_output(self, train):\n         raise NotImplementedError\n@@ -41,6 +43,34 @@ class Layer(object):\n     def get_config(self):\n         return {\"name\":self.__class__.__name__}\n \n+    def get_params(self):\n+        regs = []\n+        consts = []\n+\n+        if hasattr(self, 'regularizers') and len(self.regularizers) == len(self.params):\n+            for r in self.regularizers:\n+                if r:\n+                    regs.append(r)\n+                else:\n+                    regs.append(regularizers.identity)\n+        elif hasattr(self, 'regularizer') and self.regularizer:\n+            regs += [self.regularizer for _ in range(len(self.params))]\n+        else:\n+            regs += [regularizers.identity for _ in range(len(self.params))]\n+\n+        if hasattr(self, 'constraints') and len(self.constraints) == len(self.params):\n+            for c in self.constraints:\n+                if c:\n+                    consts.append(c)\n+                else:\n+                    consts.append(constraints.identity)\n+        elif hasattr(self, 'constraint') and self.constraint:\n+            consts += [self.constraint for _ in range(len(self.params))]\n+        else:\n+            consts += [constraints.identity for _ in range(len(self.params))]\n+\n+        return self.params, regs, consts\n+\n \n class Merge(object): \n     def __init__(self, models, mode='sum'):\n@@ -59,6 +89,9 @@ class Merge(object):\n             self.regularizers += m.regularizers\n             self.constraints += m.constraints\n \n+    def get_params(self):\n+        return self.params, self.regularizers, self.constraints\n+\n     def get_output(self, train=False):\n         if self.mode == 'sum':\n             s = self.models[0].get_output(train)\n\n@@ -3,15 +3,15 @@ from __future__ import print_function\n import theano\n import theano.tensor as T\n import numpy as np\n-import warnings\n+import warnings, time, copy\n \n from . import optimizers\n from . import objectives\n from . import regularizers\n from . import constraints\n from . import callbacks as cbks\n-import time, copy\n from .utils.generic_utils import Progbar, printv\n+from .layers import containers\n from six.moves import range\n \n def standardize_y(y):\n@@ -25,15 +25,6 @@ def make_batches(size, batch_size):\n     nb_batch = int(np.ceil(size/float(batch_size)))\n     return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n \n-def ndim_tensor(ndim):\n-    if ndim == 2:\n-        return T.matrix()\n-    elif ndim == 3:\n-        return T.tensor3()\n-    elif ndim == 4:\n-        return T.tensor4()\n-    return T.matrix()\n-\n def standardize_X(X):\n     if type(X) == list:\n         return X\n@@ -55,12 +46,6 @@ def slice_X(X, start=None, stop=None):\n \n class Model(object):\n \n-    def get_output(self, train):\n-        raise NotImplementedError\n-\n-    def get_input(self, train):\n-        raise NotImplementedError\n-\n     def compile(self, optimizer, loss, class_mode=\"categorical\", theano_mode=None):\n         self.optimizer = optimizers.get(optimizer)\n         self.loss = objectives.get(loss)\n@@ -112,57 +97,6 @@ class Model(object):\n             allow_input_downcast=True, mode=theano_mode)\n \n \n-class Sequential(Model):\n-    def __init__(self):\n-        self.layers = []\n-        self.params = [] # learnable\n-        self.regularizers = [] # same size as params\n-        self.constraints = [] # same size as params\n-\n-\n-    def add(self, layer):\n-        self.layers.append(layer)\n-        if len(self.layers) > 1:\n-            self.layers[-1].connect(self.layers[-2])\n-        self.params += [p for p in layer.params]\n-        \n-        if hasattr(layer, 'regularizers') and len(layer.regularizers) == len(layer.params):\n-            for r in layer.regularizers:\n-                if r:\n-                    self.regularizers.append(r)\n-                else:\n-                    self.regularizers.append(regularizers.identity)\n-        elif hasattr(layer, 'regularizer') and layer.regularizer:\n-            self.regularizers += [layer.regularizer for _ in range(len(layer.params))]\n-        else:\n-            self.regularizers += [regularizers.identity for _ in range(len(layer.params))]\n-\n-        if hasattr(layer, 'constraints') and len(layer.constraints) == len(layer.params):\n-            for c in layer.constraints:\n-                if c:\n-                    self.constraints.append(c)\n-                else:\n-                    self.constraints.append(constraints.identity)\n-        elif hasattr(layer, 'constraint') and layer.constraint:\n-            self.constraints += [layer.constraint for _ in range(len(layer.params))]\n-        else:\n-            self.constraints += [constraints.identity for _ in range(len(layer.params))]\n-\n-\n-    def get_output(self, train=False):\n-        return self.layers[-1].get_output(train)\n-\n-\n-    def get_input(self, train=False):\n-        if not hasattr(self.layers[0], 'input'):\n-            for l in self.layers:\n-                if hasattr(l, 'input'):\n-                    break\n-            ndim = l.input.ndim \n-            self.layers[0].input = ndim_tensor(ndim)\n-        return self.layers[0].get_input(train)\n-\n-\n     def train(self, X, y, accuracy=False):\n         X = standardize_X(X)\n         y = standardize_y(y)\n@@ -345,6 +279,33 @@ class Sequential(Model):\n         else:\n             return tot_score / seen\n \n+\n+class Sequential(Model, containers.Sequential):\n+    '''\n+        Inherits from Model the following methods:\n+            - compile\n+            - train\n+            - test\n+            - evaluate\n+            - fit\n+            - predict\n+            - predict_proba\n+            - predict_classes\n+\n+        Inherits from containers.Sequential the following methods:\n+            - add \n+            - get_output\n+            - get_input\n+            - get_weights\n+            - set_weights\n+    '''\n+    def __init__(self):\n+        self.layers = []\n+        self.params = [] # learnable\n+        self.regularizers = [] # same size as params\n+        self.constraints = [] # same size as params\n+\n+\n     def get_config(self, verbose=0):\n         layers = []\n         for i, l in enumerate(self.layers):\n@@ -354,17 +315,6 @@ class Sequential(Model):\n             printv(layers)\n         return layers\n \n-    def get_weights(self):\n-        res = []\n-        for l in self.layers:\n-            res += l.get_weights()\n-        return res\n-\n-    def set_weights(self, weights):\n-        for i in range(len(self.layers)):\n-            nb_param = len(self.layers[i].params)\n-            self.layers[i].set_weights(weights[:nb_param])\n-            weights = weights[nb_param:]\n \n     def save_weights(self, filepath, overwrite=False):\n         # Save weights from all layers to HDF5\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a32b7ef00471333a1496b36ce4fc8d6e75d5957e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1217 | Contributors (this commit): 16 | Commits (past 90d): 50 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -7,8 +7,6 @@ import warnings, time, copy\n \n from . import optimizers\n from . import objectives\n-from . import regularizers\n-from . import constraints\n from . import callbacks as cbks\n from .utils.generic_utils import Progbar, printv\n from .layers import containers\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6892a20a9290c6a19e05ece4a428ed2c11461451", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 8 | Churn Cumulative: 1225 | Contributors (this commit): 16 | Commits (past 90d): 51 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -320,7 +320,13 @@ class Sequential(Model, containers.Sequential):\n         import os.path\n         # if file exists and should not be overwritten\n         if not overwrite and os.path.isfile(filepath):\n-            raise IOError('%s already exists' % (filepath))\n+            overwrite = input('[WARNING] %s already exists - overwrite? [y/n]' % (filepath))\n+            while overwrite not in ['y', 'n']:\n+                overwrite = input('Enter \"y\" (overwrite) or \"n\" (cancel).')\n+            if overwrite == 'n':\n+                return\n+            print('[TIP] Next time specify overwrite=True in save_weights!')\n+\n         f = h5py.File(filepath, 'w')\n         f.attrs['nb_layers'] = len(self.layers)\n         for k, l in enumerate(self.layers):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#01682915e824bef81456c9ba526b7293308e7ab4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 566 | Contributors (this commit): 3 | Commits (past 90d): 14 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -182,12 +182,11 @@ class History(Callback):\n                 self.validation_accuracy.append(val_acc)\n \n class ModelCheckpoint(Callback):\n-    def __init__(self, filename=\"model_weights.hdf5\", path=\".\", verbose=0, save_best_only=False):\n+    def __init__(self, filepath, verbose=0, save_best_only=False):\n         super(Callback, self).__init__()\n-        import os\n-        self.store_filename = os.path.join(path, filename)\n-        self.verbose = verbose\n         \n+        self.verbose = verbose\n+        self.filepath = filepath\n         self.save_best_only = save_best_only\n         self.loss = []\n         self.best_loss = np.Inf\n@@ -203,9 +202,9 @@ class ModelCheckpoint(Callback):\n             if cur_val_loss < self.best_val_loss:\n                 if self.verbose > 0:\n                     print(\"Epoch %05d: valdidation loss improved from %0.5f to %0.5f, saving model to %s\"\n-                        % (epoch, self.best_val_loss, cur_val_loss, self.store_filename))\n+                        % (epoch, self.best_val_loss, cur_val_loss, self.filepath))\n                 self.best_val_loss = cur_val_loss\n-                self.model.save_weights(self.store_filename, overwrite=True)\n+                self.model.save_weights(self.filepath, overwrite=True)\n             else:\n                 if self.verbose > 0:\n                     print(\"Epoch %05d: validation loss did not improve\" % (epoch))\n@@ -214,6 +213,6 @@ class ModelCheckpoint(Callback):\n             warnings.warn(\"Can save best model only with validation data, skipping\", RuntimeWarning)\n         elif not self.save_best_only:\n             if self.verbose > 0:\n-                print(\"Epoch %05d: saving model to %s\" % (epoch, self.store_filename))\n-            self.model.save_weights(self.store_filename, overwrite=True)\n+                print(\"Epoch %05d: saving model to %s\" % (epoch, self.filepath))\n+            self.model.save_weights(self.filepath, overwrite=True)\n \n\n@@ -51,7 +51,7 @@ f = os.path.join(path, filename)\n \n print(\"Test model checkpointer\")\n # only store best validation model in checkpointer\n-checkpointer = cbks.ModelCheckpoint(filename=filename, path=path, verbose=1, save_best_only=True)\n+checkpointer = cbks.ModelCheckpoint(filepath=f, verbose=1, save_best_only=True)\n model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=(X_test, Y_test), callbacks =[checkpointer])\n \n if not os.path.isfile(f):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d6eb8a47d7293ed2d81d77e2e855acecd855487d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 8 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 20 | Churn Cumulative: 47 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,16 +6,28 @@ from six.moves import range\n \n epsilon = 1.0e-15\n \n-def mean_squared_error(y_true, y_pred):\n+def mean_squared_error(y_true, y_pred, weight=None):\n+    if weight is not None:\n+        return T.sqr(weight*(y_pred - y_true)).mean()\n+    else:\n         return T.sqr(y_pred - y_true).mean()\n \n-def mean_absolute_error(y_true, y_pred):\n+def mean_absolute_error(y_true, y_pred, weight=None):\n+    if weight is not None:\n+        return T.abs_(weight*(y_pred - y_true)).mean()\n+    else:\n         return T.abs_(y_pred - y_true).mean()\n \n-def squared_hinge(y_true, y_pred):\n+def squared_hinge(y_true, y_pred, weight=None):\n+    if weight is not None:\n+        return T.sqr(T.maximum(1. - weight*(y_true * y_pred), 0.)).mean()\n+    else:\n         return T.sqr(T.maximum(1. - y_true * y_pred, 0.)).mean()\n \n-def hinge(y_true, y_pred):\n+def hinge(y_true, y_pred, weight=None):\n+    if weight is not None:\n+        return T.maximum(1. - weight*(y_true * y_pred), 0.).mean()\n+    else:\n         return T.maximum(1. - y_true * y_pred, 0.).mean()\n \n def categorical_crossentropy(y_true, y_pred, weight=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bde147ce3a8356c8362b96df02586c7e907bc82e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 12 | Churn Cumulative: 59 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,27 +7,15 @@ from six.moves import range\n epsilon = 1.0e-15\n \n def mean_squared_error(y_true, y_pred, weight=None):\n-    if weight is not None:\n-        return T.sqr(weight*(y_pred - y_true)).mean()\n-    else:\n     return T.sqr(y_pred - y_true).mean()\n \n def mean_absolute_error(y_true, y_pred, weight=None):\n-    if weight is not None:\n-        return T.abs_(weight*(y_pred - y_true)).mean()\n-    else:\n     return T.abs_(y_pred - y_true).mean()\n \n def squared_hinge(y_true, y_pred, weight=None):\n-    if weight is not None:\n-        return T.sqr(T.maximum(1. - weight*(y_true * y_pred), 0.)).mean()\n-    else:\n     return T.sqr(T.maximum(1. - y_true * y_pred, 0.)).mean()\n \n def hinge(y_true, y_pred, weight=None):\n-    if weight is not None:\n-        return T.maximum(1. - weight*(y_true * y_pred), 0.).mean()\n-    else:\n     return T.maximum(1. - y_true * y_pred, 0.).mean()\n \n def categorical_crossentropy(y_true, y_pred, weight=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c0b7044ce68e288e1cd34cadd60cbf4960bc3645", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 65 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,9 +13,15 @@ def mean_absolute_error(y_true, y_pred, weight=None):\n     return T.abs_(y_pred - y_true).mean()\n \n def squared_hinge(y_true, y_pred, weight=None):\n+    if weight is not None:\n+        return T.sqr(T.maximum(1. - weight*(y_true * y_pred), 0.)).mean()\n+    else:\n         return T.sqr(T.maximum(1. - y_true * y_pred, 0.)).mean()\n \n def hinge(y_true, y_pred, weight=None):\n+    if weight is not None:\n+        return T.maximum(1. - weight*(y_true * y_pred), 0.).mean()\n+    else:\n         return T.maximum(1. - y_true * y_pred, 0.).mean()\n \n def categorical_crossentropy(y_true, y_pred, weight=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#39f05c34365bf5895e42a3a9a87d5d1e2ecd3c7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 44/42 | Churn Δ: 16 | Churn Cumulative: 1306 | Contributors (this commit): 16 | Commits (past 90d): 62 | Contributors (cumulative): 19 | DMM Complexity: 0.7142857142857143\n\nDIFF:\n@@ -178,6 +178,11 @@ class Sequential(Model):\n         return self.layers[0].get_input(train)\n \n     def train(self, X, y, accuracy=False, class_weight=None):\n+        if class_weight is not None and (self.loss is objectives.get('mean_absolute_error') or self.loss is objectives.get('mean_squared_error')):\n+            import warnings\n+            warnings.warn(\"Using class_weight with incompatible loss, ignoring class_weight parameter\", RuntimeWarning)\n+            class_weight = None\n+\n         X = standardize_X(X)\n         y = standardize_y(y)\n \n@@ -204,6 +209,11 @@ class Sequential(Model):\n     def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n             validation_split=0., validation_data=None, shuffle=True, show_accuracy=False, class_weight=None):\n \n+        if class_weight is not None and (self.loss is objectives.get('mean_absolute_error') or self.loss is objectives.get('mean_squared_error')):\n+            import warnings\n+            warnings.warn(\"Using class_weight with incompatible loss, ignoring class_weight parameter\", RuntimeWarning)\n+            class_weight = None\n+\n         X = standardize_X(X)\n         y = standardize_y(y)\n \n\n@@ -7,9 +7,15 @@ from six.moves import range\n epsilon = 1.0e-15\n \n def mean_squared_error(y_true, y_pred, weight=None):\n+    if weight is not None:\n+        return T.sqr(weight*(y_pred - y_true)).mean()\n+    else:\n         return T.sqr(y_pred - y_true).mean()\n \n def mean_absolute_error(y_true, y_pred, weight=None):\n+    if weight is not None:\n+        return T.abs_(weight*(y_pred - y_true)).mean()\n+    else:\n         return T.abs_(y_pred - y_true).mean()\n \n def squared_hinge(y_true, y_pred, weight=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8a946bb1268910e2611460c203382c4ba49ea312", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 77 | Files Changed: 2 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 157 | Churn Cumulative: 448 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1,72 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-from keras.datasets import mnist\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Activation, Merge\n-from keras.utils import np_utils\n-import keras.callbacks as cbks\n-import numpy as np\n-\n-nb_classes = 10\n-batch_size = 128\n-nb_epoch = 20\n-\n-# small sample size to overfit on training data\n-max_train_samples = 50\n-max_test_samples = 1000\n-\n-np.random.seed(1337) # for reproducibility\n-\n-# the data, shuffled and split between tran and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n-\n-X_train = X_train.reshape(60000,784)[:max_train_samples]\n-X_test = X_test.reshape(10000,784)[:max_test_samples]\n-X_train = X_train.astype(\"float32\")\n-X_test = X_test.astype(\"float32\")\n-X_train /= 255\n-X_test /= 255\n-\n-# convert class vectors to binary class matrices\n-Y_train = np_utils.to_categorical(y_train, nb_classes)[:max_train_samples]\n-Y_test = np_utils.to_categorical(y_test, nb_classes)[:max_test_samples]\n-\n-##########################\n-# model checkpoint tests #\n-##########################\n-\n-# Create a slightly larger network than required to test best validation save only \n-model = Sequential()\n-model.add(Dense(784, 500))\n-model.add(Activation('relu'))\n-model.add(Dense(500, 10))\n-model.add(Activation('softmax'))\n-model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-# test file location\n-path = \"/tmp\"\n-filename = \"model_weights.hdf5\"\n-import os\n-f = os.path.join(path, filename)\n-\n-print(\"Test model checkpointer\")\n-# only store best validation model in checkpointer\n-checkpointer = cbks.ModelCheckpoint(filepath=f, verbose=1, save_best_only=True)\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=(X_test, Y_test), callbacks =[checkpointer])\n-\n-if not os.path.isfile(f):\n-    raise Exception(\"Model weights were not saved to %s\" % (f))\n-\n-print(\"Test model checkpointer without validation data\")\n-import warnings\n-warnings.filterwarnings('error')\n-try:\n-    # this should issue a warning\n-    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, callbacks =[checkpointer])\n-except:\n-    print(\"Tests passed\")\n-    import sys\n-    sys.exit(0)\n-\n-raise Exception(\"Tests did not pass\")\n-\n\n@@ -9,15 +9,22 @@ from keras.regularizers import l2\n from keras.layers.convolutional import Convolution2D, MaxPooling2D\n from keras.utils import np_utils\n from keras.datasets import mnist\n+import keras.callbacks as cbks\n \n from matplotlib import pyplot as plt\n from matplotlib import animation\n \n+##############################\n+# model DrawActivations test #\n+##############################\n+\n+print('Running DrawActivations test')\n+\n nb_classes = 10\n batch_size = 128\n nb_epoch = 10\n \n-max_train_samples = 5000\n+max_train_samples = 512\n max_test_samples = 1\n \n np.random.seed(1337)\n@@ -91,7 +98,7 @@ class DrawActivations(Callback):\n     def __init__(self, figsize):\n         self.fig = plt.figure(figsize=figsize)\n \n-    def on_train_begin(self):\n+    def on_train_begin(self, logs={}):\n         self.imgs = Frames(n_plots=5)\n \n         layers_0_ids = np.random.choice(32, 16, replace=False)\n@@ -102,10 +109,10 @@ class DrawActivations(Callback):\n \n         self.test_layer2 = theano.function([self.model.get_input()], self.model.layers[10].get_output(train=False)[0])\n \n-    def on_epoch_begin(self, epoch):\n+    def on_epoch_begin(self, epoch, logs={}):\n         self.epoch = epoch\n \n-    def on_batch_end(self, batch):\n+    def on_batch_end(self, batch, logs={}):\n         if batch % 5 == 0:\n             self.imgs.add_frame(0, X_test[0,0])\n             self.imgs.add_frame(1, combine_imgs(self.test_layer0(X_test), grid=(4, 4)))\n@@ -114,7 +121,7 @@ class DrawActivations(Callback):\n             self.imgs.add_frame(4, self.model._predict(X_test)[0].reshape((1,10)))\n             self.imgs.set_title('Epoch #%d - Batch #%d' % (self.epoch, batch))\n \n-    def on_train_end(self):\n+    def on_train_end(self, logs={}):\n         anim = SubplotTimedAnimation(self.fig, self.imgs, grid=(1,5), interval=10, blit=False, repeat_delay=1000)\n         # anim.save('test_gif.gif', fps=15, writer='imagemagick')\n         plt.show()\n@@ -149,3 +156,71 @@ model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n # Fit the model\n draw_weights = DrawActivations(figsize=(5.4, 1.35))\n model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, callbacks=[draw_weights])\n+\n+\n+##########################\n+# model checkpoint tests #\n+##########################\n+\n+print('Running model checkpointer test')\n+\n+nb_classes = 10\n+batch_size = 128\n+nb_epoch = 20\n+\n+# small sample size to overfit on training data\n+max_train_samples = 50\n+max_test_samples = 1000\n+\n+np.random.seed(1337) # for reproducibility\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+\n+X_train = X_train.reshape(60000,784)[:max_train_samples]\n+X_test = X_test.reshape(10000,784)[:max_test_samples]\n+X_train = X_train.astype(\"float32\")\n+X_test = X_test.astype(\"float32\")\n+X_train /= 255\n+X_test /= 255\n+\n+# convert class vectors to binary class matrices\n+Y_train = np_utils.to_categorical(y_train, nb_classes)[:max_train_samples]\n+Y_test = np_utils.to_categorical(y_test, nb_classes)[:max_test_samples]\n+\n+\n+# Create a slightly larger network than required to test best validation save only\n+model = Sequential()\n+model.add(Dense(784, 500))\n+model.add(Activation('relu'))\n+model.add(Dense(500, 10))\n+model.add(Activation('softmax'))\n+model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+# test file location\n+path = \"/tmp\"\n+filename = \"model_weights.hdf5\"\n+import os\n+f = os.path.join(path, filename)\n+\n+print(\"Test model checkpointer\")\n+# only store best validation model in checkpointer\n+checkpointer = cbks.ModelCheckpoint(filepath=f, verbose=1, save_best_only=True)\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=(X_test, Y_test), callbacks =[checkpointer])\n+\n+if not os.path.isfile(f):\n+    raise Exception(\"Model weights were not saved to %s\" % (f))\n+\n+print(\"Test model checkpointer without validation data\")\n+import warnings\n+warnings.filterwarnings('error')\n+try:\n+    # this should issue a warning\n+    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, callbacks =[checkpointer])\n+except:\n+    print(\"Tests passed\")\n+    import sys\n+    sys.exit(0)\n+\n+raise Exception(\"Tests did not pass\")\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8e84006760dda565881148788e81032c5aa75085", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 306 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -162,7 +162,7 @@ model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1,\n # model checkpoint tests #\n ##########################\n \n-print('Running model checkpointer test')\n+print('Running ModelCheckpoint test')\n \n nb_classes = 10\n batch_size = 128\n@@ -222,5 +222,5 @@ except:\n     import sys\n     sys.exit(0)\n \n-raise Exception(\"Tests did not pass\")\n+raise Exception(\"Modelcheckpoint tests did not pass\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
