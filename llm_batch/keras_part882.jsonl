{"custom_id": "keras#93b393f617cac2bf98881e5dc33bdd88f72a9f00", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 527 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 5 | Methods Changed: 18 | Complexity Δ (Sum/Max): 38/26 | Churn Δ: 527 | Churn Cumulative: 1425 | Contributors (this commit): 8 | Commits (past 90d): 56 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -190,6 +190,9 @@ from keras.src.layers.preprocessing.image_preprocessing.random_hue import (\n from keras.src.layers.preprocessing.image_preprocessing.random_invert import (\n     RandomInvert,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.random_perspective import (\n+    RandomPerspective,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.random_posterization import (\n     RandomPosterization,\n )\n\n@@ -190,6 +190,9 @@ from keras.src.layers.preprocessing.image_preprocessing.random_hue import (\n from keras.src.layers.preprocessing.image_preprocessing.random_invert import (\n     RandomInvert,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.random_perspective import (\n+    RandomPerspective,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.random_posterization import (\n     RandomPosterization,\n )\n\n@@ -134,6 +134,9 @@ from keras.src.layers.preprocessing.image_preprocessing.random_hue import (\n from keras.src.layers.preprocessing.image_preprocessing.random_invert import (\n     RandomInvert,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.random_perspective import (\n+    RandomPerspective,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.random_posterization import (\n     RandomPosterization,\n )\n\n@@ -0,0 +1,300 @@\n+from keras.src.api_export import keras_export\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (  # noqa: E501\n+    clip_to_image_size,\n+)\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (  # noqa: E501\n+    convert_format,\n+)\n+from keras.src.random.seed_generator import SeedGenerator\n+from keras.src.utils import backend_utils\n+\n+\n+@keras_export(\"keras.layers.RandomPerspective\")\n+class RandomPerspective(BaseImagePreprocessingLayer):\n+    \"\"\"A preprocessing layer that applies random perspective transformations.\n+\n+    This layer distorts the perspective of input images by shifting their\n+    corner points, simulating a 3D-like transformation. The amount of distortion\n+    is controlled by the `factor` and `scale` parameters.\n+\n+    Args:\n+        factor: A float or a tuple of two floats.\n+            Represents the probability of applying the perspective\n+            transformation to each image in the batch.\n+            - `factor=0.0` ensures no transformation is applied.\n+            - `factor=1.0` means the transformation is always applied.\n+            - If a tuple `(min, max)` is provided, a probability is randomly\n+              sampled between `min` and `max` for each image.\n+            - If a single float is given, the probability is sampled between\n+              `0.0` and the provided float.\n+            Default is 1.0.\n+        scale: A float defining the relative amount of perspective shift.\n+            Determines how much the image corners are displaced, affecting\n+            the intensity of the perspective effect.\n+        interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n+            `\"bilinear\"`.\n+        fill_value: a float represents the value to be filled outside the\n+            boundaries when `fill_mode=\"constant\"`.\n+        seed: Integer. Used to create a random seed.\n+\n+    \"\"\"\n+\n+    _USE_BASE_FACTOR = False\n+    _FACTOR_BOUNDS = (0, 1)\n+    _SUPPORTED_INTERPOLATION = (\"nearest\", \"bilinear\")\n+\n+    def __init__(\n+        self,\n+        factor=1.0,\n+        scale=0.3,\n+        interpolation=\"bilinear\",\n+        fill_value=0.0,\n+        seed=None,\n+        data_format=None,\n+        **kwargs,\n+    ):\n+        super().__init__(data_format=data_format, **kwargs)\n+        self._set_factor(factor)\n+        self.scale = scale\n+        self.fill_value = fill_value\n+        self.interpolation = interpolation\n+        self.seed = seed\n+        self.generator = SeedGenerator(seed)\n+        self.supports_jit = False\n+\n+        if scale < 0.0 or scale > 1.0:\n+            raise ValueError(\n+                \"The `scale` argument should be a number \"\n+                \"in the range \"\n+                f\"[0,1]. \"\n+                f\"Received: scale={scale}\"\n+            )\n+\n+        if interpolation not in self._SUPPORTED_INTERPOLATION:\n+            raise NotImplementedError(\n+                f\"Unknown `interpolation` {interpolation}. Expected of one \"\n+                f\"{self._SUPPORTED_INTERPOLATION}.\"\n+            )\n+\n+        if self.data_format == \"channels_first\":\n+            self.height_axis = -2\n+            self.width_axis = -1\n+            self.channel_axis = -3\n+        else:\n+            self.height_axis = -3\n+            self.width_axis = -2\n+            self.channel_axis = -1\n+\n+    def get_random_transformation(self, data, training=True, seed=None):\n+        if not training:\n+            return None\n+\n+        if isinstance(data, dict):\n+            images = data[\"images\"]\n+        else:\n+            images = data\n+\n+        images_shape = self.backend.shape(images)\n+        unbatched = len(images_shape) == 3\n+        if unbatched:\n+            batch_size = 1\n+        else:\n+            batch_size = images_shape[0]\n+\n+        seed = seed or self._get_seed_generator(self.backend._backend)\n+\n+        transformation_probability = self.backend.random.uniform(\n+            shape=(batch_size,),\n+            minval=self.factor[0],\n+            maxval=self.factor[1],\n+            seed=seed,\n+        )\n+\n+        random_threshold = self.backend.random.uniform(\n+            shape=(batch_size,),\n+            minval=0.0,\n+            maxval=1.0,\n+            seed=seed,\n+        )\n+        apply_perspective = random_threshold < transformation_probability\n+\n+        perspective_factor = self.backend.random.uniform(\n+            minval=-self.scale,\n+            maxval=self.scale,\n+            shape=[batch_size, 4],\n+            seed=seed,\n+            dtype=self.compute_dtype,\n+        )\n+\n+        return {\n+            \"apply_perspective\": apply_perspective,\n+            \"perspective_factor\": perspective_factor,\n+            \"input_shape\": images_shape,\n+        }\n+\n+    def transform_images(self, images, transformation, training=True):\n+        images = self.backend.cast(images, self.compute_dtype)\n+        if training and transformation is not None:\n+            apply_perspective = transformation[\"apply_perspective\"]\n+            perspective_images = self._perspective_inputs(\n+                images, transformation\n+            )\n+\n+            images = self.backend.numpy.where(\n+                apply_perspective[:, None, None, None],\n+                perspective_images,\n+                images,\n+            )\n+        return images\n+\n+    def _perspective_inputs(self, inputs, transformation):\n+        if transformation is None:\n+            return inputs\n+\n+        inputs_shape = self.backend.shape(inputs)\n+        unbatched = len(inputs_shape) == 3\n+        if unbatched:\n+            inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n+\n+        perspective_factor = transformation[\"perspective_factor\"]\n+        outputs = self.backend.image.affine_transform(\n+            inputs,\n+            transform=self._get_perspective_matrix(perspective_factor),\n+            interpolation=self.interpolation,\n+            fill_mode=\"constant\",\n+            fill_value=self.fill_value,\n+            data_format=self.data_format,\n+        )\n+\n+        if unbatched:\n+            outputs = self.backend.numpy.squeeze(outputs, axis=0)\n+        return outputs\n+\n+    def _get_perspective_matrix(self, perspectives):\n+        num_perspectives = self.backend.shape(perspectives)[0]\n+        return self.backend.numpy.concatenate(\n+            [\n+                self.backend.numpy.ones(\n+                    (num_perspectives, 1), dtype=self.compute_dtype\n+                )\n+                + perspectives[:, :1],\n+                perspectives[:, :1],\n+                perspectives[:, 2:3],\n+                perspectives[:, 1:2],\n+                self.backend.numpy.ones(\n+                    (num_perspectives, 1), dtype=self.compute_dtype\n+                )\n+                + perspectives[:, 1:2],\n+                perspectives[:, 3:4],\n+                self.backend.numpy.zeros((num_perspectives, 2)),\n+            ],\n+            axis=1,\n+        )\n+\n+    def _get_transformed_coordinates(self, x, y, transform):\n+        a0, a1, a2, b0, b1, b2, c0, c1 = self.backend.numpy.split(\n+            transform, 8, axis=-1\n+        )\n+\n+        x_transformed = (a1 * (y - b2) - b1 * (x - a2)) / (a1 * b0 - a0 * b1)\n+        y_transformed = (b0 * (x - a2) - a0 * (y - b2)) / (a1 * b0 - a0 * b1)\n+\n+        return x_transformed, y_transformed\n+\n+    def transform_bounding_boxes(\n+        self,\n+        bounding_boxes,\n+        transformation,\n+        training=True,\n+    ):\n+        if training:\n+            if backend_utils.in_tf_graph():\n+                self.backend.set_backend(\"tensorflow\")\n+\n+            input_height, input_width = (\n+                transformation[\"input_shape\"][self.height_axis],\n+                transformation[\"input_shape\"][self.width_axis],\n+            )\n+\n+            bounding_boxes = convert_format(\n+                bounding_boxes,\n+                source=self.bounding_box_format,\n+                target=\"xyxy\",\n+                height=input_height,\n+                width=input_width,\n+            )\n+\n+            boxes = bounding_boxes[\"boxes\"]\n+\n+            x0, y0, x1, y1 = self.backend.numpy.split(boxes, 4, axis=-1)\n+\n+            perspective_factor = transformation[\"perspective_factor\"]\n+            transform = self._get_perspective_matrix(perspective_factor)\n+            transform = self.backend.numpy.expand_dims(transform, axis=1)\n+            transform = self.backend.cast(transform, dtype=self.compute_dtype)\n+\n+            x_1, y_1 = self._get_transformed_coordinates(x0, y0, transform)\n+            x_2, y_2 = self._get_transformed_coordinates(x1, y1, transform)\n+            x_3, y_3 = self._get_transformed_coordinates(x0, y1, transform)\n+            x_4, y_4 = self._get_transformed_coordinates(x1, y0, transform)\n+\n+            xs = self.backend.numpy.concatenate([x_1, x_2, x_3, x_4], axis=-1)\n+            ys = self.backend.numpy.concatenate([y_1, y_2, y_3, y_4], axis=-1)\n+\n+            min_x = self.backend.numpy.min(xs, axis=-1)\n+            max_x = self.backend.numpy.max(xs, axis=-1)\n+            min_y = self.backend.numpy.min(ys, axis=-1)\n+            max_y = self.backend.numpy.max(ys, axis=-1)\n+\n+            min_x = self.backend.numpy.expand_dims(min_x, axis=-1)\n+            max_x = self.backend.numpy.expand_dims(max_x, axis=-1)\n+            min_y = self.backend.numpy.expand_dims(min_y, axis=-1)\n+            max_y = self.backend.numpy.expand_dims(max_y, axis=-1)\n+\n+            boxes = self.backend.numpy.concatenate(\n+                [min_x, min_y, max_x, max_y], axis=-1\n+            )\n+\n+            apply_perspective = transformation[\"apply_perspective\"]\n+\n+            bounding_boxes[\"boxes\"] = self.backend.numpy.where(\n+                apply_perspective[:, None, None],\n+                boxes,\n+                bounding_boxes[\"boxes\"],\n+            )\n+\n+            bounding_boxes = clip_to_image_size(\n+                bounding_boxes=bounding_boxes,\n+                height=input_height,\n+                width=input_width,\n+                bounding_box_format=\"xyxy\",\n+            )\n+\n+        return bounding_boxes\n+\n+    def transform_labels(self, labels, transformation, training=True):\n+        return labels\n+\n+    def transform_segmentation_masks(\n+        self, segmentation_masks, transformation, training=True\n+    ):\n+        return self.transform_images(\n+            segmentation_masks, transformation, training=training\n+        )\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        config = {\n+            \"factor\": self.factor,\n+            \"scale\": self.scale,\n+            \"interpolation\": self.interpolation,\n+            \"fill_value\": self.fill_value,\n+            \"seed\": self.seed,\n+        }\n+        return {**base_config, **config}\n\n@@ -0,0 +1,218 @@\n+import numpy as np\n+import pytest\n+from absl.testing import parameterized\n+from tensorflow import data as tf_data\n+\n+from keras.src import backend\n+from keras.src import layers\n+from keras.src import testing\n+\n+\n+class RandomPerspectiveTest(testing.TestCase):\n+    @pytest.mark.requires_trainable_backend\n+    def test_layer(self):\n+        self.run_layer_test(\n+            layers.RandomPerspective,\n+            init_kwargs={\n+                \"factor\": 1.0,\n+                \"scale\": 0.5,\n+                \"interpolation\": \"bilinear\",\n+                \"fill_value\": 0,\n+                \"seed\": 1,\n+            },\n+            input_shape=(8, 3, 4, 3),\n+            supports_masking=False,\n+            expected_output_shape=(8, 3, 4, 3),\n+        )\n+\n+    def test_random_perspective_inference(self):\n+        seed = 3481\n+        layer = layers.RandomPerspective()\n+\n+        np.random.seed(seed)\n+        inputs = np.random.randint(0, 255, size=(224, 224, 3))\n+        output = layer(inputs, training=False)\n+        self.assertAllClose(inputs, output)\n+\n+    def test_random_perspective_no_op(self):\n+        seed = 3481\n+        layer = layers.RandomPerspective(factor=0)\n+\n+        np.random.seed(seed)\n+        inputs = np.random.randint(0, 255, size=(224, 224, 3))\n+        output = layer(inputs)\n+        self.assertAllClose(inputs, output)\n+\n+        layer = layers.RandomPerspective(scale=0)\n+\n+        np.random.seed(seed)\n+        inputs = np.random.randint(0, 255, size=(224, 224, 3))\n+        output = layer(inputs)\n+        self.assertAllClose(inputs, output)\n+\n+    def test_random_perspective_basic(self):\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            inputs = np.ones((2, 2, 1))\n+            expected_output = np.array(\n+                [[[[1.0], [0.89476013]], [[0.84097195], [0.6412543]]]]\n+            )\n+\n+        else:\n+            inputs = np.ones((1, 2, 2))\n+            expected_output = np.array([[[[1.0000, 0.8948], [0.8410, 0.6413]]]])\n+\n+        layer = layers.RandomPerspective(data_format=data_format)\n+\n+        transformation = {\n+            \"apply_perspective\": np.asarray([True]),\n+            \"perspective_factor\": np.asarray(\n+                [[0.05261999, 0.07951406, 0.05261999, 0.07951406]]\n+            ),\n+        }\n+\n+        output = layer.transform_images(inputs, transformation)\n+\n+        print(output)\n+\n+        self.assertAllClose(expected_output, output, atol=1e-4, rtol=1e-4)\n+\n+    def test_tf_data_compatibility(self):\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            input_data = np.random.random((2, 8, 8, 3))\n+        else:\n+            input_data = np.random.random((2, 3, 8, 8))\n+        layer = layers.RandomPerspective(data_format=data_format)\n+\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        for output in ds.take(1):\n+            output.numpy()\n+\n+    @parameterized.named_parameters(\n+        (\n+            \"with_negative_shift\",\n+            [[-0.5, -0.1, -0.3, -0.2]],\n+            [\n+                [\n+                    [6.6750007, 2.0750003, 8.0, 5.0750003],\n+                    [8.0, 6.8250003, 8.0, 9.825],\n+                ]\n+            ],\n+        ),\n+        (\n+            \"with_positive_shift\",\n+            [[0.5, 0.1, 0.3, 0.2]],\n+            [\n+                [\n+                    [0.29375008, 0.51874995, 2.2937498, 2.5187497],\n+                    [2.1062498, 3.0812497, 4.10625, 5.08125],\n+                ]\n+            ],\n+        ),\n+    )\n+    def test_random_perspective_bounding_boxes(self, factor, expected_boxes):\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            image_shape = (10, 8, 3)\n+        else:\n+            image_shape = (3, 10, 8)\n+        input_image = np.random.random(image_shape)\n+        bounding_boxes = {\n+            \"boxes\": np.array(\n+                [\n+                    [2, 1, 4, 3],\n+                    [6, 4, 8, 6],\n+                ]\n+            ),\n+            \"labels\": np.array([[1, 2]]),\n+        }\n+        input_data = {\"images\": input_image, \"bounding_boxes\": bounding_boxes}\n+        layer = layers.RandomPerspective(\n+            data_format=data_format,\n+            seed=42,\n+            bounding_box_format=\"xyxy\",\n+        )\n+\n+        transformation = {\n+            \"apply_perspective\": np.asarray([True]),\n+            \"perspective_factor\": np.asarray(factor),\n+            \"input_shape\": image_shape,\n+        }\n+        output = layer.transform_bounding_boxes(\n+            input_data[\"bounding_boxes\"],\n+            transformation=transformation,\n+            training=True,\n+        )\n+\n+        print(output)\n+\n+        self.assertAllClose(output[\"boxes\"], expected_boxes)\n+\n+    @parameterized.named_parameters(\n+        (\n+            \"with_negative_shift\",\n+            [[-0.5, -0.1, -0.3, -0.2]],\n+            [\n+                [\n+                    [6.6750007, 2.0750003, 8.0, 5.0750003],\n+                    [8.0, 6.8250003, 8.0, 9.825],\n+                ]\n+            ],\n+        ),\n+        (\n+            \"with_positive_shift\",\n+            [[0.5, 0.1, 0.3, 0.2]],\n+            [\n+                [\n+                    [0.29375008, 0.51874995, 2.2937498, 2.5187497],\n+                    [2.1062498, 3.0812497, 4.10625, 5.08125],\n+                ]\n+            ],\n+        ),\n+    )\n+    def test_random_flip_tf_data_bounding_boxes(self, factor, expected_boxes):\n+        data_format = backend.config.image_data_format()\n+        if backend.config.image_data_format() == \"channels_last\":\n+            image_shape = (1, 10, 8, 3)\n+        else:\n+            image_shape = (1, 3, 10, 8)\n+        input_image = np.random.random(image_shape)\n+        bounding_boxes = {\n+            \"boxes\": np.array(\n+                [\n+                    [\n+                        [2, 1, 4, 3],\n+                        [6, 4, 8, 6],\n+                    ]\n+                ]\n+            ),\n+            \"labels\": np.array([[1, 2]]),\n+        }\n+\n+        input_data = {\"images\": input_image, \"bounding_boxes\": bounding_boxes}\n+\n+        ds = tf_data.Dataset.from_tensor_slices(input_data)\n+        layer = layers.RandomPerspective(\n+            data_format=data_format,\n+            seed=42,\n+            bounding_box_format=\"xyxy\",\n+        )\n+\n+        transformation = {\n+            \"apply_perspective\": np.asarray([True]),\n+            \"perspective_factor\": np.asarray(factor),\n+            \"input_shape\": image_shape,\n+        }\n+\n+        ds = ds.map(\n+            lambda x: layer.transform_bounding_boxes(\n+                x[\"bounding_boxes\"],\n+                transformation=transformation,\n+                training=True,\n+            )\n+        )\n+\n+        output = next(iter(ds))\n+        expected_boxes = np.array(expected_boxes)\n+        self.assertAllClose(output[\"boxes\"], expected_boxes)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c04cf9db5e765fb20ef400638c39205460cbf245", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 11 | Churn Cumulative: 422 | Contributors (this commit): 12 | Commits (past 90d): 6 | Contributors (cumulative): 12 | DMM Complexity: 0.5555555555555556\n\nDIFF:\n@@ -497,6 +497,7 @@ class JAXTrainer(base_trainer.Trainer):\n             del self._eval_epoch_iterator\n         callbacks.on_train_end(logs=training_logs)\n         self._jax_state = None\n+        self._clear_jax_state_sharding()\n         return self.history\n \n     @traceback_utils.filter_traceback\n@@ -601,6 +602,9 @@ class JAXTrainer(base_trainer.Trainer):\n             logs = self._get_metrics_result_or_logs(logs)\n         callbacks.on_test_end(logs)\n         self._jax_state = None\n+        if not use_cached_eval_dataset:\n+            # Only clear sharding if evaluate is not called from `fit`.\n+            self._clear_jax_state_sharding()\n         if return_dict:\n             return logs\n         return self._flatten_metrics_in_order(logs)\n@@ -696,6 +700,7 @@ class JAXTrainer(base_trainer.Trainer):\n         self.jax_state_sync()\n         callbacks.on_predict_end()\n         self._jax_state = None\n+        self._clear_jax_state_sharding()\n         return tree.map_structure_up_to(batch_outputs, np.concatenate, outputs)\n \n     def train_on_batch(\n@@ -873,6 +878,12 @@ class JAXTrainer(base_trainer.Trainer):\n             v.value.sharding for v in self.metrics_variables\n         ]\n \n+    def _clear_jax_state_sharding(self):\n+        self._trainable_variable_shardings = None\n+        self._non_trainable_variable_shardings = None\n+        self._optimizer_variable_shardings = None\n+        self._metrics_variable_shardings = None\n+\n     def _enforce_jax_state_sharding(\n         self,\n         trainable_variables=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#43f40d81b2d444bb89d9513c8d78032209b4fa5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 197 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -20,16 +20,16 @@ class RematScope:\n     Args:\n         mode: Rematerialization mode to apply.\n             Options:\n-            - \"full\": Apply rematerialization globally to all supported\n+            - `\"full\"`: Apply rematerialization globally to all supported\n               operations.\n-            - \"activations\": Apply rematerialization to activations on any\n+            - `\"activations\"`: Apply rematerialization to activations on any\n               layers that contain `keras.activations` (e.g., `Dense(...,\n               activation=relu)`).\n-            - \"larger_than\": Apply rematerialization to layers with output sizes\n-              larger than `output_size_threshold`.\n-            - \"list_of_layers\": Apply rematerialization to a specific list of\n+            - `\"larger_than\"`: Apply rematerialization to layers with output\n+              sizes larger than `output_size_threshold`.\n+            - `\"list_of_layers\"`: Apply rematerialization to a specific list of\n               layer names.\n-            - None: Disable rematerialization.\n+            - `None`: Disable rematerialization.\n         output_size_threshold: Output size threshold for the\n             `\"larger_than\"` mode. Layers producing outputs larger than this\n             threshold will be rematerialized. Default is `1024`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3906e324b917bfaca4d248558383cd9ad4617400", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 27 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -12,6 +12,15 @@ class Conv2D(BaseConv):\n     and added to the outputs. Finally, if `activation` is not `None`, it is\n     applied to the outputs as well.\n \n+    Note on numerical precision: While in general Keras operation execution\n+    results are identical across backends up to 1e-7 precision in float32,\n+    `Conv2D` operations may show larger variations. Due to the large\n+    number of element-wise multiplications and additions in convolution\n+    operations, especially with large inputs or kernel sizes, accumulated\n+    floating-point differences can exceed this 1e-7 threshold. These variations\n+    are particularly noticeable when using different backends (e.g., TensorFlow\n+    vs JAX) or different hardware.\n+\n     Args:\n         filters: int, the dimension of the output space (the number of filters\n             in the convolution).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b2d5b88247ffab627a416d02e92f34be98d981db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 84 | Files Changed: 3 | Hunks: 22 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 167 | Churn Cumulative: 669 | Contributors (this commit): 9 | Commits (past 90d): 7 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,7 @@ from keras.src import backend\n BACKEND_REQ = {\n     \"tensorflow\": (\"tensorflow-cpu\", \"\"),\n     \"torch\": (\n-        \"torch torchvision\",\n+        \"torch\",\n         \"--extra-index-url https://download.pytorch.org/whl/cpu \",\n     ),\n     \"jax\": (\"jax[cpu]\", \"\"),\n\n@@ -3,12 +3,16 @@ import itertools\n import operator\n \n import torch\n+import torch.nn.functional as F\n \n from keras.src import backend\n from keras.src.backend.torch.core import convert_to_tensor\n-from keras.src.utils.module_utils import torchvision\n \n-RESIZE_INTERPOLATIONS = {}  # populated after torchvision import\n+RESIZE_INTERPOLATIONS = {\n+    \"bilinear\": \"bilinear\",\n+    \"nearest\": \"nearest-exact\",\n+    \"bicubic\": \"bicubic\",\n+}\n \n UNSUPPORTED_INTERPOLATIONS = (\n     \"lanczos3\",\n@@ -19,23 +23,27 @@ UNSUPPORTED_INTERPOLATIONS = (\n def rgb_to_grayscale(images, data_format=None):\n     images = convert_to_tensor(images)\n     data_format = backend.standardize_data_format(data_format)\n-    if data_format == \"channels_last\":\n-        if images.ndim == 4:\n-            images = images.permute((0, 3, 1, 2))\n-        elif images.ndim == 3:\n-            images = images.permute((2, 0, 1))\n-        else:\n+    if images.ndim not in (3, 4):\n         raise ValueError(\n             \"Invalid images rank: expected rank 3 (single image) \"\n             \"or rank 4 (batch of images). Received input with shape: \"\n             f\"images.shape={images.shape}\"\n         )\n-    images = torchvision.transforms.functional.rgb_to_grayscale(img=images)\n-    if data_format == \"channels_last\":\n-        if len(images.shape) == 4:\n-            images = images.permute((0, 2, 3, 1))\n-        elif len(images.shape) == 3:\n-            images = images.permute((1, 2, 0))\n+    channel_axis = -3 if data_format == \"channels_first\" else -1\n+    if images.shape[channel_axis] not in (1, 3):\n+        raise ValueError(\n+            \"Invalid channel size: expected 3 (RGB) or 1 (Grayscale). \"\n+            f\"Received input with shape: images.shape={images.shape}\"\n+        )\n+\n+    # This implementation is based on\n+    # https://github.com/pytorch/vision/blob/main/torchvision/transforms/_functional_tensor.py\n+    if images.shape[channel_axis] == 3:\n+        r, g, b = images.unbind(dim=channel_axis)\n+        images = (0.2989 * r + 0.587 * g + 0.114 * b).to(images.dtype)\n+        images = images.unsqueeze(dim=channel_axis)\n+    else:\n+        images = images.clone()\n     return images\n \n \n@@ -129,6 +137,40 @@ def hsv_to_rgb(images, data_format=None):\n     return images\n \n \n+def _cast_squeeze_in(image, req_dtypes):\n+    need_squeeze = False\n+    # make image NCHW\n+    if image.ndim < 4:\n+        image = image.unsqueeze(dim=0)\n+        need_squeeze = True\n+\n+    out_dtype = image.dtype\n+    need_cast = False\n+    if out_dtype not in req_dtypes:\n+        need_cast = True\n+        req_dtype = req_dtypes[0]\n+        image = image.to(req_dtype)\n+    return image, need_cast, need_squeeze, out_dtype\n+\n+\n+def _cast_squeeze_out(image, need_cast, need_squeeze, out_dtype):\n+    if need_squeeze:\n+        image = image.squeeze(dim=0)\n+\n+    if need_cast:\n+        if out_dtype in (\n+            torch.uint8,\n+            torch.int8,\n+            torch.int16,\n+            torch.int32,\n+            torch.int64,\n+        ):\n+            # it is better to round before cast\n+            image = torch.round(image)\n+        image = image.to(out_dtype)\n+    return image\n+\n+\n def resize(\n     images,\n     size,\n@@ -141,13 +183,6 @@ def resize(\n     data_format=None,\n ):\n     data_format = backend.standardize_data_format(data_format)\n-    RESIZE_INTERPOLATIONS.update(\n-        {\n-            \"bilinear\": torchvision.transforms.InterpolationMode.BILINEAR,\n-            \"nearest\": torchvision.transforms.InterpolationMode.NEAREST_EXACT,\n-            \"bicubic\": torchvision.transforms.InterpolationMode.BICUBIC,\n-        }\n-    )\n     if interpolation in UNSUPPORTED_INTERPOLATIONS:\n         raise ValueError(\n             \"Resizing with Lanczos interpolation is \"\n@@ -182,11 +217,11 @@ def resize(\n             \"or rank 4 (batch of images). Received input with shape: \"\n             f\"images.shape={images.shape}\"\n         )\n+    images, need_cast, need_squeeze, out_dtype = _cast_squeeze_in(\n+        images, [torch.float32, torch.float64]\n+    )\n     if data_format == \"channels_last\":\n-        if images.ndim == 4:\n         images = images.permute((0, 3, 1, 2))\n-        else:\n-            images = images.permute((2, 0, 1))\n \n     if crop_to_aspect_ratio:\n         shape = images.shape\n@@ -198,19 +233,12 @@ def resize(\n         crop_width = max(min(width, crop_width), 1)\n         crop_box_hstart = int(float(height - crop_height) / 2)\n         crop_box_wstart = int(float(width - crop_width) / 2)\n-        if len(images.shape) == 4:\n         images = images[\n             :,\n             :,\n             crop_box_hstart : crop_box_hstart + crop_height,\n             crop_box_wstart : crop_box_wstart + crop_width,\n         ]\n-        else:\n-            images = images[\n-                :,\n-                crop_box_hstart : crop_box_hstart + crop_height,\n-                crop_box_wstart : crop_box_wstart + crop_width,\n-            ]\n     elif pad_to_aspect_ratio:\n         shape = images.shape\n         height, width = shape[-2], shape[-1]\n@@ -221,7 +249,7 @@ def resize(\n         pad_width = max(width, pad_width)\n         img_box_hstart = int(float(pad_height - height) / 2)\n         img_box_wstart = int(float(pad_width - width) / 2)\n-        if len(images.shape) == 4:\n+\n         batch_size = images.shape[0]\n         channels = images.shape[1]\n         if img_box_hstart > 0:\n@@ -245,7 +273,6 @@ def resize(\n             )\n         else:\n             padded_img = images\n-\n         if img_box_wstart > 0:\n             padded_img = torch.cat(\n                 [\n@@ -264,62 +291,35 @@ def resize(\n                 ],\n                 axis=3,\n             )\n-\n-        else:\n-            channels = images.shape[0]\n-            if img_box_wstart > 0:\n-                padded_img = torch.cat(\n-                    [\n-                        torch.ones(\n-                            (channels, img_box_hstart, width),\n-                            dtype=images.dtype,\n-                            device=images.device,\n-                        )\n-                        * fill_value,\n-                        images,\n-                        torch.ones(\n-                            (channels, img_box_hstart, width),\n-                            dtype=images.dtype,\n-                            device=images.device,\n-                        )\n-                        * fill_value,\n-                    ],\n-                    axis=1,\n-                )\n-            else:\n-                padded_img = images\n-            if img_box_wstart > 0:\n-                torch.cat(\n-                    [\n-                        torch.ones(\n-                            (channels, height, img_box_wstart),\n-                            dtype=images.dtype,\n-                            device=images.device,\n-                        )\n-                        * fill_value,\n-                        padded_img,\n-                        torch.ones(\n-                            (channels, height, img_box_wstart),\n-                            dtype=images.dtype,\n-                            device=images.device,\n-                        )\n-                        * fill_value,\n-                    ],\n-                    axis=2,\n-                )\n         images = padded_img\n \n-    resized = torchvision.transforms.functional.resize(\n-        img=images,\n+    # This implementation is based on\n+    # https://github.com/pytorch/vision/blob/main/torchvision/transforms/_functional_tensor.py\n+    if antialias and interpolation not in (\"bilinear\", \"bicubic\"):\n+        # We manually set it to False to avoid an error downstream in\n+        # interpolate(). This behaviour is documented: the parameter is\n+        # irrelevant for modes that are not bilinear or bicubic. We used to\n+        # raise an error here, but now we don't use True as the default.\n+        antialias = False\n+    # Define align_corners to avoid warnings\n+    align_corners = False if interpolation in (\"bilinear\", \"bicubic\") else None\n+    resized = F.interpolate(\n+        images,\n         size=size,\n-        interpolation=RESIZE_INTERPOLATIONS[interpolation],\n+        mode=RESIZE_INTERPOLATIONS[interpolation],\n+        align_corners=align_corners,\n         antialias=antialias,\n     )\n+    if interpolation == \"bicubic\" and out_dtype == torch.uint8:\n+        resized = resized.clamp(min=0, max=255)\n     if data_format == \"channels_last\":\n-        if len(images.shape) == 4:\n         resized = resized.permute((0, 2, 3, 1))\n-        elif len(images.shape) == 3:\n-            resized = resized.permute((1, 2, 0))\n+    resized = _cast_squeeze_out(\n+        resized,\n+        need_cast=need_cast,\n+        need_squeeze=need_squeeze,\n+        out_dtype=out_dtype,\n+    )\n     return resized\n \n \n\n@@ -44,7 +44,6 @@ gfile = LazyModule(\"tensorflow.io.gfile\", pip_name=\"tensorflow\")\n tensorflow_io = LazyModule(\"tensorflow_io\")\n scipy = LazyModule(\"scipy\")\n jax = LazyModule(\"jax\")\n-torchvision = LazyModule(\"torchvision\")\n torch_xla = LazyModule(\n     \"torch_xla\",\n     import_error_msg=(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cc467763c84589fec2e08f21475815f0b4c25bb6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 47 | Churn Cumulative: 71 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 0.5454545454545454\n\nDIFF:\n@@ -77,10 +77,29 @@ class TimeDistributed(Wrapper):\n         batch_size = input_shape[0]\n         timesteps = input_shape[1]\n \n-        if mask_shape is not None and mask_shape[:2] != (batch_size, timesteps):\n+        # For TF backend with graph mode and `partial_batch_size`, skip\n+        # evaluation of `batch_size` as it can be a `strided_slice` and\n+        # not a constant.\n+        if backend.backend() == \"tensorflow\":\n+            from keras.src.utils.module_utils import tensorflow as tf\n+\n+            if (\n+                not tf.executing_eagerly\n+                and mask_shape is not None\n+                and mask_shape[1:2] != (timesteps,)\n+            ):\n                 raise ValueError(\n-                \"`TimeDistributed` Layer should be passed a `mask` of shape \"\n-                f\"({batch_size}, {timesteps}, ...), \"\n+                    \"`TimeDistributed` Layer should be passed a `mask` of \"\n+                    f\"shape ({batch_size}, {timesteps}, ...), \"\n+                    f\"received: mask.shape={mask_shape}\"\n+                )\n+        elif mask_shape is not None and mask_shape[:2] != (\n+            batch_size,\n+            timesteps,\n+        ):\n+            raise ValueError(\n+                \"`TimeDistributed` Layer should be passed a `mask` of \"\n+                f\"shape ({batch_size}, {timesteps}, ...), \"\n                 f\"received: mask.shape={mask_shape}\"\n             )\n \n\n@@ -6,6 +6,7 @@ from keras.src import initializers\n from keras.src import layers\n from keras.src import ops\n from keras.src import testing\n+from keras.src.models import Sequential\n \n \n class TimeDistributedTest(testing.TestCase):\n@@ -77,3 +78,24 @@ class TimeDistributedTest(testing.TestCase):\n             np.array([[[0], [0.22]], [[0.38], [0]], [[0.7], [0.86]]]),\n             output,\n         )\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_with_mask_zero(self):\n+        model = Sequential(\n+            [\n+                layers.Input(shape=(20,)),\n+                layers.Embedding(input_dim=10, output_dim=5, mask_zero=True),\n+                layers.TimeDistributed(\n+                    layers.Dense(units=5, activation=\"softmax\")\n+                ),\n+            ]\n+        )\n+        model.compile(\n+            optimizer=\"adam\",\n+            loss=\"sparse_categorical_crossentropy\",\n+            metrics=[\"accuracy\"],\n+        )\n+        X_train = np.random.uniform(1, 10, size=(22, 20))\n+        Y_train = np.random.randint(1, 2, size=(22, 20))\n+\n+        model.fit(X_train, Y_train, epochs=1, batch_size=16)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#084ac364ac45daa90dba99c1bb886eecf03065df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 311 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -159,7 +159,9 @@ class RandomPerspective(BaseImagePreprocessingLayer):\n         if unbatched:\n             inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n \n-        perspective_factor = transformation[\"perspective_factor\"]\n+        perspective_factor = self.backend.core.convert_to_tensor(\n+            transformation[\"perspective_factor\"], dtype=self.compute_dtype\n+        )\n         outputs = self.backend.image.affine_transform(\n             inputs,\n             transform=self._get_perspective_matrix(perspective_factor),\n@@ -174,6 +176,9 @@ class RandomPerspective(BaseImagePreprocessingLayer):\n         return outputs\n \n     def _get_perspective_matrix(self, perspectives):\n+        perspectives = self.backend.core.convert_to_tensor(\n+            perspectives, dtype=self.compute_dtype\n+        )\n         num_perspectives = self.backend.shape(perspectives)[0]\n         return self.backend.numpy.concatenate(\n             [\n@@ -258,7 +263,9 @@ class RandomPerspective(BaseImagePreprocessingLayer):\n                 [min_x, min_y, max_x, max_y], axis=-1\n             )\n \n-            apply_perspective = transformation[\"apply_perspective\"]\n+            apply_perspective = self.backend.core.convert_to_tensor(\n+                transformation[\"apply_perspective\"], dtype=boxes.dtype\n+            )\n \n             bounding_boxes[\"boxes\"] = self.backend.numpy.where(\n                 apply_perspective[:, None, None],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b9a49ea6b4971493ee57c71c646b5228fbfc48f8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 67 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -169,7 +169,7 @@ def qr(x, mode=\"reduced\"):\n \n def solve(a, b):\n     # tensorflow.linalg.solve only supports same rank inputs\n-    if tf.rank(b) == tf.rank(a) - 1:\n+    if b.shape.ndims == a.shape.ndims - 1:\n         b = tf.expand_dims(b, axis=-1)\n         return tf.squeeze(tf.linalg.solve(a, b), axis=-1)\n     return tf.linalg.solve(a, b)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a41827de8f81783af08138c3497774c3bf3685a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 34 | Churn Cumulative: 1128 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -152,11 +152,41 @@ def any(x, axis=None, keepdims=False):\n \n \n def amax(x, axis=None, keepdims=False):\n-    raise NotImplementedError(\"`amax` is not supported with openvino backend\")\n+    if axis == () or axis == []:\n+        return x\n+    x = get_ov_output(x)\n+    x_type = x.get_element_type()\n+    if axis is None:\n+        flatten_shape = ov_opset.constant([-1], Type.i32).output(0)\n+        x = ov_opset.reshape(x, flatten_shape, False).output(0)\n+        axis = 0\n+    if isinstance(axis, tuple):\n+        axis = list(axis)\n+    axis = ov_opset.constant(axis, Type.i32).output(0)\n+    if x_type == Type.boolean:\n+        return OpenVINOKerasTensor(\n+            ov_opset.reduce_logical_or(x, axis, keepdims).output(0)\n+        )\n+    return OpenVINOKerasTensor(ov_opset.reduce_max(x, axis, keepdims).output(0))\n \n \n def amin(x, axis=None, keepdims=False):\n-    raise NotImplementedError(\"`amin` is not supported with openvino backend\")\n+    if axis == () or axis == []:\n+        return x\n+    x = get_ov_output(x)\n+    x_type = x.get_element_type()\n+    if axis is None:\n+        flatten_shape = ov_opset.constant([-1], Type.i32).output(0)\n+        x = ov_opset.reshape(x, flatten_shape, False).output(0)\n+        axis = 0\n+    if isinstance(axis, tuple):\n+        axis = list(axis)\n+    axis = ov_opset.constant(axis, Type.i32).output(0)\n+    if x_type == Type.boolean:\n+        return OpenVINOKerasTensor(\n+            ov_opset.reduce_logical_and(x, axis, keepdims).output(0)\n+        )\n+    return OpenVINOKerasTensor(ov_opset.reduce_min(x, axis, keepdims).output(0))\n \n \n def append(x1, x2, axis=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#04ca9c8938baf1b9ded1f082b83dc72c8c9a80c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 33 | Churn Cumulative: 98 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 0.8823529411764706\n\nDIFF:\n@@ -128,7 +128,7 @@ class HashedCrossing(Layer):\n                 return ()\n             return (self.num_bins,)\n         if self.output_mode == \"int\":\n-            return input_shape[0]\n+            return tuple(input_shape[0])\n \n         if self.output_mode == \"one_hot\" and input_shape[0][-1] != 1:\n             return tuple(input_shape[0]) + (self.num_bins,)\n@@ -143,7 +143,8 @@ class HashedCrossing(Layer):\n         self._check_input_shape_and_type(inputs)\n \n         # Uprank to rank 2 for the cross_hashed op.\n-        rank = len(inputs[0].shape)\n+        first_shape = tuple(inputs[0].shape)\n+        rank = len(first_shape)\n         if rank < 2:\n             inputs = [tf_backend.numpy.expand_dims(x, -1) for x in inputs]\n         if rank < 1:\n@@ -153,14 +154,13 @@ class HashedCrossing(Layer):\n         outputs = tf.sparse.cross_hashed(inputs, self.num_bins)\n         outputs = tf.sparse.to_dense(outputs)\n \n-        # Fix output shape and downrank to match input rank.\n+        # tf.sparse.cross_hashed output shape will always have None dimensions.\n+        # Re-apply the known static shape and downrank to match input rank.\n         if rank == 2:\n-            # tf.sparse.cross_hashed output shape will always be None on the\n-            # last dimension. Given our input shape restrictions, we want to\n-            # force shape 1 instead.\n-            outputs = tf.reshape(outputs, [-1, 1])\n+            outputs.set_shape(first_shape)\n         elif rank == 1:\n-            outputs = tf.reshape(outputs, [-1])\n+            outputs.set_shape(first_shape + (1,))\n+            outputs = tf.squeeze(outputs, axis=1)\n         elif rank == 0:\n             outputs = tf.reshape(outputs, [])\n \n\n@@ -89,6 +89,23 @@ class HashedCrossingTest(testing.TestCase):\n         output = next(iter(ds)).numpy()\n         self.assertAllClose(np.array([1, 4, 1, 1, 3]), output)\n \n+    def test_static_shape_preserved(self):\n+        layer = layers.HashedCrossing(num_bins=5)\n+\n+        def call_layer(x1, x2):\n+            result = layer((x1, x2))\n+            self.assertEqual(result.shape, (5,))\n+            return result\n+\n+        feat1 = np.array([\"A\", \"B\", \"A\", \"B\", \"A\"])\n+        feat2 = np.array([101, 101, 101, 102, 102])\n+        ds = (\n+            tf.data.Dataset.from_tensor_slices((feat1, feat2))\n+            .batch(5, drop_remainder=True)\n+            .map(call_layer)\n+        )\n+        next(iter(ds))\n+\n     def test_unsupported_shape_input_fails(self):\n         with self.assertRaisesRegex(ValueError, \"inputs should have shape\"):\n             layers.HashedCrossing(num_bins=10)(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b5261a08c62d0a97e36810812f315cd026c5f2f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 50 | Churn Cumulative: 226 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: 0.8928571428571429\n\nDIFF:\n@@ -590,13 +590,10 @@ class TextVectorization(Layer):\n             # dimension to the bounding shape of the ragged dimension.\n             shape[-1] = self._output_sequence_length\n             outputs = lookup_data.to_tensor(default_value=0, shape=shape)\n-        else:\n-            outputs = lookup_data\n-\n         # If we have a dense tensor, we need to pad/trim directly.\n-        if self._output_sequence_length is not None:\n+        elif self._output_sequence_length is not None:\n             # Maybe trim the output.\n-            outputs = outputs[..., : self._output_sequence_length]\n+            outputs = lookup_data[..., : self._output_sequence_length]\n \n             # Maybe pad the output. We need to be careful to use dynamic shape\n             # here as required_space_to_batch_paddings requires a fully known\n@@ -610,6 +607,13 @@ class TextVectorization(Layer):\n                     shape, padded_shape\n                 )\n                 outputs = tf.pad(outputs, padding)\n+                # Because `tf.pad` used a dynamic shape, the output shape is\n+                # dynamic. Apply the known static `_output_sequence_length`.\n+                static_padded_shape = lookup_data.shape.as_list()\n+                static_padded_shape[-1] = self._output_sequence_length\n+                outputs.set_shape(static_padded_shape)\n+        else:\n+            outputs = lookup_data\n \n         return backend_utils.convert_tf_tensor(outputs)\n \n\n@@ -3,6 +3,7 @@ import os\n import numpy as np\n import pytest\n import tensorflow as tf\n+from absl.testing import parameterized\n from tensorflow import data as tf_data\n \n from keras.src import Sequential\n@@ -13,7 +14,7 @@ from keras.src import saving\n from keras.src import testing\n \n \n-class TextVectorizationTest(testing.TestCase):\n+class TextVectorizationTest(testing.TestCase, parameterized.TestCase):\n     # TODO: increase coverage. Most features aren't being tested.\n \n     def test_config(self):\n@@ -108,6 +109,39 @@ class TextVectorizationTest(testing.TestCase):\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n         next(iter(ds)).numpy()\n \n+    @parameterized.named_parameters(\n+        [\n+            (\"from_ragged\", \"whitespace\"),  # intermediate tensor is ragged\n+            (\"from_dense\", None),  # intermediate tensor is dense\n+        ]\n+    )\n+    def test_static_output_sequence_length(self, split):\n+        max_tokens = 5000\n+        max_len = 4\n+        layer = layers.TextVectorization(\n+            max_tokens=max_tokens,\n+            output_mode=\"int\",\n+            output_sequence_length=max_len,\n+            split=split,\n+            vocabulary=[\"baz\", \"bar\", \"foo\"],\n+        )\n+        if split:\n+            input_data = [[\"foo qux bar\"], [\"qux baz\"]]\n+        else:\n+            input_data = [[\"foo\"], [\"baz\"]]\n+\n+        def call_layer(x):\n+            result = layer(x)\n+            self.assertEqual(result.shape, (None, 4))\n+            return result\n+\n+        ds = (\n+            tf_data.Dataset.from_tensor_slices(input_data)\n+            .batch(2)\n+            .map(call_layer)\n+        )\n+        next(iter(ds))\n+\n     @pytest.mark.skipif(\n         backend.backend() != \"tensorflow\", reason=\"Requires string tensors.\"\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
