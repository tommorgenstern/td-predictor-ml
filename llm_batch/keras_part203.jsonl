{"custom_id": "keras#f140638d8024aa7cfb634ba00792554a2b413dc9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 6 | Churn Cumulative: 9524 | Contributors (this commit): 76 | Commits (past 90d): 16 | Contributors (cumulative): 76 | DMM Complexity: 0.0\n\nDIFF:\n@@ -662,12 +662,6 @@ class Sequential(Model):\n             self.build()\n         return self.model.regularizers\n \n-    @property\n-    def constraints(self):\n-        if not self.built:\n-            self.build()\n-        return self.model.constraints\n-\n     def get_weights(self):\n         \"\"\"Retrieves the weights of the model.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#14a12e94c8edb683ae0c85d7d2db54724e902a6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 347 | Contributors (this commit): 13 | Commits (past 90d): 10 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -20,4 +20,4 @@ from . import regularizers\n # Importable from root because it's technically not a layer\r\n from .layers import Input\r\n \r\n-__version__ = '2.0.6'\r\n+__version__ = '2.0.7'\r\n\n@@ -3,12 +3,12 @@ from setuptools import find_packages\n \n \n setup(name='Keras',\n-      version='2.0.6',\n+      version='2.0.7',\n       description='Deep Learning for Python',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n-      download_url='https://github.com/fchollet/keras/tarball/2.0.6',\n+      download_url='https://github.com/fchollet/keras/tarball/2.0.7',\n       license='MIT',\n       install_requires=['theano', 'pyyaml', 'six'],\n       extras_require={\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3c31489046d144aa09b68f611c241d9ed770c07a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 239 | Contributors (this commit): 11 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -19,4 +19,17 @@ setup(name='Keras',\n                     'pytest-xdist',\n                     'pytest-cov'],\n       },\n+      classifiers=[\n+          'Development Status :: 5 - Production/Stable',\n+          'Intended Audience :: Developers',\n+          'Intended Audience :: Education',\n+          'Intended Audience :: Science/Research',\n+          'License :: OSI Approved :: MIT License',\n+          'Programming Language :: Python :: 2',\n+          'Programming Language :: Python :: 2.7',\n+          'Programming Language :: Python :: 3',\n+          'Programming Language :: Python :: 3.6',\n+          'Topic :: Software Development :: Libraries',\n+          'Topic :: Software Development :: Libraries :: Python Modules'\n+      ],\n       packages=find_packages())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9eb32b0a78254f7f29e1945fc8b1c9682effd2f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 37 | Files Changed: 8 | Hunks: 34 | Methods Changed: 9 | Complexity Δ (Sum/Max): 3/4 | Churn Δ: 99 | Churn Cumulative: 3742 | Contributors (this commit): 13 | Commits (past 90d): 35 | Contributors (cumulative): 39 | DMM Complexity: 0.0\n\nDIFF:\n@@ -85,8 +85,8 @@ def _obtain_input_shape(input_shape,\n                         default_size,\n                         min_size,\n                         data_format,\n-                        include_top,\n-                        weights='imagenet'):\n+                        require_flatten,\n+                        weights=None):\n     \"\"\"Internal utility to compute/validate an ImageNet model's input shape.\n \n     # Arguments\n@@ -95,7 +95,7 @@ def _obtain_input_shape(input_shape,\n         default_size: default input width/height for the model.\n         min_size: minimum input width/height accepted by the model.\n         data_format: image data format to use.\n-        include_top: whether the model is expected to\n+        require_flatten: whether the model is expected to\n             be linked to a classifier via a Flatten layer.\n         weights: one of `None` (random initialization)\n             or 'imagenet' (pre-training on ImageNet).\n@@ -107,57 +107,72 @@ def _obtain_input_shape(input_shape,\n     # Raises\n         ValueError: in case of invalid argument values.\n     \"\"\"\n-    if weights != 'imagenet' and input_shape is not None and len(input_shape) == 3:\n+    if weights != 'imagenet' and input_shape and len(input_shape) == 3:\n         if data_format == 'channels_first':\n-            if input_shape[0] != 3 and input_shape[0] != 1:\n+            if input_shape[0] not in {1, 3}:\n                 warnings.warn(\n                     'This model usually expects 1 or 3 input channels. '\n-                    'However, it was passed ' + str(input_shape[0]) + ' input channels.')\n+                    'However, it was passed an input_shape with ' +\n+                    str(input_shape[0]) + ' input channels.')\n             default_shape = (input_shape[0], default_size, default_size)\n         else:\n-            if input_shape[-1] != 3 and input_shape[-1] != 1:\n+            if input_shape[-1] not in {1, 3}:\n                 warnings.warn(\n                     'This model usually expects 1 or 3 input channels. '\n-                    'However, it was passed ' + str(input_shape[-1]) + ' input channels.')\n+                    'However, it was passed an input_shape with ' +\n+                    str(input_shape[-1]) + ' input channels.')\n             default_shape = (default_size, default_size, input_shape[-1])\n     else:\n         if data_format == 'channels_first':\n             default_shape = (3, default_size, default_size)\n         else:\n             default_shape = (default_size, default_size, 3)\n-    if include_top:\n+    if weights == 'imagenet' and require_flatten:\n         if input_shape is not None:\n             if input_shape != default_shape:\n-                raise ValueError('When setting`include_top=True`, '\n-                                 '`input_shape` should be ' + str(default_shape) + '.')\n-        input_shape = default_shape\n-    else:\n+                raise ValueError('When setting`include_top=True` '\n+                                 'and loading `imagenet` weights, '\n+                                 '`input_shape` should be ' +\n+                                 str(default_shape) + '.')\n+        return default_shape\n+    if input_shape:\n         if data_format == 'channels_first':\n             if input_shape is not None:\n                 if len(input_shape) != 3:\n-                    raise ValueError('`input_shape` must be a tuple of three integers.')\n+                    raise ValueError(\n+                        '`input_shape` must be a tuple of three integers.')\n                 if input_shape[0] != 3 and weights == 'imagenet':\n                     raise ValueError('The input must have 3 channels; got '\n                                      '`input_shape=' + str(input_shape) + '`')\n                 if ((input_shape[1] is not None and input_shape[1] < min_size) or\n                    (input_shape[2] is not None and input_shape[2] < min_size)):\n                     raise ValueError('Input size must be at least ' +\n-                                     str(min_size) + 'x' + str(min_size) + ', got '\n+                                     str(min_size) + 'x' + str(min_size) + '; got '\n                                      '`input_shape=' + str(input_shape) + '`')\n-            else:\n-                input_shape = (3, None, None)\n         else:\n             if input_shape is not None:\n                 if len(input_shape) != 3:\n-                    raise ValueError('`input_shape` must be a tuple of three integers.')\n+                    raise ValueError(\n+                        '`input_shape` must be a tuple of three integers.')\n                 if input_shape[-1] != 3 and weights == 'imagenet':\n                     raise ValueError('The input must have 3 channels; got '\n                                      '`input_shape=' + str(input_shape) + '`')\n                 if ((input_shape[0] is not None and input_shape[0] < min_size) or\n                    (input_shape[1] is not None and input_shape[1] < min_size)):\n                     raise ValueError('Input size must be at least ' +\n-                                     str(min_size) + 'x' + str(min_size) + ', got '\n+                                     str(min_size) + 'x' + str(min_size) + '; got '\n                                      '`input_shape=' + str(input_shape) + '`')\n+    else:\n+        if require_flatten:\n+            input_shape = default_shape\n+        else:\n+            if data_format == 'channels_first':\n+                input_shape = (3, None, None)\n             else:\n                 input_shape = (None, None, 3)\n+    if require_flatten:\n+        if None in input_shape:\n+            raise ValueError('If `include_top` is True, '\n+                             'you should specify a static `input_shape`. '\n+                             'Got `input_shape=' + str(input_shape) + '`')\n     return input_shape\n\n@@ -151,7 +151,7 @@ def InceptionV3(include_top=True,\n         default_size=299,\n         min_size=139,\n         data_format=K.image_data_format(),\n-        include_top=include_top,\n+        require_flatten=False,\n         weights=weights)\n \n     if input_tensor is None:\n\n@@ -397,7 +397,7 @@ def MobileNet(input_shape=None,\n                                       default_size=default_size,\n                                       min_size=32,\n                                       data_format=K.image_data_format(),\n-                                      include_top=include_top or weights,\n+                                      require_flatten=include_top,\n                                       weights=weights)\n \n     if K.image_data_format() == 'channels_last':\n\n@@ -188,7 +188,7 @@ def ResNet50(include_top=True, weights='imagenet',\n                                       default_size=224,\n                                       min_size=197,\n                                       data_format=K.image_data_format(),\n-                                      include_top=include_top,\n+                                      require_flatten=include_top,\n                                       weights=weights)\n \n     if input_tensor is None:\n\n@@ -98,7 +98,7 @@ def VGG16(include_top=True, weights='imagenet',\n                                       default_size=224,\n                                       min_size=48,\n                                       data_format=K.image_data_format(),\n-                                      include_top=include_top,\n+                                      require_flatten=include_top,\n                                       weights=weights)\n \n     if input_tensor is None:\n\n@@ -98,7 +98,7 @@ def VGG19(include_top=True, weights='imagenet',\n                                       default_size=224,\n                                       min_size=48,\n                                       data_format=K.image_data_format(),\n-                                      include_top=include_top,\n+                                      require_flatten=include_top,\n                                       weights=weights)\n \n     if input_tensor is None:\n\n@@ -128,7 +128,7 @@ def Xception(include_top=True, weights='imagenet',\n                                       default_size=299,\n                                       min_size=71,\n                                       data_format=K.image_data_format(),\n-                                      include_top=include_top,\n+                                      require_flatten=False,\n                                       weights=weights)\n \n     if input_tensor is None:\n\n@@ -46,7 +46,8 @@ def test_obtain_input_shape():\n             default_size=299,\n             min_size=139,\n             data_format='channels_last',\n-            include_top=True)\n+            require_flatten=True,\n+            weights='imagenet')\n \n     # Test invalid use cases\n     for data_format in ['channels_last', 'channels_first']:\n@@ -60,7 +61,7 @@ def test_obtain_input_shape():\n                 default_size=None,\n                 min_size=139,\n                 data_format=data_format,\n-                include_top=False,\n+                require_flatten=False,\n                 weights='fake_weights')\n \n         # input_shape is smaller than min_size.\n@@ -72,7 +73,7 @@ def test_obtain_input_shape():\n                 default_size=None,\n                 min_size=139,\n                 data_format=data_format,\n-                include_top=False)\n+                require_flatten=False)\n \n         # shape is 1D.\n         shape = (100,)\n@@ -83,7 +84,7 @@ def test_obtain_input_shape():\n                 default_size=None,\n                 min_size=139,\n                 data_format=data_format,\n-                include_top=False)\n+                require_flatten=False)\n \n         # the number of channels is 5 not 3.\n         shape = (100, 100)\n@@ -94,50 +95,59 @@ def test_obtain_input_shape():\n                 default_size=None,\n                 min_size=139,\n                 data_format=data_format,\n-                include_top=False)\n+                require_flatten=False)\n+\n+        # require_flatten=True with dynamic input shape.\n+        with pytest.raises(ValueError):\n+            utils._obtain_input_shape(\n+                input_shape=None,\n+                default_size=None,\n+                min_size=139,\n+                data_format='channels_first',\n+                require_flatten=True)\n \n     # test include top\n     assert utils._obtain_input_shape(\n-        input_shape=None,\n+        input_shape=(3, 200, 200),\n         default_size=None,\n         min_size=139,\n         data_format='channels_first',\n-        include_top=True,) == (3, None, None)\n+        require_flatten=True) == (3, 200, 200)\n \n     assert utils._obtain_input_shape(\n         input_shape=None,\n         default_size=None,\n         min_size=139,\n         data_format='channels_last',\n-        include_top=False) == (None, None, 3)\n+        require_flatten=False) == (None, None, 3)\n \n     assert utils._obtain_input_shape(\n         input_shape=None,\n         default_size=None,\n         min_size=139,\n         data_format='channels_first',\n-        include_top=False) == (3, None, None)\n+        require_flatten=False) == (3, None, None)\n \n     assert utils._obtain_input_shape(\n         input_shape=None,\n         default_size=None,\n         min_size=139,\n         data_format='channels_last',\n-        include_top=False) == (None, None, 3)\n+        require_flatten=False) == (None, None, 3)\n \n     assert utils._obtain_input_shape(\n         input_shape=(150, 150, 3),\n         default_size=None,\n         min_size=139,\n         data_format='channels_last',\n-        include_top=False) == (150, 150, 3)\n+        require_flatten=False) == (150, 150, 3)\n \n     assert utils._obtain_input_shape(\n         input_shape=(3, None, None),\n         default_size=None,\n         min_size=139,\n         data_format='channels_first',\n-        include_top=False) == (3, None, None)\n+        require_flatten=False) == (3, None, None)\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#619259c1f067e342f3003865c39631f84eafc2a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 8437 | Contributors (this commit): 87 | Commits (past 90d): 30 | Contributors (cumulative): 87 | DMM Complexity: 0.0\n\nDIFF:\n@@ -319,8 +319,11 @@ def variable(value, dtype=None, name=None, constraint=None):\n     elif hasattr(value, 'get_shape'):\n         v._keras_shape = int_shape(value)\n     v._uses_learning_phase = False\n-    # TODO: move to `tf.get_variable` when supported in public release.\n+    # TODO: move to Variable constructor when supported in public release.\n+    try:\n         v.constraint = constraint\n+    except AttributeError:\n+        v._constraint = constraint\n     return v\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1386dbae617641d6c60025bc9f68391766323ffe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 8439 | Contributors (this commit): 88 | Commits (past 90d): 31 | Contributors (cumulative): 88 | DMM Complexity: None\n\nDIFF:\n@@ -3605,7 +3605,7 @@ def ctc_label_dense_to_sparse(labels, label_lengths):\n         label_lengths: length of the labels.\n \n     # Returns\n-        A sparse tensor representation of the lablels.\n+        A sparse tensor representation of the labels.\n     \"\"\"\n     label_shape = tf.shape(labels)\n     num_batches_tns = tf.stack([label_shape[0]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#22e6bea8c2e23c6bbd6d98b4d3fe8b2e74c33c3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2307 | Contributors (this commit): 34 | Commits (past 90d): 10 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -971,7 +971,7 @@ class DirectoryIterator(Iterator):\n         self.save_prefix = save_prefix\n         self.save_format = save_format\n \n-        white_list_formats = {'png', 'jpg', 'jpeg', 'bmp'}\n+        white_list_formats = {'png', 'jpg', 'jpeg', 'bmp', 'ppm'}\n \n         # first, count the number of samples and classes\n         self.samples = 0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#19100c1c0ee50f5ed7d09a620e5942ee9c3f89ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 15 | Churn Cumulative: 6476 | Contributors (this commit): 62 | Commits (past 90d): 45 | Contributors (cumulative): 73 | DMM Complexity: None\n\nDIFF:\n@@ -52,7 +52,7 @@ def _standardize_input_data(data, names, shapes=None,\n         ValueError: in case of improperly formatted user-provided data.\n     \"\"\"\n     if not names:\n-        if data:\n+        if data is not None and hasattr(data, '__len__') and len(data):\n             raise ValueError('Error when checking model ' +\n                              exception_prefix + ': '\n                              'expected no data, but got:', data)\n@@ -602,6 +602,8 @@ class Model(Container):\n                 If the model has multiple outputs, you can use a different\n                 `sample_weight_mode` on each output by passing a\n                 dictionary or a list of modes.\n+            weighted_metrics: List of metrics to be evaluated and weighted\n+                by sample_weight or class_weight during training and testing.\n             target_tensors: By default, Keras will create placeholders for the\n                 model's target, which will be fed with the target data during\n                 training. If instead you would like to use your own\n@@ -610,8 +612,6 @@ class Model(Container):\n                 can specify them via the `target_tensors` argument. It can be\n                 a single tensor (for a single-output model), a list of tensors,\n                 or a dict mapping output names to target tensors.\n-            weighted_metrics: List of metrics to be evaluated and weighted\n-                by sample_weight or class_weight during training and testing\n             **kwargs: When using the Theano/CNTK backends, these arguments\n                 are passed into K.function. When using the TensorFlow backend,\n                 these arguments are passed into `tf.Session.run`.\n@@ -857,7 +857,7 @@ class Model(Container):\n                     total_loss += loss_weight * output_loss\n             if total_loss is None:\n                 if not self.losses:\n-                    raise RuntimeError('The model cannot be compiled '\n+                    raise ValueError('The model cannot be compiled '\n                                      'because it has no loss to optimize.')\n                 else:\n                     total_loss = 0.\n@@ -1013,7 +1013,7 @@ class Model(Container):\n             steps_name: The public API's parameter name for `steps`.\n \n         # Raises\n-            ValueError when `steps` is `None` and the attribute `ins.shape`\n+            ValueError: when `steps` is `None` and the attribute `ins.shape`\n             does not exist. Also raises ValueError when `steps` is not `None`\n             and `batch_size` is not `None` because they are mutually\n             exclusive.\n@@ -1067,6 +1067,9 @@ class Model(Container):\n             steps_per_epoch: Total number of steps (batches of samples)\n                 before declaring one epoch finished and starting the\n                 next epoch. Ignored with the default value of `None`.\n+            validation_steps: Number of steps to run validation for\n+                (only if doing validation from data tensors).\n+                Ignored with the default value of `None`.\n \n         # Returns\n             `History` object.\n\n@@ -361,7 +361,7 @@ def test_model_methods():\n         model.compile(optimizer, loss='mse', sample_weight_mode={'dense_1': 'temporal'})\n \n     # `loss` does not exist.\n-    with pytest.raises(RuntimeError):\n+    with pytest.raises(ValueError):\n         model.compile(optimizer, loss=[])\n \n     model.compile(optimizer, loss=['mse', 'mae'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f8b0dc22c6aebbc09ae15b6b522bbf90ccde9c32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 694 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,7 @@ epochs = 50\n epsilon_std = 1.0\n \n \n-x = Input(batch_shape=(batch_size, original_dim))\n+x = Input(shape=(original_dim,))\n h = Dense(intermediate_dim, activation='relu')(x)\n z_mean = Dense(latent_dim)(h)\n z_log_var = Dense(latent_dim)(h)\n@@ -28,7 +28,7 @@ z_log_var = Dense(latent_dim)(h)\n \n def sampling(args):\n     z_mean, z_log_var = args\n-    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n+    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n                               stddev=epsilon_std)\n     return z_mean + K.exp(z_log_var / 2) * epsilon\n \n\n@@ -31,7 +31,7 @@ intermediate_dim = 128\n epsilon_std = 1.0\n epochs = 5\n \n-x = Input(batch_shape=(batch_size,) + original_img_size)\n+x = Input(shape=original_img_size)\n conv_1 = Conv2D(img_chns,\n                 kernel_size=(2, 2),\n                 padding='same', activation='relu')(x)\n@@ -56,7 +56,7 @@ z_log_var = Dense(latent_dim)(hidden)\n \n def sampling(args):\n     z_mean, z_log_var = args\n-    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n+    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n                               mean=0., stddev=epsilon_std)\n     return z_mean + K.exp(z_log_var) * epsilon\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dc091b06354988fe20a49852b244f0bc5e4c05fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 784 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -409,6 +409,7 @@ def train(run_name, start_epoch, stop_epoch, img_w):\n     pool_size = 2\n     time_dense_size = 32\n     rnn_size = 512\n+    minibatch_size = 32\n \n     if K.image_data_format() == 'channels_first':\n         input_shape = (1, img_w, img_h)\n@@ -420,7 +421,7 @@ def train(run_name, start_epoch, stop_epoch, img_w):\n \n     img_gen = TextImageGenerator(monogram_file=os.path.join(fdir, 'wordlist_mono_clean.txt'),\n                                  bigram_file=os.path.join(fdir, 'wordlist_bi_clean.txt'),\n-                                 minibatch_size=32,\n+                                 minibatch_size=minibatch_size,\n                                  img_w=img_w,\n                                  img_h=img_h,\n                                  downsample_factor=(pool_size ** 2),\n@@ -479,9 +480,13 @@ def train(run_name, start_epoch, stop_epoch, img_w):\n \n     viz_cb = VizCallback(run_name, test_func, img_gen.next_val())\n \n-    model.fit_generator(generator=img_gen.next_train(), steps_per_epoch=(words_per_epoch - val_words),\n-                        epochs=stop_epoch, validation_data=img_gen.next_val(), validation_steps=val_words,\n-                        callbacks=[viz_cb, img_gen], initial_epoch=start_epoch)\n+    model.fit_generator(generator=img_gen.next_train(), \n+                        steps_per_epoch=(words_per_epoch - val_words) // minibatch_size,\n+                        epochs=stop_epoch, \n+                        validation_data=img_gen.next_val(), \n+                        validation_steps=val_words // minibatch_size,\n+                        callbacks=[viz_cb, img_gen], \n+                        initial_epoch=start_epoch)\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d19ae8f97514bc37693f95f83b2968103673b2a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 28 | Files Changed: 2 | Hunks: 12 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 103 | Churn Cumulative: 6579 | Contributors (this commit): 62 | Commits (past 90d): 47 | Contributors (cumulative): 73 | DMM Complexity: 0.9705882352941176\n\nDIFF:\n@@ -1077,22 +1077,20 @@ class Model(Container):\n         do_validation = False\n         if val_f and val_ins:\n             do_validation = True\n-            if steps_per_epoch and not validation_steps:\n-                if hasattr(ins[0], 'shape'):\n-                    validation_steps = steps_per_epoch\n-                else:\n-                    raise ValueError('You must specify a value for '\n-                                     '`validation_steps` when using '\n-                                     '`steps_per_epoch` validation '\n-                                     'without a `shape` attribute.')\n-\n-            if verbose and ins and hasattr(ins[0], 'shape'):\n+            if verbose and ins and hasattr(ins[0], 'shape') and hasattr(val_ins[0], 'shape'):\n                 print('Train on %d samples, validate on %d samples' %\n                       (ins[0].shape[0], val_ins[0].shape[0]))\n+        if validation_steps:\n+            do_validation = True\n+            if steps_per_epoch is None:\n+                raise ValueError('Can only use `validation_steps` '\n+                                 'when doing step-wise '\n+                                 'training, i.e. `steps_per_epoch` '\n+                                 'must be set.')\n \n         num_train_samples = self._check_num_samples(ins, batch_size,\n-                                                    steps_per_epoch, 'steps_per_epoch')\n-\n+                                                    steps_per_epoch,\n+                                                    'steps_per_epoch')\n         if num_train_samples is not None:\n             index_array = np.arange(num_train_samples)\n \n@@ -1523,6 +1521,7 @@ class Model(Container):\n             check_batch_axis=False,\n             batch_size=batch_size)\n         # Prepare validation data.\n+        do_validation = False\n         if validation_data:\n             do_validation = True\n             if len(validation_data) == 2:\n@@ -1542,8 +1541,6 @@ class Model(Container):\n                 sample_weight=val_sample_weight,\n                 check_batch_axis=False,\n                 batch_size=batch_size)\n-            self._make_test_function()\n-            val_f = self.test_function\n             if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n                 val_ins = val_x + val_y + val_sample_weights + [0.]\n             else:\n@@ -1560,16 +1557,15 @@ class Model(Container):\n             sample_weights, val_sample_weights = (\n                 _slice_arrays(sample_weights, 0, split_at),\n                 _slice_arrays(sample_weights, split_at))\n-            self._make_test_function()\n-            val_f = self.test_function\n             if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n                 val_ins = val_x + val_y + val_sample_weights + [0.]\n             else:\n                 val_ins = val_x + val_y + val_sample_weights\n-        else:\n-            do_validation = False\n-            val_f = None\n-            val_ins = None\n+\n+        elif validation_steps:\n+            do_validation = True\n+            if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+                val_ins = [0.]\n \n         # Prepare input arrays and training function.\n         if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n@@ -1583,9 +1579,13 @@ class Model(Container):\n         out_labels = self._get_deduped_metrics_names()\n \n         if do_validation:\n+            self._make_test_function()\n+            val_f = self.test_function\n             callback_metrics = copy.copy(out_labels) + ['val_' + n for n in out_labels]\n         else:\n             callback_metrics = copy.copy(out_labels)\n+            val_f = None\n+            val_ins = []\n \n         # Delegate logic to `_fit_loop`.\n         return self._fit_loop(f, ins, out_labels=out_labels,\n\n@@ -141,14 +141,6 @@ def test_model_methods():\n     out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                     [output_a_np, output_b_np],\n                     epochs=1, batch_size=4, validation_split=0.5)\n-    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n-                    {'dense_1': output_a_np, 'dropout': output_b_np},\n-                    epochs=1, batch_size=None, validation_split=0.5,\n-                    steps_per_epoch=1)\n-    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n-                    {'dense_1': output_a_np, 'dropout': output_b_np},\n-                    epochs=1, batch_size=None, validation_split=0.5,\n-                    steps_per_epoch=1, validation_steps=1)\n \n     # test validation data\n     out = model.fit([input_a_np, input_b_np],\n@@ -743,6 +735,17 @@ def test_model_with_external_loss():\n             out = model.fit(None, None, epochs=1, batch_size=10)\n         out = model.fit(None, None, epochs=1, steps_per_epoch=1)\n \n+        # test fit with validation data\n+        with pytest.raises(ValueError):\n+            out = model.fit(None, None,\n+                            epochs=1,\n+                            steps_per_epoch=None,\n+                            validation_steps=2)\n+        out = model.fit(None, None,\n+                        epochs=1,\n+                        steps_per_epoch=2,\n+                        validation_steps=2)\n+\n         # test evaluate\n         with pytest.raises(ValueError):\n             out = model.evaluate(None, None, batch_size=10)\n@@ -754,6 +757,50 @@ def test_model_with_external_loss():\n         out = model.predict(None, steps=3)\n         assert out.shape == (10 * 3, 4)\n \n+        # Test multi-output model without external data.\n+        a = Input(tensor=tf.Variable(input_a_np, dtype=tf.float32))\n+        a_1 = Dense(4, name='dense_1')(a)\n+        a_2 = Dropout(0.5, name='dropout')(a_1)\n+        model = Model(a, [a_1, a_2])\n+        model.add_loss(K.mean(a_2))\n+        model.compile(optimizer='rmsprop',\n+                      loss=None,\n+                      metrics=['mean_squared_error'])\n+\n+        # test train_on_batch\n+        out = model.train_on_batch(None, None)\n+        out = model.test_on_batch(None, None)\n+        out = model.predict_on_batch(None)\n+\n+        # test fit\n+        with pytest.raises(ValueError):\n+            out = model.fit(None, None, epochs=1, batch_size=10)\n+        out = model.fit(None, None, epochs=1, steps_per_epoch=1)\n+\n+        # test fit with validation data\n+        with pytest.raises(ValueError):\n+            out = model.fit(None, None,\n+                            epochs=1,\n+                            steps_per_epoch=None,\n+                            validation_steps=2)\n+        out = model.fit(None, None,\n+                        epochs=1,\n+                        steps_per_epoch=2,\n+                        validation_steps=2)\n+\n+        # test evaluate\n+        with pytest.raises(ValueError):\n+            out = model.evaluate(None, None, batch_size=10)\n+        out = model.evaluate(None, None, steps=3)\n+\n+        # test predict\n+        with pytest.raises(ValueError):\n+            out = model.predict(None, batch_size=10)\n+        out = model.predict(None, steps=3)\n+        assert len(out) == 2\n+        assert out[0].shape == (10 * 3, 4)\n+        assert out[1].shape == (10 * 3, 4)\n+\n \n @keras_test\n def test_target_tensors():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d440e4bf7a2dc84bb7ccfed023d0532fa2f9db83", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 52 | Churn Cumulative: 7687 | Contributors (this commit): 54 | Commits (past 90d): 17 | Contributors (cumulative): 63 | DMM Complexity: 0.9\n\nDIFF:\n@@ -252,6 +252,14 @@ class Recurrent(Layer):\n         return inputs\n \n     def __call__(self, inputs, initial_state=None, **kwargs):\n+\n+        # If there are multiple inputs, then\n+        # they should be the main input and `initial_state`\n+        # e.g. when loading model from file\n+        if isinstance(inputs, (list, tuple)) and len(inputs) > 1 and initial_state is None:\n+            initial_state = inputs[1:]\n+            inputs = inputs[0]\n+\n         # If `initial_state` is specified,\n         # and if it a Keras tensor,\n         # then add it to the inputs and temporarily\n\n@@ -268,6 +268,29 @@ def test_reset_states_with_values(layer_class):\n         layer.reset_states([1] * (len(layer.states) + 1))\n \n \n+@rnn_test\n+def test_initial_states_as_other_inputs(layer_class):\n+    num_states = 2 if layer_class is recurrent.LSTM else 1\n+\n+    # Test with Keras tensor\n+    main_inputs = Input((timesteps, embedding_dim))\n+    initial_state = [Input((units,)) for _ in range(num_states)]\n+    inputs = [main_inputs] + initial_state\n+\n+    layer = layer_class(units)\n+    output = layer(inputs)\n+    assert initial_state[0] in layer.inbound_nodes[0].input_tensors\n+\n+    model = Model(inputs, output)\n+    model.compile(loss='categorical_crossentropy', optimizer='adam')\n+\n+    main_inputs = np.random.random((num_samples, timesteps, embedding_dim))\n+    initial_state = [np.random.random((num_samples, units))\n+                     for _ in range(num_states)]\n+    targets = np.random.random((num_samples, units))\n+    model.train_on_batch([main_inputs] + initial_state, targets)\n+\n+\n @rnn_test\n def test_specify_state_with_masking(layer_class):\n     ''' This test based on a previously failing issue here:\n\n@@ -6,7 +6,7 @@ from numpy.testing import assert_allclose\n \n from keras import backend as K\n from keras.models import Model, Sequential\n-from keras.layers import Dense, Lambda, RepeatVector, TimeDistributed\n+from keras.layers import Dense, Lambda, RepeatVector, TimeDistributed, LSTM\n from keras.layers import Input\n from keras import optimizers\n from keras import losses\n@@ -351,5 +351,24 @@ def test_saving_custom_activation_function():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n+@keras_test\n+def test_saving_recurrent_layer_with_init_state():\n+    vector_size = 8\n+    input_length = 20\n+\n+    input_initial_state = Input(shape=(vector_size,))\n+    input_x = Input(shape=(input_length, vector_size))\n+\n+    lstm = LSTM(vector_size, return_sequences=True)(\n+        input_x, initial_state=[input_initial_state, input_initial_state])\n+\n+    model = Model(inputs=[input_x, input_initial_state], outputs=[lstm])\n+\n+    _, fname = tempfile.mkstemp('.h5')\n+    model.save(fname)\n+\n+    loaded_model = load_model(fname)\n+    os.remove(fname)\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2d8739dda9859b91bf2b7da5402d242555c48d7d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 39 | Files Changed: 4 | Hunks: 34 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 79 | Churn Cumulative: 16856 | Contributors (this commit): 152 | Commits (past 90d): 73 | Contributors (cumulative): 183 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2153,7 +2153,7 @@ def set_value(x, value):\n         value: Value to set the tensor to, as a Numpy array\n             (of the same shape).\n     \"\"\"\n-    value = np.asarray(value)\n+    value = np.asarray(value, dtype=dtype(x))\n     tf_dtype = _convert_string_dtype(x.dtype.name.split('_')[0])\n     if hasattr(x, '_assign_placeholder'):\n         assign_placeholder = x._assign_placeholder\n@@ -2177,7 +2177,7 @@ def batch_set_value(tuples):\n         assign_ops = []\n         feed_dict = {}\n         for x, value in tuples:\n-            value = np.asarray(value)\n+            value = np.asarray(value, dtype=dtype(x))\n             tf_dtype = _convert_string_dtype(x.dtype.name.split('_')[0])\n             if hasattr(x, '_assign_placeholder'):\n                 assign_placeholder = x._assign_placeholder\n\n@@ -875,7 +875,7 @@ class Model(Container):\n         def append_metric(layer_index, metric_name, metric_tensor):\n             \"\"\"Helper function used in loop below.\"\"\"\n             if len(self.output_names) > 1:\n-                metric_name = self.output_layers[layer_index].name + '_' + metric_name\n+                metric_name = self.output_names[layer_index] + '_' + metric_name\n             self.metrics_names.append(metric_name)\n             self.metrics_tensors.append(metric_tensor)\n \n\n@@ -23,7 +23,7 @@ class Wrapper(Layer):\n     def __init__(self, layer, **kwargs):\n         self.layer = layer\n         # Tracks mapping of Wrapper inputs to inner layer inputs. Useful when\n-        # the inner layer has update ops that depend on it's inputs (as opposed\n+        # the inner layer has update ops that depend on its inputs (as opposed\n         # to the inputs to the Wrapper layer).\n         self._input_map = {}\n         super(Wrapper, self).__init__(**kwargs)\n\n@@ -144,7 +144,7 @@ class SGD(Optimizer):\n                  nesterov=False, **kwargs):\n         super(SGD, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n-            self.iterations = K.variable(0., name='iterations')\n+            self.iterations = K.variable(0, dtype='int64', name='iterations')\n             self.lr = K.variable(lr, name='lr')\n             self.momentum = K.variable(momentum, name='momentum')\n             self.decay = K.variable(decay, name='decay')\n@@ -154,15 +154,14 @@ class SGD(Optimizer):\n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n-        self.updates = []\n+        self.updates = [K.update_add(self.iterations, 1)]\n \n         lr = self.lr\n         if self.initial_decay > 0:\n-            lr *= (1. / (1. + self.decay * self.iterations))\n-            self.updates.append(K.update_add(self.iterations, 1))\n-\n+            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n+                                                  K.dtype(self.decay))))\n         # momentum\n-        shapes = [K.get_variable_shape(p) for p in params]\n+        shapes = [K.int_shape(p) for p in params]\n         moments = [K.zeros(shape) for shape in shapes]\n         self.weights = [self.iterations] + moments\n         for p, g, m in zip(params, grads, moments):\n@@ -217,21 +216,21 @@ class RMSprop(Optimizer):\n             self.lr = K.variable(lr, name='lr')\n             self.rho = K.variable(rho, name='rho')\n             self.decay = K.variable(decay, name='decay')\n-            self.iterations = K.variable(0., name='iterations')\n+            self.iterations = K.variable(0, dtype='int64', name='iterations')\n         self.epsilon = epsilon\n         self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n-        accumulators = [K.zeros(K.get_variable_shape(p), dtype=K.dtype(p)) for p in params]\n+        accumulators = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n         self.weights = accumulators\n-        self.updates = []\n+        self.updates = [K.update_add(self.iterations, 1)]\n \n         lr = self.lr\n         if self.initial_decay > 0:\n-            lr *= (1. / (1. + self.decay * self.iterations))\n-            self.updates.append(K.update_add(self.iterations, 1))\n+            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n+                                                  K.dtype(self.decay))))\n \n         for p, g, a in zip(params, grads, accumulators):\n             # update accumulator\n@@ -275,22 +274,22 @@ class Adagrad(Optimizer):\n         with K.name_scope(self.__class__.__name__):\n             self.lr = K.variable(lr, name='lr')\n             self.decay = K.variable(decay, name='decay')\n-            self.iterations = K.variable(0., name='iterations')\n+            self.iterations = K.variable(0, dtype='int64', name='iterations')\n         self.epsilon = epsilon\n         self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n-        shapes = [K.get_variable_shape(p) for p in params]\n+        shapes = [K.int_shape(p) for p in params]\n         accumulators = [K.zeros(shape) for shape in shapes]\n         self.weights = accumulators\n-        self.updates = []\n+        self.updates = [K.update_add(self.iterations, 1)]\n \n         lr = self.lr\n         if self.initial_decay > 0:\n-            lr *= (1. / (1. + self.decay * self.iterations))\n-            self.updates.append(K.update_add(self.iterations, 1))\n+            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n+                                                  K.dtype(self.decay))))\n \n         for p, g, a in zip(params, grads, accumulators):\n             new_a = a + K.square(g)  # update accumulator\n@@ -335,7 +334,7 @@ class Adadelta(Optimizer):\n         with K.name_scope(self.__class__.__name__):\n             self.lr = K.variable(lr, name='lr')\n             self.decay = K.variable(decay, name='decay')\n-            self.iterations = K.variable(0., name='iterations')\n+            self.iterations = K.variable(0, dtype='int64', name='iterations')\n         self.rho = rho\n         self.epsilon = epsilon\n         self.initial_decay = decay\n@@ -343,16 +342,16 @@ class Adadelta(Optimizer):\n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n-        shapes = [K.get_variable_shape(p) for p in params]\n+        shapes = [K.int_shape(p) for p in params]\n         accumulators = [K.zeros(shape) for shape in shapes]\n         delta_accumulators = [K.zeros(shape) for shape in shapes]\n         self.weights = accumulators + delta_accumulators\n-        self.updates = []\n+        self.updates = [K.update_add(self.iterations, 1)]\n \n         lr = self.lr\n         if self.initial_decay > 0:\n-            lr *= (1. / (1. + self.decay * self.iterations))\n-            self.updates.append(K.update_add(self.iterations, 1))\n+            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n+                                                  K.dtype(self.decay))))\n \n         for p, g, a, d_a in zip(params, grads, accumulators, delta_accumulators):\n             # update accumulator\n@@ -403,7 +402,7 @@ class Adam(Optimizer):\n                  epsilon=1e-8, decay=0., **kwargs):\n         super(Adam, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n-            self.iterations = K.variable(0, name='iterations')\n+            self.iterations = K.variable(0, dtype='int64', name='iterations')\n             self.lr = K.variable(lr, name='lr')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n@@ -418,14 +417,15 @@ class Adam(Optimizer):\n \n         lr = self.lr\n         if self.initial_decay > 0:\n-            lr *= (1. / (1. + self.decay * self.iterations))\n+            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n+                                                  K.dtype(self.decay))))\n \n-        t = self.iterations + 1\n+        t = K.cast(self.iterations, K.floatx()) + 1\n         lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n                      (1. - K.pow(self.beta_1, t)))\n \n-        ms = [K.zeros(K.get_variable_shape(p), dtype=K.dtype(p)) for p in params]\n-        vs = [K.zeros(K.get_variable_shape(p), dtype=K.dtype(p)) for p in params]\n+        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n+        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n         self.weights = [self.iterations] + ms + vs\n \n         for p, g, m, v in zip(params, grads, ms, vs):\n@@ -474,7 +474,7 @@ class Adamax(Optimizer):\n                  epsilon=1e-8, decay=0., **kwargs):\n         super(Adamax, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n-            self.iterations = K.variable(0., name='iterations')\n+            self.iterations = K.variable(0, dtype='int64', name='iterations')\n             self.lr = K.variable(lr, name='lr')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n@@ -489,12 +489,13 @@ class Adamax(Optimizer):\n \n         lr = self.lr\n         if self.initial_decay > 0:\n-            lr *= (1. / (1. + self.decay * self.iterations))\n+            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n+                                                  K.dtype(self.decay))))\n \n-        t = self.iterations + 1\n+        t = K.cast(self.iterations, K.floatx()) + 1\n         lr_t = lr / (1. - K.pow(self.beta_1, t))\n \n-        shapes = [K.get_variable_shape(p) for p in params]\n+        shapes = [K.int_shape(p) for p in params]\n         # zero init of 1st moment\n         ms = [K.zeros(shape) for shape in shapes]\n         # zero init of exponentially weighted infinity norm\n@@ -552,7 +553,7 @@ class Nadam(Optimizer):\n                  epsilon=1e-8, schedule_decay=0.004, **kwargs):\n         super(Nadam, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n-            self.iterations = K.variable(0., name='iterations')\n+            self.iterations = K.variable(0, dtype='int64', name='iterations')\n             self.m_schedule = K.variable(1., name='m_schedule')\n             self.lr = K.variable(lr, name='lr')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n@@ -565,7 +566,7 @@ class Nadam(Optimizer):\n         grads = self.get_gradients(loss, params)\n         self.updates = [K.update_add(self.iterations, 1)]\n \n-        t = self.iterations + 1\n+        t = K.cast(self.iterations, K.floatx()) + 1\n \n         # Due to the recommendations in [2], i.e. warming momentum schedule\n         momentum_cache_t = self.beta_1 * (1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), t * self.schedule_decay)))\n@@ -574,7 +575,7 @@ class Nadam(Optimizer):\n         m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n         self.updates.append((self.m_schedule, m_schedule_new))\n \n-        shapes = [K.get_variable_shape(p) for p in params]\n+        shapes = [K.int_shape(p) for p in params]\n         ms = [K.zeros(shape) for shape in shapes]\n         vs = [K.zeros(shape) for shape in shapes]\n \n@@ -618,13 +619,13 @@ class TFOptimizer(Optimizer):\n \n     def __init__(self, optimizer):\n         self.optimizer = optimizer\n-        self.updates = []\n         with K.name_scope(self.__class__.__name__):\n-            self.iterations = K.variable(0., name='iterations')\n+            self.iterations = K.variable(0, dtype='int64', name='iterations')\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n         grads = self.optimizer.compute_gradients(loss, params)\n+        self.updates = [K.update_add(self.iterations, 1)]\n         opt_update = self.optimizer.apply_gradients(\n             grads, global_step=self.iterations)\n         self.updates.append(opt_update)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
