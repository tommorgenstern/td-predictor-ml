{"custom_id": "keras#13248e101fac235260237ea0bce66c21385e1254", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 692 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 692 | Churn Cumulative: 692 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,692 @@\n+# -*- coding: utf-8 -*-\n+\"\"\"\n+Author: [lukewood](https://lukewood.xyz)\n+Date created: 03/28/2023\n+Last modified: 07/25/2023\n+Description: Use KerasCV to train powerful image classifiers.\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+Classification is the process of predicting a categorical label for a given\n+input image.\n+While classification is a relatively straightforward computer vision task,\n+modern approaches still are built of several complex components.\n+Luckily, KerasCV provides APIs to construct commonly used components.\n+\n+This guide demonstrates KerasCV's modular approach to solving image\n+classification problems at three levels of complexity:\n+\n+- Inference with a pretrained classifier\n+- Fine-tuning a pretrained backbone\n+- Training a image classifier from scratch\n+\n+## Multi-Backend Support\n+\n+KerasCV's `ImageClassifier` model supports several backends like JAX, PyTorch,\n+and TensorFlow with the help of `keras_core`. To enable multi-backend support\n+in KerasCV, set the `KERAS_CV_MULTI_BACKEND` environment variable. We can\n+then switch between different backends by setting the `KERAS_BACKEND`\n+environment variable. Currently, `\"tensorflow\"`, `\"jax\"`, and `\"torch\"` are\n+supported.\n+\n+This demonstration uses the Jax backend.\n+\"\"\"\n+\n+import os\n+\n+os.environ[\"KERAS_CV_MULTI_BACKEND\"] = \"1\"\n+os.environ[\"KERAS_BACKEND\"] = \"jax\"\n+\n+import json\n+import math\n+import keras_cv\n+import keras_core as keras\n+from keras_core import operations as ops\n+from keras_core import losses\n+from keras_core import optimizers\n+from keras_core.optimizers import schedules\n+from keras_core import metrics\n+import tensorflow as tf\n+import tensorflow_datasets as tfds\n+import numpy as np\n+\n+\"\"\"## Inference with a pretrained classifier\n+\n+Let's get started with the simplest KerasCV API: a pretrained classifier.\n+In this example, we will construct a classifier that was\n+pretrained on the ImageNet dataset.\n+We'll use this model to solve the age old \"Cat or Dog\" problem.\n+\n+The highest level module in KerasCV is a *task*. A *task* is a `keras.Model`\n+consisting of a (generally pretrained) backbone model and task-specific\n+layers. Here's an example using `keras_cv.models.ImageClassifier` with an\n+EfficientNetV2B0 Backbone.\n+\n+EfficientNetV2B0 is a great starting model when constructing an image\n+classification pipeline.\n+This architecture manages to achieve high accuracy, while using a\n+parameter count of 7M.\n+If an EfficientNetV2B0 is not powerful enough for the task you are hoping to\n+solve, be sure to check out\n+[KerasCV's other available Backbones](https://github.com/keras-team/keras-cv/tree/master/keras_cv/models/backbones)!\n+\"\"\"\n+\n+classifier = keras_cv.models.ImageClassifier.from_preset(\n+    \"efficientnetv2_b0_imagenet_classifier\"\n+)\n+\n+\"\"\"You may notice a small deviation from the old `keras.applications` API;\n+where you would construct the class with\n+`EfficientNetV2B0(weights=\"imagenet\")`. While the old API was great for\n+classification, it did not scale effectively to other use cases that required\n+complex architectures, like object deteciton and semantic segmentation.\n+\n+Now that our classifier is built, let's apply it to this cute cat picture!\n+\"\"\"\n+\n+filepath = keras.utils.get_file(origin=\"https://i.imgur.com/9i63gLN.jpg\")\n+image = keras.utils.image_utils.load_img(filepath)\n+image = np.array(image)\n+keras_cv.visualization.plot_image_gallery(\n+    [image], rows=1, cols=1, value_range=(0, 255), show=True, scale=4\n+)\n+\n+\"\"\"Next, let's get some predictions from our classifier:\"\"\"\n+\n+predictions = classifier.predict(np.expand_dims(image, axis=0))\n+\n+\"\"\"Predictions come in the form of softmax-ed category rankings.\n+We can find the index of the top classes using a simple argsort function:\n+\"\"\"\n+\n+top_classes = predictions[0].argsort(axis=-1)\n+\n+\"\"\"In order to decode the class mappings, we can construct a mapping from\n+category indices to ImageNet class names.\n+For convenience, I've stored the ImageNet class mapping in a GitHub gist.\n+Let's download and load it now.\n+\"\"\"\n+\n+classes = keras.utils.get_file(\n+    origin=\"https://gist.githubusercontent.com/LukeWood/62eebcd5c5c4a4d0e0b7845780f76d55/raw/fde63e5e4c09e2fa0a3436680f436bdcb8325aac/ImagenetClassnames.json\"\n+)\n+with open(classes, \"rb\") as f:\n+    classes = json.load(f)\n+\n+\"\"\"Now we can simply look up the class names via index:\"\"\"\n+\n+top_two = [classes[str(i)] for i in top_classes[-2:]]\n+print(\"Top two classes are:\", top_two)\n+\n+\"\"\"Great!  Both of these appear to be correct!\n+However, one of the classes is \"Velvet\".\n+We're trying to classify Cats VS Dogs.\n+We don't care about the velvet blanket!\n+\n+Ideally, we'd have a classifier that only performs computation to determine if\n+an image is a cat or a dog, and has all of its resources dedicated to this\n+task. This can be solved by fine tuning our own classifier.\n+\n+# Fine tuning a pretrained classifier\n+\n+When labeled images specific to our task are available, fine-tuning a custom\n+classifier can improve performance.\n+If we want to train a Cats vs Dogs Classifier, using explicitly labeled Cat vs\n+Dog data should perform better than the generic classifier!\n+For many tasks, no relevant pretrained model\n+will be available (e.g., categorizing images specific to your application).\n+\n+First, let's get started by loading some data:\n+\"\"\"\n+\n+BATCH_SIZE = 32\n+IMAGE_SIZE = (224, 224)\n+AUTOTUNE = tf.data.AUTOTUNE\n+tfds.disable_progress_bar()\n+\n+data, dataset_info = tfds.load(\n+    \"cats_vs_dogs\",\n+    with_info=True,\n+    as_supervised=True\n+)\n+train_steps_per_epoch = (\n+    dataset_info.splits[\"train\"].num_examples // BATCH_SIZE\n+)\n+train_dataset = data[\"train\"]\n+\n+num_classes = dataset_info.features[\"label\"].num_classes\n+\n+resizing = keras_cv.layers.Resizing(\n+    IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True\n+)\n+\n+\n+def preprocess_inputs(image, label):\n+    image = tf.cast(image, tf.float32)\n+    # Staticly resize images as we only iterate the dataset once.\n+    return resizing(image), tf.one_hot(label, num_classes)\n+\n+\n+# Shuffle the dataset to increase diversity of batches.\n+# 10*BATCH_SIZE follows the assumption that bigger machines can handle bigger\n+# shuffle buffers.\n+train_dataset = train_dataset.shuffle(\n+    10 * BATCH_SIZE, reshuffle_each_iteration=True\n+).map(preprocess_inputs, num_parallel_calls=AUTOTUNE)\n+train_dataset = train_dataset.batch(BATCH_SIZE)\n+\n+images = next(iter(train_dataset.take(1)))[0]\n+keras_cv.visualization.plot_image_gallery(images, value_range=(0, 255))\n+\n+\"\"\"Meow!\n+\n+Next let's construct our model.\n+The use of imagenet in the preset name indicates that the backbone was\n+pretrained on the ImageNet dataset.\n+Pretrained backbones extract more information from our labeled examples by\n+leveraging patterns extracted from potentially much larger datasets.\n+\n+Next lets put together our classifier:\n+\"\"\"\n+\n+model = keras_cv.models.ImageClassifier.from_preset(\n+    \"efficientnetv2_b0_imagenet\", num_classes=2\n+)\n+model.compile(\n+    loss=\"categorical_crossentropy\",\n+    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n+    metrics=[\"accuracy\"],\n+)\n+\n+\"\"\"Here our classifier is just a simple `keras.Sequential`.\n+All that is left to do is call `model.fit()`:\n+\"\"\"\n+\n+model.fit(train_dataset)\n+\n+\"\"\"Let's look at how our model performs after the fine tuning:\"\"\"\n+\n+predictions = model.predict(np.expand_dims(image, axis=0))\n+\n+classes = {0: \"cat\", 1: \"dog\"}\n+print(\"Top class is:\", classes[predictions[0].argmax()])\n+\n+\"\"\"Awesome - looks like the model correctly classified the image.\n+\n+# Train a Classifier from Scratch\n+\n+Now that we've gotten our hands dirty with classification, let's take on one\n+last task: training a classification model from scratch!\n+A standard benchmark for image classification is the ImageNet dataset, however\n+due to licensing constraints we will use the CalTech 101 image classification\n+dataset in this tutorial.\n+While we use the simpler CalTech 101 dataset in this guide, the same training\n+template may be used on ImageNet to achieve near state-of-the-art scores.\n+\n+Let's start out by tackling data loading:\n+\"\"\"\n+\n+NUM_CLASSES = 101\n+# Change epochs to 100~ to fully train.\n+EPOCHS = 1\n+\n+\n+def package_inputs(image, label):\n+    return {\"images\": image, \"labels\": tf.one_hot(label, NUM_CLASSES)}\n+\n+\n+train_ds, eval_ds = tfds.load(\n+    \"caltech101\", split=[\"train\", \"test\"], as_supervised=\"true\"\n+)\n+train_ds = train_ds.map(package_inputs, num_parallel_calls=tf.data.AUTOTUNE)\n+eval_ds = eval_ds.map(package_inputs, num_parallel_calls=tf.data.AUTOTUNE)\n+\n+train_ds = train_ds.shuffle(BATCH_SIZE * 16)\n+\n+\"\"\"The CalTech101 dataset has different sizes for every image, so we use the\n+`ragged_batch()` API to batch them together while maintaining each individual\n+image's shape information.\n+\"\"\"\n+\n+train_ds = train_ds.ragged_batch(BATCH_SIZE)\n+eval_ds = eval_ds.ragged_batch(BATCH_SIZE)\n+\n+batch = next(iter(train_ds.take(1)))\n+image_batch = batch[\"images\"]\n+label_batch = batch[\"labels\"]\n+\n+keras_cv.visualization.plot_image_gallery(\n+    image_batch.to_tensor(),\n+    rows=3,\n+    cols=3,\n+    value_range=(0, 255),\n+    show=True,\n+)\n+\n+\"\"\"## Data Augmentation\n+\n+In our previous finetuning exmaple, we performed a static resizing operation\n+and did not utilize any image augmentation.\n+This is because a single pass over the training set was sufficient to achieve\n+decent results.\n+When training to solve a more difficult task, you'll want to include data\n+augmentation in your data pipeline.\n+\n+Data augmentation is a technique to make your model robust to changes in input\n+data such as lighting, cropping, and orientation.\n+KerasCV includes some of the most useful augmentations in the\n+`keras_cv.layers` API.\n+Creating an optimal pipeline of augmentations is an art, but in this section\n+of the guide we'll offer some tips on best practices for classification.\n+\n+One caveat to be aware of with image data augmentation is that you must be\n+careful to not shift your augmented data distribution too far from the\n+original data distribution.\n+The goal is to prevent overfitting and increase generalization,\n+but samples that lie completely out of the data distribution simply add noise\n+to the training process.\n+\n+The first augmentation we'll use is `RandomFlip`.\n+This augmentation behaves more or less how you'd expect: it either flips the\n+image or not.\n+While this augmentation is useful in CalTech101 and ImageNet, it should be\n+noted that it should not be used on tasks where the data distribution is not\n+vertical mirror invariant.\n+An example of a dataset where this occurs is MNIST hand written digits.\n+Flipping a `6` over the\n+vertical axis will make the digit appear more like a `7` than a `6`, but the\n+label will still show a `6`.\n+\"\"\"\n+\n+random_flip = keras_cv.layers.RandomFlip()\n+augmenters = [random_flip]\n+\n+image_batch = random_flip(image_batch)\n+keras_cv.visualization.plot_image_gallery(\n+    image_batch.to_tensor(),\n+    rows=3,\n+    cols=3,\n+    value_range=(0, 255),\n+    show=True,\n+)\n+\n+\"\"\"Half of the images have been flipped!\n+\n+The next augmentation we'll use is `RandomCropAndResize`.\n+This operation selects a random subset of the image, then resizes it to the\n+provided target size.\n+By using this augmentation, we force our classifier to become spatially\n+invariant.\n+Additionally, this layer accepts an `aspect_ratio_factor` which can be used to\n+distort the aspect ratio of the image.\n+While this can improve model performance, it should be used with caution.\n+It is very easy for an aspect ratio distortion to shift a sample too far from\n+the original training set's data distribution.\n+Remember - the goal of data augmentation is to produce more training samples\n+that align with the data distribution of your training set!\n+\n+`RandomCropAndResize` also can handle `tf.RaggedTensor` inputs.  In the\n+CalTech101 image dataset images come in a wide variety of sizes.\n+As such they cannot easily be batched together into a dense training batch.\n+Luckily, `RandomCropAndResize` handles the Ragged -> Dense conversion process\n+for you!\n+\n+Let's add a `RandomCropAndResize` to our set of augmentations:\n+\"\"\"\n+\n+crop_and_resize = keras_cv.layers.RandomCropAndResize(\n+    target_size=IMAGE_SIZE,\n+    crop_area_factor=(0.8, 1.0),\n+    aspect_ratio_factor=(0.9, 1.1),\n+)\n+augmenters += [crop_and_resize]\n+\n+image_batch = crop_and_resize(image_batch)\n+keras_cv.visualization.plot_image_gallery(\n+    image_batch,\n+    rows=3,\n+    cols=3,\n+    value_range=(0, 255),\n+    show=True,\n+)\n+\n+\"\"\"Great!  We are now working with a batch of dense images.\n+Next up, lets include some spatial and color-based jitter to our training set.\n+This will allow us to produce a classifier that is robust to lighting\n+flickers, shadows, and more.\n+\n+There are limitless ways to augment an image by altering color and spatial\n+features, but perhaps the most battle tested technique is\n+[`RandAugment`](https://arxiv.org/abs/1909.13719).\n+`RandAugment` is actually a set of 10 different augmentations:\n+`AutoContrast`, `Equalize`, `Solarize`, `RandomColorJitter`, `RandomContrast`,\n+`RandomBrightness`, `ShearX`, `ShearY`, `TranslateX` and `TranslateY`.\n+At inference time, `num_augmentations` augmenters are sampled for each image,\n+and random magnitude factors are sampled for each.\n+These augmentations are then applied sequentially.\n+\n+KerasCV makes tuning these parameters easy using the `augmentations_per_image`\n+and `magnitude` parameters!\n+Let's take it for a spin:\n+\"\"\"\n+\n+rand_augment = keras_cv.layers.RandAugment(\n+    augmentations_per_image=3,\n+    magnitude=0.3,\n+    value_range=(0, 255),\n+)\n+augmenters += [rand_augment]\n+\n+image_batch = rand_augment(image_batch)\n+keras_cv.visualization.plot_image_gallery(\n+    image_batch,\n+    rows=3,\n+    cols=3,\n+    value_range=(0, 255),\n+    show=True,\n+)\n+\n+\"\"\"Looks great; but we're not done yet!\n+What if an image is missing one critical feature of a class?  For example,\n+what if a leaf is blocking the view of a cat's ear, but our classifier\n+learned to classify cats simply by observing their ears?\n+\n+One easy approach to tackling this is to use `RandomCutout`, which randomly\n+strips out a sub-section of the image:\n+\"\"\"\n+\n+random_cutout = keras_cv.layers.RandomCutout(\n+    width_factor=0.4, height_factor=0.4\n+)\n+keras_cv.visualization.plot_image_gallery(\n+    random_cutout(image_batch),\n+    rows=3,\n+    cols=3,\n+    value_range=(0, 255),\n+    show=True,\n+)\n+\n+\"\"\"While this tackles the problem reasonably well, it can cause the classifier\n+to develop responses to borders between features and black pixel areas caused\n+by the cutout.\n+\n+[`CutMix`](https://arxiv.org/abs/1905.04899) solves the same issue by using\n+a more complex (and more effective) technique.\n+Instead of replacing the cut-out areas with black pixels, `CutMix` replaces\n+these regions with regions of other images sampled from within your training\n+set!\n+Following this replacement, the image's classification label is updated to be\n+a blend of the original and mixed image's class label.\n+\n+What does this look like in practice?  Let's check it out:\n+\"\"\"\n+\n+cut_mix = keras_cv.layers.CutMix()\n+# CutMix needs to modify both images and labels\n+inputs = {\"images\": image_batch, \"labels\": label_batch}\n+\n+keras_cv.visualization.plot_image_gallery(\n+    cut_mix(inputs)[\"images\"],\n+    rows=3,\n+    cols=3,\n+    value_range=(0, 255),\n+    show=True,\n+)\n+\n+\"\"\"Let's hold off from adding it to our augmenter for a minute - more on that\n+soon!\n+\n+Next, let's look into `MixUp()`.\n+Unfortunately, while `MixUp()` has been empirically shown to *substantially*\n+improve both the robustness and the generalization of the trained model,\n+it is not well-understood why such improvement occurs... but\n+a little alchemy never hurt anyone!\n+\n+`MixUp()` works by sampling two images from a batch, then proceeding to\n+literally blend together their pixel intensities as well as their\n+classification labels.\n+\n+Let's see it in action:\n+\"\"\"\n+\n+mix_up = keras_cv.layers.MixUp()\n+# MixUp needs to modify both images and labels\n+inputs = {\"images\": image_batch, \"labels\": label_batch}\n+\n+keras_cv.visualization.plot_image_gallery(\n+    mix_up(inputs)[\"images\"],\n+    rows=3,\n+    cols=3,\n+    value_range=(0, 255),\n+    show=True,\n+)\n+\n+\"\"\"If you look closely, you'll see that the images have been blended together.\n+\n+Instead of applying `CutMix()` and `MixUp()` to every image, we instead pick\n+one or the other to apply to each batch.\n+This can be expressed using `keras_cv.layers.RandomChoice()`\n+\"\"\"\n+\n+cut_mix_or_mix_up = keras_cv.layers.RandomChoice(\n+    [cut_mix, mix_up], batchwise=True\n+)\n+augmenters += [cut_mix_or_mix_up]\n+\n+\"\"\"Now let's apply our final augmenter to the training data:\"\"\"\n+\n+augmenter = tf.keras.Sequential(augmenters)\n+train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\n+\n+image_batch = next(iter(train_ds.take(1)))[\"images\"]\n+keras_cv.visualization.plot_image_gallery(\n+    image_batch,\n+    rows=3,\n+    cols=3,\n+    value_range=(0, 255),\n+    show=True,\n+)\n+\n+\"\"\"We also need to resize our evaluation set to get dense batches of the image\n+size expected by our model. We use the deterministic\n+`keras_cv.layers.Resizing` in this case to avoid adding noise to our\n+evaluation metric.\n+\"\"\"\n+\n+inference_resizing = keras_cv.layers.Resizing(\n+    IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True\n+)\n+eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE)\n+\n+inference_resizing = keras_cv.layers.Resizing(\n+    IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True\n+)\n+eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE)\n+\n+image_batch = next(iter(eval_ds.take(1)))[\"images\"]\n+keras_cv.visualization.plot_image_gallery(\n+    image_batch,\n+    rows=3,\n+    cols=3,\n+    value_range=(0, 255),\n+    show=True,\n+)\n+\n+\"\"\"Finally, lets unpackage our datasets and prepare to pass them to\n+`model.fit()`, which accepts a tuple of `(images, labels)`.\n+\"\"\"\n+\n+def unpackage_dict(inputs):\n+    return inputs[\"images\"], inputs[\"labels\"]\n+\n+\n+train_ds = train_ds.map(unpackage_dict, num_parallel_calls=tf.data.AUTOTUNE)\n+eval_ds = eval_ds.map(unpackage_dict, num_parallel_calls=tf.data.AUTOTUNE)\n+\n+\"\"\"Data augmentation is by far the hardest piece of training a modern\n+classifier.\n+Congratulations on making it this far!\n+\n+## Optimizer Tuning\n+\n+To achieve optimal performance, we need to use a learning rate schedule\n+instead of a single learning rate. While we won't go into detail on the\n+Cosine decay with warmup schedule used here, [you can read more about it\n+here](https://scorrea92.medium.com/cosine-learning-rate-decay-e8b50aa455b).\n+\"\"\"\n+\n+def lr_warmup_cosine_decay(\n+    global_step,\n+    warmup_steps,\n+    hold=0,\n+    total_steps=0,\n+    start_lr=0.0,\n+    target_lr=1e-2,\n+):\n+    # Cosine decay\n+    learning_rate = (\n+        0.5\n+        * target_lr\n+        * (\n+            1\n+            + ops.cos(\n+                math.pi\n+                * ops.convert_to_tensor(\n+                    global_step - warmup_steps - hold, dtype=\"float32\"\n+                )\n+                / ops.convert_to_tensor(\n+                    total_steps - warmup_steps - hold, dtype=\"float32\"\n+                )\n+            )\n+        )\n+    )\n+\n+    warmup_lr = (target_lr * (global_step / warmup_steps))\n+\n+    if hold > 0:\n+        learning_rate = ops.where(\n+            global_step > warmup_steps + hold, learning_rate, target_lr\n+        )\n+\n+    learning_rate = ops.where(\n+        global_step < warmup_steps, warmup_lr, learning_rate\n+    )\n+    return learning_rate\n+\n+\n+class WarmUpCosineDecay(\n+    schedules.learning_rate_schedule.LearningRateSchedule\n+):\n+    def __init__(\n+        self, warmup_steps, total_steps, hold, start_lr=0.0, target_lr=1e-2\n+    ):\n+        super().__init__()\n+        self.start_lr = start_lr\n+        self.target_lr = target_lr\n+        self.warmup_steps = warmup_steps\n+        self.total_steps = total_steps\n+        self.hold = hold\n+\n+    def __call__(self, step):\n+        lr = lr_warmup_cosine_decay(\n+            global_step=step,\n+            total_steps=self.total_steps,\n+            warmup_steps=self.warmup_steps,\n+            start_lr=self.start_lr,\n+            target_lr=self.target_lr,\n+            hold=self.hold,\n+        )\n+\n+        return ops.where(step > self.total_steps, 0.0, lr)\n+\n+\"\"\"![WarmUpCosineDecay schedule](https://i.imgur.com/YCr5pII.png)\n+\n+The schedule looks a as we expect.\n+\n+Next let's construct this optimizer:\n+\"\"\"\n+\n+total_images = 9000\n+total_steps = (total_images // BATCH_SIZE) * EPOCHS\n+warmup_steps = int(0.1 * total_steps)\n+hold_steps = int(0.45 * total_steps)\n+schedule = WarmUpCosineDecay(\n+    start_lr=0.05,\n+    target_lr=1e-2,\n+    warmup_steps=warmup_steps,\n+    total_steps=total_steps,\n+    hold=hold_steps,\n+)\n+optimizer = optimizers.SGD(\n+    weight_decay=5e-4,\n+    learning_rate=schedule,\n+    momentum=0.9,\n+)\n+\n+\"\"\"At long last, we can now build our model and call `fit()`!\n+`keras_cv.models.EfficientNetV2B0Backbone()` is a convenience alias for\n+`keras_cv.models.EfficientNetV2Backbone.from_preset('efficientnetv2_b0')`.\n+Note that this preset does not come with any pretrained weights.\n+\"\"\"\n+\n+backbone = keras_cv.models.ResNet18V2Backbone()\n+model = keras.Sequential(\n+    [\n+        backbone,\n+        keras.layers.GlobalMaxPooling2D(),\n+        keras.layers.Dropout(rate=0.5),\n+        keras.layers.Dense(101, activation=\"softmax\"),\n+    ]\n+)\n+\n+\"\"\"Since the labels produced by MixUp() and CutMix() are somewhat artificial,\n+we employ label smoothing to prevent the model from overfitting to artifacts\n+of this augmentation process.\n+\"\"\"\n+\n+loss = losses.CategoricalCrossentropy(label_smoothing=0.1)\n+\n+\"\"\"Let's compile our model:\"\"\"\n+\n+model.compile(\n+    loss=loss,\n+    optimizer=optimizer,\n+    metrics=[\n+        metrics.CategoricalAccuracy(),\n+        metrics.TopKCategoricalAccuracy(k=5),\n+    ],\n+)\n+\n+\"\"\"and finally call fit().\"\"\"\n+\n+model.fit(\n+    train_ds,\n+    epochs=EPOCHS,\n+    validation_data=eval_ds,\n+)\n+\n+\"\"\"Congratulations!  You now know how to train a powerful image classifier\n+from scratch in KerasCV.\n+Depending on the availability of labeled data for your application, training\n+from scratch may or may not be more powerful than using transfer learning in\n+addition to the data augmentations discussed above. For smaller datasets,\n+pretrained models generally produce high accuracy and faster convergence.\n+\n+## Conclusions\n+\n+While image classification is perhaps the simplest problem in computer vision,\n+the modern landscape has numerous complex components.\n+Luckily, KerasCV offers robust, production-grade APIs to make assembling most\n+of these components possible in one line of code.\n+Through the use of KerasCV's `ImageClassifier` API, pretrained weights, and\n+KerasCV data augmentations you can assemble everything you need to train a\n+powerful classifier in a few hundred lines of code!\n+\n+As a follow up exercise, give the following a try:\n+\n+- Fine tune a KerasCV classifier on your own dataset\n+- Learn more about [KerasCV's data augmentations](https://keras.io/guides/keras_cv/cut_mix_mix_up_and_rand_augment/)\n+- Check out how we train our models on [ImageNet](https://github.com/keras-team/keras-cv/blob/master/examples/training/classification/imagenet/basic_training.py)\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b7162e19a5382aa3168d51b271645bcb3b5a9987", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 443 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 11 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 443 | Churn Cumulative: 443 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,443 @@\n+\"\"\"\n+Title: Self-supervised contrastive learning with SimSiam\n+Author: [Sayak Paul](https://twitter.com/RisingSayak)\n+Date created: 2021/03/19\n+Last modified: 2021/03/20\n+Description: Implementation of a self-supervised learning method for computer vision.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+Self-supervised learning (SSL) is an interesting branch of study in the field of\n+representation learning. SSL systems try to formulate a supervised signal from a corpus\n+of unlabeled data points.  An example is we train a deep neural network to predict the\n+next word from a given set of words. In literature, these tasks are known as *pretext\n+tasks* or *auxiliary tasks*. If we [train such a network](https://arxiv.org/abs/1801.06146) on a huge dataset (such as\n+the [Wikipedia text corpus](https://www.corpusdata.org/wikipedia.asp)) it learns very effective\n+representations that transfer well to downstream tasks. Language models like\n+[BERT](https://arxiv.org/abs/1810.04805), [GPT-3](https://arxiv.org/abs/2005.14165),\n+[ELMo](https://allennlp.org/elmo) all benefit from this.\n+\n+Much like the language models we can train computer vision models using similar\n+approaches. To make things work in computer vision, we need to formulate the learning\n+tasks such that the underlying model (a deep neural network) is able to make sense of the\n+semantic information present in vision data. One such task is to a model to _contrast_\n+between two different versions of the same image. The hope is that in this way the model\n+will have learn representations where the similar images are grouped as together possible\n+while the dissimilar images are further away.\n+\n+In this example, we will be implementing one such system called **SimSiam** proposed in\n+[Exploring Simple Siamese Representation Learning](https://arxiv.org/abs/2011.10566). It\n+is implemented as the following:\n+\n+1. We create two different versions of the same dataset with a stochastic data\n+augmentation pipeline. Note that the random initialization seed needs to be the same\n+during create these versions.\n+2. We take a ResNet without any classification head (**backbone**) and we add a shallow\n+fully-connected network (**projection head**) on top of it. Collectively, this is known\n+as the **encoder**.\n+3. We pass the output of the encoder through a **predictor** which is again a shallow\n+fully-connected network having an\n+[AutoEncoder](https://en.wikipedia.org/wiki/Autoencoder) like structure.\n+4. We then train our encoder to maximize the cosine similarity between the two different\n+versions of our dataset.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import os\n+os.environ['KERAS_BACKEND'] = 'tensorflow'\n+\n+from keras_core import layers\n+from keras_core import regularizers\n+import keras_core as keras\n+import tensorflow as tf\n+\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+\"\"\"\n+## Define hyperparameters\n+\"\"\"\n+\n+AUTO = tf.data.AUTOTUNE\n+BATCH_SIZE = 128\n+EPOCHS = 5\n+CROP_TO = 32\n+SEED = 26\n+\n+PROJECT_DIM = 2048\n+LATENT_DIM = 512\n+WEIGHT_DECAY = 0.0005\n+\n+\"\"\"\n+## Load the CIFAR-10 dataset\n+\"\"\"\n+\n+(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n+print(f\"Total training examples: {len(x_train)}\")\n+print(f\"Total test examples: {len(x_test)}\")\n+\n+\"\"\"\n+## Defining our data augmentation pipeline\n+\n+As studied in [SimCLR](https://arxiv.org/abs/2002.05709) having the right data\n+augmentation pipeline is critical for SSL systems to work effectively in computer vision.\n+Two particular augmentation transforms that seem to matter the most are: 1.) Random\n+resized crops and 2.) Color distortions. Most of the other SSL systems for computer\n+vision (such as [BYOL](https://arxiv.org/abs/2006.07733),\n+[MoCoV2](https://arxiv.org/abs/2003.04297), [SwAV](https://arxiv.org/abs/2006.09882),\n+etc.) include these in their training pipelines.\n+\"\"\"\n+\n+\n+def flip_random_crop(image):\n+    # With random crops we also apply horizontal flipping.\n+    image = tf.image.random_flip_left_right(image)\n+    image = tf.image.random_crop(image, (CROP_TO, CROP_TO, 3))\n+    return image\n+\n+\n+def color_jitter(x, strength=[0.4, 0.4, 0.4, 0.1]):\n+    x = tf.image.random_brightness(x, max_delta=0.8 * strength[0])\n+    x = tf.image.random_contrast(\n+        x, lower=1 - 0.8 * strength[1], upper=1 + 0.8 * strength[1]\n+    )\n+    x = tf.image.random_saturation(\n+        x, lower=1 - 0.8 * strength[2], upper=1 + 0.8 * strength[2]\n+    )\n+    x = tf.image.random_hue(x, max_delta=0.2 * strength[3])\n+    # Affine transformations can disturb the natural range of\n+    # RGB images, hence this is needed.\n+    x = tf.clip_by_value(x, 0, 255)\n+    return x\n+\n+\n+def color_drop(x):\n+    x = tf.image.rgb_to_grayscale(x)\n+    x = tf.tile(x, [1, 1, 3])\n+    return x\n+\n+\n+def random_apply(func, x, p):\n+    if tf.random.uniform([], minval=0, maxval=1) < p:\n+        return func(x)\n+    else:\n+        return x\n+\n+\n+def custom_augment(image):\n+    # As discussed in the SimCLR paper, the series of augmentation\n+    # transformations (except for random crops) need to be applied\n+    # randomly to impose translational invariance.\n+    image = flip_random_crop(image)\n+    image = random_apply(color_jitter, image, p=0.8)\n+    image = random_apply(color_drop, image, p=0.2)\n+    return image\n+\n+\n+\"\"\"\n+It should be noted that an augmentation pipeline is generally dependent on various\n+properties of the dataset we are dealing with. For example, if images in the dataset are\n+heavily object-centric then taking random crops with a very high probability may hurt the\n+training performance.\n+\n+Let's now apply our augmentation pipeline to our dataset and visualize a few outputs.\n+\"\"\"\n+\n+\"\"\"\n+## Convert the data into TensorFlow `Dataset` objects\n+\n+Here we create two different versions of our dataset *without* any ground-truth labels.\n+\"\"\"\n+\n+ssl_ds_one = tf.data.Dataset.from_tensor_slices(x_train)\n+ssl_ds_one = (\n+    ssl_ds_one.shuffle(1024, seed=SEED)\n+    .map(custom_augment, num_parallel_calls=AUTO)\n+    .batch(BATCH_SIZE)\n+    .prefetch(AUTO)\n+)\n+\n+ssl_ds_two = tf.data.Dataset.from_tensor_slices(x_train)\n+ssl_ds_two = (\n+    ssl_ds_two.shuffle(1024, seed=SEED)\n+    .map(custom_augment, num_parallel_calls=AUTO)\n+    .batch(BATCH_SIZE)\n+    .prefetch(AUTO)\n+)\n+\n+# We then zip both of these datasets.\n+ssl_ds = tf.data.Dataset.zip((ssl_ds_one, ssl_ds_two))\n+\n+# Visualize a few augmented images.\n+sample_images_one = next(iter(ssl_ds_one))\n+plt.figure(figsize=(10, 10))\n+for n in range(25):\n+    ax = plt.subplot(5, 5, n + 1)\n+    plt.imshow(sample_images_one[n].numpy().astype(\"int\"))\n+    plt.axis(\"off\")\n+plt.show()\n+\n+# Ensure that the different versions of the dataset actually contain\n+# identical images.\n+sample_images_two = next(iter(ssl_ds_two))\n+plt.figure(figsize=(10, 10))\n+for n in range(25):\n+    ax = plt.subplot(5, 5, n + 1)\n+    plt.imshow(sample_images_two[n].numpy().astype(\"int\"))\n+    plt.axis(\"off\")\n+plt.show()\n+\n+\"\"\"\n+Notice that the images in `samples_images_one` and `sample_images_two` are essentially\n+the same but are augmented differently.\n+\"\"\"\n+\n+\"\"\"\n+## Defining the encoder and the predictor\n+\n+We use an implementation of ResNet20 that is specifically configured for the CIFAR10\n+dataset. The code is taken from the\n+[keras-idiomatic-programmer](https://github.com/GoogleCloudPlatform/keras-idiomatic-programmer/blob/master/zoo/resnet/resnet_cifar10_v2.py) repository. The hyperparameters of\n+these architectures have been referred from Section 3 and Appendix A of [the original\n+paper](https://arxiv.org/abs/2011.10566).\n+\"\"\"\n+\n+\"\"\"shell\n+wget -q https://shorturl.at/QS369 -O resnet_cifar10_v2.py\n+\"\"\"\n+\n+import resnet_cifar10_v2\n+\n+N = 2\n+DEPTH = N * 9 + 2\n+NUM_BLOCKS = ((DEPTH - 2) // 9) - 1\n+\n+\n+def get_encoder():\n+    # Input and backbone.\n+    inputs = layers.Input((CROP_TO, CROP_TO, 3))\n+    x = layers.Rescaling(scale=1.0 / 127.5, offset=-1)(inputs)\n+    x = resnet_cifar10_v2.stem(x)\n+    x = resnet_cifar10_v2.learner(x, NUM_BLOCKS)\n+    x = layers.GlobalAveragePooling2D(name=\"backbone_pool\")(x)\n+\n+    # Projection head.\n+    x = layers.Dense(\n+        PROJECT_DIM,\n+        use_bias=False,\n+        kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n+    )(x)\n+    x = layers.BatchNormalization()(x)\n+    x = layers.ReLU()(x)\n+    x = layers.Dense(\n+        PROJECT_DIM,\n+        use_bias=False,\n+        kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n+    )(x)\n+    outputs = layers.BatchNormalization()(x)\n+    return keras.Model(inputs, outputs, name=\"encoder\")\n+\n+\n+def get_predictor():\n+    model = keras.Sequential(\n+        [\n+            # Note the AutoEncoder-like structure.\n+            layers.Input((PROJECT_DIM,)),\n+            layers.Dense(\n+                LATENT_DIM,\n+                use_bias=False,\n+                kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n+            ),\n+            layers.ReLU(),\n+            layers.BatchNormalization(),\n+            layers.Dense(PROJECT_DIM),\n+        ],\n+        name=\"predictor\",\n+    )\n+    return model\n+\n+\n+\"\"\"\n+## Defining the (pre-)training loop\n+\n+One of the main reasons behind training networks with these kinds of approaches is to\n+utilize the learned representations for downstream tasks like classification. This is why\n+this particular training phase is also referred to as _pre-training_.\n+\n+We start by defining the loss function.\n+\"\"\"\n+\n+\n+def compute_loss(p, z):\n+    # The authors of SimSiam emphasize the impact of\n+    # the `stop_gradient` operator in the paper as it\n+    # has an important role in the overall optimization.\n+    z = tf.stop_gradient(z)\n+    p = tf.math.l2_normalize(p, axis=1)\n+    z = tf.math.l2_normalize(z, axis=1)\n+    # Negative cosine similarity (minimizing this is\n+    # equivalent to maximizing the similarity).\n+    return -tf.reduce_mean(tf.reduce_sum((p * z), axis=1))\n+\n+\n+\"\"\"\n+We then define our training loop by overriding the `train_step()` function of the\n+`keras.Model` class.\n+\"\"\"\n+\n+\n+class SimSiam(keras.Model):\n+    def __init__(self, encoder, predictor):\n+        super().__init__()\n+        self.encoder = encoder\n+        self.predictor = predictor\n+        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n+\n+    @property\n+    def metrics(self):\n+        return [self.loss_tracker]\n+\n+    def train_step(self, data):\n+        # Unpack the data.\n+        ds_one, ds_two = data\n+\n+        # Forward pass through the encoder and predictor.\n+        with tf.GradientTape() as tape:\n+            z1, z2 = self.encoder(ds_one), self.encoder(ds_two)\n+            p1, p2 = self.predictor(z1), self.predictor(z2)\n+            # Note that here we are enforcing the network to match\n+            # the representations of two differently augmented batches\n+            # of data.\n+            loss = compute_loss(p1, z2) / 2 + compute_loss(p2, z1) / 2\n+\n+        # Compute gradients and update the parameters.\n+        learnable_params = (\n+            self.encoder.trainable_variables\n+            + self.predictor.trainable_variables\n+        )\n+        gradients = tape.gradient(loss, learnable_params)\n+        self.optimizer.apply_gradients(zip(gradients, learnable_params))\n+\n+        # Monitor loss.\n+        self.loss_tracker.update_state(loss)\n+        return {\"loss\": self.loss_tracker.result()}\n+\n+\n+\"\"\"\n+## Pre-training our networks\n+\n+In the interest of this example, we will train the model for only 5 epochs. In reality,\n+this should at least be 100 epochs.\n+\"\"\"\n+\n+# Create a cosine decay learning scheduler.\n+num_training_samples = len(x_train)\n+steps = EPOCHS * (num_training_samples // BATCH_SIZE)\n+lr_decayed_fn = keras.optimizers.schedules.CosineDecay(\n+    initial_learning_rate=0.03, decay_steps=steps\n+)\n+\n+# Create an early stopping callback.\n+early_stopping = keras.callbacks.EarlyStopping(\n+    monitor=\"loss\", patience=5, restore_best_weights=True\n+)\n+\n+# Compile model and start training.\n+simsiam = SimSiam(get_encoder(), get_predictor())\n+simsiam.compile(optimizer=keras.optimizers.SGD(lr_decayed_fn, momentum=0.6))\n+history = simsiam.fit(ssl_ds, epochs=EPOCHS, callbacks=[early_stopping])\n+\n+# Visualize the training progress of the model.\n+plt.plot(history.history[\"loss\"])\n+plt.grid()\n+plt.title(\"Negative Cosine Similairty\")\n+plt.show()\n+\n+\"\"\"\n+If your solution gets very close to -1 (minimum value of our loss) very quickly with a\n+different dataset and a different backbone architecture that is likely because of\n+*representation collapse*. It is a phenomenon where the encoder yields similar output for\n+all the images. In that case additional hyperparameter tuning is required especially in\n+the following areas:\n+\n+* Strength of the color distortions and their probabilities.\n+* Learning rate and its schedule.\n+* Architecture of both the backbone and their projection head.\n+\n+\"\"\"\n+\n+\"\"\"\n+## Evaluating our SSL method\n+\n+The most popularly used method to evaluate a SSL method in computer vision (or any other\n+pre-training method as such) is to learn a linear classifier on the frozen features of\n+the trained backbone model (in this case it is ResNet20) and evaluate the classifier on\n+unseen images. Other methods include\n+[fine-tuning](https://keras.io/guides/transfer_learning/) on the source dataset or even a\n+target dataset with 5% or 10% labels present. Practically, we can use the backbone model\n+for any downstream task such as semantic segmentation, object detection, and so on where\n+the backbone models are usually pre-trained with *pure supervised learning*.\n+\"\"\"\n+\n+# We first create labeled `Dataset` objects.\n+train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n+\n+# Then we shuffle, batch, and prefetch this dataset for performance. We\n+# also apply random resized crops as an augmentation but only to the\n+# training set.\n+train_ds = (\n+    train_ds.shuffle(1024)\n+    .map(lambda x, y: (flip_random_crop(x), y), num_parallel_calls=AUTO)\n+    .batch(BATCH_SIZE)\n+    .prefetch(AUTO)\n+)\n+test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTO)\n+\n+# Extract the backbone ResNet20.\n+backbone = keras.Model(\n+    simsiam.encoder.input, simsiam.encoder.get_layer(\"backbone_pool\").output\n+)\n+\n+# We then create our linear classifier and train it.\n+backbone.trainable = False\n+inputs = layers.Input((CROP_TO, CROP_TO, 3))\n+x = backbone(inputs, training=False)\n+outputs = layers.Dense(10, activation=\"softmax\")(x)\n+linear_model = keras.Model(inputs, outputs, name=\"linear_model\")\n+\n+# Compile model and start training.\n+linear_model.compile(\n+    loss=\"sparse_categorical_crossentropy\",\n+    metrics=[\"accuracy\"],\n+    optimizer=keras.optimizers.SGD(lr_decayed_fn, momentum=0.9),\n+)\n+history = linear_model.fit(\n+    train_ds, validation_data=test_ds, epochs=EPOCHS, callbacks=[early_stopping]\n+)\n+_, test_acc = linear_model.evaluate(test_ds)\n+print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))\n+\n+\"\"\"\n+\n+## Notes\n+* More data and longer pre-training schedule benefit SSL in general.\n+* SSL is particularly very helpful when you do not have access to very limited *labeled*\n+training data but you can manage to build a large corpus of unlabeled data. Recently,\n+using an SSL method called [SwAV](https://arxiv.org/abs/2006.09882), a group of\n+researchers at Facebook trained a [RegNet](https://arxiv.org/abs/2006.09882) on 2 Billion\n+images. They were able to achieve downstream performance very close to those achieved by\n+pure supervised pre-training. For some downstream tasks, their method even outperformed\n+the supervised counterparts. You can check out [their\n+paper](https://arxiv.org/pdf/2103.01988.pdf) to know the details.\n+* If you are interested to understand why contrastive SSL helps networks learn meaningful\n+representations, you can check out the following resources:\n+   * [Self-supervised learning: The dark matter of\n+intelligence](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)\n+   * [Understanding self-supervised learning using controlled datasets with known\n+structure](https://sslneuips20.github.io/files/CameraReadys%203-77/64/CameraReady/Understanding_self_supervised_learning.pdf)\n+\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#397ad5771574dd96e8aa47c5d52735ad860ecd0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 66/65 | Churn Δ: 33 | Churn Cumulative: 2797 | Contributors (this commit): 17 | Commits (past 90d): 2 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -407,6 +407,7 @@ class MultiHeadAttention(Layer):\n             activity_regularizer=self._activity_regularizer,\n             kernel_constraint=self._kernel_constraint,\n             bias_constraint=self._bias_constraint,\n+            dtype=self._dtype_policy,\n         )\n         # Create new clone of kernel/bias initializer, so that we don't reuse\n         # the initializer instance, which could lead to same init value since\n@@ -474,8 +475,12 @@ class MultiHeadAttention(Layer):\n                 attn_scores_rank - len(self._attention_axes), attn_scores_rank\n             )\n         )\n-        self._softmax = activation.Softmax(axis=norm_axes)\n-        self._dropout_layer = regularization.Dropout(rate=self._dropout)\n+        self._softmax = activation.Softmax(\n+            axis=norm_axes, dtype=self._dtype_policy\n+        )\n+        self._dropout_layer = regularization.Dropout(\n+            rate=self._dropout, dtype=self._dtype_policy\n+        )\n \n     def _masked_softmax(self, attention_scores, attention_mask=None):\n         # Normalize the attention scores to probabilities.\n@@ -525,17 +530,14 @@ class MultiHeadAttention(Layer):\n         # Take the dot product between \"query\" and \"key\" to get the raw\n         # attention scores.\n         attention_scores = tf.einsum(self._dot_product_equation, key, query)\n-\n         attention_scores = self._masked_softmax(\n             attention_scores, attention_mask\n         )\n-\n         # This is actually dropping out entire tokens to attend to, which might\n         # seem a bit unusual, but is taken from the original Transformer paper.\n         attention_scores_dropout = self._dropout_layer(\n             attention_scores, training=training\n         )\n-\n         # `context_layer` = [B, T, N, H]\n         attention_output = tf.einsum(\n             self._combine_equation, attention_scores_dropout, value\n@@ -702,7 +704,6 @@ class MultiHeadAttention(Layer):\n         )\n \n     def compute_output_shape(self, query_shape, value_shape, key_shape=None):\n-\n         if key_shape is None:\n             key_shape = value_shape\n \n\n@@ -163,6 +163,26 @@ class MultiHeadAttentionTest(test_combinations.TestCase):\n             keras.backend.eval(test_layer._output_dense.kernel),\n         )\n \n+    @parameterized.named_parameters(\n+        (\"bfloat16\", tf.bfloat16),\n+        (\"float16\", tf.float16),\n+        (\"float32\", tf.float32),\n+        (\"float64\", tf.float64),\n+    )\n+    def test_sublayer_dtypes(self, dtype):\n+        test_layer = keras.layers.MultiHeadAttention(\n+            num_heads=12, key_dim=64, dtype=dtype\n+        )\n+\n+        query = keras.Input(shape=(40, 80), dtype=dtype)\n+        # Build the layer\n+        test_layer(query=query, value=query)\n+\n+        self.assertEqual(test_layer._query_dense.dtype, dtype)\n+        self.assertEqual(test_layer._key_dense.dtype, dtype)\n+        self.assertEqual(test_layer._value_dense.dtype, dtype)\n+        self.assertEqual(test_layer._output_dense.dtype, dtype)\n+\n     def test_masked_attention_with_scores(self):\n         \"\"\"Test with a mask tensor.\"\"\"\n         test_layer = keras.layers.MultiHeadAttention(num_heads=2, key_dim=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6a08b332fa0cf8dfefc4ec405c3ba430483750db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 18 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 39 | Churn Cumulative: 731 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -43,12 +43,13 @@ import json\n import math\n import keras_cv\n import keras_core as keras\n-from keras_core import operations as ops\n+from keras_core import ops\n from keras_core import losses\n from keras_core import optimizers\n from keras_core.optimizers import schedules\n from keras_core import metrics\n import tensorflow as tf\n+from tensorflow import data as tf_data\n import tensorflow_datasets as tfds\n import numpy as np\n \n@@ -87,10 +88,10 @@ Now that our classifier is built, let's apply it to this cute cat picture!\n \"\"\"\n \n filepath = keras.utils.get_file(origin=\"https://i.imgur.com/9i63gLN.jpg\")\n-image = keras.utils.image_utils.load_img(filepath)\n+image = keras.utils.load_img(filepath)\n image = np.array(image)\n keras_cv.visualization.plot_image_gallery(\n-    [image], rows=1, cols=1, value_range=(0, 255), show=True, scale=4\n+    image[None, ...], rows=1, cols=1, value_range=(0, 255), show=True, scale=4\n )\n \n \"\"\"Next, let's get some predictions from our classifier:\"\"\"\n@@ -143,7 +144,7 @@ First, let's get started by loading some data:\n \n BATCH_SIZE = 32\n IMAGE_SIZE = (224, 224)\n-AUTOTUNE = tf.data.AUTOTUNE\n+AUTOTUNE = tf_data.AUTOTUNE\n tfds.disable_progress_bar()\n \n data, dataset_info = tfds.load(\n@@ -161,12 +162,12 @@ num_classes = dataset_info.features[\"label\"].num_classes\n resizing = keras_cv.layers.Resizing(\n     IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True\n )\n+encoder = keras.layers.CategoryEncoding(num_classes, \"one_hot\", dtype=\"int32\")\n \n \n def preprocess_inputs(image, label):\n-    image = tf.cast(image, tf.float32)\n     # Staticly resize images as we only iterate the dataset once.\n-    return resizing(image), tf.one_hot(label, num_classes)\n+    return resizing(image), encoder(label)\n \n \n # Shuffle the dataset to increase diversity of batches.\n@@ -232,16 +233,18 @@ NUM_CLASSES = 101\n # Change epochs to 100~ to fully train.\n EPOCHS = 1\n \n+encoder = keras.layers.CategoryEncoding(NUM_CLASSES, \"one_hot\", dtype=\"int32\")\n+\n \n def package_inputs(image, label):\n-    return {\"images\": image, \"labels\": tf.one_hot(label, NUM_CLASSES)}\n+    return {\"images\": image, \"labels\": encoder(label)}\n \n \n train_ds, eval_ds = tfds.load(\n     \"caltech101\", split=[\"train\", \"test\"], as_supervised=\"true\"\n )\n-train_ds = train_ds.map(package_inputs, num_parallel_calls=tf.data.AUTOTUNE)\n-eval_ds = eval_ds.map(package_inputs, num_parallel_calls=tf.data.AUTOTUNE)\n+train_ds = train_ds.map(package_inputs, num_parallel_calls=tf_data.AUTOTUNE)\n+eval_ds = eval_ds.map(package_inputs, num_parallel_calls=tf_data.AUTOTUNE)\n \n train_ds = train_ds.shuffle(BATCH_SIZE * 16)\n \n@@ -425,7 +428,7 @@ What does this look like in practice?  Let's check it out:\n \n cut_mix = keras_cv.layers.CutMix()\n # CutMix needs to modify both images and labels\n-inputs = {\"images\": image_batch, \"labels\": label_batch}\n+inputs = {\"images\": image_batch, \"labels\": tf.cast(label_batch, \"float32\")}\n \n keras_cv.visualization.plot_image_gallery(\n     cut_mix(inputs)[\"images\"],\n@@ -453,7 +456,7 @@ Let's see it in action:\n \n mix_up = keras_cv.layers.MixUp()\n # MixUp needs to modify both images and labels\n-inputs = {\"images\": image_batch, \"labels\": label_batch}\n+inputs = {\"images\": image_batch, \"labels\": tf.cast(label_batch, \"float32\")}\n \n keras_cv.visualization.plot_image_gallery(\n     mix_up(inputs)[\"images\"],\n@@ -477,8 +480,8 @@ augmenters += [cut_mix_or_mix_up]\n \n \"\"\"Now let's apply our final augmenter to the training data:\"\"\"\n \n-augmenter = tf.keras.Sequential(augmenters)\n-train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\n+augmenter = keras_cv.layers.Augmenter(augmenters)\n+train_ds = train_ds.map(augmenter, num_parallel_calls=tf_data.AUTOTUNE)\n \n image_batch = next(iter(train_ds.take(1)))[\"images\"]\n keras_cv.visualization.plot_image_gallery(\n@@ -498,12 +501,12 @@ evaluation metric.\n inference_resizing = keras_cv.layers.Resizing(\n     IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True\n )\n-eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE)\n+eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf_data.AUTOTUNE)\n \n inference_resizing = keras_cv.layers.Resizing(\n     IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True\n )\n-eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE)\n+eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf_data.AUTOTUNE)\n \n image_batch = next(iter(eval_ds.take(1)))[\"images\"]\n keras_cv.visualization.plot_image_gallery(\n@@ -522,8 +525,8 @@ def unpackage_dict(inputs):\n     return inputs[\"images\"], inputs[\"labels\"]\n \n \n-train_ds = train_ds.map(unpackage_dict, num_parallel_calls=tf.data.AUTOTUNE)\n-eval_ds = eval_ds.map(unpackage_dict, num_parallel_calls=tf.data.AUTOTUNE)\n+train_ds = train_ds.map(unpackage_dict, num_parallel_calls=tf_data.AUTOTUNE)\n+eval_ds = eval_ds.map(unpackage_dict, num_parallel_calls=tf_data.AUTOTUNE)\n \n \"\"\"Data augmentation is by far the hardest piece of training a modern\n classifier.\n@@ -577,7 +580,7 @@ def lr_warmup_cosine_decay(\n \n \n class WarmUpCosineDecay(\n-    schedules.learning_rate_schedule.LearningRateSchedule\n+    schedules.LearningRateSchedule\n ):\n     def __init__(\n         self, warmup_steps, total_steps, hold, start_lr=0.0, target_lr=1e-2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
