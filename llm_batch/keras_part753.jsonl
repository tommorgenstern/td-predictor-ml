{"custom_id": "keras#a35287ada8ecd69d6d4643cee3f1dee74ba4612a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 643 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 17 | Complexity Δ (Sum/Max): 43/33 | Churn Δ: 643 | Churn Cumulative: 2577 | Contributors (this commit): 23 | Commits (past 90d): 5 | Contributors (cumulative): 25 | DMM Complexity: 0.8014705882352942\n\nDIFF:\n@@ -7,6 +7,7 @@ from keras.layers.activations.relu import ReLU\n from keras.layers.activations.softmax import Softmax\n from keras.layers.attention.additive_attention import AdditiveAttention\n from keras.layers.attention.attention import Attention\n+from keras.layers.attention.grouped_query_attention import GroupedQueryAttention\n from keras.layers.attention.multi_head_attention import MultiHeadAttention\n from keras.layers.convolutional.conv1d import Conv1D\n from keras.layers.convolutional.conv1d_transpose import Conv1DTranspose\n\n@@ -0,0 +1,433 @@\n+from keras import constraints\n+from keras import initializers\n+from keras import ops\n+from keras import regularizers\n+from keras.api_export import keras_export\n+from keras.layers.activations.softmax import Softmax\n+from keras.layers.core.einsum_dense import EinsumDense\n+from keras.layers.layer import Layer\n+from keras.layers.regularization.dropout import Dropout\n+\n+\n+@keras_export(\"keras.layers.GroupQueryAttention\")\n+class GroupedQueryAttention(Layer):\n+    \"\"\"Grouped Query Attention layer.\n+\n+    This is an implementation of grouped-query attention introduced by\n+    [Ainslie et al., 2023](https://arxiv.org/abs/2305.13245). Here\n+    `num_key_value_heads` denotes number of groups, setting\n+    `num_key_value_heads` to 1 is equivalent to multi-query attention, and\n+    when `num_key_value_heads` is equal to `num_query_heads` it is equivalent\n+    to multi-head attention.\n+\n+    This layer first projects `query`, `key`, and `value` tensors. Then, `key`\n+    and `value` are repeated to match the number of heads of `query`.\n+\n+    Then, the `query` is scaled and dot-producted with `key` tensors. These are\n+    softmaxed to obtain attention probabilities. The value tensors are then\n+    interpolated by these probabilities and concatenated back to a single\n+    tensor.\n+\n+    Args:\n+        head_dim: Size of each attention head.\n+        num_query_heads: Number of query attention heads.\n+        num_key_value_heads: Number of key and value attention heads.\n+        dropout: Dropout probability.\n+        use_bias: Boolean, whether the dense layers use bias vectors/matrices.\n+        kernel_initializer: Initializer for dense layer kernels.\n+        bias_initializer: Initializer for dense layer biases.\n+        kernel_regularizer: Regularizer for dense layer kernels.\n+        bias_regularizer: Regularizer for dense layer biases.\n+        activity_regularizer: Regularizer for dense layer activity.\n+        kernel_constraint: Constraint for dense layer kernels.\n+        bias_constraint: Constraint for dense layer kernels.\n+\n+    Call arguments:\n+        query: Query tensor of shape `(batch_dim, target_seq_len, feature_dim)`,\n+            where `batch_dim` is batch size, `target_seq_len` is the length of\n+            target sequence, and `feature_dim` is dimension of feature.\n+        value: Value tensor of shape `(batch_dim, source_seq_len, feature_dim)`,\n+            where `batch_dim` is batch size, `source_seq_len` is the length of\n+            source sequence, and `feature_dim` is dimension of feature.\n+        key: Optional key tensor of shape\n+            `(batch_dim, source_seq_len, feature_dim)`. If not given, will use\n+            `value` for both `key` and `value`, which is most common case.\n+        attention_mask : A boolean mask of shape\n+            `(batch_dim, target_seq_len, source_seq_len)`, that prevents\n+            attention to certain positions. The boolean mask specifies which\n+            query elements can attend to which key elements, where 1 indicates\n+            attention and 0 indicates no attention. Broadcasting can happen for\n+            the missing batch dimensions and the head dimension.\n+        return_attention_scores: A boolean to indicate whether the output\n+            should be `(attention_output, attention_scores)` if `True`, or\n+            `attention_output` if `False`. Defaults to `False`.\n+        training: Python boolean indicating whether the layer should behave in\n+            training mode (adding dropout) or in inference mode (no dropout).\n+            Will go with either using the training mode of the parent\n+            layer/model or `False` (inference) if there is no parent layer.\n+        use_causal_mask: A boolean to indicate whether to apply a causal mask to\n+            prevent tokens from attending to future tokens (e.g., used in a\n+            decoder Transformer).\n+\n+    Returns:\n+        attention_output: Result of the computation, of shape\n+            `(batch_dim, target_seq_len, feature_dim)`, where `target_seq_len`\n+            is for target sequence length and `feature_dim` is the query input\n+            last dim.\n+        attention_scores: (Optional) attention coefficients of shape\n+            `(batch_dim, num_query_heads, target_seq_len, source_seq_len)`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        head_dim,\n+        num_query_heads,\n+        num_key_value_heads,\n+        dropout=0.0,\n+        use_bias=True,\n+        kernel_initializer=\"glorot_uniform\",\n+        bias_initializer=\"zeros\",\n+        kernel_regularizer=None,\n+        bias_regularizer=None,\n+        activity_regularizer=None,\n+        kernel_constraint=None,\n+        bias_constraint=None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.supports_masking = True\n+        self.head_dim = head_dim\n+        self.num_query_heads = num_query_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        if num_query_heads % num_key_value_heads != 0:\n+            raise ValueError(\n+                \"`num_query_heads` must be divisible\"\n+                \" by `num_key_value_heads`.\"\n+            )\n+        self.num_repeats = num_query_heads // num_key_value_heads\n+        self.dropout = dropout\n+        self.use_bias = use_bias\n+        self.kernel_initializer = initializers.get(kernel_initializer)\n+        self.bias_initializer = initializers.get(bias_initializer)\n+        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+        self.bias_regularizer = regularizers.get(bias_regularizer)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+        self.kernel_constraint = constraints.get(kernel_constraint)\n+        self.bias_constraint = constraints.get(bias_constraint)\n+\n+    def build(\n+        self,\n+        query_shape,\n+        value_shape,\n+        key_shape=None,\n+    ):\n+        # Einsum variables:\n+        # b = batch size\n+        # q = query length\n+        # k = key/value length\n+        # m = model dim\n+        # u = num query heads\n+        # v = num key/value heads\n+        # h = head dim\n+        key_shape = value_shape if key_shape is None else key_shape\n+        self.feature_dim = query_shape[-1]\n+        self._query_dense = EinsumDense(\n+            \"bqm,muh->bquh\",\n+            output_shape=(None, self.num_query_heads, self.head_dim),\n+            bias_axes=\"uh\" if self.use_bias else None,\n+            name=\"query\",\n+            **self._get_common_kwargs_for_sublayer(),\n+        )\n+        self._query_dense.build(query_shape)\n+\n+        self._key_dense = EinsumDense(\n+            \"bkm,mvh->bkvh\",\n+            output_shape=(None, self.num_key_value_heads, self.head_dim),\n+            bias_axes=\"vh\" if self.use_bias else None,\n+            name=\"key\",\n+            **self._get_common_kwargs_for_sublayer(),\n+        )\n+        self._key_dense.build(key_shape)\n+\n+        self._value_dense = EinsumDense(\n+            \"bkm,mvh->bkvh\",\n+            output_shape=(None, self.num_key_value_heads, self.head_dim),\n+            bias_axes=\"vh\" if self.use_bias else None,\n+            name=\"value\",\n+            **self._get_common_kwargs_for_sublayer(),\n+        )\n+        self._value_dense.build(value_shape)\n+\n+        self._softmax = Softmax(axis=-1, dtype=self.dtype_policy)\n+        self._dropout_layer = Dropout(\n+            rate=self.dropout, dtype=self.dtype_policy\n+        )\n+\n+        self._dot_product_equation = \"bquh,bkuh->buqk\"\n+        self._combine_equation = \"buqk,bkuh->bquh\"\n+\n+        self._output_dense = EinsumDense(\n+            \"bquh,uhm->bqm\",\n+            output_shape=(None, self.feature_dim),\n+            bias_axes=\"m\" if self.use_bias else None,\n+            name=\"attention_output\",\n+            **self._get_common_kwargs_for_sublayer(),\n+        )\n+        self._output_dense.build(\n+            (None, None, self.num_query_heads, self.head_dim)\n+        )\n+        self.built = True\n+\n+    def _get_common_kwargs_for_sublayer(self):\n+        common_kwargs = dict(\n+            kernel_regularizer=self.kernel_regularizer,\n+            bias_regularizer=self.bias_regularizer,\n+            activity_regularizer=self.activity_regularizer,\n+            kernel_constraint=self.kernel_constraint,\n+            bias_constraint=self.bias_constraint,\n+            dtype=self.dtype_policy,\n+        )\n+        # Create new clone of kernel/bias initializer, so that we don't reuse\n+        # the initializer instance, which could lead to same init value since\n+        # initializer is stateless.\n+        kernel_initializer = self.kernel_initializer.__class__.from_config(\n+            self.kernel_initializer.get_config()\n+        )\n+        bias_initializer = self.bias_initializer.__class__.from_config(\n+            self.bias_initializer.get_config()\n+        )\n+        common_kwargs[\"kernel_initializer\"] = kernel_initializer\n+        common_kwargs[\"bias_initializer\"] = bias_initializer\n+        return common_kwargs\n+\n+    def call(\n+        self,\n+        query,\n+        value,\n+        key=None,\n+        query_mask=None,\n+        value_mask=None,\n+        key_mask=None,\n+        attention_mask=None,\n+        return_attention_scores=False,\n+        training=None,\n+        use_causal_mask=False,\n+    ):\n+        if key is None:\n+            key = value\n+\n+        attention_mask = self._compute_attention_mask(\n+            query,\n+            value,\n+            query_mask=query_mask,\n+            value_mask=value_mask,\n+            key_mask=key_mask,\n+            attention_mask=attention_mask,\n+            use_causal_mask=use_causal_mask,\n+        )\n+\n+        query = self._query_dense(query)\n+        key = self._key_dense(key)\n+        value = self._value_dense(value)\n+\n+        key = ops.repeat(\n+            key, self.num_repeats, axis=2\n+        )  # (batch_dim, source_seq_len, query_heads, head_dim)\n+        value = ops.repeat(\n+            value, self.num_repeats, axis=2\n+        )  # (batch_dim, source_seq_len, query_heads, head_dim)\n+\n+        output, scores = self._compute_attention(\n+            query,\n+            key,\n+            value,\n+            attention_mask=attention_mask,\n+            training=training,\n+        )\n+\n+        output = self._output_dense(\n+            output\n+        )  # (batch_dim, target_seq_len, feature_dim)\n+\n+        if return_attention_scores:\n+            return output, scores\n+        return output\n+\n+    def _compute_attention_mask(\n+        self,\n+        query,\n+        value,\n+        query_mask=None,\n+        value_mask=None,\n+        key_mask=None,\n+        attention_mask=None,\n+        use_causal_mask=False,\n+    ):\n+        \"\"\"Computes the attention mask, using the Keras masks of the inputs.\n+\n+        * The `query`'s mask is reshaped from [B, T] to [B, T, 1].\n+        * The `value`'s mask is reshaped from [B, S] to [B, 1, S].\n+        * The `key`'s mask is reshaped from [B, S] to [B, 1, S]. The `key`'s\n+          mask is ignored if `key` is `None` or if `key is value`.\n+        * If `use_causal_mask=True`, then the causal mask is computed. Its shape\n+          is [1, T, S].\n+\n+        All defined masks are merged using a logical AND operation (`&`).\n+\n+        In general, if the `query` and `value` are masked, then there is no need\n+        to define the `attention_mask`.\n+\n+        Args:\n+            query: Projected query tensor of shape `(B, T, N, key_dim)`.\n+            key: Projected key tensor of shape `(B, T, N, key_dim)`.\n+            value: Projected value tensor of shape `(B, T, N, value_dim)`.\n+            attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n+                attention to certain positions.\n+            use_causal_mask: A boolean to indicate whether to apply a causal\n+                mask to prevent tokens from attending to future tokens (e.g.,\n+                used in a decoder Transformer).\n+\n+        Returns:\n+            attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n+                attention to certain positions, based on the Keras masks of the\n+                `query`, `key`, `value`, and `attention_mask` tensors, and the\n+                causal mask if `use_causal_mask=True`.\n+        \"\"\"\n+        auto_mask = None\n+        if query_mask is not None:\n+            query_mask = ops.cast(query_mask, \"bool\")  # defensive casting\n+            # B = batch size, T = max query length\n+            auto_mask = ops.expand_dims(query_mask, -1)  # shape is [B, T, 1]\n+        if value_mask is not None:\n+            value_mask = ops.cast(value_mask, \"bool\")  # defensive casting\n+            # B = batch size, S == max value length\n+            mask = ops.expand_dims(value_mask, -2)  # shape is [B, 1, S]\n+            auto_mask = mask if auto_mask is None else auto_mask & mask\n+        if key_mask is not None:\n+            key_mask = ops.cast(key_mask, \"bool\")  # defensive casting\n+            # B == batch size, S == max key length == max value length\n+            mask = ops.expand_dims(key_mask, -2)  # shape is [B, 1, S]\n+            auto_mask = mask if auto_mask is None else auto_mask & mask\n+        if use_causal_mask:\n+            # the shape of the causal mask is [1, T, S]\n+            mask = self._compute_causal_mask(query, value)\n+            auto_mask = mask if auto_mask is None else auto_mask & mask\n+        if auto_mask is not None:\n+            # merge attention_mask & automatic mask, to shape [B, T, S]\n+            attention_mask = (\n+                auto_mask\n+                if attention_mask is None\n+                else ops.cast(attention_mask, bool) & auto_mask\n+            )\n+        return attention_mask\n+\n+    def _compute_causal_mask(self, query, value=None):\n+        \"\"\"Computes a causal mask (e.g., for masked self-attention layers).\n+\n+        For example, if query and value both contain sequences of length 4,\n+        this function returns a boolean tensor equal to:\n+\n+        ```\n+        [[[True,  False, False, False],\n+          [True,  True,  False, False],\n+          [True,  True,  True,  False],\n+          [True,  True,  True,  True]]]\n+        ```\n+\n+        Args:\n+            query: query tensor of shape `(B, T, ...)`.\n+            value: value tensor of shape `(B, S, ...)` (optional, defaults to\n+                query).\n+\n+        Returns:\n+            mask: a boolean tensor of shape `(1, T, S)` containing a lower\n+                triangular matrix of shape `(T, S)`.\n+        \"\"\"\n+        q_seq_length = ops.shape(query)[1]\n+        v_seq_length = q_seq_length if value is None else ops.shape(value)[1]\n+        ones_mask = ops.ones((1, q_seq_length, v_seq_length), dtype=\"int32\")\n+        row_index = ops.cumsum(ones_mask, axis=-2)\n+        col_index = ops.cumsum(ones_mask, axis=-1)\n+        return ops.greater_equal(row_index, col_index)\n+\n+    def _compute_attention(\n+        self, query, key, value, attention_mask=None, training=None\n+    ):\n+        query = ops.multiply(\n+            query,\n+            1.0 / ops.sqrt(ops.cast(self.head_dim, query.dtype)),\n+        )\n+        # Take the dot product between \"query\" and \"key\" to get the raw\n+        # attention scores.\n+        scores = ops.einsum(\n+            self._dot_product_equation, query, key\n+        )  # (batch_dim, query_heads, target_seq_len, source_seq_len)\n+        scores = self._masked_softmax(scores, attention_mask=attention_mask)\n+        # This is actually dropping out entire tokens to attend to, which might\n+        # seem a bit unusual, but is taken from the original Transformer paper.\n+        scores_dropout = self._dropout_layer(scores, training=training)\n+        output = ops.einsum(self._combine_equation, scores_dropout, value)\n+        return output, scores\n+\n+    def _masked_softmax(self, scores, attention_mask=None):\n+        # Normalize the attention scores to probabilities.\n+        # scores = [B, N, T, S]\n+        if attention_mask is not None:\n+            # The expand dim happens starting from the `num_heads` dimension,\n+            # (<batch_dims>, num_heads, <query_attention_dims,\n+            # key_attention_dims>)\n+            mask_expansion_axis = -1 * 2 - 1\n+            for _ in range(len(scores.shape) - len(attention_mask.shape)):\n+                attention_mask = ops.expand_dims(\n+                    attention_mask, axis=mask_expansion_axis\n+                )\n+        return self._softmax(scores, mask=attention_mask)\n+\n+    def compute_output_shape(\n+        self,\n+        query_shape,\n+        value_shape,\n+        key_shape=None,\n+    ):\n+        if key_shape is None:\n+            key_shape = value_shape\n+\n+        if query_shape[-1] != value_shape[-1]:\n+            raise ValueError(\n+                \"The last dimension of `query_shape` and `value_shape` \"\n+                f\"must be equal, but are {query_shape[-1]}, {value_shape[-1]}. \"\n+                \"Received: query_shape={query_shape}, value_shape={value_shape}\"\n+            )\n+\n+        if value_shape[1:-1] != key_shape[1:-1]:\n+            raise ValueError(\n+                \"All dimensions of `value` and `key`, except the last one, \"\n+                f\"must be equal. Received: value_shape={value_shape} and \"\n+                f\"key_shape={key_shape}\"\n+            )\n+\n+        return query_shape\n+\n+    def get_config(self):\n+        config = {\n+            \"head_dim\": self.head_dim,\n+            \"num_query_heads\": self.num_query_heads,\n+            \"num_key_value_heads\": self.num_key_value_heads,\n+            \"use_bias\": self.use_bias,\n+            \"dropout\": self.dropout,\n+            \"kernel_initializer\": initializers.serialize(\n+                self.kernel_initializer\n+            ),\n+            \"bias_initializer\": initializers.serialize(self.bias_initializer),\n+            \"kernel_regularizer\": regularizers.serialize(\n+                self.kernel_regularizer\n+            ),\n+            \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\n+            \"activity_regularizer\": regularizers.serialize(\n+                self.activity_regularizer\n+            ),\n+            \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n+            \"bias_constraint\": constraints.serialize(self.bias_constraint),\n+        }\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,209 @@\n+import numpy as np\n+import pytest\n+from absl.testing import parameterized\n+\n+from keras import backend\n+from keras import initializers\n+from keras import layers\n+from keras import testing\n+\n+\n+class GroupedQueryAttentionTest(testing.TestCase, parameterized.TestCase):\n+    def test_basics(self):\n+        self.run_layer_test(\n+            layers.GroupedQueryAttention,\n+            init_kwargs={\n+                \"num_query_heads\": 2,\n+                \"num_key_value_heads\": 2,\n+                \"head_dim\": 2,\n+            },\n+            input_shape={\"query_shape\": (2, 8, 16), \"value_shape\": (2, 4, 16)},\n+            expected_output_shape=(2, 8, 16),\n+            expected_num_trainable_weights=8,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+            run_training_check=False,\n+        )\n+\n+        self.run_layer_test(\n+            layers.GroupedQueryAttention,\n+            init_kwargs={\n+                \"num_query_heads\": 2,\n+                \"num_key_value_heads\": 2,\n+                \"head_dim\": 2,\n+                \"use_bias\": False,\n+                \"dropout\": 0.5,\n+            },\n+            input_shape={\"query_shape\": (2, 8, 16), \"value_shape\": (2, 4, 16)},\n+            expected_output_shape=(2, 8, 16),\n+            expected_num_trainable_weights=4,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+            run_training_check=False,\n+        )\n+\n+    @parameterized.named_parameters(\n+        (\"without_key_proj_mha\", (4, 8), (2, 8), None, 2, 2),\n+        (\"with_key_proj_mha\", (4, 8), (2, 8), (2, 3), 2, 2),\n+        (\"without_key_proj_gqa\", (4, 8), (2, 8), None, 4, 2),\n+        (\"with_key_proj_gqa\", (4, 8), (2, 8), (2, 3), 4, 2),\n+        (\"without_key_value_proj_mqa\", (4, 8), (2, 8), None, 4, 1),\n+        (\"with_key_value_proj_mqa\", (4, 8), (2, 8), (2, 3), 4, 1),\n+    )\n+    def test_compute_output_shape(\n+        self,\n+        query_dims,\n+        value_dims,\n+        key_dims,\n+        num_query_heads,\n+        num_key_value_heads,\n+    ):\n+        \"\"\"Test computed shape is equal to the layer output's shape.\"\"\"\n+        layer = layers.GroupedQueryAttention(\n+            num_query_heads=num_query_heads,\n+            num_key_value_heads=num_key_value_heads,\n+            head_dim=2,\n+        )\n+        batch_size = 7\n+        query_shape = (batch_size,) + query_dims\n+        value_shape = (batch_size,) + value_dims\n+        key_shape = (batch_size,) + key_dims if key_dims else None\n+\n+        query = np.ones(query_shape)\n+        value = np.ones(value_shape)\n+        key = np.ones(key_shape) if key_shape else None\n+        output = layer(query=query, value=value, key=key)\n+        comp_output_shape = layer.compute_output_shape(\n+            query_shape, value_shape, key_shape\n+        )\n+        self.assertEqual(output.shape, comp_output_shape)\n+\n+    @parameterized.named_parameters(\n+        (\"query_value_dim_mismatch\", (2, 4, 8), (2, 2, 7), 2),\n+        (\"key_value_dim_mismatch\", (2, 4, 8), (2, 2, 8), (2, 1, 7)),\n+    )\n+    def test_shape_mismatch_error(self, query_shape, value_shape, key_shape):\n+        \"\"\"Test dimension mismatches\"\"\"\n+        layer = layers.GroupedQueryAttention(\n+            num_query_heads=4,\n+            num_key_value_heads=4,\n+            head_dim=2,\n+        )\n+        with self.assertRaisesRegex(ValueError, r\"must be equal\"):\n+            layer.compute_output_shape(query_shape, value_shape, key_shape)\n+\n+    def test_initializer(self):\n+        # Test with a specified initializer.\n+        layer = layers.GroupedQueryAttention(\n+            num_query_heads=16,\n+            num_key_value_heads=16,\n+            head_dim=64,\n+            kernel_initializer=initializers.TruncatedNormal(stddev=0.02),\n+        )\n+        layer.build((2, 4, 8), (2, 4, 8))\n+\n+        # Make sure the sub layers have different kernel init value.\n+        self.assertNotAllClose(\n+            layer._query_dense.kernel,\n+            layer._key_dense.kernel,\n+        )\n+        self.assertNotAllClose(\n+            layer._query_dense.kernel,\n+            layer._value_dense.kernel,\n+        )\n+        self.assertNotAllClose(\n+            layer._query_dense.kernel,\n+            layer._output_dense.kernel,\n+        )\n+\n+    @pytest.mark.skipif(\n+        backend.backend() == \"numpy\",\n+        reason=\"Numpy backend does not support masking.\",\n+    )\n+    def test_query_mask_progagation(self):\n+        \"\"\"Test automatic propagation of the query's mask.\"\"\"\n+        layer = layers.GroupedQueryAttention(\n+            num_query_heads=2, num_key_value_heads=2, head_dim=2\n+        )\n+        self.assertTrue(layer.supports_masking)\n+        query = np.array([[1, 2, 3, 0, 0], [3, 3, 1, 1, 2], [1, 0, 0, 0, 0]])\n+        masked_query = layers.Embedding(4, 8, mask_zero=True)(query)\n+        value = np.random.normal(size=(3, 3, 8))\n+        output = layer(query=masked_query, value=value)\n+        self.assertAllClose(masked_query._keras_mask, output._keras_mask)\n+\n+    @parameterized.named_parameters((\"causal\", True), (\"not_causal\", 0))\n+    @pytest.mark.skipif(\n+        backend.backend() == \"numpy\",\n+        reason=\"Numpy backend does not support masking.\",\n+    )\n+    def test_masking(self, use_causal_mask):\n+        \"\"\"Test that the value and causal masks are taken into account.\"\"\"\n+        layer = layers.GroupedQueryAttention(\n+            num_query_heads=2, num_key_value_heads=2, head_dim=2\n+        )\n+        query = np.array([[1, 2, 3, 0, 0], [3, 3, 1, 1, 2], [1, 0, 0, 0, 0]])\n+        masked_query = layers.Embedding(4, 8, mask_zero=True)(query)\n+        value = np.array([[5, 4, 0], [3, 0, 0], [2, 1, 1]])\n+        masked_value = layers.Embedding(6, 8, mask_zero=True)(value)\n+        output = layer(\n+            query=masked_query,\n+            value=masked_value,\n+            use_causal_mask=use_causal_mask,\n+        )\n+        mask = np.array(\n+            [[[1, 1, 0]] * 3 + [[0, 0, 0]] * 2]\n+            + [[[1, 0, 0]] * 5]\n+            + [[[1, 1, 1]] + [[0, 0, 0]] * 4]\n+        ).astype(bool)\n+        if use_causal_mask:\n+            mask = mask & np.array(\n+                [[[1, 0, 0], [1, 1, 0]] + [[1, 1, 1]] * 3]\n+            ).astype(bool)\n+        del masked_query._keras_mask\n+        del masked_value._keras_mask\n+        output_with_manual_mask = layer(\n+            query=masked_query, value=masked_value, attention_mask=mask\n+        )\n+        self.assertAllClose(output, output_with_manual_mask)\n+\n+    def test_correctness(self):\n+        query = np.array([[[1.0, 0.0], [0.0, 1.0]]])\n+        key = np.array([[[0.0, 1.0], [1.0, 0.0]]])\n+        value = np.array([[[1.0, 2.0], [3.0, 4.0]]])\n+\n+        # Setup layer.\n+        num_heads = 2\n+        key_dim = 2\n+        layer = layers.MultiHeadAttention(\n+            num_heads=num_heads,\n+            key_dim=key_dim,\n+        )\n+        layer.build(query.shape, key.shape, value.shape)\n+\n+        # Set layer weights.\n+        kernel = np.identity(key_dim)\n+        # To get an identity kernel we need to add a head dim and repeat on it.\n+        kernel = np.repeat(kernel[:, np.newaxis, :], num_heads, axis=1)\n+        # Zeros for all biases.\n+        bias = np.zeros((2, 2))\n+        output_bias = np.zeros((2,))\n+        layer.set_weights([kernel, bias] * 3 + [kernel, output_bias])\n+\n+        # Call layer and assert output.\n+        output, scores = layer(\n+            query=query,\n+            value=value,\n+            key=key,\n+            return_attention_scores=True,\n+        )\n+        self.assertAllClose(output, [[[5.679, 5.679], [4.32, 4.32]]], atol=1e-3)\n+        self.assertAllClose(\n+            scores,\n+            [[[[0.33, 0.67], [0.67, 0.33]], [[0.33, 0.67], [0.67, 0.33]]]],\n+            atol=1e-3,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1f80ca11adb3aebc23a0dbf76f89bf32043060af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 9 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,2 +1,9 @@\n+from keras.api_export import keras_export\n+\n # Unique source of truth for the version number.\n __version__ = \"3.0.0\"\n+\n+\n+@keras_export(\"keras.version\")\n+def version():\n+    return __version__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5cce1b26115612f8c0bbd2dbc9b048c297541d50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 46 | Churn Cumulative: 314 | Contributors (this commit): 5 | Commits (past 90d): 17 | Contributors (cumulative): 12 | DMM Complexity: 0.9210526315789473\n\nDIFF:\n@@ -352,6 +352,7 @@ class JAXTrainer(base_trainer.Trainer):\n     ):\n         self._assert_compile_called(\"fit\")\n         # TODO: respect compiled trainable state\n+        self._eval_epoch_iterator = None\n         if validation_split and validation_data is None:\n             # Create the validation data using the training data. Only supported\n             # for TF/numpy/jax arrays.\n\n@@ -265,6 +265,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n     ):\n         self._assert_compile_called(\"fit\")\n         # TODO: respect compiled trainable state\n+        self._eval_epoch_iterator = None\n         if validation_split and validation_data is None:\n             # Create the validation data using the training data. Only supported\n             # for TF/numpy/jax arrays.\n\n@@ -236,6 +236,7 @@ class TorchTrainer(base_trainer.Trainer):\n             )\n \n         # TODO: respect compiled trainable state\n+        self._eval_epoch_iterator = None\n         if validation_split and validation_data is None:\n             # Create the validation data using the training data. Only supported\n             # for TF/numpy/jax arrays.\n\n@@ -650,6 +650,49 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         out = model.predict({\"a\": x1, \"b\": x2})\n         self.assertEqual(out.shape, (3, 4))\n \n+    @pytest.mark.requires_trainable_backend\n+    def test_for_eval_epoch_iterator(self):\n+        model = ExampleModel(units=3)\n+        model.compile(\n+            optimizer=\"adam\", loss=\"mse\", metrics=[\"mean_absolute_error\"]\n+        )\n+        x = np.ones((16, 4))\n+        y = np.zeros((16, 3))\n+        x_test = np.ones((16, 4))\n+        y_test = np.zeros((16, 3))\n+        model.fit(\n+            x,\n+            y,\n+            batch_size=4,\n+            validation_data=(x_test, y_test),\n+        )\n+        assert getattr(model, \"_eval_epoch_iterator\", None) is None\n+\n+        # Try model.fit with reshaped validation_data\n+        # This will throw an exception which is intended\n+        try:\n+            model.fit(\n+                x,\n+                y,\n+                batch_size=4,\n+                validation_data=(\n+                    x_test.reshape((-1, 16, 4)),\n+                    y_test.reshape((-1, 16, 3)),\n+                ),\n+            )\n+        except:\n+            pass\n+\n+        # Try model.fit with correct validation_data this should work.\n+        # After successful training `_eval_epoch_iterator` should be None\n+        model.fit(\n+            x,\n+            y,\n+            batch_size=4,\n+            validation_data=(x_test, y_test),\n+        )\n+        assert getattr(model, \"_eval_epoch_iterator\", None) is None\n+\n     @pytest.mark.requires_trainable_backend\n     def test_callback_methods_keys(self):\n         class CustomCallback(Callback):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#080276f698f0b27e06619a0afd3d2726f5a973fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): 7/3 | Churn Δ: 61 | Churn Cumulative: 155 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -156,3 +156,34 @@ class InitializersTest(testing.TestCase):\n \n         with self.assertRaises(ValueError):\n             initializers.get(\"typo\")\n+\n+    def test_variance_scaling_invalid_scale(self):\n+        seed = 1234\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"Argument `scale` must be positive float.\"\n+        ):\n+            initializers.VarianceScaling(scale=-1.0, seed=seed, mode=\"fan_in\")\n+\n+    def test_variance_scaling_invalid_mode(self):\n+        scale = 2.0\n+        seed = 1234\n+\n+        with self.assertRaisesRegex(ValueError, \"Invalid `mode` argument:\"):\n+            initializers.VarianceScaling(\n+                scale=scale, seed=seed, mode=\"invalid_mode\"\n+            )\n+\n+    def test_variance_scaling_invalid_distribution(self):\n+        scale = 2.0\n+        seed = 1234\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid `distribution` argument:\"\n+        ):\n+            initializers.VarianceScaling(\n+                scale=scale,\n+                seed=seed,\n+                mode=\"fan_in\",\n+                distribution=\"invalid_dist\",\n+            )\n\n@@ -239,3 +239,10 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n             ValueError, \"`keras.random.randint` requires an integer `dtype`.\"\n         ):\n             random.randint((3, 4), minval=0, maxval=10, dtype=\"float64\")\n+\n+    def test_uniform_dtype_validation(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"`keras.random.uniform` requires a floating point `dtype`.\",\n+        ):\n+            random.uniform((3, 4), minval=0, maxval=10, dtype=\"int64\")\n\n@@ -1,4 +1,5 @@\n import numpy as np\n+import pytest\n \n from keras import backend\n from keras import ops\n@@ -54,3 +55,25 @@ class SeedGeneratorTest(testing.TestCase):\n             ValueError, \"Argument `seed` must be either an integer\"\n         ):\n             seed_generator.draw_seed(\"invalid_seed\")\n+\n+    def test_seed_generator_unexpected_kwargs(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"Unrecognized keyword arguments\"\n+        ):\n+            seed_generator.SeedGenerator(invalid_arg=\"unexpected_value\")\n+\n+    @pytest.mark.skipif(\n+        backend.backend() != \"jax\", reason=\"This test requires the JAX backend\"\n+    )\n+    def test_jax_tracing_with_global_seed_generator(self):\n+        import jax\n+\n+        @jax.jit\n+        def traced_function():\n+            return seed_generator.global_seed_generator().next()\n+\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"When tracing a JAX function, you should only use seeded random\",\n+        ):\n+            traced_function()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#666b8d30b842f9346f7c7d8ab392bf9ebd58742a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 70 | Churn Cumulative: 5326 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: 0.10810810810810811\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from keras import backend\n from keras import constraints\n from keras import initializers\n from keras import ops\n@@ -67,6 +68,10 @@ class BatchNormalization(Layer):\n         gamma_regularizer: Optional regularizer for the gamma weight.\n         beta_constraint: Optional constraint for the beta weight.\n         gamma_constraint: Optional constraint for the gamma weight.\n+        synchronized: If `True`, synchronizes the global batch statistics (mean and\n+            variance) for the layer across all devices at each training step in a\n+            distributed training strategy. If `False`, each replica uses its own\n+            local batch statistics.\n         **kwargs: Base layer keyword arguments (e.g. `name` and `dtype`).\n \n     Call arguments:\n@@ -125,10 +130,17 @@ class BatchNormalization(Layer):\n         gamma_regularizer=None,\n         beta_constraint=None,\n         gamma_constraint=None,\n+        synchronized=False,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n         self.axis = int(axis)\n+\n+        if synchronized and backend.backend() != \"tensorflow\":\n+            raise ValueError(\"Argument synchronized=True is only supported with the TensorFlow backend.\")\n+\n+        self.synchronized = synchronized\n+\n         self.momentum = float(momentum)\n         self.epsilon = float(epsilon)\n         self.center = center\n@@ -188,6 +200,60 @@ class BatchNormalization(Layer):\n     def compute_output_shape(self, input_shape):\n         return input_shape\n     \n+\n+    def _calculate_mean_and_var(self, inputs):\n+\n+        if not self.synchronized:\n+            return ops.moments(\n+                inputs, axes=self._reduction_axes, keepdims=True\n+            )\n+\n+        axes = self._reduction_axes\n+        y = inputs\n+\n+        if backend() == \"tensorflow\":\n+            \n+            from keras.utils.module_utils import tensorflow as tf\n+            replica_ctx = tf.distribute.get_replica_context()\n+\n+            if replica_ctx:\n+                local_sum = ops.sum(y, axis=axes, keepdims=True)\n+                local_squared_sum = ops.sum(ops.square(y), axis=axes, keepdims=True)\n+                batch_size = ops.cast(ops.shape(y)[axes[0]], dtype=\"float32\")\n+\n+                y_sum = replica_ctx.all_reduce(tf.distribute.ReduceOp.SUM, local_sum)\n+                y_squared_sum = replica_ctx.all_reduce(tf.distribute.ReduceOp.SUM, local_squared_sum)\n+                global_batch_size = replica_ctx.all_reduce(tf.distribute.ReduceOp.SUM, batch_size)\n+\n+                axes_vals = [(ops.shape(y))[axes[i]] for i in range(1, len(axes))]\n+                multiplier = ops.cast(ops.prod(axes_vals), \"float32\")\n+                multiplier = multiplier * global_batch_size\n+\n+                mean = y_sum / multiplier\n+                y_squared_mean = y_squared_sum / multiplier\n+                # var = E(x^2) - E(x)^2\n+                variance = y_squared_mean - ops.square(mean)\n+            else:\n+                # Compute true mean while keeping the dims for proper broadcasting.\n+                mean = ops.mean(y, axes, keepdims=True, name=\"mean\")\n+                # sample variance, not unbiased variance\n+                # Note: stop_gradient does not change the gradient that gets\n+                #       backpropagated to the mean from the variance calculation,\n+                #       because that gradient is zero\n+                variance = ops.reduce_mean(\n+                    tf.math.squared_difference(y, ops.stop_gradient(mean)), axes, keepdims=True, name=\"variance\"\n+                )\n+\n+            mean = ops.cast(mean, inputs.dtype)\n+            variance = ops.cast(variance, inputs.dtype)\n+                \n+            return (mean, variance)\n+        elif backend() == \"jax\":\n+            raise NotImplementedError\n+        elif backend() == \"torch\":\n+            raise NotImplementedError\n+\n+\n     def call(self, inputs, training=None, mask=None):\n         input_dtype = standardize_dtype(inputs.dtype)\n         if input_dtype in (\"float16\", \"bfloat16\"):\n@@ -198,9 +264,7 @@ class BatchNormalization(Layer):\n         broadcast_shape = [1] * len(inputs.shape)\n         broadcast_shape[self.axis] = inputs.shape[self.axis]\n         if training and self.trainable:\n-            mean, variance = ops.moments(\n-                inputs, axes=self._reduction_axes, keepdims=True\n-            )\n+            mean, variance = self._calculate_mean_and_var(inputs)\n             moving_mean = ops.cast(self.moving_mean, inputs.dtype)\n             moving_variance = ops.cast(self.moving_variance, inputs.dtype)\n             self.moving_mean.assign(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fd48a4924cfcba4ebd5d9bb37060fb5c353a44c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 33 | Churn Cumulative: 1012 | Contributors (this commit): 5 | Commits (past 90d): 19 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,7 @@ from keras import losses\n from keras import metrics\n from keras import models\n from keras import optimizers\n+from keras.callbacks import LearningRateScheduler\n \n \n def test_model_fit():\n@@ -31,6 +32,8 @@ def test_model_fit():\n         outputs = layers.Dense(16)(x)\n         model = models.Model(inputs, outputs)\n \n+    callbacks = [LearningRateScheduler(lambda _: 0.1)]\n+\n     model.summary()\n \n     x = np.random.random((5000, 100))\n@@ -49,7 +52,12 @@ def test_model_fit():\n             jit_compile=False,\n         )\n         history = model.fit(\n-            x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n+            x,\n+            y,\n+            batch_size=batch_size,\n+            epochs=epochs,\n+            validation_split=0.2,\n+            callbacks=callbacks,\n         )\n \n     print(\"History:\")\n@@ -59,7 +67,7 @@ def test_model_fit():\n     with strategy.scope():\n         dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size)\n         dataset = strategy.experimental_distribute_dataset(dataset)\n-        history = model.fit(dataset, epochs=epochs)\n+        history = model.fit(dataset, epochs=epochs, callbacks=callbacks)\n \n     print(\"History:\")\n     print(history.history)\n\n@@ -179,7 +179,11 @@ class KerasVariable:\n         return self.value.__getitem__(idx)\n \n     def __array__(self, dtype=None):\n-        return self.value.__array__(dtype)\n+        # We can't directly use self.value.__array__ here because of scalar.\n+        # Numpy require this method to return as array like object. In the case\n+        # of scalar, it will fail the type checking from numpy. We need to\n+        # return a 0d array via numpy.\n+        return np.asarray(self.value.__array__(dtype))\n \n     def __bool__(self):\n         raise TypeError(\"A Keras Variable cannot be used as a boolean.\")\n\n@@ -211,6 +211,21 @@ class VariableNumpyValueAndAssignmentTest(test_case.TestCase):\n         self.assertIsInstance(v.numpy(), np.ndarray)\n         self.assertAllClose(v.numpy(), np.array([1, 2, 3]))\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"tf\",\n+        reason=\"Tests for MirroredVariable under tf backend\",\n+    )\n+    def test_variable_numpy_scalar(self):\n+        from keras.utils.module_utils import tensorflow as tf\n+\n+        strategy = tf.distribute.MirroredStrategy([\"cpu:0\", \"cpu:1\"])\n+        with strategy.scope():\n+            v = backend.Variable(initializer=0.0)\n+\n+        np_value = backend.convert_to_numpy(v)\n+        self.assertIsInstance(np_value, np.ndarray)\n+        self.assertAllClose(np_value, 0.0)\n+\n     def test_variable_value(self):\n         \"\"\"Test retrieving the value of a variable.\"\"\"\n         v = backend.Variable(initializer=np.array([1, 2, 3]))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#555644e6540480b43ba8e264c7ab1f2739f273cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 36 | Churn Cumulative: 458 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1462,3 +1462,39 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         expected_variance = np.var(x, axis=(0, 1, 2), keepdims=True)\n         self.assertAllClose(mean, expected_mean, atol=1e-5, rtol=1e-5)\n         self.assertAllClose(variance, expected_variance, atol=1e-5, rtol=1e-5)\n+\n+    @parameterized.product(dtype=[\"float16\", \"float32\"])\n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\",\n+        reason=\"synchronized=True only implemented for TF backend\",\n+    )\n+    def test_moments_sync_with_distribution_strategy(self, dtype):\n+        from keras.utils.module_utils import tensorflow as tf\n+\n+        # Config 2 CPUs for testing.\n+        logical_cpus = tf.config.list_logical_devices(\"CPU\")\n+        if len(logical_cpus) == 1:\n+            from tensorflow.python.eager import context\n+\n+            context._reset_context()\n+            tf.config.set_logical_device_configuration(\n+                tf.config.list_physical_devices(\"CPU\")[0],\n+                [\n+                    tf.config.LogicalDeviceConfiguration(),\n+                    tf.config.LogicalDeviceConfiguration(),\n+                ],\n+            )\n+\n+        @tf.function()\n+        def test_on_moments(inputs):\n+            return knn.moments(\n+                inputs, axes=-1, keepdims=True, synchronized=True\n+            )\n+\n+        # Test output of moments.\n+        inputs = tf.constant([5.0, 9.0, 1.0, 3.0], dtype=dtype)\n+        strategy = tf.distribute.MirroredStrategy([\"CPU:0\", \"CPU:1\"])\n+        with strategy.scope():\n+            mean, variance = strategy.run(test_on_moments, args=(inputs,))\n+            self.assertEqual(mean.values[0], 4.5)\n+            self.assertEqual(variance.values[0], 8.75)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dd3acdb5349df8b7056e84e839fbfd83a9b9d1be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 291 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 5 | Methods Changed: 7 | Complexity Δ (Sum/Max): 31/26 | Churn Δ: 291 | Churn Cumulative: 335 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.4660633484162896\n\nDIFF:\n@@ -1,4 +1,5 @@\n from keras import backend\n+from keras import ops\n from keras.api_export import keras_export\n from keras.backend import KerasTensor\n from keras.backend import any_symbolic_tensors\n@@ -501,3 +502,226 @@ def map_coordinates(\n         fill_mode,\n         fill_value,\n     )\n+\n+\n+class PadImages(Operation):\n+    def __init__(\n+        self,\n+        top_padding,\n+        bottom_padding,\n+        left_padding,\n+        right_padding,\n+        target_height,\n+        target_width,\n+    ):\n+        super().__init__()\n+        self.top_padding = top_padding\n+        self.bottom_padding = bottom_padding\n+        self.left_padding = left_padding\n+        self.right_padding = right_padding\n+        self.target_height = target_height\n+        self.target_width = target_width\n+\n+    def call(self, images):\n+        return _pad_images(\n+            images,\n+            self.top_padding,\n+            self.bottom_padding,\n+            self.left_padding,\n+            self.right_padding,\n+            self.target_height,\n+            self.target_width,\n+        )\n+\n+    def compute_output_spec(self, images):\n+        images_shape = ops.shape(images)\n+        if self.target_height is None:\n+            height_axis = 0 if len(images_shape) == 3 else 1\n+            self.target_height = (\n+                self.top_padding\n+                + images_shape[height_axis]\n+                + self.bottom_padding\n+            )\n+        if self.target_width is None:\n+            width_axis = 0 if len(images_shape) == 3 else 2\n+            self.target_width = (\n+                self.left_padding\n+                + images_shape[width_axis]\n+                + self.right_padding\n+            )\n+        out_shape = (\n+            images_shape[0],\n+            self.target_height,\n+            self.target_width,\n+            images_shape[-1],\n+        )\n+        if len(images_shape) == 3:\n+            out_shape = out_shape[1:]\n+        return KerasTensor(\n+            shape=out_shape,\n+            dtype=images.dtype,\n+        )\n+\n+\n+def _pad_images(\n+    images,\n+    top_padding,\n+    bottom_padding,\n+    left_padding,\n+    right_padding,\n+    target_height,\n+    target_width,\n+):\n+    images = backend.convert_to_tensor(images)\n+    is_batch = True\n+    images_shape = ops.shape(images)\n+    if len(images_shape) == 3:\n+        is_batch = False\n+        images = backend.numpy.expand_dims(images, 0)\n+    elif len(images_shape) != 4:\n+        raise ValueError(\n+            f\"Invalid shape for argument `images`: \"\n+            \"it must have rank 3 or 4. \"\n+            f\"Received: images.shape={images_shape}\"\n+        )\n+\n+    batch, height, width, depth = ops.shape(images)\n+\n+    if [top_padding, bottom_padding, target_height].count(None) != 1:\n+        raise ValueError(\n+            \"Must specify exactly two of \"\n+            \"top_padding, bottom_padding, target_height. \"\n+            f\"Received: top_padding={top_padding}, \"\n+            f\"bottom_padding={bottom_padding}, \"\n+            f\"target_height={target_height}\"\n+        )\n+    if [left_padding, right_padding, target_width].count(None) != 1:\n+        raise ValueError(\n+            \"Must specify exactly two of \"\n+            \"left_padding, right_padding, target_width. \"\n+            f\"Received: left_padding={left_padding}, \"\n+            f\"right_padding={right_padding}, \"\n+            f\"target_width={target_width}\"\n+        )\n+\n+    if top_padding is None:\n+        top_padding = target_height - bottom_padding - height\n+    if bottom_padding is None:\n+        bottom_padding = target_height - top_padding - height\n+    if left_padding is None:\n+        left_padding = target_width - right_padding - width\n+    if right_padding is None:\n+        right_padding = target_width - left_padding - width\n+\n+    if top_padding < 0:\n+        raise ValueError(\n+            \"top_padding must be >= 0. \" f\"Received: top_padding={top_padding}\"\n+        )\n+    if left_padding < 0:\n+        raise ValueError(\n+            \"left_padding must be >= 0. \"\n+            f\"Received: left_padding={left_padding}\"\n+        )\n+    if right_padding < 0:\n+        raise ValueError(\n+            \"right_padding must be >= 0. \"\n+            f\"Received: right_padding={right_padding}\"\n+        )\n+    if bottom_padding < 0:\n+        raise ValueError(\n+            \"bottom_padding must be >= 0. \"\n+            f\"Received: bottom_padding={bottom_padding}\"\n+        )\n+\n+    paddings = backend.numpy.reshape(\n+        backend.numpy.stack(\n+            [\n+                0,\n+                0,\n+                top_padding,\n+                bottom_padding,\n+                left_padding,\n+                right_padding,\n+                0,\n+                0,\n+            ]\n+        ),\n+        [4, 2],\n+    )\n+    padded = backend.numpy.pad(images, paddings)\n+\n+    if target_height is None:\n+        target_height = top_padding + height + bottom_padding\n+    if target_width is None:\n+        target_width = left_padding + width + right_padding\n+    padded_shape = [batch, target_height, target_width, depth]\n+    padded = backend.numpy.reshape(padded, padded_shape)\n+\n+    if not is_batch:\n+        padded = backend.numpy.squeeze(padded, axis=[0])\n+    return padded\n+\n+\n+@keras_export(\"keras.ops.image.pad_images\")\n+def pad_images(\n+    images,\n+    top_padding=None,\n+    left_padding=None,\n+    target_height=None,\n+    target_width=None,\n+    bottom_padding=None,\n+    right_padding=None,\n+):\n+    \"\"\"Pad `images` with zeros to the specified `height` and `width`.\n+\n+    Args:\n+        images: 4D Tensor of shape `(batch, height, width, channels)` or 3D\n+            Tensor of shape `(height, width, channels)`.\n+        top_padding: Number of rows of zeros to add on top.\n+        bottom_padding: Number of rows of zeros to add at the bottom.\n+        left_padding: Number of columns of zeros to add on the left.\n+        right_padding: Number of columns of zeros to add on the right.\n+        target_height: Height of output images.\n+        target_width: Width of output images.\n+\n+    Returns:\n+        If `images` were 4D, a 4D float Tensor of shape\n+            `(batch, target_height, target_width, channels)`\n+        If `images` were 3D, a 3D float Tensor of shape\n+            `(target_height, target_width, channels)`\n+\n+    Example:\n+\n+    >>> images = np.random.random((15, 25, 3))\n+    >>> padded_images = keras.ops.image.pad_images(\n+    ...     images, 2, 3, target_height=20, target_width=30\n+    ... )\n+    >>> padded_images.shape\n+    (20, 30, 3)\n+\n+    >>> batch_images = np.random.random((2, 15, 25, 3))\n+    >>> padded_batch = keras.ops.image.pad_images(\n+    ...     batch_images, 2, 3, target_height=20, target_width=30\n+    ... )\n+    >>> padded_batch.shape\n+    (2, 20, 30, 3)\"\"\"\n+\n+    if any_symbolic_tensors((images,)):\n+        return PadImages(\n+            top_padding,\n+            bottom_padding,\n+            left_padding,\n+            right_padding,\n+            target_height,\n+            target_width,\n+        ).symbolic_call(images)\n+\n+    return _pad_images(\n+        images,\n+        top_padding,\n+        bottom_padding,\n+        left_padding,\n+        right_padding,\n+        target_height,\n+        target_width,\n+    )\n\n@@ -42,6 +42,15 @@ class ImageOpsDynamicShapeTest(testing.TestCase):\n         out = kimage.map_coordinates(input, coordinates, 0)\n         self.assertEqual(out.shape, coordinates.shape[1:])\n \n+    def test_pad_images(self):\n+        x = KerasTensor([None, 15, 25, 3])\n+        out = kimage.pad_images(x, 2, 3, target_height=20, target_width=30)\n+        self.assertEqual(out.shape, (None, 20, 30, 3))\n+\n+        x = KerasTensor([None, None, 3])\n+        out = kimage.pad_images(x, 2, 3, target_height=20, target_width=30)\n+        self.assertEqual(out.shape, (20, 30, 3))\n+\n \n class ImageOpsStaticShapeTest(testing.TestCase):\n     def test_resize(self):\n@@ -69,6 +78,17 @@ class ImageOpsStaticShapeTest(testing.TestCase):\n         out = kimage.map_coordinates(input, coordinates, 0)\n         self.assertEqual(out.shape, coordinates.shape[1:])\n \n+    def test_pad_images(self):\n+        x = KerasTensor([15, 25, 3])\n+        out = kimage.pad_images(x, 2, 3, target_height=20, target_width=30)\n+        self.assertEqual(out.shape, (20, 30, 3))\n+\n+        x_batch = KerasTensor([2, 15, 25, 3])\n+        out_batch = kimage.pad_images(\n+            x_batch, 2, 3, target_height=20, target_width=30\n+        )\n+        self.assertEqual(out_batch.shape, (2, 20, 30, 3))\n+\n \n AFFINE_TRANSFORM_INTERPOLATIONS = {  # map to order\n     \"nearest\": 0,\n@@ -413,3 +433,50 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         expected = _fixed_map_coordinates(input, coordinates, order, fill_mode)\n \n         self.assertAllClose(output, expected)\n+\n+    @parameterized.parameters(\n+        [\n+            (0, 0, 3, 3, None, None),\n+            (1, 0, 4, 3, None, None),\n+            (0, 1, 3, 4, None, None),\n+            (0, 0, 4, 3, None, None),\n+            (0, 0, 3, 4, None, None),\n+            (0, 0, None, None, 0, 1),\n+            (0, 0, None, None, 1, 0),\n+            (1, 2, None, None, 3, 4),\n+        ]\n+    )\n+    def test_pad_images(\n+        self,\n+        top_padding,\n+        left_padding,\n+        target_height,\n+        target_width,\n+        bottom_padding,\n+        right_padding,\n+    ):\n+        image = np.random.uniform(size=(3, 3, 1))\n+        padded_image = kimage.pad_images(\n+            image,\n+            top_padding,\n+            left_padding,\n+            target_height,\n+            target_width,\n+            bottom_padding,\n+            right_padding,\n+        )\n+        if target_height is None:\n+            target_height = image.shape[0] + top_padding + bottom_padding\n+        if target_width is None:\n+            target_width = image.shape[1] + left_padding + right_padding\n+        ref_padded_image = tf.image.pad_to_bounding_box(\n+            image, top_padding, left_padding, target_height, target_width\n+        )\n+        self.assertEqual(\n+            tuple(padded_image.shape), tuple(ref_padded_image.shape)\n+        )\n+        self.assertAllClose(\n+            ref_padded_image.numpy(),\n+            backend.convert_to_numpy(padded_image),\n+            atol=1e-5,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
