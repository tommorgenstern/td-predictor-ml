{"custom_id": "keras#c2150dd2f45f1525c132158445343c999587ab29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 484 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 11 | Complexity Δ (Sum/Max): 20/20 | Churn Δ: 484 | Churn Cumulative: 484 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,484 @@\n+\"\"\"\n+Title: Classification with Neural Decision Forests\n+Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n+Date created: 2021/01/15\n+Last modified: 2021/01/15\n+Description: How to train differentiable decision trees for end-to-end learning in deep neural networks.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+This example provides an implementation of the\n+[Deep Neural Decision Forest](https://ieeexplore.ieee.org/document/7410529)\n+model introduced by P. Kontschieder et al. for structured data classification.\n+It demonstrates how to build a stochastic and differentiable decision tree model,\n+train it end-to-end, and unify decision trees with deep representation learning.\n+\n+## The dataset\n+\n+This example uses the\n+[United States Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/census+income)\n+provided by the\n+[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).\n+The task is binary classification\n+to predict whether a person is likely to be making over USD 50,000 a year.\n+\n+The dataset includes 48,842 instances with 14 input features (such as age, work class, education, occupation, and so on): 5 numerical features\n+and 9 categorical features.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core.layers import StringLookup\n+from keras_core import ops\n+\n+\n+from tensorflow import data as tf_data\n+import numpy as np\n+import pandas as pd\n+\n+import math\n+\n+\n+_dtype = \"float32\"\n+\n+\"\"\"\n+## Prepare the data\n+\"\"\"\n+\n+CSV_HEADER = [\n+    \"age\",\n+    \"workclass\",\n+    \"fnlwgt\",\n+    \"education\",\n+    \"education_num\",\n+    \"marital_status\",\n+    \"occupation\",\n+    \"relationship\",\n+    \"race\",\n+    \"gender\",\n+    \"capital_gain\",\n+    \"capital_loss\",\n+    \"hours_per_week\",\n+    \"native_country\",\n+    \"income_bracket\",\n+]\n+\n+train_data_url = (\n+    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n+)\n+train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n+\n+test_data_url = (\n+    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n+)\n+test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)\n+\n+print(f\"Train dataset shape: {train_data.shape}\")\n+print(f\"Test dataset shape: {test_data.shape}\")\n+\n+\"\"\"\n+Remove the first record (because it is not a valid data example) and a trailing\n+'dot' in the class labels.\n+\"\"\"\n+\n+test_data = test_data[1:]\n+test_data.income_bracket = test_data.income_bracket.apply(\n+    lambda value: value.replace(\".\", \"\")\n+)\n+\n+\"\"\"\n+We store the training and test data splits locally as CSV files.\n+\"\"\"\n+\n+train_data_file = \"train_data.csv\"\n+test_data_file = \"test_data.csv\"\n+\n+train_data.to_csv(train_data_file, index=False, header=False)\n+test_data.to_csv(test_data_file, index=False, header=False)\n+\n+\"\"\"\n+## Define dataset metadata\n+\n+Here, we define the metadata of the dataset that will be useful for reading and parsing\n+and encoding input features.\n+\"\"\"\n+\n+# A list of the numerical feature names.\n+NUMERIC_FEATURE_NAMES = [\n+    \"age\",\n+    \"education_num\",\n+    \"capital_gain\",\n+    \"capital_loss\",\n+    \"hours_per_week\",\n+]\n+# A dictionary of the categorical features and their vocabulary.\n+CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n+    \"workclass\": sorted(list(train_data[\"workclass\"].unique())),\n+    \"education\": sorted(list(train_data[\"education\"].unique())),\n+    \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),\n+    \"occupation\": sorted(list(train_data[\"occupation\"].unique())),\n+    \"relationship\": sorted(list(train_data[\"relationship\"].unique())),\n+    \"race\": sorted(list(train_data[\"race\"].unique())),\n+    \"gender\": sorted(list(train_data[\"gender\"].unique())),\n+    \"native_country\": sorted(list(train_data[\"native_country\"].unique())),\n+}\n+# A list of the columns to ignore from the dataset.\n+IGNORE_COLUMN_NAMES = [\"fnlwgt\"]\n+# A list of the categorical feature names.\n+CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n+# A list of all the input features.\n+FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n+# A list of column default values for each feature.\n+COLUMN_DEFAULTS = [\n+    [0.0]\n+    if feature_name in NUMERIC_FEATURE_NAMES + IGNORE_COLUMN_NAMES\n+    else [\"NA\"]\n+    for feature_name in CSV_HEADER\n+]\n+# The name of the target feature.\n+TARGET_FEATURE_NAME = \"income_bracket\"\n+# A list of the labels of the target features.\n+TARGET_LABELS = [\" <=50K\", \" >50K\"]\n+\n+\"\"\"\n+## Create `tf_data.Dataset` objects for training and validation\n+\n+We create an input function to read and parse the file, and convert features and labels\n+into a [`tf_data.Dataset`](https://www.tensorflow.org/guide/datasets)\n+for training and validation. We also preprocess the input by mapping the target label\n+to an index.\n+\"\"\"\n+\n+\n+target_label_lookup = StringLookup(\n+    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n+)\n+\n+\n+lookup_dict = {}\n+for feature_name in CATEGORICAL_FEATURE_NAMES:\n+    vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n+    # Create a lookup to convert a string values to an integer indices.\n+    # Since we are not using a mask token, nor expecting any out of vocabulary\n+    # (oov) token, we set mask_token to None and num_oov_indices to 0.\n+    lookup = StringLookup(\n+        vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n+    )\n+    lookup_dict[feature_name] = lookup\n+\n+\n+def encode_categorical(batch_x, batch_y):\n+    for feature_name in CATEGORICAL_FEATURE_NAMES:\n+        batch_x[feature_name] = lookup_dict[feature_name](batch_x[feature_name])\n+\n+    return batch_x, batch_y\n+\n+\n+def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n+    dataset = (\n+        tf_data.experimental.make_csv_dataset(\n+            csv_file_path,\n+            batch_size=batch_size,\n+            column_names=CSV_HEADER,\n+            column_defaults=COLUMN_DEFAULTS,\n+            label_name=TARGET_FEATURE_NAME,\n+            num_epochs=1,\n+            header=False,\n+            na_value=\"?\",\n+            shuffle=shuffle,\n+        )\n+        .map(lambda features, target: (features, target_label_lookup(target)))\n+        .map(encode_categorical)\n+    )\n+\n+    return dataset.cache()\n+\n+\n+\"\"\"\n+## Create model inputs\n+\"\"\"\n+\n+\n+def create_model_inputs():\n+    inputs = {}\n+    for feature_name in FEATURE_NAMES:\n+        if feature_name in NUMERIC_FEATURE_NAMES:\n+            inputs[feature_name] = layers.Input(\n+                name=feature_name, shape=(), dtype=_dtype\n+            )\n+        else:\n+            inputs[feature_name] = layers.Input(\n+                name=feature_name, shape=(), dtype=\"int32\"\n+            )\n+    return inputs\n+\n+\n+\"\"\"\n+## Encode input features\n+\"\"\"\n+\n+\n+def encode_inputs(inputs):\n+    encoded_features = []\n+    for feature_name in inputs:\n+        if feature_name in CATEGORICAL_FEATURE_NAMES:\n+            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n+            # Create a lookup to convert a string values to an integer indices.\n+            # Since we are not using a mask token, nor expecting any out of vocabulary\n+            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n+            value_index = inputs[feature_name]\n+            embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n+            # Create an embedding layer with the specified dimensions.\n+            embedding = layers.Embedding(\n+                input_dim=lookup.vocabulary_size(), output_dim=embedding_dims\n+            )\n+            # Convert the index values to embedding representations.\n+            encoded_feature = embedding(value_index)\n+        else:\n+            # Use the numerical features as-is.\n+            encoded_feature = inputs[feature_name]\n+            if inputs[feature_name].shape[-1] is None:\n+                encoded_feature = keras.ops.expand_dims(encoded_feature, -1)\n+\n+        encoded_features.append(encoded_feature)\n+\n+    encoded_features = layers.concatenate(encoded_features)\n+    return encoded_features\n+\n+\n+\"\"\"\n+## Deep Neural Decision Tree\n+\n+A neural decision tree model has two sets of weights to learn. The first set is `pi`,\n+which represents the probability distribution of the classes in the tree leaves.\n+The second set is the weights of the routing layer `decision_fn`, which represents the probability\n+of going to each leave. The forward pass of the model works as follows:\n+\n+1. The model expects input `features` as a single vector encoding all the features of an instance\n+in the batch. This vector can be generated from a Convolution Neural Network (CNN) applied to images\n+or dense transformations applied to structured data features.\n+2. The model first applies a `used_features_mask` to randomly select a subset of input features to use.\n+3. Then, the model computes the probabilities (`mu`) for the input instances to reach the tree leaves\n+by iteratively performing a *stochastic* routing throughout the tree levels.\n+4. Finally, the probabilities of reaching the leaves are combined by the class probabilities at the\n+leaves to produce the final `outputs`.\n+\"\"\"\n+\n+\n+class NeuralDecisionTree(keras.Model):\n+    def __init__(self, depth, num_features, used_features_rate, num_classes):\n+        super().__init__()\n+        self.depth = depth\n+        self.num_leaves = 2**depth\n+        self.num_classes = num_classes\n+\n+        # Create a mask for the randomly selected features.\n+        num_used_features = int(num_features * used_features_rate)\n+        one_hot = np.eye(num_features)\n+        sampled_feature_indices = np.random.choice(\n+            np.arange(num_features), num_used_features, replace=False\n+        )\n+        self.used_features_mask = ops.convert_to_tensor(\n+            one_hot[sampled_feature_indices], dtype=_dtype\n+        )\n+\n+        # Initialize the weights of the classes in leaves.\n+        self.pi = self.add_weight(\n+            initializer=\"random_normal\",\n+            shape=[self.num_leaves, self.num_classes],\n+            dtype=_dtype,\n+            trainable=True,\n+        )\n+\n+        # Initialize the stochastic routing layer.\n+        self.decision_fn = layers.Dense(\n+            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n+        )\n+\n+    def call(self, features):\n+        batch_size = ops.shape(features)[0]\n+\n+        # Apply the feature mask to the input features.\n+        features = ops.matmul(\n+            features, ops.transpose(self.used_features_mask)\n+        )  # [batch_size, num_used_features]\n+        # Compute the routing probabilities.\n+        decisions = ops.expand_dims(\n+            self.decision_fn(features), axis=2\n+        )  # [batch_size, num_leaves, 1]\n+        # Concatenate the routing probabilities with their complements.\n+        decisions = layers.concatenate(\n+            [decisions, 1 - decisions], axis=2\n+        )  # [batch_size, num_leaves, 2]\n+\n+        mu = ops.ones([batch_size, 1, 1])\n+\n+        begin_idx = 1\n+        end_idx = 2\n+        # Traverse the tree in breadth-first order.\n+        for level in range(self.depth):\n+            mu = ops.reshape(\n+                mu, [batch_size, -1, 1]\n+            )  # [batch_size, 2 ** level, 1]\n+            mu = ops.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n+            level_decisions = decisions[\n+                :, begin_idx:end_idx, :\n+            ]  # [batch_size, 2 ** level, 2]\n+            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n+            begin_idx = end_idx\n+            end_idx = begin_idx + 2 ** (level + 1)\n+\n+        mu = ops.reshape(\n+            mu, [batch_size, self.num_leaves]\n+        )  # [batch_size, num_leaves]\n+        probabilities = keras.activations.softmax(\n+            self.pi\n+        )  # [num_leaves, num_classes]\n+        outputs = ops.matmul(mu, probabilities)  # [batch_size, num_classes]\n+        return outputs\n+\n+\n+\"\"\"\n+## Deep Neural Decision Forest\n+\n+The neural decision forest model consists of a set of neural decision trees that are\n+trained simultaneously. The output of the forest model is the average outputs of its trees.\n+\"\"\"\n+\n+\n+class NeuralDecisionForest(keras.Model):\n+    def __init__(\n+        self, num_trees, depth, num_features, used_features_rate, num_classes\n+    ):\n+        super().__init__()\n+        self.ensemble = []\n+        # Initialize the ensemble by adding NeuralDecisionTree instances.\n+        # Each tree will have its own randomly selected input features to use.\n+        for _ in range(num_trees):\n+            self.ensemble.append(\n+                NeuralDecisionTree(\n+                    depth, num_features, used_features_rate, num_classes\n+                )\n+            )\n+\n+    def call(self, inputs):\n+        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n+        batch_size = ops.shape(inputs)[0]\n+        outputs = ops.zeros([batch_size, num_classes])\n+\n+        # Aggregate the outputs of trees in the ensemble.\n+        for tree in self.ensemble:\n+            outputs += tree(inputs)\n+        # Divide the outputs by the ensemble size to get the average.\n+        outputs /= len(self.ensemble)\n+        return outputs\n+\n+\n+\"\"\"\n+Finally, let's set up the code that will train and evaluate the model.\n+\"\"\"\n+\n+learning_rate = 0.01\n+batch_size = 265\n+num_epochs = 10\n+\n+\n+def run_experiment(model):\n+    model.compile(\n+        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n+        loss=keras.losses.SparseCategoricalCrossentropy(),\n+        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n+    )\n+\n+    print(\"Start training the model...\")\n+    train_dataset = get_dataset_from_csv(\n+        train_data_file, shuffle=True, batch_size=batch_size\n+    )\n+\n+    model.fit(train_dataset, epochs=num_epochs)\n+    print(\"Model training finished\")\n+\n+    print(\"Evaluating the model on the test data...\")\n+    test_dataset = get_dataset_from_csv(test_data_file, batch_size=batch_size)\n+\n+    _, accuracy = model.evaluate(test_dataset)\n+    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n+\n+\n+\"\"\"\n+## Experiment 1: train a decision tree model\n+\n+In this experiment, we train a single neural decision tree model\n+where we use all input features.\n+\"\"\"\n+\n+num_trees = 10\n+depth = 10\n+used_features_rate = 1.0\n+num_classes = len(TARGET_LABELS)\n+\n+\n+def create_tree_model():\n+    inputs = create_model_inputs()\n+    features = encode_inputs(inputs)\n+    features = layers.BatchNormalization()(features)\n+    num_features = features.shape[1]\n+\n+    tree = NeuralDecisionTree(\n+        depth, num_features, used_features_rate, num_classes\n+    )\n+\n+    outputs = tree(features)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+    return model\n+\n+\n+tree_model = create_tree_model()\n+run_experiment(tree_model)\n+\n+\n+\"\"\"\n+## Experiment 2: train a forest model\n+\n+In this experiment, we train a neural decision forest with `num_trees` trees\n+where each tree uses randomly selected 50% of the input features. You can control the number\n+of features to be used in each tree by setting the `used_features_rate` variable.\n+In addition, we set the depth to 5 instead of 10 compared to the previous experiment.\n+\"\"\"\n+\n+num_trees = 25\n+depth = 5\n+used_features_rate = 0.5\n+\n+\n+def create_forest_model():\n+    inputs = create_model_inputs()\n+    features = encode_inputs(inputs)\n+    features = layers.BatchNormalization()(features)\n+    num_features = features.shape[1]\n+\n+    forest_model = NeuralDecisionForest(\n+        num_trees, depth, num_features, used_features_rate, num_classes\n+    )\n+\n+    outputs = forest_model(features)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+    return model\n+\n+\n+forest_model = create_forest_model()\n+\n+run_experiment(forest_model)\n+\n+\n+\"\"\"\n+You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/neural-decision-forest)\n+and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/Neural-Decision-Forest).\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#96b359d566c0dfaec54349a3555cd0b82013d171", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 14/15 | Churn Δ: 36 | Churn Cumulative: 863 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -64,15 +64,11 @@ class MnistTest(test_util.DTensorBaseTest):\n         self.assertEqual(train_losses, sorted(train_losses, reverse=True))\n \n     def test_model_fit(self):\n-        if self.mesh.device_type() == \"GPU\":\n-            self.skipTest(\"TODO(b/292596476)\")\n-\n         layout_map = layout_map_lib.LayoutMap(self.mesh)\n         with layout_map.scope():\n             model = integration_test_utils.get_model()\n \n         optimizer = adam.Adam(learning_rate=0.001, mesh=self.mesh)\n-        optimizer.build(model.trainable_variables)\n \n         global_batch_size = 64\n         model.compile(\n\n@@ -117,19 +117,18 @@ def reset_logical_devices(device_type, count):\n       device_type: The device_type to reset.\n       count: numbers of virtual device to reset to.\n     \"\"\"\n-    reset_context()\n-    devices = tf.config.list_physical_devices(device_type)\n-    if device_type.upper() == \"CPU\":\n-        tf.config.set_logical_device_configuration(\n-            devices[0],\n-            [\n-                tf.config.LogicalDeviceConfiguration(),\n-            ]\n-            * count,\n+    if device_type.upper() not in [\"CPU\", \"GPU\"]:\n+        raise ValueError(\n+            \"resetting logical device for non-supported device type: \"\n+            f\"{device_type}\"\n         )\n-    elif device_type.upper() == \"GPU\":\n+    reset_context()\n+\n+    cpus = tf.config.list_physical_devices(\"CPU\")\n+    if device_type.upper() == \"GPU\":\n+        gpus = tf.config.list_physical_devices(device_type)\n         tf.config.set_logical_device_configuration(\n-            devices[0],\n+            gpus[0],\n             [\n                 tf.config.LogicalDeviceConfiguration(\n                     memory_limit=_DEFAULT_GPU_MEMORY_LIMIT\n@@ -137,10 +136,13 @@ def reset_logical_devices(device_type, count):\n             ]\n             * count,\n         )\n-    else:\n-        dt = device_type\n-        raise ValueError(\n-            f\"resetting logical device for non-supported device type: {dt}\"\n+    # Always config CPU mesh as the host mesh for DTensor\n+    tf.config.set_logical_device_configuration(\n+        cpus[0],\n+        [\n+            tf.config.LogicalDeviceConfiguration(),\n+        ]\n+        * count,\n     )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#42c3aefa29952a9a29ef0baa468c3fcad6d589fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 40 | Files Changed: 3 | Hunks: 26 | Methods Changed: 10 | Complexity Δ (Sum/Max): 13/11 | Churn Δ: 87 | Churn Cumulative: 471 | Contributors (this commit): 5 | Commits (past 90d): 18 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,5 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n \n from keras_core import testing\n from keras_core.layers.activations import elu\n@@ -21,11 +20,12 @@ class ELUTest(testing.TestCase):\n         )\n \n     def test_correctness(self):\n+        def np_elu(x, alpha=1.0):\n+            return (x > 0) * x + (x <= 0) * alpha * (np.exp(x) - 1)\n+\n         x = np.random.random((2, 2, 5))\n         elu_layer = elu.ELU()\n-        tf_elu_layer = tf.keras.layers.ELU()\n-        self.assertAllClose(elu_layer(x), tf_elu_layer(x))\n+        self.assertAllClose(elu_layer(x), np_elu(x))\n \n         elu_layer = elu.ELU(alpha=0.7)\n-        tf_elu_layer = tf.keras.layers.ELU(alpha=0.7)\n-        self.assertAllClose(elu_layer(x), tf_elu_layer(x))\n+        self.assertAllClose(elu_layer(x), np_elu(x, alpha=0.7))\n\n@@ -1,6 +1,5 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n \n from keras_core import testing\n from keras_core.layers.activations import prelu\n@@ -22,6 +21,9 @@ class PReLUTest(testing.TestCase):\n         )\n \n     def test_prelu_correctness(self):\n+        def np_prelu(x, alpha):\n+            return (x > 0) * x + (x <= 0) * alpha * x\n+\n         inputs = np.random.randn(2, 10, 5, 3)\n         prelu_layer = prelu.PReLU(\n             alpha_initializer=\"glorot_uniform\",\n@@ -29,18 +31,9 @@ class PReLUTest(testing.TestCase):\n             alpha_constraint=\"non_neg\",\n             shared_axes=(1, 2),\n         )\n-        tf_prelu_layer = tf.keras.layers.PReLU(\n-            alpha_initializer=\"glorot_uniform\",\n-            alpha_regularizer=\"l1\",\n-            alpha_constraint=\"non_neg\",\n-            shared_axes=(1, 2),\n-        )\n-\n         prelu_layer.build(inputs.shape)\n-        tf_prelu_layer.build(inputs.shape)\n \n         weights = np.random.random((1, 1, 3))\n         prelu_layer.alpha.assign(weights)\n-        tf_prelu_layer.alpha.assign(weights)\n-\n-        self.assertAllClose(prelu_layer(inputs), tf_prelu_layer(inputs))\n+        ref_out = np_prelu(inputs, weights)\n+        self.assertAllClose(prelu_layer(inputs), ref_out)\n\n@@ -1,6 +1,5 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n from absl.testing import parameterized\n \n from keras_core import layers\n@@ -95,21 +94,30 @@ class GlobalAveragePoolingCorrectnessTest(\n         (\"channels_last\", False),\n         (\"channels_last\", True),\n         (\"channels_first\", False),\n+        (\"channels_first\", True),\n     )\n     def test_global_average_pooling1d(self, data_format, keepdims):\n-        inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n+        def np_gap1d(x, data_format, keepdims, mask=None):\n+            steps_axis = 1 if data_format == \"channels_last\" else 2\n+            if mask is not None:\n+                mask = np.expand_dims(\n+                    mask, 2 if data_format == \"channels_last\" else 1\n+                )\n+                x *= mask\n+                res = np.sum(x, axis=steps_axis) / np.sum(mask, axis=steps_axis)\n+            else:\n+                res = np.mean(x, axis=steps_axis)\n+            if keepdims:\n+                res = np.expand_dims(res, axis=steps_axis)\n+            return res\n \n+        inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n         layer = layers.GlobalAveragePooling1D(\n             data_format=data_format,\n             keepdims=keepdims,\n         )\n-        tf_keras_layer = tf.keras.layers.GlobalAveragePooling1D(\n-            data_format=data_format,\n-            keepdims=keepdims,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = np_gap1d(inputs, data_format, keepdims)\n         self.assertAllClose(outputs, expected)\n \n         if data_format == \"channels_last\":\n@@ -117,47 +125,53 @@ class GlobalAveragePoolingCorrectnessTest(\n         else:\n             mask = np.array([[1, 1, 0, 0], [0, 1, 0, 1]], dtype=\"int32\")\n         outputs = layer(inputs, mask)\n-        expected = tf_keras_layer(inputs, mask)\n+        expected = np_gap1d(inputs, data_format, keepdims, mask)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (\"channels_last\", False),\n         (\"channels_last\", True),\n         (\"channels_first\", False),\n+        (\"channels_first\", True),\n     )\n     def test_global_average_pooling2d(self, data_format, keepdims):\n-        inputs = np.arange(96, dtype=\"float32\").reshape((2, 3, 4, 4))\n+        def np_gap2d(x, data_format, keepdims):\n+            steps_axis = [1, 2] if data_format == \"channels_last\" else [2, 3]\n+            res = np.apply_over_axes(np.mean, x, steps_axis)\n+            if not keepdims:\n+                res = res.squeeze()\n+            return res\n \n+        inputs = np.arange(96, dtype=\"float32\").reshape((2, 3, 4, 4))\n         layer = layers.GlobalAveragePooling2D(\n             data_format=data_format,\n             keepdims=keepdims,\n         )\n-        tf_keras_layer = tf.keras.layers.GlobalAveragePooling2D(\n-            data_format=data_format,\n-            keepdims=keepdims,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = np_gap2d(inputs, data_format, keepdims)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (\"channels_last\", False),\n         (\"channels_last\", True),\n         (\"channels_first\", False),\n+        (\"channels_first\", True),\n     )\n     def test_global_average_pooling3d(self, data_format, keepdims):\n-        inputs = np.arange(360, dtype=\"float32\").reshape((2, 3, 3, 5, 4))\n+        def np_gap3d(x, data_format, keepdims):\n+            steps_axis = (\n+                [1, 2, 3] if data_format == \"channels_last\" else [2, 3, 4]\n+            )\n+            res = np.apply_over_axes(np.mean, x, steps_axis)\n+            if not keepdims:\n+                res = res.squeeze()\n+            return res\n \n+        inputs = np.arange(360, dtype=\"float32\").reshape((2, 3, 3, 5, 4))\n         layer = layers.GlobalAveragePooling3D(\n             data_format=data_format,\n             keepdims=keepdims,\n         )\n-        tf_keras_layer = tf.keras.layers.GlobalAveragePooling3D(\n-            data_format=data_format,\n-            keepdims=keepdims,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = np_gap3d(inputs, data_format, keepdims)\n         self.assertAllClose(outputs, expected)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1b7847e9abd752d644a1b1a155d76f246f9de65", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 20 | Churn Cumulative: 742 | Contributors (this commit): 5 | Commits (past 90d): 13 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -271,6 +271,8 @@ class ExtractPatches(Operation):\n         data_format=\"channels_last\",\n     ):\n         super().__init__()\n+        if isinstance(size, int):\n+            size = (size, size)\n         self.size = size\n         self.strides = strides\n         self.dilation_rate = dilation_rate\n@@ -348,14 +350,16 @@ def extract_patches(\n \n     Examples:\n \n-    >>> image = np.random.random((1, 20, 20, 3)) # batch of 2 RGB images\n+    >>> image = np.random.random(\n+    ...     (2, 20, 20, 3)\n+    ... ).astype(\"float32\") # batch of 2 RGB images\n     >>> patches = keras_core.ops.image.extract_patches(image, (5, 5))\n     >>> patches.shape\n-    (1, 4, 4, 75)\n-    >>> image = np.random.random((20, 20, 3)) # batch of 2 RGB images\n+    (2, 4, 4, 75)\n+    >>> image = np.random.random((20, 20, 3)).astype(\"float32\") # 1 RGB image\n     >>> patches = keras_core.ops.image.extract_patches(image, (3, 3), (1, 1))\n     >>> patches.shape\n-    (4, 4, 75)\n+    (18, 18, 27)\n     \"\"\"\n     if any_symbolic_tensors((image,)):\n         return ExtractPatches(\n\n@@ -31,6 +31,8 @@ class ImageOpsDynamicShapeTest(testing.TestCase):\n         p_h, p_w = 5, 5\n         out = kimage.extract_patches(x, (p_h, p_w))\n         self.assertEqual(out.shape, (None, 4, 4, 75))\n+        out = kimage.extract_patches(x, 5)\n+        self.assertEqual(out.shape, (None, 4, 4, 75))\n \n \n class ImageOpsStaticShapeTest(testing.TestCase):\n@@ -50,6 +52,8 @@ class ImageOpsStaticShapeTest(testing.TestCase):\n         p_h, p_w = 5, 5\n         out = kimage.extract_patches(x, (p_h, p_w))\n         self.assertEqual(out.shape, (4, 4, 75))\n+        out = kimage.extract_patches(x, 5)\n+        self.assertEqual(out.shape, (4, 4, 75))\n \n \n AFFINE_TRANSFORM_INTERPOLATIONS = {  # map to order\n@@ -310,9 +314,7 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             and backend.backend() == \"tensorflow\"\n             and dilation_rate > 1\n         ):\n-            pytest.skip(\n-                \"dilation_rate>1 with strides>1 than not supported with TF\"\n-            )\n+            pytest.skip(\"dilation_rate>1 with strides>1 not supported with TF\")\n         if data_format == \"channels_first\":\n             image = np.random.uniform(size=(1, 3, 20, 20))\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b716a08fb280984d95c9b0a922064322e300d999", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1353 | Contributors (this commit): 8 | Commits (past 90d): 13 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -391,7 +391,6 @@ class TestTensorBoardV2(testing.TestCase):\n             },\n         )\n         expected_image_summaries = {\n-            _ObservedSummary(logdir=train_dir, tag=\"image\"),\n             _ObservedSummary(logdir=train_dir, tag=\"bias/image\"),\n             _ObservedSummary(logdir=train_dir, tag=\"kernel/image\"),\n         }\n\n@@ -87,12 +87,14 @@ class Dense(Layer):\n     def build(self, input_shape):\n         input_dim = input_shape[-1]\n         self.kernel = self.add_weight(\n+            name=\"kernel\",\n             shape=(input_dim, self.units),\n             initializer=self.kernel_initializer,\n             regularizer=self.kernel_regularizer,\n         )\n         if self.use_bias:\n             self.bias = self.add_weight(\n+                name=\"bias\",\n                 shape=(self.units,),\n                 initializer=self.bias_initializer,\n                 regularizer=self.bias_regularizer,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#871f03e14a9b75aabfdfded0ca7478b763dbd15d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 219 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -76,4 +76,6 @@ class LearningRateScheduler(Callback):\n \n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}\n-        logs[\"learning_rate\"] = self.model.optimizer.learning_rate.value\n+        logs[\"learning_rate\"] = float(\n+            backend.convert_to_numpy(self.model.optimizer.learning_rate)\n+        )\n\n@@ -107,3 +107,18 @@ class LearningRateSchedulerTest(testing.TestCase):\n                 callbacks=[lr_scheduler],\n                 epochs=2,\n             )\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_learning_rate_in_history(self):\n+        lr_scheduler = callbacks.LearningRateScheduler(lambda step, lr: 0.5)\n+\n+        history = self.model.fit(\n+            self.x_train,\n+            self.y_train,\n+            callbacks=[lr_scheduler],\n+            epochs=1,\n+        )\n+\n+        self.assertTrue(\"learning_rate\" in history.history)\n+        self.assertEqual(type(history.history[\"learning_rate\"][0]), float)\n+        self.assertEqual(history.history[\"learning_rate\"][0], 0.5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#710c933c8f9b0448cb43ffff21c94aeb21fafa68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 662 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ import string\n \n import numpy as np\n \n+from keras_core import backend\n from keras_core import constraints\n from keras_core import initializers\n from keras_core import ops\n@@ -115,6 +116,8 @@ class MultiHeadAttention(Layer):\n         self.supports_masking = True\n         self._num_heads = num_heads\n         self._key_dim = key_dim\n+        # Cache 1.0 / math.sqrt(self._key_dim).\n+        self._inverse_sqrt_key_dim = None\n         self._value_dim = value_dim if value_dim else key_dim\n         self._dropout = dropout\n         self._use_bias = use_bias\n@@ -311,6 +314,9 @@ class MultiHeadAttention(Layer):\n         )\n         self._softmax = Softmax(axis=norm_axes)\n         self._dropout_layer = Dropout(rate=self._dropout)\n+        self._inverse_sqrt_key_dim = backend.convert_to_tensor(\n+            1.0 / math.sqrt(float(self._key_dim))\n+        )\n \n     def _masked_softmax(self, attention_scores, attention_mask=None):\n         # Normalize the attention scores to probabilities.\n@@ -355,7 +361,7 @@ class MultiHeadAttention(Layer):\n         # Note: Applying scalar multiply at the smaller end of einsum improves\n         # XLA performance, but may introduce slight numeric differences in\n         # the Transformer attention head.\n-        query = ops.multiply(query, 1.0 / math.sqrt(float(self._key_dim)))\n+        query = ops.multiply(query, self._inverse_sqrt_key_dim)\n \n         # Take the dot product between \"query\" and \"key\" to get the raw\n         # attention scores.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1ffa864cfe61258699a82494a4c1d2e66f6d6384", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 1927 | Files Changed: 16 | Hunks: 55 | Methods Changed: 58 | Complexity Δ (Sum/Max): -59/1 | Churn Δ: 2003 | Churn Cumulative: 7693 | Contributors (this commit): 18 | Commits (past 90d): 84 | Contributors (cumulative): 58 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,484 +0,0 @@\n-\"\"\"\n-Title: Classification with Neural Decision Forests\n-Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n-Date created: 2021/01/15\n-Last modified: 2021/01/15\n-Description: How to train differentiable decision trees for end-to-end learning in deep neural networks.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-This example provides an implementation of the\n-[Deep Neural Decision Forest](https://ieeexplore.ieee.org/document/7410529)\n-model introduced by P. Kontschieder et al. for structured data classification.\n-It demonstrates how to build a stochastic and differentiable decision tree model,\n-train it end-to-end, and unify decision trees with deep representation learning.\n-\n-## The dataset\n-\n-This example uses the\n-[United States Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/census+income)\n-provided by the\n-[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).\n-The task is binary classification\n-to predict whether a person is likely to be making over USD 50,000 a year.\n-\n-The dataset includes 48,842 instances with 14 input features (such as age, work class, education, occupation, and so on): 5 numerical features\n-and 9 categorical features.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import keras_core as keras\n-from keras_core import layers\n-from keras_core.layers import StringLookup\n-from keras_core import ops\n-\n-\n-from tensorflow import data as tf_data\n-import numpy as np\n-import pandas as pd\n-\n-import math\n-\n-\n-_dtype = \"float32\"\n-\n-\"\"\"\n-## Prepare the data\n-\"\"\"\n-\n-CSV_HEADER = [\n-    \"age\",\n-    \"workclass\",\n-    \"fnlwgt\",\n-    \"education\",\n-    \"education_num\",\n-    \"marital_status\",\n-    \"occupation\",\n-    \"relationship\",\n-    \"race\",\n-    \"gender\",\n-    \"capital_gain\",\n-    \"capital_loss\",\n-    \"hours_per_week\",\n-    \"native_country\",\n-    \"income_bracket\",\n-]\n-\n-train_data_url = (\n-    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n-)\n-train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n-\n-test_data_url = (\n-    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n-)\n-test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)\n-\n-print(f\"Train dataset shape: {train_data.shape}\")\n-print(f\"Test dataset shape: {test_data.shape}\")\n-\n-\"\"\"\n-Remove the first record (because it is not a valid data example) and a trailing\n-'dot' in the class labels.\n-\"\"\"\n-\n-test_data = test_data[1:]\n-test_data.income_bracket = test_data.income_bracket.apply(\n-    lambda value: value.replace(\".\", \"\")\n-)\n-\n-\"\"\"\n-We store the training and test data splits locally as CSV files.\n-\"\"\"\n-\n-train_data_file = \"train_data.csv\"\n-test_data_file = \"test_data.csv\"\n-\n-train_data.to_csv(train_data_file, index=False, header=False)\n-test_data.to_csv(test_data_file, index=False, header=False)\n-\n-\"\"\"\n-## Define dataset metadata\n-\n-Here, we define the metadata of the dataset that will be useful for reading and parsing\n-and encoding input features.\n-\"\"\"\n-\n-# A list of the numerical feature names.\n-NUMERIC_FEATURE_NAMES = [\n-    \"age\",\n-    \"education_num\",\n-    \"capital_gain\",\n-    \"capital_loss\",\n-    \"hours_per_week\",\n-]\n-# A dictionary of the categorical features and their vocabulary.\n-CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n-    \"workclass\": sorted(list(train_data[\"workclass\"].unique())),\n-    \"education\": sorted(list(train_data[\"education\"].unique())),\n-    \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),\n-    \"occupation\": sorted(list(train_data[\"occupation\"].unique())),\n-    \"relationship\": sorted(list(train_data[\"relationship\"].unique())),\n-    \"race\": sorted(list(train_data[\"race\"].unique())),\n-    \"gender\": sorted(list(train_data[\"gender\"].unique())),\n-    \"native_country\": sorted(list(train_data[\"native_country\"].unique())),\n-}\n-# A list of the columns to ignore from the dataset.\n-IGNORE_COLUMN_NAMES = [\"fnlwgt\"]\n-# A list of the categorical feature names.\n-CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n-# A list of all the input features.\n-FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n-# A list of column default values for each feature.\n-COLUMN_DEFAULTS = [\n-    [0.0]\n-    if feature_name in NUMERIC_FEATURE_NAMES + IGNORE_COLUMN_NAMES\n-    else [\"NA\"]\n-    for feature_name in CSV_HEADER\n-]\n-# The name of the target feature.\n-TARGET_FEATURE_NAME = \"income_bracket\"\n-# A list of the labels of the target features.\n-TARGET_LABELS = [\" <=50K\", \" >50K\"]\n-\n-\"\"\"\n-## Create `tf_data.Dataset` objects for training and validation\n-\n-We create an input function to read and parse the file, and convert features and labels\n-into a [`tf_data.Dataset`](https://www.tensorflow.org/guide/datasets)\n-for training and validation. We also preprocess the input by mapping the target label\n-to an index.\n-\"\"\"\n-\n-\n-target_label_lookup = StringLookup(\n-    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n-)\n-\n-\n-lookup_dict = {}\n-for feature_name in CATEGORICAL_FEATURE_NAMES:\n-    vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n-    # Create a lookup to convert a string values to an integer indices.\n-    # Since we are not using a mask token, nor expecting any out of vocabulary\n-    # (oov) token, we set mask_token to None and num_oov_indices to 0.\n-    lookup = StringLookup(\n-        vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n-    )\n-    lookup_dict[feature_name] = lookup\n-\n-\n-def encode_categorical(batch_x, batch_y):\n-    for feature_name in CATEGORICAL_FEATURE_NAMES:\n-        batch_x[feature_name] = lookup_dict[feature_name](batch_x[feature_name])\n-\n-    return batch_x, batch_y\n-\n-\n-def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n-    dataset = (\n-        tf_data.experimental.make_csv_dataset(\n-            csv_file_path,\n-            batch_size=batch_size,\n-            column_names=CSV_HEADER,\n-            column_defaults=COLUMN_DEFAULTS,\n-            label_name=TARGET_FEATURE_NAME,\n-            num_epochs=1,\n-            header=False,\n-            na_value=\"?\",\n-            shuffle=shuffle,\n-        )\n-        .map(lambda features, target: (features, target_label_lookup(target)))\n-        .map(encode_categorical)\n-    )\n-\n-    return dataset.cache()\n-\n-\n-\"\"\"\n-## Create model inputs\n-\"\"\"\n-\n-\n-def create_model_inputs():\n-    inputs = {}\n-    for feature_name in FEATURE_NAMES:\n-        if feature_name in NUMERIC_FEATURE_NAMES:\n-            inputs[feature_name] = layers.Input(\n-                name=feature_name, shape=(), dtype=_dtype\n-            )\n-        else:\n-            inputs[feature_name] = layers.Input(\n-                name=feature_name, shape=(), dtype=\"int32\"\n-            )\n-    return inputs\n-\n-\n-\"\"\"\n-## Encode input features\n-\"\"\"\n-\n-\n-def encode_inputs(inputs):\n-    encoded_features = []\n-    for feature_name in inputs:\n-        if feature_name in CATEGORICAL_FEATURE_NAMES:\n-            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n-            # Create a lookup to convert a string values to an integer indices.\n-            # Since we are not using a mask token, nor expecting any out of vocabulary\n-            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n-            value_index = inputs[feature_name]\n-            embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n-            # Create an embedding layer with the specified dimensions.\n-            embedding = layers.Embedding(\n-                input_dim=lookup.vocabulary_size(), output_dim=embedding_dims\n-            )\n-            # Convert the index values to embedding representations.\n-            encoded_feature = embedding(value_index)\n-        else:\n-            # Use the numerical features as-is.\n-            encoded_feature = inputs[feature_name]\n-            if inputs[feature_name].shape[-1] is None:\n-                encoded_feature = keras.ops.expand_dims(encoded_feature, -1)\n-\n-        encoded_features.append(encoded_feature)\n-\n-    encoded_features = layers.concatenate(encoded_features)\n-    return encoded_features\n-\n-\n-\"\"\"\n-## Deep Neural Decision Tree\n-\n-A neural decision tree model has two sets of weights to learn. The first set is `pi`,\n-which represents the probability distribution of the classes in the tree leaves.\n-The second set is the weights of the routing layer `decision_fn`, which represents the probability\n-of going to each leave. The forward pass of the model works as follows:\n-\n-1. The model expects input `features` as a single vector encoding all the features of an instance\n-in the batch. This vector can be generated from a Convolution Neural Network (CNN) applied to images\n-or dense transformations applied to structured data features.\n-2. The model first applies a `used_features_mask` to randomly select a subset of input features to use.\n-3. Then, the model computes the probabilities (`mu`) for the input instances to reach the tree leaves\n-by iteratively performing a *stochastic* routing throughout the tree levels.\n-4. Finally, the probabilities of reaching the leaves are combined by the class probabilities at the\n-leaves to produce the final `outputs`.\n-\"\"\"\n-\n-\n-class NeuralDecisionTree(keras.Model):\n-    def __init__(self, depth, num_features, used_features_rate, num_classes):\n-        super().__init__()\n-        self.depth = depth\n-        self.num_leaves = 2**depth\n-        self.num_classes = num_classes\n-\n-        # Create a mask for the randomly selected features.\n-        num_used_features = int(num_features * used_features_rate)\n-        one_hot = np.eye(num_features)\n-        sampled_feature_indices = np.random.choice(\n-            np.arange(num_features), num_used_features, replace=False\n-        )\n-        self.used_features_mask = ops.convert_to_tensor(\n-            one_hot[sampled_feature_indices], dtype=_dtype\n-        )\n-\n-        # Initialize the weights of the classes in leaves.\n-        self.pi = self.add_weight(\n-            initializer=\"random_normal\",\n-            shape=[self.num_leaves, self.num_classes],\n-            dtype=_dtype,\n-            trainable=True,\n-        )\n-\n-        # Initialize the stochastic routing layer.\n-        self.decision_fn = layers.Dense(\n-            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n-        )\n-\n-    def call(self, features):\n-        batch_size = ops.shape(features)[0]\n-\n-        # Apply the feature mask to the input features.\n-        features = ops.matmul(\n-            features, ops.transpose(self.used_features_mask)\n-        )  # [batch_size, num_used_features]\n-        # Compute the routing probabilities.\n-        decisions = ops.expand_dims(\n-            self.decision_fn(features), axis=2\n-        )  # [batch_size, num_leaves, 1]\n-        # Concatenate the routing probabilities with their complements.\n-        decisions = layers.concatenate(\n-            [decisions, 1 - decisions], axis=2\n-        )  # [batch_size, num_leaves, 2]\n-\n-        mu = ops.ones([batch_size, 1, 1])\n-\n-        begin_idx = 1\n-        end_idx = 2\n-        # Traverse the tree in breadth-first order.\n-        for level in range(self.depth):\n-            mu = ops.reshape(\n-                mu, [batch_size, -1, 1]\n-            )  # [batch_size, 2 ** level, 1]\n-            mu = ops.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n-            level_decisions = decisions[\n-                :, begin_idx:end_idx, :\n-            ]  # [batch_size, 2 ** level, 2]\n-            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n-            begin_idx = end_idx\n-            end_idx = begin_idx + 2 ** (level + 1)\n-\n-        mu = ops.reshape(\n-            mu, [batch_size, self.num_leaves]\n-        )  # [batch_size, num_leaves]\n-        probabilities = keras.activations.softmax(\n-            self.pi\n-        )  # [num_leaves, num_classes]\n-        outputs = ops.matmul(mu, probabilities)  # [batch_size, num_classes]\n-        return outputs\n-\n-\n-\"\"\"\n-## Deep Neural Decision Forest\n-\n-The neural decision forest model consists of a set of neural decision trees that are\n-trained simultaneously. The output of the forest model is the average outputs of its trees.\n-\"\"\"\n-\n-\n-class NeuralDecisionForest(keras.Model):\n-    def __init__(\n-        self, num_trees, depth, num_features, used_features_rate, num_classes\n-    ):\n-        super().__init__()\n-        self.ensemble = []\n-        # Initialize the ensemble by adding NeuralDecisionTree instances.\n-        # Each tree will have its own randomly selected input features to use.\n-        for _ in range(num_trees):\n-            self.ensemble.append(\n-                NeuralDecisionTree(\n-                    depth, num_features, used_features_rate, num_classes\n-                )\n-            )\n-\n-    def call(self, inputs):\n-        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n-        batch_size = ops.shape(inputs)[0]\n-        outputs = ops.zeros([batch_size, num_classes])\n-\n-        # Aggregate the outputs of trees in the ensemble.\n-        for tree in self.ensemble:\n-            outputs += tree(inputs)\n-        # Divide the outputs by the ensemble size to get the average.\n-        outputs /= len(self.ensemble)\n-        return outputs\n-\n-\n-\"\"\"\n-Finally, let's set up the code that will train and evaluate the model.\n-\"\"\"\n-\n-learning_rate = 0.01\n-batch_size = 265\n-num_epochs = 10\n-\n-\n-def run_experiment(model):\n-    model.compile(\n-        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n-        loss=keras.losses.SparseCategoricalCrossentropy(),\n-        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n-    )\n-\n-    print(\"Start training the model...\")\n-    train_dataset = get_dataset_from_csv(\n-        train_data_file, shuffle=True, batch_size=batch_size\n-    )\n-\n-    model.fit(train_dataset, epochs=num_epochs)\n-    print(\"Model training finished\")\n-\n-    print(\"Evaluating the model on the test data...\")\n-    test_dataset = get_dataset_from_csv(test_data_file, batch_size=batch_size)\n-\n-    _, accuracy = model.evaluate(test_dataset)\n-    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n-\n-\n-\"\"\"\n-## Experiment 1: train a decision tree model\n-\n-In this experiment, we train a single neural decision tree model\n-where we use all input features.\n-\"\"\"\n-\n-num_trees = 10\n-depth = 10\n-used_features_rate = 1.0\n-num_classes = len(TARGET_LABELS)\n-\n-\n-def create_tree_model():\n-    inputs = create_model_inputs()\n-    features = encode_inputs(inputs)\n-    features = layers.BatchNormalization()(features)\n-    num_features = features.shape[1]\n-\n-    tree = NeuralDecisionTree(\n-        depth, num_features, used_features_rate, num_classes\n-    )\n-\n-    outputs = tree(features)\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-    return model\n-\n-\n-tree_model = create_tree_model()\n-run_experiment(tree_model)\n-\n-\n-\"\"\"\n-## Experiment 2: train a forest model\n-\n-In this experiment, we train a neural decision forest with `num_trees` trees\n-where each tree uses randomly selected 50% of the input features. You can control the number\n-of features to be used in each tree by setting the `used_features_rate` variable.\n-In addition, we set the depth to 5 instead of 10 compared to the previous experiment.\n-\"\"\"\n-\n-num_trees = 25\n-depth = 5\n-used_features_rate = 0.5\n-\n-\n-def create_forest_model():\n-    inputs = create_model_inputs()\n-    features = encode_inputs(inputs)\n-    features = layers.BatchNormalization()(features)\n-    num_features = features.shape[1]\n-\n-    forest_model = NeuralDecisionForest(\n-        num_trees, depth, num_features, used_features_rate, num_classes\n-    )\n-\n-    outputs = forest_model(features)\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-    return model\n-\n-\n-forest_model = create_forest_model()\n-\n-run_experiment(forest_model)\n-\n-\n-\"\"\"\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/neural-decision-forest)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/Neural-Decision-Forest).\n-\"\"\"\n\n@@ -1,443 +0,0 @@\n-\"\"\"\n-Title: Self-supervised contrastive learning with SimSiam\n-Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Date created: 2021/03/19\n-Last modified: 2021/03/20\n-Description: Implementation of a self-supervised learning method for computer vision.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-Self-supervised learning (SSL) is an interesting branch of study in the field of\n-representation learning. SSL systems try to formulate a supervised signal from a corpus\n-of unlabeled data points.  An example is we train a deep neural network to predict the\n-next word from a given set of words. In literature, these tasks are known as *pretext\n-tasks* or *auxiliary tasks*. If we [train such a network](https://arxiv.org/abs/1801.06146) on a huge dataset (such as\n-the [Wikipedia text corpus](https://www.corpusdata.org/wikipedia.asp)) it learns very effective\n-representations that transfer well to downstream tasks. Language models like\n-[BERT](https://arxiv.org/abs/1810.04805), [GPT-3](https://arxiv.org/abs/2005.14165),\n-[ELMo](https://allennlp.org/elmo) all benefit from this.\n-\n-Much like the language models we can train computer vision models using similar\n-approaches. To make things work in computer vision, we need to formulate the learning\n-tasks such that the underlying model (a deep neural network) is able to make sense of the\n-semantic information present in vision data. One such task is to a model to _contrast_\n-between two different versions of the same image. The hope is that in this way the model\n-will have learn representations where the similar images are grouped as together possible\n-while the dissimilar images are further away.\n-\n-In this example, we will be implementing one such system called **SimSiam** proposed in\n-[Exploring Simple Siamese Representation Learning](https://arxiv.org/abs/2011.10566). It\n-is implemented as the following:\n-\n-1. We create two different versions of the same dataset with a stochastic data\n-augmentation pipeline. Note that the random initialization seed needs to be the same\n-during create these versions.\n-2. We take a ResNet without any classification head (**backbone**) and we add a shallow\n-fully-connected network (**projection head**) on top of it. Collectively, this is known\n-as the **encoder**.\n-3. We pass the output of the encoder through a **predictor** which is again a shallow\n-fully-connected network having an\n-[AutoEncoder](https://en.wikipedia.org/wiki/Autoencoder) like structure.\n-4. We then train our encoder to maximize the cosine similarity between the two different\n-versions of our dataset.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-os.environ['KERAS_BACKEND'] = 'tensorflow'\n-\n-from keras_core import layers\n-from keras_core import regularizers\n-import keras_core as keras\n-import tensorflow as tf\n-\n-import matplotlib.pyplot as plt\n-import numpy as np\n-\n-\"\"\"\n-## Define hyperparameters\n-\"\"\"\n-\n-AUTO = tf.data.AUTOTUNE\n-BATCH_SIZE = 128\n-EPOCHS = 5\n-CROP_TO = 32\n-SEED = 26\n-\n-PROJECT_DIM = 2048\n-LATENT_DIM = 512\n-WEIGHT_DECAY = 0.0005\n-\n-\"\"\"\n-## Load the CIFAR-10 dataset\n-\"\"\"\n-\n-(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n-print(f\"Total training examples: {len(x_train)}\")\n-print(f\"Total test examples: {len(x_test)}\")\n-\n-\"\"\"\n-## Defining our data augmentation pipeline\n-\n-As studied in [SimCLR](https://arxiv.org/abs/2002.05709) having the right data\n-augmentation pipeline is critical for SSL systems to work effectively in computer vision.\n-Two particular augmentation transforms that seem to matter the most are: 1.) Random\n-resized crops and 2.) Color distortions. Most of the other SSL systems for computer\n-vision (such as [BYOL](https://arxiv.org/abs/2006.07733),\n-[MoCoV2](https://arxiv.org/abs/2003.04297), [SwAV](https://arxiv.org/abs/2006.09882),\n-etc.) include these in their training pipelines.\n-\"\"\"\n-\n-\n-def flip_random_crop(image):\n-    # With random crops we also apply horizontal flipping.\n-    image = tf.image.random_flip_left_right(image)\n-    image = tf.image.random_crop(image, (CROP_TO, CROP_TO, 3))\n-    return image\n-\n-\n-def color_jitter(x, strength=[0.4, 0.4, 0.4, 0.1]):\n-    x = tf.image.random_brightness(x, max_delta=0.8 * strength[0])\n-    x = tf.image.random_contrast(\n-        x, lower=1 - 0.8 * strength[1], upper=1 + 0.8 * strength[1]\n-    )\n-    x = tf.image.random_saturation(\n-        x, lower=1 - 0.8 * strength[2], upper=1 + 0.8 * strength[2]\n-    )\n-    x = tf.image.random_hue(x, max_delta=0.2 * strength[3])\n-    # Affine transformations can disturb the natural range of\n-    # RGB images, hence this is needed.\n-    x = tf.clip_by_value(x, 0, 255)\n-    return x\n-\n-\n-def color_drop(x):\n-    x = tf.image.rgb_to_grayscale(x)\n-    x = tf.tile(x, [1, 1, 3])\n-    return x\n-\n-\n-def random_apply(func, x, p):\n-    if tf.random.uniform([], minval=0, maxval=1) < p:\n-        return func(x)\n-    else:\n-        return x\n-\n-\n-def custom_augment(image):\n-    # As discussed in the SimCLR paper, the series of augmentation\n-    # transformations (except for random crops) need to be applied\n-    # randomly to impose translational invariance.\n-    image = flip_random_crop(image)\n-    image = random_apply(color_jitter, image, p=0.8)\n-    image = random_apply(color_drop, image, p=0.2)\n-    return image\n-\n-\n-\"\"\"\n-It should be noted that an augmentation pipeline is generally dependent on various\n-properties of the dataset we are dealing with. For example, if images in the dataset are\n-heavily object-centric then taking random crops with a very high probability may hurt the\n-training performance.\n-\n-Let's now apply our augmentation pipeline to our dataset and visualize a few outputs.\n-\"\"\"\n-\n-\"\"\"\n-## Convert the data into TensorFlow `Dataset` objects\n-\n-Here we create two different versions of our dataset *without* any ground-truth labels.\n-\"\"\"\n-\n-ssl_ds_one = tf.data.Dataset.from_tensor_slices(x_train)\n-ssl_ds_one = (\n-    ssl_ds_one.shuffle(1024, seed=SEED)\n-    .map(custom_augment, num_parallel_calls=AUTO)\n-    .batch(BATCH_SIZE)\n-    .prefetch(AUTO)\n-)\n-\n-ssl_ds_two = tf.data.Dataset.from_tensor_slices(x_train)\n-ssl_ds_two = (\n-    ssl_ds_two.shuffle(1024, seed=SEED)\n-    .map(custom_augment, num_parallel_calls=AUTO)\n-    .batch(BATCH_SIZE)\n-    .prefetch(AUTO)\n-)\n-\n-# We then zip both of these datasets.\n-ssl_ds = tf.data.Dataset.zip((ssl_ds_one, ssl_ds_two))\n-\n-# Visualize a few augmented images.\n-sample_images_one = next(iter(ssl_ds_one))\n-plt.figure(figsize=(10, 10))\n-for n in range(25):\n-    ax = plt.subplot(5, 5, n + 1)\n-    plt.imshow(sample_images_one[n].numpy().astype(\"int\"))\n-    plt.axis(\"off\")\n-plt.show()\n-\n-# Ensure that the different versions of the dataset actually contain\n-# identical images.\n-sample_images_two = next(iter(ssl_ds_two))\n-plt.figure(figsize=(10, 10))\n-for n in range(25):\n-    ax = plt.subplot(5, 5, n + 1)\n-    plt.imshow(sample_images_two[n].numpy().astype(\"int\"))\n-    plt.axis(\"off\")\n-plt.show()\n-\n-\"\"\"\n-Notice that the images in `samples_images_one` and `sample_images_two` are essentially\n-the same but are augmented differently.\n-\"\"\"\n-\n-\"\"\"\n-## Defining the encoder and the predictor\n-\n-We use an implementation of ResNet20 that is specifically configured for the CIFAR10\n-dataset. The code is taken from the\n-[keras-idiomatic-programmer](https://github.com/GoogleCloudPlatform/keras-idiomatic-programmer/blob/master/zoo/resnet/resnet_cifar10_v2.py) repository. The hyperparameters of\n-these architectures have been referred from Section 3 and Appendix A of [the original\n-paper](https://arxiv.org/abs/2011.10566).\n-\"\"\"\n-\n-\"\"\"shell\n-wget -q https://shorturl.at/QS369 -O resnet_cifar10_v2.py\n-\"\"\"\n-\n-import resnet_cifar10_v2\n-\n-N = 2\n-DEPTH = N * 9 + 2\n-NUM_BLOCKS = ((DEPTH - 2) // 9) - 1\n-\n-\n-def get_encoder():\n-    # Input and backbone.\n-    inputs = layers.Input((CROP_TO, CROP_TO, 3))\n-    x = layers.Rescaling(scale=1.0 / 127.5, offset=-1)(inputs)\n-    x = resnet_cifar10_v2.stem(x)\n-    x = resnet_cifar10_v2.learner(x, NUM_BLOCKS)\n-    x = layers.GlobalAveragePooling2D(name=\"backbone_pool\")(x)\n-\n-    # Projection head.\n-    x = layers.Dense(\n-        PROJECT_DIM,\n-        use_bias=False,\n-        kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n-    )(x)\n-    x = layers.BatchNormalization()(x)\n-    x = layers.ReLU()(x)\n-    x = layers.Dense(\n-        PROJECT_DIM,\n-        use_bias=False,\n-        kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n-    )(x)\n-    outputs = layers.BatchNormalization()(x)\n-    return keras.Model(inputs, outputs, name=\"encoder\")\n-\n-\n-def get_predictor():\n-    model = keras.Sequential(\n-        [\n-            # Note the AutoEncoder-like structure.\n-            layers.Input((PROJECT_DIM,)),\n-            layers.Dense(\n-                LATENT_DIM,\n-                use_bias=False,\n-                kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n-            ),\n-            layers.ReLU(),\n-            layers.BatchNormalization(),\n-            layers.Dense(PROJECT_DIM),\n-        ],\n-        name=\"predictor\",\n-    )\n-    return model\n-\n-\n-\"\"\"\n-## Defining the (pre-)training loop\n-\n-One of the main reasons behind training networks with these kinds of approaches is to\n-utilize the learned representations for downstream tasks like classification. This is why\n-this particular training phase is also referred to as _pre-training_.\n-\n-We start by defining the loss function.\n-\"\"\"\n-\n-\n-def compute_loss(p, z):\n-    # The authors of SimSiam emphasize the impact of\n-    # the `stop_gradient` operator in the paper as it\n-    # has an important role in the overall optimization.\n-    z = tf.stop_gradient(z)\n-    p = tf.math.l2_normalize(p, axis=1)\n-    z = tf.math.l2_normalize(z, axis=1)\n-    # Negative cosine similarity (minimizing this is\n-    # equivalent to maximizing the similarity).\n-    return -tf.reduce_mean(tf.reduce_sum((p * z), axis=1))\n-\n-\n-\"\"\"\n-We then define our training loop by overriding the `train_step()` function of the\n-`keras.Model` class.\n-\"\"\"\n-\n-\n-class SimSiam(keras.Model):\n-    def __init__(self, encoder, predictor):\n-        super().__init__()\n-        self.encoder = encoder\n-        self.predictor = predictor\n-        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n-\n-    @property\n-    def metrics(self):\n-        return [self.loss_tracker]\n-\n-    def train_step(self, data):\n-        # Unpack the data.\n-        ds_one, ds_two = data\n-\n-        # Forward pass through the encoder and predictor.\n-        with tf.GradientTape() as tape:\n-            z1, z2 = self.encoder(ds_one), self.encoder(ds_two)\n-            p1, p2 = self.predictor(z1), self.predictor(z2)\n-            # Note that here we are enforcing the network to match\n-            # the representations of two differently augmented batches\n-            # of data.\n-            loss = compute_loss(p1, z2) / 2 + compute_loss(p2, z1) / 2\n-\n-        # Compute gradients and update the parameters.\n-        learnable_params = (\n-            self.encoder.trainable_variables\n-            + self.predictor.trainable_variables\n-        )\n-        gradients = tape.gradient(loss, learnable_params)\n-        self.optimizer.apply_gradients(zip(gradients, learnable_params))\n-\n-        # Monitor loss.\n-        self.loss_tracker.update_state(loss)\n-        return {\"loss\": self.loss_tracker.result()}\n-\n-\n-\"\"\"\n-## Pre-training our networks\n-\n-In the interest of this example, we will train the model for only 5 epochs. In reality,\n-this should at least be 100 epochs.\n-\"\"\"\n-\n-# Create a cosine decay learning scheduler.\n-num_training_samples = len(x_train)\n-steps = EPOCHS * (num_training_samples // BATCH_SIZE)\n-lr_decayed_fn = keras.optimizers.schedules.CosineDecay(\n-    initial_learning_rate=0.03, decay_steps=steps\n-)\n-\n-# Create an early stopping callback.\n-early_stopping = keras.callbacks.EarlyStopping(\n-    monitor=\"loss\", patience=5, restore_best_weights=True\n-)\n-\n-# Compile model and start training.\n-simsiam = SimSiam(get_encoder(), get_predictor())\n-simsiam.compile(optimizer=keras.optimizers.SGD(lr_decayed_fn, momentum=0.6))\n-history = simsiam.fit(ssl_ds, epochs=EPOCHS, callbacks=[early_stopping])\n-\n-# Visualize the training progress of the model.\n-plt.plot(history.history[\"loss\"])\n-plt.grid()\n-plt.title(\"Negative Cosine Similairty\")\n-plt.show()\n-\n-\"\"\"\n-If your solution gets very close to -1 (minimum value of our loss) very quickly with a\n-different dataset and a different backbone architecture that is likely because of\n-*representation collapse*. It is a phenomenon where the encoder yields similar output for\n-all the images. In that case additional hyperparameter tuning is required especially in\n-the following areas:\n-\n-* Strength of the color distortions and their probabilities.\n-* Learning rate and its schedule.\n-* Architecture of both the backbone and their projection head.\n-\n-\"\"\"\n-\n-\"\"\"\n-## Evaluating our SSL method\n-\n-The most popularly used method to evaluate a SSL method in computer vision (or any other\n-pre-training method as such) is to learn a linear classifier on the frozen features of\n-the trained backbone model (in this case it is ResNet20) and evaluate the classifier on\n-unseen images. Other methods include\n-[fine-tuning](https://keras.io/guides/transfer_learning/) on the source dataset or even a\n-target dataset with 5% or 10% labels present. Practically, we can use the backbone model\n-for any downstream task such as semantic segmentation, object detection, and so on where\n-the backbone models are usually pre-trained with *pure supervised learning*.\n-\"\"\"\n-\n-# We first create labeled `Dataset` objects.\n-train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n-test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n-\n-# Then we shuffle, batch, and prefetch this dataset for performance. We\n-# also apply random resized crops as an augmentation but only to the\n-# training set.\n-train_ds = (\n-    train_ds.shuffle(1024)\n-    .map(lambda x, y: (flip_random_crop(x), y), num_parallel_calls=AUTO)\n-    .batch(BATCH_SIZE)\n-    .prefetch(AUTO)\n-)\n-test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTO)\n-\n-# Extract the backbone ResNet20.\n-backbone = keras.Model(\n-    simsiam.encoder.input, simsiam.encoder.get_layer(\"backbone_pool\").output\n-)\n-\n-# We then create our linear classifier and train it.\n-backbone.trainable = False\n-inputs = layers.Input((CROP_TO, CROP_TO, 3))\n-x = backbone(inputs, training=False)\n-outputs = layers.Dense(10, activation=\"softmax\")(x)\n-linear_model = keras.Model(inputs, outputs, name=\"linear_model\")\n-\n-# Compile model and start training.\n-linear_model.compile(\n-    loss=\"sparse_categorical_crossentropy\",\n-    metrics=[\"accuracy\"],\n-    optimizer=keras.optimizers.SGD(lr_decayed_fn, momentum=0.9),\n-)\n-history = linear_model.fit(\n-    train_ds, validation_data=test_ds, epochs=EPOCHS, callbacks=[early_stopping]\n-)\n-_, test_acc = linear_model.evaluate(test_ds)\n-print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))\n-\n-\"\"\"\n-\n-## Notes\n-* More data and longer pre-training schedule benefit SSL in general.\n-* SSL is particularly very helpful when you do not have access to very limited *labeled*\n-training data but you can manage to build a large corpus of unlabeled data. Recently,\n-using an SSL method called [SwAV](https://arxiv.org/abs/2006.09882), a group of\n-researchers at Facebook trained a [RegNet](https://arxiv.org/abs/2006.09882) on 2 Billion\n-images. They were able to achieve downstream performance very close to those achieved by\n-pure supervised pre-training. For some downstream tasks, their method even outperformed\n-the supervised counterparts. You can check out [their\n-paper](https://arxiv.org/pdf/2103.01988.pdf) to know the details.\n-* If you are interested to understand why contrastive SSL helps networks learn meaningful\n-representations, you can check out the following resources:\n-   * [Self-supervised learning: The dark matter of\n-intelligence](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)\n-   * [Understanding self-supervised learning using controlled datasets with known\n-structure](https://sslneuips20.github.io/files/CameraReadys%203-77/64/CameraReady/Understanding_self_supervised_learning.pdf)\n-\n-\"\"\"\n\n@@ -1,209 +0,0 @@\n-\"\"\"\n-Title: Visualizing what convnets learn\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2020/05/29\n-Last modified: 2020/05/29\n-Description: Displaying the visual patterns that convnet filters respond to.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-In this example, we look into what sort of visual patterns image classification models\n-learn. We'll be using the `ResNet50V2` model, trained on the ImageNet dataset.\n-\n-Our process is simple: we will create input images that maximize the activation of\n-specific filters in a target layer (picked somewhere in the middle of the model: layer\n-`conv3_block4_out`). Such images represent a visualization of the\n-pattern that the filter responds to.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import keras_core as keras\n-\n-\n-import numpy as np\n-import tensorflow as tf\n-\n-# The dimensions of our input image\n-img_width = 180\n-img_height = 180\n-# Our target layer: we will visualize the filters from this layer.\n-# See `model.summary()` for list of layer names, if you want to change this.\n-layer_name = \"conv3_block4_out\"\n-\n-\"\"\"\n-## Build a feature extraction model\n-\"\"\"\n-\n-# Build a ResNet50V2 model loaded with pre-trained ImageNet weights\n-model = keras.applications.ResNet50V2(weights=\"imagenet\", include_top=False)\n-\n-# Set up a model that returns the activation values for our target layer\n-layer = model.get_layer(name=layer_name)\n-feature_extractor = keras.Model(inputs=model.inputs, outputs=layer.output)\n-\n-\"\"\"\n-## Set up the gradient ascent process\n-\n-The \"loss\" we will maximize is simply the mean of the activation of a specific filter in\n-our target layer. To avoid border effects, we exclude border pixels.\n-\"\"\"\n-\n-\n-def compute_loss(input_image, filter_index):\n-    activation = feature_extractor(input_image)\n-    # We avoid border artifacts by only involving non-border pixels in the loss.\n-    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n-    return tf.reduce_mean(filter_activation)\n-\n-\n-\"\"\"\n-Our gradient ascent function simply computes the gradients of the loss above\n-with regard to the input image, and update the update image so as to move it\n-towards a state that will activate the target filter more strongly.\n-\"\"\"\n-\n-\n-@tf.function\n-def gradient_ascent_step(img, filter_index, learning_rate):\n-    with tf.GradientTape() as tape:\n-        tape.watch(img)\n-        loss = compute_loss(img, filter_index)\n-    # Compute gradients.\n-    grads = tape.gradient(loss, img)\n-    # Normalize gradients.\n-    grads = tf.math.l2_normalize(grads)\n-    img += learning_rate * grads\n-    return loss, img\n-\n-\n-\"\"\"\n-## Set up the end-to-end filter visualization loop\n-\n-Our process is as follow:\n-\n-- Start from a random image that is close to \"all gray\" (i.e. visually netural)\n-- Repeatedly apply the gradient ascent step function defined above\n-- Convert the resulting input image back to a displayable form, by normalizing it,\n-center-cropping it, and restricting it to the [0, 255] range.\n-\"\"\"\n-\n-\n-def initialize_image():\n-    # We start from a gray image with some random noise\n-    img = tf.random.uniform((1, img_width, img_height, 3))\n-    # ResNet50V2 expects inputs in the range [-1, +1].\n-    # Here we scale our random inputs to [-0.125, +0.125]\n-    return (img - 0.5) * 0.25\n-\n-\n-def visualize_filter(filter_index):\n-    # We run gradient ascent for 20 steps\n-    iterations = 30\n-    learning_rate = 10.0\n-    img = initialize_image()\n-    for iteration in range(iterations):\n-        loss, img = gradient_ascent_step(img, filter_index, learning_rate)\n-\n-    # Decode the resulting input image\n-    img = deprocess_image(img[0].numpy())\n-    return loss, img\n-\n-\n-def deprocess_image(img):\n-    # Normalize array: center on 0., ensure variance is 0.15\n-    img -= img.mean()\n-    img /= img.std() + 1e-5\n-    img *= 0.15\n-\n-    # Center crop\n-    img = img[25:-25, 25:-25, :]\n-\n-    # Clip to [0, 1]\n-    img += 0.5\n-    img = np.clip(img, 0, 1)\n-\n-    # Convert to RGB array\n-    img *= 255\n-    img = np.clip(img, 0, 255).astype(\"uint8\")\n-    return img\n-\n-\n-\"\"\"\n-Let's try it out with filter 0 in the target layer:\n-\"\"\"\n-\n-from IPython.display import Image, display\n-\n-loss, img = visualize_filter(0)\n-keras.utils.save_img(\"0.png\", img)\n-\n-\"\"\"\n-This is what an input that maximizes the response of filter 0 in the target layer would\n-look like:\n-\"\"\"\n-\n-display(Image(\"0.png\"))\n-\n-\"\"\"\n-## Visualize the first 64 filters in the target layer\n-\n-Now, let's make a 8x8 grid of the first 64 filters\n-in the target layer to get of feel for the range\n-of different visual patterns that the model has learned.\n-\"\"\"\n-\n-# Compute image inputs that maximize per-filter activations\n-# for the first 64 filters of our target layer\n-all_imgs = []\n-for filter_index in range(64):\n-    print(\"Processing filter %d\" % (filter_index,))\n-    loss, img = visualize_filter(filter_index)\n-    all_imgs.append(img)\n-\n-# Build a black picture with enough space for\n-# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n-margin = 5\n-n = 8\n-cropped_width = img_width - 25 * 2\n-cropped_height = img_height - 25 * 2\n-width = n * cropped_width + (n - 1) * margin\n-height = n * cropped_height + (n - 1) * margin\n-stitched_filters = np.zeros((width, height, 3))\n-\n-# Fill the picture with our saved filters\n-for i in range(n):\n-    for j in range(n):\n-        img = all_imgs[i * n + j]\n-        stitched_filters[\n-            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n-            (cropped_height + margin) * j : (cropped_height + margin) * j\n-            + cropped_height,\n-            :,\n-        ] = img\n-keras.utils.save_img(\"stiched_filters.png\", stitched_filters)\n-\n-from IPython.display import Image, display\n-\n-display(Image(\"stiched_filters.png\"))\n-\n-\"\"\"\n-Image classification models see the world by decomposing their inputs over a \"vector\n-basis\" of texture filters such as these.\n-\n-See also\n-[this old blog post](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)\n-for analysis and interpretation.\n-\n-Example available on HuggingFace.\n-\n-[![Generic badge](https://img.shields.io/badge/🤗%20Spaces-What%20Convnets%20Learn-black.svg)](https://huggingface.co/spaces/keras-io/what-convnets-learn)\n-\"\"\"\n\n@@ -1,695 +0,0 @@\n-# -*- coding: utf-8 -*-\n-\"\"\"\n-Author: [lukewood](https://lukewood.xyz)\n-Date created: 03/28/2023\n-Last modified: 07/25/2023\n-Description: Use KerasCV to train powerful image classifiers.\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-Classification is the process of predicting a categorical label for a given\n-input image.\n-While classification is a relatively straightforward computer vision task,\n-modern approaches still are built of several complex components.\n-Luckily, KerasCV provides APIs to construct commonly used components.\n-\n-This guide demonstrates KerasCV's modular approach to solving image\n-classification problems at three levels of complexity:\n-\n-- Inference with a pretrained classifier\n-- Fine-tuning a pretrained backbone\n-- Training a image classifier from scratch\n-\n-## Multi-Backend Support\n-\n-KerasCV's `ImageClassifier` model supports several backends like JAX, PyTorch,\n-and TensorFlow with the help of `keras_core`. To enable multi-backend support\n-in KerasCV, set the `KERAS_CV_MULTI_BACKEND` environment variable. We can\n-then switch between different backends by setting the `KERAS_BACKEND`\n-environment variable. Currently, `\"tensorflow\"`, `\"jax\"`, and `\"torch\"` are\n-supported.\n-\n-This demonstration uses the Jax backend.\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_CV_MULTI_BACKEND\"] = \"1\"\n-os.environ[\"KERAS_BACKEND\"] = \"jax\"\n-\n-import json\n-import math\n-import keras_cv\n-import keras_core as keras\n-from keras_core import ops\n-from keras_core import losses\n-from keras_core import optimizers\n-from keras_core.optimizers import schedules\n-from keras_core import metrics\n-import tensorflow as tf\n-from tensorflow import data as tf_data\n-import tensorflow_datasets as tfds\n-import numpy as np\n-\n-\"\"\"## Inference with a pretrained classifier\n-\n-Let's get started with the simplest KerasCV API: a pretrained classifier.\n-In this example, we will construct a classifier that was\n-pretrained on the ImageNet dataset.\n-We'll use this model to solve the age old \"Cat or Dog\" problem.\n-\n-The highest level module in KerasCV is a *task*. A *task* is a `keras.Model`\n-consisting of a (generally pretrained) backbone model and task-specific\n-layers. Here's an example using `keras_cv.models.ImageClassifier` with an\n-EfficientNetV2B0 Backbone.\n-\n-EfficientNetV2B0 is a great starting model when constructing an image\n-classification pipeline.\n-This architecture manages to achieve high accuracy, while using a\n-parameter count of 7M.\n-If an EfficientNetV2B0 is not powerful enough for the task you are hoping to\n-solve, be sure to check out\n-[KerasCV's other available Backbones](https://github.com/keras-team/keras-cv/tree/master/keras_cv/models/backbones)!\n-\"\"\"\n-\n-classifier = keras_cv.models.ImageClassifier.from_preset(\n-    \"efficientnetv2_b0_imagenet_classifier\"\n-)\n-\n-\"\"\"You may notice a small deviation from the old `keras.applications` API;\n-where you would construct the class with\n-`EfficientNetV2B0(weights=\"imagenet\")`. While the old API was great for\n-classification, it did not scale effectively to other use cases that required\n-complex architectures, like object deteciton and semantic segmentation.\n-\n-Now that our classifier is built, let's apply it to this cute cat picture!\n-\"\"\"\n-\n-filepath = keras.utils.get_file(origin=\"https://i.imgur.com/9i63gLN.jpg\")\n-image = keras.utils.load_img(filepath)\n-image = np.array(image)\n-keras_cv.visualization.plot_image_gallery(\n-    image[None, ...], rows=1, cols=1, value_range=(0, 255), show=True, scale=4\n-)\n-\n-\"\"\"Next, let's get some predictions from our classifier:\"\"\"\n-\n-predictions = classifier.predict(np.expand_dims(image, axis=0))\n-\n-\"\"\"Predictions come in the form of softmax-ed category rankings.\n-We can find the index of the top classes using a simple argsort function:\n-\"\"\"\n-\n-top_classes = predictions[0].argsort(axis=-1)\n-\n-\"\"\"In order to decode the class mappings, we can construct a mapping from\n-category indices to ImageNet class names.\n-For convenience, I've stored the ImageNet class mapping in a GitHub gist.\n-Let's download and load it now.\n-\"\"\"\n-\n-classes = keras.utils.get_file(\n-    origin=\"https://gist.githubusercontent.com/LukeWood/62eebcd5c5c4a4d0e0b7845780f76d55/raw/fde63e5e4c09e2fa0a3436680f436bdcb8325aac/ImagenetClassnames.json\"\n-)\n-with open(classes, \"rb\") as f:\n-    classes = json.load(f)\n-\n-\"\"\"Now we can simply look up the class names via index:\"\"\"\n-\n-top_two = [classes[str(i)] for i in top_classes[-2:]]\n-print(\"Top two classes are:\", top_two)\n-\n-\"\"\"Great!  Both of these appear to be correct!\n-However, one of the classes is \"Velvet\".\n-We're trying to classify Cats VS Dogs.\n-We don't care about the velvet blanket!\n-\n-Ideally, we'd have a classifier that only performs computation to determine if\n-an image is a cat or a dog, and has all of its resources dedicated to this\n-task. This can be solved by fine tuning our own classifier.\n-\n-# Fine tuning a pretrained classifier\n-\n-When labeled images specific to our task are available, fine-tuning a custom\n-classifier can improve performance.\n-If we want to train a Cats vs Dogs Classifier, using explicitly labeled Cat vs\n-Dog data should perform better than the generic classifier!\n-For many tasks, no relevant pretrained model\n-will be available (e.g., categorizing images specific to your application).\n-\n-First, let's get started by loading some data:\n-\"\"\"\n-\n-BATCH_SIZE = 32\n-IMAGE_SIZE = (224, 224)\n-AUTOTUNE = tf_data.AUTOTUNE\n-tfds.disable_progress_bar()\n-\n-data, dataset_info = tfds.load(\n-    \"cats_vs_dogs\",\n-    with_info=True,\n-    as_supervised=True\n-)\n-train_steps_per_epoch = (\n-    dataset_info.splits[\"train\"].num_examples // BATCH_SIZE\n-)\n-train_dataset = data[\"train\"]\n-\n-num_classes = dataset_info.features[\"label\"].num_classes\n-\n-resizing = keras_cv.layers.Resizing(\n-    IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True\n-)\n-encoder = keras.layers.CategoryEncoding(num_classes, \"one_hot\", dtype=\"int32\")\n-\n-\n-def preprocess_inputs(image, label):\n-    # Staticly resize images as we only iterate the dataset once.\n-    return resizing(image), encoder(label)\n-\n-\n-# Shuffle the dataset to increase diversity of batches.\n-# 10*BATCH_SIZE follows the assumption that bigger machines can handle bigger\n-# shuffle buffers.\n-train_dataset = train_dataset.shuffle(\n-    10 * BATCH_SIZE, reshuffle_each_iteration=True\n-).map(preprocess_inputs, num_parallel_calls=AUTOTUNE)\n-train_dataset = train_dataset.batch(BATCH_SIZE)\n-\n-images = next(iter(train_dataset.take(1)))[0]\n-keras_cv.visualization.plot_image_gallery(images, value_range=(0, 255))\n-\n-\"\"\"Meow!\n-\n-Next let's construct our model.\n-The use of imagenet in the preset name indicates that the backbone was\n-pretrained on the ImageNet dataset.\n-Pretrained backbones extract more information from our labeled examples by\n-leveraging patterns extracted from potentially much larger datasets.\n-\n-Next lets put together our classifier:\n-\"\"\"\n-\n-model = keras_cv.models.ImageClassifier.from_preset(\n-    \"efficientnetv2_b0_imagenet\", num_classes=2\n-)\n-model.compile(\n-    loss=\"categorical_crossentropy\",\n-    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n-    metrics=[\"accuracy\"],\n-)\n-\n-\"\"\"Here our classifier is just a simple `keras.Sequential`.\n-All that is left to do is call `model.fit()`:\n-\"\"\"\n-\n-model.fit(train_dataset)\n-\n-\"\"\"Let's look at how our model performs after the fine tuning:\"\"\"\n-\n-predictions = model.predict(np.expand_dims(image, axis=0))\n-\n-classes = {0: \"cat\", 1: \"dog\"}\n-print(\"Top class is:\", classes[predictions[0].argmax()])\n-\n-\"\"\"Awesome - looks like the model correctly classified the image.\n-\n-# Train a Classifier from Scratch\n-\n-Now that we've gotten our hands dirty with classification, let's take on one\n-last task: training a classification model from scratch!\n-A standard benchmark for image classification is the ImageNet dataset, however\n-due to licensing constraints we will use the CalTech 101 image classification\n-dataset in this tutorial.\n-While we use the simpler CalTech 101 dataset in this guide, the same training\n-template may be used on ImageNet to achieve near state-of-the-art scores.\n-\n-Let's start out by tackling data loading:\n-\"\"\"\n-\n-NUM_CLASSES = 101\n-# Change epochs to 100~ to fully train.\n-EPOCHS = 1\n-\n-encoder = keras.layers.CategoryEncoding(NUM_CLASSES, \"one_hot\", dtype=\"int32\")\n-\n-\n-def package_inputs(image, label):\n-    return {\"images\": image, \"labels\": encoder(label)}\n-\n-\n-train_ds, eval_ds = tfds.load(\n-    \"caltech101\", split=[\"train\", \"test\"], as_supervised=\"true\"\n-)\n-train_ds = train_ds.map(package_inputs, num_parallel_calls=tf_data.AUTOTUNE)\n-eval_ds = eval_ds.map(package_inputs, num_parallel_calls=tf_data.AUTOTUNE)\n-\n-train_ds = train_ds.shuffle(BATCH_SIZE * 16)\n-\n-\"\"\"The CalTech101 dataset has different sizes for every image, so we use the\n-`ragged_batch()` API to batch them together while maintaining each individual\n-image's shape information.\n-\"\"\"\n-\n-train_ds = train_ds.ragged_batch(BATCH_SIZE)\n-eval_ds = eval_ds.ragged_batch(BATCH_SIZE)\n-\n-batch = next(iter(train_ds.take(1)))\n-image_batch = batch[\"images\"]\n-label_batch = batch[\"labels\"]\n-\n-keras_cv.visualization.plot_image_gallery(\n-    image_batch.to_tensor(),\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"## Data Augmentation\n-\n-In our previous finetuning exmaple, we performed a static resizing operation\n-and did not utilize any image augmentation.\n-This is because a single pass over the training set was sufficient to achieve\n-decent results.\n-When training to solve a more difficult task, you'll want to include data\n-augmentation in your data pipeline.\n-\n-Data augmentation is a technique to make your model robust to changes in input\n-data such as lighting, cropping, and orientation.\n-KerasCV includes some of the most useful augmentations in the\n-`keras_cv.layers` API.\n-Creating an optimal pipeline of augmentations is an art, but in this section\n-of the guide we'll offer some tips on best practices for classification.\n-\n-One caveat to be aware of with image data augmentation is that you must be\n-careful to not shift your augmented data distribution too far from the\n-original data distribution.\n-The goal is to prevent overfitting and increase generalization,\n-but samples that lie completely out of the data distribution simply add noise\n-to the training process.\n-\n-The first augmentation we'll use is `RandomFlip`.\n-This augmentation behaves more or less how you'd expect: it either flips the\n-image or not.\n-While this augmentation is useful in CalTech101 and ImageNet, it should be\n-noted that it should not be used on tasks where the data distribution is not\n-vertical mirror invariant.\n-An example of a dataset where this occurs is MNIST hand written digits.\n-Flipping a `6` over the\n-vertical axis will make the digit appear more like a `7` than a `6`, but the\n-label will still show a `6`.\n-\"\"\"\n-\n-random_flip = keras_cv.layers.RandomFlip()\n-augmenters = [random_flip]\n-\n-image_batch = random_flip(image_batch)\n-keras_cv.visualization.plot_image_gallery(\n-    image_batch.to_tensor(),\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"Half of the images have been flipped!\n-\n-The next augmentation we'll use is `RandomCropAndResize`.\n-This operation selects a random subset of the image, then resizes it to the\n-provided target size.\n-By using this augmentation, we force our classifier to become spatially\n-invariant.\n-Additionally, this layer accepts an `aspect_ratio_factor` which can be used to\n-distort the aspect ratio of the image.\n-While this can improve model performance, it should be used with caution.\n-It is very easy for an aspect ratio distortion to shift a sample too far from\n-the original training set's data distribution.\n-Remember - the goal of data augmentation is to produce more training samples\n-that align with the data distribution of your training set!\n-\n-`RandomCropAndResize` also can handle `tf.RaggedTensor` inputs.  In the\n-CalTech101 image dataset images come in a wide variety of sizes.\n-As such they cannot easily be batched together into a dense training batch.\n-Luckily, `RandomCropAndResize` handles the Ragged -> Dense conversion process\n-for you!\n-\n-Let's add a `RandomCropAndResize` to our set of augmentations:\n-\"\"\"\n-\n-crop_and_resize = keras_cv.layers.RandomCropAndResize(\n-    target_size=IMAGE_SIZE,\n-    crop_area_factor=(0.8, 1.0),\n-    aspect_ratio_factor=(0.9, 1.1),\n-)\n-augmenters += [crop_and_resize]\n-\n-image_batch = crop_and_resize(image_batch)\n-keras_cv.visualization.plot_image_gallery(\n-    image_batch,\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"Great!  We are now working with a batch of dense images.\n-Next up, lets include some spatial and color-based jitter to our training set.\n-This will allow us to produce a classifier that is robust to lighting\n-flickers, shadows, and more.\n-\n-There are limitless ways to augment an image by altering color and spatial\n-features, but perhaps the most battle tested technique is\n-[`RandAugment`](https://arxiv.org/abs/1909.13719).\n-`RandAugment` is actually a set of 10 different augmentations:\n-`AutoContrast`, `Equalize`, `Solarize`, `RandomColorJitter`, `RandomContrast`,\n-`RandomBrightness`, `ShearX`, `ShearY`, `TranslateX` and `TranslateY`.\n-At inference time, `num_augmentations` augmenters are sampled for each image,\n-and random magnitude factors are sampled for each.\n-These augmentations are then applied sequentially.\n-\n-KerasCV makes tuning these parameters easy using the `augmentations_per_image`\n-and `magnitude` parameters!\n-Let's take it for a spin:\n-\"\"\"\n-\n-rand_augment = keras_cv.layers.RandAugment(\n-    augmentations_per_image=3,\n-    magnitude=0.3,\n-    value_range=(0, 255),\n-)\n-augmenters += [rand_augment]\n-\n-image_batch = rand_augment(image_batch)\n-keras_cv.visualization.plot_image_gallery(\n-    image_batch,\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"Looks great; but we're not done yet!\n-What if an image is missing one critical feature of a class?  For example,\n-what if a leaf is blocking the view of a cat's ear, but our classifier\n-learned to classify cats simply by observing their ears?\n-\n-One easy approach to tackling this is to use `RandomCutout`, which randomly\n-strips out a sub-section of the image:\n-\"\"\"\n-\n-random_cutout = keras_cv.layers.RandomCutout(\n-    width_factor=0.4, height_factor=0.4\n-)\n-keras_cv.visualization.plot_image_gallery(\n-    random_cutout(image_batch),\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"While this tackles the problem reasonably well, it can cause the classifier\n-to develop responses to borders between features and black pixel areas caused\n-by the cutout.\n-\n-[`CutMix`](https://arxiv.org/abs/1905.04899) solves the same issue by using\n-a more complex (and more effective) technique.\n-Instead of replacing the cut-out areas with black pixels, `CutMix` replaces\n-these regions with regions of other images sampled from within your training\n-set!\n-Following this replacement, the image's classification label is updated to be\n-a blend of the original and mixed image's class label.\n-\n-What does this look like in practice?  Let's check it out:\n-\"\"\"\n-\n-cut_mix = keras_cv.layers.CutMix()\n-# CutMix needs to modify both images and labels\n-inputs = {\"images\": image_batch, \"labels\": tf.cast(label_batch, \"float32\")}\n-\n-keras_cv.visualization.plot_image_gallery(\n-    cut_mix(inputs)[\"images\"],\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"Let's hold off from adding it to our augmenter for a minute - more on that\n-soon!\n-\n-Next, let's look into `MixUp()`.\n-Unfortunately, while `MixUp()` has been empirically shown to *substantially*\n-improve both the robustness and the generalization of the trained model,\n-it is not well-understood why such improvement occurs... but\n-a little alchemy never hurt anyone!\n-\n-`MixUp()` works by sampling two images from a batch, then proceeding to\n-literally blend together their pixel intensities as well as their\n-classification labels.\n-\n-Let's see it in action:\n-\"\"\"\n-\n-mix_up = keras_cv.layers.MixUp()\n-# MixUp needs to modify both images and labels\n-inputs = {\"images\": image_batch, \"labels\": tf.cast(label_batch, \"float32\")}\n-\n-keras_cv.visualization.plot_image_gallery(\n-    mix_up(inputs)[\"images\"],\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"If you look closely, you'll see that the images have been blended together.\n-\n-Instead of applying `CutMix()` and `MixUp()` to every image, we instead pick\n-one or the other to apply to each batch.\n-This can be expressed using `keras_cv.layers.RandomChoice()`\n-\"\"\"\n-\n-cut_mix_or_mix_up = keras_cv.layers.RandomChoice(\n-    [cut_mix, mix_up], batchwise=True\n-)\n-augmenters += [cut_mix_or_mix_up]\n-\n-\"\"\"Now let's apply our final augmenter to the training data:\"\"\"\n-\n-augmenter = keras_cv.layers.Augmenter(augmenters)\n-train_ds = train_ds.map(augmenter, num_parallel_calls=tf_data.AUTOTUNE)\n-\n-image_batch = next(iter(train_ds.take(1)))[\"images\"]\n-keras_cv.visualization.plot_image_gallery(\n-    image_batch,\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"We also need to resize our evaluation set to get dense batches of the image\n-size expected by our model. We use the deterministic\n-`keras_cv.layers.Resizing` in this case to avoid adding noise to our\n-evaluation metric.\n-\"\"\"\n-\n-inference_resizing = keras_cv.layers.Resizing(\n-    IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True\n-)\n-eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf_data.AUTOTUNE)\n-\n-inference_resizing = keras_cv.layers.Resizing(\n-    IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True\n-)\n-eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf_data.AUTOTUNE)\n-\n-image_batch = next(iter(eval_ds.take(1)))[\"images\"]\n-keras_cv.visualization.plot_image_gallery(\n-    image_batch,\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"Finally, lets unpackage our datasets and prepare to pass them to\n-`model.fit()`, which accepts a tuple of `(images, labels)`.\n-\"\"\"\n-\n-def unpackage_dict(inputs):\n-    return inputs[\"images\"], inputs[\"labels\"]\n-\n-\n-train_ds = train_ds.map(unpackage_dict, num_parallel_calls=tf_data.AUTOTUNE)\n-eval_ds = eval_ds.map(unpackage_dict, num_parallel_calls=tf_data.AUTOTUNE)\n-\n-\"\"\"Data augmentation is by far the hardest piece of training a modern\n-classifier.\n-Congratulations on making it this far!\n-\n-## Optimizer Tuning\n-\n-To achieve optimal performance, we need to use a learning rate schedule\n-instead of a single learning rate. While we won't go into detail on the\n-Cosine decay with warmup schedule used here, [you can read more about it\n-here](https://scorrea92.medium.com/cosine-learning-rate-decay-e8b50aa455b).\n-\"\"\"\n-\n-def lr_warmup_cosine_decay(\n-    global_step,\n-    warmup_steps,\n-    hold=0,\n-    total_steps=0,\n-    start_lr=0.0,\n-    target_lr=1e-2,\n-):\n-    # Cosine decay\n-    learning_rate = (\n-        0.5\n-        * target_lr\n-        * (\n-            1\n-            + ops.cos(\n-                math.pi\n-                * ops.convert_to_tensor(\n-                    global_step - warmup_steps - hold, dtype=\"float32\"\n-                )\n-                / ops.convert_to_tensor(\n-                    total_steps - warmup_steps - hold, dtype=\"float32\"\n-                )\n-            )\n-        )\n-    )\n-\n-    warmup_lr = (target_lr * (global_step / warmup_steps))\n-\n-    if hold > 0:\n-        learning_rate = ops.where(\n-            global_step > warmup_steps + hold, learning_rate, target_lr\n-        )\n-\n-    learning_rate = ops.where(\n-        global_step < warmup_steps, warmup_lr, learning_rate\n-    )\n-    return learning_rate\n-\n-\n-class WarmUpCosineDecay(\n-    schedules.LearningRateSchedule\n-):\n-    def __init__(\n-        self, warmup_steps, total_steps, hold, start_lr=0.0, target_lr=1e-2\n-    ):\n-        super().__init__()\n-        self.start_lr = start_lr\n-        self.target_lr = target_lr\n-        self.warmup_steps = warmup_steps\n-        self.total_steps = total_steps\n-        self.hold = hold\n-\n-    def __call__(self, step):\n-        lr = lr_warmup_cosine_decay(\n-            global_step=step,\n-            total_steps=self.total_steps,\n-            warmup_steps=self.warmup_steps,\n-            start_lr=self.start_lr,\n-            target_lr=self.target_lr,\n-            hold=self.hold,\n-        )\n-\n-        return ops.where(step > self.total_steps, 0.0, lr)\n-\n-\"\"\"![WarmUpCosineDecay schedule](https://i.imgur.com/YCr5pII.png)\n-\n-The schedule looks a as we expect.\n-\n-Next let's construct this optimizer:\n-\"\"\"\n-\n-total_images = 9000\n-total_steps = (total_images // BATCH_SIZE) * EPOCHS\n-warmup_steps = int(0.1 * total_steps)\n-hold_steps = int(0.45 * total_steps)\n-schedule = WarmUpCosineDecay(\n-    start_lr=0.05,\n-    target_lr=1e-2,\n-    warmup_steps=warmup_steps,\n-    total_steps=total_steps,\n-    hold=hold_steps,\n-)\n-optimizer = optimizers.SGD(\n-    weight_decay=5e-4,\n-    learning_rate=schedule,\n-    momentum=0.9,\n-)\n-\n-\"\"\"At long last, we can now build our model and call `fit()`!\n-`keras_cv.models.EfficientNetV2B0Backbone()` is a convenience alias for\n-`keras_cv.models.EfficientNetV2Backbone.from_preset('efficientnetv2_b0')`.\n-Note that this preset does not come with any pretrained weights.\n-\"\"\"\n-\n-backbone = keras_cv.models.ResNet18V2Backbone()\n-model = keras.Sequential(\n-    [\n-        backbone,\n-        keras.layers.GlobalMaxPooling2D(),\n-        keras.layers.Dropout(rate=0.5),\n-        keras.layers.Dense(101, activation=\"softmax\"),\n-    ]\n-)\n-\n-\"\"\"Since the labels produced by MixUp() and CutMix() are somewhat artificial,\n-we employ label smoothing to prevent the model from overfitting to artifacts\n-of this augmentation process.\n-\"\"\"\n-\n-loss = losses.CategoricalCrossentropy(label_smoothing=0.1)\n-\n-\"\"\"Let's compile our model:\"\"\"\n-\n-model.compile(\n-    loss=loss,\n-    optimizer=optimizer,\n-    metrics=[\n-        metrics.CategoricalAccuracy(),\n-        metrics.TopKCategoricalAccuracy(k=5),\n-    ],\n-)\n-\n-\"\"\"and finally call fit().\"\"\"\n-\n-model.fit(\n-    train_ds,\n-    epochs=EPOCHS,\n-    validation_data=eval_ds,\n-)\n-\n-\"\"\"Congratulations!  You now know how to train a powerful image classifier\n-from scratch in KerasCV.\n-Depending on the availability of labeled data for your application, training\n-from scratch may or may not be more powerful than using transfer learning in\n-addition to the data augmentations discussed above. For smaller datasets,\n-pretrained models generally produce high accuracy and faster convergence.\n-\n-## Conclusions\n-\n-While image classification is perhaps the simplest problem in computer vision,\n-the modern landscape has numerous complex components.\n-Luckily, KerasCV offers robust, production-grade APIs to make assembling most\n-of these components possible in one line of code.\n-Through the use of KerasCV's `ImageClassifier` API, pretrained weights, and\n-KerasCV data augmentations you can assemble everything you need to train a\n-powerful classifier in a few hundred lines of code!\n-\n-As a follow up exercise, give the following a try:\n-\n-- Fine tune a KerasCV classifier on your own dataset\n-- Learn more about [KerasCV's data augmentations](https://keras.io/guides/keras_cv/cut_mix_mix_up_and_rand_augment/)\n-- Check out how we train our models on [ImageNet](https://github.com/keras-team/keras-cv/blob/master/examples/training/classification/imagenet/basic_training.py)\n-\"\"\"\n\n@@ -76,6 +76,4 @@ class LearningRateScheduler(Callback):\n \n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}\n-        logs[\"learning_rate\"] = float(\n-            backend.convert_to_numpy(self.model.optimizer.learning_rate)\n-        )\n+        logs[\"learning_rate\"] = self.model.optimizer.learning_rate.value\n\n@@ -107,18 +107,3 @@ class LearningRateSchedulerTest(testing.TestCase):\n                 callbacks=[lr_scheduler],\n                 epochs=2,\n             )\n-\n-    @pytest.mark.requires_trainable_backend\n-    def test_learning_rate_in_history(self):\n-        lr_scheduler = callbacks.LearningRateScheduler(lambda step, lr: 0.5)\n-\n-        history = self.model.fit(\n-            self.x_train,\n-            self.y_train,\n-            callbacks=[lr_scheduler],\n-            epochs=1,\n-        )\n-\n-        self.assertTrue(\"learning_rate\" in history.history)\n-        self.assertEqual(type(history.history[\"learning_rate\"][0]), float)\n-        self.assertEqual(history.history[\"learning_rate\"][0], 0.5)\n\n@@ -391,6 +391,7 @@ class TestTensorBoardV2(testing.TestCase):\n             },\n         )\n         expected_image_summaries = {\n+            _ObservedSummary(logdir=train_dir, tag=\"image\"),\n             _ObservedSummary(logdir=train_dir, tag=\"bias/image\"),\n             _ObservedSummary(logdir=train_dir, tag=\"kernel/image\"),\n         }\n\n@@ -1,5 +1,6 @@\n import numpy as np\n import pytest\n+import tensorflow as tf\n \n from keras_core import testing\n from keras_core.layers.activations import elu\n@@ -20,12 +21,11 @@ class ELUTest(testing.TestCase):\n         )\n \n     def test_correctness(self):\n-        def np_elu(x, alpha=1.0):\n-            return (x > 0) * x + (x <= 0) * alpha * (np.exp(x) - 1)\n-\n         x = np.random.random((2, 2, 5))\n         elu_layer = elu.ELU()\n-        self.assertAllClose(elu_layer(x), np_elu(x))\n+        tf_elu_layer = tf.keras.layers.ELU()\n+        self.assertAllClose(elu_layer(x), tf_elu_layer(x))\n \n         elu_layer = elu.ELU(alpha=0.7)\n-        self.assertAllClose(elu_layer(x), np_elu(x, alpha=0.7))\n+        tf_elu_layer = tf.keras.layers.ELU(alpha=0.7)\n+        self.assertAllClose(elu_layer(x), tf_elu_layer(x))\n\n@@ -1,5 +1,6 @@\n import numpy as np\n import pytest\n+import tensorflow as tf\n \n from keras_core import testing\n from keras_core.layers.activations import prelu\n@@ -21,9 +22,6 @@ class PReLUTest(testing.TestCase):\n         )\n \n     def test_prelu_correctness(self):\n-        def np_prelu(x, alpha):\n-            return (x > 0) * x + (x <= 0) * alpha * x\n-\n         inputs = np.random.randn(2, 10, 5, 3)\n         prelu_layer = prelu.PReLU(\n             alpha_initializer=\"glorot_uniform\",\n@@ -31,9 +29,18 @@ class PReLUTest(testing.TestCase):\n             alpha_constraint=\"non_neg\",\n             shared_axes=(1, 2),\n         )\n+        tf_prelu_layer = tf.keras.layers.PReLU(\n+            alpha_initializer=\"glorot_uniform\",\n+            alpha_regularizer=\"l1\",\n+            alpha_constraint=\"non_neg\",\n+            shared_axes=(1, 2),\n+        )\n+\n         prelu_layer.build(inputs.shape)\n+        tf_prelu_layer.build(inputs.shape)\n \n         weights = np.random.random((1, 1, 3))\n         prelu_layer.alpha.assign(weights)\n-        ref_out = np_prelu(inputs, weights)\n-        self.assertAllClose(prelu_layer(inputs), ref_out)\n+        tf_prelu_layer.alpha.assign(weights)\n+\n+        self.assertAllClose(prelu_layer(inputs), tf_prelu_layer(inputs))\n\n@@ -4,7 +4,6 @@ import string\n \n import numpy as np\n \n-from keras_core import backend\n from keras_core import constraints\n from keras_core import initializers\n from keras_core import ops\n@@ -116,8 +115,6 @@ class MultiHeadAttention(Layer):\n         self.supports_masking = True\n         self._num_heads = num_heads\n         self._key_dim = key_dim\n-        # Cache 1.0 / math.sqrt(self._key_dim).\n-        self._inverse_sqrt_key_dim = None\n         self._value_dim = value_dim if value_dim else key_dim\n         self._dropout = dropout\n         self._use_bias = use_bias\n@@ -314,9 +311,6 @@ class MultiHeadAttention(Layer):\n         )\n         self._softmax = Softmax(axis=norm_axes)\n         self._dropout_layer = Dropout(rate=self._dropout)\n-        self._inverse_sqrt_key_dim = backend.convert_to_tensor(\n-            1.0 / math.sqrt(float(self._key_dim))\n-        )\n \n     def _masked_softmax(self, attention_scores, attention_mask=None):\n         # Normalize the attention scores to probabilities.\n@@ -361,7 +355,7 @@ class MultiHeadAttention(Layer):\n         # Note: Applying scalar multiply at the smaller end of einsum improves\n         # XLA performance, but may introduce slight numeric differences in\n         # the Transformer attention head.\n-        query = ops.multiply(query, self._inverse_sqrt_key_dim)\n+        query = ops.multiply(query, 1.0 / math.sqrt(float(self._key_dim)))\n \n         # Take the dot product between \"query\" and \"key\" to get the raw\n         # attention scores.\n\n@@ -87,14 +87,12 @@ class Dense(Layer):\n     def build(self, input_shape):\n         input_dim = input_shape[-1]\n         self.kernel = self.add_weight(\n-            name=\"kernel\",\n             shape=(input_dim, self.units),\n             initializer=self.kernel_initializer,\n             regularizer=self.kernel_regularizer,\n         )\n         if self.use_bias:\n             self.bias = self.add_weight(\n-                name=\"bias\",\n                 shape=(self.units,),\n                 initializer=self.bias_initializer,\n                 regularizer=self.bias_regularizer,\n\n@@ -177,21 +177,23 @@ class GroupNormalization(Layer):\n         variance = ops.var(\n             reshaped_inputs, axis=group_reduction_axes, keepdims=True\n         )\n-\n         gamma, beta = self._get_reshaped_weights(input_shape)\n \n         # Compute the batch normalization.\n         inv = 1 / ops.sqrt(variance + self.epsilon)\n-        if gamma is not None:\n-            inv *= gamma\n \n-        x = beta - mean * inv if beta is not None else -mean * inv\n+        if gamma is not None:\n+            inv = ops.multiply(inv, gamma)\n+\n+        if beta is not None:\n+            x = beta - ops.multiply(mean, inv)\n+        else:\n+            x = -ops.multiply(mean, inv)\n+\n         normalized_inputs = reshaped_inputs * ops.cast(\n             inv, reshaped_inputs.dtype\n         ) + ops.cast(x, reshaped_inputs.dtype)\n-\n         normalized_inputs = ops.cast(normalized_inputs, reshaped_inputs.dtype)\n-\n         return normalized_inputs\n \n     def _get_reshaped_weights(self, input_shape):\n@@ -200,17 +202,14 @@ class GroupNormalization(Layer):\n         beta = None\n         if self.scale:\n             gamma = ops.reshape(self.gamma, broadcast_shape)\n-\n         if self.center:\n             beta = ops.reshape(self.beta, broadcast_shape)\n         return gamma, beta\n \n     def _create_broadcast_shape(self, input_shape):\n         broadcast_shape = [1] * len(input_shape)\n-\n         broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n         broadcast_shape.insert(self.axis, self.groups)\n-\n         return broadcast_shape\n \n     def compute_output_shape(self, input_shape):\n\n@@ -114,3 +114,21 @@ class GroupNormalizationTest(testing.TestCase):\n             expected_output_2_groups,\n             atol=1e-3,\n         )\n+\n+    def test_broadcasting_2d_channels_first(self):\n+        x = np.arange(16).reshape((1, 4, 2, 2)).astype(\"float32\")\n+        x = layers.GroupNormalization(groups=2, axis=1)(x)\n+        self.assertAllClose(\n+            x,\n+            np.array(\n+                [\n+                    [\n+                        [[-1.5274, -1.0910], [-0.6546, -0.2182]],\n+                        [[0.2182, 0.6546], [1.0910, 1.5274]],\n+                        [[-1.5274, -1.0910], [-0.6546, -0.2182]],\n+                        [[0.2182, 0.6546], [1.0910, 1.5274]],\n+                    ]\n+                ]\n+            ),\n+            atol=1e-3,\n+        )\n\n@@ -1,5 +1,6 @@\n import numpy as np\n import pytest\n+import tensorflow as tf\n from absl.testing import parameterized\n \n from keras_core import layers\n@@ -94,30 +95,21 @@ class GlobalAveragePoolingCorrectnessTest(\n         (\"channels_last\", False),\n         (\"channels_last\", True),\n         (\"channels_first\", False),\n-        (\"channels_first\", True),\n     )\n     def test_global_average_pooling1d(self, data_format, keepdims):\n-        def np_gap1d(x, data_format, keepdims, mask=None):\n-            steps_axis = 1 if data_format == \"channels_last\" else 2\n-            if mask is not None:\n-                mask = np.expand_dims(\n-                    mask, 2 if data_format == \"channels_last\" else 1\n-                )\n-                x *= mask\n-                res = np.sum(x, axis=steps_axis) / np.sum(mask, axis=steps_axis)\n-            else:\n-                res = np.mean(x, axis=steps_axis)\n-            if keepdims:\n-                res = np.expand_dims(res, axis=steps_axis)\n-            return res\n-\n         inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n+\n         layer = layers.GlobalAveragePooling1D(\n             data_format=data_format,\n             keepdims=keepdims,\n         )\n+        tf_keras_layer = tf.keras.layers.GlobalAveragePooling1D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+\n         outputs = layer(inputs)\n-        expected = np_gap1d(inputs, data_format, keepdims)\n+        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n \n         if data_format == \"channels_last\":\n@@ -125,53 +117,47 @@ class GlobalAveragePoolingCorrectnessTest(\n         else:\n             mask = np.array([[1, 1, 0, 0], [0, 1, 0, 1]], dtype=\"int32\")\n         outputs = layer(inputs, mask)\n-        expected = np_gap1d(inputs, data_format, keepdims, mask)\n+        expected = tf_keras_layer(inputs, mask)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (\"channels_last\", False),\n         (\"channels_last\", True),\n         (\"channels_first\", False),\n-        (\"channels_first\", True),\n     )\n     def test_global_average_pooling2d(self, data_format, keepdims):\n-        def np_gap2d(x, data_format, keepdims):\n-            steps_axis = [1, 2] if data_format == \"channels_last\" else [2, 3]\n-            res = np.apply_over_axes(np.mean, x, steps_axis)\n-            if not keepdims:\n-                res = res.squeeze()\n-            return res\n-\n         inputs = np.arange(96, dtype=\"float32\").reshape((2, 3, 4, 4))\n+\n         layer = layers.GlobalAveragePooling2D(\n             data_format=data_format,\n             keepdims=keepdims,\n         )\n+        tf_keras_layer = tf.keras.layers.GlobalAveragePooling2D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+\n         outputs = layer(inputs)\n-        expected = np_gap2d(inputs, data_format, keepdims)\n+        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (\"channels_last\", False),\n         (\"channels_last\", True),\n         (\"channels_first\", False),\n-        (\"channels_first\", True),\n     )\n     def test_global_average_pooling3d(self, data_format, keepdims):\n-        def np_gap3d(x, data_format, keepdims):\n-            steps_axis = (\n-                [1, 2, 3] if data_format == \"channels_last\" else [2, 3, 4]\n-            )\n-            res = np.apply_over_axes(np.mean, x, steps_axis)\n-            if not keepdims:\n-                res = res.squeeze()\n-            return res\n-\n         inputs = np.arange(360, dtype=\"float32\").reshape((2, 3, 3, 5, 4))\n+\n         layer = layers.GlobalAveragePooling3D(\n             data_format=data_format,\n             keepdims=keepdims,\n         )\n+        tf_keras_layer = tf.keras.layers.GlobalAveragePooling3D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+\n         outputs = layer(inputs)\n-        expected = np_gap3d(inputs, data_format, keepdims)\n+        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n\n@@ -271,8 +271,6 @@ class ExtractPatches(Operation):\n         data_format=\"channels_last\",\n     ):\n         super().__init__()\n-        if isinstance(size, int):\n-            size = (size, size)\n         self.size = size\n         self.strides = strides\n         self.dilation_rate = dilation_rate\n@@ -350,16 +348,14 @@ def extract_patches(\n \n     Examples:\n \n-    >>> image = np.random.random(\n-    ...     (2, 20, 20, 3)\n-    ... ).astype(\"float32\") # batch of 2 RGB images\n+    >>> image = np.random.random((1, 20, 20, 3)) # batch of 2 RGB images\n     >>> patches = keras_core.ops.image.extract_patches(image, (5, 5))\n     >>> patches.shape\n-    (2, 4, 4, 75)\n-    >>> image = np.random.random((20, 20, 3)).astype(\"float32\") # 1 RGB image\n+    (1, 4, 4, 75)\n+    >>> image = np.random.random((20, 20, 3)) # batch of 2 RGB images\n     >>> patches = keras_core.ops.image.extract_patches(image, (3, 3), (1, 1))\n     >>> patches.shape\n-    (18, 18, 27)\n+    (4, 4, 75)\n     \"\"\"\n     if any_symbolic_tensors((image,)):\n         return ExtractPatches(\n\n@@ -31,8 +31,6 @@ class ImageOpsDynamicShapeTest(testing.TestCase):\n         p_h, p_w = 5, 5\n         out = kimage.extract_patches(x, (p_h, p_w))\n         self.assertEqual(out.shape, (None, 4, 4, 75))\n-        out = kimage.extract_patches(x, 5)\n-        self.assertEqual(out.shape, (None, 4, 4, 75))\n \n \n class ImageOpsStaticShapeTest(testing.TestCase):\n@@ -52,8 +50,6 @@ class ImageOpsStaticShapeTest(testing.TestCase):\n         p_h, p_w = 5, 5\n         out = kimage.extract_patches(x, (p_h, p_w))\n         self.assertEqual(out.shape, (4, 4, 75))\n-        out = kimage.extract_patches(x, 5)\n-        self.assertEqual(out.shape, (4, 4, 75))\n \n \n AFFINE_TRANSFORM_INTERPOLATIONS = {  # map to order\n@@ -314,7 +310,9 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             and backend.backend() == \"tensorflow\"\n             and dilation_rate > 1\n         ):\n-            pytest.skip(\"dilation_rate>1 with strides>1 not supported with TF\")\n+            pytest.skip(\n+                \"dilation_rate>1 with strides>1 than not supported with TF\"\n+            )\n         if data_format == \"channels_first\":\n             image = np.random.uniform(size=(1, 3, 20, 20))\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
