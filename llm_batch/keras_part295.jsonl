{"custom_id": "keras#cd3f27b5d2ccd843a2c98ee196f81217542e36c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 2 | Churn Cumulative: 125 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -153,7 +153,7 @@ def multi_gpu_model(model, gpus=None, cpu_merge=True, cpu_relocation=False):\n     if not gpus:\n         # Using all visible GPUs when not specifying `gpus`\n         # e.g. CUDA_VISIBLE_DEVICES=0,2 python keras_mgpu.py\n-        gpus = len([x for x in available_devices if 'gpu' in x])\n+        gpus = len([x for x in available_devices if '/gpu:' in x])\n \n     if isinstance(gpus, (list, tuple)):\n         if len(gpus) <= 1:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f312e5acf16100c6e5c1ce4b37a66eed3e62e846", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 31 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 31 | Churn Cumulative: 1196 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,2 +0,0 @@\n-# Only for backwards compatibility.\n-from .mobilenet_v2 import *\n\n@@ -1,29 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-try:\n-    from keras_applications import resnext\n-except:\n-    resnext = None\n-from . import keras_modules_injection\n-\n-\n-@keras_modules_injection\n-def ResNeXt50(*args, **kwargs):\n-    return resnext.ResNeXt50(*args, **kwargs)\n-\n-\n-@keras_modules_injection\n-def ResNeXt101(*args, **kwargs):\n-    return resnext.ResNeXt101(*args, **kwargs)\n-\n-\n-@keras_modules_injection\n-def decode_predictions(*args, **kwargs):\n-    return resnext.decode_predictions(*args, **kwargs)\n-\n-\n-@keras_modules_injection\n-def preprocess_input(*args, **kwargs):\n-    return resnext.preprocess_input(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c658993cf596fbd39cf800873bc457e69cfb0cdb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 12132 | Contributors (this commit): 70 | Commits (past 90d): 3 | Contributors (cumulative): 70 | DMM Complexity: None\n\nDIFF:\n@@ -1546,7 +1546,7 @@ class SeparableConv1D(_SeparableConv):\n class SeparableConv2D(_SeparableConv):\n     \"\"\"Depthwise separable 2D convolution.\n \n-    Separable convolutions consist in first performing\n+    Separable convolution performs first\n     a depthwise spatial convolution\n     (which acts on each input channel separately)\n     followed by a pointwise convolution which mixes together the resulting\n@@ -1685,10 +1685,10 @@ class SeparableConv2D(_SeparableConv):\n \n \n class DepthwiseConv2D(Conv2D):\n-    \"\"\"Depthwise separable 2D convolution.\n+    \"\"\"Depthwise 2D convolution.\n \n-    Depthwise Separable convolutions consists in performing\n-    just the first step in a depthwise spatial convolution\n+    Depthwise convolution performs\n+    just the first step of a depthwise spatial convolution\n     (which acts on each input channel separately).\n     The `depth_multiplier` argument controls how many\n     output channels are generated per input channel in the depthwise step.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b810de658d022c38ce14e49693e6cb45d024333a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 5 | Churn Cumulative: 1931 | Contributors (this commit): 16 | Commits (past 90d): 3 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -319,6 +319,10 @@ def _deserialize_model(h5dict, custom_objects=None, compile=True):\n         # Recover loss functions and metrics.\n         loss = convert_custom_objects(training_config['loss'])\n         metrics = convert_custom_objects(training_config['metrics'])\n+        # Earlier versions of keras didn't dump weighted_metrics properly. Use\n+        # a get to avoid failing if the key is missing\n+        weighted_metrics = convert_custom_objects(\n+            training_config.get('weighted_metrics'))\n         sample_weight_mode = training_config['sample_weight_mode']\n         loss_weights = training_config['loss_weights']\n \n@@ -326,6 +330,7 @@ def _deserialize_model(h5dict, custom_objects=None, compile=True):\n         model.compile(optimizer=optimizer,\n                       loss=loss,\n                       metrics=metrics,\n+                      weighted_metrics=weighted_metrics,\n                       loss_weights=loss_weights,\n                       sample_weight_mode=sample_weight_mode)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2d2fb47e321faa032c98e92d34e6215b6026f1f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 71 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,18 +9,10 @@ from .. import utils\n \n import keras_applications\n \n-if not hasattr(keras_applications, 'get_submodules_from_kwargs'):\n-    keras_applications.set_keras_submodules(\n-        backend=backend,\n-        layers=layers,\n-        models=models,\n-        utils=utils)\n-\n \n def keras_modules_injection(base_fun):\n \n     def wrapper(*args, **kwargs):\n-        if hasattr(keras_applications, 'get_submodules_from_kwargs'):\n         kwargs['backend'] = backend\n         kwargs['layers'] = layers\n         kwargs['models'] = models\n@@ -42,4 +34,3 @@ from .densenet import DenseNet121, DenseNet169, DenseNet201\n from .nasnet import NASNetMobile, NASNetLarge\n from .resnet import ResNet101, ResNet152\n from .resnet_v2 import ResNet50V2, ResNet101V2, ResNet152V2\n-from .resnext import ResNeXt50, ResNeXt101\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c714efa31658ef43eaddc7b9c4340cb915bf8211", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 27 | Files Changed: 6 | Hunks: 20 | Methods Changed: 9 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 56 | Churn Cumulative: 12277 | Contributors (this commit): 109 | Commits (past 90d): 12 | Contributors (cumulative): 134 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,6 +20,7 @@ from .. import metrics as metrics_module\n from ..utils.generic_utils import slice_arrays\n from ..utils.generic_utils import to_list\n from ..utils.generic_utils import unpack_singleton\n+from ..utils import losses_utils\n from ..legacy import interfaces\n \n \n@@ -122,7 +123,10 @@ class Model(Network):\n                 skip_target_weighing_indices.append(i)\n \n         # Prepare output masks.\n-        masks = [getattr(x, '_keras_mask', None) for x in self.outputs]\n+        masks = self.compute_mask(self.inputs, mask=None)\n+        if masks is None:\n+            masks = [None for _ in self.outputs]\n+        masks = to_list(masks)\n \n         # Prepare list loss weights, same size of model outputs.\n         self.loss_weights_list = training_utils.prepare_loss_weights(\n@@ -214,15 +218,18 @@ class Model(Network):\n                     # custom handling of accuracy/crossentropy\n                     # (because of class mode duality)\n                     output_shape = K.int_shape(self.outputs[i])\n+                    loss_fn = self.loss_functions[i]\n+                    if isinstance(loss_fn, losses.LossFunctionWrapper):\n+                        loss_fn = loss_fn.fn\n+\n                     if (output_shape[-1] == 1 or\n-                       self.loss_functions[i] == losses.binary_crossentropy):\n+                       loss_fn == losses.binary_crossentropy):\n                         # case: binary accuracy/crossentropy\n                         if metric in ('accuracy', 'acc'):\n                             metric_fn = metrics_module.binary_accuracy\n                         elif metric in ('crossentropy', 'ce'):\n                             metric_fn = metrics_module.binary_crossentropy\n-                    elif (self.loss_functions[i] ==\n-                          losses.sparse_categorical_crossentropy):\n+                    elif (loss_fn == losses.sparse_categorical_crossentropy):\n                         # case: categorical accuracy/crossentropy\n                         # with sparse targets\n                         if metric in ('accuracy', 'acc'):\n@@ -692,7 +699,7 @@ class Model(Network):\n                 loss_name = self.output_names[i] + '_loss'\n                 with K.name_scope(loss_name):\n                     if mask is not None:\n-                        mask = math_ops.cast(mask, y_pred.dtype)\n+                        mask = K.cast(mask, y_pred.dtype)\n                         # Update weights with mask.\n                         if sample_weight is None:\n                             sample_weight = mask\n\n@@ -498,15 +498,13 @@ def logcosh(y_true, y_pred):\n \n \n def categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0):\n-    if not K.is_tensor(y_pred):\n-        y_pred = K.constant(y_pred)\n+    y_pred = K.constant(y_pred) if not K.is_tensor(y_pred) else y_pred\n     y_true = K.cast(y_true, y_pred.dtype)\n     label_smoothing = K.cast_to_floatx(label_smoothing)\n \n     def _smooth_labels():\n         num_classes = K.cast(K.shape(y_true)[1], y_pred.dtype)\n         return y_true * (1.0 - label_smoothing) + (label_smoothing / num_classes)\n-\n     y_true = K.switch(K.greater(label_smoothing, 0), _smooth_labels, lambda: y_true)\n     return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n \n@@ -517,15 +515,13 @@ def sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1):\n \n \n def binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0):\n-    if not K.is_tensor(y_pred):\n-        y_pred = K.constant(y_pred)\n+    y_pred = K.constant(y_pred) if not K.is_tensor(y_pred) else y_pred\n     y_true = K.cast(y_true, y_pred.dtype)\n     label_smoothing = K.cast_to_floatx(label_smoothing)\n-\n-    def _smooth_labels():\n-        return y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing\n-\n-    y_true = K.switch(K.greater(label_smoothing, 0), _smooth_labels, lambda: y_true)\n+    y_true = K.switch(\n+        K.greater(label_smoothing, 0),\n+        lambda: y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing,\n+        lambda: y_true)\n     return K.mean(\n         K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n \n\n@@ -15,7 +15,7 @@ accepted_name = ['from_config']\n accepted_module = ['keras.legacy.layers', 'keras.utils.generic_utils']\n \n # Functions or classes with less than 'MIN_CODE_SIZE' lines can be ignored\n-MIN_CODE_SIZE = 10\n+MIN_CODE_SIZE = 12\n \n \n def handle_class_init(name, member):\n\n@@ -180,7 +180,7 @@ def test_masked_temporal():\n     The ground-truth best cross-entropy loss should, then be -log(0.5) = 0.69\n \n     '''\n-    np.random.seed(1338)\n+    np.random.seed(1337)\n \n     model = Sequential()\n     model.add(layers.Embedding(10, 10, mask_zero=True))\n\n@@ -43,7 +43,7 @@ def test_vector_classification():\n \n \n def test_vector_classification_functional():\n-    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,\n+    (x_train, y_train), _ = get_test_data(num_train=500,\n                                           num_test=200,\n                                           input_shape=(20,),\n                                           classification=True,\n@@ -59,7 +59,7 @@ def test_vector_classification_functional():\n                   optimizer=keras.optimizers.Adam(1e-3),\n                   metrics=['acc'])\n     history = model.fit(x_train, y_train, epochs=15, batch_size=16,\n-                        validation_data=(x_test, y_test),\n+                        validation_data=(x_train, y_train),\n                         verbose=0)\n     assert(history.history['val_acc'][-1] > 0.8)\n \n\n@@ -135,17 +135,16 @@ def test_sequential(in_tmpdir):\n \n     model.train_on_batch(x_train[:32], y_train[:32])\n \n-    loss = model.evaluate(x_test, y_test)\n+    loss_np = model.evaluate(x_test, y_test)\n+    predict_np = model.predict(x_test)\n \n-    prediction = model.predict_generator(data_generator(x_test, y_test), 1,\n+    generator_pred_np = model.predict_generator(data_generator(x_test, y_test), 1,\n                                                 max_queue_size=2, verbose=1)\n-    gen_loss = model.evaluate_generator(data_generator(x_test, y_test, 50), 1,\n+    generator_loss_np = model.evaluate_generator(data_generator(x_test, y_test, 50), 1,\n                                                  max_queue_size=2)\n-    pred_loss = K.eval(K.mean(losses.get(model.loss)(K.variable(y_test),\n-                                                     K.variable(prediction))))\n \n-    assert(np.isclose(pred_loss, loss))\n-    assert(np.isclose(gen_loss, loss))\n+    assert_allclose(loss_np, generator_loss_np, atol=1e-5)\n+    assert_allclose(predict_np, generator_pred_np, atol=1e-5)\n \n     model.predict(x_test, verbose=0)\n     model.predict_classes(x_test, verbose=0)\n@@ -163,7 +162,7 @@ def test_sequential(in_tmpdir):\n     os.remove(fname)\n \n     nloss = model.evaluate(x_test, y_test, verbose=0)\n-    assert(loss == nloss)\n+    assert(loss_np == nloss)\n \n     # Test serialization\n     config = model.get_config()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1575b8c33bb1c391cd574ccfbc2812bd5bae54ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 40 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 41 | Churn Cumulative: 469 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -68,7 +68,7 @@ def test_image_data_generator_training():\n                   optimizer='rmsprop',\n                   metrics=['accuracy'])\n     history = model.fit_generator(img_gen.flow(x_train, y_train, batch_size=16),\n-                                  epochs=10,\n+                                  epochs=12,\n                                   validation_data=img_gen.flow(x_test, y_test,\n                                                                batch_size=16),\n                                   verbose=0)\n\n@@ -168,45 +168,6 @@ def test_stacked_lstm_char_prediction():\n     assert(generated == alphabet)\n \n \n-def test_masked_temporal():\n-    '''\n-    Confirm that even with masking on both inputs and outputs, cross-entropies are\n-    of the expected scale.\n-\n-    In this task, there are variable length inputs of integers from 1-9, and a random\n-    subset of unmasked outputs. Each of these outputs has a 50% probability of being\n-    the input number unchanged, and a 50% probability of being 2*input%10.\n-\n-    The ground-truth best cross-entropy loss should, then be -log(0.5) = 0.69\n-\n-    '''\n-    np.random.seed(1337)\n-\n-    model = Sequential()\n-    model.add(layers.Embedding(10, 10, mask_zero=True))\n-    model.add(layers.Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='adam')\n-\n-    x = np.random.randint(1, 10, size=(20000, 10))\n-    for rowi in range(x.shape[0]):\n-        padding = np.random.randint(0, x.shape[1] / 2 + 1)\n-        x[rowi, :padding] = 0\n-\n-    # 50% of the time the correct output is the input.\n-    # The other 50% of the time it's 2 * input % 10\n-    y = (x * np.random.randint(1, 3, size=x.shape)) % 10\n-    ys = np.zeros((y.size, 10), dtype='int32')\n-    for i, target in enumerate(y.flat):\n-        ys[i, target] = 1\n-    ys = ys.reshape(y.shape + (10,))\n-\n-    history = model.fit(x, ys, validation_split=0.05, batch_size=10,\n-                        verbose=0, epochs=3)\n-    ground_truth = -np.log(0.5)\n-    assert(np.abs(history.history['loss'][-1] - ground_truth) < 0.06)\n-\n-\n @pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TF backend')\n def test_embedding_with_clipnorm():\n     model = Sequential()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5f37eaac31b8fd6b04db77b4e5de83e05353d810", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1888 | Contributors (this commit): 16 | Commits (past 90d): 2 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -138,9 +138,11 @@ def test_sequential(in_tmpdir):\n     loss_np = model.evaluate(x_test, y_test)\n     predict_np = model.predict(x_test)\n \n-    generator_pred_np = model.predict_generator(data_generator(x_test, y_test), 1,\n+    generator_pred_np = model.predict_generator(\n+        data_generator(x_test, y_test), 1,\n         max_queue_size=2, verbose=1)\n-    generator_loss_np = model.evaluate_generator(data_generator(x_test, y_test, 50), 1,\n+    generator_loss_np = model.evaluate_generator(\n+        data_generator(x_test, y_test, 50), 1,\n         max_queue_size=2)\n \n     assert_allclose(loss_np, generator_loss_np, atol=1e-5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5949aeede9efd19447770cb1acb33192c08b54ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 8 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 9774 | Contributors (this commit): 62 | Commits (past 90d): 2 | Contributors (cumulative): 62 | DMM Complexity: None\n\nDIFF:\n@@ -1222,7 +1222,7 @@ class GRUCell(Layer):\n \n     def __init__(self, units,\n                  activation='tanh',\n-                 recurrent_activation='hard_sigmoid',\n+                 recurrent_activation='sigmoid',\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',\n                  recurrent_initializer='orthogonal',\n@@ -1235,7 +1235,7 @@ class GRUCell(Layer):\n                  bias_constraint=None,\n                  dropout=0.,\n                  recurrent_dropout=0.,\n-                 implementation=1,\n+                 implementation=2,\n                  reset_after=False,\n                  **kwargs):\n         super(GRUCell, self).__init__(**kwargs)\n@@ -1589,7 +1589,7 @@ class GRU(RNN):\n     @interfaces.legacy_recurrent_support\n     def __init__(self, units,\n                  activation='tanh',\n-                 recurrent_activation='hard_sigmoid',\n+                 recurrent_activation='sigmoid',\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',\n                  recurrent_initializer='orthogonal',\n@@ -1603,7 +1603,7 @@ class GRU(RNN):\n                  bias_constraint=None,\n                  dropout=0.,\n                  recurrent_dropout=0.,\n-                 implementation=1,\n+                 implementation=2,\n                  return_sequences=False,\n                  return_state=False,\n                  go_backwards=False,\n@@ -1826,7 +1826,7 @@ class LSTMCell(Layer):\n \n     def __init__(self, units,\n                  activation='tanh',\n-                 recurrent_activation='hard_sigmoid',\n+                 recurrent_activation='sigmoid',\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',\n                  recurrent_initializer='orthogonal',\n@@ -1840,7 +1840,7 @@ class LSTMCell(Layer):\n                  bias_constraint=None,\n                  dropout=0.,\n                  recurrent_dropout=0.,\n-                 implementation=1,\n+                 implementation=2,\n                  **kwargs):\n         super(LSTMCell, self).__init__(**kwargs)\n         self.units = units\n@@ -2145,7 +2145,7 @@ class LSTM(RNN):\n     @interfaces.legacy_recurrent_support\n     def __init__(self, units,\n                  activation='tanh',\n-                 recurrent_activation='hard_sigmoid',\n+                 recurrent_activation='sigmoid',\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',\n                  recurrent_initializer='orthogonal',\n@@ -2160,7 +2160,7 @@ class LSTM(RNN):\n                  bias_constraint=None,\n                  dropout=0.,\n                  recurrent_dropout=0.,\n-                 implementation=1,\n+                 implementation=2,\n                  return_sequences=False,\n                  return_state=False,\n                  go_backwards=False,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#36098bf0d7ca1e8285f63963130877746dc2809d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 35 | Files Changed: 1 | Hunks: 41 | Methods Changed: 16 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 78 | Churn Cumulative: 3088 | Contributors (this commit): 38 | Commits (past 90d): 3 | Contributors (cumulative): 38 | DMM Complexity: 0.875\n\nDIFF:\n@@ -162,19 +162,20 @@ class SGD(Optimizer):\n     learning rate decay, and Nesterov momentum.\n \n     # Arguments\n-        lr: float >= 0. Learning rate.\n+        learning_rate: float >= 0. Learning rate.\n         momentum: float >= 0. Parameter that accelerates SGD\n             in the relevant direction and dampens oscillations.\n         decay: float >= 0. Learning rate decay over each update.\n         nesterov: boolean. Whether to apply Nesterov momentum.\n     \"\"\"\n \n-    def __init__(self, lr=0.01, momentum=0., decay=0.,\n+    def __init__(self, learning_rate=0.01, momentum=0., decay=0.,\n                  nesterov=False, **kwargs):\n+        learning_rate = kwargs.pop('lr', None) or learning_rate\n         super(SGD, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n-            self.lr = K.variable(lr, name='lr')\n+            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n             self.momentum = K.variable(momentum, name='momentum')\n             self.decay = K.variable(decay, name='decay')\n         self.initial_decay = decay\n@@ -186,7 +187,7 @@ class SGD(Optimizer):\n         grads = self.get_gradients(loss, params)\n         self.updates = [K.update_add(self.iterations, 1)]\n \n-        lr = self.lr\n+        lr = self.learning_rate\n         if self.initial_decay > 0:\n             lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                       K.dtype(self.decay))))\n@@ -212,7 +213,7 @@ class SGD(Optimizer):\n         return self.updates\n \n     def get_config(self):\n-        config = {'lr': float(K.get_value(self.lr)),\n+        config = {'learning_rate': float(K.get_value(self.learning_rate)),\n                   'momentum': float(K.get_value(self.momentum)),\n                   'decay': float(K.get_value(self.decay)),\n                   'nesterov': self.nesterov}\n@@ -228,7 +229,7 @@ class RMSprop(Optimizer):\n     (except the learning rate, which can be freely tuned).\n \n     # Arguments\n-        lr: float >= 0. Learning rate.\n+        learning_rate: float >= 0. Learning rate.\n         rho: float >= 0.\n         epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n         decay: float >= 0. Learning rate decay over each update.\n@@ -238,11 +239,12 @@ class RMSprop(Optimizer):\n            ](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n     \"\"\"\n \n-    def __init__(self, lr=0.001, rho=0.9, epsilon=None, decay=0.,\n+    def __init__(self, learning_rate=0.001, rho=0.9, epsilon=None, decay=0.,\n                  **kwargs):\n+        learning_rate = kwargs.pop('lr', None) or learning_rate\n         super(RMSprop, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n-            self.lr = K.variable(lr, name='lr')\n+            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n             self.rho = K.variable(rho, name='rho')\n             self.decay = K.variable(decay, name='decay')\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n@@ -262,7 +264,7 @@ class RMSprop(Optimizer):\n         self.weights = accumulators\n         self.updates = [K.update_add(self.iterations, 1)]\n \n-        lr = self.lr\n+        lr = self.learning_rate\n         if self.initial_decay > 0:\n             lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                       K.dtype(self.decay))))\n@@ -281,7 +283,7 @@ class RMSprop(Optimizer):\n         return self.updates\n \n     def get_config(self):\n-        config = {'lr': float(K.get_value(self.lr)),\n+        config = {'learning_rate': float(K.get_value(self.learning_rate)),\n                   'rho': float(K.get_value(self.rho)),\n                   'decay': float(K.get_value(self.decay)),\n                   'epsilon': self.epsilon}\n@@ -301,7 +303,7 @@ class Adagrad(Optimizer):\n     at their default values.\n \n     # Arguments\n-        lr: float >= 0. Initial learning rate.\n+        learning_rate: float >= 0. Initial learning rate.\n         epsilon: float >= 0. If `None`, defaults to `K.epsilon()`.\n         decay: float >= 0. Learning rate decay over each update.\n \n@@ -310,10 +312,11 @@ class Adagrad(Optimizer):\n            Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n     \"\"\"\n \n-    def __init__(self, lr=0.01, epsilon=None, decay=0., **kwargs):\n+    def __init__(self, learning_rate=0.01, epsilon=None, decay=0., **kwargs):\n+        learning_rate = kwargs.pop('lr', None) or learning_rate\n         super(Adagrad, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n-            self.lr = K.variable(lr, name='lr')\n+            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n             self.decay = K.variable(decay, name='decay')\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n         if epsilon is None:\n@@ -331,7 +334,7 @@ class Adagrad(Optimizer):\n         self.weights = accumulators\n         self.updates = [K.update_add(self.iterations, 1)]\n \n-        lr = self.lr\n+        lr = self.learning_rate\n         if self.initial_decay > 0:\n             lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                       K.dtype(self.decay))))\n@@ -349,7 +352,7 @@ class Adagrad(Optimizer):\n         return self.updates\n \n     def get_config(self):\n-        config = {'lr': float(K.get_value(self.lr)),\n+        config = {'learning_rate': float(K.get_value(self.learning_rate)),\n                   'decay': float(K.get_value(self.decay)),\n                   'epsilon': self.epsilon}\n         base_config = super(Adagrad, self).get_config()\n@@ -371,7 +374,7 @@ class Adadelta(Optimizer):\n     at their default values.\n \n     # Arguments\n-        lr: float >= 0. Initial learning rate, defaults to 1.\n+        learning_rate: float >= 0. Initial learning rate, defaults to 1.\n             It is recommended to leave it at the default value.\n         rho: float >= 0. Adadelta decay factor, corresponding to fraction of\n             gradient to keep at each time step.\n@@ -383,11 +386,12 @@ class Adadelta(Optimizer):\n            https://arxiv.org/abs/1212.5701)\n     \"\"\"\n \n-    def __init__(self, lr=1.0, rho=0.95, epsilon=None, decay=0.,\n+    def __init__(self, learning_rate=1.0, rho=0.95, epsilon=None, decay=0.,\n                  **kwargs):\n+        learning_rate = kwargs.pop('lr', None) or learning_rate\n         super(Adadelta, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n-            self.lr = K.variable(lr, name='lr')\n+            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n             self.decay = K.variable(decay, name='decay')\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n         if epsilon is None:\n@@ -408,7 +412,7 @@ class Adadelta(Optimizer):\n         self.weights = accumulators + delta_accumulators\n         self.updates = [K.update_add(self.iterations, 1)]\n \n-        lr = self.lr\n+        lr = self.learning_rate\n         if self.initial_decay > 0:\n             lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                       K.dtype(self.decay))))\n@@ -434,7 +438,7 @@ class Adadelta(Optimizer):\n         return self.updates\n \n     def get_config(self):\n-        config = {'lr': float(K.get_value(self.lr)),\n+        config = {'learning_rate': float(K.get_value(self.learning_rate)),\n                   'rho': self.rho,\n                   'decay': float(K.get_value(self.decay)),\n                   'epsilon': self.epsilon}\n@@ -448,7 +452,7 @@ class Adam(Optimizer):\n     Default parameters follow those provided in the original paper.\n \n     # Arguments\n-        lr: float >= 0. Learning rate.\n+        learning_rate: float >= 0. Learning rate.\n         beta_1: float, 0 < beta < 1. Generally close to 1.\n         beta_2: float, 0 < beta < 1. Generally close to 1.\n         epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n@@ -464,12 +468,13 @@ class Adam(Optimizer):\n            https://openreview.net/forum?id=ryQu7f-RZ)\n     \"\"\"\n \n-    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n+    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n                  epsilon=None, decay=0., amsgrad=False, **kwargs):\n+        learning_rate = kwargs.pop('lr', None) or learning_rate\n         super(Adam, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n-            self.lr = K.variable(lr, name='lr')\n+            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n             self.decay = K.variable(decay, name='decay')\n@@ -485,7 +490,7 @@ class Adam(Optimizer):\n         grads = self.get_gradients(loss, params)\n         self.updates = [K.update_add(self.iterations, 1)]\n \n-        lr = self.lr\n+        lr = self.learning_rate\n         if self.initial_decay > 0:\n             lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                       K.dtype(self.decay))))\n@@ -535,7 +540,7 @@ class Adam(Optimizer):\n         return self.updates\n \n     def get_config(self):\n-        config = {'lr': float(K.get_value(self.lr)),\n+        config = {'learning_rate': float(K.get_value(self.learning_rate)),\n                   'beta_1': float(K.get_value(self.beta_1)),\n                   'beta_2': float(K.get_value(self.beta_2)),\n                   'decay': float(K.get_value(self.decay)),\n@@ -552,7 +557,7 @@ class Adamax(Optimizer):\n     Default parameters follow those provided in the paper.\n \n     # Arguments\n-        lr: float >= 0. Learning rate.\n+        learning_rate: float >= 0. Learning rate.\n         beta_1: float, 0 < beta < 1. Generally close to 1.\n         beta_2: float, 0 < beta < 1. Generally close to 1.\n         epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n@@ -563,12 +568,13 @@ class Adamax(Optimizer):\n            https://arxiv.org/abs/1412.6980v8)\n     \"\"\"\n \n-    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n+    def __init__(self, learning_rate=0.002, beta_1=0.9, beta_2=0.999,\n                  epsilon=None, decay=0., **kwargs):\n+        learning_rate = kwargs.pop('lr', None) or learning_rate\n         super(Adamax, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n-            self.lr = K.variable(lr, name='lr')\n+            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n             self.decay = K.variable(decay, name='decay')\n@@ -583,7 +589,7 @@ class Adamax(Optimizer):\n         grads = self.get_gradients(loss, params)\n         self.updates = [K.update_add(self.iterations, 1)]\n \n-        lr = self.lr\n+        lr = self.learning_rate\n         if self.initial_decay > 0:\n             lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                       K.dtype(self.decay))))\n@@ -618,7 +624,7 @@ class Adamax(Optimizer):\n         return self.updates\n \n     def get_config(self):\n-        config = {'lr': float(K.get_value(self.lr)),\n+        config = {'learning_rate': float(K.get_value(self.learning_rate)),\n                   'beta_1': float(K.get_value(self.beta_1)),\n                   'beta_2': float(K.get_value(self.beta_2)),\n                   'decay': float(K.get_value(self.decay)),\n@@ -638,7 +644,7 @@ class Nadam(Optimizer):\n     at their default values.\n \n     # Arguments\n-        lr: float >= 0. Learning rate.\n+        learning_rate: float >= 0. Learning rate.\n         beta_1: float, 0 < beta < 1. Generally close to 1.\n         beta_2: float, 0 < beta < 1. Generally close to 1.\n         epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n@@ -650,13 +656,14 @@ class Nadam(Optimizer):\n            http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n     \"\"\"\n \n-    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n+    def __init__(self, learning_rate=0.002, beta_1=0.9, beta_2=0.999,\n                  epsilon=None, schedule_decay=0.004, **kwargs):\n+        learning_rate = kwargs.pop('lr', None) or learning_rate\n         super(Nadam, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n             self.m_schedule = K.variable(1., name='m_schedule')\n-            self.lr = K.variable(lr, name='lr')\n+            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n         if epsilon is None:\n@@ -702,7 +709,8 @@ class Nadam(Optimizer):\n             self.updates.append(K.update(m, m_t))\n             self.updates.append(K.update(v, v_t))\n \n-            p_t = p - self.lr * m_t_bar / (K.sqrt(v_t_prime) + self.epsilon)\n+            p_t = (p - self.learning_rate * m_t_bar / (K.sqrt(v_t_prime) +\n+                   self.epsilon))\n             new_p = p_t\n \n             # Apply constraints.\n@@ -713,7 +721,7 @@ class Nadam(Optimizer):\n         return self.updates\n \n     def get_config(self):\n-        config = {'lr': float(K.get_value(self.lr)),\n+        config = {'learning_rate': float(K.get_value(self.learning_rate)),\n                   'beta_1': float(K.get_value(self.beta_1)),\n                   'beta_2': float(K.get_value(self.beta_2)),\n                   'epsilon': self.epsilon,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ba78b1bf436c2e4d57b44fe7ded87ac2fef65c3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 211 | Lines Deleted: 27 | Files Changed: 2 | Hunks: 22 | Methods Changed: 14 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 238 | Churn Cumulative: 1616 | Contributors (this commit): 11 | Commits (past 90d): 11 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -142,9 +142,6 @@ class LossFunctionWrapper(Loss):\n class MeanSquaredError(LossFunctionWrapper):\n     \"\"\"Computes the mean of squares of errors between labels and predictions.\n \n-    For example, if `y_true` is [0., 0., 1., 1.] and `y_pred` is [1., 1., 1., 0.]\n-    then the mean squared error value is 3/4 (0.75).\n-\n     Standalone usage:\n \n     ```python\n@@ -175,9 +172,6 @@ class MeanSquaredError(LossFunctionWrapper):\n class MeanAbsoluteError(LossFunctionWrapper):\n     \"\"\"Computes the mean of absolute difference between labels and predictions.\n \n-    For example, if `y_true` is [0., 0., 1., 1.] and `y_pred` is [1., 1., 1., 0.]\n-    then the mean absolute error value is 3/4 (0.75).\n-\n     Standalone usage:\n \n     ```python\n@@ -208,9 +202,6 @@ class MeanAbsoluteError(LossFunctionWrapper):\n class MeanAbsolutePercentageError(LossFunctionWrapper):\n     \"\"\"Computes the mean absolute percentage error between `y_true` and `y_pred`.\n \n-    For example, if `y_true` is [0., 0., 1., 1.] and `y_pred` is [1., 1., 1., 0.]\n-    then the mean absolute percentage error value is 5e+08.\n-\n     Standalone usage:\n \n     ```python\n@@ -241,9 +232,6 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n class MeanSquaredLogarithmicError(LossFunctionWrapper):\n     \"\"\"Computes the mean squared logarithmic error between `y_true` and `y_pred`.\n \n-    For example, if `y_true` is [0., 0., 1., 1.] and `y_pred` is [1., 1., 1., 0.]\n-    then the mean squared logarithmic error value is 0.36034.\n-\n     Standalone usage:\n \n     ```python\n@@ -430,6 +418,157 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n             from_logits=from_logits)\n \n \n+class Hinge(LossFunctionWrapper):\n+    \"\"\"Computes the hinge loss between `y_true` and `y_pred`.\n+\n+    `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n+    provided we will convert them to -1 or 1.\n+\n+    Usage with the `compile` API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss=keras.losses.Hinge())\n+    ```\n+    \"\"\"\n+\n+    def __init__(self,\n+                 reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE,\n+                 name='hinge'):\n+        super(Hinge, self).__init__(hinge, name=name, reduction=reduction)\n+\n+\n+class SquaredHinge(LossFunctionWrapper):\n+    \"\"\"Computes the squared hinge loss between `y_true` and `y_pred`.\n+\n+    `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n+    provided we will convert them to -1 or 1.\n+\n+    Usage with the `compile` API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss=keras.losses.SquaredHinge())\n+    ```\n+    \"\"\"\n+\n+    def __init__(self,\n+                 reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE,\n+                 name='squared_hinge'):\n+        super(SquaredHinge, self).__init__(\n+            squared_hinge, name=name, reduction=reduction)\n+\n+\n+class CategoricalHinge(LossFunctionWrapper):\n+    \"\"\"Computes the categorical hinge loss between `y_true` and `y_pred`.\n+\n+    Usage with the `compile` API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss=keras.losses.CategoricalHinge())\n+    ```\n+    \"\"\"\n+\n+    def __init__(self,\n+                 reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE,\n+                 name='categorical_hinge'):\n+        super(CategoricalHinge, self).__init__(\n+            categorical_hinge, name=name, reduction=reduction)\n+\n+\n+class Poisson(LossFunctionWrapper):\n+    \"\"\"Computes the Poisson loss between `y_true` and `y_pred`.\n+\n+    `loss = y_pred - y_true * log(y_pred)`\n+\n+    Usage with the `compile` API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss=keras.losses.Poisson())\n+    ```\n+    \"\"\"\n+\n+    def __init__(self,\n+                 reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE,\n+                 name='poisson'):\n+        super(Poisson, self).__init__(poisson, name=name, reduction=reduction)\n+\n+\n+class LogCosh(LossFunctionWrapper):\n+    \"\"\"Computes the logarithm of the hyperbolic cosine of the prediction error.\n+\n+    `logcosh = log((exp(x) + exp(-x))/2)`,\n+    where x is the error (y_pred - y_true)\n+\n+    Usage with the `compile` API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss=keras.losses.LogCosh())\n+    ```\n+    \"\"\"\n+\n+    def __init__(self,\n+                 reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE,\n+                 name='logcosh'):\n+        super(LogCosh, self).__init__(logcosh, name=name, reduction=reduction)\n+\n+\n+class KLDivergence(LossFunctionWrapper):\n+    \"\"\"Computes Kullback-Leibler divergence loss between `y_true` and `y_pred`.\n+\n+    `loss = y_true * log(y_true / y_pred)`\n+\n+    See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n+\n+    Usage with the `compile` API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss=keras.losses.KLDivergence())\n+    ```\n+    \"\"\"\n+\n+    def __init__(self,\n+                 reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE,\n+                 name='kullback_leibler_divergence'):\n+        super(KLDivergence, self).__init__(\n+            kullback_leibler_divergence, name=name, reduction=reduction)\n+\n+\n+class Huber(LossFunctionWrapper):\n+    \"\"\"Computes the Huber loss between `y_true` and `y_pred`.\n+\n+    Given `x = y_true - y_pred`:\n+    ```\n+    loss = 0.5 * x^2                  if |x| <= d\n+    loss = 0.5 * d^2 + d * (|x| - d)  if |x| > d\n+    ```\n+    where d is `delta`. See: https://en.wikipedia.org/wiki/Huber_loss\n+\n+    Usage with the `compile` API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss=keras.losses.Huber())\n+    ```\n+\n+    # Arguments\n+        delta: A float, the point where the Huber loss function changes from a\n+            quadratic to linear.\n+        reduction: (Optional) Type of reduction to apply to loss.\n+        name: Optional name for the op.\n+    \"\"\"\n+    def __init__(self,\n+                 delta=1.0,\n+                 reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE,\n+                 name='huber_loss'):\n+        super(Huber, self).__init__(\n+            huber_loss, name=name, reduction=reduction, delta=delta)\n+\n+\n def mean_squared_error(y_true, y_pred):\n     if not K.is_tensor(y_pred):\n         y_pred = K.constant(y_pred)\n@@ -497,6 +636,14 @@ def logcosh(y_true, y_pred):\n     return K.mean(_logcosh(y_pred - y_true), axis=-1)\n \n \n+def huber_loss(y_true, y_pred, delta=1.0):\n+    error = y_pred - y_true\n+    abs_error = K.abs(error)\n+    quadratic = K.minimum(abs_error, delta)\n+    linear = abs_error - quadratic\n+    return 0.5 * K.square(quadratic) + delta * linear\n+\n+\n def categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0):\n     y_pred = K.constant(y_pred) if not K.is_tensor(y_pred) else y_pred\n     y_true = K.cast(y_true, y_pred.dtype)\n\n@@ -8,7 +8,7 @@ from keras.utils import losses_utils\n from keras.utils.generic_utils import custom_object_scope\n \n \n-allobj = [losses.mean_squared_error,\n+all_functions = [losses.mean_squared_error,\n                  losses.mean_absolute_error,\n                  losses.mean_absolute_percentage_error,\n                  losses.mean_squared_logarithmic_error,\n@@ -21,10 +21,26 @@ allobj = [losses.mean_squared_error,\n                  losses.cosine_proximity,\n                  losses.logcosh,\n                  losses.categorical_hinge]\n+all_classes = [\n+    losses.Hinge,\n+    losses.SquaredHinge,\n+    losses.CategoricalHinge,\n+    losses.Poisson,\n+    losses.LogCosh,\n+    losses.KLDivergence,\n+    losses.Huber,\n+    # losses.SparseCategoricalCrossentropy,\n+    losses.BinaryCrossentropy,\n+    losses.MeanSquaredLogarithmicError,\n+    losses.MeanAbsolutePercentageError,\n+    losses.MeanAbsoluteError,\n+    losses.MeanSquaredError,\n+]\n \n \n-class MSE_MAE_loss:\n+class MSE_MAE_loss(object):\n     \"\"\"Loss function with internal state, for testing serialization code.\"\"\"\n+\n     def __init__(self, mse_fraction):\n         self.mse_fraction = mse_fraction\n \n@@ -36,20 +52,20 @@ class MSE_MAE_loss:\n         return {'mse_fraction': self.mse_fraction}\n \n \n-class TestLossFunctions:\n+class TestLossFunctions(object):\n \n-    def test_objective_shapes_3d(self):\n+    @pytest.mark.parametrize('loss_fn', all_functions)\n+    def test_objective_shapes_3d(self, loss_fn):\n         y_a = K.variable(np.random.random((5, 6, 7)))\n         y_b = K.variable(np.random.random((5, 6, 7)))\n-        for obj in allobj:\n-            objective_output = obj(y_a, y_b)\n+        objective_output = loss_fn(y_a, y_b)\n         assert K.eval(objective_output).shape == (5, 6)\n \n-    def test_objective_shapes_2d(self):\n+    @pytest.mark.parametrize('loss_fn', all_functions)\n+    def test_objective_shapes_2d(self, loss_fn):\n         y_a = K.variable(np.random.random((6, 7)))\n         y_b = K.variable(np.random.random((6, 7)))\n-        for obj in allobj:\n-            objective_output = obj(y_a, y_b)\n+        objective_output = loss_fn(y_a, y_b)\n         assert K.eval(objective_output).shape == (6,)\n \n     def test_cce_one_hot(self):\n@@ -143,6 +159,27 @@ class TestLossFunctions:\n         np.allclose(K.eval(loss), 16, atol=1e-2)\n \n \n+class TestLossClasses(object):\n+\n+    @pytest.mark.parametrize('cls', all_classes)\n+    def test_objective_shapes_3d(self, cls):\n+        y_a = K.variable(np.random.random((5, 6, 7)))\n+        y_b = K.variable(np.random.random((5, 6, 7)))\n+        sw = K.variable(np.random.random((5, 6)))\n+        obj_fn = cls(name='test')\n+        objective_output = obj_fn(y_a, y_b, sample_weight=sw)\n+        assert K.eval(objective_output).shape == ()\n+\n+    @pytest.mark.parametrize('cls', all_classes)\n+    def test_objective_shapes_2d(self, cls):\n+        y_a = K.variable(np.random.random((6, 7)))\n+        y_b = K.variable(np.random.random((6, 7)))\n+        sw = K.variable(np.random.random((6,)))\n+        obj_fn = cls(name='test')\n+        objective_output = obj_fn(y_a, y_b, sample_weight=sw)\n+        assert K.eval(objective_output).shape == ()\n+\n+\n class TestMeanSquaredError:\n \n     def test_config(self):\n@@ -219,7 +256,7 @@ class TestMeanSquaredError:\n         assert np.isclose(K.eval(loss), 227.69998, rtol=1e-3)\n \n \n-class TestMeanAbsoluteError:\n+class TestMeanAbsoluteError(object):\n \n     def test_config(self):\n         mae_obj = losses.MeanAbsoluteError(\n@@ -295,7 +332,7 @@ class TestMeanAbsoluteError:\n         assert np.isclose(K.eval(loss), 25.29999, rtol=1e-3)\n \n \n-class TestMeanAbsolutePercentageError:\n+class TestMeanAbsolutePercentageError(object):\n \n     def test_config(self):\n         mape_obj = losses.MeanAbsolutePercentageError(\n@@ -355,7 +392,7 @@ class TestMeanAbsolutePercentageError:\n         assert np.allclose(K.eval(loss), [621.8518, 352.6666], rtol=1e-3)\n \n \n-class TestMeanSquaredLogarithmicError:\n+class TestMeanSquaredLogarithmicError(object):\n \n     def test_config(self):\n         msle_obj = losses.MeanSquaredLogarithmicError(\n@@ -401,7 +438,7 @@ class TestMeanSquaredLogarithmicError:\n         assert np.allclose(K.eval(loss), 0.0, rtol=1e-3)\n \n \n-class TestBinaryCrossentropy:\n+class TestBinaryCrossentropy(object):\n \n     def test_config(self):\n         bce_obj = losses.BinaryCrossentropy(\n@@ -561,7 +598,7 @@ class TestBinaryCrossentropy:\n         assert np.isclose(K.eval(loss), expected_value, atol=1e-3)\n \n \n-class TestCategoricalCrossentropy:\n+class TestCategoricalCrossentropy(object):\n \n     def test_config(self):\n         cce_obj = losses.CategoricalCrossentropy(\n@@ -656,7 +693,7 @@ class TestCategoricalCrossentropy:\n         assert np.isclose(K.eval(loss), expected_value, atol=1e-3)\n \n \n-class TestSparseCategoricalCrossentropy:\n+class TestSparseCategoricalCrossentropy(object):\n \n     def test_config(self):\n         cce_obj = losses.SparseCategoricalCrossentropy(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
