{"custom_id": "scrapy#e112def7546c4e0acee028ed1a702b119806f8ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 39 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 39 | Churn Cumulative: 49 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,39 +0,0 @@\n-\"\"\"Functions for dealing with databases\"\"\"\n-\n-import re\n-\n-import MySQLdb\n-\n-from scrapy.conf import settings\n-from scrapy import log\n-\n-mysql_uri_re = r\"mysql:\\/\\/(?P<user>[^:]+)(:(?P<passwd>[^@]+))?@(?P<host>[^/:]+)(:(?P<port>\\d+))?/(?P<db>.*)$\"\n-\n-def parse_uri(mysql_uri):\n-    \"\"\"Parse mysql URI and return a dict with its parameters\"\"\"\n-    m = re.search(mysql_uri_re, mysql_uri)\n-    if m:\n-        d = m.groupdict()\n-        if d['passwd'] is None:\n-            del(d['passwd'])\n-        if d['port'] is None:\n-            del(d['port'])\n-        else:\n-            d['port'] = int(d['port'])\n-        return d\n-\n-def mysql_connect(db_uri_or_dict, **kwargs):\n-    \"\"\"Connects to a MySQL DB given a mysql URI\"\"\"\n-    if isinstance(db_uri_or_dict, dict):\n-        d = db_uri_or_dict\n-    else:\n-        d = parse_uri(db_uri_or_dict)\n-    if not d:\n-        return\n-    d.update(settings.get(\"MYSQL_CONNECTION_SETTINGS\", {}))\n-    d.update(kwargs)\n-    log.msg(\"Connecting to MySQL: db=%r, host=%r, user=%r\" % (d['db'], \\\n-        d['host'], d['user']), level=log.DEBUG)\n-    conn = MySQLdb.connect(**d)\n-\n-    return conn\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9511a3154e8847d618a3b859c6697b0b3b2bc65d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 35/35 | Churn Δ: 3 | Churn Cumulative: 3 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -84,6 +84,9 @@ class Command(ScrapyCommand):\n                 return\n         else:\n             spider = spiders.create_for_request(request)\n+            if spider is None:\n+                log.msg('Unable to find spider for URL: %s' % args[0], log.ERROR)\n+                return\n \n         scrapymanager.configure()\n         scrapymanager.queue.append_request(request, spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6d06824488f6d84b5efcd420f215eb12daf8a8de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 29 | Files Changed: 7 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/8 | Churn Δ: 96 | Churn Cumulative: 321 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,29 +0,0 @@\n-import unittest\n-from cStringIO import StringIO\n-\n-from scrapy.utils.misc import load_object, arg_to_iter\n-\n-class UtilsMiscTestCase(unittest.TestCase):\n-\n-    def test_load_object(self):\n-        obj = load_object('scrapy.utils.misc.load_object')\n-        assert obj is load_object\n-        self.assertRaises(ImportError, load_object, 'nomodule999.mod.function')\n-        self.assertRaises(NameError, load_object, 'scrapy.utils.misc.load_object999')\n-\n-    def test_arg_to_iter(self):\n-        assert hasattr(arg_to_iter(None), '__iter__')\n-        assert hasattr(arg_to_iter(100), '__iter__')\n-        assert hasattr(arg_to_iter('lala'), '__iter__')\n-        assert hasattr(arg_to_iter([1,2,3]), '__iter__')\n-        assert hasattr(arg_to_iter(l for l in 'abcd'), '__iter__')\n-\n-        self.assertEqual(list(arg_to_iter(None)), [])\n-        self.assertEqual(list(arg_to_iter('lala')), ['lala'])\n-        self.assertEqual(list(arg_to_iter(100)), [100])\n-        self.assertEqual(list(arg_to_iter(l for l in 'abc')), ['a', 'b', 'c'])\n-        self.assertEqual(list(arg_to_iter([1,2,3])), [1,2,3])\n-        self.assertEqual(list(arg_to_iter({'a':1})), [{'a': 1}])\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -0,0 +1,54 @@\n+import unittest\n+from cStringIO import StringIO\n+\n+from scrapy.utils.misc import load_object, arg_to_iter, walk_modules\n+\n+class UtilsMiscTestCase(unittest.TestCase):\n+\n+    def test_load_object(self):\n+        obj = load_object('scrapy.utils.misc.load_object')\n+        assert obj is load_object\n+        self.assertRaises(ImportError, load_object, 'nomodule999.mod.function')\n+        self.assertRaises(NameError, load_object, 'scrapy.utils.misc.load_object999')\n+\n+    def test_walk_modules(self):\n+        mods = walk_modules('scrapy.tests.test_utils_misc.test_walk_modules')\n+        expected = [\n+            'scrapy.tests.test_utils_misc.test_walk_modules',\n+            'scrapy.tests.test_utils_misc.test_walk_modules.mod',\n+            'scrapy.tests.test_utils_misc.test_walk_modules.mod.mod0',\n+            'scrapy.tests.test_utils_misc.test_walk_modules.mod1',\n+        ]\n+        self.assertEquals(set([m.__name__ for m in mods]), set(expected))\n+\n+        mods = walk_modules('scrapy.tests.test_utils_misc.test_walk_modules.mod')\n+        expected = [\n+            'scrapy.tests.test_utils_misc.test_walk_modules.mod',\n+            'scrapy.tests.test_utils_misc.test_walk_modules.mod.mod0',\n+        ]\n+        self.assertEquals(set([m.__name__ for m in mods]), set(expected))\n+\n+        mods = walk_modules('scrapy.tests.test_utils_misc.test_walk_modules.mod1')\n+        expected = [\n+            'scrapy.tests.test_utils_misc.test_walk_modules.mod1',\n+        ]\n+        self.assertEquals(set([m.__name__ for m in mods]), set(expected))\n+\n+        self.assertRaises(ImportError, list, walk_modules('nomodule999'))\n+\n+    def test_arg_to_iter(self):\n+        assert hasattr(arg_to_iter(None), '__iter__')\n+        assert hasattr(arg_to_iter(100), '__iter__')\n+        assert hasattr(arg_to_iter('lala'), '__iter__')\n+        assert hasattr(arg_to_iter([1,2,3]), '__iter__')\n+        assert hasattr(arg_to_iter(l for l in 'abcd'), '__iter__')\n+\n+        self.assertEqual(list(arg_to_iter(None)), [])\n+        self.assertEqual(list(arg_to_iter('lala')), ['lala'])\n+        self.assertEqual(list(arg_to_iter(100)), [100])\n+        self.assertEqual(list(arg_to_iter(l for l in 'abc')), ['a', 'b', 'c'])\n+        self.assertEqual(list(arg_to_iter([1,2,3])), [1,2,3])\n+        self.assertEqual(list(arg_to_iter({'a':1})), [{'a': 1}])\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n\n\n\n\n@@ -2,6 +2,7 @@\n \n import re\n import hashlib\n+from pkgutil import walk_packages\n \n from scrapy.utils.python import flatten\n from scrapy.utils.markup import remove_entities\n@@ -44,6 +45,18 @@ def load_object(path):\n \n     return obj\n \n+def walk_modules(path):\n+    \"\"\"Loads a module and all its submodules given its absolute path and\n+    returns them.\n+\n+    path ie: 'scrapy.contrib.downloadermiddelware.redirect'\n+    \"\"\"\n+    mod = __import__(path, {}, {}, [''])\n+    if hasattr(mod, '__path__'):\n+        for _, path, _ in walk_packages(mod.__path__, mod.__name__ + '.'):\n+            yield __import__(path, {}, {}, [''])\n+    yield mod\n+\n def extract_regex(regex, text, encoding='utf-8'):\n     \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#65c3de8e6acd0a29e6fcfeab600a43d4fdb4d65e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 44 | Churn Cumulative: 281 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import sys\n+import os\n import unittest\n from cStringIO import StringIO\n \n@@ -34,7 +36,22 @@ class UtilsMiscTestCase(unittest.TestCase):\n         ]\n         self.assertEquals(set([m.__name__ for m in mods]), set(expected))\n \n-        self.assertRaises(ImportError, list, walk_modules('nomodule999'))\n+        self.assertRaises(ImportError, walk_modules, 'nomodule999')\n+\n+    def test_walk_modules_egg(self):\n+        egg = os.path.join(os.path.dirname(__file__), 'test.egg')\n+        sys.path.append(egg)\n+        try:\n+            mods = walk_modules('testegg')\n+            expected = [\n+                'testegg.spiders',\n+                'testegg.spiders.a',\n+                'testegg.spiders.b',\n+                'testegg'\n+            ]\n+            self.assertEquals(set([m.__name__ for m in mods]), set(expected))\n+        finally:\n+            sys.path.remove(egg)\n \n     def test_arg_to_iter(self):\n         assert hasattr(arg_to_iter(None), '__iter__')\n\n@@ -2,7 +2,7 @@\n \n import re\n import hashlib\n-from pkgutil import walk_packages\n+from pkgutil import iter_modules\n \n from scrapy.utils.python import flatten\n from scrapy.utils.markup import remove_entities\n@@ -45,17 +45,26 @@ def load_object(path):\n \n     return obj\n \n-def walk_modules(path):\n-    \"\"\"Loads a module and all its submodules given its absolute path and\n-    returns them.\n+def walk_modules(path, load=False):\n+    \"\"\"Loads a module and all its submodules from a the given module path and\n+    returns them. If *any* module throws an exception while importing, that\n+    exception is thrown back.\n \n-    path ie: 'scrapy.contrib.downloadermiddelware.redirect'\n+    For example: walk_modules('scrapy.utils')\n     \"\"\"\n+\n+    mods = []\n     mod = __import__(path, {}, {}, [''])\n+    mods.append(mod)\n     if hasattr(mod, '__path__'):\n-        for _, path, _ in walk_packages(mod.__path__, mod.__name__ + '.'):\n-            yield __import__(path, {}, {}, [''])\n-    yield mod\n+        for _, subpath, ispkg in iter_modules(mod.__path__):\n+            fullpath = path + '.' + subpath\n+            if ispkg:\n+                mods += walk_modules(fullpath)\n+            else:\n+                submod = __import__(fullpath, {}, {}, [''])\n+                mods.append(submod)\n+    return mods\n \n def extract_regex(regex, text, encoding='utf-8'):\n     \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e145ec686c09a011ee54749af8062eb99dc4cdd8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 89 | Files Changed: 10 | Hunks: 26 | Methods Changed: 11 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 121 | Churn Cumulative: 1300 | Contributors (this commit): 6 | Commits (past 90d): 25 | Contributors (cumulative): 29 | DMM Complexity: 0.0\n\nDIFF:\n@@ -36,6 +36,3 @@ class GoogleDirectorySpider(CrawlSpider):\n \n             # Here we populate the item and yield it\n             yield l.load_item()\n-\n-SPIDER = GoogleDirectorySpider()\n-\n\n@@ -137,5 +137,3 @@ class ImdbSiteSpider(CrawlSpider):\n     def _urljoin(self, response, url):\n         \"\"\"Helper to convert relative urls to absolute\"\"\"\n         return urljoin_rfc(response.url, url, response.encoding)\n-\n-SPIDER = ImdbSiteSpider()\n\n@@ -38,5 +38,3 @@ class GoogleDirectorySpider(CrawlSpider):\n \n             # Here we populate the item and yield it\n             yield l.load_item()\n-\n-SPIDER = GoogleDirectorySpider()\n\n@@ -191,7 +191,7 @@ SCHEDULER_ORDER = 'DFO'\n \n SERVICE_QUEUE = 'scrapy.core.queue.KeepAliveExecutionQueue'\n \n-SPIDER_MANAGER_CLASS = 'scrapy.contrib.spidermanager.TwistedPluginSpiderManager'\n+SPIDER_MANAGER_CLASS = 'scrapy.contrib.spidermanager.SpiderManager'\n \n SPIDER_MIDDLEWARES = {}\n \n\n@@ -3,18 +3,16 @@ SpiderManager is the class which locates and manages all website-specific\n spiders\n \"\"\"\n \n-import sys\n+import inspect\n \n-from twisted.plugin import getCache\n-from twisted.python.rebuild import rebuild\n-\n-from scrapy.spider.models import ISpider\n from scrapy import log\n from scrapy.conf import settings\n from scrapy.utils.url import url_is_from_spider\n+from scrapy.utils.misc import walk_modules\n+from scrapy.spider import BaseSpider\n \n-class TwistedPluginSpiderManager(object):\n-    \"\"\"Spider manager based in Twisted Plugin System\"\"\"\n+\n+class SpiderManager(object):\n \n     def __init__(self):\n         self.loaded = False\n@@ -25,9 +23,7 @@ class TwistedPluginSpiderManager(object):\n         spider arguments. If the sipder name is not found, it raises a\n         KeyError.\n         \"\"\"\n-        spider = self._spiders[spider_name]\n-        spider.__dict__.update(spider_kwargs)\n-        return spider\n+        return self._spiders[spider_name](**spider_kwargs)\n \n     def find_by_request(self, request):\n         \"\"\"Returns list of spiders names that match the given Request\"\"\"\n@@ -60,47 +56,23 @@ class TwistedPluginSpiderManager(object):\n         return self._spiders.keys()\n \n     def load(self, spider_modules=None):\n-        \"\"\"Load spiders from module directory.\"\"\"\n+        \"\"\"Load spiders from spider_modules or SPIDER_MODULES setting.\"\"\"\n         if spider_modules is None:\n             spider_modules = settings.getlist('SPIDER_MODULES')\n         self.spider_modules = spider_modules\n-        self._spiders = {}\n \n-        modules = [__import__(m, {}, {}, ['']) for m in self.spider_modules]\n-        for module in modules:\n-            for spider in self._getspiders(ISpider, module):\n-                ISpider.validateInvariants(spider)\n-                self._spiders[spider.name] = spider\n+        self._spiders = {}\n+        for name in self.spider_modules:\n+            for module in walk_modules(name):\n+                self._load_spiders(module)\n         self.loaded = True\n \n-    def _getspiders(self, interface, package):\n-        \"\"\"This is an override of twisted.plugin.getPlugin, because we're\n-        interested in catching exceptions thrown when loading spiders such as\n-        KeyboardInterrupt\n-        \"\"\"\n-        try:\n-            allDropins = getCache(package)\n-            for dropin in allDropins.itervalues():\n-                for plugin in dropin.plugins:\n-                    adapted = interface(plugin, None)\n-                    if adapted is not None:\n-                        yield adapted\n-        except KeyboardInterrupt:\n-            sys.stderr.write(\"Interrupted while loading Scrapy spiders\\n\")\n-            sys.exit(2)\n+    def _load_spiders(self, module):\n+        for obj in vars(module).itervalues():\n+            if inspect.isclass(obj) and issubclass(obj, BaseSpider):\n+                name = getattr(obj, 'name', None)\n+                if name is not None:\n+                    self._spiders[name] = obj\n \n     def close_spider(self, spider):\n-        \"\"\"Reload spider module to release any resources held on to by the\n-        spider\n-        \"\"\"\n-        name = spider.name\n-        if name not in self._spiders:\n-            return\n-        spider = self._spiders[name]\n-        module_name = spider.__module__\n-        module = sys.modules[module_name]\n-        if hasattr(module, 'SPIDER'):\n-            log.msg(\"Reloading module %s\" % module_name, spider=spider, \\\n-                level=log.DEBUG)\n-            new_module = rebuild(module, doLog=0)\n-            self._spiders[name] = new_module.SPIDER\n+        pass\n\n@@ -6,25 +6,17 @@ See documentation in docs/topics/spiders.rst\n \n import warnings\n \n-from zope.interface import Interface, Attribute, invariant, implements\n-from twisted.plugin import IPlugin\n-\n from scrapy import log\n from scrapy.http import Request\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.trackref import object_ref\n \n-class ISpider(Interface, IPlugin) :\n-    \"\"\"Interface used by TwistedPluginSpiderManager to discover spiders\"\"\"\n-    pass\n \n class BaseSpider(object_ref):\n     \"\"\"Base class for scrapy spiders. All spiders must inherit from this\n     class.\n     \"\"\"\n \n-    implements(ISpider)\n-\n     # XXX: class attributes kept for backwards compatibility\n     name = None\n     start_urls = []\n\n@@ -8,12 +8,12 @@ from twisted.trial import unittest\n # ugly hack to avoid cyclic imports of scrapy.spider when running this test\n # alone\n import scrapy.spider\n-from scrapy.contrib.spidermanager import TwistedPluginSpiderManager\n+from scrapy.contrib.spidermanager import SpiderManager\n from scrapy.http import Request\n \n module_dir = os.path.dirname(os.path.abspath(__file__))\n \n-class TwistedPluginSpiderManagerTest(unittest.TestCase):\n+class SpiderManagerTest(unittest.TestCase):\n \n     def setUp(self):\n         orig_spiders_dir = os.path.join(module_dir, 'test_spiders')\n@@ -22,7 +22,7 @@ class TwistedPluginSpiderManagerTest(unittest.TestCase):\n         self.spiders_dir = os.path.join(self.tmpdir, 'test_spiders_xxx')\n         shutil.copytree(orig_spiders_dir, self.spiders_dir)\n         sys.path.append(self.tmpdir)\n-        self.spiderman = TwistedPluginSpiderManager()\n+        self.spiderman = SpiderManager()\n         assert not self.spiderman.loaded\n         self.spiderman.load(['test_spiders_xxx'])\n         assert self.spiderman.loaded\n@@ -42,14 +42,6 @@ class TwistedPluginSpiderManagerTest(unittest.TestCase):\n         self.assertEqual(spider2.__class__.__name__, 'Spider2')\n         self.assertEqual(spider2.foo, 'bar')\n \n-    def test_create_uses_cache(self):\n-        # TwistedPluginSpiderManager uses an internal cache which is\n-        # invalidated in close_spider() but this isn't necessarily the best\n-        # thing to do in all cases.\n-        spider1 = self.spiderman.create(\"spider1\")\n-        spider2 = self.spiderman.create(\"spider1\")\n-        assert spider1 is spider2\n-\n     def test_find_by_request(self):\n         self.assertEqual(self.spiderman.find_by_request(Request('http://scrapy1.org/test')),\n             ['spider1'])\n@@ -60,16 +52,10 @@ class TwistedPluginSpiderManagerTest(unittest.TestCase):\n         self.assertEqual(self.spiderman.find_by_request(Request('http://scrapy999.org/test')),\n             [])\n \n-    def test_close_spider_remove_refs(self):\n-        spider = self.spiderman.create(\"spider1\")\n-        wref = weakref.ref(spider)\n-        assert wref()\n-        self.spiderman.close_spider(spider)\n-        del spider\n-        assert not wref()\n+    def test_load_spider_module(self):\n+        self.spiderman.load(['scrapy.tests.test_contrib_spidermanager.test_spiders.spider1'])\n+        assert len(self.spiderman._spiders) == 1\n \n-    def test_close_spider_invalidates_cache(self):\n-        spider1 = self.spiderman.create(\"spider1\")\n-        self.spiderman.close_spider(spider1)\n-        spider2 = self.spiderman.create(\"spider1\")\n-        assert spider1 is not spider2\n+    def test_load_base_spider(self):\n+        self.spiderman.load(['scrapy.tests.test_contrib_spidermanager.test_spiders.spider0'])\n+        assert len(self.spiderman._spiders) == 0\n\n@@ -0,0 +1,4 @@\n+from scrapy.spider import BaseSpider\n+\n+class Spider0(BaseSpider):\n+    allowed_domains = [\"scrapy1.org\", \"scrapy3.org\"]\n\n@@ -3,5 +3,3 @@ from scrapy.spider import BaseSpider\n class Spider1(BaseSpider):\n     name = \"spider1\"\n     allowed_domains = [\"scrapy1.org\", \"scrapy3.org\"]\n-\n-SPIDER = Spider1()\n\n@@ -3,5 +3,3 @@ from scrapy.spider import BaseSpider\n class Spider2(BaseSpider):\n     name = \"spider2\"\n     allowed_domains = [\"scrapy2.org\", \"scrapy3.org\"]\n-\n-SPIDER = Spider2()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f6eac8e348ef2d616da0d57e544c96b3b61752ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#453e7bf38c497e995a0edeca29574b96906e46d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 183 | Lines Deleted: 69 | Files Changed: 3 | Hunks: 26 | Methods Changed: 32 | Complexity Δ (Sum/Max): 19/23 | Churn Δ: 252 | Churn Cumulative: 444 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,42 +4,64 @@ Scrapy logging facility\n See documentation in docs/topics/logging.rst\n \"\"\"\n import sys\n-from traceback import format_exc\n+import logging\n \n from twisted.python import log\n-from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.conf import settings\n from scrapy.utils.python import unicode_to_str\n  \n # Logging levels\n-SILENT, CRITICAL, ERROR, WARNING, INFO, DEBUG = range(6)\n+DEBUG = logging.DEBUG\n+INFO = logging.INFO\n+WARNING = logging.WARNING\n+ERROR = logging.ERROR\n+CRITICAL = logging.CRITICAL\n+SILENT = CRITICAL + 1\n+\n level_names = {\n-    0: \"SILENT\",\n-    1: \"CRITICAL\",\n-    2: \"ERROR\",\n-    3: \"WARNING\",\n-    4: \"INFO\",\n-    5: \"DEBUG\",\n+    logging.DEBUG: \"DEBUG\",\n+    logging.INFO: \"INFO\",\n+    logging.WARNING: \"WARNING\",\n+    logging.ERROR: \"ERROR\",\n+    logging.CRITICAL: \"CRITICAL\",\n+    SILENT: \"SILENT\",\n }\n \n-BOT_NAME = settings['BOT_NAME']\n-\n-# signal sent when log message is received\n-# args: message, level, spider\n-logmessage_received = object()\n-\n-# default values\n-log_level = DEBUG\n-log_encoding = 'utf-8'\n-\n started = False\n \n+class ScrapyFileLogObserver(log.FileLogObserver):\n+\n+    def __init__(self, f, level=INFO, encoding='utf-8'):\n+        self.level = level\n+        self.encoding = encoding\n+        log.FileLogObserver.__init__(self, f)\n+\n+    def emit(self, eventDict):\n+        if eventDict.get('system') != 'scrapy':\n+            return\n+        level = eventDict.get('logLevel')\n+        if level < self.level:\n+            return\n+        spider = eventDict.get('spider')\n+        message = eventDict.get('message')\n+        lvlname = level_names.get(level, 'NOLEVEL')\n+        if message:\n+            message = [unicode_to_str(x, self.encoding) for x in message]\n+            message[0] = \"%s: %s\" % (lvlname, message[0])\n+        why = eventDict.get('why')\n+        if why:\n+            why = \"%s: %s\" % (lvlname, unicode_to_str(why, self.encoding))\n+        eventDict['message'] = message\n+        eventDict['why'] = why\n+        eventDict['system'] = spider.name if spider else '-'\n+        log.FileLogObserver.emit(self, eventDict)\n+\n def _get_log_level(level_name_or_id=None):\n     if level_name_or_id is None:\n-        lvlname = settings['LOG_LEVEL'] or settings['LOGLEVEL']\n+        lvlname = settings['LOG_LEVEL']\n         return globals()[lvlname]\n-    elif isinstance(level_name_or_id, int) and 0 <= level_name_or_id <= 5:\n+    elif isinstance(level_name_or_id, int):\n         return level_name_or_id\n     elif isinstance(level_name_or_id, basestring):\n         return globals()[level_name_or_id]\n@@ -47,53 +69,31 @@ def _get_log_level(level_name_or_id=None):\n         raise ValueError(\"Unknown log level: %r\" % level_name_or_id)\n \n def start(logfile=None, loglevel=None, logstdout=None):\n-    \"\"\"Initialize and start logging facility\"\"\"\n-    global log_level, log_encoding, started\n-\n+    global started\n     if started or not settings.getbool('LOG_ENABLED'):\n         return\n-    log_level = _get_log_level(loglevel)\n-    log_encoding = settings['LOG_ENCODING']\n     started = True\n \n-    # set log observer\n     if log.defaultObserver: # check twisted log not already started\n-        logfile = logfile or settings['LOG_FILE'] or settings['LOGFILE']\n+        loglevel = _get_log_level(loglevel)\n+        logfile = logfile or settings['LOG_FILE']\n+        file = open(logfile, 'a') if logfile else sys.stderr\n         if logstdout is None:\n             logstdout = settings.getbool('LOG_STDOUT')\n+        sflo = ScrapyFileLogObserver(file, loglevel, settings['LOG_ENCODING'])\n+        log.startLoggingWithObserver(sflo.emit, setStdout=logstdout)\n+        msg(\"Started project: %s\" % settings['BOT_NAME'])\n \n-        file = open(logfile, 'a') if logfile else sys.stderr\n-        log.startLogging(file, setStdout=logstdout)\n-\n-def msg(message, level=INFO, component=BOT_NAME, domain=None, spider=None):\n-    \"\"\"Log message according to the level\"\"\"\n-    if level > log_level:\n-        return\n-    if domain is not None:\n+def msg(message, level=INFO, **kw):\n+    if 'component' in kw:\n         import warnings\n-        warnings.warn(\"'domain' argument of scrapy.log.msg() is deprecated, \" \\\n-            \"use 'spider' argument instead\", DeprecationWarning, stacklevel=2)\n-    dispatcher.send(signal=logmessage_received, message=message, level=level, \\\n-        spider=spider)\n-    system = domain or (spider.name if spider else component)\n-    msg_txt = unicode_to_str(\"%s: %s\" % (level_names[level], message), log_encoding)\n-    log.msg(msg_txt, system=system)\n+        warnings.warn(\"Argument `component` of scrapy.log.msg() is deprecated\", \\\n+            DeprecationWarning, stacklevel=2)\n+    kw.setdefault('system', 'scrapy')\n+    kw['logLevel'] = level\n+    log.msg(message, **kw)\n \n-def exc(message, level=ERROR, component=BOT_NAME, domain=None, spider=None):\n-    message = message + '\\n' + format_exc()\n-    msg(message, level, component, domain, spider)\n-\n-def err(_stuff=None, _why=None, **kwargs):\n-    if ERROR > log_level:\n-        return\n-    domain = kwargs.pop('domain', None)\n-    spider = kwargs.pop('spider', None)\n-    component = kwargs.pop('component', BOT_NAME)\n-    if domain is not None:\n-        import warnings\n-        warnings.warn(\"'domain' argument of scrapy.log.err() is deprecated, \" \\\n-            \"use 'spider' argument instead\", DeprecationWarning, stacklevel=2)\n-    kwargs['system'] = domain or (spider.name if spider else component)\n-    if _why:\n-        _why = unicode_to_str(\"ERROR: %s\" % _why, log_encoding)\n-    log.err(_stuff, _why, **kwargs)\n+def err(_stuff=None, _why=None, **kw):\n+    kw.setdefault('system', 'scrapy')\n+    kw['logLevel'] = kw.pop('level', ERROR)\n+    log.err(_stuff, _why, **kw)\n\n@@ -1,17 +1,129 @@\n-import unittest\n+from cStringIO import StringIO\n+\n+from twisted.python import log as txlog, failure\n+from twisted.trial import unittest\n \n from scrapy import log\n+from scrapy.spider import BaseSpider\n from scrapy.conf import settings\n \n-class ItemTest(unittest.TestCase):\n+class LogTest(unittest.TestCase):\n \n     def test_get_log_level(self):\n         default_log_level = getattr(log, settings['LOG_LEVEL'])\n         self.assertEqual(log._get_log_level(), default_log_level)\n         self.assertEqual(log._get_log_level('WARNING'), log.WARNING)\n         self.assertEqual(log._get_log_level(log.WARNING), log.WARNING)\n-        self.assertRaises(ValueError, log._get_log_level, 99999)\n         self.assertRaises(ValueError, log._get_log_level, object())\n \n+class ScrapyFileLogObserverTest(unittest.TestCase):\n+\n+    level = log.INFO\n+    encoding = 'utf-8'\n+\n+    def setUp(self):\n+        self.f = StringIO()\n+        self.sflo = log.ScrapyFileLogObserver(self.f, self.level, self.encoding)\n+        self.sflo.start()\n+\n+    def tearDown(self):\n+        self.flushLoggedErrors()\n+        self.sflo.stop()\n+\n+    def logged(self):\n+        return self.f.getvalue().strip()[25:]\n+\n+    def first_log_line(self):\n+        logged = self.logged()\n+        return logged.splitlines()[0] if logged else ''\n+\n+    def test_msg_basic(self):\n+        log.msg(\"Hello\")\n+        self.assertEqual(self.logged(), \"[-] INFO: Hello\")\n+\n+    def test_msg_spider(self):\n+        spider = BaseSpider(\"myspider\")\n+        log.msg(\"Hello\", spider=spider)\n+        self.assertEqual(self.logged(), \"[myspider] INFO: Hello\")\n+\n+    def test_msg_level1(self):\n+        log.msg(\"Hello\", level=log.WARNING)\n+        self.assertEqual(self.logged(), \"[-] WARNING: Hello\")\n+\n+    def test_msg_level2(self):\n+        log.msg(\"Hello\", log.WARNING)\n+        self.assertEqual(self.logged(), \"[-] WARNING: Hello\")\n+\n+    def test_msg_wrong_level(self):\n+        log.msg(\"Hello\", level=9999)\n+        self.assertEqual(self.logged(), \"[-] NOLEVEL: Hello\")\n+\n+    def test_msg_level_spider(self):\n+        spider = BaseSpider(\"myspider\")\n+        log.msg(\"Hello\", spider=spider, level=log.WARNING)\n+        self.assertEqual(self.logged(), \"[myspider] WARNING: Hello\")\n+\n+    def test_msg_encoding(self):\n+        log.msg(u\"Price: \\xa3100\")\n+        self.assertEqual(self.logged(), \"[-] INFO: Price: \\xc2\\xa3100\")\n+\n+    def test_msg_ignore_level(self):\n+        log.msg(\"Hello\", level=log.DEBUG)\n+        log.msg(\"World\", level=log.INFO)\n+        self.assertEqual(self.logged(), \"[-] INFO: World\")\n+\n+    def test_msg_ignore_system(self):\n+        txlog.msg(\"Hello\")\n+        self.failIf(self.logged())\n+\n+    def test_msg_ignore_system_err(self):\n+        txlog.msg(\"Hello\")\n+        self.failIf(self.logged())\n+\n+    def test_err_noargs(self):\n+        try:\n+            a = 1/0\n+        except:\n+            log.err()\n+        self.failUnless('Traceback' in self.logged())\n+        self.failUnless('ZeroDivisionError' in self.logged())\n+\n+    def test_err_why(self):\n+        log.err(TypeError(\"bad type\"), \"Wrong type\")\n+        self.assertEqual(self.first_log_line(), \"[-] ERROR: Wrong type\")\n+        self.failUnless('TypeError' in self.logged())\n+        self.failUnless('bad type' in self.logged())\n+\n+    def test_err_why_encoding(self):\n+        log.err(TypeError(\"bad type\"), u\"\\xa3\")\n+        self.assertEqual(self.first_log_line(), \"[-] ERROR: \\xc2\\xa3\")\n+\n+    def test_err_exc(self):\n+        log.err(TypeError(\"bad type\"))\n+        self.failUnless('Unhandled Error' in self.logged())\n+        self.failUnless('TypeError' in self.logged())\n+        self.failUnless('bad type' in self.logged())\n+\n+    def test_err_failure(self):\n+        log.err(failure.Failure(TypeError(\"bad type\")))\n+        self.failUnless('Unhandled Error' in self.logged())\n+        self.failUnless('TypeError' in self.logged())\n+        self.failUnless('bad type' in self.logged())\n+\n+\n+class Latin1ScrapyFileLogObserverTest(ScrapyFileLogObserverTest):\n+\n+    encoding = 'latin-1'\n+\n+    def test_msg_encoding(self):\n+        log.msg(u\"Price: \\xa3100\")\n+        logged = self.f.getvalue().strip()[25:]\n+        self.assertEqual(self.logged(), \"[-] INFO: Price: \\xa3100\")\n+\n+    def test_err_why_encoding(self):\n+        log.err(TypeError(\"bad type\"), u\"\\xa3\")\n+        self.assertEqual(self.first_log_line(), \"[-] ERROR: \\xa3\")\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -1,5 +1,7 @@\n import unittest\n \n+from twisted.python import log as txlog\n+\n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.utils.signal import send_catch_log\n from scrapy import log\n@@ -20,12 +22,12 @@ class SignalUtilsTest(unittest.TestCase):\n             assert arg == 'test'\n             return \"OK\"\n \n-        def log_received(message, level):\n+        def log_received(event):\n             handlers_called.add(log_received)\n-            assert \"test_handler_error\" in message\n-            assert level == log.ERROR\n+            assert \"test_handler_error\" in event['message'][0]\n+            assert event['logLevel'] == log.ERROR\n \n-        dispatcher.connect(log_received, signal=log.logmessage_received)\n+        txlog.addObserver(log_received)\n         dispatcher.connect(test_handler_error, signal=test_signal)\n         dispatcher.connect(test_handler_check, signal=test_signal)\n         result = send_catch_log(test_signal, arg='test')\n@@ -37,7 +39,7 @@ class SignalUtilsTest(unittest.TestCase):\n         self.assert_(isinstance(result[0][1], Exception))\n         self.assertEqual(result[1], (test_handler_check, \"OK\"))\n \n-        dispatcher.disconnect(log_received, signal=log.logmessage_received)\n+        txlog.removeObserver(log_received)\n         dispatcher.disconnect(test_handler_error, signal=test_signal)\n         dispatcher.disconnect(test_handler_check, signal=test_signal)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b2878fbbef321dc7cfc5fb0f71714e2c8fd880ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 220 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,8 @@\n import unittest\n+from scrapy.spider import BaseSpider\n from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, \\\n     url_query_parameter, add_or_replace_parameter, url_query_cleaner, canonicalize_url, \\\n-    urljoin_rfc\n-\n+    urljoin_rfc, url_is_from_spider\n \n class UrlUtilsTest(unittest.TestCase):\n \n@@ -19,6 +19,15 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertFalse(url_is_from_any_domain(url, ['testdomain.com']))\n         self.assertFalse(url_is_from_any_domain(url+'.testdomain.com', ['testdomain.com']))\n \n+    def test_url_is_from_any_domain(self):\n+        spider = BaseSpider(name='example.com', allowed_domains=['example.org', 'example.net'])\n+        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))\n+        self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', spider))\n+        self.assertTrue(url_is_from_spider('http://example.com/some/page.html', spider))\n+        self.assertTrue(url_is_from_spider('http://www.example.org/some/page.html', spider))\n+        self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', spider))\n+        self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', spider))\n+\n     def test_urljoin_rfc(self):\n         self.assertEqual(urljoin_rfc('http://example.com/some/path', 'newpath/test'),\n                                      'http://example.com/some/newpath/test')\n\n@@ -22,7 +22,7 @@ def url_is_from_any_domain(url, domains):\n \n def url_is_from_spider(url, spider):\n     \"\"\"Return True if the url belongs to the given spider\"\"\"\n-    return url_is_from_any_domain(url, spider.allowed_domains)\n+    return url_is_from_any_domain(url, [spider.name] + spider.allowed_domains)\n \n def urljoin_rfc(base, ref, encoding='utf-8'):\n     \"\"\"Same as urlparse.urljoin but supports unicode values in base and ref\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#daa9506b34c392c4afdc6a9569cc959b9993722f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 575 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -42,7 +42,7 @@ class ExecutionManager(object):\n             self._start_spider(spider, requests)\n         if self.engine.has_capacity() and not self._nextcall.active():\n             self._nextcall = reactor.callLater(self.queue.polling_delay, \\\n-                self._start_next_spider)\n+                self._spider_closed)\n \n     @defer.inlineCallbacks\n     def _start_spider(self, spider, requests):\n@@ -64,7 +64,7 @@ class ExecutionManager(object):\n     @defer.inlineCallbacks\n     def start(self):\n         yield defer.maybeDeferred(self.engine.start)\n-        self._nextcall = reactor.callLater(0, self._spider_closed)\n+        self._nextcall = reactor.callLater(0, self._start_next_spider)\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n         if self.control_reactor:\n             reactor.run(installSignalHandlers=False)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4d66f4c6f8ed174de4aa03ec46a86bfd1f33a129", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 22 | Files Changed: 2 | Hunks: 15 | Methods Changed: 10 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 59 | Churn Cumulative: 443 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -38,24 +38,30 @@ class ScrapyFileLogObserver(log.FileLogObserver):\n         log.FileLogObserver.__init__(self, f)\n \n     def emit(self, eventDict):\n-        if eventDict.get('system') != 'scrapy':\n+        ev = eventDict.copy()\n+        if ev['isError']:\n+            ev.setdefault('logLevel', ERROR)\n+        # ignore non-error messages from outside scrapy\n+        if ev.get('system') != 'scrapy' and not ev['isError']:\n             return\n-        level = eventDict.get('logLevel')\n+        level = ev.get('logLevel')\n         if level < self.level:\n             return\n-        spider = eventDict.get('spider')\n-        message = eventDict.get('message')\n+        spider = ev.get('spider')\n+        if spider:\n+            ev['system'] = spider.name\n+        message = ev.get('message')\n         lvlname = level_names.get(level, 'NOLEVEL')\n         if message:\n             message = [unicode_to_str(x, self.encoding) for x in message]\n             message[0] = \"%s: %s\" % (lvlname, message[0])\n-        why = eventDict.get('why')\n+        ev['message'] = message\n+        why = ev.get('why')\n         if why:\n             why = \"%s: %s\" % (lvlname, unicode_to_str(why, self.encoding))\n-        eventDict['message'] = message\n-        eventDict['why'] = why\n-        eventDict['system'] = spider.name if spider else '-'\n-        log.FileLogObserver.emit(self, eventDict)\n+        ev['why'] = why\n+        log.FileLogObserver.emit(self, ev)\n+\n \n def _get_log_level(level_name_or_id=None):\n     if level_name_or_id is None:\n\n@@ -39,7 +39,7 @@ class ScrapyFileLogObserverTest(unittest.TestCase):\n \n     def test_msg_basic(self):\n         log.msg(\"Hello\")\n-        self.assertEqual(self.logged(), \"[-] INFO: Hello\")\n+        self.assertEqual(self.logged(), \"[scrapy] INFO: Hello\")\n \n     def test_msg_spider(self):\n         spider = BaseSpider(\"myspider\")\n@@ -48,15 +48,15 @@ class ScrapyFileLogObserverTest(unittest.TestCase):\n \n     def test_msg_level1(self):\n         log.msg(\"Hello\", level=log.WARNING)\n-        self.assertEqual(self.logged(), \"[-] WARNING: Hello\")\n+        self.assertEqual(self.logged(), \"[scrapy] WARNING: Hello\")\n \n     def test_msg_level2(self):\n         log.msg(\"Hello\", log.WARNING)\n-        self.assertEqual(self.logged(), \"[-] WARNING: Hello\")\n+        self.assertEqual(self.logged(), \"[scrapy] WARNING: Hello\")\n \n     def test_msg_wrong_level(self):\n         log.msg(\"Hello\", level=9999)\n-        self.assertEqual(self.logged(), \"[-] NOLEVEL: Hello\")\n+        self.assertEqual(self.logged(), \"[scrapy] NOLEVEL: Hello\")\n \n     def test_msg_level_spider(self):\n         spider = BaseSpider(\"myspider\")\n@@ -65,12 +65,12 @@ class ScrapyFileLogObserverTest(unittest.TestCase):\n \n     def test_msg_encoding(self):\n         log.msg(u\"Price: \\xa3100\")\n-        self.assertEqual(self.logged(), \"[-] INFO: Price: \\xc2\\xa3100\")\n+        self.assertEqual(self.logged(), \"[scrapy] INFO: Price: \\xc2\\xa3100\")\n \n     def test_msg_ignore_level(self):\n         log.msg(\"Hello\", level=log.DEBUG)\n         log.msg(\"World\", level=log.INFO)\n-        self.assertEqual(self.logged(), \"[-] INFO: World\")\n+        self.assertEqual(self.logged(), \"[scrapy] INFO: World\")\n \n     def test_msg_ignore_system(self):\n         txlog.msg(\"Hello\")\n@@ -90,13 +90,21 @@ class ScrapyFileLogObserverTest(unittest.TestCase):\n \n     def test_err_why(self):\n         log.err(TypeError(\"bad type\"), \"Wrong type\")\n+        self.assertEqual(self.first_log_line(), \"[scrapy] ERROR: Wrong type\")\n+        self.failUnless('TypeError' in self.logged())\n+        self.failUnless('bad type' in self.logged())\n+\n+    def test_error_outside_scrapy(self):\n+        \"\"\"Scrapy logger should still print outside errors\"\"\"\n+        txlog.err(TypeError(\"bad type\"), \"Wrong type\")\n         self.assertEqual(self.first_log_line(), \"[-] ERROR: Wrong type\")\n         self.failUnless('TypeError' in self.logged())\n         self.failUnless('bad type' in self.logged())\n \n-    def test_err_why_encoding(self):\n-        log.err(TypeError(\"bad type\"), u\"\\xa3\")\n-        self.assertEqual(self.first_log_line(), \"[-] ERROR: \\xc2\\xa3\")\n+# this test fails in twisted trial observer, not in scrapy observer\n+#    def test_err_why_encoding(self):\n+#        log.err(TypeError(\"bad type\"), u\"\\xa3\")\n+#        self.assertEqual(self.first_log_line(), \"[scrapy] ERROR: \\xc2\\xa3\")\n \n     def test_err_exc(self):\n         log.err(TypeError(\"bad type\"))\n@@ -118,11 +126,12 @@ class Latin1ScrapyFileLogObserverTest(ScrapyFileLogObserverTest):\n     def test_msg_encoding(self):\n         log.msg(u\"Price: \\xa3100\")\n         logged = self.f.getvalue().strip()[25:]\n-        self.assertEqual(self.logged(), \"[-] INFO: Price: \\xa3100\")\n+        self.assertEqual(self.logged(), \"[scrapy] INFO: Price: \\xa3100\")\n \n-    def test_err_why_encoding(self):\n-        log.err(TypeError(\"bad type\"), u\"\\xa3\")\n-        self.assertEqual(self.first_log_line(), \"[-] ERROR: \\xa3\")\n+# this test fails in twisted trial observer, not in scrapy observer\n+#    def test_err_why_encoding(self):\n+#        log.err(TypeError(\"bad type\"), u\"\\xa3\")\n+#        self.assertEqual(self.first_log_line(), \"[scrapy] ERROR: \\xa3\")\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#35e6c8725bf8cdd35c23d41caaf7c88e31ff3118", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 8 | Files Changed: 5 | Hunks: 11 | Methods Changed: 7 | Complexity Δ (Sum/Max): 21/19 | Churn Δ: 34 | Churn Cumulative: 686 | Contributors (this commit): 4 | Commits (past 90d): 13 | Contributors (cumulative): 11 | DMM Complexity: 0.8888888888888888\n\nDIFF:\n@@ -58,11 +58,11 @@ class Command(ScrapyCommand):\n         for url in urls:\n             spider_names = spiders.find_by_request(Request(url))\n             if not spider_names:\n-                log.msg('Could not find spider for url: %s' % url,\n+                log.msg('Could not find spider that handles url: %s' % url,\n                         log.ERROR)\n             elif len(spider_names) > 1:\n-                log.msg('More than one spider found for url: %s' % url,\n-                        log.ERROR)\n+                log.msg('More than one spider can handle url: %s - %s' % \\\n+                    (url, \", \".join(spider_names)), log.ERROR)\n             else:\n                 spider_urls[spider_names[0]].append(url)\n         return spider_urls.items()\n\n@@ -7,7 +7,6 @@ import inspect\n \n from scrapy import log\n from scrapy.conf import settings\n-from scrapy.utils.url import url_is_from_spider\n from scrapy.utils.misc import walk_modules\n from scrapy.spider import BaseSpider\n \n@@ -28,7 +27,7 @@ class SpiderManager(object):\n     def find_by_request(self, request):\n         \"\"\"Returns list of spiders names that match the given Request\"\"\"\n         return [name for name, spider in self._spiders.iteritems()\n-                if url_is_from_spider(request.url, spider)]\n+                if spider.handles_request(request)]\n \n     def create_for_request(self, request, default_spider=None, \\\n             log_none=False, log_multiple=False, **spider_kwargs):\n@@ -46,9 +45,10 @@ class SpiderManager(object):\n         if len(snames) == 1:\n             return self.create(snames[0], **spider_kwargs)\n         if len(snames) > 1 and log_multiple:\n-            log.msg('More than one spider found for: %s' % request, log.ERROR)\n+            log.msg('More than one spider can handle: %s - %s' % \\\n+                (request, \", \".join(snames)), log.ERROR)\n         if len(snames) == 0 and log_none:\n-            log.msg('Unable to find spider for: %s' % request, log.ERROR)\n+            log.msg('Unable to find spider that handles: %s' % request, log.ERROR)\n         return default_spider\n \n     def list(self):\n\n@@ -10,6 +10,7 @@ from scrapy import log\n from scrapy.http import Request\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.trackref import object_ref\n+from scrapy.utils.url import url_is_from_spider\n \n \n class BaseSpider(object_ref):\n@@ -68,6 +69,10 @@ class BaseSpider(object_ref):\n     def parse(self, response):\n         raise NotImplementedError\n \n+    @classmethod\n+    def handles_request(cls, request):\n+        return url_is_from_spider(request.url, cls)\n+\n     def __str__(self):\n         return \"<%s %r at 0x%0x>\" % (type(self).__name__, self.name, id(self))\n \n\n@@ -33,7 +33,7 @@ class SpiderManagerTest(unittest.TestCase):\n \n     def test_list(self):\n         self.assertEqual(set(self.spiderman.list()),\n-            set(['spider1', 'spider2']))\n+            set(['spider1', 'spider2', 'spider3']))\n \n     def test_create(self):\n         spider1 = self.spiderman.create(\"spider1\")\n@@ -51,6 +51,10 @@ class SpiderManagerTest(unittest.TestCase):\n             set(['spider1', 'spider2']))\n         self.assertEqual(self.spiderman.find_by_request(Request('http://scrapy999.org/test')),\n             [])\n+        self.assertEqual(self.spiderman.find_by_request(Request('http://spider3.com')),\n+            [])\n+        self.assertEqual(self.spiderman.find_by_request(Request('http://spider3.com/onlythis')),\n+            ['spider3'])\n \n     def test_load_spider_module(self):\n         self.spiderman.load(['scrapy.tests.test_contrib_spidermanager.test_spiders.spider1'])\n\n@@ -0,0 +1,9 @@\n+from scrapy.spider import BaseSpider\n+\n+class Spider3(BaseSpider):\n+    name = \"spider3\"\n+    allowed_domains = ['spider3.com']\n+\n+    @classmethod\n+    def handles_request(cls, request):\n+        return request.url == 'http://spider3.com/onlythis'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ba2369bfa1e1e7105243ae962cb7509596d2c103", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 370 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -26,8 +26,8 @@ class SpiderManager(object):\n \n     def find_by_request(self, request):\n         \"\"\"Returns list of spiders names that match the given Request\"\"\"\n-        return [name for name, spider in self._spiders.iteritems()\n-                if spider.handles_request(request)]\n+        return [name for name, cls in self._spiders.iteritems()\n+                if cls.handles_request(request)]\n \n     def create_for_request(self, request, default_spider=None, \\\n             log_none=False, log_multiple=False, **spider_kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c7d9f6e2700a572924bc1880f2f902a714fc0fe7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 17 | Files Changed: 4 | Hunks: 10 | Methods Changed: 10 | Complexity Δ (Sum/Max): 9/7 | Churn Δ: 81 | Churn Cumulative: 1083 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: 0.9333333333333333\n\nDIFF:\n@@ -7,9 +7,12 @@ import pprint\n from cPickle import Pickler\n from xml.sax.saxutils import XMLGenerator\n \n+from scrapy.utils.py26 import json\n+\n \n __all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter', \\\n-    'CsvItemExporter', 'XmlItemExporter']\n+    'CsvItemExporter', 'XmlItemExporter', 'JsonLinesItemExporter', \\\n+    'JsonItemExporter']\n \n class BaseItemExporter(object):\n \n@@ -71,6 +74,41 @@ class BaseItemExporter(object):\n             yield field_name, value\n \n \n+class JsonLinesItemExporter(BaseItemExporter):\n+\n+    def __init__(self, file, **kwargs):\n+        self._configure(kwargs)\n+        self.file = file\n+        self.encoder = json.JSONEncoder(**kwargs)\n+\n+    def export_item(self, item):\n+        itemdict = dict(self._get_serialized_fields(item))\n+        self.file.write(self.encoder.encode(itemdict) + '\\n')\n+\n+\n+class JsonItemExporter(JsonLinesItemExporter):\n+\n+    def __init__(self, file, **kwargs):\n+        self._configure(kwargs)\n+        self.file = file\n+        self.encoder = json.JSONEncoder(**kwargs)\n+        self.first_item = True\n+\n+    def start_exporting(self):\n+        self.file.write(\"[\")\n+\n+    def finish_exporting(self):\n+        self.file.write(\"]\")\n+\n+    def export_item(self, item):\n+        if self.first_item:\n+            self.first_item = False\n+        else:\n+            self.file.write(',\\n')\n+        itemdict = dict(self._get_serialized_fields(item))\n+        self.file.write(self.encoder.encode(itemdict))\n+\n+\n class XmlItemExporter(BaseItemExporter):\n \n     def __init__(self, file, **kwargs):\n\n@@ -1,13 +1,5 @@\n-from scrapy.contrib.exporter import BaseItemExporter\n-from scrapy.utils.py26 import json\n+from scrapy.contrib.exporter import JsonLinesItemExporter\n \n-class JsonLinesItemExporter(BaseItemExporter):\n-\n-    def __init__(self, file, **kwargs):\n-        self._configure(kwargs)\n-        self.file = file\n-        self.encoder = json.JSONEncoder(**kwargs)\n-\n-    def export_item(self, item):\n-        itemdict = dict(self._get_serialized_fields(item))\n-        self.file.write(self.encoder.encode(itemdict) + '\\n')\n+import warnings\n+warnings.warn(\"Module `scrapy.contrib.exporter.jsonlines` is deprecated - use `scrapy.contrib.exporter` instead\",\n+    DeprecationWarning, stacklevel=2)\n\n@@ -8,7 +8,6 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core import signals\n from scrapy.core.exceptions import NotConfigured\n from scrapy.contrib import exporter\n-from scrapy.contrib.exporter import jsonlines\n from scrapy.conf import settings\n \n class FileExportPipeline(object):\n@@ -49,7 +48,9 @@ class FileExportPipeline(object):\n         elif format == 'pickle':\n             exp = exporter.PickleItemExporter(file, **exp_kwargs)\n         elif format == 'json':\n-            exp = jsonlines.JsonLinesItemExporter(file, **exp_kwargs)\n+            exp = exporter.JsonLinesItemExporter(file, **exp_kwargs)\n+        elif format == 'jsonlines':\n+            exp = exporter.JsonItemExporter(file, **exp_kwargs)\n         else:\n             raise NotConfigured(\"Unsupported export format: %s\" % format)\n         return exp, file\n\n@@ -4,9 +4,9 @@ from cStringIO import StringIO\n from scrapy.item import Item, Field\n from scrapy.utils.python import str_to_unicode\n from scrapy.utils.py26 import json\n-from scrapy.contrib.exporter.jsonlines import JsonLinesItemExporter\n from scrapy.contrib.exporter import BaseItemExporter, PprintItemExporter, \\\n-    PickleItemExporter, CsvItemExporter, XmlItemExporter\n+    PickleItemExporter, CsvItemExporter, XmlItemExporter, JsonLinesItemExporter, \\\n+    JsonItemExporter\n \n class TestItem(Item):\n     name = Field()\n@@ -156,6 +156,22 @@ class JsonLinesItemExporterTest(BaseItemExporterTest):\n         exported = json.loads(self.output.getvalue().strip())\n         self.assertEqual(exported, dict(self.i))\n \n+class JsonItemExporterTest(JsonLinesItemExporterTest):\n+\n+    def _get_exporter(self, **kwargs):\n+        return JsonItemExporter(self.output, **kwargs)\n+\n+    def _check_output(self):\n+        exported = json.loads(self.output.getvalue().strip())\n+        self.assertEqual(exported, [dict(self.i)])\n+\n+    def test_two_items(self):\n+        self.ie.start_exporting()\n+        self.ie.export_item(self.i)\n+        self.ie.export_item(self.i)\n+        self.ie.finish_exporting()\n+        exported = json.loads(self.output.getvalue())\n+        self.assertEqual(exported, [dict(self.i), dict(self.i)])\n \n class CustomItemExporterTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
