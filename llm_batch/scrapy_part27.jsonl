{"custom_id": "scrapy#4b2d29a6ce786f71c3bd6a084a21759464a4c7b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 193 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -112,7 +112,7 @@ html_style = 'default.css'\n # Add any paths that contain custom static files (such as style sheets) here,\n # relative to this directory. They are copied after the builtin static files,\n # so a file named \"default.css\" will overwrite the builtin \"default.css\".\n-html_static_path = ['.static']\n+html_static_path = ['_static']\n \n # If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n # using the given strftime format.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6766a3962873b6532320dac8b83681750f363127", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 67 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,11 +2,13 @@ from django.conf import settings\n from django.conf.urls.defaults import *\n \n from scrapyorg.docs.views import document\n+from scrapyorg.docs.views import images\n \n \n urlpatterns = patterns('',\n     url(r'^$', document, {'url': ''}),\n     url(r'^(?P<url>[\\w./-]*)/$', document),\n+    url(r'^_images/(?P<path>.*)$', images),\n )\n \n if settings.DEBUG: # devel\n\n@@ -1,6 +1,7 @@\n import cPickle as pickle\n import os\n \n+import django.views.static\n from django.conf import settings\n from django.http import Http404\n from django.shortcuts import render_to_response\n@@ -22,3 +23,11 @@ def document(request, url):\n \n     return render_to_response('docs/doc.html', {'doc': doc},\n                               context_instance=RequestContext(request))\n+\n+\n+def images(request, path):\n+    return django.views.static.serve(\n+\t    request,\n+        document_root = os.path.join(settings.DOC_PICKLE_ROOT, '_images'),\n+        path = path,\n+    )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#570f49d4071983c901e0d7f2ee8c350fb15d1637", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 34 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,11 +12,13 @@ class Command(ScrapyCommand):\n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_option(\"-p\", \"--path\", dest=\"path\", help=\"restrict stats to PATH\", metavar=\"PATH\")\n+        parser.add_option(\"--limit\", dest=\"limit\", default='10', help=\"limit the number of stats\")\n \n     def run(self, args, opts):\n         if not args:\n             print \"A domain is required\"\n             return\n+\n         if not settings['SCRAPING_DB']:\n             print \"SCRAPING_DB setting is required for this command\"\n             return\n@@ -26,5 +28,5 @@ class Command(ScrapyCommand):\n \n         for domain in args:\n             print \"# %s\" % domain\n-            pprint.pprint(list(ddh.getall(domain, opts.path)))\n+            pprint.pprint(list(ddh.get(domain, path=opts.path, count=int(opts.limit))))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#770094697a2c1ac060a31049ae24f9ecb6f3909b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 1804 | Contributors (this commit): 2 | Commits (past 90d): 32 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -56,6 +56,9 @@ class AdaptorPipe(list):\n         elif callable(other):\n             return AdaptorPipe(self + [other])\n \n+    def __repr__(self):\n+        return '<AdaptorPipe %s >' % super(AdaptorPipe, self).__repr__()\n+\n def adaptize(adaptor, my_args=None):\n     \"\"\"\n     This decorator helps you add to your pipelines adaptors that are able\n\n@@ -14,7 +14,7 @@ class ScrapedItem(object):\n         Set the adaptors to use for this item. Receives a dict of the adaptors\n         desired for each attribute and returns the item itself.\n         \"\"\"\n-        _adaptors_dict = dict(item for item in adaptors_dict.items() if isinstance(item[1], AdaptorPipe))\n+        _adaptors_dict = dict((attrib, AdaptorPipe(pipe)) for attrib, pipe in adaptors_dict.items() if hasattr(pipe, '__iter__'))\n         self.__dict__['_adaptors_dict'] = _adaptors_dict\n         return self\n     \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#08150f5acb5e9fa60c4304d31aec4cc7fab9d206", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 49 | Churn Cumulative: 406 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@ import unittest\n \n from scrapy.utils.markup import remove_entities, replace_tags, remove_comments\n from scrapy.utils.markup import remove_tags_with_content, remove_escape_chars, remove_tags\n+from scrapy.utils.markup import unquote_markup\n \n class UtilsMarkupTest(unittest.TestCase):\n \n@@ -111,3 +112,22 @@ class UtilsMarkupTest(unittest.TestCase):\n         self.assertEqual(remove_escape_chars(u'escape\\n', which_ones=('\\t',)), u'escape\\n')\n         self.assertEqual(remove_escape_chars(u'escape\\tchars\\n', which_ones=('\\t')), 'escapechars\\n')\n \n+    def test_unquote_markup(self):\n+        sample_txt1 = u\"\"\"<node1>hi, this is sample text with entities: &amp; &copy;\n+<![CDATA[although this is inside a cdata! &amp; &quot;]]></node1>\"\"\"\n+        sample_txt2 = u'<node2>blah&amp;blah<![CDATA[blahblahblah!&pound;]]>moreblah&lt;&gt;</node2>'\n+        sample_txt3 = u'something&pound;&amp;more<node3><![CDATA[things, stuff, and such]]>what&quot;ever</node3><node4'\n+\n+        # make sure it always return unicode\n+        assert isinstance(unquote_markup(sample_txt1.encode('latin-1')), unicode)\n+        assert isinstance(unquote_markup(sample_txt2), unicode)\n+\n+        self.assertEqual(unquote_markup(sample_txt1), u\"\"\"<node1>hi, this is sample text with entities: & \\xa9\n+although this is inside a cdata! &amp; &quot;</node1>\"\"\")\n+\n+        self.assertEqual(unquote_markup(sample_txt2), u'<node2>blah&blahblahblahblah!&pound;moreblah<></node2>')\n+\n+        self.assertEqual(unquote_markup(sample_txt1 + sample_txt2), u\"\"\"<node1>hi, this is sample text with entities: & \\xa9\n+although this is inside a cdata! &amp; &quot;</node1><node2>blah&blahblahblahblah!&pound;moreblah<></node2>\"\"\")\n+\n+        self.assertEqual(unquote_markup(sample_txt3), u'something\\xa3&more<node3>things, stuff, and suchwhat\"ever</node3><node4')\n\n@@ -98,3 +98,32 @@ def remove_escape_chars(text, which_ones=('\\n','\\t','\\r')):\n     re_escape_chars = re.compile('[%s]' % ''.join(which_ones))\n     return re_escape_chars.sub(u'', text.decode('utf-8'))\n \n+def unquote_markup(text):\n+    \"\"\"\n+    This function receives markup as a text and does the following:\n+     - removes entities from any part of it that it's not inside a CDATA\n+     - searches for CDATAs and extracts their text (if any) without modifying it.\n+     - removes the found CDATAs\n+    \"\"\"\n+    _cdata_re = re.compile(r'((?P<cdata_s><!\\[CDATA\\[)(?P<cdata_d>.*?)(?P<cdata_e>\\]\\]>))', re.DOTALL)\n+\n+    def _get_fragments(txt, pattern):\n+        fragments = []\n+        offset = 0\n+        for match in pattern.finditer(txt):\n+            match_s, match_e = match.span(1)\n+            fragments.append(txt[offset:match_s])\n+            fragments.append(match)\n+            offset = match_e\n+        fragments.append(txt[offset:])\n+        return fragments\n+\n+    ret_text = ''\n+    for fragment in _get_fragments(text.decode('utf-8'), _cdata_re):\n+        if isinstance(fragment, basestring):\n+            # it's not a CDATA (so we try to remove its entities)\n+            ret_text += remove_entities(fragment)\n+        else:\n+            # it's a CDATA (so we just extract its content)\n+            ret_text += fragment.group('cdata_d')\n+    return unicode(ret_text)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bda22a539c9e371c05bf776264d86da9323c8ee4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 273 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -98,10 +98,10 @@ def remove_escape_chars(text, which_ones=('\\n','\\t','\\r')):\n     re_escape_chars = re.compile('[%s]' % ''.join(which_ones))\n     return re_escape_chars.sub(u'', text.decode('utf-8'))\n \n-def unquote_markup(text):\n+def unquote_markup(text, keep=(), remove_illegal=True):\n     \"\"\"\n     This function receives markup as a text and does the following:\n-     - removes entities from any part of it that it's not inside a CDATA\n+     - removes entities (except the ones in 'keep') from any part of it that it's not inside a CDATA\n      - searches for CDATAs and extracts their text (if any) without modifying it.\n      - removes the found CDATAs\n     \"\"\"\n@@ -122,7 +122,7 @@ def unquote_markup(text):\n     for fragment in _get_fragments(text.decode('utf-8'), _cdata_re):\n         if isinstance(fragment, basestring):\n             # it's not a CDATA (so we try to remove its entities)\n-            ret_text += remove_entities(fragment)\n+            ret_text += remove_entities(fragment, keep=keep, remove_illegal=remove_illegal)\n         else:\n             # it's a CDATA (so we just extract its content)\n             ret_text += fragment.group('cdata_d')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a98ba31816fa4502c359a4b83ddec1c9262760e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 398 | Contributors (this commit): 2 | Commits (past 90d): 20 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,6 +22,7 @@ class Command(ScrapyCommand):\n         parser.add_option(\"-c\", \"--callbacks\", dest=\"callbacks\", action=\"store\", help=\"use the provided callback(s) for parsing the url (separated with commas)\")\n \n     def process_options(self, args, opts):\n+        super(Command, self).process_options(args, opts)\n         self.callbacks = opts.callbacks.split(',') if opts.callbacks else []\n \n     def pipeline_process(self, item, spider, opts):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d76ff606b9dd182c19cbd728ac6a3067b249793", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 88 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 11/8 | Churn Δ: 91 | Churn Cumulative: 351 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n import os\n import unittest\n+import libxml2\n \n from scrapy.utils.iterators import csviter, xmliter\n from scrapy.http import Response\n@@ -33,6 +34,53 @@ class UtilsXmlTestCase(unittest.TestCase):\n         self.assertEqual([x.x(\"text()\").extract() for x in xmliter(body, 'product')],\n                          [[u'one'], [u'two']])\n \n+    def test_iterator_namespaces(self):\n+        body = \"\"\"\n+            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+            <rss version=\"2.0\" xmlns:g=\"http://base.google.com/ns/1.0\">\n+                <channel>\n+                <title>My Dummy Company</title>\n+                <link>http://www.mydummycompany.com</link>\n+                <description>This is a dummy company. We do nothing.</description>\n+                <item>\n+                    <title>Item 1</title>\n+                    <description>This is item 1</description>\n+                    <link>http://www.mydummycompany.com/items/1</link>\n+                    <g:image_link>http://www.mydummycompany.com/images/item1.jpg</g:image_link>\n+                    <g:id>ITEM_1</g:id>\n+                    <g:price>400</g:price>\n+                </item>\n+                <item>\n+                    <title>Item 2</title>\n+                    <description>This is item 2</description>\n+                    <link>http://www.mydummycompany.com/items/2</link>\n+                    <g:image_link>http://www.mydummycompany.com/images/item2.jpg</g:image_link>\n+                    <g:id>ITEM_2</g:id>\n+                    <g:price>100</g:price>\n+                </item>\n+                </channel>\n+            </rss>\n+        \"\"\"\n+        response = Response(domain='mydummycompany.com', url='http://mydummycompany.com', body=body)\n+        my_iter = xmliter(response, 'item')\n+\n+        node = my_iter.next()\n+        node.register_namespace('g', 'http://base.google.com/ns/1.0')\n+        self.assertEqual(node.x('title/text()').extract(), ['Item 1'])\n+        self.assertEqual(node.x('description/text()').extract(), ['This is item 1'])\n+        self.assertEqual(node.x('link/text()').extract(), ['http://www.mydummycompany.com/items/1'])\n+        self.assertEqual(node.x('g:image_link/text()').extract(), ['http://www.mydummycompany.com/images/item1.jpg'])\n+        self.assertEqual(node.x('g:id/text()').extract(), ['ITEM_1'])\n+        self.assertEqual(node.x('g:price/text()').extract(), ['400'])\n+\n+        node = my_iter.next()\n+        self.assertEqual(node.x('title/text()').extract(), ['Item 2'])\n+        self.assertEqual(node.x('description/text()').extract(), ['This is item 2'])\n+        self.assertEqual(node.x('link/text()').extract(), ['http://www.mydummycompany.com/items/2'])\n+        self.assertRaises(libxml2.xpathError, node.x, 'g:image_link/text()')\n+        self.assertRaises(libxml2.xpathError, node.x, 'g:id/text()')\n+        self.assertRaises(libxml2.xpathError, node.x, 'g:price/text()')\n+\n     def test_iterator_exception(self):\n         body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n         \n\n@@ -3,6 +3,7 @@ import re, csv\n from scrapy.xpath import XmlXPathSelector\n from scrapy.http import Response\n from scrapy import log\n+from scrapy.utils.python import re_rsearch\n \n def _normalize_input(obj):\n     assert isinstance(obj, (Response, basestring)), \"obj must be Response or basestring, not %s\" % type(obj).__name__\n@@ -22,12 +23,19 @@ def xmliter(obj, nodename):\n     - a unicode string\n     - a string encoded as utf-8\n     \"\"\"\n+    HEADER_START_RE = re.compile(r'^(.*?)<\\s*%s(?:\\s|>)' % nodename, re.S)\n+    HEADER_END_RE = re.compile(r'<\\s*/%s\\s*>' % nodename, re.S)\n     text = _normalize_input(obj)\n \n+    header_start = re.search(HEADER_START_RE, text)\n+    header_start = header_start.group(1).strip() if header_start else ''\n+    header_end = re_rsearch(HEADER_END_RE, text)\n+    header_end = text[header_end[1]:].strip() if header_end else ''\n+\n     r = re.compile(r\"<%s[\\s>].*?</%s>\" % (nodename, nodename), re.DOTALL)\n     for match in r.finditer(text):\n-        nodetext = match.group()\n-        yield XmlXPathSelector(text=nodetext).x('/' + nodename)[0]\n+        nodetext = header_start + match.group() + header_end\n+        yield XmlXPathSelector(text=nodetext).x('//' + nodename)[0]\n \n def csviter(obj, delimiter=None, headers=None):\n     \"\"\" Returns an iterator of dictionaries from the given csv object\n\n@@ -1,7 +1,7 @@\n \"\"\"\n This module contains essential stuff that should've come with Python itself ;)\n \"\"\"\n-\n+import re\n from sgmllib import SGMLParser\n \n class FixedSGMLParser(SGMLParser):\n@@ -53,3 +53,32 @@ def unique(list_):\n     return result\n \n \n+def re_rsearch(pattern, text, chunk_size=1024):\n+    \"\"\"\n+    This function does a reverse search in a text using a regular expression\n+    given in the attribute 'pattern'.\n+    Since the re module does not provide this functionality, we have to find for\n+    the expression into chunks of text extracted from the end (for the sake of efficiency).\n+    At first, a chunk of 'chunk_size' kilobytes is extracted from the end, and searched for\n+    the pattern. If the pattern is not found, another chunk is extracted, and another\n+    search is performed.\n+    This process continues until a match is found, or until the whole file is read.\n+    In case the pattern wasn't found, None is returned, otherwise it returns a tuple containing\n+    the start position of the match, and the ending (regarding the entire text).\n+    \"\"\"\n+    def _chunk_iter():\n+        offset = text_len = len(text)\n+        while True:\n+            offset -= (chunk_size * 1024)\n+            if offset <= 0:\n+                break\n+            yield (text[offset:], offset)\n+        yield (text, 0)\n+\n+    pattern = re.compile(pattern) if isinstance(pattern, basestring) else pattern\n+    for chunk, offset in _chunk_iter():\n+        matches = [match for match in pattern.finditer(chunk)]\n+        if matches:\n+            return (offset + matches[-1].span()[0], offset + matches[-1].span()[1])\n+    return None\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ac7ecb7810bbfcf7b223ebf868fcbb94714a0dc8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 111 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,6 +26,11 @@ class XMLFeedSpider(BaseSpider):\n         of results (Items or Requests).\"\"\"\n         return results\n \n+    def adapt_response(self, response):\n+        \"\"\"You can override this function in order to make any changes you want\n+        to into the feed before parsing it. This function must return a response.\"\"\"\n+        return response\n+\n     def parse_nodes(self, response, nodes):\n         for xSel in nodes:\n             ret = self.parse_item(response, xSel)\n@@ -40,6 +45,7 @@ class XMLFeedSpider(BaseSpider):\n         if not hasattr(self, 'parse_item'):\n             raise NotConfigured('You must define parse_item method in order to scrape this XML feed')\n \n+        response = self.adapt_response(response)\n         if self.iternodes:\n             nodes = xmliter(response, self.itertag)\n         else:\n@@ -64,8 +70,7 @@ class CSVFeedSpider(BaseSpider):\n         return results\n \n     def adapt_response(self, response):\n-        \"\"\"You can override this function in order to make any changes you want\n-        to into the feed before parsing it. This function must return a response.\"\"\"\n+        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n         return response\n \n     def parse_rows(self, response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#70af2e8e1d00755c68e8aa701b9c1419604a5395", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 86 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n import re\n-from scrapy.utils.markup import replace_tags, remove_entities\n+from scrapy.utils.markup import replace_tags, unquote_markup\n from scrapy.item.adaptors import adaptize\n \n def remove_tags(value):\n@@ -29,8 +29,8 @@ def remove_root(value):\n class Unquote(object):\n     \"\"\"\n     Receives a list of strings, removes all of the\n-    entities the strings may have, and returns\n-    a new list\n+    CDATAs and entities (except the ones in CDATAs) the strings\n+    may have, and returns a new list.\n \n     Input: iterable with strings\n     Output: list of strings\n@@ -41,4 +41,4 @@ class Unquote(object):\n     def __call__(self, value, keep=None):\n         if keep is not None:\n             self.keep = keep\n-        return [ remove_entities(v, keep=self.keep) for v in value ]\n+        return [ unquote_markup(v, keep=self.keep) for v in value ]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bd8bd25cd89a7d9ea32359e794f500d45fdd0999", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 29 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 45 | Churn Cumulative: 210 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,18 +8,18 @@ from scrapy.http import Response\n class UtilsXmlTestCase(unittest.TestCase):\n     ### NOTE: Encoding issues have been found with BeautifulSoup for utf-16 files, utf-16 test removed ###\n     def test_iterator(self):\n-        body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<products xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"someschmea.xsd\">\n-  <product id=\"001\">\n-    <type>Type 1</type>\n-    <name>Name 1</name>\n-  </product>\n-  <product id=\"002\">\n-    <type>Type 2</type>\n-    <name>Name 2</name>\n-  </product>\n-</products>\n-        \"\"\"\n+        body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\\n+            <products xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"someschmea.xsd\">\\\n+              <product id=\"001\">\\\n+                <type>Type 1</type>\\\n+                <name>Name 1</name>\\\n+              </product>\\\n+              <product id=\"002\">\\\n+                <type>Type 2</type>\\\n+                <name>Name 2</name>\\\n+              </product>\\\n+            </products>\"\"\"\n+\n         response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n         attrs = []\n         for x in xmliter(response, 'product'):\n@@ -35,7 +35,7 @@ class UtilsXmlTestCase(unittest.TestCase):\n                          [[u'one'], [u'two']])\n \n     def test_iterator_namespaces(self):\n-        body = \"\"\"\n+        body = \"\"\"\\\n             <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n             <rss version=\"2.0\" xmlns:g=\"http://base.google.com/ns/1.0\">\n                 <channel>\n@@ -50,14 +50,6 @@ class UtilsXmlTestCase(unittest.TestCase):\n                     <g:id>ITEM_1</g:id>\n                     <g:price>400</g:price>\n                 </item>\n-                <item>\n-                    <title>Item 2</title>\n-                    <description>This is item 2</description>\n-                    <link>http://www.mydummycompany.com/items/2</link>\n-                    <g:image_link>http://www.mydummycompany.com/images/item2.jpg</g:image_link>\n-                    <g:id>ITEM_2</g:id>\n-                    <g:price>100</g:price>\n-                </item>\n                 </channel>\n             </rss>\n         \"\"\"\n@@ -72,14 +64,9 @@ class UtilsXmlTestCase(unittest.TestCase):\n         self.assertEqual(node.x('g:image_link/text()').extract(), ['http://www.mydummycompany.com/images/item1.jpg'])\n         self.assertEqual(node.x('g:id/text()').extract(), ['ITEM_1'])\n         self.assertEqual(node.x('g:price/text()').extract(), ['400'])\n-\n-        node = my_iter.next()\n-        self.assertEqual(node.x('title/text()').extract(), ['Item 2'])\n-        self.assertEqual(node.x('description/text()').extract(), ['This is item 2'])\n-        self.assertEqual(node.x('link/text()').extract(), ['http://www.mydummycompany.com/items/2'])\n-        self.assertRaises(libxml2.xpathError, node.x, 'g:image_link/text()')\n-        self.assertRaises(libxml2.xpathError, node.x, 'g:id/text()')\n-        self.assertRaises(libxml2.xpathError, node.x, 'g:price/text()')\n+        self.assertEqual(node.x('image_link/text()').extract(), [])\n+        self.assertEqual(node.x('id/text()').extract(), [])\n+        self.assertEqual(node.x('price/text()').extract(), [])\n \n     def test_iterator_exception(self):\n         body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0cd074c94c7a979f5e0624e1b9212b541bc1a729", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 21 | Files Changed: 2 | Hunks: 12 | Methods Changed: 7 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 42 | Churn Cumulative: 106 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n from twisted.internet import reactor\n \n+import urlparse\n import scrapy\n from scrapy.command import ScrapyCommand\n from scrapy.spider import spiders\n@@ -8,10 +9,9 @@ from scrapy.utils.misc import load_class\n from scrapy.extension import extensions\n from scrapy.conf import settings\n from scrapy.core.manager import scrapymanager\n-from scrapy.http import Request, Response\n-from scrapy.core.downloader.handlers import download_any\n+from scrapy.core.engine import scrapyengine\n+from scrapy.http import Request\n from scrapy.fetcher import get_or_create_spider\n-from scrapy.utils.decompressor import Decompressor\n \n #This code comes from twisted 8. We define here while\n #using old twisted version.\n@@ -58,25 +58,26 @@ class Command(ScrapyCommand):\n         \"\"\" You can use this function to update the local variables that will be available in the scrape console \"\"\"\n         pass\n \n-    def get_url(self, url, decompress=False):\n+    def get_url(self, url):\n+        u = urlparse.urlparse(url)\n+        if u.scheme not in ('http', 'https', 'file'):\n+            print \"Unsupported scheme '%s' in URL: <%s>\" % (u.scheme, url)\n+            return\n+\n+        self.result = None\n+        def _get_response(response):\n+            self.result = response\n+            return []\n \n         print \"Downloading URL...           \",\n-        r = Request(url)\n+        r = Request(url, callback=_get_response)\n         spider = get_or_create_spider(url)\n-        try:\n-            result = blockingCallFromThread(reactor, download_any, r, spider)\n-            if isinstance(result, Response):\n+        blockingCallFromThread(reactor, scrapyengine.crawl, r, spider)\n+        if self.result:\n             print \"Done.\"\n-                if decompress:\n-                    print \"Decompressing response...\",\n-                    d = Decompressor()\n-                    result = d.extract(result)\n-                    print \"Done.\"\n-                result.request = r\n-                self.generate_vars(url, result)\n+            self.result.request = r\n+            self.generate_vars(url, self.result)\n             return True\n-        except Exception, e:\n-            print \"Error: %s\" % e\n \n     def generate_vars(self, url, response):\n         itemcls = load_class(settings['DEFAULT_ITEM_CLASS'])\n@@ -102,7 +103,6 @@ class Command(ScrapyCommand):\n                 print \"   %s: %s\" % (key, val.__class__)\n         print \"Available commands:\"\n         print \"   get <url>: Fetches an url and updates all variables.\"\n-        print \"   getd <url>: Similar to get, but filter with decompress.\"\n         print \"   scrapehelp: Prints this help.\"\n         print '-' * 78\n     \n@@ -125,8 +125,6 @@ class Command(ScrapyCommand):\n                 self.get_url(arg.strip())\n             def _help_magic(shell, _):\n                 self.print_vars()\n-            def _getd_magic(shell, arg):\n-                self.get_url(arg.strip(), decompress=True)\n                 \n             if url:\n                 result = self.get_url(url)\n@@ -139,7 +137,6 @@ class Command(ScrapyCommand):\n                 shell = IPython.Shell.IPShell(argv=[], user_ns=self.user_ns)\n                 ip = shell.IP.getapi()\n                 ip.expose_magic(\"get\", _get_magic)\n-                ip.expose_magic(\"getd\", _getd_magic)\n                 ip.expose_magic(\"scrapehelp\", _help_magic)\n                 shell.mainloop()\n                 reactor.callFromThread(scrapymanager.stop)\n\n@@ -0,0 +1,3 @@\n+from decobot.conf.commands.start import *\n+\n+LOGLEVEL='WARNING'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6d33e12c8b4e0ceb1da325f3e8d3a6308dbadfda", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 149 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -81,6 +81,7 @@ class AWSMiddleware(object):\n         self.secret_key = settings['AWS_SECRET_ACCESS_KEY']\n \n     def process_request(self, request, spider):\n-        if spider.domain_name in 's3.amazonaws.com':\n+        if spider.domain_name == 's3.amazonaws.com' \\\n+                or request.url.hostname.endswith('s3.amazonaws.com'):\n             request.headers['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime())\n             sign_request(request, self.access_key, self.secret_key)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f4cecafc0284734181d9bd9096db5d86fc41f30e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 11 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 297 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@ _tag_re = re.compile(r'<[a-zA-Z\\/!].*?>', re.DOTALL)\n def remove_entities(text, keep=(), remove_illegal=True):\n     \"\"\"Remove entities from the given text.\n \n-    'text' can be a unicode string or a regular string encoded as 'utf-8'\n+    'text' must be a unicode string.\n \n     If 'keep' is passed (with a list of entity names) those entities will\n     be kept (they won't be removed).\n@@ -45,7 +45,7 @@ def remove_entities(text, keep=(), remove_illegal=True):\n             else:\n                 return u'&%s;' % m.group(2)\n \n-    return _ent_re.sub(convert_entity, text.decode('utf-8'))\n+    return _ent_re.sub(convert_entity, text)\n \n def has_entities(text):\n     return bool(_ent_re.search(text))\n@@ -54,16 +54,16 @@ def replace_tags(text, token=''):\n     \"\"\"Replace all markup tags found in the given text by the given token. By\n     default token is a null string so it just remove all tags.\n \n-    'text' can be a unicode string or a regular string encoded as 'utf-8'\n+    'text' must be a unicode string.\n \n     Always returns a unicode string.\n     \"\"\"\n-    return _tag_re.sub(token, text.decode('utf-8'))\n+    return _tag_re.sub(token, text)\n \n \n def remove_comments(text):\n     \"\"\" Remove HTML Comments. \"\"\"\n-    return re.sub('<!--.*?-->', u'', text.decode('utf-8'), re.DOTALL)\n+    return re.sub('<!--.*?-->', u'', text, re.DOTALL)\n       \n def remove_tags(text, which_ones=()):\n     \"\"\" Remove HTML Tags only. \n@@ -77,7 +77,7 @@ def remove_tags(text, which_ones=()):\n     else:\n         reg_exp_remove_tags = '<.*?>'\n     re_tags = re.compile(reg_exp_remove_tags, re.DOTALL)\n-    return re_tags.sub(u'', text.decode('utf-8'))\n+    return re_tags.sub(u'', text)\n \n def remove_tags_with_content(text, which_ones=()):\n     \"\"\" Remove tags and its content.\n@@ -87,7 +87,7 @@ def remove_tags_with_content(text, which_ones=()):\n     \"\"\"\n     tags = [ '<%s.*?</%s>' % (tag,tag) for tag in which_ones ]\n     re_tags_remove = re.compile('|'.join(tags), re.DOTALL)\n-    return re_tags_remove.sub(u'', text.decode('utf-8'))\n+    return re_tags_remove.sub(u'', text)\n \n def remove_escape_chars(text, which_ones=('\\n','\\t','\\r')):\n     \"\"\" Remove escape chars. Default : \\\\n, \\\\t, \\\\r\n@@ -96,11 +96,11 @@ def remove_escape_chars(text, which_ones=('\\n','\\t','\\r')):\n                       By default removes \\n, \\t, \\r.\n     \"\"\"\n     re_escape_chars = re.compile('[%s]' % ''.join(which_ones))\n-    return re_escape_chars.sub(u'', text.decode('utf-8'))\n+    return re_escape_chars.sub(u'', text)\n \n def unquote_markup(text, keep=(), remove_illegal=True):\n     \"\"\"\n-    This function receives markup as a text and does the following:\n+    This function receives markup as a text (always a unicode string) and does the following:\n      - removes entities (except the ones in 'keep') from any part of it that it's not inside a CDATA\n      - searches for CDATAs and extracts their text (if any) without modifying it.\n      - removes the found CDATAs\n@@ -118,12 +118,12 @@ def unquote_markup(text, keep=(), remove_illegal=True):\n         fragments.append(txt[offset:])\n         return fragments\n \n-    ret_text = ''\n-    for fragment in _get_fragments(text.decode('utf-8'), _cdata_re):\n+    ret_text = u''\n+    for fragment in _get_fragments(text, _cdata_re):\n         if isinstance(fragment, basestring):\n             # it's not a CDATA (so we try to remove its entities)\n             ret_text += remove_entities(fragment, keep=keep, remove_illegal=remove_illegal)\n         else:\n             # it's a CDATA (so we just extract its content)\n             ret_text += fragment.group('cdata_d')\n-    return unicode(ret_text)\n+    return ret_text\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e2b08709574f8be3b7092f3cb6ddad946437c546", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 32 | Files Changed: 1 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 32 | Churn Cumulative: 171 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,10 +7,6 @@ from scrapy.utils.markup import unquote_markup\n class UtilsMarkupTest(unittest.TestCase):\n \n     def test_remove_entities(self):\n-        # make sure it always return uncode\n-        assert isinstance(remove_entities('no entities'), unicode)\n-        assert isinstance(remove_entities('Price: &pound;100!'),  unicode)\n-\n         # regular conversions\n         self.assertEqual(remove_entities(u'As low as &#163;100!'),\n                          u'As low as \\xa3100!')\n@@ -31,9 +27,6 @@ class UtilsMarkupTest(unittest.TestCase):\n \n \n     def test_replace_tags(self):\n-        # make sure it always return uncode\n-        assert isinstance(replace_tags('no entities'), unicode)\n-\n         self.assertEqual(replace_tags(u'This text contains <a>some tag</a>'),\n                          u'This text contains some tag')\n \n@@ -45,10 +38,6 @@ class UtilsMarkupTest(unittest.TestCase):\n                          u'Click here')\n \n     def test_remove_comments(self):\n-        # make sure it always return unicode\n-        assert isinstance(remove_comments('without comments'), unicode)\n-        assert isinstance(remove_comments('<!-- with comments -->'), unicode)\n-\n         # text without comments \n         self.assertEqual(remove_comments(u'text without comments'), u'text without comments')\n \n@@ -57,13 +46,6 @@ class UtilsMarkupTest(unittest.TestCase):\n         self.assertEqual(remove_comments(u'Hello<!--World-->'),u'Hello')\n \n     def test_remove_tags(self):\n-        # make sure it always return unicode\n-        assert isinstance(remove_tags('no tags'), unicode)\n-        assert isinstance(remove_tags('no tags', which_ones=('p',)), unicode)\n-        assert isinstance(remove_tags('<p>one tag</p>'), unicode)\n-        assert isinstance(remove_tags('<p>one tag</p>', which_ones=('p')), unicode)\n-        assert isinstance(remove_tags('<a>link</a>', which_ones=('b',)), unicode)\n-\n         # text without tags\n         self.assertEqual(remove_tags(u'no tags'), u'no tags')\n         self.assertEqual(remove_tags(u'no tags', which_ones=('p','b',)), u'no tags')\n@@ -81,12 +63,6 @@ class UtilsMarkupTest(unittest.TestCase):\n                          u'<p align=\"center\" class=\"one\">texty</p>')\n \n     def test_remove_tags_with_content(self):\n-        # make sure it always return unicode\n-        assert isinstance(remove_tags_with_content('no tags'), unicode)\n-        assert isinstance(remove_tags_with_content('no tags', which_ones=('p',)), unicode)\n-        assert isinstance(remove_tags_with_content('<p>one tag</p>', which_ones=('p',)), unicode)\n-        assert isinstance(remove_tags_with_content('<a>link</a>', which_ones=('b',)), unicode)\n-\n         # text without tags\n         self.assertEqual(remove_tags_with_content(u'no tags'), u'no tags')\n         self.assertEqual(remove_tags_with_content(u'no tags', which_ones=('p','b',)), u'no tags')\n@@ -99,10 +75,6 @@ class UtilsMarkupTest(unittest.TestCase):\n                          u'<b>not will removed</b>')\n \n     def test_remove_escape_chars(self):\n-        # make sure it always return unicode\n-        assert isinstance(remove_escape_chars('no ec'), unicode)\n-        assert isinstance(remove_escape_chars('no ec', which_ones=('\\n','\\t',)), unicode)\n-\n         # text without escape chars\n         self.assertEqual(remove_escape_chars(u'no ec'), u'no ec')\n         self.assertEqual(remove_escape_chars(u'no ec', which_ones=('\\n',)), u'no ec')\n@@ -118,10 +90,6 @@ class UtilsMarkupTest(unittest.TestCase):\n         sample_txt2 = u'<node2>blah&amp;blah<![CDATA[blahblahblah!&pound;]]>moreblah&lt;&gt;</node2>'\n         sample_txt3 = u'something&pound;&amp;more<node3><![CDATA[things, stuff, and such]]>what&quot;ever</node3><node4'\n \n-        # make sure it always return unicode\n-        assert isinstance(unquote_markup(sample_txt1.encode('latin-1')), unicode)\n-        assert isinstance(unquote_markup(sample_txt2), unicode)\n-\n         self.assertEqual(unquote_markup(sample_txt1), u\"\"\"<node1>hi, this is sample text with entities: & \\xa9\n although this is inside a cdata! &amp; &quot;</node1>\"\"\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#95ed5d86a2973f06e5606ff9b848a0efffdf0ae3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 18 | Methods Changed: 15 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 62 | Churn Cumulative: 530 | Contributors (this commit): 4 | Commits (past 90d): 13 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,10 @@ from scrapy.utils.markup import unquote_markup\n class UtilsMarkupTest(unittest.TestCase):\n \n     def test_remove_entities(self):\n+        # make sure it always return uncode\n+        assert isinstance(remove_entities('no entities'), unicode)\n+        assert isinstance(remove_entities('Price: &pound;100!'),  unicode)\n+\n         # regular conversions\n         self.assertEqual(remove_entities(u'As low as &#163;100!'),\n                          u'As low as \\xa3100!')\n@@ -27,6 +31,9 @@ class UtilsMarkupTest(unittest.TestCase):\n \n \n     def test_replace_tags(self):\n+        # make sure it always return uncode\n+        assert isinstance(replace_tags('no entities'), unicode)\n+\n         self.assertEqual(replace_tags(u'This text contains <a>some tag</a>'),\n                          u'This text contains some tag')\n \n@@ -38,6 +45,10 @@ class UtilsMarkupTest(unittest.TestCase):\n                          u'Click here')\n \n     def test_remove_comments(self):\n+        # make sure it always return unicode\n+        assert isinstance(remove_comments('without comments'), unicode)\n+        assert isinstance(remove_comments('<!-- with comments -->'), unicode)\n+\n         # text without comments \n         self.assertEqual(remove_comments(u'text without comments'), u'text without comments')\n \n@@ -46,6 +57,13 @@ class UtilsMarkupTest(unittest.TestCase):\n         self.assertEqual(remove_comments(u'Hello<!--World-->'),u'Hello')\n \n     def test_remove_tags(self):\n+        # make sure it always return unicode\n+        assert isinstance(remove_tags('no tags'), unicode)\n+        assert isinstance(remove_tags('no tags', which_ones=('p',)), unicode)\n+        assert isinstance(remove_tags('<p>one tag</p>'), unicode)\n+        assert isinstance(remove_tags('<p>one tag</p>', which_ones=('p')), unicode)\n+        assert isinstance(remove_tags('<a>link</a>', which_ones=('b',)), unicode)\n+\n         # text without tags\n         self.assertEqual(remove_tags(u'no tags'), u'no tags')\n         self.assertEqual(remove_tags(u'no tags', which_ones=('p','b',)), u'no tags')\n@@ -63,6 +81,12 @@ class UtilsMarkupTest(unittest.TestCase):\n                          u'<p align=\"center\" class=\"one\">texty</p>')\n \n     def test_remove_tags_with_content(self):\n+        # make sure it always return unicode\n+        assert isinstance(remove_tags_with_content('no tags'), unicode)\n+        assert isinstance(remove_tags_with_content('no tags', which_ones=('p',)), unicode)\n+        assert isinstance(remove_tags_with_content('<p>one tag</p>', which_ones=('p',)), unicode)\n+        assert isinstance(remove_tags_with_content('<a>link</a>', which_ones=('b',)), unicode)\n+\n         # text without tags\n         self.assertEqual(remove_tags_with_content(u'no tags'), u'no tags')\n         self.assertEqual(remove_tags_with_content(u'no tags', which_ones=('p','b',)), u'no tags')\n@@ -75,6 +99,10 @@ class UtilsMarkupTest(unittest.TestCase):\n                          u'<b>not will removed</b>')\n \n     def test_remove_escape_chars(self):\n+        # make sure it always return unicode\n+        assert isinstance(remove_escape_chars('no ec'), unicode)\n+        assert isinstance(remove_escape_chars('no ec', which_ones=('\\n','\\t',)), unicode)\n+\n         # text without escape chars\n         self.assertEqual(remove_escape_chars(u'no ec'), u'no ec')\n         self.assertEqual(remove_escape_chars(u'no ec', which_ones=('\\n',)), u'no ec')\n@@ -90,6 +118,10 @@ class UtilsMarkupTest(unittest.TestCase):\n         sample_txt2 = u'<node2>blah&amp;blah<![CDATA[blahblahblah!&pound;]]>moreblah&lt;&gt;</node2>'\n         sample_txt3 = u'something&pound;&amp;more<node3><![CDATA[things, stuff, and such]]>what&quot;ever</node3><node4'\n \n+        # make sure it always return unicode\n+        assert isinstance(unquote_markup(sample_txt1.encode('latin-1')), unicode)\n+        assert isinstance(unquote_markup(sample_txt2), unicode)\n+\n         self.assertEqual(unquote_markup(sample_txt1), u\"\"\"<node1>hi, this is sample text with entities: & \\xa9\n although this is inside a cdata! &amp; &quot;</node1>\"\"\")\n \n\n@@ -11,7 +11,7 @@ _tag_re = re.compile(r'<[a-zA-Z\\/!].*?>', re.DOTALL)\n def remove_entities(text, keep=(), remove_illegal=True):\n     \"\"\"Remove entities from the given text.\n \n-    'text' must be a unicode string.\n+    'text' can be a unicode string or a regular string encoded as 'utf-8'\n \n     If 'keep' is passed (with a list of entity names) those entities will\n     be kept (they won't be removed).\n@@ -45,24 +45,36 @@ def remove_entities(text, keep=(), remove_illegal=True):\n             else:\n                 return u'&%s;' % m.group(2)\n \n+    if not isinstance(text, unicode):\n+        text = text.decode('utf-8')\n+\n     return _ent_re.sub(convert_entity, text)\n \n def has_entities(text):\n+    if not isinstance(text, unicode):\n+        text = text.decode('utf-8')\n+\n     return bool(_ent_re.search(text))\n \n def replace_tags(text, token=''):\n     \"\"\"Replace all markup tags found in the given text by the given token. By\n     default token is a null string so it just remove all tags.\n \n-    'text' must be a unicode string.\n+    'text' can be a unicode string or a regular string encoded as 'utf-8'\n \n     Always returns a unicode string.\n     \"\"\"\n+    if not isinstance(text, unicode):\n+        text = text.decode('utf-8')\n+\n     return _tag_re.sub(token, text)\n \n \n def remove_comments(text):\n     \"\"\" Remove HTML Comments. \"\"\"\n+    if not isinstance(text, unicode):\n+        text = text.decode('utf-8')\n+\n     return re.sub('<!--.*?-->', u'', text, re.DOTALL)\n       \n def remove_tags(text, which_ones=()):\n@@ -71,6 +83,9 @@ def remove_tags(text, which_ones=()):\n         which_ones -- is a tuple of which tags we want to remove.\n                       if is empty remove all tags.\n     \"\"\"\n+    if not isinstance(text, unicode):\n+        text = text.decode('utf-8')\n+\n     if len(which_ones) > 0:\n         tags = [ '<%s>|<%s .*?>|</%s>' % (tag,tag,tag) for tag in which_ones ]\n         reg_exp_remove_tags = '|'.join(tags)\n@@ -85,6 +100,9 @@ def remove_tags_with_content(text, which_ones=()):\n         which_ones -- is a tuple of which tags with its content we want to remove.\n                       if is empty do nothing.\n     \"\"\"\n+    if not isinstance(text, unicode):\n+        text = text.decode('utf-8')\n+\n     tags = [ '<%s.*?</%s>' % (tag,tag) for tag in which_ones ]\n     re_tags_remove = re.compile('|'.join(tags), re.DOTALL)\n     return re_tags_remove.sub(u'', text)\n@@ -95,12 +113,15 @@ def remove_escape_chars(text, which_ones=('\\n','\\t','\\r')):\n         which_ones -- is a tuple of which escape chars we want to remove.\n                       By default removes \\n, \\t, \\r.\n     \"\"\"\n+    if not isinstance(text, unicode):\n+        text = text.decode('utf-8')\n+\n     re_escape_chars = re.compile('[%s]' % ''.join(which_ones))\n     return re_escape_chars.sub(u'', text)\n \n def unquote_markup(text, keep=(), remove_illegal=True):\n     \"\"\"\n-    This function receives markup as a text (always a unicode string) and does the following:\n+    This function receives markup as a text (always a unicode string or a utf-8 encoded string) and does the following:\n      - removes entities (except the ones in 'keep') from any part of it that it's not inside a CDATA\n      - searches for CDATAs and extracts their text (if any) without modifying it.\n      - removes the found CDATAs\n@@ -118,6 +139,9 @@ def unquote_markup(text, keep=(), remove_illegal=True):\n         fragments.append(txt[offset:])\n         return fragments\n \n+    if not isinstance(text, unicode):\n+        text = text.decode('utf-8')\n+\n     ret_text = u''\n     for fragment in _get_fragments(text, _cdata_re):\n         if isinstance(fragment, basestring):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
