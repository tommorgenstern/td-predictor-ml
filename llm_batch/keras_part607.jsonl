{"custom_id": "keras#9afe6d6bb09cc5e56795ffafd7f106c0a130650c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 29 | Files Changed: 2 | Hunks: 15 | Methods Changed: 15 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 82 | Churn Cumulative: 1836 | Contributors (this commit): 8 | Commits (past 90d): 58 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -115,13 +115,13 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             )\n             return outputs\n \n-        def mutli_step_on_iterator(iterator):\n+        def multi_step_on_iterator(iterator):\n             for _ in tf.range(self.steps_per_execution):\n                 outputs = one_step_on_iterator(iterator)\n             return outputs\n \n         if self.steps_per_execution > 1:\n-            train_function = mutli_step_on_iterator\n+            train_function = multi_step_on_iterator\n         else:\n             train_function = one_step_on_iterator\n \n@@ -157,13 +157,13 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             )\n             return outputs\n \n-        def mutli_step_on_iterator(iterator):\n+        def multi_step_on_iterator(iterator):\n             for _ in tf.range(self.steps_per_execution):\n                 outputs = one_step_on_iterator(iterator)\n             return outputs\n \n         if self.steps_per_execution > 1:\n-            test_function = mutli_step_on_iterator\n+            test_function = multi_step_on_iterator\n         else:\n             test_function = one_step_on_iterator\n \n@@ -186,9 +186,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 one_step_on_data, jit_compile=True, reduce_retracing=True\n             )\n \n-        def one_step_on_iterator(iterator):\n-            \"\"\"Runs a single predict step given a Dataset iterator.\"\"\"\n-            data = next(iterator)\n+        def one_step_on_data_distributed(data):\n+            data = data[0]\n             outputs = self.distribute_strategy.run(\n                 one_step_on_data, args=(data,)\n             )\n@@ -197,20 +196,21 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 self.distribute_strategy,\n                 reduction=self.distribute_reduction_method,\n             )\n-            return [outputs]\n+            return outputs\n \n-        def mutli_step_on_iterator(iterator):\n-            outputs = []\n-            for _ in tf.range(self.steps_per_execution):\n-                outputs += one_step_on_iterator(iterator)\n+        def multi_step_on_data(data):\n+            outputs = one_step_on_data_distributed(data[:1])\n+            for single_step_data in data[1:]:\n+                step_outputs = one_step_on_data_distributed([single_step_data])\n+                outputs = tf.nest.map_structure(\n+                    lambda t1, t2: concat([t1, t2]), outputs, step_outputs\n+                )\n             return outputs\n \n         if self.steps_per_execution > 1:\n-            # TODO(haifengj): Use multi_step_on_iterator.\n-            # predict_function = mutli_step_on_iterator\n-            predict_function = one_step_on_iterator\n+            predict_function = multi_step_on_data\n         else:\n-            predict_function = one_step_on_iterator\n+            predict_function = one_step_on_data_distributed\n \n         if not self.run_eagerly:\n             predict_function = tf.function(\n@@ -431,14 +431,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 model=self,\n             )\n \n-        self.make_predict_function()\n-        callbacks.on_predict_begin()\n-        outputs = None\n-        with epoch_iterator.catch_stop_iteration():\n-            for step, iterator in epoch_iterator.enumerate_epoch():\n-                callbacks.on_predict_batch_begin(step)\n-                multi_batch_outputs = self.predict_function(iterator)\n-                for batch_outputs in multi_batch_outputs:\n+        def append_to_outputs(batch_outputs, outputs):\n             if outputs is None:\n                 outputs = tf.nest.map_structure(\n                     lambda batch_output: [batch_output],\n@@ -447,12 +440,39 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             else:\n                 tf.__internal__.nest.map_structure_up_to(\n                     batch_outputs,\n-                            lambda output, batch_output: output.append(\n-                                batch_output\n-                            ),\n+                    lambda output, batch_output: output.append(batch_output),\n                     outputs,\n                     batch_outputs,\n                 )\n+            return outputs\n+\n+        def get_data(iterator):\n+            \"\"\"Returns data for the next execution.\"\"\"\n+            data = []\n+            for _ in range(self.steps_per_execution):\n+                try:\n+                    single_step_data = next(iterator)\n+                except (StopIteration, tf.errors.OutOfRangeError) as e:\n+                    if len(data) > 0:\n+                        # Suppress the error when still have remaining data.\n+                        return data\n+                    else:\n+                        # Re-raise the error for\n+                        # TFEpochIterator.catch_stop_iteration() to catch when\n+                        # no data left.\n+                        raise e\n+                data.append(single_step_data)\n+            return data\n+\n+        self.make_predict_function()\n+        callbacks.on_predict_begin()\n+        outputs = None\n+        with epoch_iterator.catch_stop_iteration():\n+            for step, iterator in epoch_iterator.enumerate_epoch():\n+                callbacks.on_predict_batch_begin(step)\n+                data = get_data(iterator)\n+                batch_outputs = self.predict_function(data)\n+                outputs = append_to_outputs(batch_outputs, outputs)\n                 callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n         callbacks.on_predict_end()\n         return tf.__internal__.nest.map_structure_up_to(\n\n@@ -228,6 +228,7 @@ class TestTrainer(testing.TestCase):\n \n         x = np.ones((100, 4))\n         y = np.ones((100, 1))\n+        batch_size = 16\n         model = ExampleModel(units=1)\n         model.compile(\n             loss=\"mse\",\n@@ -240,8 +241,11 @@ class TestTrainer(testing.TestCase):\n \n         model_2 = ExampleModel(units=1)\n         model_2.compile(loss=\"mse\", optimizer=\"adam\", steps_per_execution=1)\n-        model_2.fit(x=x, y=y, batch_size=16, verbose=0)\n+        model_2.fit(x=x, y=y, batch_size=batch_size, verbose=0)\n \n         self.assertAllClose(model.get_weights(), model_2.get_weights())\n-        self.assertAllClose(model.predict(x), model_2.predict(x))\n+        self.assertAllClose(\n+            model.predict(x, batch_size=batch_size),\n+            model_2.predict(x, batch_size=batch_size),\n+        )\n         self.assertAllClose(model.evaluate(x, y), model_2.evaluate(x, y))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a2aa306b499d6a4800cd21283c8aa8eba32a3c42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 48 | Files Changed: 10 | Hunks: 13 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 101 | Churn Cumulative: 4329 | Contributors (this commit): 7 | Commits (past 90d): 50 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -21,15 +21,15 @@ class Adadelta(optimizer.Optimizer):\n     learning rate can be set, as in most other Keras optimizers.\n \n     Args:\n-        learning_rate: Initial value for the learning rate: a floating\n-            point value, Defaults to 0.001. Note that `Adadelta` tends\n-            to benefit from higher initial learning rate values compared to\n-            other optimizers.\n-            To match the exact form in the original paper, use 1.0.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001. Note that `Adadelta`\n+            tends to benefit from higher initial learning rate values compared\n+            to other optimizers. To match the exact form in the original paper,\n+            use 1.0.\n         rho: A floating point value. The decay rate. Defaults to 0.95.\n-        epsilon: Small floating point value used to maintain numerical\n-            stability.\n-            Defaults to 1e-7.\n+        epsilon: Small floating point value for maintaining numerical stability.\n         {{base_optimizer_keyword_args}}\n \n     Reference:\n\n@@ -17,8 +17,10 @@ class Adafactor(optimizer.Optimizer):\n     last 2 dimensions separately in its accumulator variables.\n \n     Args:\n-        learning_rate: Initial value for the learning rate:\n-            a floating point value, Defaults to 0.001.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001.\n         beta_2_decay: float, defaults to -0.8. The decay rate of `beta_2`.\n         epsilon_1: float, defaults to 1e-30. A small offset to keep demoninator\n             away from 0.\n@@ -129,9 +131,7 @@ class Adafactor(optimizer.Optimizer):\n         epsilon_2 = ops.cast(self.epsilon_2, variable.dtype)\n         one = ops.cast(1.0, variable.dtype)\n         local_step = ops.cast(self.iterations + 1, variable.dtype)\n-        if self.relative_step:  # TODO: add learning_rate_schedule logic\n-            # If `relative_step=True` and learning rate is a constant, we\n-            # apply the relative step algorithm.\n+        if not callable(self._learning_rate) and self.relative_step:\n             lr = ops.minimum(lr, 1 / ops.sqrt(local_step))\n \n         r = self._r[self._get_variable_index(variable)]\n\n@@ -14,18 +14,16 @@ class Adagrad(optimizer.Optimizer):\n     the smaller the updates.\n \n     Args:\n-        learning_rate: Initial value for the learning rate:\n-            a floating point value,\n-            Defaults to 0.001.\n-            Note that `Adagrad` tends to benefit from higher initial\n-            learning rate values compared to other optimizers.\n-            To match the exact form in the original paper, use 1.0.\n-        initial_accumulator_value: Floating point value.\n-            Starting value for the accumulators (per-parameter\n-            momentum values).\n-            Must be non-negative.\n-        epsilon: Small floating point value used to maintain\n-            numerical stability.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001. Note that `Adagrad` tends\n+            to benefit from higher initial learning rate values compared to\n+            other optimizers. To match the exact form in the original paper,\n+            use 1.0.\n+        initial_accumulator_value: Floating point value. Starting value for the\n+            accumulators (per-parameter momentum values). Must be non-negative.\n+        epsilon: Small floating point value for maintaining numerical stability.\n         {{base_optimizer_keyword_args}}\n \n     Reference:\n\n@@ -18,9 +18,10 @@ class Adam(optimizer.Optimizer):\n     data/parameters*\".\n \n     Args:\n-        learning_rate: A floating point value or a callable that takes no\n-            arguments and returns the actual value to use. The learning rate.\n-            Defaults to `0.001`.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001.\n         beta_1: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n             exponential decay rate for the 1st moment estimates. Defaults to\n\n@@ -34,9 +34,10 @@ class Adamax(optimizer.Optimizer):\n     ```\n \n     Args:\n-        learning_rate: A floating point value, or a callable\n-            that takes no arguments and returns the actual value to use. The\n-            learning rate. Defaults to `0.001`.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001.\n         beta_1: A float value or a constant float tensor. The exponential decay\n             rate for the 1st moment estimates.\n         beta_2: A float value or a constant float tensor. The exponential decay\n\n@@ -21,9 +21,10 @@ class AdamW(optimizer.Optimizer):\n     data/parameters*\".\n \n     Args:\n-        learning_rate: A floating point value or a callable that takes no\n-            arguments and returns the actual value to use. The learning rate.\n-            Defaults to 0.001.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001.\n         beta_1: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n             exponential decay rate for the 1st moment estimates.\n\n@@ -52,9 +52,10 @@ class Ftrl(optimizer.Optimizer):\n     is replaced with a gradient with shrinkage.\n \n     Args:\n-        learning_rate: A floating point value, or a callable that\n-            takes no arguments and returns the actual value to use. The learning\n-            rate.  Defaults to `0.001`.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001.\n         learning_rate_power: A float value, must be less or equal to zero.\n             Controls how the learning rate decreases during training. Use zero\n             for a fixed learning rate.\n\n@@ -12,9 +12,10 @@ class Nadam(optimizer.Optimizer):\n     Nesterov momentum.\n \n     Args:\n-        learning_rate: A floating point value or a callable\n-            that takes no arguments and returns the actual value to use. The\n-            learning rate. Defaults to `0.001`.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001.\n         beta_1: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n             exponential decay rate for the 1st moment estimates.\n\n@@ -18,8 +18,10 @@ class RMSprop(optimizer.Optimizer):\n     gradients, and uses that average to estimate the variance.\n \n     Args:\n-        learning_rate: Initial value for the learning rate: a floating point\n-            value, defaults to 0.001.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001.\n         rho: float, defaults to 0.9. Discounting factor for the old gradients.\n         momentum: float, defaults to 0.0. If not 0.0., the optimizer tracks the\n             momentum value, with a decay rate equals to `1 - momentum`.\n\n@@ -26,13 +26,13 @@ class SGD(optimizer.Optimizer):\n     ```\n \n     Args:\n-      learning_rate: A `Tensor`, floating point value, or a schedule that is a\n-        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n-        that takes no arguments and returns the actual value to use. The\n-        learning rate. Defaults to 0.001.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.01.\n         momentum: float hyperparameter >= 0 that accelerates gradient descent in\n-        the relevant direction and dampens oscillations. Defaults to 0, i.e.,\n-        vanilla gradient descent.\n+            the relevant direction and dampens oscillations. Defaults to 0,\n+            i.e., vanilla gradient descent.\n         nesterov: boolean. Whether to apply Nesterov momentum.\n             Defaults to `False`.\n         {{base_optimizer_keyword_args}}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dfa24a73262aada3c89dcbdc4c6b602057675e85", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 43 | Churn Cumulative: 1166 | Contributors (this commit): 3 | Commits (past 90d): 26 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -481,22 +481,28 @@ class JAXTrainer(base_trainer.Trainer):\n             )\n \n         def _predict_step(trainable_variables, non_trainable_variables, data):\n-            return [\n-                self.stateless_call(\n+            outputs, _ = self.stateless_call(\n                 trainable_variables, non_trainable_variables, data[0]\n             )\n-            ]\n+            return outputs\n \n         def _predict_multi_step(\n             trainable_variables, non_trainable_variables, data\n         ):\n-            outputs = []\n-            for single_step_data in data:\n-                outputs += _predict_step(\n+            outputs = _predict_step(\n+                trainable_variables, non_trainable_variables, data[:1]\n+            )\n+            for single_step_data in data[1:]:\n+                step_outputs = _predict_step(\n                     trainable_variables,\n                     non_trainable_variables,\n                     [single_step_data],\n                 )\n+                outputs = tf.nest.map_structure(\n+                    lambda t1, t2: jax.numpy.concatenate([t1, t2]),\n+                    outputs,\n+                    step_outputs,\n+                )\n             return outputs\n \n         if self.steps_per_execution > 1:\n@@ -519,15 +525,7 @@ class JAXTrainer(base_trainer.Trainer):\n \n         callbacks.on_predict_begin()\n \n-        trainable_variables = self.trainable_variables\n-        non_trainable_variables = self.non_trainable_variables\n-        outputs = None\n-        for step, x in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n-            callbacks.on_predict_batch_begin(step)\n-            multi_step_return_values = predict_step(\n-                trainable_variables, non_trainable_variables, x\n-            )\n-            for batch_outputs, _ in multi_step_return_values:\n+        def append_to_outputs(batch_outputs, outputs):\n             if outputs is None:\n                 outputs = tf.nest.map_structure(\n                     lambda batch_output: [batch_output],\n@@ -536,12 +534,21 @@ class JAXTrainer(base_trainer.Trainer):\n             else:\n                 tf.__internal__.nest.map_structure_up_to(\n                     batch_outputs,\n-                        lambda output, batch_output: output.append(\n-                            batch_output\n-                        ),\n+                    lambda output, batch_output: output.append(batch_output),\n                     outputs,\n                     batch_outputs,\n                 )\n+            return outputs\n+\n+        trainable_variables = self.trainable_variables\n+        non_trainable_variables = self.non_trainable_variables\n+        outputs = None\n+        for step, x in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+            callbacks.on_predict_batch_begin(step)\n+            batch_outputs = predict_step(\n+                trainable_variables, non_trainable_variables, x\n+            )\n+            outputs = append_to_outputs(batch_outputs, outputs)\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n         callbacks.on_predict_end()\n         return tf.__internal__.nest.map_structure_up_to(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9cecfef6a229313878f473dc32d608f174f76fb7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 207 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 8 | Complexity Δ (Sum/Max): 28/14 | Churn Δ: 207 | Churn Cumulative: 356 | Contributors (this commit): 10 | Commits (past 90d): 76 | Contributors (cumulative): 12 | DMM Complexity: 0.4375\n\nDIFF:\n@@ -103,6 +103,7 @@ from keras_core.layers.reshaping.permute import Permute\n from keras_core.layers.reshaping.repeat_vector import RepeatVector\n from keras_core.layers.reshaping.reshape import Reshape\n from keras_core.layers.reshaping.up_sampling1d import UpSampling1D\n+from keras_core.layers.reshaping.zero_padding3d import ZeroPadding3D\n from keras_core.layers.rnn.bidirectional import Bidirectional\n from keras_core.layers.rnn.gru import GRU\n from keras_core.layers.rnn.lstm import LSTM\n\n@@ -0,0 +1,122 @@\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.input_spec import InputSpec\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.ZeroPadding3D\")\n+class ZeroPadding3D(Layer):\n+    \"\"\"Zero-padding layer for 3D data (spatial or spatio-temporal).\n+\n+    Examples:\n+\n+    >>> input_shape = (1, 1, 2, 2, 3)\n+    >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+    >>> y = keras_core.layers.ZeroPadding3D(padding=2)(x)\n+    >>> y.shape\n+    (1, 5, 6, 6, 3)\n+\n+    Args:\n+        padding: Int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints.\n+            - If int: the same symmetric padding is applied to depth, height,\n+              and width.\n+            - If tuple of 3 ints: interpreted as three different symmetric\n+              padding values for depth, height, and width:\n+              `(symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad)`.\n+            - If tuple of 3 tuples of 2 ints: interpreted as\n+              `((left_dim1_pad, right_dim1_pad), (left_dim2_pad,\n+              right_dim2_pad), (left_dim3_pad, right_dim3_pad))`\n+        data_format: A string, one of `\"channels_last\"` (default) or\n+            `\"channels_first\"`. The ordering of the dimensions in the inputs.\n+            `\"channels_last\"` corresponds to inputs with shape\n+            `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n+            When unspecified, uses `image_data_format` value found in your Keras\n+            config file at `~/.keras/keras.json` (if exists). Defaults to\n+            `\"channels_last\"`.\n+\n+    Input shape:\n+        5D tensor with shape:\n+        - If `data_format` is `\"channels_last\"`:\n+          `(batch_size, first_axis_to_pad, second_axis_to_pad,\n+          third_axis_to_pad, depth)`\n+        - If `data_format` is `\"channels_first\"`:\n+          `(batch_size, depth, first_axis_to_pad, second_axis_to_pad,\n+          third_axis_to_pad)`\n+\n+    Output shape:\n+        5D tensor with shape:\n+        - If `data_format` is `\"channels_last\"`:\n+          `(batch_size, first_padded_axis, second_padded_axis,\n+          third_axis_to_pad, depth)`\n+        - If `data_format` is `\"channels_first\"`:\n+          `(batch_size, depth, first_padded_axis, second_padded_axis,\n+          third_axis_to_pad)`\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        padding=((1, 1), (1, 1), (1, 1)),\n+        data_format=None,\n+        name=None,\n+        dtype=None,\n+    ):\n+        super().__init__(name=name, dtype=dtype)\n+        self.data_format = backend.standardize_data_format(data_format)\n+        if isinstance(padding, int):\n+            self.padding = (\n+                (padding, padding),\n+                (padding, padding),\n+                (padding, padding),\n+            )\n+        elif hasattr(padding, \"__len__\"):\n+            if len(padding) != 3:\n+                raise ValueError(\n+                    f\"`padding` should have 3 elements. Received: {padding}.\"\n+                )\n+            dim1_padding = padding[0]\n+            if isinstance(dim1_padding, int):\n+                dim1_padding = (dim1_padding, dim1_padding)\n+            dim2_padding = padding[1]\n+            if isinstance(dim2_padding, int):\n+                dim2_padding = (dim2_padding, dim2_padding)\n+            dim3_padding = padding[2]\n+            if isinstance(dim3_padding, int):\n+                dim3_padding = (dim3_padding, dim3_padding)\n+            self.padding = (dim1_padding, dim2_padding, dim3_padding)\n+            self.padding = (dim1_padding, dim2_padding, dim3_padding)\n+        else:\n+            raise ValueError(\n+                \"`padding` should be either an int, a tuple of 3 ints \"\n+                \"(symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad), \"\n+                \"or a tuple of 3 tuples of 2 ints \"\n+                \"((left_dim1_pad, right_dim1_pad),\"\n+                \" (left_dim2_pad, right_dim2_pad),\"\n+                \" (left_dim3_pad, right_dim2_pad)). \"\n+                f\"Received: {padding}.\"\n+            )\n+        self.input_spec = InputSpec(ndim=5)\n+\n+    def compute_output_shape(self, input_shape):\n+        output_shape = list(input_shape)\n+        spatial_dims_offset = 2 if self.data_format == \"channels_first\" else 1\n+        for index in range(0, 3):\n+            if output_shape[index + spatial_dims_offset] is not None:\n+                output_shape[index + spatial_dims_offset] += (\n+                    self.padding[index][0] + self.padding[index][1]\n+                )\n+        return tuple(output_shape)\n+\n+    def call(self, inputs):\n+        if self.data_format == \"channels_first\":\n+            all_dims_padding = ((0, 0), (0, 0), *self.padding)\n+        else:\n+            all_dims_padding = ((0, 0), *self.padding, (0, 0))\n+        return ops.pad(inputs, all_dims_padding)\n+\n+    def get_config(self):\n+        config = {\"padding\": self.padding, \"data_format\": self.data_format}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,84 @@\n+import numpy as np\n+import pytest\n+from absl.testing import parameterized\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class ZeroPaddingTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(\n+        (\"channels_first\", \"channels_first\"), (\"channels_last\", \"channels_last\")\n+    )\n+    def test_zero_padding_3d(self, data_format):\n+        inputs = np.random.rand(1, 2, 3, 4, 5)\n+        outputs = layers.ZeroPadding3D(\n+            padding=((1, 2), (3, 4), (0, 2)), data_format=data_format\n+        )(inputs)\n+\n+        if data_format == \"channels_first\":\n+            for index in [0, -1, -2]:\n+                self.assertAllClose(outputs[:, :, index, :, :], 0.0)\n+            for index in [0, 1, 2, -1, -2, -3, -4]:\n+                self.assertAllClose(outputs[:, :, :, index, :], 0.0)\n+            for index in [-1, -2]:\n+                self.assertAllClose(outputs[:, :, :, :, index], 0.0)\n+            self.assertAllClose(outputs[:, :, 1:-2, 3:-4, 0:-2], inputs)\n+        else:\n+            for index in [0, -1, -2]:\n+                self.assertAllClose(outputs[:, index, :, :, :], 0.0)\n+            for index in [0, 1, 2, -1, -2, -3, -4]:\n+                self.assertAllClose(outputs[:, :, index, :, :], 0.0)\n+            for index in [-1, -2]:\n+                self.assertAllClose(outputs[:, :, :, index, :], 0.0)\n+            self.assertAllClose(outputs[:, 1:-2, 3:-4, 0:-2, :], inputs)\n+\n+    @parameterized.product(\n+        (\n+            {\"padding\": ((2, 2), (2, 2), (2, 2))},  # 3 tuples\n+            {\"padding\": (2, 2, 2)},  # 1 tuple\n+            {\"padding\": 2},  # 1 int\n+        ),\n+        (\n+            {\"data_format\": \"channels_first\"},\n+            {\"data_format\": \"channels_last\"},\n+        ),\n+    )\n+    def test_zero_padding_3d_with_same_padding(self, padding, data_format):\n+        inputs = np.random.rand(1, 2, 3, 4, 5)\n+        outputs = layers.ZeroPadding3D(\n+            padding=padding, data_format=data_format\n+        )(inputs)\n+\n+        if data_format == \"channels_first\":\n+            for index in [0, 1, -1, -2]:\n+                self.assertAllClose(outputs[:, :, index, :, :], 0.0)\n+                self.assertAllClose(outputs[:, :, :, index, :], 0.0)\n+                self.assertAllClose(outputs[:, :, :, :, index], 0.0)\n+            self.assertAllClose(outputs[:, :, 2:-2, 2:-2, 2:-2], inputs)\n+        else:\n+            for index in [0, 1, -1, -2]:\n+                self.assertAllClose(outputs[:, index, :, :, :], 0.0)\n+                self.assertAllClose(outputs[:, :, index, :, :], 0.0)\n+                self.assertAllClose(outputs[:, :, :, index, :], 0.0)\n+            self.assertAllClose(outputs[:, 2:-2, 2:-2, 2:-2, :], inputs)\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_zero_padding_3d_with_dynamic_batch_size(self):\n+        input_layer = layers.Input(batch_shape=(None, 2, 3, 4, 5))\n+        permuted = layers.ZeroPadding3D(((1, 2), (3, 4), (5, 6)))(input_layer)\n+        self.assertEqual(permuted.shape, (None, 5, 10, 15, 5))\n+\n+    def test_zero_padding_3d_errors_if_padding_argument_invalid(self):\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding3D(padding=(1,))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding3D(padding=(1, 2))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding3D(padding=(1, 2, 3, 4))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding3D(padding=\"1\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f94df3479a4590ef097624f87aaff2c1aa1713b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 11 | Methods Changed: 2 | Complexity Δ (Sum/Max): 17/10 | Churn Δ: 24 | Churn Cumulative: 57 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -58,20 +58,19 @@ def loss_fn(y_true, y_pred):\n \n model = MyModel(hidden_dim=256, output_dim=4)\n \n-optimizer = optimizers.SGD(learning_rate=0.0001)\n+optimizer = optimizers.SGD(learning_rate=0.001)\n dataset = Dataset()\n \n-\n-######### Custom JAX workflow ###############\n-\n-\n # Build model\n-x = ops.convert_to_tensor(np.random.random((1, 128)))\n+x = np.random.random((1, 128))\n model(x)\n # Build optimizer\n optimizer.build(model.trainable_variables)\n \n \n+######### Custom JAX workflow ###############\n+\n+\n def compute_loss_and_updates(\n     trainable_variables, non_trainable_variables, x, y\n ):\n\n@@ -1,5 +1,6 @@\n import numpy as np\n \n+import keras_core\n from keras_core import Model\n from keras_core import backend\n from keras_core import initializers\n@@ -42,11 +43,11 @@ class MyDropout(layers.Layer):\n         # Use seed_generator for managing RNG state.\n         # It is a state element and its seed variable is\n         # tracked as part of `layer.variables`.\n-        self.seed_generator = backend.random.SeedGenerator(1337)\n+        self.seed_generator = keras_core.random.SeedGenerator(1337)\n \n     def call(self, inputs):\n-        # Use `backend.random` for random ops.\n-        return backend.random.dropout(\n+        # Use `keras_core.random` for random ops.\n+        return keras_core.random.dropout(\n             inputs, self.rate, seed=self.seed_generator\n         )\n \n\n@@ -8,8 +8,10 @@ from keras_core import optimizers\n \n inputs = layers.Input((100,), batch_size=32)\n x = layers.Dense(256, activation=\"relu\")(inputs)\n+residual = x\n x = layers.Dense(256, activation=\"relu\")(x)\n x = layers.Dense(256, activation=\"relu\")(x)\n+x += residual\n outputs = layers.Dense(16)(x)\n model = Model(inputs, outputs)\n \n@@ -21,9 +23,9 @@ batch_size = 32\n epochs = 5\n \n model.compile(\n-    optimizer=optimizers.SGD(learning_rate=0.001),\n+    optimizer=optimizers.Adam(learning_rate=0.001),\n     loss=losses.MeanSquaredError(),\n-    metrics=[metrics.MeanSquaredError()],\n+    metrics=[metrics.CategoricalAccuracy(name=\"acc\"), metrics.MeanSquaredError(name=\"mse\")],\n )\n history = model.fit(\n     x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b781ef7e531186d575a9df9d6377ed22dc96fa30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 338 | Lines Deleted: 95 | Files Changed: 16 | Hunks: 38 | Methods Changed: 28 | Complexity Δ (Sum/Max): 33/14 | Churn Δ: 433 | Churn Cumulative: 8557 | Contributors (this commit): 15 | Commits (past 90d): 242 | Contributors (cumulative): 63 | DMM Complexity: 0.6453900709219859\n\nDIFF:\n@@ -481,22 +481,28 @@ class JAXTrainer(base_trainer.Trainer):\n             )\n \n         def _predict_step(trainable_variables, non_trainable_variables, data):\n-            return [\n-                self.stateless_call(\n+            outputs, _ = self.stateless_call(\n                 trainable_variables, non_trainable_variables, data[0]\n             )\n-            ]\n+            return outputs\n \n         def _predict_multi_step(\n             trainable_variables, non_trainable_variables, data\n         ):\n-            outputs = []\n-            for single_step_data in data:\n-                outputs += _predict_step(\n+            outputs = _predict_step(\n+                trainable_variables, non_trainable_variables, data[:1]\n+            )\n+            for single_step_data in data[1:]:\n+                step_outputs = _predict_step(\n                     trainable_variables,\n                     non_trainable_variables,\n                     [single_step_data],\n                 )\n+                outputs = tf.nest.map_structure(\n+                    lambda t1, t2: jax.numpy.concatenate([t1, t2]),\n+                    outputs,\n+                    step_outputs,\n+                )\n             return outputs\n \n         if self.steps_per_execution > 1:\n@@ -519,15 +525,7 @@ class JAXTrainer(base_trainer.Trainer):\n \n         callbacks.on_predict_begin()\n \n-        trainable_variables = self.trainable_variables\n-        non_trainable_variables = self.non_trainable_variables\n-        outputs = None\n-        for step, x in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n-            callbacks.on_predict_batch_begin(step)\n-            multi_step_return_values = predict_step(\n-                trainable_variables, non_trainable_variables, x\n-            )\n-            for batch_outputs, _ in multi_step_return_values:\n+        def append_to_outputs(batch_outputs, outputs):\n             if outputs is None:\n                 outputs = tf.nest.map_structure(\n                     lambda batch_output: [batch_output],\n@@ -536,12 +534,21 @@ class JAXTrainer(base_trainer.Trainer):\n             else:\n                 tf.__internal__.nest.map_structure_up_to(\n                     batch_outputs,\n-                        lambda output, batch_output: output.append(\n-                            batch_output\n-                        ),\n+                    lambda output, batch_output: output.append(batch_output),\n                     outputs,\n                     batch_outputs,\n                 )\n+            return outputs\n+\n+        trainable_variables = self.trainable_variables\n+        non_trainable_variables = self.non_trainable_variables\n+        outputs = None\n+        for step, x in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+            callbacks.on_predict_batch_begin(step)\n+            batch_outputs = predict_step(\n+                trainable_variables, non_trainable_variables, x\n+            )\n+            outputs = append_to_outputs(batch_outputs, outputs)\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n         callbacks.on_predict_end()\n         return tf.__internal__.nest.map_structure_up_to(\n\n@@ -115,13 +115,13 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             )\n             return outputs\n \n-        def mutli_step_on_iterator(iterator):\n+        def multi_step_on_iterator(iterator):\n             for _ in tf.range(self.steps_per_execution):\n                 outputs = one_step_on_iterator(iterator)\n             return outputs\n \n         if self.steps_per_execution > 1:\n-            train_function = mutli_step_on_iterator\n+            train_function = multi_step_on_iterator\n         else:\n             train_function = one_step_on_iterator\n \n@@ -157,13 +157,13 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             )\n             return outputs\n \n-        def mutli_step_on_iterator(iterator):\n+        def multi_step_on_iterator(iterator):\n             for _ in tf.range(self.steps_per_execution):\n                 outputs = one_step_on_iterator(iterator)\n             return outputs\n \n         if self.steps_per_execution > 1:\n-            test_function = mutli_step_on_iterator\n+            test_function = multi_step_on_iterator\n         else:\n             test_function = one_step_on_iterator\n \n@@ -186,9 +186,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 one_step_on_data, jit_compile=True, reduce_retracing=True\n             )\n \n-        def one_step_on_iterator(iterator):\n-            \"\"\"Runs a single predict step given a Dataset iterator.\"\"\"\n-            data = next(iterator)\n+        def one_step_on_data_distributed(data):\n+            data = data[0]\n             outputs = self.distribute_strategy.run(\n                 one_step_on_data, args=(data,)\n             )\n@@ -197,20 +196,21 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 self.distribute_strategy,\n                 reduction=self.distribute_reduction_method,\n             )\n-            return [outputs]\n+            return outputs\n \n-        def mutli_step_on_iterator(iterator):\n-            outputs = []\n-            for _ in tf.range(self.steps_per_execution):\n-                outputs += one_step_on_iterator(iterator)\n+        def multi_step_on_data(data):\n+            outputs = one_step_on_data_distributed(data[:1])\n+            for single_step_data in data[1:]:\n+                step_outputs = one_step_on_data_distributed([single_step_data])\n+                outputs = tf.nest.map_structure(\n+                    lambda t1, t2: concat([t1, t2]), outputs, step_outputs\n+                )\n             return outputs\n \n         if self.steps_per_execution > 1:\n-            # TODO(haifengj): Use multi_step_on_iterator.\n-            # predict_function = mutli_step_on_iterator\n-            predict_function = one_step_on_iterator\n+            predict_function = multi_step_on_data\n         else:\n-            predict_function = one_step_on_iterator\n+            predict_function = one_step_on_data_distributed\n \n         if not self.run_eagerly:\n             predict_function = tf.function(\n@@ -431,14 +431,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 model=self,\n             )\n \n-        self.make_predict_function()\n-        callbacks.on_predict_begin()\n-        outputs = None\n-        with epoch_iterator.catch_stop_iteration():\n-            for step, iterator in epoch_iterator.enumerate_epoch():\n-                callbacks.on_predict_batch_begin(step)\n-                multi_batch_outputs = self.predict_function(iterator)\n-                for batch_outputs in multi_batch_outputs:\n+        def append_to_outputs(batch_outputs, outputs):\n             if outputs is None:\n                 outputs = tf.nest.map_structure(\n                     lambda batch_output: [batch_output],\n@@ -447,12 +440,39 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             else:\n                 tf.__internal__.nest.map_structure_up_to(\n                     batch_outputs,\n-                            lambda output, batch_output: output.append(\n-                                batch_output\n-                            ),\n+                    lambda output, batch_output: output.append(batch_output),\n                     outputs,\n                     batch_outputs,\n                 )\n+            return outputs\n+\n+        def get_data(iterator):\n+            \"\"\"Returns data for the next execution.\"\"\"\n+            data = []\n+            for _ in range(self.steps_per_execution):\n+                try:\n+                    single_step_data = next(iterator)\n+                except (StopIteration, tf.errors.OutOfRangeError) as e:\n+                    if len(data) > 0:\n+                        # Suppress the error when still have remaining data.\n+                        return data\n+                    else:\n+                        # Re-raise the error for\n+                        # TFEpochIterator.catch_stop_iteration() to catch when\n+                        # no data left.\n+                        raise e\n+                data.append(single_step_data)\n+            return data\n+\n+        self.make_predict_function()\n+        callbacks.on_predict_begin()\n+        outputs = None\n+        with epoch_iterator.catch_stop_iteration():\n+            for step, iterator in epoch_iterator.enumerate_epoch():\n+                callbacks.on_predict_batch_begin(step)\n+                data = get_data(iterator)\n+                batch_outputs = self.predict_function(data)\n+                outputs = append_to_outputs(batch_outputs, outputs)\n                 callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n         callbacks.on_predict_end()\n         return tf.__internal__.nest.map_structure_up_to(\n\n@@ -103,6 +103,7 @@ from keras_core.layers.reshaping.permute import Permute\n from keras_core.layers.reshaping.repeat_vector import RepeatVector\n from keras_core.layers.reshaping.reshape import Reshape\n from keras_core.layers.reshaping.up_sampling1d import UpSampling1D\n+from keras_core.layers.reshaping.zero_padding3d import ZeroPadding3D\n from keras_core.layers.rnn.bidirectional import Bidirectional\n from keras_core.layers.rnn.conv_lstm1d import ConvLSTM1D\n from keras_core.layers.rnn.conv_lstm2d import ConvLSTM2D\n\n@@ -0,0 +1,122 @@\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.input_spec import InputSpec\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.ZeroPadding3D\")\n+class ZeroPadding3D(Layer):\n+    \"\"\"Zero-padding layer for 3D data (spatial or spatio-temporal).\n+\n+    Examples:\n+\n+    >>> input_shape = (1, 1, 2, 2, 3)\n+    >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+    >>> y = keras_core.layers.ZeroPadding3D(padding=2)(x)\n+    >>> y.shape\n+    (1, 5, 6, 6, 3)\n+\n+    Args:\n+        padding: Int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints.\n+            - If int: the same symmetric padding is applied to depth, height,\n+              and width.\n+            - If tuple of 3 ints: interpreted as three different symmetric\n+              padding values for depth, height, and width:\n+              `(symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad)`.\n+            - If tuple of 3 tuples of 2 ints: interpreted as\n+              `((left_dim1_pad, right_dim1_pad), (left_dim2_pad,\n+              right_dim2_pad), (left_dim3_pad, right_dim3_pad))`\n+        data_format: A string, one of `\"channels_last\"` (default) or\n+            `\"channels_first\"`. The ordering of the dimensions in the inputs.\n+            `\"channels_last\"` corresponds to inputs with shape\n+            `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n+            When unspecified, uses `image_data_format` value found in your Keras\n+            config file at `~/.keras/keras.json` (if exists). Defaults to\n+            `\"channels_last\"`.\n+\n+    Input shape:\n+        5D tensor with shape:\n+        - If `data_format` is `\"channels_last\"`:\n+          `(batch_size, first_axis_to_pad, second_axis_to_pad,\n+          third_axis_to_pad, depth)`\n+        - If `data_format` is `\"channels_first\"`:\n+          `(batch_size, depth, first_axis_to_pad, second_axis_to_pad,\n+          third_axis_to_pad)`\n+\n+    Output shape:\n+        5D tensor with shape:\n+        - If `data_format` is `\"channels_last\"`:\n+          `(batch_size, first_padded_axis, second_padded_axis,\n+          third_axis_to_pad, depth)`\n+        - If `data_format` is `\"channels_first\"`:\n+          `(batch_size, depth, first_padded_axis, second_padded_axis,\n+          third_axis_to_pad)`\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        padding=((1, 1), (1, 1), (1, 1)),\n+        data_format=None,\n+        name=None,\n+        dtype=None,\n+    ):\n+        super().__init__(name=name, dtype=dtype)\n+        self.data_format = backend.standardize_data_format(data_format)\n+        if isinstance(padding, int):\n+            self.padding = (\n+                (padding, padding),\n+                (padding, padding),\n+                (padding, padding),\n+            )\n+        elif hasattr(padding, \"__len__\"):\n+            if len(padding) != 3:\n+                raise ValueError(\n+                    f\"`padding` should have 3 elements. Received: {padding}.\"\n+                )\n+            dim1_padding = padding[0]\n+            if isinstance(dim1_padding, int):\n+                dim1_padding = (dim1_padding, dim1_padding)\n+            dim2_padding = padding[1]\n+            if isinstance(dim2_padding, int):\n+                dim2_padding = (dim2_padding, dim2_padding)\n+            dim3_padding = padding[2]\n+            if isinstance(dim3_padding, int):\n+                dim3_padding = (dim3_padding, dim3_padding)\n+            self.padding = (dim1_padding, dim2_padding, dim3_padding)\n+            self.padding = (dim1_padding, dim2_padding, dim3_padding)\n+        else:\n+            raise ValueError(\n+                \"`padding` should be either an int, a tuple of 3 ints \"\n+                \"(symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad), \"\n+                \"or a tuple of 3 tuples of 2 ints \"\n+                \"((left_dim1_pad, right_dim1_pad),\"\n+                \" (left_dim2_pad, right_dim2_pad),\"\n+                \" (left_dim3_pad, right_dim2_pad)). \"\n+                f\"Received: {padding}.\"\n+            )\n+        self.input_spec = InputSpec(ndim=5)\n+\n+    def compute_output_shape(self, input_shape):\n+        output_shape = list(input_shape)\n+        spatial_dims_offset = 2 if self.data_format == \"channels_first\" else 1\n+        for index in range(0, 3):\n+            if output_shape[index + spatial_dims_offset] is not None:\n+                output_shape[index + spatial_dims_offset] += (\n+                    self.padding[index][0] + self.padding[index][1]\n+                )\n+        return tuple(output_shape)\n+\n+    def call(self, inputs):\n+        if self.data_format == \"channels_first\":\n+            all_dims_padding = ((0, 0), (0, 0), *self.padding)\n+        else:\n+            all_dims_padding = ((0, 0), *self.padding, (0, 0))\n+        return ops.pad(inputs, all_dims_padding)\n+\n+    def get_config(self):\n+        config = {\"padding\": self.padding, \"data_format\": self.data_format}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,84 @@\n+import numpy as np\n+import pytest\n+from absl.testing import parameterized\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class ZeroPaddingTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(\n+        (\"channels_first\", \"channels_first\"), (\"channels_last\", \"channels_last\")\n+    )\n+    def test_zero_padding_3d(self, data_format):\n+        inputs = np.random.rand(1, 2, 3, 4, 5)\n+        outputs = layers.ZeroPadding3D(\n+            padding=((1, 2), (3, 4), (0, 2)), data_format=data_format\n+        )(inputs)\n+\n+        if data_format == \"channels_first\":\n+            for index in [0, -1, -2]:\n+                self.assertAllClose(outputs[:, :, index, :, :], 0.0)\n+            for index in [0, 1, 2, -1, -2, -3, -4]:\n+                self.assertAllClose(outputs[:, :, :, index, :], 0.0)\n+            for index in [-1, -2]:\n+                self.assertAllClose(outputs[:, :, :, :, index], 0.0)\n+            self.assertAllClose(outputs[:, :, 1:-2, 3:-4, 0:-2], inputs)\n+        else:\n+            for index in [0, -1, -2]:\n+                self.assertAllClose(outputs[:, index, :, :, :], 0.0)\n+            for index in [0, 1, 2, -1, -2, -3, -4]:\n+                self.assertAllClose(outputs[:, :, index, :, :], 0.0)\n+            for index in [-1, -2]:\n+                self.assertAllClose(outputs[:, :, :, index, :], 0.0)\n+            self.assertAllClose(outputs[:, 1:-2, 3:-4, 0:-2, :], inputs)\n+\n+    @parameterized.product(\n+        (\n+            {\"padding\": ((2, 2), (2, 2), (2, 2))},  # 3 tuples\n+            {\"padding\": (2, 2, 2)},  # 1 tuple\n+            {\"padding\": 2},  # 1 int\n+        ),\n+        (\n+            {\"data_format\": \"channels_first\"},\n+            {\"data_format\": \"channels_last\"},\n+        ),\n+    )\n+    def test_zero_padding_3d_with_same_padding(self, padding, data_format):\n+        inputs = np.random.rand(1, 2, 3, 4, 5)\n+        outputs = layers.ZeroPadding3D(\n+            padding=padding, data_format=data_format\n+        )(inputs)\n+\n+        if data_format == \"channels_first\":\n+            for index in [0, 1, -1, -2]:\n+                self.assertAllClose(outputs[:, :, index, :, :], 0.0)\n+                self.assertAllClose(outputs[:, :, :, index, :], 0.0)\n+                self.assertAllClose(outputs[:, :, :, :, index], 0.0)\n+            self.assertAllClose(outputs[:, :, 2:-2, 2:-2, 2:-2], inputs)\n+        else:\n+            for index in [0, 1, -1, -2]:\n+                self.assertAllClose(outputs[:, index, :, :, :], 0.0)\n+                self.assertAllClose(outputs[:, :, index, :, :], 0.0)\n+                self.assertAllClose(outputs[:, :, :, index, :], 0.0)\n+            self.assertAllClose(outputs[:, 2:-2, 2:-2, 2:-2, :], inputs)\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_zero_padding_3d_with_dynamic_batch_size(self):\n+        input_layer = layers.Input(batch_shape=(None, 2, 3, 4, 5))\n+        permuted = layers.ZeroPadding3D(((1, 2), (3, 4), (5, 6)))(input_layer)\n+        self.assertEqual(permuted.shape, (None, 5, 10, 15, 5))\n+\n+    def test_zero_padding_3d_errors_if_padding_argument_invalid(self):\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding3D(padding=(1,))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding3D(padding=(1, 2))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding3D(padding=(1, 2, 3, 4))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding3D(padding=\"1\")\n\n@@ -21,15 +21,15 @@ class Adadelta(optimizer.Optimizer):\n     learning rate can be set, as in most other Keras optimizers.\n \n     Args:\n-        learning_rate: Initial value for the learning rate: a floating\n-            point value, Defaults to 0.001. Note that `Adadelta` tends\n-            to benefit from higher initial learning rate values compared to\n-            other optimizers.\n-            To match the exact form in the original paper, use 1.0.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001. Note that `Adadelta`\n+            tends to benefit from higher initial learning rate values compared\n+            to other optimizers. To match the exact form in the original paper,\n+            use 1.0.\n         rho: A floating point value. The decay rate. Defaults to 0.95.\n-        epsilon: Small floating point value used to maintain numerical\n-            stability.\n-            Defaults to 1e-7.\n+        epsilon: Small floating point value for maintaining numerical stability.\n         {{base_optimizer_keyword_args}}\n \n     Reference:\n\n@@ -17,8 +17,10 @@ class Adafactor(optimizer.Optimizer):\n     last 2 dimensions separately in its accumulator variables.\n \n     Args:\n-        learning_rate: Initial value for the learning rate:\n-            a floating point value, Defaults to 0.001.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001.\n         beta_2_decay: float, defaults to -0.8. The decay rate of `beta_2`.\n         epsilon_1: float, defaults to 1e-30. A small offset to keep demoninator\n             away from 0.\n@@ -129,9 +131,7 @@ class Adafactor(optimizer.Optimizer):\n         epsilon_2 = ops.cast(self.epsilon_2, variable.dtype)\n         one = ops.cast(1.0, variable.dtype)\n         local_step = ops.cast(self.iterations + 1, variable.dtype)\n-        if self.relative_step:  # TODO: add learning_rate_schedule logic\n-            # If `relative_step=True` and learning rate is a constant, we\n-            # apply the relative step algorithm.\n+        if not callable(self._learning_rate) and self.relative_step:\n             lr = ops.minimum(lr, 1 / ops.sqrt(local_step))\n \n         r = self._r[self._get_variable_index(variable)]\n\n@@ -14,18 +14,16 @@ class Adagrad(optimizer.Optimizer):\n     the smaller the updates.\n \n     Args:\n-        learning_rate: Initial value for the learning rate:\n-            a floating point value,\n-            Defaults to 0.001.\n-            Note that `Adagrad` tends to benefit from higher initial\n-            learning rate values compared to other optimizers.\n-            To match the exact form in the original paper, use 1.0.\n-        initial_accumulator_value: Floating point value.\n-            Starting value for the accumulators (per-parameter\n-            momentum values).\n-            Must be non-negative.\n-        epsilon: Small floating point value used to maintain\n-            numerical stability.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001. Note that `Adagrad` tends\n+            to benefit from higher initial learning rate values compared to\n+            other optimizers. To match the exact form in the original paper,\n+            use 1.0.\n+        initial_accumulator_value: Floating point value. Starting value for the\n+            accumulators (per-parameter momentum values). Must be non-negative.\n+        epsilon: Small floating point value for maintaining numerical stability.\n         {{base_optimizer_keyword_args}}\n \n     Reference:\n\n@@ -18,9 +18,10 @@ class Adam(optimizer.Optimizer):\n     data/parameters*\".\n \n     Args:\n-        learning_rate: A floating point value or a callable that takes no\n-            arguments and returns the actual value to use. The learning rate.\n-            Defaults to `0.001`.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001.\n         beta_1: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n             exponential decay rate for the 1st moment estimates. Defaults to\n\n@@ -34,9 +34,10 @@ class Adamax(optimizer.Optimizer):\n     ```\n \n     Args:\n-        learning_rate: A floating point value, or a callable\n-            that takes no arguments and returns the actual value to use. The\n-            learning rate. Defaults to `0.001`.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001.\n         beta_1: A float value or a constant float tensor. The exponential decay\n             rate for the 1st moment estimates.\n         beta_2: A float value or a constant float tensor. The exponential decay\n\n@@ -21,9 +21,10 @@ class AdamW(optimizer.Optimizer):\n     data/parameters*\".\n \n     Args:\n-        learning_rate: A floating point value or a callable that takes no\n-            arguments and returns the actual value to use. The learning rate.\n-            Defaults to 0.001.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001.\n         beta_1: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n             exponential decay rate for the 1st moment estimates.\n\n@@ -52,9 +52,10 @@ class Ftrl(optimizer.Optimizer):\n     is replaced with a gradient with shrinkage.\n \n     Args:\n-        learning_rate: A floating point value, or a callable that\n-            takes no arguments and returns the actual value to use. The learning\n-            rate.  Defaults to `0.001`.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001.\n         learning_rate_power: A float value, must be less or equal to zero.\n             Controls how the learning rate decreases during training. Use zero\n             for a fixed learning rate.\n\n@@ -12,9 +12,10 @@ class Nadam(optimizer.Optimizer):\n     Nesterov momentum.\n \n     Args:\n-        learning_rate: A floating point value or a callable\n-            that takes no arguments and returns the actual value to use. The\n-            learning rate. Defaults to `0.001`.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001.\n         beta_1: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n             exponential decay rate for the 1st moment estimates.\n\n@@ -18,8 +18,10 @@ class RMSprop(optimizer.Optimizer):\n     gradients, and uses that average to estimate the variance.\n \n     Args:\n-        learning_rate: Initial value for the learning rate: a floating point\n-            value, defaults to 0.001.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001.\n         rho: float, defaults to 0.9. Discounting factor for the old gradients.\n         momentum: float, defaults to 0.0. If not 0.0., the optimizer tracks the\n             momentum value, with a decay rate equals to `1 - momentum`.\n\n@@ -28,13 +28,13 @@ class SGD(optimizer.Optimizer):\n     ```\n \n     Args:\n-      learning_rate: A `Tensor`, floating point value, or a schedule that is a\n-        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n-        that takes no arguments and returns the actual value to use. The\n-        learning rate. Defaults to 0.001.\n+        learning_rate: A float, a\n+            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.01.\n         momentum: float hyperparameter >= 0 that accelerates gradient descent in\n-        the relevant direction and dampens oscillations. Defaults to 0, i.e.,\n-        vanilla gradient descent.\n+            the relevant direction and dampens oscillations. Defaults to 0,\n+            i.e., vanilla gradient descent.\n         nesterov: boolean. Whether to apply Nesterov momentum.\n             Defaults to `False`.\n         {{base_optimizer_keyword_args}}\n\n@@ -228,6 +228,7 @@ class TestTrainer(testing.TestCase):\n \n         x = np.ones((100, 4))\n         y = np.ones((100, 1))\n+        batch_size = 16\n         model = ExampleModel(units=1)\n         model.compile(\n             loss=\"mse\",\n@@ -240,8 +241,11 @@ class TestTrainer(testing.TestCase):\n \n         model_2 = ExampleModel(units=1)\n         model_2.compile(loss=\"mse\", optimizer=\"adam\", steps_per_execution=1)\n-        model_2.fit(x=x, y=y, batch_size=16, verbose=0)\n+        model_2.fit(x=x, y=y, batch_size=batch_size, verbose=0)\n \n         self.assertAllClose(model.get_weights(), model_2.get_weights())\n-        self.assertAllClose(model.predict(x), model_2.predict(x))\n+        self.assertAllClose(\n+            model.predict(x, batch_size=batch_size),\n+            model_2.predict(x, batch_size=batch_size),\n+        )\n         self.assertAllClose(model.evaluate(x, y), model_2.evaluate(x, y))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
