{"custom_id": "keras#a2ac73d61d27a5bed68d82a38bdda0e3aea4dc88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 449 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -364,8 +364,8 @@ def smart_resize(\n         data_format: `\"channels_last\"` or `\"channels_first\"`.\n \n     Returns:\n-        Array with shape `(size[0], size[1], channels)`. If the input image was a\n-        NumPy array, the output is a NumPy array,\n+        Array with shape `(size[0], size[1], channels)`.\n+        If the input image was a NumPy array, the output is a NumPy array,\n         and if it was a backend-native tensor,\n         the output is a backend-native tensor.\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8facd3fb40d52c73485b0542f245cc0c5ee47898", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1146 | Lines Deleted: 12 | Files Changed: 4 | Hunks: 19 | Methods Changed: 39 | Complexity Δ (Sum/Max): 84/42 | Churn Δ: 1158 | Churn Cumulative: 4212 | Contributors (this commit): 5 | Commits (past 90d): 41 | Contributors (cumulative): 11 | DMM Complexity: 0.7222222222222222\n\nDIFF:\n@@ -5,6 +5,7 @@ from keras_core.metrics.accuracy_metrics import CategoricalAccuracy\n from keras_core.metrics.accuracy_metrics import SparseCategoricalAccuracy\n from keras_core.metrics.accuracy_metrics import SparseTopKCategoricalAccuracy\n from keras_core.metrics.accuracy_metrics import TopKCategoricalAccuracy\n+from keras_core.metrics.confusion_metrics import AUC\n from keras_core.metrics.confusion_metrics import FalseNegatives\n from keras_core.metrics.confusion_metrics import FalsePositives\n from keras_core.metrics.confusion_metrics import Precision\n@@ -41,6 +42,7 @@ ALL_OBJECTS = {\n     # Regression\n     MeanSquaredError,\n     # Classification\n+    AUC,\n     FalseNegatives,\n     FalsePositives,\n     Precision,\n\n@@ -1,3 +1,6 @@\n+import numpy as np\n+\n+from keras_core import activations\n from keras_core import backend\n from keras_core import initializers\n from keras_core import operations as ops\n@@ -366,7 +369,7 @@ class Precision(Metric):\n             y_pred: The predicted values. Each element must be in the range\n                 `[0, 1]`.\n             sample_weight: Optional weighting of each example. Defaults to 1.\n-                Can be a `Tensor` whose rank is either 0, or the same rank as\n+                Can be a tensor whose rank is either 0, or the same rank as\n                 `y_true`, and must be broadcastable to `y_true`.\n         \"\"\"\n         metrics_utils.update_confusion_matrix_variables(\n@@ -507,7 +510,7 @@ class Recall(Metric):\n             y_pred: The predicted values. Each element must be in the range\n                 `[0, 1]`.\n             sample_weight: Optional weighting of each example. Defaults to 1.\n-                Can be a `Tensor` whose rank is either 0, or the same rank as\n+                Can be a tensor whose rank is either 0, or the same rank as\n                 `y_true`, and must be broadcastable to `y_true`.\n         \"\"\"\n         metrics_utils.update_confusion_matrix_variables(\n@@ -605,7 +608,7 @@ class SensitivitySpecificityBase(Metric):\n             y_true: The ground truth values.\n             y_pred: The predicted values.\n             sample_weight: Optional weighting of each example. Defaults to 1.\n-                Can be a `Tensor` whose rank is either 0, or the same rank as\n+                Can be a tensor whose rank is either 0, or the same rank as\n                 `y_true`, and must be broadcastable to `y_true`.\n         \"\"\"\n         metrics_utils.update_confusion_matrix_variables(\n@@ -1052,3 +1055,518 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n         }\n         base_config = super().get_config()\n         return {**base_config, **config}\n+\n+\n+@keras_core_export(\"keras_core.metrics.AUC\")\n+class AUC(Metric):\n+    \"\"\"Approximates the AUC (Area under the curve) of the ROC or PR curves.\n+\n+    The AUC (Area under the curve) of the ROC (Receiver operating\n+    characteristic; default) or PR (Precision Recall) curves are quality\n+    measures of binary classifiers. Unlike the accuracy, and like cross-entropy\n+    losses, ROC-AUC and PR-AUC evaluate all the operational points of a model.\n+\n+    This class approximates AUCs using a Riemann sum. During the metric\n+    accumulation phrase, predictions are accumulated within predefined buckets\n+    by value. The AUC is then computed by interpolating per-bucket averages.\n+    These buckets define the evaluated operational points.\n+\n+    This metric creates four local variables, `true_positives`,\n+    `true_negatives`, `false_positives` and `false_negatives` that are used to\n+    compute the AUC.  To discretize the AUC curve, a linearly spaced set of\n+    thresholds is used to compute pairs of recall and precision values. The area\n+    under the ROC-curve is therefore computed using the height of the recall\n+    values by the false positive rate, while the area under the PR-curve is the\n+    computed using the height of the precision values by the recall.\n+\n+    This value is ultimately returned as `auc`, an idempotent operation that\n+    computes the area under a discretized curve of precision versus recall\n+    values (computed using the aforementioned variables). The `num_thresholds`\n+    variable controls the degree of discretization with larger numbers of\n+    thresholds more closely approximating the true AUC. The quality of the\n+    approximation may vary dramatically depending on `num_thresholds`. The\n+    `thresholds` parameter can be used to manually specify thresholds which\n+    split the predictions more evenly.\n+\n+    For a best approximation of the real AUC, `predictions` should be\n+    distributed approximately uniformly in the range `[0, 1]` (if\n+    `from_logits=False`). The quality of the AUC approximation may be poor if\n+    this is not the case. Setting `summation_method` to 'minoring' or 'majoring'\n+    can help quantify the error in the approximation by providing lower or upper\n+    bound estimate of the AUC.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+        num_thresholds: (Optional) The number of thresholds to\n+            use when discretizing the roc curve. Values must be > 1.\n+            Defaults to `200`.\n+        curve: (Optional) Specifies the name of the curve to be computed,\n+            `'ROC'` (default) or `'PR'` for the Precision-Recall-curve.\n+        summation_method: (Optional) Specifies the [Riemann summation method](\n+              https://en.wikipedia.org/wiki/Riemann_sum) used.\n+              'interpolation' (default) applies mid-point summation scheme for\n+              `ROC`.  For PR-AUC, interpolates (true/false) positives but not\n+              the ratio that is precision (see Davis & Goadrich 2006 for\n+              details); 'minoring' applies left summation for increasing\n+              intervals and right summation for decreasing intervals; 'majoring'\n+              does the opposite.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+        thresholds: (Optional) A list of floating point values to use as the\n+            thresholds for discretizing the curve. If set, the `num_thresholds`\n+            parameter is ignored. Values should be in `[0, 1]`. Endpoint\n+            thresholds equal to {`-epsilon`, `1+epsilon`} for a small positive\n+            epsilon value will be automatically included with these to correctly\n+            handle predictions equal to exactly 0 or 1.\n+        multi_label: boolean indicating whether multilabel data should be\n+            treated as such, wherein AUC is computed separately for each label\n+            and then averaged across labels, or (when `False`) if the data\n+            should be flattened into a single label before AUC computation. In\n+            the latter case, when multilabel data is passed to AUC, each\n+            label-prediction pair is treated as an individual data point. Should\n+            be set to False for multi-class data.\n+        num_labels: (Optional) The number of labels, used when `multi_label` is\n+            True. If `num_labels` is not specified, then state variables get\n+            created on the first call to `update_state`.\n+        label_weights: (Optional) list, array, or tensor of non-negative weights\n+            used to compute AUCs for multilabel data. When `multi_label` is\n+            True, the weights are applied to the individual label AUCs when they\n+            are averaged to produce the multi-label AUC. When it's False, they\n+            are used to weight the individual label predictions in computing the\n+            confusion matrix on the flattened data. Note that this is unlike\n+            `class_weights` in that `class_weights` weights the example\n+            depending on the value of its label, whereas `label_weights` depends\n+            only on the index of that label before flattening; therefore\n+            `label_weights` should not be used for multi-class data.\n+        from_logits: boolean indicating whether the predictions (`y_pred` in\n+        `update_state`) are probabilities or sigmoid logits. As a rule of thumb,\n+        when using a keras loss, the `from_logits` constructor argument of the\n+        loss should match the AUC `from_logits` constructor argument.\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.AUC(num_thresholds=3)\n+    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])\n+    >>> # threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]\n+    >>> # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n+    >>> # tp_rate = recall = [1, 0.5, 0], fp_rate = [1, 0, 0]\n+    >>> # auc = ((((1 + 0.5) / 2) * (1 - 0)) + (((0.5 + 0) / 2) * (0 - 0)))\n+    >>> #     = 0.75\n+    >>> m.result()\n+    0.75\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],\n+    ...                sample_weight=[1, 0, 0, 1])\n+    >>> m.result()\n+    1.0\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    # Reports the AUC of a model outputting a probability.\n+    model.compile(optimizer='sgd',\n+                  loss=keras_core.losses.BinaryCrossentropy(),\n+                  metrics=[keras_core.metrics.AUC()])\n+\n+    # Reports the AUC of a model outputting a logit.\n+    model.compile(optimizer='sgd',\n+                  loss=keras_core.losses.BinaryCrossentropy(from_logits=True),\n+                  metrics=[keras_core.metrics.AUC(from_logits=True)])\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        num_thresholds=200,\n+        curve=\"ROC\",\n+        summation_method=\"interpolation\",\n+        name=None,\n+        dtype=None,\n+        thresholds=None,\n+        multi_label=False,\n+        num_labels=None,\n+        label_weights=None,\n+        from_logits=False,\n+    ):\n+        # Validate configurations.\n+        if isinstance(curve, metrics_utils.AUCCurve) and curve not in list(\n+            metrics_utils.AUCCurve\n+        ):\n+            raise ValueError(\n+                f'Invalid `curve` argument value \"{curve}\". '\n+                f\"Expected one of: {list(metrics_utils.AUCCurve)}\"\n+            )\n+        if isinstance(\n+            summation_method, metrics_utils.AUCSummationMethod\n+        ) and summation_method not in list(metrics_utils.AUCSummationMethod):\n+            raise ValueError(\n+                \"Invalid `summation_method` \"\n+                f'argument value \"{summation_method}\". '\n+                f\"Expected one of: {list(metrics_utils.AUCSummationMethod)}\"\n+            )\n+\n+        # Update properties.\n+        self._init_from_thresholds = thresholds is not None\n+        if thresholds is not None:\n+            # If specified, use the supplied thresholds.\n+            self.num_thresholds = len(thresholds) + 2\n+            thresholds = sorted(thresholds)\n+            self._thresholds_distributed_evenly = (\n+                metrics_utils.is_evenly_distributed_thresholds(\n+                    np.array([0.0] + thresholds + [1.0])\n+                )\n+            )\n+        else:\n+            if num_thresholds <= 1:\n+                raise ValueError(\n+                    \"Argument `num_thresholds` must be an integer > 1. \"\n+                    f\"Received: num_thresholds={num_thresholds}\"\n+                )\n+\n+            # Otherwise, linearly interpolate (num_thresholds - 2) thresholds in\n+            # (0, 1).\n+            self.num_thresholds = num_thresholds\n+            thresholds = [\n+                (i + 1) * 1.0 / (num_thresholds - 1)\n+                for i in range(num_thresholds - 2)\n+            ]\n+            self._thresholds_distributed_evenly = True\n+\n+        # Add an endpoint \"threshold\" below zero and above one for either\n+        # threshold method to account for floating point imprecisions.\n+        self._thresholds = np.array(\n+            [0.0 - backend.epsilon()] + thresholds + [1.0 + backend.epsilon()]\n+        )\n+\n+        if isinstance(curve, metrics_utils.AUCCurve):\n+            self.curve = curve\n+        else:\n+            self.curve = metrics_utils.AUCCurve.from_str(curve)\n+        if isinstance(summation_method, metrics_utils.AUCSummationMethod):\n+            self.summation_method = summation_method\n+        else:\n+            self.summation_method = metrics_utils.AUCSummationMethod.from_str(\n+                summation_method\n+            )\n+        super().__init__(name=name, dtype=dtype)\n+\n+        # Handle multilabel arguments.\n+        self.multi_label = multi_label\n+        self.num_labels = num_labels\n+        if label_weights is not None:\n+            label_weights = ops.array(label_weights, dtype=self.dtype)\n+            self.label_weights = label_weights\n+\n+        else:\n+            self.label_weights = None\n+\n+        self._from_logits = from_logits\n+\n+        self._built = False\n+        if self.multi_label:\n+            if num_labels:\n+                shape = [None, num_labels]\n+                self._build(shape)\n+        else:\n+            if num_labels:\n+                raise ValueError(\n+                    \"`num_labels` is needed only when `multi_label` is True.\"\n+                )\n+            self._build(None)\n+\n+    @property\n+    def thresholds(self):\n+        \"\"\"The thresholds used for evaluating AUC.\"\"\"\n+        return list(self._thresholds)\n+\n+    def _build(self, shape):\n+        \"\"\"Initialize TP, FP, TN, and FN tensors, given the shape of the\n+        data.\"\"\"\n+        if self.multi_label:\n+            if len(shape) != 2:\n+                raise ValueError(\n+                    \"`y_pred` must have rank 2 when `multi_label=True`. \"\n+                    f\"Found rank {len(shape)}. \"\n+                    f\"Full shape received for `y_pred`: {shape}\"\n+                )\n+            self._num_labels = shape[1]\n+            variable_shape = [self.num_thresholds, self._num_labels]\n+        else:\n+            variable_shape = [self.num_thresholds]\n+\n+        self._build_input_shape = shape\n+        # Create metric variables\n+        self.true_positives = self.add_variable(\n+            shape=variable_shape,\n+            initializer=initializers.Zeros(),\n+            name=\"true_positives\",\n+        )\n+        self.false_positives = self.add_variable(\n+            shape=variable_shape,\n+            initializer=initializers.Zeros(),\n+            name=\"false_positives\",\n+        )\n+        self.true_negatives = self.add_variable(\n+            shape=variable_shape,\n+            initializer=initializers.Zeros(),\n+            name=\"true_negatives\",\n+        )\n+        self.false_negatives = self.add_variable(\n+            shape=variable_shape,\n+            initializer=initializers.Zeros(),\n+            name=\"false_negatives\",\n+        )\n+\n+        self._built = True\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates confusion matrix statistics.\n+\n+        Args:\n+            y_true: The ground truth values.\n+            y_pred: The predicted values.\n+            sample_weight: Optional weighting of each example. Can\n+                be a tensor whose rank is either 0, or the same rank as\n+                `y_true`, and must be broadcastable to `y_true`. Defaults to\n+                `1`.\n+        \"\"\"\n+        if not self._built:\n+            self._build(y_pred.shape)\n+\n+        if self.multi_label or (self.label_weights is not None):\n+            # y_true should have shape (number of examples, number of labels).\n+            shapes = [(y_true, (\"N\", \"L\"))]\n+            if self.multi_label:\n+                # TP, TN, FP, and FN should all have shape\n+                # (number of thresholds, number of labels).\n+                shapes.extend(\n+                    [\n+                        (self.true_positives, (\"T\", \"L\")),\n+                        (self.true_negatives, (\"T\", \"L\")),\n+                        (self.false_positives, (\"T\", \"L\")),\n+                        (self.false_negatives, (\"T\", \"L\")),\n+                    ]\n+                )\n+            if self.label_weights is not None:\n+                # label_weights should be of length equal to the number of\n+                # labels.\n+                shapes.append((self.label_weights, (\"L\",)))\n+\n+        # Only forward label_weights to update_confusion_matrix_variables when\n+        # multi_label is False. Otherwise the averaging of individual label AUCs\n+        # is handled in AUC.result\n+        label_weights = None if self.multi_label else self.label_weights\n+\n+        if self._from_logits:\n+            y_pred = activations.sigmoid(y_pred)\n+\n+        metrics_utils.update_confusion_matrix_variables(\n+            {\n+                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.TRUE_NEGATIVES: self.true_negatives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives,  # noqa: E501\n+            },\n+            y_true,\n+            y_pred,\n+            self._thresholds,\n+            thresholds_distributed_evenly=self._thresholds_distributed_evenly,\n+            sample_weight=sample_weight,\n+            multi_label=self.multi_label,\n+            label_weights=label_weights,\n+        )\n+\n+    def interpolate_pr_auc(self):\n+        \"\"\"Interpolation formula inspired by section 4 of Davis & Goadrich 2006.\n+\n+        https://www.biostat.wisc.edu/~page/rocpr.pdf\n+\n+        Note here we derive & use a closed formula not present in the paper\n+        as follows:\n+\n+            Precision = TP / (TP + FP) = TP / P\n+\n+        Modeling all of TP (true positive), FP (false positive) and their sum\n+        P = TP + FP (predicted positive) as varying linearly within each\n+        interval [A, B] between successive thresholds, we get\n+\n+            Precision slope = dTP / dP\n+                            = (TP_B - TP_A) / (P_B - P_A)\n+                            = (TP - TP_A) / (P - P_A)\n+            Precision = (TP_A + slope * (P - P_A)) / P\n+\n+        The area within the interval is (slope / total_pos_weight) times\n+\n+            int_A^B{Precision.dP} = int_A^B{(TP_A + slope * (P - P_A)) * dP / P}\n+            int_A^B{Precision.dP} = int_A^B{slope * dP + intercept * dP / P}\n+\n+        where intercept = TP_A - slope * P_A = TP_B - slope * P_B, resulting in\n+\n+            int_A^B{Precision.dP} = TP_B - TP_A + intercept * log(P_B / P_A)\n+\n+        Bringing back the factor (slope / total_pos_weight) we'd put aside, we\n+        get\n+\n+            slope * [dTP + intercept *  log(P_B / P_A)] / total_pos_weight\n+\n+        where dTP == TP_B - TP_A.\n+\n+        Note that when P_A == 0 the above calculation simplifies into\n+\n+            int_A^B{Precision.dTP} = int_A^B{slope * dTP}\n+                                   = slope * (TP_B - TP_A)\n+\n+        which is really equivalent to imputing constant precision throughout the\n+        first bucket having >0 true positives.\n+\n+        Returns:\n+            pr_auc: an approximation of the area under the P-R curve.\n+        \"\"\"\n+\n+        dtp = (\n+            self.true_positives[: self.num_thresholds - 1]\n+            - self.true_positives[1:]\n+        )\n+        p = ops.add(self.true_positives, self.false_positives)\n+        dp = p[: self.num_thresholds - 1] - p[1:]\n+        prec_slope = ops.divide(dtp, ops.maximum(dp, backend.epsilon()))\n+        intercept = self.true_positives[1:] - ops.multiply(prec_slope, p[1:])\n+\n+        safe_p_ratio = ops.where(\n+            ops.logical_and(p[: self.num_thresholds - 1] > 0, p[1:] > 0),\n+            ops.divide(\n+                p[: self.num_thresholds - 1],\n+                ops.maximum(p[1:], backend.epsilon()),\n+            ),\n+            ops.ones_like(p[1:]),\n+        )\n+\n+        pr_auc_increment = ops.divide(\n+            prec_slope * (dtp + intercept * ops.log(safe_p_ratio)),\n+            ops.maximum(\n+                self.true_positives[1:] + self.false_negatives[1:],\n+                backend.epsilon(),\n+            ),\n+        )\n+\n+        if self.multi_label:\n+            by_label_auc = ops.sum(pr_auc_increment, axis=0)\n+            if self.label_weights is None:\n+                # Evenly weighted average of the label AUCs.\n+                return ops.mean(by_label_auc)\n+            else:\n+                # Weighted average of the label AUCs.\n+                return ops.divide(\n+                    ops.sum(ops.multiply(by_label_auc, self.label_weights)),\n+                    ops.maximum(ops.sum(self.label_weights), backend.epsilon()),\n+                )\n+        else:\n+            return ops.sum(pr_auc_increment)\n+\n+    def result(self):\n+        if (\n+            self.curve == metrics_utils.AUCCurve.PR\n+            and self.summation_method\n+            == metrics_utils.AUCSummationMethod.INTERPOLATION\n+        ):\n+            # This use case is different and is handled separately.\n+            return self.interpolate_pr_auc()\n+\n+        # Set `x` and `y` values for the curves based on `curve` config.\n+        recall = ops.divide(\n+            self.true_positives,\n+            ops.maximum(\n+                ops.add(self.true_positives, self.false_negatives),\n+                backend.epsilon(),\n+            ),\n+        )\n+        if self.curve == metrics_utils.AUCCurve.ROC:\n+            fp_rate = ops.divide(\n+                self.false_positives,\n+                ops.maximum(\n+                    ops.add(self.false_positives, self.true_negatives),\n+                    backend.epsilon(),\n+                ),\n+            )\n+            x = fp_rate\n+            y = recall\n+        else:  # curve == 'PR'.\n+            precision = ops.divide(\n+                self.true_positives,\n+                ops.maximum(\n+                    ops.add(self.true_positives, self.false_positives),\n+                    backend.epsilon(),\n+                ),\n+            )\n+            x = recall\n+            y = precision\n+\n+        # Find the rectangle heights based on `summation_method`.\n+        if (\n+            self.summation_method\n+            == metrics_utils.AUCSummationMethod.INTERPOLATION\n+        ):\n+            # Note: the case ('PR', 'interpolation') has been handled above.\n+            heights = (y[: self.num_thresholds - 1] + y[1:]) / 2.0\n+        elif self.summation_method == metrics_utils.AUCSummationMethod.MINORING:\n+            heights = ops.minimum(y[: self.num_thresholds - 1], y[1:])\n+        # self.summation_method = metrics_utils.AUCSummationMethod.MAJORING:\n+        else:\n+            heights = ops.maximum(y[: self.num_thresholds - 1], y[1:])\n+\n+        # Sum up the areas of all the rectangles.\n+        if self.multi_label:\n+            riemann_terms = ops.multiply(\n+                x[: self.num_thresholds - 1] - x[1:], heights\n+            )\n+            by_label_auc = ops.sum(riemann_terms, axis=0)\n+\n+            if self.label_weights is None:\n+                # Unweighted average of the label AUCs.\n+                return ops.mean(by_label_auc)\n+            else:\n+                # Weighted average of the label AUCs.\n+                return ops.divide(\n+                    ops.sum(ops.multiply(by_label_auc, self.label_weights)),\n+                    ops.maximum(ops.sum(self.label_weights), backend.epsilon()),\n+                )\n+        else:\n+            return ops.sum(\n+                ops.multiply(x[: self.num_thresholds - 1] - x[1:], heights)\n+            )\n+\n+    def reset_state(self):\n+        if self._built:\n+            if self.multi_label:\n+                variable_shape = (self.num_thresholds, self._num_labels)\n+            else:\n+                variable_shape = (self.num_thresholds,)\n+\n+            self.true_positives.assign(ops.zeros(variable_shape))\n+            self.false_positives.assign(ops.zeros(variable_shape))\n+            self.true_negatives.assign(ops.zeros(variable_shape))\n+            self.false_negatives.assign(ops.zeros(variable_shape))\n+\n+    def get_config(self):\n+        label_weights = self.label_weights\n+        config = {\n+            \"num_thresholds\": self.num_thresholds,\n+            \"curve\": self.curve.value,\n+            \"summation_method\": self.summation_method.value,\n+            \"multi_label\": self.multi_label,\n+            \"num_labels\": self.num_labels,\n+            \"label_weights\": label_weights,\n+            \"from_logits\": self._from_logits,\n+        }\n+        # optimization to avoid serializing a large number of generated\n+        # thresholds\n+        if self._init_from_thresholds:\n+            # We remove the endpoint thresholds as an inverse of how the\n+            # thresholds were initialized. This ensures that a metric\n+            # initialized from this config has the same thresholds.\n+            config[\"thresholds\"] = self.thresholds[1:-1]\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -1,10 +1,16 @@\n+import json\n+\n import numpy as np\n+from absl import logging\n from absl.testing import parameterized\n from tensorflow.python.ops.numpy_ops import np_config\n \n+from keras_core import layers\n from keras_core import metrics\n+from keras_core import models\n from keras_core import operations as ops\n from keras_core import testing\n+from keras_core.metrics import metrics_utils\n \n # TODO: remove reliance on this (or alternatively, turn it on by default).\n # This is no longer needed with tf-nightly.\n@@ -1101,3 +1107,558 @@ class RecallAtPrecisionTest(testing.TestCase, parameterized.TestCase):\n             ValueError, \"Argument `num_thresholds` must be an integer > 0\"\n         ):\n             metrics.RecallAtPrecision(0.4, num_thresholds=-1)\n+\n+\n+class AUCTest(testing.TestCase):\n+    def setUp(self):\n+        self.num_thresholds = 3\n+        self.y_pred = np.array([0, 0.5, 0.3, 0.9], dtype=\"float32\")\n+        self.y_pred_multi_label = np.array(\n+            [[0.0, 0.4], [0.5, 0.7], [0.3, 0.2], [0.9, 0.3]], dtype=\"float32\"\n+        )\n+        epsilon = 1e-12\n+        self.y_pred_logits = -ops.log(1.0 / (self.y_pred + epsilon) - 1.0)\n+        self.y_true = np.array([0, 0, 1, 1])\n+        self.y_true_multi_label = np.array([[0, 0], [1, 1], [1, 1], [1, 0]])\n+        self.sample_weight = [1, 2, 3, 4]\n+\n+        # threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]\n+        # y_pred when threshold = 0 - 1e-7  : [1, 1, 1, 1]\n+        # y_pred when threshold = 0.5       : [0, 0, 0, 1]\n+        # y_pred when threshold = 1 + 1e-7  : [0, 0, 0, 0]\n+\n+        # without sample_weight:\n+        # tp = np.sum([[0, 0, 1, 1], [0, 0, 0, 1], [0, 0, 0, 0]], axis=1)\n+        # fp = np.sum([[1, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], axis=1)\n+        # fn = np.sum([[0, 0, 0, 0], [0, 0, 1, 0], [0, 0, 1, 1]], axis=1)\n+        # tn = np.sum([[0, 0, 0, 0], [1, 1, 0, 0], [1, 1, 0, 0]], axis=1)\n+\n+        # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n+\n+        # with sample_weight:\n+        # tp = np.sum([[0, 0, 3, 4], [0, 0, 0, 4], [0, 0, 0, 0]], axis=1)\n+        # fp = np.sum([[1, 2, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], axis=1)\n+        # fn = np.sum([[0, 0, 0, 0], [0, 0, 3, 0], [0, 0, 3, 4]], axis=1)\n+        # tn = np.sum([[0, 0, 0, 0], [1, 2, 0, 0], [1, 2, 0, 0]], axis=1)\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+\n+    def test_config(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=100,\n+            curve=\"PR\",\n+            summation_method=\"majoring\",\n+            name=\"auc_1\",\n+            dtype=\"float64\",\n+            multi_label=True,\n+            num_labels=2,\n+            from_logits=True,\n+        )\n+        auc_obj.update_state(self.y_true_multi_label, self.y_pred_multi_label)\n+        self.assertEqual(auc_obj.name, \"auc_1\")\n+        self.assertEqual(auc_obj._dtype, \"float64\")\n+        self.assertLen(auc_obj.variables, 4)\n+        self.assertEqual(auc_obj.num_thresholds, 100)\n+        self.assertEqual(auc_obj.curve, metrics_utils.AUCCurve.PR)\n+        self.assertEqual(\n+            auc_obj.summation_method, metrics_utils.AUCSummationMethod.MAJORING\n+        )\n+        self.assertTrue(auc_obj.multi_label)\n+        self.assertEqual(auc_obj.num_labels, 2)\n+        self.assertTrue(auc_obj._from_logits)\n+        old_config = auc_obj.get_config()\n+        self.assertNotIn(\"thresholds\", old_config)\n+        self.assertDictEqual(old_config, json.loads(json.dumps(old_config)))\n+\n+        # Check save and restore config.\n+        auc_obj2 = metrics.AUC.from_config(auc_obj.get_config())\n+        auc_obj2.update_state(self.y_true_multi_label, self.y_pred_multi_label)\n+        self.assertEqual(auc_obj2.name, \"auc_1\")\n+        self.assertLen(auc_obj2.variables, 4)\n+        self.assertEqual(auc_obj2.num_thresholds, 100)\n+        self.assertEqual(auc_obj2.curve, metrics_utils.AUCCurve.PR)\n+        self.assertEqual(\n+            auc_obj2.summation_method, metrics_utils.AUCSummationMethod.MAJORING\n+        )\n+        self.assertTrue(auc_obj2.multi_label)\n+        self.assertEqual(auc_obj2.num_labels, 2)\n+        self.assertTrue(auc_obj2._from_logits)\n+        new_config = auc_obj2.get_config()\n+        self.assertNotIn(\"thresholds\", new_config)\n+        self.assertDictEqual(old_config, new_config)\n+        self.assertAllClose(auc_obj.thresholds, auc_obj2.thresholds)\n+\n+    def test_config_manual_thresholds(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=None,\n+            curve=\"PR\",\n+            summation_method=\"majoring\",\n+            name=\"auc_1\",\n+            thresholds=[0.3, 0.5],\n+        )\n+        auc_obj.update_state(self.y_true, self.y_pred)\n+        self.assertEqual(auc_obj.name, \"auc_1\")\n+        self.assertLen(auc_obj.variables, 4)\n+        self.assertEqual(auc_obj.num_thresholds, 4)\n+        self.assertAllClose(auc_obj.thresholds, [0.0, 0.3, 0.5, 1.0])\n+        self.assertEqual(auc_obj.curve, metrics_utils.AUCCurve.PR)\n+        self.assertEqual(\n+            auc_obj.summation_method, metrics_utils.AUCSummationMethod.MAJORING\n+        )\n+        old_config = auc_obj.get_config()\n+        self.assertDictEqual(old_config, json.loads(json.dumps(old_config)))\n+\n+        # Check save and restore config.\n+        auc_obj2 = metrics.AUC.from_config(auc_obj.get_config())\n+        auc_obj2.update_state(self.y_true, self.y_pred)\n+        self.assertEqual(auc_obj2.name, \"auc_1\")\n+        self.assertLen(auc_obj2.variables, 4)\n+        self.assertEqual(auc_obj2.num_thresholds, 4)\n+        self.assertEqual(auc_obj2.curve, metrics_utils.AUCCurve.PR)\n+        self.assertEqual(\n+            auc_obj2.summation_method, metrics_utils.AUCSummationMethod.MAJORING\n+        )\n+        new_config = auc_obj2.get_config()\n+        self.assertDictEqual(old_config, new_config)\n+        self.assertAllClose(auc_obj.thresholds, auc_obj2.thresholds)\n+\n+    def test_unweighted_all_correct(self):\n+        auc_obj = metrics.AUC()\n+        self.assertEqual(auc_obj(self.y_true, self.y_true), 1)\n+\n+    def test_unweighted(self):\n+        auc_obj = metrics.AUC(num_thresholds=self.num_thresholds)\n+        result = auc_obj(self.y_true, self.y_pred)\n+\n+        # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n+        # recall = [2/2, 1/(1+1), 0] = [1, 0.5, 0]\n+        # fp_rate = [2/2, 0, 0] = [1, 0, 0]\n+        # heights = [(1 + 0.5)/2, (0.5 + 0)/2] = [0.75, 0.25]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = 0.75 * 1 + 0.25 * 0\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_unweighted_from_logits(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, from_logits=True\n+        )\n+        result = auc_obj(self.y_true, self.y_pred_logits)\n+\n+        # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n+        # recall = [2/2, 1/(1+1), 0] = [1, 0.5, 0]\n+        # fp_rate = [2/2, 0, 0] = [1, 0, 0]\n+        # heights = [(1 + 0.5)/2, (0.5 + 0)/2] = [0.75, 0.25]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = 0.75 * 1 + 0.25 * 0\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_manual_thresholds(self):\n+        # Verify that when specified, thresholds are used instead of\n+        # num_thresholds.\n+        auc_obj = metrics.AUC(num_thresholds=2, thresholds=[0.5])\n+        self.assertEqual(auc_obj.num_thresholds, 3)\n+        self.assertAllClose(auc_obj.thresholds, [0.0, 0.5, 1.0])\n+        result = auc_obj(self.y_true, self.y_pred)\n+\n+        # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n+        # recall = [2/2, 1/(1+1), 0] = [1, 0.5, 0]\n+        # fp_rate = [2/2, 0, 0] = [1, 0, 0]\n+        # heights = [(1 + 0.5)/2, (0.5 + 0)/2] = [0.75, 0.25]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = 0.75 * 1 + 0.25 * 0\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_weighted_roc_interpolation(self):\n+        auc_obj = metrics.AUC(num_thresholds=self.num_thresholds)\n+        result = auc_obj(\n+            self.y_true, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n+        # fp_rate = [3/3, 0, 0] = [1, 0, 0]\n+        # heights = [(1 + 0.571)/2, (0.571 + 0)/2] = [0.7855, 0.2855]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = 0.7855 * 1 + 0.2855 * 0\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_weighted_roc_majoring(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, summation_method=\"majoring\"\n+        )\n+        result = auc_obj(\n+            self.y_true, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n+        # fp_rate = [3/3, 0, 0] = [1, 0, 0]\n+        # heights = [max(1, 0.571), max(0.571, 0)] = [1, 0.571]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = 1 * 1 + 0.571 * 0\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_weighted_roc_minoring(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, summation_method=\"minoring\"\n+        )\n+        result = auc_obj(\n+            self.y_true, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n+        # fp_rate = [3/3, 0, 0] = [1, 0, 0]\n+        # heights = [min(1, 0.571), min(0.571, 0)] = [0.571, 0]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = 0.571 * 1 + 0 * 0\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_weighted_pr_majoring(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds,\n+            curve=\"PR\",\n+            summation_method=\"majoring\",\n+        )\n+        result = auc_obj(\n+            self.y_true, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # precision = [7/(7+3), 4/4, 0] = [0.7, 1, 0]\n+        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n+        # heights = [max(0.7, 1), max(1, 0)] = [1, 1]\n+        # widths = [(1 - 0.571), (0.571 - 0)] = [0.429, 0.571]\n+        expected_result = 1 * 0.429 + 1 * 0.571\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_weighted_pr_minoring(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds,\n+            curve=\"PR\",\n+            summation_method=\"minoring\",\n+        )\n+        result = auc_obj(\n+            self.y_true, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # precision = [7/(7+3), 4/4, 0] = [0.7, 1, 0]\n+        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n+        # heights = [min(0.7, 1), min(1, 0)] = [0.7, 0]\n+        # widths = [(1 - 0.571), (0.571 - 0)] = [0.429, 0.571]\n+        expected_result = 0.7 * 0.429 + 0 * 0.571\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_weighted_pr_interpolation(self):\n+        auc_obj = metrics.AUC(num_thresholds=self.num_thresholds, curve=\"PR\")\n+        result = auc_obj(\n+            self.y_true, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # auc = (slope / Total Pos) * [dTP - intercept * log(Pb/Pa)]\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # P = tp + fp = [10, 4, 0]\n+        # dTP = [7-4, 4-0] = [3, 4]\n+        # dP = [10-4, 4-0] = [6, 4]\n+        # slope = dTP/dP = [0.5, 1]\n+        # intercept = (TPa+(slope*Pa) = [(4 - 0.5*4), (0 - 1*0)] = [2, 0]\n+        # (Pb/Pa) = (Pb/Pa) if Pb > 0 AND Pa > 0 else 1 = [10/4, 4/0] = [2.5, 1]\n+        # auc * TotalPos = [(0.5 * (3 + 2 * log(2.5))), (1 * (4 + 0))]\n+        #                = [2.416, 4]\n+        # auc = [2.416, 4]/(tp[1:]+fn[1:])\n+        expected_result = 2.416 / 7 + 4 / 7\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_invalid_num_thresholds(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"Argument `num_thresholds` must be an integer > 1\"\n+        ):\n+            metrics.AUC(num_thresholds=-1)\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"Argument `num_thresholds` must be an integer > 1.\"\n+        ):\n+            metrics.AUC(num_thresholds=1)\n+\n+    def test_invalid_curve(self):\n+        with self.assertRaisesRegex(\n+            ValueError, 'Invalid AUC curve value: \"Invalid\".'\n+        ):\n+            metrics.AUC(curve=\"Invalid\")\n+\n+    def test_invalid_summation_method(self):\n+        with self.assertRaisesRegex(\n+            ValueError, 'Invalid AUC summation method value: \"Invalid\".'\n+        ):\n+            metrics.AUC(summation_method=\"Invalid\")\n+\n+    def test_extra_dims(self):\n+        try:\n+            from scipy import special\n+\n+            logits = special.expit(\n+                -np.array(\n+                    [\n+                        [[-10.0, 10.0, -10.0], [10.0, -10.0, 10.0]],\n+                        [[-12.0, 12.0, -12.0], [12.0, -12.0, 12.0]],\n+                    ],\n+                    dtype=np.float32,\n+                )\n+            )\n+            labels = np.array(\n+                [[[1, 0, 0], [1, 0, 0]], [[0, 1, 1], [0, 1, 1]]], dtype=np.int64\n+            )\n+            auc_obj = metrics.AUC()\n+            result = auc_obj(labels, logits)\n+            self.assertEqual(result, 0.5)\n+        except ImportError as e:\n+            logging.warning(f\"Cannot test special functions: {str(e)}\")\n+\n+\n+class MultiAUCTest(testing.TestCase):\n+    def setUp(self):\n+        self.num_thresholds = 5\n+        self.y_pred = np.array(\n+            [[0, 0.5, 0.3, 0.9], [0.1, 0.2, 0.3, 0.4]], dtype=\"float32\"\n+        ).T\n+\n+        epsilon = 1e-12\n+        self.y_pred_logits = -ops.log(1.0 / (self.y_pred + epsilon) - 1.0)\n+\n+        self.y_true_good = np.array([[0, 0, 1, 1], [0, 0, 1, 1]]).T\n+        self.y_true_bad = np.array([[0, 0, 1, 1], [1, 1, 0, 0]]).T\n+        self.sample_weight = [1, 2, 3, 4]\n+\n+        # threshold values are [0 - 1e-7, 0.25, 0.5, 0.75, 1 + 1e-7]\n+        # y_pred when threshold = 0 - 1e-7   : [[1, 1, 1, 1], [1, 1, 1, 1]]\n+        # y_pred when threshold = 0.25       : [[0, 1, 1, 1], [0, 0, 1, 1]]\n+        # y_pred when threshold = 0.5        : [[0, 0, 0, 1], [0, 0, 0, 0]]\n+        # y_pred when threshold = 0.75       : [[0, 0, 0, 1], [0, 0, 0, 0]]\n+        # y_pred when threshold = 1 + 1e-7   : [[0, 0, 0, 0], [0, 0, 0, 0]]\n+\n+        # for y_true_good, over thresholds:\n+        # tp = [[2, 2, 1, 1, 0], [2, 2, 0, 0, 0]]\n+        # fp = [[2, 1, 0, 0 , 0], [2, 0, 0 ,0, 0]]\n+        # fn = [[0, 0, 1, 1, 2], [0, 0, 2, 2, 2]]\n+        # tn = [[0, 1, 2, 2, 2], [0, 2, 2, 2, 2]]\n+\n+        # tpr = [[1, 1, 0.5, 0.5, 0], [1, 1, 0, 0, 0]]\n+        # fpr = [[1, 0.5, 0, 0, 0], [1, 0, 0, 0, 0]]\n+\n+        # for y_true_bad:\n+        # tp = [[2, 2, 1, 1, 0], [2, 0, 0, 0, 0]]\n+        # fp = [[2, 1, 0, 0 , 0], [2, 2, 0 ,0, 0]]\n+        # fn = [[0, 0, 1, 1, 2], [0, 2, 2, 2, 2]]\n+        # tn = [[0, 1, 2, 2, 2], [0, 0, 2, 2, 2]]\n+\n+        # tpr = [[1, 1, 0.5, 0.5, 0], [1, 0, 0, 0, 0]]\n+        # fpr = [[1, 0.5, 0, 0, 0], [1, 1, 0, 0, 0]]\n+\n+        # for y_true_good with sample_weights:\n+\n+        # tp = [[7, 7, 4, 4, 0], [7, 7, 0, 0, 0]]\n+        # fp = [[3, 2, 0, 0, 0], [3, 0, 0, 0, 0]]\n+        # fn = [[0, 0, 3, 3, 7], [0, 0, 7, 7, 7]]\n+        # tn = [[0, 1, 3, 3, 3], [0, 3, 3, 3, 3]]\n+\n+        # tpr = [[1, 1,    0.57, 0.57, 0], [1, 1, 0, 0, 0]]\n+        # fpr = [[1, 0.67, 0,    0,    0], [1, 0, 0, 0, 0]]\n+\n+    def test_unweighted_all_correct(self):\n+        auc_obj = metrics.AUC(multi_label=True)\n+        result = auc_obj(self.y_true_good, self.y_true_good)\n+        self.assertEqual(result, 1)\n+\n+    def test_unweighted_all_correct_flat(self):\n+        auc_obj = metrics.AUC(multi_label=False)\n+        result = auc_obj(self.y_true_good, self.y_true_good)\n+        self.assertEqual(result, 1)\n+\n+    def test_unweighted(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, multi_label=True\n+        )\n+        result = auc_obj(self.y_true_good, self.y_pred)\n+\n+        # tpr = [[1, 1, 0.5, 0.5, 0], [1, 1, 0, 0, 0]]\n+        # fpr = [[1, 0.5, 0, 0, 0], [1, 0, 0, 0, 0]]\n+        expected_result = (0.875 + 1.0) / 2.0\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_unweighted_from_logits(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds,\n+            multi_label=True,\n+            from_logits=True,\n+        )\n+        result = auc_obj(self.y_true_good, self.y_pred_logits)\n+\n+        # tpr = [[1, 1, 0.5, 0.5, 0], [1, 1, 0, 0, 0]]\n+        # fpr = [[1, 0.5, 0, 0, 0], [1, 0, 0, 0, 0]]\n+        expected_result = (0.875 + 1.0) / 2.0\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_sample_weight_flat(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, multi_label=False\n+        )\n+        result = auc_obj(\n+            self.y_true_good, self.y_pred, sample_weight=[1, 2, 3, 4]\n+        )\n+\n+        # tpr = [1, 1, 0.2857, 0.2857, 0]\n+        # fpr = [1, 0.3333, 0, 0, 0]\n+        expected_result = 1.0 - (0.3333 * (1.0 - 0.2857) / 2.0)\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_full_sample_weight_flat(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, multi_label=False\n+        )\n+        sw = np.arange(4 * 2)\n+        sw = sw.reshape(4, 2)\n+        result = auc_obj(self.y_true_good, self.y_pred, sample_weight=sw)\n+\n+        # tpr = [1, 1, 0.2727, 0.2727, 0]\n+        # fpr = [1, 0.3333, 0, 0, 0]\n+        expected_result = 1.0 - (0.3333 * (1.0 - 0.2727) / 2.0)\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_label_weights(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds,\n+            multi_label=True,\n+            label_weights=[0.75, 0.25],\n+        )\n+        result = auc_obj(self.y_true_good, self.y_pred)\n+\n+        # tpr = [[1, 1, 0.5, 0.5, 0], [1, 1, 0, 0, 0]]\n+        # fpr = [[1, 0.5, 0, 0, 0], [1, 0, 0, 0, 0]]\n+        expected_result = (0.875 * 0.75 + 1.0 * 0.25) / (0.75 + 0.25)\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_label_weights_flat(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds,\n+            multi_label=False,\n+            label_weights=[0.75, 0.25],\n+        )\n+        result = auc_obj(self.y_true_good, self.y_pred)\n+\n+        # tpr = [1, 1, 0.375, 0.375, 0]\n+        # fpr = [1, 0.375, 0, 0, 0]\n+        expected_result = 1.0 - ((1.0 - 0.375) * 0.375 / 2.0)\n+        self.assertAllClose(result, expected_result, 1e-2)\n+\n+    def test_unweighted_flat(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, multi_label=False\n+        )\n+        result = auc_obj(self.y_true_good, self.y_pred)\n+\n+        # tp = [4, 4, 1, 1, 0]\n+        # fp = [4, 1, 0, 0, 0]\n+        # fn = [0, 0, 3, 3, 4]\n+        # tn = [0, 3, 4, 4, 4]\n+\n+        # tpr = [1, 1, 0.25, 0.25, 0]\n+        # fpr = [1, 0.25, 0, 0, 0]\n+        expected_result = 1.0 - (3.0 / 32.0)\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_unweighted_flat_from_logits(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds,\n+            multi_label=False,\n+            from_logits=True,\n+        )\n+        result = auc_obj(self.y_true_good, self.y_pred_logits)\n+\n+        # tp = [4, 4, 1, 1, 0]\n+        # fp = [4, 1, 0, 0, 0]\n+        # fn = [0, 0, 3, 3, 4]\n+        # tn = [0, 3, 4, 4, 4]\n+\n+        # tpr = [1, 1, 0.25, 0.25, 0]\n+        # fpr = [1, 0.25, 0, 0, 0]\n+        expected_result = 1.0 - (3.0 / 32.0)\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_manual_thresholds(self):\n+        # Verify that when specified, thresholds are used instead of\n+        # num_thresholds.\n+        auc_obj = metrics.AUC(\n+            num_thresholds=2, thresholds=[0.5], multi_label=True\n+        )\n+        self.assertEqual(auc_obj.num_thresholds, 3)\n+        self.assertAllClose(auc_obj.thresholds, [0.0, 0.5, 1.0])\n+        result = auc_obj(self.y_true_good, self.y_pred)\n+\n+        # tp = [[2, 1, 0], [2, 0, 0]]\n+        # fp = [2, 0, 0], [2, 0, 0]]\n+        # fn = [[0, 1, 2], [0, 2, 2]]\n+        # tn = [[0, 2, 2], [0, 2, 2]]\n+\n+        # tpr = [[1, 0.5, 0], [1, 0, 0]]\n+        # fpr = [[1, 0, 0], [1, 0, 0]]\n+\n+        # auc by slice = [0.75, 0.5]\n+        expected_result = (0.75 + 0.5) / 2.0\n+\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_weighted_roc_interpolation(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, multi_label=True\n+        )\n+        result = auc_obj(\n+            self.y_true_good, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # tpr = [[1, 1,    0.57, 0.57, 0], [1, 1, 0, 0, 0]]\n+        # fpr = [[1, 0.67, 0,    0,    0], [1, 0, 0, 0, 0]]\n+        expected_result = 1.0 - 0.5 * 0.43 * 0.67\n+        self.assertAllClose(result, expected_result, 1e-1)\n+\n+    def test_pr_interpolation_unweighted(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, curve=\"PR\", multi_label=True\n+        )\n+        good_result = auc_obj(self.y_true_good, self.y_pred)\n+        with self.subTest(name=\"good\"):\n+            # PR AUCs are 0.917 and 1.0 respectively\n+            self.assertAllClose(good_result, (0.91667 + 1.0) / 2.0, 1e-1)\n+        bad_result = auc_obj(self.y_true_bad, self.y_pred)\n+        with self.subTest(name=\"bad\"):\n+            # PR AUCs are 0.917 and 0.5 respectively\n+            self.assertAllClose(bad_result, (0.91667 + 0.5) / 2.0, 1e-1)\n+\n+    def test_pr_interpolation(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, curve=\"PR\", multi_label=True\n+        )\n+        good_result = auc_obj(\n+            self.y_true_good, self.y_pred, sample_weight=self.sample_weight\n+        )\n+        # PR AUCs are 0.939 and 1.0 respectively\n+        self.assertAllClose(good_result, (0.939 + 1.0) / 2.0, 1e-1)\n+\n+    def test_keras_model_compiles(self):\n+        inputs = layers.Input(shape=(10,), batch_size=1)\n+        output = layers.Dense(3, activation=\"sigmoid\")(inputs)\n+        model = models.Model(inputs=inputs, outputs=output)\n+        model.compile(\n+            optimizer=\"adam\",\n+            loss=\"binary_crossentropy\",\n+            metrics=[metrics.AUC(multi_label=True)],\n+        )\n+\n+    def test_reset_state(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, multi_label=True\n+        )\n+        auc_obj(self.y_true_good, self.y_pred)\n+        auc_obj.reset_state()\n+        self.assertAllClose(auc_obj.true_positives, np.zeros((5, 2)))\n\n@@ -38,6 +38,59 @@ class ConfusionMatrix(Enum):\n     FALSE_NEGATIVES = \"fn\"\n \n \n+class AUCCurve(Enum):\n+    \"\"\"Type of AUC Curve (ROC or PR).\"\"\"\n+\n+    ROC = \"ROC\"\n+    PR = \"PR\"\n+\n+    @staticmethod\n+    def from_str(key):\n+        if key in (\"pr\", \"PR\"):\n+            return AUCCurve.PR\n+        elif key in (\"roc\", \"ROC\"):\n+            return AUCCurve.ROC\n+        else:\n+            raise ValueError(\n+                f'Invalid AUC curve value: \"{key}\". '\n+                'Expected values are [\"PR\", \"ROC\"]'\n+            )\n+\n+\n+class AUCSummationMethod(Enum):\n+    \"\"\"Type of AUC summation method.\n+\n+    https://en.wikipedia.org/wiki/Riemann_sum)\n+\n+    Contains the following values:\n+    * 'interpolation': Applies mid-point summation scheme for `ROC` curve. For\n+      `PR` curve, interpolates (true/false) positives but not the ratio that is\n+      precision (see Davis & Goadrich 2006 for details).\n+    * 'minoring': Applies left summation for increasing intervals and right\n+      summation for decreasing intervals.\n+    * 'majoring': Applies right summation for increasing intervals and left\n+      summation for decreasing intervals.\n+    \"\"\"\n+\n+    INTERPOLATION = \"interpolation\"\n+    MAJORING = \"majoring\"\n+    MINORING = \"minoring\"\n+\n+    @staticmethod\n+    def from_str(key):\n+        if key in (\"interpolation\", \"Interpolation\"):\n+            return AUCSummationMethod.INTERPOLATION\n+        elif key in (\"majoring\", \"Majoring\"):\n+            return AUCSummationMethod.MAJORING\n+        elif key in (\"minoring\", \"Minoring\"):\n+            return AUCSummationMethod.MINORING\n+        else:\n+            raise ValueError(\n+                f'Invalid AUC summation method value: \"{key}\". '\n+                'Expected values are [\"interpolation\", \"majoring\", \"minoring\"]'\n+            )\n+\n+\n def _update_confusion_matrix_variables_optimized(\n     variables_to_update,\n     y_true,\n@@ -203,14 +256,15 @@ def _update_confusion_matrix_variables_optimized(\n                 num_segments=num_thresholds,\n             )\n \n-        tp_bucket_v = ops.vectorized_map(\n-            gather_bucket, (true_labels, bucket_indices), warn=False\n+        tp_bucket_v = backend.vectorized_map(\n+            gather_bucket,\n+            (true_labels, bucket_indices),\n         )\n-        fp_bucket_v = ops.vectorized_map(\n-            gather_bucket, (false_labels, bucket_indices), warn=False\n+        fp_bucket_v = backend.vectorized_map(\n+            gather_bucket, (false_labels, bucket_indices)\n         )\n-        tp = ops.transpose(ops.cumsum(ops.flip(tp_bucket_v), axis=1))\n-        fp = ops.transpose(ops.cumsum(ops.flip(fp_bucket_v), axis=1))\n+        tp = ops.transpose(ops.flip(ops.cumsum(ops.flip(tp_bucket_v), axis=1)))\n+        fp = ops.transpose(ops.flip(ops.cumsum(ops.flip(fp_bucket_v), axis=1)))\n     else:\n         tp_bucket_v = ops.segment_sum(\n             data=true_labels,\n@@ -222,8 +276,8 @@ def _update_confusion_matrix_variables_optimized(\n             segment_ids=bucket_indices,\n             num_segments=num_thresholds,\n         )\n-        tp = ops.cumsum(ops.flip(tp_bucket_v))\n-        fp = ops.cumsum(ops.flip(fp_bucket_v))\n+        tp = ops.flip(ops.cumsum(ops.flip(tp_bucket_v)))\n+        fp = ops.flip(ops.cumsum(ops.flip(fp_bucket_v)))\n \n     # fn = sum(true_labels) - tp\n     # tn = sum(false_labels) - fp\n@@ -383,7 +437,6 @@ def update_confusion_matrix_variables(\n         one_thresh = ops.equal(\n             ops.cast(1, dtype=\"int32\"),\n             thresholds.ndim,\n-            name=\"one_set_of_thresholds_cond\",\n         )\n     else:\n         one_thresh = ops.cast(True, dtype=\"bool\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
