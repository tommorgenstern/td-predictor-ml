{"custom_id": "keras#ea47e6de279397cb1adf953ae971cf884587447d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 44 | Files Changed: 1 | Hunks: 49 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 106 | Churn Cumulative: 1678 | Contributors (this commit): 17 | Commits (past 90d): 8 | Contributors (cumulative): 17 | DMM Complexity: 0.5\n\nDIFF:\n@@ -11,6 +11,16 @@ def clip_norm(g, c, n):\n \n \n def optimizer_from_config(config, custom_objects=None):\n+    \"\"\"Instantiate an optimizer given a config dictionary.\n+\n+    # Arguments\n+        config: Config dictionary\n+            (e.g. output of `optimizer.get_config()`).\n+        custom_objects: Optional dictionary of custom optimizer classes.\n+\n+    # Returns\n+        An optimizer instance.\n+    \"\"\"\n     all_classes = {\n         'sgd': SGD,\n         'rmsprop': RMSprop,\n@@ -32,7 +42,7 @@ def optimizer_from_config(config, custom_objects=None):\n \n \n class Optimizer(object):\n-    '''Abstract optimizer base class.\n+    \"\"\"Abstract optimizer base class.\n \n     Note: this is the parent class of all optimizers, not an actual optimizer\n     that can be used for training models.\n@@ -43,7 +53,7 @@ class Optimizer(object):\n             when their L2 norm exceeds this value.\n         clipvalue: float >= 0. Gradients will be clipped\n             when their absolute value exceeds this value.\n-    '''\n+    \"\"\"\n     def __init__(self, **kwargs):\n         allowed_kwargs = {'clipnorm', 'clipvalue'}\n         for k in kwargs:\n@@ -67,7 +77,7 @@ class Optimizer(object):\n         return grads\n \n     def set_weights(self, weights):\n-        '''Sets the weights of the optimizer, from Numpy arrays.\n+        \"\"\"Sets the weights of the optimizer, from Numpy arrays.\n \n         Should only be called after computing the gradients\n         (otherwise the optimizer has no weights).\n@@ -78,7 +88,7 @@ class Optimizer(object):\n                 number of the dimensions of the weights\n                 of the optimizer (i.e. it should match the\n                 output of `get_weights`).\n-        '''\n+        \"\"\"\n         params = self.weights\n         weight_value_tuples = []\n         param_values = K.batch_get_value(params)\n@@ -92,9 +102,11 @@ class Optimizer(object):\n         K.batch_set_value(weight_value_tuples)\n \n     def get_weights(self):\n-        '''Returns the current weights of the optimizer,\n-        as a list of numpy arrays.\n-        '''\n+        \"\"\"Returns the current value of the weights of the optimizer.\n+\n+        # Returns:\n+            A list of numpy arrays.\n+        \"\"\"\n         return K.batch_get_value(self.weights)\n \n     def get_config(self):\n@@ -111,7 +123,7 @@ class Optimizer(object):\n \n \n class SGD(Optimizer):\n-    '''Stochastic gradient descent, with support for momentum,\n+    \"\"\"Stochastic gradient descent, with support for momentum,\n     learning rate decay, and Nesterov momentum.\n \n     # Arguments\n@@ -119,23 +131,23 @@ class SGD(Optimizer):\n         momentum: float >= 0. Parameter updates momentum.\n         decay: float >= 0. Learning rate decay over each update.\n         nesterov: boolean. Whether to apply Nesterov momentum.\n-    '''\n+    \"\"\"\n     def __init__(self, lr=0.01, momentum=0., decay=0.,\n                  nesterov=False, **kwargs):\n         super(SGD, self).__init__(**kwargs)\n-        self.__dict__.update(locals())\n         self.iterations = K.variable(0.)\n         self.lr = K.variable(lr)\n         self.momentum = K.variable(momentum)\n         self.decay = K.variable(decay)\n-        self.inital_decay = decay\n+        self.initial_decay = decay\n+        self.nesterov = nesterov\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n         self.updates = []\n \n         lr = self.lr\n-        if self.inital_decay > 0:\n+        if self.initial_decay > 0:\n             lr *= (1. / (1. + self.decay * self.iterations))\n             self.updates .append(K.update_add(self.iterations, 1))\n \n@@ -170,7 +182,7 @@ class SGD(Optimizer):\n \n \n class RMSprop(Optimizer):\n-    '''RMSProp optimizer.\n+    \"\"\"RMSProp optimizer.\n \n     It is recommended to leave the parameters of this optimizer\n     at their default values\n@@ -184,15 +196,15 @@ class RMSprop(Optimizer):\n         rho: float >= 0.\n         epsilon: float >= 0. Fuzz factor.\n         decay: float >= 0. Learning rate decay over each update.\n-    '''\n+    \"\"\"\n     def __init__(self, lr=0.001, rho=0.9, epsilon=1e-8, decay=0.,\n                  **kwargs):\n         super(RMSprop, self).__init__(**kwargs)\n-        self.__dict__.update(locals())\n         self.lr = K.variable(lr)\n         self.rho = K.variable(rho)\n+        self.epsilon = epsilon\n         self.decay = K.variable(decay)\n-        self.inital_decay = decay\n+        self.initial_decay = decay\n         self.iterations = K.variable(0.)\n \n     def get_updates(self, params, constraints, loss):\n@@ -203,7 +215,7 @@ class RMSprop(Optimizer):\n         self.updates = []\n \n         lr = self.lr\n-        if self.inital_decay > 0:\n+        if self.initial_decay > 0:\n             lr *= (1. / (1. + self.decay * self.iterations))\n             self.updates.append(K.update_add(self.iterations, 1))\n \n@@ -230,7 +242,7 @@ class RMSprop(Optimizer):\n \n \n class Adagrad(Optimizer):\n-    '''Adagrad optimizer.\n+    \"\"\"Adagrad optimizer.\n \n     It is recommended to leave the parameters of this optimizer\n     at their default values.\n@@ -241,13 +253,13 @@ class Adagrad(Optimizer):\n \n     # References\n         - [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n-    '''\n+    \"\"\"\n     def __init__(self, lr=0.01, epsilon=1e-8, decay=0., **kwargs):\n         super(Adagrad, self).__init__(**kwargs)\n-        self.__dict__.update(locals())\n         self.lr = K.variable(lr)\n+        self.epsilon = epsilon\n         self.decay = K.variable(decay)\n-        self.inital_decay = decay\n+        self.initial_decay = decay\n         self.iterations = K.variable(0.)\n \n     def get_updates(self, params, constraints, loss):\n@@ -258,7 +270,7 @@ class Adagrad(Optimizer):\n         self.updates = []\n \n         lr = self.lr\n-        if self.inital_decay > 0:\n+        if self.initial_decay > 0:\n             lr *= (1. / (1. + self.decay * self.iterations))\n             self.updates.append(K.update_add(self.iterations, 1))\n \n@@ -282,7 +294,7 @@ class Adagrad(Optimizer):\n \n \n class Adadelta(Optimizer):\n-    '''Adadelta optimizer.\n+    \"\"\"Adadelta optimizer.\n \n     It is recommended to leave the parameters of this optimizer\n     at their default values.\n@@ -295,14 +307,15 @@ class Adadelta(Optimizer):\n \n     # References\n         - [Adadelta - an adaptive learning rate method](http://arxiv.org/abs/1212.5701)\n-    '''\n+    \"\"\"\n     def __init__(self, lr=1.0, rho=0.95, epsilon=1e-8, decay=0.,\n                  **kwargs):\n         super(Adadelta, self).__init__(**kwargs)\n-        self.__dict__.update(locals())\n         self.lr = K.variable(lr)\n+        self.rho = rho\n+        self.epsilon = epsilon\n         self.decay = K.variable(decay)\n-        self.inital_decay = decay\n+        self.initial_decay = decay\n         self.iterations = K.variable(0.)\n \n     def get_updates(self, params, constraints, loss):\n@@ -314,7 +327,7 @@ class Adadelta(Optimizer):\n         self.updates = []\n \n         lr = self.lr\n-        if self.inital_decay > 0:\n+        if self.initial_decay > 0:\n             lr *= (1. / (1. + self.decay * self.iterations))\n             self.updates.append(K.update_add(self.iterations, 1))\n \n@@ -348,7 +361,7 @@ class Adadelta(Optimizer):\n \n \n class Adam(Optimizer):\n-    '''Adam optimizer.\n+    \"\"\"Adam optimizer.\n \n     Default parameters follow those provided in the original paper.\n \n@@ -359,28 +372,29 @@ class Adam(Optimizer):\n \n     # References\n         - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n-    '''\n+    \"\"\"\n     def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n                  epsilon=1e-8, decay=0., **kwargs):\n         super(Adam, self).__init__(**kwargs)\n-        self.__dict__.update(locals())\n         self.iterations = K.variable(0)\n         self.lr = K.variable(lr)\n         self.beta_1 = K.variable(beta_1)\n         self.beta_2 = K.variable(beta_2)\n+        self.epsilon = epsilon\n         self.decay = K.variable(decay)\n-        self.inital_decay = decay\n+        self.initial_decay = decay\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n         self.updates = [K.update_add(self.iterations, 1)]\n \n         lr = self.lr\n-        if self.inital_decay > 0:\n+        if self.initial_decay > 0:\n             lr *= (1. / (1. + self.decay * self.iterations))\n \n         t = self.iterations + 1\n-        lr_t = lr * K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t))\n+        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n+                     (1. - K.pow(self.beta_1, t)))\n \n         shapes = [K.get_variable_shape(p) for p in params]\n         ms = [K.zeros(shape) for shape in shapes]\n@@ -414,9 +428,9 @@ class Adam(Optimizer):\n \n \n class Adamax(Optimizer):\n-    '''Adamax optimizer from Adam paper's Section 7. It is a variant\n-     of Adam based on the infinity norm.\n+    \"\"\"Adamax optimizer from Adam paper's Section 7.\n \n+    It is a variant of Adam based on the infinity norm.\n     Default parameters follow those provided in the paper.\n \n     # Arguments\n@@ -426,24 +440,24 @@ class Adamax(Optimizer):\n \n     # References\n         - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n-    '''\n+    \"\"\"\n     def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n                  epsilon=1e-8, decay=0., **kwargs):\n         super(Adamax, self).__init__(**kwargs)\n-        self.__dict__.update(locals())\n         self.iterations = K.variable(0.)\n         self.lr = K.variable(lr)\n         self.beta_1 = K.variable(beta_1)\n         self.beta_2 = K.variable(beta_2)\n+        self.epsilon = epsilon\n         self.decay = K.variable(decay)\n-        self.inital_decay = decay\n+        self.initial_decay = decay\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n         self.updates = [K.update_add(self.iterations, 1)]\n \n         lr = self.lr\n-        if self.inital_decay > 0:\n+        if self.initial_decay > 0:\n             lr *= (1. / (1. + self.decay * self.iterations))\n \n         t = self.iterations + 1\n@@ -484,8 +498,9 @@ class Adamax(Optimizer):\n \n \n class Nadam(Optimizer):\n-    '''\n-    Nesterov Adam optimizer: Much like Adam is essentially RMSprop with momentum,\n+    \"\"\"Nesterov Adam optimizer.\n+\n+    Much like Adam is essentially RMSprop with momentum,\n     Nadam is Adam RMSprop with Nesterov momentum.\n \n     Default parameters follow those provided in the paper.\n@@ -500,16 +515,16 @@ class Nadam(Optimizer):\n     # References\n         - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n         - [On the importance of initialization and momentum in deep learning](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n-    '''\n+    \"\"\"\n     def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n                  epsilon=1e-8, schedule_decay=0.004, **kwargs):\n         super(Nadam, self).__init__(**kwargs)\n-        self.__dict__.update(locals())\n         self.iterations = K.variable(0.)\n         self.m_schedule = K.variable(1.)\n         self.lr = K.variable(lr)\n         self.beta_1 = K.variable(beta_1)\n         self.beta_2 = K.variable(beta_2)\n+        self.epsilon = epsilon\n         self.schedule_decay = schedule_decay\n \n     def get_updates(self, params, constraints, loss):\n@@ -564,6 +579,8 @@ class Nadam(Optimizer):\n \n \n class TFOptimizer(Optimizer):\n+    \"\"\"Wrapper class for native TensorFlow optimizers.\n+    \"\"\"\n \n     def __init__(self, optimizer):\n         self.optimizer = optimizer\n@@ -593,7 +610,8 @@ class TFOptimizer(Optimizer):\n         raise NotImplementedError\n \n \n-# aliases\n+# Aliases.\n+\n sgd = SGD\n rmsprop = RMSprop\n adagrad = Adagrad\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8f7574437951a8f1042407d25bffb57292e3b6a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 848 | Lines Deleted: 798 | Files Changed: 39 | Hunks: 720 | Methods Changed: 293 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1646 | Churn Cumulative: 57086 | Contributors (this commit): 238 | Commits (past 90d): 287 | Contributors (cumulative): 544 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,8 +25,8 @@ def librosa_exists():\n \n \n def preprocess_input(audio_path, dim_ordering='default'):\n-    '''Reads an audio file and outputs a Mel-spectrogram.\n-    '''\n+    \"\"\"Reads an audio file and outputs a Mel-spectrogram.\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = K.image_dim_ordering()\n     assert dim_ordering in {'tf', 'th'}\n@@ -70,13 +70,13 @@ def preprocess_input(audio_path, dim_ordering='default'):\n \n \n def decode_predictions(preds, top_n=5):\n-    '''Decode the output of a music tagger model.\n+    \"\"\"Decode the output of a music tagger model.\n \n     # Arguments\n         preds: 2-dimensional numpy array\n         top_n: integer in [0, 50], number of items to show\n \n-    '''\n+    \"\"\"\n     assert len(preds.shape) == 2 and preds.shape[1] == 50\n     results = []\n     for pred in preds:\n\n@@ -1,5 +1,5 @@\n # -*- coding: utf-8 -*-\n-'''Inception V3 model for Keras.\n+\"\"\"Inception V3 model for Keras.\n \n Note that the ImageNet weights provided are from a model that had not fully converged.\n Inception v3 should be able to reach 6.9% top-5 error, but our model\n@@ -10,11 +10,11 @@ Also, do note that the input image format for this model is different than for\n the VGG16 and ResNet models (299x299 instead of 224x224), and that the input preprocessing function\n is also different (same as Xception).\n \n-# Reference:\n+# Reference\n \n - [Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567)\n \n-'''\n+\"\"\"\n from __future__ import print_function\n from __future__ import absolute_import\n \n@@ -39,8 +39,8 @@ TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/relea\n def conv2d_bn(x, nb_filter, nb_row, nb_col,\n               border_mode='same', subsample=(1, 1),\n               name=None):\n-    '''Utility function to apply conv + BN.\n-    '''\n+    \"\"\"Utility function to apply conv + BN.\n+    \"\"\"\n     if name is not None:\n         bn_name = name + '_bn'\n         conv_name = name + '_conv'\n@@ -62,7 +62,7 @@ def conv2d_bn(x, nb_filter, nb_row, nb_col,\n \n def InceptionV3(include_top=True, weights='imagenet',\n                 input_tensor=None, input_shape=None):\n-    '''Instantiate the Inception v3 architecture,\n+    \"\"\"Instantiate the Inception v3 architecture,\n     optionally loading weights pre-trained\n     on ImageNet. Note that when using TensorFlow,\n     for best performance you should set\n@@ -93,7 +93,7 @@ def InceptionV3(include_top=True, weights='imagenet',\n \n     # Returns\n         A Keras model instance.\n-    '''\n+    \"\"\"\n     if weights not in {'imagenet', None}:\n         raise ValueError('The `weights` argument should be either '\n                          '`None` (random initialization) or `imagenet` '\n\n@@ -1,11 +1,11 @@\n # -*- coding: utf-8 -*-\n-'''MusicTaggerCRNN model for Keras.\n+\"\"\"MusicTaggerCRNN model for Keras.\n \n # Reference:\n \n - [Music-auto_tagging-keras](https://github.com/keunwoochoi/music-auto_tagging-keras)\n \n-'''\n+\"\"\"\n from __future__ import print_function\n from __future__ import absolute_import\n \n@@ -29,7 +29,7 @@ TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/dow\n \n def MusicTaggerCRNN(weights='msd', input_tensor=None,\n                     include_top=True):\n-    '''Instantiate the MusicTaggerCRNN architecture,\n+    \"\"\"Instantiate the MusicTaggerCRNN architecture,\n     optionally loading weights pre-trained\n     on Million Song Dataset. Note that when using TensorFlow,\n     for best performance you should set\n@@ -58,7 +58,7 @@ def MusicTaggerCRNN(weights='msd', input_tensor=None,\n \n     # Returns\n         A Keras model instance.\n-    '''\n+    \"\"\"\n     if weights not in {'msd', None}:\n         raise ValueError('The `weights` argument should be either '\n                          '`None` (random initialization) or `msd` '\n\n@@ -1,12 +1,12 @@\n # -*- coding: utf-8 -*-\n-'''ResNet50 model for Keras.\n+\"\"\"ResNet50 model for Keras.\n \n # Reference:\n \n - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n \n Adapted from code contributed by BigMoyan.\n-'''\n+\"\"\"\n from __future__ import print_function\n from __future__ import absolute_import\n \n@@ -31,7 +31,7 @@ TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/relea\n \n \n def identity_block(input_tensor, kernel_size, filters, stage, block):\n-    '''The identity_block is the block that has no conv layer at shortcut\n+    \"\"\"The identity_block is the block that has no conv layer at shortcut\n \n     # Arguments\n         input_tensor: input tensor\n@@ -39,7 +39,7 @@ def identity_block(input_tensor, kernel_size, filters, stage, block):\n         filters: list of integers, the nb_filters of 3 conv layer at main path\n         stage: integer, current stage label, used for generating layer names\n         block: 'a','b'..., current block label, used for generating layer names\n-    '''\n+    \"\"\"\n     nb_filter1, nb_filter2, nb_filter3 = filters\n     if K.image_dim_ordering() == 'tf':\n         bn_axis = 3\n@@ -66,7 +66,7 @@ def identity_block(input_tensor, kernel_size, filters, stage, block):\n \n \n def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n-    '''conv_block is the block that has a conv layer at shortcut\n+    \"\"\"conv_block is the block that has a conv layer at shortcut\n \n     # Arguments\n         input_tensor: input tensor\n@@ -77,7 +77,7 @@ def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2))\n \n     Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n     And the shortcut should have subsample=(2,2) as well\n-    '''\n+    \"\"\"\n     nb_filter1, nb_filter2, nb_filter3 = filters\n     if K.image_dim_ordering() == 'tf':\n         bn_axis = 3\n@@ -110,7 +110,7 @@ def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2))\n \n def ResNet50(include_top=True, weights='imagenet',\n              input_tensor=None, input_shape=None):\n-    '''Instantiate the ResNet50 architecture,\n+    \"\"\"Instantiate the ResNet50 architecture,\n     optionally loading weights pre-trained\n     on ImageNet. Note that when using TensorFlow,\n     for best performance you should set\n@@ -139,7 +139,7 @@ def ResNet50(include_top=True, weights='imagenet',\n \n     # Returns\n         A Keras model instance.\n-    '''\n+    \"\"\"\n     if weights not in {'imagenet', None}:\n         raise ValueError('The `weights` argument should be either '\n                          '`None` (random initialization) or `imagenet` '\n\n@@ -1,11 +1,11 @@\n # -*- coding: utf-8 -*-\n-'''VGG16 model for Keras.\n+\"\"\"VGG16 model for Keras.\n \n-# Reference:\n+# Reference\n \n - [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n \n-'''\n+\"\"\"\n from __future__ import print_function\n from __future__ import absolute_import\n \n@@ -29,7 +29,7 @@ TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/relea\n \n def VGG16(include_top=True, weights='imagenet',\n           input_tensor=None, input_shape=None):\n-    '''Instantiate the VGG16 architecture,\n+    \"\"\"Instantiate the VGG16 architecture,\n     optionally loading weights pre-trained\n     on ImageNet. Note that when using TensorFlow,\n     for best performance you should set\n@@ -58,7 +58,7 @@ def VGG16(include_top=True, weights='imagenet',\n \n     # Returns\n         A Keras model instance.\n-    '''\n+    \"\"\"\n     if weights not in {'imagenet', None}:\n         raise ValueError('The `weights` argument should be either '\n                          '`None` (random initialization) or `imagenet` '\n\n@@ -1,11 +1,11 @@\n # -*- coding: utf-8 -*-\n-'''VGG19 model for Keras.\n+\"\"\"VGG19 model for Keras.\n \n-# Reference:\n+# Reference\n \n - [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n \n-'''\n+\"\"\"\n from __future__ import print_function\n from __future__ import absolute_import\n \n@@ -29,7 +29,7 @@ TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/relea\n \n def VGG19(include_top=True, weights='imagenet',\n           input_tensor=None, input_shape=None):\n-    '''Instantiate the VGG19 architecture,\n+    \"\"\"Instantiate the VGG19 architecture,\n     optionally loading weights pre-trained\n     on ImageNet. Note that when using TensorFlow,\n     for best performance you should set\n@@ -58,7 +58,7 @@ def VGG19(include_top=True, weights='imagenet',\n \n     # Returns\n         A Keras model instance.\n-    '''\n+    \"\"\"\n     if weights not in {'imagenet', None}:\n         raise ValueError('The `weights` argument should be either '\n                          '`None` (random initialization) or `imagenet` '\n\n@@ -1,5 +1,5 @@\n # -*- coding: utf-8 -*-\n-'''Xception V1 model for Keras.\n+\"\"\"Xception V1 model for Keras.\n \n On ImageNet, this model gets to a top-1 validation accuracy of 0.790\n and a top-5 validation accuracy of 0.945.\n@@ -12,11 +12,11 @@ is also different (same as Inception V3).\n Also do note that this model is only available for the TensorFlow backend,\n due to its reliance on `SeparableConvolution` layers.\n \n-# Reference:\n+# Reference\n \n - [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)\n \n-'''\n+\"\"\"\n from __future__ import print_function\n from __future__ import absolute_import\n \n@@ -37,7 +37,7 @@ TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/relea\n \n def Xception(include_top=True, weights='imagenet',\n              input_tensor=None, input_shape=None):\n-    '''Instantiate the Xception architecture,\n+    \"\"\"Instantiate the Xception architecture,\n     optionally loading weights pre-trained\n     on ImageNet. This model is available for TensorFlow only,\n     and can only be used with inputs following the TensorFlow\n@@ -63,7 +63,7 @@ def Xception(include_top=True, weights='imagenet',\n \n     # Returns\n         A Keras model instance.\n-    '''\n+    \"\"\"\n     if weights not in {'imagenet', None}:\n         raise ValueError('The `weights` argument should be either '\n                          '`None` (random initialization) or `imagenet` '\n\n@@ -70,7 +70,7 @@ else:\n \n \n def backend():\n-    '''Publicly accessible method\n+    \"\"\"Publicly accessible method\n     for determining the current backend.\n-    '''\n+    \"\"\"\n     return _BACKEND\n\n@@ -11,7 +11,7 @@ _LEGACY_WEIGHT_ORDERING = False\n \n \n def epsilon():\n-    '''Returns the value of the fuzz\n+    \"\"\"Returns the value of the fuzz\n     factor used in numeric expressions.\n \n     # Returns\n@@ -22,12 +22,12 @@ def epsilon():\n         >>> keras.backend.epsilon()\n         1e-08\n     ```\n-    '''\n+    \"\"\"\n     return _EPSILON\n \n \n def set_epsilon(e):\n-    '''Sets the value of the fuzz\n+    \"\"\"Sets the value of the fuzz\n     factor used in numeric expressions.\n \n     # Arguments\n@@ -42,13 +42,13 @@ def set_epsilon(e):\n         >>> K.epsilon()\n         1e-05\n     ```\n-    '''\n+    \"\"\"\n     global _EPSILON\n     _EPSILON = e\n \n \n def floatx():\n-    '''Returns the default float type, as a string\n+    \"\"\"Returns the default float type, as a string\n     (e.g. 'float16', 'float32', 'float64').\n \n     # Returns\n@@ -59,12 +59,12 @@ def floatx():\n         >>> keras.backend.floatx()\n         'float32'\n     ```\n-    '''\n+    \"\"\"\n     return _FLOATX\n \n \n def set_floatx(floatx):\n-    '''Sets the default float type.\n+    \"\"\"Sets the default float type.\n \n     # Arguments\n         String: 'float16', 'float32', or 'float64'.\n@@ -78,7 +78,7 @@ def set_floatx(floatx):\n         >>> K.floatx()\n         'float16'\n     ```\n-    '''\n+    \"\"\"\n     global _FLOATX\n     if floatx not in {'float16', 'float32', 'float64'}:\n         raise ValueError('Unknown floatx type: ' + str(floatx))\n@@ -86,7 +86,7 @@ def set_floatx(floatx):\n \n \n def cast_to_floatx(x):\n-    '''Cast a Numpy array to the default Keras float type.\n+    \"\"\"Cast a Numpy array to the default Keras float type.\n \n     # Arguments\n         x: Numpy array.\n@@ -108,12 +108,12 @@ def cast_to_floatx(x):\n         >>> new_arr.dtype\n         dtype('float32')\n     ```\n-    '''\n+    \"\"\"\n     return np.asarray(x, dtype=_FLOATX)\n \n \n def image_dim_ordering():\n-    '''Returns the default image dimension ordering\n+    \"\"\"Returns the default image dimension ordering\n     convention ('th' or 'tf').\n \n     # Returns\n@@ -124,12 +124,12 @@ def image_dim_ordering():\n         >>> keras.backend.image_dim_ordering()\n         'th'\n     ```\n-    '''\n+    \"\"\"\n     return _IMAGE_DIM_ORDERING\n \n \n def set_image_dim_ordering(dim_ordering):\n-    '''Sets the value of the image dimension\n+    \"\"\"Sets the value of the image dimension\n     ordering convention ('th' or 'tf').\n \n     # Arguments\n@@ -144,7 +144,7 @@ def set_image_dim_ordering(dim_ordering):\n         >>> K.image_dim_ordering()\n         'tf'\n     ```\n-    '''\n+    \"\"\"\n     global _IMAGE_DIM_ORDERING\n     if dim_ordering not in {'tf', 'th'}:\n         raise ValueError('Unknown dim_ordering:', dim_ordering)\n@@ -152,7 +152,7 @@ def set_image_dim_ordering(dim_ordering):\n \n \n def get_uid(prefix=''):\n-    '''Provides a unique UID given a string prefix.\n+    \"\"\"Provides a unique UID given a string prefix.\n \n     # Arguments\n         prefix: string.\n@@ -168,7 +168,7 @@ def get_uid(prefix=''):\n         >>> 2\n     ```\n \n-    '''\n+    \"\"\"\n     _UID_PREFIXES[prefix] += 1\n     return _UID_PREFIXES[prefix]\n \n@@ -179,7 +179,7 @@ def reset_uids():\n \n \n def is_keras_tensor(x):\n-    '''Returns whether `x` is a Keras tensor.\n+    \"\"\"Returns whether `x` is a Keras tensor.\n \n     # Arguments\n         x: a potential tensor.\n@@ -200,7 +200,7 @@ def is_keras_tensor(x):\n         >>> K.is_keras_tensor(keras_placeholder)  # A placeholder is a Tensor.\n         True\n     ```\n-    '''\n+    \"\"\"\n     if hasattr(x, '_keras_shape'):\n         return True\n     else:\n\n@@ -31,10 +31,10 @@ _MANUAL_VAR_INIT = False\n \n \n def clear_session():\n-    '''Destroys the current TF graph and creates a new one.\n+    \"\"\"Destroys the current TF graph and creates a new one.\n \n     Useful to avoid clutter from old models / layers.\n-    '''\n+    \"\"\"\n     global _SESSION\n     global _GRAPH_LEARNING_PHASES\n     tf.reset_default_graph()\n@@ -45,7 +45,7 @@ def clear_session():\n \n \n def manual_variable_initialization(value):\n-    '''Sets the manual variable initialization flag.\n+    \"\"\"Sets the manual variable initialization flag.\n \n     This boolean flag determines whether\n     variables should be initialized\n@@ -55,18 +55,18 @@ def manual_variable_initialization(value):\n \n     # Arguments\n         value: Python boolean.\n-    '''\n+    \"\"\"\n     global _MANUAL_VAR_INIT\n     _MANUAL_VAR_INIT = value\n \n \n def learning_phase():\n-    '''Returns the learning phase flag.\n+    \"\"\"Returns the learning phase flag.\n \n     The learning phase flag is a bool tensor (0 = test, 1 = train)\n     to be passed as input to any Keras function\n     that uses a different behavior at train time and test time.\n-    '''\n+    \"\"\"\n     graph = tf.get_default_graph()\n     if graph not in _GRAPH_LEARNING_PHASES:\n         phase = tf.placeholder(dtype='bool',\n@@ -76,9 +76,9 @@ def learning_phase():\n \n \n def set_learning_phase(value):\n-    '''Sets the learning phase to a fixed value,\n+    \"\"\"Sets the learning phase to a fixed value,\n     either 0 or 1 (integers).\n-    '''\n+    \"\"\"\n     global _GRAPH_LEARNING_PHASES\n     if value not in {0, 1}:\n         raise ValueError('Expected learning phase to be '\n@@ -87,7 +87,7 @@ def set_learning_phase(value):\n \n \n def get_session():\n-    '''Returns the TF session to be used by the backend.\n+    \"\"\"Returns the TF session to be used by the backend.\n \n     If a default TensorFlow session is available, we will return it.\n \n@@ -101,7 +101,7 @@ def get_session():\n \n     # Returns\n         A TensorFlow session.\n-    '''\n+    \"\"\"\n     global _SESSION\n     if tf.get_default_session() is not None:\n         session = tf.get_default_session()\n@@ -121,8 +121,8 @@ def get_session():\n \n \n def set_session(session):\n-    '''Sets the global TF session.\n-    '''\n+    \"\"\"Sets the global TF session.\n+    \"\"\"\n     global _SESSION\n     _SESSION = session\n \n@@ -158,7 +158,7 @@ def _to_tensor(x, dtype):\n \n \n def is_sparse(tensor):\n-    '''Returns whether a tensor is a sparse tensor.\n+    \"\"\"Returns whether a tensor is a sparse tensor.\n \n     # Arguments\n         tensor: A tensor instance.\n@@ -176,12 +176,12 @@ def is_sparse(tensor):\n         >>> print(K.is_sparse(b))\n         True\n     ```\n-    '''\n+    \"\"\"\n     return isinstance(tensor, tf.SparseTensor)\n \n \n def to_dense(tensor):\n-    '''Converts a sparse tensor into a dense tensor\n+    \"\"\"Converts a sparse tensor into a dense tensor\n     and returns it.\n \n     # Arguments\n@@ -200,7 +200,7 @@ def to_dense(tensor):\n         >>> print(K.is_sparse(c))\n         False\n     ```\n-    '''\n+    \"\"\"\n     if is_sparse(tensor):\n         return tf.sparse_tensor_to_dense(tensor)\n     else:\n@@ -208,7 +208,7 @@ def to_dense(tensor):\n \n \n def variable(value, dtype=None, name=None):\n-    '''Instantiates a variable and returns it.\n+    \"\"\"Instantiates a variable and returns it.\n \n     # Arguments\n         value: Numpy array, initial value of the tensor.\n@@ -231,7 +231,7 @@ def variable(value, dtype=None, name=None):\n         array([[ 1.,  2.],\n                [ 3.,  4.]])\n     ```\n-    '''\n+    \"\"\"\n     if dtype is None:\n         dtype = floatx()\n     if hasattr(value, 'tocoo'):\n@@ -274,7 +274,7 @@ def _initialize_variables():\n \n \n def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n-    '''Instantiates a placeholder tensor and returns it.\n+    \"\"\"Instantiates a placeholder tensor and returns it.\n \n     # Arguments\n         shape: Shape of the placeholder\n@@ -297,7 +297,7 @@ def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n         >>> input_ph\n         <tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32>\n     ```\n-    '''\n+    \"\"\"\n     if dtype is None:\n         dtype = floatx()\n     if not shape:\n@@ -314,7 +314,7 @@ def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n \n \n def shape(x):\n-    '''Returns the symbolic shape of a tensor or variable.\n+    \"\"\"Returns the symbolic shape of a tensor or variable.\n \n     # Arguments\n         x: A tensor or variable.\n@@ -340,12 +340,12 @@ def shape(x):\n         >>> K.shape(input).eval(session=tf_session)\n         array([2, 4, 5], dtype=int32)\n     ```\n-    '''\n+    \"\"\"\n     return tf.shape(x)\n \n \n def int_shape(x):\n-    '''Returns the shape of a Keras tensor or a Keras variable as a tuple of\n+    \"\"\"Returns the shape of a Keras tensor or a Keras variable as a tuple of\n     integers or None entries.\n \n     # Arguments\n@@ -365,13 +365,13 @@ def int_shape(x):\n         >>> K.int_shape(kvar)\n         (2, 2)\n     ```\n-    '''\n+    \"\"\"\n     shape = x.get_shape()\n     return tuple([i.__int__() for i in shape])\n \n \n def ndim(x):\n-    '''Returns the number of axes in a tensor, as an integer.\n+    \"\"\"Returns the number of axes in a tensor, as an integer.\n \n     # Arguments\n         x: Tensor or variable.\n@@ -390,7 +390,7 @@ def ndim(x):\n         >>> K.ndim(kvar)\n         2\n     ```\n-    '''\n+    \"\"\"\n     if is_sparse(x):\n         return x._dims\n \n@@ -401,7 +401,7 @@ def ndim(x):\n \n \n def dtype(x):\n-    '''Returns the dtype of a Keras tensor or variable, as a string.\n+    \"\"\"Returns the dtype of a Keras tensor or variable, as a string.\n \n     # Arguments\n         x: Tensor or variable.\n@@ -426,12 +426,12 @@ def dtype(x):\n         >>> K.dtype(kvar)\n         'float32_ref'\n     ```\n-    '''\n+    \"\"\"\n     return x.dtype.name\n \n \n def eval(x):\n-    '''Evaluates the value of a variable.\n+    \"\"\"Evaluates the value of a variable.\n     Returns a Numpy array.\n \n     # Arguments\n@@ -448,12 +448,12 @@ def eval(x):\n         array([[ 1.,  2.],\n                [ 3.,  4.]], dtype=float32)\n     ```\n-    '''\n+    \"\"\"\n     return to_dense(x).eval(session=get_session())\n \n \n def zeros(shape, dtype=None, name=None):\n-    '''Instantiates an all-zeros variable and returns it.\n+    \"\"\"Instantiates an all-zeros variable and returns it.\n \n     # Arguments\n         shape: Tuple of integers, shape of returned Keras variable\n@@ -472,7 +472,7 @@ def zeros(shape, dtype=None, name=None):\n                [ 0.,  0.,  0.,  0.],\n                [ 0.,  0.,  0.,  0.]], dtype=float32)\n     ```\n-    '''\n+    \"\"\"\n     if dtype is None:\n         dtype = floatx()\n     shape = tuple(map(int, shape))\n@@ -482,7 +482,7 @@ def zeros(shape, dtype=None, name=None):\n \n \n def ones(shape, dtype=None, name=None):\n-    '''Instantiates an all-ones tensor variable and returns it.\n+    \"\"\"Instantiates an all-ones tensor variable and returns it.\n \n     # Arguments\n         shape: Tuple of integers, shape of returned Keras variable.\n@@ -501,7 +501,7 @@ def ones(shape, dtype=None, name=None):\n                [ 1.,  1.,  1.,  1.],\n                [ 1.,  1.,  1.,  1.]], dtype=float32)\n     ```\n-    '''\n+    \"\"\"\n     if dtype is None:\n         dtype = floatx()\n     shape = tuple(map(int, shape))\n@@ -511,7 +511,7 @@ def ones(shape, dtype=None, name=None):\n \n \n def eye(size, dtype=None, name=None):\n-    '''Instantiate an identity matrix and returns it.\n+    \"\"\"Instantiate an identity matrix and returns it.\n \n     # Arguments\n         size: Integer, number of rows/columns.\n@@ -531,12 +531,12 @@ def eye(size, dtype=None, name=None):\n                [ 0.,  0.,  1.]], dtype=float32)\n     ```\n \n-    '''\n+    \"\"\"\n     return variable(np.eye(size), dtype, name)\n \n \n def zeros_like(x, name=None):\n-    '''Instantiates an all-zeros Keras variable\n+    \"\"\"Instantiates an all-zeros Keras variable\n     of the same shape as another Keras variable or tensor and returns it.\n \n     # Arguments\n@@ -554,12 +554,12 @@ def zeros_like(x, name=None):\n         array([[ 0.,  0.,  0.],\n                [ 0.,  0.,  0.]], dtype=float32)\n     ```\n-    '''\n+    \"\"\"\n     return tf.zeros_like(x, name=name)\n \n \n def ones_like(x, name=None):\n-    '''Instantiates an all-ones Keras variable\n+    \"\"\"Instantiates an all-ones Keras variable\n     of the same shape as another Keras variable or tensor and returns it.\n \n     # Arguments\n@@ -577,13 +577,13 @@ def ones_like(x, name=None):\n         array([[ 1.,  1.,  1.],\n                [ 1.,  1.,  1.]], dtype=float32)\n     ```\n-    '''\n+    \"\"\"\n     return tf.ones_like(x, name=name)\n \n \n def random_uniform_variable(shape, low, high, dtype=None,\n                             name=None, seed=None):\n-    '''Instantiates an Keras variable filled with\n+    \"\"\"Instantiates an Keras variable filled with\n     samples drawn from a uniform distribution and returns it.\n \n     # Arguments\n@@ -607,7 +607,7 @@ def random_uniform_variable(shape, low, high, dtype=None,\n         array([[ 0.10940075,  0.10047495,  0.476143  ],\n                [ 0.66137183,  0.00869417,  0.89220798]], dtype=float32)\n     ```\n-    '''\n+    \"\"\"\n     if dtype is None:\n         dtype = floatx()\n     shape = tuple(map(int, shape))\n@@ -622,7 +622,7 @@ def random_uniform_variable(shape, low, high, dtype=None,\n \n def random_normal_variable(shape, mean, scale, dtype=None,\n                            name=None, seed=None):\n-    '''Instantiates an Keras variable filled with\n+    \"\"\"Instantiates an Keras variable filled with\n     samples drawn from a normal distribution and returns it.\n \n     # Arguments\n@@ -646,7 +646,7 @@ def random_normal_variable(shape, mean, scale, dtype=None,\n         array([[ 1.19591331,  0.68685907, -0.63814116],\n                [ 0.92629528,  0.28055015,  1.70484698]], dtype=float32)\n     ```\n-    '''\n+    \"\"\"\n     if dtype is None:\n         dtype = floatx()\n     shape = tuple(map(int, shape))\n@@ -660,7 +660,7 @@ def random_normal_variable(shape, mean, scale, dtype=None,\n \n \n def count_params(x):\n-    '''Returns the number of scalars in a Keras variable.\n+    \"\"\"Returns the number of scalars in a Keras variable.\n \n     # Arguments\n         x: Keras variable.\n@@ -677,13 +677,13 @@ def count_params(x):\n         array([[ 0.,  0.,  0.],\n                [ 0.,  0.,  0.]], dtype=float32)\n     ```\n-    '''\n+    \"\"\"\n     shape = x.get_shape()\n     return np.prod([shape[i]._value for i in range(len(shape))])\n \n \n def cast(x, dtype):\n-    '''Casts a tensor to a different dtype and returns it.\n+    \"\"\"Casts a tensor to a different dtype and returns it.\n \n     You can cast a Keras variable but it still returns a Keras tensor.\n \n@@ -709,7 +709,7 @@ def cast(x, dtype):\n         >>> input = K.cast(input, dtype='float16')\n         >>> input\n         <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>    ```\n-    '''\n+    \"\"\"\n     return tf.cast(x, dtype)\n \n \n@@ -740,7 +740,7 @@ def moving_average_update(variable, value, momentum):\n # LINEAR ALGEBRA\n \n def dot(x, y):\n-    '''Multiplies 2 tensors (and/or variables) and returns a *tensor*.\n+    \"\"\"Multiplies 2 tensors (and/or variables) and returns a *tensor*.\n     When attempting to multiply a ND tensor\n     with a ND tensor, it reproduces the Theano behavior.\n     (e.g. (2, 3).(4, 3, 5) = (2, 4, 5))\n@@ -779,7 +779,7 @@ def dot(x, y):\n         >>> K.int_shape(xy)\n         (2, 4, 5)\n     ```\n-    '''\n+    \"\"\"\n     if ndim(x) is not None and (ndim(x) > 2 or ndim(y) > 2):\n         x_shape = []\n         for i, s in zip(int_shape(x), tf.unpack(tf.shape(x))):\n@@ -809,7 +809,7 @@ def dot(x, y):\n \n \n def batch_dot(x, y, axes=None):\n-    '''Batchwise dot product.\n+    \"\"\"Batchwise dot product.\n \n     `batch_dot` is used to compute dot product of `x` and `y` when\n     `x` and `y` are data in batch, i.e. in a shape of\n@@ -858,7 +858,7 @@ def batch_dot(x, y, axes=None):\n         >>> K.int_shape(xy_batch_dot)\n         (32, 1, 30)\n     ```\n-    '''\n+    \"\"\"\n     if isinstance(axes, int):\n         axes = (axes, axes)\n     if axes is not None:\n@@ -877,7 +877,7 @@ def batch_dot(x, y, axes=None):\n \n \n def transpose(x):\n-    '''Transposes a tensor and returns it.\n+    \"\"\"Transposes a tensor and returns it.\n \n     # Arguments\n         x: Tensor or variable.\n@@ -907,12 +907,12 @@ def transpose(x):\n         <tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32>\n \n     ```\n-    '''\n+    \"\"\"\n     return tf.transpose(x)\n \n \n def gather(reference, indices):\n-    '''Retrieves the elements of indices `indices`\n+    \"\"\"Retrieves the elements of indices `indices`\n     in the tensor `reference`.\n \n     # Arguments\n@@ -921,7 +921,7 @@ def gather(reference, indices):\n \n     # Returns\n         A tensor of same type as `reference`.\n-    '''\n+    \"\"\"\n     return tf.gather(reference, indices)\n \n \n@@ -941,36 +941,36 @@ def _normalize_axis(axis, ndim):\n \n \n def max(x, axis=None, keepdims=False):\n-    '''Maximum value in a tensor.\n-    '''\n+    \"\"\"Maximum value in a tensor.\n+    \"\"\"\n     axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_max(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n def min(x, axis=None, keepdims=False):\n-    '''Minimum value in a tensor.\n-    '''\n+    \"\"\"Minimum value in a tensor.\n+    \"\"\"\n     axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_min(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n def sum(x, axis=None, keepdims=False):\n-    '''Sum of the values in a tensor, alongside the specified axis.\n-    '''\n+    \"\"\"Sum of the values in a tensor, alongside the specified axis.\n+    \"\"\"\n     axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_sum(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n def prod(x, axis=None, keepdims=False):\n-    '''Multiplies the values in a tensor, alongside the specified axis.\n-    '''\n+    \"\"\"Multiplies the values in a tensor, alongside the specified axis.\n+    \"\"\"\n     axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_prod(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n def var(x, axis=None, keepdims=False):\n-    '''Variance of a tensor, alongside the specified axis.\n-    '''\n+    \"\"\"Variance of a tensor, alongside the specified axis.\n+    \"\"\"\n     axis = _normalize_axis(axis, ndim(x))\n     if x.dtype.base_dtype == tf.bool:\n         x = tf.cast(x, floatx())\n@@ -982,14 +982,14 @@ def var(x, axis=None, keepdims=False):\n \n \n def std(x, axis=None, keepdims=False):\n-    '''Standard deviation of a tensor, alongside the specified axis.\n-    '''\n+    \"\"\"Standard deviation of a tensor, alongside the specified axis.\n+    \"\"\"\n     return tf.sqrt(var(x, axis=axis, keepdims=keepdims))\n \n \n def mean(x, axis=None, keepdims=False):\n-    '''Mean of a tensor, alongside the specified axis.\n-    '''\n+    \"\"\"Mean of a tensor, alongside the specified axis.\n+    \"\"\"\n     axis = _normalize_axis(axis, ndim(x))\n     if x.dtype.base_dtype == tf.bool:\n         x = tf.cast(x, floatx())\n@@ -997,10 +997,10 @@ def mean(x, axis=None, keepdims=False):\n \n \n def any(x, axis=None, keepdims=False):\n-    '''Bitwise reduction (logical OR).\n+    \"\"\"Bitwise reduction (logical OR).\n \n     Returns an uint8 tensor (0s and 1s).\n-    '''\n+    \"\"\"\n     axis = _normalize_axis(axis, ndim(x))\n     x = tf.cast(x, tf.bool)\n     x = tf.reduce_any(x, reduction_indices=axis, keep_dims=keepdims)\n@@ -1008,10 +1008,10 @@ def any(x, axis=None, keepdims=False):\n \n \n def all(x, axis=None, keepdims=False):\n-    '''Bitwise reduction (logical AND).\n+    \"\"\"Bitwise reduction (logical AND).\n \n     Returns an uint8 tensor\n-    '''\n+    \"\"\"\n     axis = _normalize_axis(axis, ndim(x))\n     x = tf.cast(x, tf.bool)\n     x = tf.reduce_all(x, reduction_indices=axis, keep_dims=keepdims)\n@@ -1019,38 +1019,38 @@ def all(x, axis=None, keepdims=False):\n \n \n def argmax(x, axis=-1):\n-    '''Returns the index of the maximum value\n+    \"\"\"Returns the index of the maximum value\n     along a tensor axis.\n-    '''\n+    \"\"\"\n     if axis < 0:\n         axis = axis % len(x.get_shape())\n     return tf.argmax(x, axis)\n \n \n def argmin(x, axis=-1):\n-    '''Returns the index of the minimum value\n+    \"\"\"Returns the index of the minimum value\n     along a tensor axis.\n-    '''\n+    \"\"\"\n     if axis < 0:\n         axis = axis % len(x.get_shape())\n     return tf.argmin(x, axis)\n \n \n def square(x):\n-    '''Element-wise square.\n-    '''\n+    \"\"\"Element-wise square.\n+    \"\"\"\n     return tf.square(x)\n \n \n def abs(x):\n-    '''Element-wise absolute value.\n-    '''\n+    \"\"\"Element-wise absolute value.\n+    \"\"\"\n     return tf.abs(x)\n \n \n def sqrt(x):\n-    '''Element-wise square root.\n-    '''\n+    \"\"\"Element-wise square root.\n+    \"\"\"\n     zero = _to_tensor(0., x.dtype.base_dtype)\n     inf = _to_tensor(np.inf, x.dtype.base_dtype)\n     x = tf.clip_by_value(x, zero, inf)\n@@ -1058,38 +1058,38 @@ def sqrt(x):\n \n \n def exp(x):\n-    '''Element-wise exponential.\n-    '''\n+    \"\"\"Element-wise exponential.\n+    \"\"\"\n     return tf.exp(x)\n \n \n def log(x):\n-    '''Element-wise log.\n-    '''\n+    \"\"\"Element-wise log.\n+    \"\"\"\n     return tf.log(x)\n \n \n def round(x):\n-    '''Element-wise rounding to the closest integer.\n-    '''\n+    \"\"\"Element-wise rounding to the closest integer.\n+    \"\"\"\n     return tf.round(x)\n \n \n def sign(x):\n-    '''Element-wise sign.\n-    '''\n+    \"\"\"Element-wise sign.\n+    \"\"\"\n     return tf.sign(x)\n \n \n def pow(x, a):\n-    '''Element-wise exponentiation.\n-    '''\n+    \"\"\"Element-wise exponentiation.\n+    \"\"\"\n     return tf.pow(x, a)\n \n \n def clip(x, min_value, max_value):\n-    '''Element-wise value clipping.\n-    '''\n+    \"\"\"Element-wise value clipping.\n+    \"\"\"\n     if max_value < min_value:\n         max_value = min_value\n     min_value = _to_tensor(min_value, x.dtype.base_dtype)\n@@ -1098,75 +1098,75 @@ def clip(x, min_value, max_value):\n \n \n def equal(x, y):\n-    '''Element-wise equality between two tensors.\n+    \"\"\"Element-wise equality between two tensors.\n     Returns a bool tensor.\n-    '''\n+    \"\"\"\n     return tf.equal(x, y)\n \n \n def not_equal(x, y):\n-    '''Element-wise inequality between two tensors.\n+    \"\"\"Element-wise inequality between two tensors.\n     Returns a bool tensor.\n-    '''\n+    \"\"\"\n     return tf.not_equal(x, y)\n \n \n def greater(x, y):\n-    '''Element-wise truth value of (x > y).\n+    \"\"\"Element-wise truth value of (x > y).\n     Returns a bool tensor.\n-    '''\n+    \"\"\"\n     return tf.greater(x, y)\n \n \n def greater_equal(x, y):\n-    '''Element-wise truth value of (x >= y).\n+    \"\"\"Element-wise truth value of (x >= y).\n     Returns a bool tensor.\n-    '''\n+    \"\"\"\n     return tf.greater_equal(x, y)\n \n \n def lesser(x, y):\n-    '''Element-wise truth value of (x < y).\n+    \"\"\"Element-wise truth value of (x < y).\n     Returns a bool tensor.\n-    '''\n+    \"\"\"\n     return tf.less(x, y)\n \n \n def lesser_equal(x, y):\n-    '''Element-wise truth value of (x <= y).\n+    \"\"\"Element-wise truth value of (x <= y).\n     Returns a bool tensor.\n-    '''\n+    \"\"\"\n     return tf.less_equal(x, y)\n \n \n def maximum(x, y):\n-    '''Element-wise maximum of two tensors.\n-    '''\n+    \"\"\"Element-wise maximum of two tensors.\n+    \"\"\"\n     return tf.maximum(x, y)\n \n \n def minimum(x, y):\n-    '''Element-wise minimum of two tensors.\n-    '''\n+    \"\"\"Element-wise minimum of two tensors.\n+    \"\"\"\n     return tf.minimum(x, y)\n \n \n def sin(x):\n-    '''Computes sin of x element-wise.\n-    '''\n+    \"\"\"Computes sin of x element-wise.\n+    \"\"\"\n     return tf.sin(x)\n \n \n def cos(x):\n-    '''Computes cos of x element-wise.\n-    '''\n+    \"\"\"Computes cos of x element-wise.\n+    \"\"\"\n     return tf.cos(x)\n \n \n def normalize_batch_in_training(x, gamma, beta,\n                                 reduction_axes, epsilon=1e-3):\n-    '''Computes mean and std for batch then apply batch_normalization on batch.\n-    '''\n+    \"\"\"Computes mean and std for batch then apply batch_normalization on batch.\n+    \"\"\"\n     mean, var = tf.nn.moments(x, reduction_axes,\n                               shift=None, name=None, keep_dims=False)\n     if sorted(reduction_axes) == range(ndim(x))[:-1]:\n@@ -1194,18 +1194,18 @@ def normalize_batch_in_training(x, gamma, beta,\n \n \n def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n-    '''Applies batch normalization on x given mean, var, beta and gamma:\n+    \"\"\"Applies batch normalization on x given mean, var, beta and gamma:\n \n     output = (x - mean) / sqrt(var + epsilon) * gamma + beta\n-    '''\n+    \"\"\"\n     return tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon)\n \n \n # SHAPE OPERATIONS\n \n def concatenate(tensors, axis=-1):\n-    '''Concatenates a list of tensors alongside the specified axis.\n-    '''\n+    \"\"\"Concatenates a list of tensors alongside the specified axis.\n+    \"\"\"\n     if axis < 0:\n         dims = ndim(tensors[0])\n         if dims:\n@@ -1223,28 +1223,28 @@ def concatenate(tensors, axis=-1):\n \n \n def reshape(x, shape):\n-    '''Reshapes a tensor to the specified shape.\n-    '''\n+    \"\"\"Reshapes a tensor to the specified shape.\n+    \"\"\"\n     return tf.reshape(x, shape)\n \n \n def permute_dimensions(x, pattern):\n-    '''Permutes axes in a tensor.\n+    \"\"\"Permutes axes in a tensor.\n \n     # Arguments\n         pattern: should be a tuple of\n             dimension indices, e.g. (0, 2, 1).\n-    '''\n+    \"\"\"\n     return tf.transpose(x, perm=pattern)\n \n \n def resize_images(X, height_factor, width_factor, dim_ordering):\n-    '''Resizes the images contained in a 4D tensor of shape\n+    \"\"\"Resizes the images contained in a 4D tensor of shape\n     - [batch, channels, height, width] (for 'th' dim_ordering)\n     - [batch, height, width, channels] (for 'tf' dim_ordering)\n     by a factor of (height_factor, width_factor). Both factors should be\n     positive integers.\n-    '''\n+    \"\"\"\n     if dim_ordering == 'th':\n         original_shape = int_shape(X)\n         new_shape = tf.shape(X)[2:]\n@@ -1268,12 +1268,12 @@ def resize_images(X, height_factor, width_factor, dim_ordering):\n \n \n def resize_volumes(X, depth_factor, height_factor, width_factor, dim_ordering):\n-    '''Resizes the volume contained in a 5D tensor of shape\n+    \"\"\"Resizes the volume contained in a 5D tensor of shape\n     - [batch, channels, depth, height, width] (for 'th' dim_ordering)\n     - [batch, depth, height, width, channels] (for 'tf' dim_ordering)\n     by a factor of (depth_factor, height_factor, width_factor).\n     All three factors should be positive integers.\n-    '''\n+    \"\"\"\n     if dim_ordering == 'th':\n         output = repeat_elements(X, depth_factor, axis=2)\n         output = repeat_elements(output, height_factor, axis=3)\n@@ -1289,11 +1289,11 @@ def resize_volumes(X, depth_factor, height_factor, width_factor, dim_ordering):\n \n \n def repeat_elements(x, rep, axis):\n-    '''Repeats the elements of a tensor along an axis, like np.repeat\n+    \"\"\"Repeats the elements of a tensor along an axis, like np.repeat\n \n     If x has shape (s1, s2, s3) and axis=1, the output\n     will have shape (s1, s2 * rep, s3)\n-    '''\n+    \"\"\"\n     x_shape = x.get_shape().as_list()\n     # slices along the repeat axis\n     try:\n@@ -1306,11 +1306,11 @@ def repeat_elements(x, rep, axis):\n \n \n def repeat(x, n):\n-    '''Repeats a 2D tensor:\n+    \"\"\"Repeats a 2D tensor:\n \n     if x has shape (samples, dim) and n=2,\n     the output will have shape (samples, 2, dim)\n-    '''\n+    \"\"\"\n     assert ndim(x) == 2\n     x = tf.expand_dims(x, 1)\n     pattern = stack([1, n, 1])\n@@ -1318,7 +1318,7 @@ def repeat(x, n):\n \n \n def arange(start, stop=None, step=1, dtype='int32'):\n-    '''Creates a 1-D tensor containing a sequence of integers.\n+    \"\"\"Creates a 1-D tensor containing a sequence of integers.\n \n     The function arguments use the same convention as\n     Theano's arange: if only one argument is provided,\n@@ -1326,7 +1326,7 @@ def arange(start, stop=None, step=1, dtype='int32'):\n \n     The default type of the returned tensor is 'int32' to\n     match TensorFlow's default.\n-    '''\n+    \"\"\"\n     # Match the behavior of numpy and Theano by returning an empty seqence.\n     if stop is None and start < 0:\n         start = 0\n@@ -1347,45 +1347,45 @@ def flatten(x):\n \n \n def batch_flatten(x):\n-    '''Turn a n-D tensor into a 2D tensor where\n+    \"\"\"Turn a n-D tensor into a 2D tensor where\n     the first dimension is conserved.\n-    '''\n+    \"\"\"\n     x = tf.reshape(x, stack([-1, prod(shape(x)[1:])]))\n     return x\n \n \n def expand_dims(x, dim=-1):\n-    '''Adds a 1-sized dimension at index \"dim\".\n-    '''\n+    \"\"\"Adds a 1-sized dimension at index \"dim\".\n+    \"\"\"\n     return tf.expand_dims(x, dim)\n \n \n def squeeze(x, axis):\n-    '''Removes a 1-dimension from the tensor at index \"axis\".\n-    '''\n+    \"\"\"Removes a 1-dimension from the tensor at index \"axis\".\n+    \"\"\"\n     return tf.squeeze(x, [axis])\n \n \n def temporal_padding(x, padding=1):\n-    '''Pads the middle dimension of a 3D tensor\n+    \"\"\"Pads the middle dimension of a 3D tensor\n     with \"padding\" zeros left and right.\n-    '''\n+    \"\"\"\n     pattern = [[0, 0], [padding, padding], [0, 0]]\n     return tf.pad(x, pattern)\n \n \n def asymmetric_temporal_padding(x, left_pad=1, right_pad=1):\n-    '''Pad the middle dimension of a 3D tensor\n+    \"\"\"Pad the middle dimension of a 3D tensor\n     with \"left_pad\" zeros left and \"right_pad\" right.\n-    '''\n+    \"\"\"\n     pattern = [[0, 0], [left_pad, right_pad], [0, 0]]\n     return tf.pad(x, pattern)\n \n \n def spatial_2d_padding(x, padding=(1, 1), dim_ordering='default'):\n-    '''Pads the 2nd and 3rd dimensions of a 4D tensor\n+    \"\"\"Pads the 2nd and 3rd dimensions of a 4D tensor\n     with \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n-    '''\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n     if dim_ordering not in {'th', 'tf'}:\n@@ -1404,10 +1404,10 @@ def spatial_2d_padding(x, padding=(1, 1), dim_ordering='default'):\n def asymmetric_spatial_2d_padding(x, top_pad=1, bottom_pad=1,\n                                   left_pad=1, right_pad=1,\n                                   dim_ordering='default'):\n-    '''Pad the rows and columns of a 4D tensor\n+    \"\"\"Pad the rows and columns of a 4D tensor\n     with \"top_pad\", \"bottom_pad\", \"left_pad\", \"right_pad\" (resp.) zeros\n     rows on top, bottom; cols on left, right.\n-    '''\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n     if dim_ordering not in {'th', 'tf'}:\n@@ -1427,12 +1427,12 @@ def asymmetric_spatial_2d_padding(x, top_pad=1, bottom_pad=1,\n \n \n def spatial_3d_padding(x, padding=(1, 1, 1), dim_ordering='default'):\n-    '''Pads 5D tensor with zeros for the depth, height, width dimension with\n+    \"\"\"Pads 5D tensor with zeros for the depth, height, width dimension with\n     \"padding[0]\", \"padding[1]\" and \"padding[2]\" (resp.) zeros left and right\n \n     For 'tf' dim_ordering, the 2nd, 3rd and 4th dimension will be padded.\n     For 'th' dim_ordering, the 3rd, 4th and 5th dimension will be padded.\n-    '''\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n     if dim_ordering not in {'th', 'tf'}:\n@@ -1465,16 +1465,16 @@ def stack(x):\n \n \n def one_hot(indices, nb_classes):\n-    '''Input: nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1))\n+    \"\"\"Input: nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1))\n     Output: (n + 1)D one hot representation of the input\n     with shape (batch_size, dim1, dim2, ... dim(n-1), nb_classes)\n-    '''\n+    \"\"\"\n     return tf.one_hot(indices, depth=nb_classes, axis=-1)\n \n \n def reverse(x, axes):\n-    '''Reverse a tensor along the the specified axes\n-    '''\n+    \"\"\"Reverse a tensor along the the specified axes\n+    \"\"\"\n     if isinstance(axes, int):\n         axes = [axes]\n     try:\n@@ -1489,16 +1489,16 @@ def reverse(x, axes):\n \n \n def get_value(x):\n-    '''Returns the value of a tensor variable,\n+    \"\"\"Returns the value of a tensor variable,\n     as a Numpy array.\n-    '''\n+    \"\"\"\n     return x.eval(session=get_session())\n \n \n def batch_get_value(xs):\n-    '''Returns the value of more than one tensor variable,\n+    \"\"\"Returns the value of more than one tensor variable,\n     as a list of Numpy arrays.\n-    '''\n+    \"\"\"\n     if xs:\n         return get_session().run(xs)\n     else:\n@@ -1506,9 +1506,9 @@ def batch_get_value(xs):\n \n \n def set_value(x, value):\n-    '''Sets the value of a tensor variable,\n+    \"\"\"Sets the value of a tensor variable,\n     from a Numpy array.\n-    '''\n+    \"\"\"\n     value = np.asarray(value)\n     tf_dtype = _convert_string_dtype(x.dtype.name.split('_')[0])\n     if hasattr(x, '_assign_placeholder'):\n@@ -1523,12 +1523,12 @@ def set_value(x, value):\n \n \n def batch_set_value(tuples):\n-    '''Sets the values of many tensor variables at once.\n+    \"\"\"Sets the values of many tensor variables at once.\n \n     # Arguments\n         tuples: a list of tuples `(tensor, value)`.\n             `value` should be a Numpy array.\n-    '''\n+    \"\"\"\n     if tuples:\n         assign_ops = []\n         feed_dict = {}\n@@ -1554,9 +1554,9 @@ def get_variable_shape(x):\n \n \n def print_tensor(x, message=''):\n-    '''Print the message and the tensor when evaluated and return the same\n+    \"\"\"Print the message and the tensor when evaluated and return the same\n     tensor.\n-    '''\n+    \"\"\"\n     return tf.Print(x, [x], message)\n \n \n@@ -1605,13 +1605,13 @@ class Function(object):\n \n \n def function(inputs, outputs, updates=[], **kwargs):\n-    '''Instantiates a Keras function.\n+    \"\"\"Instantiates a Keras function.\n \n     # Arguments\n         inputs: list of placeholder/variable tensors.\n         outputs: list of output tensors.\n         updates: list of update tuples (old_tensor, new_tensor).\n-    '''\n+    \"\"\"\n     if len(kwargs) > 0:\n         msg = [\n             'Expected no kwargs, you passed %s' % len(kwargs),\n@@ -1622,16 +1622,16 @@ def function(inputs, outputs, updates=[], **kwargs):\n \n \n def gradients(loss, variables):\n-    '''Returns the gradients of `variables` (list of tensor variables)\n+    \"\"\"Returns the gradients of `variables` (list of tensor variables)\n     with regard to `loss`.\n-    '''\n+    \"\"\"\n     return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n \n \n def stop_gradient(variables):\n-    '''Returns `variables` but with zero gradient with respect to every other\n+    \"\"\"Returns `variables` but with zero gradient with respect to every other\n     variables.\n-    '''\n+    \"\"\"\n     return tf.stop_gradient(variables)\n \n \n@@ -1640,7 +1640,7 @@ def stop_gradient(variables):\n def rnn(step_function, inputs, initial_states,\n         go_backwards=False, mask=None, constants=None,\n         unroll=False, input_length=None):\n-    '''Iterates over the time dimension of a tensor.\n+    \"\"\"Iterates over the time dimension of a tensor.\n \n     # Arguments\n         inputs: tensor of temporal data of shape (samples, time, ...)\n@@ -1680,7 +1680,7 @@ def rnn(step_function, inputs, initial_states,\n             at time t for sample s.\n         new_states: list of tensors, latest states returned by\n             the step function, of shape (samples, ...).\n-    '''\n+    \"\"\"\n     ndim = len(inputs.get_shape())\n     if ndim < 3:\n         raise ValueError('Input should be at least 3D.')\n@@ -1839,8 +1839,8 @@ def rnn(step_function, inputs, initial_states,\n \n \n def _cond(condition, then_lambda, else_lambda):\n-    '''Backwards compatible interface to tf.cond prior to public introduction.\n-    '''\n+    \"\"\"Backwards compatible interface to tf.cond prior to public introduction.\n+    \"\"\"\n     try:\n         cond_fn = tf.cond\n     except AttributeError:\n@@ -1850,7 +1850,7 @@ def _cond(condition, then_lambda, else_lambda):\n \n \n def switch(condition, then_expression, else_expression):\n-    '''Switches between two operations\n+    \"\"\"Switches between two operations\n     depending on a scalar value (int or bool).\n     Note that both `then_expression` and `else_expression`\n     should be symbolic tensors of the *same shape*.\n@@ -1859,7 +1859,7 @@ def switch(condition, then_expression, else_expression):\n         condition: scalar tensor.\n         then_expression: TensorFlow operation.\n         else_expression: TensorFlow operation.\n-    '''\n+    \"\"\"\n     x_shape = copy.copy(then_expression.get_shape())\n     if condition.dtype != tf.bool:\n         condition = tf.cast(condition, 'bool')\n@@ -1871,9 +1871,9 @@ def switch(condition, then_expression, else_expression):\n \n \n def in_train_phase(x, alt):\n-    '''Selects `x` in train phase, and `alt` otherwise.\n+    \"\"\"Selects `x` in train phase, and `alt` otherwise.\n     Note that `alt` should have the *same shape* as `x`.\n-    '''\n+    \"\"\"\n     if learning_phase() is 1:\n         return x\n     elif learning_phase() is 0:\n@@ -1885,9 +1885,9 @@ def in_train_phase(x, alt):\n \n \n def in_test_phase(x, alt):\n-    '''Selects `x` in test phase, and `alt` otherwise.\n+    \"\"\"Selects `x` in test phase, and `alt` otherwise.\n     Note that `alt` should have the *same shape* as `x`.\n-    '''\n+    \"\"\"\n     if learning_phase() is 1:\n         return alt\n     elif learning_phase() is 0:\n@@ -1901,12 +1901,12 @@ def in_test_phase(x, alt):\n # NN OPERATIONS\n \n def relu(x, alpha=0., max_value=None):\n-    '''Rectified linear unit\n+    \"\"\"Rectified linear unit\n \n     # Arguments\n         alpha: slope of negative section.\n         max_value: saturation threshold.\n-    '''\n+    \"\"\"\n     if alpha != 0.:\n         negative_part = tf.nn.relu(-x)\n     x = tf.nn.relu(x)\n@@ -1921,12 +1921,12 @@ def relu(x, alpha=0., max_value=None):\n \n \n def elu(x, alpha=1.):\n-    '''Exponential linear unit.\n+    \"\"\"Exponential linear unit.\n \n     # Arguments\n         x: Tensor to compute the activation function for.\n         alpha: scalar\n-    '''\n+    \"\"\"\n     res = tf.nn.elu(x)\n     if alpha == 1:\n         return res\n@@ -1935,28 +1935,28 @@ def elu(x, alpha=1.):\n \n \n def softmax(x):\n-    '''Softmax of a tensor.\n-    '''\n+    \"\"\"Softmax of a tensor.\n+    \"\"\"\n     return tf.nn.softmax(x)\n \n \n def softplus(x):\n-    '''Softplus of a tensor.\n-    '''\n+    \"\"\"Softplus of a tensor.\n+    \"\"\"\n     return tf.nn.softplus(x)\n \n \n def softsign(x):\n-    '''Softsign of a tensor.\n-    '''\n+    \"\"\"Softsign of a tensor.\n+    \"\"\"\n     return tf.nn.softsign(x)\n \n \n def categorical_crossentropy(output, target, from_logits=False):\n-    '''Categorical crossentropy between an output tensor\n+    \"\"\"Categorical crossentropy between an output tensor\n     and a target tensor, where the target is a tensor of the same\n     shape as the output.\n-    '''\n+    \"\"\"\n     # Note: tf.nn.softmax_cross_entropy_with_logits\n     # expects logits, Keras expects probabilities.\n     if not from_logits:\n@@ -1974,9 +1974,9 @@ def categorical_crossentropy(output, target, from_logits=False):\n \n \n def sparse_categorical_crossentropy(output, target, from_logits=False):\n-    '''Categorical crossentropy between an output tensor\n+    \"\"\"Categorical crossentropy between an output tensor\n     and a target tensor, where the target is an integer tensor.\n-    '''\n+    \"\"\"\n     # Note: tf.nn.softmax_cross_entropy_with_logits\n     # expects logits, Keras expects probabilities.\n     if not from_logits:\n@@ -1996,8 +1996,8 @@ def sparse_categorical_crossentropy(output, target, from_logits=False):\n \n \n def binary_crossentropy(output, target, from_logits=False):\n-    '''Binary crossentropy between an output tensor and a target tensor.\n-    '''\n+    \"\"\"Binary crossentropy between an output tensor and a target tensor.\n+    \"\"\"\n     # Note: tf.nn.softmax_cross_entropy_with_logits\n     # expects logits, Keras expects probabilities.\n     if not from_logits:\n@@ -2009,15 +2009,15 @@ def binary_crossentropy(output, target, from_logits=False):\n \n \n def sigmoid(x):\n-    '''Element-wise sigmoid.\n-    '''\n+    \"\"\"Element-wise sigmoid.\n+    \"\"\"\n     return tf.nn.sigmoid(x)\n \n \n def hard_sigmoid(x):\n-    '''Segment-wise linear approximation of sigmoid.\n+    \"\"\"Segment-wise linear approximation of sigmoid.\n     Faster than sigmoid.\n-    '''\n+    \"\"\"\n     x = (0.2 * x) + 0.5\n     zero = _to_tensor(0., x.dtype.base_dtype)\n     one = _to_tensor(1., x.dtype.base_dtype)\n@@ -2026,13 +2026,13 @@ def hard_sigmoid(x):\n \n \n def tanh(x):\n-    '''Element-wise tanh.\n-    '''\n+    \"\"\"Element-wise tanh.\n+    \"\"\"\n     return tf.nn.tanh(x)\n \n \n def dropout(x, level, noise_shape=None, seed=None):\n-    '''Sets entries in `x` to zero at random,\n+    \"\"\"Sets entries in `x` to zero at random,\n     while scaling the entire tensor.\n \n     # Arguments\n@@ -2042,7 +2042,7 @@ def dropout(x, level, noise_shape=None, seed=None):\n         noise_shape: shape for randomly generated keep/drop flags,\n             must be broadcastable to the shape of `x`\n         seed: random seed to ensure determinism.\n-    '''\n+    \"\"\"\n     retain_prob = 1. - level\n     if seed is None:\n         seed = np.random.randint(10e6)\n@@ -2052,15 +2052,15 @@ def dropout(x, level, noise_shape=None, seed=None):\n \n \n def l2_normalize(x, axis):\n-    '''Normalizes a tensor wrt the L2 norm alongside the specified axis.\n-    '''\n+    \"\"\"Normalizes a tensor wrt the L2 norm alongside the specified axis.\n+    \"\"\"\n     if axis < 0:\n         axis = axis % len(x.get_shape())\n     return tf.nn.l2_normalize(x, dim=axis)\n \n \n def in_top_k(predictions, targets, k):\n-    '''Returns whether the `targets` are in the top `k` `predictions`\n+    \"\"\"Returns whether the `targets` are in the top `k` `predictions`\n \n     # Arguments\n         predictions: A tensor of shape batch_size x classess and type float32.\n@@ -2070,7 +2070,7 @@ def in_top_k(predictions, targets, k):\n     # Returns\n         A tensor of shape batch_size and type bool. output_i is True if\n         targets_i is within top-k values of predictions_i\n-    '''\n+    \"\"\"\n     return tf.nn.in_top_k(predictions, targets, k)\n \n \n@@ -2163,13 +2163,13 @@ def _postprocess_conv3d_output(x, dim_ordering):\n \n def conv1d(x, kernel, stride=1, border_mode='valid',\n            image_shape=None, filter_shape=None):\n-    '''1D convolution.\n+    \"\"\"1D convolution.\n \n     # Arguments\n         kernel: kernel tensor.\n         strides: stride integer.\n         border_mode: string, \"same\" or \"valid\".\n-    '''\n+    \"\"\"\n     # pre-process dtype\n     x_dtype = dtype(x)\n     if x_dtype == 'float64':\n@@ -2186,7 +2186,7 @@ def conv1d(x, kernel, stride=1, border_mode='valid',\n def conv2d(x, kernel, strides=(1, 1), border_mode='valid',\n            dim_ordering='default',\n            image_shape=None, filter_shape=None, filter_dilation=(1, 1)):\n-    '''2D convolution.\n+    \"\"\"2D convolution.\n \n     # Arguments\n         kernel: kernel tensor.\n@@ -2195,7 +2195,7 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid',\n         dim_ordering: \"tf\" or \"th\".\n             Whether to use Theano or TensorFlow dimension ordering\n             for inputs/kernels/ouputs.\n-    '''\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n     if dim_ordering not in {'th', 'tf'}:\n@@ -2218,7 +2218,7 @@ def deconv2d(x, kernel, output_shape, strides=(1, 1),\n              border_mode='valid',\n              dim_ordering='default',\n              image_shape=None, filter_shape=None):\n-    '''2D deconvolution (i.e. transposed convolution).\n+    \"\"\"2D deconvolution (i.e. transposed convolution).\n \n     # Arguments\n         x: input tensor.\n@@ -2229,7 +2229,7 @@ def deconv2d(x, kernel, output_shape, strides=(1, 1),\n         dim_ordering: \"tf\" or \"th\".\n             Whether to use Theano or TensorFlow dimension ordering\n             for inputs/kernels/ouputs.\n-    '''\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n     if dim_ordering not in {'th', 'tf'}:\n@@ -2290,7 +2290,7 @@ def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n def conv3d(x, kernel, strides=(1, 1, 1),\n            border_mode='valid', dim_ordering='default',\n            volume_shape=None, filter_shape=None):\n-    '''3D convolution.\n+    \"\"\"3D convolution.\n \n     # Arguments\n         kernel: kernel tensor.\n@@ -2299,7 +2299,7 @@ def conv3d(x, kernel, strides=(1, 1, 1),\n         dim_ordering: \"tf\" or \"th\".\n             Whether to use Theano or TensorFlow dimension ordering\n             for inputs/kernels/ouputs.\n-    '''\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n     if dim_ordering not in {'th', 'tf'}:\n@@ -2317,7 +2317,7 @@ def conv3d(x, kernel, strides=(1, 1, 1),\n def pool2d(x, pool_size, strides=(1, 1),\n            border_mode='valid', dim_ordering='default',\n            pool_mode='max'):\n-    '''2D Pooling.\n+    \"\"\"2D Pooling.\n \n     # Arguments\n         pool_size: tuple of 2 integers.\n@@ -2325,7 +2325,7 @@ def pool2d(x, pool_size, strides=(1, 1),\n         border_mode: one of \"valid\", \"same\".\n         dim_ordering: one of \"th\", \"tf\".\n         pool_mode: one of \"max\", \"avg\".\n-    '''\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n     if dim_ordering not in {'th', 'tf'}:\n@@ -2349,7 +2349,7 @@ def pool2d(x, pool_size, strides=(1, 1),\n \n def pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n            dim_ordering='default', pool_mode='max'):\n-    '''3D Pooling.\n+    \"\"\"3D Pooling.\n \n     # Arguments\n         pool_size: tuple of 3 integers.\n@@ -2357,7 +2357,7 @@ def pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n         border_mode: one of \"valid\", \"same\".\n         dim_ordering: one of \"th\", \"tf\".\n         pool_mode: one of \"max\", \"avg\".\n-    '''\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n     if dim_ordering not in {'th', 'tf'}:\n@@ -2446,7 +2446,7 @@ def ctc_label_dense_to_sparse(labels, label_lengths):\n \n \n def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n-    '''Runs CTC loss algorithm on each batch element.\n+    \"\"\"Runs CTC loss algorithm on each batch element.\n \n     # Arguments\n         y_true: tensor (samples, max_string_length) containing the truth labels\n@@ -2460,7 +2460,7 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n     # Returns\n         Tensor with shape (samples,1) containing the\n             CTC loss of each element\n-    '''\n+    \"\"\"\n     label_length = tf.to_int32(tf.squeeze(label_length))\n     input_length = tf.to_int32(tf.squeeze(input_length))\n     sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\n@@ -2474,7 +2474,7 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n \n def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n                top_paths=1):\n-    '''Decodes the output of a softmax using either\n+    \"\"\"Decodes the output of a softmax using either\n        greedy (also known as best path) or a constrained dictionary\n        search.\n \n@@ -2495,7 +2495,7 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n                 the decoded sequence. If false, returns the `top_paths` most probable\n                 decoded sequences. Important: blank labels are returned as -1\n             Tensor (top_paths,) that contains the log probability of each decoded sequence\n-    '''\n+    \"\"\"\n     y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + 1e-8)\n     input_length = tf.to_int32(input_length)\n \n@@ -2518,7 +2518,7 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n # HIGH ORDER FUNCTIONS\n \n def map_fn(fn, elems, name=None):\n-    '''Map the function fn over the elements elems and return the outputs.\n+    \"\"\"Map the function fn over the elements elems and return the outputs.\n \n     # Arguments\n         fn: Callable that will be called upon each element in elems\n@@ -2528,12 +2528,12 @@ def map_fn(fn, elems, name=None):\n     # Returns\n         Tensor with first dimension equal to the elems and second depending on\n         fn\n-    '''\n+    \"\"\"\n     return tf.map_fn(fn, elems, name=name)\n \n \n def foldl(fn, elems, initializer=None, name=None):\n-    '''Reduce elems using fn to combine them from left to right.\n+    \"\"\"Reduce elems using fn to combine them from left to right.\n \n     # Arguments\n         fn: Callable that will be called upon each element in elems and an\n@@ -2544,12 +2544,12 @@ def foldl(fn, elems, initializer=None, name=None):\n \n     # Returns\n         Same type and shape as initializer\n-    '''\n+    \"\"\"\n     return tf.foldl(fn, elems, initializer=initializer, name=name)\n \n \n def foldr(fn, elems, initializer=None, name=None):\n-    '''Reduce elems using fn to combine them from right to left.\n+    \"\"\"Reduce elems using fn to combine them from right to left.\n \n     # Arguments\n         fn: Callable that will be called upon each element in elems and an\n@@ -2560,5 +2560,5 @@ def foldr(fn, elems, initializer=None, name=None):\n \n     # Returns\n         Same type and shape as initializer\n-    '''\n+    \"\"\"\n     return tf.foldr(fn, elems, initializer=initializer, name=name)\n\n@@ -67,7 +67,7 @@ def is_explicit_shape(shape):\n \n \n def variable(value, dtype=None, name=None):\n-    '''Instantiates a variable and returns it.\n+    \"\"\"Instantiates a variable and returns it.\n \n     # Arguments\n         value: Numpy array, initial value of the tensor.\n@@ -76,7 +76,7 @@ def variable(value, dtype=None, name=None):\n \n     # Returns\n         A variable instance (with Keras metadata included).\n-    '''\n+    \"\"\"\n     if dtype is None:\n         dtype = floatx()\n     if hasattr(value, 'tocoo'):\n@@ -91,8 +91,8 @@ def variable(value, dtype=None, name=None):\n \n \n def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n-    '''Instantiate an input data placeholder variable.\n-    '''\n+    \"\"\"Instantiate an input data placeholder variable.\n+    \"\"\"\n     if dtype is None:\n         dtype = floatx()\n     if shape is None and ndim is None:\n@@ -114,16 +114,16 @@ def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n \n \n def shape(x):\n-    '''Returns the shape of a tensor.\n+    \"\"\"Returns the shape of a tensor.\n \n     Warning: type returned will be different for\n     Theano backend (Theano tensor type) and TF backend (TF TensorShape).\n-    '''\n+    \"\"\"\n     return x.shape\n \n \n def int_shape(x):\n-    '''Returns the shape of a Keras tensor or a Keras variable as a tuple of\n+    \"\"\"Returns the shape of a Keras tensor or a Keras variable as a tuple of\n     integers or None entries.\n \n     # Arguments\n@@ -131,7 +131,7 @@ def int_shape(x):\n \n     # Returns\n         A tuple of integers (or None entries).\n-    '''\n+    \"\"\"\n     if hasattr(x, '_keras_shape'):\n         return x._keras_shape\n     else:\n@@ -147,30 +147,30 @@ def dtype(x):\n \n \n def eval(x):\n-    '''Returns the value of a tensor.\n-    '''\n+    \"\"\"Returns the value of a tensor.\n+    \"\"\"\n     return to_dense(x).eval()\n \n \n def zeros(shape, dtype=None, name=None):\n-    '''Instantiates an all-zeros variable.\n-    '''\n+    \"\"\"Instantiates an all-zeros variable.\n+    \"\"\"\n     if dtype is None:\n         dtype = floatx()\n     return variable(np.zeros(shape), dtype, name)\n \n \n def ones(shape, dtype=None, name=None):\n-    '''Instantiates an all-ones variable.\n-    '''\n+    \"\"\"Instantiates an all-ones variable.\n+    \"\"\"\n     if dtype is None:\n         dtype = floatx()\n     return variable(np.ones(shape), dtype, name)\n \n \n def eye(size, dtype=None, name=None):\n-    '''Instantiates an identity matrix.\n-    '''\n+    \"\"\"Instantiates an identity matrix.\n+    \"\"\"\n     if dtype is None:\n         dtype = floatx()\n     return variable(np.eye(size), dtype, name)\n@@ -195,10 +195,10 @@ def random_normal_variable(shape, mean, scale, dtype=None, name=None):\n \n \n def count_params(x):\n-    '''Returns the number of scalars in a tensor.\n+    \"\"\"Returns the number of scalars in a tensor.\n \n     Return: numpy integer.\n-    '''\n+    \"\"\"\n     return np.prod(x.shape.eval())\n \n \n@@ -227,10 +227,10 @@ def moving_average_update(variable, value, momentum):\n \n # LINEAR ALGEBRA\n \n-'''\n+\"\"\"\n Assumed overridden:\n +, -, /, *, +=, -=, *=, /=\n-'''\n+\"\"\"\n \n \n def dot(x, y):\n@@ -242,7 +242,7 @@ def dot(x, y):\n \n \n def batch_dot(x, y, axes=None):\n-    '''Batchwise dot product.\n+    \"\"\"Batchwise dot product.\n \n     batch_dot results in a tensor with less dimensions than the input.\n     If the number of dimensions is reduced to 1, we use `expand_dims` to\n@@ -278,7 +278,7 @@ def batch_dot(x, y, axes=None):\n             dimension 2 of y has been summed over. (dot_axes[1] = 2)\n \n         output_shape = (100, 30)\n-    '''\n+    \"\"\"\n     # TODO: `keras_shape` inference.\n     if isinstance(axes, int):\n         axes = (axes, axes)\n@@ -297,11 +297,11 @@ def transpose(x):\n \n \n def gather(reference, indices):\n-    '''reference: a tensor.\n+    \"\"\"reference: a tensor.\n     indices: an int tensor of indices.\n \n     Return: a tensor of same type as reference.\n-    '''\n+    \"\"\"\n     # TODO: `keras_shape` inference.\n     return reference[indices]\n \n@@ -318,20 +318,20 @@ def min(x, axis=None, keepdims=False):\n \n \n def sum(x, axis=None, keepdims=False):\n-    '''Sum of the values in a tensor, alongside the specified axis.\n-    '''\n+    \"\"\"Sum of the values in a tensor, alongside the specified axis.\n+    \"\"\"\n     return T.sum(x, axis=axis, keepdims=keepdims)\n \n \n def prod(x, axis=None, keepdims=False):\n-    '''Multiply the values in a tensor, alongside the specified axis.\n-    '''\n+    \"\"\"Multiply the values in a tensor, alongside the specified axis.\n+    \"\"\"\n     return T.prod(x, axis=axis, keepdims=keepdims)\n \n \n def mean(x, axis=None, keepdims=False):\n-    '''Mean of a tensor, alongside the specified axis.\n-    '''\n+    \"\"\"Mean of a tensor, alongside the specified axis.\n+    \"\"\"\n     dtype = None\n     # bool is available since theano v0.9dev\n     if 'int' in x.dtype or x.dtype == 'bool':\n@@ -348,14 +348,14 @@ def var(x, axis=None, keepdims=False):\n \n \n def any(x, axis=None, keepdims=False):\n-    '''Bitwise reduction (logical OR).\n-    '''\n+    \"\"\"Bitwise reduction (logical OR).\n+    \"\"\"\n     return T.any(x, axis=axis, keepdims=keepdims)\n \n \n def all(x, axis=None, keepdims=False):\n-    '''Bitwise reduction (logical AND).\n-    '''\n+    \"\"\"Bitwise reduction (logical AND).\n+    \"\"\"\n     return T.all(x, axis=axis, keepdims=keepdims)\n \n \n@@ -448,8 +448,8 @@ def cos(x):\n \n def normalize_batch_in_training(x, gamma, beta,\n                                 reduction_axes, epsilon=1e-3):\n-    '''Computes mean and std for batch then apply batch_normalization on batch.\n-    '''\n+    \"\"\"Computes mean and std for batch then apply batch_normalization on batch.\n+    \"\"\"\n     # TODO remove this if statement when Theano without\n     # T.nnet.bn.batch_normalization_train is deprecated\n     if not hasattr(T.nnet.bn, 'batch_normalization_train'):\n@@ -462,8 +462,8 @@ def normalize_batch_in_training(x, gamma, beta,\n \n \n def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n-    '''Apply batch normalization on x given mean, var, beta and gamma.\n-    '''\n+    \"\"\"Apply batch normalization on x given mean, var, beta and gamma.\n+    \"\"\"\n     # TODO remove this if statement when Theano without\n     # T.nnet.bn.batch_normalization_test is deprecated\n     if not hasattr(T.nnet.bn, 'batch_normalization_test'):\n@@ -483,8 +483,8 @@ def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n # T.nnet.bn.batch_normalization_train is deprecated\n def _old_normalize_batch_in_training(x, gamma, beta,\n                                      reduction_axes, epsilon=1e-3):\n-    '''Computes mean and std for batch then apply batch_normalization on batch.\n-    '''\n+    \"\"\"Computes mean and std for batch then apply batch_normalization on batch.\n+    \"\"\"\n     dev = theano.config.device\n     use_cudnn = ndim(x) < 5 and reduction_axes == [0, 2, 3] and (dev.startswith('cuda') or dev.startswith('gpu'))\n     if use_cudnn:\n@@ -525,8 +525,8 @@ def _old_normalize_batch_in_training(x, gamma, beta,\n # TODO remove this if statement when Theano without\n # T.nnet.bn.batch_normalization_test is deprecated\n def _old_batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n-    '''Apply batch normalization on x given mean, var, beta and gamma.\n-    '''\n+    \"\"\"Apply batch normalization on x given mean, var, beta and gamma.\n+    \"\"\"\n     if mean.ndim == 1 and x.ndim > 1:\n         # in TensorFlow's batch_normalization, if the parameters are vectors\n         # the batch normalization should be applied along the rightmost axis.\n@@ -593,33 +593,33 @@ def reshape(x, shape):\n \n \n def permute_dimensions(x, pattern):\n-    '''Transpose dimensions.\n+    \"\"\"Transpose dimensions.\n \n     pattern should be a tuple or list of\n     dimension indices, e.g. [0, 2, 1].\n-    '''\n+    \"\"\"\n     # TODO: `keras_shape` inference.\n     pattern = tuple(pattern)\n     return x.dimshuffle(pattern)\n \n \n def repeat_elements(x, rep, axis):\n-    '''Repeat the elements of a tensor along an axis, like np.repeat.\n+    \"\"\"Repeat the elements of a tensor along an axis, like np.repeat.\n \n     If x has shape (s1, s2, s3) and axis=1, the output\n     will have shape (s1, s2 * rep, s3).\n-    '''\n+    \"\"\"\n     # TODO: `keras_shape` inference.\n     return T.repeat(x, rep, axis=axis)\n \n \n def resize_images(X, height_factor, width_factor, dim_ordering):\n-    '''Resize the images contained in a 4D tensor of shape\n+    \"\"\"Resize the images contained in a 4D tensor of shape\n     - [batch, channels, height, width] (for 'th' dim_ordering)\n     - [batch, height, width, channels] (for 'tf' dim_ordering)\n     by a factor of (height_factor, width_factor). Both factors should be\n     positive integers.\n-    '''\n+    \"\"\"\n     # TODO: `keras_shape` inference.\n     if dim_ordering == 'th':\n         output = repeat_elements(X, height_factor, axis=2)\n@@ -634,12 +634,12 @@ def resize_images(X, height_factor, width_factor, dim_ordering):\n \n \n def resize_volumes(X, depth_factor, height_factor, width_factor, dim_ordering):\n-    '''Resize the volume contained in a 5D tensor of shape\n+    \"\"\"Resize the volume contained in a 5D tensor of shape\n     - [batch, channels, depth, height, width] (for 'th' dim_ordering)\n     - [batch, depth, height, width, channels] (for 'tf' dim_ordering)\n     by a factor of (depth_factor, height_factor, width_factor).\n     Both factors should be positive integers.\n-    '''\n+    \"\"\"\n     # TODO: `keras_shape` inference.\n     if dim_ordering == 'th':\n         output = repeat_elements(X, depth_factor, axis=2)\n@@ -656,11 +656,11 @@ def resize_volumes(X, depth_factor, height_factor, width_factor, dim_ordering):\n \n \n def repeat(x, n):\n-    '''Repeat a 2D tensor.\n+    \"\"\"Repeat a 2D tensor.\n \n     If x has shape (samples, dim) and n=2,\n     the output will have shape (samples, 2, dim).\n-    '''\n+    \"\"\"\n     # TODO: `keras_shape` inference.\n     assert x.ndim == 2\n     x = x.dimshuffle((0, 'x', 1))\n@@ -668,7 +668,7 @@ def repeat(x, n):\n \n \n def arange(start, stop=None, step=1, dtype='int32'):\n-    '''Creates a 1-D tensor containing a sequence of integers.\n+    \"\"\"Creates a 1-D tensor containing a sequence of integers.\n \n     The function arguments use the same convention as\n     Theano's arange: if only one argument is provided,\n@@ -676,7 +676,7 @@ def arange(start, stop=None, step=1, dtype='int32'):\n \n     The default type of the returned tensor is 'int32' to\n     match TensorFlow's default.\n-    '''\n+    \"\"\"\n     return T.arange(start, stop=stop, step=step, dtype=dtype)\n \n \n@@ -691,17 +691,17 @@ def flatten(x):\n \n \n def batch_flatten(x):\n-    '''Turn a n-D tensor into a 2D tensor where\n+    \"\"\"Turn a n-D tensor into a 2D tensor where\n     the first dimension is conserved.\n-    '''\n+    \"\"\"\n     # TODO: `keras_shape` inference.\n     x = T.reshape(x, (x.shape[0], T.prod(x.shape) // x.shape[0]))\n     return x\n \n \n def expand_dims(x, dim=-1):\n-    '''Add a 1-sized dimension at index \"dim\".\n-    '''\n+    \"\"\"Add a 1-sized dimension at index \"dim\".\n+    \"\"\"\n     # TODO: `keras_shape` inference.\n     pattern = [i for i in range(x.type.ndim)]\n     if dim < 0:\n@@ -714,8 +714,8 @@ def expand_dims(x, dim=-1):\n \n \n def squeeze(x, axis):\n-    '''Remove a 1-dimension from the tensor at index \"axis\".\n-    '''\n+    \"\"\"Remove a 1-dimension from the tensor at index \"axis\".\n+    \"\"\"\n     # TODO: `keras_shape` inference.\n     shape = list(x.shape)\n     shape.pop(axis)\n@@ -723,12 +723,12 @@ def squeeze(x, axis):\n \n \n def temporal_padding(x, padding=1):\n-    '''Pad the middle dimension of a 3D tensor\n+    \"\"\"Pad the middle dimension of a 3D tensor\n     with \"padding\" zeros left and right.\n \n     Apologies for the inane API, but Theano makes this\n     really hard.\n-    '''\n+    \"\"\"\n     # TODO: `keras_shape` inference.\n     input_shape = x.shape\n     output_shape = (input_shape[0],\n@@ -739,12 +739,12 @@ def temporal_padding(x, padding=1):\n \n \n def asymmetric_temporal_padding(x, left_pad=1, right_pad=1):\n-    '''Pad the middle dimension of a 3D tensor\n+    \"\"\"Pad the middle dimension of a 3D tensor\n     with \"left_pad\" zeros left and \"right_pad\" right.\n \n     Apologies for the inane API, but Theano makes this\n     really hard.\n-    '''\n+    \"\"\"\n     # TODO: `keras_shape` inference.\n     input_shape = x.shape\n     output_shape = (input_shape[0],\n@@ -755,9 +755,9 @@ def asymmetric_temporal_padding(x, left_pad=1, right_pad=1):\n \n \n def spatial_2d_padding(x, padding=(1, 1), dim_ordering='default'):\n-    '''Pad the 2nd and 3rd dimensions of a 4D tensor\n+    \"\"\"Pad the 2nd and 3rd dimensions of a 4D tensor\n     with \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n-    '''\n+    \"\"\"\n     # TODO: `keras_shape` inference.\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n@@ -794,10 +794,10 @@ def spatial_2d_padding(x, padding=(1, 1), dim_ordering='default'):\n def asymmetric_spatial_2d_padding(x, top_pad=1, bottom_pad=1,\n                                   left_pad=1, right_pad=1,\n                                   dim_ordering='default'):\n-    '''Pad the rows and columns of a 4D tensor\n+    \"\"\"Pad the rows and columns of a 4D tensor\n     with \"top_pad\", \"bottom_pad\", \"left_pad\", \"right_pad\" (resp.) zeros\n     rows on top, bottom; cols on left, right.\n-    '''\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n     if dim_ordering not in {'th', 'tf'}:\n@@ -832,9 +832,9 @@ def asymmetric_spatial_2d_padding(x, top_pad=1, bottom_pad=1,\n \n \n def spatial_3d_padding(x, padding=(1, 1, 1), dim_ordering='default'):\n-    '''Pad the 2nd, 3rd and 4th dimensions of a 5D tensor\n+    \"\"\"Pad the 2nd, 3rd and 4th dimensions of a 5D tensor\n     with \"padding[0]\", \"padding[1]\" and \"padding[2]\" (resp.) zeros left and right.\n-    '''\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n     if dim_ordering not in {'th', 'tf'}:\n@@ -876,10 +876,10 @@ def stack(x):\n \n \n def one_hot(indices, nb_classes):\n-    '''Input: nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1))\n+    \"\"\"Input: nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1))\n     Output: (n + 1)D one hot representation of the input\n     with shape (batch_size, dim1, dim2, ... dim(n-1), nb_classes)\n-    '''\n+    \"\"\"\n     input_shape = tuple((indices.shape[i] for i in range(indices.ndim)))\n     indices = T.flatten(indices)\n     oh = T.extra_ops.to_one_hot(indices, nb_classes)\n@@ -888,8 +888,8 @@ def one_hot(indices, nb_classes):\n \n \n def reverse(x, axes):\n-    '''Reverse a tensor along the the specified axes\n-    '''\n+    \"\"\"Reverse a tensor along the the specified axes\n+    \"\"\"\n     if isinstance(axes, int):\n         axes = [axes]\n     slices = [slice(None, None, -1) if i in axes else slice(None, None, None) for i in range(x.ndim)]\n@@ -910,9 +910,9 @@ def get_value(x):\n \n \n def batch_get_value(xs):\n-    '''Returns the value of more than one tensor variable,\n+    \"\"\"Returns the value of more than one tensor variable,\n     as a list of Numpy arrays.\n-    '''\n+    \"\"\"\n     return [get_value(x) for x in xs]\n \n \n@@ -930,9 +930,9 @@ def get_variable_shape(x):\n \n \n def print_tensor(x, message=''):\n-    '''Print the message and the tensor when evaluated and return the same\n+    \"\"\"Print the message and the tensor when evaluated and return the same\n     tensor.\n-    '''\n+    \"\"\"\n     p_op = Print(message)\n     return p_op(x)\n \n@@ -972,9 +972,9 @@ def gradients(loss, variables):\n \n \n def stop_gradient(variables):\n-    '''Returns `variables` but with zero gradient with respect to every other\n+    \"\"\"Returns `variables` but with zero gradient with respect to every other\n     variables.\n-    '''\n+    \"\"\"\n     return theano.gradient.disconnected_grad(variables)\n \n \n@@ -983,7 +983,7 @@ def stop_gradient(variables):\n def rnn(step_function, inputs, initial_states,\n         go_backwards=False, mask=None, constants=None,\n         unroll=False, input_length=None):\n-    '''Iterates over the time dimension of a tensor.\n+    \"\"\"Iterates over the time dimension of a tensor.\n \n     # Arguments\n         inputs: tensor of temporal data of shape (samples, time, ...)\n@@ -1017,7 +1017,7 @@ def rnn(step_function, inputs, initial_states,\n                 at time t for sample s.\n             new_states: list of tensors, latest states returned by\n                 the step function, of shape (samples, ...).\n-    '''\n+    \"\"\"\n     ndim = inputs.ndim\n     assert ndim >= 3, 'Input should be at least 3D.'\n \n@@ -1145,8 +1145,8 @@ def rnn(step_function, inputs, initial_states,\n \n \n def switch(condition, then_expression, else_expression):\n-    '''condition: scalar tensor.\n-    '''\n+    \"\"\"condition: scalar tensor.\n+    \"\"\"\n     return T.switch(condition, then_expression, else_expression)\n \n \n@@ -1252,7 +1252,7 @@ def tanh(x):\n \n \n def dropout(x, level, noise_shape=None, seed=None):\n-    '''Sets entries in `x` to zero at random,\n+    \"\"\"Sets entries in `x` to zero at random,\n     while scaling the entire tensor.\n \n     # Arguments\n@@ -1262,7 +1262,7 @@ def dropout(x, level, noise_shape=None, seed=None):\n         noise_shape: shape for randomly generated keep/drop flags,\n             must be broadcastable to the shape of `x`\n         seed: random seed to ensure determinism.\n-    '''\n+    \"\"\"\n     if level < 0. or level >= 1:\n         raise ValueError('Dropout level must be in interval [0, 1[.')\n     if seed is None:\n@@ -1288,7 +1288,7 @@ def l2_normalize(x, axis):\n \n \n def in_top_k(predictions, targets, k):\n-    '''Returns whether the `targets` are in the top `k` `predictions`\n+    \"\"\"Returns whether the `targets` are in the top `k` `predictions`\n \n     # Arguments\n         predictions: A tensor of shape batch_size x classess and type float32.\n@@ -1298,7 +1298,7 @@ def in_top_k(predictions, targets, k):\n     # Returns\n         A tensor of shape batch_size and type int. output_i is 1 if\n         targets_i is within top-k values of predictions_i\n-    '''\n+    \"\"\"\n     predictions_top_k = T.argsort(predictions)[:, -k:]\n     result, _ = theano.map(lambda prediction, target: any(equal(prediction, target)), sequences=[predictions_top_k, targets])\n     return result\n@@ -1448,20 +1448,20 @@ def _postprocess_conv3d_output(conv_out, x, border_mode, np_kernel, strides, dim\n \n def conv1d(x, kernel, stride=1, border_mode='valid',\n            image_shape=None, filter_shape=None):\n-    '''1D convolution.\n+    \"\"\"1D convolution.\n \n     # Arguments\n         kernel: kernel tensor.\n         strides: stride integer.\n         border_mode: string, \"same\" or \"valid\".\n-    '''\n+    \"\"\"\n     raise NotImplementedError\n \n \n def conv2d(x, kernel, strides=(1, 1), border_mode='valid',\n            dim_ordering='default', image_shape=None,\n            filter_shape=None, filter_dilation=(1, 1)):\n-    '''2D convolution.\n+    \"\"\"2D convolution.\n \n     # Arguments\n         kernel: kernel tensor.\n@@ -1470,7 +1470,7 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid',\n         dim_ordering: \"tf\" or \"th\".\n             Whether to use Theano or TensorFlow dimension ordering\n         in inputs/kernels/ouputs.\n-    '''\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n     if dim_ordering not in {'th', 'tf'}:\n@@ -1507,7 +1507,7 @@ def deconv2d(x, kernel, output_shape, strides=(1, 1),\n              border_mode='valid',\n              dim_ordering='default',\n              image_shape=None, filter_shape=None):\n-    '''2D deconvolution (transposed convolution).\n+    \"\"\"2D deconvolution (transposed convolution).\n \n     # Arguments\n         kernel: kernel tensor.\n@@ -1517,7 +1517,7 @@ def deconv2d(x, kernel, output_shape, strides=(1, 1),\n         dim_ordering: \"tf\" or \"th\".\n             Whether to use Theano or TensorFlow dimension ordering\n         in inputs/kernels/ouputs.\n-    '''\n+    \"\"\"\n     flip_filters = False\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n@@ -1560,7 +1560,7 @@ def conv3d(x, kernel, strides=(1, 1, 1),\n            border_mode='valid', dim_ordering='default',\n            volume_shape=None, filter_shape=None,\n            filter_dilation=(1, 1, 1)):\n-    '''3D convolution.\n+    \"\"\"3D convolution.\n \n     # Arguments\n         kernel: kernel tensor.\n@@ -1569,7 +1569,7 @@ def conv3d(x, kernel, strides=(1, 1, 1),\n         dim_ordering: \"tf\" or \"th\".\n             Whether to use Theano or TensorFlow dimension ordering\n         in inputs/kernels/ouputs.\n-    '''\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n     if dim_ordering not in {'th', 'tf'}:\n@@ -1607,10 +1607,10 @@ def conv3d(x, kernel, strides=(1, 1, 1),\n def _old_theano_conv3d(x, kernel, strides=(1, 1, 1),\n                        border_mode='valid', dim_ordering='default',\n                        volume_shape=None, filter_shape=None):\n-    '''\n+    \"\"\"\n     Run on cuDNN if available.\n     border_mode: string, \"same\" or \"valid\".\n-    '''\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n     if dim_ordering not in {'th', 'tf'}:\n@@ -1979,7 +1979,7 @@ def ctc_cost(predict, Y):\n \n # batchifies original CTC code\n def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n-    '''Runs CTC loss algorithm on each batch element.\n+    \"\"\"Runs CTC loss algorithm on each batch element.\n \n     # Arguments\n         y_true: tensor (samples, max_string_length) containing the truth labels\n@@ -1993,7 +1993,7 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n     # Returns\n         Tensor with shape (samples,1) containing the\n             CTC loss of each element\n-    '''\n+    \"\"\"\n \n     def ctc_step(y_true_step, y_pred_step, input_length_step, label_length_step):\n         y_pred_step = y_pred_step[0: input_length_step[0]]\n@@ -2013,7 +2013,7 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n # HIGH ORDER FUNCTIONS\n \n def map_fn(fn, elems, name=None):\n-    '''Map the function fn over the elements elems and return the outputs.\n+    \"\"\"Map the function fn over the elements elems and return the outputs.\n \n     # Arguments\n         fn: Callable that will be called upon each element in elems\n@@ -2023,12 +2023,12 @@ def map_fn(fn, elems, name=None):\n     # Returns\n         Tensor with first dimension equal to the elems and second depending on\n         fn\n-    '''\n+    \"\"\"\n     return theano.map(fn, elems, name=name)[0]\n \n \n def foldl(fn, elems, initializer=None, name=None):\n-    '''Reduce elems using fn to combine them from left to right.\n+    \"\"\"Reduce elems using fn to combine them from left to right.\n \n     # Arguments\n         fn: Callable that will be called upon each element in elems and an\n@@ -2039,7 +2039,7 @@ def foldl(fn, elems, initializer=None, name=None):\n \n     # Returns\n         Same type and shape as initializer\n-    '''\n+    \"\"\"\n     if initializer is None:\n         initializer = elems[0]\n         elems = elems[1:]\n@@ -2052,7 +2052,7 @@ def foldl(fn, elems, initializer=None, name=None):\n \n \n def foldr(fn, elems, initializer=None, name=None):\n-    '''Reduce elems using fn to combine them from right to left.\n+    \"\"\"Reduce elems using fn to combine them from right to left.\n \n     # Arguments\n         fn: Callable that will be called upon each element in elems and an\n@@ -2063,7 +2063,7 @@ def foldr(fn, elems, initializer=None, name=None):\n \n     # Returns\n         Same type and shape as initializer\n-    '''\n+    \"\"\"\n     if initializer is None:\n         initializer = elems[-1]\n         elems = elems[:-1]\n\n@@ -10,7 +10,7 @@ import sys\n def load_data(path='imdb_full.pkl', nb_words=None, skip_top=0,\n               maxlen=None, seed=113,\n               start_char=1, oov_char=2, index_from=3):\n-    '''Loads IMDB dataset.\n+    \"\"\"Loads IMDB dataset.\n \n     # Arguments\n         path: where to store the data (in `/.keras/dataset`)\n@@ -32,7 +32,7 @@ def load_data(path='imdb_full.pkl', nb_words=None, skip_top=0,\n     because they're not making the `nb_words` cut here.\n     Words that were not seen in the trining set but are in the test set\n     have simply been skipped.\n-    '''\n+    \"\"\"\n     path = get_file(path,\n                     origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n                     md5_hash='d091312047c43cf9e4e38fef92437263')\n\n@@ -10,7 +10,7 @@ import sys\n def load_data(path='reuters.pkl', nb_words=None, skip_top=0,\n               maxlen=None, test_split=0.2, seed=113,\n               start_char=1, oov_char=2, index_from=3):\n-    '''Loads the Reuters newswire classification dataset.\n+    \"\"\"Loads the Reuters newswire classification dataset.\n \n     # Arguments\n         path: where to store the data (in `/.keras/dataset`)\n@@ -33,7 +33,7 @@ def load_data(path='reuters.pkl', nb_words=None, skip_top=0,\n     because they're not making the `nb_words` cut here.\n     Words that were not seen in the trining set but are in the test set\n     have simply been skipped.\n-    '''\n+    \"\"\"\n \n     path = get_file(path, origin='https://s3.amazonaws.com/text-datasets/reuters.pkl')\n     f = open(path, 'rb')\n\n@@ -19,11 +19,11 @@ from ..utils.generic_utils import func_dump, func_load\n \n \n def to_list(x):\n-    '''This normalizes a list/tensor into a list.\n+    \"\"\"This normalizes a list/tensor into a list.\n \n     If a tensor is passed, we return\n     a list of size 1 containing the tensor.\n-    '''\n+    \"\"\"\n     if isinstance(x, list):\n         return x\n     return [x]\n@@ -35,13 +35,13 @@ def object_list_uid(object_list):\n \n \n class InputSpec(object):\n-    '''This specifies the ndim, dtype and shape of every input to a layer.\n+    \"\"\"This specifies the ndim, dtype and shape of every input to a layer.\n     Every layer should expose (if appropriate) an `input_spec` attribute:\n     a list of instances of InputSpec (one per input tensor).\n \n     A None entry in a shape is compatible with any dimension,\n     a None shape is compatible with any shape.\n-    '''\n+    \"\"\"\n     def __init__(self, dtype=None, shape=None, ndim=None):\n         if isinstance(ndim, str):\n             if '+' not in ndim:\n@@ -60,7 +60,7 @@ class InputSpec(object):\n \n \n class Node(object):\n-    '''A `Node` describes the connectivity between two layers.\n+    \"\"\"A `Node` describes the connectivity between two layers.\n \n     Each time a layer is connected to some new input,\n     a node is added to `layer.inbound_nodes`.\n@@ -98,7 +98,7 @@ class Node(object):\n     A node from layer A to layer B is added to:\n         A.outbound_nodes\n         B.inbound_nodes\n-    '''\n+    \"\"\"\n     def __init__(self, outbound_layer,\n                  inbound_layers, node_indices, tensor_indices,\n                  input_tensors, output_tensors,\n@@ -214,7 +214,7 @@ class Node(object):\n \n \n class Layer(object):\n-    '''Abstract base layer class.\n+    \"\"\"Abstract base layer class.\n \n     # Properties\n         name: String, must be unique within a model.\n@@ -281,7 +281,7 @@ class Layer(object):\n         add_inbound_node(layer, index=0)\n         create_input_layer()\n         assert_input_compatibility()\n-    '''\n+    \"\"\"\n     def __init__(self, **kwargs):\n         # These properties should have been set\n         # by the child class, as appropriate.\n@@ -401,7 +401,7 @@ class Layer(object):\n                    trainable=True,\n                    regularizer=None,\n                    constraint=None):\n-        '''Adds a weight variable to the layer.\n+        \"\"\"Adds a weight variable to the layer.\n \n         # Arguments:\n             shape: The shape tuple of the weight.\n@@ -410,7 +410,7 @@ class Layer(object):\n                 be trained via backprop or not (assuming\n                 that the layer itself is also trainable).\n             regularizer: An optional Regularizer instance.\n-        '''\n+        \"\"\"\n         initializer = initializations.get(initializer)\n         weight = initializer(shape, name=name)\n         if regularizer is not None:\n@@ -424,10 +424,10 @@ class Layer(object):\n         return weight\n \n     def assert_input_compatibility(self, input):\n-        '''This checks that the tensor(s) `input`\n+        \"\"\"This checks that the tensor(s) `input`\n         verify the input assumptions of the layer\n         (if any). If not, exceptions are raised.\n-        '''\n+        \"\"\"\n         if not self.input_spec:\n             return True\n         if not isinstance(self.input_spec, list):\n@@ -490,7 +490,7 @@ class Layer(object):\n                                 str(x_shape))\n \n     def call(self, x, mask=None):\n-        '''This is where the layer's logic lives.\n+        \"\"\"This is where the layer's logic lives.\n \n         # Arguments\n             x: input tensor, or list/tuple of input tensors.\n@@ -498,11 +498,11 @@ class Layer(object):\n \n         # Returns:\n             A tensor or list/tuple of tensors.\n-        '''\n+        \"\"\"\n         return x\n \n     def __call__(self, x, mask=None):\n-        '''Wrapper around self.call(), for handling\n+        \"\"\"Wrapper around self.call(), for handling\n         internal Keras references.\n \n         If a Keras tensor is passed:\n@@ -519,7 +519,7 @@ class Layer(object):\n         # Arguments\n             x: Can be a tensor or list/tuple of tensors.\n             mask: Tensor or list/tuple of tensors.\n-        '''\n+        \"\"\"\n         if not self.built:\n             # Raise exceptions in case the input is not compatible\n             # with the input_spec specified in the layer constructor.\n@@ -587,7 +587,7 @@ class Layer(object):\n \n     def add_inbound_node(self, inbound_layers,\n                          node_indices=None, tensor_indices=None):\n-        '''\n+        \"\"\"\n         # Arguments\n             inbound_layers: Can be a layer instance\n                 or a list/tuple of layer instances.\n@@ -604,7 +604,7 @@ class Layer(object):\n                 the entry in the output list\n                 (if applicable). \"None\" means that we take all outputs\n                 (as a list).\n-        '''\n+        \"\"\"\n         inbound_layers = to_list(inbound_layers)\n         if not node_indices:\n             node_indices = [0 for _ in range(len(inbound_layers))]\n@@ -632,7 +632,7 @@ class Layer(object):\n         Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n \n     def get_output_shape_for(self, input_shape):\n-        '''Computes the output shape of the layer given\n+        \"\"\"Computes the output shape of the layer given\n         an input shape (assumes that the layer will be built\n         to match that input shape).\n \n@@ -641,11 +641,11 @@ class Layer(object):\n                 or list of shape tuples (one per output tensor of the layer).\n                 Shape tuples can include None for free dimensions,\n                 instead of an integer.\n-        '''\n+        \"\"\"\n         return input_shape\n \n     def compute_mask(self, input, input_mask=None):\n-        '''Computes an output masking tensor, given an input tensor\n+        \"\"\"Computes an output masking tensor, given an input tensor\n         (or list thereof) and an input mask (or list thereof).\n \n         # Arguments\n@@ -655,7 +655,7 @@ class Layer(object):\n         # Returns\n             None or a tensor (or list of tensors,\n                 one per output tensor of the layer).\n-        '''\n+        \"\"\"\n         if not hasattr(self, 'supports_masking') or not self.supports_masking:\n             if input_mask is not None:\n                 if isinstance(input_mask, list):\n@@ -676,25 +676,25 @@ class Layer(object):\n         return input_mask\n \n     def build(self, input_shape):\n-        '''Creates the layer weights.\n+        \"\"\"Creates the layer weights.\n         Must be implemented on all layers that have weights.\n \n         # Arguments\n             input_shape: Keras tensor (future input to layer)\n                 or list/tuple of Keras tensors to reference\n                 for weight shape computations.\n-        '''\n+        \"\"\"\n         self.built = True\n \n     def _get_node_attribute_at_index(self, node_index, attr, attr_name):\n-        '''Retrieves an attribute (e.g. input_tensors) from a node.\n+        \"\"\"Retrieves an attribute (e.g. input_tensors) from a node.\n \n         # Arguments\n             node_index: Integer index of the node from which\n                 to retrieve the attribute.\n             attr: Exact node attribute name.\n             attr_name: Human-readable attribute name, for error messages.\n-        '''\n+        \"\"\"\n         if not self.inbound_nodes:\n             raise RuntimeError('The layer has never been called '\n                                'and thus has no defined ' + attr_name + '.')\n@@ -710,53 +710,53 @@ class Layer(object):\n             return values\n \n     def get_input_shape_at(self, node_index):\n-        '''Retrieves the input shape(s) of a layer at a given node.\n-        '''\n+        \"\"\"Retrieves the input shape(s) of a layer at a given node.\n+        \"\"\"\n         return self._get_node_attribute_at_index(node_index,\n                                                  'input_shapes',\n                                                  'input shape')\n \n     def get_output_shape_at(self, node_index):\n-        '''Retrieves the output shape(s) of a layer at a given node.\n-        '''\n+        \"\"\"Retrieves the output shape(s) of a layer at a given node.\n+        \"\"\"\n         return self._get_node_attribute_at_index(node_index,\n                                                  'output_shapes',\n                                                  'output shape')\n \n     def get_input_at(self, node_index):\n-        '''Retrieves the input tensor(s) of a layer at a given node.\n-        '''\n+        \"\"\"Retrieves the input tensor(s) of a layer at a given node.\n+        \"\"\"\n         return self._get_node_attribute_at_index(node_index,\n                                                  'input_tensors',\n                                                  'input')\n \n     def get_output_at(self, node_index):\n-        '''Retrieves the output tensor(s) of a layer at a given node.\n-        '''\n+        \"\"\"Retrieves the output tensor(s) of a layer at a given node.\n+        \"\"\"\n         return self._get_node_attribute_at_index(node_index,\n                                                  'output_tensors',\n                                                  'output')\n \n     def get_input_mask_at(self, node_index):\n-        '''Retrieves the input mask tensor(s) of a layer at a given node.\n-        '''\n+        \"\"\"Retrieves the input mask tensor(s) of a layer at a given node.\n+        \"\"\"\n         return self._get_node_attribute_at_index(node_index,\n                                                  'input_masks',\n                                                  'input mask')\n \n     def get_output_mask_at(self, node_index):\n-        '''Retrieves the output mask tensor(s) of a layer at a given node.\n-        '''\n+        \"\"\"Retrieves the output mask tensor(s) of a layer at a given node.\n+        \"\"\"\n         return self._get_node_attribute_at_index(node_index,\n                                                  'output_masks',\n                                                  'output mask')\n \n     @property\n     def input(self):\n-        '''Retrieves the input tensor(s) of a layer (only applicable if\n+        \"\"\"Retrieves the input tensor(s) of a layer (only applicable if\n         the layer has exactly one inbound node, i.e. if it is connected\n         to one incoming layer).\n-        '''\n+        \"\"\"\n         if len(self.inbound_nodes) > 1:\n             raise AttributeError('Layer ' + self.name +\n                                  ' has multiple inbound nodes, '\n@@ -771,10 +771,10 @@ class Layer(object):\n \n     @property\n     def output(self):\n-        '''Retrieves the output tensor(s) of a layer (only applicable if\n+        \"\"\"Retrieves the output tensor(s) of a layer (only applicable if\n         the layer has exactly one inbound node, i.e. if it is connected\n         to one incoming layer).\n-        '''\n+        \"\"\"\n         if len(self.inbound_nodes) == 0:\n             raise AttributeError('Layer ' + self.name +\n                                  ' has no inbound nodes.')\n@@ -789,10 +789,10 @@ class Layer(object):\n \n     @property\n     def input_mask(self):\n-        '''Retrieves the input mask tensor(s) of a layer (only applicable if\n+        \"\"\"Retrieves the input mask tensor(s) of a layer (only applicable if\n         the layer has exactly one inbound node, i.e. if it is connected\n         to one incoming layer).\n-        '''\n+        \"\"\"\n         if len(self.inbound_nodes) != 1:\n             raise AttributeError('Layer ' + self.name +\n                                  ' has multiple inbound nodes, ' +\n@@ -804,10 +804,10 @@ class Layer(object):\n \n     @property\n     def output_mask(self):\n-        '''Retrieves the output mask tensor(s) of a layer (only applicable if\n+        \"\"\"Retrieves the output mask tensor(s) of a layer (only applicable if\n         the layer has exactly one inbound node, i.e. if it is connected\n         to one incoming layer).\n-        '''\n+        \"\"\"\n         if len(self.inbound_nodes) != 1:\n             raise AttributeError('Layer ' + self.name +\n                                  ' has multiple inbound nodes, '\n@@ -820,10 +820,10 @@ class Layer(object):\n \n     @property\n     def input_shape(self):\n-        '''Retrieves the input shape tuple(s) of a layer. Only applicable\n+        \"\"\"Retrieves the input shape tuple(s) of a layer. Only applicable\n         if the layer has one inbound node,\n         or if all inbound nodes have the same input shape.\n-        '''\n+        \"\"\"\n         if not self.inbound_nodes:\n             raise AttributeError('The layer has never been called '\n                                  'and thus has no defined input shape.')\n@@ -845,10 +845,10 @@ class Layer(object):\n \n     @property\n     def output_shape(self):\n-        '''Retrieves the output shape tuple(s) of a layer. Only applicable\n+        \"\"\"Retrieves the output shape tuple(s) of a layer. Only applicable\n         if the layer has one inbound node,\n         or if all inbound nodes have the same output shape.\n-        '''\n+        \"\"\"\n         if not self.inbound_nodes:\n             raise AttributeError('The layer has never been called '\n                                  'and thus has no defined output shape.')\n@@ -951,7 +951,7 @@ class Layer(object):\n         return self.trainable_weights + self.non_trainable_weights\n \n     def set_weights(self, weights):\n-        '''Sets the weights of the layer, from Numpy arrays.\n+        \"\"\"Sets the weights of the layer, from Numpy arrays.\n \n         # Arguments\n             weights: a list of Numpy arrays. The number\n@@ -959,7 +959,7 @@ class Layer(object):\n                 number of the dimensions of the weights\n                 of the layer (i.e. it should match the\n                 output of `get_weights`).\n-        '''\n+        \"\"\"\n         params = self.weights\n         if len(params) != len(weights):\n             raise ValueError('You called `set_weights(weights)` on layer \"' +\n@@ -984,14 +984,14 @@ class Layer(object):\n         K.batch_set_value(weight_value_tuples)\n \n     def get_weights(self):\n-        '''Returns the current weights of the layer,\n+        \"\"\"Returns the current weights of the layer,\n         as a list of numpy arrays.\n-        '''\n+        \"\"\"\n         params = self.weights\n         return K.batch_get_value(params)\n \n     def get_config(self):\n-        '''Returns a Python dictionary (serializable)\n+        \"\"\"Returns a Python dictionary (serializable)\n         containing the configuration of a layer.\n         The same layer can be reinstantiated later\n         (without its trained weights) from this configuration.\n@@ -999,7 +999,7 @@ class Layer(object):\n         The config of a layer does not include connectivity\n         information, nor the layer class name. These are handled\n         by Container (one layer of abstraction above).\n-        '''\n+        \"\"\"\n         config = {'name': self.name,\n                   'trainable': self.trainable}\n         if hasattr(self, 'batch_input_shape'):\n@@ -1010,7 +1010,7 @@ class Layer(object):\n \n     @classmethod\n     def from_config(cls, config):\n-        '''This method is the reverse of get_config,\n+        \"\"\"This method is the reverse of get_config,\n         capable of instantiating the same layer from the config\n         dictionary. It does not handle layer connectivity\n         (handled by Container), nor weights (handled by `set_weights`).\n@@ -1018,13 +1018,13 @@ class Layer(object):\n         # Arguments\n             config: A Python dictionary, typically the\n                 output of get_config.\n-        '''\n+        \"\"\"\n         return cls(**config)\n \n     def count_params(self):\n-        '''Returns the total number of floats (or ints)\n+        \"\"\"Returns the total number of floats (or ints)\n         composing the weights of the layer.\n-        '''\n+        \"\"\"\n         if not self.built:\n             if self.__class__.__name__ == 'Sequential':\n                 self.build()\n@@ -1037,7 +1037,7 @@ class Layer(object):\n \n \n class InputLayer(Layer):\n-    '''Layer to be used as an entry point into a graph.\n+    \"\"\"Layer to be used as an entry point into a graph.\n     It can either wrap an existing tensor (pass an `input_tensor` argument)\n     or create its a placeholder tensor (pass arguments `input_shape`\n     or `batch_input_shape` as well as `input_dtype`).\n@@ -1051,7 +1051,7 @@ class InputLayer(Layer):\n         sparse: Boolean, whether the placeholder created\n             is meant to be sparse.\n         name: Name of the layer (string).\n-    '''\n+    \"\"\"\n     def __init__(self, input_shape=None, batch_input_shape=None,\n                  input_dtype=None, input_tensor=None, sparse=False, name=None):\n         self.input_spec = None\n@@ -1138,7 +1138,7 @@ class InputLayer(Layer):\n def Input(shape=None, batch_shape=None,\n           name=None, dtype=K.floatx(), sparse=False,\n           tensor=None):\n-    '''`Input()` is used to instantiate a Keras tensor.\n+    \"\"\"`Input()` is used to instantiate a Keras tensor.\n     A Keras tensor is a tensor object from the underlying backend\n     (Theano or TensorFlow), which we augment with certain\n     attributes that allow us to build a Keras model\n@@ -1180,7 +1180,7 @@ def Input(shape=None, batch_shape=None,\n         b = Dense(16, activation='softmax')(a)\n         model = Model(input=a, output=b)\n         ```\n-    '''\n+    \"\"\"\n     if not batch_shape and tensor is None:\n         assert shape, ('Please provide to Input either a `shape`'\n                        ' or a `batch_shape` argument. Note that '\n@@ -1202,7 +1202,7 @@ def Input(shape=None, batch_shape=None,\n \n \n class Merge(Layer):\n-    '''A `Merge` layer can be used to merge a list of tensors\n+    \"\"\"A `Merge` layer can be used to merge a list of tensors\n     into a single tensor, following some merge `mode`.\n \n     # Example\n@@ -1252,7 +1252,7 @@ class Merge(Layer):\n         output_mask: Mask or lambda/function to compute the output mask (only\n             if merge mode is a lambda/function). If the latter case, it should\n             take as input a list of masks and return a single mask.\n-    '''\n+    \"\"\"\n     def __init__(self, layers=None, mode='sum', concat_axis=-1,\n                  dot_axes=-1, output_shape=None, output_mask=None,\n                  arguments=None, node_indices=None, tensor_indices=None,\n@@ -1299,9 +1299,9 @@ class Merge(Layer):\n \n     def _arguments_validation(self, layers, mode, concat_axis, dot_axes,\n                               node_indices, tensor_indices):\n-        '''Validates user-passed arguments and raises exceptions\n+        \"\"\"Validates user-passed arguments and raises exceptions\n         as appropriate.\n-        '''\n+        \"\"\"\n         if not callable(mode):\n             if mode not in {'sum', 'mul', 'concat', 'ave', 'cos', 'dot', 'max'}:\n                 raise ValueError('Invalid merge mode: ' + str(mode))\n@@ -1417,11 +1417,11 @@ class Merge(Layer):\n             raise ValueError('Unknown merge mode.')\n \n     def __call__(self, inputs, mask=None):\n-        '''We disable successive calls to __call__ for Merge layers.\n+        \"\"\"We disable successive calls to __call__ for Merge layers.\n         Although there is no technical obstacle to\n         making it possible to __call__ a Merge instance many times\n         (it is just a layer), it would make for a rather inelegant API.\n-        '''\n+        \"\"\"\n         if not isinstance(inputs, list):\n             raise TypeError('Merge can only be called on a list of tensors, '\n                             'not a single tensor. Received: ' + str(inputs))\n@@ -1617,7 +1617,7 @@ class Merge(Layer):\n def merge(inputs, mode='sum', concat_axis=-1,\n           dot_axes=-1, output_shape=None, output_mask=None,\n           arguments=None, name=None):\n-    '''Functional merge, to apply to Keras tensors (NOT layers).\n+    \"\"\"Functional merge, to apply to Keras tensors (NOT layers).\n     Returns a Keras tensor.\n \n     # Example\n@@ -1649,7 +1649,7 @@ def merge(inputs, mode='sum', concat_axis=-1,\n         tensor_indices: Optional list of indices of output tensors\n             to consider for merging\n             (in case some input layer node returns multiple tensors).\n-    '''\n+    \"\"\"\n     all_keras_tensors = True\n     for x in inputs:\n         if not hasattr(x, '_keras_history'):\n@@ -1686,7 +1686,7 @@ def merge(inputs, mode='sum', concat_axis=-1,\n \n \n class Container(Layer):\n-    '''A Container is a directed acyclic graph of layers.\n+    \"\"\"A Container is a directed acyclic graph of layers.\n \n     It is the topological form of a \"model\". A Model\n     is simply a Container with added training routines.\n@@ -1721,7 +1721,7 @@ class Container(Layer):\n \n     # Class Methods\n         from_config\n-    '''\n+    \"\"\"\n     def __init__(self, input, output, name=None):\n         # Handle name argument.\n         if not name:\n@@ -1871,7 +1871,7 @@ class Container(Layer):\n \n         def build_map_of_graph(tensor, seen_nodes=set(), depth=0,\n                                layer=None, node_index=None, tensor_index=None):\n-            '''This recursively updates the maps nodes_depths,\n+            \"\"\"This recursively updates the maps nodes_depths,\n             layers_depths and the set container_nodes.\n             Does not try to detect cycles in graph (TODO?)\n \n@@ -1884,7 +1884,7 @@ class Container(Layer):\n                     will be obtained from `tensor._keras_history`.\n                 node_index: Node index from which `tensor` comes from.\n                 tensor_index: Tensor_index from which `tensor` comes from.\n-            '''\n+            \"\"\"\n             if not layer or node_index is None or tensor_index is None:\n                 layer, node_index, tensor_index = tensor._keras_history\n             node = layer.inbound_nodes[node_index]\n@@ -2030,7 +2030,7 @@ class Container(Layer):\n         # self.input_spec\n \n     def get_layer(self, name=None, index=None):\n-        '''Returns a layer based on either its name (unique)\n+        \"\"\"Returns a layer based on either its name (unique)\n         or its index in the graph. Indices are based on\n         order of horizontal graph traversal (bottom-up).\n \n@@ -2040,7 +2040,7 @@ class Container(Layer):\n \n         # Returns\n             A layer instance.\n-        '''\n+        \"\"\"\n         # It would be unreliable to build a dictionary\n         # based on layer names, because names can potentially\n         # be changed at any point by the user\n@@ -2112,11 +2112,11 @@ class Container(Layer):\n \n     @property\n     def state_updates(self):\n-        '''Returns the `updates` from all layers that are\n+        \"\"\"Returns the `updates` from all layers that are\n         stateful.  This is useful for separating training updates and\n         state updates, e.g. when we need to update a layer's internal state\n         during prediction.\n-        '''\n+        \"\"\"\n         state_updates = []\n         for layer in self.layers:\n             if getattr(layer, 'stateful', False):\n@@ -2167,20 +2167,20 @@ class Container(Layer):\n         return weights\n \n     def get_weights(self):\n-        '''Returns the weights of the model,\n+        \"\"\"Returns the weights of the model,\n         as a flat list of Numpy arrays.\n-        '''\n+        \"\"\"\n         weights = []\n         for layer in self.layers:\n             weights += layer.weights\n         return K.batch_get_value(weights)\n \n     def set_weights(self, weights):\n-        '''Sets the weights of the model.\n+        \"\"\"Sets the weights of the model.\n         The `weights` argument should be a list\n         of Numpy arrays with shapes and types matching\n         the output of `model.get_weights()`.\n-        '''\n+        \"\"\"\n         tuples = []\n         for layer in self.layers:\n             nb_param = len(layer.weights)\n@@ -2208,13 +2208,13 @@ class Container(Layer):\n \n     @property\n     def uses_learning_phase(self):\n-        '''True if any layer in the graph uses it.\n-        '''\n+        \"\"\"True if any layer in the graph uses it.\n+        \"\"\"\n         layers_learning_phase = any([layer.uses_learning_phase for layer in self.layers])\n         return layers_learning_phase\n \n     def call(self, input, mask=None):\n-        '''`call` just reapplies all ops in the graph to the new inputs\n+        \"\"\"`call` just reapplies all ops in the graph to the new inputs\n         (e.g. build a new computational graph from the provided inputs).\n \n         It is callable on non-Keras tensors.\n@@ -2227,7 +2227,7 @@ class Container(Layer):\n         # Returns\n             A tensor if there is a single output, or\n             a list of tensors if there are more than one outputs.\n-        '''\n+        \"\"\"\n         inputs = to_list(input)\n         if mask is None:\n             masks = [None for _ in range(len(inputs))]\n@@ -2334,7 +2334,7 @@ class Container(Layer):\n             return output_shapes\n \n     def run_internal_graph(self, inputs, masks=None):\n-        '''Computes output tensors for new inputs.\n+        \"\"\"Computes output tensors for new inputs.\n \n         # Note:\n             - Expects `inputs` to be a list (potentially with 1 element).\n@@ -2346,7 +2346,7 @@ class Container(Layer):\n \n         # Returns\n             Three lists: output_tensors, output_masks, output_shapes\n-        '''\n+        \"\"\"\n         if masks is None:\n             masks = [None for _ in range(len(inputs))]\n \n@@ -2536,8 +2536,8 @@ class Container(Layer):\n \n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n-        '''Instantiates a Model from its config (output of `get_config()`).\n-        '''\n+        \"\"\"Instantiates a Model from its config (output of `get_config()`).\n+        \"\"\"\n         from keras.utils.layer_utils import layer_from_config\n \n         # layer instances created during\n@@ -2593,7 +2593,7 @@ class Container(Layer):\n         return cls(input=input_tensors, output=output_tensors, name=name)\n \n     def save(self, filepath, overwrite=True):\n-        '''Save into a single HDF5 file:\n+        \"\"\"Save into a single HDF5 file:\n             - The model architecture, allowing to re-instantiate the model.\n             - The model weights.\n             - The state of the optimizer, allowing to resume training\n@@ -2619,12 +2619,12 @@ class Container(Layer):\n         # identical to the previous one\n         model = load_model('my_model.h5')\n         ```\n-        '''\n+        \"\"\"\n         from ..models import save_model\n         save_model(self, filepath, overwrite)\n \n     def save_weights(self, filepath, overwrite=True):\n-        '''Dumps all layer weights to a HDF5 file.\n+        \"\"\"Dumps all layer weights to a HDF5 file.\n \n         The weight file has:\n             - `layer_names` (attribute), a list of strings\n@@ -2635,7 +2635,7 @@ class Container(Layer):\n                     (ordered names of weights tensor of the layer).\n                 - For every weight in the layer, a dataset\n                     storing the weight value, named after the weight tensor.\n-        '''\n+        \"\"\"\n         import h5py\n         # If file exists and should not be overwritten:\n         if not overwrite and os.path.isfile(filepath):\n@@ -2678,7 +2678,7 @@ class Container(Layer):\n                     param_dset[:] = val\n \n     def load_weights(self, filepath, by_name=False):\n-        '''Loads all layer weights from a HDF5 save file.\n+        \"\"\"Loads all layer weights from a HDF5 save file.\n \n         If `by_name` is False (default) weights are loaded\n         based on the network's topology, meaning the architecture\n@@ -2691,7 +2691,7 @@ class Container(Layer):\n         only if they share the same name. This is useful\n         for fine-tuning or transfer-learning models where\n         some of the layers have changed.\n-        '''\n+        \"\"\"\n         import h5py\n         f = h5py.File(filepath, mode='r')\n         if 'layer_names' not in f.attrs and 'model_weights' in f:\n@@ -2705,12 +2705,12 @@ class Container(Layer):\n             f.close()\n \n     def load_weights_from_hdf5_group(self, f):\n-        '''Weight loading is based on layer order in a list\n+        \"\"\"Weight loading is based on layer order in a list\n         (matching model.flattened_layers for Sequential models,\n         and model.layers for Model class instances), not\n         on layer names.\n         Layers that have no weights are skipped.\n-        '''\n+        \"\"\"\n         if hasattr(self, 'flattened_layers'):\n             # Support for legacy Sequential/Merge behavior.\n             flattened_layers = self.flattened_layers\n@@ -2788,10 +2788,10 @@ class Container(Layer):\n             K.batch_set_value(weight_value_tuples)\n \n     def load_weights_from_hdf5_group_by_name(self, f):\n-        ''' Name-based weight loading\n+        \"\"\" Name-based weight loading\n         (instead of topological weight loading).\n         Layers that have no matching name are skipped.\n-        '''\n+        \"\"\"\n         if hasattr(self, 'flattened_layers'):\n             # Support for legacy Sequential/Merge behavior.\n             flattened_layers = self.flattened_layers\n@@ -2837,7 +2837,7 @@ class Container(Layer):\n             K.batch_set_value(weight_value_tuples)\n \n     def _updated_config(self):\n-        '''Shared between different serialization methods.'''\n+        \"\"\"Shared between different serialization methods.\"\"\"\n         from keras import __version__ as keras_version\n \n         config = self.get_config()\n@@ -2849,11 +2849,11 @@ class Container(Layer):\n         return model_config\n \n     def to_json(self, **kwargs):\n-        '''Returns a JSON string containing the network configuration.\n+        \"\"\"Returns a JSON string containing the network configuration.\n \n         To load a network from a JSON save file, use\n         `keras.models.model_from_json(json_string, custom_objects={})`.\n-        '''\n+        \"\"\"\n         import json\n \n         def get_json_type(obj):\n@@ -2871,7 +2871,7 @@ class Container(Layer):\n         return json.dumps(model_config, default=get_json_type, **kwargs)\n \n     def to_yaml(self, **kwargs):\n-        '''Returns a yaml string containing the network configuration.\n+        \"\"\"Returns a yaml string containing the network configuration.\n \n         To load a network from a yaml save file, use\n         `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n@@ -2879,7 +2879,7 @@ class Container(Layer):\n         `custom_objects` should be a dictionary mapping\n         the names of custom losses / layers / etc to the corresponding\n         functions / classes.\n-        '''\n+        \"\"\"\n         import yaml\n         return yaml.dump(self._updated_config(), **kwargs)\n \n@@ -2898,7 +2898,7 @@ class Container(Layer):\n \n \n def get_source_inputs(tensor, layer=None, node_index=None):\n-    '''Returns the list of input tensors\n+    \"\"\"Returns the list of input tensors\n     necessary to compute `tensor`.\n \n     Output will always be a list of tensors\n@@ -2909,7 +2909,7 @@ def get_source_inputs(tensor, layer=None, node_index=None):\n         layer: Origin layer of the tensor. Will be\n             determined via tensor._keras_history if not provided.\n         node_index: Origin node index of the tensor.\n-    '''\n+    \"\"\"\n     if not hasattr(tensor, '_keras_history'):\n         return tensor\n \n\n@@ -40,8 +40,8 @@ def normal(shape, scale=0.05, name=None):\n def lecun_uniform(shape, name=None, dim_ordering='th'):\n     \"\"\"LeCun uniform variance scaling initializer.\n \n-    # Reference\n-        LeCun 98, Efficient Backprop\n+    # References\n+        LeCun 98, Efficient Backprop,\n         http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n     \"\"\"\n     fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n@@ -52,7 +52,7 @@ def lecun_uniform(shape, name=None, dim_ordering='th'):\n def glorot_normal(shape, name=None, dim_ordering='th'):\n     \"\"\"Glorot normal variance scaling initializer.\n \n-    # Reference\n+    # References\n         Glorot & Bengio, AISTATS 2010\n     \"\"\"\n     fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n@@ -69,7 +69,7 @@ def glorot_uniform(shape, name=None, dim_ordering='th'):\n def he_normal(shape, name=None, dim_ordering='th'):\n     \"\"\"He normal variance scaling initializer.\n \n-    # Reference\n+    # References\n         He et al., http://arxiv.org/abs/1502.01852\n     \"\"\"\n     fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n@@ -88,7 +88,7 @@ def he_uniform(shape, name=None, dim_ordering='th'):\n def orthogonal(shape, scale=1.1, name=None):\n     \"\"\"Orthogonal initializer.\n \n-    # Reference\n+    # References\n         Saxe et al., http://arxiv.org/abs/1312.6120\n     \"\"\"\n     flat_shape = (shape[0], np.prod(shape[1:]))\n\n@@ -5,7 +5,7 @@ import numpy as np\n \n \n class LeakyReLU(Layer):\n-    '''Special version of a Rectified Linear Unit\n+    \"\"\"Special version of a Rectified Linear Unit\n     that allows a small gradient when the unit is not active:\n     `f(x) = alpha * x for x < 0`,\n     `f(x) = x for x >= 0`.\n@@ -23,7 +23,8 @@ class LeakyReLU(Layer):\n \n     # References\n         - [Rectifier Nonlinearities Improve Neural Network Acoustic Models](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, alpha=0.3, **kwargs):\n         self.supports_masking = True\n         self.alpha = alpha\n@@ -39,7 +40,7 @@ class LeakyReLU(Layer):\n \n \n class PReLU(Layer):\n-    '''Parametric Rectified Linear Unit:\n+    \"\"\"Parametric Rectified Linear Unit:\n     `f(x) = alphas * x for x < 0`,\n     `f(x) = x for x >= 0`,\n     where `alphas` is a learned array with the same shape as x.\n@@ -66,7 +67,8 @@ class PReLU(Layer):\n \n     # References\n         - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, init='zero', weights=None, shared_axes=None, **kwargs):\n         self.supports_masking = True\n         self.init = initializations.get(init)\n@@ -96,7 +98,8 @@ class PReLU(Layer):\n     def call(self, x, mask=None):\n         pos = K.relu(x)\n         if K.backend() == 'theano':\n-            neg = K.pattern_broadcast(self.alphas, self.param_broadcast) * (x - abs(x)) * 0.5\n+            neg = (K.pattern_broadcast(self.alphas, self.param_broadcast) *\n+                   (x - abs(x)) * 0.5)\n         else:\n             neg = self.alphas * (x - abs(x)) * 0.5\n         return pos + neg\n@@ -108,7 +111,7 @@ class PReLU(Layer):\n \n \n class ELU(Layer):\n-    '''Exponential Linear Unit:\n+    \"\"\"Exponential Linear Unit:\n     `f(x) =  alpha * (exp(x) - 1.) for x < 0`,\n     `f(x) = x for x >= 0`.\n \n@@ -125,7 +128,8 @@ class ELU(Layer):\n \n     # References\n         - [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](https://arxiv.org/abs/1511.07289v1)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, alpha=1.0, **kwargs):\n         self.supports_masking = True\n         self.alpha = K.cast_to_floatx(alpha)\n@@ -141,7 +145,7 @@ class ELU(Layer):\n \n \n class ParametricSoftplus(Layer):\n-    '''Parametric Softplus:\n+    \"\"\"Parametric Softplus:\n     `f(x) = alpha * log(1 + exp(beta * x))`\n \n     # Input shape\n@@ -167,7 +171,8 @@ class ParametricSoftplus(Layer):\n \n     # References\n         - [Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003143)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, alpha_init=0.2, beta_init=5.0,\n                  weights=None, shared_axes=None, **kwargs):\n         self.supports_masking = True\n@@ -200,7 +205,9 @@ class ParametricSoftplus(Layer):\n \n     def call(self, x, mask=None):\n         if K.backend() == 'theano':\n-            return K.softplus(K.pattern_broadcast(self.betas, self.param_broadcast) * x) * K.pattern_broadcast(self.alphas, self.param_broadcast)\n+            return (K.softplus(K.pattern_broadcast(self.betas,\n+                                                   self.param_broadcast) * x) *\n+                    K.pattern_broadcast(self.alphas, self.param_broadcast))\n         else:\n             return K.softplus(self.betas * x) * self.alphas\n \n@@ -212,7 +219,7 @@ class ParametricSoftplus(Layer):\n \n \n class ThresholdedReLU(Layer):\n-    '''Thresholded Rectified Linear Unit:\n+    \"\"\"Thresholded Rectified Linear Unit:\n     `f(x) = x for x > theta`,\n     `f(x) = 0 otherwise`.\n \n@@ -229,7 +236,8 @@ class ThresholdedReLU(Layer):\n \n     # References\n         - [Zero-Bias Autoencoders and the Benefits of Co-Adapting Features](http://arxiv.org/abs/1402.3337)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, theta=1.0, **kwargs):\n         self.supports_masking = True\n         self.theta = K.cast_to_floatx(theta)\n@@ -245,7 +253,7 @@ class ThresholdedReLU(Layer):\n \n \n class SReLU(Layer):\n-    '''S-shaped Rectified Linear Unit:\n+    \"\"\"S-shaped Rectified Linear Unit:\n     `f(x) = t^r + a^r(x - t^r) for x >= t^r`,\n     `f(x) = x for t^r > x > t^l`,\n     `f(x) = t^l + a^l(x - t^l) for x <= t^l`.\n@@ -274,9 +282,11 @@ class SReLU(Layer):\n \n     # References\n         - [Deep Learning with S-shaped Rectified Linear Activation Units](http://arxiv.org/abs/1512.07030)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, t_left_init='zero', a_left_init='glorot_uniform',\n-                 t_right_init='glorot_uniform', a_right_init='one', shared_axes=None, **kwargs):\n+                 t_right_init='glorot_uniform', a_right_init='one',\n+                 shared_axes=None, **kwargs):\n         self.supports_masking = True\n         self.t_left_init = t_left_init\n         self.a_left_init = a_left_init\n@@ -319,7 +329,8 @@ class SReLU(Layer):\n             t_left = K.pattern_broadcast(self.t_left, self.param_broadcast)\n             a_left = K.pattern_broadcast(self.a_left, self.param_broadcast)\n             a_right = K.pattern_broadcast(self.a_right, self.param_broadcast)\n-            t_right_actual = K.pattern_broadcast(self.t_right_actual, self.param_broadcast)\n+            t_right_actual = K.pattern_broadcast(self.t_right_actual,\n+                                                 self.param_broadcast)\n         else:\n             t_left = self.t_left\n             a_left = self.a_left\n\n@@ -13,7 +13,7 @@ from .pooling import MaxPooling1D, MaxPooling2D, MaxPooling3D\n \n \n class Convolution1D(Layer):\n-    '''Convolution operator for filtering neighborhoods of one-dimensional inputs.\n+    \"\"\"Convolution operator for filtering neighborhoods of one-dimensional inputs.\n     When using this layer as the first layer in a model,\n     either provide the keyword argument `input_dim`\n     (int, e.g. 128 for sequences of 128-dimensional vectors),\n@@ -76,7 +76,8 @@ class Convolution1D(Layer):\n     # Output shape\n         3D tensor with shape: `(samples, new_steps, nb_filter)`.\n         `steps` value might have changed due to padding.\n-    '''\n+    \"\"\"\n+\n     def __init__(self, nb_filter, filter_length,\n                  init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample_length=1,\n@@ -173,7 +174,7 @@ class Convolution1D(Layer):\n \n \n class AtrousConvolution1D(Convolution1D):\n-    '''Atrous Convolution operator for filtering neighborhoods of one-dimensional inputs.\n+    \"\"\"Atrous Convolution operator for filtering neighborhoods of one-dimensional inputs.\n     A.k.a dilated convolution or convolution with holes.\n     When using this layer as the first layer in a model,\n     either provide the keyword argument `input_dim`\n@@ -239,7 +240,8 @@ class AtrousConvolution1D(Convolution1D):\n     # Output shape\n         3D tensor with shape: `(samples, new_steps, nb_filter)`.\n         `steps` value might have changed due to padding.\n-    '''\n+    \"\"\"\n+\n     def __init__(self, nb_filter, filter_length,\n                  init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample_length=1, atrous_rate=1,\n@@ -288,7 +290,7 @@ class AtrousConvolution1D(Convolution1D):\n \n \n class Convolution2D(Layer):\n-    '''Convolution operator for filtering windows of two-dimensional inputs.\n+    \"\"\"Convolution operator for filtering windows of two-dimensional inputs.\n     When using this layer as the first layer in a model,\n     provide the keyword argument `input_shape`\n     (tuple of integers, does not include the sample axis),\n@@ -355,7 +357,8 @@ class Convolution2D(Layer):\n         or 4D tensor with shape:\n         `(samples, new_rows, new_cols, nb_filter)` if dim_ordering='tf'.\n         `rows` and `cols` values might have changed due to padding.\n-    '''\n+    \"\"\"\n+\n     def __init__(self, nb_filter, nb_row, nb_col,\n                  init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample=(1, 1), dim_ordering='default',\n@@ -473,7 +476,7 @@ class Convolution2D(Layer):\n \n \n class Deconvolution2D(Convolution2D):\n-    '''Transposed convolution operator for filtering windows of two-dimensional inputs.\n+    \"\"\"Transposed convolution operator for filtering windows of two-dimensional inputs.\n     The need for transposed convolutions generally arises from the desire\n     to use a transformation going in the opposite direction of a normal convolution,\n     i.e., from something that has the shape of the output of some convolution\n@@ -583,10 +586,11 @@ class Deconvolution2D(Convolution2D):\n         `rows` and `cols` values might have changed due to padding.\n \n     # References\n-        [1] [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285v1)\n-        [2] [Transposed convolution arithmetic](http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html#transposed-convolution-arithmetic)\n-        [3] [Deconvolutional Networks](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n-    '''\n+        - [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285v1)\n+        - [Transposed convolution arithmetic](http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html#transposed-convolution-arithmetic)\n+        - [Deconvolutional Networks](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n+    \"\"\"\n+\n     def __init__(self, nb_filter, nb_row, nb_col, output_shape,\n                  init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample=(1, 1),\n@@ -654,7 +658,7 @@ class Deconvolution2D(Convolution2D):\n \n \n class AtrousConvolution2D(Convolution2D):\n-    '''Atrous Convolution operator for filtering windows of two-dimensional inputs.\n+    \"\"\"Atrous Convolution operator for filtering windows of two-dimensional inputs.\n     A.k.a dilated convolution or convolution with holes.\n     When using this layer as the first layer in a model,\n     provide the keyword argument `input_shape`\n@@ -723,7 +727,8 @@ class AtrousConvolution2D(Convolution2D):\n \n     # References\n         - [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, nb_filter, nb_row, nb_col,\n                  init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample=(1, 1),\n@@ -799,7 +804,7 @@ class AtrousConvolution2D(Convolution2D):\n \n \n class SeparableConvolution2D(Layer):\n-    '''Separable convolution operator for 2D inputs.\n+    \"\"\"Separable convolution operator for 2D inputs.\n \n     Separable convolutions consist in first performing\n     a depthwise spatial convolution\n@@ -876,7 +881,8 @@ class SeparableConvolution2D(Layer):\n         or 4D tensor with shape:\n         `(samples, new_rows, new_cols, nb_filter)` if dim_ordering='tf'.\n         `rows` and `cols` values might have changed due to padding.\n-    '''\n+    \"\"\"\n+\n     def __init__(self, nb_filter, nb_row, nb_col,\n                  init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample=(1, 1),\n@@ -1026,7 +1032,7 @@ class SeparableConvolution2D(Layer):\n \n \n class Convolution3D(Layer):\n-    '''Convolution operator for filtering windows of three-dimensional inputs.\n+    \"\"\"Convolution operator for filtering windows of three-dimensional inputs.\n     When using this layer as the first layer in a model,\n     provide the keyword argument `input_shape`\n     (tuple of integers, does not include the sample axis),\n@@ -1081,7 +1087,7 @@ class Convolution3D(Layer):\n         or 5D tensor with shape:\n         `(samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, nb_filter)` if dim_ordering='tf'.\n         `new_conv_dim1`, `new_conv_dim2` and `new_conv_dim3` values might have changed due to padding.\n-    '''\n+    \"\"\"\n \n     def __init__(self, nb_filter, kernel_dim1, kernel_dim2, kernel_dim3,\n                  init='glorot_uniform', activation=None, weights=None,\n@@ -1217,7 +1223,7 @@ class Convolution3D(Layer):\n \n \n class UpSampling1D(Layer):\n-    '''Repeat each temporal step `length` times along the time axis.\n+    \"\"\"Repeat each temporal step `length` times along the time axis.\n \n     # Arguments\n         length: integer. Upsampling factor.\n@@ -1227,7 +1233,7 @@ class UpSampling1D(Layer):\n \n     # Output shape\n         3D tensor with shape: `(samples, upsampled_steps, features)`.\n-    '''\n+    \"\"\"\n \n     def __init__(self, length=2, **kwargs):\n         self.length = length\n@@ -1249,7 +1255,7 @@ class UpSampling1D(Layer):\n \n \n class UpSampling2D(Layer):\n-    '''Repeat the rows and columns of the data\n+    \"\"\"Repeat the rows and columns of the data\n     by size[0] and size[1] respectively.\n \n     # Arguments\n@@ -1272,7 +1278,7 @@ class UpSampling2D(Layer):\n         `(samples, channels, upsampled_rows, upsampled_cols)` if dim_ordering='th'\n         or 4D tensor with shape:\n         `(samples, upsampled_rows, upsampled_cols, channels)` if dim_ordering='tf'.\n-    '''\n+    \"\"\"\n \n     def __init__(self, size=(2, 2), dim_ordering='default', **kwargs):\n         if dim_ordering == 'default':\n@@ -1313,7 +1319,7 @@ class UpSampling2D(Layer):\n \n \n class UpSampling3D(Layer):\n-    '''Repeat the first, second and third dimension of the data\n+    \"\"\"Repeat the first, second and third dimension of the data\n     by size[0], size[1] and size[2] respectively.\n \n     # Arguments\n@@ -1336,7 +1342,7 @@ class UpSampling3D(Layer):\n         `(samples, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3)` if dim_ordering='th'\n         or 5D tensor with shape:\n         `(samples, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels)` if dim_ordering='tf'.\n-    '''\n+    \"\"\"\n \n     def __init__(self, size=(2, 2, 2), dim_ordering='default', **kwargs):\n         if dim_ordering == 'default':\n@@ -1381,7 +1387,7 @@ class UpSampling3D(Layer):\n \n \n class ZeroPadding1D(Layer):\n-    '''Zero-padding layer for 1D input (e.g. temporal sequence).\n+    \"\"\"Zero-padding layer for 1D input (e.g. temporal sequence).\n \n     # Arguments\n         padding: int, or tuple of int (length 2), or dictionary.\n@@ -1400,7 +1406,7 @@ class ZeroPadding1D(Layer):\n \n     # Output shape\n         3D tensor with shape (samples, padded_axis, features)\n-    '''\n+    \"\"\"\n \n     def __init__(self, padding=1, **kwargs):\n         super(ZeroPadding1D, self).__init__(**kwargs)\n@@ -1444,7 +1450,7 @@ class ZeroPadding1D(Layer):\n \n \n class ZeroPadding2D(Layer):\n-    '''Zero-padding layer for 2D input (e.g. picture).\n+    \"\"\"Zero-padding layer for 2D input (e.g. picture).\n \n     # Arguments\n         padding: tuple of int (length 2), or tuple of int (length 4), or dictionary.\n@@ -1476,7 +1482,7 @@ class ZeroPadding2D(Layer):\n         `(samples, channels, padded_rows, padded_cols)` if dim_ordering='th'\n         or 4D tensor with shape:\n         `(samples, padded_rows, padded_cols, channels)` if dim_ordering='tf'.\n-    '''\n+    \"\"\"\n \n     def __init__(self,\n                  padding=(1, 1),\n@@ -1553,7 +1559,7 @@ class ZeroPadding2D(Layer):\n \n \n class ZeroPadding3D(Layer):\n-    '''Zero-padding layer for 3D data (spatial or spatio-temporal).\n+    \"\"\"Zero-padding layer for 3D data (spatial or spatio-temporal).\n \n     # Arguments\n         padding: tuple of int (length 3)\n@@ -1574,7 +1580,7 @@ class ZeroPadding3D(Layer):\n     # Output shape\n         5D tensor with shape:\n         (samples, depth, first_padded_axis, second_padded_axis, third_axis_to_pad)\n-    '''\n+    \"\"\"\n \n     def __init__(self, padding=(1, 1, 1), dim_ordering='default', **kwargs):\n         super(ZeroPadding3D, self).__init__(**kwargs)\n@@ -1619,7 +1625,7 @@ class ZeroPadding3D(Layer):\n \n \n class Cropping1D(Layer):\n-    '''Cropping layer for 1D input (e.g. temporal sequence).\n+    \"\"\"Cropping layer for 1D input (e.g. temporal sequence).\n     It crops along the time dimension (axis 1).\n \n     # Arguments\n@@ -1632,7 +1638,7 @@ class Cropping1D(Layer):\n \n     # Output shape\n         3D tensor with shape (samples, cropped_axis, features)\n-    '''\n+    \"\"\"\n \n     def __init__(self, cropping=(1, 1), **kwargs):\n         super(Cropping1D, self).__init__(**kwargs)\n@@ -1667,7 +1673,7 @@ class Cropping1D(Layer):\n \n \n class Cropping2D(Layer):\n-    '''Cropping layer for 2D input (e.g. picture).\n+    \"\"\"Cropping layer for 2D input (e.g. picture).\n     It crops along spatial dimensions, i.e. width and height.\n \n     # Arguments\n@@ -1701,8 +1707,7 @@ class Cropping2D(Layer):\n         # now model.output_shape == (None, 64, 20, 16)\n \n     ```\n-\n-    '''\n+    \"\"\"\n \n     def __init__(self, cropping=((0, 0), (0, 0)), dim_ordering='default', **kwargs):\n         super(Cropping2D, self).__init__(**kwargs)\n@@ -1757,7 +1762,7 @@ class Cropping2D(Layer):\n \n \n class Cropping3D(Layer):\n-    '''Cropping layer for 3D data (e.g. spatial or spatio-temporal).\n+    \"\"\"Cropping layer for 3D data (e.g. spatial or spatio-temporal).\n \n     # Arguments\n         cropping: tuple of tuple of int (length 3)\n@@ -1778,7 +1783,7 @@ class Cropping3D(Layer):\n         5D tensor with shape:\n         (samples, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis)\n \n-    '''\n+    \"\"\"\n \n     def __init__(self, cropping=((1, 1), (1, 1), (1, 1)),\n                  dim_ordering='default', **kwargs):\n\n@@ -8,7 +8,7 @@ import warnings\n \n \n class ConvRecurrent2D(Layer):\n-    '''Abstract base class for convolutional recurrent layers.\n+    \"\"\"Abstract base class for convolutional recurrent layers.\n     Do not use in a model -- it's not a functional layer!\n \n     ConvLSTM2D\n@@ -73,7 +73,7 @@ class ConvRecurrent2D(Layer):\n \n         To reset the states of your model, call `.reset_states()` on either\n         a specific layer, or on your entire model.\n-    '''\n+    \"\"\"\n \n     def __init__(self, weights=None, nb_row=None, nb_col=None, nb_filter=None,\n                  return_sequences=False, go_backwards=False, stateful=False,\n@@ -187,7 +187,7 @@ class ConvRecurrent2D(Layer):\n \n \n class ConvLSTM2D(ConvRecurrent2D):\n-    '''Convolutional LSTM.\n+    \"\"\"Convolutional LSTM.\n \n     # Input shape\n         - if dim_ordering='th'\n@@ -246,7 +246,8 @@ class ConvLSTM2D(ConvRecurrent2D):\n         Precipitation Nowcasting](http://arxiv.org/abs/1506.04214v1)\n         The current implementation does not include the feedback loop on the\n         cells output\n-    '''\n+    \"\"\"\n+\n     def __init__(self, nb_filter, nb_row, nb_col,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  forget_bias_init='one', activation='tanh',\n\n@@ -17,7 +17,7 @@ from ..utils.generic_utils import func_dump, func_load\n \n \n class Masking(Layer):\n-    '''Masks an input sequence by using a mask value to\n+    \"\"\"Masks an input sequence by using a mask value to\n     identify timesteps to be skipped.\n \n     For each timestep in the input tensor (dimension #1 in the tensor),\n@@ -43,7 +43,8 @@ class Masking(Layer):\n         model.add(Masking(mask_value=0., input_shape=(timesteps, features)))\n         model.add(LSTM(32))\n     ```\n-    '''\n+    \"\"\"\n+\n     def __init__(self, mask_value=0., **kwargs):\n         self.supports_masking = True\n         self.mask_value = mask_value\n@@ -64,7 +65,7 @@ class Masking(Layer):\n \n \n class Dropout(Layer):\n-    '''Applies Dropout to the input. Dropout consists in randomly setting\n+    \"\"\"Applies Dropout to the input. Dropout consists in randomly setting\n     a fraction `p` of input units to 0 at each update during training time,\n     which helps prevent overfitting.\n \n@@ -73,7 +74,8 @@ class Dropout(Layer):\n \n     # References\n         - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, p, **kwargs):\n         self.p = p\n         if 0. < self.p < 1.:\n@@ -97,7 +99,7 @@ class Dropout(Layer):\n \n \n class SpatialDropout1D(Dropout):\n-    '''This version performs the same function as Dropout, however it drops\n+    \"\"\"This version performs the same function as Dropout, however it drops\n     entire 1D feature maps instead of individual elements. If adjacent frames\n     within feature maps are strongly correlated (as is normally the case in\n     early convolution layers) then regular dropout will not regularize the\n@@ -117,7 +119,8 @@ class SpatialDropout1D(Dropout):\n \n     # References\n         - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, p, **kwargs):\n         super(SpatialDropout1D, self).__init__(p, **kwargs)\n \n@@ -128,7 +131,7 @@ class SpatialDropout1D(Dropout):\n \n \n class SpatialDropout2D(Dropout):\n-    '''This version performs the same function as Dropout, however it drops\n+    \"\"\"This version performs the same function as Dropout, however it drops\n     entire 2D feature maps instead of individual elements. If adjacent pixels\n     within feature maps are strongly correlated (as is normally the case in\n     early convolution layers) then regular dropout will not regularize the\n@@ -155,7 +158,8 @@ class SpatialDropout2D(Dropout):\n \n     # References\n         - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, p, dim_ordering='default', **kwargs):\n         if dim_ordering == 'default':\n             dim_ordering = K.image_dim_ordering()\n@@ -175,7 +179,7 @@ class SpatialDropout2D(Dropout):\n \n \n class SpatialDropout3D(Dropout):\n-    '''This version performs the same function as Dropout, however it drops\n+    \"\"\"This version performs the same function as Dropout, however it drops\n     entire 3D feature maps instead of individual elements. If adjacent voxels\n     within feature maps are strongly correlated (as is normally the case in\n     early convolution layers) then regular dropout will not regularize the\n@@ -203,7 +207,8 @@ class SpatialDropout3D(Dropout):\n \n     # References\n         - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, p, dim_ordering='default', **kwargs):\n         if dim_ordering == 'default':\n             dim_ordering = K.image_dim_ordering()\n@@ -223,7 +228,7 @@ class SpatialDropout3D(Dropout):\n \n \n class Activation(Layer):\n-    '''Applies an activation function to an output.\n+    \"\"\"Applies an activation function to an output.\n \n     # Arguments\n         activation: name of activation function to use\n@@ -237,7 +242,8 @@ class Activation(Layer):\n \n     # Output shape\n         Same shape as input.\n-    '''\n+    \"\"\"\n+\n     def __init__(self, activation, **kwargs):\n         self.supports_masking = True\n         self.activation = activations.get(activation)\n@@ -253,7 +259,7 @@ class Activation(Layer):\n \n \n class Reshape(Layer):\n-    '''Reshapes an output to a certain shape.\n+    \"\"\"Reshapes an output to a certain shape.\n \n     # Arguments\n         target_shape: target shape. Tuple of integers,\n@@ -285,13 +291,14 @@ class Reshape(Layer):\n         model.add(Reshape((-1, 2, 2)))\n         # now: model.output_shape == (None, 3, 2, 2)\n     ```\n-    '''\n+    \"\"\"\n+\n     def __init__(self, target_shape, **kwargs):\n         super(Reshape, self).__init__(**kwargs)\n         self.target_shape = tuple(target_shape)\n \n     def _fix_unknown_dimension(self, input_shape, output_shape):\n-        '''Find and replace a single missing dimension in an output shape\n+        \"\"\"Find and replace a single missing dimension in an output shape\n         given an input shape.\n \n         A near direct port of the internal Numpy function\n@@ -310,7 +317,7 @@ class Reshape(Layer):\n             Raises a ValueError if the total array size of the output_shape is\n             different then the input_shape, or more then one unknown dimension\n             is specified.\n-        '''\n+        \"\"\"\n         output_shape = list(output_shape)\n \n         msg = 'total size of new array must be unchanged'\n@@ -364,7 +371,7 @@ class Reshape(Layer):\n \n \n class Permute(Layer):\n-    '''Permutes the dimensions of the input according to a given pattern.\n+    \"\"\"Permutes the dimensions of the input according to a given pattern.\n \n     Useful for e.g. connecting RNNs and convnets together.\n \n@@ -391,7 +398,8 @@ class Permute(Layer):\n     # Output shape\n         Same as the input shape, but with the dimensions re-ordered according\n         to the specified pattern.\n-    '''\n+    \"\"\"\n+\n     def __init__(self, dims, **kwargs):\n         self.dims = tuple(dims)\n         super(Permute, self).__init__(**kwargs)\n@@ -414,7 +422,7 @@ class Permute(Layer):\n \n \n class Flatten(Layer):\n-    '''Flattens the input. Does not affect the batch size.\n+    \"\"\"Flattens the input. Does not affect the batch size.\n \n     # Example\n \n@@ -428,7 +436,8 @@ class Flatten(Layer):\n         model.add(Flatten())\n         # now: model.output_shape == (None, 65536)\n     ```\n-    '''\n+    \"\"\"\n+\n     def __init__(self, **kwargs):\n         self.input_spec = [InputSpec(ndim='3+')]\n         super(Flatten, self).__init__(**kwargs)\n@@ -448,7 +457,7 @@ class Flatten(Layer):\n \n \n class RepeatVector(Layer):\n-    '''Repeats the input n times.\n+    \"\"\"Repeats the input n times.\n \n     # Example\n \n@@ -470,7 +479,8 @@ class RepeatVector(Layer):\n \n     # Output shape\n         3D tensor of shape `(nb_samples, n, features)`.\n-    '''\n+    \"\"\"\n+\n     def __init__(self, n, **kwargs):\n         self.n = n\n         self.input_spec = [InputSpec(ndim=2)]\n@@ -489,7 +499,7 @@ class RepeatVector(Layer):\n \n \n class Lambda(Layer):\n-    '''Used for evaluating an arbitrary Theano / TensorFlow expression\n+    \"\"\"Used for evaluating an arbitrary Theano / TensorFlow expression\n     on the output of the previous layer.\n \n     # Examples\n@@ -542,7 +552,8 @@ class Lambda(Layer):\n \n     # Output shape\n         Specified by `output_shape` argument.\n-    '''\n+    \"\"\"\n+\n     def __init__(self, function, output_shape=None, arguments=None, **kwargs):\n         self.function = function\n         self.arguments = arguments if arguments else {}\n@@ -656,7 +667,7 @@ class Lambda(Layer):\n \n \n class Dense(Layer):\n-    '''Just your regular fully connected NN layer.\n+    \"\"\"Just your regular fully connected NN layer.\n \n     # Example\n \n@@ -716,7 +727,8 @@ class Dense(Layer):\n         nD tensor with shape: `(nb_samples, ..., output_dim)`.\n         For instance, for a 2D input with shape `(nb_samples, input_dim)`,\n         the output would have shape `(nb_samples, output_dim)`.\n-    '''\n+    \"\"\"\n+\n     def __init__(self, output_dim, init='glorot_uniform',\n                  activation=None, weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n@@ -797,7 +809,7 @@ class Dense(Layer):\n \n \n class ActivityRegularization(Layer):\n-    '''Layer that returns its input unchanged, but applies an update\n+    \"\"\"Layer that returns its input unchanged, but applies an update\n     to the cost function based on the activity of the input.\n \n     # Arguments\n@@ -811,7 +823,8 @@ class ActivityRegularization(Layer):\n \n     # Output shape\n         Same shape as input.\n-    '''\n+    \"\"\"\n+\n     def __init__(self, l1=0., l2=0., **kwargs):\n         self.supports_masking = True\n         self.l1 = l1\n@@ -829,7 +842,7 @@ class ActivityRegularization(Layer):\n \n \n class MaxoutDense(Layer):\n-    '''A dense maxout layer.\n+    \"\"\"A dense maxout layer.\n \n     A `MaxoutDense` layer takes the element-wise maximum of\n     `nb_feature` `Dense(input_dim, output_dim)` linear layers.\n@@ -876,7 +889,8 @@ class MaxoutDense(Layer):\n \n     # References\n         - [Maxout Networks](http://arxiv.org/abs/1302.4389)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, output_dim,\n                  nb_feature=4,\n                  init='glorot_uniform',\n@@ -961,7 +975,7 @@ class MaxoutDense(Layer):\n \n \n class Highway(Layer):\n-    '''Densely connected highway network,\n+    \"\"\"Densely connected highway network,\n     a natural extension of LSTMs to feedforward networks.\n \n     # Arguments\n@@ -1002,7 +1016,8 @@ class Highway(Layer):\n \n     # References\n         - [Highway Networks](http://arxiv.org/abs/1505.00387v2)\n-    '''\n+    \"\"\"\n+\n     def __init__(self,\n                  init='glorot_uniform',\n                  activation=None,\n@@ -1096,7 +1111,7 @@ class Highway(Layer):\n \n \n class TimeDistributedDense(Layer):\n-    '''Apply a same Dense layer for each dimension[1] (time_dimension) input.\n+    \"\"\"Apply a same Dense layer for each dimension[1] (time_dimension) input.\n     Especially useful after a recurrent network with 'return_sequence=True'.\n \n     Note: this layer is deprecated, prefer using the `TimeDistributed` wrapper:\n@@ -1142,7 +1157,7 @@ class TimeDistributedDense(Layer):\n             is required when using this layer as the first layer in a model.\n         input_length: length of inputs sequences\n             (integer, or None for variable-length sequences).\n-    '''\n+    \"\"\"\n \n     def __init__(self, output_dim,\n                  init='glorot_uniform',\n\n@@ -6,7 +6,7 @@ from ..engine import Layer\n \n \n class Embedding(Layer):\n-    '''Turn positive integers (indexes) into dense vectors of fixed size.\n+    \"\"\"Turn positive integers (indexes) into dense vectors of fixed size.\n     eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n \n     This layer can only be used as the first layer in a model.\n@@ -62,8 +62,7 @@ class Embedding(Layer):\n \n     # References\n         - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n-    '''\n-    input_ndim = 2\n+    \"\"\"\n \n     def __init__(self, input_dim, output_dim,\n                  init='uniform', input_length=None,\n\n@@ -8,7 +8,7 @@ from ..utils.np_utils import conv_output_length\n \n \n class LocallyConnected1D(Layer):\n-    '''The `LocallyConnected1D` layer works similarly to\n+    \"\"\"The `LocallyConnected1D` layer works similarly to\n     the `Convolution1D` layer, except that weights are unshared,\n     that is, a different set of filters is applied at each different patch\n     of the input.\n@@ -73,7 +73,8 @@ class LocallyConnected1D(Layer):\n     # Output shape\n         3D tensor with shape: `(samples, new_steps, nb_filter)`.\n         `steps` value might have changed due to padding.\n-    '''\n+    \"\"\"\n+\n     def __init__(self, nb_filter, filter_length,\n                  init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample_length=1,\n@@ -178,7 +179,7 @@ class LocallyConnected1D(Layer):\n \n \n class LocallyConnected2D(Layer):\n-    '''The `LocallyConnected2D` layer works similarly\n+    \"\"\"The `LocallyConnected2D` layer works similarly\n     to the `Convolution2D` layer, except that weights are unshared,\n     that is, a different set of filters is applied at each\n     different patch of the input.\n@@ -247,7 +248,8 @@ class LocallyConnected2D(Layer):\n         or 4D tensor with shape:\n         `(samples, new_rows, new_cols, nb_filter)` if dim_ordering='tf'.\n         `rows` and `cols` values might have changed due to padding.\n-    '''\n+    \"\"\"\n+\n     def __init__(self, nb_filter, nb_row, nb_col,\n                  init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample=(1, 1),\n\n@@ -5,7 +5,7 @@ import numpy as np\n \n \n class GaussianNoise(Layer):\n-    '''Apply to the input an additive zero-centered Gaussian noise with\n+    \"\"\"Apply to the input an additive zero-centered Gaussian noise with\n     standard deviation `sigma`. This is useful to mitigate overfitting\n     (you could see it as a kind of random data augmentation).\n     Gaussian Noise (GS) is a natural choice as corruption process\n@@ -23,7 +23,8 @@ class GaussianNoise(Layer):\n \n     # Output shape\n         Same shape as input.\n-    '''\n+    \"\"\"\n+\n     def __init__(self, sigma, **kwargs):\n         self.supports_masking = True\n         self.sigma = sigma\n@@ -43,7 +44,7 @@ class GaussianNoise(Layer):\n \n \n class GaussianDropout(Layer):\n-    '''Apply to the input an multiplicative one-centered Gaussian noise\n+    \"\"\"Apply to the input an multiplicative one-centered Gaussian noise\n     with standard deviation `sqrt(p/(1-p))`.\n \n     As it is a regularization layer, it is only active at training time.\n@@ -60,8 +61,9 @@ class GaussianDropout(Layer):\n         Same shape as input.\n \n     # References\n-        [Dropout: A Simple Way to Prevent Neural Networks from Overfitting Srivastava, Hinton, et al. 2014](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n-    '''\n+        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting Srivastava, Hinton, et al. 2014](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n+    \"\"\"\n+\n     def __init__(self, p, **kwargs):\n         self.supports_masking = True\n         self.p = p\n\n@@ -4,7 +4,7 @@ from .. import backend as K\n \n \n class BatchNormalization(Layer):\n-    '''Normalize the activations of the previous layer at each batch,\n+    \"\"\"Normalize the activations of the previous layer at each batch,\n     i.e. applies a transformation that maintains the mean activation\n     close to 0 and the activation standard deviation close to 1.\n \n@@ -60,7 +60,8 @@ class BatchNormalization(Layer):\n \n     # References\n         - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, epsilon=1e-3, mode=0, axis=-1, momentum=0.99,\n                  weights=None, beta_init='zero', gamma_init='one',\n                  gamma_regularizer=None, beta_regularizer=None, **kwargs):\n\n@@ -7,9 +7,8 @@ from ..utils.np_utils import conv_output_length\n \n \n class _Pooling1D(Layer):\n-    '''Abstract class for different pooling 1D layers.\n-    '''\n-    input_dim = 3\n+    \"\"\"Abstract class for different pooling 1D layers.\n+    \"\"\"\n \n     def __init__(self, pool_length=2, stride=None,\n                  border_mode='valid', **kwargs):\n@@ -51,7 +50,7 @@ class _Pooling1D(Layer):\n \n \n class MaxPooling1D(_Pooling1D):\n-    '''Max pooling operation for temporal data.\n+    \"\"\"Max pooling operation for temporal data.\n \n     # Input shape\n         3D tensor with shape: `(samples, steps, features)`.\n@@ -65,7 +64,7 @@ class MaxPooling1D(_Pooling1D):\n             2 will halve the input.\n             If None, it will default to `pool_length`.\n         border_mode: 'valid' or 'same'.\n-    '''\n+    \"\"\"\n \n     def __init__(self, pool_length=2, stride=None,\n                  border_mode='valid', **kwargs):\n@@ -80,7 +79,7 @@ class MaxPooling1D(_Pooling1D):\n \n \n class AveragePooling1D(_Pooling1D):\n-    '''Average pooling for temporal data.\n+    \"\"\"Average pooling for temporal data.\n \n     # Arguments\n         pool_length: factor by which to downscale. 2 will halve the input.\n@@ -93,7 +92,7 @@ class AveragePooling1D(_Pooling1D):\n \n     # Output shape\n         3D tensor with shape: `(samples, downsampled_steps, features)`.\n-    '''\n+    \"\"\"\n \n     def __init__(self, pool_length=2, stride=None,\n                  border_mode='valid', **kwargs):\n@@ -108,8 +107,8 @@ class AveragePooling1D(_Pooling1D):\n \n \n class _Pooling2D(Layer):\n-    '''Abstract class for different pooling 2D layers.\n-    '''\n+    \"\"\"Abstract class for different pooling 2D layers.\n+    \"\"\"\n \n     def __init__(self, pool_size=(2, 2), strides=None, border_mode='valid',\n                  dim_ordering='default', **kwargs):\n@@ -170,7 +169,7 @@ class _Pooling2D(Layer):\n \n \n class MaxPooling2D(_Pooling2D):\n-    '''Max pooling operation for spatial data.\n+    \"\"\"Max pooling operation for spatial data.\n \n     # Arguments\n         pool_size: tuple of 2 integers,\n@@ -196,7 +195,7 @@ class MaxPooling2D(_Pooling2D):\n         `(nb_samples, channels, pooled_rows, pooled_cols)` if dim_ordering='th'\n         or 4D tensor with shape:\n         `(samples, pooled_rows, pooled_cols, channels)` if dim_ordering='tf'.\n-    '''\n+    \"\"\"\n \n     def __init__(self, pool_size=(2, 2), strides=None, border_mode='valid',\n                  dim_ordering='default', **kwargs):\n@@ -212,7 +211,7 @@ class MaxPooling2D(_Pooling2D):\n \n \n class AveragePooling2D(_Pooling2D):\n-    '''Average pooling operation for spatial data.\n+    \"\"\"Average pooling operation for spatial data.\n \n     # Arguments\n         pool_size: tuple of 2 integers,\n@@ -238,7 +237,7 @@ class AveragePooling2D(_Pooling2D):\n         `(nb_samples, channels, pooled_rows, pooled_cols)` if dim_ordering='th'\n         or 4D tensor with shape:\n         `(samples, pooled_rows, pooled_cols, channels)` if dim_ordering='tf'.\n-    '''\n+    \"\"\"\n \n     def __init__(self, pool_size=(2, 2), strides=None, border_mode='valid',\n                  dim_ordering='default', **kwargs):\n@@ -253,8 +252,8 @@ class AveragePooling2D(_Pooling2D):\n \n \n class _Pooling3D(Layer):\n-    '''Abstract class for different pooling 3D layers.\n-    '''\n+    \"\"\"Abstract class for different pooling 3D layers.\n+    \"\"\"\n \n     def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode='valid',\n                  dim_ordering='default', **kwargs):\n@@ -321,7 +320,7 @@ class _Pooling3D(Layer):\n \n \n class MaxPooling3D(_Pooling3D):\n-    '''Max pooling operation for 3D data (spatial or spatio-temporal).\n+    \"\"\"Max pooling operation for 3D data (spatial or spatio-temporal).\n \n     # Arguments\n         pool_size: tuple of 3 integers,\n@@ -346,7 +345,7 @@ class MaxPooling3D(_Pooling3D):\n         `(nb_samples, channels, pooled_dim1, pooled_dim2, pooled_dim3)` if dim_ordering='th'\n         or 5D tensor with shape:\n         `(samples, pooled_dim1, pooled_dim2, pooled_dim3, channels)` if dim_ordering='tf'.\n-    '''\n+    \"\"\"\n \n     def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode='valid',\n                  dim_ordering='default', **kwargs):\n@@ -361,7 +360,7 @@ class MaxPooling3D(_Pooling3D):\n \n \n class AveragePooling3D(_Pooling3D):\n-    '''Average pooling operation for 3D data (spatial or spatio-temporal).\n+    \"\"\"Average pooling operation for 3D data (spatial or spatio-temporal).\n \n     # Arguments\n         pool_size: tuple of 3 integers,\n@@ -386,7 +385,7 @@ class AveragePooling3D(_Pooling3D):\n         `(nb_samples, channels, pooled_dim1, pooled_dim2, pooled_dim3)` if dim_ordering='th'\n         or 5D tensor with shape:\n         `(samples, pooled_dim1, pooled_dim2, pooled_dim3, channels)` if dim_ordering='tf'.\n-    '''\n+    \"\"\"\n \n     def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode='valid',\n                  dim_ordering='default', **kwargs):\n@@ -415,28 +414,28 @@ class _GlobalPooling1D(Layer):\n \n \n class GlobalAveragePooling1D(_GlobalPooling1D):\n-    '''Global average pooling operation for temporal data.\n+    \"\"\"Global average pooling operation for temporal data.\n \n     # Input shape\n         3D tensor with shape: `(samples, steps, features)`.\n \n     # Output shape\n         2D tensor with shape: `(samples, features)`.\n-    '''\n+    \"\"\"\n \n     def call(self, x, mask=None):\n         return K.mean(x, axis=1)\n \n \n class GlobalMaxPooling1D(_GlobalPooling1D):\n-    '''Global max pooling operation for temporal data.\n+    \"\"\"Global max pooling operation for temporal data.\n \n     # Input shape\n         3D tensor with shape: `(samples, steps, features)`.\n \n     # Output shape\n         2D tensor with shape: `(samples, features)`.\n-    '''\n+    \"\"\"\n \n     def call(self, x, mask=None):\n         return K.max(x, axis=1)\n@@ -467,7 +466,7 @@ class _GlobalPooling2D(Layer):\n \n \n class GlobalAveragePooling2D(_GlobalPooling2D):\n-    '''Global average pooling operation for spatial data.\n+    \"\"\"Global average pooling operation for spatial data.\n \n     # Arguments\n         dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n@@ -485,7 +484,7 @@ class GlobalAveragePooling2D(_GlobalPooling2D):\n     # Output shape\n         2D tensor with shape:\n         `(nb_samples, channels)`\n-    '''\n+    \"\"\"\n \n     def call(self, x, mask=None):\n         if self.dim_ordering == 'tf':\n@@ -495,7 +494,7 @@ class GlobalAveragePooling2D(_GlobalPooling2D):\n \n \n class GlobalMaxPooling2D(_GlobalPooling2D):\n-    '''Global max pooling operation for spatial data.\n+    \"\"\"Global max pooling operation for spatial data.\n \n     # Arguments\n         dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n@@ -513,7 +512,7 @@ class GlobalMaxPooling2D(_GlobalPooling2D):\n     # Output shape\n         2D tensor with shape:\n         `(nb_samples, channels)`\n-    '''\n+    \"\"\"\n \n     def call(self, x, mask=None):\n         if self.dim_ordering == 'tf':\n@@ -547,7 +546,7 @@ class _GlobalPooling3D(Layer):\n \n \n class GlobalAveragePooling3D(_GlobalPooling3D):\n-    '''Global Average pooling operation for 3D data.\n+    \"\"\"Global Average pooling operation for 3D data.\n \n     # Arguments\n         dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n@@ -565,7 +564,7 @@ class GlobalAveragePooling3D(_GlobalPooling3D):\n     # Output shape\n         2D tensor with shape:\n         `(nb_samples, channels)`\n-    '''\n+    \"\"\"\n \n     def call(self, x, mask=None):\n         if self.dim_ordering == 'tf':\n@@ -575,7 +574,7 @@ class GlobalAveragePooling3D(_GlobalPooling3D):\n \n \n class GlobalMaxPooling3D(_GlobalPooling3D):\n-    '''Global Max pooling operation for 3D data.\n+    \"\"\"Global Max pooling operation for 3D data.\n \n     # Arguments\n         dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n@@ -593,7 +592,7 @@ class GlobalMaxPooling3D(_GlobalPooling3D):\n     # Output shape\n         2D tensor with shape:\n         `(nb_samples, channels)`\n-    '''\n+    \"\"\"\n \n     def call(self, x, mask=None):\n         if self.dim_ordering == 'tf':\n\n@@ -9,8 +9,8 @@ from ..engine import Layer, InputSpec\n \n def time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None, timesteps=None):\n-    '''Apply y.w + b for every temporal slice y of x.\n-    '''\n+    \"\"\"Apply y.w + b for every temporal slice y of x.\n+    \"\"\"\n     if not input_dim:\n         input_dim = K.shape(x)[2]\n     if not timesteps:\n@@ -40,7 +40,7 @@ def time_distributed_dense(x, w, b=None, dropout=None,\n \n \n class Recurrent(Layer):\n-    '''Abstract base class for recurrent layers.\n+    \"\"\"Abstract base class for recurrent layers.\n     Do not use in a model -- it's not a valid layer!\n     Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n \n@@ -144,7 +144,8 @@ class Recurrent(Layer):\n \n         To reset the states of your model, call `.reset_states()` on either\n         a specific layer, or on your entire model.\n-    '''\n+    \"\"\"\n+\n     def __init__(self, weights=None,\n                  return_sequences=False, go_backwards=False, stateful=False,\n                  unroll=False, consume_less='cpu',\n@@ -253,7 +254,7 @@ class Recurrent(Layer):\n \n \n class SimpleRNN(Recurrent):\n-    '''Fully-connected RNN where the output is to be fed back to input.\n+    \"\"\"Fully-connected RNN where the output is to be fed back to input.\n \n     # Arguments\n         output_dim: dimension of the internal projections and the final output.\n@@ -275,7 +276,8 @@ class SimpleRNN(Recurrent):\n \n     # References\n         - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='tanh',\n@@ -402,7 +404,7 @@ class SimpleRNN(Recurrent):\n \n \n class GRU(Recurrent):\n-    '''Gated Recurrent Unit - Cho et al. 2014.\n+    \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n \n     # Arguments\n         output_dim: dimension of the internal projections and the final output.\n@@ -427,7 +429,8 @@ class GRU(Recurrent):\n         - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n         - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n         - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='tanh', inner_activation='hard_sigmoid',\n@@ -621,7 +624,7 @@ class GRU(Recurrent):\n \n \n class LSTM(Recurrent):\n-    '''Long-Short Term Memory unit - Hochreiter 1997.\n+    \"\"\"Long-Short Term Memory unit - Hochreiter 1997.\n \n     For a step-by-step description of the algorithm, see\n     [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n@@ -653,7 +656,8 @@ class LSTM(Recurrent):\n         - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n         - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n         - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  forget_bias_init='one', activation='tanh',\n\n@@ -10,10 +10,10 @@ class Wrapper(Layer):\n         super(Wrapper, self).__init__(**kwargs)\n \n     def build(self, input_shape=None):\n-        '''Assumes that self.layer is already set.\n+        \"\"\"Assumes that self.layer is already set.\n         Should be called at the end of .build() in the\n         children classes.\n-        '''\n+        \"\"\"\n         self.trainable_weights = getattr(self.layer, 'trainable_weights', [])\n         self.non_trainable_weights = getattr(self.layer, 'non_trainable_weights', [])\n         self.updates = getattr(self.layer, 'updates', [])\n@@ -79,6 +79,7 @@ class TimeDistributed(Wrapper):\n     # Arguments\n         layer: a layer instance.\n     \"\"\"\n+\n     def __init__(self, layer, **kwargs):\n         self.supports_masking = True\n         super(TimeDistributed, self).__init__(layer, **kwargs)\n@@ -132,7 +133,7 @@ class TimeDistributed(Wrapper):\n \n \n class Bidirectional(Wrapper):\n-    ''' Bidirectional wrapper for RNNs.\n+    \"\"\" Bidirectional wrapper for RNNs.\n \n     # Arguments\n         layer: `Recurrent` instance.\n@@ -152,7 +153,8 @@ class Bidirectional(Wrapper):\n         model.add(Activation('softmax'))\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n     ```\n-    '''\n+    \"\"\"\n+\n     def __init__(self, layer, merge_mode='concat', weights=None, **kwargs):\n         if merge_mode not in ['sum', 'mul', 'ave', 'concat', None]:\n             raise ValueError('Invalid merge mode. '\n\n@@ -194,9 +194,9 @@ def model_from_config(config, custom_objects=None):\n \n \n def model_from_yaml(yaml_string, custom_objects=None):\n-    '''Parses a yaml model configuration file\n+    \"\"\"Parses a yaml model configuration file\n     and returns a model instance.\n-    '''\n+    \"\"\"\n     import yaml\n     from keras.utils.layer_utils import layer_from_config\n     config = yaml.load(yaml_string)\n@@ -204,9 +204,9 @@ def model_from_yaml(yaml_string, custom_objects=None):\n \n \n def model_from_json(json_string, custom_objects=None):\n-    '''Parses a JSON model configuration file\n+    \"\"\"Parses a JSON model configuration file\n     and returns a model instance.\n-    '''\n+    \"\"\"\n     import json\n     from keras.utils.layer_utils import layer_from_config\n     config = json.loads(json_string)\n@@ -214,7 +214,7 @@ def model_from_json(json_string, custom_objects=None):\n \n \n class Sequential(Model):\n-    '''Linear stack of layers.\n+    \"\"\"Linear stack of layers.\n \n     # Arguments\n         layers: list of layers to add to the model.\n@@ -248,7 +248,7 @@ class Sequential(Model):\n             model.add(Dense(32, batch_input_shape=(None, 500)))\n             model.add(Dense(32))\n         ```\n-    '''\n+    \"\"\"\n     def __init__(self, layers=None, name=None):\n         self.layers = []  # stack of layers\n         self.model = None  # internal Model instance\n@@ -272,11 +272,11 @@ class Sequential(Model):\n                 self.add(layer)\n \n     def add(self, layer):\n-        '''Adds a layer instance on top of the layer stack.\n+        \"\"\"Adds a layer instance on top of the layer stack.\n \n         # Arguments\n             layer: layer instance.\n-        '''\n+        \"\"\"\n         if not isinstance(layer, Layer):\n             raise TypeError('The added layer must be '\n                             'an instance of class Layer. '\n@@ -344,8 +344,8 @@ class Sequential(Model):\n         self._flattened_layers = None\n \n     def pop(self):\n-        '''Removes the last layer in the model.\n-        '''\n+        \"\"\"Removes the last layer in the model.\n+        \"\"\"\n         if not self.layers:\n             raise TypeError('There are no layers in the model.')\n \n@@ -364,7 +364,7 @@ class Sequential(Model):\n         self._flattened_layers = None\n \n     def get_layer(self, name=None, index=None):\n-        '''Returns a layer based on either its name (unique)\n+        \"\"\"Returns a layer based on either its name (unique)\n         or its index in the graph. Indices are based on\n         order of horizontal graph traversal (bottom-up).\n \n@@ -374,7 +374,7 @@ class Sequential(Model):\n \n         # Returns\n             A layer instance.\n-        '''\n+        \"\"\"\n         if not self.built:\n             self.build()\n         return self.model.get_layer(name, index)\n@@ -519,9 +519,9 @@ class Sequential(Model):\n         return self._gather_dict_attr('constraints')\n \n     def get_weights(self):\n-        '''Returns the weights of the model,\n+        \"\"\"Returns the weights of the model,\n         as a flat list of Numpy arrays.\n-        '''\n+        \"\"\"\n         # support for legacy behavior\n         weights = []\n         for layer in self.flattened_layers:\n@@ -529,11 +529,11 @@ class Sequential(Model):\n         return weights\n \n     def set_weights(self, weights):\n-        '''Sets the weights of the model.\n+        \"\"\"Sets the weights of the model.\n         The `weights` argument should be a list\n         of Numpy arrays with shapes and types matching\n         the output of `model.get_weights()`.\n-        '''\n+        \"\"\"\n         # support for legacy behavior\n         for layer in self.flattened_layers:\n             nb_param = len(layer.weights)\n@@ -552,7 +552,7 @@ class Sequential(Model):\n                 metrics=None,\n                 sample_weight_mode=None,\n                 **kwargs):\n-        '''Configures the learning process.\n+        \"\"\"Configures the learning process.\n \n         # Arguments\n             optimizer: str (name of optimizer) or optimizer object.\n@@ -578,7 +578,7 @@ class Sequential(Model):\n                               loss='categorical_crossentropy',\n                               metrics=['accuracy'])\n             ```\n-        '''\n+        \"\"\"\n         # create the underlying model\n         self.build()\n         # legacy kwarg support\n@@ -602,7 +602,7 @@ class Sequential(Model):\n     def fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=None,\n             validation_split=0., validation_data=None, shuffle=True,\n             class_weight=None, sample_weight=None, initial_epoch=0, **kwargs):\n-        '''Trains the model for a fixed number of epochs.\n+        \"\"\"Trains the model for a fixed number of epochs.\n \n         # Arguments\n             x: input data, as a Numpy array or list of Numpy arrays\n@@ -644,7 +644,7 @@ class Sequential(Model):\n             a record of training loss values and metrics values\n             at successive epochs, as well as validation loss values\n             and validation metrics values (if applicable).\n-        '''\n+        \"\"\"\n         if self.model is None:\n             raise RuntimeError('The model needs to be compiled '\n                                'before being used.')\n@@ -672,7 +672,7 @@ class Sequential(Model):\n \n     def evaluate(self, x, y, batch_size=32, verbose=1,\n                  sample_weight=None, **kwargs):\n-        '''Computes the loss on some input data, batch by batch.\n+        \"\"\"Computes the loss on some input data, batch by batch.\n \n         # Arguments\n             x: input data, as a Numpy array or list of Numpy arrays\n@@ -687,7 +687,7 @@ class Sequential(Model):\n             or list of scalars (if the model computes other metrics).\n             The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n-        '''\n+        \"\"\"\n         if self.model is None:\n             raise RuntimeError('The model needs to be compiled '\n                                'before being used.')\n@@ -707,7 +707,7 @@ class Sequential(Model):\n                                    sample_weight=sample_weight)\n \n     def predict(self, x, batch_size=32, verbose=0):\n-        '''Generates output predictions for the input samples,\n+        \"\"\"Generates output predictions for the input samples,\n         processing the samples in a batched way.\n \n         # Arguments\n@@ -717,21 +717,21 @@ class Sequential(Model):\n \n         # Returns\n             A Numpy array of predictions.\n-        '''\n+        \"\"\"\n         if self.model is None:\n             self.build()\n         return self.model.predict(x, batch_size=batch_size, verbose=verbose)\n \n     def predict_on_batch(self, x):\n-        '''Returns predictions for a single batch of samples.\n-        '''\n+        \"\"\"Returns predictions for a single batch of samples.\n+        \"\"\"\n         if self.model is None:\n             self.build()\n         return self.model.predict_on_batch(x)\n \n     def train_on_batch(self, x, y, class_weight=None,\n                        sample_weight=None, **kwargs):\n-        '''Single gradient update over one batch of samples.\n+        \"\"\"Single gradient update over one batch of samples.\n \n         # Arguments\n             x: input data, as a Numpy array or list of Numpy arrays\n@@ -746,7 +746,7 @@ class Sequential(Model):\n             or list of scalars (if the model computes other metrics).\n             The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n-        '''\n+        \"\"\"\n         if self.model is None:\n             raise RuntimeError('The model needs to be compiled '\n                                'before being used.')\n@@ -766,7 +766,7 @@ class Sequential(Model):\n \n     def test_on_batch(self, x, y,\n                       sample_weight=None, **kwargs):\n-        '''Evaluates the model over a single batch of samples.\n+        \"\"\"Evaluates the model over a single batch of samples.\n \n         # Arguments\n             x: input data, as a Numpy array or list of Numpy arrays\n@@ -779,7 +779,7 @@ class Sequential(Model):\n             or list of scalars (if the model computes other metrics).\n             The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n-        '''\n+        \"\"\"\n         if self.model is None:\n             raise RuntimeError('The model needs to be compiled '\n                                'before being used.')\n@@ -797,7 +797,7 @@ class Sequential(Model):\n                                         sample_weight=sample_weight)\n \n     def predict_proba(self, x, batch_size=32, verbose=1):\n-        '''Generates class probability predictions for the input samples\n+        \"\"\"Generates class probability predictions for the input samples\n         batch by batch.\n \n         # Arguments\n@@ -808,7 +808,7 @@ class Sequential(Model):\n \n         # Returns\n             A Numpy array of probability predictions.\n-        '''\n+        \"\"\"\n         preds = self.predict(x, batch_size, verbose)\n         if preds.min() < 0. or preds.max() > 1.:\n             warnings.warn('Network returning invalid probability values. '\n@@ -818,7 +818,7 @@ class Sequential(Model):\n         return preds\n \n     def predict_classes(self, x, batch_size=32, verbose=1):\n-        '''Generate class predictions for the input samples\n+        \"\"\"Generate class predictions for the input samples\n         batch by batch.\n \n         # Arguments\n@@ -829,7 +829,7 @@ class Sequential(Model):\n \n         # Returns\n             A numpy array of class predictions.\n-        '''\n+        \"\"\"\n         proba = self.predict(x, batch_size=batch_size, verbose=verbose)\n         if proba.shape[-1] > 1:\n             return proba.argmax(axis=-1)\n@@ -841,7 +841,7 @@ class Sequential(Model):\n                       validation_data=None, nb_val_samples=None,\n                       class_weight=None, max_q_size=10, nb_worker=1,\n                       pickle_safe=False, initial_epoch=0, **kwargs):\n-        '''Fits the model on data generated batch-by-batch by\n+        \"\"\"Fits the model on data generated batch-by-batch by\n         a Python generator.\n         The generator is run in parallel to the model, for efficiency.\n         For instance, this allows you to do real-time data augmentation\n@@ -898,7 +898,7 @@ class Sequential(Model):\n             model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n                                 samples_per_epoch=10000, nb_epoch=10)\n         ```\n-        '''\n+        \"\"\"\n         if self.model is None:\n             raise RuntimeError('The model needs to be compiled '\n                                'before being used.')\n@@ -936,7 +936,7 @@ class Sequential(Model):\n     def evaluate_generator(self, generator, val_samples,\n                            max_q_size=10, nb_worker=1,\n                            pickle_safe=False, **kwargs):\n-        '''Evaluates the model on a data generator. The generator should\n+        \"\"\"Evaluates the model on a data generator. The generator should\n         return the same kind of data as accepted by `test_on_batch`.\n \n         # Arguments\n@@ -952,7 +952,7 @@ class Sequential(Model):\n                 this implementation relies on multiprocessing, you should not pass non\n                 non picklable arguments to the generator as they can't be passed\n                 easily to children processes.\n-        '''\n+        \"\"\"\n         if self.model is None:\n             raise RuntimeError('The model needs to be compiled '\n                                'before being used.')\n@@ -981,7 +981,7 @@ class Sequential(Model):\n \n     def predict_generator(self, generator, val_samples,\n                           max_q_size=10, nb_worker=1, pickle_safe=False):\n-        '''Generates predictions for the input samples from a data generator.\n+        \"\"\"Generates predictions for the input samples from a data generator.\n         The generator should return the same kind of data as accepted by\n         `predict_on_batch`.\n \n@@ -998,7 +998,7 @@ class Sequential(Model):\n \n         # Returns\n             A Numpy array of predictions.\n-        '''\n+        \"\"\"\n         if self.model is None:\n             self.build()\n         if nb_worker > 1 and not pickle_safe:\n@@ -1011,9 +1011,9 @@ class Sequential(Model):\n                                             pickle_safe=pickle_safe)\n \n     def get_config(self):\n-        '''Returns the model configuration\n+        \"\"\"Returns the model configuration\n         as a Python list.\n-        '''\n+        \"\"\"\n         config = []\n         if isinstance(self.layers[0], Merge):\n             assert hasattr(self.layers[0], 'layers')\n@@ -1035,8 +1035,8 @@ class Sequential(Model):\n \n     @classmethod\n     def from_config(cls, config, layer_cache=None):\n-        '''Supports legacy formats\n-        '''\n+        \"\"\"Supports legacy formats\n+        \"\"\"\n         from keras.utils.layer_utils import layer_from_config\n         from keras.layers import Merge\n \n\n@@ -34,17 +34,17 @@ def hinge(y_true, y_pred):\n \n \n def categorical_crossentropy(y_true, y_pred):\n-    '''Expects a binary class matrix instead of a vector of scalar classes.\n-    '''\n+    \"\"\"Expects a binary class matrix instead of a vector of scalar classes.\n+    \"\"\"\n     return K.categorical_crossentropy(y_pred, y_true)\n \n \n def sparse_categorical_crossentropy(y_true, y_pred):\n-    '''Expects an array of integer classes.\n+    \"\"\"Expects an array of integer classes.\n \n     Note: labels shape must have the same number of dimensions as output shape.\n     If you get a shape error, add a length-1 dimension to labels.\n-    '''\n+    \"\"\"\n     return K.sparse_categorical_crossentropy(y_pred, y_true)\n \n \n\n@@ -104,7 +104,7 @@ class Optimizer(object):\n     def get_weights(self):\n         \"\"\"Returns the current value of the weights of the optimizer.\n \n-        # Returns:\n+        # Returns\n             A list of numpy arrays.\n         \"\"\"\n         return K.batch_get_value(self.weights)\n@@ -250,6 +250,7 @@ class Adagrad(Optimizer):\n     # Arguments\n         lr: float >= 0. Learning rate.\n         epsilon: float >= 0.\n+        decay: float >= 0. Learning rate decay over each update.\n \n     # References\n         - [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n@@ -304,6 +305,7 @@ class Adadelta(Optimizer):\n             It is recommended to leave it at the default value.\n         rho: float >= 0.\n         epsilon: float >= 0. Fuzz factor.\n+        decay: float >= 0. Learning rate decay over each update.\n \n     # References\n         - [Adadelta - an adaptive learning rate method](http://arxiv.org/abs/1212.5701)\n@@ -367,8 +369,10 @@ class Adam(Optimizer):\n \n     # Arguments\n         lr: float >= 0. Learning rate.\n-        beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n+        beta_1: float, 0 < beta < 1. Generally close to 1.\n+        beta_2: float, 0 < beta < 1. Generally close to 1.\n         epsilon: float >= 0. Fuzz factor.\n+        decay: float >= 0. Learning rate decay over each update.\n \n     # References\n         - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n@@ -437,6 +441,7 @@ class Adamax(Optimizer):\n         lr: float >= 0. Learning rate.\n         beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n         epsilon: float >= 0. Fuzz factor.\n+        decay: float >= 0. Learning rate decay over each update.\n \n     # References\n         - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n\n@@ -1,7 +1,7 @@\n-'''Fairly basic set of tools for real-time data augmentation on image data.\n+\"\"\"Fairly basic set of tools for real-time data augmentation on image data.\n Can easily be extended to include new transformations,\n new preprocessing methods, etc...\n-'''\n+\"\"\"\n from __future__ import absolute_import\n from __future__ import print_function\n \n@@ -175,14 +175,14 @@ def img_to_array(img, dim_ordering='default'):\n \n \n def load_img(path, grayscale=False, target_size=None):\n-    '''Load an image into PIL format.\n+    \"\"\"Load an image into PIL format.\n \n     # Arguments\n         path: path to image file\n         grayscale: boolean\n         target_size: None (default to original size)\n             or (img_height, img_width)\n-    '''\n+    \"\"\"\n     from PIL import Image\n     img = Image.open(path)\n     if grayscale:\n@@ -201,7 +201,7 @@ def list_pictures(directory, ext='jpg|jpeg|bmp|png'):\n \n \n class ImageDataGenerator(object):\n-    '''Generate minibatches with\n+    \"\"\"Generate minibatches with\n     real-time data augmentation.\n \n     # Arguments\n@@ -237,7 +237,7 @@ class ImageDataGenerator(object):\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"th\".\n-    '''\n+    \"\"\"\n     def __init__(self,\n                  featurewise_center=False,\n                  samplewise_center=False,\n@@ -421,7 +421,7 @@ class ImageDataGenerator(object):\n             augment=False,\n             rounds=1,\n             seed=None):\n-        '''Required for featurewise_center, featurewise_std_normalization\n+        \"\"\"Required for featurewise_center, featurewise_std_normalization\n         and zca_whitening.\n \n         # Arguments\n@@ -433,7 +433,7 @@ class ImageDataGenerator(object):\n             rounds: If `augment`,\n                 how many augmentation passes to do over the data\n             seed: random seed.\n-        '''\n+        \"\"\"\n         X = np.asarray(X)\n         if X.ndim != 4:\n             raise ValueError('Input to `.fit()` should have rank 4. '\n\n@@ -7,7 +7,7 @@ from six.moves import range\n \n def pad_sequences(sequences, maxlen=None, dtype='int32',\n                   padding='pre', truncating='pre', value=0.):\n-    '''Pads each sequence to the same length:\n+    \"\"\"Pads each sequence to the same length:\n     the length of the longest sequence.\n \n     If maxlen is provided, any sequence longer\n@@ -28,7 +28,7 @@ def pad_sequences(sequences, maxlen=None, dtype='int32',\n \n     # Returns\n         x: numpy array with dimensions (number_of_sequences, maxlen)\n-    '''\n+    \"\"\"\n     lengths = [len(s) for s in sequences]\n \n     nb_samples = len(sequences)\n@@ -70,7 +70,7 @@ def pad_sequences(sequences, maxlen=None, dtype='int32',\n \n \n def make_sampling_table(size, sampling_factor=1e-5):\n-    '''This generates an array where the ith element\n+    \"\"\"This generates an array where the ith element\n     is the probability that a word of rank i would be sampled,\n     according to the sampling distribution used in word2vec.\n \n@@ -84,7 +84,7 @@ def make_sampling_table(size, sampling_factor=1e-5):\n \n     # Arguments\n         size: int, number of possible words to sample.\n-    '''\n+    \"\"\"\n     gamma = 0.577\n     rank = np.array(list(range(size)))\n     rank[0] = 1\n@@ -97,7 +97,7 @@ def make_sampling_table(size, sampling_factor=1e-5):\n def skipgrams(sequence, vocabulary_size,\n               window_size=4, negative_samples=1., shuffle=True,\n               categorical=False, sampling_table=None):\n-    '''Take a sequence (list of indexes of words),\n+    \"\"\"Take a sequence (list of indexes of words),\n     returns couples of [word_index, other_word index] and labels (1s or 0s),\n     where label = 1 if 'other_word' belongs to the context of 'word',\n     and label=0 if 'other_word' is randomly sampled\n@@ -119,7 +119,7 @@ def skipgrams(sequence, vocabulary_size,\n     # Notes\n         By convention, index 0 in the vocabulary is\n         a non-word and will be skipped.\n-    '''\n+    \"\"\"\n     couples = []\n     labels = []\n     for i, wi in enumerate(sequence):\n\n@@ -1,7 +1,7 @@\n # -*- coding: utf-8 -*-\n-'''These preprocessing utilities would greatly benefit\n+\"\"\"These preprocessing utilities would greatly benefit\n from a fast Cython rewrite.\n-'''\n+\"\"\"\n from __future__ import absolute_import\n from __future__ import division\n \n@@ -25,8 +25,8 @@ def base_filter():\n \n \n def text_to_word_sequence(text, filters=base_filter(), lower=True, split=\" \"):\n-    '''prune: sequence of characters to filter out\n-    '''\n+    \"\"\"prune: sequence of characters to filter out\n+    \"\"\"\n     if lower:\n         text = text.lower()\n     text = text.translate(maketrans(filters, split*len(filters)))\n@@ -42,7 +42,7 @@ def one_hot(text, n, filters=base_filter(), lower=True, split=\" \"):\n class Tokenizer(object):\n     def __init__(self, nb_words=None, filters=base_filter(),\n                  lower=True, split=' ', char_level=False):\n-        '''The class allows to vectorize a text corpus, by turning each\n+        \"\"\"The class allows to vectorize a text corpus, by turning each\n         text into either a sequence of integers (each integer being the index\n         of a token in a dictionary) or into a vector where the coefficient\n         for each token could be binary, based on word count, based on tf-idf...\n@@ -64,7 +64,7 @@ class Tokenizer(object):\n         split into lists of tokens. They will then be indexed or vectorized.\n \n         `0` is a reserved index that won't be assigned to any word.\n-        '''\n+        \"\"\"\n         self.word_counts = {}\n         self.word_docs = {}\n         self.filters = filters\n@@ -75,12 +75,12 @@ class Tokenizer(object):\n         self.char_level = char_level\n \n     def fit_on_texts(self, texts):\n-        '''Required before using texts_to_sequences or texts_to_matrix\n+        \"\"\"Required before using texts_to_sequences or texts_to_matrix\n \n         # Arguments\n             texts: can be a list of strings,\n                 or a generator of strings (for memory-efficiency)\n-        '''\n+        \"\"\"\n         self.document_count = 0\n         for text in texts:\n             self.document_count += 1\n@@ -107,9 +107,9 @@ class Tokenizer(object):\n             self.index_docs[self.word_index[w]] = c\n \n     def fit_on_sequences(self, sequences):\n-        '''Required before using sequences_to_matrix\n+        \"\"\"Required before using sequences_to_matrix\n         (if fit_on_texts was never called)\n-        '''\n+        \"\"\"\n         self.document_count = len(sequences)\n         self.index_docs = {}\n         for seq in sequences:\n@@ -121,19 +121,19 @@ class Tokenizer(object):\n                     self.index_docs[i] += 1\n \n     def texts_to_sequences(self, texts):\n-        '''Transforms each text in texts in a sequence of integers.\n+        \"\"\"Transforms each text in texts in a sequence of integers.\n         Only top \"nb_words\" most frequent words will be taken into account.\n         Only words known by the tokenizer will be taken into account.\n \n         Returns a list of sequences.\n-        '''\n+        \"\"\"\n         res = []\n         for vect in self.texts_to_sequences_generator(texts):\n             res.append(vect)\n         return res\n \n     def texts_to_sequences_generator(self, texts):\n-        '''Transforms each text in texts in a sequence of integers.\n+        \"\"\"Transforms each text in texts in a sequence of integers.\n         Only top \"nb_words\" most frequent words will be taken into account.\n         Only words known by the tokenizer will be taken into account.\n \n@@ -141,7 +141,7 @@ class Tokenizer(object):\n \n         # Arguments:\n             texts: list of strings.\n-        '''\n+        \"\"\"\n         nb_words = self.nb_words\n         for text in texts:\n             seq = text if self.char_level else text_to_word_sequence(text, self.filters, self.lower, self.split)\n@@ -156,25 +156,25 @@ class Tokenizer(object):\n             yield vect\n \n     def texts_to_matrix(self, texts, mode='binary'):\n-        '''Convert a list of texts to a Numpy matrix,\n+        \"\"\"Convert a list of texts to a Numpy matrix,\n         according to some vectorization mode.\n \n         # Arguments:\n             texts: list of strings.\n             modes: one of \"binary\", \"count\", \"tfidf\", \"freq\"\n-        '''\n+        \"\"\"\n         sequences = self.texts_to_sequences(texts)\n         return self.sequences_to_matrix(sequences, mode=mode)\n \n     def sequences_to_matrix(self, sequences, mode='binary'):\n-        '''Converts a list of sequences into a Numpy matrix,\n+        \"\"\"Converts a list of sequences into a Numpy matrix,\n         according to some vectorization mode.\n \n         # Arguments:\n             sequences: list of sequences\n                 (a sequence is a list of integer word indices).\n             modes: one of \"binary\", \"count\", \"tfidf\", \"freq\"\n-        '''\n+        \"\"\"\n         if not self.nb_words:\n             if self.word_index:\n                 nb_words = len(self.word_index) + 1\n\n@@ -40,7 +40,7 @@ else:\n \n def get_file(fname, origin, untar=False,\n              md5_hash=None, cache_subdir='datasets'):\n-    '''Downloads a file from a URL if it not already in the cache.\n+    \"\"\"Downloads a file from a URL if it not already in the cache.\n \n     Passing the MD5 hash will verify the file after download as well as if it is already present in the cache.\n \n@@ -53,7 +53,7 @@ def get_file(fname, origin, untar=False,\n \n     # Returns\n         Path to the downloaded file\n-    '''\n+    \"\"\"\n     datadir_base = os.path.expanduser(os.path.join('~', '.keras'))\n     if not os.access(datadir_base, os.W_OK):\n         datadir_base = os.path.join('/tmp', '.keras')\n@@ -124,7 +124,7 @@ def get_file(fname, origin, untar=False,\n \n \n def validate_file(fpath, md5_hash):\n-    '''Validates a file against a MD5 hash\n+    \"\"\"Validates a file against a MD5 hash\n \n     # Arguments\n         fpath: path to the file being validated\n@@ -132,7 +132,7 @@ def validate_file(fpath, md5_hash):\n \n     # Returns\n         Whether the file is valid\n-    '''\n+    \"\"\"\n     hasher = hashlib.md5()\n     with open(fpath, 'rb') as f:\n         buf = f.read()\n\n@@ -36,7 +36,8 @@ def make_tuple(*args):\n \n \n def func_dump(func):\n-    '''Serialize user defined function.'''\n+    \"\"\"Serialize user defined function.\n+    \"\"\"\n     code = marshal.dumps(func.__code__).decode('raw_unicode_escape')\n     defaults = func.__defaults__\n     if func.__closure__:\n@@ -47,7 +48,7 @@ def func_dump(func):\n \n \n def func_load(code, defaults=None, closure=None, globs=None):\n-    '''Deserialize user defined function.'''\n+    \"\"\"Deserialize user defined function.\"\"\"\n     if isinstance(code, (tuple, list)):  # unpack previous dump\n         code, defaults, closure = code\n     code = marshal.loads(code.encode('raw_unicode_escape'))\n@@ -62,12 +63,12 @@ def func_load(code, defaults=None, closure=None, globs=None):\n class Progbar(object):\n \n     def __init__(self, target, width=30, verbose=1, interval=0.01):\n-        '''Dislays a progress bar.\n+        \"\"\"Dislays a progress bar.\n \n         # Arguments:\n             target: Total number of steps expected.\n             interval: Minimum visual progress update interval (in seconds).\n-        '''\n+        \"\"\"\n         self.width = width\n         self.target = target\n         self.sum_values = {}\n@@ -80,14 +81,14 @@ class Progbar(object):\n         self.verbose = verbose\n \n     def update(self, current, values=[], force=False):\n-        '''Updates the progress bar.\n+        \"\"\"Updates the progress bar.\n \n         # Arguments\n             current: Index of current step.\n             values: List of tuples (name, value_for_last_step).\n                 The progress bar will display averages for these values.\n             force: Whether to force visual progress update.\n-        '''\n+        \"\"\"\n         for k, v in values:\n             if k not in self.sum_values:\n                 self.sum_values[k] = [v * (current - self.seen_so_far),\n\n@@ -6,7 +6,7 @@ from collections import defaultdict\n \n \n class HDF5Matrix():\n-    '''Representation of HDF5 dataset which can be used instead of a\n+    \"\"\"Representation of HDF5 dataset which can be used instead of a\n     Numpy array.\n \n     # Example\n@@ -29,7 +29,7 @@ class HDF5Matrix():\n         end: int, end of desired slice of the specified dataset\n         normalizer: function to be called on data when retrieved\n \n-    '''\n+    \"\"\"\n     refs = defaultdict(int)\n \n     def __init__(self, datapath, dataset, start=0, end=None, normalizer=None):\n\n@@ -9,7 +9,8 @@ from .. import backend as K\n \n \n def layer_from_config(config, custom_objects=None):\n-    '''\n+    \"\"\"Instantiate a layer from a config dictionary.\n+\n     # Arguments\n         config: dict of the form {'class_name': str, 'config': dict}\n         custom_objects: dict mapping class names (or function names)\n@@ -17,7 +18,7 @@ def layer_from_config(config, custom_objects=None):\n \n     # Returns\n         Layer instance (may be Model, Sequential, Layer...)\n-    '''\n+    \"\"\"\n     # Insert custom layers into globals so they can\n     # be accessed by `get_from_module`.\n     if custom_objects:\n@@ -44,14 +45,14 @@ def layer_from_config(config, custom_objects=None):\n \n def print_summary(layers, relevant_nodes=None,\n                   line_length=100, positions=[.33, .55, .67, 1.]):\n-    '''Prints a summary of a layer\n+    \"\"\"Prints a summary of a layer.\n \n     # Arguments\n         layers: list of layers to print summaries of\n         relevant_nodes: list of relevant nodes\n         line_length: total length of printed lines\n         positions: relative or absolute positions of log elements in each line\n-    '''\n+    \"\"\"\n     if positions[-1] <= 1:\n         positions = [int(line_length * p) for p in positions]\n     # header names for the different log elements\n\n@@ -7,7 +7,8 @@ from .. import backend as K\n \n \n def to_categorical(y, nb_classes=None):\n-    '''Convert class vector (integers from 0 to nb_classes) to binary class matrix, for use with categorical_crossentropy.\n+    \"\"\"Converts class vector (integers from 0 to nb_classes)\n+    to binary class matrix, for use with categorical_crossentropy.\n \n     # Arguments\n         y: class vector to be converted into a matrix\n@@ -15,7 +16,7 @@ def to_categorical(y, nb_classes=None):\n \n     # Returns\n         A binary matrix representation of the input.\n-    '''\n+    \"\"\"\n     y = np.array(y, dtype='int')\n     if not nb_classes:\n         nb_classes = np.max(y)+1\n@@ -61,11 +62,11 @@ def categorical_probas_to_classes(p):\n \n \n def convert_kernel(kernel, dim_ordering='default'):\n-    '''Converts a kernel matrix (Numpy array)\n+    \"\"\"Converts a kernel matrix (Numpy array)\n     from Theano format to TensorFlow format\n     (or reciprocally, since the transformation\n     is its own inverse).\n-    '''\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = K.image_dim_ordering()\n     if not 4 <= kernel.ndim <= 5:\n@@ -83,7 +84,8 @@ def convert_kernel(kernel, dim_ordering='default'):\n     return np.copy(kernel[slices])\n \n \n-def conv_output_length(input_length, filter_size, border_mode, stride, dilation=1):\n+def conv_output_length(input_length, filter_size,\n+                       border_mode, stride, dilation=1):\n     if input_length is None:\n         return None\n     assert border_mode in {'same', 'valid', 'full'}\n\n@@ -11,13 +11,13 @@ from .. import backend as K\n def get_test_data(nb_train=1000, nb_test=500, input_shape=(10,),\n                   output_shape=(2,),\n                   classification=True, nb_class=2):\n-    '''\n+    \"\"\"\n         classification=True overrides output_shape\n         (i.e. output_shape is set to (1,)) and the output\n         consists in integers in [0, nb_class-1].\n \n         Otherwise: float output with shape output_shape.\n-    '''\n+    \"\"\"\n     nb_sample = nb_train + nb_test\n     if classification:\n         y = np.random.randint(0, nb_class, size=(nb_sample,))\n@@ -38,9 +38,9 @@ def get_test_data(nb_train=1000, nb_test=500, input_shape=(10,),\n def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n                input_data=None, expected_output=None,\n                expected_output_dtype=None, fixed_batch_size=False):\n-    '''Test routine for a layer with a single input tensor\n+    \"\"\"Test routine for a layer with a single input tensor\n     and single output tensor.\n-    '''\n+    \"\"\"\n     if input_data is None:\n         assert input_shape\n         if not input_dtype:\n@@ -121,8 +121,8 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n \n \n def keras_test(func):\n-    '''Clean up after tensorflow tests.\n-    '''\n+    \"\"\"Clean up after tensorflow tests.\n+    \"\"\"\n     @six.wraps(func)\n     def wrapper(*args, **kwargs):\n         output = func(*args, **kwargs)\n\n@@ -9,7 +9,7 @@ from ..models import Sequential\n \n \n class BaseWrapper(object):\n-    '''Base class for the Keras scikit-learn wrapper.\n+    \"\"\"Base class for the Keras scikit-learn wrapper.\n \n     Warning: This class should not be used directly.\n     Use descendant classes instead.\n@@ -47,7 +47,7 @@ class BaseWrapper(object):\n     those you could pass to `sk_params`, including fitting parameters.\n     In other words, you could use `grid_search` to search for the best\n     `batch_size` or `nb_epoch` as well as the model parameters.\n-    '''\n+    \"\"\"\n \n     def __init__(self, build_fn=None, **sk_params):\n         self.build_fn = build_fn\n@@ -55,13 +55,13 @@ class BaseWrapper(object):\n         self.check_params(sk_params)\n \n     def check_params(self, params):\n-        '''Check for user typos in \"params\" keys to avoid\n+        \"\"\"Check for user typos in \"params\" keys to avoid\n         unwanted usage of default values\n \n         # Arguments\n             params: dictionary\n                 The parameters to be checked\n-        '''\n+        \"\"\"\n         legal_params_fns = [Sequential.fit, Sequential.predict,\n                             Sequential.predict_classes, Sequential.evaluate]\n         if self.build_fn is None:\n@@ -81,7 +81,7 @@ class BaseWrapper(object):\n                 raise ValueError('{} is not a legal parameter'.format(params_name))\n \n     def get_params(self, deep=True):\n-        '''Get parameters for this estimator.\n+        \"\"\"Get parameters for this estimator.\n \n         # Arguments\n             deep: boolean, optional\n@@ -91,13 +91,13 @@ class BaseWrapper(object):\n         # Returns\n             params : dict\n                 Dictionary of parameter names mapped to their values.\n-        '''\n+        \"\"\"\n         res = copy.deepcopy(self.sk_params)\n         res.update({'build_fn': self.build_fn})\n         return res\n \n     def set_params(self, **params):\n-        '''Set the parameters of this estimator.\n+        \"\"\"Set the parameters of this estimator.\n \n         # Arguments\n         params: dict\n@@ -105,13 +105,13 @@ class BaseWrapper(object):\n \n         # Returns\n             self\n-        '''\n+        \"\"\"\n         self.check_params(params)\n         self.sk_params.update(params)\n         return self\n \n     def fit(self, X, y, **kwargs):\n-        '''Construct a new model with build_fn and fit the model according\n+        \"\"\"Construct a new model with build_fn and fit the model according\n         to the given training data.\n \n         # Arguments\n@@ -126,7 +126,7 @@ class BaseWrapper(object):\n         # Returns\n             history : object\n                 details about the training history at each epoch.\n-        '''\n+        \"\"\"\n \n         if self.build_fn is None:\n             self.model = self.__call__(**self.filter_sk_params(self.__call__))\n@@ -150,7 +150,7 @@ class BaseWrapper(object):\n         return history\n \n     def filter_sk_params(self, fn, override={}):\n-        '''Filter sk_params and return those in fn's arguments\n+        \"\"\"Filter sk_params and return those in fn's arguments\n \n         # Arguments\n             fn : arbitrary function\n@@ -159,7 +159,7 @@ class BaseWrapper(object):\n         # Returns\n             res : dictionary dictionary containing variables\n                 in both sk_params and fn's arguments.\n-        '''\n+        \"\"\"\n         res = {}\n         fn_args = inspect.getargspec(fn)[0]\n         for name, value in self.sk_params.items():\n@@ -170,11 +170,11 @@ class BaseWrapper(object):\n \n \n class KerasClassifier(BaseWrapper):\n-    '''Implementation of the scikit-learn classifier API for Keras.\n-    '''\n+    \"\"\"Implementation of the scikit-learn classifier API for Keras.\n+    \"\"\"\n \n     def predict(self, X, **kwargs):\n-        '''Returns the class predictions for the given test data.\n+        \"\"\"Returns the class predictions for the given test data.\n \n         # Arguments\n             X: array-like, shape `(n_samples, n_features)`\n@@ -186,12 +186,12 @@ class KerasClassifier(BaseWrapper):\n         # Returns\n             preds: array-like, shape `(n_samples,)`\n                 Class predictions.\n-        '''\n+        \"\"\"\n         kwargs = self.filter_sk_params(Sequential.predict_classes, kwargs)\n         return self.model.predict_classes(X, **kwargs)\n \n     def predict_proba(self, X, **kwargs):\n-        '''Returns class probability estimates for the given test data.\n+        \"\"\"Returns class probability estimates for the given test data.\n \n         # Arguments\n             X: array-like, shape `(n_samples, n_features)`\n@@ -207,7 +207,7 @@ class KerasClassifier(BaseWrapper):\n                 tp match the scikit-learn API,\n                 will return an array of shape '(n_samples, 2)'\n                 (instead of `(n_sample, 1)` as in Keras).\n-        '''\n+        \"\"\"\n         kwargs = self.filter_sk_params(Sequential.predict_proba, kwargs)\n         probs = self.model.predict_proba(X, **kwargs)\n \n@@ -218,7 +218,7 @@ class KerasClassifier(BaseWrapper):\n         return probs\n \n     def score(self, X, y, **kwargs):\n-        '''Returns the mean accuracy on the given test data and labels.\n+        \"\"\"Returns the mean accuracy on the given test data and labels.\n \n         # Arguments\n             X: array-like, shape `(n_samples, n_features)`\n@@ -232,7 +232,7 @@ class KerasClassifier(BaseWrapper):\n         # Returns\n             score: float\n                 Mean accuracy of predictions on X wrt. y.\n-        '''\n+        \"\"\"\n         kwargs = self.filter_sk_params(Sequential.evaluate, kwargs)\n \n         loss_name = self.model.loss\n@@ -253,11 +253,11 @@ class KerasClassifier(BaseWrapper):\n \n \n class KerasRegressor(BaseWrapper):\n-    '''Implementation of the scikit-learn regressor API for Keras.\n-    '''\n+    \"\"\"Implementation of the scikit-learn regressor API for Keras.\n+    \"\"\"\n \n     def predict(self, X, **kwargs):\n-        '''Returns predictions for the given test data.\n+        \"\"\"Returns predictions for the given test data.\n \n         # Arguments\n             X: array-like, shape `(n_samples, n_features)`\n@@ -268,12 +268,12 @@ class KerasRegressor(BaseWrapper):\n         # Returns\n             preds: array-like, shape `(n_samples,)`\n                 Predictions.\n-        '''\n+        \"\"\"\n         kwargs = self.filter_sk_params(Sequential.predict, kwargs)\n         return np.squeeze(self.model.predict(X, **kwargs))\n \n     def score(self, X, y, **kwargs):\n-        '''Returns the mean loss on the given test data and labels.\n+        \"\"\"Returns the mean loss on the given test data and labels.\n \n         # Arguments\n             X: array-like, shape `(n_samples, n_features)`\n@@ -287,7 +287,7 @@ class KerasRegressor(BaseWrapper):\n         # Returns\n             score: float\n                 Mean accuracy of predictions on X wrt. y.\n-        '''\n+        \"\"\"\n         kwargs = self.filter_sk_params(Sequential.evaluate, kwargs)\n         loss = self.model.evaluate(X, y, **kwargs)\n         if isinstance(loss, list):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
