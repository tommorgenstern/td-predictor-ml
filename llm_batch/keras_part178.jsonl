{"custom_id": "keras#e98379b7d952caa1e24a139333d68ca711f64eea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 193 | Contributors (this commit): 8 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ setup(name='Keras',\n       url='https://github.com/fchollet/keras',\n       download_url='https://github.com/fchollet/keras/tarball/2.0.0',\n       license='MIT',\n-      install_requires=['tensorflow', 'pyyaml', 'six'],\n+      install_requires=['theano', 'pyyaml', 'six'],\n       extras_require={\n           'h5py': ['h5py'],\n           'visualize': ['pydot-ng'],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c45f48eaeab2a9c9a809488950f07d83abe18411", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 33 | Churn Cumulative: 312 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -51,20 +51,45 @@ from keras.datasets import mnist\n from keras.models import Model\r\n from keras.layers import Activation\r\n from keras.layers import UpSampling2D, Conv2D, MaxPooling2D\r\n-from keras.layers import Input, BatchNormalization\r\n+from keras.layers import Input, BatchNormalization, ELU\r\n import matplotlib.pyplot as plt\r\n import keras.backend as K\r\n from keras import layers\r\n \r\n \r\n-def convresblock(x, nfeats=8, ksize=3, nskipped=2):\r\n-    ''' The proposed residual block from [4]'''\r\n+def convresblock(x, nfeats=8, ksize=3, nskipped=2, elu=True):\r\n+    \"\"\"The proposed residual block from [4].\r\n+    \r\n+    Running with elu=True will use ELU nonlinearity and running with\r\n+    elu=False will use BatchNorm + RELU nonlinearity.  While ELU's are fast\r\n+    due to the fact they do not suffer from BatchNorm overhead, they may\r\n+    overfit because they do not offer the stochastic element of the batch\r\n+    formation process of BatchNorm, which acts as a good regularizer.\r\n+\r\n+    # Arguments\r\n+        x: 4D tensor, the tensor to feed through the block\r\n+        nfeats: Integer, number of feature maps for conv layers.\r\n+        ksize: Integer, width and height of conv kernels in first convolution.\r\n+        nskipped: Integer, number of conv layers for the residual function.\r\n+        elu: Boolean, whether to use ELU or BN+RELU.\r\n+\r\n+    # Input shape\r\n+        4D tensor with shape:\r\n+        `(batch, channels, rows, cols)`\r\n+\r\n+    # Output shape\r\n+        4D tensor with shape:\r\n+        `(batch, filters, rows, cols)`\r\n+    \"\"\"                                            \r\n     y0 = Conv2D(nfeats, ksize, padding='same')(x)\r\n     y = y0\r\n     for i in range(nskipped):\r\n+        if elu:\r\n+            y = ELU()(y)\r\n+        else:\r\n             y = BatchNormalization(axis=1)(y)\r\n             y = Activation('relu')(y)\r\n-        y = Conv2D(nfeats, ksize, padding='same')(y)\r\n+        y = Conv2D(nfeats, 1, padding='same')(y)\r\n     return layers.add([y0, y])\r\n \r\n \r\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0cc52cf2511fb024ebb6045544b58fe6334fb105", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 28535 | Contributors (this commit): 146 | Commits (past 90d): 183 | Contributors (cumulative): 219 | DMM Complexity: None\n\nDIFF:\n@@ -630,7 +630,7 @@ def random_uniform_variable(shape, low, high, dtype=None,\n \n     # Arguments\n         shape: Tuple of integers, shape of returned Keras variable.\n-        low: Float, lower boundary of the output inteval.\n+        low: Float, lower boundary of the output interval.\n         high: Float, upper boundary of the output interval.\n         dtype: String, dtype of returned Keras variable.\n         name: String, name of returned Keras variable.\n@@ -1906,7 +1906,7 @@ def one_hot(indices, num_classes):\n \n \n def reverse(x, axes):\n-    \"\"\"Reverse a tensor along the the specified axes.\n+    \"\"\"Reverse a tensor along the specified axes.\n \n     # Arguments\n         x: Tensor to reverse.\n@@ -3153,7 +3153,7 @@ def random_binomial(shape, p=0.0, dtype=None, seed=None):\n \n     # Arguments\n         shape: A tuple of integers, the shape of tensor to create.\n-        p: A float, `0. <= p <= 1`, probability of binomlai distribution.\n+        p: A float, `0. <= p <= 1`, probability of binomial distribution.\n         dtype: String, dtype of returned tensor.\n         seed: Integer, random seed.\n \n\n@@ -1002,7 +1002,7 @@ def one_hot(indices, num_classes):\n \n \n def reverse(x, axes):\n-    \"\"\"Reverse a tensor along the the specified axes\n+    \"\"\"Reverse a tensor along the specified axes\n     \"\"\"\n     if isinstance(axes, int):\n         axes = [axes]\n\n@@ -1594,7 +1594,7 @@ class Model(Container):\n                 In this case you should make sure to specify\n                 sample_weight_mode=\"temporal\" in compile().\n             class_weight: optional dictionary mapping\n-                lass indices (integers) to\n+                class indices (integers) to\n                 a weight (float) to apply to the model's loss for the samples\n                 from this class during training.\n                 This can be useful to tell the model to \"pay more attention\" to\n\n@@ -27,7 +27,7 @@ class Masking(Layer):\n \n     For each timestep in the input tensor (dimension #1 in the tensor),\n     if all values in the input tensor at that timestep\n-    are equal to `mask_value`, then the timestep will masked (skipped)\n+    are equal to `mask_value`, then the timestep will be masked (skipped)\n     in all downstream layers (as long as they support masking).\n \n     If any downstream layer does not support masking yet receives such\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#51c5f3f9c677eee3b900d9eda633da86f82df31c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 23 | Churn Cumulative: 2210 | Contributors (this commit): 6 | Commits (past 90d): 63 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -148,7 +148,7 @@ legacy_gaussiannoise_support = generate_legacy_interface(\n     conversions=[('sigma', 'stddev')])\n \n \n-def lstm_args_preprocessor(args, kwargs):\n+def recurrent_args_preprocessor(args, kwargs):\n     converted = []\n     if 'forget_bias_init' in kwargs:\n         if kwargs['forget_bias_init'] == 'one':\n@@ -160,6 +160,15 @@ def lstm_args_preprocessor(args, kwargs):\n             warnings.warn('The `forget_bias_init` argument '\n                           'has been ignored. Use `unit_forget_bias=True` '\n                           'instead to intialize with ones.')\n+    if 'input_dim' in kwargs:\n+        input_length = kwargs.pop('input_length', None)\n+        input_dim = kwargs.pop('input_dim')\n+        input_shape = (input_length, input_dim)\n+        kwargs['input_shape'] = input_shape\n+        converted.append(('input_dim', 'input_shape'))\n+        warnings.warn('The `input_dim` and `input_length` arguments '\n+                      'in recurrent layers are deprecated. '\n+                      'Use `input_shape` instead.')\n     return args, kwargs, converted\n \n legacy_recurrent_support = generate_legacy_interface(\n@@ -177,7 +186,7 @@ legacy_recurrent_support = generate_legacy_interface(\n     value_conversions={'consume_less': {'cpu': 0,\n                                         'mem': 1,\n                                         'gpu': 2}},\n-    preprocessor=lstm_args_preprocessor)\n+    preprocessor=recurrent_args_preprocessor)\n \n legacy_gaussiandropout_support = generate_legacy_interface(\n     allowed_positional_args=['rate'],\n\n@@ -113,6 +113,16 @@ def test_lstm_legacy_interface():\n     new_layer = keras.layers.LSTM(2, input_shape=[3, 5], name='d', implementation=1)\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n+    old_layer = keras.layers.LSTM(input_dim=5, input_length=3,\n+                                  output_dim=2, name='d', consume_less='mem')\n+    new_layer = keras.layers.LSTM(2, input_shape=[3, 5], name='d', implementation=1)\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n+\n+    old_layer = keras.layers.LSTM(input_dim=5,\n+                                  output_dim=2, name='d', consume_less='mem')\n+    new_layer = keras.layers.LSTM(2, input_shape=[None, 5], name='d', implementation=1)\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n+\n     old_layer = keras.layers.LSTM(input_shape=[3, 5], output_dim=2, name='d', consume_less='gpu')\n     new_layer = keras.layers.LSTM(2, input_shape=[3, 5], name='d', implementation=2)\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ce3093a3b219e1a7e3fe352e78484383d77f3d3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 315 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -168,7 +168,8 @@ y = img_input\n for i in range(nlayers):\r\n     y_prepool = convresblock(y, nfeats=nfeats_all[i + 1], ksize=ksize)\r\n     y = MaxPooling2D(pool_size=(pool_sizes[i], pool_sizes[i]))(y_prepool)\r\n-    wheres[i] = layers.Lambda(getwhere, output_shape=lambda x: x[0])([y_prepool, y])\r\n+    wheres[i] = layers.Lambda(\r\n+        getwhere, output_shape=lambda x: x[0])([y_prepool, y])\r\n \r\n # Now build the decoder, and use the stored 'where' masks to place the features\r\n for i in range(nlayers):\r\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e8484633473c340defbe03a092be2d4856d56302", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 8 | Methods Changed: 10 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 74 | Churn Cumulative: 648 | Contributors (this commit): 10 | Commits (past 90d): 8 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,7 +6,7 @@ import numpy as np\n import matplotlib.pyplot as plt\n from scipy.stats import norm\n \n-from keras.layers import Input, Dense, Lambda\n+from keras.layers import Input, Dense, Lambda, Layer\n from keras.models import Model\n from keras import backend as K\n from keras import metrics\n@@ -19,6 +19,7 @@ intermediate_dim = 256\n epochs = 50\n epsilon_std = 1.0\n \n+\n x = Input(batch_shape=(batch_size, original_dim))\n h = Dense(intermediate_dim, activation='relu')(x)\n z_mean = Dense(latent_dim)(h)\n@@ -41,13 +42,34 @@ h_decoded = decoder_h(z)\n x_decoded_mean = decoder_mean(h_decoded)\n \n \n-def vae_loss(x, x_decoded_mean):\n+# placeholder loss\n+def zero_loss(y_true, y_pred):\n+    return K.zeros_like(y_pred)\n+\n+\n+# Custom loss layer\n+class CustomVariationalLayer(Layer):\n+    def __init__(self, **kwargs):\n+        self.is_placeholder = True\n+        super(CustomVariationalLayer, self).__init__(**kwargs)\n+\n+    def vae_loss(self, x, x_decoded_mean):\n         xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n         kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n-    return xent_loss + kl_loss\n+        return K.mean(xent_loss + kl_loss)\n+\n+    def call(self, inputs):\n+        x = inputs[0]\n+        x_decoded_mean = inputs[1]\n+        loss = self.vae_loss(x, x_decoded_mean)\n+        self.add_loss(loss, inputs=inputs)\n+        # we don't use this output, but it has to have the correct shape:\n+        return K.ones_like(x)\n+\n+loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n+vae = Model(x, [loss_layer])\n+vae.compile(optimizer='rmsprop', loss=[zero_loss])\n \n-vae = Model(x, x_decoded_mean)\n-vae.compile(optimizer='rmsprop', loss=vae_loss)\n \n # train the VAE on MNIST digits\n (x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n@@ -7,7 +7,7 @@ import numpy as np\n import matplotlib.pyplot as plt\n from scipy.stats import norm\n \n-from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n+from keras.layers import Input, Dense, Lambda, Flatten, Reshape, Layer\n from keras.layers import Conv2D, Conv2DTranspose\n from keras.models import Model\n from keras import backend as K\n@@ -106,17 +106,37 @@ x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n \n \n-def vae_loss(x, x_decoded_mean):\n-    # NOTE: binary_crossentropy expects a batch_size by dim\n-    # for x and x_decoded_mean, so we MUST flatten these!\n-    x = K.flatten(x)\n-    x_decoded_mean = K.flatten(x_decoded_mean)\n-    xent_loss = img_rows * img_cols * metrics.binary_crossentropy(x, x_decoded_mean)\n-    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n-    return xent_loss + kl_loss\n+# placeholder loss\n+def zero_loss(y_true, y_pred):\n+    return K.zeros_like(y_pred)\n \n-vae = Model(x, x_decoded_mean_squash)\n-vae.compile(optimizer='rmsprop', loss=vae_loss)\n+\n+# Custom loss layer\n+class CustomVariationalLayer(Layer):\n+    def __init__(self, **kwargs):\n+        self.is_placeholder = True\n+        super(CustomVariationalLayer, self).__init__(**kwargs)\n+\n+    def vae_loss(self, x, x_decoded_mean_squash):\n+        x = K.flatten(x)\n+        x_decoded_mean_squash = K.flatten(x_decoded_mean_squash)\n+        xent_loss = img_rows * img_cols * metrics.binary_crossentropy(x, x_decoded_mean_squash)\n+        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n+        return K.mean(xent_loss + kl_loss)\n+\n+    def call(self, inputs):\n+        x = inputs[0]\n+        x_decoded_mean_squash = inputs[1]\n+        loss = self.vae_loss(x, x_decoded_mean_squash)\n+        self.add_loss(loss, inputs=inputs)\n+        # we don't use this output, but it has to have the correct shape:\n+        return K.ones_like(x)\n+\n+\n+loss_layer = CustomVariationalLayer()([x, x_decoded_mean_squash])\n+\n+vae = Model(x, [loss_layer])\n+vae.compile(optimizer='rmsprop', loss=zero_loss)\n vae.summary()\n \n # train the VAE on MNIST digits\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7095aca51b92cdc621a5c97e7b150b31ee2f207e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 773 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -286,7 +286,7 @@ class Merge(Layer):\n \n         assert hasattr(mask, '__len__') and len(mask) == len(inputs)\n \n-        if self.mode in ['sum', 'mul', 'ave']:\n+        if self.mode in ['sum', 'mul', 'ave', 'max']:\n             masks = [K.expand_dims(m, 0) for m in mask if m is not None]\n             return K.all(K.concatenate(masks, axis=0), axis=0, keepdims=False)\n         elif self.mode == 'concat':\n@@ -361,6 +361,7 @@ class Merge(Layer):\n \n     @classmethod\n     def from_config(cls, config):\n+        config = config.copy()\n         mode_type = config.pop('mode_type')\n         if mode_type == 'function':\n             mode = globals()[config['mode']]\n@@ -406,7 +407,7 @@ def merge(inputs, mode='sum', concat_axis=-1,\n     ```\n     # Arguments\n         mode: String or lambda/function. If string, must be one\n-            of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot'.\n+            of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot', 'max'.\n             If lambda/function, it should take as input a list of tensors\n             and return a single tensor.\n         concat_axis: Integer, axis to use in mode `concat`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aff40d800891799dc9ed765617fcbfa665349d0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 119 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -62,6 +62,14 @@ model.compile(loss='mse', optimizer='rmsprop')\n print('Training')\n for i in range(epochs):\n     print('Epoch', i, '/', epochs)\n+\n+    # Note that the last state for sample i in a batch will\n+    # be used as initial state for sample i in the next batch.\n+    # Thus we are simultaneously training on batch_size series with\n+    # lower resolution than the original series contained in cos.\n+    # Each of these series are offset by one step and can be\n+    # extracted with cos[i::batch_size].\n+\n     model.fit(cos,\n               expected_output,\n               batch_size=batch_size,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8590e086a37d86105755859373135cbed7893c21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 12168 | Contributors (this commit): 62 | Commits (past 90d): 32 | Contributors (cumulative): 62 | DMM Complexity: None\n\nDIFF:\n@@ -107,9 +107,9 @@ class Dropout(Layer):\n             def dropped_inputs():\n                 return K.dropout(inputs, self.rate, noise_shape,\n                                  seed=self.seed)\n-            output = K.in_train_phase(dropped_inputs, inputs,\n+            return K.in_train_phase(dropped_inputs, inputs,\n                                     training=training)\n-        return output\n+        return inputs\n \n     def get_config(self):\n         config = {'rate': self.rate}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5cef75219abc339b64e35f221775692d3cf04b84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 14 | Churn Cumulative: 193 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -140,10 +140,12 @@ def conv_input_length(output_length, filter_size, padding, stride):\n \n \n def deconv_length(dim_size, stride_size, kernel_size, padding):\n-    if dim_size is not None:\n-        dim_size *= stride_size\n-    if padding == 'valid' and dim_size is not None:\n-        dim_size += max(kernel_size - stride_size, 0)\n-    if padding == 'full' and dim_size is not None:\n-        dim_size -= stride_size + kernel_size - 2\n+    if dim_size is None:\n+        return None\n+    if padding == 'valid':\n+        dim_size = dim_size * stride_size + max(kernel_size - stride_size, 0)\n+    elif padding == 'full':\n+        dim_size = dim_size * stride_size - (stride_size + kernel_size - 2)\n+    elif padding == 'same':\n+        dim_size = dim_size * stride_size\n     return dim_size\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0703d79606fa74da25a6dfbae95c583c8a609caf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 587 | Contributors (this commit): 12 | Commits (past 90d): 11 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -69,5 +69,14 @@ else:\n def backend():\n     \"\"\"Publicly accessible method\n     for determining the current backend.\n+\n+    # Returns\n+        String, the name of the backend Keras is currently using.\n+\n+    # Example\n+    ```python\n+        >>> keras.backend.backend()\n+        'tensorflow'\n+    ```\n     \"\"\"\n     return _BACKEND\n\n@@ -178,10 +178,10 @@ def is_keras_tensor(x):\n # Legacy methods\n \n def set_image_dim_ordering(dim_ordering):\n-    \"\"\"Sets the value of the image data format.\n+    \"\"\"Legacy setter for `image_data_format`.\n \n     # Arguments\n-        data_format: string. `'channels_first'` or `'channels_last'`.\n+        dim_ordering: string. `'tf'` or `'th'`.\n \n     # Example\n     ```python\n@@ -204,7 +204,7 @@ def set_image_dim_ordering(dim_ordering):\n \n \n def image_dim_ordering():\n-    \"\"\"Legacy getter for data format.\n+    \"\"\"Legacy getter for `image_data_format`.\n     \"\"\"\n     if _IMAGE_DATA_FORMAT == 'channels_first':\n         return 'th'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#02ba149e57123da9943fbae95410153c6da70c1e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 609 | Contributors (this commit): 14 | Commits (past 90d): 14 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -197,6 +197,8 @@ def func_load(code, defaults=None, closure=None, globs=None):\n     \"\"\"\n     if isinstance(code, (tuple, list)):  # unpack previous dump\n         code, defaults, closure = code\n+        if isinstance(defaults, list):\n+            defaults = tuple(defaults)\n     code = marshal.loads(code.encode('raw_unicode_escape'))\n     if globs is None:\n         globs = globals()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c88a11c378189443cf1f95ed14edd47c9666171a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 7646 | Contributors (this commit): 29 | Commits (past 90d): 50 | Contributors (cumulative): 29 | DMM Complexity: 0.0\n\nDIFF:\n@@ -269,7 +269,9 @@ class Layer(object):\n                           'dtype',\n                           'name',\n                           'trainable',\n-                          'weights'}\n+                          'weights',\n+                          'input_dtype',  # legacy\n+                          }\n         for kwarg in kwargs:\n             if kwarg not in allowed_kwargs:\n                 raise TypeError('Keyword argument not understood:', kwarg)\n@@ -292,8 +294,15 @@ class Layer(object):\n                     batch_size = None\n                 batch_input_shape = (batch_size,) + tuple(kwargs['input_shape'])\n             self.batch_input_shape = batch_input_shape\n-            dtype = kwargs.get('dtype', K.floatx())\n+\n+            # Set dtype.\n+            dtype = kwargs.get('dtype')\n+            if dtype is None:\n+                dtype = kwargs.get('input_dtype')\n+            if dtype is None:\n+                dtype = K.floatx()\n             self.dtype = dtype\n+\n         if 'weights' in kwargs:\n             self._initial_weights = kwargs['weights']\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b5a02391e003c33c8f8258a7e3d0736503c3c048", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 40 | Files Changed: 1 | Hunks: 12 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 88 | Churn Cumulative: 568 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -14,9 +14,9 @@ Time per epoch: 3s on CPU (core i7).\n '''\n from __future__ import print_function\n \n-from keras.models import Sequential\n+from keras.models import Sequential, Model\n from keras.layers.embeddings import Embedding\n-from keras.layers import Activation, Dense, Merge, Permute, Dropout\n+from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate\n from keras.layers import LSTM\n from keras.utils.data_utils import get_file\n from keras.preprocessing.sequence import pad_sequences\n@@ -93,7 +93,6 @@ def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n     return (pad_sequences(X, maxlen=story_maxlen),\n             pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n \n-\n try:\n     path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n except:\n@@ -153,13 +152,25 @@ print('answers_test shape:', answers_test.shape)\n print('-')\n print('Compiling...')\n \n+# placeholders\n+input_sequence = Input((story_maxlen,))\n+question = Input((query_maxlen,))\n+\n+# encoders\n # embed the input sequence into a sequence of vectors\n input_encoder_m = Sequential()\n input_encoder_m.add(Embedding(input_dim=vocab_size,\n-                              output_dim=64,\n-                              input_length=story_maxlen))\n+                              output_dim=64))\n input_encoder_m.add(Dropout(0.3))\n # output: (samples, story_maxlen, embedding_dim)\n+\n+# embed the input into a sequence of vectors of size query_maxlen\n+input_encoder_c = Sequential()\n+input_encoder_c.add(Embedding(input_dim=vocab_size,\n+                              output_dim=query_maxlen))\n+input_encoder_c.add(Dropout(0.3))\n+# output: (samples, story_maxlen, query_maxlen)\n+\n # embed the question into a sequence of vectors\n question_encoder = Sequential()\n question_encoder.add(Embedding(input_dim=vocab_size,\n@@ -167,44 +178,41 @@ question_encoder.add(Embedding(input_dim=vocab_size,\n                                input_length=query_maxlen))\n question_encoder.add(Dropout(0.3))\n # output: (samples, query_maxlen, embedding_dim)\n-# compute a 'match' between input sequence elements (which are vectors)\n-# and the question vector sequence\n-match = Sequential()\n-match.add(Merge([input_encoder_m, question_encoder],\n-                mode='dot',\n-                dot_axes=[2, 2]))\n-match.add(Activation('softmax'))\n-# output: (samples, story_maxlen, query_maxlen)\n-# embed the input into a single vector with size = story_maxlen:\n-input_encoder_c = Sequential()\n-input_encoder_c.add(Embedding(input_dim=vocab_size,\n-                              output_dim=query_maxlen,\n-                              input_length=story_maxlen))\n-input_encoder_c.add(Dropout(0.3))\n-# output: (samples, story_maxlen, query_maxlen)\n-# sum the match vector with the input vector:\n-response = Sequential()\n-response.add(Merge([match, input_encoder_c], mode='sum'))\n-# output: (samples, story_maxlen, query_maxlen)\n-response.add(Permute((2, 1)))  # output: (samples, query_maxlen, story_maxlen)\n \n-# concatenate the match vector with the question vector,\n-# and do logistic regression on top\n-answer = Sequential()\n-answer.add(Merge([response, question_encoder], mode='concat', concat_axis=-1))\n+# encode input sequence and questions (which are indices) to sequences of dense vectors\n+input_encoded_m = input_encoder_m(input_sequence)\n+input_encoded_c = input_encoder_c(input_sequence)\n+question_encoded = question_encoder(question)\n+\n+# compute a 'match' between the first input vector sequence\n+# and the question vector sequence\n+match = dot([input_encoded_m, question_encoded], axes=(2, 2))  # (samples, story_maxlen, query_maxlen)\n+match = Activation('softmax')(match)\n+\n+# add the match matrix with the second input vector sequence\n+response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n+response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n+\n+# concatenate the match matrix with the question vector sequence\n+answer = concatenate([response, question_encoded])\n+\n # the original paper uses a matrix multiplication for this reduction step.\n # we choose to use a RNN instead.\n-answer.add(LSTM(32))\n-# one regularization layer -- more would probably be needed.\n-answer.add(Dropout(0.3))\n-answer.add(Dense(vocab_size))\n-# we output a probability distribution over the vocabulary\n-answer.add(Activation('softmax'))\n+answer = LSTM(32)(answer)  # (samples, 32)\n \n-answer.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n+# one regularization layer -- more would probably be needed.\n+answer = Dropout(0.3)(answer)\n+answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n+# we output a probability distribution over the vocabulary\n+answer = Activation('softmax')(answer)\n+\n+# build the final model\n+model = Model([input_sequence, question], answer)\n+model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n                metrics=['accuracy'])\n-# Note: you could use a Graph model to avoid repeat the input twice\n-answer.fit([inputs_train, queries_train, inputs_train], answers_train,\n+\n+# train\n+model.fit([inputs_train, queries_train], answers_train,\n            batch_size=32,\n-           epochs=120,\n-           validation_data=([inputs_test, queries_test, inputs_test], answers_test))\n+           epochs=3,\n+           validation_data=([inputs_test, queries_test], answers_test))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1a0792ae13ce625d10ae881a19a9ee38c27e7e4f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 570 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -214,5 +214,5 @@ model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n # train\n model.fit([inputs_train, queries_train], answers_train,\n            batch_size=32,\n-           epochs=3,\n+           epochs=120,\n            validation_data=([inputs_test, queries_test], answers_test))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#459d7fe3d7aeb065c42249e955a4053270e81814", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 16 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 68 | Churn Cumulative: 1026 | Contributors (this commit): 8 | Commits (past 90d): 9 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -38,7 +38,8 @@ def tokenize(sent):\n def parse_stories(lines, only_supporting=False):\n     '''Parse stories provided in the bAbi tasks format\n \n-    If only_supporting is true, only the sentences that support the answer are kept.\n+    If only_supporting is true, only the sentences\n+    that support the answer are kept.\n     '''\n     data = []\n     story = []\n@@ -68,9 +69,12 @@ def parse_stories(lines, only_supporting=False):\n \n \n def get_stories(f, only_supporting=False, max_length=None):\n-    '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story.\n+    '''Given a file name, read the file,\n+    retrieve the stories,\n+    and then convert the sentences into a single story.\n \n-    If max_length is supplied, any stories longer than max_length tokens will be discarded.\n+    If max_length is supplied,\n+    any stories longer than max_length tokens will be discarded.\n     '''\n     data = parse_stories(f.readlines(), only_supporting=only_supporting)\n     flatten = lambda data: reduce(lambda x, y: x + y, data)\n@@ -85,7 +89,8 @@ def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n     for story, query, answer in data:\n         x = [word_idx[w] for w in story]\n         xq = [word_idx[w] for w in query]\n-        y = np.zeros(len(word_idx) + 1)  # let's not forget that index 0 is reserved\n+        # let's not forget that index 0 is reserved\n+        y = np.zeros(len(word_idx) + 1)\n         y[word_idx[answer]] = 1\n         X.append(x)\n         Xq.append(xq)\n@@ -115,7 +120,11 @@ print('Extracting stories for the challenge:', challenge_type)\n train_stories = get_stories(tar.extractfile(challenge.format('train')))\n test_stories = get_stories(tar.extractfile(challenge.format('test')))\n \n-vocab = sorted(reduce(lambda x, y: x | y, (set(story + q + [answer]) for story, q, answer in train_stories + test_stories)))\n+vocab = set()\n+for story, q, answer in train_stories + test_stories:\n+    vocab |= set(story + q + [answer])\n+vocab = sorted(vocab)\n+\n # Reserve 0 for masking via pad_sequences\n vocab_size = len(vocab) + 1\n story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n@@ -134,8 +143,14 @@ print('-')\n print('Vectorizing the word sequences...')\n \n word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n-inputs_train, queries_train, answers_train = vectorize_stories(train_stories, word_idx, story_maxlen, query_maxlen)\n-inputs_test, queries_test, answers_test = vectorize_stories(test_stories, word_idx, story_maxlen, query_maxlen)\n+inputs_train, queries_train, answers_train = vectorize_stories(train_stories,\n+                                                               word_idx,\n+                                                               story_maxlen,\n+                                                               query_maxlen)\n+inputs_test, queries_test, answers_test = vectorize_stories(test_stories,\n+                                                            word_idx,\n+                                                            story_maxlen,\n+                                                            query_maxlen)\n \n print('-')\n print('inputs: integer tensor of shape (samples, max_length)')\n@@ -179,14 +194,16 @@ question_encoder.add(Embedding(input_dim=vocab_size,\n question_encoder.add(Dropout(0.3))\n # output: (samples, query_maxlen, embedding_dim)\n \n-# encode input sequence and questions (which are indices) to sequences of dense vectors\n+# encode input sequence and questions (which are indices)\n+# to sequences of dense vectors\n input_encoded_m = input_encoder_m(input_sequence)\n input_encoded_c = input_encoder_c(input_sequence)\n question_encoded = question_encoder(question)\n \n # compute a 'match' between the first input vector sequence\n # and the question vector sequence\n-match = dot([input_encoded_m, question_encoded], axes=(2, 2))  # (samples, story_maxlen, query_maxlen)\n+# shape: `(samples, story_maxlen, query_maxlen)`\n+match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n match = Activation('softmax')(match)\n \n # add the match matrix with the second input vector sequence\n\n@@ -83,7 +83,8 @@ def tokenize(sent):\n def parse_stories(lines, only_supporting=False):\n     '''Parse stories provided in the bAbi tasks format\n \n-    If only_supporting is true, only the sentences that support the answer are kept.\n+    If only_supporting is true,\n+    only the sentences that support the answer are kept.\n     '''\n     data = []\n     story = []\n@@ -113,9 +114,11 @@ def parse_stories(lines, only_supporting=False):\n \n \n def get_stories(f, only_supporting=False, max_length=None):\n-    '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story.\n+    '''Given a file name, read the file, retrieve the stories,\n+    and then convert the sentences into a single story.\n \n-    If max_length is supplied, any stories longer than max_length tokens will be discarded.\n+    If max_length is supplied,\n+    any stories longer than max_length tokens will be discarded.\n     '''\n     data = parse_stories(f.readlines(), only_supporting=only_supporting)\n     flatten = lambda data: reduce(lambda x, y: x + y, data)\n@@ -130,7 +133,8 @@ def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n     for story, query, answer in data:\n         x = [word_idx[w] for w in story]\n         xq = [word_idx[w] for w in query]\n-        y = np.zeros(len(word_idx) + 1)  # let's not forget that index 0 is reserved\n+        # let's not forget that index 0 is reserved\n+        y = np.zeros(len(word_idx) + 1)\n         y[word_idx[answer]] = 1\n         xs.append(x)\n         xqs.append(xq)\n@@ -140,10 +144,13 @@ def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n RNN = recurrent.LSTM\n EMBED_HIDDEN_SIZE = 50\n SENT_HIDDEN_SIZE = 100\n-QUERy_HIDDEN_SIZE = 100\n+QUERY_HIDDEN_SIZE = 100\n BATCH_SIZE = 32\n EPOCHS = 40\n-print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE, QUERy_HIDDEN_SIZE))\n+print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n+                                                           EMBED_HIDDEN_SIZE,\n+                                                           SENT_HIDDEN_SIZE,\n+                                                           QUERY_HIDDEN_SIZE))\n \n try:\n     path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n@@ -164,7 +171,11 @@ challenge = 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt'\n train = get_stories(tar.extractfile(challenge.format('train')))\n test = get_stories(tar.extractfile(challenge.format('test')))\n \n-vocab = sorted(reduce(lambda x, y: x | y, (set(story + q + [answer]) for story, q, answer in train + test)))\n+vocab = set()\n+for story, q, answer in train + test:\n+    vocab |= set(story + q + [answer])\n+vocab = sorted(vocab)\n+\n # Reserve 0 for masking via pad_sequences\n vocab_size = len(vocab) + 1\n word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n@@ -203,6 +214,10 @@ model.compile(optimizer='adam',\n               metrics=['accuracy'])\n \n print('Training')\n-model.fit([x, xq], y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.05)\n-loss, acc = model.evaluate([tx, txq], ty, batch_size=BATCH_SIZE)\n+model.fit([x, xq], y,\n+          batch_size=BATCH_SIZE,\n+          epochs=EPOCHS,\n+          validation_split=0.05)\n+loss, acc = model.evaluate([tx, txq], ty,\n+                           batch_size=BATCH_SIZE)\n print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#10399242459eb222afece5edc5645ad458ea43c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 23106 | Contributors (this commit): 86 | Commits (past 90d): 88 | Contributors (cumulative): 103 | DMM Complexity: None\n\nDIFF:\n@@ -219,7 +219,7 @@ class _Conv(Layer):\n             'activation': activations.serialize(self.activation),\n             'use_bias': self.use_bias,\n             'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-            'bias_initializer': initializers.serialize(self.kernel_initializer),\n+            'bias_initializer': initializers.serialize(self.bias_initializer),\n             'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n             'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n             'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n\n@@ -857,7 +857,7 @@ class Dense(Layer):\n             'activation': activations.serialize(self.activation),\n             'use_bias': self.use_bias,\n             'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-            'bias_initializer': initializers.serialize(self.kernel_initializer),\n+            'bias_initializer': initializers.serialize(self.bias_initializer),\n             'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n             'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n             'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n\n@@ -175,7 +175,7 @@ class LocallyConnected1D(Layer):\n             'activation': activations.serialize(self.activation),\n             'use_bias': self.use_bias,\n             'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-            'bias_initializer': initializers.serialize(self.kernel_initializer),\n+            'bias_initializer': initializers.serialize(self.bias_initializer),\n             'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n             'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n             'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n@@ -432,7 +432,7 @@ class LocallyConnected2D(Layer):\n             'activation': activations.serialize(self.activation),\n             'use_bias': self.use_bias,\n             'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-            'bias_initializer': initializers.serialize(self.kernel_initializer),\n+            'bias_initializer': initializers.serialize(self.bias_initializer),\n             'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n             'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n             'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9cf7f816f28a6ac7d7a4264ae015addd72e57321", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 298 | Contributors (this commit): 10 | Commits (past 90d): 17 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -18,4 +18,4 @@ from . import losses\n from . import optimizers\r\n from . import regularizers\r\n \r\n-__version__ = '2.0.0'\r\n+__version__ = '2.0.1'\r\n\n@@ -3,12 +3,12 @@ from setuptools import find_packages\n \n \n setup(name='Keras',\n-      version='2.0.0',\n+      version='2.0.1',\n       description='Deep Learning for Python',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n-      download_url='https://github.com/fchollet/keras/tarball/2.0.0',\n+      download_url='https://github.com/fchollet/keras/tarball/2.0.1',\n       license='MIT',\n       install_requires=['theano', 'pyyaml', 'six'],\n       extras_require={\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#90529b222e7d10ee1a987197071aa60a0ef3ac46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 22 | Files Changed: 3 | Hunks: 22 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 44 | Churn Cumulative: 12560 | Contributors (this commit): 45 | Commits (past 90d): 72 | Contributors (cumulative): 52 | DMM Complexity: None\n\nDIFF:\n@@ -370,9 +370,9 @@ class Conv2D(_Conv):\n             one of `channels_last` (default) or `channels_first`.\n             The ordering of the dimensions in the inputs.\n             `channels_last` corresponds to inputs with shape\n-            `(batch, width, height, channels)` while `channels_first`\n+            `(batch, height, width, channels)` while `channels_first`\n             corresponds to inputs with shape\n-            `(batch, channels, width, height)`.\n+            `(batch, channels, height, width)`.\n             It defaults to the `image_data_format` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"channels_last\".\n@@ -620,9 +620,9 @@ class Conv2DTranspose(Conv2D):\n             one of `channels_last` (default) or `channels_first`.\n             The ordering of the dimensions in the inputs.\n             `channels_last` corresponds to inputs with shape\n-            `(batch, width, height, channels)` while `channels_first`\n+            `(batch, height, width, channels)` while `channels_first`\n             corresponds to inputs with shape\n-            `(batch, channels, width, height)`.\n+            `(batch, channels, height, width)`.\n             It defaults to the `image_data_format` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"channels_last\".\n@@ -835,9 +835,9 @@ class SeparableConv2D(Conv2D):\n             one of `channels_last` (default) or `channels_first`.\n             The ordering of the dimensions in the inputs.\n             `channels_last` corresponds to inputs with shape\n-            `(batch, width, height, channels)` while `channels_first`\n+            `(batch, height, width, channels)` while `channels_first`\n             corresponds to inputs with shape\n-            `(batch, channels, width, height)`.\n+            `(batch, channels, height, width)`.\n             It defaults to the `image_data_format` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"channels_last\".\n@@ -1077,9 +1077,9 @@ class UpSampling2D(Layer):\n             one of `channels_last` (default) or `channels_first`.\n             The ordering of the dimensions in the inputs.\n             `channels_last` corresponds to inputs with shape\n-            `(batch, width, height, channels)` while `channels_first`\n+            `(batch, height, width, channels)` while `channels_first`\n             corresponds to inputs with shape\n-            `(batch, channels, width, height)`.\n+            `(batch, channels, height, width)`.\n             It defaults to the `image_data_format` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"channels_last\".\n@@ -1267,9 +1267,9 @@ class ZeroPadding2D(Layer):\n             one of `channels_last` (default) or `channels_first`.\n             The ordering of the dimensions in the inputs.\n             `channels_last` corresponds to inputs with shape\n-            `(batch, width, height, channels)` while `channels_first`\n+            `(batch, height, width, channels)` while `channels_first`\n             corresponds to inputs with shape\n-            `(batch, channels, width, height)`.\n+            `(batch, channels, height, width)`.\n             It defaults to the `image_data_format` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"channels_last\".\n@@ -1509,9 +1509,9 @@ class Cropping2D(Layer):\n             one of `channels_last` (default) or `channels_first`.\n             The ordering of the dimensions in the inputs.\n             `channels_last` corresponds to inputs with shape\n-            `(batch, width, height, channels)` while `channels_first`\n+            `(batch, height, width, channels)` while `channels_first`\n             corresponds to inputs with shape\n-            `(batch, channels, width, height)`.\n+            `(batch, channels, height, width)`.\n             It defaults to the `image_data_format` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"channels_last\".\n\n@@ -226,9 +226,9 @@ class LocallyConnected2D(Layer):\n             one of `channels_last` (default) or `channels_first`.\n             The ordering of the dimensions in the inputs.\n             `channels_last` corresponds to inputs with shape\n-            `(batch, width, height, channels)` while `channels_first`\n+            `(batch, height, width, channels)` while `channels_first`\n             corresponds to inputs with shape\n-            `(batch, channels, width, height)`.\n+            `(batch, channels, height, width)`.\n             It defaults to the `image_data_format` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"channels_last\".\n\n@@ -180,9 +180,9 @@ class MaxPooling2D(_Pooling2D):\n             one of `channels_last` (default) or `channels_first`.\n             The ordering of the dimensions in the inputs.\n             `channels_last` corresponds to inputs with shape\n-            `(batch, width, height, channels)` while `channels_first`\n+            `(batch, height, width, channels)` while `channels_first`\n             corresponds to inputs with shape\n-            `(batch, channels, width, height)`.\n+            `(batch, channels, height, width)`.\n             It defaults to the `image_data_format` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"channels_last\".\n@@ -235,9 +235,9 @@ class AveragePooling2D(_Pooling2D):\n             one of `channels_last` (default) or `channels_first`.\n             The ordering of the dimensions in the inputs.\n             `channels_last` corresponds to inputs with shape\n-            `(batch, width, height, channels)` while `channels_first`\n+            `(batch, height, width, channels)` while `channels_first`\n             corresponds to inputs with shape\n-            `(batch, channels, width, height)`.\n+            `(batch, channels, height, width)`.\n             It defaults to the `image_data_format` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"channels_last\".\n@@ -511,9 +511,9 @@ class GlobalAveragePooling2D(_GlobalPooling2D):\n             one of `channels_last` (default) or `channels_first`.\n             The ordering of the dimensions in the inputs.\n             `channels_last` corresponds to inputs with shape\n-            `(batch, width, height, channels)` while `channels_first`\n+            `(batch, height, width, channels)` while `channels_first`\n             corresponds to inputs with shape\n-            `(batch, channels, width, height)`.\n+            `(batch, channels, height, width)`.\n             It defaults to the `image_data_format` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"channels_last\".\n@@ -546,9 +546,9 @@ class GlobalMaxPooling2D(_GlobalPooling2D):\n             one of `channels_last` (default) or `channels_first`.\n             The ordering of the dimensions in the inputs.\n             `channels_last` corresponds to inputs with shape\n-            `(batch, width, height, channels)` while `channels_first`\n+            `(batch, height, width, channels)` while `channels_first`\n             corresponds to inputs with shape\n-            `(batch, channels, width, height)`.\n+            `(batch, channels, height, width)`.\n             It defaults to the `image_data_format` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"channels_last\".\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
