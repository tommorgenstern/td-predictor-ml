{"custom_id": "keras#ea7847328f53516521d53ad68ccc7c613c90816d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 736 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -66,7 +66,7 @@ class Merge(Layer):\n         warnings.warn('The `Merge` layer is deprecated '\n                       'and will be removed after 08/2017. '\n                       'Use instead layers from `keras.layers.merge`, '\n-                      'e.g. `sum`, `concatenate`, etc.')\n+                      'e.g. `add`, `concatenate`, etc.')\n         self.layers = layers\n         self.mode = mode\n         self.concat_axis = concat_axis\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#152d896a776cbaea0c9402961fee6931679b2100", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 48/48 | Churn Δ: 16 | Churn Cumulative: 4736 | Contributors (this commit): 57 | Commits (past 90d): 54 | Contributors (cumulative): 57 | DMM Complexity: 0.0\n\nDIFF:\n@@ -307,11 +307,21 @@ Assumed overridden:\n \n \n def dot(x, y):\n-    # TODO: `keras_shape` inference.\n     if is_sparse(x):\n-        return th_sparse_module.basic.structured_dot(x, y)\n+        out = th_sparse_module.basic.structured_dot(x, y)\n     else:\n-        return T.dot(x, y)\n+        out = T.dot(x, y)\n+    if hasattr(x, '_keras_shape') and hasattr(y, '_keras_shape'):\n+        x_shape = list(x._keras_shape)\n+        y_shape = list(y._keras_shape)\n+        if len(x_shape) > 0:\n+            x_shape.pop()\n+        if len(y_shape) == 1:\n+            y_shape.pop()\n+        elif len(y_shape) > 1:\n+            y_shape.pop(-2)\n+        out._keras_shape = tuple(x_shape + y_shape)\n+    return out\n \n \n def batch_dot(x, y, axes=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7e6ccb8a488f949de2ed6a28c47e973279400634", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 144 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 10 | Methods Changed: 9 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 146 | Churn Cumulative: 7061 | Contributors (this commit): 38 | Commits (past 90d): 48 | Contributors (cumulative): 46 | DMM Complexity: 0.9716981132075472\n\nDIFF:\n@@ -9,6 +9,7 @@ from .. import regularizers\n from .. import constraints\n from ..engine import Layer\n from ..engine import InputSpec\n+from ..legacy import interfaces\n \n \n def _time_distributed_dense(x, w, b=None, dropout=None,\n@@ -317,6 +318,7 @@ class SimpleRNN(Recurrent):\n         - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n     \"\"\"\n \n+    @interfaces.legacy_recurrent_support\n     def __init__(self, units,\n                  activation='tanh',\n                  use_bias=True,\n@@ -552,6 +554,7 @@ class GRU(Recurrent):\n         - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n     \"\"\"\n \n+    @interfaces.legacy_recurrent_support\n     def __init__(self, units,\n                  activation='tanh',\n                  recurrent_activation='hard_sigmoid',\n@@ -840,7 +843,7 @@ class LSTM(Recurrent):\n         - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n         - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n     \"\"\"\n-\n+    @interfaces.legacy_recurrent_support\n     def __init__(self, units,\n                  activation='tanh',\n                  recurrent_activation='hard_sigmoid',\n\n@@ -5,7 +5,7 @@ import warnings\n \n \n def generate_legacy_interface(allowed_positional_args=None,\n-                              conversions=None):\n+                              conversions=None, preprocessor=None):\n     allowed_positional_args = allowed_positional_args or []\n     conversions = conversions or []\n \n@@ -13,6 +13,9 @@ def generate_legacy_interface(allowed_positional_args=None,\n         @six.wraps(func)\n         def wrapper(*args, **kwargs):\n             layer_name = args[0].__class__.__name__\n+            if preprocessor:\n+                args, kwargs, converted = preprocessor(args, kwargs)\n+            else:\n                 converted = []\n             if len(args) > len(allowed_positional_args) + 1:\n                 raise TypeError('Layer `' + layer_name +\n@@ -61,6 +64,7 @@ def raise_duplicate_arg_error(old_arg, new_arg):\n                     '`' + old_arg + '` and the Keras 2 keyword argument '\n                     '`' + new_arg + '`. Stick to the latter!')\n \n+\n legacy_dense_support = generate_legacy_interface(\n     allowed_positional_args=['units'],\n     conversions=[('output_dim', 'units'),\n@@ -89,6 +93,34 @@ legacy_gaussiannoise_support = generate_legacy_interface(\n     allowed_positional_args=['stddev'],\n     conversions=[('sigma', 'stddev')])\n \n+\n+def lstm_args_preprocessor(args, kwargs):\n+    converted = []\n+    if 'forget_bias_init' in kwargs:\n+        if kwargs['forget_bias_init'] == 'one':\n+            kwargs.pop('forget_bias_init')\n+            kwargs['unit_forget_bias'] = True\n+            converted.append(('forget_bias_init', 'unit_forget_bias'))\n+        else:\n+            kwargs.pop('forget_bias_init')\n+            warnings.warn('The `forget_bias_init` argument '\n+                          'has been ignored. Use `unit_forget_bias=True` '\n+                          'instead to intialize with ones')\n+    return args, kwargs, converted\n+\n+legacy_recurrent_support = generate_legacy_interface(\n+    allowed_positional_args=['units'],\n+    conversions=[('output_dim', 'units'),\n+                 ('init', 'kernel_initializer'),\n+                 ('inner_init', 'recurrent_initializer'),\n+                 ('inner_activation', 'recurrent_activation'),\n+                 ('W_regularizer', 'kernel_regularizer'),\n+                 ('b_regularizer', 'bias_regularizer'),\n+                 ('U_regularizer', 'recurrent_regularizer'),\n+                 ('dropout_W', 'dropout'),\n+                 ('dropout_U', 'recurrent_dropout')],\n+    preprocessor=lstm_args_preprocessor)\n+\n legacy_gaussiandropout_support = generate_legacy_interface(\n     allowed_positional_args=['rate'],\n     conversions=[('p', 'rate')])\n\n@@ -63,6 +63,7 @@ def test_avgpooling1d_legacy_interface():\n def test_prelu_legacy_interface():\n     old_layer = keras.layers.PReLU(init='zero', name='p')\n     new_layer = keras.layers.PReLU('zero', name='p')\n+\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n@@ -73,6 +74,112 @@ def test_gaussiannoise_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n+@keras_test\n+def test_lstm_legacy_interface():\n+    old_layer = keras.layers.LSTM(input_shape=[3, 5], output_dim=2, name='d')\n+    new_layer = keras.layers.LSTM(2, input_shape=[3, 5], name='d')\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n+\n+    old_layer = keras.layers.LSTM(2, init='normal',\n+                                  inner_init='glorot_uniform',\n+                                  forget_bias_init='one',\n+                                  inner_activation='hard_sigmoid',\n+                                  W_regularizer='l1',\n+                                  U_regularizer='l1',\n+                                  b_regularizer='l1',\n+                                  dropout_W=0.1,\n+                                  dropout_U=0.1,\n+                                  name='LSTM')\n+\n+    new_layer = keras.layers.LSTM(2, kernel_initializer='normal',\n+                                  recurrent_initializer='glorot_uniform',\n+                                  unit_forget_bias=True,\n+                                  recurrent_activation='hard_sigmoid',\n+                                  kernel_regularizer='l1',\n+                                  recurrent_regularizer='l1',\n+                                  bias_regularizer='l1',\n+                                  dropout=0.1,\n+                                  recurrent_dropout=0.1,\n+                                  name='LSTM')\n+\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n+\n+    old_layer = keras.layers.LSTM(2, init='normal',\n+                                  inner_init='glorot_uniform',\n+                                  forget_bias_init='zero',\n+                                  inner_activation='hard_sigmoid',\n+                                  W_regularizer='l1',\n+                                  U_regularizer='l1',\n+                                  b_regularizer='l1',\n+                                  dropout_W=0.1,\n+                                  dropout_U=0.1,\n+                                  name='LSTM')\n+\n+    new_layer = keras.layers.LSTM(2, kernel_initializer='normal',\n+                                  recurrent_initializer='glorot_uniform',\n+                                  unit_forget_bias=True,\n+                                  recurrent_activation='hard_sigmoid',\n+                                  kernel_regularizer='l1',\n+                                  recurrent_regularizer='l1',\n+                                  bias_regularizer='l1',\n+                                  dropout=0.1,\n+                                  recurrent_dropout=0.1,\n+                                  name='LSTM')\n+\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n+\n+\n+@keras_test\n+def test_simplernn_legacy_interface():\n+    old_layer = keras.layers.SimpleRNN(input_shape=[3, 5], output_dim=2, name='d')\n+    new_layer = keras.layers.SimpleRNN(2, input_shape=[3, 5], name='d')\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n+\n+    old_layer = keras.layers.SimpleRNN(2, init='normal',\n+                                       inner_init='glorot_uniform',\n+                                       W_regularizer='l1',\n+                                       U_regularizer='l1',\n+                                       b_regularizer='l1',\n+                                       dropout_W=0.1,\n+                                       dropout_U=0.1,\n+                                       name='SimpleRNN')\n+    new_layer = keras.layers.SimpleRNN(2, kernel_initializer='normal',\n+                                       recurrent_initializer='glorot_uniform',\n+                                       kernel_regularizer='l1',\n+                                       recurrent_regularizer='l1',\n+                                       bias_regularizer='l1',\n+                                       dropout=0.1,\n+                                       recurrent_dropout=0.1,\n+                                       name='SimpleRNN')\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n+\n+\n+@keras_test\n+def test_gru_legacy_interface():\n+    old_layer = keras.layers.GRU(input_shape=[3, 5], output_dim=2, name='d')\n+    new_layer = keras.layers.GRU(2, input_shape=[3, 5], name='d')\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n+\n+    old_layer = keras.layers.GRU(2, init='normal',\n+                                 inner_init='glorot_uniform',\n+                                 inner_activation='hard_sigmoid',\n+                                 W_regularizer='l1',\n+                                 U_regularizer='l1',\n+                                 b_regularizer='l1',\n+                                 dropout_W=0.1,\n+                                 dropout_U=0.1,\n+                                 name='GRU')\n+    new_layer = keras.layers.GRU(2, kernel_initializer='normal',\n+                                 recurrent_initializer='glorot_uniform',\n+                                 recurrent_activation='hard_sigmoid',\n+                                 kernel_regularizer='l1',\n+                                 recurrent_regularizer='l1',\n+                                 bias_regularizer='l1',\n+                                 dropout=0.1,\n+                                 recurrent_dropout=0.1,\n+                                 name='GRU')\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n+\n @keras_test\n def test_gaussiandropout_legacy_interface():\n     old_layer = keras.layers.GaussianDropout(p=0.6, name='drop')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6f4a86407db17654d669732d3100e2b4778e211d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 7387 | Contributors (this commit): 62 | Commits (past 90d): 63 | Contributors (cumulative): 62 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3076,8 +3076,7 @@ def bias_add(x, bias, data_format=None):\n             x += reshape(bias, (1, 1, 1, 1, int_shape(bias)[0]))\n     elif ndim(x) == 4:\n         if data_format == 'channels_first':\n-            x = tf.nn.bias_add(x, bias,\n-                               data_format='NCHW')\n+            x += reshape(bias, (1, int_shape(bias)[0], 1, 1))\n         elif data_format == 'channels_last':\n             x = tf.nn.bias_add(x, bias,\n                                data_format='NHWC')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#43c0c825016d2d1df6015e89ee8eaba8cb43c2ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 4 | Churn Cumulative: 543 | Contributors (this commit): 6 | Commits (past 90d): 18 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -136,8 +136,6 @@ legacy_gaussiandropout_support = generate_legacy_interface(\n \n legacy_pooling2d_support = generate_legacy_interface(\n     allowed_positional_args=['pool_size', 'strides', 'padding'],\n-    conversions=[('pool_length', 'pool_size'),\n-                 ('stride', 'strides'),\n-                 ('border_mode', 'padding'),\n+    conversions=[('border_mode', 'padding'),\n                  ('dim_ordering', 'data_format')],\n     value_conversions={'dim_ordering': {'tf': 'channels_last', 'th': 'channels_first', 'default': None}})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#195cbdfb3f24aa509d5df036ad13f6939053642f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 14 | Churn Cumulative: 293 | Contributors (this commit): 6 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -191,12 +191,12 @@ def test_gaussiandropout_legacy_interface():\n \n @keras_test\n def test_maxpooling2d_legacy_interface():\n-    old_layer = keras.layers.MaxPool2D(pool_length=2, border_mode='valid', name='maxpool2d')\n+    old_layer = keras.layers.MaxPool2D(pool_size=2, border_mode='valid', name='maxpool2d')\n     new_layer = keras.layers.MaxPool2D(pool_size=2, padding='valid', name='maxpool2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.MaxPool2D(2, padding='valid', name='maxpool2d')\n-    new_layer = keras.layers.MaxPool2D(pool_size=2, padding='valid', name='maxpool2d')\n+    old_layer = keras.layers.MaxPool2D(2, 2, 'valid', name='maxpool2d')\n+    new_layer = keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid', name='maxpool2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n     old_layer = keras.layers.MaxPool2D(2, padding='valid', dim_ordering='tf', name='maxpool2d')\n@@ -214,12 +214,12 @@ def test_maxpooling2d_legacy_interface():\n \n @keras_test\n def test_avgpooling2d_legacy_interface():\n-    old_layer = keras.layers.AvgPool2D(pool_length=2, border_mode='valid', name='avgpooling2d')\n-    new_layer = keras.layers.AvgPool2D(pool_size=2, padding='valid', name='avgpooling2d')\n+    old_layer = keras.layers.MaxPool2D(pool_size=2, border_mode='valid', name='avgpooling2d')\n+    new_layer = keras.layers.MaxPool2D(pool_size=2, padding='valid', name='avgpooling2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.AvgPool2D(2, padding='valid', name='avgpooling2d')\n-    new_layer = keras.layers.AvgPool2D(pool_size=2, padding='valid', name='avgpooling2d')\n+    old_layer = keras.layers.MaxPool2D(2, 2, 'valid', name='avgpooling2d')\n+    new_layer = keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid', name='avgpooling2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n     old_layer = keras.layers.AvgPool2D(2, padding='valid', dim_ordering='tf', name='avgpooling2d')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2fc0d063d13320b71a70cbc1a1667a6e6b33165e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 17 | Churn Cumulative: 310 | Contributors (this commit): 6 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -39,18 +39,26 @@ def test_dropout_legacy_interface():\n \n @keras_test\n def test_maxpooling1d_legacy_interface():\n-    old_layer = keras.layers.MaxPool1D(pool_length=2, border_mode='valid', name='maxpool1d')\n-    new_layer = keras.layers.MaxPool1D(pool_size=2, padding='valid', name='maxpool1d')\n+    old_layer = keras.layers.MaxPool1D(pool_length=2,\n+                                       border_mode='valid',\n+                                       name='maxpool1d')\n+    new_layer = keras.layers.MaxPool1D(pool_size=2,\n+                                       padding='valid',\n+                                       name='maxpool1d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n     old_layer = keras.layers.MaxPool1D(2, padding='valid', name='maxpool1d')\n-    new_layer = keras.layers.MaxPool1D(pool_size=2, padding='valid', name='maxpool1d')\n+    new_layer = keras.layers.MaxPool1D(pool_size=2,\n+                                       padding='valid',\n+                                       name='maxpool1d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n @keras_test\n def test_avgpooling1d_legacy_interface():\n-    old_layer = keras.layers.AvgPool1D(pool_length=2, border_mode='valid', name='d')\n+    old_layer = keras.layers.AvgPool1D(pool_length=2,\n+                                       border_mode='valid',\n+                                       name='d')\n     new_layer = keras.layers.AvgPool1D(pool_size=2, padding='valid', name='d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n@@ -180,6 +188,7 @@ def test_gru_legacy_interface():\n                                  name='GRU')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n+\n @keras_test\n def test_gaussiandropout_legacy_interface():\n     old_layer = keras.layers.GaussianDropout(p=0.6, name='drop')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#93cf06c4c617aecaa34aa8a133c9af7fbc3f80c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 30 | Churn Cumulative: 883 | Contributors (this commit): 6 | Commits (past 90d): 32 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -31,9 +31,9 @@ def generate_legacy_interface(allowed_positional_args=None,\n                                 str(args[1:]))\n             for key in value_conversions:\n                 if key in kwargs:\n-                    for old_value, new_value in value_conversions[key].items():\n-                        if kwargs[key] == old_value:\n-                            kwargs[key] = new_value\n+                    old_value = kwargs[key]\n+                    if old_value in value_conversions[key]:\n+                        kwargs[key] = value_conversions[key][old_value]\n             for old_name, new_name in conversions:\n                 if old_name in kwargs:\n                     value = kwargs.pop(old_name)\n@@ -138,4 +138,6 @@ legacy_pooling2d_support = generate_legacy_interface(\n     allowed_positional_args=['pool_size', 'strides', 'padding'],\n     conversions=[('border_mode', 'padding'),\n                  ('dim_ordering', 'data_format')],\n-    value_conversions={'dim_ordering': {'tf': 'channels_last', 'th': 'channels_first', 'default': None}})\n+    value_conversions={'dim_ordering': {'tf': 'channels_last',\n+                                        'th': 'channels_first',\n+                                        'default': None}})\n\n@@ -222,24 +222,24 @@ def test_maxpooling2d_legacy_interface():\n \n @keras_test\n def test_avgpooling2d_legacy_interface():\n-    old_layer = keras.layers.MaxPool2D(pool_size=2, border_mode='valid', name='avgpooling2d')\n-    new_layer = keras.layers.MaxPool2D(pool_size=2, padding='valid', name='avgpooling2d')\n+    old_layer = keras.layers.MaxPooling2D(pool_size=(2, 2), border_mode='valid', name='avgpooling2d')\n+    new_layer = keras.layers.MaxPooling2D(pool_size=(2, 2), padding='valid', name='avgpooling2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.MaxPool2D(2, 2, 'valid', name='avgpooling2d')\n-    new_layer = keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid', name='avgpooling2d')\n+    old_layer = keras.layers.MaxPooling2D((2, 2), (2, 2), 'valid', name='avgpooling2d')\n+    new_layer = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid', name='avgpooling2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.AvgPool2D(2, padding='valid', dim_ordering='tf', name='avgpooling2d')\n-    new_layer = keras.layers.AvgPool2D(pool_size=2, padding='valid', data_format='channels_last', name='avgpooling2d')\n+    old_layer = keras.layers.AveragePooling2D(2, padding='valid', dim_ordering='tf', name='avgpooling2d')\n+    new_layer = keras.layers.AveragePooling2D(pool_size=2, padding='valid', data_format='channels_last', name='avgpooling2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.AvgPool2D(2, padding='valid', dim_ordering='th', name='avgpooling2d')\n-    new_layer = keras.layers.AvgPool2D(pool_size=2, padding='valid', data_format='channels_first', name='avgpooling2d')\n+    old_layer = keras.layers.AveragePooling2D(2, padding='valid', dim_ordering='th', name='avgpooling2d')\n+    new_layer = keras.layers.AveragePooling2D(pool_size=2, padding='valid', data_format='channels_first', name='avgpooling2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.AvgPool2D(2, padding='valid', dim_ordering='default', name='avgpooling2d')\n-    new_layer = keras.layers.AvgPool2D(pool_size=2, padding='valid', name='avgpooling2d')\n+    old_layer = keras.layers.AveragePooling2D(2, padding='valid', dim_ordering='default', name='avgpooling2d')\n+    new_layer = keras.layers.AveragePooling2D(pool_size=2, padding='valid', name='avgpooling2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eaa827584fb07ee03f26248c8d4ad8af126b7ed2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 89 | Lines Deleted: 54 | Files Changed: 1 | Hunks: 34 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 143 | Churn Cumulative: 717 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 3 | DMM Complexity: 0.13333333333333333\n\nDIFF:\n@@ -1,14 +1,9 @@\n # -*- coding: utf-8 -*-\n \"\"\"Inception V3 model for Keras.\n \n-Note that the ImageNet weights provided are from a model that had not fully converged.\n-Inception v3 should be able to reach 6.9% top-5 error, but our model\n-only gets to 7.8% (same as a fully-converged ResNet 50).\n-For comparison, VGG16 only gets to 9.9%, quite a bit worse.\n-\n-Also, do note that the input image format for this model is different than for\n-the VGG16 and ResNet models (299x299 instead of 224x224), and that the input preprocessing function\n-is also different (same as Xception).\n+Note that the input image format for this model is different than for\n+the VGG16 and ResNet models (299x299 instead of 224x224),\n+and that the input preprocessing function is also different (same as Xception).\n \n # Reference\n \n@@ -22,6 +17,7 @@ import warnings\n \n from ..models import Model\n from .. import layers\n+from ..layers import Activation\n from ..layers import Dense\n from ..layers import Input\n from ..layers import BatchNormalization\n@@ -38,12 +34,16 @@ from .imagenet_utils import decode_predictions\n from .imagenet_utils import _obtain_input_shape\n \n \n-WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/inception_v3_weights_tf_dim_ordering_tf_kernels.h5'\n-WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n+WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5'\n+WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n \n \n-def conv2d_bn(x, filters, num_row, num_col,\n-              padding='same', strides=(1, 1),\n+def conv2d_bn(x,\n+              filters,\n+              num_row,\n+              num_col,\n+              padding='same',\n+              strides=(1, 1),\n               name=None):\n     \"\"\"Utility function to apply conv + BN.\n \n@@ -71,17 +71,21 @@ def conv2d_bn(x, filters, num_row, num_col,\n         bn_axis = 1\n     else:\n         bn_axis = 3\n-    x = Conv2D(filters, (num_row, num_col),\n+    x = Conv2D(\n+        filters, (num_row, num_col),\n         strides=strides,\n-               activation='relu',\n         padding=padding,\n+        use_bias=False,\n         name=conv_name)(x)\n-    x = BatchNormalization(axis=bn_axis, name=bn_name)(x)\n+    x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n+    x = Activation('relu', name=name)(x)\n     return x\n \n \n-def InceptionV3(include_top=True, weights='imagenet',\n-                input_tensor=None, input_shape=None,\n+def InceptionV3(include_top=True,\n+                weights='imagenet',\n+                input_tensor=None,\n+                input_shape=None,\n                 pooling=None,\n                 classes=1000):\n     \"\"\"Instantiates the Inception v3 architecture.\n@@ -91,12 +95,10 @@ def InceptionV3(include_top=True, weights='imagenet',\n     for best performance you should set\n     `image_data_format=\"channels_last\"` in your Keras config\n     at ~/.keras/keras.json.\n-\n     The model and the weights are compatible with both\n     TensorFlow and Theano. The data format\n     convention used by the model is the one\n     specified in your Keras config file.\n-\n     Note that the default input image size for this model is 299x299.\n \n     # Arguments\n@@ -145,7 +147,8 @@ def InceptionV3(include_top=True, weights='imagenet',\n                          ' as true, `classes` should be 1000')\n \n     # Determine proper input shape\n-    input_shape = _obtain_input_shape(input_shape,\n+    input_shape = _obtain_input_shape(\n+        input_shape,\n         default_size=299,\n         min_size=139,\n         data_format=K.image_data_format(),\n@@ -154,10 +157,7 @@ def InceptionV3(include_top=True, weights='imagenet',\n     if input_tensor is None:\n         img_input = Input(shape=input_shape)\n     else:\n-        if not K.is_keras_tensor(input_tensor):\n         img_input = Input(tensor=input_tensor, shape=input_shape)\n-        else:\n-            img_input = input_tensor\n \n     if K.image_data_format() == 'channels_first':\n         channel_axis = 1\n@@ -174,7 +174,6 @@ def InceptionV3(include_top=True, weights='imagenet',\n     x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n \n     # mixed 0, 1, 2: 35 x 35 x 256\n-    for i in range(3):\n     branch1x1 = conv2d_bn(x, 64, 1, 1)\n \n     branch5x5 = conv2d_bn(x, 48, 1, 1)\n@@ -184,25 +183,58 @@ def InceptionV3(include_top=True, weights='imagenet',\n     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n \n-        branch_pool = AveragePooling2D(\n-            (3, 3), strides=(1, 1), padding='same')(x)\n+    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n     branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n-        x = layers.concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool],\n+    x = layers.concatenate(\n+        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n         axis=channel_axis,\n-                               name='mixed' + str(i))\n+        name='mixed0')\n+\n+    # mixed 1: 35 x 35 x 256\n+    branch1x1 = conv2d_bn(x, 64, 1, 1)\n+\n+    branch5x5 = conv2d_bn(x, 48, 1, 1)\n+    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n+\n+    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n+    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n+    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n+\n+    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n+    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n+    x = layers.concatenate(\n+        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n+        axis=channel_axis,\n+        name='mixed1')\n+\n+    # mixed 2: 35 x 35 x 256\n+    branch1x1 = conv2d_bn(x, 64, 1, 1)\n+\n+    branch5x5 = conv2d_bn(x, 48, 1, 1)\n+    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n+\n+    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n+    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n+    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n+\n+    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n+    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n+    x = layers.concatenate(\n+        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n+        axis=channel_axis,\n+        name='mixed2')\n \n     # mixed 3: 17 x 17 x 768\n     branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding='valid')\n \n     branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n-    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3,\n-                             strides=(2, 2), padding='valid')\n+    branch3x3dbl = conv2d_bn(\n+        branch3x3dbl, 96, 3, 3, strides=(2, 2), padding='valid')\n \n     branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n-    x = layers.concatenate([branch3x3, branch3x3dbl, branch_pool],\n-                           axis=channel_axis,\n-                           name='mixed3')\n+    x = layers.concatenate(\n+        [branch3x3, branch3x3dbl, branch_pool], axis=channel_axis, name='mixed3')\n \n     # mixed 4: 17 x 17 x 768\n     branch1x1 = conv2d_bn(x, 192, 1, 1)\n@@ -219,7 +251,8 @@ def InceptionV3(include_top=True, weights='imagenet',\n \n     branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n     branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n-    x = layers.concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n+    x = layers.concatenate(\n+        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n         axis=channel_axis,\n         name='mixed4')\n \n@@ -240,7 +273,8 @@ def InceptionV3(include_top=True, weights='imagenet',\n         branch_pool = AveragePooling2D(\n             (3, 3), strides=(1, 1), padding='same')(x)\n         branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n-        x = layers.concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n+        x = layers.concatenate(\n+            [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n             axis=channel_axis,\n             name='mixed' + str(5 + i))\n \n@@ -251,7 +285,7 @@ def InceptionV3(include_top=True, weights='imagenet',\n     branch7x7 = conv2d_bn(branch7x7, 192, 1, 7)\n     branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n \n-    branch7x7dbl = conv2d_bn(x, 160, 1, 1)\n+    branch7x7dbl = conv2d_bn(x, 192, 1, 1)\n     branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n     branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n     branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n@@ -259,7 +293,8 @@ def InceptionV3(include_top=True, weights='imagenet',\n \n     branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n     branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n-    x = layers.concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n+    x = layers.concatenate(\n+        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n         axis=channel_axis,\n         name='mixed7')\n \n@@ -271,13 +306,12 @@ def InceptionV3(include_top=True, weights='imagenet',\n     branch7x7x3 = conv2d_bn(x, 192, 1, 1)\n     branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)\n     branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)\n-    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 3, 3,\n-                            strides=(2, 2), padding='valid')\n+    branch7x7x3 = conv2d_bn(\n+        branch7x7x3, 192, 3, 3, strides=(2, 2), padding='valid')\n \n-    branch_pool = AveragePooling2D((3, 3), strides=(2, 2))(x)\n-    x = layers.concatenate([branch3x3, branch7x7x3, branch_pool],\n-                           axis=channel_axis,\n-                           name='mixed8')\n+    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n+    x = layers.concatenate(\n+        [branch3x3, branch7x7x3, branch_pool], axis=channel_axis, name='mixed8')\n \n     # mixed 9: 8 x 8 x 2048\n     for i in range(2):\n@@ -286,24 +320,23 @@ def InceptionV3(include_top=True, weights='imagenet',\n         branch3x3 = conv2d_bn(x, 384, 1, 1)\n         branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)\n         branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)\n-        branch3x3 = layers.concatenate([branch3x3_1, branch3x3_2],\n-                                       axis=channel_axis,\n-                                       name='mixed9_' + str(i))\n+        branch3x3 = layers.concatenate(\n+            [branch3x3_1, branch3x3_2], axis=channel_axis, name='mixed9_' + str(i))\n \n         branch3x3dbl = conv2d_bn(x, 448, 1, 1)\n         branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)\n         branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)\n         branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)\n-        branch3x3dbl = layers.concatenate([branch3x3dbl_1, branch3x3dbl_2],\n-                                          axis=channel_axis)\n+        branch3x3dbl = layers.concatenate(\n+            [branch3x3dbl_1, branch3x3dbl_2], axis=channel_axis)\n \n         branch_pool = AveragePooling2D(\n             (3, 3), strides=(1, 1), padding='same')(x)\n         branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n-        x = layers.concatenate([branch1x1, branch3x3, branch3x3dbl, branch_pool],\n+        x = layers.concatenate(\n+            [branch1x1, branch3x3, branch3x3dbl, branch_pool],\n             axis=channel_axis,\n             name='mixed' + str(9 + i))\n-\n     if include_top:\n         # Classification block\n         x = GlobalAveragePooling2D(name='avg_pool')(x)\n@@ -336,15 +369,17 @@ def InceptionV3(include_top=True, weights='imagenet',\n                               'your Keras config '\n                               'at ~/.keras/keras.json.')\n         if include_top:\n-            weights_path = get_file('inception_v3_weights_tf_dim_ordering_tf_kernels.h5',\n+            weights_path = get_file(\n+                'inception_v3_weights_tf_dim_ordering_tf_kernels.h5',\n                 WEIGHTS_PATH,\n                 cache_subdir='models',\n-                                    md5_hash='fe114b3ff2ea4bf891e9353d1bbfb32f')\n+                md5_hash='9a0d58056eeedaa3f26cb7ebd46da564')\n         else:\n-            weights_path = get_file('inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n+            weights_path = get_file(\n+                'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                 WEIGHTS_PATH_NO_TOP,\n                 cache_subdir='models',\n-                                    md5_hash='2f3609166de1d967d1a481094754f691')\n+                md5_hash='bcbd6486424b2319ff4ef7d526e38f63')\n         model.load_weights(weights_path)\n         if K.backend() == 'theano':\n             convert_all_kernels_in_model(model)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#16343b3261da08a8aa52f209a8609ce4b51c7fdf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 15 | Methods Changed: 6 | Complexity Δ (Sum/Max): 14/10 | Churn Δ: 91 | Churn Cumulative: 6525 | Contributors (this commit): 34 | Commits (past 90d): 26 | Contributors (cumulative): 36 | DMM Complexity: 0.0\n\nDIFF:\n@@ -165,14 +165,10 @@ class Recurrent(Layer):\n         a specific layer, or on your entire model.\n \n     # Note on specifying initial states in RNNs\n-        Most RNN layers can be called with either a single tensor, or a list\n-        of tensors.\n-        If the an RNN layer is called with a single tensor, that tensor is\n-        treated as the input, and the initial states are assigned an\n-        appropriate default value.\n-        If an RNN layer is called with a list of tensors, the first element is\n-        treated as the input, and the remaining tensors are treated as the\n-        initial states.\n+        You can specify the initial state of RNN layers by calling theme with\n+        the keyword argument `initial_state`. The value of `initial_state`\n+        should be a tensor or list of tensors representing the initial state\n+        of the RNN layer.\n     \"\"\"\n \n     def __init__(self, return_sequences=False,\n@@ -188,7 +184,8 @@ class Recurrent(Layer):\n         self.unroll = unroll\n         self.implementation = 0\n         self.supports_masking = True\n-        self.input_spec = None\n+        self.input_spec = InputSpec(ndim=3)\n+        self.state_spec = None\n         self.dropout = 0\n         self.recurrent_dropout = 0\n \n@@ -224,10 +221,58 @@ class Recurrent(Layer):\n     def preprocess_input(self, inputs, training=None):\n         return inputs\n \n+    def __call__(self, inputs, **kwargs):\n+        with K.name_scope(self.name):\n+            # Handle laying building (weight creating, input spec locking,\n+            # state spec locking)\n+            if not self.built:\n+                # Raise exceptions in case the input is not compatible\n+                # with the input_spec specified in the layer constructor.\n+                self.assert_input_compatibility(inputs)\n+\n+                if hasattr(inputs, '_keras_shape'):\n+                    input_shape = inputs._keras_shape\n+                elif hasattr(K, 'int_shape'):\n+                    input_shape = K.int_shape(inputs)\n+                else:\n+                    raise ValueError('You tried to call layer \"' + self.name +\n+                                        '\". This layer has no information'\n+                                        ' about its expected input shape, '\n+                                        'and thus cannot be built. '\n+                                        'You can build it manually via: '\n+                                        '`layer.build(batch_input_shape)`')\n+                self.build(input_shape)\n+                self.built = True\n+\n+        # If initial_state is specified, add it to the inputs and temporarily\n+        # modify the input spec to include the state\n+        if 'initial_state' in kwargs:\n+            # Compute the full input spec, including state\n+            input_spec = self.input_spec\n+            state_spec = self.state_spec\n+            if not isinstance(state_spec, list):\n+                state_spec = [state_spec]\n+            self.input_spec = [input_spec] + state_spec\n+\n+            # Compute the full inputs, including state\n+            initial_state = kwargs.pop('initial_state')\n+            if not isinstance(initial_state, list):\n+                initial_state = [initial_state]\n+            inputs = [inputs] + initial_state\n+\n+            # Perform the call\n+            output = super(Recurrent, self).__call__(inputs, **kwargs)\n+\n+            # Restore original input spec\n+            self.input_spec = input_spec\n+            return output\n+\n+        return super(Recurrent, self).__call__(inputs, **kwargs)\n+\n     def call(self, inputs, mask=None, training=None):\n         # input shape: (nbias_samples, time (padded with zeros), input_dim)\n         # note that the .build() method of subclasses MUST define\n-        # self.input_spec with a complete input shape.\n+        # self.input_spec and self.state_spec with complete input shapes.\n         if isinstance(inputs, list):\n             initial_states = inputs[1:]\n             inputs = inputs[0]\n@@ -277,7 +322,8 @@ class Recurrent(Layer):\n     def reset_states(self):\n         if not self.stateful:\n             raise AttributeError('Layer must be stateful.')\n-        if not self.batch_size:\n+        batch_size = self.input_spec.shape[0]\n+        if not batch_size:\n             raise ValueError('If a RNN is stateful, it needs to know '\n                              'its batch size. Specify the batch size '\n                              'of your input tensors: \\n'\n@@ -290,9 +336,9 @@ class Recurrent(Layer):\n                              '`batch_shape` argument to your Input layer.')\n         if self.states[0] is not None:\n             for state in self.states:\n-                K.set_value(state, np.zeros((self.batch_size, self.units)))\n+                K.set_value(state, np.zeros((batch_size, self.units)))\n         else:\n-            self.states = [K.zeros((self.batch_size, self.units))\n+            self.states = [K.zeros((batch_size, self.units))\n                            for _ in self.states]\n \n     def get_config(self):\n@@ -394,8 +440,10 @@ class SimpleRNN(Recurrent):\n         if isinstance(input_shape, list):\n             input_shape = input_shape[0]\n \n-        self.batch_size = input_shape[0]\n+        batch_size = input_shape[0] if self.stateful else None\n         self.input_dim = input_shape[2]\n+        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n+        self.state_spec = InputSpec(shape=(batch_size, self.units))\n \n         self.states = [None]\n         if self.stateful:\n@@ -608,8 +656,10 @@ class GRU(Recurrent):\n         if isinstance(input_shape, list):\n             input_shape = input_shape[0]\n \n-        self.batch_size = input_shape[0]\n+        batch_size = input_shape[0] if self.stateful else None\n         self.input_dim = input_shape[2]\n+        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n+        self.state_spec = InputSpec(shape=(batch_size, self.units))\n \n         self.states = [None]\n         if self.stateful:\n@@ -883,8 +933,11 @@ class LSTM(Recurrent):\n         if isinstance(input_shape, list):\n             input_shape = input_shape[0]\n \n-        self.batch_size = input_shape[0]\n+        batch_size = input_shape[0] if self.stateful else None\n         self.input_dim = input_shape[2]\n+        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n+        self.state_spec = [InputSpec(shape=(batch_size, self.units)),\n+                           InputSpec(shape=(batch_size, self.units))]\n \n         self.states = [None, None]\n         if self.stateful:\n\n@@ -170,10 +170,10 @@ def test_from_config(layer_class):\n def test_specify_state(layer_class):\n     states = 2 if layer_class is recurrent.LSTM else 1\n     inputs = Input((timesteps, embedding_dim))\n-    initial_states = [Input((units,)) for _ in range(states)]\n+    initial_state = [Input((units,)) for _ in range(states)]\n     layer = layer_class(units)\n-    output = layer([inputs] + initial_states)\n-    model = Model([inputs] + initial_states, output)\n+    output = layer(inputs, initial_state=initial_state)\n+    model = Model([inputs] + initial_state, output)\n     model.compile(loss='categorical_crossentropy', optimizer='adam')\n \n     inputs = np.random.random((num_samples, timesteps, embedding_dim))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#118027fcaeeec2ab6cc88b032d490494a14f1a4d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 12/10 | Churn Δ: 31 | Churn Cumulative: 12154 | Contributors (this commit): 82 | Commits (past 90d): 119 | Contributors (cumulative): 119 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1465,7 +1465,13 @@ def normalize_batch_in_training(x, gamma, beta,\n \n         broadcast_mean = tf.reshape(mean, target_shape)\n         broadcast_var = tf.reshape(var, target_shape)\n+        if gamma is None:\n+            broadcast_gamma = None\n+        else:\n             broadcast_gamma = tf.reshape(gamma, target_shape)\n+        if beta is None:\n+            broadcast_beta = None\n+        else:\n             broadcast_beta = tf.reshape(beta, target_shape)\n         normed = tf.nn.batch_normalization(x, broadcast_mean, broadcast_var,\n                                            broadcast_beta, broadcast_gamma,\n\n@@ -556,6 +556,16 @@ def normalize_batch_in_training(x, gamma, beta,\n     if not hasattr(T.nnet.bn, 'batch_normalization_train'):\n         return _old_normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon)\n \n+    if gamma is None:\n+        if beta is None:\n+            gamma = ones_like(x)\n+        else:\n+            gamma = ones_like(beta)\n+    if beta is None:\n+        if gamma is None:\n+            beta = zeros_like(x)\n+        beta = zeros_like(gamma)\n+\n     normed, mean, stdinv = T.nnet.bn.batch_normalization_train(\n         x, gamma, beta, reduction_axes, epsilon)\n \n@@ -570,6 +580,11 @@ def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n     if not hasattr(T.nnet.bn, 'batch_normalization_test'):\n         return _old_batch_normalization(x, mean, var, beta, gamma, epsilon)\n \n+    if gamma is None:\n+        gamma = ones_like(var)\n+    if beta is None:\n+        beta = zeros_like(mean)\n+\n     if mean.ndim == 1:\n         # based on TensorFlow's default: normalize along rightmost dimension\n         reduction_axes = range(x.ndim - 1)\n@@ -586,6 +601,11 @@ def _old_normalize_batch_in_training(x, gamma, beta,\n                                      reduction_axes, epsilon=1e-3):\n     \"\"\"Computes mean and std for batch then apply batch_normalization on batch.\n     \"\"\"\n+    if gamma is None:\n+        gamma = ones_like(x)\n+    if beta is None:\n+        beta = zeros_like(x)\n+\n     dev = theano.config.device\n     use_cudnn = ndim(x) < 5 and reduction_axes == [0, 2, 3] and (dev.startswith('cuda') or dev.startswith('gpu'))\n     if use_cudnn:\n@@ -628,6 +648,11 @@ def _old_normalize_batch_in_training(x, gamma, beta,\n def _old_batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n     \"\"\"Apply batch normalization on x given mean, var, beta and gamma.\n     \"\"\"\n+    if gamma is None:\n+        gamma = ones_like(var)\n+    if beta is None:\n+        beta = zeros_like(mean)\n+\n     if mean.ndim == 1 and x.ndim > 1:\n         # in TensorFlow's batch_normalization, if the parameters are vectors\n         # the batch normalization should be applied along the rightmost axis.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
