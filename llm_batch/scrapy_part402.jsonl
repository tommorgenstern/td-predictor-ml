{"custom_id": "scrapy#5a605969bdc102e0193ad15ccc571dc6164e5d26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 761 | Lines Deleted: 830 | Files Changed: 21 | Hunks: 462 | Methods Changed: 263 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 1591 | Churn Cumulative: 15731 | Contributors (this commit): 73 | Commits (past 90d): 65 | Contributors (cumulative): 265 | DMM Complexity: 0.025974025974025976\n\nDIFF:\n@@ -39,7 +39,7 @@ class CreateInstanceAddon:\n         settings.update(self.config, \"addon\")\n \n \n-class AddonTest(unittest.TestCase):\n+class TestAddon:\n     def test_update_settings(self):\n         settings = BaseSettings()\n         settings.set(\"KEY1\", \"default\", priority=\"default\")\n@@ -47,19 +47,19 @@ class AddonTest(unittest.TestCase):\n         addon_config = {\"KEY1\": \"addon\", \"KEY2\": \"addon\", \"KEY3\": \"addon\"}\n         testaddon = get_addon_cls(addon_config)()\n         testaddon.update_settings(settings)\n-        self.assertEqual(settings[\"KEY1\"], \"addon\")\n-        self.assertEqual(settings[\"KEY2\"], \"project\")\n-        self.assertEqual(settings[\"KEY3\"], \"addon\")\n+        assert settings[\"KEY1\"] == \"addon\"\n+        assert settings[\"KEY2\"] == \"project\"\n+        assert settings[\"KEY3\"] == \"addon\"\n \n \n-class AddonManagerTest(unittest.TestCase):\n+class TestAddonManager(unittest.TestCase):\n     def test_load_settings(self):\n         settings_dict = {\n             \"ADDONS\": {\"tests.test_addons.SimpleAddon\": 0},\n         }\n         crawler = get_crawler(settings_dict=settings_dict)\n         manager = crawler.addons\n-        self.assertIsInstance(manager.addons[0], SimpleAddon)\n+        assert isinstance(manager.addons[0], SimpleAddon)\n \n     def test_notconfigured(self):\n         class NotConfiguredAddon:\n@@ -71,7 +71,7 @@ class AddonManagerTest(unittest.TestCase):\n         }\n         crawler = get_crawler(settings_dict=settings_dict)\n         manager = crawler.addons\n-        self.assertFalse(manager.addons)\n+        assert not manager.addons\n \n     def test_load_settings_order(self):\n         # Get three addons with different settings\n@@ -86,8 +86,8 @@ class AddonManagerTest(unittest.TestCase):\n             settings = {\"ADDONS\": {a: i for i, a in enumerate(ordered_addons)}}\n             crawler = get_crawler(settings_dict=settings)\n             manager = crawler.addons\n-            self.assertEqual([a.number for a in manager.addons], expected_order)\n-            self.assertEqual(crawler.settings.getint(\"KEY1\"), expected_order[-1])\n+            assert [a.number for a in manager.addons] == expected_order\n+            assert crawler.settings.getint(\"KEY1\") == expected_order[-1]\n \n     def test_build_from_crawler(self):\n         settings_dict = {\n@@ -96,8 +96,8 @@ class AddonManagerTest(unittest.TestCase):\n         }\n         crawler = get_crawler(settings_dict=settings_dict)\n         manager = crawler.addons\n-        self.assertIsInstance(manager.addons[0], CreateInstanceAddon)\n-        self.assertEqual(crawler.settings.get(\"MYADDON_KEY\"), \"val\")\n+        assert isinstance(manager.addons[0], CreateInstanceAddon)\n+        assert crawler.settings.get(\"MYADDON_KEY\") == \"val\"\n \n     def test_settings_priority(self):\n         config = {\n@@ -107,14 +107,14 @@ class AddonManagerTest(unittest.TestCase):\n             \"ADDONS\": {get_addon_cls(config): 1},\n         }\n         crawler = get_crawler(settings_dict=settings_dict)\n-        self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n+        assert crawler.settings.getint(\"KEY\") == 15\n \n         settings = Settings(settings_dict)\n         settings.set(\"KEY\", 0, priority=\"default\")\n         runner = CrawlerRunner(settings)\n         crawler = runner.create_crawler(Spider)\n         crawler._apply_settings()\n-        self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n+        assert crawler.settings.getint(\"KEY\") == 15\n \n         settings_dict = {\n             \"KEY\": 20,  # priority=project\n@@ -124,7 +124,7 @@ class AddonManagerTest(unittest.TestCase):\n         settings.set(\"KEY\", 0, priority=\"default\")\n         runner = CrawlerRunner(settings)\n         crawler = runner.create_crawler(Spider)\n-        self.assertEqual(crawler.settings.getint(\"KEY\"), 20)\n+        assert crawler.settings.getint(\"KEY\") == 20\n \n     def test_fallback_workflow(self):\n         FALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"\n@@ -143,12 +143,12 @@ class AddonManagerTest(unittest.TestCase):\n             \"ADDONS\": {AddonWithFallback: 1},\n         }\n         crawler = get_crawler(settings_dict=settings_dict)\n-        self.assertEqual(\n-            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n+        assert (\n+            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"] == \"AddonHandler\"\n         )\n-        self.assertEqual(\n-            crawler.settings.get(FALLBACK_SETTING),\n-            \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n+        assert (\n+            crawler.settings.get(FALLBACK_SETTING)\n+            == \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\"\n         )\n \n         settings_dict = {\n@@ -156,10 +156,10 @@ class AddonManagerTest(unittest.TestCase):\n             \"DOWNLOAD_HANDLERS\": {\"https\": \"UserHandler\"},\n         }\n         crawler = get_crawler(settings_dict=settings_dict)\n-        self.assertEqual(\n-            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n+        assert (\n+            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"] == \"AddonHandler\"\n         )\n-        self.assertEqual(crawler.settings.get(FALLBACK_SETTING), \"UserHandler\")\n+        assert crawler.settings.get(FALLBACK_SETTING) == \"UserHandler\"\n \n     def test_logging_message(self):\n         class LoggedAddon:\n@@ -199,6 +199,6 @@ class AddonManagerTest(unittest.TestCase):\n         settings.set(\"KEY\", \"default\", priority=\"default\")\n         runner = CrawlerRunner(settings)\n         crawler = runner.create_crawler(MySpider)\n-        self.assertEqual(crawler.settings.get(\"KEY\"), \"default\")\n+        assert crawler.settings.get(\"KEY\") == \"default\"\n         yield crawler.crawl()\n-        self.assertEqual(crawler.settings.get(\"KEY\"), \"addon\")\n+        assert crawler.settings.get(\"KEY\") == \"addon\"\n\n@@ -28,9 +28,9 @@ class TestCloseSpider(TestCase):\n         crawler = get_crawler(ItemSpider, {\"CLOSESPIDER_ITEMCOUNT\": close_on})\n         yield crawler.crawl(mockserver=self.mockserver)\n         reason = crawler.spider.meta[\"close_reason\"]\n-        self.assertEqual(reason, \"closespider_itemcount\")\n+        assert reason == \"closespider_itemcount\"\n         itemcount = crawler.stats.get_value(\"item_scraped_count\")\n-        self.assertTrue(itemcount >= close_on)\n+        assert itemcount >= close_on\n \n     @defer.inlineCallbacks\n     def test_closespider_pagecount(self):\n@@ -38,9 +38,9 @@ class TestCloseSpider(TestCase):\n         crawler = get_crawler(FollowAllSpider, {\"CLOSESPIDER_PAGECOUNT\": close_on})\n         yield crawler.crawl(mockserver=self.mockserver)\n         reason = crawler.spider.meta[\"close_reason\"]\n-        self.assertEqual(reason, \"closespider_pagecount\")\n+        assert reason == \"closespider_pagecount\"\n         pagecount = crawler.stats.get_value(\"response_received_count\")\n-        self.assertTrue(pagecount >= close_on)\n+        assert pagecount >= close_on\n \n     @defer.inlineCallbacks\n     def test_closespider_pagecount_no_item(self):\n@@ -57,10 +57,10 @@ class TestCloseSpider(TestCase):\n             max_items=max_items, max_requests=max_requests, mockserver=self.mockserver\n         )\n         reason = crawler.spider.meta[\"close_reason\"]\n-        self.assertEqual(reason, \"closespider_pagecount_no_item\")\n+        assert reason == \"closespider_pagecount_no_item\"\n         pagecount = crawler.stats.get_value(\"response_received_count\")\n         itemcount = crawler.stats.get_value(\"item_scraped_count\")\n-        self.assertLessEqual(pagecount, close_on + itemcount)\n+        assert pagecount <= close_on + itemcount\n \n     @defer.inlineCallbacks\n     def test_closespider_pagecount_no_item_with_pagecount(self):\n@@ -75,9 +75,9 @@ class TestCloseSpider(TestCase):\n         )\n         yield crawler.crawl(mockserver=self.mockserver)\n         reason = crawler.spider.meta[\"close_reason\"]\n-        self.assertEqual(reason, \"closespider_pagecount_no_item\")\n+        assert reason == \"closespider_pagecount_no_item\"\n         pagecount = crawler.stats.get_value(\"response_received_count\")\n-        self.assertLess(pagecount, close_on_pagecount)\n+        assert pagecount < close_on_pagecount\n \n     @defer.inlineCallbacks\n     def test_closespider_errorcount(self):\n@@ -85,10 +85,10 @@ class TestCloseSpider(TestCase):\n         crawler = get_crawler(ErrorSpider, {\"CLOSESPIDER_ERRORCOUNT\": close_on})\n         yield crawler.crawl(total=1000000, mockserver=self.mockserver)\n         reason = crawler.spider.meta[\"close_reason\"]\n-        self.assertEqual(reason, \"closespider_errorcount\")\n+        assert reason == \"closespider_errorcount\"\n         key = f\"spider_exceptions/{crawler.spider.exception_cls.__name__}\"\n         errorcount = crawler.stats.get_value(key)\n-        self.assertTrue(errorcount >= close_on)\n+        assert errorcount >= close_on\n \n     @defer.inlineCallbacks\n     def test_closespider_timeout(self):\n@@ -96,9 +96,9 @@ class TestCloseSpider(TestCase):\n         crawler = get_crawler(FollowAllSpider, {\"CLOSESPIDER_TIMEOUT\": close_on})\n         yield crawler.crawl(total=1000000, mockserver=self.mockserver)\n         reason = crawler.spider.meta[\"close_reason\"]\n-        self.assertEqual(reason, \"closespider_timeout\")\n+        assert reason == \"closespider_timeout\"\n         total_seconds = crawler.stats.get_value(\"elapsed_time_seconds\")\n-        self.assertTrue(total_seconds >= close_on)\n+        assert total_seconds >= close_on\n \n     @defer.inlineCallbacks\n     def test_closespider_timeout_no_item(self):\n@@ -106,6 +106,6 @@ class TestCloseSpider(TestCase):\n         crawler = get_crawler(SlowSpider, {\"CLOSESPIDER_TIMEOUT_NO_ITEM\": timeout})\n         yield crawler.crawl(n=3, mockserver=self.mockserver)\n         reason = crawler.spider.meta[\"close_reason\"]\n-        self.assertEqual(reason, \"closespider_timeout_no_item\")\n+        assert reason == \"closespider_timeout_no_item\"\n         total_seconds = crawler.stats.get_value(\"elapsed_time_seconds\")\n-        self.assertTrue(total_seconds >= timeout)\n+        assert total_seconds >= timeout\n\n@@ -4,7 +4,6 @@ import pstats\n import shutil\n import sys\n import tempfile\n-import unittest\n from io import StringIO\n from pathlib import Path\n from subprocess import PIPE, Popen\n@@ -12,8 +11,8 @@ from subprocess import PIPE, Popen\n from scrapy.utils.test import get_testenv\n \n \n-class CmdlineTest(unittest.TestCase):\n-    def setUp(self):\n+class TestCmdline:\n+    def setup_method(self):\n         self.env = get_testenv()\n         tests_path = Path(__file__).parent.parent\n         self.env[\"PYTHONPATH\"] += os.pathsep + str(tests_path.parent)\n@@ -27,12 +26,12 @@ class CmdlineTest(unittest.TestCase):\n         return comm.decode(encoding)\n \n     def test_default_settings(self):\n-        self.assertEqual(self._execute(\"settings\", \"--get\", \"TEST1\"), \"default\")\n+        assert self._execute(\"settings\", \"--get\", \"TEST1\") == \"default\"\n \n     def test_override_settings_using_set_arg(self):\n-        self.assertEqual(\n-            self._execute(\"settings\", \"--get\", \"TEST1\", \"-s\", \"TEST1=override\"),\n-            \"override\",\n+        assert (\n+            self._execute(\"settings\", \"--get\", \"TEST1\", \"-s\", \"TEST1=override\")\n+            == \"override\"\n         )\n \n     def test_profiling(self):\n@@ -40,14 +39,14 @@ class CmdlineTest(unittest.TestCase):\n         filename = path / \"res.prof\"\n         try:\n             self._execute(\"version\", \"--profile\", str(filename))\n-            self.assertTrue(filename.exists())\n+            assert filename.exists()\n             out = StringIO()\n             stats = pstats.Stats(str(filename), stream=out)\n             stats.print_stats()\n             out.seek(0)\n             stats = out.read()\n-            self.assertIn(str(Path(\"scrapy\", \"commands\", \"version.py\")), stats)\n-            self.assertIn(\"tottime\", stats)\n+            assert str(Path(\"scrapy\", \"commands\", \"version.py\")) in stats\n+            assert \"tottime\" in stats\n         finally:\n             shutil.rmtree(path)\n \n@@ -62,15 +61,14 @@ class CmdlineTest(unittest.TestCase):\n             \"EXTENSIONS=\" + json.dumps(EXTENSIONS),\n         )\n         # XXX: There's gotta be a smarter way to do this...\n-        self.assertNotIn(\"...\", settingsstr)\n+        assert \"...\" not in settingsstr\n         for char in (\"'\", \"<\", \">\"):\n             settingsstr = settingsstr.replace(char, '\"')\n         settingsdict = json.loads(settingsstr)\n-        self.assertCountEqual(settingsdict.keys(), EXTENSIONS.keys())\n-        self.assertEqual(200, settingsdict[EXT_PATH])\n+        assert set(settingsdict.keys()) == set(EXTENSIONS.keys())\n+        assert settingsdict[EXT_PATH] == 200\n \n     def test_pathlib_path_as_feeds_key(self):\n-        self.assertEqual(\n-            self._execute(\"settings\", \"--get\", \"FEEDS\"),\n-            json.dumps({\"items.csv\": {\"format\": \"csv\", \"fields\": [\"price\", \"name\"]}}),\n+        assert self._execute(\"settings\", \"--get\", \"FEEDS\") == json.dumps(\n+            {\"items.csv\": {\"format\": \"csv\", \"fields\": [\"price\", \"name\"]}}\n         )\n\n@@ -1,10 +1,9 @@\n import sys\n-import unittest\n from pathlib import Path\n from subprocess import PIPE, Popen\n \n \n-class CmdlineCrawlPipelineTest(unittest.TestCase):\n+class TestCmdlineCrawlPipeline:\n     def _execute(self, spname):\n         args = (sys.executable, \"-m\", \"scrapy.cmdline\", \"crawl\", spname)\n         cwd = Path(__file__).resolve().parent\n@@ -13,7 +12,7 @@ class CmdlineCrawlPipelineTest(unittest.TestCase):\n         return proc.returncode\n \n     def test_open_spider_normally_in_pipeline(self):\n-        self.assertEqual(self._execute(\"normal\"), 0)\n+        assert self._execute(\"normal\") == 0\n \n     def test_exception_at_open_spider_in_pipeline(self):\n-        self.assertEqual(self._execute(\"exception\"), 1)\n+        assert self._execute(\"exception\") == 1\n\n@@ -3,10 +3,10 @@ from io import StringIO\n from unittest.mock import Mock, PropertyMock, call, patch\n \n from scrapy.commands.check import Command, TextTestResult\n-from tests.test_commands import CommandTest\n+from tests.test_commands import TestCommandBase\n \n \n-class CheckCommandTest(CommandTest):\n+class TestCheckCommand(TestCommandBase):\n     command = \"check\"\n \n     def setUp(self):\n@@ -36,9 +36,9 @@ class CheckSpider(scrapy.Spider):\n     def _test_contract(self, contracts=\"\", parse_def=\"pass\"):\n         self._write_contract(contracts, parse_def)\n         p, out, err = self.proc(\"check\")\n-        self.assertNotIn(\"F\", out)\n-        self.assertIn(\"OK\", err)\n-        self.assertEqual(p.returncode, 0)\n+        assert \"F\" not in out\n+        assert \"OK\" in err\n+        assert p.returncode == 0\n \n     def test_check_returns_requests_contract(self):\n         contracts = \"\"\"\n@@ -171,9 +171,7 @@ class CheckSpider(scrapy.Spider):\n \n         cmd.run([spider_name], Mock(list=True))\n \n-        self.assertEqual(\n-            \"FakeSpider\\n  * fakeMethod1\\n  * fakeMethod2\\n\", output.getvalue()\n-        )\n+        assert output.getvalue() == \"FakeSpider\\n  * fakeMethod1\\n  * fakeMethod2\\n\"\n         sys.stdout = sys.__stdout__\n \n     @patch(\"scrapy.commands.check.ContractsManager\")\n\n@@ -5,18 +5,18 @@ from tests.utils.testproc import ProcessTest\n from tests.utils.testsite import SiteTest\n \n \n-class FetchTest(ProcessTest, SiteTest, unittest.TestCase):\n+class TestFetchCommand(ProcessTest, SiteTest, unittest.TestCase):\n     command = \"fetch\"\n \n     @defer.inlineCallbacks\n     def test_output(self):\n         _, out, _ = yield self.execute([self.url(\"/text\")])\n-        self.assertEqual(out.strip(), b\"Works\")\n+        assert out.strip() == b\"Works\"\n \n     @defer.inlineCallbacks\n     def test_redirect_default(self):\n         _, out, _ = yield self.execute([self.url(\"/redirect\")])\n-        self.assertEqual(out.strip(), b\"Redirected here\")\n+        assert out.strip() == b\"Redirected here\"\n \n     @defer.inlineCallbacks\n     def test_redirect_disabled(self):\n@@ -24,8 +24,8 @@ class FetchTest(ProcessTest, SiteTest, unittest.TestCase):\n             [\"--no-redirect\", self.url(\"/redirect-no-meta-refresh\")]\n         )\n         err = err.strip()\n-        self.assertIn(b\"downloader/response_status_count/302\", err, err)\n-        self.assertNotIn(b\"downloader/response_status_count/200\", err, err)\n+        assert b\"downloader/response_status_count/302\" in err, err\n+        assert b\"downloader/response_status_count/200\" not in err, err\n \n     @defer.inlineCallbacks\n     def test_headers(self):\n\n@@ -1,5 +1,6 @@\n import argparse\n import os\n+import re\n from pathlib import Path\n \n from twisted.internet import defer\n@@ -7,18 +8,18 @@ from twisted.internet import defer\n from scrapy.commands import parse\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n-from tests.test_commands import CommandTest\n+from tests.test_commands import TestCommandBase\n from tests.utils.testproc import ProcessTest\n from tests.utils.testsite import SiteTest\n \n \n-def _textmode(bstr):\n+def _textmode(bstr: bytes) -> str:\n     \"\"\"Normalize input the same as writing to a file\n     and reading from it in text mode\"\"\"\n     return to_unicode(bstr).replace(os.linesep, \"\\n\")\n \n \n-class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n+class TestParseCommand(ProcessTest, SiteTest, TestCommandBase):\n     command = \"parse\"\n \n     def setUp(self):\n@@ -184,7 +185,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n                 self.url(\"/html\"),\n             ]\n         )\n-        self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n+        assert \"DEBUG: It Works!\" in _textmode(stderr)\n \n     @defer.inlineCallbacks\n     def test_request_with_meta(self):\n@@ -201,7 +202,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n                 self.url(\"/html\"),\n             ]\n         )\n-        self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n+        assert \"DEBUG: It Works!\" in _textmode(stderr)\n \n         _, _, stderr = yield self.execute(\n             [\n@@ -215,7 +216,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n                 self.url(\"/html\"),\n             ]\n         )\n-        self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n+        assert \"DEBUG: It Works!\" in _textmode(stderr)\n \n     @defer.inlineCallbacks\n     def test_request_with_cb_kwargs(self):\n@@ -233,9 +234,9 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n             ]\n         )\n         log = _textmode(stderr)\n-        self.assertIn(\"DEBUG: It Works!\", log)\n-        self.assertIn(\n-            \"DEBUG: request.callback signature: (response, foo=None, key=None)\", log\n+        assert \"DEBUG: It Works!\" in log\n+        assert (\n+            \"DEBUG: request.callback signature: (response, foo=None, key=None)\" in log\n         )\n \n     @defer.inlineCallbacks\n@@ -250,7 +251,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n                 self.url(\"/html\"),\n             ]\n         )\n-        self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n+        assert \"DEBUG: It Works!\" in _textmode(stderr)\n \n     @defer.inlineCallbacks\n     def test_pipelines(self):\n@@ -265,7 +266,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n                 self.url(\"/html\"),\n             ]\n         )\n-        self.assertIn(\"INFO: It Works!\", _textmode(stderr))\n+        assert \"INFO: It Works!\" in _textmode(stderr)\n \n     @defer.inlineCallbacks\n     def test_async_def_asyncio_parse_items_list(self):\n@@ -278,9 +279,9 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n                 self.url(\"/html\"),\n             ]\n         )\n-        self.assertIn(\"INFO: Got response 200\", _textmode(stderr))\n-        self.assertIn(\"{'id': 1}\", _textmode(out))\n-        self.assertIn(\"{'id': 2}\", _textmode(out))\n+        assert \"INFO: Got response 200\" in _textmode(stderr)\n+        assert \"{'id': 1}\" in _textmode(out)\n+        assert \"{'id': 2}\" in _textmode(out)\n \n     @defer.inlineCallbacks\n     def test_async_def_asyncio_parse_items_single_element(self):\n@@ -293,8 +294,8 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n                 self.url(\"/html\"),\n             ]\n         )\n-        self.assertIn(\"INFO: Got response 200\", _textmode(stderr))\n-        self.assertIn(\"{'foo': 42}\", _textmode(out))\n+        assert \"INFO: Got response 200\" in _textmode(stderr)\n+        assert \"{'foo': 42}\" in _textmode(out)\n \n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse_loop(self):\n@@ -307,9 +308,9 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n                 self.url(\"/html\"),\n             ]\n         )\n-        self.assertIn(\"INFO: Got response 200\", _textmode(stderr))\n+        assert \"INFO: Got response 200\" in _textmode(stderr)\n         for i in range(10):\n-            self.assertIn(f\"{{'foo': {i}}}\", _textmode(out))\n+            assert f\"{{'foo': {i}}}\" in _textmode(out)\n \n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse_exc(self):\n@@ -322,9 +323,9 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n                 self.url(\"/html\"),\n             ]\n         )\n-        self.assertIn(\"ValueError\", _textmode(stderr))\n+        assert \"ValueError\" in _textmode(stderr)\n         for i in range(7):\n-            self.assertIn(f\"{{'foo': {i}}}\", _textmode(out))\n+            assert f\"{{'foo': {i}}}\" in _textmode(out)\n \n     @defer.inlineCallbacks\n     def test_async_def_asyncio_parse(self):\n@@ -337,29 +338,29 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n                 self.url(\"/html\"),\n             ]\n         )\n-        self.assertIn(\"DEBUG: Got response 200\", _textmode(stderr))\n+        assert \"DEBUG: Got response 200\" in _textmode(stderr)\n \n     @defer.inlineCallbacks\n     def test_parse_items(self):\n         status, out, stderr = yield self.execute(\n             [\"--spider\", self.spider_name, \"-c\", \"parse\", self.url(\"/html\")]\n         )\n-        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", _textmode(out))\n+        assert \"[{}, {'foo': 'bar'}]\" in _textmode(out)\n \n     @defer.inlineCallbacks\n     def test_parse_items_no_callback_passed(self):\n         status, out, stderr = yield self.execute(\n             [\"--spider\", self.spider_name, self.url(\"/html\")]\n         )\n-        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", _textmode(out))\n+        assert \"[{}, {'foo': 'bar'}]\" in _textmode(out)\n \n     @defer.inlineCallbacks\n     def test_wrong_callback_passed(self):\n         status, out, stderr = yield self.execute(\n             [\"--spider\", self.spider_name, \"-c\", \"dummy\", self.url(\"/html\")]\n         )\n-        self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n-        self.assertIn(\"\"\"Cannot find callback\"\"\", _textmode(stderr))\n+        assert re.search(r\"# Scraped Items  -+\\n\\[\\]\", _textmode(out))\n+        assert \"Cannot find callback\" in _textmode(stderr)\n \n     @defer.inlineCallbacks\n     def test_crawlspider_matching_rule_callback_set(self):\n@@ -367,7 +368,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         status, out, stderr = yield self.execute(\n             [\"--spider\", \"goodcrawl\" + self.spider_name, \"-r\", self.url(\"/html\")]\n         )\n-        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", _textmode(out))\n+        assert \"[{}, {'foo': 'bar'}]\" in _textmode(out)\n \n     @defer.inlineCallbacks\n     def test_crawlspider_matching_rule_default_callback(self):\n@@ -375,7 +376,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         status, out, stderr = yield self.execute(\n             [\"--spider\", \"goodcrawl\" + self.spider_name, \"-r\", self.url(\"/text\")]\n         )\n-        self.assertIn(\"\"\"[{}, {'nomatch': 'default'}]\"\"\", _textmode(out))\n+        assert \"[{}, {'nomatch': 'default'}]\" in _textmode(out)\n \n     @defer.inlineCallbacks\n     def test_spider_with_no_rules_attribute(self):\n@@ -383,15 +384,15 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         status, out, stderr = yield self.execute(\n             [\"--spider\", self.spider_name, \"-r\", self.url(\"/html\")]\n         )\n-        self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n-        self.assertIn(\"\"\"No CrawlSpider rules found\"\"\", _textmode(stderr))\n+        assert re.search(r\"# Scraped Items  -+\\n\\[\\]\", _textmode(out))\n+        assert \"No CrawlSpider rules found\" in _textmode(stderr)\n \n     @defer.inlineCallbacks\n     def test_crawlspider_missing_callback(self):\n         status, out, stderr = yield self.execute(\n             [\"--spider\", \"badcrawl\" + self.spider_name, \"-r\", self.url(\"/html\")]\n         )\n-        self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n+        assert re.search(r\"# Scraped Items  -+\\n\\[\\]\", _textmode(out))\n \n     @defer.inlineCallbacks\n     def test_crawlspider_no_matching_rule(self):\n@@ -399,13 +400,13 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         status, out, stderr = yield self.execute(\n             [\"--spider\", \"badcrawl\" + self.spider_name, \"-r\", self.url(\"/enc-gb18030\")]\n         )\n-        self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n-        self.assertIn(\"\"\"Cannot find a rule that matches\"\"\", _textmode(stderr))\n+        assert re.search(r\"# Scraped Items  -+\\n\\[\\]\", _textmode(out))\n+        assert \"Cannot find a rule that matches\" in _textmode(stderr)\n \n     @defer.inlineCallbacks\n     def test_crawlspider_not_exists_with_not_matched_url(self):\n         status, out, stderr = yield self.execute([self.url(\"/invalid_url\")])\n-        self.assertEqual(status, 0)\n+        assert status == 0\n \n     @defer.inlineCallbacks\n     def test_output_flag(self):\n@@ -426,11 +427,11 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n             ]\n         )\n \n-        self.assertTrue(file_path.exists())\n-        self.assertTrue(file_path.is_file())\n+        assert file_path.exists()\n+        assert file_path.is_file()\n \n         content = '[\\n{},\\n{\"foo\": \"bar\"}\\n]'\n-        self.assertEqual(file_path.read_text(encoding=\"utf-8\"), content)\n+        assert file_path.read_text(encoding=\"utf-8\") == content\n \n     def test_parse_add_options(self):\n         command = parse.Command()\n@@ -445,7 +446,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         namespace = parser.parse_args(\n             [\"--verbose\", \"--nolinks\", \"-d\", \"2\", \"--spider\", self.spider_name]\n         )\n-        self.assertTrue(namespace.nolinks)\n-        self.assertEqual(namespace.depth, 2)\n-        self.assertEqual(namespace.spider, self.spider_name)\n-        self.assertTrue(namespace.verbose)\n+        assert namespace.nolinks\n+        assert namespace.depth == 2\n+        assert namespace.spider == self.spider_name\n+        assert namespace.verbose\n\n@@ -13,7 +13,7 @@ from tests.utils.testproc import ProcessTest\n from tests.utils.testsite import SiteTest\n \n \n-class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n+class TestShellCommand(ProcessTest, SiteTest, unittest.TestCase):\n     command = \"shell\"\n \n     @defer.inlineCallbacks\n@@ -40,14 +40,14 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n     def test_response_selector_html(self):\n         xpath = \"response.xpath(\\\"//p[@class='one']/text()\\\").get()\"\n         _, out, _ = yield self.execute([self.url(\"/html\"), \"-c\", xpath])\n-        self.assertEqual(out.strip(), b\"Works\")\n+        assert out.strip() == b\"Works\"\n \n     @defer.inlineCallbacks\n     def test_response_encoding_gb18030(self):\n         _, out, _ = yield self.execute(\n             [self.url(\"/enc-gb18030\"), \"-c\", \"response.encoding\"]\n         )\n-        self.assertEqual(out.strip(), b\"gb18030\")\n+        assert out.strip() == b\"gb18030\"\n \n     @defer.inlineCallbacks\n     def test_redirect(self):\n@@ -79,7 +79,7 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n         url = self.url(\"/redirect-no-meta-refresh\")\n         code = f\"fetch('{url}')\"\n         errcode, out, errout = yield self.execute([\"-c\", code])\n-        self.assertEqual(errcode, 0, out)\n+        assert errcode == 0, out\n         assert b\"Redirecting (302)\" in errout\n         assert b\"Crawled (200)\" in errout\n \n@@ -89,7 +89,7 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n         url = self.url(\"/redirect-no-meta-refresh\")\n         code = f\"fetch('{url}', redirect=False)\"\n         errcode, out, errout = yield self.execute([\"-c\", code])\n-        self.assertEqual(errcode, 0, out)\n+        assert errcode == 0, out\n         assert b\"Crawled (302)\" in errout\n \n     @defer.inlineCallbacks\n@@ -97,14 +97,14 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n         url = self.url(\"/text\")\n         code = f\"fetch('{url}') or fetch(response.request.replace(method='POST'))\"\n         errcode, out, _ = yield self.execute([\"-c\", code])\n-        self.assertEqual(errcode, 0, out)\n+        assert errcode == 0, out\n \n     @defer.inlineCallbacks\n     def test_scrapy_import(self):\n         url = self.url(\"/text\")\n         code = f\"fetch(scrapy.Request('{url}'))\"\n         errcode, out, _ = yield self.execute([\"-c\", code])\n-        self.assertEqual(errcode, 0, out)\n+        assert errcode == 0, out\n \n     @defer.inlineCallbacks\n     def test_local_file(self):\n@@ -118,8 +118,8 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n         errcode, out, err = yield self.execute(\n             [filepath, \"-c\", \"item\"], check_code=False\n         )\n-        self.assertEqual(errcode, 1, out or err)\n-        self.assertIn(b\"No such file or directory\", err)\n+        assert errcode == 1, out or err\n+        assert b\"No such file or directory\" in err\n \n     @defer.inlineCallbacks\n     def test_dns_failures(self):\n@@ -127,8 +127,8 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n             raise unittest.SkipTest(\"Non-existing hosts are resolvable\")\n         url = \"www.somedomainthatdoesntexi.st\"\n         errcode, out, err = yield self.execute([url, \"-c\", \"item\"], check_code=False)\n-        self.assertEqual(errcode, 1, out or err)\n-        self.assertIn(b\"DNS lookup failed\", err)\n+        assert errcode == 1, out or err\n+        assert b\"DNS lookup failed\" in err\n \n     @defer.inlineCallbacks\n     def test_shell_fetch_async(self):\n@@ -137,10 +137,10 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n         code = f\"fetch('{url}')\"\n         args = [\"-c\", code, \"--set\", f\"TWISTED_REACTOR={reactor_path}\"]\n         _, _, err = yield self.execute(args, check_code=True)\n-        self.assertNotIn(b\"RuntimeError: There is no current event loop in thread\", err)\n+        assert b\"RuntimeError: There is no current event loop in thread\" not in err\n \n \n-class InteractiveShellTest(unittest.TestCase):\n+class TestInteractiveShell:\n     def test_fetch(self):\n         args = (\n             sys.executable,\n@@ -161,4 +161,4 @@ class InteractiveShellTest(unittest.TestCase):\n         p.sendeof()\n         p.wait()\n         logfile.seek(0)\n-        self.assertNotIn(\"Traceback\", logfile.read().decode())\n+        assert \"Traceback\" not in logfile.read().decode()\n\n@@ -7,17 +7,14 @@ import scrapy\n from tests.utils.testproc import ProcessTest\n \n \n-class VersionTest(ProcessTest, unittest.TestCase):\n+class TestVersionCommand(ProcessTest, unittest.TestCase):\n     command = \"version\"\n \n     @defer.inlineCallbacks\n     def test_output(self):\n         encoding = sys.stdout.encoding or \"utf-8\"\n         _, out, _ = yield self.execute([])\n-        self.assertEqual(\n-            out.strip().decode(encoding),\n-            f\"Scrapy {scrapy.__version__}\",\n-        )\n+        assert out.strip().decode(encoding) == f\"Scrapy {scrapy.__version__}\"\n \n     @defer.inlineCallbacks\n     def test_verbose_output(self):\n@@ -27,9 +24,7 @@ class VersionTest(ProcessTest, unittest.TestCase):\n             line.partition(\":\")[0].strip()\n             for line in out.strip().decode(encoding).splitlines()\n         ]\n-        self.assertEqual(\n-            headers,\n-            [\n+        assert headers == [\n             \"Scrapy\",\n             \"lxml\",\n             \"libxml2\",\n@@ -41,5 +36,4 @@ class VersionTest(ProcessTest, unittest.TestCase):\n             \"pyOpenSSL\",\n             \"cryptography\",\n             \"Platform\",\n-            ],\n-        )\n+        ]\n\n@@ -14,7 +14,7 @@ from itertools import chain\n from pathlib import Path\n from shutil import copytree, rmtree\n from stat import S_IWRITE as ANYONE_WRITE_PERMISSION\n-from tempfile import TemporaryFile, mkdtemp\n+from tempfile import TemporaryDirectory, TemporaryFile, mkdtemp\n from threading import Timer\n from typing import TYPE_CHECKING\n from unittest import mock, skipIf\n@@ -35,8 +35,8 @@ if TYPE_CHECKING:\n     from collections.abc import Iterator\n \n \n-class CommandSettings(unittest.TestCase):\n-    def setUp(self):\n+class TestCommandSettings:\n+    def setup_method(self):\n         self.command = ScrapyCommand()\n         self.command.settings = Settings()\n         self.parser = argparse.ArgumentParser(\n@@ -50,10 +50,8 @@ class CommandSettings(unittest.TestCase):\n             args=[\"-s\", f\"FEEDS={feeds_json}\", \"spider.py\"]\n         )\n         self.command.process_options(args, opts)\n-        self.assertIsInstance(\n-            self.command.settings[\"FEEDS\"], scrapy.settings.BaseSettings\n-        )\n-        self.assertEqual(dict(self.command.settings[\"FEEDS\"]), json.loads(feeds_json))\n+        assert isinstance(self.command.settings[\"FEEDS\"], scrapy.settings.BaseSettings)\n+        assert dict(self.command.settings[\"FEEDS\"]) == json.loads(feeds_json)\n \n     def test_help_formatter(self):\n         formatter = ScrapyHelpFormatter(prog=\"scrapy\")\n@@ -64,17 +62,14 @@ class CommandSettings(unittest.TestCase):\n             \"\\n\",\n             \"Global Options:\\n\",\n         ]\n-        self.assertEqual(\n-            formatter._join_parts(part_strings),\n-            (\n+        assert formatter._join_parts(part_strings) == (\n             \"Usage\\n=====\\n  scrapy genspider [options] <name> <domain>\\n\\n\\n\"\n             \"Optional Arguments\\n==================\\n\\n\"\n             \"Global Options\\n--------------\\n\"\n-            ),\n         )\n \n \n-class ProjectTest(unittest.TestCase):\n+class TestProjectBase(unittest.TestCase):\n     project_name = \"testproject\"\n \n     def setUp(self):\n@@ -130,12 +125,12 @@ class ProjectTest(unittest.TestCase):\n         return None\n \n \n-class StartprojectTest(ProjectTest):\n+class TestStartprojectCommand(TestProjectBase):\n     def test_startproject(self):\n         p, out, err = self.proc(\"startproject\", self.project_name)\n         print(out)\n         print(err, file=sys.stderr)\n-        self.assertEqual(p.returncode, 0)\n+        assert p.returncode == 0\n \n         assert Path(self.proj_path, \"scrapy.cfg\").exists()\n         assert Path(self.proj_path, \"testproject\").exists()\n@@ -145,13 +140,13 @@ class StartprojectTest(ProjectTest):\n         assert Path(self.proj_mod_path, \"settings.py\").exists()\n         assert Path(self.proj_mod_path, \"spiders\", \"__init__.py\").exists()\n \n-        self.assertEqual(1, self.call(\"startproject\", self.project_name))\n-        self.assertEqual(1, self.call(\"startproject\", \"wrong---project---name\"))\n-        self.assertEqual(1, self.call(\"startproject\", \"sys\"))\n+        assert self.call(\"startproject\", self.project_name) == 1\n+        assert self.call(\"startproject\", \"wrong---project---name\") == 1\n+        assert self.call(\"startproject\", \"sys\") == 1\n \n     def test_startproject_with_project_dir(self):\n         project_dir = mkdtemp()\n-        self.assertEqual(0, self.call(\"startproject\", self.project_name, project_dir))\n+        assert self.call(\"startproject\", self.project_name, project_dir) == 0\n \n         assert Path(project_dir, \"scrapy.cfg\").exists()\n         assert Path(project_dir, \"testproject\").exists()\n@@ -161,20 +156,16 @@ class StartprojectTest(ProjectTest):\n         assert Path(project_dir, self.project_name, \"settings.py\").exists()\n         assert Path(project_dir, self.project_name, \"spiders\", \"__init__.py\").exists()\n \n-        self.assertEqual(\n-            0, self.call(\"startproject\", self.project_name, project_dir + \"2\")\n-        )\n+        assert self.call(\"startproject\", self.project_name, project_dir + \"2\") == 0\n \n-        self.assertEqual(1, self.call(\"startproject\", self.project_name, project_dir))\n-        self.assertEqual(\n-            1, self.call(\"startproject\", self.project_name + \"2\", project_dir)\n-        )\n-        self.assertEqual(1, self.call(\"startproject\", \"wrong---project---name\"))\n-        self.assertEqual(1, self.call(\"startproject\", \"sys\"))\n-        self.assertEqual(2, self.call(\"startproject\"))\n-        self.assertEqual(\n-            2,\n-            self.call(\"startproject\", self.project_name, project_dir, \"another_params\"),\n+        assert self.call(\"startproject\", self.project_name, project_dir) == 1\n+        assert self.call(\"startproject\", self.project_name + \"2\", project_dir) == 1\n+        assert self.call(\"startproject\", \"wrong---project---name\") == 1\n+        assert self.call(\"startproject\", \"sys\") == 1\n+        assert self.call(\"startproject\") == 2\n+        assert (\n+            self.call(\"startproject\", self.project_name, project_dir, \"another_params\")\n+            == 2\n         )\n \n     def test_existing_project_dir(self):\n@@ -186,7 +177,7 @@ class StartprojectTest(ProjectTest):\n         p, out, err = self.proc(\"startproject\", project_name, cwd=project_dir)\n         print(out)\n         print(err, file=sys.stderr)\n-        self.assertEqual(p.returncode, 0)\n+        assert p.returncode == 0\n \n         assert Path(project_path, \"scrapy.cfg\").exists()\n         assert Path(project_path, project_name).exists()\n@@ -224,7 +215,7 @@ def get_permissions_dict(\n     return permissions_dict\n \n \n-class StartprojectTemplatesTest(ProjectTest):\n+class TestStartprojectTemplates(TestProjectBase):\n     maxDiff = None\n \n     def setUp(self):\n@@ -239,11 +230,10 @@ class StartprojectTemplatesTest(ProjectTest):\n \n         args = [\"--set\", f\"TEMPLATES_DIR={self.tmpl}\"]\n         p, out, err = self.proc(\"startproject\", self.project_name, *args)\n-        self.assertIn(\n-            f\"New Scrapy project '{self.project_name}', using template directory\",\n-            out,\n+        assert (\n+            f\"New Scrapy project '{self.project_name}', using template directory\" in out\n         )\n-        self.assertIn(self.tmpl_proj, out)\n+        assert self.tmpl_proj in out\n         assert Path(self.proj_path, \"root_template\").exists()\n \n     def test_startproject_permissions_from_writable(self):\n@@ -280,7 +270,7 @@ class StartprojectTemplatesTest(ProjectTest):\n         project_dir = Path(destination, project_name)\n         actual_permissions = get_permissions_dict(project_dir)\n \n-        self.assertEqual(actual_permissions, expected_permissions)\n+        assert actual_permissions == expected_permissions\n \n     def test_startproject_permissions_from_read_only(self):\n         \"\"\"Check that generated files have the right permissions when the\n@@ -333,7 +323,7 @@ class StartprojectTemplatesTest(ProjectTest):\n         project_dir = Path(destination, project_name)\n         actual_permissions = get_permissions_dict(project_dir)\n \n-        self.assertEqual(actual_permissions, expected_permissions)\n+        assert actual_permissions == expected_permissions\n \n     def test_startproject_permissions_unchanged_in_destination(self):\n         \"\"\"Check that preexisting folders and files in the destination folder\n@@ -391,7 +381,7 @@ class StartprojectTemplatesTest(ProjectTest):\n \n         actual_permissions = get_permissions_dict(project_dir)\n \n-        self.assertEqual(actual_permissions, expected_permissions)\n+        assert actual_permissions == expected_permissions\n \n     def test_startproject_permissions_umask_022(self):\n         \"\"\"Check that generated files have the right permissions when the\n@@ -435,10 +425,10 @@ class StartprojectTemplatesTest(ProjectTest):\n             project_dir = Path(destination, project_name)\n             actual_permissions = get_permissions_dict(project_dir)\n \n-            self.assertEqual(actual_permissions, expected_permissions)\n+            assert actual_permissions == expected_permissions\n \n \n-class CommandTest(ProjectTest):\n+class TestCommandBase(TestProjectBase):\n     def setUp(self):\n         super().setUp()\n         self.call(\"startproject\", self.project_name)\n@@ -446,13 +436,13 @@ class CommandTest(ProjectTest):\n         self.env[\"SCRAPY_SETTINGS_MODULE\"] = f\"{self.project_name}.settings\"\n \n \n-class GenspiderCommandTest(CommandTest):\n+class TestGenspiderCommand(TestCommandBase):\n     def test_arguments(self):\n         # only pass one argument. spider script shouldn't be created\n-        self.assertEqual(2, self.call(\"genspider\", \"test_name\"))\n+        assert self.call(\"genspider\", \"test_name\") == 2\n         assert not Path(self.proj_mod_path, \"spiders\", \"test_name.py\").exists()\n         # pass two arguments <name> <domain>. spider script should be created\n-        self.assertEqual(0, self.call(\"genspider\", \"test_name\", \"test.com\"))\n+        assert self.call(\"genspider\", \"test_name\", \"test.com\") == 0\n         assert Path(self.proj_mod_path, \"spiders\", \"test_name.py\").exists()\n \n     def test_template(self, tplname=\"crawl\"):\n@@ -460,20 +450,20 @@ class GenspiderCommandTest(CommandTest):\n         spname = \"test_spider\"\n         spmodule = f\"{self.project_name}.spiders.{spname}\"\n         p, out, err = self.proc(\"genspider\", spname, \"test.com\", *args)\n-        self.assertIn(\n-            f\"Created spider {spname!r} using template {tplname!r} in module:{os.linesep}  {spmodule}\",\n-            out,\n+        assert (\n+            f\"Created spider {spname!r} using template {tplname!r} in module:{os.linesep}  {spmodule}\"\n+            in out\n         )\n-        self.assertTrue(Path(self.proj_mod_path, \"spiders\", \"test_spider.py\").exists())\n+        assert Path(self.proj_mod_path, \"spiders\", \"test_spider.py\").exists()\n         modify_time_before = (\n             Path(self.proj_mod_path, \"spiders\", \"test_spider.py\").stat().st_mtime\n         )\n         p, out, err = self.proc(\"genspider\", spname, \"test.com\", *args)\n-        self.assertIn(f\"Spider {spname!r} already exists in module\", out)\n+        assert f\"Spider {spname!r} already exists in module\" in out\n         modify_time_after = (\n             Path(self.proj_mod_path, \"spiders\", \"test_spider.py\").stat().st_mtime\n         )\n-        self.assertEqual(modify_time_after, modify_time_before)\n+        assert modify_time_after == modify_time_before\n \n     def test_template_basic(self):\n         self.test_template(\"basic\")\n@@ -485,14 +475,14 @@ class GenspiderCommandTest(CommandTest):\n         self.test_template(\"xmlfeed\")\n \n     def test_list(self):\n-        self.assertEqual(0, self.call(\"genspider\", \"--list\"))\n+        assert self.call(\"genspider\", \"--list\") == 0\n \n     def test_dump(self):\n-        self.assertEqual(0, self.call(\"genspider\", \"--dump=basic\"))\n-        self.assertEqual(0, self.call(\"genspider\", \"-d\", \"basic\"))\n+        assert self.call(\"genspider\", \"--dump=basic\") == 0\n+        assert self.call(\"genspider\", \"-d\", \"basic\") == 0\n \n     def test_same_name_as_project(self):\n-        self.assertEqual(2, self.call(\"genspider\", self.project_name))\n+        assert self.call(\"genspider\", self.project_name) == 2\n         assert not Path(\n             self.proj_mod_path, \"spiders\", f\"{self.project_name}.py\"\n         ).exists()\n@@ -500,7 +490,7 @@ class GenspiderCommandTest(CommandTest):\n     def test_same_filename_as_existing_spider(self, force=False):\n         file_name = \"example\"\n         file_path = Path(self.proj_mod_path, \"spiders\", f\"{file_name}.py\")\n-        self.assertEqual(0, self.call(\"genspider\", file_name, \"example.com\"))\n+        assert self.call(\"genspider\", file_name, \"example.com\") == 0\n         assert file_path.exists()\n \n         # change name of spider but not its file name\n@@ -515,39 +505,39 @@ class GenspiderCommandTest(CommandTest):\n \n         if force:\n             p, out, err = self.proc(\"genspider\", \"--force\", file_name, \"example.com\")\n-            self.assertIn(\n-                f\"Created spider {file_name!r} using template 'basic' in module\", out\n+            assert (\n+                f\"Created spider {file_name!r} using template 'basic' in module\" in out\n             )\n             modify_time_after = file_path.stat().st_mtime\n-            self.assertNotEqual(modify_time_after, modify_time_before)\n+            assert modify_time_after != modify_time_before\n             file_contents_after = file_path.read_text(encoding=\"utf-8\")\n-            self.assertNotEqual(file_contents_after, file_contents_before)\n+            assert file_contents_after != file_contents_before\n         else:\n             p, out, err = self.proc(\"genspider\", file_name, \"example.com\")\n-            self.assertIn(f\"{file_path.resolve()} already exists\", out)\n+            assert f\"{file_path.resolve()} already exists\" in out\n             modify_time_after = file_path.stat().st_mtime\n-            self.assertEqual(modify_time_after, modify_time_before)\n+            assert modify_time_after == modify_time_before\n             file_contents_after = file_path.read_text(encoding=\"utf-8\")\n-            self.assertEqual(file_contents_after, file_contents_before)\n+            assert file_contents_after == file_contents_before\n \n     def test_same_filename_as_existing_spider_force(self):\n         self.test_same_filename_as_existing_spider(force=True)\n \n     def test_url(self, url=\"test.com\", domain=\"test.com\"):\n-        self.assertEqual(0, self.call(\"genspider\", \"--force\", \"test_name\", url))\n-        self.assertEqual(\n-            domain,\n+        assert self.call(\"genspider\", \"--force\", \"test_name\", url) == 0\n+        assert (\n             self.find_in_file(\n                 Path(self.proj_mod_path, \"spiders\", \"test_name.py\"),\n                 r\"allowed_domains\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n-            ).group(1),\n+            ).group(1)\n+            == domain\n         )\n-        self.assertEqual(\n-            f\"https://{domain}\",\n+        assert (\n             self.find_in_file(\n                 Path(self.proj_mod_path, \"spiders\", \"test_name.py\"),\n                 r\"start_urls\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n-            ).group(1),\n+            ).group(1)\n+            == f\"https://{domain}\"\n         )\n \n     def test_url_schema(self):\n@@ -556,15 +546,13 @@ class GenspiderCommandTest(CommandTest):\n     def test_template_start_urls(\n         self, url=\"test.com\", expected=\"https://test.com\", template=\"basic\"\n     ):\n-        self.assertEqual(\n-            0, self.call(\"genspider\", \"-t\", template, \"--force\", \"test_name\", url)\n-        )\n-        self.assertEqual(\n-            expected,\n+        assert self.call(\"genspider\", \"-t\", template, \"--force\", \"test_name\", url) == 0\n+        assert (\n             self.find_in_file(\n                 Path(self.proj_mod_path, \"spiders\", \"test_name.py\"),\n                 r\"start_urls\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n-            ).group(1),\n+            ).group(1)\n+            == expected\n         )\n \n     def test_genspider_basic_start_urls(self):\n@@ -611,7 +599,7 @@ class GenspiderCommandTest(CommandTest):\n         )\n \n \n-class GenspiderStandaloneCommandTest(ProjectTest):\n+class TestGenspiderStandaloneCommand(TestProjectBase):\n     def test_generate_standalone_spider(self):\n         self.call(\"genspider\", \"example\", \"example.com\")\n         assert Path(self.temp_path, \"example.py\").exists()\n@@ -620,7 +608,7 @@ class GenspiderStandaloneCommandTest(ProjectTest):\n         file_name = \"example\"\n         file_path = Path(self.temp_path, file_name + \".py\")\n         p, out, err = self.proc(\"genspider\", file_name, \"example.com\")\n-        self.assertIn(f\"Created spider {file_name!r} using template 'basic' \", out)\n+        assert f\"Created spider {file_name!r} using template 'basic' \" in out\n         assert file_path.exists()\n         modify_time_before = file_path.stat().st_mtime\n         file_contents_before = file_path.read_text(encoding=\"utf-8\")\n@@ -630,29 +618,29 @@ class GenspiderStandaloneCommandTest(ProjectTest):\n             p, out, err = self.proc(\n                 \"genspider\", \"--force\", \"-t\", \"crawl\", file_name, \"example.com\"\n             )\n-            self.assertIn(f\"Created spider {file_name!r} using template 'crawl' \", out)\n+            assert f\"Created spider {file_name!r} using template 'crawl' \" in out\n             modify_time_after = file_path.stat().st_mtime\n-            self.assertNotEqual(modify_time_after, modify_time_before)\n+            assert modify_time_after != modify_time_before\n             file_contents_after = file_path.read_text(encoding=\"utf-8\")\n-            self.assertNotEqual(file_contents_after, file_contents_before)\n+            assert file_contents_after != file_contents_before\n         else:\n             p, out, err = self.proc(\"genspider\", file_name, \"example.com\")\n-            self.assertIn(\n-                f\"{Path(self.temp_path, file_name + '.py').resolve()} already exists\",\n-                out,\n+            assert (\n+                f\"{Path(self.temp_path, file_name + '.py').resolve()} already exists\"\n+                in out\n             )\n             modify_time_after = file_path.stat().st_mtime\n-            self.assertEqual(modify_time_after, modify_time_before)\n+            assert modify_time_after == modify_time_before\n             file_contents_after = file_path.read_text(encoding=\"utf-8\")\n-            self.assertEqual(file_contents_after, file_contents_before)\n+            assert file_contents_after == file_contents_before\n \n     def test_same_name_as_existing_file_force(self):\n         self.test_same_name_as_existing_file(force=True)\n \n \n-class MiscCommandsTest(CommandTest):\n+class TestMiscCommands(TestCommandBase):\n     def test_list(self):\n-        self.assertEqual(0, self.call(\"list\"))\n+        assert self.call(\"list\") == 0\n \n     def test_command_not_found(self):\n         na_msg = \"\"\"\n@@ -670,10 +658,10 @@ Unknown command: abc\n         for cmdname, inproject, message in params:\n             with mock.patch(\"sys.stdout\", new=StringIO()) as out:\n                 _print_unknown_command_msg(Settings(), cmdname, inproject)\n-                self.assertEqual(out.getvalue().strip(), message.strip())\n+                assert out.getvalue().strip() == message.strip()\n \n \n-class RunSpiderCommandTest(CommandTest):\n+class TestRunSpiderCommand(TestCommandBase):\n     spider_filename = \"myspider.py\"\n \n     debug_log_spider = \"\"\"\n@@ -697,18 +685,14 @@ class BadSpider(scrapy.Spider):\n         \"\"\"\n \n     @contextmanager\n-    def _create_file(self, content, name=None) -> Iterator[str]:\n-        tmpdir = Path(self.mktemp())\n-        tmpdir.mkdir()\n+    def _create_file(self, content: str, name: str | None = None) -> Iterator[str]:\n+        with TemporaryDirectory() as tmpdir:\n             if name:\n-            fname = (tmpdir / name).resolve()\n+                fname = Path(tmpdir, name).resolve()\n             else:\n-            fname = (tmpdir / self.spider_filename).resolve()\n+                fname = Path(tmpdir, self.spider_filename).resolve()\n             fname.write_text(content, encoding=\"utf-8\")\n-        try:\n             yield str(fname)\n-        finally:\n-            rmtree(tmpdir)\n \n     def runspider(self, code, name=None, args=()):\n         with self._create_file(code, name) as fname:\n@@ -720,29 +704,29 @@ class BadSpider(scrapy.Spider):\n \n     def test_runspider(self):\n         log = self.get_log(self.debug_log_spider)\n-        self.assertIn(\"DEBUG: It Works!\", log)\n-        self.assertIn(\"INFO: Spider opened\", log)\n-        self.assertIn(\"INFO: Closing spider (finished)\", log)\n-        self.assertIn(\"INFO: Spider closed (finished)\", log)\n+        assert \"DEBUG: It Works!\" in log\n+        assert \"INFO: Spider opened\" in log\n+        assert \"INFO: Closing spider (finished)\" in log\n+        assert \"INFO: Spider closed (finished)\" in log\n \n     def test_run_fail_spider(self):\n         proc, _, _ = self.runspider(\n             \"import scrapy\\n\" + inspect.getsource(ExceptionSpider)\n         )\n         ret = proc.returncode\n-        self.assertNotEqual(ret, 0)\n+        assert ret != 0\n \n     def test_run_good_spider(self):\n         proc, _, _ = self.runspider(\n             \"import scrapy\\n\" + inspect.getsource(NoRequestsSpider)\n         )\n         ret = proc.returncode\n-        self.assertEqual(ret, 0)\n+        assert ret == 0\n \n     def test_runspider_log_level(self):\n         log = self.get_log(self.debug_log_spider, args=(\"-s\", \"LOG_LEVEL=INFO\"))\n-        self.assertNotIn(\"DEBUG: It Works!\", log)\n-        self.assertIn(\"INFO: Spider opened\", log)\n+        assert \"DEBUG: It Works!\" not in log\n+        assert \"INFO: Spider opened\" in log\n \n     def test_runspider_dnscache_disabled(self):\n         # see https://github.com/scrapy/scrapy/issues/2811\n@@ -761,36 +745,36 @@ class MySpider(scrapy.Spider):\n         return {'test': 'value'}\n \"\"\"\n         log = self.get_log(dnscache_spider, args=(\"-s\", \"DNSCACHE_ENABLED=False\"))\n-        self.assertNotIn(\"DNSLookupError\", log)\n-        self.assertIn(\"INFO: Spider opened\", log)\n+        assert \"DNSLookupError\" not in log\n+        assert \"INFO: Spider opened\" in log\n \n     def test_runspider_log_short_names(self):\n         log1 = self.get_log(self.debug_log_spider, args=(\"-s\", \"LOG_SHORT_NAMES=1\"))\n-        self.assertIn(\"[myspider] DEBUG: It Works!\", log1)\n-        self.assertIn(\"[scrapy]\", log1)\n-        self.assertNotIn(\"[scrapy.core.engine]\", log1)\n+        assert \"[myspider] DEBUG: It Works!\" in log1\n+        assert \"[scrapy]\" in log1\n+        assert \"[scrapy.core.engine]\" not in log1\n \n         log2 = self.get_log(self.debug_log_spider, args=(\"-s\", \"LOG_SHORT_NAMES=0\"))\n-        self.assertIn(\"[myspider] DEBUG: It Works!\", log2)\n-        self.assertNotIn(\"[scrapy]\", log2)\n-        self.assertIn(\"[scrapy.core.engine]\", log2)\n+        assert \"[myspider] DEBUG: It Works!\" in log2\n+        assert \"[scrapy]\" not in log2\n+        assert \"[scrapy.core.engine]\" in log2\n \n     def test_runspider_no_spider_found(self):\n         log = self.get_log(\"from scrapy.spiders import Spider\\n\")\n-        self.assertIn(\"No spider found in file\", log)\n+        assert \"No spider found in file\" in log\n \n     def test_runspider_file_not_found(self):\n         _, _, log = self.proc(\"runspider\", \"some_non_existent_file\")\n-        self.assertIn(\"File not found: some_non_existent_file\", log)\n+        assert \"File not found: some_non_existent_file\" in log\n \n     def test_runspider_unable_to_load(self):\n         log = self.get_log(\"\", name=\"myspider.txt\")\n-        self.assertIn(\"Unable to load\", log)\n+        assert \"Unable to load\" in log\n \n     def test_start_requests_errors(self):\n         log = self.get_log(self.badspider, name=\"badspider.py\")\n-        self.assertIn(\"start_requests\", log)\n-        self.assertIn(\"badspider.py\", log)\n+        assert \"start_requests\" in log\n+        assert \"badspider.py\" in log\n \n     def test_asyncio_enabled_true(self):\n         log = self.get_log(\n@@ -800,14 +784,16 @@ class MySpider(scrapy.Spider):\n                 \"TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n             ],\n         )\n-        self.assertIn(\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n         )\n \n     def test_asyncio_enabled_default(self):\n         log = self.get_log(self.debug_log_spider, args=[])\n-        self.assertIn(\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n         )\n \n     def test_asyncio_enabled_false(self):\n@@ -815,11 +801,10 @@ class MySpider(scrapy.Spider):\n             self.debug_log_spider,\n             args=[\"-s\", \"TWISTED_REACTOR=twisted.internet.selectreactor.SelectReactor\"],\n         )\n-        self.assertIn(\n-            \"Using reactor: twisted.internet.selectreactor.SelectReactor\", log\n-        )\n-        self.assertNotIn(\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        assert \"Using reactor: twisted.internet.selectreactor.SelectReactor\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            not in log\n         )\n \n     @pytest.mark.requires_uvloop\n@@ -833,7 +818,7 @@ class MySpider(scrapy.Spider):\n                 \"ASYNCIO_EVENT_LOOP=uvloop.Loop\",\n             ],\n         )\n-        self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n+        assert \"Using asyncio event loop: uvloop.Loop\" in log\n \n     def test_custom_asyncio_loop_enabled_false(self):\n         log = self.get_log(\n@@ -849,9 +834,9 @@ class MySpider(scrapy.Spider):\n             loop = asyncio.new_event_loop()\n         else:\n             loop = asyncio.SelectorEventLoop()\n-        self.assertIn(\n-            f\"Using asyncio event loop: {loop.__module__}.{loop.__class__.__name__}\",\n-            log,\n+        assert (\n+            f\"Using asyncio event loop: {loop.__module__}.{loop.__class__.__name__}\"\n+            in log\n         )\n \n     def test_output(self):\n@@ -867,9 +852,7 @@ class MySpider(scrapy.Spider):\n \"\"\"\n         args = [\"-o\", \"example.json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn(\n-            \"[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}\", log\n-        )\n+        assert \"[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}\" in log\n \n     def test_overwrite_output(self):\n         spider_code = \"\"\"\n@@ -890,13 +873,13 @@ class MySpider(scrapy.Spider):\n         Path(self.cwd, \"example.json\").write_text(\"not empty\", encoding=\"utf-8\")\n         args = [\"-O\", \"example.json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn(\n-            '[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}',\n-            log,\n+        assert (\n+            '[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}'\n+            in log\n         )\n         with Path(self.cwd, \"example.json\").open(encoding=\"utf-8\") as f2:\n             first_line = f2.readline()\n-        self.assertNotEqual(first_line, \"not empty\")\n+        assert first_line != \"not empty\"\n \n     def test_output_and_overwrite_output(self):\n         spider_code = \"\"\"\n@@ -910,8 +893,8 @@ class MySpider(scrapy.Spider):\n \"\"\"\n         args = [\"-o\", \"example1.json\", \"-O\", \"example2.json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn(\n-            \"error: Please use only one of -o/--output and -O/--overwrite-output\", log\n+        assert (\n+            \"error: Please use only one of -o/--output and -O/--overwrite-output\" in log\n         )\n \n     def test_output_stdout(self):\n@@ -927,7 +910,7 @@ class MySpider(scrapy.Spider):\n \"\"\"\n         args = [\"-o\", \"-:json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn(\"[myspider] DEBUG: FEEDS: {'stdout:': {'format': 'json'}}\", log)\n+        assert \"[myspider] DEBUG: FEEDS: {'stdout:': {'format': 'json'}}\" in log\n \n     @skipIf(platform.system() == \"Windows\", reason=\"Linux only\")\n     def test_absolute_path_linux(self):\n@@ -946,16 +929,16 @@ class MySpider(scrapy.Spider):\n \n         args = [\"-o\", f\"{temp_dir}/output1.json:json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn(\n-            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}/output1.json\",\n-            log,\n+        assert (\n+            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}/output1.json\"\n+            in log\n         )\n \n         args = [\"-o\", f\"{temp_dir}/output2.json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn(\n-            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}/output2.json\",\n-            log,\n+        assert (\n+            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}/output2.json\"\n+            in log\n         )\n \n     @skipIf(platform.system() != \"Windows\", reason=\"Windows only\")\n@@ -975,16 +958,16 @@ class MySpider(scrapy.Spider):\n \n         args = [\"-o\", f\"{temp_dir}\\\\output1.json:json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn(\n-            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}\\\\output1.json\",\n-            log,\n+        assert (\n+            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}\\\\output1.json\"\n+            in log\n         )\n \n         args = [\"-o\", f\"{temp_dir}\\\\output2.json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn(\n-            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}\\\\output2.json\",\n-            log,\n+        assert (\n+            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}\\\\output2.json\"\n+            in log\n         )\n \n     def test_args_change_settings(self):\n@@ -1006,11 +989,11 @@ class MySpider(scrapy.Spider):\n \"\"\"\n         args = [\"-a\", \"foo=42\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\"The value of FOO is 42\", log)\n+        assert \"Spider closed (finished)\" in log\n+        assert \"The value of FOO is 42\" in log\n \n \n-class WindowsRunSpiderCommandTest(RunSpiderCommandTest):\n+class TestWindowsRunSpiderCommand(TestRunSpiderCommand):\n     spider_filename = \"myspider.pyw\"\n \n     def setUp(self):\n@@ -1020,24 +1003,24 @@ class WindowsRunSpiderCommandTest(RunSpiderCommandTest):\n \n     def test_start_requests_errors(self):\n         log = self.get_log(self.badspider, name=\"badspider.pyw\")\n-        self.assertIn(\"start_requests\", log)\n-        self.assertIn(\"badspider.pyw\", log)\n+        assert \"start_requests\" in log\n+        assert \"badspider.pyw\" in log\n \n     def test_runspider_unable_to_load(self):\n         raise unittest.SkipTest(\"Already Tested in 'RunSpiderCommandTest' \")\n \n \n-class BenchCommandTest(CommandTest):\n+class TestBenchCommand(TestCommandBase):\n     def test_run(self):\n         _, _, log = self.proc(\n             \"bench\", \"-s\", \"LOGSTATS_INTERVAL=0.001\", \"-s\", \"CLOSESPIDER_TIMEOUT=0.01\"\n         )\n-        self.assertIn(\"INFO: Crawled\", log)\n-        self.assertNotIn(\"Unhandled Error\", log)\n-        self.assertNotIn(\"log_count/ERROR\", log)\n+        assert \"INFO: Crawled\" in log\n+        assert \"Unhandled Error\" not in log\n+        assert \"log_count/ERROR\" not in log\n \n \n-class ViewCommandTest(CommandTest):\n+class TestViewCommand(TestCommandBase):\n     def test_methods(self):\n         command = view.Command()\n         command.settings = Settings()\n@@ -1048,13 +1031,11 @@ class ViewCommandTest(CommandTest):\n             conflict_handler=\"resolve\",\n         )\n         command.add_options(parser)\n-        self.assertEqual(command.short_desc(), \"Open URL in browser, as seen by Scrapy\")\n-        self.assertIn(\n-            \"URL using the Scrapy downloader and show its\", command.long_desc()\n-        )\n+        assert command.short_desc() == \"Open URL in browser, as seen by Scrapy\"\n+        assert \"URL using the Scrapy downloader and show its\" in command.long_desc()\n \n \n-class CrawlCommandTest(CommandTest):\n+class TestCrawlCommand(TestCommandBase):\n     def crawl(self, code, args=()):\n         Path(self.proj_mod_path, \"spiders\", \"myspider.py\").write_text(\n             code, encoding=\"utf-8\"\n@@ -1077,7 +1058,7 @@ class MySpider(scrapy.Spider):\n         return []\n \"\"\"\n         log = self.get_log(spider_code)\n-        self.assertIn(\"[myspider] DEBUG: It works!\", log)\n+        assert \"[myspider] DEBUG: It works!\" in log\n \n     def test_output(self):\n         spider_code = \"\"\"\n@@ -1092,9 +1073,7 @@ class MySpider(scrapy.Spider):\n \"\"\"\n         args = [\"-o\", \"example.json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn(\n-            \"[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}\", log\n-        )\n+        assert \"[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}\" in log\n \n     def test_overwrite_output(self):\n         spider_code = \"\"\"\n@@ -1115,13 +1094,13 @@ class MySpider(scrapy.Spider):\n         Path(self.cwd, \"example.json\").write_text(\"not empty\", encoding=\"utf-8\")\n         args = [\"-O\", \"example.json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn(\n-            '[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}',\n-            log,\n+        assert (\n+            '[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}'\n+            in log\n         )\n         with Path(self.cwd, \"example.json\").open(encoding=\"utf-8\") as f2:\n             first_line = f2.readline()\n-        self.assertNotEqual(first_line, \"not empty\")\n+        assert first_line != \"not empty\"\n \n     def test_output_and_overwrite_output(self):\n         spider_code = \"\"\"\n@@ -1135,12 +1114,12 @@ class MySpider(scrapy.Spider):\n \"\"\"\n         args = [\"-o\", \"example1.json\", \"-O\", \"example2.json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn(\n-            \"error: Please use only one of -o/--output and -O/--overwrite-output\", log\n+        assert (\n+            \"error: Please use only one of -o/--output and -O/--overwrite-output\" in log\n         )\n \n \n-class HelpMessageTest(CommandTest):\n+class TestHelpMessage(TestCommandBase):\n     def setUp(self):\n         super().setUp()\n         self.commands = [\n@@ -1163,30 +1142,30 @@ class HelpMessageTest(CommandTest):\n     def test_help_messages(self):\n         for command in self.commands:\n             _, out, _ = self.proc(command, \"-h\")\n-            self.assertIn(\"Usage\", out)\n+            assert \"Usage\" in out\n \n \n-class PopCommandNameTest(unittest.TestCase):\n+class TestPopCommandName:\n     def test_valid_command(self):\n         argv = [\"scrapy\", \"crawl\", \"my_spider\"]\n         command = _pop_command_name(argv)\n-        self.assertEqual(command, \"crawl\")\n-        self.assertEqual(argv, [\"scrapy\", \"my_spider\"])\n+        assert command == \"crawl\"\n+        assert argv == [\"scrapy\", \"my_spider\"]\n \n     def test_no_command(self):\n         argv = [\"scrapy\"]\n         command = _pop_command_name(argv)\n-        self.assertIsNone(command)\n-        self.assertEqual(argv, [\"scrapy\"])\n+        assert command is None\n+        assert argv == [\"scrapy\"]\n \n     def test_option_before_command(self):\n         argv = [\"scrapy\", \"-h\", \"crawl\"]\n         command = _pop_command_name(argv)\n-        self.assertEqual(command, \"crawl\")\n-        self.assertEqual(argv, [\"scrapy\", \"-h\"])\n+        assert command == \"crawl\"\n+        assert argv == [\"scrapy\", \"-h\"]\n \n     def test_option_after_command(self):\n         argv = [\"scrapy\", \"crawl\", \"-h\"]\n         command = _pop_command_name(argv)\n-        self.assertEqual(command, \"crawl\")\n-        self.assertEqual(argv, [\"scrapy\", \"-h\"])\n+        assert command == \"crawl\"\n+        assert argv == [\"scrapy\", \"-h\"]\n\n@@ -1,5 +1,6 @@\n from unittest import TextTestResult\n \n+import pytest\n from twisted.internet import defer\n from twisted.python import failure\n from twisted.trial import unittest\n@@ -246,7 +247,7 @@ class InheritsDemoSpider(DemoSpider):\n     name = \"inherits_demo_spider\"\n \n \n-class ContractsManagerTest(unittest.TestCase):\n+class TestContractsManager(unittest.TestCase):\n     contracts = [\n         UrlContract,\n         CallbackKeywordArgumentsContract,\n@@ -263,34 +264,33 @@ class ContractsManagerTest(unittest.TestCase):\n         self.results = TextTestResult(stream=None, descriptions=False, verbosity=0)\n \n     def should_succeed(self):\n-        self.assertFalse(self.results.failures)\n-        self.assertFalse(self.results.errors)\n+        assert not self.results.failures\n+        assert not self.results.errors\n \n     def should_fail(self):\n-        self.assertTrue(self.results.failures)\n-        self.assertFalse(self.results.errors)\n+        assert self.results.failures\n+        assert not self.results.errors\n \n     def should_error(self):\n-        self.assertTrue(self.results.errors)\n+        assert self.results.errors\n \n     def test_contracts(self):\n         spider = DemoSpider()\n \n         # extract contracts correctly\n         contracts = self.conman.extract_contracts(spider.returns_request)\n-        self.assertEqual(len(contracts), 2)\n-        self.assertEqual(\n-            frozenset(type(x) for x in contracts),\n-            frozenset([UrlContract, ReturnsContract]),\n+        assert len(contracts) == 2\n+        assert frozenset(type(x) for x in contracts) == frozenset(\n+            [UrlContract, ReturnsContract]\n         )\n \n         # returns request for valid method\n         request = self.conman.from_method(spider.returns_request, self.results)\n-        self.assertNotEqual(request, None)\n+        assert request is not None\n \n         # no request for missing url\n         request = self.conman.from_method(spider.parse_no_url, self.results)\n-        self.assertEqual(request, None)\n+        assert request is None\n \n     def test_cb_kwargs(self):\n         spider = DemoSpider()\n@@ -298,35 +298,31 @@ class ContractsManagerTest(unittest.TestCase):\n \n         # extract contracts correctly\n         contracts = self.conman.extract_contracts(spider.returns_request_cb_kwargs)\n-        self.assertEqual(len(contracts), 3)\n-        self.assertEqual(\n-            frozenset(type(x) for x in contracts),\n-            frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]),\n+        assert len(contracts) == 3\n+        assert frozenset(type(x) for x in contracts) == frozenset(\n+            [UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]\n         )\n \n         contracts = self.conman.extract_contracts(spider.returns_item_cb_kwargs)\n-        self.assertEqual(len(contracts), 3)\n-        self.assertEqual(\n-            frozenset(type(x) for x in contracts),\n-            frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]),\n+        assert len(contracts) == 3\n+        assert frozenset(type(x) for x in contracts) == frozenset(\n+            [UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]\n         )\n \n         contracts = self.conman.extract_contracts(\n             spider.returns_item_cb_kwargs_error_unexpected_keyword\n         )\n-        self.assertEqual(len(contracts), 3)\n-        self.assertEqual(\n-            frozenset(type(x) for x in contracts),\n-            frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]),\n+        assert len(contracts) == 3\n+        assert frozenset(type(x) for x in contracts) == frozenset(\n+            [UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]\n         )\n \n         contracts = self.conman.extract_contracts(\n             spider.returns_item_cb_kwargs_error_missing_argument\n         )\n-        self.assertEqual(len(contracts), 2)\n-        self.assertEqual(\n-            frozenset(type(x) for x in contracts),\n-            frozenset([UrlContract, ReturnsContract]),\n+        assert len(contracts) == 2\n+        assert frozenset(type(x) for x in contracts) == frozenset(\n+            [UrlContract, ReturnsContract]\n         )\n \n         # returns_request\n@@ -360,17 +356,15 @@ class ContractsManagerTest(unittest.TestCase):\n \n         # extract contracts correctly\n         contracts = self.conman.extract_contracts(spider.returns_request_meta)\n-        self.assertEqual(len(contracts), 3)\n-        self.assertEqual(\n-            frozenset(type(x) for x in contracts),\n-            frozenset([UrlContract, MetadataContract, ReturnsContract]),\n+        assert len(contracts) == 3\n+        assert frozenset(type(x) for x in contracts) == frozenset(\n+            [UrlContract, MetadataContract, ReturnsContract]\n         )\n \n         contracts = self.conman.extract_contracts(spider.returns_item_meta)\n-        self.assertEqual(len(contracts), 3)\n-        self.assertEqual(\n-            frozenset(type(x) for x in contracts),\n-            frozenset([UrlContract, MetadataContract, ReturnsContract]),\n+        assert len(contracts) == 3\n+        assert frozenset(type(x) for x in contracts) == frozenset(\n+            [UrlContract, MetadataContract, ReturnsContract]\n         )\n \n         response = ResponseMetaMock()\n@@ -505,8 +499,8 @@ class ContractsManagerTest(unittest.TestCase):\n         request = self.conman.from_method(spider.returns_request, self.results)\n         request.errback(failure_mock)\n \n-        self.assertFalse(self.results.failures)\n-        self.assertTrue(self.results.errors)\n+        assert not self.results.failures\n+        assert self.results.errors\n \n     @defer.inlineCallbacks\n     def test_same_url(self):\n@@ -537,19 +531,19 @@ class ContractsManagerTest(unittest.TestCase):\n             crawler = get_crawler(TestSameUrlSpider)\n             yield crawler.crawl()\n \n-        self.assertEqual(crawler.spider.visited, 2)\n+        assert crawler.spider.visited == 2\n \n     def test_form_contract(self):\n         spider = DemoSpider()\n         request = self.conman.from_method(spider.custom_form, self.results)\n-        self.assertEqual(request.method, \"POST\")\n-        self.assertIsInstance(request, FormRequest)\n+        assert request.method == \"POST\"\n+        assert isinstance(request, FormRequest)\n \n     def test_inherited_contracts(self):\n         spider = InheritsDemoSpider()\n \n         requests = self.conman.from_spider(spider, self.results)\n-        self.assertTrue(requests)\n+        assert requests\n \n \n class CustomFailContractPreProcess(Contract):\n@@ -566,8 +560,8 @@ class CustomFailContractPostProcess(Contract):\n         raise KeyboardInterrupt(\"Post-process exception\")\n \n \n-class CustomContractPrePostProcess(unittest.TestCase):\n-    def setUp(self):\n+class TestCustomContractPrePostProcess:\n+    def setup_method(self):\n         self.results = TextTestResult(stream=None, descriptions=False, verbosity=0)\n \n     def test_pre_hook_keyboard_interrupt(self):\n@@ -576,18 +570,13 @@ class CustomContractPrePostProcess(unittest.TestCase):\n         contract = CustomFailContractPreProcess(spider.returns_request)\n         conman = ContractsManager([contract])\n \n-        try:\n         request = conman.from_method(spider.returns_request, self.results)\n         contract.add_pre_hook(request, self.results)\n-            # Expect this to raise a KeyboardInterrupt\n+        with pytest.raises(KeyboardInterrupt, match=\"Pre-process exception\"):\n             request.callback(response, **request.cb_kwargs)\n-        except KeyboardInterrupt as e:\n-            self.assertEqual(str(e), \"Pre-process exception\")\n-        else:\n-            self.fail(\"KeyboardInterrupt not raised\")\n \n-        self.assertFalse(self.results.failures)\n-        self.assertFalse(self.results.errors)\n+        assert not self.results.failures\n+        assert not self.results.errors\n \n     def test_post_hook_keyboard_interrupt(self):\n         spider = DemoSpider()\n@@ -595,15 +584,10 @@ class CustomContractPrePostProcess(unittest.TestCase):\n         contract = CustomFailContractPostProcess(spider.returns_request)\n         conman = ContractsManager([contract])\n \n-        try:\n         request = conman.from_method(spider.returns_request, self.results)\n         contract.add_post_hook(request, self.results)\n-            # Expect this to raise a KeyboardInterrupt\n+        with pytest.raises(KeyboardInterrupt, match=\"Post-process exception\"):\n             request.callback(response, **request.cb_kwargs)\n-        except KeyboardInterrupt as e:\n-            self.assertEqual(str(e), \"Post-process exception\")\n-        else:\n-            self.fail(\"KeyboardInterrupt not raised\")\n \n-        self.assertFalse(self.results.failures)\n-        self.assertFalse(self.results.errors)\n+        assert not self.results.failures\n+        assert not self.results.errors\n\n@@ -30,16 +30,13 @@ from scrapy.utils.test import get_crawler\n from tests.mockserver import PayloadResource, ssl_context_factory\n \n \n-class SlotTest(unittest.TestCase):\n+class TestSlot:\n     def test_repr(self):\n         slot = Slot(concurrency=8, delay=0.1, randomize_delay=True)\n-        self.assertEqual(\n-            repr(slot),\n-            \"Slot(concurrency=8, delay=0.10, randomize_delay=True)\",\n-        )\n+        assert repr(slot) == \"Slot(concurrency=8, delay=0.10, randomize_delay=True)\"\n \n \n-class ContextFactoryBaseTestCase(unittest.TestCase):\n+class TestContextFactoryBase(unittest.TestCase):\n     context_factory = None\n \n     def _listen(self, site):\n@@ -90,7 +87,7 @@ class ContextFactoryBaseTestCase(unittest.TestCase):\n         return await maybe_deferred_to_future(d)\n \n \n-class ContextFactoryTestCase(ContextFactoryBaseTestCase):\n+class TestContextFactory(TestContextFactoryBase):\n     @deferred_f_from_coro_f\n     async def testPayload(self):\n         s = \"0123456789\" * 10\n@@ -100,7 +97,7 @@ class ContextFactoryTestCase(ContextFactoryBaseTestCase):\n         body = await self.get_page(\n             self.getURL(\"payload\"), client_context_factory, body=s\n         )\n-        self.assertEqual(body, to_bytes(s))\n+        assert body == to_bytes(s)\n \n     def test_override_getContext(self):\n         class MyFactory(ScrapyClientContextFactory):\n@@ -112,14 +109,14 @@ class ContextFactoryTestCase(ContextFactoryBaseTestCase):\n \n         with warnings.catch_warnings(record=True) as w:\n             MyFactory()\n-            self.assertEqual(len(w), 1)\n-            self.assertIn(\n-                \"Overriding ScrapyClientContextFactory.getContext() is deprecated\",\n-                str(w[0].message),\n+            assert len(w) == 1\n+            assert (\n+                \"Overriding ScrapyClientContextFactory.getContext() is deprecated\"\n+                in str(w[0].message)\n             )\n \n \n-class ContextFactoryTLSMethodTestCase(ContextFactoryBaseTestCase):\n+class TestContextFactoryTLSMethod(TestContextFactoryBase):\n     async def _assert_factory_works(\n         self, client_context_factory: ScrapyClientContextFactory\n     ) -> None:\n@@ -127,7 +124,7 @@ class ContextFactoryTLSMethodTestCase(ContextFactoryBaseTestCase):\n         body = await self.get_page(\n             self.getURL(\"payload\"), client_context_factory, body=s\n         )\n-        self.assertEqual(body, to_bytes(s))\n+        assert body == to_bytes(s)\n \n     @deferred_f_from_coro_f\n     async def test_setting_default(self):\n\n@@ -55,7 +55,7 @@ from tests.spiders import (\n )\n \n \n-class CrawlTestCase(TestCase):\n+class TestCrawl(TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.mockserver = MockServer()\n@@ -69,7 +69,7 @@ class CrawlTestCase(TestCase):\n     def test_follow_all(self):\n         crawler = get_crawler(FollowAllSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertEqual(len(crawler.spider.urls_visited), 11)  # 10 + start_url\n+        assert len(crawler.spider.urls_visited) == 11  # 10 + start_url\n \n     @defer.inlineCallbacks\n     def test_fixed_delay(self):\n@@ -94,9 +94,7 @@ class CrawlTestCase(TestCase):\n         times = crawler.spider.times\n         total_time = times[-1] - times[0]\n         average = total_time / (len(times) - 1)\n-        self.assertTrue(\n-            average > delay * tolerance, f\"download delay too small: {average}\"\n-        )\n+        assert average > delay * tolerance, f\"download delay too small: {average}\"\n \n         # Ensure that the same test parameters would cause a failure if no\n         # download delay is set. Otherwise, it means we are using a combination\n@@ -108,34 +106,32 @@ class CrawlTestCase(TestCase):\n         times = crawler.spider.times\n         total_time = times[-1] - times[0]\n         average = total_time / (len(times) - 1)\n-        self.assertFalse(\n-            average > delay / tolerance, \"test total or delay values are too small\"\n-        )\n+        assert average <= delay / tolerance, \"test total or delay values are too small\"\n \n     @defer.inlineCallbacks\n     def test_timeout_success(self):\n         crawler = get_crawler(DelaySpider)\n         yield crawler.crawl(n=0.5, mockserver=self.mockserver)\n-        self.assertTrue(crawler.spider.t1 > 0)\n-        self.assertTrue(crawler.spider.t2 > 0)\n-        self.assertTrue(crawler.spider.t2 > crawler.spider.t1)\n+        assert crawler.spider.t1 > 0\n+        assert crawler.spider.t2 > 0\n+        assert crawler.spider.t2 > crawler.spider.t1\n \n     @defer.inlineCallbacks\n     def test_timeout_failure(self):\n         crawler = get_crawler(DelaySpider, {\"DOWNLOAD_TIMEOUT\": 0.35})\n         yield crawler.crawl(n=0.5, mockserver=self.mockserver)\n-        self.assertTrue(crawler.spider.t1 > 0)\n-        self.assertTrue(crawler.spider.t2 == 0)\n-        self.assertTrue(crawler.spider.t2_err > 0)\n-        self.assertTrue(crawler.spider.t2_err > crawler.spider.t1)\n+        assert crawler.spider.t1 > 0\n+        assert crawler.spider.t2 == 0\n+        assert crawler.spider.t2_err > 0\n+        assert crawler.spider.t2_err > crawler.spider.t1\n \n         # server hangs after receiving response headers\n         crawler = get_crawler(DelaySpider, {\"DOWNLOAD_TIMEOUT\": 0.35})\n         yield crawler.crawl(n=0.5, b=1, mockserver=self.mockserver)\n-        self.assertTrue(crawler.spider.t1 > 0)\n-        self.assertTrue(crawler.spider.t2 == 0)\n-        self.assertTrue(crawler.spider.t2_err > 0)\n-        self.assertTrue(crawler.spider.t2_err > crawler.spider.t1)\n+        assert crawler.spider.t1 > 0\n+        assert crawler.spider.t2 == 0\n+        assert crawler.spider.t2_err > 0\n+        assert crawler.spider.t2_err > crawler.spider.t1\n \n     @defer.inlineCallbacks\n     def test_retry_503(self):\n@@ -173,10 +169,10 @@ class CrawlTestCase(TestCase):\n             crawler = get_crawler(BrokenStartRequestsSpider)\n             yield crawler.crawl(fail_before_yield=1, mockserver=self.mockserver)\n \n-        self.assertEqual(len(log.records), 1)\n+        assert len(log.records) == 1\n         record = log.records[0]\n-        self.assertIsNotNone(record.exc_info)\n-        self.assertIs(record.exc_info[0], ZeroDivisionError)\n+        assert record.exc_info is not None\n+        assert record.exc_info[0] is ZeroDivisionError\n \n     @defer.inlineCallbacks\n     def test_start_requests_bug_yielding(self):\n@@ -184,10 +180,10 @@ class CrawlTestCase(TestCase):\n             crawler = get_crawler(BrokenStartRequestsSpider)\n             yield crawler.crawl(fail_yielding=1, mockserver=self.mockserver)\n \n-        self.assertEqual(len(log.records), 1)\n+        assert len(log.records) == 1\n         record = log.records[0]\n-        self.assertIsNotNone(record.exc_info)\n-        self.assertIs(record.exc_info[0], ZeroDivisionError)\n+        assert record.exc_info is not None\n+        assert record.exc_info[0] is ZeroDivisionError\n \n     @defer.inlineCallbacks\n     def test_start_requests_items(self):\n@@ -195,7 +191,7 @@ class CrawlTestCase(TestCase):\n             crawler = get_crawler(StartRequestsItemSpider)\n             yield crawler.crawl(mockserver=self.mockserver)\n \n-        self.assertEqual(len(log.records), 0)\n+        assert len(log.records) == 0\n \n     @defer.inlineCallbacks\n     def test_start_requests_unsupported_output(self):\n@@ -203,16 +199,12 @@ class CrawlTestCase(TestCase):\n             crawler = get_crawler(StartRequestsGoodAndBadOutput)\n             yield crawler.crawl(mockserver=self.mockserver)\n \n-        self.assertEqual(len(log.records), 2)\n-        self.assertEqual(\n-            log.records[0].msg,\n-            (\n+        assert len(log.records) == 2\n+        assert log.records[0].msg == (\n             \"Got 'data:,b' among start requests. Only requests and items \"\n             \"are supported. It will be ignored.\"\n-            ),\n         )\n-        self.assertTrue(\n-            re.match(\n+        assert re.match(\n             (\n                 r\"^Got <object object at 0x[0-9a-fA-F]+> among start \"\n                 r\"requests\\. Only requests and items are supported\\. It \"\n@@ -220,17 +212,15 @@ class CrawlTestCase(TestCase):\n             ),\n             log.records[1].msg,\n         )\n-        )\n \n     @defer.inlineCallbacks\n     def test_start_requests_laziness(self):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n         crawler = get_crawler(BrokenStartRequestsSpider, settings)\n         yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertTrue(\n-            crawler.spider.seedsseen.index(None) < crawler.spider.seedsseen.index(99),\n-            crawler.spider.seedsseen,\n-        )\n+        assert crawler.spider.seedsseen.index(None) < crawler.spider.seedsseen.index(\n+            99\n+        ), crawler.spider.seedsseen\n \n     @defer.inlineCallbacks\n     def test_start_requests_dupes(self):\n@@ -239,7 +229,7 @@ class CrawlTestCase(TestCase):\n         yield crawler.crawl(\n             dont_filter=True, distinct_urls=2, dupe_factor=3, mockserver=self.mockserver\n         )\n-        self.assertEqual(crawler.spider.visited, 6)\n+        assert crawler.spider.visited == 6\n \n         crawler = get_crawler(DuplicateStartRequestsSpider, settings)\n         yield crawler.crawl(\n@@ -248,7 +238,7 @@ class CrawlTestCase(TestCase):\n             dupe_factor=4,\n             mockserver=self.mockserver,\n         )\n-        self.assertEqual(crawler.spider.visited, 3)\n+        assert crawler.spider.visited == 3\n \n     @defer.inlineCallbacks\n     def test_unbounded_response(self):\n@@ -282,7 +272,7 @@ with multiples lines\n             yield crawler.crawl(\n                 self.mockserver.url(f\"/raw?{query}\"), mockserver=self.mockserver\n             )\n-        self.assertEqual(str(log).count(\"Got response 200\"), 1)\n+        assert str(log).count(\"Got response 200\") == 1\n \n     @defer.inlineCallbacks\n     def test_retry_conn_lost(self):\n@@ -305,8 +295,8 @@ with multiples lines\n         self._assert_retried(log)\n \n     def _assert_retried(self, log):\n-        self.assertEqual(str(log).count(\"Retrying\"), 2)\n-        self.assertEqual(str(log).count(\"Gave up retrying\"), 1)\n+        assert str(log).count(\"Retrying\") == 2\n+        assert str(log).count(\"Gave up retrying\") == 1\n \n     @defer.inlineCallbacks\n     def test_referer_header(self):\n@@ -321,20 +311,20 @@ with multiples lines\n         crawler = get_crawler(SingleRequestSpider)\n         yield crawler.crawl(seed=req0, mockserver=self.mockserver)\n         # basic asserts in case of weird communication errors\n-        self.assertIn(\"responses\", crawler.spider.meta)\n-        self.assertNotIn(\"failures\", crawler.spider.meta)\n+        assert \"responses\" in crawler.spider.meta\n+        assert \"failures\" not in crawler.spider.meta\n         # start requests doesn't set Referer header\n         echo0 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][2].body))\n-        self.assertNotIn(\"Referer\", echo0[\"headers\"])\n+        assert \"Referer\" not in echo0[\"headers\"]\n         # following request sets Referer to start request url\n         echo1 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][1].body))\n-        self.assertEqual(echo1[\"headers\"].get(\"Referer\"), [req0.url])\n+        assert echo1[\"headers\"].get(\"Referer\") == [req0.url]\n         # next request avoids Referer header\n         echo2 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][2].body))\n-        self.assertNotIn(\"Referer\", echo2[\"headers\"])\n+        assert \"Referer\" not in echo2[\"headers\"]\n         # last request explicitly sets a Referer header\n         echo3 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][3].body))\n-        self.assertEqual(echo3[\"headers\"].get(\"Referer\"), [\"http://example.com\"])\n+        assert echo3[\"headers\"].get(\"Referer\") == [\"http://example.com\"]\n \n     @defer.inlineCallbacks\n     def test_engine_status(self):\n@@ -349,10 +339,10 @@ with multiples lines\n         yield crawler.crawl(\n             seed=self.mockserver.url(\"/\"), callback_func=cb, mockserver=self.mockserver\n         )\n-        self.assertEqual(len(est), 1, est)\n+        assert len(est) == 1, est\n         s = dict(est[0])\n-        self.assertEqual(s[\"engine.spider.name\"], crawler.spider.name)\n-        self.assertEqual(s[\"len(engine.scraper.slot.active)\"], 1)\n+        assert s[\"engine.spider.name\"] == crawler.spider.name\n+        assert s[\"len(engine.scraper.slot.active)\"] == 1\n \n     @defer.inlineCallbacks\n     def test_format_engine_status(self):\n@@ -367,7 +357,7 @@ with multiples lines\n         yield crawler.crawl(\n             seed=self.mockserver.url(\"/\"), callback_func=cb, mockserver=self.mockserver\n         )\n-        self.assertEqual(len(est), 1, est)\n+        assert len(est) == 1, est\n         est = est[0].split(\"\\n\")[2:-2]  # remove header & footer\n         # convert to dict\n         est = [x.split(\":\") for x in est]\n@@ -376,8 +366,8 @@ with multiples lines\n         it = iter(est)\n         s = dict(zip(it, it))\n \n-        self.assertEqual(s[\"engine.spider.name\"], crawler.spider.name)\n-        self.assertEqual(s[\"len(engine.scraper.slot.active)\"], \"1\")\n+        assert s[\"engine.spider.name\"] == crawler.spider.name\n+        assert s[\"len(engine.scraper.slot.active)\"] == \"1\"\n \n     @defer.inlineCallbacks\n     def test_graceful_crawl_error_handling(self):\n@@ -398,7 +388,7 @@ with multiples lines\n \n         crawler = get_crawler(FaultySpider)\n         yield self.assertFailure(crawler.crawl(mockserver=self.mockserver), TestError)\n-        self.assertFalse(crawler.crawling)\n+        assert not crawler.crawling\n \n     @defer.inlineCallbacks\n     def test_open_spider_error_on_faulty_pipeline(self):\n@@ -414,7 +404,7 @@ with multiples lines\n             ),\n             ZeroDivisionError,\n         )\n-        self.assertFalse(crawler.crawling)\n+        assert not crawler.crawling\n \n     @defer.inlineCallbacks\n     def test_crawlerrunner_accepts_crawler(self):\n@@ -426,7 +416,7 @@ with multiples lines\n                 self.mockserver.url(\"/status?n=200\"),\n                 mockserver=self.mockserver,\n             )\n-        self.assertIn(\"Got response 200\", str(log))\n+        assert \"Got response 200\" in str(log)\n \n     @defer.inlineCallbacks\n     def test_crawl_multiple(self):\n@@ -446,10 +436,10 @@ with multiples lines\n             yield runner.join()\n \n         self._assert_retried(log)\n-        self.assertIn(\"Got response 200\", str(log))\n+        assert \"Got response 200\" in str(log)\n \n \n-class CrawlSpiderTestCase(TestCase):\n+class TestCrawlSpider(TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.mockserver = MockServer()\n@@ -480,9 +470,9 @@ class CrawlSpiderTestCase(TestCase):\n         with LogCapture() as log:\n             yield crawler.crawl(mockserver=self.mockserver)\n \n-        self.assertIn(\"[parse] status 200 (foo: None)\", str(log))\n-        self.assertIn(\"[parse] status 201 (foo: None)\", str(log))\n-        self.assertIn(\"[parse] status 202 (foo: bar)\", str(log))\n+        assert \"[parse] status 200 (foo: None)\" in str(log)\n+        assert \"[parse] status 201 (foo: None)\" in str(log)\n+        assert \"[parse] status 202 (foo: bar)\" in str(log)\n \n     @defer.inlineCallbacks\n     def test_crawlspider_with_async_callback(self):\n@@ -490,9 +480,9 @@ class CrawlSpiderTestCase(TestCase):\n         with LogCapture() as log:\n             yield crawler.crawl(mockserver=self.mockserver)\n \n-        self.assertIn(\"[parse_async] status 200 (foo: None)\", str(log))\n-        self.assertIn(\"[parse_async] status 201 (foo: None)\", str(log))\n-        self.assertIn(\"[parse_async] status 202 (foo: bar)\", str(log))\n+        assert \"[parse_async] status 200 (foo: None)\" in str(log)\n+        assert \"[parse_async] status 201 (foo: None)\" in str(log)\n+        assert \"[parse_async] status 202 (foo: bar)\" in str(log)\n \n     @defer.inlineCallbacks\n     def test_crawlspider_with_async_generator_callback(self):\n@@ -500,9 +490,9 @@ class CrawlSpiderTestCase(TestCase):\n         with LogCapture() as log:\n             yield crawler.crawl(mockserver=self.mockserver)\n \n-        self.assertIn(\"[parse_async_gen] status 200 (foo: None)\", str(log))\n-        self.assertIn(\"[parse_async_gen] status 201 (foo: None)\", str(log))\n-        self.assertIn(\"[parse_async_gen] status 202 (foo: bar)\", str(log))\n+        assert \"[parse_async_gen] status 200 (foo: None)\" in str(log)\n+        assert \"[parse_async_gen] status 201 (foo: None)\" in str(log)\n+        assert \"[parse_async_gen] status 202 (foo: bar)\" in str(log)\n \n     @defer.inlineCallbacks\n     def test_crawlspider_with_errback(self):\n@@ -510,12 +500,12 @@ class CrawlSpiderTestCase(TestCase):\n         with LogCapture() as log:\n             yield crawler.crawl(mockserver=self.mockserver)\n \n-        self.assertIn(\"[parse] status 200 (foo: None)\", str(log))\n-        self.assertIn(\"[parse] status 201 (foo: None)\", str(log))\n-        self.assertIn(\"[parse] status 202 (foo: bar)\", str(log))\n-        self.assertIn(\"[errback] status 404\", str(log))\n-        self.assertIn(\"[errback] status 500\", str(log))\n-        self.assertIn(\"[errback] status 501\", str(log))\n+        assert \"[parse] status 200 (foo: None)\" in str(log)\n+        assert \"[parse] status 201 (foo: None)\" in str(log)\n+        assert \"[parse] status 202 (foo: bar)\" in str(log)\n+        assert \"[errback] status 404\" in str(log)\n+        assert \"[errback] status 500\" in str(log)\n+        assert \"[errback] status 501\" in str(log)\n \n     @defer.inlineCallbacks\n     def test_crawlspider_process_request_cb_kwargs(self):\n@@ -523,9 +513,9 @@ class CrawlSpiderTestCase(TestCase):\n         with LogCapture() as log:\n             yield crawler.crawl(mockserver=self.mockserver)\n \n-        self.assertIn(\"[parse] status 200 (foo: process_request)\", str(log))\n-        self.assertIn(\"[parse] status 201 (foo: process_request)\", str(log))\n-        self.assertIn(\"[parse] status 202 (foo: bar)\", str(log))\n+        assert \"[parse] status 200 (foo: process_request)\" in str(log)\n+        assert \"[parse] status 201 (foo: process_request)\" in str(log)\n+        assert \"[parse] status 202 (foo: bar)\" in str(log)\n \n     @defer.inlineCallbacks\n     def test_async_def_parse(self):\n@@ -534,7 +524,7 @@ class CrawlSpiderTestCase(TestCase):\n             yield crawler.crawl(\n                 self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n             )\n-        self.assertIn(\"Got response 200\", str(log))\n+        assert \"Got response 200\" in str(log)\n \n     @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n@@ -549,15 +539,15 @@ class CrawlSpiderTestCase(TestCase):\n             yield crawler.crawl(\n                 self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n             )\n-        self.assertIn(\"Got response 200\", str(log))\n+        assert \"Got response 200\" in str(log)\n \n     @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_asyncio_parse_items_list(self):\n         log, items, _ = yield self._run_spider(AsyncDefAsyncioReturnSpider)\n-        self.assertIn(\"Got response 200\", str(log))\n-        self.assertIn({\"id\": 1}, items)\n-        self.assertIn({\"id\": 2}, items)\n+        assert \"Got response 200\" in str(log)\n+        assert {\"id\": 1} in items\n+        assert {\"id\": 2} in items\n \n     @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n@@ -573,81 +563,81 @@ class CrawlSpiderTestCase(TestCase):\n             yield crawler.crawl(\n                 self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n             )\n-        self.assertIn(\"Got response 200\", str(log))\n-        self.assertIn({\"foo\": 42}, items)\n+        assert \"Got response 200\" in str(log)\n+        assert {\"foo\": 42} in items\n \n     @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse(self):\n         log, _, stats = yield self._run_spider(AsyncDefAsyncioGenSpider)\n-        self.assertIn(\"Got response 200\", str(log))\n+        assert \"Got response 200\" in str(log)\n         itemcount = stats.get_value(\"item_scraped_count\")\n-        self.assertEqual(itemcount, 1)\n+        assert itemcount == 1\n \n     @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse_loop(self):\n         log, items, stats = yield self._run_spider(AsyncDefAsyncioGenLoopSpider)\n-        self.assertIn(\"Got response 200\", str(log))\n+        assert \"Got response 200\" in str(log)\n         itemcount = stats.get_value(\"item_scraped_count\")\n-        self.assertEqual(itemcount, 10)\n+        assert itemcount == 10\n         for i in range(10):\n-            self.assertIn({\"foo\": i}, items)\n+            assert {\"foo\": i} in items\n \n     @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse_exc(self):\n         log, items, stats = yield self._run_spider(AsyncDefAsyncioGenExcSpider)\n         log = str(log)\n-        self.assertIn(\"Spider error processing\", log)\n-        self.assertIn(\"ValueError\", log)\n+        assert \"Spider error processing\" in log\n+        assert \"ValueError\" in log\n         itemcount = stats.get_value(\"item_scraped_count\")\n-        self.assertEqual(itemcount, 7)\n+        assert itemcount == 7\n         for i in range(7):\n-            self.assertIn({\"foo\": i}, items)\n+            assert {\"foo\": i} in items\n \n     @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse_complex(self):\n         _, items, stats = yield self._run_spider(AsyncDefAsyncioGenComplexSpider)\n         itemcount = stats.get_value(\"item_scraped_count\")\n-        self.assertEqual(itemcount, 156)\n+        assert itemcount == 156\n         # some random items\n         for i in [1, 4, 21, 22, 207, 311]:\n-            self.assertIn({\"index\": i}, items)\n+            assert {\"index\": i} in items\n         for i in [10, 30, 122]:\n-            self.assertIn({\"index2\": i}, items)\n+            assert {\"index2\": i} in items\n \n     @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_asyncio_parse_reqs_list(self):\n         log, *_ = yield self._run_spider(AsyncDefAsyncioReqsReturnSpider)\n         for req_id in range(3):\n-            self.assertIn(f\"Got response 200, req_id {req_id}\", str(log))\n+            assert f\"Got response 200, req_id {req_id}\" in str(log)\n \n     @pytest.mark.only_not_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_deferred_direct(self):\n         _, items, _ = yield self._run_spider(AsyncDefDeferredDirectSpider)\n-        self.assertEqual(items, [{\"code\": 200}])\n+        assert items == [{\"code\": 200}]\n \n     @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_deferred_wrapped(self):\n         log, items, _ = yield self._run_spider(AsyncDefDeferredWrappedSpider)\n-        self.assertEqual(items, [{\"code\": 200}])\n+        assert items == [{\"code\": 200}]\n \n     @defer.inlineCallbacks\n     def test_async_def_deferred_maybe_wrapped(self):\n         _, items, _ = yield self._run_spider(AsyncDefDeferredMaybeWrappedSpider)\n-        self.assertEqual(items, [{\"code\": 200}])\n+        assert items == [{\"code\": 200}]\n \n     @defer.inlineCallbacks\n     def test_response_ssl_certificate_none(self):\n         crawler = get_crawler(SingleRequestSpider)\n         url = self.mockserver.url(\"/echo?body=test\", is_secure=False)\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n-        self.assertIsNone(crawler.spider.meta[\"responses\"][0].certificate)\n+        assert crawler.spider.meta[\"responses\"][0].certificate is None\n \n     @defer.inlineCallbacks\n     def test_response_ssl_certificate(self):\n@@ -655,9 +645,9 @@ class CrawlSpiderTestCase(TestCase):\n         url = self.mockserver.url(\"/echo?body=test\", is_secure=True)\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         cert = crawler.spider.meta[\"responses\"][0].certificate\n-        self.assertIsInstance(cert, Certificate)\n-        self.assertEqual(cert.getSubject().commonName, b\"localhost\")\n-        self.assertEqual(cert.getIssuer().commonName, b\"localhost\")\n+        assert isinstance(cert, Certificate)\n+        assert cert.getSubject().commonName == b\"localhost\"\n+        assert cert.getIssuer().commonName == b\"localhost\"\n \n     @pytest.mark.xfail(\n         reason=\"Responses with no body return early and contain no certificate\"\n@@ -668,9 +658,9 @@ class CrawlSpiderTestCase(TestCase):\n         url = self.mockserver.url(\"/status?n=200\", is_secure=True)\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         cert = crawler.spider.meta[\"responses\"][0].certificate\n-        self.assertIsInstance(cert, Certificate)\n-        self.assertEqual(cert.getSubject().commonName, b\"localhost\")\n-        self.assertEqual(cert.getIssuer().commonName, b\"localhost\")\n+        assert isinstance(cert, Certificate)\n+        assert cert.getSubject().commonName == b\"localhost\"\n+        assert cert.getIssuer().commonName == b\"localhost\"\n \n     @defer.inlineCallbacks\n     def test_dns_server_ip_address_none(self):\n@@ -678,7 +668,7 @@ class CrawlSpiderTestCase(TestCase):\n         url = self.mockserver.url(\"/status?n=200\")\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         ip_address = crawler.spider.meta[\"responses\"][0].ip_address\n-        self.assertIsNone(ip_address)\n+        assert ip_address is None\n \n     @defer.inlineCallbacks\n     def test_dns_server_ip_address(self):\n@@ -687,61 +677,57 @@ class CrawlSpiderTestCase(TestCase):\n         expected_netloc, _ = urlparse(url).netloc.split(\":\")\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         ip_address = crawler.spider.meta[\"responses\"][0].ip_address\n-        self.assertIsInstance(ip_address, IPv4Address)\n-        self.assertEqual(str(ip_address), gethostbyname(expected_netloc))\n+        assert isinstance(ip_address, IPv4Address)\n+        assert str(ip_address) == gethostbyname(expected_netloc)\n \n     @defer.inlineCallbacks\n     def test_bytes_received_stop_download_callback(self):\n         crawler = get_crawler(BytesReceivedCallbackSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertIsNone(crawler.spider.meta.get(\"failure\"))\n-        self.assertIsInstance(crawler.spider.meta[\"response\"], Response)\n-        self.assertEqual(\n-            crawler.spider.meta[\"response\"].body,\n-            crawler.spider.meta.get(\"bytes_received\"),\n+        assert crawler.spider.meta.get(\"failure\") is None\n+        assert isinstance(crawler.spider.meta[\"response\"], Response)\n+        assert crawler.spider.meta[\"response\"].body == crawler.spider.meta.get(\n+            \"bytes_received\"\n         )\n-        self.assertLess(\n-            len(crawler.spider.meta[\"response\"].body),\n-            crawler.spider.full_response_length,\n+        assert (\n+            len(crawler.spider.meta[\"response\"].body)\n+            < crawler.spider.full_response_length\n         )\n \n     @defer.inlineCallbacks\n     def test_bytes_received_stop_download_errback(self):\n         crawler = get_crawler(BytesReceivedErrbackSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertIsNone(crawler.spider.meta.get(\"response\"))\n-        self.assertIsInstance(crawler.spider.meta[\"failure\"], Failure)\n-        self.assertIsInstance(crawler.spider.meta[\"failure\"].value, StopDownload)\n-        self.assertIsInstance(crawler.spider.meta[\"failure\"].value.response, Response)\n-        self.assertEqual(\n-            crawler.spider.meta[\"failure\"].value.response.body,\n-            crawler.spider.meta.get(\"bytes_received\"),\n-        )\n-        self.assertLess(\n-            len(crawler.spider.meta[\"failure\"].value.response.body),\n-            crawler.spider.full_response_length,\n+        assert crawler.spider.meta.get(\"response\") is None\n+        assert isinstance(crawler.spider.meta[\"failure\"], Failure)\n+        assert isinstance(crawler.spider.meta[\"failure\"].value, StopDownload)\n+        assert isinstance(crawler.spider.meta[\"failure\"].value.response, Response)\n+        assert crawler.spider.meta[\n+            \"failure\"\n+        ].value.response.body == crawler.spider.meta.get(\"bytes_received\")\n+        assert (\n+            len(crawler.spider.meta[\"failure\"].value.response.body)\n+            < crawler.spider.full_response_length\n         )\n \n     @defer.inlineCallbacks\n     def test_headers_received_stop_download_callback(self):\n         crawler = get_crawler(HeadersReceivedCallbackSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertIsNone(crawler.spider.meta.get(\"failure\"))\n-        self.assertIsInstance(crawler.spider.meta[\"response\"], Response)\n-        self.assertEqual(\n-            crawler.spider.meta[\"response\"].headers,\n-            crawler.spider.meta.get(\"headers_received\"),\n+        assert crawler.spider.meta.get(\"failure\") is None\n+        assert isinstance(crawler.spider.meta[\"response\"], Response)\n+        assert crawler.spider.meta[\"response\"].headers == crawler.spider.meta.get(\n+            \"headers_received\"\n         )\n \n     @defer.inlineCallbacks\n     def test_headers_received_stop_download_errback(self):\n         crawler = get_crawler(HeadersReceivedErrbackSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertIsNone(crawler.spider.meta.get(\"response\"))\n-        self.assertIsInstance(crawler.spider.meta[\"failure\"], Failure)\n-        self.assertIsInstance(crawler.spider.meta[\"failure\"].value, StopDownload)\n-        self.assertIsInstance(crawler.spider.meta[\"failure\"].value.response, Response)\n-        self.assertEqual(\n-            crawler.spider.meta[\"failure\"].value.response.headers,\n-            crawler.spider.meta.get(\"headers_received\"),\n-        )\n+        assert crawler.spider.meta.get(\"response\") is None\n+        assert isinstance(crawler.spider.meta[\"failure\"], Failure)\n+        assert isinstance(crawler.spider.meta[\"failure\"].value, StopDownload)\n+        assert isinstance(crawler.spider.meta[\"failure\"].value.response, Response)\n+        assert crawler.spider.meta[\n+            \"failure\"\n+        ].value.response.headers == crawler.spider.meta.get(\"headers_received\")\n\n@@ -39,13 +39,13 @@ def get_raw_crawler(spidercls=None, settings_dict=None):\n     return Crawler(spidercls or DefaultSpider, settings)\n \n \n-class BaseCrawlerTest(unittest.TestCase):\n+class TestBaseCrawler(unittest.TestCase):\n     def assertOptionIsDefault(self, settings, key):\n-        self.assertIsInstance(settings, Settings)\n-        self.assertEqual(settings[key], getattr(default_settings, key))\n+        assert isinstance(settings, Settings)\n+        assert settings[key] == getattr(default_settings, key)\n \n \n-class CrawlerTestCase(BaseCrawlerTest):\n+class TestCrawler(TestBaseCrawler):\n     def test_populate_spidercls_settings(self):\n         spider_settings = {\"TEST1\": \"spider\", \"TEST2\": \"spider\"}\n         project_settings = {**BASE_SETTINGS, \"TEST1\": \"project\", \"TEST3\": \"project\"}\n@@ -58,16 +58,16 @@ class CrawlerTestCase(BaseCrawlerTest):\n         crawler = Crawler(CustomSettingsSpider, settings)\n         crawler._apply_settings()\n \n-        self.assertEqual(crawler.settings.get(\"TEST1\"), \"spider\")\n-        self.assertEqual(crawler.settings.get(\"TEST2\"), \"spider\")\n-        self.assertEqual(crawler.settings.get(\"TEST3\"), \"project\")\n+        assert crawler.settings.get(\"TEST1\") == \"spider\"\n+        assert crawler.settings.get(\"TEST2\") == \"spider\"\n+        assert crawler.settings.get(\"TEST3\") == \"project\"\n \n-        self.assertFalse(settings.frozen)\n-        self.assertTrue(crawler.settings.frozen)\n+        assert not settings.frozen\n+        assert crawler.settings.frozen\n \n     def test_crawler_accepts_dict(self):\n         crawler = get_crawler(DefaultSpider, {\"foo\": \"bar\"})\n-        self.assertEqual(crawler.settings[\"foo\"], \"bar\")\n+        assert crawler.settings[\"foo\"] == \"bar\"\n         self.assertOptionIsDefault(crawler.settings, \"RETRY_ENABLED\")\n \n     def test_crawler_accepts_None(self):\n@@ -107,23 +107,23 @@ class CrawlerTestCase(BaseCrawlerTest):\n             },\n         }\n         crawler = get_crawler(settings_dict=settings)\n-        self.assertEqual(len(TrackingAddon.instances), 1)\n+        assert len(TrackingAddon.instances) == 1\n         expected = TrackingAddon.instances[-1]\n \n         addon = crawler.get_addon(TrackingAddon)\n-        self.assertEqual(addon, expected)\n+        assert addon == expected\n \n         addon = crawler.get_addon(DefaultSpider)\n-        self.assertIsNone(addon)\n+        assert addon is None\n \n         addon = crawler.get_addon(ParentAddon)\n-        self.assertEqual(addon, expected)\n+        assert addon == expected\n \n         class ChildAddon(TrackingAddon):\n             pass\n \n         addon = crawler.get_addon(ChildAddon)\n-        self.assertIsNone(addon)\n+        assert addon is None\n \n     @inlineCallbacks\n     def test_get_downloader_middleware(self):\n@@ -162,18 +162,18 @@ class CrawlerTestCase(BaseCrawlerTest):\n         crawler = get_raw_crawler(MySpider, settings)\n         MySpider.cls = TrackingDownloaderMiddleware\n         yield crawler.crawl()\n-        self.assertEqual(len(TrackingDownloaderMiddleware.instances), 1)\n-        self.assertEqual(MySpider.result, TrackingDownloaderMiddleware.instances[-1])\n+        assert len(TrackingDownloaderMiddleware.instances) == 1\n+        assert MySpider.result == TrackingDownloaderMiddleware.instances[-1]\n \n         crawler = get_raw_crawler(MySpider, settings)\n         MySpider.cls = DefaultSpider\n         yield crawler.crawl()\n-        self.assertIsNone(MySpider.result)\n+        assert MySpider.result is None\n \n         crawler = get_raw_crawler(MySpider, settings)\n         MySpider.cls = ParentDownloaderMiddleware\n         yield crawler.crawl()\n-        self.assertEqual(MySpider.result, TrackingDownloaderMiddleware.instances[-1])\n+        assert MySpider.result == TrackingDownloaderMiddleware.instances[-1]\n \n         class ChildDownloaderMiddleware(TrackingDownloaderMiddleware):\n             pass\n@@ -181,7 +181,7 @@ class CrawlerTestCase(BaseCrawlerTest):\n         crawler = get_raw_crawler(MySpider, settings)\n         MySpider.cls = ChildDownloaderMiddleware\n         yield crawler.crawl()\n-        self.assertIsNone(MySpider.result)\n+        assert MySpider.result is None\n \n     def test_get_downloader_middleware_not_crawling(self):\n         crawler = get_raw_crawler(settings_dict=BASE_SETTINGS)\n@@ -242,18 +242,18 @@ class CrawlerTestCase(BaseCrawlerTest):\n         crawler = get_raw_crawler(MySpider, settings)\n         MySpider.cls = TrackingExtension\n         yield crawler.crawl()\n-        self.assertEqual(len(TrackingExtension.instances), 1)\n-        self.assertEqual(MySpider.result, TrackingExtension.instances[-1])\n+        assert len(TrackingExtension.instances) == 1\n+        assert MySpider.result == TrackingExtension.instances[-1]\n \n         crawler = get_raw_crawler(MySpider, settings)\n         MySpider.cls = DefaultSpider\n         yield crawler.crawl()\n-        self.assertIsNone(MySpider.result)\n+        assert MySpider.result is None\n \n         crawler = get_raw_crawler(MySpider, settings)\n         MySpider.cls = ParentExtension\n         yield crawler.crawl()\n-        self.assertEqual(MySpider.result, TrackingExtension.instances[-1])\n+        assert MySpider.result == TrackingExtension.instances[-1]\n \n         class ChildExtension(TrackingExtension):\n             pass\n@@ -261,7 +261,7 @@ class CrawlerTestCase(BaseCrawlerTest):\n         crawler = get_raw_crawler(MySpider, settings)\n         MySpider.cls = ChildExtension\n         yield crawler.crawl()\n-        self.assertIsNone(MySpider.result)\n+        assert MySpider.result is None\n \n     def test_get_extension_not_crawling(self):\n         crawler = get_raw_crawler(settings_dict=BASE_SETTINGS)\n@@ -322,18 +322,18 @@ class CrawlerTestCase(BaseCrawlerTest):\n         crawler = get_raw_crawler(MySpider, settings)\n         MySpider.cls = TrackingItemPipeline\n         yield crawler.crawl()\n-        self.assertEqual(len(TrackingItemPipeline.instances), 1)\n-        self.assertEqual(MySpider.result, TrackingItemPipeline.instances[-1])\n+        assert len(TrackingItemPipeline.instances) == 1\n+        assert MySpider.result == TrackingItemPipeline.instances[-1]\n \n         crawler = get_raw_crawler(MySpider, settings)\n         MySpider.cls = DefaultSpider\n         yield crawler.crawl()\n-        self.assertIsNone(MySpider.result)\n+        assert MySpider.result is None\n \n         crawler = get_raw_crawler(MySpider, settings)\n         MySpider.cls = ParentItemPipeline\n         yield crawler.crawl()\n-        self.assertEqual(MySpider.result, TrackingItemPipeline.instances[-1])\n+        assert MySpider.result == TrackingItemPipeline.instances[-1]\n \n         class ChildItemPipeline(TrackingItemPipeline):\n             pass\n@@ -341,7 +341,7 @@ class CrawlerTestCase(BaseCrawlerTest):\n         crawler = get_raw_crawler(MySpider, settings)\n         MySpider.cls = ChildItemPipeline\n         yield crawler.crawl()\n-        self.assertIsNone(MySpider.result)\n+        assert MySpider.result is None\n \n     def test_get_item_pipeline_not_crawling(self):\n         crawler = get_raw_crawler(settings_dict=BASE_SETTINGS)\n@@ -402,18 +402,18 @@ class CrawlerTestCase(BaseCrawlerTest):\n         crawler = get_raw_crawler(MySpider, settings)\n         MySpider.cls = TrackingSpiderMiddleware\n         yield crawler.crawl()\n-        self.assertEqual(len(TrackingSpiderMiddleware.instances), 1)\n-        self.assertEqual(MySpider.result, TrackingSpiderMiddleware.instances[-1])\n+        assert len(TrackingSpiderMiddleware.instances) == 1\n+        assert MySpider.result == TrackingSpiderMiddleware.instances[-1]\n \n         crawler = get_raw_crawler(MySpider, settings)\n         MySpider.cls = DefaultSpider\n         yield crawler.crawl()\n-        self.assertIsNone(MySpider.result)\n+        assert MySpider.result is None\n \n         crawler = get_raw_crawler(MySpider, settings)\n         MySpider.cls = ParentSpiderMiddleware\n         yield crawler.crawl()\n-        self.assertEqual(MySpider.result, TrackingSpiderMiddleware.instances[-1])\n+        assert MySpider.result == TrackingSpiderMiddleware.instances[-1]\n \n         class ChildSpiderMiddleware(TrackingSpiderMiddleware):\n             pass\n@@ -421,7 +421,7 @@ class CrawlerTestCase(BaseCrawlerTest):\n         crawler = get_raw_crawler(MySpider, settings)\n         MySpider.cls = ChildSpiderMiddleware\n         yield crawler.crawl()\n-        self.assertIsNone(MySpider.result)\n+        assert MySpider.result is None\n \n     def test_get_spider_middleware_not_crawling(self):\n         crawler = get_raw_crawler(settings_dict=BASE_SETTINGS)\n@@ -446,7 +446,7 @@ class CrawlerTestCase(BaseCrawlerTest):\n             yield crawler.crawl()\n \n \n-class SpiderSettingsTestCase(unittest.TestCase):\n+class TestSpiderSettings:\n     def test_spider_custom_settings(self):\n         class MySpider(scrapy.Spider):\n             name = \"spider\"\n@@ -454,10 +454,10 @@ class SpiderSettingsTestCase(unittest.TestCase):\n \n         crawler = get_crawler(MySpider)\n         enabled_exts = [e.__class__ for e in crawler.extensions.middlewares]\n-        self.assertIn(AutoThrottle, enabled_exts)\n+        assert AutoThrottle in enabled_exts\n \n \n-class CrawlerLoggingTestCase(unittest.TestCase):\n+class TestCrawlerLogging:\n     def test_no_root_handler_installed(self):\n         handler = get_scrapy_root_handler()\n         if handler is not None:\n@@ -469,8 +469,8 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n         get_crawler(MySpider)\n         assert get_scrapy_root_handler() is None\n \n-    def test_spider_custom_settings_log_level(self):\n-        log_file = Path(self.mktemp())\n+    def test_spider_custom_settings_log_level(self, tmp_path):\n+        log_file = Path(tmp_path, \"log.txt\")\n         log_file.write_text(\"previous message\\n\", encoding=\"utf-8\")\n \n         class MySpider(scrapy.Spider):\n@@ -481,9 +481,9 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n             }\n \n         configure_logging()\n-        self.assertEqual(get_scrapy_root_handler().level, logging.DEBUG)\n+        assert get_scrapy_root_handler().level == logging.DEBUG\n         crawler = get_crawler(MySpider)\n-        self.assertEqual(get_scrapy_root_handler().level, logging.INFO)\n+        assert get_scrapy_root_handler().level == logging.INFO\n         info_count = crawler.stats.get_value(\"log_count/INFO\")\n         logging.debug(\"debug message\")\n         logging.info(\"info message\")\n@@ -492,18 +492,18 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n \n         logged = log_file.read_text(encoding=\"utf-8\")\n \n-        self.assertIn(\"previous message\", logged)\n-        self.assertNotIn(\"debug message\", logged)\n-        self.assertIn(\"info message\", logged)\n-        self.assertIn(\"warning message\", logged)\n-        self.assertIn(\"error message\", logged)\n-        self.assertEqual(crawler.stats.get_value(\"log_count/ERROR\"), 1)\n-        self.assertEqual(crawler.stats.get_value(\"log_count/WARNING\"), 1)\n-        self.assertEqual(crawler.stats.get_value(\"log_count/INFO\") - info_count, 1)\n-        self.assertEqual(crawler.stats.get_value(\"log_count/DEBUG\", 0), 0)\n+        assert \"previous message\" in logged\n+        assert \"debug message\" not in logged\n+        assert \"info message\" in logged\n+        assert \"warning message\" in logged\n+        assert \"error message\" in logged\n+        assert crawler.stats.get_value(\"log_count/ERROR\") == 1\n+        assert crawler.stats.get_value(\"log_count/WARNING\") == 1\n+        assert crawler.stats.get_value(\"log_count/INFO\") - info_count == 1\n+        assert crawler.stats.get_value(\"log_count/DEBUG\", 0) == 0\n \n-    def test_spider_custom_settings_log_append(self):\n-        log_file = Path(self.mktemp())\n+    def test_spider_custom_settings_log_append(self, tmp_path):\n+        log_file = Path(tmp_path, \"log.txt\")\n         log_file.write_text(\"previous message\\n\", encoding=\"utf-8\")\n \n         class MySpider(scrapy.Spider):\n@@ -519,8 +519,8 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n \n         logged = log_file.read_text(encoding=\"utf-8\")\n \n-        self.assertNotIn(\"previous message\", logged)\n-        self.assertIn(\"debug message\", logged)\n+        assert \"previous message\" not in logged\n+        assert \"debug message\" in logged\n \n \n class SpiderLoaderWithWrongInterface:\n@@ -532,7 +532,7 @@ class CustomSpiderLoader(SpiderLoader):\n     pass\n \n \n-class CrawlerRunnerTestCase(BaseCrawlerTest):\n+class TestCrawlerRunner(TestBaseCrawler):\n     def test_spider_manager_verify_interface(self):\n         settings = Settings(\n             {\n@@ -544,7 +544,7 @@ class CrawlerRunnerTestCase(BaseCrawlerTest):\n \n     def test_crawler_runner_accepts_dict(self):\n         runner = CrawlerRunner({\"foo\": \"bar\"})\n-        self.assertEqual(runner.settings[\"foo\"], \"bar\")\n+        assert runner.settings[\"foo\"] == \"bar\"\n         self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n \n     def test_crawler_runner_accepts_None(self):\n@@ -552,10 +552,10 @@ class CrawlerRunnerTestCase(BaseCrawlerTest):\n         self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n \n \n-class CrawlerProcessTest(BaseCrawlerTest):\n+class TestCrawlerProcess(TestBaseCrawler):\n     def test_crawler_process_accepts_dict(self):\n         runner = CrawlerProcess({\"foo\": \"bar\"})\n-        self.assertEqual(runner.settings[\"foo\"], \"bar\")\n+        assert runner.settings[\"foo\"] == \"bar\"\n         self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n \n     def test_crawler_process_accepts_None(self):\n@@ -579,7 +579,7 @@ class NoRequestsSpider(scrapy.Spider):\n \n \n @pytest.mark.usefixtures(\"reactor_pytest\")\n-class CrawlerRunnerHasSpider(unittest.TestCase):\n+class TestCrawlerRunnerHasSpider(unittest.TestCase):\n     def _runner(self):\n         return CrawlerRunner()\n \n@@ -587,14 +587,14 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n     def test_crawler_runner_bootstrap_successful(self):\n         runner = self._runner()\n         yield runner.crawl(NoRequestsSpider)\n-        self.assertFalse(runner.bootstrap_failed)\n+        assert not runner.bootstrap_failed\n \n     @inlineCallbacks\n     def test_crawler_runner_bootstrap_successful_for_several(self):\n         runner = self._runner()\n         yield runner.crawl(NoRequestsSpider)\n         yield runner.crawl(NoRequestsSpider)\n-        self.assertFalse(runner.bootstrap_failed)\n+        assert not runner.bootstrap_failed\n \n     @inlineCallbacks\n     def test_crawler_runner_bootstrap_failed(self):\n@@ -605,9 +605,9 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n         except ValueError:\n             pass\n         else:\n-            self.fail(\"Exception should be raised from spider\")\n+            pytest.fail(\"Exception should be raised from spider\")\n \n-        self.assertTrue(runner.bootstrap_failed)\n+        assert runner.bootstrap_failed\n \n     @inlineCallbacks\n     def test_crawler_runner_bootstrap_failed_for_several(self):\n@@ -618,11 +618,11 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n         except ValueError:\n             pass\n         else:\n-            self.fail(\"Exception should be raised from spider\")\n+            pytest.fail(\"Exception should be raised from spider\")\n \n         yield runner.crawl(NoRequestsSpider)\n \n-        self.assertTrue(runner.bootstrap_failed)\n+        assert runner.bootstrap_failed\n \n     @inlineCallbacks\n     def test_crawler_runner_asyncio_enabled_true(self):\n@@ -664,31 +664,34 @@ class ScriptRunnerMixin:\n         return stderr.decode(\"utf-8\")\n \n \n-class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n+class TestCrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n     script_dir = Path(__file__).parent.resolve() / \"CrawlerProcess\"\n \n     def test_simple(self):\n         log = self.run_script(\"simple.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertNotIn(\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            not in log\n         )\n \n     def test_multi(self):\n         log = self.run_script(\"multi.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertNotIn(\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            not in log\n         )\n-        self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n+        assert \"ReactorAlreadyInstalledError\" not in log\n \n     def test_reactor_default(self):\n         log = self.run_script(\"reactor_default.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertNotIn(\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            not in log\n         )\n-        self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n+        assert \"ReactorAlreadyInstalledError\" not in log\n \n     def test_reactor_default_twisted_reactor_select(self):\n         log = self.run_script(\"reactor_default_twisted_reactor_select.py\")\n@@ -703,50 +706,46 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n             # If that ever becomes the case on more platforms (i.e. if Linux\n             # also starts using the select reactor by default in a future\n             # version of Twisted), then we will need to rethink this test.\n-            self.assertIn(\"Spider closed (finished)\", log)\n+            assert \"Spider closed (finished)\" in log\n         else:\n-            self.assertNotIn(\"Spider closed (finished)\", log)\n-            self.assertIn(\n-                (\n+            assert \"Spider closed (finished)\" not in log\n+            assert (\n                 \"does not match the requested one \"\n                 \"(twisted.internet.selectreactor.SelectReactor)\"\n-                ),\n-                log,\n-            )\n+            ) in log\n \n     def test_reactor_select(self):\n         log = self.run_script(\"reactor_select.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n+        assert \"Spider closed (finished)\" in log\n+        assert \"ReactorAlreadyInstalledError\" not in log\n \n     def test_reactor_select_twisted_reactor_select(self):\n         log = self.run_script(\"reactor_select_twisted_reactor_select.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n+        assert \"Spider closed (finished)\" in log\n+        assert \"ReactorAlreadyInstalledError\" not in log\n \n     def test_reactor_select_subclass_twisted_reactor_select(self):\n         log = self.run_script(\"reactor_select_subclass_twisted_reactor_select.py\")\n-        self.assertNotIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\n-            (\n+        assert \"Spider closed (finished)\" not in log\n+        assert (\n             \"does not match the requested one \"\n             \"(twisted.internet.selectreactor.SelectReactor)\"\n-            ),\n-            log,\n-        )\n+        ) in log\n \n     def test_asyncio_enabled_no_reactor(self):\n         log = self.run_script(\"asyncio_enabled_no_reactor.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n         )\n \n     def test_asyncio_enabled_reactor(self):\n         log = self.run_script(\"asyncio_enabled_reactor.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n         )\n \n     @pytest.mark.skipif(\n@@ -755,129 +754,129 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n     )\n     def test_ipv6_default_name_resolver(self):\n         log = self.run_script(\"default_name_resolver.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\n-            \"'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,\",\n-            log,\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,\"\n+            in log\n         )\n-        self.assertIn(\n-            \"twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ::1.\",\n-            log,\n+        assert (\n+            \"twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ::1.\"\n+            in log\n         )\n \n     def test_caching_hostname_resolver_ipv6(self):\n         log = self.run_script(\"caching_hostname_resolver_ipv6.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertNotIn(\"twisted.internet.error.DNSLookupError\", log)\n+        assert \"Spider closed (finished)\" in log\n+        assert \"twisted.internet.error.DNSLookupError\" not in log\n \n     def test_caching_hostname_resolver_finite_execution(self):\n         with MockServer() as mock_server:\n             http_address = mock_server.http_address.replace(\"0.0.0.0\", \"127.0.0.1\")\n             log = self.run_script(\"caching_hostname_resolver.py\", http_address)\n-            self.assertIn(\"Spider closed (finished)\", log)\n-            self.assertNotIn(\"ERROR: Error downloading\", log)\n-            self.assertNotIn(\"TimeoutError\", log)\n-            self.assertNotIn(\"twisted.internet.error.DNSLookupError\", log)\n+            assert \"Spider closed (finished)\" in log\n+            assert \"ERROR: Error downloading\" not in log\n+            assert \"TimeoutError\" not in log\n+            assert \"twisted.internet.error.DNSLookupError\" not in log\n \n     def test_twisted_reactor_select(self):\n         log = self.run_script(\"twisted_reactor_select.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\n-            \"Using reactor: twisted.internet.selectreactor.SelectReactor\", log\n-        )\n+        assert \"Spider closed (finished)\" in log\n+        assert \"Using reactor: twisted.internet.selectreactor.SelectReactor\" in log\n \n     @pytest.mark.skipif(\n         platform.system() == \"Windows\", reason=\"PollReactor is not supported on Windows\"\n     )\n     def test_twisted_reactor_poll(self):\n         log = self.run_script(\"twisted_reactor_poll.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\"Using reactor: twisted.internet.pollreactor.PollReactor\", log)\n+        assert \"Spider closed (finished)\" in log\n+        assert \"Using reactor: twisted.internet.pollreactor.PollReactor\" in log\n \n     def test_twisted_reactor_asyncio(self):\n         log = self.run_script(\"twisted_reactor_asyncio.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n         )\n \n     def test_twisted_reactor_asyncio_custom_settings(self):\n         log = self.run_script(\"twisted_reactor_custom_settings.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n         )\n \n     def test_twisted_reactor_asyncio_custom_settings_same(self):\n         log = self.run_script(\"twisted_reactor_custom_settings_same.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n         )\n \n     def test_twisted_reactor_asyncio_custom_settings_conflict(self):\n         log = self.run_script(\"twisted_reactor_custom_settings_conflict.py\")\n-        self.assertIn(\n-            \"Using reactor: twisted.internet.selectreactor.SelectReactor\", log\n-        )\n-        self.assertIn(\n-            \"(twisted.internet.selectreactor.SelectReactor) does not match the requested one\",\n-            log,\n+        assert \"Using reactor: twisted.internet.selectreactor.SelectReactor\" in log\n+        assert (\n+            \"(twisted.internet.selectreactor.SelectReactor) does not match the requested one\"\n+            in log\n         )\n \n     @pytest.mark.requires_uvloop\n     def test_custom_loop_asyncio(self):\n         log = self.run_script(\"asyncio_custom_loop.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n         )\n-        self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n+        assert \"Using asyncio event loop: uvloop.Loop\" in log\n \n     @pytest.mark.requires_uvloop\n     def test_custom_loop_asyncio_deferred_signal(self):\n         log = self.run_script(\"asyncio_deferred_signal.py\", \"uvloop.Loop\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n         )\n-        self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n-        self.assertIn(\"async pipeline opened!\", log)\n+        assert \"Using asyncio event loop: uvloop.Loop\" in log\n+        assert \"async pipeline opened!\" in log\n \n     @pytest.mark.requires_uvloop\n     def test_asyncio_enabled_reactor_same_loop(self):\n         log = self.run_script(\"asyncio_enabled_reactor_same_loop.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n         )\n-        self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n+        assert \"Using asyncio event loop: uvloop.Loop\" in log\n \n     @pytest.mark.requires_uvloop\n     def test_asyncio_enabled_reactor_different_loop(self):\n         log = self.run_script(\"asyncio_enabled_reactor_different_loop.py\")\n-        self.assertNotIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\n-            (\n+        assert \"Spider closed (finished)\" not in log\n+        assert (\n             \"does not match the one specified in the ASYNCIO_EVENT_LOOP \"\n             \"setting (uvloop.Loop)\"\n-            ),\n-            log,\n-        )\n+        ) in log\n \n     def test_default_loop_asyncio_deferred_signal(self):\n         log = self.run_script(\"asyncio_deferred_signal.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n         )\n-        self.assertNotIn(\"Using asyncio event loop: uvloop.Loop\", log)\n-        self.assertIn(\"async pipeline opened!\", log)\n+        assert \"Using asyncio event loop: uvloop.Loop\" not in log\n+        assert \"async pipeline opened!\" in log\n \n     def test_args_change_settings(self):\n         log = self.run_script(\"args_settings.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\"The value of FOO is 42\", log)\n+        assert \"Spider closed (finished)\" in log\n+        assert \"The value of FOO is 42\" in log\n \n     def test_shutdown_graceful(self):\n         sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n@@ -910,23 +909,23 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         p.wait()\n \n \n-class CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):\n+class TestCrawlerRunnerSubprocess(ScriptRunnerMixin):\n     script_dir = Path(__file__).parent.resolve() / \"CrawlerRunner\"\n \n     def test_response_ip_address(self):\n         log = self.run_script(\"ip_address.py\")\n-        self.assertIn(\"INFO: Spider closed (finished)\", log)\n-        self.assertIn(\"INFO: Host: not.a.real.domain\", log)\n-        self.assertIn(\"INFO: Type: <class 'ipaddress.IPv4Address'>\", log)\n-        self.assertIn(\"INFO: IP address: 127.0.0.1\", log)\n+        assert \"INFO: Spider closed (finished)\" in log\n+        assert \"INFO: Host: not.a.real.domain\" in log\n+        assert \"INFO: Type: <class 'ipaddress.IPv4Address'>\" in log\n+        assert \"INFO: IP address: 127.0.0.1\" in log\n \n     def test_change_default_reactor(self):\n         log = self.run_script(\"change_reactor.py\")\n-        self.assertIn(\n-            \"DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n-            log,\n+        assert (\n+            \"DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n         )\n-        self.assertIn(\"DEBUG: Using asyncio event loop\", log)\n+        assert \"DEBUG: Using asyncio event loop\" in log\n \n \n @pytest.mark.parametrize(\n\n@@ -4,11 +4,12 @@ from configparser import ConfigParser\n from importlib import import_module\n from pathlib import Path\n \n+import pytest\n from twisted import version as twisted_version\n from twisted.trial import unittest\n \n \n-class ScrapyUtilsTest(unittest.TestCase):\n+class TestScrapyUtils:\n     def test_required_openssl_version(self):\n         try:\n             module = import_module(\"OpenSSL\")\n@@ -27,7 +28,7 @@ class ScrapyUtilsTest(unittest.TestCase):\n         See https://github.com/scrapy/scrapy/pull/4814#issuecomment-706230011\n         \"\"\"\n         if not os.environ.get(\"_SCRAPY_PINNED\", None):\n-            self.skipTest(\"Not in a pinned environment\")\n+            pytest.skip(\"Not in a pinned environment\")\n \n         tox_config_file_path = Path(__file__).parent / \"..\" / \"tox.ini\"\n         config_parser = ConfigParser()\n@@ -36,4 +37,4 @@ class ScrapyUtilsTest(unittest.TestCase):\n         match = re.search(pattern, config_parser[\"pinned\"][\"deps\"])\n         pinned_twisted_version_string = match[1]\n \n-        self.assertEqual(twisted_version.short(), pinned_twisted_version_string)\n+        assert twisted_version.short() == pinned_twisted_version_string\n\n@@ -76,7 +76,7 @@ class CrawlTestCase(TestCase):\n             for k, v in slots.items()\n         }\n \n-        self.assertTrue(max(list(error_delta.values())) < tolerance)\n+        assert max(list(error_delta.values())) < tolerance\n \n \n def test_params():\n\n@@ -2,7 +2,6 @@ import hashlib\n import shutil\n import sys\n import tempfile\n-import unittest\n from pathlib import Path\n from warnings import catch_warnings\n \n@@ -39,7 +38,7 @@ class DirectDupeFilter:\n     method = \"n/a\"\n \n \n-class RFPDupeFilterTest(unittest.TestCase):\n+class TestRFPDupeFilter:\n     def test_df_from_crawler_scheduler(self):\n         settings = {\n             \"DUPEFILTER_DEBUG\": True,\n@@ -47,8 +46,8 @@ class RFPDupeFilterTest(unittest.TestCase):\n         }\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n-        self.assertTrue(scheduler.df.debug)\n-        self.assertEqual(scheduler.df.method, \"from_crawler\")\n+        assert scheduler.df.debug\n+        assert scheduler.df.method == \"from_crawler\"\n \n     def test_df_direct_scheduler(self):\n         settings = {\n@@ -56,7 +55,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n         }\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n-        self.assertEqual(scheduler.df.method, \"n/a\")\n+        assert scheduler.df.method == \"n/a\"\n \n     def test_filter(self):\n         dupefilter = _get_dupefilter()\n@@ -256,16 +255,16 @@ class RFPDupeFilterTest(unittest.TestCase):\n             dupefilter.close(\"finished\")\n \n \n-class BaseDupeFilterTestCase(unittest.TestCase):\n+class TestBaseDupeFilter:\n     def test_log_deprecation(self):\n         dupefilter = _get_dupefilter(\n             settings={\"DUPEFILTER_CLASS\": BaseDupeFilter},\n         )\n         with catch_warnings(record=True) as warning_list:\n             dupefilter.log(None, None)\n-        self.assertEqual(len(warning_list), 1)\n-        self.assertEqual(\n-            str(warning_list[0].message),\n-            \"Calling BaseDupeFilter.log() is deprecated.\",\n+        assert len(warning_list) == 1\n+        assert (\n+            str(warning_list[0].message)\n+            == \"Calling BaseDupeFilter.log() is deprecated.\"\n         )\n-        self.assertEqual(warning_list[0].category, ScrapyDeprecationWarning)\n+        assert warning_list[0].category == ScrapyDeprecationWarning\n\n@@ -243,8 +243,9 @@ class CrawlerRun:\n         self.signals_caught[sig] = signalargs\n \n \n-class EngineTestBase(unittest.TestCase):\n-    def _assert_visited_urls(self, run: CrawlerRun):\n+class TestEngineBase(unittest.TestCase):\n+    @staticmethod\n+    def _assert_visited_urls(run: CrawlerRun) -> None:\n         must_be_visited = [\n             \"/\",\n             \"/redirect\",\n@@ -259,8 +260,9 @@ class EngineTestBase(unittest.TestCase):\n             f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n         )\n \n-    def _assert_scheduled_requests(self, run: CrawlerRun, count=None):\n-        self.assertEqual(count, len(run.reqplug))\n+    @staticmethod\n+    def _assert_scheduled_requests(run: CrawlerRun, count: int) -> None:\n+        assert len(run.reqplug) == count\n \n         paths_expected = [\"/item999.html\", \"/item2.html\", \"/item1.html\"]\n \n@@ -270,101 +272,104 @@ class EngineTestBase(unittest.TestCase):\n         scheduled_requests_count = len(run.reqplug)\n         dropped_requests_count = len(run.reqdropped)\n         responses_count = len(run.respplug)\n-        self.assertEqual(\n-            scheduled_requests_count, dropped_requests_count + responses_count\n-        )\n-        self.assertEqual(len(run.reqreached), responses_count)\n+        assert scheduled_requests_count == dropped_requests_count + responses_count\n+        assert len(run.reqreached) == responses_count\n \n-    def _assert_dropped_requests(self, run: CrawlerRun):\n-        self.assertEqual(len(run.reqdropped), 1)\n+    @staticmethod\n+    def _assert_dropped_requests(run: CrawlerRun) -> None:\n+        assert len(run.reqdropped) == 1\n \n-    def _assert_downloaded_responses(self, run: CrawlerRun, count):\n+    @staticmethod\n+    def _assert_downloaded_responses(run: CrawlerRun, count: int) -> None:\n         # response tests\n-        self.assertEqual(count, len(run.respplug))\n-        self.assertEqual(count, len(run.reqreached))\n+        assert len(run.respplug) == count\n+        assert len(run.reqreached) == count\n \n         for response, _ in run.respplug:\n             if run.getpath(response.url) == \"/item999.html\":\n-                self.assertEqual(404, response.status)\n+                assert response.status == 404\n             if run.getpath(response.url) == \"/redirect\":\n-                self.assertEqual(302, response.status)\n+                assert response.status == 302\n \n-    def _assert_items_error(self, run: CrawlerRun):\n-        self.assertEqual(2, len(run.itemerror))\n+    @staticmethod\n+    def _assert_items_error(run: CrawlerRun) -> None:\n+        assert len(run.itemerror) == 2\n         for item, response, spider, failure in run.itemerror:\n-            self.assertEqual(failure.value.__class__, ZeroDivisionError)\n-            self.assertEqual(spider, run.spider)\n+            assert failure.value.__class__ is ZeroDivisionError\n+            assert spider == run.spider\n \n-            self.assertEqual(item[\"url\"], response.url)\n+            assert item[\"url\"] == response.url\n             if \"item1.html\" in item[\"url\"]:\n-                self.assertEqual(\"Item 1 name\", item[\"name\"])\n-                self.assertEqual(\"100\", item[\"price\"])\n+                assert item[\"name\"] == \"Item 1 name\"\n+                assert item[\"price\"] == \"100\"\n             if \"item2.html\" in item[\"url\"]:\n-                self.assertEqual(\"Item 2 name\", item[\"name\"])\n-                self.assertEqual(\"200\", item[\"price\"])\n+                assert item[\"name\"] == \"Item 2 name\"\n+                assert item[\"price\"] == \"200\"\n \n-    def _assert_scraped_items(self, run: CrawlerRun):\n-        self.assertEqual(2, len(run.itemresp))\n+    @staticmethod\n+    def _assert_scraped_items(run: CrawlerRun) -> None:\n+        assert len(run.itemresp) == 2\n         for item, response in run.itemresp:\n             item = ItemAdapter(item)\n-            self.assertEqual(item[\"url\"], response.url)\n+            assert item[\"url\"] == response.url\n             if \"item1.html\" in item[\"url\"]:\n-                self.assertEqual(\"Item 1 name\", item[\"name\"])\n-                self.assertEqual(\"100\", item[\"price\"])\n+                assert item[\"name\"] == \"Item 1 name\"\n+                assert item[\"price\"] == \"100\"\n             if \"item2.html\" in item[\"url\"]:\n-                self.assertEqual(\"Item 2 name\", item[\"name\"])\n-                self.assertEqual(\"200\", item[\"price\"])\n+                assert item[\"name\"] == \"Item 2 name\"\n+                assert item[\"price\"] == \"200\"\n \n-    def _assert_headers_received(self, run: CrawlerRun):\n+    @staticmethod\n+    def _assert_headers_received(run: CrawlerRun) -> None:\n         for headers in run.headers.values():\n-            self.assertIn(b\"Server\", headers)\n-            self.assertIn(b\"TwistedWeb\", headers[b\"Server\"])\n-            self.assertIn(b\"Date\", headers)\n-            self.assertIn(b\"Content-Type\", headers)\n+            assert b\"Server\" in headers\n+            assert b\"TwistedWeb\" in headers[b\"Server\"]\n+            assert b\"Date\" in headers\n+            assert b\"Content-Type\" in headers\n \n-    def _assert_bytes_received(self, run: CrawlerRun):\n-        self.assertEqual(9, len(run.bytes))\n+    @staticmethod\n+    def _assert_bytes_received(run: CrawlerRun) -> None:\n+        assert len(run.bytes) == 9\n         for request, data in run.bytes.items():\n             joined_data = b\"\".join(data)\n             if run.getpath(request.url) == \"/\":\n-                self.assertEqual(joined_data, get_testdata(\"test_site\", \"index.html\"))\n+                assert joined_data == get_testdata(\"test_site\", \"index.html\")\n             elif run.getpath(request.url) == \"/item1.html\":\n-                self.assertEqual(joined_data, get_testdata(\"test_site\", \"item1.html\"))\n+                assert joined_data == get_testdata(\"test_site\", \"item1.html\")\n             elif run.getpath(request.url) == \"/item2.html\":\n-                self.assertEqual(joined_data, get_testdata(\"test_site\", \"item2.html\"))\n+                assert joined_data == get_testdata(\"test_site\", \"item2.html\")\n             elif run.getpath(request.url) == \"/redirected\":\n-                self.assertEqual(joined_data, b\"Redirected here\")\n+                assert joined_data == b\"Redirected here\"\n             elif run.getpath(request.url) == \"/redirect\":\n-                self.assertEqual(\n-                    joined_data,\n-                    b\"\\n<html>\\n\"\n+                assert (\n+                    joined_data == b\"\\n<html>\\n\"\n                     b\"    <head>\\n\"\n                     b'        <meta http-equiv=\"refresh\" content=\"0;URL=/redirected\">\\n'\n                     b\"    </head>\\n\"\n                     b'    <body bgcolor=\"#FFFFFF\" text=\"#000000\">\\n'\n                     b'    <a href=\"/redirected\">click here</a>\\n'\n                     b\"    </body>\\n\"\n-                    b\"</html>\\n\",\n+                    b\"</html>\\n\"\n                 )\n             elif run.getpath(request.url) == \"/tem999.html\":\n-                self.assertEqual(\n-                    joined_data,\n-                    b\"\\n<html>\\n\"\n+                assert (\n+                    joined_data == b\"\\n<html>\\n\"\n                     b\"  <head><title>404 - No Such Resource</title></head>\\n\"\n                     b\"  <body>\\n\"\n                     b\"    <h1>No Such Resource</h1>\\n\"\n                     b\"    <p>File not found.</p>\\n\"\n                     b\"  </body>\\n\"\n-                    b\"</html>\\n\",\n+                    b\"</html>\\n\"\n                 )\n             elif run.getpath(request.url) == \"/numbers\":\n                 # signal was fired multiple times\n-                self.assertTrue(len(data) > 1)\n+                assert len(data) > 1\n                 # bytes were received in order\n                 numbers = [str(x).encode(\"utf8\") for x in range(2**18)]\n-                self.assertEqual(joined_data, b\"\".join(numbers))\n+                assert joined_data == b\"\".join(numbers)\n \n-    def _assert_signals_caught(self, run: CrawlerRun):\n+    @staticmethod\n+    def _assert_signals_caught(run: CrawlerRun) -> None:\n         assert signals.engine_started in run.signals_caught\n         assert signals.engine_stopped in run.signals_caught\n         assert signals.spider_opened in run.signals_caught\n@@ -372,19 +377,14 @@ class EngineTestBase(unittest.TestCase):\n         assert signals.spider_closed in run.signals_caught\n         assert signals.headers_received in run.signals_caught\n \n-        self.assertEqual(\n-            {\"spider\": run.spider}, run.signals_caught[signals.spider_opened]\n-        )\n-        self.assertEqual(\n-            {\"spider\": run.spider}, run.signals_caught[signals.spider_idle]\n-        )\n-        self.assertEqual(\n-            {\"spider\": run.spider, \"reason\": \"finished\"},\n-            run.signals_caught[signals.spider_closed],\n-        )\n+        assert {\"spider\": run.spider} == run.signals_caught[signals.spider_opened]\n+        assert {\"spider\": run.spider} == run.signals_caught[signals.spider_idle]\n+        assert {\"spider\": run.spider, \"reason\": \"finished\"} == run.signals_caught[\n+            signals.spider_closed\n+        ]\n \n \n-class EngineTest(EngineTestBase):\n+class TestEngine(TestEngineBase):\n     @defer.inlineCallbacks\n     def test_crawler(self):\n         for spider in (\n@@ -419,10 +419,9 @@ class EngineTest(EngineTestBase):\n     def test_crawler_change_close_reason_on_idle(self):\n         run = CrawlerRun(ChangeCloseReasonSpider)\n         yield run.run()\n-        self.assertEqual(\n-            {\"spider\": run.spider, \"reason\": \"custom_reason\"},\n-            run.signals_caught[signals.spider_closed],\n-        )\n+        assert {\"spider\": run.spider, \"reason\": \"custom_reason\"} == run.signals_caught[\n+            signals.spider_closed\n+        ]\n \n     @defer.inlineCallbacks\n     def test_close_downloader(self):\n@@ -470,7 +469,7 @@ class EngineTest(EngineTestBase):\n         finally:\n             timer.cancel()\n \n-        self.assertNotIn(b\"Traceback\", stderr)\n+        assert b\"Traceback\" not in stderr\n \n \n def test_request_scheduled_signal(caplog):\n\n@@ -7,8 +7,8 @@ from tests.test_engine import (\n     CrawlerRun,\n     DataClassItemsSpider,\n     DictItemsSpider,\n-    EngineTestBase,\n     MySpider,\n+    TestEngineBase,\n )\n \n \n@@ -18,7 +18,7 @@ class BytesReceivedCrawlerRun(CrawlerRun):\n         raise StopDownload(fail=False)\n \n \n-class BytesReceivedEngineTest(EngineTestBase):\n+class TestBytesReceivedEngine(TestEngineBase):\n     @defer.inlineCallbacks\n     def test_crawler(self):\n         for spider in (\n@@ -61,14 +61,15 @@ class BytesReceivedEngineTest(EngineTestBase):\n             self._assert_headers_received(run)\n             self._assert_bytes_received(run)\n \n-    def _assert_bytes_received(self, run: CrawlerRun):\n-        self.assertEqual(9, len(run.bytes))\n+    @staticmethod\n+    def _assert_bytes_received(run: CrawlerRun) -> None:\n+        assert len(run.bytes) == 9\n         for request, data in run.bytes.items():\n             joined_data = b\"\".join(data)\n-            self.assertTrue(len(data) == 1)  # signal was fired only once\n+            assert len(data) == 1  # signal was fired only once\n             if run.getpath(request.url) == \"/numbers\":\n                 # Received bytes are not the complete response. The exact amount depends\n                 # on the buffer size, which can vary, so we only check that the amount\n                 # of received bytes is strictly less than the full response.\n                 numbers = [str(x).encode(\"utf8\") for x in range(2**18)]\n-                self.assertTrue(len(joined_data) < len(b\"\".join(numbers)))\n+                assert len(joined_data) < len(b\"\".join(numbers))\n\n@@ -7,8 +7,8 @@ from tests.test_engine import (\n     CrawlerRun,\n     DataClassItemsSpider,\n     DictItemsSpider,\n-    EngineTestBase,\n     MySpider,\n+    TestEngineBase,\n )\n \n \n@@ -18,7 +18,7 @@ class HeadersReceivedCrawlerRun(CrawlerRun):\n         raise StopDownload(fail=False)\n \n \n-class HeadersReceivedEngineTest(EngineTestBase):\n+class TestHeadersReceivedEngine(TestEngineBase):\n     @defer.inlineCallbacks\n     def test_crawler(self):\n         for spider in (\n@@ -60,10 +60,12 @@ class HeadersReceivedEngineTest(EngineTestBase):\n             self._assert_bytes_received(run)\n             self._assert_headers_received(run)\n \n-    def _assert_bytes_received(self, run: CrawlerRun):\n-        self.assertEqual(0, len(run.bytes))\n+    @staticmethod\n+    def _assert_bytes_received(run: CrawlerRun) -> None:\n+        assert len(run.bytes) == 0\n \n-    def _assert_visited_urls(self, run: CrawlerRun):\n+    @staticmethod\n+    def _assert_visited_urls(run: CrawlerRun) -> None:\n         must_be_visited = [\"/\", \"/redirect\", \"/redirected\"]\n         urls_visited = {rp[0].url for rp in run.respplug}\n         urls_expected = {run.geturl(p) for p in must_be_visited}\n\n@@ -36,7 +36,7 @@ from tests.mockserver import (\n     PayloadResource,\n     ssl_context_factory,\n )\n-from tests.test_core_downloader import ContextFactoryBaseTestCase\n+from tests.test_core_downloader import TestContextFactoryBase\n \n \n def getPage(url, contextFactory=None, response_transform=None, *args, **kwargs):\n@@ -63,7 +63,7 @@ def getPage(url, contextFactory=None, response_transform=None, *args, **kwargs):\n \n \n @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n-class ScrapyHTTPPageGetterTests(unittest.TestCase):\n+class TestScrapyHTTPPageGetter:\n     def test_earlyHeaders(self):\n         # basic test stolen from twisted HTTPageGetter\n         factory = client.ScrapyHTTPClientFactory(\n@@ -177,9 +177,7 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n         protocol = client.ScrapyHTTPPageGetter()\n         protocol.factory = factory\n         protocol.makeConnection(transport)\n-        self.assertEqual(\n-            set(transport.value().splitlines()), set(testvalue.splitlines())\n-        )\n+        assert set(transport.value().splitlines()) == set(testvalue.splitlines())\n         return testvalue\n \n     def test_non_standard_line_endings(self):\n@@ -192,9 +190,7 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n         protocol.dataReceived(b\"Hello: World\\n\")\n         protocol.dataReceived(b\"Foo: Bar\\n\")\n         protocol.dataReceived(b\"\\n\")\n-        self.assertEqual(\n-            protocol.headers, Headers({\"Hello\": [\"World\"], \"Foo\": [\"Bar\"]})\n-        )\n+        assert protocol.headers == Headers({\"Hello\": [\"World\"], \"Foo\": [\"Bar\"]})\n \n \n class EncodingResource(resource.Resource):\n@@ -207,7 +203,7 @@ class EncodingResource(resource.Resource):\n \n \n @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n-class WebClientTestCase(unittest.TestCase):\n+class TestWebClient(unittest.TestCase):\n     def _listen(self, site):\n         return reactor.listenTCP(0, site, interface=\"127.0.0.1\")\n \n@@ -319,7 +315,7 @@ class WebClientTestCase(unittest.TestCase):\n         return getPage(self.getURL(\"notsuchfile\")).addCallback(self._cbNoSuchFile)\n \n     def _cbNoSuchFile(self, pageData):\n-        self.assertIn(b\"404 - No Such Resource\", pageData)\n+        assert b\"404 - No Such Resource\" in pageData\n \n     def testFactoryInfo(self):\n         url = self.getURL(\"file\")\n@@ -329,20 +325,20 @@ class WebClientTestCase(unittest.TestCase):\n         return factory.deferred.addCallback(self._cbFactoryInfo, factory)\n \n     def _cbFactoryInfo(self, ignoredResult, factory):\n-        self.assertEqual(factory.status, b\"200\")\n-        self.assertTrue(factory.version.startswith(b\"HTTP/\"))\n-        self.assertEqual(factory.message, b\"OK\")\n-        self.assertEqual(factory.response_headers[b\"content-length\"], b\"10\")\n+        assert factory.status == b\"200\"\n+        assert factory.version.startswith(b\"HTTP/\")\n+        assert factory.message == b\"OK\"\n+        assert factory.response_headers[b\"content-length\"] == b\"10\"\n \n     def testRedirect(self):\n         return getPage(self.getURL(\"redirect\")).addCallback(self._cbRedirect)\n \n     def _cbRedirect(self, pageData):\n-        self.assertEqual(\n-            pageData,\n-            b'\\n<html>\\n    <head>\\n        <meta http-equiv=\"refresh\" content=\"0;URL=/file\">\\n'\n+        assert (\n+            pageData\n+            == b'\\n<html>\\n    <head>\\n        <meta http-equiv=\"refresh\" content=\"0;URL=/file\">\\n'\n             b'    </head>\\n    <body bgcolor=\"#FFFFFF\" text=\"#000000\">\\n    '\n-            b'<a href=\"/file\">click here</a>\\n    </body>\\n</html>\\n',\n+            b'<a href=\"/file\">click here</a>\\n    </body>\\n</html>\\n'\n         )\n \n     def test_encoding(self):\n@@ -356,14 +352,12 @@ class WebClientTestCase(unittest.TestCase):\n \n     def _check_Encoding(self, response, original_body):\n         content_encoding = to_unicode(response.headers[b\"Content-Encoding\"])\n-        self.assertEqual(content_encoding, EncodingResource.out_encoding)\n-        self.assertEqual(\n-            response.body.decode(content_encoding), to_unicode(original_body)\n-        )\n+        assert content_encoding == EncodingResource.out_encoding\n+        assert response.body.decode(content_encoding) == to_unicode(original_body)\n \n \n @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n-class WebClientSSLTestCase(ContextFactoryBaseTestCase):\n+class WebClientSSLTestCase(TestContextFactoryBase):\n     def testPayload(self):\n         s = \"0123456789\" * 10\n         return getPage(self.getURL(\"payload\"), body=s).addCallback(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
