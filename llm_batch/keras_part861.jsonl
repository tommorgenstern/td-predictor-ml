{"custom_id": "keras#c014c5e6f07f5e22ed4f879df341f9ddf9753a2b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 90 | Lines Deleted: 62 | Files Changed: 3 | Hunks: 18 | Methods Changed: 6 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 152 | Churn Cumulative: 1598 | Contributors (this commit): 13 | Commits (past 90d): 31 | Contributors (cumulative): 26 | DMM Complexity: 0.0\n\nDIFF:\n@@ -982,29 +982,25 @@ def psnr(x1, x2, max_val):\n \n \n def _can_use_flash_attention(query, key, value, bias, raise_error=False):\n-    # Ref: https://github.com/jax-ml/jax/blob/main/jax/_src/cudnn/fused_attention_stablehlo.py\n-    from jax._src.cudnn.fused_attention_stablehlo import _normalize_layout\n-    from jax._src.cudnn.fused_attention_stablehlo import check_cudnn_version\n-    from jax._src.cudnn.fused_attention_stablehlo import check_layout\n-\n+    \"\"\"Verify the availability of flash attention.\"\"\"\n     try:\n-        # The older version of jax doesn't have `check_compute_capability`\n+        from jax._src.cudnn.fused_attention_stablehlo import _normalize_layout\n         from jax._src.cudnn.fused_attention_stablehlo import (\n             check_compute_capability,\n         )\n+        from jax._src.cudnn.fused_attention_stablehlo import check_cudnn_version\n+        from jax._src.cudnn.fused_attention_stablehlo import check_layout\n+        from jax.nn import dot_product_attention as dot_product_attention\n     except ImportError:\n         if raise_error:\n-            raise\n+            raise ImportError(\n+                \"Flash attention is not supported in your current JAX version. \"\n+                \"Please update it by following the official guide: \"\n+                \"https://jax.readthedocs.io/en/latest/installation.html\"\n+            )\n         return False\n \n     try:\n-        # `dot_product_attention` is only available in jax>=0.4.31\n-        if not hasattr(jax.nn, \"dot_product_attention\"):\n-            raise ValueError(\n-                \"Flash attention is not supported in your \"\n-                \"current JAX version. Please update it \"\n-                \"using `pip install -U jax jaxlib`.\"\n-            )\n         # Check if cuDNN is installed and raise RuntimeError if cuDNN is not\n         # detected\n         check_cudnn_version()\n@@ -1119,10 +1115,10 @@ def dot_product_attention(\n         )\n \n     if flash_attention:\n-        raise ValueError(\n-            \"Flash attention is not supported in your \"\n-            \"current JAX version. Please update it \"\n-            \"using `pip install -U jax jaxlib`.\"\n+        raise RuntimeError(\n+            \"Flash attention is not supported in your current JAX version. \"\n+            \"Please update it by following the official guide: \"\n+            \"https://jax.readthedocs.io/en/latest/installation.html\"\n         )\n     # Ref: jax.nn.dot_product_attention\n     # https://github.com/jax-ml/jax/blob/jax-v0.4.33/jax/_src/nn/functions.py#L886\n\n@@ -895,20 +895,23 @@ def _get_large_negative(dtype):\n \n \n def _can_use_flash_attention(\n-    query, key, value, mask=None, is_causal=False, debug=False\n+    query, key, value, mask=None, is_causal=False, raise_error=False\n ):\n+    \"\"\"Verify the availability of flash attention.\"\"\"\n     try:\n-        spda_params = torch.backends.cuda.SDPAParams(\n-            query,\n-            key,\n-            value,\n-            mask,\n-            0.0,  # dropout_p\n-            is_causal,\n+        from torch.backends.cuda import SDPAParams\n+        from torch.backends.cuda import can_use_flash_attention\n+    except ImportError:\n+        if raise_error:\n+            raise ImportError(\n+                \"Flash attention is not supported in your current PyTorch \"\n+                \"version. Please update it by following the official guide: \"\n+                \"https://pytorch.org/get-started/locally/\"\n             )\n-    except TypeError:\n-        # The signature changed in newer version of torch.\n-        spda_params = torch.backends.cuda.SDPAParams(\n+        return False\n+\n+    try:\n+        spda_params = SDPAParams(\n             query,\n             key,\n             value,\n@@ -917,7 +920,22 @@ def _can_use_flash_attention(\n             is_causal,\n             False,  # enable_gqa\n         )\n-    return torch.backends.cuda.can_use_flash_attention(spda_params, debug)\n+    except TypeError:\n+        # The old function signature for the older version of PyTorch\n+        spda_params = SDPAParams(\n+            query,\n+            key,\n+            value,\n+            mask,\n+            0.0,  # dropout_p\n+            is_causal,\n+        )\n+    if raise_error and can_use_flash_attention(spda_params, True) is False:\n+        raise RuntimeError(\n+            \"Flash attention is not supported with the provided inputs. \"\n+            \"Please check the warnings for more details.\"\n+        )\n+    return can_use_flash_attention(spda_params, False)\n \n \n def dot_product_attention(\n@@ -943,7 +961,6 @@ def dot_product_attention(\n             f\"Received: query.shape={query.shape}, key.shape={key.shape}, \"\n             f\"value.shape={value.shape}.\"\n         )\n-    bias = bias if bias is None else convert_to_tensor(bias)\n     mask = mask if mask is None else convert_to_tensor(mask, dtype=\"bool\")\n     if mask is not None:\n         # Explicit set `is_causal` to `False` when `mask` is not `None`.\n@@ -957,23 +974,13 @@ def dot_product_attention(\n \n     if flash_attention is None:\n         flash_attention = _can_use_flash_attention(\n-            query=query, key=key, value=value, mask=mask, is_causal=is_causal\n+            query, key, value, mask, is_causal\n         )\n-    elif (\n-        flash_attention is True\n-        and _can_use_flash_attention(\n-            query=query,\n-            key=key,\n-            value=value,\n-            mask=mask,\n-            is_causal=is_causal,\n-            debug=True,\n-        )\n-        is False\n-    ):\n-        raise ValueError(\n-            \"Flash attention is not supported with the provided inputs. \"\n-            \"Please check the warnings for more details.\"\n+    elif flash_attention is True:\n+        # Use `raise_error=True` to provide more details if the inputs failed to\n+        # use flash attention\n+        _can_use_flash_attention(\n+            query, key, value, mask, is_causal, raise_error=True\n         )\n     if flash_attention:\n         with torch.nn.attention.sdpa_kernel(\n\n@@ -85,10 +85,26 @@ class MultiHeadAttentionTest(testing.TestCase):\n                     run_training_check=False,\n                 )\n                 disable_flash_attention()\n-            except ValueError as e:\n-                self.assertStartsWith(\n-                    e.args[0],\n-                    \"Flash attention is not supported with the provided inputs\",\n+            except ImportError as e:\n+                if \"Flash attention is not supported\" in str(e.args[0]):\n+                    self.assertTrue(\n+                        (\n+                            \"Flash attention is not supported in your current \"\n+                            \"PyTorch version.\"\n+                        )\n+                        in str(e.args[0])\n+                    )\n+            except RuntimeError as e:\n+                if (\n+                    \"Flash attention is not supported with the provided inputs\"\n+                    in str(e.args[0])\n+                ):\n+                    self.assertTrue(\n+                        (\n+                            \"Flash attention is not supported with the \"\n+                            \"provided inputs\"\n+                        )\n+                        in str(e.args[0])\n                     )\n         elif backend.backend() == \"jax\":\n             try:\n@@ -113,20 +129,29 @@ class MultiHeadAttentionTest(testing.TestCase):\n                     run_training_check=False,\n                 )\n                 disable_flash_attention()\n-            except ValueError as e:\n-                self.assertStartsWith(\n-                    e.args[0],\n+            except ImportError as e:\n+                if \"Flash attention is not supported\" in str(e.args[0]):\n+                    self.assertTrue(\n                         (\n-                        \"Flash attention is not supported in your current JAX \"\n-                        \"version.\"\n-                    ),\n+                            \"Flash attention is not supported in your current \"\n+                            \"JAX version.\"\n+                        )\n+                        in str(e.args[0])\n                     )\n             except RuntimeError as e:\n-                if str(e.args[0]).startswith(\"cuDNN\"):\n-                    self.assertStartsWith(e.args[0], \"cuDNN is not detected.\")\n-                elif str(e.args[0]).startswith(\"Require at least\"):\n-                    self.assertStartsWith(\n-                        e.args[0], \"Require at least Ampere arch to run\"\n+                if \"cuDNN\" in str(e.args[0]):\n+                    self.assertTrue(\"cuDNN is not detected.\" in str(e.args[0]))\n+                elif \"Require at least\" in str(e.args[0]):\n+                    self.assertTrue(\n+                        \"Require at least Ampere arch to run\" in str(e.args[0])\n+                    )\n+                elif \"Flash attention\" in str(e.args[0]):\n+                    self.assertTrue(\n+                        (\n+                            \"Flash attention is not supported in your current \"\n+                            \"JAX version.\"\n+                        )\n+                        in str(e.args[0])\n                     )\n \n     @parameterized.named_parameters(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1354fdeebede41583bd26e757c88bf7cebaea328", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 199 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 11/7 | Churn Δ: 200 | Churn Cumulative: 276 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,12 @@ from keras.src.api_export import keras_export\n from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n     BaseImagePreprocessingLayer,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (  # noqa: E501\n+    clip_to_image_size,\n+)\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (  # noqa: E501\n+    convert_format,\n+)\n from keras.src.utils import image_utils\n \n \n@@ -53,13 +59,120 @@ class CenterCrop(BaseImagePreprocessingLayer):\n         self.height = height\n         self.width = width\n \n+    def get_random_transformation(self, data, training=True, seed=None):\n+        if isinstance(data, dict):\n+            images = data[\"images\"]\n+        else:\n+            images = data\n+        shape = self.backend.core.shape(images)\n+        return {\"input_shape\": shape}\n+\n     def transform_labels(self, labels, transformation, training=True):\n         return labels\n \n     def transform_bounding_boxes(\n         self, bounding_boxes, transformation, training=True\n     ):\n-        raise NotImplementedError\n+        def _get_height_width(input_shape):\n+            if self.data_format == \"channels_first\":\n+                input_height = input_shape[-2]\n+                input_width = input_shape[-1]\n+            else:\n+                input_height = input_shape[-3]\n+                input_width = input_shape[-2]\n+            return input_height, input_width\n+\n+        def _get_clipped_bbox(bounding_boxes, h_end, h_start, w_end, w_start):\n+            bboxes = bounding_boxes[\"boxes\"]\n+            x1, y1, x2, y2 = self.backend.numpy.split(bboxes, 4, axis=-1)\n+            x1 = self.backend.numpy.clip(x1, w_start, w_end) - w_start\n+            y1 = self.backend.numpy.clip(y1, h_start, h_end) - h_start\n+            x2 = self.backend.numpy.clip(x2, w_start, w_end) - w_start\n+            y2 = self.backend.numpy.clip(y2, h_start, h_end) - h_start\n+            bounding_boxes[\"boxes\"] = self.backend.numpy.concatenate(\n+                [x1, y1, x2, y2], axis=-1\n+            )\n+            return bounding_boxes\n+\n+        input_shape = transformation[\"input_shape\"]\n+\n+        init_height, init_width = _get_height_width(input_shape)\n+\n+        bounding_boxes = convert_format(\n+            bounding_boxes,\n+            source=self.bounding_box_format,\n+            target=\"xyxy\",\n+            height=init_height,\n+            width=init_width,\n+        )\n+\n+        h_diff = init_height - self.height\n+        w_diff = init_width - self.width\n+\n+        if h_diff >= 0 and w_diff >= 0:\n+            h_start = int(h_diff / 2)\n+            w_start = int(w_diff / 2)\n+\n+            h_end = h_start + self.height\n+            w_end = w_start + self.width\n+\n+            bounding_boxes = _get_clipped_bbox(\n+                bounding_boxes, h_end, h_start, w_end, w_start\n+            )\n+        else:\n+            width = init_width\n+            height = init_height\n+            target_height = self.height\n+            target_width = self.width\n+\n+            crop_height = int(float(width * target_height) / target_width)\n+            crop_height = max(min(height, crop_height), 1)\n+            crop_width = int(float(height * target_width) / target_height)\n+            crop_width = max(min(width, crop_width), 1)\n+            crop_box_hstart = int(float(height - crop_height) / 2)\n+            crop_box_wstart = int(float(width - crop_width) / 2)\n+\n+            h_start = crop_box_hstart\n+            w_start = crop_box_wstart\n+\n+            h_end = crop_box_hstart + crop_height\n+            w_end = crop_box_wstart + crop_width\n+            bounding_boxes = _get_clipped_bbox(\n+                bounding_boxes, h_end, h_start, w_end, w_start\n+            )\n+\n+            bounding_boxes = convert_format(\n+                bounding_boxes,\n+                source=\"xyxy\",\n+                target=\"rel_xyxy\",\n+                height=crop_height,\n+                width=crop_width,\n+            )\n+\n+            bounding_boxes = convert_format(\n+                bounding_boxes,\n+                source=\"rel_xyxy\",\n+                target=\"xyxy\",\n+                height=self.height,\n+                width=self.width,\n+            )\n+\n+        bounding_boxes = clip_to_image_size(\n+            bounding_boxes=bounding_boxes,\n+            height=self.height,\n+            width=self.width,\n+            format=\"xyxy\",\n+        )\n+\n+        bounding_boxes = convert_format(\n+            bounding_boxes,\n+            source=\"xyxy\",\n+            target=self.bounding_box_format,\n+            height=self.height,\n+            width=self.width,\n+        )\n+\n+        return bounding_boxes\n \n     def transform_segmentation_masks(\n         self, segmentation_masks, transformation, training=True\n\n@@ -209,3 +209,88 @@ class CenterCropTest(testing.TestCase):\n             size[0], size[1], data_format=data_format, crop_to_aspect_ratio=True\n         )(img)\n         self.assertAllClose(ref_out, out)\n+\n+    @parameterized.named_parameters(\n+        (\n+            \"normal\",\n+            5,\n+            5,\n+            [[1.0, 0.0, 3.0, 1.0], [5.0, 2.0, 5.0, 4.0]],\n+        ),\n+        (\n+            \"with_stretch\",\n+            20,\n+            20,\n+            [[5.0, 0.0, 10.0, 5.0], [15.0, 7.5, 20.0, 12.5]],\n+        ),\n+    )\n+    def test_center_crop_bounding_boxes(self, height, width, expected_boxes):\n+        if backend.config.image_data_format() == \"channels_last\":\n+            image_shape = (10, 8, 3)\n+        else:\n+            image_shape = (3, 10, 8)\n+        input_image = np.random.random(image_shape)\n+        bounding_boxes = {\n+            \"boxes\": np.array(\n+                [\n+                    [2, 1, 4, 3],\n+                    [6, 4, 8, 6],\n+                ]\n+            ),\n+            \"labels\": np.array([[1, 2]]),\n+        }\n+        input_data = {\"images\": input_image, \"bounding_boxes\": bounding_boxes}\n+        center_crop_layer = layers.CenterCrop(\n+            height=height,\n+            width=width,\n+            bounding_box_format=\"xyxy\",\n+        )\n+        output = center_crop_layer(input_data)\n+        self.assertAllClose(output[\"bounding_boxes\"][\"boxes\"], expected_boxes)\n+\n+    @parameterized.named_parameters(\n+        (\n+            \"normal\",\n+            5,\n+            5,\n+            [[1.0, 0.0, 3.0, 1.0], [5.0, 2.0, 5.0, 4.0]],\n+        ),\n+        (\n+            \"with_stretch\",\n+            20,\n+            20,\n+            [[5.0, 0.0, 10.0, 5.0], [15.0, 7.5, 20.0, 12.5]],\n+        ),\n+    )\n+    def test_center_crop_tf_data_bounding_boxes(\n+        self, height, width, expected_boxes\n+    ):\n+        if backend.config.image_data_format() == \"channels_last\":\n+            image_shape = (1, 10, 8, 3)\n+        else:\n+            image_shape = (1, 3, 10, 8)\n+        input_image = np.random.random(image_shape)\n+        bounding_boxes = {\n+            \"boxes\": np.array(\n+                [\n+                    [\n+                        [2, 1, 4, 3],\n+                        [6, 4, 8, 6],\n+                    ]\n+                ]\n+            ),\n+            \"labels\": np.array([[1, 2]]),\n+        }\n+\n+        input_data = {\"images\": input_image, \"bounding_boxes\": bounding_boxes}\n+\n+        ds = tf_data.Dataset.from_tensor_slices(input_data)\n+        center_crop_layer = layers.CenterCrop(\n+            height=height,\n+            width=width,\n+            bounding_box_format=\"xyxy\",\n+        )\n+        ds = ds.map(center_crop_layer)\n+        output = next(iter(ds))\n+        expected_boxes = np.array(expected_boxes)\n+        self.assertAllClose(output[\"bounding_boxes\"][\"boxes\"], expected_boxes)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d5d3e6050b713160c6eac236e016042e411aca15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 300 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -29,6 +29,8 @@ if torch.backends.mps.is_available():\n     DEFAULT_DEVICE = \"mps\"\n elif torch.cuda.is_available():\n     DEFAULT_DEVICE = \"cuda\"\n+elif hasattr(torch, \"xpu\") and torch.xpu.is_available():\n+    DEFAULT_DEVICE = \"xpu\"\n else:\n     DEFAULT_DEVICE = \"cpu\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5a4a7cc9194d55d01d252aa8044d258f4b992abf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 45 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -118,7 +118,6 @@ class Mean(Metric):\n     >>> m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])\n     >>> m.result()\n     2.0\n-    ```\n     \"\"\"\n \n     def __init__(self, name=\"mean\", dtype=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#13ab92bf5a926641c6e4e9cf5148f70802bc992c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 88 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 9 | Methods Changed: 9 | Complexity Δ (Sum/Max): 23/6 | Churn Δ: 88 | Churn Cumulative: 3497 | Contributors (this commit): 16 | Commits (past 90d): 39 | Contributors (cumulative): 58 | DMM Complexity: 1.0\n\nDIFF:\n@@ -573,6 +573,15 @@ def exp(x):\n     return jnp.exp(x)\n \n \n+@sparse.densifying_unary\n+def exp2(x):\n+    x = convert_to_tensor(x)\n+    ori_dtype = standardize_dtype(x.dtype)\n+    if \"int\" in ori_dtype or ori_dtype == \"bool\":\n+        x = cast(x, config.floatx())\n+    return jnp.exp2(x)\n+\n+\n def expand_dims(x, axis):\n     x = convert_to_tensor(x)\n     if isinstance(x, jax_sparse.BCOO):\n\n@@ -489,6 +489,14 @@ def exp(x):\n     return np.exp(x)\n \n \n+def exp2(x):\n+    x = convert_to_tensor(x)\n+    ori_dtype = standardize_dtype(x.dtype)\n+    if \"int\" in ori_dtype or ori_dtype == \"bool\":\n+        x = x.astype(config.floatx())\n+    return np.exp2(x)\n+\n+\n def expand_dims(x, axis):\n     axis = standardize_axis_for_numpy(axis)\n     return np.expand_dims(x, axis)\n\n@@ -1239,6 +1239,15 @@ def exp(x):\n     return tf.exp(x)\n \n \n+@sparse.densifying_unary(1)\n+def exp2(x):\n+    x = convert_to_tensor(x)\n+    ori_dtype = standardize_dtype(x.dtype)\n+    if \"int\" in ori_dtype or ori_dtype == \"bool\":\n+        x = tf.cast(x, config.floatx())\n+    return tf.math.pow(2.0, x)\n+\n+\n def expand_dims(x, axis):\n     x = convert_to_tensor(x)\n     axis = to_tuple_or_list(axis)\n\n@@ -657,6 +657,14 @@ def exp(x):\n     return torch.exp(x)\n \n \n+def exp2(x):\n+    x = convert_to_tensor(x)\n+    ori_dtype = standardize_dtype(x.dtype)\n+    if \"int\" in ori_dtype or ori_dtype == \"bool\":\n+        x = cast(x, config.floatx())\n+    return torch.exp2(x)\n+\n+\n def expand_dims(x, axis):\n     x = convert_to_tensor(x)\n     axis = to_tuple_or_list(axis)\n\n@@ -2669,6 +2669,32 @@ def exp(x):\n     return backend.numpy.exp(x)\n \n \n+class Exp2(Operation):\n+    def call(self, x):\n+        return backend.numpy.exp2(x)\n+\n+    def compute_output_spec(self, x):\n+        dtype = backend.standardize_dtype(x.dtype)\n+        if \"int\" in dtype or dtype == \"bool\":\n+            dtype = backend.floatx()\n+        return KerasTensor(x.shape, dtype=dtype)\n+\n+\n+@keras_export([\"keras.ops.exp2\", \"keras.ops.numpy.exp2\"])\n+def exp2(x):\n+    \"\"\"Calculate the base-2 exponential of all elements in the input tensor.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise base-2 exponential of `x`.\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Exp2().symbolic_call(x)\n+    return backend.numpy.exp2(x)\n+\n+\n class ExpandDims(Operation):\n     def __init__(self, axis):\n         super().__init__()\n\n@@ -1253,6 +1253,10 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor((None, 3))\n         self.assertEqual(knp.exp(x).shape, (None, 3))\n \n+    def test_exp2(self):\n+        x = KerasTensor((None, 3))\n+        self.assertEqual(knp.exp2(x).shape, (None, 3))\n+\n     def test_expand_dims(self):\n         x = KerasTensor((None, 3))\n         self.assertEqual(knp.expand_dims(x, -1).shape, (None, 3, 1))\n@@ -1802,6 +1806,10 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor((2, 3))\n         self.assertEqual(knp.exp(x).shape, (2, 3))\n \n+    def test_exp2(self):\n+        x = KerasTensor((2, 3))\n+        self.assertEqual(knp.exp2(x).shape, (2, 3))\n+\n     def test_expand_dims(self):\n         x = KerasTensor((2, 3, 4))\n         self.assertEqual(knp.expand_dims(x, 0).shape, (1, 2, 3, 4))\n@@ -3674,6 +3682,11 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.exp(x), np.exp(x))\n         self.assertAllClose(knp.Exp()(x), np.exp(x))\n \n+    def test_exp2(self):\n+        x = np.array([[1, 2, 3], [3, 2, 1]])\n+        self.assertAllClose(knp.exp2(x), np.exp2(x))\n+        self.assertAllClose(knp.Exp2()(x), np.exp2(x))\n+\n     def test_expand_dims(self):\n         x = np.ones([2, 3, 4])\n         self.assertAllClose(knp.expand_dims(x, 0), np.expand_dims(x, 0))\n@@ -6551,6 +6564,21 @@ class NumpyDtypeTest(testing.TestCase):\n             standardize_dtype(knp.Exp().symbolic_call(x).dtype), expected_dtype\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_exp2(self, dtype):\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((1,), dtype=dtype)\n+        x_jax = jnp.ones((1,), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnp.exp2(x_jax).dtype)\n+        if dtype == \"int64\":\n+            expected_dtype = backend.floatx()\n+\n+        self.assertEqual(standardize_dtype(knp.exp2(x).dtype), expected_dtype)\n+        self.assertEqual(\n+            standardize_dtype(knp.Exp2().symbolic_call(x).dtype), expected_dtype\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_expand_dims(self, dtype):\n         import jax.numpy as jnp\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#70b704491f68f1b2e67864d5b7019da8c5998f0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 846 | Contributors (this commit): 12 | Commits (past 90d): 34 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -155,6 +155,7 @@ from keras.src.ops.numpy import einsum\n from keras.src.ops.numpy import empty\n from keras.src.ops.numpy import equal\n from keras.src.ops.numpy import exp\n+from keras.src.ops.numpy import exp2\n from keras.src.ops.numpy import expand_dims\n from keras.src.ops.numpy import expm1\n from keras.src.ops.numpy import eye\n\n@@ -59,6 +59,7 @@ from keras.src.ops.numpy import einsum\n from keras.src.ops.numpy import empty\n from keras.src.ops.numpy import equal\n from keras.src.ops.numpy import exp\n+from keras.src.ops.numpy import exp2\n from keras.src.ops.numpy import expand_dims\n from keras.src.ops.numpy import expm1\n from keras.src.ops.numpy import eye\n\n@@ -155,6 +155,7 @@ from keras.src.ops.numpy import einsum\n from keras.src.ops.numpy import empty\n from keras.src.ops.numpy import equal\n from keras.src.ops.numpy import exp\n+from keras.src.ops.numpy import exp2\n from keras.src.ops.numpy import expand_dims\n from keras.src.ops.numpy import expm1\n from keras.src.ops.numpy import eye\n\n@@ -59,6 +59,7 @@ from keras.src.ops.numpy import einsum\n from keras.src.ops.numpy import empty\n from keras.src.ops.numpy import equal\n from keras.src.ops.numpy import exp\n+from keras.src.ops.numpy import exp2\n from keras.src.ops.numpy import expand_dims\n from keras.src.ops.numpy import expm1\n from keras.src.ops.numpy import eye\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8a79442e4273b7e6122662227e8d1c147deb7c0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 28 | Churn Cumulative: 528 | Contributors (this commit): 9 | Commits (past 90d): 15 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -669,7 +669,10 @@ class MultiHeadAttention(Layer):\n             )\n \n         if self._output_shape:\n+            if isinstance(self._output_shape, tuple):\n                 return query_shape[:-1] + self._output_shape\n+            else:\n+                return query_shape[:-1] + (self._output_shape,)\n \n         return query_shape\n \n\n@@ -16,6 +16,7 @@ from keras.src import saving\n from keras.src import testing\n from keras.src.layers.attention.attention import disable_flash_attention\n from keras.src.layers.attention.attention import enable_flash_attention\n+from keras.src.layers.attention.multi_head_attention import MultiHeadAttention\n \n \n class MultiHeadAttentionTest(testing.TestCase):\n@@ -593,3 +594,27 @@ class MultiHeadAttentionTest(testing.TestCase):\n         )\n \n         self.assertAllClose(output_with_flash, output_without_flash)\n+\n+\n+\n+\n+def test_multi_head_attention_output_shape_as_int():\n+    \"\"\"Test MultiHeadAttention with output_shape as an int.\"\"\"\n+    mha = MultiHeadAttention(num_heads=2, key_dim=16, output_shape=8)\n+    query = random.uniform((2, 4, 16))\n+    value = random.uniform((2, 4, 16))\n+    output = mha(query=query, value=value)\n+\n+    assert output.shape == (2, 4, 8), (f\"Expected shape (2, 4, 8),\"\n+                                       f\" got {output.shape}\")\n+\n+\n+def test_multi_head_attention_output_shape_as_tuple():\n+    \"\"\"Test MultiHeadAttention with output_shape as a tuple.\"\"\"\n+    mha = MultiHeadAttention(num_heads=2, key_dim=16, output_shape=(8, 8))\n+    query = random.uniform((2, 4, 16))\n+    value = random.uniform((2, 4, 16))\n+    output = mha(query=query, value=value)\n+\n+    assert output.shape == (2, 4, 8, 8), (f\"Expected shape (2, 4, 8, 8),\"\n+                                          f\" got {output.shape}\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6479475f057236a300dd7102a27b3b5f361343d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 31 | Churn Cumulative: 559 | Contributors (this commit): 9 | Commits (past 90d): 17 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,3 @@\n-import collections\n import math\n import string\n \n@@ -336,10 +335,14 @@ class MultiHeadAttention(Layer):\n         \"\"\"\n         query_rank = len(query_shape)\n         if self._output_shape:\n-            if not isinstance(self._output_shape, collections.abc.Sized):\n+            if isinstance(self._output_shape, (tuple, list)):\n+                output_shape = self._output_shape\n+            elif isinstance(self._output_shape, int):\n                 output_shape = [self._output_shape]\n             else:\n-                output_shape = self._output_shape\n+                raise ValueError(\n+                    f\"Invalid output_shape type: {self._output_shape}\"\n+                )\n         else:\n             output_shape = [query_shape[-1]]\n         einsum_equation, bias_axes, output_rank = _build_proj_equation(\n@@ -669,10 +672,14 @@ class MultiHeadAttention(Layer):\n             )\n \n         if self._output_shape:\n-            if isinstance(self._output_shape, tuple):\n-                return query_shape[:-1] + self._output_shape\n-            else:\n+            if isinstance(self._output_shape, (tuple, list)):\n+                return query_shape[:-1] + tuple(self._output_shape)\n+            elif isinstance(self._output_shape, int):\n                 return query_shape[:-1] + (self._output_shape,)\n+            else:\n+                raise ValueError(\n+                    f\"Invalid output_shape type: {self._output_shape}\"\n+                )\n \n         return query_shape\n \n\n@@ -596,8 +596,6 @@ class MultiHeadAttentionTest(testing.TestCase):\n         self.assertAllClose(output_with_flash, output_without_flash)\n \n \n-\n-\n def test_multi_head_attention_output_shape_as_int():\n     \"\"\"Test MultiHeadAttention with output_shape as an int.\"\"\"\n     mha = MultiHeadAttention(num_heads=2, key_dim=16, output_shape=8)\n@@ -605,8 +603,9 @@ def test_multi_head_attention_output_shape_as_int():\n     value = random.uniform((2, 4, 16))\n     output = mha(query=query, value=value)\n \n-    assert output.shape == (2, 4, 8), (f\"Expected shape (2, 4, 8),\"\n-                                       f\" got {output.shape}\")\n+    assert output.shape == (2, 4, 8), (\n+        f\"Expected shape (2, 4, 8),\" f\" got {output.shape}\"\n+    )\n \n \n def test_multi_head_attention_output_shape_as_tuple():\n@@ -616,5 +615,6 @@ def test_multi_head_attention_output_shape_as_tuple():\n     value = random.uniform((2, 4, 16))\n     output = mha(query=query, value=value)\n \n-    assert output.shape == (2, 4, 8, 8), (f\"Expected shape (2, 4, 8, 8),\"\n-                                          f\" got {output.shape}\")\n+    assert output.shape == (2, 4, 8, 8), (\n+        f\"Expected shape (2, 4, 8, 8),\" f\" got {output.shape}\"\n+    )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#575ed94f91b42c324f85d3c1ed88add23b950e08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 131 | Lines Deleted: 95 | Files Changed: 6 | Hunks: 44 | Methods Changed: 24 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 226 | Churn Cumulative: 2855 | Contributors (this commit): 17 | Commits (past 90d): 60 | Contributors (cumulative): 49 | DMM Complexity: 0.7878787878787878\n\nDIFF:\n@@ -989,6 +989,9 @@ def _can_use_flash_attention(query, key, value, bias, raise_error=False):\n             check_compute_capability,\n         )\n         from jax._src.cudnn.fused_attention_stablehlo import check_cudnn_version\n+        from jax._src.cudnn.fused_attention_stablehlo import (\n+            check_is_flash_attention,\n+        )\n         from jax._src.cudnn.fused_attention_stablehlo import check_layout\n         from jax.nn import dot_product_attention as dot_product_attention\n     except ImportError:\n@@ -1003,7 +1006,7 @@ def _can_use_flash_attention(query, key, value, bias, raise_error=False):\n     try:\n         # Check if cuDNN is installed and raise RuntimeError if cuDNN is not\n         # detected\n-        check_cudnn_version()\n+        cudnn_version = check_cudnn_version()\n         # Only support at least Ampere\n         if not check_compute_capability(\"8.0\"):\n             raise RuntimeError(\"Require at least Ampere arch to run\")\n@@ -1017,6 +1020,14 @@ def _can_use_flash_attention(query, key, value, bias, raise_error=False):\n             kv_seqlen=None,\n             layout=_normalize_layout(\"BTNH\"),\n         )\n+        check_is_flash_attention(\n+            query,\n+            key,\n+            _normalize_layout(\"BTNH\"),\n+            cudnn_version,\n+            bias is not None,\n+            is_training=False,\n+        )\n         return True\n     except:\n         if raise_error:\n\n@@ -984,7 +984,7 @@ def _dot_product_attention_xla(query, key, value, bias, mask, is_causal, scale):\n         tf.cast(key, dtype=logits_dtype),\n         optimize=\"optimal\",\n     )\n-    logits = tf.multiply(logits, tf.cast(logits, logits.dtype))\n+    logits = tf.multiply(logits, tf.cast(scale, logits.dtype))\n \n     if bias is not None:\n         logits = tf.add(logits, tf.cast(bias, logits.dtype))\n\n@@ -293,10 +293,15 @@ def enable_flash_attention():\n     making it especially useful for large language models (LLMs) that\n     benefit from faster and more memory-efficient attention computations.\n \n-    Once enabled, supported layers like `MultiHeadAttention` will\n-    use flash attention for faster computations.\n+    Once enabled, supported layers like `MultiHeadAttention` will **attempt** to\n+    use flash attention for faster computations. By default, this feature is\n+    enabled.\n+\n+    Note that enabling flash attention does not guarantee it will always be\n+    used. Typically, the inputs must be in `float16` or `bfloat16` dtype, and\n+    input layout requirements may vary depending on the backend.\n     \"\"\"\n-    global_state.set_global_attribute(\"flash_attention\", True)\n+    global_state.set_global_attribute(\"flash_attention\", None)\n \n \n @keras_export(\"keras.config.disable_flash_attention\")\n@@ -323,9 +328,11 @@ def is_flash_attention_enabled():\n     configuration to determine if flash attention is enabled for compatible\n     layers (e.g., `MultiHeadAttention`).\n \n+    Note that enabling flash attention does not guarantee it will always be\n+    used. Typically, the inputs must be in `float16` or `bfloat16` dtype, and\n+    input layout requirements may vary depending on the backend.\n+\n     Returns:\n-        bool or None: Returns `True` if flash attention is enabled,\n-                      `False` if it is disabled, and `None` if the global\n-                      setting has not been defined.\n+        `False` if disabled; otherwise, it indicates that it is enabled.\n     \"\"\"\n     return global_state.get_global_attribute(\"flash_attention\", default=None)\n\n@@ -52,9 +52,11 @@ class MultiHeadAttention(Layer):\n             feature dim (the query input's last dimension).\n         attention_axes: axes over which the attention is applied. `None` means\n             attention over all axes, but batch, heads, and features.\n-        flash_attention: If unspecified, defaults to the global flash attention\n-            configuration setting (which can be set via\n-            `keras.config.enable_flash_attention().\n+        flash_attention: If `None`, the layer attempts to use flash\n+            attention for faster and more memory-efficient attention\n+            computations when possible. This behavior can be configured using\n+            `keras.config.enable_flash_attention()` or\n+            `keras.config.disable_flash_attention()`.\n         kernel_initializer: Initializer for dense layer kernels.\n         bias_initializer: Initializer for dense layer biases.\n         kernel_regularizer: Regularizer for dense layer kernels.\n@@ -122,12 +124,11 @@ class MultiHeadAttention(Layer):\n         self.supports_masking = True\n         self._num_heads = num_heads\n         self._key_dim = key_dim\n-        # Cache 1.0 / math.sqrt(self._key_dim).\n-        self._inverse_sqrt_key_dim = None\n         self._value_dim = value_dim if value_dim else key_dim\n         self._dropout = dropout\n         self._use_bias = use_bias\n         self._output_shape = output_shape\n+        self._flash_attention = flash_attention or is_flash_attention_enabled()\n         self._kernel_initializer = initializers.get(kernel_initializer)\n         self._bias_initializer = initializers.get(bias_initializer)\n         self._kernel_regularizer = regularizers.get(kernel_regularizer)\n@@ -135,9 +136,6 @@ class MultiHeadAttention(Layer):\n         self._activity_regularizer = regularizers.get(activity_regularizer)\n         self._kernel_constraint = constraints.get(kernel_constraint)\n         self._bias_constraint = constraints.get(bias_constraint)\n-        self._flash_attention = flash_attention or is_flash_attention_enabled()\n-        self._return_attention_scores = False\n-\n         if isinstance(attention_axes, int):\n             attention_axes = (attention_axes,)\n         elif attention_axes and not isinstance(attention_axes, (list, tuple)):\n@@ -148,6 +146,16 @@ class MultiHeadAttention(Layer):\n         self._attention_axes = attention_axes\n         self.seed = seed\n \n+        self._inverse_sqrt_key_dim = 1.0 / math.sqrt(float(self._key_dim))\n+        self._return_attention_scores = False\n+\n+        # Check for flash attention constraints\n+        if self._flash_attention and self._dropout > 0.0:\n+            raise ValueError(\n+                \"Dropout is not supported when flash attention is enabled. \"\n+                \"Please set dropout to 0.0 to use flash attention.\"\n+            )\n+\n     @property\n     def num_heads(self):\n         return self._num_heads\n@@ -384,7 +392,6 @@ class MultiHeadAttention(Layer):\n         self._dropout_layer = Dropout(\n             rate=self._dropout, dtype=self.dtype_policy, seed=self.seed\n         )\n-        self._inverse_sqrt_key_dim = 1.0 / math.sqrt(float(self._key_dim))\n \n     def _masked_softmax(self, attention_scores, attention_mask=None):\n         # Normalize the attention scores to probabilities.\n@@ -431,7 +438,6 @@ class MultiHeadAttention(Layer):\n           attention_output: Multi-headed outputs of attention computation.\n           attention_scores: Multi-headed attention weights.\n         \"\"\"\n-\n         # Check for flash attention constraints\n         if self._flash_attention and self._return_attention_scores:\n             raise ValueError(\n@@ -439,12 +445,6 @@ class MultiHeadAttention(Layer):\n                 \"attention is enabled. Please disable flash attention to access\"\n                 \" attention scores.\"\n             )\n-        if self._flash_attention and self._dropout > 0.0:\n-            raise ValueError(\n-                \"Dropout is not supported when flash \"\n-                \"attention is enabled. Please set dropout to 0.0 to use \"\n-                \"flash attention.\"\n-            )\n \n         # Determine whether to use dot-product attention\n         use_dot_product_attention = not (\n\n@@ -16,11 +16,22 @@ from keras.src import saving\n from keras.src import testing\n from keras.src.layers.attention.attention import disable_flash_attention\n from keras.src.layers.attention.attention import enable_flash_attention\n-from keras.src.layers.attention.multi_head_attention import MultiHeadAttention\n+from keras.src.layers.attention.attention import is_flash_attention_enabled\n \n \n class MultiHeadAttentionTest(testing.TestCase):\n+    def setUp(self):\n+        super().setUp()\n+        # Flash attention is a newly introduced feature. We need to disable it\n+        # for testing purposes.\n+        disable_flash_attention()\n+\n+    def tearDown(self):\n+        enable_flash_attention()\n+        return super().tearDown()\n+\n     def test_basics(self):\n+        self.assertFalse(is_flash_attention_enabled())\n         self.run_layer_test(\n             layers.MultiHeadAttention,\n             init_kwargs={\n@@ -57,20 +68,19 @@ class MultiHeadAttentionTest(testing.TestCase):\n         )\n \n     def test_basics_with_flash_attention(self):\n+        enable_flash_attention()\n         if backend.backend() in (\"tensorflow\", \"numpy\"):\n             self.skipTest(\n                 \"Flash attention is not supported in tensorflow and numpy \"\n                 \"backends.\"\n             )\n-\n         elif backend.backend() == \"torch\":\n             try:\n-                enable_flash_attention()\n                 self.run_layer_test(\n                     layers.MultiHeadAttention,\n                     init_kwargs={\n                         \"num_heads\": 2,\n-                        \"key_dim\": 2,\n+                        \"key_dim\": 8,\n                         \"dtype\": \"float16\",\n                     },\n                     input_shape={\n@@ -85,7 +95,6 @@ class MultiHeadAttentionTest(testing.TestCase):\n                     supports_masking=True,\n                     run_training_check=False,\n                 )\n-                disable_flash_attention()\n             except ImportError as e:\n                 if \"Flash attention is not supported\" in str(e.args[0]):\n                     self.assertTrue(\n@@ -109,12 +118,11 @@ class MultiHeadAttentionTest(testing.TestCase):\n                     )\n         elif backend.backend() == \"jax\":\n             try:\n-                enable_flash_attention()\n                 self.run_layer_test(\n                     layers.MultiHeadAttention,\n                     init_kwargs={\n                         \"num_heads\": 2,\n-                        \"key_dim\": 8,  # key_dim % 8 == 0\n+                        \"key_dim\": 8,\n                         \"dtype\": \"float16\",\n                     },\n                     input_shape={\n@@ -129,7 +137,6 @@ class MultiHeadAttentionTest(testing.TestCase):\n                     supports_masking=True,\n                     run_training_check=False,\n                 )\n-                disable_flash_attention()\n             except ImportError as e:\n                 if \"Flash attention is not supported\" in str(e.args[0]):\n                     self.assertTrue(\n@@ -360,17 +367,28 @@ class MultiHeadAttentionTest(testing.TestCase):\n         self.assertAllClose(output_1, output_2)\n         self.assertAllClose(output_1, output_3)\n \n-    def test_correctness(self):\n-        query = np.array([[[1.0, 0.0], [0.0, 1.0]]])\n-        key = np.array([[[0.0, 1.0], [1.0, 0.0]]])\n-        value = np.array([[[1.0, 2.0], [3.0, 4.0]]])\n+    @parameterized.named_parameters(\n+        (\"disable_flash_attention\", False), (\"enable_flash_attention\", True)\n+    )\n+    def test_correctness(self, flash_attention):\n+        if flash_attention:\n+            # Let the backend decide whether to use flase attention\n+            enable_flash_attention()\n+        dtype = \"float16\"  # Flash attention only accepts float16/bfloat16\n+\n+        num_heads = 8\n+        key_dim = 8  # key_dim % 8 == 0 to enable flash attention\n+\n+        query = np.identity(key_dim)[np.newaxis, ...]\n+        key = np.identity(key_dim)[np.newaxis, ...]\n+        value = (\n+            np.reshape(np.arange(key_dim * key_dim), (1, key_dim, key_dim))\n+            / 100.0  # Prevent overflow/underflow\n+        )\n \n         # Setup layer.\n-        num_heads = 2\n-        key_dim = 2\n         layer = layers.MultiHeadAttention(\n-            num_heads=num_heads,\n-            key_dim=key_dim,\n+            num_heads=num_heads, key_dim=key_dim, dtype=dtype\n         )\n         layer.build(query.shape, key.shape, value.shape)\n \n@@ -379,22 +397,43 @@ class MultiHeadAttentionTest(testing.TestCase):\n         # To get an identity kernel we need to add a head dim and repeat on it.\n         kernel = np.repeat(kernel[:, np.newaxis, :], num_heads, axis=1)\n         # Zeros for all biases.\n-        bias = np.zeros((2, 2))\n-        output_bias = np.zeros((2,))\n+        bias = np.zeros((num_heads, key_dim))\n+        output_bias = np.zeros((key_dim,))\n         layer.set_weights([kernel, bias] * 3 + [kernel, output_bias])\n         # Call layer and assert output.\n+        expected_output = np.array(\n+            [2.406, 2.440, 2.473, 2.504, 2.535, 2.568, 2.602, 2.633]\n+        )\n+        expected_output = np.tile(\n+            expected_output[np.newaxis, :, np.newaxis], (1, 1, key_dim)\n+        )\n+        expected_score = np.array(\n+            [\n+                [0.1187] * 0 + [0.1691] + [0.1187] * 7,\n+                [0.1187] * 1 + [0.1691] + [0.1187] * 6,\n+                [0.1187] * 2 + [0.1691] + [0.1187] * 5,\n+                [0.1187] * 3 + [0.1691] + [0.1187] * 4,\n+                [0.1187] * 4 + [0.1691] + [0.1187] * 3,\n+                [0.1187] * 5 + [0.1691] + [0.1187] * 2,\n+                [0.1187] * 6 + [0.1691] + [0.1187] * 1,\n+                [0.1187] * 7 + [0.1691] + [0.1187] * 0,\n+            ]\n+        )\n+        expected_score = np.tile(\n+            expected_score[np.newaxis, np.newaxis, ...], (1, key_dim, 1, 1)\n+        )\n+        if flash_attention:\n+            output = layer(query=query, value=value, key=key)\n+            self.assertAllClose(output, expected_output, atol=1e-2)\n+        else:\n             output, scores = layer(\n                 query=query,\n                 value=value,\n                 key=key,\n                 return_attention_scores=True,\n             )\n-        self.assertAllClose(output, [[[5.679, 5.679], [4.32, 4.32]]], atol=1e-3)\n-        self.assertAllClose(\n-            scores,\n-            [[[[0.33, 0.67], [0.67, 0.33]], [[0.33, 0.67], [0.67, 0.33]]]],\n-            atol=1e-3,\n-        )\n+            self.assertAllClose(output, expected_output, atol=1e-2)\n+            self.assertAllClose(scores, expected_score, atol=1e-2)\n \n     def test_mha_constraints(self):\n         query = np.array([[[1.0, 0.0], [0.0, 1.0]]])\n@@ -543,21 +582,29 @@ class MultiHeadAttentionTest(testing.TestCase):\n         self.assertDType(layer._key_dense._kernel, \"int8\")\n         self.assertDType(layer._value_dense._kernel, \"int8\")\n \n-    def test_flash_attention_with_attention_scores_error(self):\n-        # Enable flash attention globally\n-        if backend.backend() == \"numpy\" or \"tensorflow\":\n+    def test_flash_attention_with_errors(self):\n+        if backend.backend() in (\"numpy\", \"tensorflow\"):\n             pytest.skip(\n-                reason=\"Flash attention is not supported on Tensorflow\"\n-                \"and numpy.\"\n+                reason=(\n+                    \"Flash attention is not supported on tensorflow and numpy.\"\n+                )\n+            )\n+        # Check `flash_attention=True` and `dropout=0.1`\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Dropout is not supported when flash attention is enabled.\",\n+        ):\n+            layer = layers.MultiHeadAttention(\n+                num_heads=2, key_dim=2, flash_attention=True, dropout=0.1\n             )\n-        # Setup layer with required parameters\n-        layer = layers.MultiHeadAttention(num_heads=2, key_dim=2)\n \n-        # Define sample input\n+        # Check `flash_attention=True` and `return_attention_scores=True`\n+        layer = layers.MultiHeadAttention(\n+            num_heads=2, key_dim=2, flash_attention=True\n+        )\n+        self.assertTrue(layer._flash_attention)\n         query = np.random.random((2, 4, 8))\n         value = np.random.random((2, 4, 8))\n-\n-        # Check if ValueError is raised when return_attention_scores=True\n         with self.assertRaisesRegex(\n             ValueError,\n             \"Returning attention scores is not supported when flash \"\n@@ -566,39 +613,9 @@ class MultiHeadAttentionTest(testing.TestCase):\n         ):\n             layer(query=query, value=value, return_attention_scores=True)\n \n-    def test_flash_attention_numerical_correctness(self):\n-        if backend.backend() == \"numpy\" or backend.backend() == \"tensorflow\":\n-            pytest.skip(\n-                reason=\"Flash attention is not supported on Tensorflow \"\n-                \"and numpy.\"\n-            )\n-        # Create sample input data\n-        # Define sample input\n-        query = np.random.random((2, 4, 8))\n-        value = np.random.random((2, 4, 8))\n-\n-        # Initialize MultiHeadAttention layer\n-        mha_layer = layers.MultiHeadAttention(\n-            num_heads=2,\n-            key_dim=2,\n-        )\n-\n-        # Run with flash attention enabled\n-        enable_flash_attention()\n-        output_with_flash = mha_layer(query=query, value=value, training=False)\n-\n-        disable_flash_attention()\n-        # Run with flash attention disabled\n-        output_without_flash = mha_layer(\n-            query=query, value=value, training=False\n-        )\n-\n-        self.assertAllClose(output_with_flash, output_without_flash)\n-\n-\n-def test_multi_head_attention_output_shape_as_int():\n+    def test_multi_head_attention_output_shape_as_int(self):\n         \"\"\"Test MultiHeadAttention with output_shape as an int.\"\"\"\n-    mha = MultiHeadAttention(num_heads=2, key_dim=16, output_shape=8)\n+        mha = layers.MultiHeadAttention(num_heads=2, key_dim=16, output_shape=8)\n         query = random.uniform((2, 4, 16))\n         value = random.uniform((2, 4, 16))\n         output = mha(query=query, value=value)\n@@ -607,10 +624,11 @@ def test_multi_head_attention_output_shape_as_int():\n             f\"Expected shape (2, 4, 8),\" f\" got {output.shape}\"\n         )\n \n-\n-def test_multi_head_attention_output_shape_as_tuple():\n+    def test_multi_head_attention_output_shape_as_tuple(self):\n         \"\"\"Test MultiHeadAttention with output_shape as a tuple.\"\"\"\n-    mha = MultiHeadAttention(num_heads=2, key_dim=16, output_shape=(8, 8))\n+        mha = layers.MultiHeadAttention(\n+            num_heads=2, key_dim=16, output_shape=(8, 8)\n+        )\n         query = random.uniform((2, 4, 16))\n         value = random.uniform((2, 4, 16))\n         output = mha(query=query, value=value)\n\n@@ -2381,7 +2381,7 @@ class DotProductAttention(Operation):\n         bias=None,\n         mask=None,\n         scale=None,\n-        flash_attention=False,\n+        flash_attention=None,\n     ):\n         return backend.nn.dot_product_attention(\n             query,\n@@ -2402,7 +2402,7 @@ class DotProductAttention(Operation):\n         bias=None,\n         mask=None,\n         scale=None,\n-        flash_attention=False,\n+        flash_attention=None,\n     ):\n         return KerasTensor(query.shape, dtype=query.dtype)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
