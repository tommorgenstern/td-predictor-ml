{"custom_id": "keras#f5e4c9cff9882331fa0332ff4b26155620454ea1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 677 | Lines Deleted: 79 | Files Changed: 2 | Hunks: 34 | Methods Changed: 36 | Complexity Δ (Sum/Max): 85/74 | Churn Δ: 756 | Churn Cumulative: 2018 | Contributors (this commit): 20 | Commits (past 90d): 2 | Contributors (cumulative): 23 | DMM Complexity: 0.7107231920199502\n\nDIFF:\n@@ -14,22 +14,25 @@\n # ==============================================================================\n \"\"\"Utilities for text input preprocessing.\"\"\"\n # pylint: disable=invalid-name\n+# pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-direct-tensorflow-import\n \n-from keras_preprocessing import text\n+import collections\n+import hashlib\n+import json\n+import warnings\n \n from keras.preprocessing.text_dataset import text_dataset_from_directory  # pylint: disable=unused-import\n+import numpy as np\n from tensorflow.python.util.tf_export import keras_export\n \n-hashing_trick = text.hashing_trick\n-Tokenizer = text.Tokenizer\n-\n \n @keras_export('keras.preprocessing.text.text_to_word_sequence')\n def text_to_word_sequence(input_text,\n                           filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                           lower=True,\n                           split=' '):\n-  \"\"\"Converts a text to a sequence of words (or tokens).\n+  r\"\"\"Converts a text to a sequence of words (or tokens).\n \n   This function transforms a string of text into a list of words\n   while ignoring `filters` which include punctuations by default.\n@@ -49,8 +52,15 @@ def text_to_word_sequence(input_text,\n   Returns:\n       A list of words (or tokens).\n   \"\"\"\n-  return text.text_to_word_sequence(\n-      input_text, filters=filters, lower=lower, split=split)\n+  if lower:\n+    input_text = input_text.lower()\n+\n+  translate_dict = {c: split for c in filters}\n+  translate_map = str.maketrans(translate_dict)\n+  input_text = input_text.translate(translate_map)\n+\n+  seq = input_text.split(split)\n+  return [i for i in seq if i]\n \n \n @keras_export('keras.preprocessing.text.one_hot')\n@@ -58,7 +68,8 @@ def one_hot(input_text,\n             n,\n             filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n             lower=True,\n-            split=' '):\n+            split=' ',\n+            analyzer=None):\n   r\"\"\"One-hot encodes a text into a list of word indexes of size `n`.\n \n   This function receives as input a string of text and returns a\n@@ -76,21 +87,451 @@ def one_hot(input_text,\n         includes basic punctuation, tabs, and newlines.\n       lower: boolean. Whether to set the text to lowercase.\n       split: str. Separator for word splitting.\n+      analyzer: function. Custom analyzer to split the text\n \n   Returns:\n       List of integers in `[1, n]`. Each integer encodes a word\n       (unicity non-guaranteed).\n   \"\"\"\n-  return text.one_hot(input_text, n, filters=filters, lower=lower, split=split)\n+  return hashing_trick(\n+      input_text,\n+      n,\n+      hash_function=hash,\n+      filters=filters,\n+      lower=lower,\n+      split=split,\n+      analyzer=analyzer)\n \n \n-# text.tokenizer_from_json is only available if keras_preprocessing >= 1.1.0\n-try:\n-  tokenizer_from_json = text.tokenizer_from_json\n-  keras_export('keras.preprocessing.text.tokenizer_from_json', allow_multiple_exports=True)(\n-      tokenizer_from_json)\n-except AttributeError:\n-  pass\n+@keras_export('keras.preprocessing.text.hashing_trick')\n+def hashing_trick(text,\n+                  n,\n+                  hash_function=None,\n+                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n+                  lower=True,\n+                  split=' ',\n+                  analyzer=None):\n+  r\"\"\"Converts a text to a sequence of indexes in a fixed-size hashing space.\n \n-keras_export('keras.preprocessing.text.hashing_trick', allow_multiple_exports=True)(hashing_trick)\n-keras_export('keras.preprocessing.text.Tokenizer', allow_multiple_exports=True)(Tokenizer)\n+  Args:\n+      text: Input text (string).\n+      n: Dimension of the hashing space.\n+      hash_function: defaults to python `hash` function, can be 'md5' or\n+          any function that takes in input a string and returns a int.\n+          Note that 'hash' is not a stable hashing function, so\n+          it is not consistent across different runs, while 'md5'\n+          is a stable hashing function.\n+      filters: list (or concatenation) of characters to filter out, such as\n+          punctuation. Default: ``!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\\\t\\\\n``,\n+          includes basic punctuation, tabs, and newlines.\n+      lower: boolean. Whether to set the text to lowercase.\n+      split: str. Separator for word splitting.\n+      analyzer: function. Custom analyzer to split the text\n+\n+  Returns:\n+      A list of integer word indices (unicity non-guaranteed).\n+      `0` is a reserved index that won't be assigned to any word.\n+      Two or more words may be assigned to the same index, due to possible\n+      collisions by the hashing function.\n+      The [probability](\n+          https://en.wikipedia.org/wiki/Birthday_problem#Probability_table)\n+      of a collision is in relation to the dimension of the hashing space and\n+      the number of distinct objects.\n+  \"\"\"\n+  if hash_function is None:\n+    hash_function = hash\n+  elif hash_function == 'md5':\n+    hash_function = lambda w: int(hashlib.md5(w.encode()).hexdigest(), 16)\n+\n+  if analyzer is None:\n+    seq = text_to_word_sequence(text, filters=filters, lower=lower, split=split)\n+  else:\n+    seq = analyzer(text)\n+\n+  return [(hash_function(w) % (n - 1) + 1) for w in seq]\n+\n+\n+@keras_export('keras.preprocessing.text.Tokenizer')\n+class Tokenizer(object):\n+  \"\"\"Text tokenization utility class.\n+\n+  This class allows to vectorize a text corpus, by turning each\n+  text into either a sequence of integers (each integer being the index\n+  of a token in a dictionary) or into a vector where the coefficient\n+  for each token could be binary, based on word count, based on tf-idf...\n+\n+  By default, all punctuation is removed, turning the texts into\n+  space-separated sequences of words\n+  (words maybe include the `'` character). These sequences are then\n+  split into lists of tokens. They will then be indexed or vectorized.\n+\n+  `0` is a reserved index that won't be assigned to any word.\n+\n+  Args:\n+      num_words: the maximum number of words to keep, based\n+          on word frequency. Only the most common `num_words-1` words will\n+          be kept.\n+      filters: a string where each element is a character that will be\n+          filtered from the texts. The default is all punctuation, plus\n+          tabs and line breaks, minus the `'` character.\n+      lower: boolean. Whether to convert the texts to lowercase.\n+      split: str. Separator for word splitting.\n+      char_level: if True, every character will be treated as a token.\n+      oov_token: if given, it will be added to word_index and used to\n+          replace out-of-vocabulary words during text_to_sequence calls\n+      analyzer: function. Custom analyzer to split the text.\n+          The default analyzer is text_to_word_sequence\n+  \"\"\"\n+\n+  def __init__(self,\n+               num_words=None,\n+               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n+               lower=True,\n+               split=' ',\n+               char_level=False,\n+               oov_token=None,\n+               analyzer=None,\n+               **kwargs):\n+    # Legacy support\n+    if 'nb_words' in kwargs:\n+      warnings.warn('The `nb_words` argument in `Tokenizer` '\n+                    'has been renamed `num_words`.')\n+      num_words = kwargs.pop('nb_words')\n+    document_count = kwargs.pop('document_count', 0)\n+    if kwargs:\n+      raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n+\n+    self.word_counts = collections.OrderedDict()\n+    self.word_docs = collections.defaultdict(int)\n+    self.filters = filters\n+    self.split = split\n+    self.lower = lower\n+    self.num_words = num_words\n+    self.document_count = document_count\n+    self.char_level = char_level\n+    self.oov_token = oov_token\n+    self.index_docs = collections.defaultdict(int)\n+    self.word_index = {}\n+    self.index_word = {}\n+    self.analyzer = analyzer\n+\n+  def fit_on_texts(self, texts):\n+    \"\"\"Updates internal vocabulary based on a list of texts.\n+\n+    In the case where texts contains lists,\n+    we assume each entry of the lists to be a token.\n+\n+    Required before using `texts_to_sequences` or `texts_to_matrix`.\n+\n+    Args:\n+        texts: can be a list of strings,\n+            a generator of strings (for memory-efficiency),\n+            or a list of list of strings.\n+    \"\"\"\n+    for text in texts:\n+      self.document_count += 1\n+      if self.char_level or isinstance(text, list):\n+        if self.lower:\n+          if isinstance(text, list):\n+            text = [text_elem.lower() for text_elem in text]\n+          else:\n+            text = text.lower()\n+        seq = text\n+      else:\n+        if self.analyzer is None:\n+          seq = text_to_word_sequence(\n+              text, filters=self.filters, lower=self.lower, split=self.split)\n+        else:\n+          seq = self.analyzer(text)\n+      for w in seq:\n+        if w in self.word_counts:\n+          self.word_counts[w] += 1\n+        else:\n+          self.word_counts[w] = 1\n+      for w in set(seq):\n+        # In how many documents each word occurs\n+        self.word_docs[w] += 1\n+\n+    wcounts = list(self.word_counts.items())\n+    wcounts.sort(key=lambda x: x[1], reverse=True)\n+    # forcing the oov_token to index 1 if it exists\n+    if self.oov_token is None:\n+      sorted_voc = []\n+    else:\n+      sorted_voc = [self.oov_token]\n+    sorted_voc.extend(wc[0] for wc in wcounts)\n+\n+    # note that index 0 is reserved, never assigned to an existing word\n+    self.word_index = dict(zip(sorted_voc, list(range(1, len(sorted_voc) + 1))))\n+\n+    self.index_word = {c: w for w, c in self.word_index.items()}\n+\n+    for w, c in list(self.word_docs.items()):\n+      self.index_docs[self.word_index[w]] = c\n+\n+  def fit_on_sequences(self, sequences):\n+    \"\"\"Updates internal vocabulary based on a list of sequences.\n+\n+    Required before using `sequences_to_matrix`\n+    (if `fit_on_texts` was never called).\n+\n+    Args:\n+        sequences: A list of sequence.\n+            A \"sequence\" is a list of integer word indices.\n+    \"\"\"\n+    self.document_count += len(sequences)\n+    for seq in sequences:\n+      seq = set(seq)\n+      for i in seq:\n+        self.index_docs[i] += 1\n+\n+  def texts_to_sequences(self, texts):\n+    \"\"\"Transforms each text in texts to a sequence of integers.\n+\n+    Only top `num_words-1` most frequent words will be taken into account.\n+    Only words known by the tokenizer will be taken into account.\n+\n+    Args:\n+        texts: A list of texts (strings).\n+\n+    Returns:\n+        A list of sequences.\n+    \"\"\"\n+    return list(self.texts_to_sequences_generator(texts))\n+\n+  def texts_to_sequences_generator(self, texts):\n+    \"\"\"Transforms each text in `texts` to a sequence of integers.\n+\n+    Each item in texts can also be a list,\n+    in which case we assume each item of that list to be a token.\n+\n+    Only top `num_words-1` most frequent words will be taken into account.\n+    Only words known by the tokenizer will be taken into account.\n+\n+    Args:\n+        texts: A list of texts (strings).\n+\n+    Yields:\n+        Yields individual sequences.\n+    \"\"\"\n+    num_words = self.num_words\n+    oov_token_index = self.word_index.get(self.oov_token)\n+    for text in texts:\n+      if self.char_level or isinstance(text, list):\n+        if self.lower:\n+          if isinstance(text, list):\n+            text = [text_elem.lower() for text_elem in text]\n+          else:\n+            text = text.lower()\n+        seq = text\n+      else:\n+        if self.analyzer is None:\n+          seq = text_to_word_sequence(\n+              text, filters=self.filters, lower=self.lower, split=self.split)\n+        else:\n+          seq = self.analyzer(text)\n+      vect = []\n+      for w in seq:\n+        i = self.word_index.get(w)\n+        if i is not None:\n+          if num_words and i >= num_words:\n+            if oov_token_index is not None:\n+              vect.append(oov_token_index)\n+          else:\n+            vect.append(i)\n+        elif self.oov_token is not None:\n+          vect.append(oov_token_index)\n+      yield vect\n+\n+  def sequences_to_texts(self, sequences):\n+    \"\"\"Transforms each sequence into a list of text.\n+\n+    Only top `num_words-1` most frequent words will be taken into account.\n+    Only words known by the tokenizer will be taken into account.\n+\n+    Args:\n+        sequences: A list of sequences (list of integers).\n+\n+    Returns:\n+        A list of texts (strings)\n+    \"\"\"\n+    return list(self.sequences_to_texts_generator(sequences))\n+\n+  def sequences_to_texts_generator(self, sequences):\n+    \"\"\"Transforms each sequence in `sequences` to a list of texts(strings).\n+\n+    Each sequence has to a list of integers.\n+    In other words, sequences should be a list of sequences\n+\n+    Only top `num_words-1` most frequent words will be taken into account.\n+    Only words known by the tokenizer will be taken into account.\n+\n+    Args:\n+        sequences: A list of sequences.\n+\n+    Yields:\n+        Yields individual texts.\n+    \"\"\"\n+    num_words = self.num_words\n+    oov_token_index = self.word_index.get(self.oov_token)\n+    for seq in sequences:\n+      vect = []\n+      for num in seq:\n+        word = self.index_word.get(num)\n+        if word is not None:\n+          if num_words and num >= num_words:\n+            if oov_token_index is not None:\n+              vect.append(self.index_word[oov_token_index])\n+          else:\n+            vect.append(word)\n+        elif self.oov_token is not None:\n+          vect.append(self.index_word[oov_token_index])\n+      vect = ' '.join(vect)\n+      yield vect\n+\n+  def texts_to_matrix(self, texts, mode='binary'):\n+    \"\"\"Convert a list of texts to a Numpy matrix.\n+\n+    Args:\n+        texts: list of strings.\n+        mode: one of \"binary\", \"count\", \"tfidf\", \"freq\".\n+\n+    Returns:\n+        A Numpy matrix.\n+    \"\"\"\n+    sequences = self.texts_to_sequences(texts)\n+    return self.sequences_to_matrix(sequences, mode=mode)\n+\n+  def sequences_to_matrix(self, sequences, mode='binary'):\n+    \"\"\"Converts a list of sequences into a Numpy matrix.\n+\n+    Args:\n+        sequences: list of sequences\n+            (a sequence is a list of integer word indices).\n+        mode: one of \"binary\", \"count\", \"tfidf\", \"freq\"\n+\n+    Returns:\n+        A Numpy matrix.\n+\n+    Raises:\n+        ValueError: In case of invalid `mode` argument,\n+            or if the Tokenizer requires to be fit to sample data.\n+    \"\"\"\n+    if not self.num_words:\n+      if self.word_index:\n+        num_words = len(self.word_index) + 1\n+      else:\n+        raise ValueError('Specify a dimension (`num_words` argument), '\n+                         'or fit on some text data first.')\n+    else:\n+      num_words = self.num_words\n+\n+    if mode == 'tfidf' and not self.document_count:\n+      raise ValueError('Fit the Tokenizer on some data '\n+                       'before using tfidf mode.')\n+\n+    x = np.zeros((len(sequences), num_words))\n+    for i, seq in enumerate(sequences):\n+      if not seq:\n+        continue\n+      counts = collections.defaultdict(int)\n+      for j in seq:\n+        if j >= num_words:\n+          continue\n+        counts[j] += 1\n+      for j, c in list(counts.items()):\n+        if mode == 'count':\n+          x[i][j] = c\n+        elif mode == 'freq':\n+          x[i][j] = c / len(seq)\n+        elif mode == 'binary':\n+          x[i][j] = 1\n+        elif mode == 'tfidf':\n+          # Use weighting scheme 2 in\n+          # https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n+          tf = 1 + np.log(c)\n+          idf = np.log(1 + self.document_count /\n+                       (1 + self.index_docs.get(j, 0)))\n+          x[i][j] = tf * idf\n+        else:\n+          raise ValueError('Unknown vectorization mode:', mode)\n+    return x\n+\n+  def get_config(self):\n+    \"\"\"Returns the tokenizer configuration as Python dictionary.\n+\n+    The word count dictionaries used by the tokenizer get serialized\n+    into plain JSON, so that the configuration can be read by other\n+    projects.\n+\n+    Returns:\n+        A Python dictionary with the tokenizer configuration.\n+    \"\"\"\n+    json_word_counts = json.dumps(self.word_counts)\n+    json_word_docs = json.dumps(self.word_docs)\n+    json_index_docs = json.dumps(self.index_docs)\n+    json_word_index = json.dumps(self.word_index)\n+    json_index_word = json.dumps(self.index_word)\n+\n+    return {\n+        'num_words': self.num_words,\n+        'filters': self.filters,\n+        'lower': self.lower,\n+        'split': self.split,\n+        'char_level': self.char_level,\n+        'oov_token': self.oov_token,\n+        'document_count': self.document_count,\n+        'word_counts': json_word_counts,\n+        'word_docs': json_word_docs,\n+        'index_docs': json_index_docs,\n+        'index_word': json_index_word,\n+        'word_index': json_word_index\n+    }\n+\n+  def to_json(self, **kwargs):\n+    \"\"\"Returns a JSON string containing the tokenizer configuration.\n+\n+    To load a tokenizer from a JSON string, use\n+    `keras.preprocessing.text.tokenizer_from_json(json_string)`.\n+\n+    Args:\n+        **kwargs: Additional keyword arguments\n+            to be passed to `json.dumps()`.\n+\n+    Returns:\n+        A JSON string containing the tokenizer configuration.\n+    \"\"\"\n+    config = self.get_config()\n+    tokenizer_config = {'class_name': self.__class__.__name__, 'config': config}\n+    return json.dumps(tokenizer_config, **kwargs)\n+\n+\n+@keras_export('keras.preprocessing.text.tokenizer_from_json')\n+def tokenizer_from_json(json_string):\n+  \"\"\"Parses a JSON tokenizer configuration and returns a tokenizer instance.\n+\n+  Args:\n+      json_string: JSON string encoding a tokenizer configuration.\n+\n+  Returns:\n+      A Keras Tokenizer instance\n+  \"\"\"\n+  tokenizer_config = json.loads(json_string)\n+  config = tokenizer_config.get('config')\n+\n+  word_counts = json.loads(config.pop('word_counts'))\n+  word_docs = json.loads(config.pop('word_docs'))\n+  index_docs = json.loads(config.pop('index_docs'))\n+  # Integer indexing gets converted to strings with json.dumps()\n+  index_docs = {int(k): v for k, v in index_docs.items()}\n+  index_word = json.loads(config.pop('index_word'))\n+  index_word = {int(k): v for k, v in index_word.items()}\n+  word_index = json.loads(config.pop('word_index'))\n+\n+  tokenizer = Tokenizer(**config)\n+  tokenizer.word_counts = word_counts\n+  tokenizer.word_docs = word_docs\n+  tokenizer.index_docs = index_docs\n+  tokenizer.word_index = word_index\n+  tokenizer.index_word = index_word\n+  return tokenizer\n\n@@ -15,40 +15,54 @@\n # ==============================================================================\n \"\"\"Tests for text data preprocessing utils.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n+import collections\n \n+from keras.preprocessing import text\n import numpy as np\n-\n-from keras.preprocessing import text as preprocessing_text\n+import tensorflow.compat.v2 as tf\n \n \n class TestText(tf.test.TestCase):\n \n   def test_one_hot(self):\n-    text = 'The cat sat on the mat.'\n-    encoded = preprocessing_text.one_hot(text, 5)\n-    self.assertEqual(len(encoded), 6)\n+    sample_text = 'The cat sat on the mat.'\n+    encoded = text.one_hot(sample_text, 5)\n+    self.assertLen(encoded, 6)\n     self.assertLessEqual(np.max(encoded), 4)\n     self.assertGreaterEqual(np.min(encoded), 0)\n \n-    # Test on unicode.\n-    text = u'The cat sat on the mat.'\n-    encoded = preprocessing_text.one_hot(text, 5)\n-    self.assertEqual(len(encoded), 6)\n+    sample_text = 'The-cat-sat-on-the-mat'\n+    encoded2 = text.one_hot(\n+        sample_text, 5, analyzer=lambda t: t.lower().split('-'))\n+    self.assertEqual(encoded, encoded2)\n+    self.assertLen(encoded, 6)\n     self.assertLessEqual(np.max(encoded), 4)\n     self.assertGreaterEqual(np.min(encoded), 0)\n \n+  def test_hashing_trick_hash(self):\n+    sample_text = 'The cat sat on the mat.'\n+    encoded = text.hashing_trick(sample_text, 5)\n+    self.assertLen(encoded, 6)\n+    self.assertLessEqual(np.max(encoded), 4)\n+    self.assertGreaterEqual(np.min(encoded), 1)\n+\n+  def test_hashing_trick_md5(self):\n+    sample_text = 'The cat sat on the mat.'\n+    encoded = text.hashing_trick(sample_text, 5, hash_function='md5')\n+    self.assertLen(encoded, 6)\n+    self.assertLessEqual(np.max(encoded), 4)\n+    self.assertGreaterEqual(np.min(encoded), 1)\n+\n   def test_tokenizer(self):\n-    texts = [\n-        'The cat sat on the mat.',\n-        'The dog sat on the log.',\n+    sample_texts = [\n+        'The cat sat on the mat.', 'The dog sat on the log.',\n         'Dogs and cats living together.'\n     ]\n-    tokenizer = preprocessing_text.Tokenizer(num_words=10)\n-    tokenizer.fit_on_texts(texts)\n+    tokenizer = text.Tokenizer(num_words=10)\n+    tokenizer.fit_on_texts(sample_texts)\n \n     sequences = []\n-    for seq in tokenizer.texts_to_sequences_generator(texts):\n+    for seq in tokenizer.texts_to_sequences_generator(sample_texts):\n       sequences.append(seq)\n     self.assertLess(np.max(np.max(sequences)), 10)\n     self.assertEqual(np.min(np.min(sequences)), 1)\n@@ -56,39 +70,50 @@ class TestText(tf.test.TestCase):\n     tokenizer.fit_on_sequences(sequences)\n \n     for mode in ['binary', 'count', 'tfidf', 'freq']:\n-      matrix = tokenizer.texts_to_matrix(texts, mode)\n-      self.assertEqual(matrix.shape, (3, 10))\n+      tokenizer.texts_to_matrix(sample_texts, mode)\n \n-  def test_hashing_trick_hash(self):\n-    text = 'The cat sat on the mat.'\n-    encoded = preprocessing_text.hashing_trick(text, 5)\n-    self.assertEqual(len(encoded), 6)\n-    self.assertLessEqual(np.max(encoded), 4)\n-    self.assertGreaterEqual(np.min(encoded), 1)\n+  def test_tokenizer_serde_no_fitting(self):\n+    tokenizer = text.Tokenizer(num_words=100)\n \n-  def test_hashing_trick_md5(self):\n-    text = 'The cat sat on the mat.'\n-    encoded = preprocessing_text.hashing_trick(\n-        text, 5, hash_function='md5')\n-    self.assertEqual(len(encoded), 6)\n-    self.assertLessEqual(np.max(encoded), 4)\n-    self.assertGreaterEqual(np.min(encoded), 1)\n+    tokenizer_json = tokenizer.to_json()\n+    recovered = text.tokenizer_from_json(tokenizer_json)\n \n-  def test_tokenizer_oov_flag(self):\n-    x_train = ['This text has only known words']\n-    x_test = ['This text has some unknown words']  # 2 OOVs: some, unknown\n+    self.assertEqual(tokenizer.get_config(), recovered.get_config())\n \n-    # Default, without OOV flag\n-    tokenizer = preprocessing_text.Tokenizer()\n-    tokenizer.fit_on_texts(x_train)\n-    x_test_seq = tokenizer.texts_to_sequences(x_test)\n-    self.assertEqual(len(x_test_seq[0]), 4)  # discards 2 OOVs\n+    self.assertEqual(tokenizer.word_docs, recovered.word_docs)\n+    self.assertEqual(tokenizer.word_counts, recovered.word_counts)\n+    self.assertEqual(tokenizer.word_index, recovered.word_index)\n+    self.assertEqual(tokenizer.index_word, recovered.index_word)\n+    self.assertEqual(tokenizer.index_docs, recovered.index_docs)\n \n-    # With OOV feature\n-    tokenizer = preprocessing_text.Tokenizer(oov_token='<unk>')\n-    tokenizer.fit_on_texts(x_train)\n-    x_test_seq = tokenizer.texts_to_sequences(x_test)\n-    self.assertEqual(len(x_test_seq[0]), 6)  # OOVs marked in place\n+  def test_tokenizer_serde_fitting(self):\n+    sample_texts = [\n+        'There was a time that the pieces fit, but I watched them fall away',\n+        'Mildewed and smoldering, strangled by our coveting',\n+        'I\\'ve done the math enough to know the dangers of our second guessing'\n+    ]\n+    tokenizer = text.Tokenizer(num_words=100)\n+    tokenizer.fit_on_texts(sample_texts)\n+\n+    seq_generator = tokenizer.texts_to_sequences_generator(sample_texts)\n+    sequences = [seq for seq in seq_generator]\n+    tokenizer.fit_on_sequences(sequences)\n+\n+    tokenizer_json = tokenizer.to_json()\n+    recovered = text.tokenizer_from_json(tokenizer_json)\n+\n+    self.assertEqual(tokenizer.char_level, recovered.char_level)\n+    self.assertEqual(tokenizer.document_count, recovered.document_count)\n+    self.assertEqual(tokenizer.filters, recovered.filters)\n+    self.assertEqual(tokenizer.lower, recovered.lower)\n+    self.assertEqual(tokenizer.num_words, recovered.num_words)\n+    self.assertEqual(tokenizer.oov_token, recovered.oov_token)\n+\n+    self.assertEqual(tokenizer.word_docs, recovered.word_docs)\n+    self.assertEqual(tokenizer.word_counts, recovered.word_counts)\n+    self.assertEqual(tokenizer.word_index, recovered.word_index)\n+    self.assertEqual(tokenizer.index_word, recovered.index_word)\n+    self.assertEqual(tokenizer.index_docs, recovered.index_docs)\n \n   def test_sequential_fit(self):\n     texts = [\n@@ -97,7 +122,8 @@ class TestText(tf.test.TestCase):\n     ]\n     word_sequences = [['The', 'cat', 'is', 'sitting'],\n                       ['The', 'dog', 'is', 'standing']]\n-    tokenizer = preprocessing_text.Tokenizer()\n+\n+    tokenizer = text.Tokenizer()\n     tokenizer.fit_on_texts(texts)\n     tokenizer.fit_on_texts(word_sequences)\n \n@@ -107,33 +133,164 @@ class TestText(tf.test.TestCase):\n     tokenizer.texts_to_matrix(word_sequences)\n \n   def test_text_to_word_sequence(self):\n-    text = 'hello! ? world!'\n-    seq = preprocessing_text.text_to_word_sequence(text)\n-    self.assertEqual(seq, ['hello', 'world'])\n+    sample_text = 'hello! ? world!'\n+    self.assertEqual(\n+        text.text_to_word_sequence(sample_text), ['hello', 'world'])\n \n   def test_text_to_word_sequence_multichar_split(self):\n-    text = 'hello!stop?world!'\n-    seq = preprocessing_text.text_to_word_sequence(text, split='stop')\n-    self.assertEqual(seq, ['hello', 'world'])\n+    sample_text = 'hello!stop?world!'\n+    self.assertEqual(\n+        text.text_to_word_sequence(sample_text, split='stop'),\n+        ['hello', 'world'])\n \n   def test_text_to_word_sequence_unicode(self):\n-    text = u'ali! veli? kırk dokuz elli'\n-    seq = preprocessing_text.text_to_word_sequence(text)\n-    self.assertEqual(seq, [u'ali', u'veli', u'kırk', u'dokuz', u'elli'])\n+    sample_text = u'ali! veli? kırk dokuz elli'\n+    self.assertEqual(\n+        text.text_to_word_sequence(sample_text),\n+        [u'ali', u'veli', u'kırk', u'dokuz', u'elli'])\n \n   def test_text_to_word_sequence_unicode_multichar_split(self):\n-    text = u'ali!stopveli?stopkırkstopdokuzstopelli'\n-    seq = preprocessing_text.text_to_word_sequence(text, split='stop')\n-    self.assertEqual(seq, [u'ali', u'veli', u'kırk', u'dokuz', u'elli'])\n+    sample_text = u'ali!stopveli?stopkırkstopdokuzstopelli'\n+    self.assertEqual(\n+        text.text_to_word_sequence(sample_text, split='stop'),\n+        [u'ali', u'veli', u'kırk', u'dokuz', u'elli'])\n \n   def test_tokenizer_unicode(self):\n-    texts = [\n+    sample_texts = [\n         u'ali veli kırk dokuz elli', u'ali veli kırk dokuz elli veli kırk dokuz'\n     ]\n-    tokenizer = preprocessing_text.Tokenizer(num_words=5)\n-    tokenizer.fit_on_texts(texts)\n+    tokenizer = text.Tokenizer(num_words=5)\n+    tokenizer.fit_on_texts(sample_texts)\n \n-    self.assertEqual(len(tokenizer.word_counts), 5)\n+    self.assertLen(tokenizer.word_counts, 5)\n+\n+  def test_tokenizer_oov_flag(self):\n+    \"\"\"Test of Out of Vocabulary (OOV) flag in text.Tokenizer.\"\"\"\n+    x_train = ['This text has only known words']\n+    x_test = ['This text has some unknown words']  # 2 OOVs: some, unknown\n+\n+    # Default, without OOV flag\n+    tokenizer = text.Tokenizer()\n+    tokenizer.fit_on_texts(x_train)\n+    x_test_seq = tokenizer.texts_to_sequences(x_test)\n+    self.assertLen(x_test_seq[0], 4)  # discards 2 OOVs\n+\n+    # With OOV feature\n+    tokenizer = text.Tokenizer(oov_token='<unk>')\n+    tokenizer.fit_on_texts(x_train)\n+    x_test_seq = tokenizer.texts_to_sequences(x_test)\n+    self.assertLen(x_test_seq[0], 6)  # OOVs marked in place\n+\n+  def test_tokenizer_oov_flag_and_num_words(self):\n+    x_train = ['This text has only known words this text']\n+    x_test = ['This text has some unknown words']\n+\n+    tokenizer = text.Tokenizer(num_words=3, oov_token='<unk>')\n+    tokenizer.fit_on_texts(x_train)\n+    x_test_seq = tokenizer.texts_to_sequences(x_test)\n+    trans_text = ' '.join(tokenizer.index_word[t] for t in x_test_seq[0])\n+    self.assertLen(x_test_seq[0], 6)\n+    self.assertEqual(trans_text, 'this <unk> <unk> <unk> <unk> <unk>')\n+\n+  def test_sequences_to_texts_with_num_words_and_oov_token(self):\n+    x_train = ['This text has only known words this text']\n+    x_test = ['This text has some unknown words']\n+\n+    tokenizer = text.Tokenizer(num_words=3, oov_token='<unk>')\n+\n+    tokenizer.fit_on_texts(x_train)\n+    x_test_seq = tokenizer.texts_to_sequences(x_test)\n+    trans_text = tokenizer.sequences_to_texts(x_test_seq)\n+    self.assertEqual(trans_text, ['this <unk> <unk> <unk> <unk> <unk>'])\n+\n+  def test_sequences_to_texts_no_num_words(self):\n+    x_train = ['This text has only known words this text']\n+    x_test = ['This text has some unknown words']\n+\n+    tokenizer = text.Tokenizer(oov_token='<unk>')\n+\n+    tokenizer.fit_on_texts(x_train)\n+    x_test_seq = tokenizer.texts_to_sequences(x_test)\n+    trans_text = tokenizer.sequences_to_texts(x_test_seq)\n+    self.assertEqual(trans_text, ['this text has <unk> <unk> words'])\n+\n+  def test_sequences_to_texts_no_oov_token(self):\n+    x_train = ['This text has only known words this text']\n+    x_test = ['This text has some unknown words']\n+\n+    tokenizer = text.Tokenizer(num_words=3)\n+\n+    tokenizer.fit_on_texts(x_train)\n+    x_test_seq = tokenizer.texts_to_sequences(x_test)\n+    trans_text = tokenizer.sequences_to_texts(x_test_seq)\n+    self.assertEqual(trans_text, ['this text'])\n+\n+  def test_sequences_to_texts_no_num_words_no_oov_token(self):\n+    x_train = ['This text has only known words this text']\n+    x_test = ['This text has some unknown words']\n+\n+    tokenizer = text.Tokenizer()\n+\n+    tokenizer.fit_on_texts(x_train)\n+    x_test_seq = tokenizer.texts_to_sequences(x_test)\n+    trans_text = tokenizer.sequences_to_texts(x_test_seq)\n+    self.assertEqual(trans_text, ['this text has words'])\n+\n+  def test_sequences_to_texts(self):\n+    texts = [\n+        'The cat sat on the mat.', 'The dog sat on the log.',\n+        'Dogs and cats living together.'\n+    ]\n+    tokenizer = text.Tokenizer(num_words=10, oov_token='<unk>')\n+    tokenizer.fit_on_texts(texts)\n+    tokenized_text = tokenizer.texts_to_sequences(texts)\n+    trans_text = tokenizer.sequences_to_texts(tokenized_text)\n+    self.assertEqual(trans_text, [\n+        'the cat sat on the mat', 'the dog sat on the log',\n+        'dogs <unk> <unk> <unk> <unk>'\n+    ])\n+\n+  def test_tokenizer_lower_flag(self):\n+    \"\"\"Tests for `lower` flag in text.Tokenizer.\"\"\"\n+    # word level tokenizer with sentences as texts\n+    word_tokenizer = text.Tokenizer(lower=True)\n+    texts = [\n+        'The cat sat on the mat.', 'The dog sat on the log.',\n+        'Dog and Cat living Together.'\n+    ]\n+    word_tokenizer.fit_on_texts(texts)\n+    expected_word_counts = collections.OrderedDict([('the', 4), ('cat', 2),\n+                                                    ('sat', 2), ('on', 2),\n+                                                    ('mat', 1), ('dog', 2),\n+                                                    ('log', 1), ('and', 1),\n+                                                    ('living', 1),\n+                                                    ('together', 1)])\n+    self.assertEqual(word_tokenizer.word_counts, expected_word_counts)\n+\n+    # word level tokenizer with word_sequences as texts\n+    word_tokenizer = text.Tokenizer(lower=True)\n+    word_sequences = [['The', 'cat', 'is', 'sitting'],\n+                      ['The', 'dog', 'is', 'standing']]\n+    word_tokenizer.fit_on_texts(word_sequences)\n+    expected_word_counts = collections.OrderedDict([('the', 2), ('cat', 1),\n+                                                    ('is', 2), ('sitting', 1),\n+                                                    ('dog', 1),\n+                                                    ('standing', 1)])\n+    self.assertEqual(word_tokenizer.word_counts, expected_word_counts)\n+\n+    # char level tokenizer with sentences as texts\n+    char_tokenizer = text.Tokenizer(lower=True, char_level=True)\n+    texts = [\n+        'The cat sat on the mat.', 'The dog sat on the log.',\n+        'Dog and Cat living Together.'\n+    ]\n+    char_tokenizer.fit_on_texts(texts)\n+    expected_word_counts = collections.OrderedDict([\n+        ('t', 11), ('h', 5), ('e', 6), (' ', 14), ('c', 2), ('a', 6), ('s', 2),\n+        ('o', 6), ('n', 4), ('m', 1), ('.', 3), ('d', 3), ('g', 5), ('l', 2),\n+        ('i', 2), ('v', 1), ('r', 1)\n+    ])\n+    self.assertEqual(char_tokenizer.word_counts, expected_word_counts)\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#064f2d3f446a384abd1ebdd9ded8ac47dad9b4fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 85 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -13,17 +13,9 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Provides keras data preprocessing utils to pre-process tf.data.Datasets before they are fed to the model.\"\"\"\n-# pylint: disable=g-import-not-at-top\n-# TODO(mihaimaruseac): remove the import of keras_preprocessing and injecting\n-# once we update to latest version of keras_preprocessing\n-import keras_preprocessing\n-\n from keras import backend\n from keras.preprocessing import image\n from keras.preprocessing import sequence\n from keras.preprocessing import text\n from keras.preprocessing import timeseries\n from keras.utils import all_utils as utils\n-\n-# This exists for compatibility with prior version of keras_preprocessing.\n-keras_preprocessing.set_keras_submodules(backend=backend, utils=utils)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f1aa8b7d2a0c89591c5c42eca5b6f013114a7bbd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 436 | Lines Deleted: 97 | Files Changed: 2 | Hunks: 51 | Methods Changed: 17 | Complexity Δ (Sum/Max): 53/52 | Churn Δ: 533 | Churn Cumulative: 2185 | Contributors (this commit): 24 | Commits (past 90d): 2 | Contributors (cumulative): 27 | DMM Complexity: 0.5414634146341464\n\nDIFF:\n@@ -14,27 +14,47 @@\n # ==============================================================================\n \"\"\"Utilities for preprocessing sequence data.\"\"\"\n # pylint: disable=invalid-name\n+# pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-direct-tensorflow-import\n \n-from keras_preprocessing import sequence\n+import json\n+import random\n \n from keras.utils import data_utils\n+import numpy as np\n+\n from tensorflow.python.util.tf_export import keras_export\n \n-make_sampling_table = sequence.make_sampling_table\n-skipgrams = sequence.skipgrams\n-# TODO(fchollet): consider making `_remove_long_seq` public.\n-_remove_long_seq = sequence._remove_long_seq  # pylint: disable=protected-access\n+\n+def _remove_long_seq(maxlen, seq, label):\n+  \"\"\"Removes sequences that exceed the maximum length.\n+\n+  Args:\n+      maxlen: Int, maximum length of the output sequences.\n+      seq: List of lists, where each sublist is a sequence.\n+      label: List where each element is an integer.\n+\n+  Returns:\n+      new_seq, new_label: shortened lists for `seq` and `label`.\n+  \"\"\"\n+  new_seq, new_label = [], []\n+  for x, y in zip(seq, label):\n+    if len(x) < maxlen:\n+      new_seq.append(x)\n+      new_label.append(y)\n+  return new_seq, new_label\n \n \n @keras_export('keras.preprocessing.sequence.TimeseriesGenerator')\n-class TimeseriesGenerator(sequence.TimeseriesGenerator, data_utils.Sequence):\n+class TimeseriesGenerator(data_utils.Sequence):\n   \"\"\"Utility class for generating batches of temporal data.\n \n   This class takes in a sequence of data-points gathered at\n   equal intervals, along with time series parameters such as\n   stride, length of history, etc., to produce batches for\n   training/validation.\n-  # Arguments\n+\n+  Arguments:\n       data: Indexable generator (such as list or Numpy array)\n           containing consecutive data points (timesteps).\n           The data should be at 2D, and axis 0 is expected\n@@ -61,9 +81,13 @@ class TimeseriesGenerator(sequence.TimeseriesGenerator, data_utils.Sequence):\n           in reverse chronological order.\n       batch_size: Number of timeseries samples in each batch\n           (except maybe the last one).\n-  # Returns\n-      A [Sequence](https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence) instance.\n-  # Examples\n+\n+  Returns:\n+      A [Sequence](\n+      https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence)\n+      instance.\n+\n+  Examples:\n       ```python\n       from keras.preprocessing.sequence import TimeseriesGenerator\n       import numpy as np\n@@ -82,7 +106,116 @@ class TimeseriesGenerator(sequence.TimeseriesGenerator, data_utils.Sequence):\n                             np.array([[10], [11]]))\n       ```\n   \"\"\"\n-  pass\n+\n+  def __init__(self,\n+               data,\n+               targets,\n+               length,\n+               sampling_rate=1,\n+               stride=1,\n+               start_index=0,\n+               end_index=None,\n+               shuffle=False,\n+               reverse=False,\n+               batch_size=128):\n+\n+    if len(data) != len(targets):\n+      raise ValueError('Data and targets have to be' + ' of same length. '\n+                       'Data length is {}'.format(len(data)) +\n+                       ' while target length is {}'.format(len(targets)))\n+\n+    self.data = data\n+    self.targets = targets\n+    self.length = length\n+    self.sampling_rate = sampling_rate\n+    self.stride = stride\n+    self.start_index = start_index + length\n+    if end_index is None:\n+      end_index = len(data) - 1\n+    self.end_index = end_index\n+    self.shuffle = shuffle\n+    self.reverse = reverse\n+    self.batch_size = batch_size\n+\n+    if self.start_index > self.end_index:\n+      raise ValueError('`start_index+length=%i > end_index=%i` '\n+                       'is disallowed, as no part of the sequence '\n+                       'would be left to be used as current step.' %\n+                       (self.start_index, self.end_index))\n+\n+  def __len__(self):\n+    return (self.end_index - self.start_index +\n+            self.batch_size * self.stride) // (\n+                self.batch_size * self.stride)\n+\n+  def __getitem__(self, index):\n+    if self.shuffle:\n+      rows = np.random.randint(\n+          self.start_index, self.end_index + 1, size=self.batch_size)\n+    else:\n+      i = self.start_index + self.batch_size * self.stride * index\n+      rows = np.arange(\n+          i, min(i + self.batch_size * self.stride, self.end_index + 1),\n+          self.stride)\n+\n+    samples = np.array(\n+        [self.data[row - self.length:row:self.sampling_rate] for row in rows])\n+    targets = np.array([self.targets[row] for row in rows])\n+\n+    if self.reverse:\n+      return samples[:, ::-1, ...], targets\n+    return samples, targets\n+\n+  def get_config(self):\n+    \"\"\"Returns the TimeseriesGenerator configuration as Python dictionary.\n+\n+    Returns:\n+        A Python dictionary with the TimeseriesGenerator configuration.\n+    \"\"\"\n+    data = self.data\n+    if type(self.data).__module__ == np.__name__:\n+      data = self.data.tolist()\n+    try:\n+      json_data = json.dumps(data)\n+    except TypeError as e:\n+      raise TypeError('Data not JSON Serializable:', data) from e\n+\n+    targets = self.targets\n+    if type(self.targets).__module__ == np.__name__:\n+      targets = self.targets.tolist()\n+    try:\n+      json_targets = json.dumps(targets)\n+    except TypeError as e:\n+      raise TypeError('Targets not JSON Serializable:', targets) from e\n+\n+    return {\n+        'data': json_data,\n+        'targets': json_targets,\n+        'length': self.length,\n+        'sampling_rate': self.sampling_rate,\n+        'stride': self.stride,\n+        'start_index': self.start_index,\n+        'end_index': self.end_index,\n+        'shuffle': self.shuffle,\n+        'reverse': self.reverse,\n+        'batch_size': self.batch_size\n+    }\n+\n+  def to_json(self, **kwargs):\n+    \"\"\"Returns a JSON string containing the timeseries generator configuration.\n+\n+    Args:\n+        **kwargs: Additional keyword arguments\n+            to be passed to `json.dumps()`.\n+    Returns:\n+        A JSON string containing the tokenizer configuration.\n+    \"\"\"\n+    config = self.get_config()\n+    timeseries_generator_config = {\n+        'class_name': self.__class__.__name__,\n+        'config': config\n+    }\n+    return json.dumps(timeseries_generator_config, **kwargs)\n \n \n @keras_export('keras.preprocessing.sequence.pad_sequences')\n@@ -149,10 +282,197 @@ def pad_sequences(sequences, maxlen=None, dtype='int32',\n       ValueError: In case of invalid values for `truncating` or `padding`,\n           or in case of invalid shape for a `sequences` entry.\n   \"\"\"\n-  return sequence.pad_sequences(\n-      sequences, maxlen=maxlen, dtype=dtype,\n-      padding=padding, truncating=truncating, value=value)\n+  if not hasattr(sequences, '__len__'):\n+    raise ValueError('`sequences` must be iterable.')\n+  num_samples = len(sequences)\n \n-keras_export(\n-    'keras.preprocessing.sequence.make_sampling_table', allow_multiple_exports=True)(make_sampling_table)\n-keras_export('keras.preprocessing.sequence.skipgrams', allow_multiple_exports=True)(skipgrams)\n+  lengths = []\n+  sample_shape = ()\n+  flag = True\n+\n+  # take the sample shape from the first non empty sequence\n+  # checking for consistency in the main loop below.\n+\n+  for x in sequences:\n+    try:\n+      lengths.append(len(x))\n+      if flag and len(x):\n+        sample_shape = np.asarray(x).shape[1:]\n+        flag = False\n+    except TypeError as e:\n+      raise ValueError('`sequences` must be a list of iterables. '\n+                       'Found non-iterable: ' + str(x)) from e\n+\n+  if maxlen is None:\n+    maxlen = np.max(lengths)\n+\n+  is_dtype_str = np.issubdtype(dtype, np.str_) or np.issubdtype(\n+      dtype, np.unicode_)\n+  if isinstance(value, str) and dtype != object and not is_dtype_str:\n+    raise ValueError(\n+        \"`dtype` {} is not compatible with `value`'s type: {}\\n\"\n+        'You should set `dtype=object` for variable length strings.'.format(\n+            dtype, type(value)))\n+\n+  x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n+  for idx, s in enumerate(sequences):\n+    if not len(s):  # pylint: disable=g-explicit-length-test\n+      continue  # empty list/array was found\n+    if truncating == 'pre':\n+      trunc = s[-maxlen:]  # pylint: disable=invalid-unary-operand-type\n+    elif truncating == 'post':\n+      trunc = s[:maxlen]\n+    else:\n+      raise ValueError('Truncating type \"%s\" ' 'not understood' % truncating)\n+\n+    # check `trunc` has expected shape\n+    trunc = np.asarray(trunc, dtype=dtype)\n+    if trunc.shape[1:] != sample_shape:\n+      raise ValueError('Shape of sample %s of sequence at position %s '\n+                       'is different from expected shape %s' %\n+                       (trunc.shape[1:], idx, sample_shape))\n+\n+    if padding == 'post':\n+      x[idx, :len(trunc)] = trunc\n+    elif padding == 'pre':\n+      x[idx, -len(trunc):] = trunc\n+    else:\n+      raise ValueError('Padding type \"%s\" not understood' % padding)\n+  return x\n+\n+\n+@keras_export('keras.preprocessing.sequence.make_sampling_table')\n+def make_sampling_table(size, sampling_factor=1e-5):\n+  \"\"\"Generates a word rank-based probabilistic sampling table.\n+\n+  Used for generating the `sampling_table` argument for `skipgrams`.\n+  `sampling_table[i]` is the probability of sampling\n+  the word i-th most common word in a dataset\n+  (more common words should be sampled less frequently, for balance).\n+\n+  The sampling probabilities are generated according\n+  to the sampling distribution used in word2vec:\n+\n+  ```\n+  p(word) = (min(1, sqrt(word_frequency / sampling_factor) /\n+      (word_frequency / sampling_factor)))\n+  ```\n+\n+  We assume that the word frequencies follow Zipf's law (s=1) to derive\n+  a numerical approximation of frequency(rank):\n+\n+  `frequency(rank) ~ 1/(rank * (log(rank) + gamma) + 1/2 - 1/(12*rank))`\n+  where `gamma` is the Euler-Mascheroni constant.\n+\n+  Args:\n+      size: Int, number of possible words to sample.\n+      sampling_factor: The sampling factor in the word2vec formula.\n+\n+  Returns:\n+      A 1D Numpy array of length `size` where the ith entry\n+      is the probability that a word of rank i should be sampled.\n+  \"\"\"\n+  gamma = 0.577\n+  rank = np.arange(size)\n+  rank[0] = 1\n+  inv_fq = rank * (np.log(rank) + gamma) + 0.5 - 1. / (12. * rank)\n+  f = sampling_factor * inv_fq\n+\n+  return np.minimum(1., f / np.sqrt(f))\n+\n+\n+@keras_export('keras.preprocessing.sequence.skipgrams')\n+def skipgrams(sequence,\n+              vocabulary_size,\n+              window_size=4,\n+              negative_samples=1.,\n+              shuffle=True,\n+              categorical=False,\n+              sampling_table=None,\n+              seed=None):\n+  \"\"\"Generates skipgram word pairs.\n+\n+  This function transforms a sequence of word indexes (list of integers)\n+  into tuples of words of the form:\n+\n+  - (word, word in the same window), with label 1 (positive samples).\n+  - (word, random word from the vocabulary), with label 0 (negative samples).\n+\n+  Read more about Skipgram in this gnomic paper by Mikolov et al.:\n+  [Efficient Estimation of Word Representations in\n+  Vector Space](http://arxiv.org/pdf/1301.3781v3.pdf)\n+\n+  Args:\n+      sequence: A word sequence (sentence), encoded as a list\n+          of word indices (integers). If using a `sampling_table`,\n+          word indices are expected to match the rank\n+          of the words in a reference dataset (e.g. 10 would encode\n+          the 10-th most frequently occurring token).\n+          Note that index 0 is expected to be a non-word and will be skipped.\n+      vocabulary_size: Int, maximum possible word index + 1\n+      window_size: Int, size of sampling windows (technically half-window).\n+          The window of a word `w_i` will be\n+          `[i - window_size, i + window_size+1]`.\n+      negative_samples: Float >= 0. 0 for no negative (i.e. random) samples.\n+          1 for same number as positive samples.\n+      shuffle: Whether to shuffle the word couples before returning them.\n+      categorical: bool. if False, labels will be\n+          integers (eg. `[0, 1, 1 .. ]`),\n+          if `True`, labels will be categorical, e.g.\n+          `[[1,0],[0,1],[0,1] .. ]`.\n+      sampling_table: 1D array of size `vocabulary_size` where the entry i\n+          encodes the probability to sample a word of rank i.\n+      seed: Random seed.\n+\n+  Returns:\n+      couples, labels: where `couples` are int pairs and\n+          `labels` are either 0 or 1.\n+\n+  Note:\n+      By convention, index 0 in the vocabulary is\n+      a non-word and will be skipped.\n+  \"\"\"\n+  couples = []\n+  labels = []\n+  for i, wi in enumerate(sequence):\n+    if not wi:\n+      continue\n+    if sampling_table is not None:\n+      if sampling_table[wi] < random.random():\n+        continue\n+\n+    window_start = max(0, i - window_size)\n+    window_end = min(len(sequence), i + window_size + 1)\n+    for j in range(window_start, window_end):\n+      if j != i:\n+        wj = sequence[j]\n+        if not wj:\n+          continue\n+        couples.append([wi, wj])\n+        if categorical:\n+          labels.append([0, 1])\n+        else:\n+          labels.append(1)\n+\n+  if negative_samples > 0:\n+    num_negative_samples = int(len(labels) * negative_samples)\n+    words = [c[0] for c in couples]\n+    random.shuffle(words)\n+\n+    couples += [[words[i % len(words)],\n+                 random.randint(1, vocabulary_size - 1)]\n+                for i in range(num_negative_samples)]\n+    if categorical:\n+      labels += [[1, 0]] * num_negative_samples\n+    else:\n+      labels += [0] * num_negative_samples\n+\n+  if shuffle:\n+    if seed is None:\n+      seed = random.randint(0, 10e6)\n+    random.seed(seed)\n+    random.shuffle(couples)\n+    random.seed(seed)\n+    random.shuffle(labels)\n+\n+  return couples, labels\n\n@@ -14,13 +14,11 @@\n # ==============================================================================\n \"\"\"Tests for sequence data preprocessing utils.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n-from math import ceil\n+import math\n \n+from keras.preprocessing import sequence\n import numpy as np\n-\n-from keras.preprocessing import sequence as preprocessing_sequence\n+import tensorflow.compat.v2 as tf\n \n \n class TestSequence(tf.test.TestCase):\n@@ -29,123 +27,145 @@ class TestSequence(tf.test.TestCase):\n     a = [[1], [1, 2], [1, 2, 3]]\n \n     # test padding\n-    b = preprocessing_sequence.pad_sequences(a, maxlen=3, padding='pre')\n+    b = sequence.pad_sequences(a, maxlen=3, padding='pre')\n     self.assertAllClose(b, [[0, 0, 1], [0, 1, 2], [1, 2, 3]])\n-    b = preprocessing_sequence.pad_sequences(a, maxlen=3, padding='post')\n+    b = sequence.pad_sequences(a, maxlen=3, padding='post')\n     self.assertAllClose(b, [[1, 0, 0], [1, 2, 0], [1, 2, 3]])\n \n     # test truncating\n-    b = preprocessing_sequence.pad_sequences(\n-        a, maxlen=2, truncating='pre')\n+    b = sequence.pad_sequences(a, maxlen=2, truncating='pre')\n     self.assertAllClose(b, [[0, 1], [1, 2], [2, 3]])\n-    b = preprocessing_sequence.pad_sequences(\n-        a, maxlen=2, truncating='post')\n+    b = sequence.pad_sequences(a, maxlen=2, truncating='post')\n     self.assertAllClose(b, [[0, 1], [1, 2], [1, 2]])\n \n     # test value\n-    b = preprocessing_sequence.pad_sequences(a, maxlen=3, value=1)\n+    b = sequence.pad_sequences(a, maxlen=3, value=1)\n     self.assertAllClose(b, [[1, 1, 1], [1, 1, 2], [1, 2, 3]])\n \n+  def test_pad_sequences_str(self):\n+    a = [['1'], ['1', '2'], ['1', '2', '3']]\n+\n+    # test padding\n+    b = sequence.pad_sequences(\n+        a, maxlen=3, padding='pre', value='pad', dtype=object)\n+    self.assertAllEqual(\n+        b, [['pad', 'pad', '1'], ['pad', '1', '2'], ['1', '2', '3']])\n+    b = sequence.pad_sequences(\n+        a, maxlen=3, padding='post', value='pad', dtype='<U3')\n+    self.assertAllEqual(\n+        b, [['1', 'pad', 'pad'], ['1', '2', 'pad'], ['1', '2', '3']])\n+\n+    # test truncating\n+    b = sequence.pad_sequences(\n+        a, maxlen=2, truncating='pre', value='pad', dtype=object)\n+    self.assertAllEqual(b, [['pad', '1'], ['1', '2'], ['2', '3']])\n+    b = sequence.pad_sequences(\n+        a, maxlen=2, truncating='post', value='pad', dtype='<U3')\n+    self.assertAllEqual(b, [['pad', '1'], ['1', '2'], ['1', '2']])\n+\n+    with self.assertRaisesRegex(ValueError,\n+                                '`dtype` int32 is not compatible with '):\n+      sequence.pad_sequences(a, maxlen=2, truncating='post', value='pad')\n+\n   def test_pad_sequences_vector(self):\n     a = [[[1, 1]], [[2, 1], [2, 2]], [[3, 1], [3, 2], [3, 3]]]\n \n     # test padding\n-    b = preprocessing_sequence.pad_sequences(a, maxlen=3, padding='pre')\n+    b = sequence.pad_sequences(a, maxlen=3, padding='pre')\n     self.assertAllClose(b, [[[0, 0], [0, 0], [1, 1]], [[0, 0], [2, 1], [2, 2]],\n                             [[3, 1], [3, 2], [3, 3]]])\n-    b = preprocessing_sequence.pad_sequences(a, maxlen=3, padding='post')\n+    b = sequence.pad_sequences(a, maxlen=3, padding='post')\n     self.assertAllClose(b, [[[1, 1], [0, 0], [0, 0]], [[2, 1], [2, 2], [0, 0]],\n                             [[3, 1], [3, 2], [3, 3]]])\n \n     # test truncating\n-    b = preprocessing_sequence.pad_sequences(\n-        a, maxlen=2, truncating='pre')\n-    self.assertAllClose(b, [[[0, 0], [1, 1]], [[2, 1], [2, 2]], [[3, 2], [3,\n-                                                                          3]]])\n+    b = sequence.pad_sequences(a, maxlen=2, truncating='pre')\n+    self.assertAllClose(b,\n+                        [[[0, 0], [1, 1]], [[2, 1], [2, 2]], [[3, 2], [3, 3]]])\n \n-    b = preprocessing_sequence.pad_sequences(\n-        a, maxlen=2, truncating='post')\n-    self.assertAllClose(b, [[[0, 0], [1, 1]], [[2, 1], [2, 2]], [[3, 1], [3,\n-                                                                          2]]])\n+    b = sequence.pad_sequences(a, maxlen=2, truncating='post')\n+    self.assertAllClose(b,\n+                        [[[0, 0], [1, 1]], [[2, 1], [2, 2]], [[3, 1], [3, 2]]])\n \n     # test value\n-    b = preprocessing_sequence.pad_sequences(a, maxlen=3, value=1)\n+    b = sequence.pad_sequences(a, maxlen=3, value=1)\n     self.assertAllClose(b, [[[1, 1], [1, 1], [1, 1]], [[1, 1], [2, 1], [2, 2]],\n                             [[3, 1], [3, 2], [3, 3]]])\n \n   def test_make_sampling_table(self):\n-    a = preprocessing_sequence.make_sampling_table(3)\n+    a = sequence.make_sampling_table(3)\n     self.assertAllClose(\n         a, np.asarray([0.00315225, 0.00315225, 0.00547597]), rtol=.1)\n \n   def test_skipgrams(self):\n     # test with no window size and binary labels\n-    couples, labels = preprocessing_sequence.skipgrams(\n-        np.arange(3), vocabulary_size=3)\n+    couples, labels = sequence.skipgrams(np.arange(3), vocabulary_size=3)\n     for couple in couples:\n       self.assertIn(couple[0], [0, 1, 2])\n       self.assertIn(couple[1], [0, 1, 2])\n \n     # test window size and categorical labels\n-    couples, labels = preprocessing_sequence.skipgrams(\n+    couples, labels = sequence.skipgrams(\n         np.arange(5), vocabulary_size=5, window_size=1, categorical=True)\n     for couple in couples:\n       self.assertLessEqual(couple[0] - couple[1], 3)\n-    for l in labels:\n-      self.assertEqual(len(l), 2)\n+    for label in labels:\n+      self.assertLen(label, 2)\n \n   def test_remove_long_seq(self):\n-    a = [[[1, 1]], [[2, 1], [2, 2]], [[3, 1], [3, 2], [3, 3]]]\n-\n-    new_seq, new_label = preprocessing_sequence._remove_long_seq(\n-        maxlen=3, seq=a, label=['a', 'b', ['c', 'd']])\n-    self.assertEqual(new_seq, [[[1, 1]], [[2, 1], [2, 2]]])\n-    self.assertEqual(new_label, ['a', 'b'])\n+    maxlen = 5\n+    seq = [\n+        [1, 2, 3],\n+        [1, 2, 3, 4, 5, 6],\n+    ]\n+    label = ['a', 'b']\n+    new_seq, new_label = sequence._remove_long_seq(maxlen, seq, label)\n+    self.assertEqual(new_seq, [[1, 2, 3]])\n+    self.assertEqual(new_label, ['a'])\n \n   def test_TimeseriesGenerator(self):\n     data = np.array([[i] for i in range(50)])\n     targets = np.array([[i] for i in range(50)])\n \n-    data_gen = preprocessing_sequence.TimeseriesGenerator(\n+    data_gen = sequence.TimeseriesGenerator(\n         data, targets, length=10, sampling_rate=2, batch_size=2)\n-    self.assertEqual(len(data_gen), 20)\n-    self.assertAllClose(data_gen[0][0],\n-                        np.array([[[0], [2], [4], [6], [8]], [[1], [3], [5],\n-                                                              [7], [9]]]))\n+    self.assertLen(data_gen, 20)\n+    self.assertAllClose(\n+        data_gen[0][0],\n+        np.array([[[0], [2], [4], [6], [8]], [[1], [3], [5], [7], [9]]]))\n     self.assertAllClose(data_gen[0][1], np.array([[10], [11]]))\n-    self.assertAllClose(data_gen[1][0],\n-                        np.array([[[2], [4], [6], [8], [10]], [[3], [5], [7],\n-                                                               [9], [11]]]))\n+    self.assertAllClose(\n+        data_gen[1][0],\n+        np.array([[[2], [4], [6], [8], [10]], [[3], [5], [7], [9], [11]]]))\n     self.assertAllClose(data_gen[1][1], np.array([[12], [13]]))\n \n-    data_gen = preprocessing_sequence.TimeseriesGenerator(\n+    data_gen = sequence.TimeseriesGenerator(\n         data, targets, length=10, sampling_rate=2, reverse=True, batch_size=2)\n-    self.assertEqual(len(data_gen), 20)\n-    self.assertAllClose(data_gen[0][0],\n-                        np.array([[[8], [6], [4], [2], [0]], [[9], [7], [5],\n-                                                              [3], [1]]]))\n+    self.assertLen(data_gen, 20)\n+    self.assertAllClose(\n+        data_gen[0][0],\n+        np.array([[[8], [6], [4], [2], [0]], [[9], [7], [5], [3], [1]]]))\n     self.assertAllClose(data_gen[0][1], np.array([[10], [11]]))\n \n-    data_gen = preprocessing_sequence.TimeseriesGenerator(\n+    data_gen = sequence.TimeseriesGenerator(\n         data, targets, length=10, sampling_rate=2, shuffle=True, batch_size=1)\n     batch = data_gen[0]\n     r = batch[1][0][0]\n-    self.assertAllClose(batch[0],\n-                        np.array([[[r - 10], [r - 8], [r - 6], [r - 4],\n-                                   [r - 2]]]))\n+    self.assertAllClose(\n+        batch[0], np.array([[[r - 10], [r - 8], [r - 6], [r - 4], [r - 2]]]))\n     self.assertAllClose(batch[1], np.array([\n         [r],\n     ]))\n \n-    data_gen = preprocessing_sequence.TimeseriesGenerator(\n+    data_gen = sequence.TimeseriesGenerator(\n         data, targets, length=10, sampling_rate=2, stride=2, batch_size=2)\n-    self.assertEqual(len(data_gen), 10)\n-    self.assertAllClose(data_gen[1][0],\n-                        np.array([[[4], [6], [8], [10], [12]], [[6], [8], [10],\n-                                                                [12], [14]]]))\n+    self.assertLen(data_gen, 10)\n+    self.assertAllClose(\n+        data_gen[1][0],\n+        np.array([[[4], [6], [8], [10], [12]], [[6], [8], [10], [12], [14]]]))\n     self.assertAllClose(data_gen[1][1], np.array([[14], [16]]))\n \n-    data_gen = preprocessing_sequence.TimeseriesGenerator(\n+    data_gen = sequence.TimeseriesGenerator(\n         data,\n         targets,\n         length=10,\n@@ -153,15 +173,16 @@ class TestSequence(tf.test.TestCase):\n         start_index=10,\n         end_index=30,\n         batch_size=2)\n-    self.assertEqual(len(data_gen), 6)\n-    self.assertAllClose(data_gen[0][0],\n-                        np.array([[[10], [12], [14], [16], [18]],\n-                                  [[11], [13], [15], [17], [19]]]))\n+    self.assertLen(data_gen, 6)\n+    self.assertAllClose(\n+        data_gen[0][0],\n+        np.array([[[10], [12], [14], [16], [18]], [[11], [13], [15], [17],\n+                                                   [19]]]))\n     self.assertAllClose(data_gen[0][1], np.array([[20], [21]]))\n \n     data = np.array([np.random.random_sample((1, 2, 3, 4)) for i in range(50)])\n     targets = np.array([np.random.random_sample((3, 2, 1)) for i in range(50)])\n-    data_gen = preprocessing_sequence.TimeseriesGenerator(\n+    data_gen = sequence.TimeseriesGenerator(\n         data,\n         targets,\n         length=10,\n@@ -169,30 +190,28 @@ class TestSequence(tf.test.TestCase):\n         start_index=10,\n         end_index=30,\n         batch_size=2)\n-\n-    self.assertEqual(len(data_gen), 6)\n-    self.assertAllClose(data_gen[0][0],\n-                        np.array(\n-                            [np.array(data[10:19:2]),\n+    self.assertLen(data_gen, 6)\n+    self.assertAllClose(\n+        data_gen[0][0],\n+        np.array([np.array(data[10:19:2]),\n                   np.array(data[11:20:2])]))\n     self.assertAllClose(data_gen[0][1], np.array([targets[20], targets[21]]))\n \n-    with self.assertRaises(ValueError) as context:\n-      preprocessing_sequence.TimeseriesGenerator(data, targets, length=50)\n-    error = str(context.exception)\n-    self.assertIn('`start_index+length=50 > end_index=49` is disallowed', error)\n+    with self.assertRaisesRegex(\n+        ValueError, r'`start_index\\+length=50 > end_index=49` is disallowed'):\n+      sequence.TimeseriesGenerator(data, targets, length=50)\n \n   def test_TimeSeriesGenerator_doesnt_miss_any_sample(self):\n     x = np.array([[i] for i in range(10)])\n \n     for length in range(3, 10):\n-      g = preprocessing_sequence.TimeseriesGenerator(\n-          x, x, length=length, batch_size=1)\n+      g = sequence.TimeseriesGenerator(x, x, length=length, batch_size=1)\n       expected = max(0, len(x) - length)\n       actual = len(g)\n+\n       self.assertEqual(expected, actual)\n \n-      if actual > 0:\n+      if len(g) > 0:  # pylint: disable=g-explicit-length-test\n         # All elements in range(length, 10) should be used as current step\n         expected = np.arange(length, 10).reshape(-1, 1)\n \n@@ -208,7 +227,7 @@ class TestSequence(tf.test.TestCase):\n \n     for stride, length, batch_size, shuffle in zip(strides, lengths,\n                                                    batch_sizes, shuffles):\n-      g = preprocessing_sequence.TimeseriesGenerator(\n+      g = sequence.TimeseriesGenerator(\n           x,\n           x,\n           length=length,\n@@ -221,18 +240,18 @@ class TestSequence(tf.test.TestCase):\n           batch_size=batch_size)\n       if shuffle:\n         # all batches have the same size when shuffle is True.\n-        expected_sequences = ceil(\n+        expected_sequences = math.ceil(\n             (23 - length) / float(batch_size * stride)) * batch_size\n       else:\n         # last batch will be different if `(samples - length) / stride`\n         # is not a multiple of `batch_size`.\n-        expected_sequences = ceil((23 - length) / float(stride))\n+        expected_sequences = math.ceil((23 - length) / float(stride))\n \n-      expected_batches = ceil(expected_sequences / float(batch_size))\n+      expected_batches = math.ceil(expected_sequences / float(batch_size))\n \n       y = [g[ix][1] for ix in range(len(g))]\n \n-      actual_sequences = sum(len(iy) for iy in y)\n+      actual_sequences = sum(len(y_) for y_ in y)\n       actual_batches = len(y)\n \n       self.assertEqual(expected_sequences, actual_sequences)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
