{"custom_id": "keras#b36a9ac67a31c72871495286aacdd7114e2fc029", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 884 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,3 @@\n-import jax\n import ml_dtypes\n import numpy as np\n \n@@ -138,6 +137,13 @@ def resize(\n     fill_value=0.0,\n     data_format=None,\n ):\n+    try:\n+        import jax\n+    except ImportError:\n+        raise ImportError(\n+            \"The `resize` op on the numpy backend uses jax for an efficient \"\n+            \"resize implementation. Please install jax via `pip install jax`.\"\n+        )\n     data_format = backend.standardize_data_format(data_format)\n     if interpolation not in RESIZE_INTERPOLATIONS:\n         raise ValueError(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a8ccaf3652a16c0c29f9ba38f9ad37a7f01c3d1c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 16 | Churn Cumulative: 765 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -618,8 +618,6 @@ def functional_from_config(cls, config, custom_objects=None):\n \n     input_tensors = map_tensors(functional_config[\"input_layers\"])\n     output_tensors = map_tensors(functional_config[\"output_layers\"])\n-    if isinstance(input_tensors, list) and len(input_tensors) == 1:\n-        input_tensors = input_tensors[0]\n     if isinstance(output_tensors, list) and len(output_tensors) == 1:\n         output_tensors = output_tensors[0]\n \n\n@@ -16,6 +16,7 @@ from keras.src.layers.input_spec import InputSpec\n from keras.src.models import Functional\n from keras.src.models import Model\n from keras.src.models import Sequential\n+from keras.src.models.model import model_from_json\n \n \n class FunctionalTest(testing.TestCase):\n@@ -272,6 +273,19 @@ class FunctionalTest(testing.TestCase):\n         out_val = model_restored(Input(shape=(3,), batch_size=2))\n         self.assertIsInstance(out_val, out_type)\n \n+    def test_restored_nested_input(self):\n+        input_a = Input(shape=(3,), batch_size=2, name=\"input_a\")\n+        x = layers.Dense(5)(input_a)\n+        outputs = layers.Dense(4)(x)\n+        model = Functional([[input_a]], outputs)\n+\n+        # Serialize and deserialize the model\n+        json_config = model.to_json()\n+        restored_json_config = model_from_json(json_config).to_json()\n+\n+        # Check that the serialized model is the same as the original\n+        self.assertEqual(json_config, restored_json_config)\n+\n     @pytest.mark.requires_trainable_backend\n     def test_layer_getters(self):\n         # Test mixing ops and layers\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#63038a4da61476e07ef9d7ad38aa11af6d9992ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 64 | Churn Cumulative: 916 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: 0.78125\n\nDIFF:\n@@ -6,8 +6,11 @@ import jax.experimental.sparse as jax_sparse\n import jax.numpy as jnp\n from jax import lax\n from jax import nn as jnn\n-from jax.experimental.pallas.ops.tpu import (\n-    flash_attention as flash_attention_tpu,\n+from jax.experimental.pallas.ops.tpu.splash_attention import (\n+    splash_attention_kernel,\n+)\n+from jax.experimental.pallas.ops.tpu.splash_attention import (\n+    splash_attention_mask,\n )\n \n from keras.src import backend\n@@ -1036,6 +1039,8 @@ def _can_use_flash_attention(query, key, value, bias, raise_error=False):\n             )\n         return False\n \n+    if jax.devices()[0].platform == \"tpu\":\n+        return True\n     try:\n         # Check if cuDNN is installed and raise RuntimeError if cuDNN is not\n         # detected\n@@ -1109,6 +1114,38 @@ def _dot_product_attention_core(\n     return jnp.einsum(\"BNTS,BSNH->BTNH\", probs, value)\n \n \n+def wrap_flash_attention(\n+    query, key, value, decoder_segment_ids, custom_mask=None\n+):\n+    if decoder_segment_ids is not None:\n+        assert query.shape[2] == decoder_segment_ids.q.shape[1], (\n+            \"Sharding along sequence dimension not allowed in tpu kernel \"\n+            \"attention\"\n+        )\n+\n+    if custom_mask is not None:\n+        mask = splash_attention_mask.NumpyMask(mask=custom_mask)\n+\n+    else:\n+        mask = splash_attention_mask.CausalMask(\n+            shape=(query.shape[2], query.shape[2])\n+        )\n+\n+    # Create multi-head mask\n+    multi_head_mask = splash_attention_mask.MultiHeadMask(\n+        masks=(mask,) * query.shape[1]\n+    )\n+    splash_kernel = splash_attention_kernel.make_splash_mha(\n+        mask=multi_head_mask,\n+        head_shards=1,\n+        q_seq_shards=1,\n+    )\n+\n+    return jax.vmap(splash_kernel)(\n+        query, key, value, segment_ids=decoder_segment_ids\n+    )\n+\n+\n def dot_product_attention(\n     query,\n     key,\n@@ -1134,17 +1171,26 @@ def dot_product_attention(\n         # Use `raise_error=True` to provide more details if the inputs failed to\n         # use flash attention\n         _can_use_flash_attention(query, key, value, bias, raise_error=True)\n-    if jax.devices()[0].platform == \"tpu\" and flash_attention:\n-        # Use TPU-optimized flash attention from Pallas\n-        return flash_attention_tpu(\n+\n+    if jax.devices()[0].platform == \"tpu\":\n+        # Transpose to ('batch', 'heads', 'length', 'kv')\n+        query = jnp.transpose(query, axes=(0, 2, 1, 3))\n+        key = jnp.transpose(key, axes=(0, 2, 1, 3))\n+        value = jnp.transpose(value, axes=(0, 2, 1, 3))\n+        B, H, S, KV = query.shape\n+\n+        segment_ids = jnp.ones([B, S])\n+        # {token_ids, padding_mask, segment_ids} enable packing\n+        out = wrap_flash_attention(\n             query,\n             key,\n             value,\n-            ab=bias,\n-            segment_ids=mask,\n-            causal=is_causal,\n-            sm_scale=scale,\n+            splash_attention_kernel.SegmentIds(segment_ids, segment_ids),\n+            custom_mask=mask,\n         )\n+        out = jnp.transpose(out, axes=(0, 2, 1, 3))\n+        return out\n+\n     # `dot_product_attention` is only available in jax>=0.4.31\n     if hasattr(jax.nn, \"dot_product_attention\"):\n         return jax.nn.dot_product_attention(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2599816d28a423b720585bc496f090bb239908eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 2 | Files Changed: 5 | Hunks: 16 | Methods Changed: 10 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 41 | Churn Cumulative: 3837 | Contributors (this commit): 19 | Commits (past 90d): 11 | Contributors (cumulative): 56 | DMM Complexity: 0.84375\n\nDIFF:\n@@ -1115,7 +1115,12 @@ def _dot_product_attention_core(\n \n \n def wrap_flash_attention(\n-    query, key, value, decoder_segment_ids, custom_mask=None\n+    query,\n+    key,\n+    value,\n+    decoder_segment_ids,\n+    custom_mask=None,\n+    attn_logits_soft_cap=None,\n ):\n     if decoder_segment_ids is not None:\n         assert query.shape[2] == decoder_segment_ids.q.shape[1], (\n@@ -1139,6 +1144,7 @@ def wrap_flash_attention(\n         mask=multi_head_mask,\n         head_shards=1,\n         q_seq_shards=1,\n+        attn_logits_soft_cap=attn_logits_soft_cap,\n     )\n \n     return jax.vmap(splash_kernel)(\n@@ -1155,6 +1161,7 @@ def dot_product_attention(\n     scale=None,\n     is_causal=False,\n     flash_attention=None,\n+    attn_logits_soft_cap=None,\n ):\n     query = convert_to_tensor(query)\n     key = convert_to_tensor(key)\n@@ -1185,8 +1192,11 @@ def dot_product_attention(\n             query,\n             key,\n             value,\n-            splash_attention_kernel.SegmentIds(segment_ids, segment_ids),\n+            decoder_segment_ids=splash_attention_kernel.SegmentIds(\n+                segment_ids, segment_ids\n+            ),\n             custom_mask=mask,\n+            attn_logits_soft_cap=attn_logits_soft_cap,\n         )\n         out = jnp.transpose(out, axes=(0, 2, 1, 3))\n         return out\n\n@@ -1133,6 +1133,7 @@ def dot_product_attention(\n     scale=None,\n     is_causal=False,\n     flash_attention=None,\n+    attn_logits_soft_cap=None,\n ):\n     if flash_attention is None:\n         flash_attention = False\n\n@@ -1041,6 +1041,7 @@ def dot_product_attention(\n     scale=None,\n     is_causal=False,\n     flash_attention=None,\n+    attn_logits_soft_cap=None,\n ):\n     if flash_attention is None:\n         flash_attention = False\n\n@@ -1018,6 +1018,7 @@ def dot_product_attention(\n     scale=None,\n     is_causal=False,\n     flash_attention=None,\n+    attn_logits_soft_cap=None,\n ):\n     if bias is not None:\n         raise ValueError(\n\n@@ -2545,6 +2545,7 @@ class DotProductAttention(Operation):\n         mask=None,\n         scale=None,\n         flash_attention=None,\n+        attn_logits_soft_cap=None,\n     ):\n         return backend.nn.dot_product_attention(\n             query,\n@@ -2555,6 +2556,7 @@ class DotProductAttention(Operation):\n             scale=scale,\n             is_causal=self.is_causal,\n             flash_attention=flash_attention,\n+            attn_logits_soft_cap=attn_logits_soft_cap,\n         )\n \n     def compute_output_spec(\n@@ -2566,6 +2568,7 @@ class DotProductAttention(Operation):\n         mask=None,\n         scale=None,\n         flash_attention=None,\n+        attn_logits_soft_cap=None,\n     ):\n         return KerasTensor(query.shape, dtype=query.dtype)\n \n@@ -2582,6 +2585,7 @@ def dot_product_attention(\n     scale=None,\n     is_causal=False,\n     flash_attention=None,\n+    attn_logits_soft_cap=None,\n ):\n     \"\"\"Scaled dot product attention function.\n \n@@ -2620,6 +2624,9 @@ def dot_product_attention(\n             attempt to use flash attention if the required conditions are met.\n             Typically, the inputs must be in float16 and bfloat16 dtype and the\n             input layout requirements may vary depending on the backend.\n+        attn_logits_soft_cap: The value limit for maximum value of the\n+            attention logits before the softmax function is applied. This is\n+            only supported in JAX TPU backend. Defaults to None.\n \n     Returns:\n         An array of the attention output with the same shape of `query`.\n@@ -2632,6 +2639,21 @@ def dot_product_attention(\n     >>> keras.ops.nn.dot_product_attention(query, key, value).shape\n     (2, 4, 8, 16)\n     \"\"\"\n+    if attn_logits_soft_cap is not None:\n+        if backend.backend() == \"jax\":\n+            import jax\n+\n+            if jax.devices()[0].platform != \"tpu\":\n+                raise ValueError(\n+                    \"attn_logits_soft_cap is only supported for JAX on TPU. \"\n+                    \"Set attn_logits_soft_cap=None when not using JAX on TPU.\"\n+                )\n+        else:\n+            raise ValueError(\n+                \"attn_logits_soft_cap is only supported for JAX on TPU. \"\n+                \"Set attn_logits_soft_cap=None when not using JAX on TPU.\"\n+            )\n+\n     if any_symbolic_tensors((query, key, value)):\n         return DotProductAttention(is_causal=is_causal).symbolic_call(\n             query,\n@@ -2641,6 +2663,7 @@ def dot_product_attention(\n             mask=mask,\n             scale=scale,\n             flash_attention=flash_attention,\n+            attn_logits_soft_cap=attn_logits_soft_cap,\n         )\n     return backend.nn.dot_product_attention(\n         query,\n@@ -2651,6 +2674,7 @@ def dot_product_attention(\n         scale=scale,\n         is_causal=is_causal,\n         flash_attention=flash_attention,\n+        attn_logits_soft_cap=attn_logits_soft_cap,\n     )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0fac3ab004f23874e72d70fdedbb3b42c5f25912", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 136 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 4 | Methods Changed: 8 | Complexity Δ (Sum/Max): 23/23 | Churn Δ: 146 | Churn Cumulative: 1030 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 0.8\n\nDIFF:\n@@ -137,13 +137,6 @@ def resize(\n     fill_value=0.0,\n     data_format=None,\n ):\n-    try:\n-        import jax\n-    except ImportError:\n-        raise ImportError(\n-            \"The `resize` op on the numpy backend uses jax for an efficient \"\n-            \"resize implementation. Please install jax via `pip install jax`.\"\n-        )\n     data_format = backend.standardize_data_format(data_format)\n     if interpolation not in RESIZE_INTERPOLATIONS:\n         raise ValueError(\n@@ -370,11 +363,144 @@ def resize(\n                 padded_img = images\n         images = padded_img\n \n-    return np.array(\n-        jax.image.resize(\n-            images, size, method=interpolation, antialias=antialias\n+    return _resize(images, size, method=interpolation, antialias=antialias)\n+\n+\n+def compute_weight_mat(\n+    input_size, output_size, scale, translation, kernel, antialias\n+):\n+    dtype = np.result_type(scale, translation)\n+    inv_scale = 1.0 / scale\n+    kernel_scale = np.maximum(inv_scale, 1.0) if antialias else 1.0\n+\n+    sample_f = (\n+        (np.arange(output_size, dtype=dtype) + 0.5) * inv_scale\n+        - translation * inv_scale\n+        - 0.5\n     )\n+\n+    x = (\n+        np.abs(\n+            sample_f[np.newaxis, :]\n+            - np.arange(input_size, dtype=dtype)[:, np.newaxis]\n         )\n+        / kernel_scale\n+    )\n+\n+    weights = kernel(x)\n+\n+    total_weight_sum = np.sum(weights, axis=0, keepdims=True)\n+    weights = np.where(\n+        np.abs(total_weight_sum) > 1000.0 * np.finfo(np.float32).eps,\n+        np.divide(\n+            weights, np.where(total_weight_sum != 0, total_weight_sum, 1)\n+        ),\n+        0,\n+    )\n+\n+    input_size_minus_0_5 = input_size - 0.5\n+    return np.where(\n+        np.logical_and(sample_f >= -0.5, sample_f <= input_size_minus_0_5)[\n+            np.newaxis, :\n+        ],\n+        weights,\n+        0,\n+    )\n+\n+\n+def _resize(image, shape, method, antialias):\n+    def _fill_triangle_kernel(x):\n+        return np.maximum(0, 1 - np.abs(x))\n+\n+    def _fill_keys_cubic_kernel(x):\n+        out = ((1.5 * x - 2.5) * x) * x + 1.0\n+        out = np.where(x >= 1.0, ((-0.5 * x + 2.5) * x - 4.0) * x + 2.0, out)\n+        return np.where(x >= 2.0, 0.0, out)\n+\n+    def _fill_lanczos_kernel(radius, x):\n+        y = radius * np.sin(np.pi * x) * np.sin(np.pi * x / radius)\n+        out = np.where(\n+            x > 1e-3, np.divide(y, np.where(x != 0, np.pi**2 * x**2, 1)), 1\n+        )\n+        return np.where(x > radius, 0.0, out)\n+\n+    if method == \"nearest\":\n+        return _resize_nearest(image, shape)\n+    elif method == \"bilinear\":\n+        kernel = _fill_triangle_kernel\n+    elif method == \"lanczos3\":\n+        kernel = lambda x: _fill_lanczos_kernel(3.0, x)\n+    elif method == \"lanczos5\":\n+        kernel = lambda x: _fill_lanczos_kernel(5.0, x)\n+    elif method == \"bicubic\":\n+        kernel = _fill_keys_cubic_kernel\n+    else:\n+        raise ValueError(\"Unknown resize method\")\n+\n+    spatial_dims = tuple(\n+        i for i in range(len(shape)) if image.shape[i] != shape[i]\n+    )\n+    scale = [\n+        shape[d] / image.shape[d] if image.shape[d] != 0 else 1.0\n+        for d in spatial_dims\n+    ]\n+\n+    return _scale_and_translate(\n+        image,\n+        shape,\n+        spatial_dims,\n+        scale,\n+        [0.0] * len(spatial_dims),\n+        kernel,\n+        antialias,\n+    )\n+\n+\n+def _resize_nearest(x, output_shape):\n+    input_shape = x.shape\n+    spatial_dims = tuple(\n+        i for i in range(len(input_shape)) if input_shape[i] != output_shape[i]\n+    )\n+\n+    for d in spatial_dims:\n+        m, n = input_shape[d], output_shape[d]\n+        offsets = (np.arange(n, dtype=np.float32) + 0.5) * m / n\n+        offsets = np.floor(offsets).astype(np.int32)\n+        indices = [slice(None)] * len(input_shape)\n+        indices[d] = offsets\n+        x = x[tuple(indices)]\n+    return x\n+\n+\n+def _scale_and_translate(\n+    x, output_shape, spatial_dims, scale, translation, kernel, antialias\n+):\n+    input_shape = x.shape\n+\n+    if len(spatial_dims) == 0:\n+        return x\n+\n+    if np.issubdtype(x.dtype, np.integer):\n+        output = x.astype(np.float32)\n+        use_rounding = True\n+    else:\n+        output = x.copy()\n+        use_rounding = False\n+\n+    for i, d in enumerate(spatial_dims):\n+        d = d % x.ndim\n+        m, n = input_shape[d], output_shape[d]\n+\n+        w = compute_weight_mat(\n+            m, n, scale[i], translation[i], kernel, antialias\n+        ).astype(np.float32)\n+        output = np.tensordot(output, w, axes=(d, 0))\n+        output = np.moveaxis(output, -1, d)\n+\n+    if use_rounding:\n+        output = np.clip(np.round(output), x.min(), x.max())\n+        output = output.astype(x.dtype)\n+    return output\n \n \n AFFINE_TRANSFORM_INTERPOLATIONS = {  # map to order\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2ebe3d6eb1c2d56f7ba2279ab200d6ff28fc0f77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 17 | Churn Cumulative: 756 | Contributors (this commit): 12 | Commits (past 90d): 8 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -36,6 +36,9 @@ class Variable(\n         return self.value.handle\n \n     def _initialize(self, value):\n+        if isinstance(value, tf.Variable):\n+            self._value = value\n+        else:\n             self._value = tf.Variable(\n                 value,\n                 dtype=self._dtype,\n@@ -138,7 +141,7 @@ def convert_to_tensor(x, dtype=None, sparse=None, ragged=None):\n             x = tf.convert_to_tensor(x)\n             return tf.cast(x, dtype)\n         return tf.convert_to_tensor(x, dtype=dtype)\n-    elif dtype is not None and not x.dtype == dtype:\n+    elif dtype is not None and not standardize_dtype(x.dtype) == dtype:\n         if isinstance(x, tf.SparseTensor):\n             x_shape = x.shape\n             x = tf.cast(x, dtype)\n\n@@ -120,7 +120,15 @@ class TFSMLayer(layers.Layer):\n \n     def _add_existing_weight(self, weight):\n         \"\"\"Tracks an existing weight.\"\"\"\n-        self._track_variable(weight)\n+        variable = backend.Variable(\n+            initializer=weight,\n+            trainable=weight.trainable,\n+            dtype=weight.dtype,\n+            shape=weight.shape,\n+            # Keras variable names cannot contain slashes.\n+            name=weight.name.replace(\"/\", \"_\"),\n+        )\n+        self._track_variable(variable)\n \n     def call(self, inputs, training=False, **kwargs):\n         if training:\n\n@@ -63,6 +63,8 @@ class TestTFSMLayer(testing.TestCase):\n             reloaded_layer.non_trainable_weights,\n             len(model.non_trainable_weights),\n         )\n+        for keras_var in reloaded_layer.weights:\n+            self.assertIsInstance(keras_var, backend.Variable)\n \n     def test_call_training(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d8d14fd70e5c3038c762b5911ff1e286df06f25c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 1295 | Contributors (this commit): 6 | Commits (past 90d): 13 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -533,14 +533,17 @@ def equal(x1, x2):\n \n def exp(x):\n     x = get_ov_output(x)\n+    x_type = x.get_element_type()\n+    if x_type.is_integral():\n+        ov_type = OPENVINO_DTYPES[config.floatx()]\n+        x = ov_opset.convert(x, ov_type)\n     return OpenVINOKerasTensor(ov_opset.exp(x).output(0))\n \n \n def expand_dims(x, axis):\n-    if isinstance(x, OpenVINOKerasTensor):\n-        x = x.output\n-    else:\n-        assert False\n+    x = get_ov_output(x)\n+    if isinstance(axis, tuple):\n+        axis = list(axis)\n     axis = ov_opset.constant(axis, Type.i32).output(0)\n     return OpenVINOKerasTensor(ov_opset.unsqueeze(x, axis).output(0))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b4e262d5ff62830ab403a34467d9ac05bcae273f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 12 | Churn Cumulative: 1307 | Contributors (this commit): 7 | Commits (past 90d): 14 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -512,7 +512,17 @@ def digitize(x, bins):\n \n \n def dot(x, y):\n-    raise NotImplementedError(\"`dot` is not supported with openvino backend\")\n+    element_type = None\n+    if isinstance(x, OpenVINOKerasTensor):\n+        element_type = x.output.get_element_type()\n+    if isinstance(y, OpenVINOKerasTensor):\n+        element_type = y.output.get_element_type()\n+    x = get_ov_output(x, element_type)\n+    y = get_ov_output(y, element_type)\n+    x, y = _align_operand_types(x, y, \"dot()\")\n+    if x.get_partial_shape().rank == 0 or y.get_partial_shape().rank == 0:\n+        return OpenVINOKerasTensor(ov_opset.multiply(x, y).output(0))\n+    return OpenVINOKerasTensor(ov_opset.matmul(x, y, False, False).output(0))\n \n \n def empty(shape, dtype=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9afea7349e7037c65a3ace25ab2eface88b7ea65", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 729 | Lines Deleted: 3 | Files Changed: 8 | Hunks: 17 | Methods Changed: 14 | Complexity Δ (Sum/Max): 60/15 | Churn Δ: 732 | Churn Cumulative: 7601 | Contributors (this commit): 14 | Commits (past 90d): 37 | Contributors (cumulative): 48 | DMM Complexity: 0.19965576592082615\n\nDIFF:\n@@ -6,6 +6,7 @@ since your modifications would be overwritten.\n \n from keras.src.ops.image import affine_transform\n from keras.src.ops.image import crop_images\n+from keras.src.ops.image import elastic_transform\n from keras.src.ops.image import extract_patches\n from keras.src.ops.image import gaussian_blur\n from keras.src.ops.image import hsv_to_rgb\n\n@@ -6,6 +6,7 @@ since your modifications would be overwritten.\n \n from keras.src.ops.image import affine_transform\n from keras.src.ops.image import crop_images\n+from keras.src.ops.image import elastic_transform\n from keras.src.ops.image import extract_patches\n from keras.src.ops.image import gaussian_blur\n from keras.src.ops.image import hsv_to_rgb\n\n@@ -5,6 +5,7 @@ import jax.numpy as jnp\n \n from keras.src import backend\n from keras.src.backend.jax.core import convert_to_tensor\n+from keras.src.random.seed_generator import draw_seed\n \n RESIZE_INTERPOLATIONS = (\n     \"bilinear\",\n@@ -729,3 +730,132 @@ def gaussian_blur(\n         blurred_images = blurred_images.squeeze(axis=0)\n \n     return blurred_images\n+\n+\n+def elastic_transform(\n+    images,\n+    alpha=20.0,\n+    sigma=5.0,\n+    interpolation=\"bilinear\",\n+    fill_mode=\"reflect\",\n+    fill_value=0.0,\n+    seed=None,\n+    data_format=None,\n+):\n+    data_format = backend.standardize_data_format(data_format)\n+    if interpolation not in AFFINE_TRANSFORM_INTERPOLATIONS.keys():\n+        raise ValueError(\n+            \"Invalid value for argument `interpolation`. Expected of one \"\n+            f\"{set(AFFINE_TRANSFORM_INTERPOLATIONS.keys())}. Received: \"\n+            f\"interpolation={interpolation}\"\n+        )\n+    if fill_mode not in AFFINE_TRANSFORM_FILL_MODES:\n+        raise ValueError(\n+            \"Invalid value for argument `fill_mode`. Expected of one \"\n+            f\"{AFFINE_TRANSFORM_FILL_MODES}. Received: fill_mode={fill_mode}\"\n+        )\n+    if len(images.shape) not in (3, 4):\n+        raise ValueError(\n+            \"Invalid images rank: expected rank 3 (single image) \"\n+            \"or rank 4 (batch of images). Received input with shape: \"\n+            f\"images.shape={images.shape}\"\n+        )\n+\n+    images = convert_to_tensor(images)\n+    alpha = convert_to_tensor(alpha)\n+    sigma = convert_to_tensor(sigma)\n+    input_dtype = images.dtype\n+    kernel_size = (int(6 * sigma) | 1, int(6 * sigma) | 1)\n+\n+    need_squeeze = False\n+    if len(images.shape) == 3:\n+        images = jnp.expand_dims(images, axis=0)\n+        need_squeeze = True\n+\n+    if data_format == \"channels_last\":\n+        batch_size, height, width, channels = images.shape\n+        channel_axis = -1\n+    else:\n+        batch_size, channels, height, width = images.shape\n+        channel_axis = 1\n+\n+    seed = draw_seed(seed)\n+    dx = (\n+        jax.random.normal(\n+            seed, shape=(batch_size, height, width), dtype=input_dtype\n+        )\n+        * sigma\n+    )\n+    dy = (\n+        jax.random.normal(\n+            seed, shape=(batch_size, height, width), dtype=input_dtype\n+        )\n+        * sigma\n+    )\n+\n+    dx = gaussian_blur(\n+        jnp.expand_dims(dx, axis=channel_axis),\n+        kernel_size=kernel_size,\n+        sigma=(sigma, sigma),\n+        data_format=data_format,\n+    )\n+    dy = gaussian_blur(\n+        jnp.expand_dims(dy, axis=channel_axis),\n+        kernel_size=kernel_size,\n+        sigma=(sigma, sigma),\n+        data_format=data_format,\n+    )\n+\n+    dx = jnp.squeeze(dx)\n+    dy = jnp.squeeze(dy)\n+\n+    x, y = jnp.meshgrid(jnp.arange(width), jnp.arange(height))\n+    x, y = x[None, :, :], y[None, :, :]\n+\n+    distorted_x = x + alpha * dx\n+    distorted_y = y + alpha * dy\n+\n+    transformed_images = jnp.zeros_like(images)\n+\n+    if data_format == \"channels_last\":\n+        for i in range(channels):\n+            transformed_images = transformed_images.at[..., i].set(\n+                jnp.stack(\n+                    [\n+                        map_coordinates(\n+                            images[b, ..., i],\n+                            [distorted_y[b], distorted_x[b]],\n+                            order=AFFINE_TRANSFORM_INTERPOLATIONS[\n+                                interpolation\n+                            ],\n+                            fill_mode=fill_mode,\n+                            fill_value=fill_value,\n+                        )\n+                        for b in range(batch_size)\n+                    ]\n+                )\n+            )\n+    else:\n+        for i in range(channels):\n+            transformed_images = transformed_images.at[:, i, :, :].set(\n+                jnp.stack(\n+                    [\n+                        map_coordinates(\n+                            images[b, i, ...],\n+                            [distorted_y[b], distorted_x[b]],\n+                            order=AFFINE_TRANSFORM_INTERPOLATIONS[\n+                                interpolation\n+                            ],\n+                            fill_mode=fill_mode,\n+                            fill_value=fill_value,\n+                        )\n+                        for b in range(batch_size)\n+                    ]\n+                )\n+            )\n+\n+    if need_squeeze:\n+        transformed_images = jnp.squeeze(transformed_images, axis=0)\n+    transformed_images = transformed_images.astype(input_dtype)\n+\n+    return transformed_images\n\n@@ -3,6 +3,7 @@ import numpy as np\n \n from keras.src import backend\n from keras.src.backend.numpy.core import convert_to_tensor\n+from keras.src.random.seed_generator import draw_seed\n from keras.src.utils.module_utils import scipy\n \n RESIZE_INTERPOLATIONS = (\n@@ -1010,3 +1011,127 @@ def gaussian_blur(\n         blurred_images = np.squeeze(blurred_images, axis=0)\n \n     return blurred_images\n+\n+\n+def elastic_transform(\n+    images,\n+    alpha=20.0,\n+    sigma=5.0,\n+    interpolation=\"bilinear\",\n+    fill_mode=\"reflect\",\n+    fill_value=0.0,\n+    seed=None,\n+    data_format=None,\n+):\n+    data_format = backend.standardize_data_format(data_format)\n+    if interpolation not in AFFINE_TRANSFORM_INTERPOLATIONS.keys():\n+        raise ValueError(\n+            \"Invalid value for argument `interpolation`. Expected of one \"\n+            f\"{set(AFFINE_TRANSFORM_INTERPOLATIONS.keys())}. Received: \"\n+            f\"interpolation={interpolation}\"\n+        )\n+    if fill_mode not in AFFINE_TRANSFORM_FILL_MODES:\n+        raise ValueError(\n+            \"Invalid value for argument `fill_mode`. Expected of one \"\n+            f\"{AFFINE_TRANSFORM_FILL_MODES}. Received: fill_mode={fill_mode}\"\n+        )\n+    if len(images.shape) not in (3, 4):\n+        raise ValueError(\n+            \"Invalid images rank: expected rank 3 (single image) \"\n+            \"or rank 4 (batch of images). Received input with shape: \"\n+            f\"images.shape={images.shape}\"\n+        )\n+\n+    images = convert_to_tensor(images)\n+    input_dtype = images.dtype\n+\n+    alpha = convert_to_tensor(alpha, dtype=input_dtype)\n+    sigma = convert_to_tensor(sigma, dtype=input_dtype)\n+\n+    kernel_size = (int(6 * sigma) | 1, int(6 * sigma) | 1)\n+\n+    need_squeeze = False\n+    if len(images.shape) == 3:\n+        images = np.expand_dims(images, axis=0)\n+        need_squeeze = True\n+\n+    if data_format == \"channels_last\":\n+        batch_size, height, width, channels = images.shape\n+        channel_axis = -1\n+    else:\n+        batch_size, channels, height, width = images.shape\n+        channel_axis = 1\n+\n+    seed = draw_seed(seed)\n+    rng = np.random.default_rng(seed)\n+    dx = (\n+        rng.normal(size=(batch_size, height, width), loc=0.0, scale=1.0).astype(\n+            input_dtype\n+        )\n+        * sigma\n+    )\n+    dy = (\n+        rng.normal(size=(batch_size, height, width), loc=0.0, scale=1.0).astype(\n+            input_dtype\n+        )\n+        * sigma\n+    )\n+\n+    dx = gaussian_blur(\n+        np.expand_dims(dx, axis=channel_axis),\n+        kernel_size=kernel_size,\n+        sigma=(sigma, sigma),\n+        data_format=data_format,\n+    )\n+    dy = gaussian_blur(\n+        np.expand_dims(dy, axis=channel_axis),\n+        kernel_size=kernel_size,\n+        sigma=(sigma, sigma),\n+        data_format=data_format,\n+    )\n+\n+    dx = np.squeeze(dx)\n+    dy = np.squeeze(dy)\n+\n+    x, y = np.meshgrid(np.arange(width), np.arange(height))\n+    x, y = x[None, :, :], y[None, :, :]\n+\n+    distorted_x = x + alpha * dx\n+    distorted_y = y + alpha * dy\n+\n+    transformed_images = np.zeros_like(images)\n+\n+    if data_format == \"channels_last\":\n+        for i in range(channels):\n+            transformed_images[..., i] = np.stack(\n+                [\n+                    map_coordinates(\n+                        images[b, ..., i],\n+                        [distorted_y[b], distorted_x[b]],\n+                        order=AFFINE_TRANSFORM_INTERPOLATIONS[interpolation],\n+                        fill_mode=fill_mode,\n+                        fill_value=fill_value,\n+                    )\n+                    for b in range(batch_size)\n+                ]\n+            )\n+    else:\n+        for i in range(channels):\n+            transformed_images[:, i, :, :] = np.stack(\n+                [\n+                    map_coordinates(\n+                        images[b, i, ...],\n+                        [distorted_y[b], distorted_x[b]],\n+                        order=AFFINE_TRANSFORM_INTERPOLATIONS[interpolation],\n+                        fill_mode=fill_mode,\n+                        fill_value=fill_value,\n+                    )\n+                    for b in range(batch_size)\n+                ]\n+            )\n+\n+    if need_squeeze:\n+        transformed_images = np.squeeze(transformed_images, axis=0)\n+    transformed_images = transformed_images.astype(input_dtype)\n+\n+    return transformed_images\n\n@@ -6,6 +6,7 @@ import tensorflow as tf\n \n from keras.src import backend\n from keras.src.backend.tensorflow.core import convert_to_tensor\n+from keras.src.random.seed_generator import draw_seed\n \n RESIZE_INTERPOLATIONS = (\n     \"bilinear\",\n@@ -776,3 +777,141 @@ def gaussian_blur(\n         blurred_images = tf.squeeze(blurred_images, axis=0)\n \n     return blurred_images\n+\n+\n+def elastic_transform(\n+    images,\n+    alpha=20.0,\n+    sigma=5.0,\n+    interpolation=\"bilinear\",\n+    fill_mode=\"reflect\",\n+    fill_value=0.0,\n+    seed=None,\n+    data_format=None,\n+):\n+    data_format = backend.standardize_data_format(data_format)\n+    if interpolation not in AFFINE_TRANSFORM_INTERPOLATIONS:\n+        raise ValueError(\n+            \"Invalid value for argument `interpolation`. Expected of one \"\n+            f\"{AFFINE_TRANSFORM_INTERPOLATIONS}. Received: \"\n+            f\"interpolation={interpolation}\"\n+        )\n+    if fill_mode not in AFFINE_TRANSFORM_FILL_MODES:\n+        raise ValueError(\n+            \"Invalid value for argument `fill_mode`. Expected of one \"\n+            f\"{AFFINE_TRANSFORM_FILL_MODES}. Received: fill_mode={fill_mode}\"\n+        )\n+    if len(images.shape) not in (3, 4):\n+        raise ValueError(\n+            \"Invalid images rank: expected rank 3 (single image) \"\n+            \"or rank 4 (batch of images). Received input with shape: \"\n+            f\"images.shape={images.shape}\"\n+        )\n+\n+    images = convert_to_tensor(images)\n+    input_dtype = images.dtype\n+\n+    alpha = convert_to_tensor(alpha, dtype=input_dtype)\n+    sigma = convert_to_tensor(sigma, dtype=input_dtype)\n+    kernel_size = (int(6 * sigma) | 1, int(6 * sigma) | 1)\n+\n+    need_squeeze = False\n+    if len(images.shape) == 3:\n+        images = tf.expand_dims(images, axis=0)\n+        need_squeeze = True\n+\n+    if data_format == \"channels_last\":\n+        batch_size, height, width, channels = images.shape\n+        channel_axis = -1\n+    else:\n+        batch_size, channels, height, width = images.shape\n+        channel_axis = 1\n+\n+    seed = draw_seed(seed)\n+    dx = tf.random.stateless_normal(\n+        shape=(batch_size, height, width),\n+        mean=0.0,\n+        stddev=1.0,\n+        dtype=input_dtype,\n+        seed=seed,\n+    )\n+    dy = tf.random.stateless_normal(\n+        shape=(batch_size, height, width),\n+        mean=0.0,\n+        stddev=1.0,\n+        dtype=input_dtype,\n+        seed=seed,\n+    )\n+\n+    dx = gaussian_blur(\n+        tf.expand_dims(dx, axis=channel_axis),\n+        kernel_size=kernel_size,\n+        sigma=(sigma, sigma),\n+        data_format=data_format,\n+    )\n+    dy = gaussian_blur(\n+        tf.expand_dims(dy, axis=channel_axis),\n+        kernel_size=kernel_size,\n+        sigma=(sigma, sigma),\n+        data_format=data_format,\n+    )\n+\n+    dx = tf.squeeze(dx, axis=channel_axis)\n+    dy = tf.squeeze(dy, axis=channel_axis)\n+\n+    x, y = tf.meshgrid(\n+        tf.range(width, dtype=input_dtype),\n+        tf.range(height, dtype=input_dtype),\n+        indexing=\"xy\",\n+    )\n+    x = tf.expand_dims(x, axis=0)\n+    y = tf.expand_dims(y, axis=0)\n+\n+    distorted_x = x + alpha * dx\n+    distorted_y = y + alpha * dy\n+\n+    channel_outputs = []\n+    if data_format == \"channels_last\":\n+        for i in range(channels):\n+            channel_transformed = tf.stack(\n+                [\n+                    map_coordinates(\n+                        images[b, ..., i],\n+                        [distorted_y[b], distorted_x[b]],\n+                        order=AFFINE_TRANSFORM_INTERPOLATIONS.index(\n+                            interpolation\n+                        ),\n+                        fill_mode=fill_mode,\n+                        fill_value=fill_value,\n+                    )\n+                    for b in range(batch_size)\n+                ],\n+                axis=0,\n+            )\n+            channel_outputs.append(channel_transformed)\n+        transformed_images = tf.stack(channel_outputs, axis=-1)\n+    else:\n+        for i in range(channels):\n+            channel_transformed = tf.stack(\n+                [\n+                    map_coordinates(\n+                        images[b, i, ...],\n+                        [distorted_y[b], distorted_x[b]],\n+                        order=AFFINE_TRANSFORM_INTERPOLATIONS.index(\n+                            interpolation\n+                        ),\n+                        fill_mode=fill_mode,\n+                        fill_value=fill_value,\n+                    )\n+                    for b in range(batch_size)\n+                ],\n+                axis=0,\n+            )\n+            channel_outputs.append(channel_transformed)\n+        transformed_images = tf.stack(channel_outputs, axis=1)\n+\n+    if need_squeeze:\n+        transformed_images = tf.squeeze(transformed_images, axis=0)\n+    transformed_images = tf.cast(transformed_images, input_dtype)\n+\n+    return transformed_images\n\n@@ -3,10 +3,13 @@ import itertools\n import operator\n \n import torch\n+import torch._dynamo as dynamo\n import torch.nn.functional as F\n \n from keras.src import backend\n from keras.src.backend.torch.core import convert_to_tensor\n+from keras.src.backend.torch.core import get_device\n+from keras.src.random.seed_generator import draw_seed\n \n RESIZE_INTERPOLATIONS = {\n     \"bilinear\": \"bilinear\",\n@@ -833,7 +836,6 @@ def gaussian_blur(\n             return kernel1d / torch.sum(kernel1d)\n \n         def _get_gaussian_kernel2d(size, sigma):\n-            size = torch.tensor(size, dtype=dtype)\n             kernel1d_x = _get_gaussian_kernel1d(size[0], sigma[0])\n             kernel1d_y = _get_gaussian_kernel1d(size[1], sigma[1])\n             return torch.outer(kernel1d_y, kernel1d_x)\n@@ -868,8 +870,6 @@ def gaussian_blur(\n \n     kernel = kernel.expand(num_channels, 1, kernel_size[0], kernel_size[1])\n \n-    print(kernel_size[0] // 2)\n-\n     blurred_images = torch.nn.functional.conv2d(\n         images,\n         kernel,\n@@ -885,3 +885,148 @@ def gaussian_blur(\n         blurred_images = blurred_images.squeeze(dim=0)\n \n     return blurred_images\n+\n+\n+@dynamo.disable()\n+def torch_seed_generator(seed):\n+    first_seed, second_seed = draw_seed(seed)\n+    device = get_device()\n+    if device == \"meta\":\n+        return None\n+    generator = torch.Generator(device=get_device())\n+    generator.manual_seed(int(first_seed + second_seed))\n+    return generator\n+\n+\n+def elastic_transform(\n+    images,\n+    alpha=20.0,\n+    sigma=5.0,\n+    interpolation=\"bilinear\",\n+    fill_mode=\"reflect\",\n+    fill_value=0.0,\n+    seed=None,\n+    data_format=None,\n+):\n+    data_format = backend.standardize_data_format(data_format)\n+    if interpolation not in AFFINE_TRANSFORM_INTERPOLATIONS.keys():\n+        raise ValueError(\n+            \"Invalid value for argument `interpolation`. Expected of one \"\n+            f\"{set(AFFINE_TRANSFORM_INTERPOLATIONS.keys())}. Received: \"\n+            f\"interpolation={interpolation}\"\n+        )\n+    if fill_mode not in AFFINE_TRANSFORM_FILL_MODES:\n+        raise ValueError(\n+            \"Invalid value for argument `fill_mode`. Expected of one \"\n+            f\"{AFFINE_TRANSFORM_FILL_MODES}. Received: fill_mode={fill_mode}\"\n+        )\n+    if len(images.shape) not in (3, 4):\n+        raise ValueError(\n+            \"Invalid images rank: expected rank 3 (single image) \"\n+            \"or rank 4 (batch of images). Received input with shape: \"\n+            f\"images.shape={images.shape}\"\n+        )\n+\n+    images = convert_to_tensor(images)\n+    alpha = convert_to_tensor(alpha)\n+    sigma = convert_to_tensor(sigma)\n+    input_dtype = images.dtype\n+    kernel_size = (int(6 * sigma) | 1, int(6 * sigma) | 1)\n+\n+    need_squeeze = False\n+    if images.ndim == 3:\n+        images = images.unsqueeze(dim=0)\n+        need_squeeze = True\n+\n+    if data_format == \"channels_last\":\n+        batch_size, height, width, channels = images.shape\n+        channel_axis = -1\n+    else:\n+        batch_size, channels, height, width = images.shape\n+        channel_axis = 1\n+\n+    generator = torch_seed_generator(seed) if get_device() == \"meta\" else None\n+    dx = (\n+        torch.normal(\n+            0.0,\n+            1.0,\n+            size=(batch_size, height, width),\n+            generator=generator,\n+            dtype=input_dtype,\n+            device=images.device,\n+        )\n+        * sigma\n+    )\n+\n+    dy = (\n+        torch.normal(\n+            0.0,\n+            1.0,\n+            size=(batch_size, height, width),\n+            generator=generator,\n+            dtype=input_dtype,\n+            device=images.device,\n+        )\n+        * sigma\n+    )\n+\n+    dx = gaussian_blur(\n+        dx.unsqueeze(dim=channel_axis),\n+        kernel_size=kernel_size,\n+        sigma=(sigma, sigma),\n+        data_format=data_format,\n+    )\n+    dy = gaussian_blur(\n+        dy.unsqueeze(dim=channel_axis),\n+        kernel_size=kernel_size,\n+        sigma=(sigma, sigma),\n+        data_format=data_format,\n+    )\n+\n+    dx = dx.squeeze()\n+    dy = dy.squeeze()\n+\n+    x, y = torch.meshgrid(\n+        torch.arange(width), torch.arange(height), indexing=\"xy\"\n+    )\n+    x, y = x.unsqueeze(0), y.unsqueeze(0)\n+\n+    distorted_x = x + alpha * dx\n+    distorted_y = y + alpha * dy\n+\n+    transformed_images = torch.zeros_like(images)\n+\n+    if data_format == \"channels_last\":\n+        for i in range(channels):\n+            transformed_images[..., i] = torch.stack(\n+                [\n+                    map_coordinates(\n+                        images[b, ..., i],\n+                        [distorted_y[b], distorted_x[b]],\n+                        order=AFFINE_TRANSFORM_INTERPOLATIONS[interpolation],\n+                        fill_mode=fill_mode,\n+                        fill_value=fill_value,\n+                    )\n+                    for b in range(batch_size)\n+                ]\n+            )\n+    else:\n+        for i in range(channels):\n+            transformed_images[:, i, :, :] = torch.stack(\n+                [\n+                    map_coordinates(\n+                        images[b, i, ...],\n+                        [distorted_y[b], distorted_x[b]],\n+                        order=AFFINE_TRANSFORM_INTERPOLATIONS[interpolation],\n+                        fill_mode=fill_mode,\n+                        fill_value=fill_value,\n+                    )\n+                    for b in range(batch_size)\n+                ]\n+            )\n+\n+    if need_squeeze:\n+        transformed_images = transformed_images.squeeze(0)\n+    transformed_images = transformed_images.to(input_dtype)\n+\n+    return transformed_images\n\n@@ -1458,3 +1458,132 @@ def gaussian_blur(\n         sigma=sigma,\n         data_format=data_format,\n     )\n+\n+\n+class ElasticTransform(Operation):\n+    def __init__(\n+        self,\n+        alpha=20.0,\n+        sigma=5.0,\n+        interpolation=\"bilinear\",\n+        fill_mode=\"reflect\",\n+        fill_value=0.0,\n+        seed=None,\n+        data_format=None,\n+    ):\n+        super().__init__()\n+        self.alpha = alpha\n+        self.sigma = sigma\n+        self.interpolation = interpolation\n+        self.fill_mode = fill_mode\n+        self.fill_value = fill_value\n+        self.seed = seed\n+        self.data_format = backend.standardize_data_format(data_format)\n+\n+    def call(self, images):\n+        return backend.image.elastic_transform(\n+            images,\n+            alpha=self.alpha,\n+            sigma=self.sigma,\n+            interpolation=self.interpolation,\n+            fill_mode=self.fill_mode,\n+            fill_value=self.fill_value,\n+            seed=self.seed,\n+            data_format=self.data_format,\n+        )\n+\n+    def compute_output_spec(self, images):\n+        if len(images.shape) not in (3, 4):\n+            raise ValueError(\n+                \"Invalid images rank: expected rank 3 (single image) \"\n+                \"or rank 4 (batch of images). Received input with shape: \"\n+                f\"images.shape={images.shape}\"\n+            )\n+        return KerasTensor(images.shape, dtype=images.dtype)\n+\n+\n+@keras_export(\"keras.ops.image.elastic_transform\")\n+def elastic_transform(\n+    images,\n+    alpha=20.0,\n+    sigma=5.0,\n+    interpolation=\"bilinear\",\n+    fill_mode=\"reflect\",\n+    fill_value=0.0,\n+    seed=None,\n+    data_format=None,\n+):\n+    \"\"\"Applies elastic deformation to the image(s).\n+\n+    Args:\n+        images: Input image or batch of images. Must be 3D or 4D.\n+        alpha: Scaling factor that controls the intensity of the deformation.\n+        sigma: Standard deviation of the Gaussian filter used for\n+            smoothing the displacement fields.\n+        interpolation: Interpolation method. Available methods are `\"nearest\"`,\n+            and `\"bilinear\"`. Defaults to `\"bilinear\"`.\n+        fill_mode: Points outside the boundaries of the input are filled\n+            according to the given mode. Available methods are `\"constant\"`,\n+            `\"nearest\"`, `\"wrap\"` and `\"reflect\"`. Defaults to `\"constant\"`.\n+            - `\"reflect\"`: `(d c b a | a b c d | d c b a)`\n+                The input is extended by reflecting about the edge of the last\n+                pixel.\n+            - `\"constant\"`: `(k k k k | a b c d | k k k k)`\n+                The input is extended by filling all values beyond\n+                the edge with the same constant value k specified by\n+                `fill_value`.\n+            - `\"wrap\"`: `(a b c d | a b c d | a b c d)`\n+                The input is extended by wrapping around to the opposite edge.\n+            - `\"nearest\"`: `(a a a a | a b c d | d d d d)`\n+                The input is extended by the nearest pixel.\n+        fill_value: Value used for points outside the boundaries of the input if\n+            `fill_mode=\"constant\"`. Defaults to `0`.\n+        data_format: A string specifying the data format of the input tensor.\n+            It can be either `\"channels_last\"` or `\"channels_first\"`.\n+            `\"channels_last\"` corresponds to inputs with shape\n+            `(batch, height, width, channels)`, while `\"channels_first\"`\n+            corresponds to inputs with shape `(batch, channels, height, width)`.\n+            If not specified, the value will default to\n+            `keras.config.image_data_format`.\n+\n+    Returns:\n+        Transformed image or batch of images with elastic deformation.\n+\n+    Examples:\n+\n+    >>> x = np.random.random((2, 64, 80, 3))  # batch of 2 RGB images\n+    >>> y = keras.ops.image.elastic_transform(x)\n+    >>> y.shape\n+    (2, 64, 80, 3)\n+\n+    >>> x = np.random.random((64, 80, 3))  # single RGB image\n+    >>> y = keras.ops.image.elastic_transform(x)\n+    >>> y.shape\n+    (64, 80, 3)\n+\n+    >>> x = np.random.random((2, 3, 64, 80))  # batch of 2 RGB images\n+    >>> y = keras.ops.image.elastic_transform(\n+    ...     x, data_format=\"channels_first\")\n+    >>> y.shape\n+    (2, 3, 64, 80)\n+    \"\"\"\n+    if any_symbolic_tensors((images,)):\n+        return ElasticTransform(\n+            alpha=alpha,\n+            sigma=sigma,\n+            interpolation=interpolation,\n+            fill_mode=fill_mode,\n+            fill_value=fill_value,\n+            seed=seed,\n+            data_format=data_format,\n+        ).symbolic_call(images)\n+    return backend.image.elastic_transform(\n+        images,\n+        alpha=alpha,\n+        sigma=sigma,\n+        interpolation=interpolation,\n+        fill_mode=fill_mode,\n+        fill_value=fill_value,\n+        seed=seed,\n+        data_format=data_format,\n+    )\n\n@@ -191,6 +191,18 @@ class ImageOpsDynamicShapeTest(testing.TestCase):\n         out = kimage.gaussian_blur(x)\n         self.assertEqual(out.shape, (None, 3, 20, 20))\n \n+    def test_elastic_transform(self):\n+        # Test channels_last\n+        x = KerasTensor([None, 20, 20, 3])\n+        out = kimage.elastic_transform(x)\n+        self.assertEqual(out.shape, (None, 20, 20, 3))\n+\n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = KerasTensor([None, 3, 20, 20])\n+        out = kimage.elastic_transform(x)\n+        self.assertEqual(out.shape, (None, 3, 20, 20))\n+\n \n class ImageOpsStaticShapeTest(testing.TestCase):\n     def setUp(self):\n@@ -438,6 +450,18 @@ class ImageOpsStaticShapeTest(testing.TestCase):\n         out = kimage.gaussian_blur(x, kernel_size, sigma)\n         self.assertEqual(out.shape, (3, 20, 20))\n \n+    def test_elastic_transform(self):\n+        # Test channels_last\n+        x = KerasTensor([20, 20, 3])\n+        out = kimage.elastic_transform(x)\n+        self.assertEqual(out.shape, (20, 20, 3))\n+\n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = KerasTensor([3, 20, 20])\n+        out = kimage.elastic_transform(x)\n+        self.assertEqual(out.shape, (3, 20, 20))\n+\n \n AFFINE_TRANSFORM_INTERPOLATIONS = {  # map to order\n     \"nearest\": 0,\n@@ -2031,3 +2055,35 @@ class ImageOpsBehaviorTests(testing.TestCase):\n             kimage.gaussian_blur(\n                 invalid_image, kernel_size=kernel_size, sigma=sigma\n             )\n+\n+    def test_elastic_transform_invalid_images_rank(self):\n+        # Test rank=2\n+        invalid_image = np.random.uniform(size=(10, 10))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.elastic_transform(\n+                invalid_image,\n+            )\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.ElasticTransform()(invalid_image)\n+\n+        # Test rank=5\n+        invalid_image = np.random.uniform(size=(2, 10, 10, 3, 1))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.elastic_transform(invalid_image)\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.ElasticTransform()(invalid_image)\n+\n+        # Test rank=2, symbolic tensor\n+        invalid_image = KerasTensor(shape=(10, 10))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.elastic_transform(invalid_image)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
