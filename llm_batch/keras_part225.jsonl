{"custom_id": "keras#12a79c863e41290343d897ded71e8d7f8df456bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 7 | Churn Cumulative: 6079 | Contributors (this commit): 78 | Commits (past 90d): 34 | Contributors (cumulative): 78 | DMM Complexity: None\n\nDIFF:\n@@ -8,10 +8,6 @@ from __future__ import print_function\n import warnings\n import copy\n import numpy as np\n-\n-from keras.utils import Sequence\n-from keras.utils import GeneratorEnqueuer\n-from keras.utils import OrderedEnqueuer\n from scipy.sparse import issparse\n \n try:\n@@ -24,6 +20,9 @@ from .. import backend as K\n from .. import optimizers\n from .. import losses\n from .. import metrics as metrics_module\n+from ..utils.data_utils import Sequence\n+from ..utils.data_utils import GeneratorEnqueuer\n+from ..utils.data_utils import OrderedEnqueuer\n from ..utils.generic_utils import Progbar\n from .. import callbacks as cbks\n from ..legacy import interfaces\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c67adf1765d600737b0606fd3fde48045413dee4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 88 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,6 @@ from .utils.generic_utils import deserialize_keras_object\n from .utils.generic_utils import serialize_keras_object\n \n \n-# noinspection SpellCheckingInspection\n def mean_squared_error(y_true, y_pred):\n     return K.mean(K.square(y_pred - y_true), axis=-1)\n \n@@ -53,6 +52,13 @@ def logcosh(y_true, y_pred):\n     to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\n     like the mean squared error, but will not be so strongly affected by the\n     occasional wildly incorrect prediction.\n+\n+    # Arguments\n+        y_true: tensor of true targets.\n+        y_pred: tensor of predicted targets.\n+\n+    # Returns\n+        Tensor with one scalar loss entry per sample.\n     \"\"\"\n     def _logcosh(x):\n         return x + K.softplus(-2. * x) - K.log(2.)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#091685954504fb46cd77f6f40bd5a780bddbb06f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 531 | Contributors (this commit): 13 | Commits (past 90d): 3 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -55,7 +55,17 @@ def one_hot(text, n,\n     \"\"\"One-hot encodes a text into a list of word indexes of size n.\n \n     This is a wrapper to the `hashing_trick` function using `hash` as the\n-    hashing function, unicity of word to index mapping non-guaranteed.\n+    hashing function; unicity of word to index mapping non-guaranteed.\n+\n+    # Arguments\n+        text: Input text (string).\n+        n: Dimension of the hashing space.\n+        filters: Sequence of characters to filter out.\n+        lower: Whether to convert the input to lowercase.\n+        split: Sentence split marker (string).\n+\n+    # Returns\n+        A list of integer word indices (unicity non-guaranteed).\n     \"\"\"\n     return hashing_trick(text, n,\n                          hash_function=hash,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2b349696e6845fff9903e06f24c32f122fc01e0b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 17 | Methods Changed: 6 | Complexity Δ (Sum/Max): 13/5 | Churn Δ: 47 | Churn Cumulative: 15279 | Contributors (this commit): 88 | Commits (past 90d): 14 | Contributors (cumulative): 121 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,5 @@\n # -*- coding: utf-8 -*-\n-\"\"\"Advanced activation layers.\n+\"\"\"Layers that act as activation functions.\n \"\"\"\n from __future__ import absolute_import\n from __future__ import division\n@@ -50,6 +50,9 @@ class LeakyReLU(Layer):\n         base_config = super(LeakyReLU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n \n class PReLU(Layer):\n     \"\"\"Parametric Rectified Linear Unit.\n@@ -142,6 +145,9 @@ class PReLU(Layer):\n         base_config = super(PReLU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n \n class ELU(Layer):\n     \"\"\"Exponential Linear Unit.\n@@ -178,6 +184,9 @@ class ELU(Layer):\n         base_config = super(ELU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n \n class ThresholdedReLU(Layer):\n     \"\"\"Thresholded Rectified Linear Unit.\n@@ -214,6 +223,9 @@ class ThresholdedReLU(Layer):\n         base_config = super(ThresholdedReLU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n \n class Softmax(Layer):\n     \"\"\"Softmax activation function.\n@@ -242,3 +254,6 @@ class Softmax(Layer):\n         config = {'axis': self.axis}\n         base_config = super(Softmax, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n\n@@ -71,6 +71,9 @@ class Masking(Layer):\n         base_config = super(Masking, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n \n class Dropout(Layer):\n     \"\"\"Applies Dropout to the input.\n@@ -127,6 +130,9 @@ class Dropout(Layer):\n         base_config = super(Dropout, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n \n class SpatialDropout1D(Dropout):\n     \"\"\"Spatial 1D version of Dropout.\n@@ -301,6 +307,9 @@ class Activation(Layer):\n         base_config = super(Activation, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n \n class Reshape(Layer):\n     \"\"\"Reshapes an output to a certain shape.\n@@ -902,3 +911,6 @@ class ActivityRegularization(Layer):\n                   'l2': self.l2}\n         base_config = super(ActivityRegularization, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n\n@@ -1,5 +1,5 @@\n # -*- coding: utf-8 -*-\n-\"\"\"Noise regularization layers.\n+\"\"\"Layers that operate regularization via the addition of noise.\n \"\"\"\n from __future__ import absolute_import\n from __future__ import division\n@@ -9,7 +9,6 @@ from ..engine import Layer\n from .. import backend as K\n import numpy as np\n from ..legacy import interfaces\n-from ..engine import InputSpec\n \n \n class GaussianNoise(Layer):\n@@ -52,6 +51,9 @@ class GaussianNoise(Layer):\n         base_config = super(GaussianNoise, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n \n class GaussianDropout(Layer):\n     \"\"\"Apply multiplicative 1-centered Gaussian noise.\n@@ -96,6 +98,9 @@ class GaussianDropout(Layer):\n         base_config = super(GaussianDropout, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n \n class AlphaDropout(Layer):\n     \"\"\"Applies Alpha Dropout to the input.\n@@ -142,7 +147,8 @@ class AlphaDropout(Layer):\n                 scale = 1.0507009873554804934193349852946\n                 alpha_p = -alpha * scale\n \n-                kept_idx = K.greater_equal(K.random_uniform(noise_shape, seed=seed), rate)\n+                kept_idx = K.greater_equal(K.random_uniform(noise_shape,\n+                                                            seed=seed), rate)\n                 kept_idx = K.cast(kept_idx, K.floatx())\n \n                 # Get affine transformation params\n@@ -162,3 +168,6 @@ class AlphaDropout(Layer):\n         config = {'rate': self.rate}\n         base_config = super(AlphaDropout, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n\n@@ -211,3 +211,6 @@ class BatchNormalization(Layer):\n         }\n         base_config = super(BatchNormalization, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c476792e366626bec3b5a44b642c315239311516", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2727 | Contributors (this commit): 45 | Commits (past 90d): 15 | Contributors (cumulative): 45 | DMM Complexity: None\n\nDIFF:\n@@ -946,6 +946,7 @@ def _count_valid_files_in_directory(directory, white_list_formats, follow_links)\n         directory: absolute path to the directory containing files to be counted\n         white_list_formats: set of strings containing allowed extensions for\n             the files to be counted.\n+        follow_links: boolean.\n \n     # Returns\n         the count of files with extension in `white_list_formats` contained in\n@@ -969,7 +970,7 @@ def _count_valid_files_in_directory(directory, white_list_formats, follow_links)\n \n def _list_valid_filenames_in_directory(directory, white_list_formats,\n                                        class_indices, follow_links):\n-    \"\"\"List paths of files in `subdir` relative from `directory` whose extensions are in `white_list_formats`.\n+    \"\"\"List paths of files in `subdir` with extensions in `white_list_formats`.\n \n     # Arguments\n         directory: absolute path to a directory containing the files to list.\n@@ -977,6 +978,7 @@ def _list_valid_filenames_in_directory(directory, white_list_formats,\n         white_list_formats: set of strings containing allowed extensions for\n             the files to be counted.\n         class_indices: dictionary mapping a class name to its index.\n+        follow_links: boolean.\n \n     # Returns\n         classes: a list of class indices\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eac78b859beb31cafa65a3edb4eaa888d3b6c2e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 15225 | Contributors (this commit): 117 | Commits (past 90d): 52 | Contributors (cumulative): 126 | DMM Complexity: None\n\nDIFF:\n@@ -10,11 +10,6 @@ import copy\n import numpy as np\n from scipy.sparse import issparse\n \n-try:\n-    import queue\n-except ImportError:\n-    import Queue as queue\n-\n from .topology import Container\n from .. import backend as K\n from .. import optimizers\n\n@@ -1726,7 +1726,7 @@ class LSTMCell(Layer):\n \n         if self.use_bias:\n             if self.unit_forget_bias:\n-                def bias_initializer(shape, *args, **kwargs):\n+                def bias_initializer(_, *args, **kwargs):\n                     return K.concatenate([\n                         self.bias_initializer((self.units,), *args, **kwargs),\n                         initializers.Ones()((self.units,), *args, **kwargs),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f097f6956d757fdc643bf14ce23cbcfa62f4831a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 13 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 40 | Churn Cumulative: 717 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -42,8 +42,8 @@ subtract_pixel_mean = True\n # ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n # ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n # ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n-# ResNet56  | 9 (6)| 92.71     | 93.03     | 92.60     | NA        | 90 (100)\n-# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.03     | 93.63     | 165(180)\n+# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n+# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n # ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n # ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n # ---------------------------------------------------------------------------\n@@ -118,6 +118,7 @@ def resnet_block(inputs,\n                  kernel_size=3,\n                  strides=1,\n                  activation='relu',\n+                 batch_normalization=True,\n                  conv_first=True):\n     \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n \n@@ -127,24 +128,28 @@ def resnet_block(inputs,\n         kernel_size (int): Conv2D square kernel dimensions\n         strides (int): Conv2D square stride dimensions\n         activation (string): activation name\n+        batch_normalization (bool): whether to include batch normalization\n         conv_first (bool): conv-bn-activation (True) or\n             activation-bn-conv (False)\n \n     # Returns\n         x (tensor): tensor as input to the next layer\n     \"\"\"\n+    x = inputs\n     if conv_first:\n         x = Conv2D(num_filters,\n                    kernel_size=kernel_size,\n                    strides=strides,\n                    padding='same',\n                    kernel_initializer='he_normal',\n-                   kernel_regularizer=l2(1e-4))(inputs)\n+                   kernel_regularizer=l2(1e-4))(x)\n+        if batch_normalization:\n             x = BatchNormalization()(x)\n         if activation:\n             x = Activation(activation)(x)\n         return x\n-    x = BatchNormalization()(inputs)\n+    if batch_normalization:\n+        x = BatchNormalization()(x)\n     if activation:\n         x = Activation('relu')(x)\n     x = Conv2D(num_filters,\n@@ -204,7 +209,8 @@ def resnet_v1(input_shape, depth, num_classes=10):\n                                  num_filters=num_filters,\n                                  kernel_size=1,\n                                  strides=strides,\n-                                 activation=None)\n+                                 activation=None,\n+                                 batch_normalization=False)\n             x = keras.layers.add([x, y])\n             x = Activation('relu')(x)\n         num_filters = 2 * num_filters\n@@ -248,14 +254,14 @@ def resnet_v2(input_shape, depth, num_classes=10):\n     filter_multiplier = 4\n     num_sub_blocks = int((depth - 2) / 9)\n \n-    # v2 performs Conv2D on input w/o BN-ReLU\n-    x = Conv2D(num_filters_in,\n-               kernel_size=3,\n-               padding='same',\n-               kernel_initializer='he_normal',\n-               kernel_regularizer=l2(1e-4))(inputs)\n+    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n+    x = resnet_block(inputs=inputs,\n+                     num_filters=num_filters_in,\n+                     conv_first=True)\n \n     # Instantiate convolutional base (stack of blocks).\n+    activation = None\n+    batch_normalization = False\n     for i in range(3):\n         if i > 0:\n             filter_multiplier = 2\n@@ -270,7 +276,11 @@ def resnet_v2(input_shape, depth, num_classes=10):\n                              num_filters=num_filters_in,\n                              kernel_size=1,\n                              strides=strides,\n+                             activation=activation,\n+                             batch_normalization=batch_normalization,\n                              conv_first=False)\n+            activation = 'relu'\n+            batch_normalization = True\n             y = resnet_block(inputs=y,\n                              num_filters=num_filters_in,\n                              conv_first=False)\n@@ -279,12 +289,12 @@ def resnet_v2(input_shape, depth, num_classes=10):\n                              kernel_size=1,\n                              conv_first=False)\n             if j == 0:\n-                x = Conv2D(num_filters_out,\n+                x = resnet_block(inputs=x,\n+                                 num_filters=num_filters_out,\n                                  kernel_size=1,\n                                  strides=strides,\n-                           padding='same',\n-                           kernel_initializer='he_normal',\n-                           kernel_regularizer=l2(1e-4))(x)\n+                                 activation=None,\n+                                 batch_normalization=False)\n             x = keras.layers.add([x, y])\n \n         num_filters_in = num_filters_out\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b187ac51e0c9ae6fab4aabfb12ceed57c83b12ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 140 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 140 | Churn Cumulative: 140 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,140 @@\n+'''Trains a denoising autoenconder on MNIST dataset.\n+\n+Denoising is one of the classic applications of autoencoders.\n+The denoising process removes unwanted noise that corrupted the\n+true signal.\n+\n+Noise + Data ---> Denoising Autoencoder ---> Data\n+\n+Given a training dataset of corrupted data as input and\n+true signal as output, a denoising autoencoder can recover the\n+hidden structure to generate clean data.\n+\n+This example has modular design. The encoder, decoder and autoencoder\n+are 3 models that share weights. For example, after training the\n+autoencoder, the encoder can be used to  generate latent vectors\n+of input data for low-dim visualization like PCA or TSNE.\n+'''\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+import keras\n+from keras.layers import Activation, Dense, Input\n+from keras.layers import Conv2D, Flatten\n+from keras.layers import Reshape, Conv2DTranspose\n+from keras.models import Model\n+from keras import backend as K\n+from keras.datasets import mnist\n+import numpy as np\n+import matplotlib.pyplot as plt\n+\n+# MNIST dataset\n+(x_train, _), (x_test, _) = mnist.load_data()\n+\n+image_size = x_train.shape[1]\n+x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n+x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n+x_train = x_train.astype('float32') / 255\n+x_test = x_test.astype('float32') / 255\n+\n+# Generate corrupted MNIST images by adding noise with normal dist\n+# centered at 0.5 and std=0.5\n+noise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape)\n+x_train_noisy = x_train + noise\n+noise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape)\n+x_test_noisy = x_test + noise\n+\n+x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n+x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n+\n+# Network parameters\n+input_shape = (image_size, image_size, 1)\n+batch_size = 128\n+kernel_size = 3\n+latent_dim = 16\n+# Encoder/Decoder number of CNN layers and filters per layer\n+layer_filters = [32, 64]\n+\n+# Build the Autoencoder Model\n+# First build the Encoder Model\n+inputs = Input(shape=input_shape, name='encoder_input')\n+x = inputs\n+# Stack of Conv2D blocks\n+# Notes:\n+# 1) Use Batch Normalization before ReLU on deep networks\n+# 2) Use MaxPooling2D as alternative to strides>1\n+# - faster but not as good as strides>1\n+for filters in layer_filters:\n+    x = Conv2D(filters=filters,\n+               kernel_size=kernel_size,\n+               strides=2,\n+               activation='relu',\n+               padding='same')(x)\n+\n+# Shape info needed to build Decoder Model\n+shape = K.int_shape(x)\n+\n+# Generate the latent vector\n+x = Flatten()(x)\n+latent = Dense(latent_dim, name='latent_vector')(x)\n+\n+# Instantiate Encoder Model\n+encoder = Model(inputs, latent, name='encoder')\n+encoder.summary()\n+\n+# Build the Decoder Model\n+latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n+x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n+x = Reshape((shape[1], shape[2], shape[3]))(x)\n+\n+# Stack of Transposed Conv2D blocks\n+# Notes:\n+# 1) Use Batch Normalization before ReLU on deep networks\n+# 2) Use UpSampling2D as alternative to strides>1\n+# - faster but not as good as strides>1\n+for filters in layer_filters[::-1]:\n+    x = Conv2DTranspose(filters=filters,\n+                        kernel_size=kernel_size,\n+                        strides=2,\n+                        activation='relu',\n+                        padding='same')(x)\n+\n+x = Conv2DTranspose(filters=1,\n+                    kernel_size=kernel_size,\n+                    padding='same')(x)\n+\n+outputs = Activation('sigmoid', name='decoder_output')(x)\n+\n+# Instantiate Decoder Model\n+decoder = Model(latent_inputs, outputs, name='decoder')\n+decoder.summary()\n+\n+# Autoencoder = Encoder + Decoder\n+# Instantiate Autoencoder Model\n+autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\n+autoencoder.summary()\n+\n+# Mean Square Error (MSE) loss function, Adam optimizer\n+autoencoder.compile(loss='mse', optimizer='adam')\n+\n+# Train the autoencoder\n+autoencoder.fit(x_train_noisy,\n+                x_train,\n+                validation_data=(x_test_noisy, x_test),\n+                epochs=10,\n+                batch_size=batch_size)\n+\n+# Predict the Autoencoder output from corrupted test images\n+x_decoded = autoencoder.predict(x_test_noisy)\n+\n+# Display the 1st 8 corrupted and denoised images\n+imgs = np.concatenate([x_test_noisy[:8], x_decoded[:8]])\n+imgs = imgs.reshape((4, 4, image_size, image_size))\n+imgs = np.vstack([np.hstack(i) for i in imgs])\n+plt.figure()\n+plt.axis('off')\n+plt.title('Corrupted Input: top 2 rows, Output is Denoised Input: last 2 rows')\n+plt.imshow(imgs, interpolation='none', cmap='gray')\n+plt.savefig('corrupted_and_denoised.png')\n+plt.show()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d72c2b383e8fb5b13fe710a3343e54cb93d18eef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 1 | Churn Cumulative: 5411 | Contributors (this commit): 80 | Commits (past 90d): 5 | Contributors (cumulative): 80 | DMM Complexity: 1.0\n\nDIFF:\n@@ -846,7 +846,6 @@ def concatenate(tensors, axis=-1):\n def reshape(x, shape):\n     y = T.reshape(x, shape)\n     if _is_explicit_shape(shape):\n-        if -1 in shape:\n         shape = tuple(x if x != -1 else None for x in shape)\n         y._keras_shape = shape\n         if hasattr(x, '_uses_learning_phase'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab97958e0b1b9ed224d5475fb6ccb8f40242794b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 62 | Churn Cumulative: 7719 | Contributors (this commit): 82 | Commits (past 90d): 46 | Contributors (cumulative): 101 | DMM Complexity: 0.5365853658536586\n\nDIFF:\n@@ -2066,7 +2066,27 @@ class Model(Container):\n         })\n         callbacks.on_train_begin()\n \n-        if do_validation and not val_gen:\n+        enqueuer = None\n+        val_enqueuer = None\n+\n+        try:\n+            if do_validation:\n+                if val_gen:\n+                    if workers > 0:\n+                        if isinstance(validation_data, Sequence):\n+                            val_enqueuer = OrderedEnqueuer(validation_data,\n+                                                           use_multiprocessing=use_multiprocessing)\n+                            if validation_steps is None:\n+                                validation_steps = len(validation_data)\n+                        else:\n+                            val_enqueuer = GeneratorEnqueuer(validation_data,\n+                                                             use_multiprocessing=use_multiprocessing,\n+                                                             wait_time=wait_time)\n+                        val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n+                        validation_generator = val_enqueuer.get()\n+                    else:\n+                        validation_generator = validation_data\n+                else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data\n                         val_sample_weight = None\n@@ -2084,9 +2104,7 @@ class Model(Container):\n                         val_data += [0.]\n                     for cbk in callbacks:\n                         cbk.validation_data = val_data\n-        enqueuer = None\n \n-        try:\n             if workers > 0:\n                 if is_sequence:\n                     enqueuer = OrderedEnqueuer(generator,\n@@ -2157,11 +2175,9 @@ class Model(Container):\n                     if steps_done >= steps_per_epoch and do_validation:\n                         if val_gen:\n                             val_outs = self.evaluate_generator(\n-                                validation_data,\n+                                validation_generator,\n                                 validation_steps,\n-                                max_queue_size=max_queue_size,\n-                                workers=workers,\n-                                use_multiprocessing=use_multiprocessing)\n+                                workers=0)\n                         else:\n                             # No need for try/except because\n                             # data has already been validated.\n@@ -2185,8 +2201,12 @@ class Model(Container):\n                     break\n \n         finally:\n+            try:\n                 if enqueuer is not None:\n                     enqueuer.stop()\n+            finally:\n+                if val_enqueuer is not None:\n+                    val_enqueuer.stop()\n \n         callbacks.on_train_end()\n         return self.history\n\n@@ -411,6 +411,34 @@ def test_model_methods():\n                                   initial_epoch=0, validation_data=gen_data(),\n                                   callbacks=[tracker_cb])\n \n+    # Check if generator is only accessed an expected number of times\n+    gen_counters = [0, 0]\n+\n+    def gen_data(i):\n+        while True:\n+            gen_counters[i] += 1\n+            yield ([np.random.random((1, 3)), np.random.random((1, 3))],\n+                   [np.random.random((1, 4)), np.random.random((1, 3))])\n+    out = model.fit_generator(generator=gen_data(0), epochs=3,\n+                              steps_per_epoch=2,\n+                              validation_data=gen_data(1),\n+                              validation_steps=1,\n+                              max_queue_size=2,\n+                              workers=2)\n+\n+    # Need range check here as filling of the queue depends on sleep in the enqueuers\n+    assert 6 <= gen_counters[0] <= 8\n+    assert 3 <= gen_counters[1] <= 5\n+\n+    gen_counters = [0]\n+    out = model.fit_generator(generator=RandomSequence(3), epochs=3,\n+                              validation_data=gen_data(0),\n+                              validation_steps=1,\n+                              max_queue_size=2,\n+                              workers=2)\n+    # Need range check here as filling of the queue depends on sleep in the enqueuers\n+    assert 3 <= gen_counters[0] <= 5\n+\n     # predict_generator output shape behavior should be consistent\n     def expected_shape(batch_size, n_batches):\n         return (batch_size * n_batches, 4), (batch_size * n_batches, 3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3966daa90b4c47d2ad36e977c36a6895f2697dd8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 808 | Contributors (this commit): 12 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -245,7 +245,7 @@ class Subtract(_Merge):\n         if len(inputs) != 2:\n             raise ValueError('`Subtract` layer should be called '\n                              'on exactly 2 inputs')\n-        if inputs[0]._keras_shape != inputs[1]._keras_shape:\n+        if K.int_shape(inputs[0]) != K.int_shape(inputs[1]):\n             raise ValueError('`Subtract` layer should be called '\n                              'on inputs of the same shape')\n         return inputs[0] - inputs[1]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f0872fc27f684885f7507b5c1b7c60450523b5d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 9300 | Contributors (this commit): 101 | Commits (past 90d): 28 | Contributors (cumulative): 101 | DMM Complexity: 1.0\n\nDIFF:\n@@ -123,7 +123,8 @@ def learning_phase():\n     \"\"\"\n     graph = tf.get_default_graph()\n     if graph not in _GRAPH_LEARNING_PHASES:\n-        phase = tf.placeholder(dtype='bool',\n+        phase = tf.placeholder_with_default(False,\n+                                            shape=(),\n                                             name='keras_learning_phase')\n         _GRAPH_LEARNING_PHASES[graph] = phase\n     return _GRAPH_LEARNING_PHASES[graph]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a4c1d44192690a8f677b7c17bf798324883e3f75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 812 | Contributors (this commit): 12 | Commits (past 90d): 6 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -330,9 +330,9 @@ class Concatenate(_Merge):\n \n     def build(self, input_shape):\n         # Used purely for shape validation.\n-        if not isinstance(input_shape, list):\n+        if not isinstance(input_shape, list) or len(input_shape) < 2:\n             raise ValueError('`Concatenate` layer should be called '\n-                             'on a list of inputs')\n+                             'on a list of of at least 2 inputs')\n         if all([shape is None for shape in input_shape]):\n             return\n         reduced_inputs_shapes = [list(shape) for shape in input_shape]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bbc3fcfa542a2489e838b793a9778eac360f5ae4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 273 | Contributors (this commit): 14 | Commits (past 90d): 7 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ from setuptools import find_packages\n \n setup(name='Keras',\n       version='2.1.2',\n-      description='Deep Learning for Python',\n+      description='Deep Learning for humans',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/keras-team/keras',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9a50d7bece2e09093f7eec251fc2be5736b131da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 51 | Churn Cumulative: 750 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -317,8 +317,16 @@ def test_mobilenet_image_size():\n     applications.DenseNet201],\n     ids=['DenseNet121', 'DenseNet169', 'DenseNet201'])\n def test_densenet(fun):\n+    def target(queue):\n         model = fun(weights=None)\n-    assert model.output_shape == (None, 1000)\n+        queue.put(model.output_shape)\n+    queue = Queue()\n+    p = Process(target=target, args=(queue,))\n+    p.start()\n+    p.join()\n+    assert not queue.empty(), 'Model creation failed.'\n+    model_output_shape = queue.get_nowait()\n+    assert model_output_shape == (None, 1000)\n \n \n @keras_test\n@@ -328,8 +336,16 @@ def test_densenet(fun):\n     (applications.DenseNet201, 1920)],\n     ids=['DenseNet121', 'DenseNet169', 'DenseNet201'])\n def test_densenet_no_top(fun, dim):\n+    def target(queue):\n         model = fun(weights=None, include_top=False)\n-    assert model.output_shape == (None, None, None, dim)\n+        queue.put(model.output_shape)\n+    queue = Queue()\n+    p = Process(target=target, args=(queue,))\n+    p.start()\n+    p.join()\n+    assert not queue.empty(), 'Model creation failed.'\n+    model_output_shape = queue.get_nowait()\n+    assert model_output_shape == (None, None, None, dim)\n \n \n @keras_test\n@@ -339,8 +355,16 @@ def test_densenet_no_top(fun, dim):\n     (applications.DenseNet201, 1920)],\n     ids=['DenseNet121', 'DenseNet169', 'DenseNet201'])\n def test_densenet_pooling(fun, dim):\n+    def target(queue):\n         model = fun(weights=None, include_top=False, pooling='avg')\n-    assert model.output_shape == (None, None, None, dim)\n+        queue.put(model.output_shape)\n+    queue = Queue()\n+    p = Process(target=target, args=(queue,))\n+    p.start()\n+    p.join()\n+    assert not queue.empty(), 'Model creation failed.'\n+    model_output_shape = queue.get_nowait()\n+    assert model_output_shape == (None, None, None, dim)\n \n \n @keras_test\n@@ -350,13 +374,24 @@ def test_densenet_pooling(fun, dim):\n     (applications.DenseNet201, 1920)],\n     ids=['DenseNet121', 'DenseNet169', 'DenseNet201'])\n def test_densenet_variable_input_channels(fun, dim):\n-    input_shape = (1, None, None) if K.image_data_format() == 'channels_first' else (None, None, 1)\n+    def target(queue, input_shape):\n         model = fun(weights=None, include_top=False, input_shape=input_shape)\n-    assert model.output_shape == (None, None, None, dim)\n+        queue.put(model.output_shape)\n \n-    input_shape = (4, None, None) if K.image_data_format() == 'channels_first' else (None, None, 4)\n-    model = fun(weights=None, include_top=False, input_shape=input_shape)\n-    assert model.output_shape == (None, None, None, dim)\n+    queue = Queue()\n+    p = Process(target=target, args=(queue, (None, None, 1)))\n+    p.start()\n+    p.join()\n+    assert not queue.empty(), 'Model creation failed.'\n+    model_output_shape = queue.get_nowait()\n+    assert model_output_shape == (None, None, None, dim)\n+\n+    p = Process(target=target, args=(queue, (None, None, 4)))\n+    p.start()\n+    p.join()\n+    assert not queue.empty(), 'Model creation failed.'\n+    model_output_shape = queue.get_nowait()\n+    assert model_output_shape == (None, None, None, dim)\n \n \n @keras_test\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#58e702091c60aedb169df2292851fab42019cea7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 12 | Methods Changed: 7 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 34 | Churn Cumulative: 1144 | Contributors (this commit): 14 | Commits (past 90d): 8 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -96,6 +96,9 @@ class _Merge(Layer):\n             self._reshape_required = True\n \n     def call(self, inputs):\n+        if not isinstance(inputs, list):\n+            raise ValueError('A merge layer should be called '\n+                             'on a list of inputs.')\n         if self._reshape_required:\n             reshaped_inputs = []\n             input_ndims = list(map(K.ndim, inputs))\n@@ -241,13 +244,16 @@ class Subtract(_Merge):\n     ```\n     \"\"\"\n \n+    def build(self, input_shape):\n+        super(Subtract, self).build(input_shape)\n+        if len(input_shape) != 2:\n+            raise ValueError('A `Subtract` layer should be called '\n+                             'on exactly 2 inputs')\n+\n     def _merge_function(self, inputs):\n         if len(inputs) != 2:\n-            raise ValueError('`Subtract` layer should be called '\n+            raise ValueError('A `Subtract` layer should be called '\n                              'on exactly 2 inputs')\n-        if K.int_shape(inputs[0]) != K.int_shape(inputs[1]):\n-            raise ValueError('`Subtract` layer should be called '\n-                             'on inputs of the same shape')\n         return inputs[0] - inputs[1]\n \n \n@@ -327,11 +333,12 @@ class Concatenate(_Merge):\n         super(Concatenate, self).__init__(**kwargs)\n         self.axis = axis\n         self.supports_masking = True\n+        self._reshape_required = False\n \n     def build(self, input_shape):\n         # Used purely for shape validation.\n         if not isinstance(input_shape, list) or len(input_shape) < 2:\n-            raise ValueError('`Concatenate` layer should be called '\n+            raise ValueError('A `Concatenate` layer should be called '\n                              'on a list of of at least 2 inputs')\n         if all([shape is None for shape in input_shape]):\n             return\n@@ -341,15 +348,12 @@ class Concatenate(_Merge):\n             del reduced_inputs_shapes[i][self.axis]\n             shape_set.add(tuple(reduced_inputs_shapes[i]))\n         if len(shape_set) > 1:\n-            raise ValueError('`Concatenate` layer requires '\n+            raise ValueError('A `Concatenate` layer requires '\n                              'inputs with matching shapes '\n                              'except for the concat axis. '\n                              'Got inputs shapes: %s' % (input_shape))\n \n-    def call(self, inputs):\n-        if not isinstance(inputs, list):\n-            raise ValueError('A `Concatenate` layer should be called '\n-                             'on a list of inputs.')\n+    def _merge_function(self, inputs):\n         return K.concatenate(inputs, axis=self.axis)\n \n     def compute_output_shape(self, input_shape):\n@@ -434,6 +438,7 @@ class Dot(_Merge):\n         self.axes = axes\n         self.normalize = normalize\n         self.supports_masking = True\n+        self._reshape_required = False\n \n     def build(self, input_shape):\n         # Used purely for shape validation.\n@@ -457,7 +462,10 @@ class Dot(_Merge):\n                 '%s != %s. ' % (shape1[axes[0]], shape2[axes[1]]) +\n                 'Layer shapes: %s, %s' % (shape1, shape2))\n \n-    def call(self, inputs):\n+    def _merge_function(self, inputs):\n+        if len(inputs) != 2:\n+            raise ValueError('A `Dot` layer should be called '\n+                             'on exactly 2 inputs')\n         x1 = inputs[0]\n         x2 = inputs[1]\n         if isinstance(self.axes, int):\n\n@@ -75,8 +75,6 @@ def test_merge_subtract():\n         subtract_layer([i1, i2, i3])\n     with pytest.raises(ValueError):\n         subtract_layer([i1])\n-    with pytest.raises(ValueError):\n-        subtract_layer([i1, i4])\n \n \n @keras_test\n@@ -206,6 +204,8 @@ def test_merge_concatenate():\n         concat_layer.compute_mask(i1, [None, None])\n     with pytest.raises(ValueError):\n         concat_layer.compute_mask([i1, i2], [None])\n+    with pytest.raises(ValueError):\n+        concat_layer([i1])\n \n \n @keras_test\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fb083168b300b50d3085b275870a24902603cd49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1199 | Contributors (this commit): 22 | Commits (past 90d): 4 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -26,7 +26,7 @@ class BatchNormalization(Layer):\n             For instance, after a `Conv2D` layer with\n             `data_format=\"channels_first\"`,\n             set `axis=1` in `BatchNormalization`.\n-        momentum: Momentum for the moving average.\n+        momentum: Momentum for the moving mean and the moving variance.\n         epsilon: Small float added to variance to avoid dividing by zero.\n         center: If True, add offset of `beta` to normalized tensor.\n             If False, `beta` is ignored.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#958239c621a6be037c5f8b30be9270310735f725", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 109 | Lines Deleted: 114 | Files Changed: 8 | Hunks: 87 | Methods Changed: 40 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 223 | Churn Cumulative: 29544 | Contributors (this commit): 189 | Commits (past 90d): 87 | Contributors (cumulative): 274 | DMM Complexity: 1.0\n\nDIFF:\n@@ -70,9 +70,9 @@ class Node(object):\n     \"\"\"A `Node` describes the connectivity between two layers.\n \n     Each time a layer is connected to some new input,\n-    a node is added to `layer.inbound_nodes`.\n+    a node is added to `layer._inbound_nodes`.\n     Each time the output of a layer is used by another layer,\n-    a node is added to `layer.outbound_nodes`.\n+    a node is added to `layer._outbound_nodes`.\n \n     # Arguments\n         outbound_layer: the layer that takes\n@@ -104,11 +104,11 @@ class Node(object):\n     `node_indices` and `tensor_indices` are basically fine-grained coordinates\n     describing the origin of the `input_tensors`, verifying the following:\n \n-    `input_tensors[i] == inbound_layers[i].inbound_nodes[node_indices[i]].output_tensors[tensor_indices[i]]`\n+    `input_tensors[i] == inbound_layers[i]._inbound_nodes[node_indices[i]].output_tensors[tensor_indices[i]]`\n \n     A node from layer A to layer B is added to:\n-        A.outbound_nodes\n-        B.inbound_nodes\n+        A._outbound_nodes\n+        B._inbound_nodes\n     \"\"\"\n \n     def __init__(self, outbound_layer,\n@@ -163,8 +163,8 @@ class Node(object):\n         # Add nodes to all layers involved.\n         for layer in inbound_layers:\n             if layer is not None:\n-                layer.outbound_nodes.append(self)\n-        outbound_layer.inbound_nodes.append(self)\n+                layer._outbound_nodes.append(self)\n+        outbound_layer._inbound_nodes.append(self)\n \n     def get_config(self):\n         inbound_names = []\n@@ -261,8 +261,8 @@ class Layer(object):\n \n         # These lists will be filled via successive calls\n         # to self._add_inbound_node().\n-        self.inbound_nodes = []\n-        self.outbound_nodes = []\n+        self._inbound_nodes = []\n+        self._outbound_nodes = []\n \n         # These properties should be set by the user via keyword arguments.\n         # note that 'dtype', 'input_shape' and 'batch_input_shape'\n@@ -318,7 +318,7 @@ class Layer(object):\n     @staticmethod\n     def _node_key(layer, node_index):\n         \"\"\"Converts a layer and its index to a unique (immutable type) name.\n-        This function is used internally with `self.container_nodes`.\n+        This function is used internally with `self._container_nodes`.\n \n         # Arguments\n             layer: The layer.\n@@ -709,7 +709,7 @@ class Layer(object):\n             uses_lp = getattr(self, 'uses_learning_phase', False) or uses_lp\n             output_tensors[i]._uses_learning_phase = getattr(output_tensors[i], '_uses_learning_phase', False) or uses_lp\n             output_tensors[i]._keras_history = (self,\n-                                                len(self.inbound_nodes) - 1,\n+                                                len(self._inbound_nodes) - 1,\n                                                 i)\n \n     def compute_output_shape(self, input_shape):\n@@ -797,15 +797,15 @@ class Layer(object):\n             RuntimeError: If the layer has no inbound nodes.\n             ValueError: If the index is does not match any node.\n         \"\"\"\n-        if not self.inbound_nodes:\n+        if not self._inbound_nodes:\n             raise RuntimeError('The layer has never been called '\n                                'and thus has no defined ' + attr_name + '.')\n-        if not len(self.inbound_nodes) > node_index:\n+        if not len(self._inbound_nodes) > node_index:\n             raise ValueError('Asked to get ' + attr_name +\n                              ' at node ' + str(node_index) +\n                              ', but the layer has only ' +\n-                             str(len(self.inbound_nodes)) + ' inbound nodes.')\n-        values = getattr(self.inbound_nodes[node_index], attr)\n+                             str(len(self._inbound_nodes)) + ' inbound nodes.')\n+        values = getattr(self._inbound_nodes[node_index], attr)\n         if len(values) == 1:\n             return values[0]\n         else:\n@@ -925,13 +925,13 @@ class Layer(object):\n             AttributeError: if the layer is connected to\n             more than one incoming layers.\n         \"\"\"\n-        if len(self.inbound_nodes) > 1:\n+        if len(self._inbound_nodes) > 1:\n             raise AttributeError('Layer ' + self.name +\n                                  ' has multiple inbound nodes, '\n                                  'hence the notion of \"layer input\" '\n                                  'is ill-defined. '\n                                  'Use `get_input_at(node_index)` instead.')\n-        elif not self.inbound_nodes:\n+        elif not self._inbound_nodes:\n             raise AttributeError('Layer ' + self.name +\n                                  ' is not connected, no input to return.')\n         return self._get_node_attribute_at_index(0, 'input_tensors',\n@@ -951,10 +951,10 @@ class Layer(object):\n             AttributeError: if the layer is connected to\n             more than one incoming layers.\n         \"\"\"\n-        if not self.inbound_nodes:\n+        if not self._inbound_nodes:\n             raise AttributeError('Layer ' + self.name +\n                                  ' has no inbound nodes.')\n-        if len(self.inbound_nodes) > 1:\n+        if len(self._inbound_nodes) > 1:\n             raise AttributeError('Layer ' + self.name +\n                                  ' has multiple inbound nodes, '\n                                  'hence the notion of \"layer output\" '\n@@ -978,7 +978,7 @@ class Layer(object):\n             AttributeError: if the layer is connected to\n             more than one incoming layers.\n         \"\"\"\n-        if len(self.inbound_nodes) != 1:\n+        if len(self._inbound_nodes) != 1:\n             raise AttributeError('Layer ' + self.name +\n                                  ' has multiple inbound nodes, ' +\n                                  'hence the notion of \"layer input mask\" '\n@@ -1003,7 +1003,7 @@ class Layer(object):\n             AttributeError: if the layer is connected to\n             more than one incoming layers.\n         \"\"\"\n-        if len(self.inbound_nodes) != 1:\n+        if len(self._inbound_nodes) != 1:\n             raise AttributeError('Layer ' + self.name +\n                                  ' has multiple inbound nodes, '\n                                  'hence the notion of \"layer output mask\" '\n@@ -1028,12 +1028,12 @@ class Layer(object):\n             AttributeError: if the layer is connected to\n             more than one incoming layers.\n         \"\"\"\n-        if not self.inbound_nodes:\n+        if not self._inbound_nodes:\n             raise AttributeError('The layer has never been called '\n                                  'and thus has no defined input shape.')\n-        all_input_shapes = set([str(node.input_shapes) for node in self.inbound_nodes])\n+        all_input_shapes = set([str(node.input_shapes) for node in self._inbound_nodes])\n         if len(all_input_shapes) == 1:\n-            input_shapes = self.inbound_nodes[0].input_shapes\n+            input_shapes = self._inbound_nodes[0].input_shapes\n             if len(input_shapes) == 1:\n                 return input_shapes[0]\n             else:\n@@ -1062,12 +1062,12 @@ class Layer(object):\n             AttributeError: if the layer is connected to\n             more than one incoming layers.\n         \"\"\"\n-        if not self.inbound_nodes:\n+        if not self._inbound_nodes:\n             raise AttributeError('The layer has never been called '\n                                  'and thus has no defined output shape.')\n-        all_output_shapes = set([str(node.output_shapes) for node in self.inbound_nodes])\n+        all_output_shapes = set([str(node.output_shapes) for node in self._inbound_nodes])\n         if len(all_output_shapes) == 1:\n-            output_shapes = self.inbound_nodes[0].output_shapes\n+            output_shapes = self._inbound_nodes[0].output_shapes\n             if len(output_shapes) == 1:\n                 return output_shapes[0]\n             else:\n@@ -1443,7 +1443,7 @@ def Input(shape=None, batch_shape=None,\n                              input_tensor=tensor)\n     # Return tensor including _keras_shape and _keras_history.\n     # Note that in this case train_output and test_output are the same pointer.\n-    outputs = input_layer.inbound_nodes[0].output_tensors\n+    outputs = input_layer._inbound_nodes[0].output_tensors\n     if len(outputs) == 1:\n         return outputs[0]\n     else:\n@@ -1562,7 +1562,7 @@ class Container(Layer):\n                                 ' (missing Keras metadata).')\n             # Check that x is an input tensor.\n             layer, node_index, tensor_index = x._keras_history\n-            if len(layer.inbound_nodes) > 1 or (layer.inbound_nodes and layer.inbound_nodes[0].inbound_layers):\n+            if len(layer._inbound_nodes) > 1 or (layer._inbound_nodes and layer._inbound_nodes[0].inbound_layers):\n                 cls_name = self.__class__.__name__\n                 warnings.warn(cls_name + ' inputs must come from '\n                               'a Keras Input layer, '\n@@ -1593,7 +1593,7 @@ class Container(Layer):\n         masks = []\n         for x in self.inputs:\n             layer, node_index, tensor_index = x._keras_history\n-            node = layer.inbound_nodes[node_index]\n+            node = layer._inbound_nodes[node_index]\n             mask = node.output_masks[tensor_index]\n             masks.append(mask)\n         mask_cache_key = ','.join([str(id(x)) for x in self.inputs])\n@@ -1601,7 +1601,7 @@ class Container(Layer):\n         masks = []\n         for x in self.outputs:\n             layer, node_index, tensor_index = x._keras_history\n-            node = layer.inbound_nodes[node_index]\n+            node = layer._inbound_nodes[node_index]\n             mask = node.output_masks[tensor_index]\n             masks.append(mask)\n         if len(masks) == 1:\n@@ -1645,8 +1645,8 @@ class Container(Layer):\n         for layer in self.output_layers:\n             self.output_names.append(layer.name)\n \n-        self.internal_input_shapes = [x._keras_shape for x in self.inputs]\n-        self.internal_output_shapes = [x._keras_shape for x in self.outputs]\n+        self._internal_input_shapes = [x._keras_shape for x in self.inputs]\n+        self._internal_output_shapes = [x._keras_shape for x in self.outputs]\n \n         # Container_nodes: set of nodes included in the graph\n         # (not all nodes included in the layers\n@@ -1680,7 +1680,7 @@ class Container(Layer):\n             \"\"\"\n             if not layer or node_index is None or tensor_index is None:\n                 layer, node_index, tensor_index = tensor._keras_history\n-            node = layer.inbound_nodes[node_index]\n+            node = layer._inbound_nodes[node_index]\n \n             # Prevent cycles.\n             if node in nodes_in_progress:\n@@ -1737,7 +1737,7 @@ class Container(Layer):\n             for i in range(len(node.inbound_layers)):\n                 inbound_layer = node.inbound_layers[i]\n                 node_index = node.node_indices[i]\n-                inbound_node = inbound_layer.inbound_nodes[node_index]\n+                inbound_node = inbound_layer._inbound_nodes[node_index]\n                 previous_depth = nodes_depths.get(inbound_node, 0)\n                 nodes_depths[inbound_node] = max(depth + 1, previous_depth)\n \n@@ -1800,9 +1800,9 @@ class Container(Layer):\n                         computable_tensors.append(x)\n                     layers_with_complete_input.append(layer.name)\n \n-        # Set self.nodes and self.nodes_by_depth.\n-        self.container_nodes = container_nodes\n-        self.nodes_by_depth = nodes_by_depth\n+        # Set self._container_nodes and self._nodes_by_depth.\n+        self._container_nodes = container_nodes\n+        self._nodes_by_depth = nodes_by_depth\n \n         # Ensure name unicity, which will be crucial for serialization\n         # (since serialized nodes refer to layers by their name).\n@@ -1818,8 +1818,8 @@ class Container(Layer):\n         # Layer parameters.\n         # The new container starts with a single inbound node\n         # for its inputs, and no outbound nodes.\n-        self.outbound_nodes = []  # Will be appended to by future calls to __call__\n-        self.inbound_nodes = []  # Will be appended to below, and by future calls to __call__\n+        self._outbound_nodes = []  # Will be appended to by future calls to __call__\n+        self._inbound_nodes = []  # Will be appended to below, and by future calls to __call__\n         # Create the node linking internal inputs to internal outputs.\n         Node(outbound_layer=self,\n              inbound_layers=[],\n@@ -1896,9 +1896,9 @@ class Container(Layer):\n                     continue\n                 # Collect updates that are dependent on inputs\n                 # that are part of the model.\n-                for node_index, node in enumerate(layer.inbound_nodes):\n+                for node_index, node in enumerate(layer._inbound_nodes):\n                     node_key = self._node_key(layer, node_index)\n-                    if node_key in self.container_nodes:\n+                    if node_key in self._container_nodes:\n                         # The model owns this layer node.\n                         inputs = node.input_tensors\n                         updates += layer.get_updates_for(inputs)\n@@ -1924,9 +1924,9 @@ class Container(Layer):\n             if hasattr(layer, 'losses'):\n                 # Collect losses that are dependent on inputs\n                 # that are part of the model.\n-                for node_index, node in enumerate(layer.inbound_nodes):\n+                for node_index, node in enumerate(layer._inbound_nodes):\n                     node_key = self._node_key(layer, node_index)\n-                    if node_key in self.container_nodes:\n+                    if node_key in self._container_nodes:\n                         # The model owns this layer node.\n                         inputs = node.input_tensors\n                         losses += layer.get_losses_for(inputs)\n@@ -2110,12 +2110,12 @@ class Container(Layer):\n                 shape_key = layer.name + '_0_0'\n                 layers_to_output_shapes[shape_key] = input_shape\n \n-            depth_keys = list(self.nodes_by_depth.keys())\n+            depth_keys = list(self._nodes_by_depth.keys())\n             depth_keys.sort(reverse=True)\n             # Iterate over nodes, by depth level.\n             if len(depth_keys) > 1:\n                 for depth in depth_keys:\n-                    nodes = self.nodes_by_depth[depth]\n+                    nodes = self._nodes_by_depth[depth]\n                     for node in nodes:\n                         # This is always a single layer, never a list.\n                         layer = node.outbound_layer\n@@ -2140,7 +2140,7 @@ class Container(Layer):\n                             output_shape = layer.compute_output_shape(input_shapes)\n \n                         output_shapes = _to_list(output_shape)\n-                        node_index = layer.inbound_nodes.index(node)\n+                        node_index = layer._inbound_nodes.index(node)\n                         for j in range(len(output_shapes)):\n                             shape_key = layer.name + '_%s_%s' % (node_index, j)\n                             layers_to_output_shapes[shape_key] = output_shapes[j]\n@@ -2190,10 +2190,10 @@ class Container(Layer):\n         for x, y, mask in zip(self.inputs, inputs, masks):\n             tensor_map[str(id(x))] = (y, mask)\n \n-        depth_keys = list(self.nodes_by_depth.keys())\n+        depth_keys = list(self._nodes_by_depth.keys())\n         depth_keys.sort(reverse=True)\n         for depth in depth_keys:\n-            nodes = self.nodes_by_depth[depth]\n+            nodes = self._nodes_by_depth[depth]\n             for node in nodes:\n                 # This is always a single layer, never a list.\n                 layer = node.outbound_layer\n@@ -2326,9 +2326,9 @@ class Container(Layer):\n                 kept_nodes = 1\n             else:\n                 kept_nodes = 0\n-            for original_node_index, node in enumerate(layer.inbound_nodes):\n+            for original_node_index, node in enumerate(layer._inbound_nodes):\n                 node_key = self._node_key(layer, original_node_index)\n-                if node_key in self.container_nodes:\n+                if node_key in self._container_nodes:\n                     # i.e. we mark it to be saved\n                     node_conversion_map[node_key] = kept_nodes\n                     kept_nodes += 1\n@@ -2339,9 +2339,9 @@ class Container(Layer):\n             layer_class_name = layer.__class__.__name__\n             layer_config = layer.get_config()\n             filtered_inbound_nodes = []\n-            for original_node_index, node in enumerate(layer.inbound_nodes):\n+            for original_node_index, node in enumerate(layer._inbound_nodes):\n                 node_key = self._node_key(layer, original_node_index)\n-                if node_key in self.container_nodes:\n+                if node_key in self._container_nodes:\n                     # The node is relevant to the model:\n                     # add to filtered_inbound_nodes.\n                     if node.arguments:\n@@ -2387,7 +2387,7 @@ class Container(Layer):\n             node_index = self.input_layers_node_indices[i]\n \n             node_key = self._node_key(layer, node_index)\n-            if node_key not in self.container_nodes:\n+            if node_key not in self._container_nodes:\n                 continue\n             new_node_index = node_conversion_map[node_key]\n             tensor_index = self.input_layers_tensor_indices[i]\n@@ -2399,7 +2399,7 @@ class Container(Layer):\n             node_index = self.output_layers_node_indices[i]\n \n             node_key = self._node_key(layer, node_index)\n-            if node_key not in self.container_nodes:\n+            if node_key not in self._container_nodes:\n                 continue\n             new_node_index = node_conversion_map[node_key]\n             tensor_index = self.output_layers_tensor_indices[i]\n@@ -2456,10 +2456,10 @@ class Container(Layer):\n                     add_unprocessed_node(layer, node_data)\n                     return\n                 inbound_layer = created_layers[inbound_layer_name]\n-                if len(inbound_layer.inbound_nodes) <= inbound_node_index:\n+                if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                     add_unprocessed_node(layer, node_data)\n                     return\n-                inbound_node = inbound_layer.inbound_nodes[inbound_node_index]\n+                inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                 input_tensors.append(inbound_node.output_tensors[inbound_tensor_index])\n             # Call layer on its inputs, thus creating the node\n             # and building the layer if needed.\n@@ -2517,13 +2517,13 @@ class Container(Layer):\n             layer_name, node_index, tensor_index = layer_data\n             assert layer_name in created_layers\n             layer = created_layers[layer_name]\n-            layer_output_tensors = layer.inbound_nodes[node_index].output_tensors\n+            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n             input_tensors.append(layer_output_tensors[tensor_index])\n         for layer_data in config['output_layers']:\n             layer_name, node_index, tensor_index = layer_data\n             assert layer_name in created_layers\n             layer = created_layers[layer_name]\n-            layer_output_tensors = layer.inbound_nodes[node_index].output_tensors\n+            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n             output_tensors.append(layer_output_tensors[tensor_index])\n         return cls(inputs=input_tensors, outputs=output_tensors, name=name)\n \n@@ -2745,10 +2745,10 @@ def get_source_inputs(tensor, layer=None, node_index=None):\n \n     if layer is None or node_index:\n         layer, node_index, _ = tensor._keras_history\n-    if not layer.inbound_nodes:\n+    if not layer._inbound_nodes:\n         return [tensor]\n     else:\n-        node = layer.inbound_nodes[node_index]\n+        node = layer._inbound_nodes[node_index]\n         if not node.inbound_layers:\n             # Reached an Input layer, stop recursion.\n             return node.input_tensors\n@@ -2815,7 +2815,7 @@ def _collect_previous_mask(input_tensors):\n     for x in input_tensors:\n         if hasattr(x, '_keras_history'):\n             inbound_layer, node_index, tensor_index = x._keras_history\n-            node = inbound_layer.inbound_nodes[node_index]\n+            node = inbound_layer._inbound_nodes[node_index]\n             mask = node.output_masks[tensor_index]\n             masks.append(mask)\n         else:\n\n@@ -708,7 +708,7 @@ class Model(Container):\n             if i in skip_target_indices:\n                 self.targets.append(None)\n             else:\n-                shape = self.internal_output_shapes[i]\n+                shape = self._internal_output_shapes[i]\n                 name = self.output_names[i]\n                 if target_tensors is not None:\n                     target = target_tensors[i]\n@@ -874,7 +874,7 @@ class Model(Container):\n                         if metric == 'accuracy' or metric == 'acc':\n                             # custom handling of accuracy\n                             # (because of class mode duality)\n-                            output_shape = self.internal_output_shapes[i]\n+                            output_shape = self._internal_output_shapes[i]\n                             if (output_shape[-1] == 1 or\n                                self.loss_functions[i] == losses.binary_crossentropy):\n                                 # case: binary accuracy\n\n@@ -86,8 +86,8 @@ class Merge(Layer):\n         self._per_input_losses = {}\n \n         # Layer parameters.\n-        self.inbound_nodes = []\n-        self.outbound_nodes = []\n+        self._inbound_nodes = []\n+        self._outbound_nodes = []\n         self.constraints = {}\n         self._trainable_weights = []\n         self._non_trainable_weights = []\n@@ -119,7 +119,7 @@ class Merge(Layer):\n             for i, layer in enumerate(layers):\n                 node_index = node_indices[i]\n                 tensor_index = tensor_indices[i]\n-                inbound_node = layer.inbound_nodes[node_index]\n+                inbound_node = layer._inbound_nodes[node_index]\n                 input_tensors.append(inbound_node.output_tensors[tensor_index])\n                 input_masks.append(inbound_node.output_masks[tensor_index])\n             self(input_tensors, mask=input_masks)\n@@ -460,7 +460,7 @@ def merge(inputs, mode='sum', concat_axis=-1,\n                             node_indices=node_indices,\n                             tensor_indices=tensor_indices,\n                             name=name)\n-        return merge_layer.inbound_nodes[0].output_tensors[0]\n+        return merge_layer._inbound_nodes[0].output_tensors[0]\n     else:\n         merge_layer = Merge(mode=mode,\n                             concat_axis=concat_axis,\n\n@@ -396,8 +396,8 @@ class Sequential(Model):\n         self._initial_weights = None\n \n         # Model attributes.\n-        self.inbound_nodes = []\n-        self.outbound_nodes = []\n+        self._inbound_nodes = []\n+        self._outbound_nodes = []\n         self.built = False\n \n         # Set model name.\n@@ -467,13 +467,13 @@ class Sequential(Model):\n                 # to the input layer we just created.\n                 layer(x)\n \n-            if len(layer.inbound_nodes[-1].output_tensors) != 1:\n+            if len(layer._inbound_nodes[-1].output_tensors) != 1:\n                 raise ValueError('All layers in a Sequential model '\n                                  'should have a single output tensor. '\n                                  'For multi-output layers, '\n                                  'use the functional API.')\n \n-            self.outputs = [layer.inbound_nodes[-1].output_tensors[0]]\n+            self.outputs = [layer._inbound_nodes[-1].output_tensors[0]]\n             self.inputs = topology.get_source_inputs(self.outputs[0])\n \n             # We create an input node, which we will keep updated\n@@ -497,9 +497,9 @@ class Sequential(Model):\n                                 'For multi-output layers, '\n                                 'use the functional API.')\n             self.outputs = [output_tensor]\n-            # update self.inbound_nodes\n-            self.inbound_nodes[0].output_tensors = self.outputs\n-            self.inbound_nodes[0].output_shapes = [self.outputs[0]._keras_shape]\n+            # update self._inbound_nodes\n+            self._inbound_nodes[0].output_tensors = self.outputs\n+            self._inbound_nodes[0].output_shapes = [self.outputs[0]._keras_shape]\n \n         self.layers.append(layer)\n         self.built = False\n@@ -516,14 +516,14 @@ class Sequential(Model):\n         self.layers.pop()\n         if not self.layers:\n             self.outputs = []\n-            self.inbound_nodes = []\n-            self.outbound_nodes = []\n+            self._inbound_nodes = []\n+            self._outbound_nodes = []\n         else:\n-            self.layers[-1].outbound_nodes = []\n+            self.layers[-1]._outbound_nodes = []\n             self.outputs = [self.layers[-1].output]\n-            # update self.inbound_nodes\n-            self.inbound_nodes[0].output_tensors = self.outputs\n-            self.inbound_nodes[0].output_shapes = [self.outputs[0]._keras_shape]\n+            # update self._inbound_nodes\n+            self._inbound_nodes[0].output_tensors = self.outputs\n+            self._inbound_nodes[0].output_shapes = [self.outputs[0]._keras_shape]\n         self.built = False\n \n     def get_layer(self, name=None, index=None):\n@@ -570,12 +570,12 @@ class Sequential(Model):\n         self.output_layers = self.model.output_layers\n         self.output_layers_node_indices = self.model.output_layers_node_indices\n         self.output_layers_tensor_indices = self.model.output_layers_tensor_indices\n-        self.nodes_by_depth = self.model.nodes_by_depth\n-        self.container_nodes = self.model.container_nodes\n+        self._nodes_by_depth = self.model._nodes_by_depth\n         self.output_names = self.model.output_names\n         self.input_names = self.model.input_names\n         self._feed_input_names = self.model._feed_input_names\n         self._feed_inputs = self.model._feed_inputs\n+        self._container_nodes = self.model._container_nodes\n \n         # Make sure child model callbacks\n         # will call the parent Sequential model.\n@@ -1514,10 +1514,10 @@ def _clone_functional_model(model, input_tensors=None):\n         tensor_map[x] = (y, None)  # tensor, mask\n \n     # Iterated over every node in the reference model, in depth order.\n-    depth_keys = list(model.nodes_by_depth.keys())\n+    depth_keys = list(model._nodes_by_depth.keys())\n     depth_keys.sort(reverse=True)\n     for depth in depth_keys:\n-        nodes = model.nodes_by_depth[depth]\n+        nodes = model._nodes_by_depth[depth]\n         for node in nodes:\n             # Recover the corresponding layer.\n             layer = node.outbound_layer\n\n@@ -40,7 +40,7 @@ def print_summary(model, line_length=None, positions=None, print_fn=print):\n         sequential_like = True\n     else:\n         sequential_like = True\n-        nodes_by_depth = model.nodes_by_depth.values()\n+        nodes_by_depth = model._nodes_by_depth.values()\n         nodes = []\n         for v in nodes_by_depth:\n             if (len(v) > 1) or (len(v) == 1 and len(v[0].inbound_layers) > 1):\n@@ -53,7 +53,7 @@ def print_summary(model, line_length=None, positions=None, print_fn=print):\n             # search for shared layers\n             for layer in model.layers:\n                 flag = False\n-                for node in layer.inbound_nodes:\n+                for node in layer._inbound_nodes:\n                     if node in nodes:\n                         if flag:\n                             sequential_like = False\n@@ -78,7 +78,7 @@ def print_summary(model, line_length=None, positions=None, print_fn=print):\n         # header names for the different log elements\n         to_display = ['Layer (type)', 'Output Shape', 'Param #', 'Connected to']\n         relevant_nodes = []\n-        for v in model.nodes_by_depth.values():\n+        for v in model._nodes_by_depth.values():\n             relevant_nodes += v\n \n     def print_row(fields, positions):\n@@ -116,7 +116,7 @@ def print_summary(model, line_length=None, positions=None, print_fn=print):\n         except AttributeError:\n             output_shape = 'multiple'\n         connections = []\n-        for node in layer.inbound_nodes:\n+        for node in layer._inbound_nodes:\n             if relevant_nodes and node not in relevant_nodes:\n                 # node is not part of the current network\n                 continue\n\n@@ -105,9 +105,9 @@ def model_to_dot(model,\n     # Connect nodes with edges.\n     for layer in layers:\n         layer_id = str(id(layer))\n-        for i, node in enumerate(layer.inbound_nodes):\n+        for i, node in enumerate(layer._inbound_nodes):\n             node_key = layer.name + '_ib-' + str(i)\n-            if node_key in model.container_nodes:\n+            if node_key in model._container_nodes:\n                 for inbound_layer in node.inbound_layers:\n                     inbound_layer_id = str(id(inbound_layer))\n                     layer_id = str(id(layer))\n\n@@ -178,9 +178,9 @@ def test_node_construction():\n     assert a._keras_shape == (None, 32)\n     a_layer, a_node_index, a_tensor_index = a._keras_history\n     b_layer, b_node_index, b_tensor_index = b._keras_history\n-    assert len(a_layer.inbound_nodes) == 1\n+    assert len(a_layer._inbound_nodes) == 1\n     assert a_tensor_index is 0\n-    node = a_layer.inbound_nodes[a_node_index]\n+    node = a_layer._inbound_nodes[a_node_index]\n     assert node.outbound_layer == a_layer\n \n     assert isinstance(node.inbound_layers, list)\n@@ -203,18 +203,18 @@ def test_node_construction():\n     a_2 = dense(a)\n     b_2 = dense(b)\n \n-    assert len(dense.inbound_nodes) == 2\n-    assert len(dense.outbound_nodes) == 0\n-    assert dense.inbound_nodes[0].inbound_layers == [a_layer]\n-    assert dense.inbound_nodes[0].outbound_layer == dense\n-    assert dense.inbound_nodes[1].inbound_layers == [b_layer]\n-    assert dense.inbound_nodes[1].outbound_layer == dense\n+    assert len(dense._inbound_nodes) == 2\n+    assert len(dense._outbound_nodes) == 0\n+    assert dense._inbound_nodes[0].inbound_layers == [a_layer]\n+    assert dense._inbound_nodes[0].outbound_layer == dense\n+    assert dense._inbound_nodes[1].inbound_layers == [b_layer]\n+    assert dense._inbound_nodes[1].outbound_layer == dense\n \n-    assert dense.inbound_nodes[0].input_tensors == [a]\n-    assert dense.inbound_nodes[1].input_tensors == [b]\n+    assert dense._inbound_nodes[0].input_tensors == [a]\n+    assert dense._inbound_nodes[1].input_tensors == [b]\n \n-    assert dense.inbound_nodes[0].get_config()['inbound_layers'] == ['input_a']\n-    assert dense.inbound_nodes[1].get_config()['inbound_layers'] == ['input_b']\n+    assert dense._inbound_nodes[0].get_config()['inbound_layers'] == ['input_a']\n+    assert dense._inbound_nodes[1].get_config()['inbound_layers'] == ['input_b']\n \n     # test layer properties\n     test_layer = Dense(16, name='test_layer')\n@@ -268,11 +268,11 @@ def test_multi_input_layer():\n     assert merge_node_index == 0\n     assert merge_tensor_index == 0\n \n-    assert len(merge_layer.inbound_nodes) == 1\n-    assert len(merge_layer.outbound_nodes) == 0\n+    assert len(merge_layer._inbound_nodes) == 1\n+    assert len(merge_layer._outbound_nodes) == 0\n \n-    assert len(merge_layer.inbound_nodes[0].input_tensors) == 2\n-    assert len(merge_layer.inbound_nodes[0].inbound_layers) == 2\n+    assert len(merge_layer._inbound_nodes[0].input_tensors) == 2\n+    assert len(merge_layer._inbound_nodes[0].inbound_layers) == 2\n \n     c = Dense(64, name='dense_2')(merged)\n     d = Dense(5, name='dense_3')(c)\n@@ -430,12 +430,7 @@ def test_recursion():\n     assert [x.shape for x in fn_outputs] == [(10, 69)]\n \n     # test serialization\n-    print('multi_io_model.layers:', multi_io_model.layers)\n-    print('len(model.inbound_nodes):', len(model.inbound_nodes))\n-    print('len(model.outbound_nodes):', len(model.outbound_nodes))\n     model_config = multi_io_model.get_config()\n-    print(model_config)\n-    print(json.dumps(model_config, indent=4))\n     recreated_model = Model.from_config(model_config)\n \n     fn = K.function(recreated_model.inputs, recreated_model.outputs)\n\n@@ -282,7 +282,7 @@ def test_specify_initial_state_keras_tensor(layer_class):\n         output = layer(inputs, initial_state=initial_state[0])\n     else:\n         output = layer(inputs, initial_state=initial_state)\n-    assert initial_state[0] in layer.inbound_nodes[0].input_tensors\n+    assert initial_state[0] in layer._inbound_nodes[0].input_tensors\n \n     model = Model([inputs] + initial_state, output)\n     model.compile(loss='categorical_crossentropy', optimizer='adam')\n@@ -350,7 +350,7 @@ def test_initial_states_as_other_inputs(layer_class):\n \n     layer = layer_class(units)\n     output = layer(inputs)\n-    assert initial_state[0] in layer.inbound_nodes[0].input_tensors\n+    assert initial_state[0] in layer._inbound_nodes[0].input_tensors\n \n     model = Model(inputs, output)\n     model.compile(loss='categorical_crossentropy', optimizer='adam')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
