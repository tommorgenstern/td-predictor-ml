{"custom_id": "keras#49b9682b3570211c7d8f619f8538c08fd5d8bdad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 226 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -3,6 +3,7 @@ from __future__ import print_function\n import os\n import json\n import sys\n+import importlib\n from .common import epsilon\n from .common import floatx\n from .common import set_epsilon\n@@ -33,7 +34,6 @@ if os.path.exists(_config_path):\n     _epsilon = _config.get('epsilon', epsilon())\n     assert isinstance(_epsilon, float)\n     _backend = _config.get('backend', _BACKEND)\n-    assert _backend in {'theano', 'tensorflow', 'cntk'}\n     _image_data_format = _config.get('image_data_format',\n                                      image_data_format())\n     assert _image_data_format in {'channels_last', 'channels_first'}\n@@ -69,7 +69,6 @@ if not os.path.exists(_config_path):\n # Set backend based on KERAS_BACKEND flag, if applicable.\n if 'KERAS_BACKEND' in os.environ:\n     _backend = os.environ['KERAS_BACKEND']\n-    assert _backend in {'theano', 'tensorflow', 'cntk'}\n     _BACKEND = _backend\n \n # Import backend functions.\n@@ -83,7 +82,24 @@ elif _BACKEND == 'tensorflow':\n     sys.stderr.write('Using TensorFlow backend.\\n')\n     from .tensorflow_backend import *\n else:\n-    raise ValueError('Unknown backend: ' + str(_BACKEND))\n+    # Try and load external backend.\n+    try:\n+        backend_module = importlib.import_module(_BACKEND)\n+        entries = backend_module.__dict__\n+        # Check if valid backend.\n+        # Module is a valid backend if it has the required entries.\n+        required_entries = ['placeholder', 'variable', 'function']\n+        for e in required_entries:\n+            if e not in entries:\n+                raise ValueError('Invalid backend. Missing required entry : ' + e)\n+        namespace = globals()\n+        for k, v in entries.items():\n+            # Make sure we don't override any entries from common, such as epsilon.\n+            if k not in namespace:\n+                namespace[k] = v\n+        sys.stderr.write('Using ' + _BACKEND + ' backend.\\n')\n+    except ImportError:\n+        raise ValueError('Unable to import backend : ' + str(_BACKEND))\n \n \n def backend():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#19ce1ca783a995bbd0765dfd9d89f2866fccbc09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 36 | Churn Cumulative: 4478 | Contributors (this commit): 73 | Commits (past 90d): 11 | Contributors (cumulative): 95 | DMM Complexity: 0.9629629629629629\n\nDIFF:\n@@ -462,6 +462,7 @@ class EarlyStopping(Callback):\n \n     # Arguments\n         monitor: quantity to be monitored.\n+        baseline: baseline value for the monitored quantity to reach\n         min_delta: minimum change in the monitored quantity\n             to qualify as an improvement, i.e. an absolute\n             change of less than min_delta, will count as no\n@@ -478,11 +479,12 @@ class EarlyStopping(Callback):\n             from the name of the monitored quantity.\n     \"\"\"\n \n-    def __init__(self, monitor='val_loss',\n+    def __init__(self, monitor='val_loss', baseline=0,\n                  min_delta=0, patience=0, verbose=0, mode='auto'):\n         super(EarlyStopping, self).__init__()\n \n         self.monitor = monitor\n+        self.baseline = baseline\n         self.patience = patience\n         self.verbose = verbose\n         self.min_delta = min_delta\n@@ -514,6 +516,9 @@ class EarlyStopping(Callback):\n         # Allow instances to be re-used\n         self.wait = 0\n         self.stopped_epoch = 0\n+        if self.baseline > 0:\n+            self.best = self.baseline\n+        else:\n             self.best = np.Inf if self.monitor_op == np.less else -np.Inf\n \n     def on_epoch_end(self, epoch, logs=None):\n\n@@ -286,6 +286,35 @@ def test_EarlyStopping_patience():\n     assert epochs_trained == 3\n \n \n+@keras_test\n+def test_EarlyStopping_baseline():\n+    class DummyModel(object):\n+        def __init__(self):\n+            self.stop_training = False\n+\n+    def baseline_tester(acc_levels):\n+        early_stop = callbacks.EarlyStopping(monitor='val_acc', baseline=0.75, patience=2)\n+        early_stop.model = DummyModel()\n+        epochs_trained = 0\n+        early_stop.on_train_begin()\n+        for epoch in range(len(acc_levels)):\n+            epochs_trained += 1\n+            early_stop.on_epoch_end(epoch, logs={'val_acc': acc_levels[epoch]})\n+            if early_stop.model.stop_training:\n+                break\n+        return epochs_trained\n+\n+    acc_levels = [0.55, 0.76, 0.81, 0.81]\n+    baseline_met = baseline_tester(acc_levels)\n+    acc_levels = [0.55, 0.74, 0.81, 0.81]\n+    baseline_not_met = baseline_tester(acc_levels)\n+\n+    # All epochs should run because baseline was met in second epoch\n+    assert baseline_met == 4\n+    # Baseline was not met by second epoch and should stop\n+    assert baseline_not_met == 2\n+\n+\n @keras_test\n def test_LearningRateScheduler():\n     np.random.seed(1337)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#24e7ac0756aeba06ce1cf7f7e72219a11b14b101", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 316 | Lines Deleted: 32 | Files Changed: 4 | Hunks: 22 | Methods Changed: 13 | Complexity Δ (Sum/Max): 43/30 | Churn Δ: 348 | Churn Cumulative: 20608 | Contributors (this commit): 172 | Commits (past 90d): 36 | Contributors (cumulative): 220 | DMM Complexity: 0.38095238095238093\n\nDIFF:\n@@ -10,6 +10,7 @@ from tensorflow.python.ops import control_flow_ops\n from tensorflow.python.ops import functional_ops\n from tensorflow.python.ops import ctc_ops as ctc\n from tensorflow.python.client import device_lib\n+from tensorflow.core.protobuf import config_pb2\n \n from collections import defaultdict\n \n@@ -2437,11 +2438,15 @@ class Function(object):\n         outputs: Output tensors to fetch.\n         updates: Additional update ops to be run at function call.\n         name: a name to help users identify what this function does.\n-        session_kwargs: arguments to `tf.Session.run()`: `fetches`, `feed_dict`,\n+        session_kwargs: arguments to `tf.Session.run()`:\n+            `fetches`, `feed_dict`,\n             `options`, `run_metadata`\n     \"\"\"\n \n-    def __init__(self, inputs, outputs, updates=None, name=None, **session_kwargs):\n+    def __init__(self, inputs, outputs,\n+                 updates=None,\n+                 name=None,\n+                 **session_kwargs):\n         updates = updates or []\n         if not isinstance(inputs, (list, tuple)):\n             raise TypeError('`inputs` to a TensorFlow backend function '\n@@ -2471,16 +2476,121 @@ class Function(object):\n         self.fetches = session_kwargs.pop('fetches', [])\n         if not isinstance(self.fetches, list):\n             self.fetches = [self.fetches]\n+        # The main use case of `fetches` being passed to a model is the ability\n+        # to run custom updates\n+        # (since the outputs of fetches are never returned).\n+        # This requires us to wrap fetches in `identity` ops.\n+        self.fetches = [tf.identity(x) for x in self.fetches]\n         self.session_kwargs = session_kwargs\n+        if session_kwargs:\n+            raise ValueError('Some keys in session_kwargs are not '\n+                             'supported at this '\n+                             'time: %s', session_kwargs.keys())\n+        self._callable_fn = None\n+        self._feed_arrays = None\n+        self._feed_symbols = None\n+        self._symbol_vals = None\n+        self._session = None\n \n-    def __call__(self, inputs):\n+    def _make_callable(self, feed_arrays, feed_symbols, symbol_vals, session):\n+        \"\"\"Generates a callable that runs the graph.\n+\n+        # Arguments\n+            feed_arrays: List of input tensors to be fed\n+                Numpy arrays at runtime.\n+            feed_symbols: List of input tensors to be fed\n+                symbolic tensors at runtime.\n+            symbol_vals: List of symbolic tensors to be fed to `feed_symbols`.\n+            session: Session to use to generate the callable.\n+\n+        # Returns\n+            Function that runs the graph according to the above options.\n+        \"\"\"\n+        # Prepare callable options.\n+        callable_opts = config_pb2.CallableOptions()\n+        # Handle external-data feed.\n+        for x in feed_arrays:\n+            callable_opts.feed.append(x.name)\n+        if self.feed_dict:\n+            for key in sorted(self.feed_dict.keys()):\n+                callable_opts.feed.append(key.name)\n+        # Handle symbolic feed.\n+        for x, y in zip(feed_symbols, symbol_vals):\n+            connection = callable_opts.tensor_connection.add()\n+            if x.dtype != y.dtype:\n+                y = tf.cast(y, dtype=x.dtype)\n+            from_tensor = tf_ops._as_graph_element(y)\n+            if from_tensor is None:\n+                from_tensor = y\n+            connection.from_tensor = from_tensor.name  # Data tensor\n+            connection.to_tensor = x.name  # Placeholder\n+        # Handle fetches.\n+        for x in self.outputs + self.fetches:\n+            callable_opts.fetch.append(x.name)\n+        # Handle updates.\n+        callable_opts.target.append(self.updates_op.name)\n+        # Create callable.\n+        callable_fn = session._make_callable_from_options(callable_opts)\n+        # Cache parameters corresponding to the generated callable, so that\n+        # we can detect future mismatches and refresh the callable.\n+        self._callable_fn = callable_fn\n+        self._feed_arrays = feed_arrays\n+        self._feed_symbols = feed_symbols\n+        self._symbol_vals = symbol_vals\n+        self._session = session\n+\n+    def _call(self, inputs):\n+        if not isinstance(inputs, (list, tuple)):\n+            raise TypeError('`inputs` should be a list or tuple.')\n+\n+        session = get_session()\n+        feed_arrays = []\n+        array_vals = []\n+        feed_symbols = []\n+        symbol_vals = []\n+        for tensor, value in zip(self.inputs, inputs):\n+            if value is None:\n+                continue\n+            if is_tensor(value):\n+                # Case: feeding symbolic tensor.\n+                feed_symbols.append(tensor)\n+                symbol_vals.append(value)\n+            else:\n+                feed_arrays.append(tensor)\n+                # We need to do array conversion and type casting\n+                # at this level, since\n+                # `callable_fn` only supports exact matches.\n+                array_vals.append(\n+                    np.asarray(value,\n+                               dtype=tensor.dtype.base_dtype.name))\n+        if self.feed_dict:\n+            for key in sorted(self.feed_dict.keys()):\n+                array_vals.append(\n+                    np.asarray(self.feed_dict[key],\n+                               dtype=key.dtype.base_dtype.name))\n+\n+        # Refresh callable if anything has changed.\n+        if (self._callable_fn is None or\n+                feed_arrays != self._feed_arrays or\n+                symbol_vals != self._symbol_vals or\n+                feed_symbols != self._feed_symbols or\n+                session != self._session):\n+            self._make_callable(feed_arrays,\n+                                feed_symbols,\n+                                symbol_vals,\n+                                session)\n+        fetched = self._callable_fn(*array_vals)\n+        return fetched[:len(self.outputs)]\n+\n+    def _legacy_call(self, inputs):\n         if not isinstance(inputs, (list, tuple)):\n             raise TypeError('`inputs` should be a list or tuple.')\n         feed_dict = self.feed_dict.copy()\n         for tensor, value in zip(self.inputs, inputs):\n             if is_sparse(tensor):\n                 sparse_coo = value.tocoo()\n-                indices = np.concatenate((np.expand_dims(sparse_coo.row, 1),\n+                indices = np.concatenate(\n+                    (np.expand_dims(sparse_coo.row, 1),\n                      np.expand_dims(sparse_coo.col, 1)), 1)\n                 value = (indices, sparse_coo.data, sparse_coo.shape)\n             feed_dict[tensor] = value\n@@ -2490,6 +2600,23 @@ class Function(object):\n                               **self.session_kwargs)\n         return updated[:len(self.outputs)]\n \n+    def __call__(self, inputs):\n+        if hasattr(get_session(), '_make_callable_from_options'):\n+            if py_any(is_sparse(x) for x in self.inputs):\n+                if py_any(is_tensor(x) for x in inputs):\n+                    raise ValueError(\n+                        'Feeding from symbolic tensors is not '\n+                        'supported with sparse inputs.')\n+                return self._legacy_call(inputs)\n+\n+            return self._call(inputs)\n+        else:\n+            if py_any(is_tensor(x) for x in inputs):\n+                raise ValueError(\n+                    'In order to feed symbolic tensors to a Keras model '\n+                    'in TensorFlow, you need tensorflow 1.8 or higher.')\n+            return self._legacy_call(inputs)\n+\n \n def function(inputs, outputs, updates=None, **kwargs):\n     \"\"\"Instantiates a Keras function.\n\n@@ -974,9 +974,13 @@ class Model(Network):\n                 val_ins = val_x + val_y + val_sample_weights\n \n         elif validation_split and 0. < validation_split < 1.:\n+            if any(K.is_tensor(t) for t in x):\n+                raise ValueError(\n+                    'If your data is in the form of symbolic tensors, '\n+                    'you cannot use `validation_split`.')\n             do_validation = True\n             if hasattr(x[0], 'shape'):\n-                split_at = int(x[0].shape[0] * (1. - validation_split))\n+                split_at = int(int(x[0].shape[0]) * (1. - validation_split))\n             else:\n                 split_at = int(len(x[0]) * (1. - validation_split))\n             x, val_x = (slice_arrays(x, 0, split_at),\n\n@@ -11,6 +11,22 @@ from .. import backend as K\n from .. import losses\n \n \n+def standardize_single_array(x):\n+    if x is None:\n+        return None\n+    elif K.is_tensor(x):\n+        shape = K.int_shape(x)\n+        if shape is None or shape[0] is None:\n+            raise ValueError(\n+                'When feeding symbolic tensors to a model, we expect the'\n+                'tensors to have a static batch size. '\n+                'Got tensor with shape: %s' % str(shape))\n+        return x\n+    elif x.ndim == 1:\n+        x = np.expand_dims(x, 1)\n+    return x\n+\n+\n def standardize_input_data(data,\n                            names,\n                            shapes=None,\n@@ -49,23 +65,29 @@ def standardize_input_data(data,\n \n     if isinstance(data, dict):\n         try:\n-            data = [data[x].values if data[x].__class__.__name__ == 'DataFrame'\n-                    else data[x] for x in names]\n+            data = [\n+                data[x].values\n+                if data[x].__class__.__name__ == 'DataFrame' else data[x]\n+                for x in names\n+            ]\n         except KeyError as e:\n-            raise ValueError(\n-                'No data provided for \"' + e.args[0] + '\". Need data '\n+            raise ValueError('No data provided for \"' + e.args[0] +\n+                             '\". Need data '\n                              'for each key in: ' + str(names))\n     elif isinstance(data, list):\n-        if len(names) == 1 and data and isinstance(data[0], (float, int)):\n+        if isinstance(data[0], list):\n+            data = [np.asarray(d) for d in data]\n+        elif len(names) == 1 and isinstance(data[0], (float, int)):\n             data = [np.asarray(data)]\n         else:\n-            data = [x.values if x.__class__.__name__ == 'DataFrame'\n-                    else x for x in data]\n+            data = [\n+                x.values if x.__class__.__name__ == 'DataFrame'\n+                else x for x in data\n+            ]\n     else:\n         data = data.values if data.__class__.__name__ == 'DataFrame' else data\n         data = [data]\n-    data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1\n-            else x for x in data]\n+    data = [standardize_single_array(x) for x in data]\n \n     if len(data) != len(names):\n         if data and hasattr(data[0], 'shape'):\n@@ -81,11 +103,10 @@ def standardize_input_data(data,\n                 'Error when checking model ' + exception_prefix +\n                 ': you are passing a list as input to your model, '\n                 'but the model expects a list of ' + str(len(names)) +\n-                ' Numpy arrays instead. The list you passed was: ' +\n-                str(data)[:200])\n+                ' Numpy arrays instead. '\n+                'The list you passed was: ' + str(data)[:200])\n         elif len(data) == 1 and not hasattr(data[0], 'shape'):\n-            raise TypeError(\n-                'Error when checking model ' + exception_prefix +\n+            raise TypeError('Error when checking model ' + exception_prefix +\n                             ': data should be a Numpy array, or list/dict of '\n                             'Numpy arrays. Found: ' + str(data)[:200] + '...')\n         elif len(names) == 1:\n@@ -94,7 +115,7 @@ def standardize_input_data(data,\n     # Check shapes compatibility.\n     if shapes:\n         for i in range(len(names)):\n-            if shapes[i] is not None:\n+            if shapes[i] is not None and not K.is_tensor(data[i]):\n                 data_shape = data[i].shape\n                 shape = shapes[i]\n                 if data[i].ndim != len(shape):\n@@ -194,7 +215,7 @@ def check_array_length_consistency(inputs, targets, weights=None):\n         if x is None:\n             return {0}\n         else:\n-            return set([0 if y is None else y.shape[0] for y in x])\n+            return set([0 if y is None else int(y.shape[0]) for y in x])\n \n     set_x = set_of_lengths(inputs)\n     set_y = set_of_lengths(targets)\n@@ -532,15 +553,20 @@ def check_num_samples(ins,\n     # Raises\n         ValueError: In case of invalid arguments.\n     \"\"\"\n-    if steps is not None:\n-        num_samples = None\n-        if batch_size is not None:\n-            raise ValueError('If ' + steps_name +\n-                             ' is set, the `batch_size` must be None.')\n-    elif ins and hasattr(ins[0], 'shape'):\n-        num_samples = ins[0].shape[0]\n-    else:\n-        raise ValueError('Either the input data should have '\n-                         'a defined shape, or ' + steps_name +\n-                         ' should be specified.')\n-    return num_samples\n+    if steps is not None and batch_size is not None:\n+        raise ValueError(\n+            'If ' + steps_name + ' is set, the `batch_size` must be None.')\n+\n+    if not ins or any(K.is_tensor(x) for x in ins):\n+        if steps is None:\n+            raise ValueError(\n+                'If your data is in the form of symbolic tensors, '\n+                'you should specify the `' + steps_name + '` argument '\n+                '(instead of the `batch_size` argument, '\n+                'because symbolic tensors are expected to produce '\n+                'batches of input data).')\n+        return None\n+\n+    if hasattr(ins[0], 'shape'):\n+        return int(ins[0].shape[0])\n+    return None  # Edge case where ins == [static_learning_phase]\n\n@@ -1234,5 +1234,132 @@ def test_pandas_dataframe():\n                           [output_a_df, output_b_df])\n \n \n+@keras_test\n+@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TensorFlow')\n+@pytest.mark.skipif((K.backend() == 'tensorflow' and\n+                     not hasattr(K.get_session(),\n+                                 '_make_callable_from_options')),\n+                    reason='Requires TF 1.8 or higher')\n+def test_training_and_eval_methods_on_symbolic_tensors_single_io():\n+    x = keras.layers.Input(shape=(3,), name='input')\n+    y = keras.layers.Dense(4, name='dense')(x)\n+    model = keras.Model(x, y)\n+\n+    optimizer = 'rmsprop'\n+    loss = 'mse'\n+    metrics = ['mae']\n+    model.compile(optimizer, loss, metrics=metrics)\n+\n+    inputs = keras.backend.zeros(shape=(10, 3))\n+    targets = keras.backend.zeros(shape=(10, 4))\n+\n+    model.fit(inputs, targets, epochs=1, steps_per_epoch=2, verbose=0)\n+    model.evaluate(inputs, targets, steps=2, verbose=0)\n+    model.predict(inputs, steps=2)\n+    model.train_on_batch(inputs, targets)\n+    model.test_on_batch(inputs, targets)\n+    model.fit(inputs, targets,\n+              epochs=1, steps_per_epoch=2, verbose=1,\n+              validation_data=(inputs, targets), validation_steps=2)\n+\n+\n+@keras_test\n+@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TensorFlow')\n+@pytest.mark.skipif((K.backend() == 'tensorflow' and\n+                     not hasattr(K.get_session(),\n+                                 '_make_callable_from_options')),\n+                    reason='Requires TF 1.8 or higher')\n+def test_training_and_eval_methods_on_symbolic_tensors_multi_io():\n+    a = keras.layers.Input(shape=(3,), name='input_a')\n+    b = keras.layers.Input(shape=(3,), name='input_b')\n+\n+    dense = keras.layers.Dense(4, name='dense')\n+    c = dense(a)\n+    d = dense(b)\n+    e = keras.layers.Dropout(0.5, name='dropout')(c)\n+\n+    model = keras.models.Model([a, b], [d, e])\n+\n+    optimizer = 'rmsprop'\n+    loss = 'mse'\n+    loss_weights = [1., 0.5]\n+    metrics = ['mae']\n+    model.compile(optimizer, loss, metrics=metrics, loss_weights=loss_weights)\n+\n+    input_a_tf = keras.backend.zeros(shape=(10, 3))\n+    input_b_tf = keras.backend.zeros(shape=(10, 3))\n+\n+    output_d_tf = keras.backend.zeros(shape=(10, 4))\n+    output_e_tf = keras.backend.zeros(shape=(10, 4))\n+\n+    model.fit(\n+        [input_a_tf, input_b_tf], [output_d_tf, output_e_tf],\n+        epochs=1,\n+        steps_per_epoch=2,\n+        verbose=0)\n+    with pytest.raises(ValueError) as excinfo:\n+        model.fit(\n+            [input_a_tf, input_b_tf], [output_d_tf, output_e_tf],\n+            epochs=1,\n+            batch_size=5,\n+            verbose=0)\n+    assert 'should specify the `steps_per_epoch`' in str(excinfo.value)\n+    model.train_on_batch([input_a_tf, input_b_tf], [output_d_tf, output_e_tf])\n+\n+    # Test with dictionary inputs\n+    model.fit(\n+        {'input_a': input_a_tf,\n+         'input_b': input_b_tf},\n+        {'dense': output_d_tf,\n+         'dropout': output_e_tf},\n+        epochs=1,\n+        steps_per_epoch=2,\n+        verbose=0)\n+    model.fit(\n+        {'input_a': input_a_tf,\n+         'input_b': input_b_tf},\n+        {'dense': output_d_tf,\n+         'dropout': output_e_tf},\n+        validation_data=({'input_a': input_a_tf,\n+                          'input_b': input_b_tf},\n+                         {'dense': output_d_tf,\n+                          'dropout': output_e_tf}),\n+        epochs=1,\n+        steps_per_epoch=2,\n+        validation_steps=2,\n+        verbose=0)\n+    model.train_on_batch(\n+        {'input_a': input_a_tf,\n+         'input_b': input_b_tf},\n+        {'dense': output_d_tf,\n+         'dropout': output_e_tf})\n+\n+    # Test with validation data\n+    model.fit(\n+        [input_a_tf, input_b_tf], [output_d_tf, output_e_tf],\n+        validation_data=([input_a_tf, input_b_tf],\n+                         [output_d_tf, output_e_tf]),\n+        epochs=1,\n+        steps_per_epoch=2,\n+        validation_steps=2,\n+        verbose=0)\n+    # Test with validation split\n+    with pytest.raises(ValueError) as excinfo:\n+        model.fit(\n+            [input_a_tf, input_b_tf], [output_d_tf, output_e_tf],\n+            epochs=2,\n+            steps_per_epoch=2,\n+            verbose=0,\n+            validation_split=0.2,\n+            validation_steps=2)\n+    assert 'you cannot use `validation_split`' in str(excinfo.value)\n+\n+    # Test evaluation / prediction methods\n+    model.evaluate([input_a_tf, input_b_tf], [output_d_tf, output_e_tf],\n+                   steps=2, verbose=0)\n+    model.predict([input_a_tf, input_b_tf], steps=2)\n+    model.test_on_batch([input_a_tf, input_b_tf], [output_d_tf, output_e_tf])\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#87417470c8168772559be0531e297120c569a422", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 2964 | Contributors (this commit): 70 | Commits (past 90d): 8 | Contributors (cumulative): 70 | DMM Complexity: 0.0\n\nDIFF:\n@@ -462,7 +462,6 @@ class EarlyStopping(Callback):\n \n     # Arguments\n         monitor: quantity to be monitored.\n-        baseline: baseline value for the monitored quantity to reach\n         min_delta: minimum change in the monitored quantity\n             to qualify as an improvement, i.e. an absolute\n             change of less than min_delta, will count as no\n@@ -477,10 +476,18 @@ class EarlyStopping(Callback):\n             monitored has stopped increasing; in `auto`\n             mode, the direction is automatically inferred\n             from the name of the monitored quantity.\n+        baseline: Baseline value for the monitored quantity to reach.\n+            Training will stop if the model doesn't show improvement\n+            over the baseline.\n     \"\"\"\n \n-    def __init__(self, monitor='val_loss', baseline=0,\n-                 min_delta=0, patience=0, verbose=0, mode='auto'):\n+    def __init__(self,\n+                 monitor='val_loss',\n+                 min_delta=0,\n+                 patience=0,\n+                 verbose=0,\n+                 mode='auto',\n+                 baseline=None):\n         super(EarlyStopping, self).__init__()\n \n         self.monitor = monitor\n@@ -516,7 +523,7 @@ class EarlyStopping(Callback):\n         # Allow instances to be re-used\n         self.wait = 0\n         self.stopped_epoch = 0\n-        if self.baseline > 0:\n+        if self.baseline is not None:\n             self.best = self.baseline\n         else:\n             self.best = np.Inf if self.monitor_op == np.less else -np.Inf\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#97d5fa920e4f8248128f7c1b460fd9bb20d3478f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 81 | Churn Cumulative: 13774 | Contributors (this commit): 115 | Commits (past 90d): 31 | Contributors (cumulative): 135 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2871,7 +2871,10 @@ def rnn(step_function, inputs, initial_states,\n                 tiled_mask_t = tf.tile(mask_t,\n                                        tf.stack([1, tf.shape(output)[1]]))\n                 output = tf.where(tiled_mask_t, output, states[0])\n-                new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]\n+                new_states = [\n+                    tf.where(tf.tile(mask_t, tf.stack([1, tf.shape(new_states[i])[1]])),\n+                             new_states[i], states[i]) for i in range(len(states))\n+                ]\n                 output_ta_t = output_ta_t.write(time, output)\n                 return (time + 1, output_ta_t) + tuple(new_states)\n         else:\n\n@@ -599,6 +599,82 @@ class TestBackend(object):\n                 assert_allclose(outputs_list[i - 1], outputs_list[i], atol=1e-05)\n                 assert_allclose(state_list[i - 1], state_list[i], atol=1e-05)\n \n+    def test_rnn_additional_states(self):\n+        # implement a simple RNN with an additional state\n+        # whose shape is different from that of the output\n+        num_samples = 4\n+        input_dim = 5\n+        output_dim = 3\n+        timesteps = 6\n+\n+        _, x = parse_shape_or_val((num_samples, timesteps, input_dim))\n+        _, h0 = parse_shape_or_val((num_samples, output_dim))\n+        _, wi = parse_shape_or_val((input_dim, output_dim))\n+        _, wh = parse_shape_or_val((output_dim, output_dim))\n+        mask = np.random.randint(2, size=(num_samples, timesteps))\n+\n+        x_k = K.variable(x)\n+        h0_k = [K.variable(h0), K.variable(np.concatenate([h0, h0], axis=-1))]\n+        wi_k = K.variable(wi)\n+        wh_k = K.variable(wh)\n+        mask_k = K.variable(mask)\n+\n+        def rnn_fn(x_k, h_k):\n+            assert len(h_k) == 2\n+            y_k = K.dot(x_k, wi_k) + K.dot(h_k[0], wh_k)\n+            return y_k, [y_k, K.concatenate([y_k, y_k], axis=-1)]\n+\n+        # test default setup\n+        last_output_list = []\n+        outputs_list = []\n+        state_list = []\n+\n+        kwargs_list = [\n+            {'go_backwards': False, 'mask': None},\n+            {'go_backwards': False, 'mask': None, 'unroll': True, 'input_length': timesteps},\n+            {'go_backwards': True, 'mask': None},\n+            {'go_backwards': True, 'mask': None, 'unroll': True, 'input_length': timesteps},\n+            {'go_backwards': False, 'mask': mask_k},\n+            {'go_backwards': False, 'mask': mask_k, 'unroll': True, 'input_length': timesteps},\n+        ]\n+\n+        for (i, kwargs) in enumerate(kwargs_list):\n+            last_y1, y1, h1 = reference_operations.rnn(x, [wi, wh, None], h0, **kwargs)\n+            last_y2, y2, h2 = K.rnn(rnn_fn, x_k, h0_k, **kwargs)\n+\n+            assert len(h2) == 2\n+            last_y2 = K.eval(last_y2)\n+            y2 = K.eval(y2)\n+            h11 = h1[:, -1]\n+            h12 = np.concatenate([h1[:, -1], h1[:, -1]], axis=-1)\n+            h21 = K.eval(h2[0])\n+            h22 = K.eval(h2[1])\n+\n+            if kwargs['mask'] is not None:\n+                last_y1 = last_y1 * np.expand_dims(mask[:, -1], -1)\n+                last_y2 = last_y2 * np.expand_dims(mask[:, -1], -1)\n+                y1 = y1 * np.expand_dims(mask, -1)\n+                y2 = y2 * np.expand_dims(mask, -1)\n+                h11 = h11 * np.expand_dims(mask[:, -1], -1)\n+                h21 = h21 * np.expand_dims(mask[:, -1], -1)\n+                h12 = h12 * np.expand_dims(mask[:, -1], -1)\n+                h22 = h22 * np.expand_dims(mask[:, -1], -1)\n+\n+            last_output_list.append(last_y2)\n+            outputs_list.append(y2)\n+            state_list.append((h21, h22))\n+\n+            if i % 2 == 0:\n+                assert_allclose(last_y1, last_y2, atol=1e-05)\n+                assert_allclose(y1, y2, atol=1e-05)\n+                assert_allclose(h11, h21, atol=1e-05)\n+                assert_allclose(h12, h22, atol=1e-05)\n+            else:\n+                assert_allclose(last_output_list[i - 1], last_output_list[i], atol=1e-05)\n+                assert_allclose(outputs_list[i - 1], outputs_list[i], atol=1e-05)\n+                assert_allclose(state_list[i - 1][0], state_list[i][0], atol=1e-05)\n+                assert_allclose(state_list[i - 1][1], state_list[i][1], atol=1e-05)\n+\n     def test_rnn_no_states(self):\n         # implement a simple RNN without states\n         input_dim = 8\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#243338dc36c554676f1e9a5f733b445341d9f4f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 12352 | Contributors (this commit): 74 | Commits (past 90d): 3 | Contributors (cumulative): 74 | DMM Complexity: None\n\nDIFF:\n@@ -467,11 +467,15 @@ class Flatten(Layer):\n     \"\"\"Flattens the input. Does not affect the batch size.\n \n     # Arguments\n-        data_format: A string, one of `channels_last` (default) or `channels_first`.\n+        data_format: A string,\n+            one of `channels_last` (default) or `channels_first`.\n             The ordering of the dimensions in the inputs.\n             `channels_last` corresponds to inputs with shape\n             `(batch, ..., channels)` while `channels_first` corresponds to\n             inputs with shape `(batch, channels, ...)`.\n+            It defaults to the `image_data_format` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"channels_last\".\n \n     # Example\n \n@@ -487,7 +491,7 @@ class Flatten(Layer):\n     ```\n     \"\"\"\n \n-    def __init__(self, data_format='channels_last', **kwargs):\n+    def __init__(self, data_format=None, **kwargs):\n         super(Flatten, self).__init__(**kwargs)\n         self.input_spec = InputSpec(min_ndim=3)\n         self.data_format = conv_utils.normalize_data_format(data_format)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aac81c29a762fdaef6718bbc4883f208283aac9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 191 | Lines Deleted: 164 | Files Changed: 4 | Hunks: 26 | Methods Changed: 20 | Complexity Δ (Sum/Max): 10/19 | Churn Δ: 355 | Churn Cumulative: 4624 | Contributors (this commit): 26 | Commits (past 90d): 17 | Contributors (cumulative): 35 | DMM Complexity: 1.0\n\nDIFF:\n@@ -473,7 +473,7 @@ def preprocess_weights_for_loading(layer, weights,\n                                    original_keras_version=None,\n                                    original_backend=None,\n                                    reshape=False):\n-    \"\"\"Converts layers weights from Keras 1 format to Keras 2.\n+    \"\"\"Converts layers weights from Keras 1 format to Keras 2 and also weights of CuDNN layers in Keras 2.\n \n     # Arguments\n         layer: Layer instance.\n@@ -487,7 +487,14 @@ def preprocess_weights_for_loading(layer, weights,\n     # Returns\n         A list of weights values (Numpy arrays).\n     \"\"\"\n-    if layer.__class__.__name__ == 'Bidirectional':\n+    def convert_nested_bidirectional(weights):\n+        \"\"\"Converts layers nested in `Bidirectional` wrapper by `preprocess_weights_for_loading()`.\n+\n+        # Arguments\n+            weights: List of weights values (Numpy arrays).\n+        # Returns\n+            A list of weights values (Numpy arrays).\n+        \"\"\"\n         num_weights_per_layer = len(weights) // 2\n         forward_weights = preprocess_weights_for_loading(layer.forward_layer,\n                                                          weights[:num_weights_per_layer],\n@@ -497,7 +504,48 @@ def preprocess_weights_for_loading(layer, weights,\n                                                           weights[num_weights_per_layer:],\n                                                           original_keras_version,\n                                                           original_backend)\n-        weights = forward_weights + backward_weights\n+        return forward_weights + backward_weights\n+\n+    def convert_nested_model(weights):\n+        \"\"\"Converts layers nested in `Model` or `Sequential` by `preprocess_weights_for_loading()`.\n+\n+        # Arguments\n+            weights: List of weights values (Numpy arrays).\n+        # Returns\n+            A list of weights values (Numpy arrays).\n+        \"\"\"\n+        new_weights = []\n+        # trainable weights\n+        for sublayer in layer.layers:\n+            num_weights = len(sublayer.trainable_weights)\n+            if num_weights > 0:\n+                new_weights.extend(preprocess_weights_for_loading(\n+                    layer=sublayer,\n+                    weights=weights[:num_weights],\n+                    original_keras_version=original_keras_version,\n+                    original_backend=original_backend))\n+                weights = weights[num_weights:]\n+\n+        # non-trainable weights\n+        for sublayer in layer.layers:\n+            num_weights = len([l for l in sublayer.weights\n+                               if l not in sublayer.trainable_weights])\n+            if num_weights > 0:\n+                new_weights.extend(preprocess_weights_for_loading(\n+                    layer=sublayer,\n+                    weights=weights[:num_weights],\n+                    original_keras_version=original_keras_version,\n+                    original_backend=original_backend))\n+                weights = weights[num_weights:]\n+        return new_weights\n+\n+    # Convert layers nested in Bidirectional/Model/Sequential.\n+    # Both transformation should be ran for both Keras 1->2 conversion\n+    # and for conversion of CuDNN layers.\n+    if layer.__class__.__name__ == 'Bidirectional':\n+        weights = convert_nested_bidirectional(weights)\n+    elif layer.__class__.__name__ in ['Model', 'Sequential']:\n+        weights = convert_nested_model(weights)\n \n     if original_keras_version == '1':\n         if layer.__class__.__name__ == 'TimeDistributed':\n@@ -591,32 +639,6 @@ def preprocess_weights_for_loading(layer, weights,\n                                                     (2, 3, 1, 0))\n                 weights = [kernel, recurrent_kernel, bias]\n \n-        if layer.__class__.__name__ in ['Model', 'Sequential']:\n-            new_weights = []\n-            # trainable weights\n-            for sublayer in layer.layers:\n-                num_weights = len(sublayer.trainable_weights)\n-                if num_weights > 0:\n-                    new_weights.extend(preprocess_weights_for_loading(\n-                        layer=sublayer,\n-                        weights=weights[:num_weights],\n-                        original_keras_version=original_keras_version,\n-                        original_backend=original_backend))\n-                    weights = weights[num_weights:]\n-\n-            # non-trainable weights\n-            for sublayer in layer.layers:\n-                num_weights = len([l for l in sublayer.weights\n-                                  if l not in sublayer.trainable_weights])\n-                if num_weights > 0:\n-                    new_weights.extend(preprocess_weights_for_loading(\n-                        layer=sublayer,\n-                        weights=weights[:num_weights],\n-                        original_keras_version=original_keras_version,\n-                        original_backend=original_backend))\n-                    weights = weights[num_weights:]\n-            weights = new_weights\n-\n     conv_layers = ['Conv1D',\n                    'Conv2D',\n                    'Conv3D',\n@@ -645,6 +667,7 @@ def preprocess_weights_for_loading(layer, weights,\n             if layer.__class__.__name__ == 'ConvLSTM2D':\n                 weights[1] = np.transpose(weights[1], (3, 2, 0, 1))\n \n+    # convert CuDNN layers\n     weights = _convert_rnn_weights(layer, weights)\n \n     return weights\n@@ -763,7 +786,7 @@ def _convert_rnn_weights(layer, weights):\n         def convert_weights(weights, from_cudnn=True):\n             kernels = transform_kernels(weights[0], transpose_input(from_cudnn), n_gates)\n             recurrent_kernels = transform_kernels(weights[1], lambda k: k.T, n_gates)\n-            biases = weights[2].reshape((2, -1) if from_cudnn else -1)\n+            biases = np.array(weights[2]).reshape((2, -1) if from_cudnn else -1)\n             return [kernels, recurrent_kernels, biases]\n \n         if bias_shape == (2 * units * n_gates,):\n\n@@ -11,6 +11,11 @@ from keras.models import model_from_json, model_from_yaml\n from keras.utils.test_utils import keras_test\n \n \n+skipif_no_tf_gpu = pytest.mark.skipif(\n+    (K.backend() != 'tensorflow') or (not K.tensorflow_backend._get_available_gpus()),\n+    reason='Requires TensorFlow backend and a GPU')\n+\n+\n @keras_test\n def test_get_updates_for():\n     a = Input(shape=(2,))\n@@ -630,8 +635,7 @@ def test_preprocess_weights_for_loading_rnn_should_be_idempotent(layer_class, la\n     (layers.CuDNNGRU, {'units': 2, 'input_shape': [3, 5]}),\n     (layers.CuDNNLSTM, {'units': 2, 'input_shape': [3, 5]}),\n ])\n-@pytest.mark.skipif((K.backend() != 'tensorflow'), reason='Requires TensorFlow backend')\n-@pytest.mark.skipif(not K.tensorflow_backend._get_available_gpus(), reason='Requires GPU')\n+@skipif_no_tf_gpu\n def test_preprocess_weights_for_loading_cudnn_rnn_should_be_idempotent(layer_class, layer_args):\n     test_preprocess_weights_for_loading_rnn_should_be_idempotent(layer_class, layer_args)\n \n\n@@ -2,16 +2,19 @@ import pytest\n import numpy as np\n from numpy.testing import assert_allclose\n import keras\n+import keras.backend as K\n from keras.utils.test_utils import layer_test\n from keras.utils.test_utils import keras_test\n import time\n \n \n+skipif_no_tf_gpu = pytest.mark.skipif(\n+    (K.backend() != 'tensorflow') or (not K.tensorflow_backend._get_available_gpus()),\n+    reason='Requires TensorFlow backend and a GPU')\n+\n+\n @keras_test\n-@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n-                    reason='Requires GPU')\n+@skipif_no_tf_gpu\n def test_cudnn_rnn_canonical_to_params_lstm():\n     units = 1\n     input_size = 1\n@@ -69,10 +72,7 @@ def test_cudnn_rnn_canonical_to_params_lstm():\n \n \n @keras_test\n-@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n-                    reason='Requires GPU')\n+@skipif_no_tf_gpu\n def test_cudnn_rnn_canonical_to_params_gru():\n     units = 7\n     input_size = 9\n@@ -123,10 +123,7 @@ def test_cudnn_rnn_canonical_to_params_gru():\n \n @keras_test\n @pytest.mark.parametrize('rnn_type', ['lstm', 'gru'], ids=['LSTM', 'GRU'])\n-@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n-                    reason='Requires GPU')\n+@skipif_no_tf_gpu\n def test_cudnn_rnn_timing(rnn_type):\n     input_size = 1000\n     timesteps = 60\n@@ -164,10 +161,7 @@ def test_cudnn_rnn_timing(rnn_type):\n \n \n @keras_test\n-@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n-                    reason='Requires GPU')\n+@skipif_no_tf_gpu\n def test_cudnn_rnn_basics():\n     input_size = 10\n     timesteps = 6\n@@ -195,10 +189,7 @@ def test_cudnn_rnn_basics():\n \n \n @keras_test\n-@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n-                    reason='Requires GPU')\n+@skipif_no_tf_gpu\n def test_trainability():\n     input_size = 10\n     units = 2\n@@ -219,10 +210,7 @@ def test_trainability():\n \n \n @keras_test\n-@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n-                    reason='Requires GPU')\n+@skipif_no_tf_gpu\n def test_regularizer():\n     input_size = 10\n     timesteps = 6\n@@ -249,10 +237,7 @@ def test_regularizer():\n \n \n @keras_test\n-@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n-                    reason='Requires GPU')\n+@skipif_no_tf_gpu\n def test_return_state():\n     input_size = 10\n     timesteps = 6\n@@ -276,10 +261,7 @@ def test_return_state():\n \n \n @keras_test\n-@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n-                    reason='Requires GPU')\n+@skipif_no_tf_gpu\n def test_specify_initial_state_keras_tensor():\n     input_size = 10\n     timesteps = 6\n@@ -308,10 +290,7 @@ def test_specify_initial_state_keras_tensor():\n \n \n @keras_test\n-@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n-                    reason='Requires GPU')\n+@skipif_no_tf_gpu\n def test_statefulness():\n     input_size = 10\n     timesteps = 6\n@@ -358,67 +337,7 @@ def test_statefulness():\n \n \n @keras_test\n-@pytest.mark.parametrize('implementation', [1, 2], ids=['impl1', 'impl2'])\n-@pytest.mark.parametrize('bidirectional', [False, True], ids=['single', 'bidirectional'])\n-@pytest.mark.parametrize('to_cudnn', [False, True], ids=['from_cudnn', 'to_cudnn'])\n-@pytest.mark.parametrize('rnn_type', ['LSTM', 'GRU'], ids=['LSTM', 'GRU'])\n-@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n-                    reason='Requires GPU')\n-def test_load_weights_between_noncudnn_rnn(rnn_type, to_cudnn, bidirectional, implementation):\n-    input_size = 10\n-    timesteps = 6\n-    input_shape = (timesteps, input_size)\n-    units = 2\n-    num_samples = 32\n-    inputs = np.random.random((num_samples, timesteps, input_size))\n-\n-    rnn_layer_kwargs = {\n-        'recurrent_activation': 'sigmoid',\n-        # ensure biases are non-zero and properly converted\n-        'bias_initializer': 'random_uniform',\n-        'implementation': implementation\n-    }\n-    if rnn_type == 'LSTM':\n-        rnn_layer_class = keras.layers.LSTM\n-        cudnn_rnn_layer_class = keras.layers.CuDNNLSTM\n-    else:\n-        rnn_layer_class = keras.layers.GRU\n-        cudnn_rnn_layer_class = keras.layers.CuDNNGRU\n-        rnn_layer_kwargs['reset_after'] = True\n-\n-    def convert_weights(source_layer, target_layer):\n-        weights = source_layer.get_weights()\n-        weights = keras.engine.saving.preprocess_weights_for_loading(target_layer, weights)\n-        target_layer.set_weights(weights)\n-\n-    input_layer = keras.layers.InputLayer(input_shape)\n-\n-    layer = rnn_layer_class(units, **rnn_layer_kwargs)\n-    if bidirectional:\n-        layer = keras.layers.Bidirectional(layer)\n-\n-    cudnn_layer = cudnn_rnn_layer_class(units)\n-    if bidirectional:\n-        cudnn_layer = keras.layers.Bidirectional(cudnn_layer)\n-\n-    model = keras.models.Sequential([input_layer, layer])\n-    cudnn_model = keras.models.Sequential([input_layer, cudnn_layer])\n-\n-    if to_cudnn:\n-        convert_weights(layer, cudnn_layer)\n-    else:\n-        convert_weights(cudnn_layer, layer)\n-\n-    assert_allclose(model.predict(inputs), cudnn_model.predict(inputs), atol=1e-4)\n-\n-\n-@keras_test\n-@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n-                    reason='Requires GPU')\n+@skipif_no_tf_gpu\n def test_cudnnrnn_bidirectional():\n     rnn = keras.layers.CuDNNGRU\n     samples = 2\n@@ -471,38 +390,5 @@ def test_cudnnrnn_bidirectional():\n     model.fit(x, y, epochs=1, batch_size=1)\n \n \n-@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n-                    reason='Requires GPU')\n-def test_preprocess_weights_for_loading_gru_incompatible():\n-    \"\"\"\n-    Loading weights between incompatible layers should fail fast with an exception.\n-    \"\"\"\n-    def gru(cudnn=False, **kwargs):\n-        layer_class = keras.layers.CuDNNGRU if cudnn else keras.layers.GRU\n-        return layer_class(2, input_shape=[3, 5], **kwargs)\n-\n-    def initialize_weights(layer):\n-        # A model is needed to initialize weights.\n-        _ = keras.models.Sequential([layer])\n-        return layer\n-\n-    def assert_not_compatible(src, dest, message):\n-        with pytest.raises(ValueError) as ex:\n-            keras.engine.saving.preprocess_weights_for_loading(\n-                dest, initialize_weights(src).get_weights())\n-        assert message in ex.value.message\n-\n-    assert_not_compatible(gru(), gru(cudnn=True),\n-                          'GRU(reset_after=False) is not compatible with CuDNNGRU')\n-    assert_not_compatible(gru(cudnn=True), gru(),\n-                          'CuDNNGRU is not compatible with GRU(reset_after=False)')\n-    assert_not_compatible(gru(), gru(reset_after=True),\n-                          'GRU(reset_after=False) is not compatible with GRU(reset_after=True)')\n-    assert_not_compatible(gru(reset_after=True), gru(),\n-                          'GRU(reset_after=True) is not compatible with GRU(reset_after=False)')\n-\n-\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -7,10 +7,11 @@ from numpy.testing import assert_allclose\n from numpy.testing import assert_raises\n \n from keras import backend as K\n+from keras.engine.saving import preprocess_weights_for_loading\n from keras.models import Model, Sequential\n-from keras.layers import Dense, Lambda, RepeatVector, TimeDistributed, LSTM\n+from keras.layers import Dense, Lambda, RepeatVector, TimeDistributed, Bidirectional, GRU, LSTM, CuDNNGRU, CuDNNLSTM\n from keras.layers import Conv2D, Flatten\n-from keras.layers import Input\n+from keras.layers import Input, InputLayer\n from keras import optimizers\n from keras import losses\n from keras import metrics\n@@ -18,6 +19,11 @@ from keras.utils.test_utils import keras_test\n from keras.models import save_model, load_model\n \n \n+skipif_no_tf_gpu = pytest.mark.skipif(\n+    (K.backend() != 'tensorflow') or (not K.tensorflow_backend._get_available_gpus()),\n+    reason='Requires TensorFlow backend and a GPU')\n+\n+\n @keras_test\n def test_sequential_model_saving():\n     model = Sequential()\n@@ -607,5 +613,113 @@ def test_saving_recurrent_layer_without_bias():\n     os.remove(fname)\n \n \n+@keras_test\n+@pytest.mark.parametrize('implementation', [1, 2], ids=['impl1', 'impl2'])\n+@pytest.mark.parametrize('bidirectional', [False, True], ids=['single', 'bidirectional'])\n+@pytest.mark.parametrize('to_cudnn', [False, True], ids=['from_cudnn', 'to_cudnn'])\n+@pytest.mark.parametrize('rnn_type', ['LSTM', 'GRU'], ids=['LSTM', 'GRU'])\n+@pytest.mark.parametrize('model_nest_level', [1, 2], ids=['model_plain', 'model_nested'])\n+@pytest.mark.parametrize('model_type', ['func', 'seq'], ids=['model_func', 'model_seq'])\n+@skipif_no_tf_gpu\n+def test_load_weights_between_noncudnn_rnn(rnn_type, to_cudnn, bidirectional, implementation,\n+                                           model_nest_level, model_type):\n+    input_size = 10\n+    timesteps = 6\n+    input_shape = (timesteps, input_size)\n+    units = 2\n+    num_samples = 32\n+    inputs = np.random.random((num_samples, timesteps, input_size))\n+\n+    rnn_layer_kwargs = {\n+        'recurrent_activation': 'sigmoid',\n+        # ensure biases are non-zero and properly converted\n+        'bias_initializer': 'random_uniform',\n+        'implementation': implementation\n+    }\n+    if rnn_type == 'LSTM':\n+        rnn_layer_class = LSTM\n+        cudnn_rnn_layer_class = CuDNNLSTM\n+    else:\n+        rnn_layer_class = GRU\n+        cudnn_rnn_layer_class = CuDNNGRU\n+        rnn_layer_kwargs['reset_after'] = True\n+\n+    def convert_model(source_model, target_model):\n+        _, fname = tempfile.mkstemp('.h5')\n+        source_model.save_weights(fname)\n+        target_model.load_weights(fname)\n+        os.remove(fname)\n+\n+    layer = rnn_layer_class(units, **rnn_layer_kwargs)\n+    if bidirectional:\n+        layer = Bidirectional(layer)\n+\n+    cudnn_layer = cudnn_rnn_layer_class(units)\n+    if bidirectional:\n+        cudnn_layer = Bidirectional(cudnn_layer)\n+\n+    model = _make_nested_model(input_shape, layer, model_nest_level, model_type)\n+    cudnn_model = _make_nested_model(input_shape, cudnn_layer, model_nest_level, model_type)\n+\n+    if to_cudnn:\n+        convert_model(model, cudnn_model)\n+    else:\n+        convert_model(cudnn_model, model)\n+\n+    assert_allclose(model.predict(inputs), cudnn_model.predict(inputs), atol=1e-4)\n+\n+\n+def _make_nested_model(input_shape, layer, level=1, model_type='func'):\n+    # example: make_nested_seq_model((1,), Dense(10), level=2).summary()\n+    def make_nested_seq_model(input_shape, layer, level=1):\n+        model = layer\n+        for i in range(1, level + 1):\n+            layers = [InputLayer(input_shape), model] if (i == 1) else [model]\n+            model = Sequential(layers)\n+        return model\n+\n+    # example: make_nested_func_model((1,), Dense(10), level=2).summary()\n+    def make_nested_func_model(input_shape, layer, level=1):\n+        input = Input(input_shape)\n+        model = layer\n+        for i in range(level):\n+            model = Model(input, model(input))\n+        return model\n+\n+    if model_type == 'func':\n+        return make_nested_func_model(input_shape, layer, level)\n+    elif model_type == 'seq':\n+        return make_nested_seq_model(input_shape, layer, level)\n+\n+\n+@skipif_no_tf_gpu\n+def test_preprocess_weights_for_loading_gru_incompatible():\n+    \"\"\"\n+    Loading weights between incompatible layers should fail fast with an exception.\n+    \"\"\"\n+    def gru(cudnn=False, **kwargs):\n+        layer_class = CuDNNGRU if cudnn else GRU\n+        return layer_class(2, input_shape=[3, 5], **kwargs)\n+\n+    def initialize_weights(layer):\n+        # A model is needed to initialize weights.\n+        _ = Sequential([layer])\n+        return layer\n+\n+    def assert_not_compatible(src, dest, message):\n+        with pytest.raises(ValueError) as ex:\n+            preprocess_weights_for_loading(dest, initialize_weights(src).get_weights())\n+        assert message in ex.value.message\n+\n+    assert_not_compatible(gru(), gru(cudnn=True),\n+                          'GRU(reset_after=False) is not compatible with CuDNNGRU')\n+    assert_not_compatible(gru(cudnn=True), gru(),\n+                          'CuDNNGRU is not compatible with GRU(reset_after=False)')\n+    assert_not_compatible(gru(), gru(reset_after=True),\n+                          'GRU(reset_after=False) is not compatible with GRU(reset_after=True)')\n+    assert_not_compatible(gru(reset_after=True), gru(),\n+                          'GRU(reset_after=True) is not compatible with GRU(reset_after=False)')\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
