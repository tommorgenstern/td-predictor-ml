{"custom_id": "keras#fde40f288bae02b49c93e3a06419b5d59540c1d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 19 | Churn Cumulative: 8981 | Contributors (this commit): 92 | Commits (past 90d): 23 | Contributors (cumulative): 92 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3021,25 +3021,6 @@ def in_top_k(predictions, targets, k):\n \n # CONVOLUTIONS\n \n-def _preprocess_deconv3d_output_shape(x, shape, data_format):\n-    \"\"\"Get the output_shape for the 3D deconvolution.\n-\n-    # Arguments\n-        x: input tensor.\n-        shape: output shape.\n-        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n-\n-    # Returns\n-        The output shape.\n-    \"\"\"\n-    if data_format == 'channels_first':\n-        shape = (shape[0], shape[2], shape[3], shape[4], shape[1])\n-\n-    if shape[0] is None:\n-        shape = (tf.shape(x)[0], ) + tuple(shape[1:])\n-        shape = tf.stack(list(shape))\n-    return shape\n-\n \n def _preprocess_conv2d_input(x, data_format):\n     \"\"\"Transpose and cast the input before the conv2d.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5cd2ad0470f58ddd2115f20ac180aaab2f5b1fe9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 8772 | Contributors (this commit): 47 | Commits (past 90d): 8 | Contributors (cumulative): 47 | DMM Complexity: None\n\nDIFF:\n@@ -782,9 +782,6 @@ class SimpleRNNCell(Layer):\n             (see [regularizer](../regularizers.md)).\n         bias_regularizer: Regularizer function applied to the bias vector\n             (see [regularizer](../regularizers.md)).\n-        activity_regularizer: Regularizer function applied to\n-            the output of the layer (its \"activation\").\n-            (see [regularizer](../regularizers.md)).\n         kernel_constraint: Constraint function applied to\n             the `kernel` weights matrix\n             (see [constraints](../constraints.md)).\n@@ -1159,9 +1156,6 @@ class GRUCell(Layer):\n             (see [regularizer](../regularizers.md)).\n         bias_regularizer: Regularizer function applied to the bias vector\n             (see [regularizer](../regularizers.md)).\n-        activity_regularizer: Regularizer function applied to\n-            the output of the layer (its \"activation\").\n-            (see [regularizer](../regularizers.md)).\n         kernel_constraint: Constraint function applied to\n             the `kernel` weights matrix\n             (see [constraints](../constraints.md)).\n@@ -1634,9 +1628,6 @@ class LSTMCell(Layer):\n             (see [regularizer](../regularizers.md)).\n         bias_regularizer: Regularizer function applied to the bias vector\n             (see [regularizer](../regularizers.md)).\n-        activity_regularizer: Regularizer function applied to\n-            the output of the layer (its \"activation\").\n-            (see [regularizer](../regularizers.md)).\n         kernel_constraint: Constraint function applied to\n             the `kernel` weights matrix\n             (see [constraints](../constraints.md)).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a118a178026815b014ce833bcfa539ae7630bfa7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 508 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -293,10 +293,11 @@ if __name__ == '__main__':\n             'params_discriminator_epoch_{0:03d}.hdf5'.format(epoch), True)\n \n         # generate some digits to display\n-        noise = np.random.uniform(-1, 1, (100, latent_size))\n+        num_rows = 10\n+        noise = np.random.uniform(-1, 1, (num_rows * num_classes, latent_size))\n \n         sampled_labels = np.array([\n-            [i] * num_classes for i in range(num_classes)\n+            [i] * num_rows for i in range(num_classes)\n         ]).reshape(-1, 1)\n \n         # get a batch to display\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1662e8a6d2d50c413683b000e48854017ae22c51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 71 | Churn Cumulative: 12045 | Contributors (this commit): 95 | Commits (past 90d): 35 | Contributors (cumulative): 111 | DMM Complexity: 0.8823529411764706\n\nDIFF:\n@@ -2282,11 +2282,21 @@ def print_tensor(x, message=''):\n class Function(object):\n     \"\"\"Runs a computation graph.\n \n+    It's possible to pass arguments to `tf.Session.run()` via `session_kwargs`.\n+    In particular additonal operations via `fetches` argument and additional\n+    tensor substitutions via `feed_dict` arguments. Note that given\n+    substitutions are merged with substitutions from `inputs`. Even though\n+    `feed_dict` is passed once in the constructor (called in `model.compile()`)\n+    we can modify the values in the dictionary. Through this feed_dict we can\n+    provide additional substitutions besides Keras inputs.\n+\n     # Arguments\n         inputs: Feed placeholders to the computation graph.\n         outputs: Output tensors to fetch.\n         updates: Additional update ops to be run at function call.\n         name: a name to help users identify what this function does.\n+        session_kwargs: arguments to `tf.Session.run()`: `fetches`, `feed_dict`,\n+        `options`, `run_metadata`\n     \"\"\"\n \n     def __init__(self, inputs, outputs, updates=None, name=None, **session_kwargs):\n@@ -2313,12 +2323,18 @@ class Function(object):\n                     updates_ops.append(update)\n             self.updates_op = tf.group(*updates_ops)\n         self.name = name\n+        # additional tensor substitutions\n+        self.feed_dict = session_kwargs.pop('feed_dict', {})\n+        # additional operations\n+        self.fetches = session_kwargs.pop('fetches', [])\n+        if not isinstance(self.fetches, list):\n+            self.fetches = [self.fetches]\n         self.session_kwargs = session_kwargs\n \n     def __call__(self, inputs):\n         if not isinstance(inputs, (list, tuple)):\n             raise TypeError('`inputs` should be a list or tuple.')\n-        feed_dict = {}\n+        feed_dict = self.feed_dict.copy()\n         for tensor, value in zip(self.inputs, inputs):\n             if is_sparse(tensor):\n                 sparse_coo = value.tocoo()\n@@ -2326,9 +2342,9 @@ class Function(object):\n                                           np.expand_dims(sparse_coo.col, 1)), 1)\n                 value = (indices, sparse_coo.data, sparse_coo.shape)\n             feed_dict[tensor] = value\n+        fetches = self.outputs + [self.updates_op] + self.fetches\n         session = get_session()\n-        updated = session.run(self.outputs + [self.updates_op],\n-                              feed_dict=feed_dict,\n+        updated = session.run(fetches=fetches, feed_dict=feed_dict,\n                               **self.session_kwargs)\n         return updated[:len(self.outputs)]\n \n\n@@ -480,6 +480,55 @@ class TestBackend(object):\n         new_val_list = [k.get_value(x) for x, k in zip(x_list, test_backend)]\n         assert_list_pairwise(new_val_list)\n \n+    def test_function_tf_fetches(self):\n+        # Additional operations can be passed to tf.Session().run() via its\n+        # `fetches` arguments. In contrast to `updates` argument of\n+        # KTF.function() these do not have control dependency on `outputs`, so\n+        # they can run in parallel. Also they should not contribute to output of\n+        # KTF.function().\n+\n+        x = KTF.variable(0.)\n+        y = KTF.variable(0.)\n+        x_placeholder = KTF.placeholder(shape=())\n+        y_placeholder = KTF.placeholder(shape=())\n+\n+        f = KTF.function(inputs=[x_placeholder, y_placeholder],\n+                         outputs=[x_placeholder + y_placeholder],\n+                         updates=[(x, x_placeholder + 1.)],\n+                         fetches=[KTF.update(y, 5.)])\n+        output = f([10., 20.])\n+        assert output == [30.]\n+        assert KTF.get_session().run(fetches=[x, y]) == [11., 5.]\n+\n+    def test_function_tf_feed_dict(self):\n+        # Additional substitutions can be passed to `tf.Session().run()` via its\n+        # `feed_dict` arguments. Note that the feed_dict is passed once in the\n+        # constructor but we can modify the values in the dictionary. Through\n+        # this feed_dict we can provide additional substitutions besides Keras\n+        # inputs.\n+\n+        x = KTF.variable(0.)\n+        y = KTF.variable(0.)\n+        x_placeholder = KTF.placeholder(shape=())\n+        y_placeholder = KTF.placeholder(shape=())\n+\n+        feed_dict = {y_placeholder: 3.}\n+\n+        f = KTF.function(inputs=[x_placeholder],\n+                         outputs=[x_placeholder + 1.],\n+                         updates=[(x, x_placeholder + 10.)],\n+                         feed_dict=feed_dict,\n+                         fetches=[KTF.update(y, y_placeholder * 10.)])\n+        output = f([10.])\n+        assert output == [11.]\n+        assert KTF.get_session().run(fetches=[x, y]) == [20., 30.]\n+\n+        # updated value in feed_dict will be modified within the K.function()\n+        feed_dict[y_placeholder] = 4.\n+        output = f([20.])\n+        assert output == [21.]\n+        assert KTF.get_session().run(fetches=[x, y]) == [30., 40.]\n+\n     def test_rnn(self):\n         # implement a simple RNN\n         num_samples = 4\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#211ec41c1c4f62cb58a55e111010e0492d7463d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 349 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,7 +1,6 @@\n from __future__ import absolute_import\n from ..utils.data_utils import get_file\n from ..preprocessing.sequence import _remove_long_seq\n-from six.moves import zip\n import numpy as np\n import json\n import warnings\n@@ -55,6 +54,7 @@ def load_data(path='imdb.npz', num_words=None, skip_top=0,\n         x_train, labels_train = f['x_train'], f['y_train']\n         x_test, labels_test = f['x_test'], f['y_test']\n \n+    np.random.seed(seed)\n     indices = np.arange(len(x_train))\n     np.random.shuffle(indices)\n     x_train = x_train[indices]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#99fb182983e8691fcb0d267081e86564a687c1c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 8777 | Contributors (this commit): 47 | Commits (past 90d): 9 | Contributors (cumulative): 47 | DMM Complexity: None\n\nDIFF:\n@@ -243,9 +243,8 @@ class RNN(Layer):\n             at the level of the first layer\n             (e.g. via the `input_shape` argument)\n \n-    # Input shapes\n-        3D tensor with shape `(batch_size, timesteps, input_dim)`,\n-        (Optional) 2D tensors with shape `(batch_size, output_dim)`.\n+    # Input shape\n+        3D tensor with shape `(batch_size, timesteps, input_dim)`.\n \n     # Output shape\n         - if `return_state`: a list of tensors. The first tensor is\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3292aa5a30350c67627f173ceac713956f68271f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 21 | Churn Cumulative: 9662 | Contributors (this commit): 49 | Commits (past 90d): 17 | Contributors (cumulative): 58 | DMM Complexity: 1.0\n\nDIFF:\n@@ -731,6 +731,8 @@ class RNN(Layer):\n \n     @property\n     def trainable_weights(self):\n+        if not self.trainable:\n+            return []\n         if isinstance(self.cell, Layer):\n             return self.cell.trainable_weights\n         return []\n@@ -738,6 +740,8 @@ class RNN(Layer):\n     @property\n     def non_trainable_weights(self):\n         if isinstance(self.cell, Layer):\n+            if not self.trainable:\n+                return self.cell.weights\n             return self.cell.non_trainable_weights\n         return []\n \n\n@@ -212,6 +212,23 @@ def test_regularizer(layer_class):\n     assert len(layer.get_losses_for(x)) == 1\n \n \n+@rnn_test\n+def test_trainability(layer_class):\n+    layer = layer_class(units)\n+    layer.build((None, None, embedding_dim))\n+    assert len(layer.weights) == 3\n+    assert len(layer.trainable_weights) == 3\n+    assert len(layer.non_trainable_weights) == 0\n+    layer.trainable = False\n+    assert len(layer.weights) == 3\n+    assert len(layer.trainable_weights) == 0\n+    assert len(layer.non_trainable_weights) == 3\n+    layer.trainable = True\n+    assert len(layer.weights) == 3\n+    assert len(layer.trainable_weights) == 3\n+    assert len(layer.non_trainable_weights) == 0\n+\n+\n @keras_test\n def test_masking_layer():\n     ''' This test based on a previously failing issue here:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bbcf0e60af52bcee17f4aa61d5b29f7033176083", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 5468 | Contributors (this commit): 67 | Commits (past 90d): 21 | Contributors (cumulative): 67 | DMM Complexity: None\n\nDIFF:\n@@ -22,7 +22,6 @@ from .. import optimizers\n from .. import losses\n from .. import metrics as metrics_module\n from ..utils.generic_utils import Progbar\n-from ..utils.layer_utils import count_params\n from .. import callbacks as cbks\n from ..legacy import interfaces\n \n@@ -967,8 +966,8 @@ class Model(Container):\n         if not hasattr(self, '_collected_trainable_weights'):\n             return\n \n-        if (count_params(self.trainable_weights) !=\n-                count_params(self._collected_trainable_weights)):\n+        if (len(self.trainable_weights) !=\n+                len(self._collected_trainable_weights)):\n             warnings.warn(UserWarning(\n                 'Discrepancy between trainable weights and collected trainable'\n                 ' weights, did you set `model.trainable` without calling'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fabb40b2183de521cc3ef4884856e89c857bfdc6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 9176 | Contributors (this commit): 95 | Commits (past 90d): 28 | Contributors (cumulative): 98 | DMM Complexity: 0.5\n\nDIFF:\n@@ -1,5 +1,4 @@\n import tensorflow as tf\n-from tensorflow.python.client import device_lib\n from tensorflow.python.training import moving_averages\n from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import control_flow_ops\n@@ -45,9 +44,10 @@ _GRAPH_UID_DICTS = {}\n # Change its value via `manual_variable_initialization(value)`.\n _MANUAL_VAR_INIT = False\n \n-# This list queries the devices.\n+# This list holds the available devices.\n+# It is populated when `_get_available_gpus()` is called for the first time.\n # We assume our devices don't change during our lifetime.\n-_LOCAL_DEVICES = device_lib.list_local_devices()\n+_LOCAL_DEVICES = None\n \n \n def get_uid(prefix=''):\n@@ -174,6 +174,7 @@ def get_session():\n             for v in variables:\n                 if not getattr(v, '_keras_initialized', False):\n                     candidate_vars.append(v)\n+            if candidate_vars:\n                 # This step is expensive, so we only run it on variables\n                 # not already marked as initialized.\n                 is_initialized = session.run(\n@@ -250,6 +251,9 @@ def _get_available_gpus():\n     # Returns\n         A list of available GPU devices.\n     \"\"\"\n+    global _LOCAL_DEVICES\n+    if _LOCAL_DEVICES is None:\n+        _LOCAL_DEVICES = get_session().list_devices()\n     return [x.name for x in _LOCAL_DEVICES if x.device_type == 'GPU']\n \n \n\n@@ -5,13 +5,11 @@ from ..engine.training import Model\n \n \n def _get_available_devices():\n-    from tensorflow.python.client import device_lib\n-    local_device_protos = device_lib.list_local_devices()\n-    return [x.name for x in local_device_protos]\n+    return [x.name for x in K.get_session().list_devices()]\n \n \n def _normalize_device_name(name):\n-    name = name.lower().replace('device:', '')\n+    name = '/' + name.lower().split('device:')[1]\n     return name\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a8a59c71eccb0bea3ef17feeed6970112ab4aa8e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 108 | Lines Deleted: 30 | Files Changed: 2 | Hunks: 23 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 138 | Churn Cumulative: 681 | Contributors (this commit): 10 | Commits (past 90d): 7 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -377,5 +377,65 @@ def test_load_weights_into_noncudnn_lstm():\n     assert_allclose(out, cudnn_out, atol=1e-4)\n \n \n+@keras_test\n+@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n+                    reason='Requires TensorFlow backend')\n+@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n+                    reason='Requires GPU')\n+def test_cudnnrnn_bidirectional():\n+    rnn = keras.layers.CuDNNGRU\n+    samples = 2\n+    dim = 2\n+    timesteps = 2\n+    output_dim = 2\n+    dropout_rate = 0.2\n+    mode = 'concat'\n+\n+    x = np.random.random((samples, timesteps, dim))\n+    target_dim = 2 * output_dim if mode == 'concat' else output_dim\n+    y = np.random.random((samples, target_dim))\n+\n+    # test with Sequential model\n+    model = Sequential()\n+    model.add(wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n+                                         recurrent_dropout=dropout_rate),\n+                                     merge_mode=mode,\n+                                     input_shape=(None, dim)))\n+    model.compile(loss='mse', optimizer='sgd')\n+    model.fit(x, y, epochs=1, batch_size=1)\n+\n+    # test config\n+    model.get_config()\n+    model = model_from_json(model.to_json())\n+    model.summary()\n+\n+    # test stacked bidirectional layers\n+    model = Sequential()\n+    model.add(wrappers.Bidirectional(rnn(output_dim,\n+                                         return_sequences=True),\n+                                     merge_mode=mode,\n+                                     input_shape=(None, dim)))\n+    model.add(wrappers.Bidirectional(rnn(output_dim), merge_mode=mode))\n+    model.compile(loss='mse', optimizer='sgd')\n+    model.fit(x, y, epochs=1, batch_size=1)\n+\n+    # test with functional API\n+    inputs = Input((timesteps, dim))\n+    outputs = wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n+                                         recurrent_dropout=dropout_rate),\n+                                     merge_mode=mode)(inputs)\n+    model = Model(inputs, outputs)\n+    model.compile(loss='mse', optimizer='sgd')\n+    model.fit(x, y, epochs=1, batch_size=1)\n+\n+    # Bidirectional and stateful\n+    inputs = Input(batch_shape=(1, timesteps, dim))\n+    outputs = wrappers.Bidirectional(rnn(output_dim, stateful=True),\n+                                     merge_mode=mode)(inputs)\n+    model = Model(inputs, outputs)\n+    model.compile(loss='mse', optimizer='sgd')\n+    model.fit(x, y, epochs=1, batch_size=1)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -3,7 +3,7 @@ import numpy as np\n from numpy.testing import assert_allclose\n from keras.utils.test_utils import keras_test\n from keras.layers import wrappers, Input\n-from keras.layers import core, convolutional, recurrent, embeddings, normalization\n+from keras import layers\n from keras.models import Sequential, Model, model_from_json\n from keras import backend as K\n from keras.engine.topology import _object_list_uid\n@@ -13,10 +13,12 @@ from keras.engine.topology import _object_list_uid\n def test_TimeDistributed():\n     # first, test with Dense layer\n     model = Sequential()\n-    model.add(wrappers.TimeDistributed(core.Dense(2), input_shape=(3, 4)))\n-    model.add(core.Activation('relu'))\n+    model.add(wrappers.TimeDistributed(layers.Dense(2), input_shape=(3, 4)))\n+    model.add(layers.Activation('relu'))\n     model.compile(optimizer='rmsprop', loss='mse')\n-    model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 2)), epochs=1, batch_size=10)\n+    model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 2)),\n+              epochs=1,\n+              batch_size=10)\n \n     # test config\n     model.get_config()\n@@ -27,8 +29,9 @@ def test_TimeDistributed():\n     weights = model.layers[0].get_weights()\n \n     reference = Sequential()\n-    reference.add(wrappers.TimeDistributed(core.Dense(2), batch_input_shape=(1, 3, 4)))\n-    reference.add(core.Activation('relu'))\n+    reference.add(wrappers.TimeDistributed(layers.Dense(2),\n+                                           batch_input_shape=(1, 3, 4)))\n+    reference.add(layers.Activation('relu'))\n     reference.compile(optimizer='rmsprop', loss='mse')\n     reference.layers[0].set_weights(weights)\n \n@@ -37,9 +40,12 @@ def test_TimeDistributed():\n \n     # test with Embedding\n     model = Sequential()\n-    model.add(wrappers.TimeDistributed(embeddings.Embedding(5, 6), batch_input_shape=(10, 3, 4), dtype='int32'))\n+    model.add(wrappers.TimeDistributed(layers.Embedding(5, 6),\n+                                       batch_input_shape=(10, 3, 4),\n+                                       dtype='int32'))\n     model.compile(optimizer='rmsprop', loss='mse')\n-    model.fit(np.random.randint(5, size=(10, 3, 4), dtype='int32'), np.random.random((10, 3, 4, 6)), epochs=1, batch_size=10)\n+    model.fit(np.random.randint(5, size=(10, 3, 4), dtype='int32'),\n+              np.random.random((10, 3, 4, 6)), epochs=1, batch_size=10)\n \n     # compare to not using batch_input_shape\n     test_input = np.random.randint(5, size=(10, 3, 4), dtype='int32')\n@@ -47,7 +53,8 @@ def test_TimeDistributed():\n     weights = model.layers[0].get_weights()\n \n     reference = Sequential()\n-    reference.add(wrappers.TimeDistributed(embeddings.Embedding(5, 6), input_shape=(3, 4), dtype='int32'))\n+    reference.add(wrappers.TimeDistributed(layers.Embedding(5, 6),\n+                                           input_shape=(3, 4), dtype='int32'))\n     reference.compile(optimizer='rmsprop', loss='mse')\n     reference.layers[0].set_weights(weights)\n \n@@ -56,41 +63,48 @@ def test_TimeDistributed():\n \n     # test with Conv2D\n     model = Sequential()\n-    model.add(wrappers.TimeDistributed(convolutional.Conv2D(5, (2, 2), padding='same'), input_shape=(2, 4, 4, 3)))\n-    model.add(core.Activation('relu'))\n+    model.add(wrappers.TimeDistributed(layers.Conv2D(5, (2, 2),\n+                                                     padding='same'),\n+                                       input_shape=(2, 4, 4, 3)))\n+    model.add(layers.Activation('relu'))\n     model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(np.random.random((1, 2, 4, 4, 3)), np.random.random((1, 2, 4, 4, 5)))\n+    model.train_on_batch(np.random.random((1, 2, 4, 4, 3)),\n+                         np.random.random((1, 2, 4, 4, 5)))\n \n     model = model_from_json(model.to_json())\n     model.summary()\n \n     # test stacked layers\n     model = Sequential()\n-    model.add(wrappers.TimeDistributed(core.Dense(2), input_shape=(3, 4)))\n-    model.add(wrappers.TimeDistributed(core.Dense(3)))\n-    model.add(core.Activation('relu'))\n+    model.add(wrappers.TimeDistributed(layers.Dense(2), input_shape=(3, 4)))\n+    model.add(wrappers.TimeDistributed(layers.Dense(3)))\n+    model.add(layers.Activation('relu'))\n     model.compile(optimizer='rmsprop', loss='mse')\n \n-    model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 3)), epochs=1, batch_size=10)\n+    model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 3)),\n+              epochs=1, batch_size=10)\n \n     # test wrapping Sequential model\n     model = Sequential()\n-    model.add(core.Dense(3, input_dim=2))\n+    model.add(layers.Dense(3, input_dim=2))\n     outer_model = Sequential()\n     outer_model.add(wrappers.TimeDistributed(model, input_shape=(3, 2)))\n     outer_model.compile(optimizer='rmsprop', loss='mse')\n-    outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)), epochs=1, batch_size=10)\n+    outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)),\n+                    epochs=1, batch_size=10)\n \n     # test with functional API\n     x = Input(shape=(3, 2))\n     y = wrappers.TimeDistributed(model)(x)\n     outer_model = Model(x, y)\n     outer_model.compile(optimizer='rmsprop', loss='mse')\n-    outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)), epochs=1, batch_size=10)\n+    outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)),\n+                    epochs=1, batch_size=10)\n \n     # test with BatchNormalization\n     model = Sequential()\n-    model.add(wrappers.TimeDistributed(normalization.BatchNormalization(center=True, scale=True),\n+    model.add(wrappers.TimeDistributed(\n+        layers.BatchNormalization(center=True, scale=True),\n         name='bn', input_shape=(10, 2)))\n     model.compile(optimizer='rmsprop', loss='mse')\n     # Assert that mean and variance are 0 and 1.\n@@ -117,7 +131,7 @@ def test_TimeDistributed_learning_phase():\n     # test layers that need learning_phase to be set\n     np.random.seed(1234)\n     x = Input(shape=(3, 2))\n-    y = wrappers.TimeDistributed(core.Dropout(.999))(x, training=True)\n+    y = wrappers.TimeDistributed(layers.Dropout(.999))(x, training=True)\n     model = Model(x, y)\n     y = model.predict(np.random.random((10, 3, 2)))\n     assert_allclose(np.mean(y), 0., atol=1e-1, rtol=1e-1)\n@@ -127,8 +141,8 @@ def test_TimeDistributed_learning_phase():\n def test_regularizers():\n     model = Sequential()\n     model.add(wrappers.TimeDistributed(\n-        core.Dense(2, kernel_regularizer='l1'), input_shape=(3, 4)))\n-    model.add(core.Activation('relu'))\n+        layers.Dense(2, kernel_regularizer='l1'), input_shape=(3, 4)))\n+    model.add(layers.Activation('relu'))\n     model.compile(optimizer='rmsprop', loss='mse')\n     assert len(model.layers[0].layer.losses) == 1\n     assert len(model.layers[0].losses) == 1\n@@ -137,15 +151,15 @@ def test_regularizers():\n \n     model = Sequential()\n     model.add(wrappers.TimeDistributed(\n-        core.Dense(2, activity_regularizer='l1'), input_shape=(3, 4)))\n-    model.add(core.Activation('relu'))\n+        layers.Dense(2, activity_regularizer='l1'), input_shape=(3, 4)))\n+    model.add(layers.Activation('relu'))\n     model.compile(optimizer='rmsprop', loss='mse')\n     assert len(model.losses) == 1\n \n \n @keras_test\n def test_Bidirectional():\n-    rnn = recurrent.SimpleRNN\n+    rnn = layers.SimpleRNN\n     samples = 2\n     dim = 2\n     timesteps = 2\n@@ -160,7 +174,8 @@ def test_Bidirectional():\n         model = Sequential()\n         model.add(wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n                                              recurrent_dropout=dropout_rate),\n-                                         merge_mode=mode, input_shape=(timesteps, dim)))\n+                                         merge_mode=mode,\n+                                         input_shape=(timesteps, dim)))\n         model.compile(loss='mse', optimizer='sgd')\n         model.fit(x, y, epochs=1, batch_size=1)\n \n@@ -171,8 +186,10 @@ def test_Bidirectional():\n \n         # test stacked bidirectional layers\n         model = Sequential()\n-        model.add(wrappers.Bidirectional(rnn(output_dim, return_sequences=True),\n-                                         merge_mode=mode, input_shape=(timesteps, dim)))\n+        model.add(wrappers.Bidirectional(rnn(output_dim,\n+                                             return_sequences=True),\n+                                         merge_mode=mode,\n+                                         input_shape=(timesteps, dim)))\n         model.add(wrappers.Bidirectional(rnn(output_dim), merge_mode=mode))\n         model.compile(loss='mse', optimizer='sgd')\n         model.fit(x, y, epochs=1, batch_size=1)\n@@ -188,7 +205,8 @@ def test_Bidirectional():\n \n         # Bidirectional and stateful\n         inputs = Input(batch_shape=(1, timesteps, dim))\n-        outputs = wrappers.Bidirectional(rnn(output_dim, stateful=True), merge_mode=mode)(inputs)\n+        outputs = wrappers.Bidirectional(rnn(output_dim, stateful=True),\n+                                         merge_mode=mode)(inputs)\n         model = Model(inputs, outputs)\n         model.compile(loss='mse', optimizer='sgd')\n         model.fit(x, y, epochs=1, batch_size=1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7334b9a30d7515b92458b292f53bb85b630bec63", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 20 | Files Changed: 5 | Hunks: 17 | Methods Changed: 8 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 54 | Churn Cumulative: 11187 | Contributors (this commit): 103 | Commits (past 90d): 43 | Contributors (cumulative): 120 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2460,6 +2460,9 @@ def rnn(step_function, inputs, initial_states,\n     ndim = len(inputs.get_shape())\n     if ndim < 3:\n         raise ValueError('Input should be at least 3D.')\n+\n+    # Transpose to time-major, i.e.\n+    # from (batch, time, ...) to (time, batch, ...)\n     axes = [1, 0] + list(range(2, ndim))\n     inputs = tf.transpose(inputs, (axes))\n \n\n@@ -24,6 +24,7 @@ class _CuDNNRNN(RNN):\n     def __init__(self,\n                  return_sequences=False,\n                  return_state=False,\n+                 go_backwards=False,\n                  stateful=False,\n                  **kwargs):\n         if K.backend() != 'tensorflow':\n@@ -32,6 +33,7 @@ class _CuDNNRNN(RNN):\n         super(RNN, self).__init__(**kwargs)\n         self.return_sequences = return_sequences\n         self.return_state = return_state\n+        self.go_backwards = go_backwards\n         self.stateful = stateful\n         self.supports_masking = False\n         self.input_spec = [InputSpec(ndim=3)]\n@@ -76,6 +78,9 @@ class _CuDNNRNN(RNN):\n                              str(len(initial_state)) +\n                              ' initial states.')\n \n+        if self.go_backwards:\n+            # Reverse time axis.\n+            inputs = K.reverse(inputs, 1)\n         output, states = self._process_batch(inputs, initial_state)\n \n         if self.stateful:\n@@ -92,6 +97,7 @@ class _CuDNNRNN(RNN):\n     def get_config(self):\n         config = {'return_sequences': self.return_sequences,\n                   'return_state': self.return_state,\n+                  'go_backwards': self.go_backwards,\n                   'stateful': self.stateful}\n         base_config = super(RNN, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n\n@@ -303,11 +303,8 @@ class Bidirectional(Wrapper):\n             output = [y, y_rev]\n \n         # Properly set learning phase\n-        if 0 < self.layer.dropout + self.layer.recurrent_dropout:\n-            if self.merge_mode is None:\n-                for out in output:\n-                    out._uses_learning_phase = True\n-            else:\n+        if (getattr(y, '_uses_learning_phase', False) or\n+           getattr(y_rev, '_uses_learning_phase', False)):\n             output._uses_learning_phase = True\n         return output\n \n\n@@ -183,6 +183,15 @@ def test_cudnn_rnn_basics():\n                     kwargs={'units': units,\n                             'return_sequences': return_sequences},\n                     input_shape=(num_samples, timesteps, input_size))\n+        for go_backwards in [True, False]:\n+            with keras.utils.CustomObjectScope(\n+                {'keras.layers.CuDNNGRU': keras.layers.CuDNNGRU,\n+                 'keras.layers.CuDNNLSTM': keras.layers.CuDNNLSTM}):\n+                layer_test(\n+                    layer_class,\n+                    kwargs={'units': units,\n+                            'go_backwards': go_backwards},\n+                    input_shape=(num_samples, timesteps, input_size))\n \n \n @keras_test\n@@ -388,7 +397,6 @@ def test_cudnnrnn_bidirectional():\n     dim = 2\n     timesteps = 2\n     output_dim = 2\n-    dropout_rate = 0.2\n     mode = 'concat'\n \n     x = np.random.random((samples, timesteps, dim))\n@@ -396,9 +404,8 @@ def test_cudnnrnn_bidirectional():\n     y = np.random.random((samples, target_dim))\n \n     # test with Sequential model\n-    model = Sequential()\n-    model.add(wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n-                                         recurrent_dropout=dropout_rate),\n+    model = keras.Sequential()\n+    model.add(keras.layers.Bidirectional(rnn(output_dim),\n                                          merge_mode=mode,\n                                          input_shape=(None, dim)))\n     model.compile(loss='mse', optimizer='sgd')\n@@ -406,33 +413,32 @@ def test_cudnnrnn_bidirectional():\n \n     # test config\n     model.get_config()\n-    model = model_from_json(model.to_json())\n+    model = keras.models.model_from_json(model.to_json())\n     model.summary()\n \n     # test stacked bidirectional layers\n-    model = Sequential()\n-    model.add(wrappers.Bidirectional(rnn(output_dim,\n+    model = keras.Sequential()\n+    model.add(keras.layers.Bidirectional(rnn(output_dim,\n                                              return_sequences=True),\n                                          merge_mode=mode,\n                                          input_shape=(None, dim)))\n-    model.add(wrappers.Bidirectional(rnn(output_dim), merge_mode=mode))\n+    model.add(keras.layers.Bidirectional(rnn(output_dim), merge_mode=mode))\n     model.compile(loss='mse', optimizer='sgd')\n     model.fit(x, y, epochs=1, batch_size=1)\n \n     # test with functional API\n-    inputs = Input((timesteps, dim))\n-    outputs = wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n-                                         recurrent_dropout=dropout_rate),\n+    inputs = keras.Input((timesteps, dim))\n+    outputs = keras.layers.Bidirectional(rnn(output_dim),\n                                          merge_mode=mode)(inputs)\n-    model = Model(inputs, outputs)\n+    model = keras.Model(inputs, outputs)\n     model.compile(loss='mse', optimizer='sgd')\n     model.fit(x, y, epochs=1, batch_size=1)\n \n     # Bidirectional and stateful\n-    inputs = Input(batch_shape=(1, timesteps, dim))\n-    outputs = wrappers.Bidirectional(rnn(output_dim, stateful=True),\n+    inputs = keras.Input(batch_shape=(1, timesteps, dim))\n+    outputs = keras.layers.Bidirectional(rnn(output_dim, stateful=True),\n                                          merge_mode=mode)(inputs)\n-    model = Model(inputs, outputs)\n+    model = keras.Model(inputs, outputs)\n     model.compile(loss='mse', optimizer='sgd')\n     model.fit(x, y, epochs=1, batch_size=1)\n \n\n@@ -199,6 +199,8 @@ def test_Bidirectional():\n         outputs = wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n                                              recurrent_dropout=dropout_rate),\n                                          merge_mode=mode)(inputs)\n+        if dropout_rate:\n+            assert outputs._uses_learning_phase\n         model = Model(inputs, outputs)\n         model.compile(loss='mse', optimizer='sgd')\n         model.fit(x, y, epochs=1, batch_size=1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#abcd7dba35fa8e687836adc1152a709b9f67cb7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 239 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -199,7 +199,8 @@ def test_Bidirectional():\n         outputs = wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n                                              recurrent_dropout=dropout_rate),\n                                          merge_mode=mode)(inputs)\n-        if dropout_rate:\n+        if dropout_rate and K.backend() != 'cntk':\n+            # Dropout is disabled with CNTK for now.\n             assert outputs._uses_learning_phase\n         model = Model(inputs, outputs)\n         model.compile(loss='mse', optimizer='sgd')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#208170eafc717dd2a6c6a5919d76ba82d0f19a04", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 875 | Contributors (this commit): 24 | Commits (past 90d): 13 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -267,7 +267,8 @@ class Progbar(object):\n         self.total_width = 0\n         self.seen_so_far = 0\n         self.verbose = verbose\n-        self._dynamic_display = (sys.stdout.isatty() or\n+        self._dynamic_display = ((hasattr(sys.stdout, 'isatty') and\n+                                  sys.stdout.isatty()) or\n                                  'ipykernel' in sys.modules)\n \n     def update(self, current, values=None, force=False):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f5494ef81c9458d10a7cc7307f7663eabb3ad072", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 132 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -18,14 +18,13 @@ def load_data():\n \n     num_train_samples = 50000\n \n-    x_train = np.zeros((num_train_samples, 3, 32, 32), dtype='uint8')\n-    y_train = np.zeros((num_train_samples,), dtype='uint8')\n+    x_train = np.empty((num_train_samples, 3, 32, 32), dtype='uint8')\n+    y_train = np.empty((num_train_samples,), dtype='uint8')\n \n     for i in range(1, 6):\n         fpath = os.path.join(path, 'data_batch_' + str(i))\n-        data, labels = load_batch(fpath)\n-        x_train[(i - 1) * 10000: i * 10000, :, :, :] = data\n-        y_train[(i - 1) * 10000: i * 10000] = labels\n+        (x_train[(i - 1) * 10000: i * 10000, :, :, :],\n+         y_train[(i - 1) * 10000: i * 10000]) = load_batch(fpath)\n \n     fpath = os.path.join(path, 'test_batch')\n     x_test, y_test = load_batch(fpath)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e4e01c5aae883f8131ebef455939f313e7125520", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 5470 | Contributors (this commit): 68 | Commits (past 90d): 21 | Contributors (cumulative): 68 | DMM Complexity: None\n\nDIFF:\n@@ -958,7 +958,7 @@ class Model(Container):\n         \"\"\"Check trainable weights count consistency.\n \n         This will raise a warning if `trainable_weights` and\n-        `_collected_trainable_weights` are consistent (i.e. have the same\n+        `_collected_trainable_weights` are inconsistent (i.e. have different\n         number of parameters).\n         Inconsistency will typically arise when one modifies `model.trainable`\n         without calling `model.compile` again.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1fec906ab0b047546fffd5ae4d67a688eac7158b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 525 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -43,28 +43,25 @@ from keras.utils.generic_utils import Progbar\n import numpy as np\n \n np.random.seed(1337)\n-\n-K.set_image_data_format('channels_first')\n-\n num_classes = 10\n \n \n def build_generator(latent_size):\n     # we will map a pair of (z, L), where z is a latent vector and L is a\n-    # label drawn from P_c, to image space (..., 1, 28, 28)\n+    # label drawn from P_c, to image space (..., 28, 28, 1)\n     cnn = Sequential()\n \n     cnn.add(Dense(1024, input_dim=latent_size, activation='relu'))\n     cnn.add(Dense(128 * 7 * 7, activation='relu'))\n-    cnn.add(Reshape((128, 7, 7)))\n+    cnn.add(Reshape((7, 7, 128)))\n \n-    # upsample to (..., 14, 14)\n+    # upsample to (14, 14, ...)\n     cnn.add(UpSampling2D(size=(2, 2)))\n     cnn.add(Conv2D(256, 5, padding='same',\n                    activation='relu',\n                    kernel_initializer='glorot_normal'))\n \n-    # upsample to (..., 28, 28)\n+    # upsample to (28, 28, ...)\n     cnn.add(UpSampling2D(size=(2, 2)))\n     cnn.add(Conv2D(128, 5, padding='same',\n                    activation='relu',\n@@ -98,7 +95,7 @@ def build_discriminator():\n     cnn = Sequential()\n \n     cnn.add(Conv2D(32, 3, padding='same', strides=2,\n-                   input_shape=(1, 28, 28)))\n+                   input_shape=(28, 28, 1)))\n     cnn.add(LeakyReLU())\n     cnn.add(Dropout(0.3))\n \n@@ -116,7 +113,7 @@ def build_discriminator():\n \n     cnn.add(Flatten())\n \n-    image = Input(shape=(1, 28, 28))\n+    image = Input(shape=(28, 28, 1))\n \n     features = cnn(image)\n \n@@ -166,7 +163,7 @@ if __name__ == '__main__':\n         loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n     )\n \n-    # get our mnist data, and force it to be of shape (..., 1, 28, 28) with\n+    # get our mnist data, and force it to be of shape (..., 28, 28, 1) with\n     # range [-1, 1]\n     (x_train, y_train), (x_test, y_test) = mnist.load_data()\n     x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c1072cc66cb7d678d4da58d88eaa3e1162f86e67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 534 | Contributors (this commit): 8 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -31,7 +31,6 @@ from PIL import Image\n \n from six.moves import range\n \n-import keras.backend as K\n from keras.datasets import mnist\n from keras import layers\n from keras.layers import Input, Dense, Reshape, Flatten, Embedding, Dropout\n@@ -138,11 +137,13 @@ if __name__ == '__main__':\n     adam_beta_1 = 0.5\n \n     # build the discriminator\n+    print('Discriminator model:')\n     discriminator = build_discriminator()\n     discriminator.compile(\n         optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n         loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n     )\n+    discriminator.summary()\n \n     # build the generator\n     generator = build_generator(latent_size)\n@@ -158,19 +159,21 @@ if __name__ == '__main__':\n     fake, aux = discriminator(fake)\n     combined = Model([latent, image_class], [fake, aux])\n \n+    print('Combined model:')\n     combined.compile(\n         optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n         loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n     )\n+    combined.summary()\n \n     # get our mnist data, and force it to be of shape (..., 28, 28, 1) with\n     # range [-1, 1]\n     (x_train, y_train), (x_test, y_test) = mnist.load_data()\n     x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n-    x_train = np.expand_dims(x_train, axis=1)\n+    x_train = np.expand_dims(x_train, axis=-1)\n \n     x_test = (x_test.astype(np.float32) - 127.5) / 127.5\n-    x_test = np.expand_dims(x_test, axis=1)\n+    x_test = np.expand_dims(x_test, axis=-1)\n \n     num_train, num_test = x_train.shape[0], x_test.shape[0]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f9532ce0697087dd3b80d2407515014df58393a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 265 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -29,6 +29,10 @@ def test_dropout():\n                kwargs={'rate': 0.5, 'noise_shape': [3, 1]},\n                input_shape=(3, 2))\n \n+    layer_test(layers.Dropout,\n+               kwargs={'rate': 0.5, 'noise_shape': [None, 1]},\n+               input_shape=(3, 2))\n+\n     layer_test(layers.SpatialDropout1D,\n                kwargs={'rate': 0.5},\n                input_shape=(2, 3, 4))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ae6b66a56b702d5dcbf8a99576001f6354d96990", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 541 | Contributors (this commit): 8 | Commits (past 90d): 9 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -208,7 +208,10 @@ if __name__ == '__main__':\n                 [noise, sampled_labels.reshape((-1, 1))], verbose=0)\n \n             x = np.concatenate((image_batch, generated_images))\n-            y = np.array([1] * batch_size + [0] * batch_size)\n+\n+            # use soft real/fake labels\n+            soft_zero, soft_one = 0.25, 0.75\n+            y = np.array([soft_one] * batch_size + [soft_zero] * batch_size)\n             aux_y = np.concatenate((label_batch, sampled_labels), axis=0)\n \n             # see if the discriminator can figure itself out...\n@@ -223,7 +226,7 @@ if __name__ == '__main__':\n             # we want to train the generator to trick the discriminator\n             # For the generator, we want all the {fake, not-fake} labels to say\n             # not-fake\n-            trick = np.ones(2 * batch_size)\n+            trick = np.ones(2 * batch_size) * soft_one\n \n             epoch_gen_loss.append(combined.train_on_batch(\n                 [noise, sampled_labels.reshape((-1, 1))],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
