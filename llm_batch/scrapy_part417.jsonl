{"custom_id": "scrapy#d400aa3e2d8c5e3d1a7d8a483fb1a5a6f1c66d50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 217 | Lines Deleted: 9 | Files Changed: 4 | Hunks: 19 | Methods Changed: 16 | Complexity Δ (Sum/Max): 26/11 | Churn Δ: 226 | Churn Cumulative: 3332 | Contributors (this commit): 36 | Commits (past 90d): 16 | Contributors (cumulative): 54 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,6 +21,7 @@ from scrapy.exceptions import (\n     ScrapyDeprecationWarning,\n )\n from scrapy.http import Request, Response\n+from scrapy.utils.asyncio import _parallel_asyncio, is_asyncio_available\n from scrapy.utils.defer import (\n     _defer_sleep,\n     aiter_errback,\n@@ -328,11 +329,21 @@ class Scraper:\n         response: Response,\n     ) -> None:\n         \"\"\"Pass items/requests produced by a callback to ``_process_spidermw_output()`` in parallel.\"\"\"\n+        it: Iterable[_T] | AsyncIterator[_T]\n+        if is_asyncio_available():\n             if isinstance(result, AsyncIterator):\n-            ait = aiter_errback(result, self.handle_spider_error, request, response)\n+                it = aiter_errback(result, self.handle_spider_error, request, response)\n+            else:\n+                it = iter_errback(result, self.handle_spider_error, request, response)\n+            await _parallel_asyncio(\n+                it, self.concurrent_items, self._process_spidermw_output_async, response\n+            )\n+            return\n+        if isinstance(result, AsyncIterator):\n+            it = aiter_errback(result, self.handle_spider_error, request, response)\n             await maybe_deferred_to_future(\n                 parallel_async(\n-                    ait,\n+                    it,\n                     self.concurrent_items,\n                     self._process_spidermw_output,\n                     response,\n@@ -349,8 +360,19 @@ class Scraper:\n             )\n         )\n \n-    @deferred_f_from_coro_f\n-    async def _process_spidermw_output(self, output: Any, response: Response) -> None:\n+    def _process_spidermw_output(\n+        self, output: Any, response: Response\n+    ) -> Deferred[None]:\n+        \"\"\"Process each Request/Item (given in the output parameter) returned\n+        from the given spider.\n+\n+        Items are sent to the item pipelines, requests are scheduled.\n+        \"\"\"\n+        return deferred_from_coro(self._process_spidermw_output_async(output, response))\n+\n+    async def _process_spidermw_output_async(\n+        self, output: Any, response: Response\n+    ) -> None:\n         \"\"\"Process each Request/Item (given in the output parameter) returned\n         from the given spider.\n \n\n@@ -1,7 +1,23 @@\n \"\"\"Utilities related to asyncio and its support in Scrapy.\"\"\"\n \n+from __future__ import annotations\n+\n+import asyncio\n+from typing import TYPE_CHECKING, Any, TypeVar\n+\n+from scrapy.utils.asyncgen import as_async_generator\n from scrapy.utils.reactor import is_asyncio_reactor_installed, is_reactor_installed\n \n+if TYPE_CHECKING:\n+    from collections.abc import AsyncIterator, Callable, Coroutine, Iterable\n+\n+    # typing.Concatenate and typing.ParamSpec require Python 3.10\n+    from typing_extensions import Concatenate, ParamSpec\n+\n+    _P = ParamSpec(\"_P\")\n+\n+_T = TypeVar(\"_T\")\n+\n \n def is_asyncio_available() -> bool:\n     \"\"\"Check if it's possible to call asyncio code that relies on the asyncio event loop.\n@@ -36,3 +52,41 @@ def is_asyncio_available() -> bool:\n         )\n \n     return is_asyncio_reactor_installed()\n+\n+\n+async def _parallel_asyncio(\n+    iterable: Iterable[_T] | AsyncIterator[_T],\n+    count: int,\n+    callable: Callable[Concatenate[_T, _P], Coroutine[Any, Any, None]],\n+    *args: _P.args,\n+    **kwargs: _P.kwargs,\n+) -> None:\n+    \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n+    using no more than ``count`` concurrent calls.\n+\n+    This function is only used in\n+    :meth:`scrapy.core.scraper.Scraper.handle_spider_output_async` and so it\n+    assumes that neither *callable* nor iterating *iterable* will raise an\n+    exception.\n+    \"\"\"\n+    queue: asyncio.Queue[_T | None] = asyncio.Queue()\n+\n+    async def worker() -> None:\n+        while True:\n+            item = await queue.get()\n+            if item is None:\n+                break\n+            try:\n+                await callable(item, *args, **kwargs)\n+            finally:\n+                queue.task_done()\n+\n+    async def fill_queue() -> None:\n+        async for item in as_async_generator(iterable):\n+            await queue.put(item)\n+        for _ in range(count):\n+            await queue.put(None)\n+\n+    fill_task = asyncio.create_task(fill_queue())\n+    work_tasks = [asyncio.create_task(worker()) for _ in range(count)]\n+    await asyncio.wait([fill_task, *work_tasks])\n\n@@ -1,6 +1,18 @@\n-import pytest\n+from __future__ import annotations\n \n-from scrapy.utils.asyncio import is_asyncio_available\n+import asyncio\n+import random\n+from typing import TYPE_CHECKING\n+\n+import pytest\n+from twisted.trial import unittest\n+\n+from scrapy.utils.asyncgen import as_async_generator\n+from scrapy.utils.asyncio import _parallel_asyncio, is_asyncio_available\n+from scrapy.utils.defer import deferred_f_from_coro_f\n+\n+if TYPE_CHECKING:\n+    from collections.abc import AsyncGenerator\n \n \n @pytest.mark.usefixtures(\"reactor_pytest\")\n@@ -8,3 +20,80 @@ class TestAsyncio:\n     def test_is_asyncio_available(self):\n         # the result should depend only on the pytest --reactor argument\n         assert is_asyncio_available() == (self.reactor_pytest != \"default\")\n+\n+\n+@pytest.mark.only_asyncio\n+class TestParallelAsyncio(unittest.TestCase):\n+    \"\"\"Test for scrapy.utils.asyncio.parallel_asyncio(), based on tests.test_utils_defer.TestParallelAsync.\"\"\"\n+\n+    CONCURRENT_ITEMS = 50\n+\n+    @staticmethod\n+    async def callable(o: int, results: list[int]) -> None:\n+        if random.random() < 0.4:\n+            # simulate async processing\n+            await asyncio.sleep(random.random() / 8)\n+        # simulate trivial sync processing\n+        results.append(o)\n+\n+    async def callable_wrapped(\n+        self,\n+        o: int,\n+        results: list[int],\n+        parallel_count: list[int],\n+        max_parallel_count: list[int],\n+    ) -> None:\n+        parallel_count[0] += 1\n+        max_parallel_count[0] = max(max_parallel_count[0], parallel_count[0])\n+        await self.callable(o, results)\n+        assert parallel_count[0] > 0, parallel_count[0]\n+        parallel_count[0] -= 1\n+\n+    @staticmethod\n+    def get_async_iterable(length: int) -> AsyncGenerator[int, None]:\n+        # simulate a simple callback without delays between results\n+        return as_async_generator(range(length))\n+\n+    @staticmethod\n+    async def get_async_iterable_with_delays(length: int) -> AsyncGenerator[int, None]:\n+        # simulate a callback with delays between some of the results\n+        for i in range(length):\n+            if random.random() < 0.1:\n+                await asyncio.sleep(random.random() / 20)\n+            yield i\n+\n+    @deferred_f_from_coro_f\n+    async def test_simple(self):\n+        for length in [20, 50, 100]:\n+            parallel_count = [0]\n+            max_parallel_count = [0]\n+            results = []\n+            ait = self.get_async_iterable(length)\n+            await _parallel_asyncio(\n+                ait,\n+                self.CONCURRENT_ITEMS,\n+                self.callable_wrapped,\n+                results,\n+                parallel_count,\n+                max_parallel_count,\n+            )\n+            assert list(range(length)) == sorted(results)\n+            assert max_parallel_count[0] <= self.CONCURRENT_ITEMS\n+\n+    @deferred_f_from_coro_f\n+    async def test_delays(self):\n+        for length in [20, 50, 100]:\n+            parallel_count = [0]\n+            max_parallel_count = [0]\n+            results = []\n+            ait = self.get_async_iterable_with_delays(length)\n+            await _parallel_asyncio(\n+                ait,\n+                self.CONCURRENT_ITEMS,\n+                self.callable_wrapped,\n+                results,\n+                parallel_count,\n+                max_parallel_count,\n+            )\n+            assert list(range(length)) == sorted(results)\n+            assert max_parallel_count[0] <= self.CONCURRENT_ITEMS\n\n@@ -164,7 +164,7 @@ class TestAsyncDefTestsuite(unittest.TestCase):\n         raise RuntimeError(\"This is expected to be raised\")\n \n \n-class TestAsyncCooperator(unittest.TestCase):\n+class TestParallelAsync(unittest.TestCase):\n     \"\"\"This tests _AsyncCooperatorAdapter by testing parallel_async which is its only usage.\n \n     parallel_async is called with the results of a callback (so an iterable of items, requests and None,\n@@ -194,6 +194,27 @@ class TestAsyncCooperator(unittest.TestCase):\n         results.append(o)\n         return None\n \n+    def callable_wrapped(\n+        self,\n+        o: int,\n+        results: list[int],\n+        parallel_count: list[int],\n+        max_parallel_count: list[int],\n+    ) -> Deferred[None] | None:\n+        parallel_count[0] += 1\n+        max_parallel_count[0] = max(max_parallel_count[0], parallel_count[0])\n+        dfd = self.callable(o, results)\n+\n+        def decrement(_: Any = None) -> None:\n+            assert parallel_count[0] > 0, parallel_count[0]\n+            parallel_count[0] -= 1\n+\n+        if dfd is not None:\n+            dfd.addBoth(decrement)\n+        else:\n+            decrement()\n+        return dfd\n+\n     @staticmethod\n     def get_async_iterable(length: int) -> AsyncGenerator[int, None]:\n         # simulate a simple callback without delays between results\n@@ -215,20 +236,42 @@ class TestAsyncCooperator(unittest.TestCase):\n     @inlineCallbacks\n     def test_simple(self):\n         for length in [20, 50, 100]:\n+            parallel_count = [0]\n+            max_parallel_count = [0]\n             results = []\n             ait = self.get_async_iterable(length)\n-            dl = parallel_async(ait, self.CONCURRENT_ITEMS, self.callable, results)\n+            dl = parallel_async(\n+                ait,\n+                self.CONCURRENT_ITEMS,\n+                self.callable_wrapped,\n+                results,\n+                parallel_count,\n+                max_parallel_count,\n+            )\n             yield dl\n             assert list(range(length)) == sorted(results)\n+            assert parallel_count[0] == 0\n+            assert max_parallel_count[0] <= self.CONCURRENT_ITEMS, max_parallel_count[0]\n \n     @inlineCallbacks\n     def test_delays(self):\n         for length in [20, 50, 100]:\n+            parallel_count = [0]\n+            max_parallel_count = [0]\n             results = []\n             ait = self.get_async_iterable_with_delays(length)\n-            dl = parallel_async(ait, self.CONCURRENT_ITEMS, self.callable, results)\n+            dl = parallel_async(\n+                ait,\n+                self.CONCURRENT_ITEMS,\n+                self.callable_wrapped,\n+                results,\n+                parallel_count,\n+                max_parallel_count,\n+            )\n             yield dl\n             assert list(range(length)) == sorted(results)\n+            assert parallel_count[0] == 0\n+            assert max_parallel_count[0] <= self.CONCURRENT_ITEMS, max_parallel_count[0]\n \n \n class TestDeferredFromCoro(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3aa5e757871c6288f0a1e7031ad74b48d6321f5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 294 | Lines Deleted: 34 | Files Changed: 16 | Hunks: 33 | Methods Changed: 28 | Complexity Δ (Sum/Max): 25/23 | Churn Δ: 328 | Churn Cumulative: 14341 | Contributors (this commit): 125 | Commits (past 90d): 41 | Contributors (cumulative): 341 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,11 +10,12 @@ from typing import TYPE_CHECKING\n \n import scrapy\n from scrapy.commands import BaseRunSpiderCommand, ScrapyCommand, ScrapyHelpFormatter\n-from scrapy.crawler import CrawlerProcess\n+from scrapy.crawler import AsyncCrawlerProcess, CrawlerProcess\n from scrapy.exceptions import UsageError\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.project import get_project_settings, inside_project\n from scrapy.utils.python import garbage_collect\n+from scrapy.utils.reactor import _asyncio_reactor_path\n \n if TYPE_CHECKING:\n     from collections.abc import Callable, Iterable\n@@ -202,6 +203,11 @@ def execute(argv: list[str] | None = None, settings: Settings | None = None) ->\n     _run_print_help(parser, cmd.process_options, args, opts)\n \n     if cmd.requires_crawler_process:\n+        if settings[\n+            \"TWISTED_REACTOR\"\n+        ] == _asyncio_reactor_path and not settings.getbool(\"FORCE_CRAWLER_PROCESS\"):\n+            cmd.crawler_process = AsyncCrawlerProcess(settings)\n+        else:\n             cmd.crawler_process = CrawlerProcess(settings)\n     _run_print_help(parser, _run_command, cmd, args, opts)\n     sys.exit(cmd.exitcode)\n\n@@ -18,14 +18,14 @@ from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n if TYPE_CHECKING:\n     from collections.abc import Iterable\n \n-    from scrapy.crawler import Crawler, CrawlerProcess\n+    from scrapy.crawler import Crawler, CrawlerProcessBase\n     from scrapy.settings import Settings\n \n \n class ScrapyCommand:\n     requires_project: bool = False\n     requires_crawler_process: bool = True\n-    crawler_process: CrawlerProcess | None = None  # set in scrapy.cmdline\n+    crawler_process: CrawlerProcessBase | None = None  # set in scrapy.cmdline\n \n     # default settings to be used for this command instead of global defaults\n     default_settings: dict[str, Any] = {}\n\n@@ -1,8 +1,6 @@\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, cast\n-\n-from twisted.python.failure import Failure\n+from typing import TYPE_CHECKING\n \n from scrapy.commands import BaseRunSpiderCommand\n from scrapy.exceptions import UsageError\n@@ -30,17 +28,7 @@ class Command(BaseRunSpiderCommand):\n         spname = args[0]\n \n         assert self.crawler_process\n-        crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n-\n-        if getattr(crawl_defer, \"result\", None) is not None and issubclass(\n-            cast(Failure, crawl_defer.result).type, Exception\n-        ):\n-            self.exitcode = 1\n-        else:\n+        self.crawler_process.crawl(spname, **opts.spargs)\n         self.crawler_process.start()\n-\n-            if self.crawler_process.bootstrap_failed or (\n-                hasattr(self.crawler_process, \"has_exception\")\n-                and self.crawler_process.has_exception\n-            ):\n+        if self.crawler_process.bootstrap_failed:\n             self.exitcode = 1\n\n@@ -18,8 +18,6 @@ if TYPE_CHECKING:\n \n \n class Command(ScrapyCommand):\n-    requires_project = False\n-\n     def syntax(self) -> str:\n         return \"[options] <url>\"\n \n\n@@ -46,7 +46,6 @@ def verify_url_scheme(url: str) -> str:\n \n \n class Command(ScrapyCommand):\n-    requires_project = False\n     requires_crawler_process = False\n     default_settings = {\"LOG_ENABLED\": False}\n \n\n@@ -30,7 +30,6 @@ def _import_file(filepath: str | PathLike[str]) -> ModuleType:\n \n \n class Command(BaseRunSpiderCommand):\n-    requires_project = False\n     default_settings = {\"SPIDER_LOADER_CLASS\": DummySpiderLoader}\n \n     def syntax(self) -> str:\n\n@@ -6,7 +6,6 @@ from scrapy.settings import BaseSettings\n \n \n class Command(ScrapyCommand):\n-    requires_project = False\n     requires_crawler_process = False\n     default_settings = {\"LOG_ENABLED\": False}\n \n\n@@ -22,7 +22,6 @@ if TYPE_CHECKING:\n \n \n class Command(ScrapyCommand):\n-    requires_project = False\n     default_settings = {\n         \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n         \"KEEP_ALIVE\": True,\n\n@@ -33,7 +33,6 @@ def _make_writable(path: Path) -> None:\n \n \n class Command(ScrapyCommand):\n-    requires_project = False\n     requires_crawler_process = False\n     default_settings = {\"LOG_ENABLED\": False}\n \n\n@@ -5,6 +5,7 @@ import contextlib\n import logging\n import pprint\n import signal\n+from abc import ABC, abstractmethod\n from typing import TYPE_CHECKING, Any, TypeVar\n \n from twisted.internet.defer import (\n@@ -42,7 +43,7 @@ from scrapy.utils.reactor import (\n )\n \n if TYPE_CHECKING:\n-    from collections.abc import Generator, Iterable\n+    from collections.abc import Awaitable, Generator, Iterable\n \n     from scrapy.logformatter import LogFormatter\n     from scrapy.statscollectors import StatsCollector\n@@ -321,7 +322,7 @@ class Crawler:\n         return self._get_component(cls, self.engine.scraper.spidermw.middlewares)\n \n \n-class CrawlerRunnerBase:\n+class CrawlerRunnerBase(ABC):\n     def __init__(self, settings: dict[str, Any] | Settings | None = None):\n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n@@ -364,6 +365,15 @@ class CrawlerRunnerBase:\n             spidercls = self.spider_loader.load(spidercls)\n         return Crawler(spidercls, self.settings)\n \n+    @abstractmethod\n+    def crawl(\n+        self,\n+        crawler_or_spidercls: type[Spider] | str | Crawler,\n+        *args: Any,\n+        **kwargs: Any,\n+    ) -> Awaitable[None]:\n+        raise NotImplementedError\n+\n \n class CrawlerRunner(CrawlerRunnerBase):\n     \"\"\"\n@@ -560,6 +570,12 @@ class CrawlerProcessBase(CrawlerRunnerBase):\n         configure_logging(self.settings, install_root_handler)\n         log_scrapy_info(self.settings)\n \n+    @abstractmethod\n+    def start(\n+        self, stop_after_crawl: bool = True, install_signal_handlers: bool = True\n+    ) -> None:\n+        raise NotImplementedError\n+\n     def _signal_shutdown(self, signum: int, _: Any) -> None:\n         from twisted.internet import reactor\n \n@@ -597,6 +613,7 @@ class CrawlerProcessBase(CrawlerRunnerBase):\n                 \"after\", \"startup\", install_shutdown_handlers, self._signal_shutdown\n             )\n \n+    @abstractmethod\n     def _stop_dfd(self) -> Deferred[Any]:\n         raise NotImplementedError\n \n\n@@ -179,6 +179,8 @@ FEED_STORAGE_S3_ACL = \"\"\n FILES_STORE_S3_ACL = \"private\"\n FILES_STORE_GCS_ACL = \"\"\n \n+FORCE_CRAWLER_PROCESS = False\n+\n FTP_USER = \"anonymous\"\n FTP_PASSWORD = \"guest\"  # noqa: S105\n FTP_PASSIVE_MODE = True\n\n@@ -30,6 +30,11 @@ class MySpider(scrapy.Spider):\n \"\"\"\n         log = self.get_log(spider_code)\n         assert \"[myspider] DEBUG: It works!\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n+        )\n+        assert \"Spider closed (finished)\" in log\n \n     def test_output(self):\n         spider_code = \"\"\"\n@@ -91,3 +96,23 @@ class MySpider(scrapy.Spider):\n         assert (\n             \"error: Please use only one of -o/--output and -O/--overwrite-output\" in log\n         )\n+\n+    def test_default_reactor(self):\n+        spider_code = \"\"\"\n+import scrapy\n+\n+class MySpider(scrapy.Spider):\n+    name = 'myspider'\n+\n+    async def start(self):\n+        self.logger.debug('It works!')\n+        return\n+        yield\n+\"\"\"\n+        log = self.get_log(spider_code, args=(\"-s\", \"TWISTED_REACTOR=\"))\n+        assert \"[myspider] DEBUG: It works!\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            not in log\n+        )\n+        assert \"Spider closed (finished)\" in log\n\n@@ -65,8 +65,10 @@ class BadSpider(scrapy.Spider):\n     def test_runspider(self):\n         log = self.get_log(self.debug_log_spider)\n         assert \"DEBUG: It Works!\" in log\n-        assert \"INFO: Spider opened\" in log\n-        assert \"INFO: Closing spider (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n+        )\n         assert \"INFO: Spider closed (finished)\" in log\n \n     def test_run_fail_spider(self):\n@@ -88,6 +90,17 @@ class BadSpider(scrapy.Spider):\n         assert \"DEBUG: It Works!\" not in log\n         assert \"INFO: Spider opened\" in log\n \n+    def test_runspider_default_reactor(self):\n+        log = self.get_log(self.debug_log_spider, args=(\"-s\", \"TWISTED_REACTOR=\"))\n+        assert \"DEBUG: It Works!\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            not in log\n+        )\n+        assert \"INFO: Spider opened\" in log\n+        assert \"INFO: Closing spider (finished)\" in log\n+        assert \"INFO: Spider closed (finished)\" in log\n+\n     def test_runspider_dnscache_disabled(self):\n         # see https://github.com/scrapy/scrapy/issues/2811\n         # The spider below should not be able to connect to localhost:12345,\n\n@@ -7,6 +7,7 @@ from pexpect.popen_spawn import PopenSpawn\n from twisted.internet import defer\n from twisted.trial import unittest\n \n+from scrapy.utils.reactor import _asyncio_reactor_path\n from tests import NON_EXISTING_RESOLVABLE, tests_datadir\n from tests.mockserver import MockServer\n from tests.utils.testproc import ProcessTest\n@@ -132,10 +133,9 @@ class TestShellCommand(ProcessTest, SiteTest, unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_shell_fetch_async(self):\n-        reactor_path = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n         url = self.url(\"/html\")\n         code = f\"fetch('{url}')\"\n-        args = [\"-c\", code, \"--set\", f\"TWISTED_REACTOR={reactor_path}\"]\n+        args = [\"-c\", code, \"--set\", f\"TWISTED_REACTOR={_asyncio_reactor_path}\"]\n         _, _, err = yield self.execute(args, check_code=True)\n         assert b\"RuntimeError: There is no current event loop in thread\" not in err\n \n\n@@ -20,6 +20,7 @@ from scrapy.cmdline import _pop_command_name, _print_unknown_command_msg\n from scrapy.commands import ScrapyCommand, ScrapyHelpFormatter, view\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n+from scrapy.utils.reactor import _asyncio_reactor_path\n from scrapy.utils.test import get_testenv\n \n if TYPE_CHECKING:\n@@ -124,6 +125,221 @@ class TestCommandBase(TestProjectBase):\n         self.env[\"SCRAPY_SETTINGS_MODULE\"] = f\"{self.project_name}.settings\"\n \n \n+class TestCommandCrawlerProcess(TestCommandBase):\n+    \"\"\"Test that the command uses the expected kind of *CrawlerProcess\n+    and produces expected errors when needed.\"\"\"\n+\n+    name = \"crawltest\"\n+\n+    NORMAL_MSG = \"Type of self.crawler_process: <class 'scrapy.crawler.CrawlerProcess'>\"\n+    ASYNC_MSG = (\n+        \"Type of self.crawler_process: <class 'scrapy.crawler.AsyncCrawlerProcess'>\"\n+    )\n+\n+    def setUp(self):\n+        super().setUp()\n+        (self.cwd / self.project_name / \"commands\").mkdir(exist_ok=True)\n+        (self.cwd / self.project_name / \"commands\" / \"__init__.py\").touch()\n+        (self.cwd / self.project_name / \"commands\" / f\"{self.name}.py\").write_text(\"\"\"\n+from scrapy.commands.crawl import Command\n+\n+class CrawlerProcessCrawlCommand(Command):\n+    requires_project = True\n+\n+    def run(self, args, opts):\n+        print(f\"Type of self.crawler_process: {type(self.crawler_process)}\")\n+        super().run(args, opts)\n+\"\"\")\n+\n+        self._append_settings(f\"COMMANDS_MODULE = '{self.project_name}.commands'\\n\")\n+\n+        (self.cwd / self.project_name / \"spiders\" / \"sp.py\").write_text(\"\"\"\n+import scrapy\n+\n+class MySpider(scrapy.Spider):\n+    name = 'sp'\n+\n+    custom_settings = {}\n+\n+    async def start(self):\n+        self.logger.debug('It works!')\n+        return\n+        yield\n+\"\"\")\n+\n+        (self.cwd / self.project_name / \"spiders\" / \"aiosp.py\").write_text(\"\"\"\n+import asyncio\n+\n+import scrapy\n+\n+class MySpider(scrapy.Spider):\n+    name = 'aiosp'\n+\n+    custom_settings = {}\n+\n+    async def start(self):\n+        await asyncio.sleep(0.01)\n+        self.logger.debug('It works!')\n+        return\n+        yield\n+\"\"\")\n+\n+    def _append_settings(self, text: str) -> None:\n+        \"\"\"Add text to the end of the project settings.py.\"\"\"\n+        with (self.cwd / self.project_name / \"settings.py\").open(\n+            \"a\", encoding=\"utf-8\"\n+        ) as f:\n+            f.write(text)\n+\n+    def _replace_custom_settings(self, spider_name: str, text: str) -> None:\n+        \"\"\"Replace custom_settings in the given spider file with the given text.\"\"\"\n+        spider_path = self.cwd / self.project_name / \"spiders\" / f\"{spider_name}.py\"\n+        with spider_path.open(\"r+\", encoding=\"utf-8\") as f:\n+            content = f.read()\n+            content = content.replace(\n+                \"custom_settings = {}\", f\"custom_settings = {text}\"\n+            )\n+            f.seek(0)\n+            f.write(content)\n+            f.truncate()\n+\n+    def _assert_spider_works(self, msg: str, *args: str) -> None:\n+        \"\"\"The command uses the expected *CrawlerProcess, the spider works.\"\"\"\n+        _, out, err = self.proc(self.name, *args)\n+        assert msg in out, out\n+        assert \"It works!\" in err, err\n+        assert \"Spider closed (finished)\" in err, err\n+\n+    def _assert_spider_asyncio_fail(self, msg: str, *args: str) -> None:\n+        \"\"\"The command uses the expected *CrawlerProcess, the spider fails to use asyncio.\"\"\"\n+        _, out, err = self.proc(self.name, *args)\n+        assert msg in out, out\n+        assert \"no running event loop\" in err, err\n+\n+    def test_project_settings(self):\n+        \"\"\"The reactor is set via the project default settings (to the asyncio value).\n+\n+        AsyncCrawlerProcess, the asyncio reactor, both spiders work.\"\"\"\n+        for spider in [\"sp\", \"aiosp\"]:\n+            self._assert_spider_works(self.ASYNC_MSG, spider)\n+\n+    def test_cmdline_asyncio(self):\n+        \"\"\"The reactor is set via the command line to the asyncio value.\n+        AsyncCrawlerProcess, the asyncio reactor, both spiders work.\"\"\"\n+        for spider in [\"sp\", \"aiosp\"]:\n+            self._assert_spider_works(\n+                self.ASYNC_MSG, spider, \"-s\", f\"TWISTED_REACTOR={_asyncio_reactor_path}\"\n+            )\n+\n+    def test_project_settings_explicit_asyncio(self):\n+        \"\"\"The reactor explicitly is set via the project settings to the asyncio value.\n+\n+        AsyncCrawlerProcess, the asyncio reactor, both spiders work.\"\"\"\n+        self._append_settings(f\"TWISTED_REACTOR = '{_asyncio_reactor_path}'\\n\")\n+\n+        for spider in [\"sp\", \"aiosp\"]:\n+            self._assert_spider_works(self.ASYNC_MSG, spider)\n+\n+    def test_cmdline_empty(self):\n+        \"\"\"The reactor is set via the command line to the empty value.\n+\n+        CrawlerProcess, the default reactor, only the normal spider works.\"\"\"\n+        self._assert_spider_works(self.NORMAL_MSG, \"sp\", \"-s\", \"TWISTED_REACTOR=\")\n+        self._assert_spider_asyncio_fail(\n+            self.NORMAL_MSG, \"aiosp\", \"-s\", \"TWISTED_REACTOR=\"\n+        )\n+\n+    def test_project_settings_empty(self):\n+        \"\"\"The reactor is set via the project settings to the empty value.\n+\n+        CrawlerProcess, the default reactor, only the normal spider works.\"\"\"\n+        self._append_settings(\"TWISTED_REACTOR = None\\n\")\n+\n+        self._assert_spider_works(self.NORMAL_MSG, \"sp\")\n+        self._assert_spider_asyncio_fail(\n+            self.NORMAL_MSG, \"aiosp\", \"-s\", \"TWISTED_REACTOR=\"\n+        )\n+\n+    def test_spider_settings_asyncio(self):\n+        \"\"\"The reactor is set via the spider settings to the asyncio value.\n+\n+        AsyncCrawlerProcess, the asyncio reactor, both spiders work.\"\"\"\n+        for spider in [\"sp\", \"aiosp\"]:\n+            self._replace_custom_settings(\n+                spider, f\"{{'TWISTED_REACTOR': '{_asyncio_reactor_path}'}}\"\n+            )\n+            self._assert_spider_works(self.ASYNC_MSG, spider)\n+\n+    def test_spider_settings_asyncio_cmdline_empty(self):\n+        \"\"\"The reactor is set via the spider settings to the asyncio value\n+        and via command line to the empty value. The command line value takes\n+        precedence so the spider settings don't matter.\n+\n+        CrawlerProcess, the default reactor, only the normal spider works.\"\"\"\n+        for spider in [\"sp\", \"aiosp\"]:\n+            self._replace_custom_settings(\n+                spider, f\"{{'TWISTED_REACTOR': '{_asyncio_reactor_path}'}}\"\n+            )\n+\n+        self._assert_spider_works(self.NORMAL_MSG, \"sp\", \"-s\", \"TWISTED_REACTOR=\")\n+        self._assert_spider_asyncio_fail(\n+            self.NORMAL_MSG, \"aiosp\", \"-s\", \"TWISTED_REACTOR=\"\n+        )\n+\n+    def test_project_empty_spider_settings_asyncio(self):\n+        \"\"\"The reactor is set via the project settings to the empty value\n+        and via the spider settings to the asyncio value. CrawlerProcess is\n+        chosen based on the project settings, but the asyncio reactor is chosen\n+        based on the spider settings.\n+\n+        CrawlerProcess, the asyncio reactor, both spiders work.\"\"\"\n+        self._append_settings(\"TWISTED_REACTOR = None\\n\")\n+        for spider in [\"sp\", \"aiosp\"]:\n+            self._replace_custom_settings(\n+                spider, f\"{{'TWISTED_REACTOR': '{_asyncio_reactor_path}'}}\"\n+            )\n+            self._assert_spider_works(self.NORMAL_MSG, spider)\n+\n+    def test_project_asyncio_spider_settings_select(self):\n+        \"\"\"The reactor is set via the project settings to the asyncio value\n+        and via the spider settings to the select value. AsyncCrawlerProcess\n+        is chosen based on the project settings, and the conflicting reactor\n+        setting in the spider settings causes an exception.\n+\n+        AsyncCrawlerProcess, the asyncio reactor, both spiders produce a\n+        mismatched reactor exception.\"\"\"\n+        self._append_settings(f\"TWISTED_REACTOR = '{_asyncio_reactor_path}'\\n\")\n+        for spider in [\"sp\", \"aiosp\"]:\n+            self._replace_custom_settings(\n+                spider,\n+                \"{'TWISTED_REACTOR': 'twisted.internet.selectreactor.SelectReactor'}\",\n+            )\n+            _, out, err = self.proc(self.name, spider)\n+            assert self.ASYNC_MSG in out, out\n+            assert (\n+                \"The installed reactor (twisted.internet.asyncioreactor.AsyncioSelectorReactor)\"\n+                \" does not match the requested one\"\n+                \" (twisted.internet.selectreactor.SelectReactor)\"\n+            ) in err, err\n+\n+    def test_project_asyncio_spider_settings_select_forced(self):\n+        \"\"\"The reactor is set via the project settings to the asyncio value\n+        and via the spider settings to the select value, CrawlerProcess is\n+        forced via the project settings. The reactor is chosen based on the\n+        spider settings.\n+\n+        CrawlerProcess, the select reactor, only the normal spider works.\"\"\"\n+        self._append_settings(\"FORCE_CRAWLER_PROCESS = True\\n\")\n+        for spider in [\"sp\", \"aiosp\"]:\n+            self._replace_custom_settings(\n+                spider,\n+                \"{'TWISTED_REACTOR': 'twisted.internet.selectreactor.SelectReactor'}\",\n+            )\n+\n+        self._assert_spider_works(self.NORMAL_MSG, \"sp\")\n+        self._assert_spider_asyncio_fail(self.NORMAL_MSG, \"aiosp\")\n+\n+\n class TestMiscCommands(TestCommandBase):\n     def test_list(self):\n         assert self.call(\"list\") == 0\n\n@@ -299,7 +299,7 @@ class TestDeferredFromCoro(unittest.TestCase):\n     @inlineCallbacks\n     def test_coroutine_asyncio(self):\n         async def coroutine() -> int:\n-            await asyncio.sleep(0)\n+            await asyncio.sleep(0.01)\n             return 42\n \n         result = deferred_from_coro(coroutine())\n@@ -379,7 +379,7 @@ class TestDeferredToFuture(unittest.TestCase):\n     @deferred_f_from_coro_f\n     async def test_wrapped_coroutine_asyncio(self):\n         async def c_f() -> int:\n-            await asyncio.sleep(0)\n+            await asyncio.sleep(0.01)\n             return 42\n \n         d = deferred_from_coro(c_f())\n@@ -414,7 +414,7 @@ class TestMaybeDeferredToFutureAsyncio(unittest.TestCase):\n     @deferred_f_from_coro_f\n     async def test_wrapped_coroutine_asyncio(self):\n         async def c_f() -> int:\n-            await asyncio.sleep(0)\n+            await asyncio.sleep(0.01)\n             return 42\n \n         d = deferred_from_coro(c_f())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8fb8d2c6b8a83181ef6564fbc9afa9cfeea2bd05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 195 | Lines Deleted: 25 | Files Changed: 8 | Hunks: 37 | Methods Changed: 21 | Complexity Δ (Sum/Max): 21/16 | Churn Δ: 220 | Churn Cumulative: 6400 | Contributors (this commit): 51 | Commits (past 90d): 20 | Contributors (cumulative): 98 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,7 +7,6 @@ from datetime import datetime\n from time import time\n from typing import TYPE_CHECKING, Any, cast\n \n-from twisted.internet import task\n from twisted.internet.defer import Deferred, inlineCallbacks\n \n from scrapy import Request, Spider, signals\n@@ -15,6 +14,7 @@ from scrapy.core.downloader.handlers import DownloadHandlers\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.resolver import dnscache\n+from scrapy.utils.asyncio import AsyncioLoopingCall, create_looping_call\n from scrapy.utils.defer import (\n     deferred_from_coro,\n     maybe_deferred_to_future,\n@@ -25,6 +25,8 @@ from scrapy.utils.httpobj import urlparse_cached\n if TYPE_CHECKING:\n     from collections.abc import Generator\n \n+    from twisted.internet.task import LoopingCall\n+\n     from scrapy.crawler import Crawler\n     from scrapy.http import Response\n     from scrapy.settings import BaseSettings\n@@ -111,7 +113,9 @@ class Downloader:\n         self.middleware: DownloaderMiddlewareManager = (\n             DownloaderMiddlewareManager.from_crawler(crawler)\n         )\n-        self._slot_gc_loop: task.LoopingCall = task.LoopingCall(self._slot_gc)\n+        self._slot_gc_loop: AsyncioLoopingCall | LoopingCall = create_looping_call(\n+            self._slot_gc\n+        )\n         self._slot_gc_loop.start(60)\n         self.per_slot_settings: dict[str, dict[str, Any]] = self.settings.getdict(\n             \"DOWNLOAD_SLOTS\", {}\n\n@@ -13,13 +13,16 @@ from traceback import format_exc\n from typing import TYPE_CHECKING, Any, cast\n \n from twisted.internet.defer import Deferred, inlineCallbacks, succeed\n-from twisted.internet.task import LoopingCall\n from twisted.python.failure import Failure\n \n from scrapy import signals\n from scrapy.core.scraper import Scraper\n from scrapy.exceptions import CloseSpider, DontCloseSpider, IgnoreRequest\n from scrapy.http import Request, Response\n+from scrapy.utils.asyncio import (\n+    AsyncioLoopingCall,\n+    create_looping_call,\n+)\n from scrapy.utils.defer import (\n     deferred_f_from_coro_f,\n     deferred_from_coro,\n@@ -32,6 +35,8 @@ from scrapy.utils.reactor import CallLaterOnce\n if TYPE_CHECKING:\n     from collections.abc import AsyncIterator, Callable, Generator\n \n+    from twisted.internet.task import LoopingCall\n+\n     from scrapy.core.downloader import Downloader\n     from scrapy.core.scheduler import BaseScheduler\n     from scrapy.crawler import Crawler\n@@ -56,7 +61,9 @@ class _Slot:\n         self.close_if_idle: bool = close_if_idle\n         self.nextcall: CallLaterOnce[None] = nextcall\n         self.scheduler: BaseScheduler = scheduler\n-        self.heartbeat: LoopingCall = LoopingCall(nextcall.schedule)\n+        self.heartbeat: AsyncioLoopingCall | LoopingCall = create_looping_call(\n+            nextcall.schedule\n+        )\n \n     def add_request(self, request: Request) -> None:\n         self.inprogress.add(request)\n\n@@ -12,6 +12,7 @@ from typing import TYPE_CHECKING, Any\n \n from scrapy import Request, Spider, signals\n from scrapy.exceptions import NotConfigured\n+from scrapy.utils.asyncio import create_looping_call\n \n if TYPE_CHECKING:\n     from twisted.python.failure import Failure\n@@ -118,9 +119,7 @@ class CloseSpider:\n             task_no_item.stop()\n \n     def spider_opened_no_item(self, spider: Spider) -> None:\n-        from twisted.internet import task\n-\n-        self.task_no_item = task.LoopingCall(self._count_items_produced, spider)\n+        self.task_no_item = create_looping_call(self._count_items_produced, spider)\n         self.task_no_item.start(self.timeout_no_item, now=False)\n \n         logger.info(\n\n@@ -3,12 +3,16 @@ from __future__ import annotations\n import logging\n from typing import TYPE_CHECKING\n \n-from twisted.internet import task\n-\n from scrapy import Spider, signals\n from scrapy.exceptions import NotConfigured\n+from scrapy.utils.asyncio import (\n+    AsyncioLoopingCall,\n+    create_looping_call,\n+)\n \n if TYPE_CHECKING:\n+    from twisted.internet.task import LoopingCall\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -29,7 +33,7 @@ class LogStats:\n         self.stats: StatsCollector = stats\n         self.interval: float = interval\n         self.multiplier: float = 60.0 / self.interval\n-        self.task: task.LoopingCall | None = None\n+        self.task: AsyncioLoopingCall | LoopingCall | None = None\n \n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n@@ -46,7 +50,7 @@ class LogStats:\n         self.pagesprev: int = 0\n         self.itemsprev: int = 0\n \n-        self.task = task.LoopingCall(self.log, spider)\n+        self.task = create_looping_call(self.log, spider)\n         self.task.start(self.interval)\n \n     def log(self, spider: Spider) -> None:\n\n@@ -13,14 +13,18 @@ from importlib import import_module\n from pprint import pformat\n from typing import TYPE_CHECKING\n \n-from twisted.internet import task\n-\n from scrapy import signals\n from scrapy.exceptions import NotConfigured\n from scrapy.mail import MailSender\n+from scrapy.utils.asyncio import (\n+    AsyncioLoopingCall,\n+    create_looping_call,\n+)\n from scrapy.utils.engine import get_engine_status\n \n if TYPE_CHECKING:\n+    from twisted.internet.task import LoopingCall\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -66,16 +70,16 @@ class MemoryUsage:\n     def engine_started(self) -> None:\n         assert self.crawler.stats\n         self.crawler.stats.set_value(\"memusage/startup\", self.get_virtual_size())\n-        self.tasks: list[task.LoopingCall] = []\n-        tsk = task.LoopingCall(self.update)\n+        self.tasks: list[AsyncioLoopingCall | LoopingCall] = []\n+        tsk = create_looping_call(self.update)\n         self.tasks.append(tsk)\n         tsk.start(self.check_interval, now=True)\n         if self.limit:\n-            tsk = task.LoopingCall(self._check_limit)\n+            tsk = create_looping_call(self._check_limit)\n             self.tasks.append(tsk)\n             tsk.start(self.check_interval, now=True)\n         if self.warning:\n-            tsk = task.LoopingCall(self._check_warning)\n+            tsk = create_looping_call(self._check_warning)\n             self.tasks.append(tsk)\n             tsk.start(self.check_interval, now=True)\n \n\n@@ -4,16 +4,20 @@ import logging\n from datetime import datetime, timezone\n from typing import TYPE_CHECKING, Any\n \n-from twisted.internet import task\n-\n from scrapy import Spider, signals\n from scrapy.exceptions import NotConfigured\n+from scrapy.utils.asyncio import (\n+    AsyncioLoopingCall,\n+    create_looping_call,\n+)\n from scrapy.utils.serialize import ScrapyJSONEncoder\n \n if TYPE_CHECKING:\n-    # typing.Self requires Python 3.11\n     from json import JSONEncoder\n \n+    from twisted.internet.task import LoopingCall\n+\n+    # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n     from scrapy.crawler import Crawler\n@@ -37,7 +41,7 @@ class PeriodicLog:\n         self.stats: StatsCollector = stats\n         self.interval: float = interval\n         self.multiplier: float = 60.0 / self.interval\n-        self.task: task.LoopingCall | None = None\n+        self.task: AsyncioLoopingCall | LoopingCall | None = None\n         self.encoder: JSONEncoder = ScrapyJSONEncoder(sort_keys=True, indent=4)\n         self.ext_stats_enabled: bool = bool(ext_stats)\n         self.ext_stats_include: list[str] = ext_stats.get(\"include\", [])\n@@ -97,7 +101,7 @@ class PeriodicLog:\n         self.delta_prev: dict[str, int | float] = {}\n         self.stats_prev: dict[str, int | float] = {}\n \n-        self.task = task.LoopingCall(self.log)\n+        self.task = create_looping_call(self.log)\n         self.task.start(self.interval)\n \n     def log(self) -> None:\n\n@@ -3,22 +3,30 @@\n from __future__ import annotations\n \n import asyncio\n+import logging\n+import time\n+from collections.abc import AsyncIterator, Callable, Coroutine, Iterable\n from typing import TYPE_CHECKING, Any, TypeVar\n \n+from twisted.internet.defer import Deferred\n+from twisted.internet.task import LoopingCall\n+\n from scrapy.utils.asyncgen import as_async_generator\n from scrapy.utils.reactor import is_asyncio_reactor_installed, is_reactor_installed\n \n if TYPE_CHECKING:\n-    from collections.abc import AsyncIterator, Callable, Coroutine, Iterable\n-\n     # typing.Concatenate and typing.ParamSpec require Python 3.10\n     from typing_extensions import Concatenate, ParamSpec\n \n     _P = ParamSpec(\"_P\")\n \n+\n _T = TypeVar(\"_T\")\n \n \n+logger = logging.getLogger(__name__)\n+\n+\n def is_asyncio_available() -> bool:\n     \"\"\"Check if it's possible to call asyncio code that relies on the asyncio event loop.\n \n@@ -90,3 +98,97 @@ async def _parallel_asyncio(\n     fill_task = asyncio.create_task(fill_queue())\n     work_tasks = [asyncio.create_task(worker()) for _ in range(count)]\n     await asyncio.wait([fill_task, *work_tasks])\n+\n+\n+class AsyncioLoopingCall:\n+    \"\"\"A simple implementation of a periodic call using asyncio, keeping\n+    some API and behavior compatibility with the Twisted ``LoopingCall``.\n+\n+    The function is called every *interval* seconds, independent of the finish\n+    time of the previous call. If the function  is still running when it's time\n+    to call it again, calls are skipped until the function finishes.\n+\n+    The function must not return a coroutine or a ``Deferred``.\n+    \"\"\"\n+\n+    def __init__(self, func: Callable[_P, _T], *args: _P.args, **kwargs: _P.kwargs):\n+        self._func: Callable[_P, _T] = func\n+        self._args: tuple[Any, ...] = args\n+        self._kwargs: dict[str, Any] = kwargs\n+        self._task: asyncio.Task | None = None\n+        self.interval: float | None = None\n+        self._start_time: float | None = None\n+\n+    @property\n+    def running(self) -> bool:\n+        return self._start_time is not None\n+\n+    def start(self, interval: float, now: bool = True) -> None:\n+        \"\"\"Start calling the function every *interval* seconds.\n+\n+        :param interval: The interval in seconds between calls.\n+        :type interval: float\n+\n+        :param now: If ``True``, also call the function immediately.\n+        :type now: bool\n+        \"\"\"\n+        if self.running:\n+            raise RuntimeError(\"AsyncioLoopingCall already running\")\n+\n+        if interval <= 0:\n+            raise ValueError(\"Interval must be greater than 0\")\n+\n+        self.interval = interval\n+        self._start_time = time.time()\n+        if now:\n+            self._call()\n+        loop = asyncio.get_event_loop()\n+        self._task = loop.create_task(self._loop())\n+\n+    def _to_sleep(self) -> float:\n+        \"\"\"Return the time to sleep until the next call.\"\"\"\n+        assert self.interval is not None\n+        assert self._start_time is not None\n+        now = time.time()\n+        running_for = now - self._start_time\n+        return self.interval - (running_for % self.interval)\n+\n+    async def _loop(self) -> None:\n+        \"\"\"Run an infinite loop that calls the function periodically.\"\"\"\n+        while self.running:\n+            await asyncio.sleep(self._to_sleep())\n+            self._call()\n+\n+    def stop(self) -> None:\n+        \"\"\"Stop the periodic calls.\"\"\"\n+        self.interval = self._start_time = None\n+        if self._task is not None:\n+            self._task.cancel()\n+            self._task = None\n+\n+    def _call(self) -> None:\n+        \"\"\"Execute the function.\"\"\"\n+        try:\n+            result = self._func(*self._args, **self._kwargs)\n+        except Exception:\n+            logger.exception(\"Error calling the AsyncioLoopingCall function\")\n+            self.stop()\n+        else:\n+            if isinstance(result, (Coroutine, Deferred)):\n+                self.stop()\n+                raise TypeError(\n+                    \"The AsyncioLoopingCall function must not return a coroutine or a Deferred\"\n+                )\n+\n+\n+def create_looping_call(\n+    func: Callable[_P, _T], *args: _P.args, **kwargs: _P.kwargs\n+) -> AsyncioLoopingCall | LoopingCall:\n+    \"\"\"Create an instance of a looping call class.\n+\n+    This creates an instance of :class:`AsyncioLoopingCall` or\n+    :class:`LoopingCall`, depending on whether asyncio support is available.\n+    \"\"\"\n+    if is_asyncio_available():\n+        return AsyncioLoopingCall(func, *args, **kwargs)\n+    return LoopingCall(func, *args, **kwargs)\n\n@@ -3,12 +3,18 @@ from __future__ import annotations\n import asyncio\n import random\n from typing import TYPE_CHECKING\n+from unittest import mock\n \n import pytest\n+from twisted.internet.defer import Deferred\n from twisted.trial import unittest\n \n from scrapy.utils.asyncgen import as_async_generator\n-from scrapy.utils.asyncio import _parallel_asyncio, is_asyncio_available\n+from scrapy.utils.asyncio import (\n+    AsyncioLoopingCall,\n+    _parallel_asyncio,\n+    is_asyncio_available,\n+)\n from scrapy.utils.defer import deferred_f_from_coro_f\n \n if TYPE_CHECKING:\n@@ -97,3 +103,43 @@ class TestParallelAsyncio(unittest.TestCase):\n             )\n             assert list(range(length)) == sorted(results)\n             assert max_parallel_count[0] <= self.CONCURRENT_ITEMS\n+\n+\n+@pytest.mark.only_asyncio\n+class TestAsyncioLoopingCall:\n+    def test_looping_call(self):\n+        func = mock.MagicMock()\n+        looping_call = AsyncioLoopingCall(func)\n+        looping_call.start(1, now=False)\n+        assert looping_call.running\n+        looping_call.stop()\n+        assert not looping_call.running\n+        assert not func.called\n+\n+    def test_looping_call_now(self):\n+        func = mock.MagicMock()\n+        looping_call = AsyncioLoopingCall(func)\n+        looping_call.start(1)\n+        looping_call.stop()\n+        assert func.called\n+\n+    def test_looping_call_already_running(self):\n+        looping_call = AsyncioLoopingCall(lambda: None)\n+        looping_call.start(1)\n+        with pytest.raises(RuntimeError):\n+            looping_call.start(1)\n+        looping_call.stop()\n+\n+    def test_looping_call_interval(self):\n+        looping_call = AsyncioLoopingCall(lambda: None)\n+        with pytest.raises(ValueError, match=\"Interval must be greater than 0\"):\n+            looping_call.start(0)\n+        with pytest.raises(ValueError, match=\"Interval must be greater than 0\"):\n+            looping_call.start(-1)\n+        assert not looping_call.running\n+\n+    def test_looping_call_bad_function(self):\n+        looping_call = AsyncioLoopingCall(Deferred)\n+        with pytest.raises(TypeError):\n+            looping_call.start(0.1)\n+        assert not looping_call.running\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c6698b9fe8d3e1f27f2982b484ba69486a8cbc3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 75 | Files Changed: 1 | Hunks: 48 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 146 | Churn Cumulative: 1059 | Contributors (this commit): 69 | Commits (past 90d): 5 | Contributors (cumulative): 69 | DMM Complexity: None\n\nDIFF:\n@@ -1,16 +1,16 @@\n-\"\"\"\n-This module contains the default values for all settings used by Scrapy.\n+\"\"\"This module contains the default values for all settings used by Scrapy.\n \n For more information about these settings you can read the settings\n documentation in docs/topics/settings.rst\n \n Scrapy developers, if you add a setting here remember to:\n \n-* add it in alphabetical order\n+* add it in alphabetical order, with the exception that enabling flags and\n+  other high-level settings for a group should come first in their group\n+  and pairs like host/port and user/password should be in the usual order\n * group similar settings without leaving blank lines\n * add its documentation to the available settings documentation\n   (docs/topics/settings.rst)\n-\n \"\"\"\n \n import sys\n@@ -31,10 +31,10 @@ AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n \n BOT_NAME = \"scrapybot\"\n \n-CLOSESPIDER_TIMEOUT = 0\n-CLOSESPIDER_PAGECOUNT = 0\n-CLOSESPIDER_ITEMCOUNT = 0\n CLOSESPIDER_ERRORCOUNT = 0\n+CLOSESPIDER_ITEMCOUNT = 0\n+CLOSESPIDER_PAGECOUNT = 0\n+CLOSESPIDER_TIMEOUT = 0\n \n COMMANDS_MODULE = \"\"\n \n@@ -59,8 +59,8 @@ DEFAULT_REQUEST_HEADERS = {\n }\n \n DEPTH_LIMIT = 0\n-DEPTH_STATS_VERBOSE = False\n DEPTH_PRIORITY = 0\n+DEPTH_STATS_VERBOSE = False\n \n DNSCACHE_ENABLED = True\n DNSCACHE_SIZE = 10000\n@@ -69,6 +69,8 @@ DNS_TIMEOUT = 60\n \n DOWNLOAD_DELAY = 0\n \n+DOWNLOAD_FAIL_ON_DATALOSS = True\n+\n DOWNLOAD_HANDLERS = {}\n DOWNLOAD_HANDLERS_BASE = {\n     \"data\": \"scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler\",\n@@ -79,18 +81,13 @@ DOWNLOAD_HANDLERS_BASE = {\n     \"ftp\": \"scrapy.core.downloader.handlers.ftp.FTPDownloadHandler\",\n }\n \n-DOWNLOAD_TIMEOUT = 180  # 3mins\n-\n DOWNLOAD_MAXSIZE = 1024 * 1024 * 1024  # 1024m\n DOWNLOAD_WARNSIZE = 32 * 1024 * 1024  # 32m\n \n-DOWNLOAD_FAIL_ON_DATALOSS = True\n+DOWNLOAD_TIMEOUT = 180  # 3mins\n \n DOWNLOADER = \"scrapy.core.downloader.Downloader\"\n \n-DOWNLOADER_HTTPCLIENTFACTORY = (\n-    \"scrapy.core.downloader.webclient.ScrapyHTTPClientFactory\"\n-)\n DOWNLOADER_CLIENTCONTEXTFACTORY = (\n     \"scrapy.core.downloader.contextfactory.ScrapyClientContextFactory\"\n )\n@@ -99,8 +96,11 @@ DOWNLOADER_CLIENT_TLS_CIPHERS = \"DEFAULT\"\n DOWNLOADER_CLIENT_TLS_METHOD = \"TLS\"\n DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING = False\n \n-DOWNLOADER_MIDDLEWARES = {}\n+DOWNLOADER_HTTPCLIENTFACTORY = (\n+    \"scrapy.core.downloader.webclient.ScrapyHTTPClientFactory\"\n+)\n \n+DOWNLOADER_MIDDLEWARES = {}\n DOWNLOADER_MIDDLEWARES_BASE = {\n     # Engine side\n     \"scrapy.downloadermiddlewares.offsite.OffsiteMiddleware\": 50,\n@@ -130,7 +130,6 @@ if sys.platform == \"win32\":\n     EDITOR = \"%s -m idlelib.idle\"\n \n EXTENSIONS = {}\n-\n EXTENSIONS_BASE = {\n     \"scrapy.extensions.corestats.CoreStats\": 0,\n     \"scrapy.extensions.telnet.TelnetConsole\": 0,\n@@ -143,22 +142,11 @@ EXTENSIONS_BASE = {\n     \"scrapy.extensions.throttle.AutoThrottle\": 0,\n }\n \n-FEED_TEMPDIR = None\n FEEDS = {}\n-FEED_URI_PARAMS = None  # a function to extend uri arguments\n-FEED_STORE_EMPTY = True\n+FEED_EXPORT_BATCH_ITEM_COUNT = 0\n FEED_EXPORT_ENCODING = None\n FEED_EXPORT_FIELDS = None\n-FEED_STORAGES = {}\n-FEED_STORAGES_BASE = {\n-    \"\": \"scrapy.extensions.feedexport.FileFeedStorage\",\n-    \"file\": \"scrapy.extensions.feedexport.FileFeedStorage\",\n-    \"ftp\": \"scrapy.extensions.feedexport.FTPFeedStorage\",\n-    \"gs\": \"scrapy.extensions.feedexport.GCSFeedStorage\",\n-    \"s3\": \"scrapy.extensions.feedexport.S3FeedStorage\",\n-    \"stdout\": \"scrapy.extensions.feedexport.StdoutFeedStorage\",\n-}\n-FEED_EXPORT_BATCH_ITEM_COUNT = 0\n+FEED_EXPORT_INDENT = 0\n FEED_EXPORTERS = {}\n FEED_EXPORTERS_BASE = {\n     \"json\": \"scrapy.exporters.JsonItemExporter\",\n@@ -170,59 +158,69 @@ FEED_EXPORTERS_BASE = {\n     \"marshal\": \"scrapy.exporters.MarshalItemExporter\",\n     \"pickle\": \"scrapy.exporters.PickleItemExporter\",\n }\n-FEED_EXPORT_INDENT = 0\n-\n+FEED_STORE_EMPTY = True\n+FEED_STORAGES = {}\n+FEED_STORAGES_BASE = {\n+    \"\": \"scrapy.extensions.feedexport.FileFeedStorage\",\n+    \"file\": \"scrapy.extensions.feedexport.FileFeedStorage\",\n+    \"ftp\": \"scrapy.extensions.feedexport.FTPFeedStorage\",\n+    \"gs\": \"scrapy.extensions.feedexport.GCSFeedStorage\",\n+    \"s3\": \"scrapy.extensions.feedexport.S3FeedStorage\",\n+    \"stdout\": \"scrapy.extensions.feedexport.StdoutFeedStorage\",\n+}\n FEED_STORAGE_FTP_ACTIVE = False\n FEED_STORAGE_GCS_ACL = \"\"\n FEED_STORAGE_S3_ACL = \"\"\n+FEED_TEMPDIR = None\n+FEED_URI_PARAMS = None  # a function to extend uri arguments\n \n-FILES_STORE_S3_ACL = \"private\"\n FILES_STORE_GCS_ACL = \"\"\n+FILES_STORE_S3_ACL = \"private\"\n \n FORCE_CRAWLER_PROCESS = False\n \n+FTP_PASSIVE_MODE = True\n FTP_USER = \"anonymous\"\n FTP_PASSWORD = \"guest\"  # noqa: S105\n-FTP_PASSIVE_MODE = True\n \n GCS_PROJECT_ID = None\n \n HTTPCACHE_ENABLED = False\n-HTTPCACHE_DIR = \"httpcache\"\n-HTTPCACHE_IGNORE_MISSING = False\n-HTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n-HTTPCACHE_EXPIRATION_SECS = 0\n HTTPCACHE_ALWAYS_STORE = False\n-HTTPCACHE_IGNORE_HTTP_CODES = []\n-HTTPCACHE_IGNORE_SCHEMES = [\"file\"]\n-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS = []\n HTTPCACHE_DBM_MODULE = \"dbm\"\n-HTTPCACHE_POLICY = \"scrapy.extensions.httpcache.DummyPolicy\"\n+HTTPCACHE_DIR = \"httpcache\"\n+HTTPCACHE_EXPIRATION_SECS = 0\n HTTPCACHE_GZIP = False\n+HTTPCACHE_IGNORE_HTTP_CODES = []\n+HTTPCACHE_IGNORE_MISSING = False\n+HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS = []\n+HTTPCACHE_IGNORE_SCHEMES = [\"file\"]\n+HTTPCACHE_POLICY = \"scrapy.extensions.httpcache.DummyPolicy\"\n+HTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n \n HTTPPROXY_ENABLED = True\n HTTPPROXY_AUTH_ENCODING = \"latin-1\"\n \n-IMAGES_STORE_S3_ACL = \"private\"\n IMAGES_STORE_GCS_ACL = \"\"\n-\n-ITEM_PROCESSOR = \"scrapy.pipelines.ItemPipelineManager\"\n+IMAGES_STORE_S3_ACL = \"private\"\n \n ITEM_PIPELINES = {}\n ITEM_PIPELINES_BASE = {}\n \n+ITEM_PROCESSOR = \"scrapy.pipelines.ItemPipelineManager\"\n+\n JOBDIR = None\n \n LOG_ENABLED = True\n-LOG_ENCODING = \"utf-8\"\n-LOG_FORMATTER = \"scrapy.logformatter.LogFormatter\"\n-LOG_FORMAT = \"%(asctime)s [%(name)s] %(levelname)s: %(message)s\"\n LOG_DATEFORMAT = \"%Y-%m-%d %H:%M:%S\"\n-LOG_STDOUT = False\n-LOG_LEVEL = \"DEBUG\"\n+LOG_ENCODING = \"utf-8\"\n LOG_FILE = None\n LOG_FILE_APPEND = True\n+LOG_FORMAT = \"%(asctime)s [%(name)s] %(levelname)s: %(message)s\"\n+LOG_FORMATTER = \"scrapy.logformatter.LogFormatter\"\n+LOG_LEVEL = \"DEBUG\"\n LOG_SHORT_NAMES = False\n+LOG_STDOUT = False\n LOG_VERSIONS = [\n     \"lxml\",\n     \"libxml2\",\n@@ -236,21 +234,19 @@ LOG_VERSIONS = [\n     \"Platform\",\n ]\n \n-SCHEDULER_DEBUG = False\n-\n LOGSTATS_INTERVAL = 60.0\n \n+MAIL_FROM = \"scrapy@localhost\"\n MAIL_HOST = \"localhost\"\n MAIL_PORT = 25\n-MAIL_FROM = \"scrapy@localhost\"\n-MAIL_PASS = None\n MAIL_USER = None\n+MAIL_PASS = None\n \n MEMDEBUG_ENABLED = False  # enable memory debugging\n MEMDEBUG_NOTIFY = []  # send memory debugging report by mail at engine shutdown\n \n-MEMUSAGE_CHECK_INTERVAL_SECONDS = 60.0\n MEMUSAGE_ENABLED = True\n+MEMUSAGE_CHECK_INTERVAL_SECONDS = 60.0\n MEMUSAGE_LIMIT_MB = 0\n MEMUSAGE_NOTIFY_MAIL = []\n MEMUSAGE_WARNING_MB = 0\n@@ -280,9 +276,6 @@ REQUEST_FINGERPRINTER_CLASS = \"scrapy.utils.request.RequestFingerprinter\"\n REQUEST_FINGERPRINTER_IMPLEMENTATION = \"SENTINEL\"\n \n RETRY_ENABLED = True\n-RETRY_TIMES = 2  # initial response + 2 retries = 3 requests\n-RETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408, 429]\n-RETRY_PRIORITY_ADJUST = -1\n RETRY_EXCEPTIONS = [\n     \"twisted.internet.defer.TimeoutError\",\n     \"twisted.internet.error.TimeoutError\",\n@@ -298,12 +291,16 @@ RETRY_EXCEPTIONS = [\n     OSError,\n     \"scrapy.core.downloader.handlers.http11.TunnelError\",\n ]\n+RETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408, 429]\n+RETRY_PRIORITY_ADJUST = -1\n+RETRY_TIMES = 2  # initial response + 2 retries = 3 requests\n \n ROBOTSTXT_OBEY = False\n ROBOTSTXT_PARSER = \"scrapy.robotstxt.ProtegoRobotParser\"\n ROBOTSTXT_USER_AGENT = None\n \n SCHEDULER = \"scrapy.core.scheduler.Scheduler\"\n+SCHEDULER_DEBUG = False\n SCHEDULER_DISK_QUEUE = \"scrapy.squeues.PickleLifoDiskQueue\"\n SCHEDULER_MEMORY_QUEUE = \"scrapy.squeues.LifoMemoryQueue\"\n SCHEDULER_PRIORITY_QUEUE = \"scrapy.pqueues.ScrapyPriorityQueue\"\n@@ -312,11 +309,19 @@ SCHEDULER_START_MEMORY_QUEUE = \"scrapy.squeues.FifoMemoryQueue\"\n \n SCRAPER_SLOT_MAX_ACTIVE_SIZE = 5000000\n \n+SPIDER_CONTRACTS = {}\n+SPIDER_CONTRACTS_BASE = {\n+    \"scrapy.contracts.default.UrlContract\": 1,\n+    \"scrapy.contracts.default.CallbackKeywordArgumentsContract\": 1,\n+    \"scrapy.contracts.default.MetadataContract\": 1,\n+    \"scrapy.contracts.default.ReturnsContract\": 2,\n+    \"scrapy.contracts.default.ScrapesContract\": 3,\n+}\n+\n SPIDER_LOADER_CLASS = \"scrapy.spiderloader.SpiderLoader\"\n SPIDER_LOADER_WARN_ONLY = False\n \n SPIDER_MIDDLEWARES = {}\n-\n SPIDER_MIDDLEWARES_BASE = {\n     # Engine side\n     \"scrapy.spidermiddlewares.start.StartSpiderMiddleware\": 25,\n@@ -334,27 +339,18 @@ STATS_DUMP = True\n \n STATSMAILER_RCPTS = []\n \n+TELNETCONSOLE_ENABLED = 1\n+TELNETCONSOLE_HOST = \"127.0.0.1\"\n+TELNETCONSOLE_PORT = [6023, 6073]\n+TELNETCONSOLE_USERNAME = \"scrapy\"\n+TELNETCONSOLE_PASSWORD = None\n+\n TEMPLATES_DIR = str((Path(__file__).parent / \"..\" / \"templates\").resolve())\n \n+TWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+\n URLLENGTH_LIMIT = 2083\n \n USER_AGENT = f\"Scrapy/{import_module('scrapy').__version__} (+https://scrapy.org)\"\n \n-TELNETCONSOLE_ENABLED = 1\n-TELNETCONSOLE_PORT = [6023, 6073]\n-TELNETCONSOLE_HOST = \"127.0.0.1\"\n-TELNETCONSOLE_USERNAME = \"scrapy\"\n-TELNETCONSOLE_PASSWORD = None\n-\n-TWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n-\n-SPIDER_CONTRACTS = {}\n-SPIDER_CONTRACTS_BASE = {\n-    \"scrapy.contracts.default.UrlContract\": 1,\n-    \"scrapy.contracts.default.CallbackKeywordArgumentsContract\": 1,\n-    \"scrapy.contracts.default.MetadataContract\": 1,\n-    \"scrapy.contracts.default.ReturnsContract\": 2,\n-    \"scrapy.contracts.default.ScrapesContract\": 3,\n-}\n-\n WARN_ON_GENERATOR_RETURN_VALUE = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
