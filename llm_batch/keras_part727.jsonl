{"custom_id": "keras#5fca251cf067ebeec2a142b9f3cd550cad2d7e0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 1906 | Contributors (this commit): 10 | Commits (past 90d): 27 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -88,17 +88,18 @@ class Trainer:\n                 `keras_core.metrics.BinaryAccuracy`,\n                 `keras_core.metrics.CategoricalAccuracy`,\n                 `keras_core.metrics.SparseCategoricalAccuracy` based on the\n-                shapes of the targets and of the model output. We do a similar\n-                conversion for the strings 'crossentropy' and 'ce' as well.\n+                shapes of the targets and of the model output. A similar\n+                conversion is done for the strings `\"crossentropy\"`\n+                and `\"ce\"` as well.\n                 The metrics passed here are evaluated without sample weighting;\n                 if you would like sample weighting to apply, you can specify\n                 your metrics via the `weighted_metrics` argument instead.\n             weighted_metrics: List of metrics to be evaluated and weighted by\n                 `sample_weight` or `class_weight` during training and testing.\n-            run_eagerly: Bool. If `True`, this `Model`'s logic will never be\n-                compiled (e.g. with `tf.function` or `jax.jit`). Recommended to\n-                leave this as `False` when training for best performance, and\n-                `True` when debugging.\n+            run_eagerly: Bool. If `True`, this model's forward pass\n+                 will never be compiled. It is recommended to leave this\n+                 as `False` when training (for best performance),\n+                 and to set it to `True` when debugging.\n             steps_per_execution: Int. The number of batches to run\n                 during each a single compiled function call. Running multiple\n                 batches inside a single a single compiled function call can\n@@ -110,15 +111,11 @@ class Trainer:\n                 `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n                 will only be called every `N` batches (i.e. before/after\n                 each compiled function execution).\n+                Not supported with the PyTorch backend.\n             jit_compile: Bool or `\"auto\"`. Whether to use XLA compilation when\n-                compiling a model. This value should currently never be `True`\n-                on the torch backed, and should always be `True` or `\"auto\"` on\n-                the jax backend. On tensorflow, this value can be `True` or\n-                `False`, and will toggle the `jit_compile` option for any\n-                `tf.function` owned by the model. See\n-                https://www.tensorflow.org/xla/tutorials/jit_compile for more\n-                details. If `\"auto\"`, XLA compilation will be enabled if the\n-                backend supports it, and disabled otherwise.\n+                compiling a model. Not supported with the PyTorch backend.\n+                If `\"auto\"`, XLA compilation will be enabled if the\n+                the model supports it, and disabled otherwise.\n         \"\"\"\n         self.optimizer = optimizers.get(optimizer)\n         if hasattr(self, \"output_names\"):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#04e3223282153b4b9387d9d5eb1920f25fc6a87e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 23 | Churn Cumulative: 3522 | Contributors (this commit): 7 | Commits (past 90d): 18 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -187,6 +187,9 @@ def compute_conv_transpose_padding_args_for_torch(\n def _get_output_shape_given_tf_padding(\n     input_size, kernel_size, strides, padding, output_padding, dilation_rate\n ):\n+    if input_size is None:\n+        return None\n+\n     assert padding.lower() in {\"valid\", \"same\"}\n \n     kernel_size = (kernel_size - 1) * dilation_rate + 1\n\n@@ -835,3 +835,23 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         # Compare results\n         kc_res = kc_layer(input)\n         self.assertAllClose(expected_res, kc_res, atol=1e-5)\n+\n+    @parameterized.product(\n+        kernel_size=list(range(1, 5)),\n+        strides=list(range(1, 5)),\n+        padding=[\"same\", \"valid\"],\n+        output_padding=[None] + list(range(1, 5)),\n+    )\n+    def test_shape_inference_static_unknown_shape(\n+        self, kernel_size, strides, padding, output_padding\n+    ):\n+        x = layers.Input(shape=(None, None, 3))\n+        x = layers.Conv2DTranspose(\n+            filters=2,\n+            kernel_size=kernel_size,\n+            strides=strides,\n+            padding=padding,\n+            output_padding=output_padding,\n+            dilation_rate=1,\n+        )(x)\n+        self.assertEqual(x.shape, (None, None, None, 2))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9cfe2c281eeae43cd399519eeac5727a9ae157ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 134 | Lines Deleted: 16 | Files Changed: 13 | Hunks: 39 | Methods Changed: 22 | Complexity Δ (Sum/Max): 14/5 | Churn Δ: 150 | Churn Cumulative: 5160 | Contributors (this commit): 21 | Commits (past 90d): 143 | Contributors (cumulative): 92 | DMM Complexity: 0.4117647058823529\n\nDIFF:\n@@ -28,11 +28,19 @@ class KerasTensor:\n     dtype is called \"static shape inference\".\n     \"\"\"\n \n-    def __init__(self, shape, dtype=\"float32\", record_history=True, name=None):\n+    def __init__(\n+        self,\n+        shape,\n+        dtype=\"float32\",\n+        sparse=False,\n+        record_history=True,\n+        name=None,\n+    ):\n         from keras_core import backend\n \n         self.shape = backend.standardize_shape(shape)\n         self.dtype = backend.standardize_dtype(dtype)\n+        self.sparse = sparse\n         self.name = name or auto_name(self.__class__.__name__)\n         self.record_history = record_history\n \n@@ -106,7 +114,7 @@ class KerasTensor:\n     def __repr__(self):\n         return (\n             f\"<KerasTensor shape={self.shape}, dtype={self.dtype}, \"\n-            f\"name={self.name}>\"\n+            f\"sparse={self.sparse}, name={self.name}>\"\n         )\n \n     def __iter__(self):\n\n@@ -9,9 +9,10 @@ from keras_core.backend.common import keras_tensor\n \n class KerasTensorTest(testing.TestCase):\n     def test_attributes(self):\n-        x = keras_tensor.KerasTensor(shape=(3,), dtype=\"float32\")\n+        x = keras_tensor.KerasTensor(shape=(3,), dtype=\"float32\", sparse=True)\n         self.assertEqual(x.dtype, \"float32\")\n         self.assertEqual(x.shape, (3,))\n+        self.assertEqual(x.sparse, True)\n \n     def test_numpy_methods(self):\n         x = keras_tensor.KerasTensor(shape=(3, 2), dtype=\"float32\")\n\n@@ -5,6 +5,7 @@ from keras_core.backend.jax import math\n from keras_core.backend.jax import nn\n from keras_core.backend.jax import numpy\n from keras_core.backend.jax import random\n+from keras_core.backend.jax.core import SUPPORTS_SPARSE_TENSORS\n from keras_core.backend.jax.core import Variable\n from keras_core.backend.jax.core import cast\n from keras_core.backend.jax.core import compute_output_spec\n\n@@ -14,6 +14,8 @@ from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.backend.jax import distribution_lib\n from keras_core.utils.nest import pack_sequence_as\n \n+SUPPORTS_SPARSE_TENSORS = False\n+\n \n class Variable(KerasVariable):\n     def _initialize(self, value):\n@@ -44,7 +46,9 @@ class Variable(KerasVariable):\n         return self.value\n \n \n-def convert_to_tensor(x, dtype=None):\n+def convert_to_tensor(x, dtype=None, sparse=False):\n+    if sparse:\n+        raise ValueError(\"`sparse=True` is not supported with jax backend\")\n     if dtype is not None:\n         dtype = standardize_dtype(dtype)\n     if isinstance(x, Variable):\n\n@@ -4,6 +4,7 @@ from keras_core.backend.numpy import math\n from keras_core.backend.numpy import nn\n from keras_core.backend.numpy import numpy\n from keras_core.backend.numpy import random\n+from keras_core.backend.numpy.core import SUPPORTS_SPARSE_TENSORS\n from keras_core.backend.numpy.core import Variable\n from keras_core.backend.numpy.core import cast\n from keras_core.backend.numpy.core import compute_output_spec\n\n@@ -7,6 +7,8 @@ from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.utils.nest import pack_sequence_as\n \n+SUPPORTS_SPARSE_TENSORS = False\n+\n \n class Variable(KerasVariable):\n     def _initialize(self, value):\n@@ -23,7 +25,9 @@ class Variable(KerasVariable):\n         return self.value\n \n \n-def convert_to_tensor(x, dtype=None):\n+def convert_to_tensor(x, dtype=None, sparse=False):\n+    if sparse:\n+        raise ValueError(\"`sparse=True` is not supported with numpy backend\")\n     if dtype is not None:\n         dtype = standardize_dtype(dtype)\n     if isinstance(x, Variable):\n\n@@ -5,6 +5,7 @@ from keras_core.backend.tensorflow import nn\n from keras_core.backend.tensorflow import numpy\n from keras_core.backend.tensorflow import random\n from keras_core.backend.tensorflow import tensorboard\n+from keras_core.backend.tensorflow.core import SUPPORTS_SPARSE_TENSORS\n from keras_core.backend.tensorflow.core import Variable\n from keras_core.backend.tensorflow.core import cast\n from keras_core.backend.tensorflow.core import compute_output_spec\n\n@@ -12,6 +12,8 @@ from keras_core.backend.common.name_scope import name_scope as base_name_scope\n from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.utils.naming import auto_name\n \n+SUPPORTS_SPARSE_TENSORS = True\n+\n \n class Variable(\n     KerasVariable,\n@@ -70,15 +72,28 @@ class Variable(\n         return self.value._write_object_proto(proto, options)\n \n \n-def convert_to_tensor(x, dtype=None):\n+def convert_to_tensor(x, dtype=None, sparse=True):\n+    \"\"\"Convert to a TensorFlow tensor.\n+\n+    `sparse=True` means that `tf.SparseTensor`s are returned as-is, which is the\n+    default with the TensorFlow backend. An explicit `sparse=False` densifies\n+    `tf.SparseTensor`s.\n+    \"\"\"\n+    if isinstance(x, tf.SparseTensor) and not sparse:\n+        x = tf.sparse.to_dense(x)\n     if dtype is not None:\n         dtype = standardize_dtype(dtype)\n-        if tf.is_tensor(x):\n-            return tf.cast(x, dtype=dtype)\n+    if not tf.is_tensor(x):\n         return tf.convert_to_tensor(x, dtype=dtype)\n+    elif dtype is not None:\n+        return tf.cast(x, dtype=dtype)\n+    else:\n+        return x\n \n \n def convert_to_numpy(x):\n+    if isinstance(x, tf.SparseTensor):\n+        x = tf.sparse.to_dense(x)\n     return np.array(x)\n \n \n@@ -95,6 +110,7 @@ def shape(x):\n     tensor values when the shape is unknown (this is tf specific, as dynamic\n     shapes do not apply in other backends).\n     \"\"\"\n+    if not tf.is_tensor(x):\n         x = tf.convert_to_tensor(x)\n     dynamic = tf.shape(x)\n     if x.shape == tf.TensorShape(None):\n\n@@ -20,6 +20,7 @@ from keras_core.backend.torch import math\n from keras_core.backend.torch import nn\n from keras_core.backend.torch import numpy\n from keras_core.backend.torch import random\n+from keras_core.backend.torch.core import SUPPORTS_SPARSE_TENSORS\n from keras_core.backend.torch.core import Variable\n from keras_core.backend.torch.core import cast\n from keras_core.backend.torch.core import compute_output_spec\n\n@@ -12,6 +12,8 @@ from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.utils.nest import pack_sequence_as\n \n+SUPPORTS_SPARSE_TENSORS = False\n+\n # Some operators such as 'aten::_foreach_mul_.Scalar'\n # are not currently implemented for the MPS device.\n # check https://github.com/pytorch/pytorch/issues/77764.\n@@ -118,7 +120,9 @@ class Variable(KerasVariable):\n             return False\n \n \n-def convert_to_tensor(x, dtype=None):\n+def convert_to_tensor(x, dtype=None, sparse=False):\n+    if sparse:\n+        raise ValueError(\"`sparse=True` is not supported with torch backend\")\n     if is_tensor(x):\n         device = get_device()\n         if x.device != device:\n\n@@ -13,12 +13,13 @@ class InputLayer(Layer):\n         shape=None,\n         batch_size=None,\n         dtype=None,\n+        sparse=None,\n         batch_shape=None,\n         input_tensor=None,\n         name=None,\n         **kwargs,\n     ):\n-        # TODO: support for sparse, ragged.\n+        # TODO: support for ragged.\n         super().__init__(name=name)\n         if \"input_shape\" in kwargs:\n             warnings.warn(\n@@ -45,6 +46,13 @@ class InputLayer(Layer):\n         self.batch_shape = tuple(batch_shape)\n         self._dtype = backend.standardize_dtype(dtype)\n \n+        self.sparse = bool(sparse)\n+        if self.sparse and not backend.SUPPORTS_SPARSE_TENSORS:\n+            raise ValueError(\n+                \"`sparse=True` is not supported with backend: \"\n+                f\"{backend.backend()}\"\n+            )\n+\n         if input_tensor is not None:\n             if not isinstance(input_tensor, backend.KerasTensor):\n                 raise ValueError(\n@@ -54,7 +62,7 @@ class InputLayer(Layer):\n                 )\n         else:\n             input_tensor = backend.KerasTensor(\n-                shape=batch_shape, dtype=dtype, name=name\n+                shape=batch_shape, dtype=dtype, sparse=sparse, name=name\n             )\n         self._input_tensor = input_tensor\n         Node(operation=self, call_args=(), call_kwargs={}, outputs=input_tensor)\n@@ -71,6 +79,7 @@ class InputLayer(Layer):\n         return {\n             \"batch_shape\": self.batch_shape,\n             \"dtype\": self.dtype,\n+            \"sparse\": self.sparse,\n             \"name\": self.name,\n         }\n \n@@ -80,6 +89,7 @@ def Input(\n     shape=None,\n     batch_size=None,\n     dtype=None,\n+    sparse=None,\n     batch_shape=None,\n     name=None,\n     tensor=None,\n@@ -104,6 +114,11 @@ def Input(\n         batch_size: Optional static batch size (integer).\n         dtype: The data type expected by the input, as a string\n             (e.g. `\"float32\"`, `\"int32\"`...)\n+        sparse: A boolean specifying whether the expected input will be sparse\n+            tensors. Note that, if `sparse` is `False`, sparse tensors can still\n+            be passed into the input - they will be densified with a default\n+            value of 0. This feature is only supported with the TensorFlow\n+            backend. Defaults to `False`.\n         name: Optional name string for the layer.\n             Should be unique in a model (do not reuse the same name twice).\n             It will be autogenerated if it isn't provided.\n@@ -127,6 +142,7 @@ def Input(\n         shape=shape,\n         batch_size=batch_size,\n         dtype=dtype,\n+        sparse=sparse,\n         batch_shape=batch_shape,\n         name=name,\n         input_tensor=tensor,\n\n@@ -1,29 +1,51 @@\n import numpy as np\n+from absl.testing import parameterized\n \n+from keras_core import backend\n from keras_core import testing\n from keras_core.backend import KerasTensor\n from keras_core.layers import InputLayer\n \n \n-class InputLayerTest(testing.TestCase):\n+class InputLayerTest(testing.TestCase, parameterized.TestCase):\n     # Testing happy path for layer without input tensor\n-    def test_input_basic(self):\n+    @parameterized.named_parameters(\n+        [\n+            {\"testcase_name\": \"dense\", \"sparse\": False},\n+            {\"testcase_name\": \"sparse\", \"sparse\": True},\n+        ]\n+    )\n+    def test_input_basic(self, sparse):\n         input_shape = (2, 3)\n         batch_size = 4\n         dtype = \"float32\"\n         ndim = len(tuple((batch_size,) + input_shape))\n \n-        values = InputLayer(\n-            shape=input_shape, batch_size=batch_size, dtype=dtype\n-        )\n+        init_kwargs = {\n+            \"shape\": input_shape,\n+            \"batch_size\": batch_size,\n+            \"dtype\": dtype,\n+            \"sparse\": sparse,\n+        }\n+\n+        if sparse and not backend.SUPPORTS_SPARSE_TENSORS:\n+            with self.assertRaisesRegex(\n+                ValueError, \"`sparse=True` is not supported\"\n+            ):\n+                InputLayer(**init_kwargs)\n+            return\n+\n+        values = InputLayer(**init_kwargs)\n \n         self.assertEqual(values.dtype, dtype)\n         self.assertEqual(values.batch_shape[0], batch_size)\n         self.assertEqual(values.batch_shape[1:], input_shape)\n+        self.assertEqual(values.sparse, sparse)\n         self.assertEqual(values.trainable, True)\n         self.assertIsInstance(values.output, KerasTensor)\n         self.assertEqual(values.output.ndim, ndim)\n         self.assertEqual(values.output.dtype, dtype)\n+        self.assertEqual(values.output.sparse, sparse)\n \n     # Testing shape is not None and batch_shape is not None condition\n     def test_input_error1(self):\n\n@@ -1,6 +1,7 @@\n import numpy as np\n import pytest\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import losses\n from keras_core import models\n@@ -270,6 +271,18 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         x = KerasTensor((None, 3, None, 1))\n         self.assertAllEqual(core.shape(x), (None, 3, None, 1))\n \n+    @pytest.mark.skipif(\n+        not backend.SUPPORTS_SPARSE_TENSORS,\n+        reason=\"Backend does not support sparse tensors.\",\n+    )\n+    def test_shape_sparse(self):\n+        import tensorflow as tf\n+\n+        x = tf.SparseTensor(\n+            indices=[[0, 0], [1, 2]], values=[1.0, 2.0], dense_shape=(2, 3)\n+        )\n+        self.assertAllEqual(core.shape(x), (2, 3))\n+\n     def test_convert_to_tensor(self):\n         x = np.ones((2,))\n         x = ops.convert_to_tensor(x)\n@@ -284,6 +297,32 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         with self.assertRaises(ValueError):\n             ops.convert_to_numpy(KerasTensor((2,)))\n \n+    @pytest.mark.skipif(\n+        not backend.SUPPORTS_SPARSE_TENSORS,\n+        reason=\"Backend does not support sparse tensors.\",\n+    )\n+    def test_convert_to_tensor_sparse(self):\n+        import tensorflow as tf\n+\n+        x = tf.SparseTensor(\n+            indices=[[0, 0], [1, 2]], values=[1.0, 2.0], dense_shape=(2, 3)\n+        )\n+\n+        x_default = ops.convert_to_tensor(x)\n+        self.assertIsInstance(x_default, tf.SparseTensor)\n+        self.assertAllClose(x, x_default)\n+        # Note that ops.convert_to_tensor does not expose the 'sparse' arg\n+        x_sparse = backend.convert_to_tensor(x, sparse=True)\n+        self.assertIsInstance(x_sparse, tf.SparseTensor)\n+        self.assertAllClose(x, x_sparse)\n+        x_dense = backend.convert_to_tensor(x, sparse=False)\n+        self.assertNotIsInstance(x_dense, tf.SparseTensor)\n+        self.assertAllClose(x, x_dense)\n+\n+        x_numpy = ops.convert_to_numpy(x)\n+        self.assertIsInstance(x_numpy, np.ndarray)\n+        self.assertAllClose(x_numpy, x_dense)\n+\n     def test_cond(self):\n         t = ops.cond(True, lambda: 0, lambda: 1)\n         self.assertEqual(t, 0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b3a54b991bc4a929774303026ff8f681142ab914", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 337 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -17,11 +17,14 @@\n Detailed documentation and user guides are available at\n [keras.io](https://keras.io).\n \"\"\"\n+\n+from keras import applications\n from keras import distribute\n from keras import models\n from keras.engine.input_layer import Input\n from keras.engine.sequential import Sequential\n from keras.engine.training import Model\n+from keras.testing_infra import test_utils\n \n # isort: off\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fcdce1cc25dea7eee41deb5e43d47fd8215c236f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 532 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 19 | Complexity Δ (Sum/Max): 26/26 | Churn Δ: 534 | Churn Cumulative: 753 | Contributors (this commit): 7 | Commits (past 90d): 10 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,528 @@\n+\"\"\"\n+Title: End-to-end Masked Language Modeling with BERT\n+Author: [Ankur Singh](https://twitter.com/ankur310794)\n+Converted to Keras-Core: [Mrutyunjay Biswal](https://twitter.com/LearnStochastic)\n+Date created: 2020/09/18\n+Last modified: 2023/09/06\n+Description: Implement a Masked Language Model (MLM) with BERT and fine-tune it on the IMDB Reviews dataset.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+Masked Language Modeling is a fill-in-the-blank task,\n+where a model uses the context words surrounding a mask token to try to predict what the\n+masked word should be.\n+\n+For an input that contains one or more mask tokens,\n+the model will generate the most likely substitution for each.\n+\n+Example:\n+\n+- Input: \"I have watched this [MASK] and it was awesome.\"\n+- Output: \"I have watched this movie and it was awesome.\"\n+\n+Masked language modeling is a great way to train a language\n+model in a self-supervised setting (without human-annotated labels).\n+Such a model can then be fine-tuned to accomplish various supervised\n+NLP tasks.\n+\n+This example teaches you how to build a BERT model from scratch,\n+train it with the masked language modeling task,\n+and then fine-tune this model on a sentiment classification task.\n+\n+We will use the Keras-Core `TextVectorization` and `MultiHeadAttention` layers\n+to create a BERT Transformer-Encoder network architecture.\n+\n+Note: This is only tensorflow backend compatible.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import os\n+import re\n+import glob\n+import numpy as np\n+import pandas as pd\n+from pathlib import Path\n+from dataclasses import dataclass\n+\n+import tensorflow as tf\n+import keras_core as keras\n+from keras_core import layers\n+\n+\"\"\"\n+## Configuration\n+\"\"\"\n+\n+@dataclass\n+class Config:\n+    MAX_LEN = 256\n+    BATCH_SIZE = 32\n+    LR = 0.001\n+    VOCAB_SIZE = 30000\n+    EMBED_DIM = 128\n+    NUM_HEAD = 8  # used in bert model\n+    FF_DIM = 128  # used in bert model\n+    NUM_LAYERS = 1\n+    NUM_EPOCHS = 1\n+    STEPS_PER_EPOCH = 2\n+\n+\n+config = Config()\n+\n+\"\"\"\n+## Download the Data: IMDB Movie Review Sentiment Classification\n+\n+Download the IMDB data and load into a Pandas DataFrame.\n+\"\"\"\n+\n+fpath = keras.utils.get_file(\n+    origin=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n+)\n+dirpath = Path(fpath).parent.absolute()\n+os.system(f\"tar -xf {fpath} -C {dirpath}\")\n+\n+\"\"\"\n+The `aclImdb` folder contains a `train` and `test` subfolder:\n+\"\"\"\n+\n+os.system(f\"ls {dirpath}/aclImdb\")\n+os.system(f\"ls {dirpath}/aclImdb/train\")\n+os.system(f\"ls {dirpath}/aclImdb/test\")\n+\n+\"\"\"\n+We are only interested in the `pos` and `neg` subfolders, so let's delete the rest:\n+\"\"\"\n+\n+os.system(f\"rm -r {dirpath}/aclImdb/train/unsup\")\n+os.system(f\"rm -r {dirpath}/aclImdb/train/*.feat\")\n+os.system(f\"rm -r {dirpath}/aclImdb/train/*.txt\")\n+os.system(f\"rm -r {dirpath}/aclImdb/test/*.feat\")\n+os.system(f\"rm -r {dirpath}/aclImdb/test/*.txt\")\n+\n+\"\"\"\n+Let's read the dataset from the text files to a DataFrame.\n+\"\"\"\n+\n+def get_text_list_from_files(files):\n+    text_list = []\n+    for name in files:\n+        with open(name) as f:\n+            for line in f:\n+                text_list.append(line)\n+    return text_list\n+\n+\n+def get_data_from_text_files(folder_name):\n+\n+    pos_files = glob.glob(f\"{dirpath}/aclImdb/\" + folder_name + \"/pos/*.txt\")\n+    pos_texts = get_text_list_from_files(pos_files)\n+    neg_files = glob.glob(f\"{dirpath}/aclImdb/\" + folder_name + \"/neg/*.txt\")\n+    neg_texts = get_text_list_from_files(neg_files)\n+    df = pd.DataFrame(\n+        {\n+            \"review\": pos_texts + neg_texts,\n+            \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),\n+        }\n+    )\n+    df = df.sample(len(df)).reset_index(drop=True)\n+    return df\n+\n+train_df = get_data_from_text_files(\"train\")\n+test_df = get_data_from_text_files(\"test\")\n+\n+all_data = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n+assert len(all_data) != 0, f'{all_data} is empty'\n+\n+\"\"\"\n+## Dataset preparation\n+\n+We will use the `TextVectorization` layer to vectorize the text into integer token ids.\n+It transforms a batch of strings into either\n+a sequence of token indices (one sample = 1D array of integer token indices, in order)\n+or a dense representation (one sample = 1D array of float values encoding an unordered set of tokens).\n+\n+Below, we define 3 preprocessing functions.\n+\n+1.  The `get_vectorize_layer` function builds the `TextVectorization` layer.\n+2.  The `encode` function encodes raw text into integer token ids.\n+3.  The `get_masked_input_and_labels` function will mask input token ids. It masks 15% of all input tokens in each sequence at random.\n+\"\"\"\n+\n+def custom_standardization(input_data):\n+    lowercase = tf.strings.lower(input_data)\n+    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n+    return tf.strings.regex_replace(\n+        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n+    )\n+\n+\n+def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n+    \"\"\"Build Text vectorization layer\n+\n+    Args:\n+      texts (list): List of string i.e input texts\n+      vocab_size (int): vocab size\n+      max_seq (int): Maximum sequence lenght.\n+      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n+\n+    Returns:\n+        layers.Layer: Return TextVectorization Keras Layer\n+    \"\"\"\n+    vectorize_layer = layers.TextVectorization(\n+        max_tokens=vocab_size,\n+        output_mode=\"int\",\n+        standardize=custom_standardization,\n+        output_sequence_length=max_seq,\n+    )\n+    vectorize_layer.adapt(texts)\n+\n+    # Insert mask token in vocabulary\n+    vocab = vectorize_layer.get_vocabulary()\n+    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n+    vectorize_layer.set_vocabulary(vocab)\n+    return vectorize_layer\n+\n+\n+vectorize_layer = get_vectorize_layer(\n+    all_data.review.values.tolist(),\n+    config.VOCAB_SIZE,\n+    config.MAX_LEN,\n+    special_tokens=[\"[mask]\"],\n+)\n+\n+# Get mask token id for masked language model\n+mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n+\n+\n+def encode(texts):\n+    encoded_texts = vectorize_layer(texts)\n+    return encoded_texts.numpy()\n+\n+\n+def get_masked_input_and_labels(encoded_texts):\n+    # 15% BERT masking\n+    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n+    # Do not mask special tokens\n+    inp_mask[encoded_texts <= 2] = False\n+    # Set targets to -1 by default, it means ignore\n+    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n+    # Set labels for masked tokens\n+    labels[inp_mask] = encoded_texts[inp_mask]\n+\n+    # Prepare input\n+    encoded_texts_masked = np.copy(encoded_texts)\n+    # Set input to [MASK] which is the last token for the 90% of tokens\n+    # This means leaving 10% unchanged\n+    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n+    encoded_texts_masked[\n+        inp_mask_2mask\n+    ] = mask_token_id  # mask token is the last in the dict\n+\n+    # Set 10% to a random token\n+    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n+    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n+        3, mask_token_id, inp_mask_2random.sum()\n+    )\n+\n+    # Prepare sample_weights to pass to .fit() method\n+    sample_weights = np.ones(labels.shape)\n+    sample_weights[labels == -1] = 0\n+\n+    # y_labels would be same as encoded_texts i.e input tokens\n+    y_labels = np.copy(encoded_texts)\n+\n+    return encoded_texts_masked, y_labels, sample_weights\n+\n+\n+# We have 25000 examples for training\n+x_train = encode(train_df.review.values)  # encode reviews with vectorizer\n+y_train = train_df.sentiment.values\n+train_classifier_ds = (\n+    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+    .shuffle(1000)\n+    .batch(config.BATCH_SIZE)\n+)\n+\n+# We have 25000 examples for testing\n+x_test = encode(test_df.review.values)\n+y_test = test_df.sentiment.values\n+test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n+    config.BATCH_SIZE\n+)\n+\n+# Build dataset for end to end model input (will be used at the end)\n+test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices(\n+    (test_df.review.values, y_test)\n+).batch(config.BATCH_SIZE)\n+\n+# Prepare data for masked language model\n+x_all_review = encode(all_data.review.values)\n+x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n+    x_all_review\n+)\n+\n+mlm_ds = tf.data.Dataset.from_tensor_slices(\n+    (x_masked_train, y_masked_labels, sample_weights)\n+)\n+mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)\n+\n+id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n+token2id = {y: x for x, y in id2token.items()}\n+\n+class MaskedTextGenerator(keras.callbacks.Callback):\n+    def __init__(self, sample_tokens, top_k=5):\n+        self.sample_tokens = sample_tokens\n+        self.k = top_k\n+\n+    def decode(self, tokens):\n+        return \" \".join([id2token[t] for t in tokens if t != 0])\n+\n+    def convert_ids_to_tokens(self, id):\n+        return id2token[id]\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        prediction = self.model.predict(self.sample_tokens)\n+\n+        masked_index = np.where(self.sample_tokens == mask_token_id)\n+        masked_index = masked_index[1]\n+        mask_prediction = prediction[0][masked_index]\n+\n+        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n+        values = mask_prediction[0][top_indices]\n+\n+        for i in range(len(top_indices)):\n+            p = top_indices[i]\n+            v = values[i]\n+            tokens = np.copy(self.sample_tokens[0])\n+            tokens[masked_index[0]] = p\n+            result = {\n+                \"input_text\": self.decode(self.sample_tokens[0]),\n+                \"prediction\": self.decode(tokens),\n+                \"probability\": v,\n+                \"predicted mask token\": self.convert_ids_to_tokens(p),\n+            }\n+\n+sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n+generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n+\n+\"\"\"\n+## Create BERT model (Pretraining Model) for masked language modeling\n+\n+We will create a BERT-like pretraining model architecture\n+using the `MultiHeadAttention` layer.\n+It will take token ids as inputs (including masked tokens)\n+and it will predict the correct ids for the masked input tokens.\n+\"\"\"\n+\n+def bert_module(query, key, value, layer_num):\n+    # Multi headed self-attention\n+    attention_output = layers.MultiHeadAttention(\n+        num_heads=config.NUM_HEAD,\n+        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n+        name=f\"encoder_{layer_num}_multiheadattention\",\n+    )(query, key, value)\n+    attention_output = layers.Dropout(0.1, name=f\"encoder_{layer_num}_att_dropout\")(\n+        attention_output\n+    )\n+    attention_output = layers.LayerNormalization(\n+        epsilon=1e-6, name=f\"encoder_{layer_num}_att_layernormalization\"\n+    )(query + attention_output)\n+\n+    # Feed-forward layer\n+    ffn = keras.Sequential(\n+        [\n+            layers.Dense(config.FF_DIM, activation=\"relu\"),\n+            layers.Dense(config.EMBED_DIM),\n+        ],\n+        name=f\"encoder_{layer_num}_ffn\",\n+    )\n+    ffn_output = ffn(attention_output)\n+    ffn_output = layers.Dropout(0.1, name=f\"encoder_{layer_num}_ffn_dropout\")(\n+        ffn_output\n+    )\n+    sequence_output = layers.LayerNormalization(\n+        epsilon=1e-6, name=f\"encoder_{layer_num}_ffn_layernormalization\"\n+    )(attention_output + ffn_output)\n+    return sequence_output\n+\n+\n+def get_pos_encoding_matrix(max_len, d_emb):\n+    pos_enc = np.array(\n+        [\n+            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n+            if pos != 0\n+            else np.zeros(d_emb)\n+            for pos in range(max_len)\n+        ]\n+    )\n+    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n+    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n+    return pos_enc\n+\n+\n+loss_fn = keras.losses.SparseCategoricalCrossentropy(\n+    reduction=None\n+)\n+loss_tracker = keras.metrics.Mean(name=\"loss\")\n+\n+\n+class MaskedLanguageModel(keras.Model):\n+    def train_step(self, inputs):\n+        if len(inputs) == 3:\n+            features, labels, sample_weight = inputs\n+        else:\n+            features, labels = inputs\n+            sample_weight = None\n+\n+        with tf.GradientTape() as tape:\n+            predictions = self(features, training=True)\n+            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n+\n+        # Compute gradients\n+        trainable_vars = self.trainable_variables\n+        gradients = tape.gradient(loss, trainable_vars)\n+\n+        # Update weights\n+        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n+\n+        # Compute our own metrics\n+        loss_tracker.update_state(loss, sample_weight=sample_weight)\n+\n+        # Return a dict mapping metric names to current value\n+        return {\"loss\": loss_tracker.result()}\n+\n+    @property\n+    def metrics(self):\n+        # We list our `Metric` objects here so that `reset_states()` can be\n+        # called automatically at the start of each epoch\n+        # or at the start of `evaluate()`.\n+        # If you don't implement this property, you have to call\n+        # `reset_states()` yourself at the time of your choosing.\n+        return [loss_tracker]\n+\n+\n+def create_masked_language_bert_model():\n+    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n+\n+    word_embeddings = layers.Embedding(\n+        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n+    )(inputs)\n+    \n+    position_embeddings = layers.Embedding(\n+        input_dim=config.MAX_LEN,\n+        output_dim=config.EMBED_DIM,\n+        embeddings_initializer=keras.initializers.Constant(get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)),\n+        name=\"position_embedding\",\n+    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n+    \n+    embeddings = word_embeddings + position_embeddings\n+\n+    encoder_output = embeddings\n+    for i in range(config.NUM_LAYERS):\n+        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n+\n+    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n+        encoder_output\n+    )\n+    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n+\n+    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n+    mlm_model.compile(optimizer=optimizer)\n+    return mlm_model\n+\n+bert_masked_model = create_masked_language_bert_model()\n+bert_masked_model.summary()\n+\n+\"\"\"\n+## Train and Save\n+\"\"\"\n+\n+bert_masked_model.fit(mlm_ds, epochs=Config.NUM_EPOCHS, steps_per_epoch=Config.STEPS_PER_EPOCH, callbacks=[generator_callback])\n+bert_masked_model.save(\"bert_mlm_imdb.keras\")\n+\n+\"\"\"\n+## Fine-tune a sentiment classification model\n+\n+We will fine-tune our self-supervised model on a downstream task of sentiment classification.\n+To do this, let's create a classifier by adding a pooling layer and a `Dense` layer on top of the\n+pretrained BERT features.\n+\"\"\"\n+\n+# Load pretrained bert model\n+mlm_model = keras.models.load_model(\n+    \"bert_mlm_imdb.keras\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n+)\n+pretrained_bert_model = keras.Model(\n+    mlm_model.input, mlm_model.get_layer(\"encoder_0_ffn_layernormalization\").output\n+)\n+\n+# Freeze it\n+pretrained_bert_model.trainable = False\n+\n+\n+def create_classifier_bert_model():\n+    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n+    sequence_output = pretrained_bert_model(inputs)\n+    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n+    hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n+    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n+    classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n+    optimizer = keras.optimizers.Adam()\n+    classifer_model.compile(\n+        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n+    )\n+    return classifer_model\n+\n+\n+classifer_model = create_classifier_bert_model()\n+classifer_model.summary()\n+\n+# Train the classifier with frozen BERT stage\n+classifer_model.fit(\n+    train_classifier_ds,\n+    epochs=Config.NUM_EPOCHS,\n+    steps_per_epoch=Config.STEPS_PER_EPOCH,\n+    validation_data=test_classifier_ds,\n+)\n+\n+# Unfreeze the BERT model for fine-tuning\n+pretrained_bert_model.trainable = True\n+optimizer = keras.optimizers.Adam()\n+classifer_model.compile(\n+    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n+)\n+classifer_model.fit(\n+    train_classifier_ds,\n+    epochs=Config.NUM_EPOCHS,\n+    steps_per_epoch=Config.STEPS_PER_EPOCH,\n+    validation_data=test_classifier_ds,\n+)\n+\n+\"\"\"\n+## Create an end-to-end model and evaluate it\n+\n+When you want to deploy a model, it's best if it already includes its preprocessing\n+pipeline, so that you don't have to reimplement the preprocessing logic in your\n+production environment. Let's create an end-to-end model that incorporates\n+the `TextVectorization` layer, and let's evaluate. Our model will accept raw strings\n+as input.\n+\"\"\"\n+\n+def get_end_to_end(model):\n+    inputs_string = keras.Input(shape=(1,), dtype=\"string\")\n+    indices = vectorize_layer(inputs_string)\n+    outputs = model(indices)\n+    end_to_end_model = keras.Model(inputs_string, outputs, name=\"end_to_end_model\")\n+    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n+    end_to_end_model.compile(\n+        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n+    )\n+    return end_to_end_model\n+\n+\n+end_to_end_classification_model = get_end_to_end(classifer_model)\n+end_to_end_classification_model.evaluate(test_raw_classifier_ds)\n\\ No newline at end of file\n\n@@ -29,11 +29,13 @@ class Constant(Initializer):\n     \"\"\"\n \n     def __init__(self, value=0.0):\n-        self.value = float(value)\n+        self.value = value\n \n     def __call__(self, shape, dtype=None):\n         dtype = standardize_dtype(dtype)\n-        return self.value * ops.ones(shape=shape, dtype=dtype)\n+        return ops.cast(self.value, dtype=dtype) * ops.ones(\n+            shape=shape, dtype=dtype\n+        )\n \n     def get_config(self):\n         return {\"value\": self.value}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0bbe3873f8e2494e3f0e978aca5aeae762bc9002", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 553 | Contributors (this commit): 23 | Commits (past 90d): 5 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -47,11 +47,10 @@ setup(\n         \"dm-tree\",\n     ],\n     # Supported Python versions\n-    python_requires=\">=3.8\",\n+    python_requires=\">=3.9\",\n     classifiers=[\n         \"Development Status :: 4 - Beta\",\n         \"Programming Language :: Python :: 3\",\n-        \"Programming Language :: Python :: 3.8\",\n         \"Programming Language :: Python :: 3.9\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#953b83fbec6247992d76dbe6a7176b036605f4e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 33 | Churn Cumulative: 2570 | Contributors (this commit): 9 | Commits (past 90d): 23 | Contributors (cumulative): 9 | DMM Complexity: 0.4444444444444444\n\nDIFF:\n@@ -53,20 +53,31 @@ class JAXTrainer(base_trainer.Trainer):\n             and not self._compile_metrics.built\n         )\n         if model_unbuilt or compile_metrics_unbuilt:\n-            # Build the model on one batch of data.\n+\n+            def _convert_data_to_spec(d):\n+                if d is None:\n+                    return None\n+                return backend.KerasTensor(d.shape, d.dtype)\n+\n+            data_spec = tree.map_structure(_convert_data_to_spec, data_batch)\n             (\n-                x,\n-                y,\n-                sample_weight,\n-            ) = data_adapter_utils.unpack_x_y_sample_weight(data_batch)\n-            # Build model\n-            with backend.StatelessScope():\n-                y_pred = self(x)\n+                x_spec,\n+                y_spec,\n+                sample_weight_spec,\n+            ) = data_adapter_utils.unpack_x_y_sample_weight(data_spec)\n+            # Note that this __call__ run the forward path and trigger variable\n+            # creation.\n+            y_pred_spec = backend.compute_output_spec(self.__call__, x_spec)\n             if compile_metrics_unbuilt:\n-                    # Build metrics\n-                    self.compute_metrics(\n-                        x, y, y_pred, sample_weight=sample_weight\n+                # This will trigger the metric variable creation.\n+                backend.compute_output_spec(\n+                    self.compute_metrics,\n+                    x_spec,\n+                    y_spec,\n+                    y_pred_spec,\n+                    sample_weight=sample_weight_spec,\n                 )\n+\n         if self.optimizer is not None and not self.optimizer.built:\n             # Build optimizer\n             self.optimizer.build(self.trainable_variables)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d050a25256ca4bfda1478f3a619f424eaa71cf04", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 2144 | Contributors (this commit): 7 | Commits (past 90d): 14 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -201,7 +201,9 @@ class BaseOptimizer:\n \n     def apply_gradients(self, grads_and_vars):\n         grads, trainable_variables = zip(*grads_and_vars)\n-        return self.apply(grads, trainable_variables)\n+        self.apply(grads, trainable_variables)\n+        # Return iterations for compat with tf.keras.\n+        return self.iterations\n \n     def apply(self, grads, trainable_variables=None):\n         \"\"\"\n@@ -261,7 +263,6 @@ class BaseOptimizer:\n             for variable in trainable_variables:\n                 if getattr(variable, \"constraint\", None) is not None:\n                     variable.assign(variable.constraint(variable))\n-            return self.iterations\n \n     def _internal_apply_gradients(self, grads_and_vars):\n         learning_rate = self._get_current_learning_rate()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aa31b8d905e5e79fb22afd1d29e400333d403fbb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/7 | Churn Δ: 67 | Churn Cumulative: 3760 | Contributors (this commit): 18 | Commits (past 90d): 62 | Contributors (cumulative): 27 | DMM Complexity: 0.9607843137254902\n\nDIFF:\n@@ -779,6 +779,10 @@ class Layer(BackendLayer, Operation):\n                     # Clear or update the current scope if necessary.\n                     if not self.autocast:\n                         new_scope = backend.AutocastScope(None)\n+                    elif not backend.is_float_dtype(self.compute_dtype):\n+                        # Some preprocessing layers might have a non-float\n+                        # dtype, we should not autocast in this case.\n+                        new_scope = backend.AutocastScope(None)\n                     elif current_scope.dtype != self.compute_dtype:\n                         new_scope = backend.AutocastScope(self.compute_dtype)\n                 elif self.compute_dtype != self.variable_dtype:\n\n@@ -363,7 +363,6 @@ class LayerTest(testing.TestCase):\n         reason=\"Some torch ops not implemented for float16 on CPU.\",\n     )\n     def test_mixed_precision(self):\n-        print(\"Run with backend.backend()\", backend.backend())\n         x = np.ones((4, 4))\n \n         layer = layers.Dense(2, dtype=\"float16\")\n@@ -379,6 +378,68 @@ class LayerTest(testing.TestCase):\n         self.assertEqual(backend.standardize_dtype(y.dtype), \"float16\")\n         self.assertEqual(layer.kernel.dtype, \"float32\")\n \n+    @pytest.mark.skipif(\n+        backend.backend() == \"torch\",\n+        reason=\"Some torch ops not implemented for float16 on CPU.\",\n+    )\n+    def test_autocast(self):\n+        assertEqual = self.assertEqual\n+\n+        # A layer with a int dtype (some preprocessing layers do this).\n+        class InnerLayerOne(layers.Layer):\n+            def __init__(self):\n+                super().__init__(dtype=\"int\")\n+                self.v = self.add_weight(\n+                    shape=(),\n+                    initializer=\"ones\",\n+                    trainable=True,\n+                    dtype=\"float32\",\n+                )\n+                self.built = True\n+\n+            def call(self, x):\n+                # Should not autocast.\n+                assertEqual(backend.standardize_dtype(self.v.dtype), \"float32\")\n+                return ops.cast(x, \"float32\") + self.v\n+\n+        # A layer that is explicitly full precision.\n+        class InnerLayerTwo(layers.Layer):\n+            def __init__(self):\n+                super().__init__(dtype=\"float32\")\n+                self.v = self.add_weight(\n+                    shape=(),\n+                    initializer=\"ones\",\n+                    trainable=True,\n+                )\n+                self.built = True\n+\n+            def call(self, x):\n+                # Should not autocast.\n+                assertEqual(backend.standardize_dtype(self.v.dtype), \"float32\")\n+                return x + self.v\n+\n+        # A layer that is explicitly mixed precision with inner layers.\n+        class MixedPrecisionLayer(layers.Layer):\n+            def __init__(self):\n+                super().__init__(dtype=\"mixed_float16\")\n+                self.v = self.add_weight(\n+                    shape=(),\n+                    initializer=\"ones\",\n+                    trainable=True,\n+                )\n+                self.inner_one = InnerLayerOne()\n+                self.inner_two = InnerLayerTwo()\n+                self.built = True\n+\n+            def call(self, x):\n+                # Should autocast.\n+                assertEqual(backend.standardize_dtype(self.v.dtype), \"float16\")\n+                return self.inner_two(self.inner_one(x + self.v))\n+\n+        layer = MixedPrecisionLayer()\n+        y = layer(np.array(0.0))\n+        self.assertEqual(y, 3.0)\n+\n     @pytest.mark.skipif(\n         backend.backend() == \"numpy\",\n         reason=\"Numpy backend does not support masking.\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
