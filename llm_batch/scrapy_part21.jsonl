{"custom_id": "scrapy#f5eb71fb69960a43fa6dad822b47822a7259681a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 407 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 4 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -78,6 +78,8 @@ class UrlUtilsTest(unittest.TestCase):\n                          'product.html?id=200')\n         self.assertEqual(url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", ['id', 'name']),\n                          'product.html?id=200&name=wired')\n+        self.assertEqual(url_query_cleaner(\"product.html?id=200&foo=bar&name=wired#id20\", ['id', 'foo']),\n+                         'product.html?id=200&foo=bar')\n \n     def test_canonicalize_url(self):\n         # no query arguments\n\n@@ -87,6 +87,7 @@ def url_query_parameter(url, parameter, default=None, keep_blank_values=0):\n def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='='):\n     \"\"\"Clean url arguments leaving only those passed in the parameterlist\"\"\"\n     try:\n+        url = urlparse.urldefrag(url)[0]\n         base, query = url.split('?', 1)\n         parameters = [pair.split(kvsep, 1) for pair in query.split(sep)]\n     except:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c209f214a88de93361e57b3ef17e54445594ca49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 30 | Churn Cumulative: 407 | Contributors (this commit): 2 | Commits (past 90d): 16 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -43,9 +43,9 @@ def extract_unquoted(locations):\n \n class ExtractImages(object):\n     \"\"\"\n-    This adaptor receives either an XPathSelector containing\n-    the desired locations for finding urls, or a list of relative\n-    links to be resolved.\n+    This adaptor may receive either an XPathSelector containing\n+    the desired locations for finding urls, a list of relative\n+    links to be resolved, or simply a link (relative or not).\n \n     Input: XPathSelector, XPathSelectorList, iterable\n     Output: list of unicodes\n@@ -79,6 +79,9 @@ class ExtractImages(object):\n         if not self.base_url:\n             raise AttributeError('You must specify either a response or a base_url to the ExtractImages adaptor.')\n         \n+        if isinstance(locations, basestring):\n+            locations = [locations]\n+\n         rel_links = []\n         for location in flatten(locations):\n             if isinstance(location, (XPathSelector, XPathSelectorList)):\n\n@@ -11,16 +11,7 @@ from scrapy.core.exceptions import UsageError\n from scrapy.utils.iterators import xmliter, csviter\n from scrapy.utils.misc import hash_values\n \n-class BasicSpider(BaseSpider):\n-    \"\"\"\n-    This class is basically a BaseSpider with support for GUID generating\n-    \"\"\"\n-    gen_guid_attribs = []\n-\n-    def set_guid(self, item):\n-        item.guid = hash_values(self.domain_name, *[str(getattr(item, aname) or '') for aname in self.gen_guid_attribs])\n-   \n-class CrawlSpider(BasicSpider):\n+class CrawlSpider(BaseSpider):\n     \"\"\"\n     This class works as a base class for spiders that crawl over websites\n     \"\"\"\n@@ -86,7 +77,15 @@ class CrawlSpider(BasicSpider):\n                 self.set_guid(entry)\n         return ret\n \n-class XMLFeedSpider(BasicSpider):\n+    def set_guid(self, item):\n+        \"\"\"\n+        This method is called whenever the spider returns items, for each item.\n+        It should set the 'guid' attribute to the given item with a string that\n+        identifies the item uniquely.\n+        \"\"\"\n+        raise NotConfigured('You must define set_guid method in order to scrape items.')\n+\n+class XMLFeedSpider(BaseSpider):\n     \"\"\"\n     This class intends to be the base class for spiders that scrape\n     from XML feeds.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bddfbee0d497366885bc3210d3cbf5c4ef842282", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 14 | Files Changed: 3 | Hunks: 18 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 49 | Churn Cumulative: 422 | Contributors (this commit): 1 | Commits (past 90d): 16 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -3,8 +3,12 @@ from scrapy.utils.markup import replace_tags, remove_entities\n \n def remove_tags(value):\n     \"\"\"\n+    Removes any tags found in each of the provided list's string.\n+    E.g:\n+      >> remove_tags(['<head>my header</head>', '<body>my <b>body</b></body>'])\n+      [u'my header', u'my body']\n     Input: iterable with strings\n-    Output: list of strings\n+    Output: list of unicodes\n     \"\"\"\n     return [ replace_tags(v) for v in value ]\n \n@@ -18,7 +22,7 @@ def remove_root(value):\n         m = _remove_root_re.search(value)\n         if m:\n             value = m.group(1)\n-        return value\n+        return unicode(value)\n     return [ _remove_root(v) for v in value ]\n \n class Unquote(object):\n\n@@ -1,3 +1,5 @@\n+# -*- coding: utf-8 -*-\n+\n import re\n from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n from scrapy.utils.url import canonicalize_url\n@@ -8,8 +10,11 @@ def to_unicode(value):\n     \"\"\"\n     Receives a list of strings, converts\n     it to unicode, and returns a new list.\n+    E.g:\n+      >> to_unicode(['it costs 20€, or 30£'])\n+        [u'it costs 20\\u20ac, or 30\\xa3']\n \n-    Input: iterable with strings\n+    Input: iterable of strings\n     Output: list of unicodes\n     \"\"\"\n     if hasattr(value, '__iter__'):\n@@ -19,29 +24,38 @@ def to_unicode(value):\n \n def clean_spaces(value):\n     \"\"\"\n-    Converts multispaces into single spaces.\n-    E.g. \"Hello   sir\" would turn into \"Hello sir\".\n+    Converts multispaces into single spaces for each string\n+    in the provided iterable.\n+    E.g:\n+      >> clean_spaces(['Hello   sir'])\n+      [u'Hello sir']\n \n-    Input: list of unicodes\n+    Input: iterable of unicodes\n     Output: list of unicodes\n     \"\"\"\n     _clean_spaces_re = re.compile(\"\\s+\", re.U)\n-    return [ _clean_spaces_re.sub(' ', v) for v in value ]\n+    return [ _clean_spaces_re.sub(' ', v.decode('utf-8')) for v in value ]\n \n def strip_list(value):\n     \"\"\"\n     Removes any spaces at both the start and the ending\n     of each string in the provided list.\n+    E.g:\n+      >> strip_list([' hi   ', 'buddies  '])\n+      [u'hi', u'buddies']\n \n-    Input: list of unicodes\n+    Input: iterable of unicodes\n     Output: list of unicodes\n     \"\"\"\n-    return [ v.strip() for v in value ]\n+    return [ unicode(v.strip()) for v in value ]\n \n def drop_empty(value):\n     \"\"\"\n     Removes any index that evaluates to None\n     from the provided iterable.\n+    E.g:\n+      >> drop_empty([0, 'this', None, 'is', False, 'an example'])\n+      ['this', 'is', 'an example'] \n \n     Input: iterable\n     Output: list\n@@ -50,22 +64,25 @@ def drop_empty(value):\n \n def canonicalize_urls(value):\n     \"\"\"\n-    Tries to canonicalize each url in the list you provide.\n+    Canonicalizes each url in the list you provide.\n     To see what this implies, check out canonicalize_url's\n     docstring, at scrapy.utils.url.py\n \n-    Input: list of unicodes(urls)\n+    Input: iterable of unicodes(urls)\n     Output: list of unicodes(urls)\n     \"\"\"\n     if hasattr(value, '__iter__'):\n         return [canonicalize_url(url) for url in value]\n     elif isinstance(value, basestring):\n         return canonicalize_url(value)\n-    return ''\n+    return u''\n \n class Delist(object):\n     \"\"\"\n-    Input: iterable with strings\n+    Joins a list with the specified delimiter\n+    in the adaptor's constructor.\n+\n+    Input: iterable of strings\n     Output: unicode\n     \"\"\"\n     def __init__(self, delimiter=' '):\n\n@@ -74,7 +74,7 @@ class AdaptorsTestCase(unittest.TestCase):\n                          ['hi there, sweety ;D', 'I CAN HAZ TEST??'])\n         \n     def test_drop_empty_elements(self):\n-        self.assertEqual(adaptors.drop_empty([1, 2, None, 5, None, 6, None, 'hi']),\n+        self.assertEqual(adaptors.drop_empty([1, 2, None, 5, 0, 6, False, 'hi']),\n                          [1, 2, 5, 6, 'hi'])\n         \n     def test_delist(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d6d32fa67259f8f6a3a75b8064995ce90fb730ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 19 | Churn Cumulative: 214 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,11 @@\n #!/usr/bin/env python\n \"\"\"Scrapy admin script is used to create new scrapy projects and similar\n tasks\"\"\"\n+from __future__ import with_statement\n \n import os\n import shutil\n+import string\n from optparse import OptionParser\n \n import scrapy\n@@ -17,12 +19,27 @@ Available commands:\n       Starts a new project with name 'project_name'\n \"\"\"\n \n+TEMPLATES = (\n+        'scrapy_settings.py',\n+        )\n+\n+def render_templatefile(path, **kwargs):\n+    with open(path, 'rb') as file:\n+        raw = file.read()\n+\n+    content = string.Template(raw).substitute(**kwargs)\n+\n+    with open(path, 'wb') as file:\n+        file.write(content)\n+\n+\n def main():\n     parser = OptionParser(usage=usage)\n     opts, args = parser.parse_args()\n     \n     if not args:\n         parser.print_help()\n+        return\n \n     cmd = args[0]\n     if cmd == \"startproject\":\n@@ -30,6 +47,8 @@ def main():\n             project_name = args[1]\n             project_tplpath = os.path.join(scrapy.__path__[0], \"conf\", \"project_template\")\n             shutil.copytree(project_tplpath, project_name)\n+            for path in TEMPLATES:\n+                render_templatefile(os.path.join(project_name, path), project_name=project_name)\n         else:\n             print \"scrapy-admin.py: missing project name\"\n     else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#862def2d739194bc9cb572308fac5eafc5d23341", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 28 | Churn Cumulative: 639 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,14 +1,12 @@\n #!/usr/bin/env python\n \"\"\"Scrapy admin script is used to create new scrapy projects and similar\n tasks\"\"\"\n-from __future__ import with_statement\n-\n import os\n import shutil\n-import string\n from optparse import OptionParser\n \n import scrapy\n+from scrapy.utils.misc import render_templatefile\n \n usage = \"\"\"\n scrapy-admin.py [options] [command]\n@@ -23,16 +21,6 @@ TEMPLATES = (\n         'scrapy_settings.py',\n         )\n \n-def render_templatefile(path, **kwargs):\n-    with open(path, 'rb') as file:\n-        raw = file.read()\n-\n-    content = string.Template(raw).substitute(**kwargs)\n-\n-    with open(path, 'wb') as file:\n-        file.write(content)\n-\n-\n def main():\n     parser = OptionParser(usage=usage)\n     opts, args = parser.parse_args()\n\n@@ -1,7 +1,10 @@\n \"\"\"\n Auxiliary functions which doesn't fit anywhere else\n \"\"\"\n+from __future__ import with_statement\n+\n import re\n+import string\n import hashlib\n \n from twisted.internet import defer\n@@ -113,3 +116,14 @@ def hash_values(*values):\n         hash.update(value)\n     return hash.hexdigest()\n \n+\n+def render_templatefile(path, **kwargs):\n+    with open(path, 'rb') as file:\n+        raw = file.read()\n+\n+    content = string.Template(raw).substitute(**kwargs)\n+\n+    with open(path, 'wb') as file:\n+        file.write(content)\n+\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9a7b5a7cbf4e2f2ae053de93629f50c5c73740c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 32 | Files Changed: 1 | Hunks: 14 | Methods Changed: 8 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 66 | Churn Cumulative: 132 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,64 +1,66 @@\n+from __future__ import with_statement\n+\n import os\n import string\n+import shutil\n \n from scrapy.spider import spiders\n from scrapy.command import ScrapyCommand\n from scrapy.conf import settings\n+from scrapy.utils.misc import render_templatefile\n \n class Command(ScrapyCommand):\n-\n-    \"\"\" Default template file name \"\"\"\n-    template_name = 'spider.tmpl'\n-\n     \"\"\" Childs can define custom tvars \"\"\"\n     custom_tvars = {}\n \n     def syntax(self):\n-        return \"<spider_name> <spider_domain_name>\"\n+        return \"<spider_name> <spider_domain_name> [--template=template_name]\"\n \n     def short_desc(self):\n-        return \"Generate new spider based on predefined template\"\n+        return \"Generate new spider based on a predefined template\"\n+\n+    def add_options(self, parser):\n+        ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"--template\", dest=\"template\", help=\"Uses a custom template.\", default=\"crawl\")\n \n     def run(self, args, opts):\n-        if len(args) != 2:\n+        if len(args) < 2:\n             return False\n \n+        template_file = os.path.join(settings['TEMPLATES_DIR'], 'spider_%s.tmpl' % opts.template)\n+        if not os.path.exists(template_file):\n+            print \"Template named %s.tmpl does not exist\" % opts.template\n+            return\n+\n         name = args[0]\n         site = args[1]\n         spiders_dict = spiders.asdict()\n-        if not name in spiders_dict.keys():\n-            self._genspider(name, site)\n+        if not site in spiders_dict.keys():\n+            name = self.normalize_name(name)\n+            self._genspider(name, site, template_file)\n         else:\n-            print \"Spider '%s' exist\" % name\n+            print \"Spider '%s' already exists\" % name\n \n-    def _genspider(self, name, site):\n+    def normalize_name(self, name):\n+        name = name.replace('-', '_') # - are replaced by _, for valid python modules\n+        if name[0] not in string.letters: # name must start with a letter, for valid python modules\n+            name = \"a\" + name\n+            print \"Spider names must start with a letter; converted to %s.\" % name\n+        return name\n+        \n+    def _genspider(self, name, site, template_file):\n         \"\"\" Generate spider \"\"\"\n         tvars = {\n             'name': name,\n             'site': site,\n-            'classname': '%sSpider' % ''.join([s.capitalize() for s in name.split('-')])\n+            'classname': '%sSpider' % ''.join([s.capitalize() for s in name.split('_')])\n         }\n         tvars.update(self.custom_tvars)\n \n         spiders_module = __import__(settings['NEWSPIDER_MODULE'], {}, {}, [''])\n-        spidersdir = os.path.abspath(os.path.dirname(spiders_module.__file__))\n-        if name[0] not in string.letters: # must start with a letter, for valid python modules\n-            name = \"a\" + name\n-        name = name.replace('-', '_') # - are replaced by _, for valid python modules\n-        self._genfiles(self.template_name, '%s/%s.py' % (spidersdir, name), tvars)\n+        spiders_dir = os.path.abspath(os.path.dirname(spiders_module.__file__))\n+        spider_file = '%s/%s.py' % (spiders_dir, name)\n+\n+        shutil.copyfile(template_file, spider_file)\n+        render_templatefile(spider_file, **tvars)\n \n-    def _genfiles(self, template_name, source_name, tvars):\n-        \"\"\" Generate source from template, substitute variables \"\"\"\n-        template_file = os.path.join(settings['TEMPLATES_DIR'], template_name)\n-        tmpl = open(template_file)\n-        clines = []\n-        for l in tmpl.readlines():\n-            for key, val in tvars.items():\n-                l = l.replace('@%s@' % key, val)\n-            clines.append(l)\n-        tmpl.close()\n-        source = ''.join(clines)\n-        if not os.path.exists(source_name):\n-            sfile = open(source_name, \"w\")\n-            sfile.write(source)\n-            sfile.close()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d5469eabb2bc8cb79120c5c977ae2732dea0c288", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 29 | Churn Cumulative: 224 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ import time\n import hashlib\n import urllib\n import urlparse\n+from datetime import datetime\n from cStringIO import StringIO\n \n import Image\n@@ -17,7 +18,7 @@ from scrapy.conf import settings\n from scrapy.contrib.pipeline.media import MediaPipeline\n \n # the age at which we download images again\n-IMAGE_EXPIRES = settings.getint('IMAGES_EXPIRES', 90)\n+IMAGE_EXPIRES = settings.getint('IMAGES_EXPIRES', 15)\n \n class NoimagesDrop(DropItem):\n     pass\n@@ -50,12 +51,10 @@ class ImagesPipeline(MediaPipeline):\n \n     def media_to_download(self, request, info):\n         relative, absolute = self._get_paths(request)\n-        if not should_download(absolute):\n-            self.inc_stats(info.domain, 'uptodate')\n-            referer = request.headers.get('Referer')\n-            log.msg('Image (uptodate): Downloaded %s from %s referred in <%s>' % \\\n-                    (self.MEDIA_TYPE, request, referer), level=log.DEBUG, domain=info.domain)\n-            return relative\n+        expired, mtime = image_expired(absolute)\n+        if not expired:\n+            mod_date = datetime.utcfromtimestamp(mtime)\n+            request.headers['If-Modified-Since'] = mod_date.strftime('%a, %d %b %Y %H:%M:%S GMT')\n \n     def media_downloaded(self, response, request, info):\n         mtype = self.MEDIA_TYPE\n@@ -75,8 +74,15 @@ class ImagesPipeline(MediaPipeline):\n         return result\n \n     def media_failed(self, failure, request, info):\n+        mtype = self.MEDIA_TYPE\n         referer = request.headers.get('Referer')\n         errmsg = str(failure.value) if isinstance(failure.value, HttpException) else str(failure)\n+\n+        if '304 Not Modified' in errmsg:\n+            msg = 'Image (notmodified): Downloaded %s from %s referred in <%s>' % (mtype, request, referer)\n+            log.msg(msg, level=log.DEBUG, domain=info.domain)\n+            return self._get_paths(request)[0]\n+\n         msg = 'Image (http-error): Error downloading %s from %s referred in <%s>: %s' % (self.MEDIA_TYPE, request, referer, errmsg)\n         log.msg(msg, level=log.WARNING, domain=info.domain)\n         raise ImageException(msg)\n@@ -118,16 +124,17 @@ class ImagesPipeline(MediaPipeline):\n \n \n \n-def should_download(path):\n-    \"\"\"Should the image downloader download the image to the location specified\n+def image_expired(path):\n+    \"\"\"\n+    Is the image older than the expiration time?\n     \"\"\"\n     try:\n         mtime = os.path.getmtime(path)\n         age_seconds = time.time() - mtime\n         age_days = age_seconds / 60 / 60 / 24\n-        return age_days > IMAGE_EXPIRES\n+        return (age_days > IMAGE_EXPIRES, mtime)\n     except:\n-        return True\n+        return (True, 0)\n \n _MULTIPLE_SLASHES_REGEXP = re.compile(r\"\\/{2,}\")\n _FINAL_SLASH_REGEXP = re.compile(r\"\\/$\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#56e3d5ae66d9d81094f099fb89acfd3795df6535", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 29 | Churn Cumulative: 253 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,7 +4,6 @@ import time\n import hashlib\n import urllib\n import urlparse\n-from datetime import datetime\n from cStringIO import StringIO\n \n import Image\n@@ -18,7 +17,7 @@ from scrapy.conf import settings\n from scrapy.contrib.pipeline.media import MediaPipeline\n \n # the age at which we download images again\n-IMAGE_EXPIRES = settings.getint('IMAGES_EXPIRES', 15)\n+IMAGE_EXPIRES = settings.getint('IMAGES_EXPIRES', 90)\n \n class NoimagesDrop(DropItem):\n     pass\n@@ -51,10 +50,12 @@ class ImagesPipeline(MediaPipeline):\n \n     def media_to_download(self, request, info):\n         relative, absolute = self._get_paths(request)\n-        expired, mtime = image_expired(absolute)\n-        if not expired:\n-            mod_date = datetime.utcfromtimestamp(mtime)\n-            request.headers['If-Modified-Since'] = mod_date.strftime('%a, %d %b %Y %H:%M:%S GMT')\n+        if not should_download(absolute):\n+            self.inc_stats(info.domain, 'uptodate')\n+            referer = request.headers.get('Referer')\n+            log.msg('Image (uptodate): Downloaded %s from %s referred in <%s>' % \\\n+                    (self.MEDIA_TYPE, request, referer), level=log.DEBUG, domain=info.domain)\n+            return relative\n \n     def media_downloaded(self, response, request, info):\n         mtype = self.MEDIA_TYPE\n@@ -74,15 +75,8 @@ class ImagesPipeline(MediaPipeline):\n         return result\n \n     def media_failed(self, failure, request, info):\n-        mtype = self.MEDIA_TYPE\n         referer = request.headers.get('Referer')\n         errmsg = str(failure.value) if isinstance(failure.value, HttpException) else str(failure)\n-\n-        if '304 Not Modified' in errmsg:\n-            msg = 'Image (notmodified): Downloaded %s from %s referred in <%s>' % (mtype, request, referer)\n-            log.msg(msg, level=log.DEBUG, domain=info.domain)\n-            return self._get_paths(request)[0]\n-\n         msg = 'Image (http-error): Error downloading %s from %s referred in <%s>: %s' % (self.MEDIA_TYPE, request, referer, errmsg)\n         log.msg(msg, level=log.WARNING, domain=info.domain)\n         raise ImageException(msg)\n@@ -124,17 +118,16 @@ class ImagesPipeline(MediaPipeline):\n \n \n \n-def image_expired(path):\n-    \"\"\"\n-    Is the image older than the expiration time?\n+def should_download(path):\n+    \"\"\"Should the image downloader download the image to the location specified\n     \"\"\"\n     try:\n         mtime = os.path.getmtime(path)\n         age_seconds = time.time() - mtime\n         age_days = age_seconds / 60 / 60 / 24\n-        return (age_days > IMAGE_EXPIRES, mtime)\n+        return age_days > IMAGE_EXPIRES\n     except:\n-        return (True, 0)\n+        return True\n \n _MULTIPLE_SLASHES_REGEXP = re.compile(r\"\\/{2,}\")\n _FINAL_SLASH_REGEXP = re.compile(r\"\\/$\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#90fe6ef1b80809c94ed68358a74ae154cb16e82e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 344 | Contributors (this commit): 2 | Commits (past 90d): 16 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,7 +14,7 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\", help=\"don't show extracted links\")\n+        parser.add_option(\"--links\", dest=\"links\", action=\"store_true\", help=\"show extracted links\")\n         parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", help=\"don't show scraped items\")\n         parser.add_option(\"--identify\", dest=\"identify\", action=\"store_true\", help=\"try to use identify instead of parse\")\n         parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", help=\"avoid using pygments to colorize the output\")\n@@ -46,7 +46,7 @@ class Command(ScrapyCommand):\n                 print \"# Scraped Items\", \"-\"*60\n                 display.pprint(items)\n \n-            if not opts.nolinks:\n+            if opts.links:\n                 print \"# Links\", \"-\"*68\n                 display.pprint(links)\n \n\n@@ -68,6 +68,7 @@ class CrawlSpider(BaseSpider):\n         ret = []\n         for name in extractor_names:\n             extractor = getattr(self, name)\n+            ret.extend(self._links_to_follow(response))\n             callback_name = 'parse_%s' % name[6:]\n             if hasattr(self, callback_name):\n                 if extractor.match(response.url):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#99c4c06380d1ef6e99a1fc56e915f42cab9217e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 16 | Churn Cumulative: 356 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 4 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -18,6 +18,7 @@ class Command(ScrapyCommand):\n         parser.add_option(\"--restrict\", dest=\"restrict\", action=\"store_true\", help=\"restrict crawling only to the given urls\")\n         parser.add_option(\"--record\", dest=\"record\", help=\"use FILE for recording session (see replay command)\", metavar=\"FILE\")\n         parser.add_option(\"--record-dir\", dest=\"recorddir\", help=\"use DIR for recording (instead of file)\", metavar=\"DIR\")\n+        parser.add_option(\"-n\", \"--nofollow\", dest=\"nofollow\", action=\"store_true\", help=\"don't follow links (for use with URLs only)\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -30,6 +31,9 @@ class Command(ScrapyCommand):\n         if opts.restrict:\n             settings.overrides['RESTRICT_TO_URLS'] = args\n \n+        if opts.nofollow:\n+            settings.overrides['FOLLOW_LINKS'] = False\n+\n         if opts.record or opts.recorddir:\n             # self.replay is used for preventing Replay signals handler from\n             # disconnecting since pydispatcher uses weak references \n\n@@ -3,6 +3,7 @@ This module contains some basic spiders for scraping websites (CrawlSpider)\n and XML feeds (XMLFeedSpider).\n \"\"\"\n \n+from scrapy.conf import settings\n from scrapy.http import Request, Response, ResponseBody\n from scrapy.spider import BaseSpider\n from scrapy.item import ScrapedItem\n@@ -15,6 +16,7 @@ class CrawlSpider(BaseSpider):\n     \"\"\"\n     This class works as a base class for spiders that crawl over websites\n     \"\"\"\n+\n     def __init__(self):\n         super(BaseSpider, self).__init__()\n         \n@@ -29,7 +31,10 @@ class CrawlSpider(BaseSpider):\n     def parse(self, response):\n         \"\"\"This function is called by the core for all the start_urls. Do not\n         override this function, override parse_start_url instead.\"\"\"\n+        if response.url in self.start_urls:\n             return self._parse_wrapper(response, self.parse_start_url)\n+        else:\n+            return self.parse_url(response)\n \n     def parse_start_url(self, response):\n         \"\"\"Callback function for processing start_urls. It must return a list\n@@ -52,8 +57,10 @@ class CrawlSpider(BaseSpider):\n         return res\n \n     def _parse_wrapper(self, response, callback):\n-        res = self._links_to_follow(response)\n-        res += callback(response) if callback else ()\n+        res = []\n+        if settings.getbool('FOLLOW_LINKS', True):\n+            res.extend(self._links_to_follow(response))\n+        res.extend(callback(response) if callback else ())\n         for entry in res:\n             if isinstance(entry, ScrapedItem):\n                self.set_guid(entry)\n@@ -68,6 +75,7 @@ class CrawlSpider(BaseSpider):\n         ret = []\n         for name in extractor_names:\n             extractor = getattr(self, name)\n+            if settings.getbool('FOLLOW_LINKS', True):\n                 ret.extend(self._links_to_follow(response))\n             callback_name = 'parse_%s' % name[6:]\n             if hasattr(self, callback_name):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#77a917df6ba9a5490f5a3bff8fb048c9ef2d2148", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 362 | Contributors (this commit): 2 | Commits (past 90d): 17 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -32,7 +32,7 @@ class Command(ScrapyCommand):\n             settings.overrides['RESTRICT_TO_URLS'] = args\n \n         if opts.nofollow:\n-            settings.overrides['FOLLOW_LINKS'] = False\n+            settings.overrides['CRAWLSPIDER_FOLLOW_LINKS'] = False\n \n         if opts.record or opts.recorddir:\n             # self.replay is used for preventing Replay signals handler from\n\n@@ -58,7 +58,7 @@ class CrawlSpider(BaseSpider):\n \n     def _parse_wrapper(self, response, callback):\n         res = []\n-        if settings.getbool('FOLLOW_LINKS', True):\n+        if settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True):\n             res.extend(self._links_to_follow(response))\n         res.extend(callback(response) if callback else ())\n         for entry in res:\n@@ -75,7 +75,7 @@ class CrawlSpider(BaseSpider):\n         ret = []\n         for name in extractor_names:\n             extractor = getattr(self, name)\n-            if settings.getbool('FOLLOW_LINKS', True):\n+            if settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True):\n                 ret.extend(self._links_to_follow(response))\n             callback_name = 'parse_%s' % name[6:]\n             if hasattr(self, callback_name):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#17d45f5216acbf1ba3d5903c3d19ff5a030fd82d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 67 | Churn Cumulative: 367 | Contributors (this commit): 2 | Commits (past 90d): 16 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,22 +3,36 @@ This module contains some basic spiders for scraping websites (CrawlSpider)\n and XML feeds (XMLFeedSpider).\n \"\"\"\n \n+from scrapy import log\n from scrapy.conf import settings\n-from scrapy.http import Request, Response, ResponseBody\n+from scrapy.http import Request\n from scrapy.spider import BaseSpider\n from scrapy.item import ScrapedItem\n from scrapy.xpath.selector import XmlXPathSelector\n-from scrapy.core.exceptions import UsageError\n+from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.iterators import xmliter, csviter\n-from scrapy.utils.misc import hash_values\n+\n+\n+def _set_guid(spider, item):\n+    \"\"\"\n+    This method is called whenever the spider returns items, for each item.\n+    It should set the 'guid' attribute to the given item with a string that\n+    identifies the item uniquely.\n+    \"\"\"\n+    raise NotConfigured('You must define a set_guid method in order to scrape items.')\n+\n+def _log(spider, message, level=log.DEBUG):\n+    log.msg(message, domain=spider.domain_name, level=level)\n \n class CrawlSpider(BaseSpider):\n     \"\"\"\n     This class works as a base class for spiders that crawl over websites\n     \"\"\"\n+    log = _log\n+    set_guid = _set_guid\n \n     def __init__(self):\n-        super(BaseSpider, self).__init__()\n+        super(CrawlSpider, self).__init__()\n         \n         self._links_callback = []\n         for attr in dir(self):\n@@ -86,14 +100,6 @@ class CrawlSpider(BaseSpider):\n                 self.set_guid(entry)\n         return ret\n \n-    def set_guid(self, item):\n-        \"\"\"\n-        This method is called whenever the spider returns items, for each item.\n-        It should set the 'guid' attribute to the given item with a string that\n-        identifies the item uniquely.\n-        \"\"\"\n-        raise NotConfigured('You must define set_guid method in order to scrape items.')\n-\n class XMLFeedSpider(BaseSpider):\n     \"\"\"\n     This class intends to be the base class for spiders that scrape\n@@ -102,6 +108,8 @@ class XMLFeedSpider(BaseSpider):\n     You can choose whether to parse the file using the iternodes tool,\n     or not using it (which just splits the tags using xpath)\n     \"\"\"\n+    log = _log\n+    set_guid = _set_guid\n     iternodes = True\n     itertag = 'item'\n \n@@ -113,7 +121,7 @@ class XMLFeedSpider(BaseSpider):\n \n     def parse(self, response):\n         if not hasattr(self, 'parse_item'):\n-            raise NotConfigured('You must define parse_item method in order to scrape this feed')\n+            raise NotConfigured('You must define parse_item method in order to scrape this XML feed')\n \n         if self.iternodes:\n             nodes = xmliter(response, self.itertag)\n@@ -122,3 +130,36 @@ class XMLFeedSpider(BaseSpider):\n \n         return (self.parse_item_wrapper(response, xSel) for xSel in nodes)\n \n+class CSVFeedSpider(BaseSpider):\n+    \"\"\"\n+    Spider for parsing CSV feeds.\n+    It receives a CSV file in a response; iterates through each of its rows,\n+    and calls parse_row with a dict containing each field's data.\n+\n+    You can set some options regarding the CSV file, such as the delimiter\n+    and the file's headers.\n+    \"\"\"\n+    log = _log\n+    set_guid = _set_guid\n+    delimiter = None # When this is None, python's csv module's default delimiter is used\n+    headers = None\n+\n+    # You can override this function in order to make any changes you want to\n+    # into the feed before parsing it.\n+    # This function may return either a response or a string.\n+    def adapt_feed(self, response):\n+        return response\n+\n+    def parse_row_wrapper(self, response, row):\n+        ret = self.parse_row(response, row)\n+        if isinstance(ret, ScrapedItem):\n+            self.set_guid(ret)\n+        return ret\n+\n+    def parse(self, response):\n+        if not hasattr(self, 'parse_row'):\n+            raise NotConfigured('You must define parse_row method in order to scrape this CSV feed')\n+\n+        feed = self.adapt_feed(response)\n+        return (self.parse_row_wrapper(feed, row) for row in csviter(response, self.delimiter, self.headers))\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#836c532ef1d5a075aca5a067db22a6a925eb8ca9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 74 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,7 @@ from scrapy.http import Request\n from scrapy.item import ScrapedItem\n from scrapy.spider import spiders\n from scrapy.utils import display\n+from scrapy import log\n \n class Command(ScrapyCommand):\n     def syntax(self):\n@@ -27,14 +28,12 @@ class Command(ScrapyCommand):\n             print \"A URL is required\"\n             return\n \n-        responses = fetch([args[0]])\n-        if responses:\n-            response = responses[0]\n+        responses = fetch(args)\n+        for response in responses:\n             spider = spiders.fromurl(response.url)\n-            if opts.identify:\n+            if spider:\n+                if opts.identify and hasattr(spider, 'identify_products'):\n                     result = spider.identify_products(response)\n-            elif hasattr(spider, 'parse_url'):\n-                result = spider.parse_url(response)\n                 else:\n                     result = spider.parse(response)\n \n@@ -49,4 +48,5 @@ class Command(ScrapyCommand):\n                 if opts.links:\n                     print \"# Links\", \"-\"*68\n                     display.pprint(links)\n-\n+            else:\n+                log.msg('cannot find spider for url: %s' % response.url, level=log.ERROR)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c15a24c25ef4c696bb75dd88c4bc3c265b21ceda", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 143 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,6 +22,7 @@ class Command(ScrapyCommand):\n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_option(\"--template\", dest=\"template\", help=\"Uses a custom template.\", default=\"crawl\")\n+        parser.add_option(\"--force\", dest=\"force\", help=\"If the spider already exists, overwrite it with the template\", action=\"store_true\")\n \n     def run(self, args, opts):\n         if len(args) < 2:\n@@ -32,14 +33,16 @@ class Command(ScrapyCommand):\n             print \"Template named %s.tmpl does not exist\" % opts.template\n             return\n \n-        name = args[0]\n+        name = self.normalize_name(args[0])\n         site = args[1]\n         spiders_dict = spiders.asdict()\n-        if not site in spiders_dict.keys():\n-            name = self.normalize_name(name)\n-            self._genspider(name, site, template_file)\n+        if site in spiders_dict.keys():\n+            if opts.force:\n+                print \"Spider '%s' already exists. Overwriting it...\" % name\n             else:\n                 print \"Spider '%s' already exists\" % name\n+                return\n+        self._genspider(name, site, template_file)\n \n     def normalize_name(self, name):\n         name = name.replace('-', '_') # - are replaced by _, for valid python modules\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1458686cbf67713f3fe7b285185c56922a9d5fe4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 24 | Files Changed: 1 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 65 | Churn Cumulative: 558 | Contributors (this commit): 2 | Commits (past 90d): 19 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,7 +33,8 @@ class MediaPipeline(object):\n                 'get_media_requests should return None or iterable'\n \n         def _bugtrap(_failure, request):\n-            log.msg('Unhandled ERROR in MediaPipeline.item_media_{downloaded,failed} for %s: %s' % (request, _failure), log.ERROR, domain=domain)\n+            log.msg('Unhandled ERROR in MediaPipeline.item_media_{downloaded,failed} for %s: %s' \\\n+                    % (request, _failure), log.ERROR, domain=domain)\n \n         lst = []\n         for request in requests or ():\n@@ -53,46 +54,62 @@ class MediaPipeline(object):\n \n     def _enqueue(self, request, info):\n         wad = request.deferred or defer.Deferred()\n-\n         fp = request.fingerprint()\n+\n+        # if already downloaded, return cached result.\n         if fp in info.downloaded:\n             cached = info.downloaded[fp]\n             defer_result(cached).chainDeferred(wad)\n-        else:\n-            info.waiting.setdefault(fp, []).append(wad)\n-            if fp not in info.downloading:\n-                self._download(request, info, fp)\n+            return wad # break\n \n-        return wad\n+        # add to pending list for this request, and wait for result like the others.\n+        info.waiting.setdefault(fp, []).append(wad)\n+\n+        # if request is already downloading, just wait.\n+        if fp in info.downloading:\n+            return wad # break\n+\n+        # if not, this is the first time for request, try to download it.\n+        dfd = self._download(request, info, fp)\n+        dfd.addCallback(lambda _: wad)\n+        return dfd\n \n     def _download(self, request, info, fp):\n         def _bugtrap(_failure):\n-            log.msg('Unhandled ERROR in MediaPipeline._downloaded: %s' % (_failure), log.ERROR, domain=info.domain)\n+            log.msg('Unhandled ERROR in MediaPipeline._downloaded: %s' \\\n+                    % (_failure), log.ERROR, domain=info.domain)\n \n-        result = self.media_to_download(request, info)\n-        if result is not None:\n-            dwld = defer_result(result)\n-        else:\n+        def _downloaded(result):\n+            info.downloaded[fp] = result # cache result\n+            waiting = info.waiting[fp] # client list\n+            del info.waiting[fp]\n+            del info.downloading[fp]\n+            for wad in waiting: # call each waiting client with result\n+                defer_result(result).chainDeferred(wad)\n+\n+        def _evaluated(result):\n+            if result is None: # continue with download\n                 dwld = mustbe_deferred(self.download, request, info)\n                 dwld.addCallbacks(\n                         callback=self.media_downloaded,\n                         callbackArgs=(request, info),\n                         errback=self.media_failed,\n-                    errbackArgs=(request, info),\n-                    )\n+                        errbackArgs=(request, info))\n+            else: # or use media_to_download return value as result\n+                dwld = defer_result(result)\n \n-        dwld.addBoth(self._downloaded, info, fp)\n-        dwld.addErrback(_bugtrap)\n-        info.downloading[fp] = (request, dwld)\n+            info.downloading[fp] = (request, dwld) # fill downloading state data\n+            dwld.addBoth(_downloaded) # append post-download hook\n+            dwld.addErrback(_bugtrap) # catch media_downloaded and media_failed unhandled errors\n+            # WARNING: dont be tempted to return 'dwld' here.\n \n-    def _downloaded(self, result, info, fp):\n-        info.downloaded[fp] = result # cache result\n-        waiting = info.waiting[fp] # client list\n-        del info.waiting[fp]\n-        del info.downloading[fp]\n-        for wad in waiting:\n-            defer_result(result).chainDeferred(wad)\n+        # declare request in downloading state (None is used as place holder)\n+        info.downloading[fp] = None\n \n+        # defer pre-download request processing\n+        dfd = mustbe_deferred(self.media_to_download, request, info)\n+        dfd.addCallback(_evaluated)\n+        return dfd\n \n     ### Overradiable Interface\n     def download(self, request, info):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d4040d73d914c044054eb2ac9b79806f44c6dd36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 88 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -30,6 +30,18 @@ def xmliter(obj, nodename):\n         yield XmlXPathSelector(text=nodetext).x('/' + nodename)[0]\n \n def csviter(obj, delimiter=None, headers=None):\n+    \"\"\" Returns an iterator of dictionaries from the given csv object\n+\n+    obj can be:\n+    - a Response object\n+    - a unicode string\n+    - a string encoded as utf-8\n+\n+    delimiter is the character used to separate field on the given obj.\n+    \n+    headers is an iterable that when provided offers the keys\n+    for the returned dictionaries, if not the first row is used.\n+    \"\"\"\n     def _getrow(csv_r):\n         return [field.decode() for field in csv_r.next()]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5ccd4df0cd75de652632841776238b088446271b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 21 | Churn Cumulative: 489 | Contributors (this commit): 2 | Commits (past 90d): 19 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,6 @@ This module contains some basic spiders for scraping websites (CrawlSpider)\n and XML feeds (XMLFeedSpider).\n \"\"\"\n \n-from scrapy import log\n from scrapy.conf import settings\n from scrapy.http import Request\n from scrapy.spider import BaseSpider\n@@ -21,14 +20,10 @@ def _set_guid(spider, item):\n     \"\"\"\n     raise NotConfigured('You must define a set_guid method in order to scrape items.')\n \n-def _log(spider, message, level=log.DEBUG):\n-    log.msg(message, domain=spider.domain_name, level=level)\n-\n class CrawlSpider(BaseSpider):\n     \"\"\"\n     This class works as a base class for spiders that crawl over websites\n     \"\"\"\n-    log = _log\n     set_guid = _set_guid\n \n     def __init__(self):\n@@ -108,7 +103,6 @@ class XMLFeedSpider(BaseSpider):\n     You can choose whether to parse the file using the iternodes tool,\n     or not using it (which just splits the tags using xpath)\n     \"\"\"\n-    log = _log\n     set_guid = _set_guid\n     iternodes = True\n     itertag = 'item'\n@@ -139,15 +133,15 @@ class CSVFeedSpider(BaseSpider):\n     You can set some options regarding the CSV file, such as the delimiter\n     and the file's headers.\n     \"\"\"\n-    log = _log\n     set_guid = _set_guid\n     delimiter = None # When this is None, python's csv module's default delimiter is used\n     headers = None\n \n-    # You can override this function in order to make any changes you want to\n-    # into the feed before parsing it.\n-    # This function may return either a response or a string.\n     def adapt_feed(self, response):\n+        \"\"\"You can override this function in order to make any changes you want\n+        to into the feed before parsing it. This function may return either a\n+        response or a string.  \"\"\"\n+\n         return response\n \n     def parse_row_wrapper(self, response, row):\n\n@@ -4,6 +4,7 @@ Base class for scrapy spiders\n from zope.interface import Interface, Attribute, invariant, implements\n from twisted.plugin import IPlugin\n \n+from scrapy import log\n from scrapy.core.exceptions import UsageError\n \n def _valid_start_urls(obj):\n@@ -87,3 +88,9 @@ class BaseSpider(object):\n     implements(ISpider)\n     domain_name = None\n     extra_domain_names = []\n+\n+    def log(self, message, level=log.DEBUG):\n+        \"\"\"Log the given messages at the given log level. Always use this\n+        method to send log messages from your spider\n+        \"\"\"\n+        log.msg(message, domain=self.domain_name, level=level)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
