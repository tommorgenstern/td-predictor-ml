{"custom_id": "scrapy#5bd27191a2fd9ec50936c05a377f76bd14aa8266", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 15 | Files Changed: 6 | Hunks: 15 | Methods Changed: 12 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 47 | Churn Cumulative: 4940 | Contributors (this commit): 55 | Commits (past 90d): 15 | Contributors (cumulative): 116 | DMM Complexity: 0.6567164179104478\n\nDIFF:\n@@ -23,7 +23,7 @@ class ScrapyCommand:\n \n     exitcode = 0\n \n-    def __init__(self):\n+    def __init__(self) -> None:\n         self.settings: Any = None  # set in scrapy.cmdline\n \n     def set_crawler(self, crawler):\n\n@@ -1,8 +1,22 @@\n \"\"\"This module implements the Scraper component which parses responses and\n extracts information from them\"\"\"\n+from __future__ import annotations\n+\n import logging\n from collections import deque\n-from typing import Any, AsyncGenerator, AsyncIterable, Deque, Generator, Iterable, Optional, Set, Tuple, Union\n+from typing import (\n+    Any,\n+    AsyncGenerator,\n+    AsyncIterable,\n+    Deque,\n+    Generator,\n+    Iterable,\n+    Optional,\n+    Set,\n+    TYPE_CHECKING,\n+    Tuple,\n+    Union,\n+)\n \n from itemadapter import is_item\n from twisted.internet.defer import Deferred, inlineCallbacks\n@@ -26,6 +40,10 @@ from scrapy.utils.misc import load_object, warn_on_generator_with_return_value\n from scrapy.utils.spider import iterate_spider_output\n \n \n+if TYPE_CHECKING:\n+    from scrapy.crawler import Crawler\n+\n+\n QueueTuple = Tuple[Union[Response, Failure], Request, Deferred]\n \n \n@@ -75,7 +93,7 @@ class Slot:\n \n class Scraper:\n \n-    def __init__(self, crawler):\n+    def __init__(self, crawler: Crawler) -> None:\n         self.slot: Optional[Slot] = None\n         self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n         itemproc_cls = load_object(crawler.settings['ITEM_PROCESSOR'])\n@@ -174,6 +192,7 @@ class Scraper:\n     def handle_spider_error(self, _failure: Failure, request: Request, response: Response, spider: Spider) -> None:\n         exc = _failure.value\n         if isinstance(exc, CloseSpider):\n+            assert self.crawler.engine is not None  # typing\n             self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n             return\n         logkws = self.logformatter.spider_error(_failure, request, response, spider)\n@@ -214,6 +233,7 @@ class Scraper:\n         \"\"\"\n         assert self.slot is not None  # typing\n         if isinstance(output, Request):\n+            assert self.crawler.engine is not None  # typing\n             self.crawler.engine.crawl(request=output)\n         elif is_item(output):\n             self.slot.itemproc_size += 1\n\n@@ -4,7 +4,7 @@ import logging\n import pprint\n import signal\n import warnings\n-from typing import TYPE_CHECKING\n+from typing import TYPE_CHECKING, Optional\n \n from twisted.internet import defer\n from zope.interface.exceptions import DoesNotImplement\n@@ -109,7 +109,7 @@ class Crawler:\n         self.settings.freeze()\n         self.crawling = False\n         self.spider = None\n-        self.engine = None\n+        self.engine: Optional[ExecutionEngine] = None\n \n     @defer.inlineCallbacks\n     def crawl(self, *args, **kwargs):\n\n@@ -140,8 +140,7 @@ class Response(object_ref):\n \n     def follow(self, url, callback=None, method='GET', headers=None, body=None,\n                cookies=None, meta=None, encoding='utf-8', priority=0,\n-               dont_filter=False, errback=None, cb_kwargs=None, flags=None):\n-        # type: (...) -> Request\n+               dont_filter=False, errback=None, cb_kwargs=None, flags=None) -> Request:\n         \"\"\"\n         Return a :class:`~.Request` instance to follow a link ``url``.\n         It accepts the same arguments as ``Request.__init__`` method,\n@@ -179,8 +178,8 @@ class Response(object_ref):\n \n     def follow_all(self, urls, callback=None, method='GET', headers=None, body=None,\n                    cookies=None, meta=None, encoding='utf-8', priority=0,\n-                   dont_filter=False, errback=None, cb_kwargs=None, flags=None):\n-        # type: (...) -> Generator[Request, None, None]\n+                   dont_filter=False, errback=None, cb_kwargs=None,\n+                   flags=None) -> Generator[Request, None, None]:\n         \"\"\"\n         .. versionadded:: 2.0\n \n\n@@ -142,8 +142,7 @@ class TextResponse(Response):\n \n     def follow(self, url, callback=None, method='GET', headers=None, body=None,\n                cookies=None, meta=None, encoding=None, priority=0,\n-               dont_filter=False, errback=None, cb_kwargs=None, flags=None):\n-        # type: (...) -> Request\n+               dont_filter=False, errback=None, cb_kwargs=None, flags=None) -> Request:\n         \"\"\"\n         Return a :class:`~.Request` instance to follow a link ``url``.\n         It accepts the same arguments as ``Request.__init__`` method,\n@@ -184,8 +183,7 @@ class TextResponse(Response):\n     def follow_all(self, urls=None, callback=None, method='GET', headers=None, body=None,\n                    cookies=None, meta=None, encoding=None, priority=0,\n                    dont_filter=False, errback=None, cb_kwargs=None, flags=None,\n-                   css=None, xpath=None):\n-        # type: (...) -> Generator[Request, None, None]\n+                   css=None, xpath=None) -> Generator[Request, None, None]:\n         \"\"\"\n         A generator that produces :class:`~.Request` instances to follow all\n         links in ``urls``. It accepts the same arguments as the :class:`~.Request`'s\n\n@@ -1,7 +1,7 @@\n import logging\n import pprint\n from collections import defaultdict, deque\n-from typing import Callable, Deque, Dict, Iterable, Tuple, Union, cast\n+from typing import Any, Callable, Deque, Dict, Iterable, Tuple, Union, cast\n \n from twisted.internet.defer import Deferred\n \n@@ -19,7 +19,7 @@ class MiddlewareManager:\n \n     component_name = 'foo middleware'\n \n-    def __init__(self, *middlewares):\n+    def __init__(self, *middlewares: Any) -> None:\n         self.middlewares = middlewares\n         # Only process_spider_output and process_spider_exception can be None.\n         # Only process_spider_output can be a tuple, and only until _async compatibility methods are removed.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e211ec0aa26ecae0da8ae55d064ea60e1efe4d0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16063 | Lines Deleted: 11497 | Files Changed: 307 | Hunks: 6439 | Methods Changed: 2604 | Complexity Δ (Sum/Max): -7/8 | Churn Δ: 27560 | Churn Cumulative: 130452 | Contributors (this commit): 317 | Commits (past 90d): 573 | Contributors (cumulative): 3204 | DMM Complexity: 0.9116837628221166\n\nDIFF:\n@@ -9,7 +9,7 @@ from tests.keys import generate_keys\n \n \n def _py_files(folder):\n-    return (str(p) for p in Path(folder).rglob('*.py'))\n+    return (str(p) for p in Path(folder).rglob(\"*.py\"))\n \n \n collect_ignore = [\n@@ -21,16 +21,16 @@ collect_ignore = [\n     *_py_files(\"tests/CrawlerRunner\"),\n ]\n \n-with Path('tests/ignores.txt').open(encoding=\"utf-8\") as reader:\n+with Path(\"tests/ignores.txt\").open(encoding=\"utf-8\") as reader:\n     for line in reader:\n         file_path = line.strip()\n-        if file_path and file_path[0] != '#':\n+        if file_path and file_path[0] != \"#\":\n             collect_ignore.append(file_path)\n \n if not H2_ENABLED:\n     collect_ignore.extend(\n         (\n-            'scrapy/core/downloader/handlers/http2.py',\n+            \"scrapy/core/downloader/handlers/http2.py\",\n             *_py_files(\"scrapy/core/http2\"),\n         )\n     )\n@@ -50,7 +50,7 @@ def pytest_addoption(parser):\n     )\n \n \n-@pytest.fixture(scope='class')\n+@pytest.fixture(scope=\"class\")\n def reactor_pytest(request):\n     if not request.cls:\n         # doctests\n@@ -61,14 +61,17 @@ def reactor_pytest(request):\n \n @pytest.fixture(autouse=True)\n def only_asyncio(request, reactor_pytest):\n-    if request.node.get_closest_marker('only_asyncio') and reactor_pytest != 'asyncio':\n-        pytest.skip('This test is only run with --reactor=asyncio')\n+    if request.node.get_closest_marker(\"only_asyncio\") and reactor_pytest != \"asyncio\":\n+        pytest.skip(\"This test is only run with --reactor=asyncio\")\n \n \n @pytest.fixture(autouse=True)\n def only_not_asyncio(request, reactor_pytest):\n-    if request.node.get_closest_marker('only_not_asyncio') and reactor_pytest == 'asyncio':\n-        pytest.skip('This test is only run without --reactor=asyncio')\n+    if (\n+        request.node.get_closest_marker(\"only_not_asyncio\")\n+        and reactor_pytest == \"asyncio\"\n+    ):\n+        pytest.skip(\"This test is only run without --reactor=asyncio\")\n \n \n def pytest_configure(config):\n\n@@ -11,15 +11,15 @@ class settingslist_node(nodes.General, nodes.Element):\n \n class SettingsListDirective(Directive):\n     def run(self):\n-        return [settingslist_node('')]\n+        return [settingslist_node(\"\")]\n \n \n def is_setting_index(node):\n-    if node.tagname == 'index' and node['entries']:\n+    if node.tagname == \"index\" and node[\"entries\"]:\n         # index entries for setting directives look like:\n         # [('pair', 'SETTING_NAME; setting', 'std:setting-SETTING_NAME', '')]\n-        entry_type, info, refid = node['entries'][0][:3]\n-        return entry_type == 'pair' and info.endswith('; setting')\n+        entry_type, info, refid = node[\"entries\"][0][:3]\n+        return entry_type == \"pair\" and info.endswith(\"; setting\")\n     return False\n \n \n@@ -30,14 +30,14 @@ def get_setting_target(node):\n \n def get_setting_name_and_refid(node):\n     \"\"\"Extract setting name from directive index node\"\"\"\n-    entry_type, info, refid = node['entries'][0][:3]\n-    return info.replace('; setting', ''), refid\n+    entry_type, info, refid = node[\"entries\"][0][:3]\n+    return info.replace(\"; setting\", \"\"), refid\n \n \n def collect_scrapy_settings_refs(app, doctree):\n     env = app.builder.env\n \n-    if not hasattr(env, 'scrapy_all_settings'):\n+    if not hasattr(env, \"scrapy_all_settings\"):\n         env.scrapy_all_settings = []\n \n     for node in doctree.traverse(is_setting_index):\n@@ -46,18 +46,23 @@ def collect_scrapy_settings_refs(app, doctree):\n \n         setting_name, refid = get_setting_name_and_refid(node)\n \n-        env.scrapy_all_settings.append({\n-            'docname': env.docname,\n-            'setting_name': setting_name,\n-            'refid': refid,\n-        })\n+        env.scrapy_all_settings.append(\n+            {\n+                \"docname\": env.docname,\n+                \"setting_name\": setting_name,\n+                \"refid\": refid,\n+            }\n+        )\n \n \n def make_setting_element(setting_data, app, fromdocname):\n-    refnode = make_refnode(app.builder, fromdocname,\n-                           todocname=setting_data['docname'],\n-                           targetid=setting_data['refid'],\n-                           child=nodes.Text(setting_data['setting_name']))\n+    refnode = make_refnode(\n+        app.builder,\n+        fromdocname,\n+        todocname=setting_data[\"docname\"],\n+        targetid=setting_data[\"refid\"],\n+        child=nodes.Text(setting_data[\"setting_name\"]),\n+    )\n     p = nodes.paragraph()\n     p += refnode\n \n@@ -71,10 +76,13 @@ def replace_settingslist_nodes(app, doctree, fromdocname):\n \n     for node in doctree.traverse(settingslist_node):\n         settings_list = nodes.bullet_list()\n-        settings_list.extend([make_setting_element(d, app, fromdocname)\n-                              for d in sorted(env.scrapy_all_settings,\n-                                              key=itemgetter('setting_name'))\n-                              if fromdocname != d['docname']])\n+        settings_list.extend(\n+            [\n+                make_setting_element(d, app, fromdocname)\n+                for d in sorted(env.scrapy_all_settings, key=itemgetter(\"setting_name\"))\n+                if fromdocname != d[\"docname\"]\n+            ]\n+        )\n         node.replace_self(settings_list)\n \n \n@@ -99,41 +107,41 @@ def setup(app):\n         rolename=\"reqmeta\",\n         indextemplate=\"pair: %s; reqmeta\",\n     )\n-    app.add_role('source', source_role)\n-    app.add_role('commit', commit_role)\n-    app.add_role('issue', issue_role)\n-    app.add_role('rev', rev_role)\n+    app.add_role(\"source\", source_role)\n+    app.add_role(\"commit\", commit_role)\n+    app.add_role(\"issue\", issue_role)\n+    app.add_role(\"rev\", rev_role)\n \n     app.add_node(settingslist_node)\n-    app.add_directive('settingslist', SettingsListDirective)\n+    app.add_directive(\"settingslist\", SettingsListDirective)\n \n-    app.connect('doctree-read', collect_scrapy_settings_refs)\n-    app.connect('doctree-resolved', replace_settingslist_nodes)\n+    app.connect(\"doctree-read\", collect_scrapy_settings_refs)\n+    app.connect(\"doctree-resolved\", replace_settingslist_nodes)\n \n \n def source_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n-    ref = 'https://github.com/scrapy/scrapy/blob/master/' + text\n+    ref = \"https://github.com/scrapy/scrapy/blob/master/\" + text\n     set_classes(options)\n     node = nodes.reference(rawtext, text, refuri=ref, **options)\n     return [node], []\n \n \n def issue_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n-    ref = 'https://github.com/scrapy/scrapy/issues/' + text\n+    ref = \"https://github.com/scrapy/scrapy/issues/\" + text\n     set_classes(options)\n-    node = nodes.reference(rawtext, 'issue ' + text, refuri=ref, **options)\n+    node = nodes.reference(rawtext, \"issue \" + text, refuri=ref, **options)\n     return [node], []\n \n \n def commit_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n-    ref = 'https://github.com/scrapy/scrapy/commit/' + text\n+    ref = \"https://github.com/scrapy/scrapy/commit/\" + text\n     set_classes(options)\n-    node = nodes.reference(rawtext, 'commit ' + text, refuri=ref, **options)\n+    node = nodes.reference(rawtext, \"commit \" + text, refuri=ref, **options)\n     return [node], []\n \n \n def rev_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n-    ref = 'http://hg.scrapy.org/scrapy/changeset/' + text\n+    ref = \"http://hg.scrapy.org/scrapy/changeset/\" + text\n     set_classes(options)\n-    node = nodes.reference(rawtext, 'r' + text, refuri=ref, **options)\n+    node = nodes.reference(rawtext, \"r\" + text, refuri=ref, **options)\n     return [node], []\n\n@@ -25,30 +25,30 @@ sys.path.insert(0, str(Path(__file__).parent.parent))\n # Add any Sphinx extension module names here, as strings. They can be extensions\n # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\n extensions = [\n-    'hoverxref.extension',\n-    'notfound.extension',\n-    'scrapydocs',\n-    'sphinx.ext.autodoc',\n-    'sphinx.ext.coverage',\n-    'sphinx.ext.intersphinx',\n-    'sphinx.ext.viewcode',\n+    \"hoverxref.extension\",\n+    \"notfound.extension\",\n+    \"scrapydocs\",\n+    \"sphinx.ext.autodoc\",\n+    \"sphinx.ext.coverage\",\n+    \"sphinx.ext.intersphinx\",\n+    \"sphinx.ext.viewcode\",\n ]\n \n # Add any paths that contain templates here, relative to this directory.\n-templates_path = ['_templates']\n+templates_path = [\"_templates\"]\n \n # The suffix of source filenames.\n-source_suffix = '.rst'\n+source_suffix = \".rst\"\n \n # The encoding of source files.\n # source_encoding = 'utf-8'\n \n # The master toctree document.\n-master_doc = 'index'\n+master_doc = \"index\"\n \n # General information about the project.\n-project = 'Scrapy'\n-copyright = f'2008–{datetime.now().year}, Scrapy developers'\n+project = \"Scrapy\"\n+copyright = f\"2008–{datetime.now().year}, Scrapy developers\"\n \n # The version info for the project you're documenting, acts as replacement for\n # |version| and |release|, also used in various other places throughout the\n@@ -57,15 +57,16 @@ copyright = f'2008–{datetime.now().year}, Scrapy developers'\n # The short X.Y version.\n try:\n     import scrapy\n-    version = '.'.join(map(str, scrapy.version_info[:2]))\n+\n+    version = \".\".join(map(str, scrapy.version_info[:2]))\n     release = scrapy.__version__\n except ImportError:\n-    version = ''\n-    release = ''\n+    version = \"\"\n+    release = \"\"\n \n # The language for content autogenerated by Sphinx. Refer to documentation\n # for a list of supported languages.\n-language = 'en'\n+language = \"en\"\n \n # There are two options for replacing |today|: either, you set today to some\n # non-false value, then it is used:\n@@ -76,11 +77,11 @@ language = 'en'\n # List of documents that shouldn't be included in the build.\n # unused_docs = []\n \n-exclude_patterns = ['build']\n+exclude_patterns = [\"build\"]\n \n # List of directories, relative to source directory, that shouldn't be searched\n # for source files.\n-exclude_trees = ['.build']\n+exclude_trees = [\".build\"]\n \n # The reST default role (used for this markup: `text`) to use for all documents.\n # default_role = None\n@@ -97,10 +98,10 @@ exclude_trees = ['.build']\n # show_authors = False\n \n # The name of the Pygments (syntax highlighting) style to use.\n-pygments_style = 'sphinx'\n+pygments_style = \"sphinx\"\n \n # List of Sphinx warnings that will not be raised\n-suppress_warnings = ['epub.unknown_project_files']\n+suppress_warnings = [\"epub.unknown_project_files\"]\n \n \n # Options for HTML output\n@@ -108,7 +109,7 @@ suppress_warnings = ['epub.unknown_project_files']\n \n # The theme to use for HTML and HTML Help pages.  See the documentation for\n # a list of builtin themes.\n-html_theme = 'sphinx_rtd_theme'\n+html_theme = \"sphinx_rtd_theme\"\n \n # Theme options are theme-specific and customize the look and feel of a theme\n # further.  For a list of options available for each theme, see the\n@@ -119,6 +120,7 @@ html_theme = 'sphinx_rtd_theme'\n # Add path to the RTD explicitly to robustify builds (otherwise might\n # fail in a clean Debian build env)\n import sphinx_rtd_theme\n+\n html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n \n # The style sheet to use for HTML and HTML Help pages. A file of that name\n@@ -145,11 +147,11 @@ html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n # Add any paths that contain custom static files (such as style sheets) here,\n # relative to this directory. They are copied after the builtin static files,\n # so a file named \"default.css\" will overwrite the builtin \"default.css\".\n-html_static_path = ['_static']\n+html_static_path = [\"_static\"]\n \n # If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n # using the given strftime format.\n-html_last_updated_fmt = '%b %d, %Y'\n+html_last_updated_fmt = \"%b %d, %Y\"\n \n # Custom sidebar templates, maps document names to template names.\n # html_sidebars = {}\n@@ -179,10 +181,10 @@ html_copy_source = True\n # html_file_suffix = ''\n \n # Output file base name for HTML help builder.\n-htmlhelp_basename = 'Scrapydoc'\n+htmlhelp_basename = \"Scrapydoc\"\n \n html_css_files = [\n-    'custom.css',\n+    \"custom.css\",\n ]\n \n \n@@ -198,8 +200,7 @@ html_css_files = [\n # Grouping the document tree into LaTeX files. List of tuples\n # (source start file, target name, title, author, document class [howto/manual]).\n latex_documents = [\n-  ('index', 'Scrapy.tex', 'Scrapy Documentation',\n-   'Scrapy developers', 'manual'),\n+    (\"index\", \"Scrapy.tex\", \"Scrapy Documentation\", \"Scrapy developers\", \"manual\"),\n ]\n \n # The name of an image file (relative to this directory) to place at the top of\n@@ -226,8 +227,9 @@ latex_documents = [\n # A list of regular expressions that match URIs that should not be checked when\n # doing a linkcheck build.\n linkcheck_ignore = [\n-    'http://localhost:\\d+', 'http://hg.scrapy.org',\n-    'http://directory.google.com/'\n+    \"http://localhost:\\d+\",\n+    \"http://hg.scrapy.org\",\n+    \"http://directory.google.com/\",\n ]\n \n \n@@ -237,44 +239,35 @@ coverage_ignore_pyobjects = [\n     # Contract’s add_pre_hook and add_post_hook are not documented because\n     # they should be transparent to contract developers, for whom pre_hook and\n     # post_hook should be the actual concern.\n-    r'\\bContract\\.add_(pre|post)_hook$',\n-\n+    r\"\\bContract\\.add_(pre|post)_hook$\",\n     # ContractsManager is an internal class, developers are not expected to\n     # interact with it directly in any way.\n-    r'\\bContractsManager\\b$',\n-\n+    r\"\\bContractsManager\\b$\",\n     # For default contracts we only want to document their general purpose in\n     # their __init__ method, the methods they reimplement to achieve that purpose\n     # should be irrelevant to developers using those contracts.\n-    r'\\w+Contract\\.(adjust_request_args|(pre|post)_process)$',\n-\n+    r\"\\w+Contract\\.(adjust_request_args|(pre|post)_process)$\",\n     # Methods of downloader middlewares are not documented, only the classes\n     # themselves, since downloader middlewares are controlled through Scrapy\n     # settings.\n-    r'^scrapy\\.downloadermiddlewares\\.\\w*?\\.(\\w*?Middleware|DownloaderStats)\\.',\n-\n+    r\"^scrapy\\.downloadermiddlewares\\.\\w*?\\.(\\w*?Middleware|DownloaderStats)\\.\",\n     # Base classes of downloader middlewares are implementation details that\n     # are not meant for users.\n-    r'^scrapy\\.downloadermiddlewares\\.\\w*?\\.Base\\w*?Middleware',\n-\n+    r\"^scrapy\\.downloadermiddlewares\\.\\w*?\\.Base\\w*?Middleware\",\n     # Private exception used by the command-line interface implementation.\n-    r'^scrapy\\.exceptions\\.UsageError',\n-\n+    r\"^scrapy\\.exceptions\\.UsageError\",\n     # Methods of BaseItemExporter subclasses are only documented in\n     # BaseItemExporter.\n-    r'^scrapy\\.exporters\\.(?!BaseItemExporter\\b)\\w*?\\.',\n-\n+    r\"^scrapy\\.exporters\\.(?!BaseItemExporter\\b)\\w*?\\.\",\n     # Extension behavior is only modified through settings. Methods of\n     # extension classes, as well as helper functions, are implementation\n     # details that are not documented.\n-    r'^scrapy\\.extensions\\.[a-z]\\w*?\\.[A-Z]\\w*?\\.',  # methods\n-    r'^scrapy\\.extensions\\.[a-z]\\w*?\\.[a-z]',  # helper functions\n-\n+    r\"^scrapy\\.extensions\\.[a-z]\\w*?\\.[A-Z]\\w*?\\.\",  # methods\n+    r\"^scrapy\\.extensions\\.[a-z]\\w*?\\.[a-z]\",  # helper functions\n     # Never documented before, and deprecated now.\n-    r'^scrapy\\.linkextractors\\.FilteringLinkExtractor$',\n-\n+    r\"^scrapy\\.linkextractors\\.FilteringLinkExtractor$\",\n     # Implementation detail of LxmlLinkExtractor\n-    r'^scrapy\\.linkextractors\\.lxmlhtml\\.LxmlParserLinkExtractor',\n+    r\"^scrapy\\.linkextractors\\.lxmlhtml\\.LxmlParserLinkExtractor\",\n ]\n \n \n@@ -282,18 +275,18 @@ coverage_ignore_pyobjects = [\n # -------------------------------------\n \n intersphinx_mapping = {\n-    'attrs': ('https://www.attrs.org/en/stable/', None),\n-    'coverage': ('https://coverage.readthedocs.io/en/stable', None),\n-    'cryptography' : ('https://cryptography.io/en/latest/', None),\n-    'cssselect': ('https://cssselect.readthedocs.io/en/latest', None),\n-    'itemloaders': ('https://itemloaders.readthedocs.io/en/latest/', None),\n-    'pytest': ('https://docs.pytest.org/en/latest', None),\n-    'python': ('https://docs.python.org/3', None),\n-    'sphinx': ('https://www.sphinx-doc.org/en/master', None),\n-    'tox': ('https://tox.wiki/en/latest/', None),\n-    'twisted': ('https://docs.twisted.org/en/stable/', None),\n-    'twistedapi': ('https://docs.twisted.org/en/stable/api/', None),\n-    'w3lib': ('https://w3lib.readthedocs.io/en/latest', None),\n+    \"attrs\": (\"https://www.attrs.org/en/stable/\", None),\n+    \"coverage\": (\"https://coverage.readthedocs.io/en/stable\", None),\n+    \"cryptography\": (\"https://cryptography.io/en/latest/\", None),\n+    \"cssselect\": (\"https://cssselect.readthedocs.io/en/latest\", None),\n+    \"itemloaders\": (\"https://itemloaders.readthedocs.io/en/latest/\", None),\n+    \"pytest\": (\"https://docs.pytest.org/en/latest\", None),\n+    \"python\": (\"https://docs.python.org/3\", None),\n+    \"sphinx\": (\"https://www.sphinx-doc.org/en/master\", None),\n+    \"tox\": (\"https://tox.wiki/en/latest/\", None),\n+    \"twisted\": (\"https://docs.twisted.org/en/stable/\", None),\n+    \"twistedapi\": (\"https://docs.twisted.org/en/stable/api/\", None),\n+    \"w3lib\": (\"https://w3lib.readthedocs.io/en/latest\", None),\n }\n intersphinx_disabled_reftypes = []\n \n@@ -313,16 +306,16 @@ hoverxref_role_types = {\n     \"setting\": \"tooltip\",\n     \"signal\": \"tooltip\",\n }\n-hoverxref_roles = ['command', 'reqmeta', 'setting', 'signal']\n+hoverxref_roles = [\"command\", \"reqmeta\", \"setting\", \"signal\"]\n \n \n def setup(app):\n-    app.connect('autodoc-skip-member', maybe_skip_member)\n+    app.connect(\"autodoc-skip-member\", maybe_skip_member)\n \n \n def maybe_skip_member(app, what, name, obj, skip, options):\n     if not skip:\n         # autodocs was generating a text \"alias of\" for the following members\n         # https://github.com/sphinx-doc/sphinx/issues/4422\n-        return name in {'default_item_class', 'default_selector_class'}\n+        return name in {\"default_item_class\", \"default_selector_class\"}\n     return skip\n\n@@ -15,20 +15,20 @@ from scrapy.http.response.html import HtmlResponse\n \n \n def load_response(url: str, filename: str) -> HtmlResponse:\n-    input_path = Path(__file__).parent / '_tests' / filename\n+    input_path = Path(__file__).parent / \"_tests\" / filename\n     return HtmlResponse(url, body=input_path.read_bytes())\n \n \n def setup(namespace):\n-    namespace['load_response'] = load_response\n+    namespace[\"load_response\"] = load_response\n \n \n pytest_collect_file = Sybil(\n     parsers=[\n         DocTestParser(optionflags=ELLIPSIS | NORMALIZE_WHITESPACE),\n-        PythonCodeBlockParser(future_imports=['print_function']),\n+        PythonCodeBlockParser(future_imports=[\"print_function\"]),\n         skip,\n     ],\n-    pattern='*.rst',\n+    pattern=\"*.rst\",\n     setup=setup,\n ).pytest()\n\n@@ -25,7 +25,7 @@ def main():\n     _contents = None\n \n     # A regex that matches standard linkcheck output lines\n-    line_re = re.compile(r'(.*)\\:\\d+\\:\\s\\[(.*)\\]\\s(?:(.*)\\sto\\s(.*)|(.*))')\n+    line_re = re.compile(r\"(.*)\\:\\d+\\:\\s\\[(.*)\\]\\s(?:(.*)\\sto\\s(.*)|(.*))\")\n \n     # Read lines from the linkcheck output file\n     try:\n@@ -66,5 +66,5 @@ def main():\n             print(\"Not Understood: \" + line)\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     main()\n\n@@ -7,7 +7,6 @@ from twisted.internet import reactor\n \n \n class Root(Resource):\n-\n     def __init__(self):\n         Resource.__init__(self)\n         self.concurrent = 0\n@@ -28,7 +27,7 @@ class Root(Resource):\n         # reset stats on high iter-request times caused by client restarts\n         if delta > 3:  # seconds\n             self._reset_stats()\n-            return ''\n+            return \"\"\n \n         self.tail.appendleft(delta)\n         self.lasttime = now\n@@ -37,15 +36,17 @@ class Root(Resource):\n         if now - self.lastmark >= 3:\n             self.lastmark = now\n             qps = len(self.tail) / sum(self.tail)\n-            print(f'samplesize={len(self.tail)} concurrent={self.concurrent} qps={qps:0.2f}')\n+            print(\n+                f\"samplesize={len(self.tail)} concurrent={self.concurrent} qps={qps:0.2f}\"\n+            )\n \n-        if 'latency' in request.args:\n-            latency = float(request.args['latency'][0])\n+        if \"latency\" in request.args:\n+            latency = float(request.args[\"latency\"][0])\n             reactor.callLater(latency, self._finish, request)\n             return NOT_DONE_YET\n \n         self.concurrent -= 1\n-        return ''\n+        return \"\"\n \n     def _finish(self, request):\n         self.concurrent -= 1\n\n@@ -13,8 +13,8 @@ from scrapy.http import Request\n \n class QPSSpider(Spider):\n \n-    name = 'qps'\n-    benchurl = 'http://localhost:8880/'\n+    name = \"qps\"\n+    benchurl = \"http://localhost:8880/\"\n \n     # Max concurrency is limited by global CONCURRENT_REQUESTS setting\n     max_concurrent_requests = 8\n@@ -37,11 +37,11 @@ class QPSSpider(Spider):\n     def start_requests(self):\n         url = self.benchurl\n         if self.latency is not None:\n-            url += f'?latency={self.latency}'\n+            url += f\"?latency={self.latency}\"\n \n         slots = int(self.slots)\n         if slots > 1:\n-            urls = [url.replace('localhost', f'127.0.0.{x + 1}') for x in range(slots)]\n+            urls = [url.replace(\"localhost\", f\"127.0.0.{x + 1}\") for x in range(slots)]\n         else:\n             urls = [url]\n \n\n@@ -16,14 +16,21 @@ from scrapy.item import Item, Field\n \n \n __all__ = [\n-    '__version__', 'version_info', 'twisted_version', 'Spider',\n-    'Request', 'FormRequest', 'Selector', 'Item', 'Field',\n+    \"__version__\",\n+    \"version_info\",\n+    \"twisted_version\",\n+    \"Spider\",\n+    \"Request\",\n+    \"FormRequest\",\n+    \"Selector\",\n+    \"Item\",\n+    \"Field\",\n ]\n \n \n # Scrapy and Twisted versions\n __version__ = (pkgutil.get_data(__package__, \"VERSION\") or b\"\").decode(\"ascii\").strip()\n-version_info = tuple(int(v) if v.isdigit() else v for v in __version__.split('.'))\n+version_info = tuple(int(v) if v.isdigit() else v for v in __version__.split(\".\"))\n twisted_version = (_txv.major, _txv.minor, _txv.micro)\n \n \n@@ -34,7 +41,7 @@ if sys.version_info < (3, 7):\n \n \n # Ignore noisy twisted deprecation warnings\n-warnings.filterwarnings('ignore', category=DeprecationWarning, module='twisted')\n+warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"twisted\")\n \n \n del pkgutil\n\n@@ -1,4 +1,4 @@\n from scrapy.cmdline import execute\n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     execute()\n\n@@ -17,7 +17,7 @@ from scrapy.utils.python import garbage_collect\n class ScrapyArgumentParser(argparse.ArgumentParser):\n     def _parse_optional(self, arg_string):\n         # if starts with -: it means that is a parameter not a argument\n-        if arg_string[:2] == '-:':\n+        if arg_string[:2] == \"-:\":\n             return None\n \n         return super()._parse_optional(arg_string)\n@@ -41,12 +41,12 @@ def _get_commands_from_module(module, inproject):\n     d = {}\n     for cmd in _iter_command_classes(module):\n         if inproject or not cmd.requires_project:\n-            cmdname = cmd.__module__.split('.')[-1]\n+            cmdname = cmd.__module__.split(\".\")[-1]\n             d[cmdname] = cmd()\n     return d\n \n \n-def _get_commands_from_entry_points(inproject, group='scrapy.commands'):\n+def _get_commands_from_entry_points(inproject, group=\"scrapy.commands\"):\n     cmds = {}\n     for entry_point in pkg_resources.iter_entry_points(group):\n         obj = entry_point.load()\n@@ -58,9 +58,9 @@ def _get_commands_from_entry_points(inproject, group='scrapy.commands'):\n \n \n def _get_commands_dict(settings, inproject):\n-    cmds = _get_commands_from_module('scrapy.commands', inproject)\n+    cmds = _get_commands_from_module(\"scrapy.commands\", inproject)\n     cmds.update(_get_commands_from_entry_points(inproject))\n-    cmds_module = settings['COMMANDS_MODULE']\n+    cmds_module = settings[\"COMMANDS_MODULE\"]\n     if cmds_module:\n         cmds.update(_get_commands_from_module(cmds_module, inproject))\n     return cmds\n@@ -69,7 +69,7 @@ def _get_commands_dict(settings, inproject):\n def _pop_command_name(argv):\n     i = 0\n     for arg in argv[1:]:\n-        if not arg.startswith('-'):\n+        if not arg.startswith(\"-\"):\n             del argv[i]\n             return arg\n         i += 1\n@@ -124,11 +124,11 @@ def execute(argv=None, settings=None):\n         settings = get_project_settings()\n         # set EDITOR from environment if available\n         try:\n-            editor = os.environ['EDITOR']\n+            editor = os.environ[\"EDITOR\"]\n         except KeyError:\n             pass\n         else:\n-            settings['EDITOR'] = editor\n+            settings[\"EDITOR\"] = editor\n \n     inproject = inside_project()\n     cmds = _get_commands_dict(settings, inproject)\n@@ -141,11 +141,13 @@ def execute(argv=None, settings=None):\n         sys.exit(2)\n \n     cmd = cmds[cmdname]\n-    parser = ScrapyArgumentParser(formatter_class=ScrapyHelpFormatter,\n+    parser = ScrapyArgumentParser(\n+        formatter_class=ScrapyHelpFormatter,\n         usage=f\"scrapy {cmdname} {cmd.syntax()}\",\n-                                  conflict_handler='resolve',\n-                                  description=cmd.long_desc())\n-    settings.setdict(cmd.default_settings, priority='command')\n+        conflict_handler=\"resolve\",\n+        description=cmd.long_desc(),\n+    )\n+    settings.setdict(cmd.default_settings, priority=\"command\")\n     cmd.settings = settings\n     cmd.add_options(parser)\n     opts, args = parser.parse_known_args(args=argv[1:])\n@@ -168,12 +170,12 @@ def _run_command_profiled(cmd, args, opts):\n         sys.stderr.write(f\"scrapy: writing cProfile stats to {opts.profile!r}\\n\")\n     loc = locals()\n     p = cProfile.Profile()\n-    p.runctx('cmd.run(args, opts)', globals(), loc)\n+    p.runctx(\"cmd.run(args, opts)\", globals(), loc)\n     if opts.profile:\n         p.dump_stats(opts.profile)\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     try:\n         execute()\n     finally:\n\n@@ -27,7 +27,7 @@ class ScrapyCommand:\n         self.settings: Any = None  # set in scrapy.cmdline\n \n     def set_crawler(self, crawler):\n-        if hasattr(self, '_crawler'):\n+        if hasattr(self, \"_crawler\"):\n             raise RuntimeError(\"crawler already set\")\n         self._crawler = crawler\n \n@@ -61,41 +61,58 @@ class ScrapyCommand:\n         \"\"\"\n         Populate option parse with options available for this command\n         \"\"\"\n-        group = parser.add_argument_group(title='Global Options')\n-        group.add_argument(\"--logfile\", metavar=\"FILE\",\n-                           help=\"log file. if omitted stderr will be used\")\n-        group.add_argument(\"-L\", \"--loglevel\", metavar=\"LEVEL\", default=None,\n-                           help=f\"log level (default: {self.settings['LOG_LEVEL']})\")\n-        group.add_argument(\"--nolog\", action=\"store_true\",\n-                           help=\"disable logging completely\")\n-        group.add_argument(\"--profile\", metavar=\"FILE\", default=None,\n-                           help=\"write python cProfile stats to FILE\")\n-        group.add_argument(\"--pidfile\", metavar=\"FILE\",\n-                           help=\"write process ID to FILE\")\n-        group.add_argument(\"-s\", \"--set\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n-                           help=\"set/override setting (may be repeated)\")\n+        group = parser.add_argument_group(title=\"Global Options\")\n+        group.add_argument(\n+            \"--logfile\", metavar=\"FILE\", help=\"log file. if omitted stderr will be used\"\n+        )\n+        group.add_argument(\n+            \"-L\",\n+            \"--loglevel\",\n+            metavar=\"LEVEL\",\n+            default=None,\n+            help=f\"log level (default: {self.settings['LOG_LEVEL']})\",\n+        )\n+        group.add_argument(\n+            \"--nolog\", action=\"store_true\", help=\"disable logging completely\"\n+        )\n+        group.add_argument(\n+            \"--profile\",\n+            metavar=\"FILE\",\n+            default=None,\n+            help=\"write python cProfile stats to FILE\",\n+        )\n+        group.add_argument(\"--pidfile\", metavar=\"FILE\", help=\"write process ID to FILE\")\n+        group.add_argument(\n+            \"-s\",\n+            \"--set\",\n+            action=\"append\",\n+            default=[],\n+            metavar=\"NAME=VALUE\",\n+            help=\"set/override setting (may be repeated)\",\n+        )\n         group.add_argument(\"--pdb\", action=\"store_true\", help=\"enable pdb on failure\")\n \n     def process_options(self, args, opts):\n         try:\n-            self.settings.setdict(arglist_to_dict(opts.set),\n-                                  priority='cmdline')\n+            self.settings.setdict(arglist_to_dict(opts.set), priority=\"cmdline\")\n         except ValueError:\n             raise UsageError(\"Invalid -s value, use -s NAME=VALUE\", print_help=False)\n \n         if opts.logfile:\n-            self.settings.set('LOG_ENABLED', True, priority='cmdline')\n-            self.settings.set('LOG_FILE', opts.logfile, priority='cmdline')\n+            self.settings.set(\"LOG_ENABLED\", True, priority=\"cmdline\")\n+            self.settings.set(\"LOG_FILE\", opts.logfile, priority=\"cmdline\")\n \n         if opts.loglevel:\n-            self.settings.set('LOG_ENABLED', True, priority='cmdline')\n-            self.settings.set('LOG_LEVEL', opts.loglevel, priority='cmdline')\n+            self.settings.set(\"LOG_ENABLED\", True, priority=\"cmdline\")\n+            self.settings.set(\"LOG_LEVEL\", opts.loglevel, priority=\"cmdline\")\n \n         if opts.nolog:\n-            self.settings.set('LOG_ENABLED', False, priority='cmdline')\n+            self.settings.set(\"LOG_ENABLED\", False, priority=\"cmdline\")\n \n         if opts.pidfile:\n-            Path(opts.pidfile).write_text(str(os.getpid()) + os.linesep, encoding=\"utf-8\")\n+            Path(opts.pidfile).write_text(\n+                str(os.getpid()) + os.linesep, encoding=\"utf-8\"\n+            )\n \n         if opts.pdb:\n             failure.startDebugMode()\n@@ -111,18 +128,39 @@ class BaseRunSpiderCommand(ScrapyCommand):\n     \"\"\"\n     Common class used to share functionality between the crawl, parse and runspider commands\n     \"\"\"\n+\n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_argument(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n-                            help=\"set spider argument (may be repeated)\")\n-        parser.add_argument(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n+        parser.add_argument(\n+            \"-a\",\n+            dest=\"spargs\",\n+            action=\"append\",\n+            default=[],\n+            metavar=\"NAME=VALUE\",\n+            help=\"set spider argument (may be repeated)\",\n+        )\n+        parser.add_argument(\n+            \"-o\",\n+            \"--output\",\n+            metavar=\"FILE\",\n+            action=\"append\",\n             help=\"append scraped items to the end of FILE (use - for stdout),\"\n-                                 \" to define format set a colon at the end of the output URI (i.e. -o FILE:FORMAT)\")\n-        parser.add_argument(\"-O\", \"--overwrite-output\", metavar=\"FILE\", action=\"append\",\n+            \" to define format set a colon at the end of the output URI (i.e. -o FILE:FORMAT)\",\n+        )\n+        parser.add_argument(\n+            \"-O\",\n+            \"--overwrite-output\",\n+            metavar=\"FILE\",\n+            action=\"append\",\n             help=\"dump scraped items into FILE, overwriting any existing file,\"\n-                                 \" to define format set a colon at the end of the output URI (i.e. -O FILE:FORMAT)\")\n-        parser.add_argument(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n-                            help=\"format to use for dumping items\")\n+            \" to define format set a colon at the end of the output URI (i.e. -O FILE:FORMAT)\",\n+        )\n+        parser.add_argument(\n+            \"-t\",\n+            \"--output-format\",\n+            metavar=\"FORMAT\",\n+            help=\"format to use for dumping items\",\n+        )\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -137,16 +175,21 @@ class BaseRunSpiderCommand(ScrapyCommand):\n                 opts.output_format,\n                 opts.overwrite_output,\n             )\n-            self.settings.set('FEEDS', feeds, priority='cmdline')\n+            self.settings.set(\"FEEDS\", feeds, priority=\"cmdline\")\n \n \n class ScrapyHelpFormatter(argparse.HelpFormatter):\n     \"\"\"\n     Help Formatter for scrapy command line help messages.\n     \"\"\"\n+\n     def __init__(self, prog, indent_increment=2, max_help_position=24, width=None):\n-        super().__init__(prog, indent_increment=indent_increment,\n-                         max_help_position=max_help_position, width=width)\n+        super().__init__(\n+            prog,\n+            indent_increment=indent_increment,\n+            max_help_position=max_help_position,\n+            width=width,\n+        )\n \n     def _join_parts(self, part_strings):\n         parts = self.format_part_strings(part_strings)\n@@ -157,11 +200,13 @@ class ScrapyHelpFormatter(argparse.HelpFormatter):\n         Underline and title case command line help message headers.\n         \"\"\"\n         if part_strings and part_strings[0].startswith(\"usage: \"):\n-            part_strings[0] = \"Usage\\n=====\\n  \" + part_strings[0][len('usage: '):]\n-        headings = [i for i in range(len(part_strings)) if part_strings[i].endswith(':\\n')]\n+            part_strings[0] = \"Usage\\n=====\\n  \" + part_strings[0][len(\"usage: \") :]\n+        headings = [\n+            i for i in range(len(part_strings)) if part_strings[i].endswith(\":\\n\")\n+        ]\n         for index in headings[::-1]:\n-            char = '-' if \"Global Options\" in part_strings[index] else '='\n+            char = \"-\" if \"Global Options\" in part_strings[index] else \"=\"\n             part_strings[index] = part_strings[index][:-2].title()\n-            underline = ''.join([\"\\n\", (char * len(part_strings[index])), \"\\n\"])\n+            underline = \"\".join([\"\\n\", (char * len(part_strings[index])), \"\\n\"])\n             part_strings.insert(index + 1, underline)\n         return part_strings\n\n@@ -11,9 +11,9 @@ from scrapy.linkextractors import LinkExtractor\n class Command(ScrapyCommand):\n \n     default_settings = {\n-        'LOG_LEVEL': 'INFO',\n-        'LOGSTATS_INTERVAL': 1,\n-        'CLOSESPIDER_TIMEOUT': 10,\n+        \"LOG_LEVEL\": \"INFO\",\n+        \"LOGSTATS_INTERVAL\": 1,\n+        \"CLOSESPIDER_TIMEOUT\": 10,\n     }\n \n     def short_desc(self):\n@@ -26,12 +26,11 @@ class Command(ScrapyCommand):\n \n \n class _BenchServer:\n-\n     def __enter__(self):\n         from scrapy.utils.test import get_testenv\n-        pargs = [sys.executable, '-u', '-m', 'scrapy.utils.benchserver']\n-        self.proc = subprocess.Popen(pargs, stdout=subprocess.PIPE,\n-                                     env=get_testenv())\n+\n+        pargs = [sys.executable, \"-u\", \"-m\", \"scrapy.utils.benchserver\"]\n+        self.proc = subprocess.Popen(pargs, stdout=subprocess.PIPE, env=get_testenv())\n         self.proc.stdout.readline()\n \n     def __exit__(self, exc_type, exc_value, traceback):\n@@ -42,15 +41,16 @@ class _BenchServer:\n \n class _BenchSpider(scrapy.Spider):\n     \"\"\"A spider that follows all links\"\"\"\n-    name = 'follow'\n+\n+    name = \"follow\"\n     total = 10000\n     show = 20\n-    baseurl = 'http://localhost:8998'\n+    baseurl = \"http://localhost:8998\"\n     link_extractor = LinkExtractor()\n \n     def start_requests(self):\n-        qargs = {'total': self.total, 'show': self.show}\n-        url = f'{self.baseurl}?{urlencode(qargs, doseq=True)}'\n+        qargs = {\"total\": self.total, \"show\": self.show}\n+        url = f\"{self.baseurl}?{urlencode(qargs, doseq=True)}\"\n         return [scrapy.Request(url, dont_filter=True)]\n \n     def parse(self, response):\n\n@@ -39,7 +39,7 @@ class TextTestResult(_TextTestResult):\n \n class Command(ScrapyCommand):\n     requires_project = True\n-    default_settings = {'LOG_ENABLED': False}\n+    default_settings = {\"LOG_ENABLED\": False}\n \n     def syntax(self):\n         return \"[options] <spider>\"\n@@ -49,14 +49,25 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_argument(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n-                            help=\"only list contracts, without checking them\")\n-        parser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", default=False, action='store_true',\n-                            help=\"print contract tests for all spiders\")\n+        parser.add_argument(\n+            \"-l\",\n+            \"--list\",\n+            dest=\"list\",\n+            action=\"store_true\",\n+            help=\"only list contracts, without checking them\",\n+        )\n+        parser.add_argument(\n+            \"-v\",\n+            \"--verbose\",\n+            dest=\"verbose\",\n+            default=False,\n+            action=\"store_true\",\n+            help=\"print contract tests for all spiders\",\n+        )\n \n     def run(self, args, opts):\n         # load contracts\n-        contracts = build_component_list(self.settings.getwithbase('SPIDER_CONTRACTS'))\n+        contracts = build_component_list(self.settings.getwithbase(\"SPIDER_CONTRACTS\"))\n         conman = ContractsManager(load_object(c) for c in contracts)\n         runner = TextTestRunner(verbosity=2 if opts.verbose else 1)\n         result = TextTestResult(runner.stream, runner.descriptions, runner.verbosity)\n@@ -66,7 +77,7 @@ class Command(ScrapyCommand):\n \n         spider_loader = self.crawler_process.spider_loader\n \n-        with set_environ(SCRAPY_CHECK='true'):\n+        with set_environ(SCRAPY_CHECK=\"true\"):\n             for spidername in args or spider_loader.list():\n                 spidercls = spider_loader.load(spidername)\n                 spidercls.start_requests = lambda s: conman.from_spider(s, result)\n@@ -85,7 +96,7 @@ class Command(ScrapyCommand):\n                         continue\n                     print(spider)\n                     for method in sorted(methods):\n-                        print(f'  * {method}')\n+                        print(f\"  * {method}\")\n             else:\n                 start = time.time()\n                 self.crawler_process.start()\n\n@@ -16,18 +16,23 @@ class Command(BaseRunSpiderCommand):\n         if len(args) < 1:\n             raise UsageError()\n         elif len(args) > 1:\n-            raise UsageError(\"running 'scrapy crawl' with more than one spider is not supported\")\n+            raise UsageError(\n+                \"running 'scrapy crawl' with more than one spider is not supported\"\n+            )\n         spname = args[0]\n \n         crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n \n-        if getattr(crawl_defer, 'result', None) is not None and issubclass(crawl_defer.result.type, Exception):\n+        if getattr(crawl_defer, \"result\", None) is not None and issubclass(\n+            crawl_defer.result.type, Exception\n+        ):\n             self.exitcode = 1\n         else:\n             self.crawler_process.start()\n \n             if (\n                 self.crawler_process.bootstrap_failed\n-                or hasattr(self.crawler_process, 'has_exception') and self.crawler_process.has_exception\n+                or hasattr(self.crawler_process, \"has_exception\")\n+                and self.crawler_process.has_exception\n             ):\n                 self.exitcode = 1\n\n@@ -8,7 +8,7 @@ from scrapy.exceptions import UsageError\n class Command(ScrapyCommand):\n \n     requires_project = True\n-    default_settings = {'LOG_ENABLED': False}\n+    default_settings = {\"LOG_ENABLED\": False}\n \n     def syntax(self):\n         return \"<spider>\"\n@@ -17,8 +17,10 @@ class Command(ScrapyCommand):\n         return \"Edit spider\"\n \n     def long_desc(self):\n-        return (\"Edit a spider using the editor defined in the EDITOR environment\"\n-                \" variable or else the EDITOR setting\")\n+        return (\n+            \"Edit a spider using the editor defined in the EDITOR environment\"\n+            \" variable or else the EDITOR setting\"\n+        )\n \n     def _err(self, msg):\n         sys.stderr.write(msg + os.linesep)\n@@ -28,12 +30,12 @@ class Command(ScrapyCommand):\n         if len(args) != 1:\n             raise UsageError()\n \n-        editor = self.settings['EDITOR']\n+        editor = self.settings[\"EDITOR\"]\n         try:\n             spidercls = self.crawler_process.spider_loader.load(args[0])\n         except KeyError:\n             return self._err(f\"Spider not found: {args[0]}\")\n \n         sfile = sys.modules[spidercls.__module__].__file__\n-        sfile = sfile.replace('.pyc', '.py')\n+        sfile = sfile.replace(\".pyc\", \".py\")\n         self.exitcode = os.system(f'{editor} \"{sfile}\"')\n\n@@ -27,38 +27,51 @@ class Command(ScrapyCommand):\n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_argument(\"--spider\", dest=\"spider\", help=\"use this spider\")\n-        parser.add_argument(\"--headers\", dest=\"headers\", action=\"store_true\",\n-                            help=\"print response HTTP headers instead of body\")\n-        parser.add_argument(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", default=False,\n-                            help=\"do not handle HTTP 3xx status codes and print response as-is\")\n+        parser.add_argument(\n+            \"--headers\",\n+            dest=\"headers\",\n+            action=\"store_true\",\n+            help=\"print response HTTP headers instead of body\",\n+        )\n+        parser.add_argument(\n+            \"--no-redirect\",\n+            dest=\"no_redirect\",\n+            action=\"store_true\",\n+            default=False,\n+            help=\"do not handle HTTP 3xx status codes and print response as-is\",\n+        )\n \n     def _print_headers(self, headers, prefix):\n         for key, values in headers.items():\n             for value in values:\n-                self._print_bytes(prefix + b' ' + key + b': ' + value)\n+                self._print_bytes(prefix + b\" \" + key + b\": \" + value)\n \n     def _print_response(self, response, opts):\n         if opts.headers:\n-            self._print_headers(response.request.headers, b'>')\n-            print('>')\n-            self._print_headers(response.headers, b'<')\n+            self._print_headers(response.request.headers, b\">\")\n+            print(\">\")\n+            self._print_headers(response.headers, b\"<\")\n         else:\n             self._print_bytes(response.body)\n \n     def _print_bytes(self, bytes_):\n-        sys.stdout.buffer.write(bytes_ + b'\\n')\n+        sys.stdout.buffer.write(bytes_ + b\"\\n\")\n \n     def run(self, args, opts):\n         if len(args) != 1 or not is_url(args[0]):\n             raise UsageError()\n-        request = Request(args[0], callback=self._print_response,\n-                          cb_kwargs={\"opts\": opts}, dont_filter=True)\n+        request = Request(\n+            args[0],\n+            callback=self._print_response,\n+            cb_kwargs={\"opts\": opts},\n+            dont_filter=True,\n+        )\n         # by default, let the framework handle redirects,\n         # i.e. command handles all codes expect 3xx\n         if not opts.no_redirect:\n-            request.meta['handle_httpstatus_list'] = SequenceExclude(range(300, 400))\n+            request.meta[\"handle_httpstatus_list\"] = SequenceExclude(range(300, 400))\n         else:\n-            request.meta['handle_httpstatus_all'] = True\n+            request.meta[\"handle_httpstatus_all\"] = True\n \n         spidercls = DefaultSpider\n         spider_loader = self.crawler_process.spider_loader\n\n@@ -18,7 +18,7 @@ def sanitize_module_name(module_name):\n     with underscores and prefixing it with a letter if it doesn't start\n     with one\n     \"\"\"\n-    module_name = module_name.replace('-', '_').replace('.', '_')\n+    module_name = module_name.replace(\"-\", \"_\").replace(\".\", \"_\")\n     if module_name[0] not in string.ascii_letters:\n         module_name = \"a\" + module_name\n     return module_name\n@@ -27,7 +27,7 @@ def sanitize_module_name(module_name):\n def extract_domain(url):\n     \"\"\"Extract domain name from URL string\"\"\"\n     o = urlparse(url)\n-    if o.scheme == '' and o.netloc == '':\n+    if o.scheme == \"\" and o.netloc == \"\":\n         o = urlparse(\"//\" + url.lstrip(\"/\"))\n     return o.netloc\n \n@@ -35,7 +35,7 @@ def extract_domain(url):\n class Command(ScrapyCommand):\n \n     requires_project = False\n-    default_settings = {'LOG_ENABLED': False}\n+    default_settings = {\"LOG_ENABLED\": False}\n \n     def syntax(self):\n         return \"[options] <name> <domain>\"\n@@ -45,16 +45,40 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_argument(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n-                            help=\"List available templates\")\n-        parser.add_argument(\"-e\", \"--edit\", dest=\"edit\", action=\"store_true\",\n-                            help=\"Edit spider after creating it\")\n-        parser.add_argument(\"-d\", \"--dump\", dest=\"dump\", metavar=\"TEMPLATE\",\n-                            help=\"Dump template to standard output\")\n-        parser.add_argument(\"-t\", \"--template\", dest=\"template\", default=\"basic\",\n-                            help=\"Uses a custom template.\")\n-        parser.add_argument(\"--force\", dest=\"force\", action=\"store_true\",\n-                            help=\"If the spider already exists, overwrite it with the template\")\n+        parser.add_argument(\n+            \"-l\",\n+            \"--list\",\n+            dest=\"list\",\n+            action=\"store_true\",\n+            help=\"List available templates\",\n+        )\n+        parser.add_argument(\n+            \"-e\",\n+            \"--edit\",\n+            dest=\"edit\",\n+            action=\"store_true\",\n+            help=\"Edit spider after creating it\",\n+        )\n+        parser.add_argument(\n+            \"-d\",\n+            \"--dump\",\n+            dest=\"dump\",\n+            metavar=\"TEMPLATE\",\n+            help=\"Dump template to standard output\",\n+        )\n+        parser.add_argument(\n+            \"-t\",\n+            \"--template\",\n+            dest=\"template\",\n+            default=\"basic\",\n+            help=\"Uses a custom template.\",\n+        )\n+        parser.add_argument(\n+            \"--force\",\n+            dest=\"force\",\n+            action=\"store_true\",\n+            help=\"If the spider already exists, overwrite it with the template\",\n+        )\n \n     def run(self, args, opts):\n         if opts.list:\n@@ -72,7 +96,7 @@ class Command(ScrapyCommand):\n         domain = extract_domain(url)\n         module = sanitize_module_name(name)\n \n-        if self.settings.get('BOT_NAME') == module:\n+        if self.settings.get(\"BOT_NAME\") == module:\n             print(\"Cannot create a spider with the same name as your project\")\n             return\n \n@@ -87,17 +111,17 @@ class Command(ScrapyCommand):\n \n     def _genspider(self, module, name, domain, template_name, template_file):\n         \"\"\"Generate the spider module, based on the given template\"\"\"\n-        capitalized_module = ''.join(s.capitalize() for s in module.split('_'))\n+        capitalized_module = \"\".join(s.capitalize() for s in module.split(\"_\"))\n         tvars = {\n-            'project_name': self.settings.get('BOT_NAME'),\n-            'ProjectName': string_camelcase(self.settings.get('BOT_NAME')),\n-            'module': module,\n-            'name': name,\n-            'domain': domain,\n-            'classname': f'{capitalized_module}Spider'\n+            \"project_name\": self.settings.get(\"BOT_NAME\"),\n+            \"ProjectName\": string_camelcase(self.settings.get(\"BOT_NAME\")),\n+            \"module\": module,\n+            \"name\": name,\n+            \"domain\": domain,\n+            \"classname\": f\"{capitalized_module}Spider\",\n         }\n-        if self.settings.get('NEWSPIDER_MODULE'):\n-            spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])\n+        if self.settings.get(\"NEWSPIDER_MODULE\"):\n+            spiders_module = import_module(self.settings[\"NEWSPIDER_MODULE\"])\n             spiders_dir = Path(spiders_module.__file__).parent.resolve()\n         else:\n             spiders_module = None\n@@ -105,13 +129,15 @@ class Command(ScrapyCommand):\n         spider_file = f\"{spiders_dir / module}.py\"\n         shutil.copyfile(template_file, spider_file)\n         render_templatefile(spider_file, **tvars)\n-        print(f\"Created spider {name!r} using template {template_name!r} \",\n-              end=('' if spiders_module else '\\n'))\n+        print(\n+            f\"Created spider {name!r} using template {template_name!r} \",\n+            end=(\"\" if spiders_module else \"\\n\"),\n+        )\n         if spiders_module:\n             print(f\"in module:\\n  {spiders_module.__name__}.{module}\")\n \n     def _find_template(self, template: str) -> Optional[Path]:\n-        template_file = Path(self.templates_dir, f'{template}.tmpl')\n+        template_file = Path(self.templates_dir, f\"{template}.tmpl\")\n         if template_file.exists():\n             return template_file\n         print(f\"Unable to find template: {template}\\n\")\n@@ -121,11 +147,11 @@ class Command(ScrapyCommand):\n     def _list_templates(self):\n         print(\"Available templates:\")\n         for file in sorted(Path(self.templates_dir).iterdir()):\n-            if file.suffix == '.tmpl':\n+            if file.suffix == \".tmpl\":\n                 print(f\"  {file.stem}\")\n \n     def _spider_exists(self, name: str) -> bool:\n-        if not self.settings.get('NEWSPIDER_MODULE'):\n+        if not self.settings.get(\"NEWSPIDER_MODULE\"):\n             # if run as a standalone command and file with same filename already exists\n             path = Path(name + \".py\")\n             if path.exists():\n@@ -148,7 +174,7 @@ class Command(ScrapyCommand):\n             return True\n \n         # a file with the same name exists in the target directory\n-        spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])\n+        spiders_module = import_module(self.settings[\"NEWSPIDER_MODULE\"])\n         spiders_dir = Path(cast(str, spiders_module.__file__)).parent\n         spiders_dir_abs = spiders_dir.resolve()\n         path = spiders_dir_abs / (name + \".py\")\n@@ -160,7 +186,9 @@ class Command(ScrapyCommand):\n \n     @property\n     def templates_dir(self) -> str:\n-        return str(Path(\n-            self.settings['TEMPLATES_DIR'] or Path(scrapy.__path__[0], 'templates'),\n-            'spiders'\n-        ))\n+        return str(\n+            Path(\n+                self.settings[\"TEMPLATES_DIR\"] or Path(scrapy.__path__[0], \"templates\"),\n+                \"spiders\",\n+            )\n+        )\n\n@@ -4,7 +4,7 @@ from scrapy.commands import ScrapyCommand\n class Command(ScrapyCommand):\n \n     requires_project = True\n-    default_settings = {'LOG_ENABLED': False}\n+    default_settings = {\"LOG_ENABLED\": False}\n \n     def short_desc(self):\n         return \"List available spiders\"\n\n@@ -32,28 +32,72 @@ class Command(BaseRunSpiderCommand):\n \n     def add_options(self, parser):\n         BaseRunSpiderCommand.add_options(self, parser)\n-        parser.add_argument(\"--spider\", dest=\"spider\", default=None,\n-                            help=\"use this spider without looking for one\")\n-        parser.add_argument(\"--pipelines\", action=\"store_true\",\n-                            help=\"process items through pipelines\")\n-        parser.add_argument(\"--nolinks\", dest=\"nolinks\", action=\"store_true\",\n-                            help=\"don't show links to follow (extracted requests)\")\n-        parser.add_argument(\"--noitems\", dest=\"noitems\", action=\"store_true\",\n-                            help=\"don't show scraped items\")\n-        parser.add_argument(\"--nocolour\", dest=\"nocolour\", action=\"store_true\",\n-                            help=\"avoid using pygments to colorize the output\")\n-        parser.add_argument(\"-r\", \"--rules\", dest=\"rules\", action=\"store_true\",\n-                            help=\"use CrawlSpider rules to discover the callback\")\n-        parser.add_argument(\"-c\", \"--callback\", dest=\"callback\",\n-                            help=\"use this callback for parsing, instead looking for a callback\")\n-        parser.add_argument(\"-m\", \"--meta\", dest=\"meta\",\n-                            help=\"inject extra meta into the Request, it must be a valid raw json string\")\n-        parser.add_argument(\"--cbkwargs\", dest=\"cbkwargs\",\n-                            help=\"inject extra callback kwargs into the Request, it must be a valid raw json string\")\n-        parser.add_argument(\"-d\", \"--depth\", dest=\"depth\", type=int, default=1,\n-                            help=\"maximum depth for parsing requests [default: %(default)s]\")\n-        parser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\",\n-                            help=\"print each depth level one by one\")\n+        parser.add_argument(\n+            \"--spider\",\n+            dest=\"spider\",\n+            default=None,\n+            help=\"use this spider without looking for one\",\n+        )\n+        parser.add_argument(\n+            \"--pipelines\", action=\"store_true\", help=\"process items through pipelines\"\n+        )\n+        parser.add_argument(\n+            \"--nolinks\",\n+            dest=\"nolinks\",\n+            action=\"store_true\",\n+            help=\"don't show links to follow (extracted requests)\",\n+        )\n+        parser.add_argument(\n+            \"--noitems\",\n+            dest=\"noitems\",\n+            action=\"store_true\",\n+            help=\"don't show scraped items\",\n+        )\n+        parser.add_argument(\n+            \"--nocolour\",\n+            dest=\"nocolour\",\n+            action=\"store_true\",\n+            help=\"avoid using pygments to colorize the output\",\n+        )\n+        parser.add_argument(\n+            \"-r\",\n+            \"--rules\",\n+            dest=\"rules\",\n+            action=\"store_true\",\n+            help=\"use CrawlSpider rules to discover the callback\",\n+        )\n+        parser.add_argument(\n+            \"-c\",\n+            \"--callback\",\n+            dest=\"callback\",\n+            help=\"use this callback for parsing, instead looking for a callback\",\n+        )\n+        parser.add_argument(\n+            \"-m\",\n+            \"--meta\",\n+            dest=\"meta\",\n+            help=\"inject extra meta into the Request, it must be a valid raw json string\",\n+        )\n+        parser.add_argument(\n+            \"--cbkwargs\",\n+            dest=\"cbkwargs\",\n+            help=\"inject extra callback kwargs into the Request, it must be a valid raw json string\",\n+        )\n+        parser.add_argument(\n+            \"-d\",\n+            \"--depth\",\n+            dest=\"depth\",\n+            type=int,\n+            default=1,\n+            help=\"maximum depth for parsing requests [default: %(default)s]\",\n+        )\n+        parser.add_argument(\n+            \"-v\",\n+            \"--verbose\",\n+            dest=\"verbose\",\n+            action=\"store_true\",\n+            help=\"print each depth level one by one\",\n+        )\n \n     @property\n     def max_level(self):\n@@ -98,13 +142,13 @@ class Command(BaseRunSpiderCommand):\n \n         if opts.verbose:\n             for level in range(1, self.max_level + 1):\n-                print(f'\\n>>> DEPTH LEVEL: {level} <<<')\n+                print(f\"\\n>>> DEPTH LEVEL: {level} <<<\")\n                 if not opts.noitems:\n                     self.print_items(level, colour)\n                 if not opts.nolinks:\n                     self.print_requests(level, colour)\n         else:\n-            print(f'\\n>>> STATUS DEPTH LEVEL {self.max_level} <<<')\n+            print(f\"\\n>>> STATUS DEPTH LEVEL {self.max_level} <<<\")\n             if not opts.noitems:\n                 self.print_items(colour=colour)\n             if not opts.nolinks:\n@@ -125,14 +169,16 @@ class Command(BaseRunSpiderCommand):\n         return d\n \n     def get_callback_from_rules(self, spider, response):\n-        if getattr(spider, 'rules', None):\n+        if getattr(spider, \"rules\", None):\n             for rule in spider.rules:\n                 if rule.link_extractor.matches(response.url):\n                     return rule.callback or \"parse\"\n         else:\n-            logger.error('No CrawlSpider rules found in spider %(spider)r, '\n-                         'please specify a callback to use for parsing',\n-                         {'spider': spider.name})\n+            logger.error(\n+                \"No CrawlSpider rules found in spider %(spider)r, \"\n+                \"please specify a callback to use for parsing\",\n+                {\"spider\": spider.name},\n+            )\n \n     def set_spidercls(self, url, opts):\n         spider_loader = self.crawler_process.spider_loader\n@@ -140,15 +186,17 @@ class Command(BaseRunSpiderCommand):\n             try:\n                 self.spidercls = spider_loader.load(opts.spider)\n             except KeyError:\n-                logger.error('Unable to find spider: %(spider)s',\n-                             {'spider': opts.spider})\n+                logger.error(\n+                    \"Unable to find spider: %(spider)s\", {\"spider\": opts.spider}\n+                )\n         else:\n             self.spidercls = spidercls_for_request(spider_loader, Request(url))\n             if not self.spidercls:\n-                logger.error('Unable to find spider for: %(url)s', {'url': url})\n+                logger.error(\"Unable to find spider for: %(url)s\", {\"url\": url})\n \n         def _start_requests(spider):\n             yield self.prepare_request(spider, Request(url), opts)\n+\n         if self.spidercls:\n             self.spidercls.start_requests = _start_requests\n \n@@ -158,8 +206,7 @@ class Command(BaseRunSpiderCommand):\n         self.crawler_process.start()\n \n         if not self.first_response:\n-            logger.error('No response downloaded for: %(url)s',\n-                         {'url': url})\n+            logger.error(\"No response downloaded for: %(url)s\", {\"url\": url})\n \n     def scraped_data(self, args):\n         items, requests, opts, depth, spider, callback = args\n@@ -173,8 +220,8 @@ class Command(BaseRunSpiderCommand):\n         scraped_data = items if opts.output else []\n         if depth < opts.depth:\n             for req in requests:\n-                req.meta['_depth'] = depth + 1\n-                req.meta['_callback'] = req.callback\n+                req.meta[\"_depth\"] = depth + 1\n+                req.meta[\"_callback\"] = req.callback\n                 req.callback = callback\n             scraped_data += requests\n \n@@ -187,7 +234,7 @@ class Command(BaseRunSpiderCommand):\n                 self.first_response = response\n \n             # determine real callback\n-            cb = response.meta['_callback']\n+            cb = response.meta[\"_callback\"]\n             if not cb:\n                 if opts.callback:\n                     cb = opts.callback\n@@ -195,23 +242,27 @@ class Command(BaseRunSpiderCommand):\n                     cb = self.get_callback_from_rules(spider, response)\n \n                     if not cb:\n-                        logger.error('Cannot find a rule that matches %(url)r in spider: %(spider)s',\n-                                     {'url': response.url, 'spider': spider.name})\n+                        logger.error(\n+                            \"Cannot find a rule that matches %(url)r in spider: %(spider)s\",\n+                            {\"url\": response.url, \"spider\": spider.name},\n+                        )\n                         return\n                 else:\n-                    cb = 'parse'\n+                    cb = \"parse\"\n \n             if not callable(cb):\n                 cb_method = getattr(spider, cb, None)\n                 if callable(cb_method):\n                     cb = cb_method\n                 else:\n-                    logger.error('Cannot find callback %(callback)r in spider: %(spider)s',\n-                                 {'callback': cb, 'spider': spider.name})\n+                    logger.error(\n+                        \"Cannot find callback %(callback)r in spider: %(spider)s\",\n+                        {\"callback\": cb, \"spider\": spider.name},\n+                    )\n                     return\n \n             # parse items and requests\n-            depth = response.meta['_depth']\n+            depth = response.meta[\"_depth\"]\n \n             d = self.run_callback(response, cb, cb_kwargs)\n             d.addCallback(self._get_items_and_requests, opts, depth, spider, callback)\n@@ -226,8 +277,8 @@ class Command(BaseRunSpiderCommand):\n         if opts.cbkwargs:\n             request.cb_kwargs.update(opts.cbkwargs)\n \n-        request.meta['_depth'] = 1\n-        request.meta['_callback'] = request.callback\n+        request.meta[\"_depth\"] = 1\n+        request.meta[\"_callback\"] = request.callback\n         request.callback = callback\n         return request\n \n@@ -242,16 +293,22 @@ class Command(BaseRunSpiderCommand):\n             try:\n                 opts.meta = json.loads(opts.meta)\n             except ValueError:\n-                raise UsageError(\"Invalid -m/--meta value, pass a valid json string to -m or --meta. \"\n-                                 \"Example: --meta='{\\\"foo\\\" : \\\"bar\\\"}'\", print_help=False)\n+                raise UsageError(\n+                    \"Invalid -m/--meta value, pass a valid json string to -m or --meta. \"\n+                    'Example: --meta=\\'{\"foo\" : \"bar\"}\\'',\n+                    print_help=False,\n+                )\n \n     def process_request_cb_kwargs(self, opts):\n         if opts.cbkwargs:\n             try:\n                 opts.cbkwargs = json.loads(opts.cbkwargs)\n             except ValueError:\n-                raise UsageError(\"Invalid --cbkwargs value, pass a valid json string to --cbkwargs. \"\n-                                 \"Example: --cbkwargs='{\\\"foo\\\" : \\\"bar\\\"}'\", print_help=False)\n+                raise UsageError(\n+                    \"Invalid --cbkwargs value, pass a valid json string to --cbkwargs. \"\n+                    'Example: --cbkwargs=\\'{\"foo\" : \"bar\"}\\'',\n+                    print_help=False,\n+                )\n \n     def run(self, args, opts):\n         # parse arguments\n\n@@ -12,7 +12,7 @@ from scrapy.commands import BaseRunSpiderCommand\n \n def _import_file(filepath: Union[str, PathLike]) -> ModuleType:\n     abspath = Path(filepath).resolve()\n-    if abspath.suffix not in ('.py', '.pyw'):\n+    if abspath.suffix not in (\".py\", \".pyw\"):\n         raise ValueError(f\"Not a Python source file: {abspath}\")\n     dirname = str(abspath.parent)\n     sys.path = [dirname] + sys.path\n@@ -26,7 +26,7 @@ def _import_file(filepath: Union[str, PathLike]) -> ModuleType:\n class Command(BaseRunSpiderCommand):\n \n     requires_project = False\n-    default_settings = {'SPIDER_LOADER_WARN_ONLY': True}\n+    default_settings = {\"SPIDER_LOADER_WARN_ONLY\": True}\n \n     def syntax(self):\n         return \"[options] <spider_file>\"\n\n@@ -7,8 +7,7 @@ from scrapy.settings import BaseSettings\n class Command(ScrapyCommand):\n \n     requires_project = False\n-    default_settings = {'LOG_ENABLED': False,\n-                        'SPIDER_LOADER_WARN_ONLY': True}\n+    default_settings = {\"LOG_ENABLED\": False, \"SPIDER_LOADER_WARN_ONLY\": True}\n \n     def syntax(self):\n         return \"[options]\"\n@@ -18,16 +17,33 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_argument(\"--get\", dest=\"get\", metavar=\"SETTING\",\n-                            help=\"print raw setting value\")\n-        parser.add_argument(\"--getbool\", dest=\"getbool\", metavar=\"SETTING\",\n-                            help=\"print setting value, interpreted as a boolean\")\n-        parser.add_argument(\"--getint\", dest=\"getint\", metavar=\"SETTING\",\n-                            help=\"print setting value, interpreted as an integer\")\n-        parser.add_argument(\"--getfloat\", dest=\"getfloat\", metavar=\"SETTING\",\n-                            help=\"print setting value, interpreted as a float\")\n-        parser.add_argument(\"--getlist\", dest=\"getlist\", metavar=\"SETTING\",\n-                            help=\"print setting value, interpreted as a list\")\n+        parser.add_argument(\n+            \"--get\", dest=\"get\", metavar=\"SETTING\", help=\"print raw setting value\"\n+        )\n+        parser.add_argument(\n+            \"--getbool\",\n+            dest=\"getbool\",\n+            metavar=\"SETTING\",\n+            help=\"print setting value, interpreted as a boolean\",\n+        )\n+        parser.add_argument(\n+            \"--getint\",\n+            dest=\"getint\",\n+            metavar=\"SETTING\",\n+            help=\"print setting value, interpreted as an integer\",\n+        )\n+        parser.add_argument(\n+            \"--getfloat\",\n+            dest=\"getfloat\",\n+            metavar=\"SETTING\",\n+            help=\"print setting value, interpreted as a float\",\n+        )\n+        parser.add_argument(\n+            \"--getlist\",\n+            dest=\"getlist\",\n+            metavar=\"SETTING\",\n+            help=\"print setting value, interpreted as a list\",\n+        )\n \n     def run(self, args, opts):\n         settings = self.crawler_process.settings\n\n@@ -16,9 +16,9 @@ class Command(ScrapyCommand):\n \n     requires_project = False\n     default_settings = {\n-        'KEEP_ALIVE': True,\n-        'LOGSTATS_INTERVAL': 0,\n-        'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n+        \"KEEP_ALIVE\": True,\n+        \"LOGSTATS_INTERVAL\": 0,\n+        \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n     }\n \n     def syntax(self):\n@@ -28,17 +28,26 @@ class Command(ScrapyCommand):\n         return \"Interactive scraping console\"\n \n     def long_desc(self):\n-        return (\"Interactive console for scraping the given url or file. \"\n-                \"Use ./file.html syntax or full path for local file.\")\n+        return (\n+            \"Interactive console for scraping the given url or file. \"\n+            \"Use ./file.html syntax or full path for local file.\"\n+        )\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_argument(\"-c\", dest=\"code\",\n-                            help=\"evaluate the code in the shell, print the result and exit\")\n-        parser.add_argument(\"--spider\", dest=\"spider\",\n-                            help=\"use this spider\")\n-        parser.add_argument(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", default=False,\n-                            help=\"do not handle HTTP 3xx status codes and print response as-is\")\n+        parser.add_argument(\n+            \"-c\",\n+            dest=\"code\",\n+            help=\"evaluate the code in the shell, print the result and exit\",\n+        )\n+        parser.add_argument(\"--spider\", dest=\"spider\", help=\"use this spider\")\n+        parser.add_argument(\n+            \"--no-redirect\",\n+            dest=\"no_redirect\",\n+            action=\"store_true\",\n+            default=False,\n+            help=\"do not handle HTTP 3xx status codes and print response as-is\",\n+        )\n \n     def update_vars(self, vars):\n         \"\"\"You can use this function to update the Scrapy objects that will be\n@@ -58,8 +67,9 @@ class Command(ScrapyCommand):\n         if opts.spider:\n             spidercls = spider_loader.load(opts.spider)\n         elif url:\n-            spidercls = spidercls_for_request(spider_loader, Request(url),\n-                                              spidercls, log_multiple=True)\n+            spidercls = spidercls_for_request(\n+                spider_loader, Request(url), spidercls, log_multiple=True\n+            )\n \n         # The crawler is created this way since the Shell manually handles the\n         # crawling engine, so the set up in the crawl method won't work\n@@ -74,7 +84,9 @@ class Command(ScrapyCommand):\n         shell.start(url=url, redirect=not opts.no_redirect)\n \n     def _start_crawler_thread(self):\n-        t = Thread(target=self.crawler_process.start,\n-                   kwargs={'stop_after_crawl': False, 'install_signal_handlers': False})\n+        t = Thread(\n+            target=self.crawler_process.start,\n+            kwargs={\"stop_after_crawl\": False, \"install_signal_handlers\": False},\n+        )\n         t.daemon = True\n         t.start()\n\n@@ -13,14 +13,14 @@ from scrapy.exceptions import UsageError\n \n \n TEMPLATES_TO_RENDER = (\n-    ('scrapy.cfg',),\n-    ('${project_name}', 'settings.py.tmpl'),\n-    ('${project_name}', 'items.py.tmpl'),\n-    ('${project_name}', 'pipelines.py.tmpl'),\n-    ('${project_name}', 'middlewares.py.tmpl'),\n+    (\"scrapy.cfg\",),\n+    (\"${project_name}\", \"settings.py.tmpl\"),\n+    (\"${project_name}\", \"items.py.tmpl\"),\n+    (\"${project_name}\", \"pipelines.py.tmpl\"),\n+    (\"${project_name}\", \"middlewares.py.tmpl\"),\n )\n \n-IGNORE = ignore_patterns('*.pyc', '__pycache__', '.svn')\n+IGNORE = ignore_patterns(\"*.pyc\", \"__pycache__\", \".svn\")\n \n \n def _make_writable(path):\n@@ -31,8 +31,7 @@ def _make_writable(path):\n class Command(ScrapyCommand):\n \n     requires_project = False\n-    default_settings = {'LOG_ENABLED': False,\n-                        'SPIDER_LOADER_WARN_ONLY': True}\n+    default_settings = {\"LOG_ENABLED\": False, \"SPIDER_LOADER_WARN_ONLY\": True}\n \n     def syntax(self):\n         return \"<project_name> [project_dir]\"\n@@ -45,11 +44,13 @@ class Command(ScrapyCommand):\n             spec = find_spec(module_name)\n             return spec is not None and spec.loader is not None\n \n-        if not re.search(r'^[_a-zA-Z]\\w*$', project_name):\n-            print('Error: Project names must begin with a letter and contain'\n-                  ' only\\nletters, numbers and underscores')\n+        if not re.search(r\"^[_a-zA-Z]\\w*$\", project_name):\n+            print(\n+                \"Error: Project names must begin with a letter and contain\"\n+                \" only\\nletters, numbers and underscores\"\n+            )\n         elif _module_exists(project_name):\n-            print(f'Error: Module {project_name!r} already exists')\n+            print(f\"Error: Module {project_name!r} already exists\")\n         else:\n             return True\n         return False\n@@ -96,9 +97,9 @@ class Command(ScrapyCommand):\n         else:\n             project_dir = Path(args[0])\n \n-        if (project_dir / 'scrapy.cfg').exists():\n+        if (project_dir / \"scrapy.cfg\").exists():\n             self.exitcode = 1\n-            print(f'Error: scrapy.cfg already exists in {project_dir.resolve()}')\n+            print(f\"Error: scrapy.cfg already exists in {project_dir.resolve()}\")\n             return\n \n         if not self._is_valid_name(project_name):\n@@ -106,12 +107,24 @@ class Command(ScrapyCommand):\n             return\n \n         self._copytree(Path(self.templates_dir), project_dir.resolve())\n-        move(project_dir / 'module', project_dir / project_name)\n+        move(project_dir / \"module\", project_dir / project_name)\n         for paths in TEMPLATES_TO_RENDER:\n-            tplfile = Path(project_dir, *(string.Template(s).substitute(project_name=project_name) for s in paths))\n-            render_templatefile(tplfile, project_name=project_name, ProjectName=string_camelcase(project_name))\n-        print(f\"New Scrapy project '{project_name}', using template directory \"\n-              f\"'{self.templates_dir}', created in:\")\n+            tplfile = Path(\n+                project_dir,\n+                *(\n+                    string.Template(s).substitute(project_name=project_name)\n+                    for s in paths\n+                ),\n+            )\n+            render_templatefile(\n+                tplfile,\n+                project_name=project_name,\n+                ProjectName=string_camelcase(project_name),\n+            )\n+        print(\n+            f\"New Scrapy project '{project_name}', using template directory \"\n+            f\"'{self.templates_dir}', created in:\"\n+        )\n         print(f\"    {project_dir.resolve()}\\n\")\n         print(\"You can start your first spider with:\")\n         print(f\"    cd {project_dir}\")\n@@ -119,7 +132,9 @@ class Command(ScrapyCommand):\n \n     @property\n     def templates_dir(self) -> str:\n-        return str(Path(\n-            self.settings['TEMPLATES_DIR'] or Path(scrapy.__path__[0], 'templates'),\n-            'project'\n-        ))\n+        return str(\n+            Path(\n+                self.settings[\"TEMPLATES_DIR\"] or Path(scrapy.__path__[0], \"templates\"),\n+                \"project\",\n+            )\n+        )\n\n@@ -5,8 +5,7 @@ from scrapy.utils.versions import scrapy_components_versions\n \n class Command(ScrapyCommand):\n \n-    default_settings = {'LOG_ENABLED': False,\n-                        'SPIDER_LOADER_WARN_ONLY': True}\n+    default_settings = {\"LOG_ENABLED\": False, \"SPIDER_LOADER_WARN_ONLY\": True}\n \n     def syntax(self):\n         return \"[-v]\"\n@@ -16,8 +15,13 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_argument(\"--verbose\", \"-v\", dest=\"verbose\", action=\"store_true\",\n-                            help=\"also display twisted/python/platform info (useful for bug reports)\")\n+        parser.add_argument(\n+            \"--verbose\",\n+            \"-v\",\n+            dest=\"verbose\",\n+            action=\"store_true\",\n+            help=\"also display twisted/python/platform info (useful for bug reports)\",\n+        )\n \n     def run(self, args, opts):\n         if opts.verbose:\n\n@@ -4,16 +4,17 @@ from scrapy.utils.response import open_in_browser\n \n \n class Command(fetch.Command):\n-\n     def short_desc(self):\n         return \"Open URL in browser, as seen by Scrapy\"\n \n     def long_desc(self):\n-        return \"Fetch a URL using the Scrapy downloader and show its contents in a browser\"\n+        return (\n+            \"Fetch a URL using the Scrapy downloader and show its contents in a browser\"\n+        )\n \n     def add_options(self, parser):\n         super().add_options(parser)\n-        parser.add_argument('--headers', help=argparse.SUPPRESS)\n+        parser.add_argument(\"--headers\", help=argparse.SUPPRESS)\n \n     def _print_response(self, response, opts):\n         open_in_browser(response)\n\n@@ -12,15 +12,16 @@ from scrapy.utils.spider import iterate_spider_output\n \n class Contract:\n     \"\"\"Abstract class for contracts\"\"\"\n+\n     request_cls = None\n \n     def __init__(self, method, *args):\n-        self.testcase_pre = _create_testcase(method, f'@{self.name} pre-hook')\n-        self.testcase_post = _create_testcase(method, f'@{self.name} post-hook')\n+        self.testcase_pre = _create_testcase(method, f\"@{self.name} pre-hook\")\n+        self.testcase_post = _create_testcase(method, f\"@{self.name} post-hook\")\n         self.args = args\n \n     def add_pre_hook(self, request, results):\n-        if hasattr(self, 'pre_process'):\n+        if hasattr(self, \"pre_process\"):\n             cb = request.callback\n \n             @wraps(cb)\n@@ -43,7 +44,7 @@ class Contract:\n         return request\n \n     def add_post_hook(self, request, results):\n-        if hasattr(self, 'post_process'):\n+        if hasattr(self, \"post_process\"):\n             cb = request.callback\n \n             @wraps(cb)\n@@ -88,12 +89,12 @@ class ContractsManager:\n \n     def extract_contracts(self, method):\n         contracts = []\n-        for line in method.__doc__.split('\\n'):\n+        for line in method.__doc__.split(\"\\n\"):\n             line = line.strip()\n \n-            if line.startswith('@'):\n-                name, args = re.match(r'@(\\w+)\\s*(.*)', line).groups()\n-                args = re.split(r'\\s+', args)\n+            if line.startswith(\"@\"):\n+                name, args = re.match(r\"@(\\w+)\\s*(.*)\", line).groups()\n+                args = re.split(r\"\\s+\", args)\n \n                 contracts.append(self.contracts[name](method, *args))\n \n@@ -106,7 +107,7 @@ class ContractsManager:\n             try:\n                 requests.append(self.from_method(bound_method, results))\n             except Exception:\n-                case = _create_testcase(bound_method, 'contract')\n+                case = _create_testcase(bound_method, \"contract\")\n                 results.addError(case, sys.exc_info())\n \n         return requests\n@@ -124,13 +125,13 @@ class ContractsManager:\n \n             # Don't filter requests to allow\n             # testing different callbacks on the same URL.\n-            kwargs['dont_filter'] = True\n-            kwargs['callback'] = method\n+            kwargs[\"dont_filter\"] = True\n+            kwargs[\"callback\"] = method\n \n             for contract in contracts:\n                 kwargs = contract.adjust_request_args(kwargs)\n \n-            args.remove('self')\n+            args.remove(\"self\")\n \n             # check if all positional arguments are defined in kwargs\n             if set(args).issubset(set(kwargs)):\n@@ -156,11 +157,11 @@ class ContractsManager:\n                 output = cb(response, **cb_kwargs)\n                 output = list(iterate_spider_output(output))\n             except Exception:\n-                case = _create_testcase(method, 'callback')\n+                case = _create_testcase(method, \"callback\")\n                 results.addError(case, sys.exc_info())\n \n         def eb_wrapper(failure):\n-            case = _create_testcase(method, 'errback')\n+            case = _create_testcase(method, \"errback\")\n             exc_info = failure.type, failure.value, failure.getTracebackObject()\n             results.addError(case, exc_info)\n \n@@ -175,6 +176,6 @@ def _create_testcase(method, desc):\n         def __str__(_self):\n             return f\"[{spider}] {method.__name__} ({desc})\"\n \n-    name = f'{spider}_{method.__name__}'\n+    name = f\"{spider}_{method.__name__}\"\n     setattr(ContractTestCase, name, lambda x: x)\n     return ContractTestCase(name)\n\n@@ -13,10 +13,10 @@ class UrlContract(Contract):\n     @url http://scrapy.org\n     \"\"\"\n \n-    name = 'url'\n+    name = \"url\"\n \n     def adjust_request_args(self, args):\n-        args['url'] = self.args[0]\n+        args[\"url\"] = self.args[0]\n         return args\n \n \n@@ -27,10 +27,10 @@ class CallbackKeywordArgumentsContract(Contract):\n     @cb_kwargs {\"arg1\": \"some value\"}\n     \"\"\"\n \n-    name = 'cb_kwargs'\n+    name = \"cb_kwargs\"\n \n     def adjust_request_args(self, args):\n-        args['cb_kwargs'] = json.loads(' '.join(self.args))\n+        args[\"cb_kwargs\"] = json.loads(\" \".join(self.args))\n         return args\n \n \n@@ -47,12 +47,12 @@ class ReturnsContract(Contract):\n     @returns request 0 10\n     \"\"\"\n \n-    name = 'returns'\n+    name = \"returns\"\n     object_type_verifiers = {\n-        'request': lambda x: isinstance(x, Request),\n-        'requests': lambda x: isinstance(x, Request),\n-        'item': is_item,\n-        'items': is_item,\n+        \"request\": lambda x: isinstance(x, Request),\n+        \"requests\": lambda x: isinstance(x, Request),\n+        \"item\": is_item,\n+        \"items\": is_item,\n     }\n \n     def __init__(self, *args, **kwargs):\n@@ -73,7 +73,7 @@ class ReturnsContract(Contract):\n         try:\n             self.max_bound = int(self.args[2])\n         except IndexError:\n-            self.max_bound = float('inf')\n+            self.max_bound = float(\"inf\")\n \n     def post_process(self, output):\n         occurrences = 0\n@@ -81,15 +81,17 @@ class ReturnsContract(Contract):\n             if self.obj_type_verifier(x):\n                 occurrences += 1\n \n-        assertion = (self.min_bound <= occurrences <= self.max_bound)\n+        assertion = self.min_bound <= occurrences <= self.max_bound\n \n         if not assertion:\n             if self.min_bound == self.max_bound:\n                 expected = self.min_bound\n             else:\n-                expected = f'{self.min_bound}..{self.max_bound}'\n+                expected = f\"{self.min_bound}..{self.max_bound}\"\n \n-            raise ContractFail(f\"Returned {occurrences} {self.obj_name}, expected {expected}\")\n+            raise ContractFail(\n+                f\"Returned {occurrences} {self.obj_name}, expected {expected}\"\n+            )\n \n \n class ScrapesContract(Contract):\n@@ -97,7 +99,7 @@ class ScrapesContract(Contract):\n     @scrapes page_name page_body\n     \"\"\"\n \n-    name = 'scrapes'\n+    name = \"scrapes\"\n \n     def post_process(self, output):\n         for x in output:\n\n@@ -41,9 +41,11 @@ class Slot:\n \n     def __repr__(self):\n         cls_name = self.__class__.__name__\n-        return (f\"{cls_name}(concurrency={self.concurrency!r}, \"\n+        return (\n+            f\"{cls_name}(concurrency={self.concurrency!r}, \"\n             f\"delay={self.delay:.2f}, \"\n-                f\"randomize_delay={self.randomize_delay!r})\")\n+            f\"randomize_delay={self.randomize_delay!r})\"\n+        )\n \n     def __str__(self):\n         return (\n@@ -56,11 +58,11 @@ class Slot:\n \n \n def _get_concurrency_delay(concurrency, spider, settings):\n-    delay = settings.getfloat('DOWNLOAD_DELAY')\n-    if hasattr(spider, 'download_delay'):\n+    delay = settings.getfloat(\"DOWNLOAD_DELAY\")\n+    if hasattr(spider, \"download_delay\"):\n         delay = spider.download_delay\n \n-    if hasattr(spider, 'max_concurrent_requests'):\n+    if hasattr(spider, \"max_concurrent_requests\"):\n         concurrency = spider.max_concurrent_requests\n \n     return concurrency, delay\n@@ -68,7 +70,7 @@ def _get_concurrency_delay(concurrency, spider, settings):\n \n class Downloader:\n \n-    DOWNLOAD_SLOT = 'download_slot'\n+    DOWNLOAD_SLOT = \"download_slot\"\n \n     def __init__(self, crawler):\n         self.settings = crawler.settings\n@@ -76,10 +78,10 @@ class Downloader:\n         self.slots = {}\n         self.active = set()\n         self.handlers = DownloadHandlers(crawler)\n-        self.total_concurrency = self.settings.getint('CONCURRENT_REQUESTS')\n-        self.domain_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n-        self.ip_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_IP')\n-        self.randomize_delay = self.settings.getbool('RANDOMIZE_DOWNLOAD_DELAY')\n+        self.total_concurrency = self.settings.getint(\"CONCURRENT_REQUESTS\")\n+        self.domain_concurrency = self.settings.getint(\"CONCURRENT_REQUESTS_PER_DOMAIN\")\n+        self.ip_concurrency = self.settings.getint(\"CONCURRENT_REQUESTS_PER_IP\")\n+        self.randomize_delay = self.settings.getbool(\"RANDOMIZE_DOWNLOAD_DELAY\")\n         self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n         self._slot_gc_loop = task.LoopingCall(self._slot_gc)\n         self._slot_gc_loop.start(60)\n@@ -99,7 +101,9 @@ class Downloader:\n     def _get_slot(self, request, spider):\n         key = self._get_slot_key(request, spider)\n         if key not in self.slots:\n-            conc = self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n+            conc = (\n+                self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n+            )\n             conc, delay = _get_concurrency_delay(conc, spider, self.settings)\n             self.slots[key] = Slot(conc, delay, self.randomize_delay)\n \n@@ -109,7 +113,7 @@ class Downloader:\n         if self.DOWNLOAD_SLOT in request.meta:\n             return request.meta[self.DOWNLOAD_SLOT]\n \n-        key = urlparse_cached(request).hostname or ''\n+        key = urlparse_cached(request).hostname or \"\"\n         if self.ip_concurrency:\n             key = dnscache.get(key, key)\n \n@@ -124,9 +128,9 @@ class Downloader:\n             return response\n \n         slot.active.add(request)\n-        self.signals.send_catch_log(signal=signals.request_reached_downloader,\n-                                    request=request,\n-                                    spider=spider)\n+        self.signals.send_catch_log(\n+            signal=signals.request_reached_downloader, request=request, spider=spider\n+        )\n         deferred = defer.Deferred().addBoth(_deactivate)\n         slot.queue.append((request, deferred))\n         self._process_queue(spider, slot)\n@@ -134,6 +138,7 @@ class Downloader:\n \n     def _process_queue(self, spider, slot):\n         from twisted.internet import reactor\n+\n         if slot.latercall and slot.latercall.active():\n             return\n \n@@ -143,7 +148,9 @@ class Downloader:\n         if delay:\n             penalty = delay - now + slot.lastseen\n             if penalty > 0:\n-                slot.latercall = reactor.callLater(penalty, self._process_queue, spider, slot)\n+                slot.latercall = reactor.callLater(\n+                    penalty, self._process_queue, spider, slot\n+                )\n                 return\n \n         # Process enqueued requests if there are free slots to transfer for this slot\n@@ -166,11 +173,14 @@ class Downloader:\n         # 2. Notify response_downloaded listeners about the recent download\n         # before querying queue for next request\n         def _downloaded(response):\n-            self.signals.send_catch_log(signal=signals.response_downloaded,\n+            self.signals.send_catch_log(\n+                signal=signals.response_downloaded,\n                 response=response,\n                 request=request,\n-                                        spider=spider)\n+                spider=spider,\n+            )\n             return response\n+\n         dfd.addCallback(_downloaded)\n \n         # 3. After response arrives, remove the request from transferring\n@@ -182,9 +192,9 @@ class Downloader:\n         def finish_transferring(_):\n             slot.transferring.remove(request)\n             self._process_queue(spider, slot)\n-            self.signals.send_catch_log(signal=signals.request_left_downloader,\n-                                        request=request,\n-                                        spider=spider)\n+            self.signals.send_catch_log(\n+                signal=signals.request_left_downloader, request=request, spider=spider\n+            )\n             return _\n \n         return dfd.addBoth(finish_transferring)\n\n@@ -2,13 +2,22 @@ import warnings\n \n from OpenSSL import SSL\n from twisted.internet._sslverify import _setAcceptableProtocols\n-from twisted.internet.ssl import optionsForClientTLS, CertificateOptions, platformTrust, AcceptableCiphers\n+from twisted.internet.ssl import (\n+    optionsForClientTLS,\n+    CertificateOptions,\n+    platformTrust,\n+    AcceptableCiphers,\n+)\n from twisted.web.client import BrowserLikePolicyForHTTPS\n from twisted.web.iweb import IPolicyForHTTPS\n from zope.interface.declarations import implementer\n from zope.interface.verify import verifyObject\n \n-from scrapy.core.downloader.tls import DEFAULT_CIPHERS, openssl_methods, ScrapyClientTLSOptions\n+from scrapy.core.downloader.tls import (\n+    DEFAULT_CIPHERS,\n+    openssl_methods,\n+    ScrapyClientTLSOptions,\n+)\n from scrapy.utils.misc import create_instance, load_object\n \n \n@@ -24,7 +33,14 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n      understand the TLSv1, TLSv1.1 and TLSv1.2 protocols.'\n     \"\"\"\n \n-    def __init__(self, method=SSL.SSLv23_METHOD, tls_verbose_logging=False, tls_ciphers=None, *args, **kwargs):\n+    def __init__(\n+        self,\n+        method=SSL.SSLv23_METHOD,\n+        tls_verbose_logging=False,\n+        tls_ciphers=None,\n+        *args,\n+        **kwargs,\n+    ):\n         super().__init__(*args, **kwargs)\n         self._ssl_method = method\n         self.tls_verbose_logging = tls_verbose_logging\n@@ -35,9 +51,15 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n \n     @classmethod\n     def from_settings(cls, settings, method=SSL.SSLv23_METHOD, *args, **kwargs):\n-        tls_verbose_logging = settings.getbool('DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING')\n-        tls_ciphers = settings['DOWNLOADER_CLIENT_TLS_CIPHERS']\n-        return cls(method=method, tls_verbose_logging=tls_verbose_logging, tls_ciphers=tls_ciphers, *args, **kwargs)\n+        tls_verbose_logging = settings.getbool(\"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\")\n+        tls_ciphers = settings[\"DOWNLOADER_CLIENT_TLS_CIPHERS\"]\n+        return cls(\n+            method=method,\n+            tls_verbose_logging=tls_verbose_logging,\n+            tls_ciphers=tls_ciphers,\n+            *args,\n+            **kwargs,\n+        )\n \n     def getCertificateOptions(self):\n         # setting verify=True will require you to provide CAs\n@@ -53,7 +75,7 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n         #   not calling super().__init__\n         return CertificateOptions(\n             verify=False,\n-            method=getattr(self, 'method', getattr(self, '_ssl_method', None)),\n+            method=getattr(self, \"method\", getattr(self, \"_ssl_method\", None)),\n             fixBrokenPeers=True,\n             acceptableCiphers=self.tls_ciphers,\n         )\n@@ -64,8 +86,11 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n         return self.getCertificateOptions().getContext()\n \n     def creatorForNetloc(self, hostname, port):\n-        return ScrapyClientTLSOptions(hostname.decode(\"ascii\"), self.getContext(),\n-                                      verbose_logging=self.tls_verbose_logging)\n+        return ScrapyClientTLSOptions(\n+            hostname.decode(\"ascii\"),\n+            self.getContext(),\n+            verbose_logging=self.tls_verbose_logging,\n+        )\n \n \n @implementer(IPolicyForHTTPS)\n@@ -95,7 +120,7 @@ class BrowserLikeContextFactory(ScrapyClientContextFactory):\n         return optionsForClientTLS(\n             hostname=hostname.decode(\"ascii\"),\n             trustRoot=platformTrust(),\n-            extraCertificateOptions={'method': self._ssl_method},\n+            extraCertificateOptions={\"method\": self._ssl_method},\n         )\n \n \n@@ -118,8 +143,8 @@ class AcceptableProtocolsContextFactory:\n \n \n def load_context_factory_from_settings(settings, crawler):\n-    ssl_method = openssl_methods[settings.get('DOWNLOADER_CLIENT_TLS_METHOD')]\n-    context_factory_cls = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n+    ssl_method = openssl_methods[settings.get(\"DOWNLOADER_CLIENT_TLS_METHOD\")]\n+    context_factory_cls = load_object(settings[\"DOWNLOADER_CLIENTCONTEXTFACTORY\"])\n     # try method-aware context factory\n     try:\n         context_factory = create_instance(\n\n@@ -15,14 +15,14 @@ logger = logging.getLogger(__name__)\n \n \n class DownloadHandlers:\n-\n     def __init__(self, crawler):\n         self._crawler = crawler\n         self._schemes = {}  # stores acceptable schemes on instancing\n         self._handlers = {}  # stores instanced handlers for schemes\n         self._notconfigured = {}  # remembers failed handlers\n         handlers = without_none_values(\n-            crawler.settings.getwithbase('DOWNLOAD_HANDLERS'))\n+            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")\n+        )\n         for scheme, clspath in handlers.items():\n             self._schemes[scheme] = clspath\n             self._load_handler(scheme, skip_lazy=True)\n@@ -38,7 +38,7 @@ class DownloadHandlers:\n         if scheme in self._notconfigured:\n             return None\n         if scheme not in self._schemes:\n-            self._notconfigured[scheme] = 'no handler available for that scheme'\n+            self._notconfigured[scheme] = \"no handler available for that scheme\"\n             return None\n \n         return self._load_handler(scheme)\n@@ -47,7 +47,7 @@ class DownloadHandlers:\n         path = self._schemes[scheme]\n         try:\n             dhcls = load_object(path)\n-            if skip_lazy and getattr(dhcls, 'lazy', True):\n+            if skip_lazy and getattr(dhcls, \"lazy\", True):\n                 return None\n             dh = create_instance(\n                 objcls=dhcls,\n@@ -58,9 +58,12 @@ class DownloadHandlers:\n             self._notconfigured[scheme] = str(ex)\n             return None\n         except Exception as ex:\n-            logger.error('Loading \"%(clspath)s\" for scheme \"%(scheme)s\"',\n+            logger.error(\n+                'Loading \"%(clspath)s\" for scheme \"%(scheme)s\"',\n                 {\"clspath\": path, \"scheme\": scheme},\n-                         exc_info=True, extra={'crawler': self._crawler})\n+                exc_info=True,\n+                extra={\"crawler\": self._crawler},\n+            )\n             self._notconfigured[scheme] = str(ex)\n             return None\n         else:\n@@ -71,11 +74,13 @@ class DownloadHandlers:\n         scheme = urlparse_cached(request).scheme\n         handler = self._get_handler(scheme)\n         if not handler:\n-            raise NotSupported(f\"Unsupported URL scheme '{scheme}': {self._notconfigured[scheme]}\")\n+            raise NotSupported(\n+                f\"Unsupported URL scheme '{scheme}': {self._notconfigured[scheme]}\"\n+            )\n         return handler.download_request(request, spider)\n \n     @defer.inlineCallbacks\n     def _close(self, *_a, **_kw):\n         for dh in self._handlers.values():\n-            if hasattr(dh, 'close'):\n+            if hasattr(dh, \"close\"):\n                 yield dh.close()\n\n@@ -14,9 +14,8 @@ class DataURIDownloadHandler:\n         respcls = responsetypes.from_mimetype(uri.media_type)\n \n         resp_kwargs = {}\n-        if (issubclass(respcls, TextResponse)\n-                and uri.media_type.split('/')[0] == 'text'):\n-            charset = uri.media_type_parameters.get('charset')\n-            resp_kwargs['encoding'] = charset\n+        if issubclass(respcls, TextResponse) and uri.media_type.split(\"/\")[0] == \"text\":\n+            charset = uri.media_type_parameters.get(\"charset\")\n+            resp_kwargs[\"encoding\"] = charset\n \n         return respcls(url=request.url, body=uri.data, **resp_kwargs)\n\n@@ -71,9 +71,9 @@ class FTPDownloadHandler:\n     }\n \n     def __init__(self, settings):\n-        self.default_user = settings['FTP_USER']\n-        self.default_password = settings['FTP_PASSWORD']\n-        self.passive_mode = settings['FTP_PASSIVE_MODE']\n+        self.default_user = settings[\"FTP_USER\"]\n+        self.default_password = settings[\"FTP_PASSWORD\"]\n+        self.passive_mode = settings[\"FTP_PASSIVE_MODE\"]\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -81,12 +81,16 @@ class FTPDownloadHandler:\n \n     def download_request(self, request, spider):\n         from twisted.internet import reactor\n+\n         parsed_url = urlparse_cached(request)\n         user = request.meta.get(\"ftp_user\", self.default_user)\n         password = request.meta.get(\"ftp_password\", self.default_password)\n-        passive_mode = 1 if bool(request.meta.get(\"ftp_passive\",\n-                                                  self.passive_mode)) else 0\n-        creator = ClientCreator(reactor, FTPClient, user, password, passive=passive_mode)\n+        passive_mode = (\n+            1 if bool(request.meta.get(\"ftp_passive\", self.passive_mode)) else 0\n+        )\n+        creator = ClientCreator(\n+            reactor, FTPClient, user, password, passive=passive_mode\n+        )\n         dfd = creator.connectTCP(parsed_url.hostname, parsed_url.port or 21)\n         return dfd.addCallback(self.gotClient, request, unquote(parsed_url.path))\n \n@@ -103,7 +107,7 @@ class FTPDownloadHandler:\n     def _build_response(self, result, request, protocol):\n         self.result = result\n         protocol.close()\n-        headers = {\"local filename\": protocol.filename or '', \"size\": protocol.size}\n+        headers = {\"local filename\": protocol.filename or \"\", \"size\": protocol.size}\n         body = to_bytes(protocol.filename or protocol.body.read())\n         respcls = responsetypes.from_args(url=request.url, body=body)\n         return respcls(url=request.url, status=200, body=body, headers=headers)\n@@ -115,5 +119,7 @@ class FTPDownloadHandler:\n             if m:\n                 ftpcode = m.group()\n                 httpcode = self.CODE_MAPPING.get(ftpcode, self.CODE_MAPPING[\"default\"])\n-                return Response(url=request.url, status=httpcode, body=to_bytes(message))\n+                return Response(\n+                    url=request.url, status=httpcode, body=to_bytes(message)\n+                )\n         raise result.type(result.value)\n\n@@ -8,8 +8,10 @@ class HTTP10DownloadHandler:\n     lazy = False\n \n     def __init__(self, settings, crawler=None):\n-        self.HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n-        self.ClientContextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n+        self.HTTPClientFactory = load_object(settings[\"DOWNLOADER_HTTPCLIENTFACTORY\"])\n+        self.ClientContextFactory = load_object(\n+            settings[\"DOWNLOADER_CLIENTCONTEXTFACTORY\"]\n+        )\n         self._settings = settings\n         self._crawler = crawler\n \n@@ -25,8 +27,9 @@ class HTTP10DownloadHandler:\n \n     def _connect(self, factory):\n         from twisted.internet import reactor\n+\n         host, port = to_unicode(factory.host), factory.port\n-        if factory.scheme == b'https':\n+        if factory.scheme == b\"https\":\n             client_context_factory = create_instance(\n                 objcls=self.ClientContextFactory,\n                 settings=self._settings,\n\n@@ -12,7 +12,13 @@ from twisted.internet import defer, protocol, ssl\n from twisted.internet.endpoints import TCP4ClientEndpoint\n from twisted.internet.error import TimeoutError\n from twisted.python.failure import Failure\n-from twisted.web.client import Agent, HTTPConnectionPool, ResponseDone, ResponseFailed, URI\n+from twisted.web.client import (\n+    Agent,\n+    HTTPConnectionPool,\n+    ResponseDone,\n+    ResponseFailed,\n+    URI,\n+)\n from twisted.web.http import _DataLoss, PotentialDataLoss\n from twisted.web.http_headers import Headers as TxHeaders\n from twisted.web.iweb import IBodyProducer, UNKNOWN_LENGTH\n@@ -36,14 +42,17 @@ class HTTP11DownloadHandler:\n         self._crawler = crawler\n \n         from twisted.internet import reactor\n+\n         self._pool = HTTPConnectionPool(reactor, persistent=True)\n-        self._pool.maxPersistentPerHost = settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n+        self._pool.maxPersistentPerHost = settings.getint(\n+            \"CONCURRENT_REQUESTS_PER_DOMAIN\"\n+        )\n         self._pool._factory.noisy = False\n \n         self._contextFactory = load_context_factory_from_settings(settings, crawler)\n-        self._default_maxsize = settings.getint('DOWNLOAD_MAXSIZE')\n-        self._default_warnsize = settings.getint('DOWNLOAD_WARNSIZE')\n-        self._fail_on_dataloss = settings.getbool('DOWNLOAD_FAIL_ON_DATALOSS')\n+        self._default_maxsize = settings.getint(\"DOWNLOAD_MAXSIZE\")\n+        self._default_warnsize = settings.getint(\"DOWNLOAD_WARNSIZE\")\n+        self._fail_on_dataloss = settings.getbool(\"DOWNLOAD_FAIL_ON_DATALOSS\")\n         self._disconnect_timeout = 1\n \n     @classmethod\n@@ -55,8 +64,8 @@ class HTTP11DownloadHandler:\n         agent = ScrapyAgent(\n             contextFactory=self._contextFactory,\n             pool=self._pool,\n-            maxsize=getattr(spider, 'download_maxsize', self._default_maxsize),\n-            warnsize=getattr(spider, 'download_warnsize', self._default_warnsize),\n+            maxsize=getattr(spider, \"download_maxsize\", self._default_maxsize),\n+            warnsize=getattr(spider, \"download_warnsize\", self._default_warnsize),\n             fail_on_dataloss=self._fail_on_dataloss,\n             crawler=self._crawler,\n         )\n@@ -64,6 +73,7 @@ class HTTP11DownloadHandler:\n \n     def close(self):\n         from twisted.internet import reactor\n+\n         d = self._pool.closeCachedConnections()\n         # closeCachedConnections will hang on network or server issues, so\n         # we'll manually timeout the deferred.\n@@ -96,11 +106,23 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     with this endpoint comes from the pool and a CONNECT has already been issued\n     for it.\n     \"\"\"\n+\n     _truncatedLength = 1000\n-    _responseAnswer = r'HTTP/1\\.. (?P<status>\\d{3})(?P<reason>.{,' + str(_truncatedLength) + r'})'\n+    _responseAnswer = (\n+        r\"HTTP/1\\.. (?P<status>\\d{3})(?P<reason>.{,\" + str(_truncatedLength) + r\"})\"\n+    )\n     _responseMatcher = re.compile(_responseAnswer.encode())\n \n-    def __init__(self, reactor, host, port, proxyConf, contextFactory, timeout=30, bindAddress=None):\n+    def __init__(\n+        self,\n+        reactor,\n+        host,\n+        port,\n+        proxyConf,\n+        contextFactory,\n+        timeout=30,\n+        bindAddress=None,\n+    ):\n         proxyHost, proxyPort, self._proxyAuthHeader = proxyConf\n         super().__init__(reactor, proxyHost, proxyPort, timeout, bindAddress)\n         self._tunnelReadyDeferred = defer.Deferred()\n@@ -111,7 +133,9 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n \n     def requestTunnel(self, protocol):\n         \"\"\"Asks the proxy to open a tunnel.\"\"\"\n-        tunnelReq = tunnel_request_data(self._tunneledHost, self._tunneledPort, self._proxyAuthHeader)\n+        tunnelReq = tunnel_request_data(\n+            self._tunneledHost, self._tunneledPort, self._proxyAuthHeader\n+        )\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse\n@@ -129,24 +153,30 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n         # from the proxy so that we don't send those bytes to the TLS layer\n         #\n         # see https://github.com/scrapy/scrapy/issues/2491\n-        if b'\\r\\n\\r\\n' not in self._connectBuffer:\n+        if b\"\\r\\n\\r\\n\" not in self._connectBuffer:\n             return\n         self._protocol.dataReceived = self._protocolDataReceived\n         respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(self._connectBuffer)\n-        if respm and int(respm.group('status')) == 200:\n+        if respm and int(respm.group(\"status\")) == 200:\n             # set proper Server Name Indication extension\n-            sslOptions = self._contextFactory.creatorForNetloc(self._tunneledHost, self._tunneledPort)\n+            sslOptions = self._contextFactory.creatorForNetloc(\n+                self._tunneledHost, self._tunneledPort\n+            )\n             self._protocol.transport.startTLS(sslOptions, self._protocolFactory)\n             self._tunnelReadyDeferred.callback(self._protocol)\n         else:\n             if respm:\n-                extra = {'status': int(respm.group('status')),\n-                         'reason': respm.group('reason').strip()}\n+                extra = {\n+                    \"status\": int(respm.group(\"status\")),\n+                    \"reason\": respm.group(\"reason\").strip(),\n+                }\n             else:\n                 extra = rcvd_bytes[: self._truncatedLength]\n             self._tunnelReadyDeferred.errback(\n-                TunnelError('Could not open CONNECT tunnel with proxy '\n-                            f'{self._host}:{self._port} [{extra!r}]')\n+                TunnelError(\n+                    \"Could not open CONNECT tunnel with proxy \"\n+                    f\"{self._host}:{self._port} [{extra!r}]\"\n+                )\n             )\n \n     def connectFailed(self, reason):\n@@ -173,12 +203,12 @@ def tunnel_request_data(host, port, proxy_auth_header=None):\n     >>> s(tunnel_request_data(b\"example.com\", \"8090\"))\n     'CONNECT example.com:8090 HTTP/1.1\\r\\nHost: example.com:8090\\r\\n\\r\\n'\n     \"\"\"\n-    host_value = to_bytes(host, encoding='ascii') + b':' + to_bytes(str(port))\n-    tunnel_req = b'CONNECT ' + host_value + b' HTTP/1.1\\r\\n'\n-    tunnel_req += b'Host: ' + host_value + b'\\r\\n'\n+    host_value = to_bytes(host, encoding=\"ascii\") + b\":\" + to_bytes(str(port))\n+    tunnel_req = b\"CONNECT \" + host_value + b\" HTTP/1.1\\r\\n\"\n+    tunnel_req += b\"Host: \" + host_value + b\"\\r\\n\"\n     if proxy_auth_header:\n-        tunnel_req += b'Proxy-Authorization: ' + proxy_auth_header + b'\\r\\n'\n-    tunnel_req += b'\\r\\n'\n+        tunnel_req += b\"Proxy-Authorization: \" + proxy_auth_header + b\"\\r\\n\"\n+    tunnel_req += b\"\\r\\n\"\n     return tunnel_req\n \n \n@@ -190,8 +220,15 @@ class TunnelingAgent(Agent):\n     proxy involved.\n     \"\"\"\n \n-    def __init__(self, reactor, proxyConf, contextFactory=None,\n-                 connectTimeout=None, bindAddress=None, pool=None):\n+    def __init__(\n+        self,\n+        reactor,\n+        proxyConf,\n+        contextFactory=None,\n+        connectTimeout=None,\n+        bindAddress=None,\n+        pool=None,\n+    ):\n         super().__init__(reactor, contextFactory, connectTimeout, bindAddress, pool)\n         self._proxyConf = proxyConf\n         self._contextFactory = contextFactory\n@@ -207,7 +244,9 @@ class TunnelingAgent(Agent):\n             bindAddress=self._endpointFactory._bindAddress,\n         )\n \n-    def _requestWithEndpoint(self, key, endpoint, method, parsedURI, headers, bodyProducer, requestPath):\n+    def _requestWithEndpoint(\n+        self, key, endpoint, method, parsedURI, headers, bodyProducer, requestPath\n+    ):\n         # proxy host and port are required for HTTP pool `key`\n         # otherwise, same remote host connection request could reuse\n         # a cached tunneled connection to a different proxy\n@@ -224,8 +263,9 @@ class TunnelingAgent(Agent):\n \n \n class ScrapyProxyAgent(Agent):\n-\n-    def __init__(self, reactor, proxyURI, connectTimeout=None, bindAddress=None, pool=None):\n+    def __init__(\n+        self, reactor, proxyURI, connectTimeout=None, bindAddress=None, pool=None\n+    ):\n         super().__init__(\n             reactor=reactor,\n             connectTimeout=connectTimeout,\n@@ -257,8 +297,17 @@ class ScrapyAgent:\n     _ProxyAgent = ScrapyProxyAgent\n     _TunnelingAgent = TunnelingAgent\n \n-    def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None,\n-                 maxsize=0, warnsize=0, fail_on_dataloss=True, crawler=None):\n+    def __init__(\n+        self,\n+        contextFactory=None,\n+        connectTimeout=10,\n+        bindAddress=None,\n+        pool=None,\n+        maxsize=0,\n+        warnsize=0,\n+        fail_on_dataloss=True,\n+        crawler=None,\n+    ):\n         self._contextFactory = contextFactory\n         self._connectTimeout = connectTimeout\n         self._bindAddress = bindAddress\n@@ -271,14 +320,15 @@ class ScrapyAgent:\n \n     def _get_agent(self, request, timeout):\n         from twisted.internet import reactor\n-        bindaddress = request.meta.get('bindaddress') or self._bindAddress\n-        proxy = request.meta.get('proxy')\n+\n+        bindaddress = request.meta.get(\"bindaddress\") or self._bindAddress\n+        proxy = request.meta.get(\"proxy\")\n         if proxy:\n             proxyScheme, proxyNetloc, proxyHost, proxyPort, proxyParams = _parse(proxy)\n             scheme = _parse(request.url)[0]\n             proxyHost = to_unicode(proxyHost)\n-            if scheme == b'https':\n-                proxyAuth = request.headers.get(b'Proxy-Authorization', None)\n+            if scheme == b\"https\":\n+                proxyAuth = request.headers.get(b\"Proxy-Authorization\", None)\n                 proxyConf = (proxyHost, proxyPort, proxyAuth)\n                 return self._TunnelingAgent(\n                     reactor=reactor,\n@@ -288,11 +338,11 @@ class ScrapyAgent:\n                     bindAddress=bindaddress,\n                     pool=self._pool,\n                 )\n-            proxyScheme = proxyScheme or b'http'\n-            proxyURI = urlunparse((proxyScheme, proxyNetloc, proxyParams, '', '', ''))\n+            proxyScheme = proxyScheme or b\"http\"\n+            proxyURI = urlunparse((proxyScheme, proxyNetloc, proxyParams, \"\", \"\", \"\"))\n             return self._ProxyAgent(\n                 reactor=reactor,\n-                proxyURI=to_bytes(proxyURI, encoding='ascii'),\n+                proxyURI=to_bytes(proxyURI, encoding=\"ascii\"),\n                 connectTimeout=timeout,\n                 bindAddress=bindaddress,\n                 pool=self._pool,\n@@ -308,7 +358,8 @@ class ScrapyAgent:\n \n     def download_request(self, request):\n         from twisted.internet import reactor\n-        timeout = request.meta.get('download_timeout') or self._connectTimeout\n+\n+        timeout = request.meta.get(\"download_timeout\") or self._connectTimeout\n         agent = self._get_agent(request, timeout)\n \n         # request details\n@@ -316,13 +367,15 @@ class ScrapyAgent:\n         method = to_bytes(request.method)\n         headers = TxHeaders(request.headers)\n         if isinstance(agent, self._TunnelingAgent):\n-            headers.removeHeader(b'Proxy-Authorization')\n+            headers.removeHeader(b\"Proxy-Authorization\")\n         if request.body:\n             bodyproducer = _RequestBodyProducer(request.body)\n         else:\n             bodyproducer = None\n         start_time = time()\n-        d = agent.request(method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n+        d = agent.request(\n+            method, to_bytes(url, encoding=\"ascii\"), headers, bodyproducer\n+        )\n         # set download latency\n         d.addCallback(self._cb_latency, request, start_time)\n         # response body is ready to be consumed\n@@ -345,14 +398,14 @@ class ScrapyAgent:\n         raise TimeoutError(f\"Getting {url} took longer than {timeout} seconds.\")\n \n     def _cb_latency(self, result, request, start_time):\n-        request.meta['download_latency'] = time() - start_time\n+        request.meta[\"download_latency\"] = time() - start_time\n         return result\n \n     @staticmethod\n     def _headers_from_twisted_response(response):\n         headers = Headers()\n         if response.length != UNKNOWN_LENGTH:\n-            headers[b'Content-Length'] = str(response.length).encode()\n+            headers[b\"Content-Length\"] = str(response.length).encode()\n         headers.update(response.headers.getAllRawHeaders())\n         return headers\n \n@@ -366,8 +419,10 @@ class ScrapyAgent:\n         )\n         for handler, result in headers_received_result:\n             if isinstance(result, Failure) and isinstance(result.value, StopDownload):\n-                logger.debug(\"Download stopped for %(request)s from signal handler %(handler)s\",\n-                             {\"request\": request, \"handler\": handler.__qualname__})\n+                logger.debug(\n+                    \"Download stopped for %(request)s from signal handler %(handler)s\",\n+                    {\"request\": request, \"handler\": handler.__qualname__},\n+                )\n                 txresponse._transport.stopProducing()\n                 txresponse._transport.loseConnection()\n                 return {\n@@ -389,15 +444,23 @@ class ScrapyAgent:\n                 \"ip_address\": None,\n             }\n \n-        maxsize = request.meta.get('download_maxsize', self._maxsize)\n-        warnsize = request.meta.get('download_warnsize', self._warnsize)\n+        maxsize = request.meta.get(\"download_maxsize\", self._maxsize)\n+        warnsize = request.meta.get(\"download_warnsize\", self._warnsize)\n         expected_size = txresponse.length if txresponse.length != UNKNOWN_LENGTH else -1\n-        fail_on_dataloss = request.meta.get('download_fail_on_dataloss', self._fail_on_dataloss)\n+        fail_on_dataloss = request.meta.get(\n+            \"download_fail_on_dataloss\", self._fail_on_dataloss\n+        )\n \n         if maxsize and expected_size > maxsize:\n-            warning_msg = (\"Cancelling download of %(url)s: expected response \"\n-                           \"size (%(size)s) larger than download max size (%(maxsize)s).\")\n-            warning_args = {'url': request.url, 'size': expected_size, 'maxsize': maxsize}\n+            warning_msg = (\n+                \"Cancelling download of %(url)s: expected response \"\n+                \"size (%(size)s) larger than download max size (%(maxsize)s).\"\n+            )\n+            warning_args = {\n+                \"url\": request.url,\n+                \"size\": expected_size,\n+                \"maxsize\": maxsize,\n+            }\n \n             logger.warning(warning_msg, warning_args)\n \n@@ -405,9 +468,11 @@ class ScrapyAgent:\n             raise defer.CancelledError(warning_msg % warning_args)\n \n         if warnsize and expected_size > warnsize:\n-            logger.warning(\"Expected response size (%(size)s) larger than \"\n+            logger.warning(\n+                \"Expected response size (%(size)s) larger than \"\n                 \"download warn size (%(warnsize)s) in request %(request)s.\",\n-                           {'size': expected_size, 'warnsize': warnsize, 'request': request})\n+                {\"size\": expected_size, \"warnsize\": warnsize, \"request\": request},\n+            )\n \n         def _cancel(_):\n             # Abort connection immediately.\n@@ -457,7 +522,6 @@ class ScrapyAgent:\n \n @implementer(IBodyProducer)\n class _RequestBodyProducer:\n-\n     def __init__(self, body):\n         self.body = body\n         self.length = len(body)\n@@ -474,8 +538,16 @@ class _RequestBodyProducer:\n \n \n class _ResponseReader(protocol.Protocol):\n-\n-    def __init__(self, finished, txresponse, request, maxsize, warnsize, fail_on_dataloss, crawler):\n+    def __init__(\n+        self,\n+        finished,\n+        txresponse,\n+        request,\n+        maxsize,\n+        warnsize,\n+        fail_on_dataloss,\n+        crawler,\n+    ):\n         self._finished = finished\n         self._txresponse = txresponse\n         self._request = request\n@@ -491,22 +563,28 @@ class _ResponseReader(protocol.Protocol):\n         self._crawler = crawler\n \n     def _finish_response(self, flags=None, failure=None):\n-        self._finished.callback({\n+        self._finished.callback(\n+            {\n                 \"txresponse\": self._txresponse,\n                 \"body\": self._bodybuf.getvalue(),\n                 \"flags\": flags,\n                 \"certificate\": self._certificate,\n                 \"ip_address\": self._ip_address,\n                 \"failure\": failure,\n-        })\n+            }\n+        )\n \n     def connectionMade(self):\n         if self._certificate is None:\n             with suppress(AttributeError):\n-                self._certificate = ssl.Certificate(self.transport._producer.getPeerCertificate())\n+                self._certificate = ssl.Certificate(\n+                    self.transport._producer.getPeerCertificate()\n+                )\n \n         if self._ip_address is None:\n-            self._ip_address = ipaddress.ip_address(self.transport._producer.getPeer().host)\n+            self._ip_address = ipaddress.ip_address(\n+                self.transport._producer.getPeer().host\n+            )\n \n     def dataReceived(self, bodyBytes):\n         # This maybe called several times after cancel was called with buffered data.\n@@ -524,29 +602,40 @@ class _ResponseReader(protocol.Protocol):\n         )\n         for handler, result in bytes_received_result:\n             if isinstance(result, Failure) and isinstance(result.value, StopDownload):\n-                logger.debug(\"Download stopped for %(request)s from signal handler %(handler)s\",\n-                             {\"request\": self._request, \"handler\": handler.__qualname__})\n+                logger.debug(\n+                    \"Download stopped for %(request)s from signal handler %(handler)s\",\n+                    {\"request\": self._request, \"handler\": handler.__qualname__},\n+                )\n                 self.transport.stopProducing()\n                 self.transport.loseConnection()\n                 failure = result if result.value.fail else None\n                 self._finish_response(flags=[\"download_stopped\"], failure=failure)\n \n         if self._maxsize and self._bytes_received > self._maxsize:\n-            logger.warning(\"Received (%(bytes)s) bytes larger than download \"\n+            logger.warning(\n+                \"Received (%(bytes)s) bytes larger than download \"\n                 \"max size (%(maxsize)s) in request %(request)s.\",\n-                           {'bytes': self._bytes_received,\n-                            'maxsize': self._maxsize,\n-                            'request': self._request})\n+                {\n+                    \"bytes\": self._bytes_received,\n+                    \"maxsize\": self._maxsize,\n+                    \"request\": self._request,\n+                },\n+            )\n             # Clear buffer earlier to avoid keeping data in memory for a long time.\n             self._bodybuf.truncate(0)\n             self._finished.cancel()\n \n-        if self._warnsize and self._bytes_received > self._warnsize and not self._reached_warnsize:\n+        if (\n+            self._warnsize\n+            and self._bytes_received > self._warnsize\n+            and not self._reached_warnsize\n+        ):\n             self._reached_warnsize = True\n-            logger.warning(\"Received more bytes than download \"\n+            logger.warning(\n+                \"Received more bytes than download \"\n                 \"warn size (%(warnsize)s) in request %(request)s.\",\n-                           {'warnsize': self._warnsize,\n-                            'request': self._request})\n+                {\"warnsize\": self._warnsize, \"request\": self._request},\n+            )\n \n     def connectionLost(self, reason):\n         if self._finished.called:\n@@ -560,16 +649,20 @@ class _ResponseReader(protocol.Protocol):\n             self._finish_response(flags=[\"partial\"])\n             return\n \n-        if reason.check(ResponseFailed) and any(r.check(_DataLoss) for r in reason.value.reasons):\n+        if reason.check(ResponseFailed) and any(\n+            r.check(_DataLoss) for r in reason.value.reasons\n+        ):\n             if not self._fail_on_dataloss:\n                 self._finish_response(flags=[\"dataloss\"])\n                 return\n \n             if not self._fail_on_dataloss_warned:\n-                logger.warning(\"Got data loss in %s. If you want to process broken \"\n+                logger.warning(\n+                    \"Got data loss in %s. If you want to process broken \"\n                     \"responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False\"\n                     \" -- This message won't be shown in further requests\",\n-                               self._txresponse.request.absoluteURI.decode())\n+                    self._txresponse.request.absoluteURI.decode(),\n+                )\n                 self._fail_on_dataloss_warned = True\n \n         self._finished.errback(reason)\n\n@@ -17,7 +17,9 @@ from scrapy.spiders import Spider\n from scrapy.utils.python import to_bytes\n \n \n-H2DownloadHandlerOrSubclass = TypeVar(\"H2DownloadHandlerOrSubclass\", bound=\"H2DownloadHandler\")\n+H2DownloadHandlerOrSubclass = TypeVar(\n+    \"H2DownloadHandlerOrSubclass\", bound=\"H2DownloadHandler\"\n+)\n \n \n class H2DownloadHandler:\n@@ -25,11 +27,14 @@ class H2DownloadHandler:\n         self._crawler = crawler\n \n         from twisted.internet import reactor\n+\n         self._pool = H2ConnectionPool(reactor, settings)\n         self._context_factory = load_context_factory_from_settings(settings, crawler)\n \n     @classmethod\n-    def from_crawler(cls: Type[H2DownloadHandlerOrSubclass], crawler: Crawler) -> H2DownloadHandlerOrSubclass:\n+    def from_crawler(\n+        cls: Type[H2DownloadHandlerOrSubclass], crawler: Crawler\n+    ) -> H2DownloadHandlerOrSubclass:\n         return cls(crawler.settings, crawler)\n \n     def download_request(self, request: Request, spider: Spider) -> Deferred:\n@@ -49,7 +54,8 @@ class ScrapyH2Agent:\n     _ProxyAgent = ScrapyProxyH2Agent\n \n     def __init__(\n-        self, context_factory,\n+        self,\n+        context_factory,\n         pool: H2ConnectionPool,\n         connect_timeout: int = 10,\n         bind_address: Optional[bytes] = None,\n@@ -63,19 +69,22 @@ class ScrapyH2Agent:\n \n     def _get_agent(self, request: Request, timeout: Optional[float]) -> H2Agent:\n         from twisted.internet import reactor\n-        bind_address = request.meta.get('bindaddress') or self._bind_address\n-        proxy = request.meta.get('proxy')\n+\n+        bind_address = request.meta.get(\"bindaddress\") or self._bind_address\n+        proxy = request.meta.get(\"proxy\")\n         if proxy:\n             _, _, proxy_host, proxy_port, proxy_params = _parse(proxy)\n             scheme = _parse(request.url)[0]\n \n-            if scheme == b'https':\n+            if scheme == b\"https\":\n                 # ToDo\n-                raise NotImplementedError('Tunneling via CONNECT method using HTTP/2.0 is not yet supported')\n+                raise NotImplementedError(\n+                    \"Tunneling via CONNECT method using HTTP/2.0 is not yet supported\"\n+                )\n             return self._ProxyAgent(\n                 reactor=reactor,\n                 context_factory=self._context_factory,\n-                proxy_uri=URI.fromBytes(to_bytes(proxy, encoding='ascii')),\n+                proxy_uri=URI.fromBytes(to_bytes(proxy, encoding=\"ascii\")),\n                 connect_timeout=timeout,\n                 bind_address=bind_address,\n                 pool=self._pool,\n@@ -91,7 +100,8 @@ class ScrapyH2Agent:\n \n     def download_request(self, request: Request, spider: Spider) -> Deferred:\n         from twisted.internet import reactor\n-        timeout = request.meta.get('download_timeout') or self._connect_timeout\n+\n+        timeout = request.meta.get(\"download_timeout\") or self._connect_timeout\n         agent = self._get_agent(request, timeout)\n \n         start_time = time()\n@@ -103,12 +113,16 @@ class ScrapyH2Agent:\n         return d\n \n     @staticmethod\n-    def _cb_latency(response: Response, request: Request, start_time: float) -> Response:\n-        request.meta['download_latency'] = time() - start_time\n+    def _cb_latency(\n+        response: Response, request: Request, start_time: float\n+    ) -> Response:\n+        request.meta[\"download_latency\"] = time() - start_time\n         return response\n \n     @staticmethod\n-    def _cb_timeout(response: Response, request: Request, timeout: float, timeout_cl: DelayedCall) -> Response:\n+    def _cb_timeout(\n+        response: Response, request: Request, timeout: float, timeout_cl: DelayedCall\n+    ) -> Response:\n         if timeout_cl.active():\n             timeout_cl.cancel()\n             return response\n\n@@ -6,40 +6,49 @@ from scrapy.utils.misc import create_instance\n \n \n class S3DownloadHandler:\n-\n-    def __init__(self, settings, *,\n+    def __init__(\n+        self,\n+        settings,\n+        *,\n         crawler=None,\n-                 aws_access_key_id=None, aws_secret_access_key=None,\n+        aws_access_key_id=None,\n+        aws_secret_access_key=None,\n         aws_session_token=None,\n-                 httpdownloadhandler=HTTPDownloadHandler, **kw):\n+        httpdownloadhandler=HTTPDownloadHandler,\n+        **kw,\n+    ):\n         if not is_botocore_available():\n-            raise NotConfigured('missing botocore library')\n+            raise NotConfigured(\"missing botocore library\")\n \n         if not aws_access_key_id:\n-            aws_access_key_id = settings['AWS_ACCESS_KEY_ID']\n+            aws_access_key_id = settings[\"AWS_ACCESS_KEY_ID\"]\n         if not aws_secret_access_key:\n-            aws_secret_access_key = settings['AWS_SECRET_ACCESS_KEY']\n+            aws_secret_access_key = settings[\"AWS_SECRET_ACCESS_KEY\"]\n         if not aws_session_token:\n-            aws_session_token = settings['AWS_SESSION_TOKEN']\n+            aws_session_token = settings[\"AWS_SESSION_TOKEN\"]\n \n         # If no credentials could be found anywhere,\n         # consider this an anonymous connection request by default;\n         # unless 'anon' was set explicitly (True/False).\n-        anon = kw.get('anon')\n+        anon = kw.get(\"anon\")\n         if anon is None and not aws_access_key_id and not aws_secret_access_key:\n-            kw['anon'] = True\n-        self.anon = kw.get('anon')\n+            kw[\"anon\"] = True\n+        self.anon = kw.get(\"anon\")\n \n         self._signer = None\n         import botocore.auth\n         import botocore.credentials\n-        kw.pop('anon', None)\n+\n+        kw.pop(\"anon\", None)\n         if kw:\n-            raise TypeError(f'Unexpected keyword arguments: {kw}')\n+            raise TypeError(f\"Unexpected keyword arguments: {kw}\")\n         if not self.anon:\n-            SignerCls = botocore.auth.AUTH_TYPE_MAPS['s3']\n-            self._signer = SignerCls(botocore.credentials.Credentials(\n-                aws_access_key_id, aws_secret_access_key, aws_session_token))\n+            SignerCls = botocore.auth.AUTH_TYPE_MAPS[\"s3\"]\n+            self._signer = SignerCls(\n+                botocore.credentials.Credentials(\n+                    aws_access_key_id, aws_secret_access_key, aws_session_token\n+                )\n+            )\n \n         _http_handler = create_instance(\n             objcls=httpdownloadhandler,\n@@ -54,20 +63,21 @@ class S3DownloadHandler:\n \n     def download_request(self, request, spider):\n         p = urlparse_cached(request)\n-        scheme = 'https' if request.meta.get('is_secure') else 'http'\n+        scheme = \"https\" if request.meta.get(\"is_secure\") else \"http\"\n         bucket = p.hostname\n-        path = p.path + '?' + p.query if p.query else p.path\n-        url = f'{scheme}://{bucket}.s3.amazonaws.com{path}'\n+        path = p.path + \"?\" + p.query if p.query else p.path\n+        url = f\"{scheme}://{bucket}.s3.amazonaws.com{path}\"\n         if self.anon:\n             request = request.replace(url=url)\n         else:\n             import botocore.awsrequest\n+\n             awsrequest = botocore.awsrequest.AWSRequest(\n                 method=request.method,\n-                url=f'{scheme}://s3.amazonaws.com/{bucket}{path}',\n+                url=f\"{scheme}://s3.amazonaws.com/{bucket}{path}\",\n                 headers=request.headers.to_unicode_dict(),\n-                data=request.body)\n+                data=request.body,\n+            )\n             self._signer.add_auth(awsrequest)\n-            request = request.replace(\n-                url=url, headers=awsrequest.headers.items())\n+            request = request.replace(url=url, headers=awsrequest.headers.items())\n         return self._download_http(request, spider)\n\n@@ -18,28 +18,31 @@ from scrapy.utils.conf import build_component_list\n \n class DownloaderMiddlewareManager(MiddlewareManager):\n \n-    component_name = 'downloader middleware'\n+    component_name = \"downloader middleware\"\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return build_component_list(\n-            settings.getwithbase('DOWNLOADER_MIDDLEWARES'))\n+        return build_component_list(settings.getwithbase(\"DOWNLOADER_MIDDLEWARES\"))\n \n     def _add_middleware(self, mw):\n-        if hasattr(mw, 'process_request'):\n-            self.methods['process_request'].append(mw.process_request)\n-        if hasattr(mw, 'process_response'):\n-            self.methods['process_response'].appendleft(mw.process_response)\n-        if hasattr(mw, 'process_exception'):\n-            self.methods['process_exception'].appendleft(mw.process_exception)\n+        if hasattr(mw, \"process_request\"):\n+            self.methods[\"process_request\"].append(mw.process_request)\n+        if hasattr(mw, \"process_response\"):\n+            self.methods[\"process_response\"].appendleft(mw.process_response)\n+        if hasattr(mw, \"process_exception\"):\n+            self.methods[\"process_exception\"].appendleft(mw.process_exception)\n \n     def download(self, download_func: Callable, request: Request, spider: Spider):\n         @defer.inlineCallbacks\n         def process_request(request: Request):\n-            for method in self.methods['process_request']:\n+            for method in self.methods[\"process_request\"]:\n                 method = cast(Callable, method)\n-                response = yield deferred_from_coro(method(request=request, spider=spider))\n-                if response is not None and not isinstance(response, (Response, Request)):\n+                response = yield deferred_from_coro(\n+                    method(request=request, spider=spider)\n+                )\n+                if response is not None and not isinstance(\n+                    response, (Response, Request)\n+                ):\n                     raise _InvalidOutput(\n                         f\"Middleware {method.__qualname__} must return None, Response or \"\n                         f\"Request, got {response.__class__.__name__}\"\n@@ -55,9 +58,11 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n             elif isinstance(response, Request):\n                 return response\n \n-            for method in self.methods['process_response']:\n+            for method in self.methods[\"process_response\"]:\n                 method = cast(Callable, method)\n-                response = yield deferred_from_coro(method(request=request, response=response, spider=spider))\n+                response = yield deferred_from_coro(\n+                    method(request=request, response=response, spider=spider)\n+                )\n                 if not isinstance(response, (Response, Request)):\n                     raise _InvalidOutput(\n                         f\"Middleware {method.__qualname__} must return Response or Request, \"\n@@ -70,10 +75,14 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n         @defer.inlineCallbacks\n         def process_exception(failure: Failure):\n             exception = failure.value\n-            for method in self.methods['process_exception']:\n+            for method in self.methods[\"process_exception\"]:\n                 method = cast(Callable, method)\n-                response = yield deferred_from_coro(method(request=request, exception=exception, spider=spider))\n-                if response is not None and not isinstance(response, (Response, Request)):\n+                response = yield deferred_from_coro(\n+                    method(request=request, exception=exception, spider=spider)\n+                )\n+                if response is not None and not isinstance(\n+                    response, (Response, Request)\n+                ):\n                     raise _InvalidOutput(\n                         f\"Middleware {method.__qualname__} must return None, Response or \"\n                         f\"Request, got {type(response)}\"\n\n@@ -2,7 +2,11 @@ import logging\n \n from OpenSSL import SSL\n from service_identity.exceptions import CertificateError\n-from twisted.internet._sslverify import ClientTLSOptions, verifyHostname, VerificationError\n+from twisted.internet._sslverify import (\n+    ClientTLSOptions,\n+    verifyHostname,\n+    VerificationError,\n+)\n from twisted.internet.ssl import AcceptableCiphers\n \n from scrapy.utils.ssl import x509name_to_string, get_temp_key_info\n@@ -10,17 +14,17 @@ from scrapy.utils.ssl import x509name_to_string, get_temp_key_info\n logger = logging.getLogger(__name__)\n \n \n-METHOD_TLS = 'TLS'\n-METHOD_TLSv10 = 'TLSv1.0'\n-METHOD_TLSv11 = 'TLSv1.1'\n-METHOD_TLSv12 = 'TLSv1.2'\n+METHOD_TLS = \"TLS\"\n+METHOD_TLSv10 = \"TLSv1.0\"\n+METHOD_TLSv11 = \"TLSv1.1\"\n+METHOD_TLSv12 = \"TLSv1.2\"\n \n \n openssl_methods = {\n     METHOD_TLS: SSL.SSLv23_METHOD,  # protocol negotiation (recommended)\n     METHOD_TLSv10: SSL.TLSv1_METHOD,  # TLS 1.0 only\n-    METHOD_TLSv11: getattr(SSL, 'TLSv1_1_METHOD', 5),   # TLS 1.1 only\n-    METHOD_TLSv12: getattr(SSL, 'TLSv1_2_METHOD', 6),   # TLS 1.2 only\n+    METHOD_TLSv11: getattr(SSL, \"TLSv1_1_METHOD\", 5),  # TLS 1.1 only\n+    METHOD_TLSv12: getattr(SSL, \"TLSv1_2_METHOD\", 6),  # TLS 1.2 only\n }\n \n \n@@ -44,32 +48,38 @@ class ScrapyClientTLSOptions(ClientTLSOptions):\n             connection.set_tlsext_host_name(self._hostnameBytes)\n         elif where & SSL.SSL_CB_HANDSHAKE_DONE:\n             if self.verbose_logging:\n-                logger.debug('SSL connection to %s using protocol %s, cipher %s',\n+                logger.debug(\n+                    \"SSL connection to %s using protocol %s, cipher %s\",\n                     self._hostnameASCII,\n                     connection.get_protocol_version_name(),\n                     connection.get_cipher_name(),\n                 )\n                 server_cert = connection.get_peer_certificate()\n-                logger.debug('SSL connection certificate: issuer \"%s\", subject \"%s\"',\n+                logger.debug(\n+                    'SSL connection certificate: issuer \"%s\", subject \"%s\"',\n                     x509name_to_string(server_cert.get_issuer()),\n                     x509name_to_string(server_cert.get_subject()),\n                 )\n                 key_info = get_temp_key_info(connection._ssl)\n                 if key_info:\n-                    logger.debug('SSL temp key: %s', key_info)\n+                    logger.debug(\"SSL temp key: %s\", key_info)\n \n             try:\n                 verifyHostname(connection, self._hostnameASCII)\n             except (CertificateError, VerificationError) as e:\n                 logger.warning(\n                     'Remote certificate is not valid for hostname \"%s\"; %s',\n-                    self._hostnameASCII, e)\n+                    self._hostnameASCII,\n+                    e,\n+                )\n \n             except ValueError as e:\n                 logger.warning(\n-                    'Ignoring error while verifying certificate '\n+                    \"Ignoring error while verifying certificate \"\n                     'from host \"%s\" (exception: %r)',\n-                    self._hostnameASCII, e)\n+                    self._hostnameASCII,\n+                    e,\n+                )\n \n \n-DEFAULT_CIPHERS = AcceptableCiphers.fromOpenSSLCipherString('DEFAULT')\n+DEFAULT_CIPHERS = AcceptableCiphers.fromOpenSSLCipherString(\"DEFAULT\")\n\n@@ -15,14 +15,14 @@ from scrapy.responsetypes import responsetypes\n def _parsed_url_args(parsed):\n     # Assume parsed is urlparse-d from Request.url,\n     # which was passed via safe_url_string and is ascii-only.\n-    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n+    path = urlunparse((\"\", \"\", parsed.path or \"/\", parsed.params, parsed.query, \"\"))\n     path = to_bytes(path, encoding=\"ascii\")\n     host = to_bytes(parsed.hostname, encoding=\"ascii\")\n     port = parsed.port\n     scheme = to_bytes(parsed.scheme, encoding=\"ascii\")\n     netloc = to_bytes(parsed.netloc, encoding=\"ascii\")\n     if port is None:\n-        port = 443 if scheme == b'https' else 80\n+        port = 443 if scheme == b\"https\" else 80\n     return scheme, netloc, host, port, path\n \n \n@@ -33,15 +33,15 @@ def _parse(url):\n     and is ascii-only.\n     \"\"\"\n     url = url.strip()\n-    if not re.match(r'^\\w+://', url):\n-        url = '//' + url\n+    if not re.match(r\"^\\w+://\", url):\n+        url = \"//\" + url\n     parsed = urlparse(url)\n     return _parsed_url_args(parsed)\n \n \n class ScrapyHTTPPageGetter(HTTPClient):\n \n-    delimiter = b'\\n'\n+    delimiter = b\"\\n\"\n \n     def connectionMade(self):\n         self.headers = Headers()  # bucket for response headers\n@@ -75,8 +75,8 @@ class ScrapyHTTPPageGetter(HTTPClient):\n         self.factory.noPage(reason)\n \n     def handleResponse(self, response):\n-        if self.factory.method.upper() == b'HEAD':\n-            self.factory.page(b'')\n+        if self.factory.method.upper() == b\"HEAD\":\n+            self.factory.page(b\"\")\n         elif self.length is not None and self.length > 0:\n             self.factory.noPage(self._connection_lost_reason)\n         else:\n@@ -87,12 +87,15 @@ class ScrapyHTTPPageGetter(HTTPClient):\n         self.transport.loseConnection()\n \n         # transport cleanup needed for HTTPS connections\n-        if self.factory.url.startswith(b'https'):\n+        if self.factory.url.startswith(b\"https\"):\n             self.transport.stopProducing()\n \n         self.factory.noPage(\n-            defer.TimeoutError(f\"Getting {self.factory.url} took longer \"\n-                               f\"than {self.factory.timeout} seconds.\"))\n+            defer.TimeoutError(\n+                f\"Getting {self.factory.url} took longer \"\n+                f\"than {self.factory.timeout} seconds.\"\n+            )\n+        )\n \n \n # This class used to inherit from Twisted’s\n@@ -109,16 +112,24 @@ class ScrapyHTTPClientFactory(ClientFactory):\n     afterFoundGet = False\n \n     def _build_response(self, body, request):\n-        request.meta['download_latency'] = self.headers_time - self.start_time\n+        request.meta[\"download_latency\"] = self.headers_time - self.start_time\n         status = int(self.status)\n         headers = Headers(self.response_headers)\n         respcls = responsetypes.from_args(headers=headers, url=self._url, body=body)\n-        return respcls(url=self._url, status=status, headers=headers, body=body, protocol=to_unicode(self.version))\n+        return respcls(\n+            url=self._url,\n+            status=status,\n+            headers=headers,\n+            body=body,\n+            protocol=to_unicode(self.version),\n+        )\n \n     def _set_connection_attributes(self, request):\n         parsed = urlparse_cached(request)\n-        self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(parsed)\n-        proxy = request.meta.get('proxy')\n+        self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(\n+            parsed\n+        )\n+        proxy = request.meta.get(\"proxy\")\n         if proxy:\n             self.scheme, _, self.host, self.port, _ = _parse(proxy)\n             self.path = self.url\n@@ -126,12 +137,12 @@ class ScrapyHTTPClientFactory(ClientFactory):\n     def __init__(self, request, timeout=180):\n         self._url = urldefrag(request.url)[0]\n         # converting to bytes to comply to Twisted interface\n-        self.url = to_bytes(self._url, encoding='ascii')\n-        self.method = to_bytes(request.method, encoding='ascii')\n+        self.url = to_bytes(self._url, encoding=\"ascii\")\n+        self.method = to_bytes(request.method, encoding=\"ascii\")\n         self.body = request.body or None\n         self.headers = Headers(request.headers)\n         self.response_headers = None\n-        self.timeout = request.meta.get('download_timeout') or timeout\n+        self.timeout = request.meta.get(\"download_timeout\") or timeout\n         self.start_time = time()\n         self.deferred = defer.Deferred().addCallback(self._build_response, request)\n \n@@ -146,16 +157,16 @@ class ScrapyHTTPClientFactory(ClientFactory):\n         self._set_connection_attributes(request)\n \n         # set Host header based on url\n-        self.headers.setdefault('Host', self.netloc)\n+        self.headers.setdefault(\"Host\", self.netloc)\n \n         # set Content-Length based len of body\n         if self.body is not None:\n-            self.headers['Content-Length'] = len(self.body)\n+            self.headers[\"Content-Length\"] = len(self.body)\n             # just in case a broken http/1.1 decides to keep connection alive\n             self.headers.setdefault(\"Connection\", \"close\")\n         # Content-Length must be specified in POST method even with no body\n-        elif self.method == b'POST':\n-            self.headers['Content-Length'] = 0\n+        elif self.method == b\"POST\":\n+            self.headers[\"Content-Length\"] = 0\n \n     def __repr__(self):\n         return f\"<{self.__class__.__name__}: {self.url}>\"\n@@ -171,6 +182,7 @@ class ScrapyHTTPClientFactory(ClientFactory):\n         p.afterFoundGet = self.afterFoundGet\n         if self.timeout:\n             from twisted.internet import reactor\n+\n             timeoutCall = reactor.callLater(self.timeout, p.timeout)\n             self.deferred.addBoth(self._cancelTimeout, timeoutCall)\n         return p\n\n@@ -79,13 +79,14 @@ class ExecutionEngine:\n         self.running = False\n         self.paused = False\n         self.scheduler_cls = self._get_scheduler_class(crawler.settings)\n-        downloader_cls = load_object(self.settings['DOWNLOADER'])\n+        downloader_cls = load_object(self.settings[\"DOWNLOADER\"])\n         self.downloader = downloader_cls(crawler)\n         self.scraper = Scraper(crawler)\n         self._spider_closed_callback = spider_closed_callback\n \n     def _get_scheduler_class(self, settings: BaseSettings) -> type:\n         from scrapy.core.scheduler import BaseScheduler\n+\n         scheduler_cls = load_object(settings[\"SCHEDULER\"])\n         if not issubclass(scheduler_cls, BaseScheduler):\n             raise TypeError(\n@@ -106,6 +107,7 @@ class ExecutionEngine:\n \n     def stop(self) -> Deferred:\n         \"\"\"Gracefully stop the execution engine\"\"\"\n+\n         @inlineCallbacks\n         def _finish_stopping_engine(_) -> Deferred:\n             yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n@@ -115,7 +117,11 @@ class ExecutionEngine:\n             raise RuntimeError(\"Engine not running\")\n \n         self.running = False\n-        dfd = self.close_spider(self.spider, reason=\"shutdown\") if self.spider is not None else succeed(None)\n+        dfd = (\n+            self.close_spider(self.spider, reason=\"shutdown\")\n+            if self.spider is not None\n+            else succeed(None)\n+        )\n         return dfd.addBoth(_finish_stopping_engine)\n \n     def close(self) -> Deferred:\n@@ -126,7 +132,9 @@ class ExecutionEngine:\n         if self.running:\n             return self.stop()  # will also close spider and downloader\n         if self.spider is not None:\n-            return self.close_spider(self.spider, reason=\"shutdown\")  # will also close downloader\n+            return self.close_spider(\n+                self.spider, reason=\"shutdown\"\n+            )  # will also close downloader\n         return succeed(self.downloader.close())\n \n     def pause(self) -> None:\n@@ -144,7 +152,10 @@ class ExecutionEngine:\n         if self.paused:\n             return None\n \n-        while not self._needs_backout() and self._next_request_from_scheduler() is not None:\n+        while (\n+            not self._needs_backout()\n+            and self._next_request_from_scheduler() is not None\n+        ):\n             pass\n \n         if self.slot.start_requests is not None and not self._needs_backout():\n@@ -154,7 +165,11 @@ class ExecutionEngine:\n                 self.slot.start_requests = None\n             except Exception:\n                 self.slot.start_requests = None\n-                logger.error('Error while obtaining start requests', exc_info=True, extra={'spider': self.spider})\n+                logger.error(\n+                    \"Error while obtaining start requests\",\n+                    exc_info=True,\n+                    extra={\"spider\": self.spider},\n+                )\n             else:\n                 self.crawl(request)\n \n@@ -179,18 +194,30 @@ class ExecutionEngine:\n \n         d = self._download(request, self.spider)\n         d.addBoth(self._handle_downloader_output, request)\n-        d.addErrback(lambda f: logger.info('Error while handling downloader output',\n+        d.addErrback(\n+            lambda f: logger.info(\n+                \"Error while handling downloader output\",\n                 exc_info=failure_to_exc_info(f),\n-                                           extra={'spider': self.spider}))\n+                extra={\"spider\": self.spider},\n+            )\n+        )\n         d.addBoth(lambda _: self.slot.remove_request(request))\n-        d.addErrback(lambda f: logger.info('Error while removing request from slot',\n+        d.addErrback(\n+            lambda f: logger.info(\n+                \"Error while removing request from slot\",\n                 exc_info=failure_to_exc_info(f),\n-                                           extra={'spider': self.spider}))\n+                extra={\"spider\": self.spider},\n+            )\n+        )\n         slot = self.slot\n         d.addBoth(lambda _: slot.nextcall.schedule())\n-        d.addErrback(lambda f: logger.info('Error while scheduling new request',\n+        d.addErrback(\n+            lambda f: logger.info(\n+                \"Error while scheduling new request\",\n                 exc_info=failure_to_exc_info(f),\n-                                           extra={'spider': self.spider}))\n+                extra={\"spider\": self.spider},\n+            )\n+        )\n         return d\n \n     def _handle_downloader_output(\n@@ -199,7 +226,9 @@ class ExecutionEngine:\n         assert self.spider is not None  # typing\n \n         if not isinstance(result, (Request, Response, Failure)):\n-            raise TypeError(f\"Incorrect type: expected Request, Response or Failure, got {type(result)}: {result!r}\")\n+            raise TypeError(\n+                f\"Incorrect type: expected Request, Response or Failure, got {type(result)}: {result!r}\"\n+            )\n \n         # downloader middleware can return requests (for example, redirects)\n         if isinstance(result, Request):\n@@ -211,7 +240,7 @@ class ExecutionEngine:\n             lambda f: logger.error(\n                 \"Error while enqueuing downloader output\",\n                 exc_info=failure_to_exc_info(f),\n-                extra={'spider': self.spider},\n+                extra={\"spider\": self.spider},\n             )\n         )\n         return d\n@@ -244,16 +273,22 @@ class ExecutionEngine:\n                 stacklevel=2,\n             )\n             if spider is not self.spider:\n-                raise RuntimeError(f\"The spider {spider.name!r} does not match the open spider\")\n+                raise RuntimeError(\n+                    f\"The spider {spider.name!r} does not match the open spider\"\n+                )\n         if self.spider is None:\n             raise RuntimeError(f\"No open spider to crawl: {request}\")\n         self._schedule_request(request, self.spider)\n         self.slot.nextcall.schedule()  # type: ignore[union-attr]\n \n     def _schedule_request(self, request: Request, spider: Spider) -> None:\n-        self.signals.send_catch_log(signals.request_scheduled, request=request, spider=spider)\n+        self.signals.send_catch_log(\n+            signals.request_scheduled, request=request, spider=spider\n+        )\n         if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]\n-            self.signals.send_catch_log(signals.request_dropped, request=request, spider=spider)\n+            self.signals.send_catch_log(\n+                signals.request_dropped, request=request, spider=spider\n+            )\n \n     def download(self, request: Request, spider: Optional[Spider] = None) -> Deferred:\n         \"\"\"Return a Deferred which fires with a Response as result, only downloader middlewares are applied\"\"\"\n@@ -264,10 +299,14 @@ class ExecutionEngine:\n                 stacklevel=2,\n             )\n             if spider is not self.spider:\n-                logger.warning(\"The spider '%s' does not match the open spider\", spider.name)\n+                logger.warning(\n+                    \"The spider '%s' does not match the open spider\", spider.name\n+                )\n         if self.spider is None:\n             raise RuntimeError(f\"No open spider to crawl: {request}\")\n-        return self._download(request, spider).addBoth(self._downloaded, request, spider)\n+        return self._download(request, spider).addBoth(\n+            self._downloaded, request, spider\n+        )\n \n     def _downloaded(\n         self, result: Union[Response, Request], request: Request, spider: Spider\n@@ -286,7 +325,9 @@ class ExecutionEngine:\n \n         def _on_success(result: Union[Response, Request]) -> Union[Response, Request]:\n             if not isinstance(result, (Response, Request)):\n-                raise TypeError(f\"Incorrect type: expected Response or Request, got {type(result)}: {result!r}\")\n+                raise TypeError(\n+                    f\"Incorrect type: expected Response or Request, got {type(result)}: {result!r}\"\n+                )\n             if isinstance(result, Response):\n                 if result.request is None:\n                     result.request = request\n@@ -311,13 +352,19 @@ class ExecutionEngine:\n         return dwld\n \n     @inlineCallbacks\n-    def open_spider(self, spider: Spider, start_requests: Iterable = (), close_if_idle: bool = True):\n+    def open_spider(\n+        self, spider: Spider, start_requests: Iterable = (), close_if_idle: bool = True\n+    ):\n         if self.slot is not None:\n             raise RuntimeError(f\"No free spider slot when opening {spider.name!r}\")\n-        logger.info(\"Spider opened\", extra={'spider': spider})\n+        logger.info(\"Spider opened\", extra={\"spider\": spider})\n         nextcall = CallLaterOnce(self._next_request)\n-        scheduler = create_instance(self.scheduler_cls, settings=None, crawler=self.crawler)\n-        start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)\n+        scheduler = create_instance(\n+            self.scheduler_cls, settings=None, crawler=self.crawler\n+        )\n+        start_requests = yield self.scraper.spidermw.process_start_requests(\n+            start_requests, spider\n+        )\n         self.slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n         self.spider = spider\n         if hasattr(scheduler, \"open\"):\n@@ -337,7 +384,9 @@ class ExecutionEngine:\n         \"\"\"\n         assert self.spider is not None  # typing\n         expected_ex = (DontCloseSpider, CloseSpider)\n-        res = self.signals.send_catch_log(signals.spider_idle, spider=self.spider, dont_log=expected_ex)\n+        res = self.signals.send_catch_log(\n+            signals.spider_idle, spider=self.spider, dont_log=expected_ex\n+        )\n         detected_ex = {\n             ex: x.value\n             for _, x in res\n@@ -347,7 +396,7 @@ class ExecutionEngine:\n         if DontCloseSpider in detected_ex:\n             return None\n         if self.spider_is_idle():\n-            ex = detected_ex.get(CloseSpider, CloseSpider(reason='finished'))\n+            ex = detected_ex.get(CloseSpider, CloseSpider(reason=\"finished\"))\n             assert isinstance(ex, CloseSpider)  # typing\n             self.close_spider(self.spider, reason=ex.reason)\n \n@@ -359,40 +408,55 @@ class ExecutionEngine:\n         if self.slot.closing is not None:\n             return self.slot.closing\n \n-        logger.info(\"Closing spider (%(reason)s)\", {'reason': reason}, extra={'spider': spider})\n+        logger.info(\n+            \"Closing spider (%(reason)s)\", {\"reason\": reason}, extra={\"spider\": spider}\n+        )\n \n         dfd = self.slot.close()\n \n         def log_failure(msg: str) -> Callable:\n             def errback(failure: Failure) -> None:\n-                logger.error(msg, exc_info=failure_to_exc_info(failure), extra={'spider': spider})\n+                logger.error(\n+                    msg, exc_info=failure_to_exc_info(failure), extra={\"spider\": spider}\n+                )\n+\n             return errback\n \n         dfd.addBoth(lambda _: self.downloader.close())\n-        dfd.addErrback(log_failure('Downloader close failure'))\n+        dfd.addErrback(log_failure(\"Downloader close failure\"))\n \n         dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n-        dfd.addErrback(log_failure('Scraper close failure'))\n+        dfd.addErrback(log_failure(\"Scraper close failure\"))\n \n         if hasattr(self.slot.scheduler, \"close\"):\n             dfd.addBoth(lambda _: self.slot.scheduler.close(reason))\n             dfd.addErrback(log_failure(\"Scheduler close failure\"))\n \n-        dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(\n-            signal=signals.spider_closed, spider=spider, reason=reason,\n-        ))\n-        dfd.addErrback(log_failure('Error while sending spider_close signal'))\n+        dfd.addBoth(\n+            lambda _: self.signals.send_catch_log_deferred(\n+                signal=signals.spider_closed,\n+                spider=spider,\n+                reason=reason,\n+            )\n+        )\n+        dfd.addErrback(log_failure(\"Error while sending spider_close signal\"))\n \n         dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))\n-        dfd.addErrback(log_failure('Stats close failure'))\n+        dfd.addErrback(log_failure(\"Stats close failure\"))\n \n-        dfd.addBoth(lambda _: logger.info(\"Spider closed (%(reason)s)\", {'reason': reason}, extra={'spider': spider}))\n+        dfd.addBoth(\n+            lambda _: logger.info(\n+                \"Spider closed (%(reason)s)\",\n+                {\"reason\": reason},\n+                extra={\"spider\": spider},\n+            )\n+        )\n \n-        dfd.addBoth(lambda _: setattr(self, 'slot', None))\n-        dfd.addErrback(log_failure('Error while unassigning slot'))\n+        dfd.addBoth(lambda _: setattr(self, \"slot\", None))\n+        dfd.addErrback(log_failure(\"Error while unassigning slot\"))\n \n-        dfd.addBoth(lambda _: setattr(self, 'spider', None))\n-        dfd.addErrback(log_failure('Error while unassigning spider'))\n+        dfd.addBoth(lambda _: setattr(self, \"spider\", None))\n+        dfd.addErrback(log_failure(\"Error while unassigning spider\"))\n \n         dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n \n@@ -408,7 +472,11 @@ class ExecutionEngine:\n         return [self.spider] if self.spider is not None else []\n \n     def has_capacity(self) -> bool:\n-        warnings.warn(\"ExecutionEngine.has_capacity is deprecated\", ScrapyDeprecationWarning, stacklevel=2)\n+        warnings.warn(\n+            \"ExecutionEngine.has_capacity is deprecated\",\n+            ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n         return not bool(self.slot)\n \n     def schedule(self, request: Request, spider: Spider) -> None:\n\n@@ -28,7 +28,9 @@ class H2ConnectionPool:\n         # Save all requests that arrive before the connection is established\n         self._pending_requests: Dict[Tuple, Deque[Deferred]] = {}\n \n-    def get_connection(self, key: Tuple, uri: URI, endpoint: HostnameEndpoint) -> Deferred:\n+    def get_connection(\n+        self, key: Tuple, uri: URI, endpoint: HostnameEndpoint\n+    ) -> Deferred:\n         if key in self._pending_requests:\n             # Received a request while connecting to remote\n             # Create a deferred which will fire with the H2ClientProtocol\n@@ -46,7 +48,9 @@ class H2ConnectionPool:\n         # No connection is established for the given URI\n         return self._new_connection(key, uri, endpoint)\n \n-    def _new_connection(self, key: Tuple, uri: URI, endpoint: HostnameEndpoint) -> Deferred:\n+    def _new_connection(\n+        self, key: Tuple, uri: URI, endpoint: HostnameEndpoint\n+    ) -> Deferred:\n         self._pending_requests[key] = deque()\n \n         conn_lost_deferred = Deferred()\n@@ -102,7 +106,9 @@ class H2Agent:\n     ) -> None:\n         self._reactor = reactor\n         self._pool = pool\n-        self._context_factory = AcceptableProtocolsContextFactory(context_factory, acceptable_protocols=[b'h2'])\n+        self._context_factory = AcceptableProtocolsContextFactory(\n+            context_factory, acceptable_protocols=[b\"h2\"]\n+        )\n         self.endpoint_factory = _StandardEndpointFactory(\n             self._reactor, self._context_factory, connect_timeout, bind_address\n         )\n@@ -118,7 +124,7 @@ class H2Agent:\n         return uri.scheme, uri.host, uri.port\n \n     def request(self, request: Request, spider: Spider) -> Deferred:\n-        uri = URI.fromBytes(bytes(request.url, encoding='utf-8'))\n+        uri = URI.fromBytes(bytes(request.url, encoding=\"utf-8\"))\n         try:\n             endpoint = self.get_endpoint(uri)\n         except SchemeNotSupported:\n\n@@ -9,9 +9,15 @@ from h2.config import H2Configuration\n from h2.connection import H2Connection\n from h2.errors import ErrorCodes\n from h2.events import (\n-    Event, ConnectionTerminated, DataReceived, ResponseReceived,\n-    SettingsAcknowledged, StreamEnded, StreamReset, UnknownFrameReceived,\n-    WindowUpdated\n+    Event,\n+    ConnectionTerminated,\n+    DataReceived,\n+    ResponseReceived,\n+    SettingsAcknowledged,\n+    StreamEnded,\n+    StreamReset,\n+    UnknownFrameReceived,\n+    WindowUpdated,\n )\n from h2.exceptions import FrameTooLargeError, H2Error\n from twisted.internet.defer import Deferred\n@@ -37,7 +43,6 @@ PROTOCOL_NAME = b\"h2\"\n \n \n class InvalidNegotiatedProtocol(H2Error):\n-\n     def __init__(self, negotiated_protocol: bytes) -> None:\n         self.negotiated_protocol = negotiated_protocol\n \n@@ -55,11 +60,13 @@ class RemoteTerminatedConnection(H2Error):\n         self.terminate_event = event\n \n     def __str__(self) -> str:\n-        return f'Received GOAWAY frame from {self.remote_ip_address!r}'\n+        return f\"Received GOAWAY frame from {self.remote_ip_address!r}\"\n \n \n class MethodNotAllowed405(H2Error):\n-    def __init__(self, remote_ip_address: Optional[Union[IPv4Address, IPv6Address]]) -> None:\n+    def __init__(\n+        self, remote_ip_address: Optional[Union[IPv4Address, IPv6Address]]\n+    ) -> None:\n         self.remote_ip_address = remote_ip_address\n \n     def __str__(self) -> str:\n@@ -70,7 +77,9 @@ class MethodNotAllowed405(H2Error):\n class H2ClientProtocol(Protocol, TimeoutMixin):\n     IDLE_TIMEOUT = 240\n \n-    def __init__(self, uri: URI, settings: Settings, conn_lost_deferred: Deferred) -> None:\n+    def __init__(\n+        self, uri: URI, settings: Settings, conn_lost_deferred: Deferred\n+    ) -> None:\n         \"\"\"\n         Arguments:\n             uri -- URI of the base url to which HTTP/2 Connection will be made.\n@@ -82,7 +91,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         \"\"\"\n         self._conn_lost_deferred = conn_lost_deferred\n \n-        config = H2Configuration(client_side=True, header_encoding='utf-8')\n+        config = H2Configuration(client_side=True, header_encoding=\"utf-8\")\n         self.conn = H2Connection(config=config)\n \n         # ID of the next request stream\n@@ -105,31 +114,25 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         # initialized when connection is successfully made\n         self.metadata: Dict = {\n             # Peer certificate instance\n-            'certificate': None,\n-\n+            \"certificate\": None,\n             # Address of the server we are connected to which\n             # is updated when HTTP/2 connection is  made successfully\n-            'ip_address': None,\n-\n+            \"ip_address\": None,\n             # URI of the peer HTTP/2 connection is made\n-            'uri': uri,\n-\n+            \"uri\": uri,\n             # Both ip_address and uri are used by the Stream before\n             # initiating the request to verify that the base address\n-\n             # Variables taken from Project Settings\n-            'default_download_maxsize': settings.getint('DOWNLOAD_MAXSIZE'),\n-            'default_download_warnsize': settings.getint('DOWNLOAD_WARNSIZE'),\n-\n+            \"default_download_maxsize\": settings.getint(\"DOWNLOAD_MAXSIZE\"),\n+            \"default_download_warnsize\": settings.getint(\"DOWNLOAD_WARNSIZE\"),\n             # Counter to keep track of opened streams. This counter\n             # is used to make sure that not more than MAX_CONCURRENT_STREAMS\n             # streams are opened which leads to ProtocolError\n             # We use simple FIFO policy to handle pending requests\n-            'active_streams': 0,\n-\n+            \"active_streams\": 0,\n             # Flag to keep track if settings were acknowledged by the remote\n             # This ensures that we have established a HTTP/2 connection\n-            'settings_acknowledged': False,\n+            \"settings_acknowledged\": False,\n         }\n \n     @property\n@@ -138,7 +141,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         This is used while initiating pending streams to make sure\n         that we initiate stream only during active HTTP/2 Connection\n         \"\"\"\n-        return bool(self.transport.connected) and self.metadata['settings_acknowledged']\n+        return bool(self.transport.connected) and self.metadata[\"settings_acknowledged\"]\n \n     @property\n     def allowed_max_concurrent_streams(self) -> int:\n@@ -149,7 +152,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         \"\"\"\n         return min(\n             self.conn.local_settings.max_concurrent_streams,\n-            self.conn.remote_settings.max_concurrent_streams\n+            self.conn.remote_settings.max_concurrent_streams,\n         )\n \n     def _send_pending_requests(self) -> None:\n@@ -159,31 +162,33 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         \"\"\"\n         while (\n             self._pending_request_stream_pool\n-            and self.metadata['active_streams'] < self.allowed_max_concurrent_streams\n+            and self.metadata[\"active_streams\"] < self.allowed_max_concurrent_streams\n             and self.h2_connected\n         ):\n-            self.metadata['active_streams'] += 1\n+            self.metadata[\"active_streams\"] += 1\n             stream = self._pending_request_stream_pool.popleft()\n             stream.initiate_request()\n             self._write_to_transport()\n \n     def pop_stream(self, stream_id: int) -> Stream:\n-        \"\"\"Perform cleanup when a stream is closed\n-        \"\"\"\n+        \"\"\"Perform cleanup when a stream is closed\"\"\"\n         stream = self.streams.pop(stream_id)\n-        self.metadata['active_streams'] -= 1\n+        self.metadata[\"active_streams\"] -= 1\n         self._send_pending_requests()\n         return stream\n \n     def _new_stream(self, request: Request, spider: Spider) -> Stream:\n-        \"\"\"Instantiates a new Stream object\n-        \"\"\"\n+        \"\"\"Instantiates a new Stream object\"\"\"\n         stream = Stream(\n             stream_id=next(self._stream_id_generator),\n             request=request,\n             protocol=self,\n-            download_maxsize=getattr(spider, 'download_maxsize', self.metadata['default_download_maxsize']),\n-            download_warnsize=getattr(spider, 'download_warnsize', self.metadata['default_download_warnsize']),\n+            download_maxsize=getattr(\n+                spider, \"download_maxsize\", self.metadata[\"default_download_maxsize\"]\n+            ),\n+            download_warnsize=getattr(\n+                spider, \"download_warnsize\", self.metadata[\"default_download_warnsize\"]\n+            ),\n         )\n         self.streams[stream.stream_id] = stream\n         return stream\n@@ -200,7 +205,9 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n \n     def request(self, request: Request, spider: Spider) -> Deferred:\n         if not isinstance(request, Request):\n-            raise TypeError(f'Expected scrapy.http.Request, received {request.__class__.__qualname__}')\n+            raise TypeError(\n+                f\"Expected scrapy.http.Request, received {request.__class__.__qualname__}\"\n+            )\n \n         stream = self._new_stream(request, spider)\n         d = stream.get_response()\n@@ -221,7 +228,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         self.setTimeout(self.IDLE_TIMEOUT)\n \n         destination = self.transport.getPeer()\n-        self.metadata['ip_address'] = ipaddress.ip_address(destination.host)\n+        self.metadata[\"ip_address\"] = ipaddress.ip_address(destination.host)\n \n         # Initiate H2 Connection\n         self.conn.initiate_connection()\n@@ -237,9 +244,14 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         \"\"\"\n         Close the connection if it's not made via the expected protocol\n         \"\"\"\n-        if self.transport.negotiatedProtocol is not None and self.transport.negotiatedProtocol != PROTOCOL_NAME:\n+        if (\n+            self.transport.negotiatedProtocol is not None\n+            and self.transport.negotiatedProtocol != PROTOCOL_NAME\n+        ):\n             # we have not initiated the connection yet, no need to send a GOAWAY frame to the remote peer\n-            self._lose_connection_with_error([InvalidNegotiatedProtocol(self.transport.negotiatedProtocol)])\n+            self._lose_connection_with_error(\n+                [InvalidNegotiatedProtocol(self.transport.negotiatedProtocol)]\n+            )\n \n     def _check_received_data(self, data: bytes) -> None:\n         \"\"\"Checks for edge cases where the connection to remote fails\n@@ -248,8 +260,8 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         Arguments:\n             data -- Data received from the remote\n         \"\"\"\n-        if data.startswith(b'HTTP/2.0 405 Method Not Allowed'):\n-            raise MethodNotAllowed405(self.metadata['ip_address'])\n+        if data.startswith(b\"HTTP/2.0 405 Method Not Allowed\"):\n+            raise MethodNotAllowed405(self.metadata[\"ip_address\"])\n \n     def dataReceived(self, data: bytes) -> None:\n         # Reset the idle timeout as connection is still actively receiving data\n@@ -284,7 +296,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         if (\n             self.conn.open_outbound_streams > 0\n             or self.conn.open_inbound_streams > 0\n-            or self.metadata['active_streams'] > 0\n+            or self.metadata[\"active_streams\"] > 0\n         ):\n             error_code = ErrorCodes.PROTOCOL_ERROR\n         else:\n@@ -292,9 +304,9 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         self.conn.close_connection(error_code=error_code)\n         self._write_to_transport()\n \n-        self._lose_connection_with_error([\n-            TimeoutError(f\"Connection was IDLE for more than {self.IDLE_TIMEOUT}s\")\n-        ])\n+        self._lose_connection_with_error(\n+            [TimeoutError(f\"Connection was IDLE for more than {self.IDLE_TIMEOUT}s\")]\n+        )\n \n     def connectionLost(self, reason: Failure = connectionDone) -> None:\n         \"\"\"Called by Twisted when the transport connection is lost.\n@@ -311,13 +323,13 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         self._conn_lost_deferred.callback(self._conn_lost_errors)\n \n         for stream in self.streams.values():\n-            if stream.metadata['request_sent']:\n+            if stream.metadata[\"request_sent\"]:\n                 close_reason = StreamCloseReason.CONNECTION_LOST\n             else:\n                 close_reason = StreamCloseReason.INACTIVE\n             stream.close(close_reason, self._conn_lost_errors, from_protocol=True)\n \n-        self.metadata['active_streams'] -= len(self.streams)\n+        self.metadata[\"active_streams\"] -= len(self.streams)\n         self.streams.clear()\n         self._pending_request_stream_pool.clear()\n         self.conn.close_connection()\n@@ -345,13 +357,13 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n             elif isinstance(event, SettingsAcknowledged):\n                 self.settings_acknowledged(event)\n             elif isinstance(event, UnknownFrameReceived):\n-                logger.warning('Unknown frame received: %s', event.frame)\n+                logger.warning(\"Unknown frame received: %s\", event.frame)\n \n     # Event handler functions starts here\n     def connection_terminated(self, event: ConnectionTerminated) -> None:\n-        self._lose_connection_with_error([\n-            RemoteTerminatedConnection(self.metadata['ip_address'], event)\n-        ])\n+        self._lose_connection_with_error(\n+            [RemoteTerminatedConnection(self.metadata[\"ip_address\"], event)]\n+        )\n \n     def data_received(self, event: DataReceived) -> None:\n         try:\n@@ -370,14 +382,14 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n             stream.receive_headers(event.headers)\n \n     def settings_acknowledged(self, event: SettingsAcknowledged) -> None:\n-        self.metadata['settings_acknowledged'] = True\n+        self.metadata[\"settings_acknowledged\"] = True\n \n         # Send off all the pending requests as now we have\n         # established a proper HTTP/2 connection\n         self._send_pending_requests()\n \n         # Update certificate when our HTTP/2 connection is established\n-        self.metadata['certificate'] = Certificate(self.transport.getPeerCertificate())\n+        self.metadata[\"certificate\"] = Certificate(self.transport.getPeerCertificate())\n \n     def stream_ended(self, event: StreamEnded) -> None:\n         try:\n@@ -406,7 +418,9 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n \n @implementer(IProtocolNegotiationFactory)\n class H2ClientFactory(Factory):\n-    def __init__(self, uri: URI, settings: Settings, conn_lost_deferred: Deferred) -> None:\n+    def __init__(\n+        self, uri: URI, settings: Settings, conn_lost_deferred: Deferred\n+    ) -> None:\n         self.uri = uri\n         self.settings = settings\n         self.conn_lost_deferred = conn_lost_deferred\n\n@@ -32,18 +32,19 @@ class InactiveStreamClosed(ConnectionClosed):\n         self.request = request\n \n     def __str__(self) -> str:\n-        return f'InactiveStreamClosed: Connection was closed without sending the request {self.request!r}'\n+        return f\"InactiveStreamClosed: Connection was closed without sending the request {self.request!r}\"\n \n \n class InvalidHostname(H2Error):\n-\n-    def __init__(self, request: Request, expected_hostname: str, expected_netloc: str) -> None:\n+    def __init__(\n+        self, request: Request, expected_hostname: str, expected_netloc: str\n+    ) -> None:\n         self.request = request\n         self.expected_hostname = expected_hostname\n         self.expected_netloc = expected_netloc\n \n     def __str__(self) -> str:\n-        return f'InvalidHostname: Expected {self.expected_hostname} or {self.expected_netloc} in {self.request}'\n+        return f\"InvalidHostname: Expected {self.expected_hostname} or {self.expected_netloc} in {self.request}\"\n \n \n class StreamCloseReason(Enum):\n@@ -100,28 +101,31 @@ class Stream:\n         self._request: Request = request\n         self._protocol: \"H2ClientProtocol\" = protocol\n \n-        self._download_maxsize = self._request.meta.get('download_maxsize', download_maxsize)\n-        self._download_warnsize = self._request.meta.get('download_warnsize', download_warnsize)\n+        self._download_maxsize = self._request.meta.get(\n+            \"download_maxsize\", download_maxsize\n+        )\n+        self._download_warnsize = self._request.meta.get(\n+            \"download_warnsize\", download_warnsize\n+        )\n \n         # Metadata of an HTTP/2 connection stream\n         # initialized when stream is instantiated\n         self.metadata: Dict = {\n-            'request_content_length': 0 if self._request.body is None else len(self._request.body),\n-\n+            \"request_content_length\": 0\n+            if self._request.body is None\n+            else len(self._request.body),\n             # Flag to keep track whether the stream has initiated the request\n-            'request_sent': False,\n-\n+            \"request_sent\": False,\n             # Flag to track whether we have logged about exceeding download warnsize\n-            'reached_warnsize': False,\n-\n+            \"reached_warnsize\": False,\n             # Each time we send a data frame, we will decrease value by the amount send.\n-            'remaining_content_length': 0 if self._request.body is None else len(self._request.body),\n-\n+            \"remaining_content_length\": 0\n+            if self._request.body is None\n+            else len(self._request.body),\n             # Flag to keep track whether client (self) have closed this stream\n-            'stream_closed_local': False,\n-\n+            \"stream_closed_local\": False,\n             # Flag to keep track whether the server has closed the stream\n-            'stream_closed_server': False,\n+            \"stream_closed_server\": False,\n         }\n \n         # Private variable used to build the response\n@@ -130,21 +134,19 @@ class Stream:\n         self._response: Dict = {\n             # Data received frame by frame from the server is appended\n             # and passed to the response Deferred when completely received.\n-            'body': BytesIO(),\n-\n+            \"body\": BytesIO(),\n             # The amount of data received that counts against the\n             # flow control window\n-            'flow_controlled_size': 0,\n-\n+            \"flow_controlled_size\": 0,\n             # Headers received after sending the request\n-            'headers': Headers({}),\n+            \"headers\": Headers({}),\n         }\n \n         def _cancel(_) -> None:\n             # Close this stream as gracefully as possible\n             # If the associated request is initiated we reset this stream\n             # else we directly call close() method\n-            if self.metadata['request_sent']:\n+            if self.metadata[\"request_sent\"]:\n                 self.reset_stream(StreamCloseReason.CANCELLED)\n             else:\n                 self.close(StreamCloseReason.CANCELLED)\n@@ -152,7 +154,7 @@ class Stream:\n         self._deferred_response = Deferred(_cancel)\n \n     def __repr__(self) -> str:\n-        return f'Stream(id={self.stream_id!r})'\n+        return f\"Stream(id={self.stream_id!r})\"\n \n     @property\n     def _log_warnsize(self) -> bool:\n@@ -163,14 +165,16 @@ class Stream:\n             True if both the above conditions hold true\n             False if any of the conditions is false\n         \"\"\"\n-        content_length_header = int(self._response['headers'].get(b'Content-Length', -1))\n+        content_length_header = int(\n+            self._response[\"headers\"].get(b\"Content-Length\", -1)\n+        )\n         return (\n             self._download_warnsize\n             and (\n-                self._response['flow_controlled_size'] > self._download_warnsize\n+                self._response[\"flow_controlled_size\"] > self._download_warnsize\n                 or content_length_header > self._download_warnsize\n             )\n-            and not self.metadata['reached_warnsize']\n+            and not self.metadata[\"reached_warnsize\"]\n         )\n \n     def get_response(self) -> Deferred:\n@@ -183,9 +187,10 @@ class Stream:\n         # Make sure that we are sending the request to the correct URL\n         url = urlparse(self._request.url)\n         return (\n-            url.netloc == str(self._protocol.metadata['uri'].host, 'utf-8')\n-            or url.netloc == str(self._protocol.metadata['uri'].netloc, 'utf-8')\n-            or url.netloc == f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}'\n+            url.netloc == str(self._protocol.metadata[\"uri\"].host, \"utf-8\")\n+            or url.netloc == str(self._protocol.metadata[\"uri\"].netloc, \"utf-8\")\n+            or url.netloc\n+            == f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}'\n         )\n \n     def _get_request_headers(self) -> List[Tuple[str, str]]:\n@@ -193,7 +198,7 @@ class Stream:\n \n         path = url.path\n         if url.query:\n-            path += '?' + url.query\n+            path += \"?\" + url.query\n \n         # This pseudo-header field MUST NOT be empty for \"http\" or \"https\"\n         # URIs; \"http\" or \"https\" URIs that do not contain a path component\n@@ -202,40 +207,40 @@ class Stream:\n         # a path component; these MUST include a \":path\" pseudo-header field\n         # with a value of '*' (refer RFC 7540 - Section 8.1.2.3)\n         if not path:\n-            path = '*' if self._request.method == 'OPTIONS' else '/'\n+            path = \"*\" if self._request.method == \"OPTIONS\" else \"/\"\n \n         # Make sure pseudo-headers comes before all the other headers\n         headers = [\n-            (':method', self._request.method),\n-            (':authority', url.netloc),\n+            (\":method\", self._request.method),\n+            (\":authority\", url.netloc),\n         ]\n \n         # The \":scheme\" and \":path\" pseudo-header fields MUST\n         # be omitted for CONNECT method (refer RFC 7540 - Section 8.3)\n-        if self._request.method != 'CONNECT':\n+        if self._request.method != \"CONNECT\":\n             headers += [\n-                (':scheme', self._protocol.metadata['uri'].scheme),\n-                (':path', path),\n+                (\":scheme\", self._protocol.metadata[\"uri\"].scheme),\n+                (\":path\", path),\n             ]\n \n         content_length = str(len(self._request.body))\n-        headers.append(('Content-Length', content_length))\n+        headers.append((\"Content-Length\", content_length))\n \n-        content_length_name = self._request.headers.normkey(b'Content-Length')\n+        content_length_name = self._request.headers.normkey(b\"Content-Length\")\n         for name, values in self._request.headers.items():\n             for value in values:\n-                value = str(value, 'utf-8')\n+                value = str(value, \"utf-8\")\n                 if name == content_length_name:\n                     if value != content_length:\n                         logger.warning(\n-                            'Ignoring bad Content-Length header %r of request %r, '\n-                            'sending %r instead',\n+                            \"Ignoring bad Content-Length header %r of request %r, \"\n+                            \"sending %r instead\",\n                             value,\n                             self._request,\n                             content_length,\n                         )\n                     continue\n-                headers.append((str(name, 'utf-8'), value))\n+                headers.append((str(name, \"utf-8\"), value))\n \n         return headers\n \n@@ -243,7 +248,7 @@ class Stream:\n         if self.check_request_url():\n             headers = self._get_request_headers()\n             self._protocol.conn.send_headers(self.stream_id, headers, end_stream=False)\n-            self.metadata['request_sent'] = True\n+            self.metadata[\"request_sent\"] = True\n             self.send_data()\n         else:\n             # Close this stream calling the response errback\n@@ -261,35 +266,44 @@ class Stream:\n            and has initiated request already by sending HEADER frame. If not then\n            stream will raise ProtocolError (raise by h2 state machine).\n         \"\"\"\n-        if self.metadata['stream_closed_local']:\n+        if self.metadata[\"stream_closed_local\"]:\n             raise StreamClosedError(self.stream_id)\n \n         # Firstly, check what the flow control window is for current stream.\n-        window_size = self._protocol.conn.local_flow_control_window(stream_id=self.stream_id)\n+        window_size = self._protocol.conn.local_flow_control_window(\n+            stream_id=self.stream_id\n+        )\n \n         # Next, check what the maximum frame size is.\n         max_frame_size = self._protocol.conn.max_outbound_frame_size\n \n         # We will send no more than the window size or the remaining file size\n         # of data in this call, whichever is smaller.\n-        bytes_to_send_size = min(window_size, self.metadata['remaining_content_length'])\n+        bytes_to_send_size = min(window_size, self.metadata[\"remaining_content_length\"])\n \n         # We now need to send a number of data frames.\n         while bytes_to_send_size > 0:\n             chunk_size = min(bytes_to_send_size, max_frame_size)\n \n-            data_chunk_start_id = self.metadata['request_content_length'] - self.metadata['remaining_content_length']\n-            data_chunk = self._request.body[data_chunk_start_id:data_chunk_start_id + chunk_size]\n+            data_chunk_start_id = (\n+                self.metadata[\"request_content_length\"]\n+                - self.metadata[\"remaining_content_length\"]\n+            )\n+            data_chunk = self._request.body[\n+                data_chunk_start_id : data_chunk_start_id + chunk_size\n+            ]\n \n             self._protocol.conn.send_data(self.stream_id, data_chunk, end_stream=False)\n \n             bytes_to_send_size -= chunk_size\n-            self.metadata['remaining_content_length'] -= chunk_size\n+            self.metadata[\"remaining_content_length\"] -= chunk_size\n \n-        self.metadata['remaining_content_length'] = max(0, self.metadata['remaining_content_length'])\n+        self.metadata[\"remaining_content_length\"] = max(\n+            0, self.metadata[\"remaining_content_length\"]\n+        )\n \n         # End the stream if no more data needs to be send\n-        if self.metadata['remaining_content_length'] == 0:\n+        if self.metadata[\"remaining_content_length\"] == 0:\n             self._protocol.conn.end_stream(self.stream_id)\n \n         # Q. What about the rest of the data?\n@@ -301,62 +315,64 @@ class Stream:\n         blocked behind the flow control.\n         \"\"\"\n         if (\n-            self.metadata['remaining_content_length']\n-            and not self.metadata['stream_closed_server']\n-            and self.metadata['request_sent']\n+            self.metadata[\"remaining_content_length\"]\n+            and not self.metadata[\"stream_closed_server\"]\n+            and self.metadata[\"request_sent\"]\n         ):\n             self.send_data()\n \n     def receive_data(self, data: bytes, flow_controlled_length: int) -> None:\n-        self._response['body'].write(data)\n-        self._response['flow_controlled_size'] += flow_controlled_length\n+        self._response[\"body\"].write(data)\n+        self._response[\"flow_controlled_size\"] += flow_controlled_length\n \n         # We check maxsize here in case the Content-Length header was not received\n-        if self._download_maxsize and self._response['flow_controlled_size'] > self._download_maxsize:\n+        if (\n+            self._download_maxsize\n+            and self._response[\"flow_controlled_size\"] > self._download_maxsize\n+        ):\n             self.reset_stream(StreamCloseReason.MAXSIZE_EXCEEDED)\n             return\n \n         if self._log_warnsize:\n-            self.metadata['reached_warnsize'] = True\n+            self.metadata[\"reached_warnsize\"] = True\n             warning_msg = (\n                 f'Received more ({self._response[\"flow_controlled_size\"]}) bytes than download '\n-                f'warn size ({self._download_warnsize}) in request {self._request}'\n+                f\"warn size ({self._download_warnsize}) in request {self._request}\"\n             )\n             logger.warning(warning_msg)\n \n         # Acknowledge the data received\n         self._protocol.conn.acknowledge_received_data(\n-            self._response['flow_controlled_size'],\n-            self.stream_id\n+            self._response[\"flow_controlled_size\"], self.stream_id\n         )\n \n     def receive_headers(self, headers: List[HeaderTuple]) -> None:\n         for name, value in headers:\n-            self._response['headers'][name] = value\n+            self._response[\"headers\"][name] = value\n \n         # Check if we exceed the allowed max data size which can be received\n-        expected_size = int(self._response['headers'].get(b'Content-Length', -1))\n+        expected_size = int(self._response[\"headers\"].get(b\"Content-Length\", -1))\n         if self._download_maxsize and expected_size > self._download_maxsize:\n             self.reset_stream(StreamCloseReason.MAXSIZE_EXCEEDED)\n             return\n \n         if self._log_warnsize:\n-            self.metadata['reached_warnsize'] = True\n+            self.metadata[\"reached_warnsize\"] = True\n             warning_msg = (\n-                f'Expected response size ({expected_size}) larger than '\n-                f'download warn size ({self._download_warnsize}) in request {self._request}'\n+                f\"Expected response size ({expected_size}) larger than \"\n+                f\"download warn size ({self._download_warnsize}) in request {self._request}\"\n             )\n             logger.warning(warning_msg)\n \n     def reset_stream(self, reason: StreamCloseReason = StreamCloseReason.RESET) -> None:\n         \"\"\"Close this stream by sending a RST_FRAME to the remote peer\"\"\"\n-        if self.metadata['stream_closed_local']:\n+        if self.metadata[\"stream_closed_local\"]:\n             raise StreamClosedError(self.stream_id)\n \n         # Clear buffer earlier to avoid keeping data in memory for a long time\n-        self._response['body'].truncate(0)\n+        self._response[\"body\"].truncate(0)\n \n-        self.metadata['stream_closed_local'] = True\n+        self.metadata[\"stream_closed_local\"] = True\n         self._protocol.conn.reset_stream(self.stream_id, ErrorCodes.REFUSED_STREAM)\n         self.close(reason)\n \n@@ -366,13 +382,14 @@ class Stream:\n         errors: Optional[List[BaseException]] = None,\n         from_protocol: bool = False,\n     ) -> None:\n-        \"\"\"Based on the reason sent we will handle each case.\n-        \"\"\"\n-        if self.metadata['stream_closed_server']:\n+        \"\"\"Based on the reason sent we will handle each case.\"\"\"\n+        if self.metadata[\"stream_closed_server\"]:\n             raise StreamClosedError(self.stream_id)\n \n         if not isinstance(reason, StreamCloseReason):\n-            raise TypeError(f'Expected StreamCloseReason, received {reason.__class__.__qualname__}')\n+            raise TypeError(\n+                f\"Expected StreamCloseReason, received {reason.__class__.__qualname__}\"\n+            )\n \n         # Have default value of errors as an empty list as\n         # some cases can add a list of exceptions\n@@ -381,7 +398,7 @@ class Stream:\n         if not from_protocol:\n             self._protocol.pop_stream(self.stream_id)\n \n-        self.metadata['stream_closed_server'] = True\n+        self.metadata[\"stream_closed_server\"] = True\n \n         # We do not check for Content-Length or Transfer-Encoding in response headers\n         # and add `partial` flag as in HTTP/1.1 as 'A request or response that includes\n@@ -392,13 +409,14 @@ class Stream:\n         # receiving DATA_FRAME's when we have received the headers (not\n         # having Content-Length)\n         if reason is StreamCloseReason.MAXSIZE_EXCEEDED:\n-            expected_size = int(self._response['headers'].get(\n-                b'Content-Length',\n-                self._response['flow_controlled_size'])\n+            expected_size = int(\n+                self._response[\"headers\"].get(\n+                    b\"Content-Length\", self._response[\"flow_controlled_size\"]\n+                )\n             )\n             error_msg = (\n-                f'Cancelling download of {self._request.url}: received response '\n-                f'size ({expected_size}) larger than download max size ({self._download_maxsize})'\n+                f\"Cancelling download of {self._request.url}: received response \"\n+                f\"size ({expected_size}) larger than download max size ({self._download_maxsize})\"\n             )\n             logger.error(error_msg)\n             self._deferred_response.errback(CancelledError(error_msg))\n@@ -416,16 +434,20 @@ class Stream:\n \n             # There maybe no :status in headers, we make\n             # HTTP Status Code: 499 - Client Closed Request\n-            self._response['headers'][':status'] = '499'\n+            self._response[\"headers\"][\":status\"] = \"499\"\n             self._fire_response_deferred()\n \n         elif reason is StreamCloseReason.RESET:\n-            self._deferred_response.errback(ResponseFailed([\n+            self._deferred_response.errback(\n+                ResponseFailed(\n+                    [\n                         Failure(\n                             f'Remote peer {self._protocol.metadata[\"ip_address\"]} sent RST_STREAM',\n-                    ProtocolError\n+                            ProtocolError,\n+                        )\n+                    ]\n+                )\n             )\n-            ]))\n \n         elif reason is StreamCloseReason.CONNECTION_LOST:\n             self._deferred_response.errback(ResponseFailed(errors))\n@@ -436,33 +458,35 @@ class Stream:\n \n         else:\n             assert reason is StreamCloseReason.INVALID_HOSTNAME\n-            self._deferred_response.errback(InvalidHostname(\n+            self._deferred_response.errback(\n+                InvalidHostname(\n                     self._request,\n-                str(self._protocol.metadata['uri'].host, 'utf-8'),\n-                f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}'\n-            ))\n+                    str(self._protocol.metadata[\"uri\"].host, \"utf-8\"),\n+                    f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}',\n+                )\n+            )\n \n     def _fire_response_deferred(self) -> None:\n         \"\"\"Builds response from the self._response dict\n         and fires the response deferred callback with the\n         generated response instance\"\"\"\n \n-        body = self._response['body'].getvalue()\n+        body = self._response[\"body\"].getvalue()\n         response_cls = responsetypes.from_args(\n-            headers=self._response['headers'],\n+            headers=self._response[\"headers\"],\n             url=self._request.url,\n             body=body,\n         )\n \n         response = response_cls(\n             url=self._request.url,\n-            status=int(self._response['headers'][':status']),\n-            headers=self._response['headers'],\n+            status=int(self._response[\"headers\"][\":status\"]),\n+            headers=self._response[\"headers\"],\n             body=body,\n             request=self._request,\n-            certificate=self._protocol.metadata['certificate'],\n-            ip_address=self._protocol.metadata['ip_address'],\n-            protocol='h2',\n+            certificate=self._protocol.metadata[\"certificate\"],\n+            ip_address=self._protocol.metadata[\"ip_address\"],\n+            protocol=\"h2\",\n         )\n \n         self._deferred_response.callback(response)\n\n@@ -20,14 +20,18 @@ class BaseSchedulerMeta(type):\n     \"\"\"\n     Metaclass to check scheduler classes against the necessary interface\n     \"\"\"\n+\n     def __instancecheck__(cls, instance):\n         return cls.__subclasscheck__(type(instance))\n \n     def __subclasscheck__(cls, subclass):\n         return (\n-            hasattr(subclass, \"has_pending_requests\") and callable(subclass.has_pending_requests)\n-            and hasattr(subclass, \"enqueue_request\") and callable(subclass.enqueue_request)\n-            and hasattr(subclass, \"next_request\") and callable(subclass.next_request)\n+            hasattr(subclass, \"has_pending_requests\")\n+            and callable(subclass.has_pending_requests)\n+            and hasattr(subclass, \"enqueue_request\")\n+            and callable(subclass.enqueue_request)\n+            and hasattr(subclass, \"next_request\")\n+            and callable(subclass.next_request)\n         )\n \n \n@@ -162,6 +166,7 @@ class Scheduler(BaseScheduler):\n     :param crawler: The crawler object corresponding to the current crawl.\n     :type crawler: :class:`scrapy.crawler.Crawler`\n     \"\"\"\n+\n     def __init__(\n         self,\n         dupefilter,\n@@ -187,15 +192,15 @@ class Scheduler(BaseScheduler):\n         \"\"\"\n         Factory method, initializes the scheduler with arguments taken from the crawl settings\n         \"\"\"\n-        dupefilter_cls = load_object(crawler.settings['DUPEFILTER_CLASS'])\n+        dupefilter_cls = load_object(crawler.settings[\"DUPEFILTER_CLASS\"])\n         return cls(\n             dupefilter=create_instance(dupefilter_cls, crawler.settings, crawler),\n             jobdir=job_dir(crawler.settings),\n-            dqclass=load_object(crawler.settings['SCHEDULER_DISK_QUEUE']),\n-            mqclass=load_object(crawler.settings['SCHEDULER_MEMORY_QUEUE']),\n-            logunser=crawler.settings.getbool('SCHEDULER_DEBUG'),\n+            dqclass=load_object(crawler.settings[\"SCHEDULER_DISK_QUEUE\"]),\n+            mqclass=load_object(crawler.settings[\"SCHEDULER_MEMORY_QUEUE\"]),\n+            logunser=crawler.settings.getbool(\"SCHEDULER_DEBUG\"),\n             stats=crawler.stats,\n-            pqclass=load_object(crawler.settings['SCHEDULER_PRIORITY_QUEUE']),\n+            pqclass=load_object(crawler.settings[\"SCHEDULER_PRIORITY_QUEUE\"]),\n             crawler=crawler,\n         )\n \n@@ -239,11 +244,11 @@ class Scheduler(BaseScheduler):\n             return False\n         dqok = self._dqpush(request)\n         if dqok:\n-            self.stats.inc_value('scheduler/enqueued/disk', spider=self.spider)\n+            self.stats.inc_value(\"scheduler/enqueued/disk\", spider=self.spider)\n         else:\n             self._mqpush(request)\n-            self.stats.inc_value('scheduler/enqueued/memory', spider=self.spider)\n-        self.stats.inc_value('scheduler/enqueued', spider=self.spider)\n+            self.stats.inc_value(\"scheduler/enqueued/memory\", spider=self.spider)\n+        self.stats.inc_value(\"scheduler/enqueued\", spider=self.spider)\n         return True\n \n     def next_request(self) -> Optional[Request]:\n@@ -257,13 +262,13 @@ class Scheduler(BaseScheduler):\n         \"\"\"\n         request = self.mqs.pop()\n         if request is not None:\n-            self.stats.inc_value('scheduler/dequeued/memory', spider=self.spider)\n+            self.stats.inc_value(\"scheduler/dequeued/memory\", spider=self.spider)\n         else:\n             request = self._dqpop()\n             if request is not None:\n-                self.stats.inc_value('scheduler/dequeued/disk', spider=self.spider)\n+                self.stats.inc_value(\"scheduler/dequeued/disk\", spider=self.spider)\n         if request is not None:\n-            self.stats.inc_value('scheduler/dequeued', spider=self.spider)\n+            self.stats.inc_value(\"scheduler/dequeued\", spider=self.spider)\n         return request\n \n     def __len__(self) -> int:\n@@ -279,13 +284,19 @@ class Scheduler(BaseScheduler):\n             self.dqs.push(request)\n         except ValueError as e:  # non serializable request\n             if self.logunser:\n-                msg = (\"Unable to serialize request: %(request)s - reason:\"\n+                msg = (\n+                    \"Unable to serialize request: %(request)s - reason:\"\n                     \" %(reason)s - no more unserializable requests will be\"\n-                       \" logged (stats being collected)\")\n-                logger.warning(msg, {'request': request, 'reason': e},\n-                               exc_info=True, extra={'spider': self.spider})\n+                    \" logged (stats being collected)\"\n+                )\n+                logger.warning(\n+                    msg,\n+                    {\"request\": request, \"reason\": e},\n+                    exc_info=True,\n+                    extra={\"spider\": self.spider},\n+                )\n                 self.logunser = False\n-            self.stats.inc_value('scheduler/unserializable', spider=self.spider)\n+            self.stats.inc_value(\"scheduler/unserializable\", spider=self.spider)\n             return False\n         else:\n             return True\n@@ -300,42 +311,49 @@ class Scheduler(BaseScheduler):\n \n     def _mq(self):\n         \"\"\"Create a new priority queue instance, with in-memory storage\"\"\"\n-        return create_instance(self.pqclass,\n+        return create_instance(\n+            self.pqclass,\n             settings=None,\n             crawler=self.crawler,\n             downstream_queue_cls=self.mqclass,\n-                               key='')\n+            key=\"\",\n+        )\n \n     def _dq(self):\n         \"\"\"Create a new priority queue instance, with disk storage\"\"\"\n         state = self._read_dqs_state(self.dqdir)\n-        q = create_instance(self.pqclass,\n+        q = create_instance(\n+            self.pqclass,\n             settings=None,\n             crawler=self.crawler,\n             downstream_queue_cls=self.dqclass,\n             key=self.dqdir,\n-                            startprios=state)\n+            startprios=state,\n+        )\n         if q:\n-            logger.info(\"Resuming crawl (%(queuesize)d requests scheduled)\",\n-                        {'queuesize': len(q)}, extra={'spider': self.spider})\n+            logger.info(\n+                \"Resuming crawl (%(queuesize)d requests scheduled)\",\n+                {\"queuesize\": len(q)},\n+                extra={\"spider\": self.spider},\n+            )\n         return q\n \n     def _dqdir(self, jobdir: Optional[str]) -> Optional[str]:\n         \"\"\"Return a folder name to keep disk queue state at\"\"\"\n         if jobdir is not None:\n-            dqdir = Path(jobdir, 'requests.queue')\n+            dqdir = Path(jobdir, \"requests.queue\")\n             if not dqdir.exists():\n                 dqdir.mkdir(parents=True)\n             return str(dqdir)\n         return None\n \n     def _read_dqs_state(self, dqdir: str) -> list:\n-        path = Path(dqdir, 'active.json')\n+        path = Path(dqdir, \"active.json\")\n         if not path.exists():\n             return []\n         with path.open(encoding=\"utf-8\") as f:\n             return json.load(f)\n \n     def _write_dqs_state(self, dqdir: str, state: list) -> None:\n-        with Path(dqdir, 'active.json').open('w', encoding=\"utf-8\") as f:\n+        with Path(dqdir, \"active.json\").open(\"w\", encoding=\"utf-8\") as f:\n             json.dump(state, f)\n\n@@ -63,7 +63,9 @@ class Slot:\n         self.itemproc_size: int = 0\n         self.closing: Optional[Deferred] = None\n \n-    def add_response_request(self, result: Union[Response, Failure], request: Request) -> Deferred:\n+    def add_response_request(\n+        self, result: Union[Response, Failure], request: Request\n+    ) -> Deferred:\n         deferred = Deferred()\n         self.queue.append((result, request, deferred))\n         if isinstance(result, Response):\n@@ -77,7 +79,9 @@ class Slot:\n         self.active.add(request)\n         return response, request, deferred\n \n-    def finish_response(self, result: Union[Response, Failure], request: Request) -> None:\n+    def finish_response(\n+        self, result: Union[Response, Failure], request: Request\n+    ) -> None:\n         self.active.remove(request)\n         if isinstance(result, Response):\n             self.active_size -= max(len(result.body), self.MIN_RESPONSE_SIZE)\n@@ -92,13 +96,12 @@ class Slot:\n \n \n class Scraper:\n-\n     def __init__(self, crawler: Crawler) -> None:\n         self.slot: Optional[Slot] = None\n         self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n-        itemproc_cls = load_object(crawler.settings['ITEM_PROCESSOR'])\n+        itemproc_cls = load_object(crawler.settings[\"ITEM_PROCESSOR\"])\n         self.itemproc = itemproc_cls.from_crawler(crawler)\n-        self.concurrent_items = crawler.settings.getint('CONCURRENT_ITEMS')\n+        self.concurrent_items = crawler.settings.getint(\"CONCURRENT_ITEMS\")\n         self.crawler = crawler\n         self.signals = crawler.signals\n         self.logformatter = crawler.logformatter\n@@ -106,7 +109,7 @@ class Scraper:\n     @inlineCallbacks\n     def open_spider(self, spider: Spider):\n         \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n-        self.slot = Slot(self.crawler.settings.getint('SCRAPER_SLOT_MAX_ACTIVE_SIZE'))\n+        self.slot = Slot(self.crawler.settings.getint(\"SCRAPER_SLOT_MAX_ACTIVE_SIZE\"))\n         yield self.itemproc.open_spider(spider)\n \n     def close_spider(self, spider: Spider) -> Deferred:\n@@ -127,7 +130,9 @@ class Scraper:\n         if self.slot.closing and self.slot.is_idle():\n             self.slot.closing.callback(spider)\n \n-    def enqueue_scrape(self, result: Union[Response, Failure], request: Request, spider: Spider) -> Deferred:\n+    def enqueue_scrape(\n+        self, result: Union[Response, Failure], request: Request, spider: Spider\n+    ) -> Deferred:\n         if self.slot is None:\n             raise RuntimeError(\"Scraper slot not assigned\")\n         dfd = self.slot.add_response_request(result, request)\n@@ -140,10 +145,13 @@ class Scraper:\n \n         dfd.addBoth(finish_scraping)\n         dfd.addErrback(\n-            lambda f: logger.error('Scraper bug processing %(request)s',\n-                                   {'request': request},\n+            lambda f: logger.error(\n+                \"Scraper bug processing %(request)s\",\n+                {\"request\": request},\n                 exc_info=failure_to_exc_info(f),\n-                                   extra={'spider': spider}))\n+                extra={\"spider\": spider},\n+            )\n+        )\n         self._scrape_next(spider)\n         return dfd\n \n@@ -153,35 +161,49 @@ class Scraper:\n             response, request, deferred = self.slot.next_response_request_deferred()\n             self._scrape(response, request, spider).chainDeferred(deferred)\n \n-    def _scrape(self, result: Union[Response, Failure], request: Request, spider: Spider) -> Deferred:\n+    def _scrape(\n+        self, result: Union[Response, Failure], request: Request, spider: Spider\n+    ) -> Deferred:\n         \"\"\"\n         Handle the downloaded response or failure through the spider callback/errback\n         \"\"\"\n         if not isinstance(result, (Response, Failure)):\n-            raise TypeError(f\"Incorrect type: expected Response or Failure, got {type(result)}: {result!r}\")\n-        dfd = self._scrape2(result, request, spider)  # returns spider's processed output\n+            raise TypeError(\n+                f\"Incorrect type: expected Response or Failure, got {type(result)}: {result!r}\"\n+            )\n+        dfd = self._scrape2(\n+            result, request, spider\n+        )  # returns spider's processed output\n         dfd.addErrback(self.handle_spider_error, request, result, spider)\n         dfd.addCallback(self.handle_spider_output, request, result, spider)\n         return dfd\n \n-    def _scrape2(self, result: Union[Response, Failure], request: Request, spider: Spider) -> Deferred:\n+    def _scrape2(\n+        self, result: Union[Response, Failure], request: Request, spider: Spider\n+    ) -> Deferred:\n         \"\"\"\n         Handle the different cases of request's result been a Response or a Failure\n         \"\"\"\n         if isinstance(result, Response):\n-            return self.spidermw.scrape_response(self.call_spider, result, request, spider)\n+            return self.spidermw.scrape_response(\n+                self.call_spider, result, request, spider\n+            )\n         # else result is a Failure\n         dfd = self.call_spider(result, request, spider)\n         return dfd.addErrback(self._log_download_errors, result, request, spider)\n \n-    def call_spider(self, result: Union[Response, Failure], request: Request, spider: Spider) -> Deferred:\n+    def call_spider(\n+        self, result: Union[Response, Failure], request: Request, spider: Spider\n+    ) -> Deferred:\n         if isinstance(result, Response):\n             if getattr(result, \"request\", None) is None:\n                 result.request = request\n             callback = result.request.callback or spider._parse\n             warn_on_generator_with_return_value(spider, callback)\n             dfd = defer_succeed(result)\n-            dfd.addCallbacks(callback=callback, callbackKeywords=result.request.cb_kwargs)\n+            dfd.addCallbacks(\n+                callback=callback, callbackKeywords=result.request.cb_kwargs\n+            )\n         else:  # result is a Failure\n             result.request = request\n             warn_on_generator_with_return_value(spider, request.errback)\n@@ -189,45 +211,69 @@ class Scraper:\n             dfd.addErrback(request.errback)\n         return dfd.addCallback(iterate_spider_output)\n \n-    def handle_spider_error(self, _failure: Failure, request: Request, response: Response, spider: Spider) -> None:\n+    def handle_spider_error(\n+        self, _failure: Failure, request: Request, response: Response, spider: Spider\n+    ) -> None:\n         exc = _failure.value\n         if isinstance(exc, CloseSpider):\n             assert self.crawler.engine is not None  # typing\n-            self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n+            self.crawler.engine.close_spider(spider, exc.reason or \"cancelled\")\n             return\n         logkws = self.logformatter.spider_error(_failure, request, response, spider)\n         logger.log(\n             *logformatter_adapter(logkws),\n             exc_info=failure_to_exc_info(_failure),\n-            extra={'spider': spider}\n+            extra={\"spider\": spider},\n         )\n         self.signals.send_catch_log(\n             signal=signals.spider_error,\n-            failure=_failure, response=response,\n-            spider=spider\n+            failure=_failure,\n+            response=response,\n+            spider=spider,\n         )\n         self.crawler.stats.inc_value(\n-            f\"spider_exceptions/{_failure.value.__class__.__name__}\",\n-            spider=spider\n+            f\"spider_exceptions/{_failure.value.__class__.__name__}\", spider=spider\n         )\n \n-    def handle_spider_output(self, result: Union[Iterable, AsyncIterable], request: Request,\n-                             response: Response, spider: Spider) -> Deferred:\n+    def handle_spider_output(\n+        self,\n+        result: Union[Iterable, AsyncIterable],\n+        request: Request,\n+        response: Response,\n+        spider: Spider,\n+    ) -> Deferred:\n         if not result:\n             return defer_succeed(None)\n         it: Union[Generator, AsyncGenerator]\n         if isinstance(result, AsyncIterable):\n-            it = aiter_errback(result, self.handle_spider_error, request, response, spider)\n-            dfd = parallel_async(it, self.concurrent_items, self._process_spidermw_output,\n-                                 request, response, spider)\n+            it = aiter_errback(\n+                result, self.handle_spider_error, request, response, spider\n+            )\n+            dfd = parallel_async(\n+                it,\n+                self.concurrent_items,\n+                self._process_spidermw_output,\n+                request,\n+                response,\n+                spider,\n+            )\n         else:\n-            it = iter_errback(result, self.handle_spider_error, request, response, spider)\n-            dfd = parallel(it, self.concurrent_items, self._process_spidermw_output,\n-                           request, response, spider)\n+            it = iter_errback(\n+                result, self.handle_spider_error, request, response, spider\n+            )\n+            dfd = parallel(\n+                it,\n+                self.concurrent_items,\n+                self._process_spidermw_output,\n+                request,\n+                response,\n+                spider,\n+            )\n         return dfd\n \n-    def _process_spidermw_output(self, output: Any, request: Request, response: Response,\n-                                 spider: Spider) -> Optional[Deferred]:\n+    def _process_spidermw_output(\n+        self, output: Any, request: Request, response: Response, spider: Spider\n+    ) -> Optional[Deferred]:\n         \"\"\"Process each Request/Item (given in the output parameter) returned\n         from the given spider\n         \"\"\"\n@@ -245,14 +291,19 @@ class Scraper:\n         else:\n             typename = type(output).__name__\n             logger.error(\n-                'Spider must return request, item, or None, got %(typename)r in %(request)s',\n-                {'request': request, 'typename': typename},\n-                extra={'spider': spider},\n+                \"Spider must return request, item, or None, got %(typename)r in %(request)s\",\n+                {\"request\": request, \"typename\": typename},\n+                extra={\"spider\": spider},\n             )\n         return None\n \n-    def _log_download_errors(self, spider_failure: Failure, download_failure: Failure, request: Request,\n-                             spider: Spider) -> Union[Failure, None]:\n+    def _log_download_errors(\n+        self,\n+        spider_failure: Failure,\n+        download_failure: Failure,\n+        request: Request,\n+        spider: Spider,\n+    ) -> Union[Failure, None]:\n         \"\"\"Log and silence errors that come from the engine (typically download\n         errors that got propagated thru here).\n \n@@ -262,29 +313,33 @@ class Scraper:\n         \"\"\"\n         if not download_failure.check(IgnoreRequest):\n             if download_failure.frames:\n-                logkws = self.logformatter.download_error(download_failure, request, spider)\n+                logkws = self.logformatter.download_error(\n+                    download_failure, request, spider\n+                )\n                 logger.log(\n                     *logformatter_adapter(logkws),\n-                    extra={'spider': spider},\n+                    extra={\"spider\": spider},\n                     exc_info=failure_to_exc_info(download_failure),\n                 )\n             else:\n                 errmsg = download_failure.getErrorMessage()\n                 if errmsg:\n                     logkws = self.logformatter.download_error(\n-                        download_failure, request, spider, errmsg)\n+                        download_failure, request, spider, errmsg\n+                    )\n                     logger.log(\n                         *logformatter_adapter(logkws),\n-                        extra={'spider': spider},\n+                        extra={\"spider\": spider},\n                     )\n \n         if spider_failure is not download_failure:\n             return spider_failure\n         return None\n \n-    def _itemproc_finished(self, output: Any, item: Any, response: Response, spider: Spider) -> None:\n-        \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n-        \"\"\"\n+    def _itemproc_finished(\n+        self, output: Any, item: Any, response: Response, spider: Spider\n+    ) -> None:\n+        \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\"\"\"\n         assert self.slot is not None  # typing\n         self.slot.itemproc_size -= 1\n         if isinstance(output, Failure):\n@@ -292,19 +347,30 @@ class Scraper:\n             if isinstance(ex, DropItem):\n                 logkws = self.logformatter.dropped(item, ex, response, spider)\n                 if logkws is not None:\n-                    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n+                    logger.log(*logformatter_adapter(logkws), extra={\"spider\": spider})\n                 return self.signals.send_catch_log_deferred(\n-                    signal=signals.item_dropped, item=item, response=response,\n-                    spider=spider, exception=output.value)\n+                    signal=signals.item_dropped,\n+                    item=item,\n+                    response=response,\n+                    spider=spider,\n+                    exception=output.value,\n+                )\n             logkws = self.logformatter.item_error(item, ex, response, spider)\n-            logger.log(*logformatter_adapter(logkws), extra={'spider': spider},\n-                       exc_info=failure_to_exc_info(output))\n+            logger.log(\n+                *logformatter_adapter(logkws),\n+                extra={\"spider\": spider},\n+                exc_info=failure_to_exc_info(output),\n+            )\n             return self.signals.send_catch_log_deferred(\n-                signal=signals.item_error, item=item, response=response,\n-                spider=spider, failure=output)\n+                signal=signals.item_error,\n+                item=item,\n+                response=response,\n+                spider=spider,\n+                failure=output,\n+            )\n         logkws = self.logformatter.scraped(output, response, spider)\n         if logkws is not None:\n-            logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n+            logger.log(*logformatter_adapter(logkws), extra={\"spider\": spider})\n         return self.signals.send_catch_log_deferred(\n-            signal=signals.item_scraped, item=output, response=response,\n-            spider=spider)\n+            signal=signals.item_scraped, item=output, response=response, spider=spider\n+        )\n\n@@ -6,7 +6,17 @@ See documentation in docs/topics/spider-middleware.rst\n import logging\n from inspect import isasyncgenfunction, iscoroutine\n from itertools import islice\n-from typing import Any, AsyncGenerator, AsyncIterable, Callable, Generator, Iterable, Tuple, Union, cast\n+from typing import (\n+    Any,\n+    AsyncGenerator,\n+    AsyncIterable,\n+    Callable,\n+    Generator,\n+    Iterable,\n+    Tuple,\n+    Union,\n+    cast,\n+)\n \n from twisted.internet.defer import Deferred, inlineCallbacks\n from twisted.python.failure import Failure\n@@ -17,7 +27,12 @@ from scrapy.http import Response\n from scrapy.middleware import MiddlewareManager\n from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n from scrapy.utils.conf import build_component_list\n-from scrapy.utils.defer import mustbe_deferred, deferred_from_coro, deferred_f_from_coro_f, maybe_deferred_to_future\n+from scrapy.utils.defer import (\n+    mustbe_deferred,\n+    deferred_from_coro,\n+    deferred_f_from_coro_f,\n+    maybe_deferred_to_future,\n+)\n from scrapy.utils.python import MutableAsyncChain, MutableChain\n \n \n@@ -33,7 +48,7 @@ def _isiterable(o) -> bool:\n \n class SpiderMiddlewareManager(MiddlewareManager):\n \n-    component_name = 'spider middleware'\n+    component_name = \"spider middleware\"\n \n     def __init__(self, *middlewares):\n         super().__init__(*middlewares)\n@@ -41,28 +56,35 @@ class SpiderMiddlewareManager(MiddlewareManager):\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return build_component_list(settings.getwithbase('SPIDER_MIDDLEWARES'))\n+        return build_component_list(settings.getwithbase(\"SPIDER_MIDDLEWARES\"))\n \n     def _add_middleware(self, mw):\n         super()._add_middleware(mw)\n-        if hasattr(mw, 'process_spider_input'):\n-            self.methods['process_spider_input'].append(mw.process_spider_input)\n-        if hasattr(mw, 'process_start_requests'):\n-            self.methods['process_start_requests'].appendleft(mw.process_start_requests)\n-        process_spider_output = self._get_async_method_pair(mw, 'process_spider_output')\n-        self.methods['process_spider_output'].appendleft(process_spider_output)\n-        process_spider_exception = getattr(mw, 'process_spider_exception', None)\n-        self.methods['process_spider_exception'].appendleft(process_spider_exception)\n+        if hasattr(mw, \"process_spider_input\"):\n+            self.methods[\"process_spider_input\"].append(mw.process_spider_input)\n+        if hasattr(mw, \"process_start_requests\"):\n+            self.methods[\"process_start_requests\"].appendleft(mw.process_start_requests)\n+        process_spider_output = self._get_async_method_pair(mw, \"process_spider_output\")\n+        self.methods[\"process_spider_output\"].appendleft(process_spider_output)\n+        process_spider_exception = getattr(mw, \"process_spider_exception\", None)\n+        self.methods[\"process_spider_exception\"].appendleft(process_spider_exception)\n \n-    def _process_spider_input(self, scrape_func: ScrapeFunc, response: Response, request: Request,\n-                              spider: Spider) -> Any:\n-        for method in self.methods['process_spider_input']:\n+    def _process_spider_input(\n+        self,\n+        scrape_func: ScrapeFunc,\n+        response: Response,\n+        request: Request,\n+        spider: Spider,\n+    ) -> Any:\n+        for method in self.methods[\"process_spider_input\"]:\n             method = cast(Callable, method)\n             try:\n                 result = method(response=response, spider=spider)\n                 if result is not None:\n-                    msg = (f\"{method.__qualname__} must return None \"\n-                           f\"or raise an exception, got {type(result)}\")\n+                    msg = (\n+                        f\"{method.__qualname__} must return None \"\n+                        f\"or raise an exception, got {type(result)}\"\n+                    )\n                     raise _InvalidOutput(msg)\n             except _InvalidOutput:\n                 raise\n@@ -70,17 +92,22 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 return scrape_func(Failure(), request, spider)\n         return scrape_func(response, request, spider)\n \n-    def _evaluate_iterable(self, response: Response, spider: Spider, iterable: Union[Iterable, AsyncIterable],\n-                           exception_processor_index: int, recover_to: Union[MutableChain, MutableAsyncChain]\n+    def _evaluate_iterable(\n+        self,\n+        response: Response,\n+        spider: Spider,\n+        iterable: Union[Iterable, AsyncIterable],\n+        exception_processor_index: int,\n+        recover_to: Union[MutableChain, MutableAsyncChain],\n     ) -> Union[Generator, AsyncGenerator]:\n-\n         def process_sync(iterable: Iterable):\n             try:\n                 for r in iterable:\n                     yield r\n             except Exception as ex:\n-                exception_result = self._process_spider_exception(response, spider, Failure(ex),\n-                                                                  exception_processor_index)\n+                exception_result = self._process_spider_exception(\n+                    response, spider, Failure(ex), exception_processor_index\n+                )\n                 if isinstance(exception_result, Failure):\n                     raise\n                 recover_to.extend(exception_result)\n@@ -90,8 +117,9 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 async for r in iterable:\n                     yield r\n             except Exception as ex:\n-                exception_result = self._process_spider_exception(response, spider, Failure(ex),\n-                                                                  exception_processor_index)\n+                exception_result = self._process_spider_exception(\n+                    response, spider, Failure(ex), exception_processor_index\n+                )\n                 if isinstance(exception_result, Failure):\n                     raise\n                 recover_to.extend(exception_result)\n@@ -100,13 +128,20 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             return process_async(iterable)\n         return process_sync(iterable)\n \n-    def _process_spider_exception(self, response: Response, spider: Spider, _failure: Failure,\n-                                  start_index: int = 0) -> Union[Failure, MutableChain]:\n+    def _process_spider_exception(\n+        self,\n+        response: Response,\n+        spider: Spider,\n+        _failure: Failure,\n+        start_index: int = 0,\n+    ) -> Union[Failure, MutableChain]:\n         exception = _failure.value\n         # don't handle _InvalidOutput exception\n         if isinstance(exception, _InvalidOutput):\n             return _failure\n-        method_list = islice(self.methods['process_spider_exception'], start_index, None)\n+        method_list = islice(\n+            self.methods[\"process_spider_exception\"], start_index, None\n+        )\n         for method_index, method in enumerate(method_list, start=start_index):\n             if method is None:\n                 continue\n@@ -115,7 +150,9 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             if _isiterable(result):\n                 # stop exception handling by handing control over to the\n                 # process_spider_output chain if an iterable has been returned\n-                dfd: Deferred = self._process_spider_output(response, spider, result, method_index + 1)\n+                dfd: Deferred = self._process_spider_output(\n+                    response, spider, result, method_index + 1\n+                )\n                 # _process_spider_output() returns a Deferred only because of downgrading so this can be\n                 # simplified when downgrading is removed.\n                 if dfd.called:\n@@ -128,8 +165,10 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             elif result is None:\n                 continue\n             else:\n-                msg = (f\"{method.__qualname__} must return None \"\n-                       f\"or an iterable, got {type(result)}\")\n+                msg = (\n+                    f\"{method.__qualname__} must return None \"\n+                    f\"or an iterable, got {type(result)}\"\n+                )\n                 raise _InvalidOutput(msg)\n         return _failure\n \n@@ -137,8 +176,12 @@ class SpiderMiddlewareManager(MiddlewareManager):\n     # being available immediately which doesn't work when it's a wrapped coroutine.\n     # It also needs @inlineCallbacks only because of downgrading so it can be removed when downgrading is removed.\n     @inlineCallbacks\n-    def _process_spider_output(self, response: Response, spider: Spider,\n-                               result: Union[Iterable, AsyncIterable], start_index: int = 0\n+    def _process_spider_output(\n+        self,\n+        response: Response,\n+        spider: Spider,\n+        result: Union[Iterable, AsyncIterable],\n+        start_index: int = 0,\n     ) -> Deferred:\n         # items in this iterable do not need to go through the process_spider_output\n         # chain, they went through it already from the process_spider_exception method\n@@ -156,7 +199,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         # Storing methods and method tuples in the same list is weird but we should be able to roll this back\n         # when we drop this compatibility feature.\n \n-        method_list = islice(self.methods['process_spider_output'], start_index, None)\n+        method_list = islice(self.methods[\"process_spider_output\"], start_index, None)\n         for method_index, method_pair in enumerate(method_list, start=start_index):\n             if method_pair is None:\n                 continue\n@@ -177,24 +220,32 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                     result = as_async_generator(result)\n                 elif need_downgrade:\n                     if not self.downgrade_warning_done:\n-                        logger.warning(f\"Async iterable passed to {method.__qualname__} \"\n-                                       f\"was downgraded to a non-async one\")\n+                        logger.warning(\n+                            f\"Async iterable passed to {method.__qualname__} \"\n+                            f\"was downgraded to a non-async one\"\n+                        )\n                         self.downgrade_warning_done = True\n                     assert isinstance(result, AsyncIterable)\n                     # AsyncIterable -> Iterable\n                     result = yield deferred_from_coro(collect_asyncgen(result))\n                     if isinstance(recovered, AsyncIterable):\n-                        recovered_collected = yield deferred_from_coro(collect_asyncgen(recovered))\n+                        recovered_collected = yield deferred_from_coro(\n+                            collect_asyncgen(recovered)\n+                        )\n                         recovered = MutableChain(recovered_collected)\n                 # might fail directly if the output value is not a generator\n                 result = method(response=response, result=result, spider=spider)\n             except Exception as ex:\n-                exception_result = self._process_spider_exception(response, spider, Failure(ex), method_index + 1)\n+                exception_result = self._process_spider_exception(\n+                    response, spider, Failure(ex), method_index + 1\n+                )\n                 if isinstance(exception_result, Failure):\n                     raise\n                 return exception_result\n             if _isiterable(result):\n-                result = self._evaluate_iterable(response, spider, result, method_index + 1, recovered)\n+                result = self._evaluate_iterable(\n+                    response, spider, result, method_index + 1, recovered\n+                )\n             else:\n                 if iscoroutine(result):\n                     result.close()  # Silence warning about not awaiting\n@@ -214,7 +265,8 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             return MutableAsyncChain(result, recovered)\n         return MutableChain(result, recovered)  # type: ignore[arg-type]\n \n-    async def _process_callback_output(self, response: Response, spider: Spider, result: Union[Iterable, AsyncIterable]\n+    async def _process_callback_output(\n+        self, response: Response, spider: Spider, result: Union[Iterable, AsyncIterable]\n     ) -> Union[MutableChain, MutableAsyncChain]:\n         recovered: Union[MutableChain, MutableAsyncChain]\n         if isinstance(result, AsyncIterable):\n@@ -222,7 +274,9 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         else:\n             recovered = MutableChain()\n         result = self._evaluate_iterable(response, spider, result, 0, recovered)\n-        result = await maybe_deferred_to_future(self._process_spider_output(response, spider, result))\n+        result = await maybe_deferred_to_future(\n+            self._process_spider_output(response, spider, result)\n+        )\n         if isinstance(result, AsyncIterable):\n             return MutableAsyncChain(result, recovered)\n         if isinstance(recovered, AsyncIterable):\n@@ -230,41 +284,60 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             recovered = MutableChain(recovered_collected)\n         return MutableChain(result, recovered)  # type: ignore[arg-type]\n \n-    def scrape_response(self, scrape_func: ScrapeFunc, response: Response, request: Request,\n-                        spider: Spider) -> Deferred:\n-        async def process_callback_output(result: Union[Iterable, AsyncIterable]\n+    def scrape_response(\n+        self,\n+        scrape_func: ScrapeFunc,\n+        response: Response,\n+        request: Request,\n+        spider: Spider,\n+    ) -> Deferred:\n+        async def process_callback_output(\n+            result: Union[Iterable, AsyncIterable]\n         ) -> Union[MutableChain, MutableAsyncChain]:\n             return await self._process_callback_output(response, spider, result)\n \n         def process_spider_exception(_failure: Failure) -> Union[Failure, MutableChain]:\n             return self._process_spider_exception(response, spider, _failure)\n \n-        dfd = mustbe_deferred(self._process_spider_input, scrape_func, response, request, spider)\n-        dfd.addCallbacks(callback=deferred_f_from_coro_f(process_callback_output), errback=process_spider_exception)\n+        dfd = mustbe_deferred(\n+            self._process_spider_input, scrape_func, response, request, spider\n+        )\n+        dfd.addCallbacks(\n+            callback=deferred_f_from_coro_f(process_callback_output),\n+            errback=process_spider_exception,\n+        )\n         return dfd\n \n     def process_start_requests(self, start_requests, spider: Spider) -> Deferred:\n-        return self._process_chain('process_start_requests', start_requests, spider)\n+        return self._process_chain(\"process_start_requests\", start_requests, spider)\n \n     # This method is only needed until _async compatibility methods are removed.\n     @staticmethod\n-    def _get_async_method_pair(mw: Any, methodname: str) -> Union[None, Callable, Tuple[Callable, Callable]]:\n+    def _get_async_method_pair(\n+        mw: Any, methodname: str\n+    ) -> Union[None, Callable, Tuple[Callable, Callable]]:\n         normal_method = getattr(mw, methodname, None)\n         methodname_async = methodname + \"_async\"\n         async_method = getattr(mw, methodname_async, None)\n         if not async_method:\n             return normal_method\n         if not normal_method:\n-            logger.error(f\"Middleware {mw.__qualname__} has {methodname_async} \"\n-                         f\"without {methodname}, skipping this method.\")\n+            logger.error(\n+                f\"Middleware {mw.__qualname__} has {methodname_async} \"\n+                f\"without {methodname}, skipping this method.\"\n+            )\n             return None\n         if not isasyncgenfunction(async_method):\n-            logger.error(f\"{async_method.__qualname__} is not \"\n-                         f\"an async generator function, skipping this method.\")\n+            logger.error(\n+                f\"{async_method.__qualname__} is not \"\n+                f\"an async generator function, skipping this method.\"\n+            )\n             return normal_method\n         if isasyncgenfunction(normal_method):\n-            logger.error(f\"{normal_method.__qualname__} is an async \"\n+            logger.error(\n+                f\"{normal_method.__qualname__} is an async \"\n                 f\"generator function while {methodname_async} exists, \"\n-                         f\"skipping both methods.\")\n+                f\"skipping both methods.\"\n+            )\n             return None\n         return normal_method, async_method\n\n@@ -49,10 +49,9 @@ logger = logging.getLogger(__name__)\n \n \n class Crawler:\n-\n     def __init__(self, spidercls, settings=None, init_reactor: bool = False):\n         if isinstance(spidercls, Spider):\n-            raise ValueError('The spidercls argument must be a class, not an object')\n+            raise ValueError(\"The spidercls argument must be a class, not an object\")\n \n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n@@ -63,14 +62,15 @@ class Crawler:\n \n         self.signals = SignalManager(self)\n \n-        self.stats = load_object(self.settings['STATS_CLASS'])(self)\n+        self.stats = load_object(self.settings[\"STATS_CLASS\"])(self)\n \n-        handler = LogCounterHandler(self, level=self.settings.get('LOG_LEVEL'))\n+        handler = LogCounterHandler(self, level=self.settings.get(\"LOG_LEVEL\"))\n         logging.root.addHandler(handler)\n \n         d = dict(overridden_settings(self.settings))\n-        logger.info(\"Overridden settings:\\n%(settings)s\",\n-                    {'settings': pprint.pformat(d)})\n+        logger.info(\n+            \"Overridden settings:\\n%(settings)s\", {\"settings\": pprint.pformat(d)}\n+        )\n \n         if get_scrapy_root_handler() is not None:\n             # scrapy root handler already installed: update it with new settings\n@@ -80,11 +80,11 @@ class Crawler:\n         self.__remove_handler = lambda: logging.root.removeHandler(handler)\n         self.signals.connect(self.__remove_handler, signals.engine_stopped)\n \n-        lf_cls = load_object(self.settings['LOG_FORMATTER'])\n+        lf_cls = load_object(self.settings[\"LOG_FORMATTER\"])\n         self.logformatter = lf_cls.from_crawler(self)\n \n         self.request_fingerprinter: RequestFingerprinter = create_instance(\n-            load_object(self.settings['REQUEST_FINGERPRINTER_CLASS']),\n+            load_object(self.settings[\"REQUEST_FINGERPRINTER_CLASS\"]),\n             settings=self.settings,\n             crawler=self,\n         )\n@@ -160,23 +160,26 @@ class CrawlerRunner:\n     crawlers = property(\n         lambda self: self._crawlers,\n         doc=\"Set of :class:`crawlers <scrapy.crawler.Crawler>` started by \"\n-            \":meth:`crawl` and managed by this class.\"\n+        \":meth:`crawl` and managed by this class.\",\n     )\n \n     @staticmethod\n     def _get_spider_loader(settings):\n         \"\"\"Get SpiderLoader instance from settings\"\"\"\n-        cls_path = settings.get('SPIDER_LOADER_CLASS')\n+        cls_path = settings.get(\"SPIDER_LOADER_CLASS\")\n         loader_cls = load_object(cls_path)\n-        excs = (DoesNotImplement, MultipleInvalid) if MultipleInvalid else DoesNotImplement\n+        excs = (\n+            (DoesNotImplement, MultipleInvalid) if MultipleInvalid else DoesNotImplement\n+        )\n         try:\n             verifyClass(ISpiderLoader, loader_cls)\n         except excs:\n             warnings.warn(\n-                'SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does '\n-                'not fully implement scrapy.interfaces.ISpiderLoader interface. '\n-                'Please add all missing methods to avoid unexpected runtime errors.',\n-                category=ScrapyDeprecationWarning, stacklevel=2\n+                \"SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does \"\n+                \"not fully implement scrapy.interfaces.ISpiderLoader interface. \"\n+                \"Please add all missing methods to avoid unexpected runtime errors.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n             )\n         return loader_cls.from_settings(settings.frozencopy())\n \n@@ -191,9 +194,12 @@ class CrawlerRunner:\n \n     @property\n     def spiders(self):\n-        warnings.warn(\"CrawlerRunner.spiders attribute is renamed to \"\n+        warnings.warn(\n+            \"CrawlerRunner.spiders attribute is renamed to \"\n             \"CrawlerRunner.spider_loader.\",\n-                      category=ScrapyDeprecationWarning, stacklevel=2)\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n         return self.spider_loader\n \n     def crawl(self, crawler_or_spidercls, *args, **kwargs):\n@@ -220,8 +226,9 @@ class CrawlerRunner:\n         \"\"\"\n         if isinstance(crawler_or_spidercls, Spider):\n             raise ValueError(\n-                'The crawler_or_spidercls argument cannot be a spider object, '\n-                'it must be a spider class (or a Crawler object)')\n+                \"The crawler_or_spidercls argument cannot be a spider object, \"\n+                \"it must be a spider class (or a Crawler object)\"\n+            )\n         crawler = self.create_crawler(crawler_or_spidercls)\n         return self._crawl(crawler, *args, **kwargs)\n \n@@ -233,7 +240,7 @@ class CrawlerRunner:\n         def _done(result):\n             self.crawlers.discard(crawler)\n             self._active.discard(d)\n-            self.bootstrap_failed |= not getattr(crawler, 'spider', None)\n+            self.bootstrap_failed |= not getattr(crawler, \"spider\", None)\n             return result\n \n         return d.addBoth(_done)\n@@ -251,8 +258,9 @@ class CrawlerRunner:\n         \"\"\"\n         if isinstance(crawler_or_spidercls, Spider):\n             raise ValueError(\n-                'The crawler_or_spidercls argument cannot be a spider object, '\n-                'it must be a spider class (or a Crawler object)')\n+                \"The crawler_or_spidercls argument cannot be a spider object, \"\n+                \"it must be a spider class (or a Crawler object)\"\n+            )\n         if isinstance(crawler_or_spidercls, Crawler):\n             return crawler_or_spidercls\n         return self._create_crawler(crawler_or_spidercls)\n@@ -314,18 +322,23 @@ class CrawlerProcess(CrawlerRunner):\n \n     def _signal_shutdown(self, signum, _):\n         from twisted.internet import reactor\n+\n         install_shutdown_handlers(self._signal_kill)\n         signame = signal_names[signum]\n-        logger.info(\"Received %(signame)s, shutting down gracefully. Send again to force \",\n-                    {'signame': signame})\n+        logger.info(\n+            \"Received %(signame)s, shutting down gracefully. Send again to force \",\n+            {\"signame\": signame},\n+        )\n         reactor.callFromThread(self._graceful_stop_reactor)\n \n     def _signal_kill(self, signum, _):\n         from twisted.internet import reactor\n+\n         install_shutdown_handlers(signal.SIG_IGN)\n         signame = signal_names[signum]\n-        logger.info('Received %(signame)s twice, forcing unclean shutdown',\n-                    {'signame': signame})\n+        logger.info(\n+            \"Received %(signame)s twice, forcing unclean shutdown\", {\"signame\": signame}\n+        )\n         reactor.callFromThread(self._stop_reactor)\n \n     def _create_crawler(self, spidercls):\n@@ -351,6 +364,7 @@ class CrawlerProcess(CrawlerRunner):\n             handlers (default: True)\n         \"\"\"\n         from twisted.internet import reactor\n+\n         if stop_after_crawl:\n             d = self.join()\n             # Don't start the reactor if the deferreds are already fired\n@@ -364,8 +378,8 @@ class CrawlerProcess(CrawlerRunner):\n         resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)\n         resolver.install_on_reactor()\n         tp = reactor.getThreadPool()\n-        tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n-        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n+        tp.adjustPoolsize(maxthreads=self.settings.getint(\"REACTOR_THREADPOOL_MAXSIZE\"))\n+        reactor.addSystemEventTrigger(\"before\", \"shutdown\", self.stop)\n         reactor.run(installSignalHandlers=False)  # blocking call\n \n     def _graceful_stop_reactor(self):\n@@ -375,6 +389,7 @@ class CrawlerProcess(CrawlerRunner):\n \n     def _stop_reactor(self, _=None):\n         from twisted.internet import reactor\n+\n         try:\n             reactor.stop()\n         except RuntimeError:  # raised if already stopped or in shutdown stage\n\n@@ -17,14 +17,14 @@ class AjaxCrawlMiddleware:\n     \"\"\"\n \n     def __init__(self, settings):\n-        if not settings.getbool('AJAXCRAWL_ENABLED'):\n+        if not settings.getbool(\"AJAXCRAWL_ENABLED\"):\n             raise NotConfigured\n \n         # XXX: Google parses at least first 100k bytes; scrapy's redirect\n         # middleware parses first 4k. 4k turns out to be insufficient\n         # for this middleware, and parsing 100k could be slow.\n         # We use something in between (32K) by default.\n-        self.lookup_bytes = settings.getint('AJAXCRAWL_MAXSIZE', 32768)\n+        self.lookup_bytes = settings.getint(\"AJAXCRAWL_MAXSIZE\", 32768)\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -35,23 +35,25 @@ class AjaxCrawlMiddleware:\n         if not isinstance(response, HtmlResponse) or response.status != 200:\n             return response\n \n-        if request.method != 'GET':\n+        if request.method != \"GET\":\n             # other HTTP methods are either not safe or don't have a body\n             return response\n \n-        if 'ajax_crawlable' in request.meta:  # prevent loops\n+        if \"ajax_crawlable\" in request.meta:  # prevent loops\n             return response\n \n         if not self._has_ajax_crawlable_variant(response):\n             return response\n \n         # scrapy already handles #! links properly\n-        ajax_crawl_request = request.replace(url=request.url + '#!')\n-        logger.debug(\"Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s\",\n-                     {'ajax_crawl_request': ajax_crawl_request, 'request': request},\n-                     extra={'spider': spider})\n+        ajax_crawl_request = request.replace(url=request.url + \"#!\")\n+        logger.debug(\n+            \"Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s\",\n+            {\"ajax_crawl_request\": ajax_crawl_request, \"request\": request},\n+            extra={\"spider\": spider},\n+        )\n \n-        ajax_crawl_request.meta['ajax_crawlable'] = True\n+        ajax_crawl_request.meta[\"ajax_crawlable\"] = True\n         return ajax_crawl_request\n \n     def _has_ajax_crawlable_variant(self, response):\n@@ -64,7 +66,9 @@ class AjaxCrawlMiddleware:\n \n \n # XXX: move it to w3lib?\n-_ajax_crawlable_re = re.compile(r'<meta\\s+name=[\"\\']fragment[\"\\']\\s+content=[\"\\']![\"\\']/?>')\n+_ajax_crawlable_re = re.compile(\n+    r'<meta\\s+name=[\"\\']fragment[\"\\']\\s+content=[\"\\']![\"\\']/?>'\n+)\n \n \n def _has_ajaxcrawlable_meta(text):\n@@ -82,12 +86,12 @@ def _has_ajaxcrawlable_meta(text):\n     # Stripping scripts and comments is slow (about 20x slower than\n     # just checking if a string is in text); this is a quick fail-fast\n     # path that should work for most pages.\n-    if 'fragment' not in text:\n+    if \"fragment\" not in text:\n         return False\n-    if 'content' not in text:\n+    if \"content\" not in text:\n         return False\n \n-    text = html.remove_tags_with_content(text, ('script', 'noscript'))\n+    text = html.remove_tags_with_content(text, (\"script\", \"noscript\"))\n     text = html.replace_entities(text)\n     text = html.remove_comments(text)\n     return _ajax_crawlable_re.search(text) is not None\n\n@@ -29,14 +29,14 @@ class CookiesMiddleware:\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        if not crawler.settings.getbool('COOKIES_ENABLED'):\n+        if not crawler.settings.getbool(\"COOKIES_ENABLED\"):\n             raise NotConfigured\n-        return cls(crawler.settings.getbool('COOKIES_DEBUG'))\n+        return cls(crawler.settings.getbool(\"COOKIES_DEBUG\"))\n \n     def _process_cookies(self, cookies, *, jar, request):\n         for cookie in cookies:\n             cookie_domain = cookie.domain\n-            if cookie_domain.startswith('.'):\n+            if cookie_domain.startswith(\".\"):\n                 cookie_domain = cookie_domain[1:]\n \n             request_domain = urlparse_cached(request).hostname.lower()\n@@ -49,7 +49,7 @@ class CookiesMiddleware:\n             jar.set_cookie_if_ok(cookie, request)\n \n     def process_request(self, request, spider):\n-        if request.meta.get('dont_merge_cookies', False):\n+        if request.meta.get(\"dont_merge_cookies\", False):\n             return\n \n         cookiejarkey = request.meta.get(\"cookiejar\")\n@@ -58,12 +58,12 @@ class CookiesMiddleware:\n         self._process_cookies(cookies, jar=jar, request=request)\n \n         # set Cookie header\n-        request.headers.pop('Cookie', None)\n+        request.headers.pop(\"Cookie\", None)\n         jar.add_cookie_header(request)\n         self._debug_cookie(request, spider)\n \n     def process_response(self, request, response, spider):\n-        if request.meta.get('dont_merge_cookies', False):\n+        if request.meta.get(\"dont_merge_cookies\", False):\n             return response\n \n         # extract cookies from Set-Cookie and drop invalid/expired cookies\n@@ -78,21 +78,25 @@ class CookiesMiddleware:\n \n     def _debug_cookie(self, request, spider):\n         if self.debug:\n-            cl = [to_unicode(c, errors='replace')\n-                  for c in request.headers.getlist('Cookie')]\n+            cl = [\n+                to_unicode(c, errors=\"replace\")\n+                for c in request.headers.getlist(\"Cookie\")\n+            ]\n             if cl:\n                 cookies = \"\\n\".join(f\"Cookie: {c}\\n\" for c in cl)\n                 msg = f\"Sending cookies to: {request}\\n{cookies}\"\n-                logger.debug(msg, extra={'spider': spider})\n+                logger.debug(msg, extra={\"spider\": spider})\n \n     def _debug_set_cookie(self, response, spider):\n         if self.debug:\n-            cl = [to_unicode(c, errors='replace')\n-                  for c in response.headers.getlist('Set-Cookie')]\n+            cl = [\n+                to_unicode(c, errors=\"replace\")\n+                for c in response.headers.getlist(\"Set-Cookie\")\n+            ]\n             if cl:\n                 cookies = \"\\n\".join(f\"Set-Cookie: {c}\\n\" for c in cl)\n                 msg = f\"Received cookies from: {response}\\n{cookies}\"\n-                logger.debug(msg, extra={'spider': spider})\n+                logger.debug(msg, extra={\"spider\": spider})\n \n     def _format_cookie(self, cookie, request):\n         \"\"\"\n@@ -113,8 +117,11 @@ class CookiesMiddleware:\n                 try:\n                     decoded[key] = cookie[key].decode(\"utf8\")\n                 except UnicodeDecodeError:\n-                    logger.warning(\"Non UTF-8 encoded cookie found in request %s: %s\",\n-                                   request, cookie)\n+                    logger.warning(\n+                        \"Non UTF-8 encoded cookie found in request %s: %s\",\n+                        request,\n+                        cookie,\n+                    )\n                     decoded[key] = cookie[key].decode(\"latin1\", errors=\"replace\")\n \n         cookie_str = f\"{decoded.pop('name')}={decoded.pop('value')}\"\n\n@@ -16,7 +16,7 @@ from scrapy.responsetypes import responsetypes\n \n \n warn(\n-    'scrapy.downloadermiddlewares.decompression is deprecated',\n+    \"scrapy.downloadermiddlewares.decompression is deprecated\",\n     ScrapyDeprecationWarning,\n     stacklevel=2,\n )\n@@ -31,10 +31,10 @@ class DecompressionMiddleware:\n \n     def __init__(self):\n         self._formats = {\n-            'tar': self._is_tar,\n-            'zip': self._is_zip,\n-            'gz': self._is_gzip,\n-            'bz2': self._is_bzip2\n+            \"tar\": self._is_tar,\n+            \"zip\": self._is_zip,\n+            \"gz\": self._is_gzip,\n+            \"bz2\": self._is_bzip2,\n         }\n \n     def _is_tar(self, response):\n@@ -86,7 +86,10 @@ class DecompressionMiddleware:\n         for fmt, func in self._formats.items():\n             new_response = func(response)\n             if new_response:\n-                logger.debug('Decompressed response with format: %(responsefmt)s',\n-                             {'responsefmt': fmt}, extra={'spider': spider})\n+                logger.debug(\n+                    \"Decompressed response with format: %(responsefmt)s\",\n+                    {\"responsefmt\": fmt},\n+                    extra={\"spider\": spider},\n+                )\n                 return new_response\n         return response\n\n@@ -8,13 +8,12 @@ from scrapy.utils.python import without_none_values\n \n \n class DefaultHeadersMiddleware:\n-\n     def __init__(self, headers):\n         self._headers = headers\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        headers = without_none_values(crawler.settings['DEFAULT_REQUEST_HEADERS'])\n+        headers = without_none_values(crawler.settings[\"DEFAULT_REQUEST_HEADERS\"])\n         return cls(headers.items())\n \n     def process_request(self, request, spider):\n\n@@ -8,19 +8,18 @@ from scrapy import signals\n \n \n class DownloadTimeoutMiddleware:\n-\n     def __init__(self, timeout=180):\n         self._timeout = timeout\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        o = cls(crawler.settings.getfloat('DOWNLOAD_TIMEOUT'))\n+        o = cls(crawler.settings.getfloat(\"DOWNLOAD_TIMEOUT\"))\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         return o\n \n     def spider_opened(self, spider):\n-        self._timeout = getattr(spider, 'download_timeout', self._timeout)\n+        self._timeout = getattr(spider, \"download_timeout\", self._timeout)\n \n     def process_request(self, request, spider):\n         if self._timeout:\n-            request.meta.setdefault('download_timeout', self._timeout)\n+            request.meta.setdefault(\"download_timeout\", self._timeout)\n\n@@ -24,27 +24,29 @@ class HttpAuthMiddleware:\n         return o\n \n     def spider_opened(self, spider):\n-        usr = getattr(spider, 'http_user', '')\n-        pwd = getattr(spider, 'http_pass', '')\n+        usr = getattr(spider, \"http_user\", \"\")\n+        pwd = getattr(spider, \"http_pass\", \"\")\n         if usr or pwd:\n             self.auth = basic_auth_header(usr, pwd)\n-            if not hasattr(spider, 'http_auth_domain'):\n-                warnings.warn('Using HttpAuthMiddleware without http_auth_domain is deprecated and can cause security '\n-                              'problems if the spider makes requests to several different domains. http_auth_domain '\n-                              'will be set to the domain of the first request, please set it to the correct value '\n-                              'explicitly.',\n-                              category=ScrapyDeprecationWarning)\n+            if not hasattr(spider, \"http_auth_domain\"):\n+                warnings.warn(\n+                    \"Using HttpAuthMiddleware without http_auth_domain is deprecated and can cause security \"\n+                    \"problems if the spider makes requests to several different domains. http_auth_domain \"\n+                    \"will be set to the domain of the first request, please set it to the correct value \"\n+                    \"explicitly.\",\n+                    category=ScrapyDeprecationWarning,\n+                )\n                 self.domain_unset = True\n             else:\n                 self.domain = spider.http_auth_domain\n                 self.domain_unset = False\n \n     def process_request(self, request, spider):\n-        auth = getattr(self, 'auth', None)\n-        if auth and b'Authorization' not in request.headers:\n+        auth = getattr(self, \"auth\", None)\n+        if auth and b\"Authorization\" not in request.headers:\n             domain = urlparse_cached(request).hostname\n             if self.domain_unset:\n                 self.domain = domain\n                 self.domain_unset = False\n             if not self.domain or url_is_from_any_domain(request.url, [self.domain]):\n-                request.headers[b'Authorization'] = auth\n+                request.headers[b\"Authorization\"] = auth\n\n@@ -29,21 +29,31 @@ HttpCacheMiddlewareTV = TypeVar(\"HttpCacheMiddlewareTV\", bound=\"HttpCacheMiddlew\n \n class HttpCacheMiddleware:\n \n-    DOWNLOAD_EXCEPTIONS = (defer.TimeoutError, TimeoutError, DNSLookupError,\n-                           ConnectionRefusedError, ConnectionDone, ConnectError,\n-                           ConnectionLost, TCPTimedOutError, ResponseFailed,\n-                           IOError)\n+    DOWNLOAD_EXCEPTIONS = (\n+        defer.TimeoutError,\n+        TimeoutError,\n+        DNSLookupError,\n+        ConnectionRefusedError,\n+        ConnectionDone,\n+        ConnectError,\n+        ConnectionLost,\n+        TCPTimedOutError,\n+        ResponseFailed,\n+        IOError,\n+    )\n \n     def __init__(self, settings: Settings, stats: StatsCollector) -> None:\n-        if not settings.getbool('HTTPCACHE_ENABLED'):\n+        if not settings.getbool(\"HTTPCACHE_ENABLED\"):\n             raise NotConfigured\n-        self.policy = load_object(settings['HTTPCACHE_POLICY'])(settings)\n-        self.storage = load_object(settings['HTTPCACHE_STORAGE'])(settings)\n-        self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')\n+        self.policy = load_object(settings[\"HTTPCACHE_POLICY\"])(settings)\n+        self.storage = load_object(settings[\"HTTPCACHE_STORAGE\"])(settings)\n+        self.ignore_missing = settings.getbool(\"HTTPCACHE_IGNORE_MISSING\")\n         self.stats = stats\n \n     @classmethod\n-    def from_crawler(cls: Type[HttpCacheMiddlewareTV], crawler: Crawler) -> HttpCacheMiddlewareTV:\n+    def from_crawler(\n+        cls: Type[HttpCacheMiddlewareTV], crawler: Crawler\n+    ) -> HttpCacheMiddlewareTV:\n         o = cls(crawler.settings, crawler.stats)\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n@@ -56,78 +66,86 @@ class HttpCacheMiddleware:\n         self.storage.close_spider(spider)\n \n     def process_request(self, request: Request, spider: Spider) -> Optional[Response]:\n-        if request.meta.get('dont_cache', False):\n+        if request.meta.get(\"dont_cache\", False):\n             return None\n \n         # Skip uncacheable requests\n         if not self.policy.should_cache_request(request):\n-            request.meta['_dont_cache'] = True  # flag as uncacheable\n+            request.meta[\"_dont_cache\"] = True  # flag as uncacheable\n             return None\n \n         # Look for cached response and check if expired\n         cachedresponse = self.storage.retrieve_response(spider, request)\n         if cachedresponse is None:\n-            self.stats.inc_value('httpcache/miss', spider=spider)\n+            self.stats.inc_value(\"httpcache/miss\", spider=spider)\n             if self.ignore_missing:\n-                self.stats.inc_value('httpcache/ignore', spider=spider)\n+                self.stats.inc_value(\"httpcache/ignore\", spider=spider)\n                 raise IgnoreRequest(f\"Ignored request not in cache: {request}\")\n             return None  # first time request\n \n         # Return cached response only if not expired\n-        cachedresponse.flags.append('cached')\n+        cachedresponse.flags.append(\"cached\")\n         if self.policy.is_cached_response_fresh(cachedresponse, request):\n-            self.stats.inc_value('httpcache/hit', spider=spider)\n+            self.stats.inc_value(\"httpcache/hit\", spider=spider)\n             return cachedresponse\n \n         # Keep a reference to cached response to avoid a second cache lookup on\n         # process_response hook\n-        request.meta['cached_response'] = cachedresponse\n+        request.meta[\"cached_response\"] = cachedresponse\n \n         return None\n \n-    def process_response(self, request: Request, response: Response, spider: Spider) -> Response:\n-        if request.meta.get('dont_cache', False):\n+    def process_response(\n+        self, request: Request, response: Response, spider: Spider\n+    ) -> Response:\n+        if request.meta.get(\"dont_cache\", False):\n             return response\n \n         # Skip cached responses and uncacheable requests\n-        if 'cached' in response.flags or '_dont_cache' in request.meta:\n-            request.meta.pop('_dont_cache', None)\n+        if \"cached\" in response.flags or \"_dont_cache\" in request.meta:\n+            request.meta.pop(\"_dont_cache\", None)\n             return response\n \n         # RFC2616 requires origin server to set Date header,\n         # https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18\n-        if 'Date' not in response.headers:\n-            response.headers['Date'] = formatdate(usegmt=True)\n+        if \"Date\" not in response.headers:\n+            response.headers[\"Date\"] = formatdate(usegmt=True)\n \n         # Do not validate first-hand responses\n-        cachedresponse = request.meta.pop('cached_response', None)\n+        cachedresponse = request.meta.pop(\"cached_response\", None)\n         if cachedresponse is None:\n-            self.stats.inc_value('httpcache/firsthand', spider=spider)\n+            self.stats.inc_value(\"httpcache/firsthand\", spider=spider)\n             self._cache_response(spider, response, request, cachedresponse)\n             return response\n \n         if self.policy.is_cached_response_valid(cachedresponse, response, request):\n-            self.stats.inc_value('httpcache/revalidate', spider=spider)\n+            self.stats.inc_value(\"httpcache/revalidate\", spider=spider)\n             return cachedresponse\n \n-        self.stats.inc_value('httpcache/invalidate', spider=spider)\n+        self.stats.inc_value(\"httpcache/invalidate\", spider=spider)\n         self._cache_response(spider, response, request, cachedresponse)\n         return response\n \n     def process_exception(\n         self, request: Request, exception: Exception, spider: Spider\n     ) -> Optional[Response]:\n-        cachedresponse = request.meta.pop('cached_response', None)\n-        if cachedresponse is not None and isinstance(exception, self.DOWNLOAD_EXCEPTIONS):\n-            self.stats.inc_value('httpcache/errorrecovery', spider=spider)\n+        cachedresponse = request.meta.pop(\"cached_response\", None)\n+        if cachedresponse is not None and isinstance(\n+            exception, self.DOWNLOAD_EXCEPTIONS\n+        ):\n+            self.stats.inc_value(\"httpcache/errorrecovery\", spider=spider)\n             return cachedresponse\n         return None\n \n     def _cache_response(\n-        self, spider: Spider, response: Response, request: Request, cachedresponse: Optional[Response]\n+        self,\n+        spider: Spider,\n+        response: Response,\n+        request: Request,\n+        cachedresponse: Optional[Response],\n     ) -> None:\n         if self.policy.should_cache_response(response, request):\n-            self.stats.inc_value('httpcache/store', spider=spider)\n+            self.stats.inc_value(\"httpcache/store\", spider=spider)\n             self.storage.store_response(spider, request, response)\n         else:\n-            self.stats.inc_value('httpcache/uncacheable', spider=spider)\n+            self.stats.inc_value(\"httpcache/uncacheable\", spider=spider)\n\n@@ -8,17 +8,19 @@ from scrapy.responsetypes import responsetypes\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.gz import gunzip\n \n-ACCEPTED_ENCODINGS = [b'gzip', b'deflate']\n+ACCEPTED_ENCODINGS = [b\"gzip\", b\"deflate\"]\n \n try:\n     import brotli\n-    ACCEPTED_ENCODINGS.append(b'br')\n+\n+    ACCEPTED_ENCODINGS.append(b\"br\")\n except ImportError:\n     pass\n \n try:\n     import zstandard\n-    ACCEPTED_ENCODINGS.append(b'zstd')\n+\n+    ACCEPTED_ENCODINGS.append(b\"zstd\")\n except ImportError:\n     pass\n \n@@ -26,12 +28,13 @@ except ImportError:\n class HttpCompressionMiddleware:\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n     sent/received from web sites\"\"\"\n+\n     def __init__(self, stats=None):\n         self.stats = stats\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        if not crawler.settings.getbool('COMPRESSION_ENABLED'):\n+        if not crawler.settings.getbool(\"COMPRESSION_ENABLED\"):\n             raise NotConfigured\n         try:\n             return cls(stats=crawler.stats)\n@@ -47,21 +50,26 @@ class HttpCompressionMiddleware:\n             return result\n \n     def process_request(self, request, spider):\n-        request.headers.setdefault('Accept-Encoding',\n-                                   b\", \".join(ACCEPTED_ENCODINGS))\n+        request.headers.setdefault(\"Accept-Encoding\", b\", \".join(ACCEPTED_ENCODINGS))\n \n     def process_response(self, request, response, spider):\n \n-        if request.method == 'HEAD':\n+        if request.method == \"HEAD\":\n             return response\n         if isinstance(response, Response):\n-            content_encoding = response.headers.getlist('Content-Encoding')\n+            content_encoding = response.headers.getlist(\"Content-Encoding\")\n             if content_encoding:\n                 encoding = content_encoding.pop()\n                 decoded_body = self._decode(response.body, encoding.lower())\n                 if self.stats:\n-                    self.stats.inc_value('httpcompression/response_bytes', len(decoded_body), spider=spider)\n-                    self.stats.inc_value('httpcompression/response_count', spider=spider)\n+                    self.stats.inc_value(\n+                        \"httpcompression/response_bytes\",\n+                        len(decoded_body),\n+                        spider=spider,\n+                    )\n+                    self.stats.inc_value(\n+                        \"httpcompression/response_count\", spider=spider\n+                    )\n                 respcls = responsetypes.from_args(\n                     headers=response.headers, url=response.url, body=decoded_body\n                 )\n@@ -69,18 +77,18 @@ class HttpCompressionMiddleware:\n                 if issubclass(respcls, TextResponse):\n                     # force recalculating the encoding until we make sure the\n                     # responsetypes guessing is reliable\n-                    kwargs['encoding'] = None\n+                    kwargs[\"encoding\"] = None\n                 response = response.replace(**kwargs)\n                 if not content_encoding:\n-                    del response.headers['Content-Encoding']\n+                    del response.headers[\"Content-Encoding\"]\n \n         return response\n \n     def _decode(self, body, encoding):\n-        if encoding == b'gzip' or encoding == b'x-gzip':\n+        if encoding == b\"gzip\" or encoding == b\"x-gzip\":\n             body = gunzip(body)\n \n-        if encoding == b'deflate':\n+        if encoding == b\"deflate\":\n             try:\n                 body = zlib.decompress(body)\n             except zlib.error:\n@@ -90,9 +98,9 @@ class HttpCompressionMiddleware:\n                 # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\n                 # http://www.gzip.org/zlib/zlib_faq.html#faq38\n                 body = zlib.decompress(body, -15)\n-        if encoding == b'br' and b'br' in ACCEPTED_ENCODINGS:\n+        if encoding == b\"br\" and b\"br\" in ACCEPTED_ENCODINGS:\n             body = brotli.decompress(body)\n-        if encoding == b'zstd' and b'zstd' in ACCEPTED_ENCODINGS:\n+        if encoding == b\"zstd\" and b\"zstd\" in ACCEPTED_ENCODINGS:\n             # Using its streaming API since its simple API could handle only cases\n             # where there is content size data embedded in the frame\n             reader = zstandard.ZstdDecompressor().stream_reader(io.BytesIO(body))\n\n@@ -8,8 +8,7 @@ from scrapy.utils.python import to_bytes\n \n \n class HttpProxyMiddleware:\n-\n-    def __init__(self, auth_encoding='latin-1'):\n+    def __init__(self, auth_encoding=\"latin-1\"):\n         self.auth_encoding = auth_encoding\n         self.proxies = {}\n         for type_, url in getproxies().items():\n@@ -22,20 +21,20 @@ class HttpProxyMiddleware:\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        if not crawler.settings.getbool('HTTPPROXY_ENABLED'):\n+        if not crawler.settings.getbool(\"HTTPPROXY_ENABLED\"):\n             raise NotConfigured\n-        auth_encoding = crawler.settings.get('HTTPPROXY_AUTH_ENCODING')\n+        auth_encoding = crawler.settings.get(\"HTTPPROXY_AUTH_ENCODING\")\n         return cls(auth_encoding)\n \n     def _basic_auth_header(self, username, password):\n         user_pass = to_bytes(\n-            f'{unquote(username)}:{unquote(password)}',\n-            encoding=self.auth_encoding)\n+            f\"{unquote(username)}:{unquote(password)}\", encoding=self.auth_encoding\n+        )\n         return base64.b64encode(user_pass)\n \n     def _get_proxy(self, url, orig_type):\n         proxy_type, user, password, hostport = _parse_proxy(url)\n-        proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n+        proxy_url = urlunparse((proxy_type or orig_type, hostport, \"\", \"\", \"\", \"\"))\n \n         if user:\n             creds = self._basic_auth_header(user, password)\n@@ -46,39 +45,36 @@ class HttpProxyMiddleware:\n \n     def process_request(self, request, spider):\n         creds, proxy_url = None, None\n-        if 'proxy' in request.meta:\n-            if request.meta['proxy'] is not None:\n-                creds, proxy_url = self._get_proxy(request.meta['proxy'], '')\n+        if \"proxy\" in request.meta:\n+            if request.meta[\"proxy\"] is not None:\n+                creds, proxy_url = self._get_proxy(request.meta[\"proxy\"], \"\")\n         elif self.proxies:\n             parsed = urlparse_cached(request)\n             scheme = parsed.scheme\n             if (\n-                (\n                 # 'no_proxy' is only supported by http schemes\n-                    scheme not in ('http', 'https')\n+                scheme not in (\"http\", \"https\")\n                 or not proxy_bypass(parsed.hostname)\n-                )\n-                and scheme in self.proxies\n-            ):\n+            ) and scheme in self.proxies:\n                 creds, proxy_url = self.proxies[scheme]\n \n         self._set_proxy_and_creds(request, proxy_url, creds)\n \n     def _set_proxy_and_creds(self, request, proxy_url, creds):\n         if proxy_url:\n-            request.meta['proxy'] = proxy_url\n-        elif request.meta.get('proxy') is not None:\n-            request.meta['proxy'] = None\n+            request.meta[\"proxy\"] = proxy_url\n+        elif request.meta.get(\"proxy\") is not None:\n+            request.meta[\"proxy\"] = None\n         if creds:\n-            request.headers[b'Proxy-Authorization'] = b'Basic ' + creds\n-            request.meta['_auth_proxy'] = proxy_url\n-        elif '_auth_proxy' in request.meta:\n-            if proxy_url != request.meta['_auth_proxy']:\n-                if b'Proxy-Authorization' in request.headers:\n-                    del request.headers[b'Proxy-Authorization']\n-                del request.meta['_auth_proxy']\n-        elif b'Proxy-Authorization' in request.headers:\n+            request.headers[b\"Proxy-Authorization\"] = b\"Basic \" + creds\n+            request.meta[\"_auth_proxy\"] = proxy_url\n+        elif \"_auth_proxy\" in request.meta:\n+            if proxy_url != request.meta[\"_auth_proxy\"]:\n+                if b\"Proxy-Authorization\" in request.headers:\n+                    del request.headers[b\"Proxy-Authorization\"]\n+                del request.meta[\"_auth_proxy\"]\n+        elif b\"Proxy-Authorization\" in request.headers:\n             if proxy_url:\n-                request.meta['_auth_proxy'] = proxy_url\n+                request.meta[\"_auth_proxy\"] = proxy_url\n             else:\n-                del request.headers[b'Proxy-Authorization']\n+                del request.headers[b\"Proxy-Authorization\"]\n\n@@ -17,57 +17,66 @@ def _build_redirect_request(source_request, *, url, **kwargs):\n         **kwargs,\n         cookies=None,\n     )\n-    if 'Cookie' in redirect_request.headers:\n+    if \"Cookie\" in redirect_request.headers:\n         source_request_netloc = urlparse_cached(source_request).netloc\n         redirect_request_netloc = urlparse_cached(redirect_request).netloc\n         if source_request_netloc != redirect_request_netloc:\n-            del redirect_request.headers['Cookie']\n+            del redirect_request.headers[\"Cookie\"]\n     return redirect_request\n \n \n class BaseRedirectMiddleware:\n \n-    enabled_setting = 'REDIRECT_ENABLED'\n+    enabled_setting = \"REDIRECT_ENABLED\"\n \n     def __init__(self, settings):\n         if not settings.getbool(self.enabled_setting):\n             raise NotConfigured\n \n-        self.max_redirect_times = settings.getint('REDIRECT_MAX_TIMES')\n-        self.priority_adjust = settings.getint('REDIRECT_PRIORITY_ADJUST')\n+        self.max_redirect_times = settings.getint(\"REDIRECT_MAX_TIMES\")\n+        self.priority_adjust = settings.getint(\"REDIRECT_PRIORITY_ADJUST\")\n \n     @classmethod\n     def from_crawler(cls, crawler):\n         return cls(crawler.settings)\n \n     def _redirect(self, redirected, request, spider, reason):\n-        ttl = request.meta.setdefault('redirect_ttl', self.max_redirect_times)\n-        redirects = request.meta.get('redirect_times', 0) + 1\n+        ttl = request.meta.setdefault(\"redirect_ttl\", self.max_redirect_times)\n+        redirects = request.meta.get(\"redirect_times\", 0) + 1\n \n         if ttl and redirects <= self.max_redirect_times:\n-            redirected.meta['redirect_times'] = redirects\n-            redirected.meta['redirect_ttl'] = ttl - 1\n-            redirected.meta['redirect_urls'] = request.meta.get('redirect_urls', []) + [request.url]\n-            redirected.meta['redirect_reasons'] = request.meta.get('redirect_reasons', []) + [reason]\n+            redirected.meta[\"redirect_times\"] = redirects\n+            redirected.meta[\"redirect_ttl\"] = ttl - 1\n+            redirected.meta[\"redirect_urls\"] = request.meta.get(\"redirect_urls\", []) + [\n+                request.url\n+            ]\n+            redirected.meta[\"redirect_reasons\"] = request.meta.get(\n+                \"redirect_reasons\", []\n+            ) + [reason]\n             redirected.dont_filter = request.dont_filter\n             redirected.priority = request.priority + self.priority_adjust\n-            logger.debug(\"Redirecting (%(reason)s) to %(redirected)s from %(request)s\",\n-                         {'reason': reason, 'redirected': redirected, 'request': request},\n-                         extra={'spider': spider})\n+            logger.debug(\n+                \"Redirecting (%(reason)s) to %(redirected)s from %(request)s\",\n+                {\"reason\": reason, \"redirected\": redirected, \"request\": request},\n+                extra={\"spider\": spider},\n+            )\n             return redirected\n-        logger.debug(\"Discarding %(request)s: max redirections reached\",\n-                     {'request': request}, extra={'spider': spider})\n+        logger.debug(\n+            \"Discarding %(request)s: max redirections reached\",\n+            {\"request\": request},\n+            extra={\"spider\": spider},\n+        )\n         raise IgnoreRequest(\"max redirections reached\")\n \n     def _redirect_request_using_get(self, request, redirect_url):\n         redirect_request = _build_redirect_request(\n             request,\n             url=redirect_url,\n-            method='GET',\n-            body='',\n+            method=\"GET\",\n+            body=\"\",\n         )\n-        redirect_request.headers.pop('Content-Type', None)\n-        redirect_request.headers.pop('Content-Length', None)\n+        redirect_request.headers.pop(\"Content-Type\", None)\n+        redirect_request.headers.pop(\"Content-Length\", None)\n         return redirect_request\n \n \n@@ -79,25 +88,25 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n \n     def process_response(self, request, response, spider):\n         if (\n-            request.meta.get('dont_redirect', False)\n-            or response.status in getattr(spider, 'handle_httpstatus_list', [])\n-            or response.status in request.meta.get('handle_httpstatus_list', [])\n-            or request.meta.get('handle_httpstatus_all', False)\n+            request.meta.get(\"dont_redirect\", False)\n+            or response.status in getattr(spider, \"handle_httpstatus_list\", [])\n+            or response.status in request.meta.get(\"handle_httpstatus_list\", [])\n+            or request.meta.get(\"handle_httpstatus_all\", False)\n         ):\n             return response\n \n         allowed_status = (301, 302, 303, 307, 308)\n-        if 'Location' not in response.headers or response.status not in allowed_status:\n+        if \"Location\" not in response.headers or response.status not in allowed_status:\n             return response\n \n-        location = safe_url_string(response.headers['Location'])\n-        if response.headers['Location'].startswith(b'//'):\n+        location = safe_url_string(response.headers[\"Location\"])\n+        if response.headers[\"Location\"].startswith(b\"//\"):\n             request_scheme = urlparse(request.url).scheme\n-            location = request_scheme + '://' + location.lstrip('/')\n+            location = request_scheme + \"://\" + location.lstrip(\"/\")\n \n         redirected_url = urljoin(request.url, location)\n \n-        if response.status in (301, 307, 308) or request.method == 'HEAD':\n+        if response.status in (301, 307, 308) or request.method == \"HEAD\":\n             redirected = _build_redirect_request(request, url=redirected_url)\n             return self._redirect(redirected, request, spider, response.status)\n \n@@ -107,25 +116,24 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n \n class MetaRefreshMiddleware(BaseRedirectMiddleware):\n \n-    enabled_setting = 'METAREFRESH_ENABLED'\n+    enabled_setting = \"METAREFRESH_ENABLED\"\n \n     def __init__(self, settings):\n         super().__init__(settings)\n-        self._ignore_tags = settings.getlist('METAREFRESH_IGNORE_TAGS')\n-        self._maxdelay = settings.getint('METAREFRESH_MAXDELAY')\n+        self._ignore_tags = settings.getlist(\"METAREFRESH_IGNORE_TAGS\")\n+        self._maxdelay = settings.getint(\"METAREFRESH_MAXDELAY\")\n \n     def process_response(self, request, response, spider):\n         if (\n-            request.meta.get('dont_redirect', False)\n-            or request.method == 'HEAD'\n+            request.meta.get(\"dont_redirect\", False)\n+            or request.method == \"HEAD\"\n             or not isinstance(response, HtmlResponse)\n         ):\n             return response\n \n-        interval, url = get_meta_refresh(response,\n-                                         ignore_tags=self._ignore_tags)\n+        interval, url = get_meta_refresh(response, ignore_tags=self._ignore_tags)\n         if url and interval < self._maxdelay:\n             redirected = self._redirect_request_using_get(request, url)\n-            return self._redirect(redirected, request, spider, 'meta refresh')\n+            return self._redirect(redirected, request, spider, \"meta refresh\")\n \n         return response\n\n@@ -39,11 +39,11 @@ def get_retry_request(\n     request: Request,\n     *,\n     spider: Spider,\n-    reason: Union[str, Exception] = 'unspecified',\n+    reason: Union[str, Exception] = \"unspecified\",\n     max_retry_times: Optional[int] = None,\n     priority_adjust: Optional[int] = None,\n     logger: Logger = retry_logger,\n-    stats_base_key: str = 'retry',\n+    stats_base_key: str = \"retry\",\n ):\n     \"\"\"\n     Returns a new :class:`~scrapy.Request` object to retry the specified\n@@ -87,22 +87,22 @@ def get_retry_request(\n     \"\"\"\n     settings = spider.crawler.settings\n     stats = spider.crawler.stats\n-    retry_times = request.meta.get('retry_times', 0) + 1\n+    retry_times = request.meta.get(\"retry_times\", 0) + 1\n     if max_retry_times is None:\n-        max_retry_times = request.meta.get('max_retry_times')\n+        max_retry_times = request.meta.get(\"max_retry_times\")\n         if max_retry_times is None:\n-            max_retry_times = settings.getint('RETRY_TIMES')\n+            max_retry_times = settings.getint(\"RETRY_TIMES\")\n     if retry_times <= max_retry_times:\n         logger.debug(\n             \"Retrying %(request)s (failed %(retry_times)d times): %(reason)s\",\n-            {'request': request, 'retry_times': retry_times, 'reason': reason},\n-            extra={'spider': spider}\n+            {\"request\": request, \"retry_times\": retry_times, \"reason\": reason},\n+            extra={\"spider\": spider},\n         )\n         new_request: Request = request.copy()\n-        new_request.meta['retry_times'] = retry_times\n+        new_request.meta[\"retry_times\"] = retry_times\n         new_request.dont_filter = True\n         if priority_adjust is None:\n-            priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')\n+            priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n         new_request.priority = request.priority + priority_adjust\n \n         if callable(reason):\n@@ -110,15 +110,14 @@ def get_retry_request(\n         if isinstance(reason, Exception):\n             reason = global_object_name(reason.__class__)\n \n-        stats.inc_value(f'{stats_base_key}/count')\n-        stats.inc_value(f'{stats_base_key}/reason_count/{reason}')\n+        stats.inc_value(f\"{stats_base_key}/count\")\n+        stats.inc_value(f\"{stats_base_key}/reason_count/{reason}\")\n         return new_request\n-    stats.inc_value(f'{stats_base_key}/max_reached')\n+    stats.inc_value(f\"{stats_base_key}/max_reached\")\n     logger.error(\n-        \"Gave up retrying %(request)s (failed %(retry_times)d times): \"\n-        \"%(reason)s\",\n-        {'request': request, 'retry_times': retry_times, 'reason': reason},\n-        extra={'spider': spider},\n+        \"Gave up retrying %(request)s (failed %(retry_times)d times): \" \"%(reason)s\",\n+        {\"request\": request, \"retry_times\": retry_times, \"reason\": reason},\n+        extra={\"spider\": spider},\n     )\n     return None\n \n@@ -127,24 +126,35 @@ class RetryMiddleware:\n \n     # IOError is raised by the HttpCompression middleware when trying to\n     # decompress an empty response\n-    EXCEPTIONS_TO_RETRY = (defer.TimeoutError, TimeoutError, DNSLookupError,\n-                           ConnectionRefusedError, ConnectionDone, ConnectError,\n-                           ConnectionLost, TCPTimedOutError, ResponseFailed,\n-                           IOError, TunnelError)\n+    EXCEPTIONS_TO_RETRY = (\n+        defer.TimeoutError,\n+        TimeoutError,\n+        DNSLookupError,\n+        ConnectionRefusedError,\n+        ConnectionDone,\n+        ConnectError,\n+        ConnectionLost,\n+        TCPTimedOutError,\n+        ResponseFailed,\n+        IOError,\n+        TunnelError,\n+    )\n \n     def __init__(self, settings):\n-        if not settings.getbool('RETRY_ENABLED'):\n+        if not settings.getbool(\"RETRY_ENABLED\"):\n             raise NotConfigured\n-        self.max_retry_times = settings.getint('RETRY_TIMES')\n-        self.retry_http_codes = set(int(x) for x in settings.getlist('RETRY_HTTP_CODES'))\n-        self.priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')\n+        self.max_retry_times = settings.getint(\"RETRY_TIMES\")\n+        self.retry_http_codes = set(\n+            int(x) for x in settings.getlist(\"RETRY_HTTP_CODES\")\n+        )\n+        self.priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n \n     @classmethod\n     def from_crawler(cls, crawler):\n         return cls(crawler.settings)\n \n     def process_response(self, request, response, spider):\n-        if request.meta.get('dont_retry', False):\n+        if request.meta.get(\"dont_retry\", False):\n             return response\n         if response.status in self.retry_http_codes:\n             reason = response_status_message(response.status)\n@@ -152,15 +162,14 @@ class RetryMiddleware:\n         return response\n \n     def process_exception(self, request, exception, spider):\n-        if (\n-            isinstance(exception, self.EXCEPTIONS_TO_RETRY)\n-            and not request.meta.get('dont_retry', False)\n+        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) and not request.meta.get(\n+            \"dont_retry\", False\n         ):\n             return self._retry(request, exception, spider)\n \n     def _retry(self, request, reason, spider):\n-        max_retry_times = request.meta.get('max_retry_times', self.max_retry_times)\n-        priority_adjust = request.meta.get('priority_adjust', self.priority_adjust)\n+        max_retry_times = request.meta.get(\"max_retry_times\", self.max_retry_times)\n+        priority_adjust = request.meta.get(\"priority_adjust\", self.priority_adjust)\n         return get_retry_request(\n             request,\n             reason=reason,\n\n@@ -20,23 +20,23 @@ class RobotsTxtMiddleware:\n     DOWNLOAD_PRIORITY = 1000\n \n     def __init__(self, crawler):\n-        if not crawler.settings.getbool('ROBOTSTXT_OBEY'):\n+        if not crawler.settings.getbool(\"ROBOTSTXT_OBEY\"):\n             raise NotConfigured\n-        self._default_useragent = crawler.settings.get('USER_AGENT', 'Scrapy')\n-        self._robotstxt_useragent = crawler.settings.get('ROBOTSTXT_USER_AGENT', None)\n+        self._default_useragent = crawler.settings.get(\"USER_AGENT\", \"Scrapy\")\n+        self._robotstxt_useragent = crawler.settings.get(\"ROBOTSTXT_USER_AGENT\", None)\n         self.crawler = crawler\n         self._parsers = {}\n-        self._parserimpl = load_object(crawler.settings.get('ROBOTSTXT_PARSER'))\n+        self._parserimpl = load_object(crawler.settings.get(\"ROBOTSTXT_PARSER\"))\n \n         # check if parser dependencies are met, this should throw an error otherwise.\n-        self._parserimpl.from_crawler(self.crawler, b'')\n+        self._parserimpl.from_crawler(self.crawler, b\"\")\n \n     @classmethod\n     def from_crawler(cls, crawler):\n         return cls(crawler)\n \n     def process_request(self, request, spider):\n-        if request.meta.get('dont_obey_robotstxt'):\n+        if request.meta.get(\"dont_obey_robotstxt\"):\n             return\n         d = maybeDeferred(self.robot_parser, request, spider)\n         d.addCallback(self.process_request_2, request, spider)\n@@ -48,11 +48,14 @@ class RobotsTxtMiddleware:\n \n         useragent = self._robotstxt_useragent\n         if not useragent:\n-            useragent = request.headers.get(b'User-Agent', self._default_useragent)\n+            useragent = request.headers.get(b\"User-Agent\", self._default_useragent)\n         if not rp.allowed(request.url, useragent):\n-            logger.debug(\"Forbidden by robots.txt: %(request)s\",\n-                         {'request': request}, extra={'spider': spider})\n-            self.crawler.stats.inc_value('robotstxt/forbidden')\n+            logger.debug(\n+                \"Forbidden by robots.txt: %(request)s\",\n+                {\"request\": request},\n+                extra={\"spider\": spider},\n+            )\n+            self.crawler.stats.inc_value(\"robotstxt/forbidden\")\n             raise IgnoreRequest(\"Forbidden by robots.txt\")\n \n     def robot_parser(self, request, spider):\n@@ -65,13 +68,13 @@ class RobotsTxtMiddleware:\n             robotsreq = Request(\n                 robotsurl,\n                 priority=self.DOWNLOAD_PRIORITY,\n-                meta={'dont_obey_robotstxt': True}\n+                meta={\"dont_obey_robotstxt\": True},\n             )\n             dfd = self.crawler.engine.download(robotsreq)\n             dfd.addCallback(self._parse_robots, netloc, spider)\n             dfd.addErrback(self._logerror, robotsreq, spider)\n             dfd.addErrback(self._robots_error, netloc)\n-            self.crawler.stats.inc_value('robotstxt/request_count')\n+            self.crawler.stats.inc_value(\"robotstxt/request_count\")\n \n         if isinstance(self._parsers[netloc], Deferred):\n             d = Deferred()\n@@ -79,21 +82,26 @@ class RobotsTxtMiddleware:\n             def cb(result):\n                 d.callback(result)\n                 return result\n+\n             self._parsers[netloc].addCallback(cb)\n             return d\n         return self._parsers[netloc]\n \n     def _logerror(self, failure, request, spider):\n         if failure.type is not IgnoreRequest:\n-            logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n-                         {'request': request, 'f_exception': failure.value},\n+            logger.error(\n+                \"Error downloading %(request)s: %(f_exception)s\",\n+                {\"request\": request, \"f_exception\": failure.value},\n                 exc_info=failure_to_exc_info(failure),\n-                         extra={'spider': spider})\n+                extra={\"spider\": spider},\n+            )\n         return failure\n \n     def _parse_robots(self, response, netloc, spider):\n-        self.crawler.stats.inc_value('robotstxt/response_count')\n-        self.crawler.stats.inc_value(f'robotstxt/response_status_count/{response.status}')\n+        self.crawler.stats.inc_value(\"robotstxt/response_count\")\n+        self.crawler.stats.inc_value(\n+            f\"robotstxt/response_status_count/{response.status}\"\n+        )\n         rp = self._parserimpl.from_crawler(self.crawler, response.body)\n         rp_dfd = self._parsers[netloc]\n         self._parsers[netloc] = rp\n@@ -101,7 +109,7 @@ class RobotsTxtMiddleware:\n \n     def _robots_error(self, failure, netloc):\n         if failure.type is not IgnoreRequest:\n-            key = f'robotstxt/exception_count/{failure.type}'\n+            key = f\"robotstxt/exception_count/{failure.type}\"\n             self.crawler.stats.inc_value(key)\n         rp_dfd = self._parsers[netloc]\n         self._parsers[netloc] = None\n\n@@ -11,40 +11,50 @@ def get_header_size(headers):\n         if isinstance(value, (list, tuple)):\n             for v in value:\n                 size += len(b\": \") + len(key) + len(v)\n-    return size + len(b'\\r\\n') * (len(headers.keys()) - 1)\n+    return size + len(b\"\\r\\n\") * (len(headers.keys()) - 1)\n \n \n def get_status_size(response_status):\n-    return len(to_bytes(http.RESPONSES.get(response_status, b''))) + 15\n+    return len(to_bytes(http.RESPONSES.get(response_status, b\"\"))) + 15\n     # resp.status + b\"\\r\\n\" + b\"HTTP/1.1 <100-599> \"\n \n \n class DownloaderStats:\n-\n     def __init__(self, stats):\n         self.stats = stats\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        if not crawler.settings.getbool('DOWNLOADER_STATS'):\n+        if not crawler.settings.getbool(\"DOWNLOADER_STATS\"):\n             raise NotConfigured\n         return cls(crawler.stats)\n \n     def process_request(self, request, spider):\n-        self.stats.inc_value('downloader/request_count', spider=spider)\n-        self.stats.inc_value(f'downloader/request_method_count/{request.method}', spider=spider)\n+        self.stats.inc_value(\"downloader/request_count\", spider=spider)\n+        self.stats.inc_value(\n+            f\"downloader/request_method_count/{request.method}\", spider=spider\n+        )\n         reqlen = len(request_httprepr(request))\n-        self.stats.inc_value('downloader/request_bytes', reqlen, spider=spider)\n+        self.stats.inc_value(\"downloader/request_bytes\", reqlen, spider=spider)\n \n     def process_response(self, request, response, spider):\n-        self.stats.inc_value('downloader/response_count', spider=spider)\n-        self.stats.inc_value(f'downloader/response_status_count/{response.status}', spider=spider)\n-        reslen = len(response.body) + get_header_size(response.headers) + get_status_size(response.status) + 4\n+        self.stats.inc_value(\"downloader/response_count\", spider=spider)\n+        self.stats.inc_value(\n+            f\"downloader/response_status_count/{response.status}\", spider=spider\n+        )\n+        reslen = (\n+            len(response.body)\n+            + get_header_size(response.headers)\n+            + get_status_size(response.status)\n+            + 4\n+        )\n         # response.body + b\"\\r\\n\"+ response.header + b\"\\r\\n\" + response.status\n-        self.stats.inc_value('downloader/response_bytes', reslen, spider=spider)\n+        self.stats.inc_value(\"downloader/response_bytes\", reslen, spider=spider)\n         return response\n \n     def process_exception(self, request, exception, spider):\n         ex_class = global_object_name(exception.__class__)\n-        self.stats.inc_value('downloader/exception_count', spider=spider)\n-        self.stats.inc_value(f'downloader/exception_type_count/{ex_class}', spider=spider)\n+        self.stats.inc_value(\"downloader/exception_count\", spider=spider)\n+        self.stats.inc_value(\n+            f\"downloader/exception_type_count/{ex_class}\", spider=spider\n+        )\n\n@@ -6,18 +6,18 @@ from scrapy import signals\n class UserAgentMiddleware:\n     \"\"\"This middleware allows spiders to override the user_agent\"\"\"\n \n-    def __init__(self, user_agent='Scrapy'):\n+    def __init__(self, user_agent=\"Scrapy\"):\n         self.user_agent = user_agent\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        o = cls(crawler.settings['USER_AGENT'])\n+        o = cls(crawler.settings[\"USER_AGENT\"])\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         return o\n \n     def spider_opened(self, spider):\n-        self.user_agent = getattr(spider, 'user_agent', self.user_agent)\n+        self.user_agent = getattr(spider, \"user_agent\", self.user_agent)\n \n     def process_request(self, request, spider):\n         if self.user_agent:\n-            request.headers.setdefault(b'User-Agent', self.user_agent)\n+            request.headers.setdefault(b\"User-Agent\", self.user_agent)\n\n@@ -18,7 +18,9 @@ BaseDupeFilterTV = TypeVar(\"BaseDupeFilterTV\", bound=\"BaseDupeFilter\")\n \n class BaseDupeFilter:\n     @classmethod\n-    def from_settings(cls: Type[BaseDupeFilterTV], settings: BaseSettings) -> BaseDupeFilterTV:\n+    def from_settings(\n+        cls: Type[BaseDupeFilterTV], settings: BaseSettings\n+    ) -> BaseDupeFilterTV:\n         return cls()\n \n     def request_seen(self, request: Request) -> bool:\n@@ -55,13 +57,15 @@ class RFPDupeFilter(BaseDupeFilter):\n         self.debug = debug\n         self.logger = logging.getLogger(__name__)\n         if path:\n-            self.file = Path(path, 'requests.seen').open('a+', encoding=\"utf-8\")\n+            self.file = Path(path, \"requests.seen\").open(\"a+\", encoding=\"utf-8\")\n             self.file.seek(0)\n             self.fingerprints.update(x.rstrip() for x in self.file)\n \n     @classmethod\n-    def from_settings(cls: Type[RFPDupeFilterTV], settings: BaseSettings, *, fingerprinter=None) -> RFPDupeFilterTV:\n-        debug = settings.getbool('DUPEFILTER_DEBUG')\n+    def from_settings(\n+        cls: Type[RFPDupeFilterTV], settings: BaseSettings, *, fingerprinter=None\n+    ) -> RFPDupeFilterTV:\n+        debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n         try:\n             return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n         except TypeError:\n@@ -100,7 +104,7 @@ class RFPDupeFilter(BaseDupeFilter):\n             return True\n         self.fingerprints.add(fp)\n         if self.file:\n-            self.file.write(fp + '\\n')\n+            self.file.write(fp + \"\\n\")\n         return False\n \n     def request_fingerprint(self, request: Request) -> str:\n@@ -113,13 +117,15 @@ class RFPDupeFilter(BaseDupeFilter):\n     def log(self, request: Request, spider: Spider) -> None:\n         if self.debug:\n             msg = \"Filtered duplicate request: %(request)s (referer: %(referer)s)\"\n-            args = {'request': request, 'referer': referer_str(request)}\n-            self.logger.debug(msg, args, extra={'spider': spider})\n+            args = {\"request\": request, \"referer\": referer_str(request)}\n+            self.logger.debug(msg, args, extra={\"spider\": spider})\n         elif self.logdupes:\n-            msg = (\"Filtered duplicate request: %(request)s\"\n+            msg = (\n+                \"Filtered duplicate request: %(request)s\"\n                 \" - no more duplicates will be shown\"\n-                   \" (see DUPEFILTER_DEBUG to show all duplicates)\")\n-            self.logger.debug(msg, {'request': request}, extra={'spider': spider})\n+                \" (see DUPEFILTER_DEBUG to show all duplicates)\"\n+            )\n+            self.logger.debug(msg, {\"request\": request}, extra={\"spider\": spider})\n             self.logdupes = False\n \n-        spider.crawler.stats.inc_value('dupefilter/filtered', spider=spider)\n+        spider.crawler.stats.inc_value(\"dupefilter/filtered\", spider=spider)\n\n@@ -10,6 +10,7 @@ new exceptions here without documenting them there.\n \n class NotConfigured(Exception):\n     \"\"\"Indicates a missing configuration situation\"\"\"\n+\n     pass\n \n \n@@ -18,6 +19,7 @@ class _InvalidOutput(TypeError):\n     Indicates an invalid value has been returned by a middleware's processing method.\n     Internal and undocumented, it should not be raised or caught by user code.\n     \"\"\"\n+\n     pass\n \n \n@@ -30,13 +32,14 @@ class IgnoreRequest(Exception):\n \n class DontCloseSpider(Exception):\n     \"\"\"Request the spider not to be closed yet\"\"\"\n+\n     pass\n \n \n class CloseSpider(Exception):\n     \"\"\"Raise this from callbacks to request the spider to be closed\"\"\"\n \n-    def __init__(self, reason='cancelled'):\n+    def __init__(self, reason=\"cancelled\"):\n         super().__init__()\n         self.reason = reason\n \n@@ -58,11 +61,13 @@ class StopDownload(Exception):\n \n class DropItem(Exception):\n     \"\"\"Drop item from the item pipeline\"\"\"\n+\n     pass\n \n \n class NotSupported(Exception):\n     \"\"\"Indicates a feature or method is not supported\"\"\"\n+\n     pass\n \n \n@@ -73,7 +78,7 @@ class UsageError(Exception):\n     \"\"\"To indicate a command-line usage error\"\"\"\n \n     def __init__(self, *a, **kw):\n-        self.print_help = kw.pop('print_help', True)\n+        self.print_help = kw.pop(\"print_help\", True)\n         super().__init__(*a, **kw)\n \n \n@@ -81,9 +86,11 @@ class ScrapyDeprecationWarning(Warning):\n     \"\"\"Warning category for deprecated features, since the default\n     DeprecationWarning is silenced on Python 2.7+\n     \"\"\"\n+\n     pass\n \n \n class ContractFail(AssertionError):\n     \"\"\"Error raised in case of a failing contract\"\"\"\n+\n     pass\n\n@@ -19,13 +19,19 @@ from scrapy.utils.python import is_listlike, to_bytes, to_unicode\n from scrapy.utils.serialize import ScrapyJSONEncoder\n \n \n-__all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter',\n-           'CsvItemExporter', 'XmlItemExporter', 'JsonLinesItemExporter',\n-           'JsonItemExporter', 'MarshalItemExporter']\n+__all__ = [\n+    \"BaseItemExporter\",\n+    \"PprintItemExporter\",\n+    \"PickleItemExporter\",\n+    \"CsvItemExporter\",\n+    \"XmlItemExporter\",\n+    \"JsonLinesItemExporter\",\n+    \"JsonItemExporter\",\n+    \"MarshalItemExporter\",\n+]\n \n \n class BaseItemExporter:\n-\n     def __init__(self, *, dont_fail=False, **kwargs):\n         self._kwargs = kwargs\n         self._configure(kwargs, dont_fail=dont_fail)\n@@ -35,10 +41,10 @@ class BaseItemExporter:\n         If dont_fail is set, it won't raise an exception on unexpected options\n         (useful for using with keyword arguments in subclasses ``__init__`` methods)\n         \"\"\"\n-        self.encoding = options.pop('encoding', None)\n-        self.fields_to_export = options.pop('fields_to_export', None)\n-        self.export_empty_fields = options.pop('export_empty_fields', False)\n-        self.indent = options.pop('indent', None)\n+        self.encoding = options.pop(\"encoding\", None)\n+        self.fields_to_export = options.pop(\"fields_to_export\", None)\n+        self.export_empty_fields = options.pop(\"export_empty_fields\", False)\n+        self.indent = options.pop(\"indent\", None)\n         if not dont_fail and options:\n             raise TypeError(f\"Unexpected options: {', '.join(options.keys())}\")\n \n@@ -46,7 +52,7 @@ class BaseItemExporter:\n         raise NotImplementedError\n \n     def serialize_field(self, field, name, value):\n-        serializer = field.get('serializer', lambda x: x)\n+        serializer = field.get(\"serializer\", lambda x: x)\n         return serializer(value)\n \n     def start_exporting(self):\n@@ -74,8 +80,7 @@ class BaseItemExporter:\n                 field_iter = self.fields_to_export.items()\n             else:\n                 field_iter = (\n-                    (x, y) for x, y in self.fields_to_export.items()\n-                    if x in item\n+                    (x, y) for x, y in self.fields_to_export.items() if x in item\n                 )\n         else:\n             if include_empty:\n@@ -98,36 +103,36 @@ class BaseItemExporter:\n \n \n class JsonLinesItemExporter(BaseItemExporter):\n-\n     def __init__(self, file, **kwargs):\n         super().__init__(dont_fail=True, **kwargs)\n         self.file = file\n-        self._kwargs.setdefault('ensure_ascii', not self.encoding)\n+        self._kwargs.setdefault(\"ensure_ascii\", not self.encoding)\n         self.encoder = ScrapyJSONEncoder(**self._kwargs)\n \n     def export_item(self, item):\n         itemdict = dict(self._get_serialized_fields(item))\n-        data = self.encoder.encode(itemdict) + '\\n'\n+        data = self.encoder.encode(itemdict) + \"\\n\"\n         self.file.write(to_bytes(data, self.encoding))\n \n \n class JsonItemExporter(BaseItemExporter):\n-\n     def __init__(self, file, **kwargs):\n         super().__init__(dont_fail=True, **kwargs)\n         self.file = file\n         # there is a small difference between the behaviour or JsonItemExporter.indent\n         # and ScrapyJSONEncoder.indent. ScrapyJSONEncoder.indent=None is needed to prevent\n         # the addition of newlines everywhere\n-        json_indent = self.indent if self.indent is not None and self.indent > 0 else None\n-        self._kwargs.setdefault('indent', json_indent)\n-        self._kwargs.setdefault('ensure_ascii', not self.encoding)\n+        json_indent = (\n+            self.indent if self.indent is not None and self.indent > 0 else None\n+        )\n+        self._kwargs.setdefault(\"indent\", json_indent)\n+        self._kwargs.setdefault(\"ensure_ascii\", not self.encoding)\n         self.encoder = ScrapyJSONEncoder(**self._kwargs)\n         self.first_item = True\n \n     def _beautify_newline(self):\n         if self.indent is not None:\n-            self.file.write(b'\\n')\n+            self.file.write(b\"\\n\")\n \n     def start_exporting(self):\n         self.file.write(b\"[\")\n@@ -141,7 +146,7 @@ class JsonItemExporter(BaseItemExporter):\n         if self.first_item:\n             self.first_item = False\n         else:\n-            self.file.write(b',')\n+            self.file.write(b\",\")\n             self._beautify_newline()\n         itemdict = dict(self._get_serialized_fields(item))\n         data = self.encoder.encode(itemdict)\n@@ -149,22 +154,21 @@ class JsonItemExporter(BaseItemExporter):\n \n \n class XmlItemExporter(BaseItemExporter):\n-\n     def __init__(self, file, **kwargs):\n-        self.item_element = kwargs.pop('item_element', 'item')\n-        self.root_element = kwargs.pop('root_element', 'items')\n+        self.item_element = kwargs.pop(\"item_element\", \"item\")\n+        self.root_element = kwargs.pop(\"root_element\", \"items\")\n         super().__init__(**kwargs)\n         if not self.encoding:\n-            self.encoding = 'utf-8'\n+            self.encoding = \"utf-8\"\n         self.xg = XMLGenerator(file, encoding=self.encoding)\n \n     def _beautify_newline(self, new_item=False):\n         if self.indent is not None and (self.indent > 0 or new_item):\n-            self.xg.characters('\\n')\n+            self.xg.characters(\"\\n\")\n \n     def _beautify_indent(self, depth=1):\n         if self.indent:\n-            self.xg.characters(' ' * self.indent * depth)\n+            self.xg.characters(\" \" * self.indent * depth)\n \n     def start_exporting(self):\n         self.xg.startDocument()\n@@ -175,7 +179,7 @@ class XmlItemExporter(BaseItemExporter):\n         self._beautify_indent(depth=1)\n         self.xg.startElement(self.item_element, {})\n         self._beautify_newline()\n-        for name, value in self._get_serialized_fields(item, default_value=''):\n+        for name, value in self._get_serialized_fields(item, default_value=\"\"):\n             self._export_xml_field(name, value, depth=2)\n         self._beautify_indent(depth=1)\n         self.xg.endElement(self.item_element)\n@@ -188,7 +192,7 @@ class XmlItemExporter(BaseItemExporter):\n     def _export_xml_field(self, name, serialized_value, depth):\n         self._beautify_indent(depth=depth)\n         self.xg.startElement(name, {})\n-        if hasattr(serialized_value, 'items'):\n+        if hasattr(serialized_value, \"items\"):\n             self._beautify_newline()\n             for subname, value in serialized_value.items():\n                 self._export_xml_field(subname, value, depth=depth + 1)\n@@ -196,7 +200,7 @@ class XmlItemExporter(BaseItemExporter):\n         elif is_listlike(serialized_value):\n             self._beautify_newline()\n             for value in serialized_value:\n-                self._export_xml_field('value', value, depth=depth + 1)\n+                self._export_xml_field(\"value\", value, depth=depth + 1)\n             self._beautify_indent(depth=depth)\n         elif isinstance(serialized_value, str):\n             self.xg.characters(serialized_value)\n@@ -207,18 +211,24 @@ class XmlItemExporter(BaseItemExporter):\n \n \n class CsvItemExporter(BaseItemExporter):\n-\n-    def __init__(self, file, include_headers_line=True, join_multivalued=',', errors=None, **kwargs):\n+    def __init__(\n+        self,\n+        file,\n+        include_headers_line=True,\n+        join_multivalued=\",\",\n+        errors=None,\n+        **kwargs,\n+    ):\n         super().__init__(dont_fail=True, **kwargs)\n         if not self.encoding:\n-            self.encoding = 'utf-8'\n+            self.encoding = \"utf-8\"\n         self.include_headers_line = include_headers_line\n         self.stream = io.TextIOWrapper(\n             file,\n             line_buffering=False,\n             write_through=True,\n             encoding=self.encoding,\n-            newline='',  # Windows needs this https://github.com/scrapy/scrapy/issues/3034\n+            newline=\"\",  # Windows needs this https://github.com/scrapy/scrapy/issues/3034\n             errors=errors,\n         )\n         self.csv_writer = csv.writer(self.stream, **self._kwargs)\n@@ -226,7 +236,7 @@ class CsvItemExporter(BaseItemExporter):\n         self._join_multivalued = join_multivalued\n \n     def serialize_field(self, field, name, value):\n-        serializer = field.get('serializer', self._join_if_needed)\n+        serializer = field.get(\"serializer\", self._join_if_needed)\n         return serializer(value)\n \n     def _join_if_needed(self, value):\n@@ -242,8 +252,7 @@ class CsvItemExporter(BaseItemExporter):\n             self._headers_not_written = False\n             self._write_headers_and_set_fields_to_export(item)\n \n-        fields = self._get_serialized_fields(item, default_value='',\n-                                             include_empty=True)\n+        fields = self._get_serialized_fields(item, default_value=\"\", include_empty=True)\n         values = list(self._build_row(x for _, x in fields))\n         self.csv_writer.writerow(values)\n \n@@ -268,7 +277,6 @@ class CsvItemExporter(BaseItemExporter):\n \n \n class PickleItemExporter(BaseItemExporter):\n-\n     def __init__(self, file, protocol=4, **kwargs):\n         super().__init__(**kwargs)\n         self.file = file\n@@ -297,14 +305,13 @@ class MarshalItemExporter(BaseItemExporter):\n \n \n class PprintItemExporter(BaseItemExporter):\n-\n     def __init__(self, file, **kwargs):\n         super().__init__(**kwargs)\n         self.file = file\n \n     def export_item(self, item):\n         itemdict = dict(self._get_serialized_fields(item))\n-        self.file.write(to_bytes(pprint.pformat(itemdict) + '\\n'))\n+        self.file.write(to_bytes(pprint.pformat(itemdict) + \"\\n\"))\n \n \n class PythonItemExporter(BaseItemExporter):\n@@ -318,17 +325,18 @@ class PythonItemExporter(BaseItemExporter):\n     \"\"\"\n \n     def _configure(self, options, dont_fail=False):\n-        self.binary = options.pop('binary', True)\n+        self.binary = options.pop(\"binary\", True)\n         super()._configure(options, dont_fail)\n         if self.binary:\n             warnings.warn(\n                 \"PythonItemExporter will drop support for binary export in the future\",\n-                ScrapyDeprecationWarning)\n+                ScrapyDeprecationWarning,\n+            )\n         if not self.encoding:\n-            self.encoding = 'utf-8'\n+            self.encoding = \"utf-8\"\n \n     def serialize_field(self, field, name, value):\n-        serializer = field.get('serializer', self._serialize_value)\n+        serializer = field.get(\"serializer\", self._serialize_value)\n         return serializer(value)\n \n     def _serialize_value(self, value):\n\n@@ -9,8 +9,8 @@ from scrapy.utils.conf import build_component_list\n \n class ExtensionManager(MiddlewareManager):\n \n-    component_name = 'extension'\n+    component_name = \"extension\"\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return build_component_list(settings.getwithbase('EXTENSIONS'))\n+        return build_component_list(settings.getwithbase(\"EXTENSIONS\"))\n\n@@ -11,15 +11,14 @@ from scrapy.exceptions import NotConfigured\n \n \n class CloseSpider:\n-\n     def __init__(self, crawler):\n         self.crawler = crawler\n \n         self.close_on = {\n-            'timeout': crawler.settings.getfloat('CLOSESPIDER_TIMEOUT'),\n-            'itemcount': crawler.settings.getint('CLOSESPIDER_ITEMCOUNT'),\n-            'pagecount': crawler.settings.getint('CLOSESPIDER_PAGECOUNT'),\n-            'errorcount': crawler.settings.getint('CLOSESPIDER_ERRORCOUNT'),\n+            \"timeout\": crawler.settings.getfloat(\"CLOSESPIDER_TIMEOUT\"),\n+            \"itemcount\": crawler.settings.getint(\"CLOSESPIDER_ITEMCOUNT\"),\n+            \"pagecount\": crawler.settings.getint(\"CLOSESPIDER_PAGECOUNT\"),\n+            \"errorcount\": crawler.settings.getint(\"CLOSESPIDER_ERRORCOUNT\"),\n         }\n \n         if not any(self.close_on.values()):\n@@ -27,13 +26,13 @@ class CloseSpider:\n \n         self.counter = defaultdict(int)\n \n-        if self.close_on.get('errorcount'):\n+        if self.close_on.get(\"errorcount\"):\n             crawler.signals.connect(self.error_count, signal=signals.spider_error)\n-        if self.close_on.get('pagecount'):\n+        if self.close_on.get(\"pagecount\"):\n             crawler.signals.connect(self.page_count, signal=signals.response_received)\n-        if self.close_on.get('timeout'):\n+        if self.close_on.get(\"timeout\"):\n             crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)\n-        if self.close_on.get('itemcount'):\n+        if self.close_on.get(\"itemcount\"):\n             crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)\n         crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)\n \n@@ -42,27 +41,31 @@ class CloseSpider:\n         return cls(crawler)\n \n     def error_count(self, failure, response, spider):\n-        self.counter['errorcount'] += 1\n-        if self.counter['errorcount'] == self.close_on['errorcount']:\n-            self.crawler.engine.close_spider(spider, 'closespider_errorcount')\n+        self.counter[\"errorcount\"] += 1\n+        if self.counter[\"errorcount\"] == self.close_on[\"errorcount\"]:\n+            self.crawler.engine.close_spider(spider, \"closespider_errorcount\")\n \n     def page_count(self, response, request, spider):\n-        self.counter['pagecount'] += 1\n-        if self.counter['pagecount'] == self.close_on['pagecount']:\n-            self.crawler.engine.close_spider(spider, 'closespider_pagecount')\n+        self.counter[\"pagecount\"] += 1\n+        if self.counter[\"pagecount\"] == self.close_on[\"pagecount\"]:\n+            self.crawler.engine.close_spider(spider, \"closespider_pagecount\")\n \n     def spider_opened(self, spider):\n         from twisted.internet import reactor\n-        self.task = reactor.callLater(self.close_on['timeout'],\n-                                      self.crawler.engine.close_spider, spider,\n-                                      reason='closespider_timeout')\n+\n+        self.task = reactor.callLater(\n+            self.close_on[\"timeout\"],\n+            self.crawler.engine.close_spider,\n+            spider,\n+            reason=\"closespider_timeout\",\n+        )\n \n     def item_scraped(self, item, spider):\n-        self.counter['itemcount'] += 1\n-        if self.counter['itemcount'] == self.close_on['itemcount']:\n-            self.crawler.engine.close_spider(spider, 'closespider_itemcount')\n+        self.counter[\"itemcount\"] += 1\n+        if self.counter[\"itemcount\"] == self.close_on[\"itemcount\"]:\n+            self.crawler.engine.close_spider(spider, \"closespider_itemcount\")\n \n     def spider_closed(self, spider):\n-        task = getattr(self, 'task', False)\n+        task = getattr(self, \"task\", False)\n         if task and task.active():\n             task.cancel()\n\n@@ -7,7 +7,6 @@ from scrapy import signals\n \n \n class CoreStats:\n-\n     def __init__(self, stats):\n         self.stats = stats\n         self.start_time = None\n@@ -24,23 +23,25 @@ class CoreStats:\n \n     def spider_opened(self, spider):\n         self.start_time = datetime.utcnow()\n-        self.stats.set_value('start_time', self.start_time, spider=spider)\n+        self.stats.set_value(\"start_time\", self.start_time, spider=spider)\n \n     def spider_closed(self, spider, reason):\n         finish_time = datetime.utcnow()\n         elapsed_time = finish_time - self.start_time\n         elapsed_time_seconds = elapsed_time.total_seconds()\n-        self.stats.set_value('elapsed_time_seconds', elapsed_time_seconds, spider=spider)\n-        self.stats.set_value('finish_time', finish_time, spider=spider)\n-        self.stats.set_value('finish_reason', reason, spider=spider)\n+        self.stats.set_value(\n+            \"elapsed_time_seconds\", elapsed_time_seconds, spider=spider\n+        )\n+        self.stats.set_value(\"finish_time\", finish_time, spider=spider)\n+        self.stats.set_value(\"finish_reason\", reason, spider=spider)\n \n     def item_scraped(self, item, spider):\n-        self.stats.inc_value('item_scraped_count', spider=spider)\n+        self.stats.inc_value(\"item_scraped_count\", spider=spider)\n \n     def response_received(self, spider):\n-        self.stats.inc_value('response_received_count', spider=spider)\n+        self.stats.inc_value(\"response_received_count\", spider=spider)\n \n     def item_dropped(self, item, spider, exception):\n         reason = exception.__class__.__name__\n-        self.stats.inc_value('item_dropped_count', spider=spider)\n-        self.stats.inc_value(f'item_dropped_reasons_count/{reason}', spider=spider)\n+        self.stats.inc_value(\"item_dropped_count\", spider=spider)\n+        self.stats.inc_value(f\"item_dropped_reasons_count/{reason}\", spider=spider)\n\n@@ -18,7 +18,6 @@ logger = logging.getLogger(__name__)\n \n \n class StackTraceDump:\n-\n     def __init__(self, crawler=None):\n         self.crawler = crawler\n         try:\n@@ -34,20 +33,23 @@ class StackTraceDump:\n \n     def dump_stacktrace(self, signum, frame):\n         log_args = {\n-            'stackdumps': self._thread_stacks(),\n-            'enginestatus': format_engine_status(self.crawler.engine),\n-            'liverefs': format_live_refs(),\n+            \"stackdumps\": self._thread_stacks(),\n+            \"enginestatus\": format_engine_status(self.crawler.engine),\n+            \"liverefs\": format_live_refs(),\n         }\n-        logger.info(\"Dumping stack trace and engine status\\n\"\n+        logger.info(\n+            \"Dumping stack trace and engine status\\n\"\n             \"%(enginestatus)s\\n%(liverefs)s\\n%(stackdumps)s\",\n-                    log_args, extra={'crawler': self.crawler})\n+            log_args,\n+            extra={\"crawler\": self.crawler},\n+        )\n \n     def _thread_stacks(self):\n         id2name = dict((th.ident, th.name) for th in threading.enumerate())\n-        dumps = ''\n+        dumps = \"\"\n         for id_, frame in sys._current_frames().items():\n-            name = id2name.get(id_, '')\n-            dump = ''.join(traceback.format_stack(frame))\n+            name = id2name.get(id_, \"\")\n+            dump = \"\".join(traceback.format_stack(frame))\n             dumps += f\"# Thread: {name}({id_})\\n{dump}\\n\"\n         return dumps\n \n\n@@ -34,15 +34,15 @@ logger = logging.getLogger(__name__)\n \n def build_storage(builder, uri, *args, feed_options=None, preargs=(), **kwargs):\n     argument_names = get_func_args(builder)\n-    if 'feed_options' in argument_names:\n-        kwargs['feed_options'] = feed_options\n+    if \"feed_options\" in argument_names:\n+        kwargs[\"feed_options\"] = feed_options\n     else:\n         warnings.warn(\n             f\"{builder.__qualname__} does not support the 'feed_options' keyword argument. Add a \"\n             \"'feed_options' parameter to its signature to remove this \"\n             \"warning. This parameter will become mandatory in a future \"\n             \"version of Scrapy.\",\n-            category=ScrapyDeprecationWarning\n+            category=ScrapyDeprecationWarning,\n         )\n     return builder(*preargs, uri, *args, **kwargs)\n \n@@ -55,6 +55,7 @@ class ItemFilter:\n     :param feed_options: feed specific options passed from FeedExporter\n     :type feed_options: dict\n     \"\"\"\n+\n     feed_options: Optional[dict]\n     item_classes: Tuple\n \n@@ -62,7 +63,8 @@ class ItemFilter:\n         self.feed_options = feed_options\n         if feed_options is not None:\n             self.item_classes = tuple(\n-                load_object(item_class) for item_class in feed_options.get(\"item_classes\") or ()\n+                load_object(item_class)\n+                for item_class in feed_options.get(\"item_classes\") or ()\n             )\n         else:\n             self.item_classes = tuple()\n@@ -98,13 +100,12 @@ class IFeedStorage(Interface):\n \n @implementer(IFeedStorage)\n class BlockingFeedStorage:\n-\n     def open(self, spider):\n-        path = spider.crawler.settings['FEED_TEMPDIR']\n+        path = spider.crawler.settings[\"FEED_TEMPDIR\"]\n         if path and not Path(path).is_dir():\n-            raise OSError('Not a Directory: ' + str(path))\n+            raise OSError(\"Not a Directory: \" + str(path))\n \n-        return NamedTemporaryFile(prefix='feed-', dir=path)\n+        return NamedTemporaryFile(prefix=\"feed-\", dir=path)\n \n     def store(self, file):\n         return threads.deferToThread(self._store_in_thread, file)\n@@ -115,16 +116,17 @@ class BlockingFeedStorage:\n \n @implementer(IFeedStorage)\n class StdoutFeedStorage:\n-\n     def __init__(self, uri, _stdout=None, *, feed_options=None):\n         if not _stdout:\n             _stdout = sys.stdout.buffer\n         self._stdout = _stdout\n-        if feed_options and feed_options.get('overwrite', False) is True:\n-            logger.warning('Standard output (stdout) storage does not support '\n-                           'overwriting. To suppress this warning, remove the '\n-                           'overwrite option from your FEEDS setting, or set '\n-                           'it to False.')\n+        if feed_options and feed_options.get(\"overwrite\", False) is True:\n+            logger.warning(\n+                \"Standard output (stdout) storage does not support \"\n+                \"overwriting. To suppress this warning, remove the \"\n+                \"overwrite option from your FEEDS setting, or set \"\n+                \"it to False.\"\n+            )\n \n     def open(self, spider):\n         return self._stdout\n@@ -135,11 +137,10 @@ class StdoutFeedStorage:\n \n @implementer(IFeedStorage)\n class FileFeedStorage:\n-\n     def __init__(self, uri, *, feed_options=None):\n         self.path = file_uri_to_path(uri)\n         feed_options = feed_options or {}\n-        self.write_mode = 'wb' if feed_options.get('overwrite', False) else 'ab'\n+        self.write_mode = \"wb\" if feed_options.get(\"overwrite\", False) else \"ab\"\n \n     def open(self, spider) -> IO[Any]:\n         dirname = Path(self.path).parent\n@@ -152,11 +153,19 @@ class FileFeedStorage:\n \n \n class S3FeedStorage(BlockingFeedStorage):\n-\n-    def __init__(self, uri, access_key=None, secret_key=None, acl=None, endpoint_url=None, *,\n-                 feed_options=None, session_token=None):\n+    def __init__(\n+        self,\n+        uri,\n+        access_key=None,\n+        secret_key=None,\n+        acl=None,\n+        endpoint_url=None,\n+        *,\n+        feed_options=None,\n+        session_token=None,\n+    ):\n         if not is_botocore_available():\n-            raise NotConfigured('missing botocore library')\n+            raise NotConfigured(\"missing botocore library\")\n         u = urlparse(uri)\n         self.bucketname = u.hostname\n         self.access_key = u.username or access_key\n@@ -166,41 +175,45 @@ class S3FeedStorage(BlockingFeedStorage):\n         self.acl = acl\n         self.endpoint_url = endpoint_url\n         import botocore.session\n+\n         session = botocore.session.get_session()\n         self.s3_client = session.create_client(\n-            's3', aws_access_key_id=self.access_key,\n+            \"s3\",\n+            aws_access_key_id=self.access_key,\n             aws_secret_access_key=self.secret_key,\n             aws_session_token=self.session_token,\n-            endpoint_url=self.endpoint_url)\n-        if feed_options and feed_options.get('overwrite', True) is False:\n-            logger.warning('S3 does not support appending to files. To '\n-                           'suppress this warning, remove the overwrite '\n-                           'option from your FEEDS setting or set it to True.')\n+            endpoint_url=self.endpoint_url,\n+        )\n+        if feed_options and feed_options.get(\"overwrite\", True) is False:\n+            logger.warning(\n+                \"S3 does not support appending to files. To \"\n+                \"suppress this warning, remove the overwrite \"\n+                \"option from your FEEDS setting or set it to True.\"\n+            )\n \n     @classmethod\n     def from_crawler(cls, crawler, uri, *, feed_options=None):\n         return build_storage(\n             cls,\n             uri,\n-            access_key=crawler.settings['AWS_ACCESS_KEY_ID'],\n-            secret_key=crawler.settings['AWS_SECRET_ACCESS_KEY'],\n-            session_token=crawler.settings['AWS_SESSION_TOKEN'],\n-            acl=crawler.settings['FEED_STORAGE_S3_ACL'] or None,\n-            endpoint_url=crawler.settings['AWS_ENDPOINT_URL'] or None,\n+            access_key=crawler.settings[\"AWS_ACCESS_KEY_ID\"],\n+            secret_key=crawler.settings[\"AWS_SECRET_ACCESS_KEY\"],\n+            session_token=crawler.settings[\"AWS_SESSION_TOKEN\"],\n+            acl=crawler.settings[\"FEED_STORAGE_S3_ACL\"] or None,\n+            endpoint_url=crawler.settings[\"AWS_ENDPOINT_URL\"] or None,\n             feed_options=feed_options,\n         )\n \n     def _store_in_thread(self, file):\n         file.seek(0)\n-        kwargs = {'ACL': self.acl} if self.acl else {}\n+        kwargs = {\"ACL\": self.acl} if self.acl else {}\n         self.s3_client.put_object(\n-            Bucket=self.bucketname, Key=self.keyname, Body=file,\n-            **kwargs)\n+            Bucket=self.bucketname, Key=self.keyname, Body=file, **kwargs\n+        )\n         file.close()\n \n \n class GCSFeedStorage(BlockingFeedStorage):\n-\n     def __init__(self, uri, project_id, acl):\n         self.project_id = project_id\n         self.acl = acl\n@@ -212,13 +225,14 @@ class GCSFeedStorage(BlockingFeedStorage):\n     def from_crawler(cls, crawler, uri):\n         return cls(\n             uri,\n-            crawler.settings['GCS_PROJECT_ID'],\n-            crawler.settings['FEED_STORAGE_GCS_ACL'] or None\n+            crawler.settings[\"GCS_PROJECT_ID\"],\n+            crawler.settings[\"FEED_STORAGE_GCS_ACL\"] or None,\n         )\n \n     def _store_in_thread(self, file):\n         file.seek(0)\n         from google.cloud.storage import Client\n+\n         client = Client(project=self.project_id)\n         bucket = client.get_bucket(self.bucket_name)\n         blob = bucket.blob(self.blob_name)\n@@ -226,37 +240,51 @@ class GCSFeedStorage(BlockingFeedStorage):\n \n \n class FTPFeedStorage(BlockingFeedStorage):\n-\n     def __init__(self, uri, use_active_mode=False, *, feed_options=None):\n         u = urlparse(uri)\n         self.host = u.hostname\n-        self.port = int(u.port or '21')\n+        self.port = int(u.port or \"21\")\n         self.username = u.username\n-        self.password = unquote(u.password or '')\n+        self.password = unquote(u.password or \"\")\n         self.path = u.path\n         self.use_active_mode = use_active_mode\n-        self.overwrite = not feed_options or feed_options.get('overwrite', True)\n+        self.overwrite = not feed_options or feed_options.get(\"overwrite\", True)\n \n     @classmethod\n     def from_crawler(cls, crawler, uri, *, feed_options=None):\n         return build_storage(\n             cls,\n             uri,\n-            crawler.settings.getbool('FEED_STORAGE_FTP_ACTIVE'),\n+            crawler.settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\"),\n             feed_options=feed_options,\n         )\n \n     def _store_in_thread(self, file):\n         ftp_store_file(\n-            path=self.path, file=file, host=self.host,\n-            port=self.port, username=self.username,\n-            password=self.password, use_active_mode=self.use_active_mode,\n+            path=self.path,\n+            file=file,\n+            host=self.host,\n+            port=self.port,\n+            username=self.username,\n+            password=self.password,\n+            use_active_mode=self.use_active_mode,\n             overwrite=self.overwrite,\n         )\n \n \n class _FeedSlot:\n-    def __init__(self, file, exporter, storage, uri, format, store_empty, batch_id, uri_template, filter):\n+    def __init__(\n+        self,\n+        file,\n+        exporter,\n+        storage,\n+        uri,\n+        format,\n+        store_empty,\n+        batch_id,\n+        uri_template,\n+        filter,\n+    ):\n         self.file = file\n         self.exporter = exporter\n         self.storage = storage\n@@ -283,7 +311,6 @@ class _FeedSlot:\n \n \n class FeedExporter:\n-\n     @classmethod\n     def from_crawler(cls, crawler):\n         exporter = cls(crawler)\n@@ -299,48 +326,55 @@ class FeedExporter:\n         self.slots = []\n         self.filters = {}\n \n-        if not self.settings['FEEDS'] and not self.settings['FEED_URI']:\n+        if not self.settings[\"FEEDS\"] and not self.settings[\"FEED_URI\"]:\n             raise NotConfigured\n \n         # Begin: Backward compatibility for FEED_URI and FEED_FORMAT settings\n-        if self.settings['FEED_URI']:\n+        if self.settings[\"FEED_URI\"]:\n             warnings.warn(\n-                'The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of '\n-                'the `FEEDS` setting. Please see the `FEEDS` setting docs for more details',\n-                category=ScrapyDeprecationWarning, stacklevel=2,\n+                \"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of \"\n+                \"the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+            uri = str(self.settings[\"FEED_URI\"])  # handle pathlib.Path objects\n+            feed_options = {\"format\": self.settings.get(\"FEED_FORMAT\", \"jsonlines\")}\n+            self.feeds[uri] = feed_complete_default_values_from_settings(\n+                feed_options, self.settings\n             )\n-            uri = str(self.settings['FEED_URI'])  # handle pathlib.Path objects\n-            feed_options = {'format': self.settings.get('FEED_FORMAT', 'jsonlines')}\n-            self.feeds[uri] = feed_complete_default_values_from_settings(feed_options, self.settings)\n             self.filters[uri] = self._load_filter(feed_options)\n         # End: Backward compatibility for FEED_URI and FEED_FORMAT settings\n \n         # 'FEEDS' setting takes precedence over 'FEED_URI'\n-        for uri, feed_options in self.settings.getdict('FEEDS').items():\n+        for uri, feed_options in self.settings.getdict(\"FEEDS\").items():\n             uri = str(uri)  # handle pathlib.Path objects\n-            self.feeds[uri] = feed_complete_default_values_from_settings(feed_options, self.settings)\n+            self.feeds[uri] = feed_complete_default_values_from_settings(\n+                feed_options, self.settings\n+            )\n             self.filters[uri] = self._load_filter(feed_options)\n \n-        self.storages = self._load_components('FEED_STORAGES')\n-        self.exporters = self._load_components('FEED_EXPORTERS')\n+        self.storages = self._load_components(\"FEED_STORAGES\")\n+        self.exporters = self._load_components(\"FEED_EXPORTERS\")\n         for uri, feed_options in self.feeds.items():\n             if not self._storage_supported(uri, feed_options):\n                 raise NotConfigured\n             if not self._settings_are_valid():\n                 raise NotConfigured\n-            if not self._exporter_supported(feed_options['format']):\n+            if not self._exporter_supported(feed_options[\"format\"]):\n                 raise NotConfigured\n \n     def open_spider(self, spider):\n         for uri, feed_options in self.feeds.items():\n-            uri_params = self._get_uri_params(spider, feed_options['uri_params'])\n-            self.slots.append(self._start_new_batch(\n+            uri_params = self._get_uri_params(spider, feed_options[\"uri_params\"])\n+            self.slots.append(\n+                self._start_new_batch(\n                     batch_id=1,\n                     uri=uri % uri_params,\n                     feed_options=feed_options,\n                     spider=spider,\n                     uri_template=uri,\n-            ))\n+                )\n+            )\n \n     def close_spider(self, spider):\n         deferred_list = []\n@@ -368,16 +402,15 @@ class FeedExporter:\n \n     def _handle_store_error(self, f, logmsg, spider, slot_type):\n         logger.error(\n-            \"Error storing %s\", logmsg,\n-            exc_info=failure_to_exc_info(f), extra={'spider': spider}\n+            \"Error storing %s\",\n+            logmsg,\n+            exc_info=failure_to_exc_info(f),\n+            extra={\"spider\": spider},\n         )\n         self.crawler.stats.inc_value(f\"feedexport/failed_count/{slot_type}\")\n \n     def _handle_store_success(self, f, logmsg, spider, slot_type):\n-        logger.info(\n-            \"Stored %s\", logmsg,\n-            extra={'spider': spider}\n-        )\n+        logger.info(\"Stored %s\", logmsg, extra={\"spider\": spider})\n         self.crawler.stats.inc_value(f\"feedexport/success_count/{slot_type}\")\n \n     def _start_new_batch(self, batch_id, uri, feed_options, spider, uri_template):\n@@ -393,26 +426,28 @@ class FeedExporter:\n         storage = self._get_storage(uri, feed_options)\n         file = storage.open(spider)\n         if \"postprocessing\" in feed_options:\n-            file = PostProcessingManager(feed_options[\"postprocessing\"], file, feed_options)\n+            file = PostProcessingManager(\n+                feed_options[\"postprocessing\"], file, feed_options\n+            )\n \n         exporter = self._get_exporter(\n             file=file,\n-            format=feed_options['format'],\n-            fields_to_export=feed_options['fields'],\n-            encoding=feed_options['encoding'],\n-            indent=feed_options['indent'],\n-            **feed_options['item_export_kwargs'],\n+            format=feed_options[\"format\"],\n+            fields_to_export=feed_options[\"fields\"],\n+            encoding=feed_options[\"encoding\"],\n+            indent=feed_options[\"indent\"],\n+            **feed_options[\"item_export_kwargs\"],\n         )\n         slot = _FeedSlot(\n             file=file,\n             exporter=exporter,\n             storage=storage,\n             uri=uri,\n-            format=feed_options['format'],\n-            store_empty=feed_options['store_empty'],\n+            format=feed_options[\"format\"],\n+            store_empty=feed_options[\"store_empty\"],\n             batch_id=batch_id,\n             uri_template=uri_template,\n-            filter=self.filters[uri_template]\n+            filter=self.filters[uri_template],\n         )\n         if slot.store_empty:\n             slot.start_exporting()\n@@ -422,7 +457,9 @@ class FeedExporter:\n         slots = []\n         for slot in self.slots:\n             if not slot.filter.accepts(item):\n-                slots.append(slot)    # if slot doesn't accept item, continue with next slot\n+                slots.append(\n+                    slot\n+                )  # if slot doesn't accept item, continue with next slot\n                 continue\n \n             slot.start_exporting()\n@@ -430,18 +467,22 @@ class FeedExporter:\n             slot.itemcount += 1\n             # create new slot for each slot with itemcount == FEED_EXPORT_BATCH_ITEM_COUNT and close the old one\n             if (\n-                self.feeds[slot.uri_template]['batch_item_count']\n-                and slot.itemcount >= self.feeds[slot.uri_template]['batch_item_count']\n+                self.feeds[slot.uri_template][\"batch_item_count\"]\n+                and slot.itemcount >= self.feeds[slot.uri_template][\"batch_item_count\"]\n             ):\n-                uri_params = self._get_uri_params(spider, self.feeds[slot.uri_template]['uri_params'], slot)\n+                uri_params = self._get_uri_params(\n+                    spider, self.feeds[slot.uri_template][\"uri_params\"], slot\n+                )\n                 self._close_slot(slot, spider)\n-                slots.append(self._start_new_batch(\n+                slots.append(\n+                    self._start_new_batch(\n                         batch_id=slot.batch_id + 1,\n                         uri=slot.uri_template % uri_params,\n                         feed_options=self.feeds[slot.uri_template],\n                         spider=spider,\n                         uri_template=slot.uri_template,\n-                ))\n+                    )\n+                )\n             else:\n                 slots.append(slot)\n         self.slots = slots\n@@ -459,7 +500,7 @@ class FeedExporter:\n     def _exporter_supported(self, format):\n         if format in self.exporters:\n             return True\n-        logger.error(\"Unknown feed format: %(format)s\", {'format': format})\n+        logger.error(\"Unknown feed format: %(format)s\", {\"format\": format})\n \n     def _settings_are_valid(self):\n         \"\"\"\n@@ -467,12 +508,14 @@ class FeedExporter:\n         %(batch_time)s or %(batch_id)d to distinguish different files of partial output\n         \"\"\"\n         for uri_template, values in self.feeds.items():\n-            if values['batch_item_count'] and not re.search(r'%\\(batch_time\\)s|%\\(batch_id\\)', uri_template):\n+            if values[\"batch_item_count\"] and not re.search(\n+                r\"%\\(batch_time\\)s|%\\(batch_id\\)\", uri_template\n+            ):\n                 logger.error(\n-                    '%%(batch_time)s or %%(batch_id)d must be in the feed URI (%s) if FEED_EXPORT_BATCH_ITEM_COUNT '\n-                    'setting or FEEDS.batch_item_count is specified and greater than 0. For more info see: '\n-                    'https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-export-batch-item-count',\n-                    uri_template\n+                    \"%%(batch_time)s or %%(batch_id)d must be in the feed URI (%s) if FEED_EXPORT_BATCH_ITEM_COUNT \"\n+                    \"setting or FEEDS.batch_item_count is specified and greater than 0. For more info see: \"\n+                    \"https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-export-batch-item-count\",\n+                    uri_template,\n                 )\n                 return False\n         return True\n@@ -484,17 +527,17 @@ class FeedExporter:\n                 self._get_storage(uri, feed_options)\n                 return True\n             except NotConfigured as e:\n-                logger.error(\"Disabled feed storage scheme: %(scheme)s. \"\n-                             \"Reason: %(reason)s\",\n-                             {'scheme': scheme, 'reason': str(e)})\n+                logger.error(\n+                    \"Disabled feed storage scheme: %(scheme)s. \" \"Reason: %(reason)s\",\n+                    {\"scheme\": scheme, \"reason\": str(e)},\n+                )\n         else:\n-            logger.error(\"Unknown feed storage scheme: %(scheme)s\",\n-                         {'scheme': scheme})\n+            logger.error(\"Unknown feed storage scheme: %(scheme)s\", {\"scheme\": scheme})\n \n     def _get_instance(self, objcls, *args, **kwargs):\n         return create_instance(\n-            objcls, self.settings, getattr(self, 'crawler', None),\n-            *args, **kwargs)\n+            objcls, self.settings, getattr(self, \"crawler\", None), *args, **kwargs\n+        )\n \n     def _get_exporter(self, file, format, *args, **kwargs):\n         return self._get_instance(self.exporters[format], file, *args, **kwargs)\n@@ -506,20 +549,22 @@ class FeedExporter:\n         do not support it, and issuing a deprecation warning instead.\n         \"\"\"\n         feedcls = self.storages[urlparse(uri).scheme]\n-        crawler = getattr(self, 'crawler', None)\n+        crawler = getattr(self, \"crawler\", None)\n \n         def build_instance(builder, *preargs):\n-            return build_storage(builder, uri, feed_options=feed_options, preargs=preargs)\n+            return build_storage(\n+                builder, uri, feed_options=feed_options, preargs=preargs\n+            )\n \n-        if crawler and hasattr(feedcls, 'from_crawler'):\n+        if crawler and hasattr(feedcls, \"from_crawler\"):\n             instance = build_instance(feedcls.from_crawler, crawler)\n-            method_name = 'from_crawler'\n-        elif hasattr(feedcls, 'from_settings'):\n+            method_name = \"from_crawler\"\n+        elif hasattr(feedcls, \"from_settings\"):\n             instance = build_instance(feedcls.from_settings, self.settings)\n-            method_name = 'from_settings'\n+            method_name = \"from_settings\"\n         else:\n             instance = build_instance(feedcls)\n-            method_name = '__new__'\n+            method_name = \"__new__\"\n         if instance is None:\n             raise TypeError(f\"{feedcls.__qualname__}.{method_name} returned None\")\n         return instance\n@@ -534,19 +579,23 @@ class FeedExporter:\n         for k in dir(spider):\n             params[k] = getattr(spider, k)\n         utc_now = datetime.utcnow()\n-        params['time'] = utc_now.replace(microsecond=0).isoformat().replace(':', '-')\n-        params['batch_time'] = utc_now.isoformat().replace(':', '-')\n-        params['batch_id'] = slot.batch_id + 1 if slot is not None else 1\n+        params[\"time\"] = utc_now.replace(microsecond=0).isoformat().replace(\":\", \"-\")\n+        params[\"batch_time\"] = utc_now.isoformat().replace(\":\", \"-\")\n+        params[\"batch_id\"] = slot.batch_id + 1 if slot is not None else 1\n         original_params = params.copy()\n-        uripar_function = load_object(uri_params_function) if uri_params_function else lambda params, _: params\n+        uripar_function = (\n+            load_object(uri_params_function)\n+            if uri_params_function\n+            else lambda params, _: params\n+        )\n         new_params = uripar_function(params, spider)\n         if new_params is None or original_params != params:\n             warnings.warn(\n-                'Modifying the params dictionary in-place in the function defined in '\n-                'the FEED_URI_PARAMS setting or in the uri_params key of the FEEDS '\n-                'setting is deprecated. The function must return a new dictionary '\n-                'instead.',\n-                category=ScrapyDeprecationWarning\n+                \"Modifying the params dictionary in-place in the function defined in \"\n+                \"the FEED_URI_PARAMS setting or in the uri_params key of the FEEDS \"\n+                \"setting is deprecated. The function must return a new dictionary \"\n+                \"instead.\",\n+                category=ScrapyDeprecationWarning,\n             )\n         return new_params if new_params is not None else params\n \n\n@@ -21,10 +21,11 @@ logger = logging.getLogger(__name__)\n \n \n class DummyPolicy:\n-\n     def __init__(self, settings):\n-        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n-        self.ignore_http_codes = [int(x) for x in settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES')]\n+        self.ignore_schemes = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n+        self.ignore_http_codes = [\n+            int(x) for x in settings.getlist(\"HTTPCACHE_IGNORE_HTTP_CODES\")\n+        ]\n \n     def should_cache_request(self, request):\n         return urlparse_cached(request).scheme not in self.ignore_schemes\n@@ -44,16 +45,17 @@ class RFC2616Policy:\n     MAXAGE = 3600 * 24 * 365  # one year\n \n     def __init__(self, settings):\n-        self.always_store = settings.getbool('HTTPCACHE_ALWAYS_STORE')\n-        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n+        self.always_store = settings.getbool(\"HTTPCACHE_ALWAYS_STORE\")\n+        self.ignore_schemes = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n         self._cc_parsed = WeakKeyDictionary()\n         self.ignore_response_cache_controls = [\n-            to_bytes(cc) for cc in settings.getlist('HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS')\n+            to_bytes(cc)\n+            for cc in settings.getlist(\"HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS\")\n         ]\n \n     def _parse_cachecontrol(self, r):\n         if r not in self._cc_parsed:\n-            cch = r.headers.get(b'Cache-Control', b'')\n+            cch = r.headers.get(b\"Cache-Control\", b\"\")\n             parsed = parse_cachecontrol(cch)\n             if isinstance(r, Response):\n                 for key in self.ignore_response_cache_controls:\n@@ -66,7 +68,7 @@ class RFC2616Policy:\n             return False\n         cc = self._parse_cachecontrol(request)\n         # obey user-agent directive \"Cache-Control: no-store\"\n-        if b'no-store' in cc:\n+        if b\"no-store\" in cc:\n             return False\n         # Any other is eligible for caching\n         return True\n@@ -77,7 +79,7 @@ class RFC2616Policy:\n         # Status code 206 is not included because cache can not deal with partial contents\n         cc = self._parse_cachecontrol(response)\n         # obey directive \"Cache-Control: no-store\"\n-        if b'no-store' in cc:\n+        if b\"no-store\" in cc:\n             return False\n         # Never cache 304 (Not Modified) responses\n         if response.status == 304:\n@@ -86,14 +88,14 @@ class RFC2616Policy:\n         if self.always_store:\n             return True\n         # Any hint on response expiration is good\n-        if b'max-age' in cc or b'Expires' in response.headers:\n+        if b\"max-age\" in cc or b\"Expires\" in response.headers:\n             return True\n         # Firefox fallbacks this statuses to one year expiration if none is set\n         if response.status in (300, 301, 308):\n             return True\n         # Other statuses without expiration requires at least one validator\n         if response.status in (200, 203, 401):\n-            return b'Last-Modified' in response.headers or b'ETag' in response.headers\n+            return b\"Last-Modified\" in response.headers or b\"ETag\" in response.headers\n         # Any other is probably not eligible for caching\n         # Makes no sense to cache responses that does not contain expiration\n         # info and can not be revalidated\n@@ -102,11 +104,13 @@ class RFC2616Policy:\n     def is_cached_response_fresh(self, cachedresponse, request):\n         cc = self._parse_cachecontrol(cachedresponse)\n         ccreq = self._parse_cachecontrol(request)\n-        if b'no-cache' in cc or b'no-cache' in ccreq:\n+        if b\"no-cache\" in cc or b\"no-cache\" in ccreq:\n             return False\n \n         now = time()\n-        freshnesslifetime = self._compute_freshness_lifetime(cachedresponse, request, now)\n+        freshnesslifetime = self._compute_freshness_lifetime(\n+            cachedresponse, request, now\n+        )\n         currentage = self._compute_current_age(cachedresponse, request, now)\n \n         reqmaxage = self._get_max_age(ccreq)\n@@ -116,7 +120,7 @@ class RFC2616Policy:\n         if currentage < freshnesslifetime:\n             return True\n \n-        if b'max-stale' in ccreq and b'must-revalidate' not in cc:\n+        if b\"max-stale\" in ccreq and b\"must-revalidate\" not in cc:\n             # From RFC2616: \"Indicates that the client is willing to\n             # accept a response that has exceeded its expiration time.\n             # If max-stale is assigned a value, then the client is\n@@ -124,7 +128,7 @@ class RFC2616Policy:\n             # expiration time by no more than the specified number of\n             # seconds. If no value is assigned to max-stale, then the\n             # client is willing to accept a stale response of any age.\"\n-            staleage = ccreq[b'max-stale']\n+            staleage = ccreq[b\"max-stale\"]\n             if staleage is None:\n                 return True\n \n@@ -143,22 +147,24 @@ class RFC2616Policy:\n         # as long as the old response didn't specify must-revalidate.\n         if response.status >= 500:\n             cc = self._parse_cachecontrol(cachedresponse)\n-            if b'must-revalidate' not in cc:\n+            if b\"must-revalidate\" not in cc:\n                 return True\n \n         # Use the cached response if the server says it hasn't changed.\n         return response.status == 304\n \n     def _set_conditional_validators(self, request, cachedresponse):\n-        if b'Last-Modified' in cachedresponse.headers:\n-            request.headers[b'If-Modified-Since'] = cachedresponse.headers[b'Last-Modified']\n+        if b\"Last-Modified\" in cachedresponse.headers:\n+            request.headers[b\"If-Modified-Since\"] = cachedresponse.headers[\n+                b\"Last-Modified\"\n+            ]\n \n-        if b'ETag' in cachedresponse.headers:\n-            request.headers[b'If-None-Match'] = cachedresponse.headers[b'ETag']\n+        if b\"ETag\" in cachedresponse.headers:\n+            request.headers[b\"If-None-Match\"] = cachedresponse.headers[b\"ETag\"]\n \n     def _get_max_age(self, cc):\n         try:\n-            return max(0, int(cc[b'max-age']))\n+            return max(0, int(cc[b\"max-age\"]))\n         except (KeyError, ValueError):\n             return None\n \n@@ -171,18 +177,18 @@ class RFC2616Policy:\n             return maxage\n \n         # Parse date header or synthesize it if none exists\n-        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now\n+        date = rfc1123_to_epoch(response.headers.get(b\"Date\")) or now\n \n         # Try HTTP/1.0 Expires header\n-        if b'Expires' in response.headers:\n-            expires = rfc1123_to_epoch(response.headers[b'Expires'])\n+        if b\"Expires\" in response.headers:\n+            expires = rfc1123_to_epoch(response.headers[b\"Expires\"])\n             # When parsing Expires header fails RFC 2616 section 14.21 says we\n             # should treat this as an expiration time in the past.\n             return max(0, expires - date) if expires else 0\n \n         # Fallback to heuristic using last-modified header\n         # This is not in RFC but on Firefox caching implementation\n-        lastmodified = rfc1123_to_epoch(response.headers.get(b'Last-Modified'))\n+        lastmodified = rfc1123_to_epoch(response.headers.get(b\"Last-Modified\"))\n         if lastmodified and lastmodified <= date:\n             return (date - lastmodified) / 10\n \n@@ -199,13 +205,13 @@ class RFC2616Policy:\n         currentage = 0\n         # If Date header is not set we assume it is a fast connection, and\n         # clock is in sync with the server\n-        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now\n+        date = rfc1123_to_epoch(response.headers.get(b\"Date\")) or now\n         if now > date:\n             currentage = now - date\n \n-        if b'Age' in response.headers:\n+        if b\"Age\" in response.headers:\n             try:\n-                age = int(response.headers[b'Age'])\n+                age = int(response.headers[b\"Age\"])\n                 currentage = max(currentage, age)\n             except ValueError:\n                 pass\n@@ -214,18 +220,21 @@ class RFC2616Policy:\n \n \n class DbmCacheStorage:\n-\n     def __init__(self, settings):\n-        self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)\n-        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n-        self.dbmodule = import_module(settings['HTTPCACHE_DBM_MODULE'])\n+        self.cachedir = data_path(settings[\"HTTPCACHE_DIR\"], createdir=True)\n+        self.expiration_secs = settings.getint(\"HTTPCACHE_EXPIRATION_SECS\")\n+        self.dbmodule = import_module(settings[\"HTTPCACHE_DBM_MODULE\"])\n         self.db = None\n \n     def open_spider(self, spider: Spider):\n-        dbpath = Path(self.cachedir, f'{spider.name}.db')\n-        self.db = self.dbmodule.open(str(dbpath), 'c')\n+        dbpath = Path(self.cachedir, f\"{spider.name}.db\")\n+        self.db = self.dbmodule.open(str(dbpath), \"c\")\n \n-        logger.debug(\"Using DBM cache storage in %(cachepath)s\", {'cachepath': dbpath}, extra={'spider': spider})\n+        logger.debug(\n+            \"Using DBM cache storage in %(cachepath)s\",\n+            {\"cachepath\": dbpath},\n+            extra={\"spider\": spider},\n+        )\n \n         self._fingerprinter = spider.crawler.request_fingerprinter\n \n@@ -236,10 +245,10 @@ class DbmCacheStorage:\n         data = self._read_data(spider, request)\n         if data is None:\n             return  # not cached\n-        url = data['url']\n-        status = data['status']\n-        headers = Headers(data['headers'])\n-        body = data['body']\n+        url = data[\"url\"]\n+        status = data[\"status\"]\n+        headers = Headers(data[\"headers\"])\n+        body = data[\"body\"]\n         respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n         response = respcls(url=url, headers=headers, status=status, body=body)\n         return response\n@@ -247,18 +256,18 @@ class DbmCacheStorage:\n     def store_response(self, spider, request, response):\n         key = self._fingerprinter.fingerprint(request).hex()\n         data = {\n-            'status': response.status,\n-            'url': response.url,\n-            'headers': dict(response.headers),\n-            'body': response.body,\n+            \"status\": response.status,\n+            \"url\": response.url,\n+            \"headers\": dict(response.headers),\n+            \"body\": response.body,\n         }\n-        self.db[f'{key}_data'] = pickle.dumps(data, protocol=4)\n-        self.db[f'{key}_time'] = str(time())\n+        self.db[f\"{key}_data\"] = pickle.dumps(data, protocol=4)\n+        self.db[f\"{key}_time\"] = str(time())\n \n     def _read_data(self, spider, request):\n         key = self._fingerprinter.fingerprint(request).hex()\n         db = self.db\n-        tkey = f'{key}_time'\n+        tkey = f\"{key}_time\"\n         if tkey not in db:\n             return  # not found\n \n@@ -266,20 +275,22 @@ class DbmCacheStorage:\n         if 0 < self.expiration_secs < time() - float(ts):\n             return  # expired\n \n-        return pickle.loads(db[f'{key}_data'])\n+        return pickle.loads(db[f\"{key}_data\"])\n \n \n class FilesystemCacheStorage:\n-\n     def __init__(self, settings):\n-        self.cachedir = data_path(settings['HTTPCACHE_DIR'])\n-        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n-        self.use_gzip = settings.getbool('HTTPCACHE_GZIP')\n+        self.cachedir = data_path(settings[\"HTTPCACHE_DIR\"])\n+        self.expiration_secs = settings.getint(\"HTTPCACHE_EXPIRATION_SECS\")\n+        self.use_gzip = settings.getbool(\"HTTPCACHE_GZIP\")\n         self._open = gzip.open if self.use_gzip else open\n \n     def open_spider(self, spider: Spider):\n-        logger.debug(\"Using filesystem cache storage in %(cachedir)s\", {'cachedir': self.cachedir},\n-                     extra={'spider': spider})\n+        logger.debug(\n+            \"Using filesystem cache storage in %(cachedir)s\",\n+            {\"cachedir\": self.cachedir},\n+            extra={\"spider\": spider},\n+        )\n \n         self._fingerprinter = spider.crawler.request_fingerprinter\n \n@@ -292,12 +303,12 @@ class FilesystemCacheStorage:\n         if metadata is None:\n             return  # not cached\n         rpath = Path(self._get_request_path(spider, request))\n-        with self._open(rpath / 'response_body', 'rb') as f:\n+        with self._open(rpath / \"response_body\", \"rb\") as f:\n             body = f.read()\n-        with self._open(rpath / 'response_headers', 'rb') as f:\n+        with self._open(rpath / \"response_headers\", \"rb\") as f:\n             rawheaders = f.read()\n-        url = metadata.get('response_url')\n-        status = metadata['status']\n+        url = metadata.get(\"response_url\")\n+        status = metadata[\"status\"]\n         headers = Headers(headers_raw_to_dict(rawheaders))\n         respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n         response = respcls(url=url, headers=headers, status=status, body=body)\n@@ -309,23 +320,23 @@ class FilesystemCacheStorage:\n         if not rpath.exists():\n             rpath.mkdir(parents=True)\n         metadata = {\n-            'url': request.url,\n-            'method': request.method,\n-            'status': response.status,\n-            'response_url': response.url,\n-            'timestamp': time(),\n+            \"url\": request.url,\n+            \"method\": request.method,\n+            \"status\": response.status,\n+            \"response_url\": response.url,\n+            \"timestamp\": time(),\n         }\n-        with self._open(rpath / 'meta', 'wb') as f:\n+        with self._open(rpath / \"meta\", \"wb\") as f:\n             f.write(to_bytes(repr(metadata)))\n-        with self._open(rpath / 'pickled_meta', 'wb') as f:\n+        with self._open(rpath / \"pickled_meta\", \"wb\") as f:\n             pickle.dump(metadata, f, protocol=4)\n-        with self._open(rpath / 'response_headers', 'wb') as f:\n+        with self._open(rpath / \"response_headers\", \"wb\") as f:\n             f.write(headers_dict_to_raw(response.headers))\n-        with self._open(rpath / 'response_body', 'wb') as f:\n+        with self._open(rpath / \"response_body\", \"wb\") as f:\n             f.write(response.body)\n-        with self._open(rpath / 'request_headers', 'wb') as f:\n+        with self._open(rpath / \"request_headers\", \"wb\") as f:\n             f.write(headers_dict_to_raw(request.headers))\n-        with self._open(rpath / 'request_body', 'wb') as f:\n+        with self._open(rpath / \"request_body\", \"wb\") as f:\n             f.write(request.body)\n \n     def _get_request_path(self, spider: Spider, request: Request) -> str:\n@@ -334,13 +345,13 @@ class FilesystemCacheStorage:\n \n     def _read_meta(self, spider: Spider, request: Request):\n         rpath = Path(self._get_request_path(spider, request))\n-        metapath = rpath / 'pickled_meta'\n+        metapath = rpath / \"pickled_meta\"\n         if not metapath.exists():\n             return  # not found\n         mtime = metapath.stat().st_mtime\n         if 0 < self.expiration_secs < time() - mtime:\n             return  # expired\n-        with self._open(metapath, 'rb') as f:\n+        with self._open(metapath, \"rb\") as f:\n             return pickle.load(f)\n \n \n@@ -357,8 +368,8 @@ def parse_cachecontrol(header):\n \n     \"\"\"\n     directives = {}\n-    for directive in header.split(b','):\n-        key, sep, val = directive.strip().partition(b'=')\n+    for directive in header.split(b\",\"):\n+        key, sep, val = directive.strip().partition(b\"=\")\n         if key:\n             directives[key.lower()] = val if sep else None\n     return directives\n@@ -366,7 +377,7 @@ def parse_cachecontrol(header):\n \n def rfc1123_to_epoch(date_str):\n     try:\n-        date_str = to_unicode(date_str, encoding='ascii')\n+        date_str = to_unicode(date_str, encoding=\"ascii\")\n         return mktime_tz(parsedate_tz(date_str))\n     except Exception:\n         return None\n\n@@ -19,7 +19,7 @@ class LogStats:\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        interval = crawler.settings.getfloat('LOGSTATS_INTERVAL')\n+        interval = crawler.settings.getfloat(\"LOGSTATS_INTERVAL\")\n         if not interval:\n             raise NotConfigured\n         o = cls(crawler.stats, interval)\n@@ -35,17 +35,23 @@ class LogStats:\n         self.task.start(self.interval)\n \n     def log(self, spider):\n-        items = self.stats.get_value('item_scraped_count', 0)\n-        pages = self.stats.get_value('response_received_count', 0)\n+        items = self.stats.get_value(\"item_scraped_count\", 0)\n+        pages = self.stats.get_value(\"response_received_count\", 0)\n         irate = (items - self.itemsprev) * self.multiplier\n         prate = (pages - self.pagesprev) * self.multiplier\n         self.pagesprev, self.itemsprev = pages, items\n \n-        msg = (\"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"\n-               \"scraped %(items)d items (at %(itemrate)d items/min)\")\n-        log_args = {'pages': pages, 'pagerate': prate,\n-                    'items': items, 'itemrate': irate}\n-        logger.info(msg, log_args, extra={'spider': spider})\n+        msg = (\n+            \"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"\n+            \"scraped %(items)d items (at %(itemrate)d items/min)\"\n+        )\n+        log_args = {\n+            \"pages\": pages,\n+            \"pagerate\": prate,\n+            \"items\": items,\n+            \"itemrate\": irate,\n+        }\n+        logger.info(msg, log_args, extra={\"spider\": spider})\n \n     def spider_closed(self, spider, reason):\n         if self.task and self.task.running:\n\n@@ -12,13 +12,12 @@ from scrapy.utils.trackref import live_refs\n \n \n class MemoryDebugger:\n-\n     def __init__(self, stats):\n         self.stats = stats\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        if not crawler.settings.getbool('MEMDEBUG_ENABLED'):\n+        if not crawler.settings.getbool(\"MEMDEBUG_ENABLED\"):\n             raise NotConfigured\n         o = cls(crawler.stats)\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n@@ -26,8 +25,12 @@ class MemoryDebugger:\n \n     def spider_closed(self, spider, reason):\n         gc.collect()\n-        self.stats.set_value('memdebug/gc_garbage_count', len(gc.garbage), spider=spider)\n+        self.stats.set_value(\n+            \"memdebug/gc_garbage_count\", len(gc.garbage), spider=spider\n+        )\n         for cls, wdict in live_refs.items():\n             if not wdict:\n                 continue\n-            self.stats.set_value(f'memdebug/live_refs/{cls.__name__}', len(wdict), spider=spider)\n+            self.stats.set_value(\n+                f\"memdebug/live_refs/{cls.__name__}\", len(wdict), spider=spider\n+            )\n\n@@ -20,22 +20,23 @@ logger = logging.getLogger(__name__)\n \n \n class MemoryUsage:\n-\n     def __init__(self, crawler):\n-        if not crawler.settings.getbool('MEMUSAGE_ENABLED'):\n+        if not crawler.settings.getbool(\"MEMUSAGE_ENABLED\"):\n             raise NotConfigured\n         try:\n             # stdlib's resource module is only available on unix platforms.\n-            self.resource = import_module('resource')\n+            self.resource = import_module(\"resource\")\n         except ImportError:\n             raise NotConfigured\n \n         self.crawler = crawler\n         self.warned = False\n-        self.notify_mails = crawler.settings.getlist('MEMUSAGE_NOTIFY_MAIL')\n-        self.limit = crawler.settings.getint('MEMUSAGE_LIMIT_MB') * 1024 * 1024\n-        self.warning = crawler.settings.getint('MEMUSAGE_WARNING_MB') * 1024 * 1024\n-        self.check_interval = crawler.settings.getfloat('MEMUSAGE_CHECK_INTERVAL_SECONDS')\n+        self.notify_mails = crawler.settings.getlist(\"MEMUSAGE_NOTIFY_MAIL\")\n+        self.limit = crawler.settings.getint(\"MEMUSAGE_LIMIT_MB\") * 1024 * 1024\n+        self.warning = crawler.settings.getint(\"MEMUSAGE_WARNING_MB\") * 1024 * 1024\n+        self.check_interval = crawler.settings.getfloat(\n+            \"MEMUSAGE_CHECK_INTERVAL_SECONDS\"\n+        )\n         self.mail = MailSender.from_settings(crawler.settings)\n         crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n         crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)\n@@ -46,13 +47,13 @@ class MemoryUsage:\n \n     def get_virtual_size(self):\n         size = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss\n-        if sys.platform != 'darwin':\n+        if sys.platform != \"darwin\":\n             # on macOS ru_maxrss is in bytes, on Linux it is in KB\n             size *= 1024\n         return size\n \n     def engine_started(self):\n-        self.crawler.stats.set_value('memusage/startup', self.get_virtual_size())\n+        self.crawler.stats.set_value(\"memusage/startup\", self.get_virtual_size())\n         self.tasks = []\n         tsk = task.LoopingCall(self.update)\n         self.tasks.append(tsk)\n@@ -72,45 +73,56 @@ class MemoryUsage:\n                 tsk.stop()\n \n     def update(self):\n-        self.crawler.stats.max_value('memusage/max', self.get_virtual_size())\n+        self.crawler.stats.max_value(\"memusage/max\", self.get_virtual_size())\n \n     def _check_limit(self):\n         peak_mem_usage = self.get_virtual_size()\n         if peak_mem_usage > self.limit:\n-            self.crawler.stats.set_value('memusage/limit_reached', 1)\n+            self.crawler.stats.set_value(\"memusage/limit_reached\", 1)\n             mem = self.limit / 1024 / 1024\n-            logger.error(\"Memory usage exceeded %(memusage)dMiB. Shutting down Scrapy...\",\n-                         {'memusage': mem}, extra={'crawler': self.crawler})\n+            logger.error(\n+                \"Memory usage exceeded %(memusage)dMiB. Shutting down Scrapy...\",\n+                {\"memusage\": mem},\n+                extra={\"crawler\": self.crawler},\n+            )\n             if self.notify_mails:\n                 subj = (\n                     f\"{self.crawler.settings['BOT_NAME']} terminated: \"\n                     f\"memory usage exceeded {mem}MiB at {socket.gethostname()}\"\n                 )\n                 self._send_report(self.notify_mails, subj)\n-                self.crawler.stats.set_value('memusage/limit_notified', 1)\n+                self.crawler.stats.set_value(\"memusage/limit_notified\", 1)\n \n             if self.crawler.engine.spider is not None:\n-                self.crawler.engine.close_spider(self.crawler.engine.spider, 'memusage_exceeded')\n+                self.crawler.engine.close_spider(\n+                    self.crawler.engine.spider, \"memusage_exceeded\"\n+                )\n             else:\n                 self.crawler.stop()\n         else:\n-            logger.info(\"Peak memory usage is %(virtualsize)dMiB\", {'virtualsize': peak_mem_usage / 1024 / 1024})\n+            logger.info(\n+                \"Peak memory usage is %(virtualsize)dMiB\",\n+                {\"virtualsize\": peak_mem_usage / 1024 / 1024},\n+            )\n \n     def _check_warning(self):\n         if self.warned:  # warn only once\n             return\n         if self.get_virtual_size() > self.warning:\n-            self.crawler.stats.set_value('memusage/warning_reached', 1)\n+            self.crawler.stats.set_value(\"memusage/warning_reached\", 1)\n             mem = self.warning / 1024 / 1024\n-            logger.warning(\"Memory usage reached %(memusage)dMiB\",\n-                           {'memusage': mem}, extra={'crawler': self.crawler})\n+            logger.warning(\n+                \"Memory usage reached %(memusage)dMiB\",\n+                {\"memusage\": mem},\n+                extra={\"crawler\": self.crawler},\n+            )\n             if self.notify_mails:\n                 subj = (\n                     f\"{self.crawler.settings['BOT_NAME']} warning: \"\n                     f\"memory usage reached {mem}MiB at {socket.gethostname()}\"\n                 )\n                 self._send_report(self.notify_mails, subj)\n-                self.crawler.stats.set_value('memusage/warning_notified', 1)\n+                self.crawler.stats.set_value(\"memusage/warning_notified\", 1)\n             self.warned = True\n \n     def _send_report(self, rcpts, subject):\n@@ -120,7 +132,9 @@ class MemoryUsage:\n         s += f\"Maximum memory usage          : {stats.get_value('memusage/max')/1024/1024}M\\r\\n\"\n         s += f\"Current memory usage          : {self.get_virtual_size()/1024/1024}M\\r\\n\"\n \n-        s += \"ENGINE STATUS ------------------------------------------------------- \\r\\n\"\n+        s += (\n+            \"ENGINE STATUS ------------------------------------------------------- \\r\\n\"\n+        )\n         s += \"\\r\\n\"\n         s += pformat(get_engine_status(self.crawler.engine))\n         s += \"\\r\\n\"\n\n@@ -29,8 +29,13 @@ class GzipPlugin:\n         compress_level = self.feed_options.get(\"gzip_compresslevel\", 9)\n         mtime = self.feed_options.get(\"gzip_mtime\")\n         filename = self.feed_options.get(\"gzip_filename\")\n-        self.gzipfile = GzipFile(fileobj=self.file, mode=\"wb\", compresslevel=compress_level,\n-                                 mtime=mtime, filename=filename)\n+        self.gzipfile = GzipFile(\n+            fileobj=self.file,\n+            mode=\"wb\",\n+            compresslevel=compress_level,\n+            mtime=mtime,\n+            filename=filename,\n+        )\n \n     def write(self, data: bytes) -> int:\n         return self.gzipfile.write(data)\n@@ -55,7 +60,9 @@ class Bz2Plugin:\n         self.file = file\n         self.feed_options = feed_options\n         compress_level = self.feed_options.get(\"bz2_compresslevel\", 9)\n-        self.bz2file = BZ2File(filename=self.file, mode=\"wb\", compresslevel=compress_level)\n+        self.bz2file = BZ2File(\n+            filename=self.file, mode=\"wb\", compresslevel=compress_level\n+        )\n \n     def write(self, data: bytes) -> int:\n         return self.bz2file.write(data)\n@@ -90,8 +97,14 @@ class LZMAPlugin:\n         check = self.feed_options.get(\"lzma_check\", -1)\n         preset = self.feed_options.get(\"lzma_preset\")\n         filters = self.feed_options.get(\"lzma_filters\")\n-        self.lzmafile = LZMAFile(filename=self.file, mode=\"wb\", format=format,\n-                                 check=check, preset=preset, filters=filters)\n+        self.lzmafile = LZMAFile(\n+            filename=self.file,\n+            mode=\"wb\",\n+            format=format,\n+            check=check,\n+            preset=preset,\n+            filters=filters,\n+        )\n \n     def write(self, data: bytes) -> int:\n         return self.lzmafile.write(data)\n@@ -114,7 +127,9 @@ class PostProcessingManager(IOBase):\n     :type file: file like object\n     \"\"\"\n \n-    def __init__(self, plugins: List[Any], file: BinaryIO, feed_options: Dict[str, Any]) -> None:\n+    def __init__(\n+        self, plugins: List[Any], file: BinaryIO, feed_options: Dict[str, Any]\n+    ) -> None:\n         self.plugins = self._load_plugins(plugins)\n         self.file = file\n         self.feed_options = feed_options\n\n@@ -25,16 +25,16 @@ class SpiderState:\n \n     def spider_closed(self, spider):\n         if self.jobdir:\n-            with Path(self.statefn).open('wb') as f:\n+            with Path(self.statefn).open(\"wb\") as f:\n                 pickle.dump(spider.state, f, protocol=4)\n \n     def spider_opened(self, spider):\n         if self.jobdir and Path(self.statefn).exists():\n-            with Path(self.statefn).open('rb') as f:\n+            with Path(self.statefn).open(\"rb\") as f:\n                 spider.state = pickle.load(f)\n         else:\n             spider.state = {}\n \n     @property\n     def statefn(self) -> str:\n-        return str(Path(self.jobdir, 'spider.state'))\n+        return str(Path(self.jobdir, \"spider.state\"))\n\n@@ -10,7 +10,6 @@ from scrapy.exceptions import NotConfigured\n \n \n class StatsMailer:\n-\n     def __init__(self, stats, recipients, mail):\n         self.stats = stats\n         self.recipients = recipients\n\n@@ -15,6 +15,7 @@ from twisted.internet import protocol\n try:\n     from twisted.conch import manhole, telnet\n     from twisted.conch.insults import insults\n+\n     TWISTED_CONCH_AVAILABLE = True\n except (ImportError, SyntaxError):\n     _TWISTED_CONCH_TRACEBACK = traceback.format_exc()\n@@ -35,24 +36,26 @@ update_telnet_vars = object()\n \n \n class TelnetConsole(protocol.ServerFactory):\n-\n     def __init__(self, crawler):\n-        if not crawler.settings.getbool('TELNETCONSOLE_ENABLED'):\n+        if not crawler.settings.getbool(\"TELNETCONSOLE_ENABLED\"):\n             raise NotConfigured\n         if not TWISTED_CONCH_AVAILABLE:\n             raise NotConfigured(\n-                'TELNETCONSOLE_ENABLED setting is True but required twisted '\n-                'modules failed to import:\\n' + _TWISTED_CONCH_TRACEBACK)\n+                \"TELNETCONSOLE_ENABLED setting is True but required twisted \"\n+                \"modules failed to import:\\n\" + _TWISTED_CONCH_TRACEBACK\n+            )\n         self.crawler = crawler\n         self.noisy = False\n-        self.portrange = [int(x) for x in crawler.settings.getlist('TELNETCONSOLE_PORT')]\n-        self.host = crawler.settings['TELNETCONSOLE_HOST']\n-        self.username = crawler.settings['TELNETCONSOLE_USERNAME']\n-        self.password = crawler.settings['TELNETCONSOLE_PASSWORD']\n+        self.portrange = [\n+            int(x) for x in crawler.settings.getlist(\"TELNETCONSOLE_PORT\")\n+        ]\n+        self.host = crawler.settings[\"TELNETCONSOLE_HOST\"]\n+        self.username = crawler.settings[\"TELNETCONSOLE_USERNAME\"]\n+        self.password = crawler.settings[\"TELNETCONSOLE_PASSWORD\"]\n \n         if not self.password:\n-            self.password = binascii.hexlify(os.urandom(8)).decode('utf8')\n-            logger.info('Telnet Password: %s', self.password)\n+            self.password = binascii.hexlify(os.urandom(8)).decode(\"utf8\")\n+            logger.info(\"Telnet Password: %s\", self.password)\n \n         self.crawler.signals.connect(self.start_listening, signals.engine_started)\n         self.crawler.signals.connect(self.stop_listening, signals.engine_stopped)\n@@ -64,9 +67,11 @@ class TelnetConsole(protocol.ServerFactory):\n     def start_listening(self):\n         self.port = listen_tcp(self.portrange, self.host, self)\n         h = self.port.getHost()\n-        logger.info(\"Telnet console listening on %(host)s:%(port)d\",\n-                    {'host': h.host, 'port': h.port},\n-                    extra={'crawler': self.crawler})\n+        logger.info(\n+            \"Telnet console listening on %(host)s:%(port)d\",\n+            {\"host\": h.host, \"port\": h.port},\n+            extra={\"crawler\": self.crawler},\n+        )\n \n     def stop_listening(self):\n         self.port.stopListening()\n@@ -74,40 +79,36 @@ class TelnetConsole(protocol.ServerFactory):\n     def protocol(self):\n         class Portal:\n             \"\"\"An implementation of IPortal\"\"\"\n+\n             @defers\n             def login(self_, credentials, mind, *interfaces):\n                 if not (\n-                    credentials.username == self.username.encode('utf8')\n-                    and credentials.checkPassword(self.password.encode('utf8'))\n+                    credentials.username == self.username.encode(\"utf8\")\n+                    and credentials.checkPassword(self.password.encode(\"utf8\"))\n                 ):\n                     raise ValueError(\"Invalid credentials\")\n \n                 protocol = telnet.TelnetBootstrapProtocol(\n-                    insults.ServerProtocol,\n-                    manhole.Manhole,\n-                    self._get_telnet_vars()\n+                    insults.ServerProtocol, manhole.Manhole, self._get_telnet_vars()\n                 )\n                 return (interfaces[0], protocol, lambda: None)\n \n-        return telnet.TelnetTransport(\n-            telnet.AuthenticatingTelnetProtocol,\n-            Portal()\n-        )\n+        return telnet.TelnetTransport(telnet.AuthenticatingTelnetProtocol, Portal())\n \n     def _get_telnet_vars(self):\n         # Note: if you add entries here also update topics/telnetconsole.rst\n         telnet_vars = {\n-            'engine': self.crawler.engine,\n-            'spider': self.crawler.engine.spider,\n-            'slot': self.crawler.engine.slot,\n-            'crawler': self.crawler,\n-            'extensions': self.crawler.extensions,\n-            'stats': self.crawler.stats,\n-            'settings': self.crawler.settings,\n-            'est': lambda: print_engine_status(self.crawler.engine),\n-            'p': pprint.pprint,\n-            'prefs': print_live_refs,\n-            'help': \"This is Scrapy telnet console. For more info see: \"\n+            \"engine\": self.crawler.engine,\n+            \"spider\": self.crawler.engine.spider,\n+            \"slot\": self.crawler.engine.slot,\n+            \"crawler\": self.crawler,\n+            \"extensions\": self.crawler.extensions,\n+            \"stats\": self.crawler.stats,\n+            \"settings\": self.crawler.settings,\n+            \"est\": lambda: print_engine_status(self.crawler.engine),\n+            \"p\": pprint.pprint,\n+            \"prefs\": print_live_refs,\n+            \"help\": \"This is Scrapy telnet console. For more info see: \"\n             \"https://docs.scrapy.org/en/latest/topics/telnetconsole.html\",\n         }\n         self.crawler.signals.send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n\n@@ -7,16 +7,19 @@ logger = logging.getLogger(__name__)\n \n \n class AutoThrottle:\n-\n     def __init__(self, crawler):\n         self.crawler = crawler\n-        if not crawler.settings.getbool('AUTOTHROTTLE_ENABLED'):\n+        if not crawler.settings.getbool(\"AUTOTHROTTLE_ENABLED\"):\n             raise NotConfigured\n \n         self.debug = crawler.settings.getbool(\"AUTOTHROTTLE_DEBUG\")\n-        self.target_concurrency = crawler.settings.getfloat(\"AUTOTHROTTLE_TARGET_CONCURRENCY\")\n+        self.target_concurrency = crawler.settings.getfloat(\n+            \"AUTOTHROTTLE_TARGET_CONCURRENCY\"\n+        )\n         crawler.signals.connect(self._spider_opened, signal=signals.spider_opened)\n-        crawler.signals.connect(self._response_downloaded, signal=signals.response_downloaded)\n+        crawler.signals.connect(\n+            self._response_downloaded, signal=signals.response_downloaded\n+        )\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -29,17 +32,19 @@ class AutoThrottle:\n \n     def _min_delay(self, spider):\n         s = self.crawler.settings\n-        return getattr(spider, 'download_delay', s.getfloat('DOWNLOAD_DELAY'))\n+        return getattr(spider, \"download_delay\", s.getfloat(\"DOWNLOAD_DELAY\"))\n \n     def _max_delay(self, spider):\n-        return self.crawler.settings.getfloat('AUTOTHROTTLE_MAX_DELAY')\n+        return self.crawler.settings.getfloat(\"AUTOTHROTTLE_MAX_DELAY\")\n \n     def _start_delay(self, spider):\n-        return max(self.mindelay, self.crawler.settings.getfloat('AUTOTHROTTLE_START_DELAY'))\n+        return max(\n+            self.mindelay, self.crawler.settings.getfloat(\"AUTOTHROTTLE_START_DELAY\")\n+        )\n \n     def _response_downloaded(self, response, request, spider):\n         key, slot = self._get_slot(request, spider)\n-        latency = request.meta.get('download_latency')\n+        latency = request.meta.get(\"download_latency\")\n         if latency is None or slot is None:\n             return\n \n@@ -54,15 +59,18 @@ class AutoThrottle:\n                 \"delay:%(delay)5d ms (%(delaydiff)+d) | \"\n                 \"latency:%(latency)5d ms | size:%(size)6d bytes\",\n                 {\n-                    'slot': key, 'concurrency': conc,\n-                    'delay': slot.delay * 1000, 'delaydiff': diff * 1000,\n-                    'latency': latency * 1000, 'size': size,\n+                    \"slot\": key,\n+                    \"concurrency\": conc,\n+                    \"delay\": slot.delay * 1000,\n+                    \"delaydiff\": diff * 1000,\n+                    \"latency\": latency * 1000,\n+                    \"size\": size,\n                 },\n-                extra={'spider': spider}\n+                extra={\"spider\": spider},\n             )\n \n     def _get_slot(self, request, spider):\n-        key = request.meta.get('download_slot')\n+        key = request.meta.get(\"download_slot\")\n         return key, self.crawler.engine.downloader.slots.get(key)\n \n     def _adjust_delay(self, slot, latency, response):\n\n@@ -3,4 +3,5 @@ def obsolete_setter(setter, attrname):\n         c = self.__class__.__name__\n         msg = f\"{c}.{attrname} is not modifiable, use {c}.replace() instead\"\n         raise AttributeError(msg)\n+\n     return newsetter\n\n@@ -36,7 +36,7 @@ class CookieJar:\n \n         if not IPV4_RE.search(req_host):\n             hosts = potential_domain_matches(req_host)\n-            if '.' not in req_host:\n+            if \".\" not in req_host:\n                 hosts += [req_host + \".local\"]\n         else:\n             hosts = [req_host]\n@@ -96,14 +96,14 @@ def potential_domain_matches(domain):\n     \"\"\"\n     matches = [domain]\n     try:\n-        start = domain.index('.') + 1\n-        end = domain.rindex('.')\n+        start = domain.index(\".\") + 1\n+        end = domain.rindex(\".\")\n         while start < end:\n             matches.append(domain[start:])\n-            start = domain.index('.', start) + 1\n+            start = domain.index(\".\", start) + 1\n     except ValueError:\n         pass\n-    return matches + ['.' + d for d in matches]\n+    return matches + [\".\" + d for d in matches]\n \n \n class _DummyLock:\n@@ -140,7 +140,7 @@ class WrappedRequest:\n         HTML document, and the user had no option to approve the automatic\n         fetching of the image, this should be true.\n         \"\"\"\n-        return self.request.meta.get('is_unverifiable', False)\n+        return self.request.meta.get(\"is_unverifiable\", False)\n \n     @property\n     def full_url(self):\n@@ -166,13 +166,14 @@ class WrappedRequest:\n         return name in self.request.headers\n \n     def get_header(self, name, default=None):\n-        return to_unicode(self.request.headers.get(name, default),\n-                          errors='replace')\n+        return to_unicode(self.request.headers.get(name, default), errors=\"replace\")\n \n     def header_items(self):\n         return [\n-            (to_unicode(k, errors='replace'),\n-             [to_unicode(x, errors='replace') for x in v])\n+            (\n+                to_unicode(k, errors=\"replace\"),\n+                [to_unicode(x, errors=\"replace\") for x in v],\n+            )\n             for k, v in self.request.headers.items()\n         ]\n \n@@ -181,7 +182,6 @@ class WrappedRequest:\n \n \n class WrappedResponse:\n-\n     def __init__(self, response):\n         self.response = response\n \n@@ -189,5 +189,6 @@ class WrappedResponse:\n         return self\n \n     def get_all(self, name, default=None):\n-        return [to_unicode(v, errors='replace')\n-                for v in self.response.headers.getlist(name)]\n+        return [\n+            to_unicode(v, errors=\"replace\") for v in self.response.headers.getlist(name)\n+        ]\n\n@@ -8,7 +8,7 @@ from scrapy.utils.python import to_unicode\n class Headers(CaselessDict):\n     \"\"\"Case insensitive http headers dictionary\"\"\"\n \n-    def __init__(self, seq=None, encoding='utf-8'):\n+    def __init__(self, seq=None, encoding=\"utf-8\"):\n         self.encoding = encoding\n         super().__init__(seq)\n \n@@ -29,7 +29,7 @@ class Headers(CaselessDict):\n             value = []\n         elif isinstance(value, (str, bytes)):\n             value = [value]\n-        elif not hasattr(value, '__iter__'):\n+        elif not hasattr(value, \"__iter__\"):\n             value = [value]\n \n         return [self._tobytes(x) for x in value]\n@@ -41,7 +41,7 @@ class Headers(CaselessDict):\n             return x.encode(self.encoding)\n         if isinstance(x, int):\n             return str(x).encode(self.encoding)\n-        raise TypeError(f'Unsupported value type: {type(x)}')\n+        raise TypeError(f\"Unsupported value type: {type(x)}\")\n \n     def __getitem__(self, key):\n         try:\n@@ -88,9 +88,12 @@ class Headers(CaselessDict):\n         and unicode values. Multiple values are joined with ','.\n         \"\"\"\n         return CaselessDict(\n-            (to_unicode(key, encoding=self.encoding),\n-             to_unicode(b','.join(value), encoding=self.encoding))\n-            for key, value in self.items())\n+            (\n+                to_unicode(key, encoding=self.encoding),\n+                to_unicode(b\",\".join(value), encoding=self.encoding),\n+            )\n+            for key, value in self.items()\n+        )\n \n     def __copy__(self):\n         return self.__class__(self)\n\n@@ -27,9 +27,19 @@ class Request(object_ref):\n     \"\"\"\n \n     attributes: Tuple[str, ...] = (\n-        \"url\", \"callback\", \"method\", \"headers\", \"body\",\n-        \"cookies\", \"meta\", \"encoding\", \"priority\",\n-        \"dont_filter\", \"errback\", \"flags\", \"cb_kwargs\",\n+        \"url\",\n+        \"callback\",\n+        \"method\",\n+        \"headers\",\n+        \"body\",\n+        \"cookies\",\n+        \"meta\",\n+        \"encoding\",\n+        \"priority\",\n+        \"dont_filter\",\n+        \"errback\",\n+        \"flags\",\n+        \"cb_kwargs\",\n     )\n     \"\"\"A tuple of :class:`str` objects containing the name of all public\n     attributes of the class that are also keyword parameters of the\n@@ -64,9 +74,11 @@ class Request(object_ref):\n         self.priority = priority\n \n         if callback is not None and not callable(callback):\n-            raise TypeError(f'callback must be a callable, got {type(callback).__name__}')\n+            raise TypeError(\n+                f\"callback must be a callable, got {type(callback).__name__}\"\n+            )\n         if errback is not None and not callable(errback):\n-            raise TypeError(f'errback must be a callable, got {type(errback).__name__}')\n+            raise TypeError(f\"errback must be a callable, got {type(errback).__name__}\")\n         self.callback = callback\n         self.errback = errback\n \n@@ -101,13 +113,13 @@ class Request(object_ref):\n         self._url = escape_ajax(s)\n \n         if (\n-            '://' not in self._url\n-            and not self._url.startswith('about:')\n-            and not self._url.startswith('data:')\n+            \"://\" not in self._url\n+            and not self._url.startswith(\"about:\")\n+            and not self._url.startswith(\"data:\")\n         ):\n-            raise ValueError(f'Missing scheme in request url: {self._url}')\n+            raise ValueError(f\"Missing scheme in request url: {self._url}\")\n \n-    url = property(_get_url, obsolete_setter(_set_url, 'url'))\n+    url = property(_get_url, obsolete_setter(_set_url, \"url\"))\n \n     def _get_body(self) -> bytes:\n         return self._body\n@@ -115,7 +127,7 @@ class Request(object_ref):\n     def _set_body(self, body: Optional[Union[str, bytes]]) -> None:\n         self._body = b\"\" if body is None else to_bytes(body, self.encoding)\n \n-    body = property(_get_body, obsolete_setter(_set_body, 'body'))\n+    body = property(_get_body, obsolete_setter(_set_body, \"body\"))\n \n     @property\n     def encoding(self) -> str:\n@@ -131,12 +143,15 @@ class Request(object_ref):\n         \"\"\"Create a new Request with the same attributes except for those given new values\"\"\"\n         for x in self.attributes:\n             kwargs.setdefault(x, getattr(self, x))\n-        cls = kwargs.pop('cls', self.__class__)\n+        cls = kwargs.pop(\"cls\", self.__class__)\n         return cls(*args, **kwargs)\n \n     @classmethod\n     def from_curl(\n-        cls: Type[RequestTypeVar], curl_command: str, ignore_unknown_options: bool = True, **kwargs\n+        cls: Type[RequestTypeVar],\n+        curl_command: str,\n+        ignore_unknown_options: bool = True,\n+        **kwargs,\n     ) -> RequestTypeVar:\n         \"\"\"Create a Request object from a string containing a `cURL\n         <https://curl.haxx.se/>`_ command. It populates the HTTP method, the\n@@ -179,21 +194,25 @@ class Request(object_ref):\n         \"\"\"\n         d = {\n             \"url\": self.url,  # urls are safe (safe_string_url)\n-            \"callback\": _find_method(spider, self.callback) if callable(self.callback) else self.callback,\n-            \"errback\": _find_method(spider, self.errback) if callable(self.errback) else self.errback,\n+            \"callback\": _find_method(spider, self.callback)\n+            if callable(self.callback)\n+            else self.callback,\n+            \"errback\": _find_method(spider, self.errback)\n+            if callable(self.errback)\n+            else self.errback,\n             \"headers\": dict(self.headers),\n         }\n         for attr in self.attributes:\n             d.setdefault(attr, getattr(self, attr))\n         if type(self) is not Request:  # pylint: disable=unidiomatic-typecheck\n-            d[\"_class\"] = self.__module__ + '.' + self.__class__.__name__\n+            d[\"_class\"] = self.__module__ + \".\" + self.__class__.__name__\n         return d\n \n \n def _find_method(obj, func):\n     \"\"\"Helper function for Request.to_dict\"\"\"\n     # Only instance methods contain ``__func__``\n-    if obj and hasattr(func, '__func__'):\n+    if obj and hasattr(func, \"__func__\"):\n         members = inspect.getmembers(obj, predicate=inspect.ismethod)\n         for name, obj_func in members:\n             # We need to use __func__ to access the original function object because instance\n\n@@ -24,22 +24,26 @@ FormdataType = Optional[Union[dict, List[Tuple[str, str]]]]\n \n \n class FormRequest(Request):\n-    valid_form_methods = ['GET', 'POST']\n+    valid_form_methods = [\"GET\", \"POST\"]\n \n     def __init__(self, *args, formdata: FormdataType = None, **kwargs) -> None:\n-        if formdata and kwargs.get('method') is None:\n-            kwargs['method'] = 'POST'\n+        if formdata and kwargs.get(\"method\") is None:\n+            kwargs[\"method\"] = \"POST\"\n \n         super().__init__(*args, **kwargs)\n \n         if formdata:\n             items = formdata.items() if isinstance(formdata, dict) else formdata\n             form_query_str = _urlencode(items, self.encoding)\n-            if self.method == 'POST':\n-                self.headers.setdefault(b'Content-Type', b'application/x-www-form-urlencoded')\n+            if self.method == \"POST\":\n+                self.headers.setdefault(\n+                    b\"Content-Type\", b\"application/x-www-form-urlencoded\"\n+                )\n                 self._set_body(form_query_str)\n             else:\n-                self._set_url(urlunsplit(urlsplit(self.url)._replace(query=form_query_str)))\n+                self._set_url(\n+                    urlunsplit(urlsplit(self.url)._replace(query=form_query_str))\n+                )\n \n     @classmethod\n     def from_response(\n@@ -55,28 +59,29 @@ class FormRequest(Request):\n         formcss: Optional[str] = None,\n         **kwargs,\n     ) -> FormRequestTypeVar:\n-        kwargs.setdefault('encoding', response.encoding)\n+        kwargs.setdefault(\"encoding\", response.encoding)\n \n         if formcss is not None:\n             from parsel.csstranslator import HTMLTranslator\n+\n             formxpath = HTMLTranslator().css_to_xpath(formcss)\n \n         form = _get_form(response, formname, formid, formnumber, formxpath)\n         formdata = _get_inputs(form, formdata, dont_click, clickdata)\n-        url = _get_form_url(form, kwargs.pop('url', None))\n+        url = _get_form_url(form, kwargs.pop(\"url\", None))\n \n-        method = kwargs.pop('method', form.method)\n+        method = kwargs.pop(\"method\", form.method)\n         if method is not None:\n             method = method.upper()\n             if method not in cls.valid_form_methods:\n-                method = 'GET'\n+                method = \"GET\"\n \n         return cls(url=url, method=method, formdata=formdata, **kwargs)\n \n \n def _get_form_url(form: FormElement, url: Optional[str]) -> str:\n     if url is None:\n-        action = form.get('action')\n+        action = form.get(\"action\")\n         if action is None:\n             return form.base_url\n         return urljoin(form.base_url, strip_html5_whitespace(action))\n@@ -84,9 +89,11 @@ def _get_form_url(form: FormElement, url: Optional[str]) -> str:\n \n \n def _urlencode(seq: Iterable, enc: str) -> str:\n-    values = [(to_bytes(k, enc), to_bytes(v, enc))\n+    values = [\n+        (to_bytes(k, enc), to_bytes(v, enc))\n         for k, vs in seq\n-              for v in (vs if is_listlike(vs) else [vs])]\n+        for v in (vs if is_listlike(vs) else [vs])\n+    ]\n     return urlencode(values, doseq=True)\n \n \n@@ -99,7 +106,7 @@ def _get_form(\n ) -> FormElement:\n     \"\"\"Find the wanted form element within the given response.\"\"\"\n     root = create_root_node(response.text, HTMLParser, base_url=get_base_url(response))\n-    forms = root.xpath('//form')\n+    forms = root.xpath(\"//form\")\n     if not forms:\n         raise ValueError(f\"No <form> element found in {response}\")\n \n@@ -119,12 +126,12 @@ def _get_form(\n         if nodes:\n             el = nodes[0]\n             while True:\n-                if el.tag == 'form':\n+                if el.tag == \"form\":\n                     return el\n                 el = el.getparent()\n                 if el is None:\n                     break\n-        raise ValueError(f'No <form> element found with {formxpath}')\n+        raise ValueError(f\"No <form> element found with {formxpath}\")\n \n     # If we get here, it means that either formname was None or invalid\n     if formnumber is not None:\n@@ -146,19 +153,21 @@ def _get_inputs(\n     try:\n         formdata_keys = dict(formdata or ()).keys()\n     except (ValueError, TypeError):\n-        raise ValueError('formdata should be a dict or iterable of tuples')\n+        raise ValueError(\"formdata should be a dict or iterable of tuples\")\n \n     if not formdata:\n         formdata = []\n-    inputs = form.xpath('descendant::textarea'\n-                        '|descendant::select'\n-                        '|descendant::input[not(@type) or @type['\n+    inputs = form.xpath(\n+        \"descendant::textarea\"\n+        \"|descendant::select\"\n+        \"|descendant::input[not(@type) or @type[\"\n         ' not(re:test(., \"^(?:submit|image|reset)$\", \"i\"))'\n-                        ' and (../@checked or'\n+        \" and (../@checked or\"\n         '  not(re:test(., \"^(?:checkbox|radio)$\", \"i\")))]]',\n-                        namespaces={\"re\": \"http://exslt.org/regular-expressions\"})\n+        namespaces={\"re\": \"http://exslt.org/regular-expressions\"},\n+    )\n     values = [\n-        (k, '' if v is None else v)\n+        (k, \"\" if v is None else v)\n         for k, v in (_value(e) for e in inputs)\n         if k and k not in formdata_keys\n     ]\n@@ -178,7 +187,7 @@ def _get_inputs(\n def _value(ele: HtmlElement):\n     n = ele.name\n     v = ele.value\n-    if ele.tag == 'select':\n+    if ele.tag == \"select\":\n         return _select_value(ele, n, v)\n     return n, v\n \n@@ -193,51 +202,57 @@ def _select_value(ele: SelectElement, n: str, v: str):\n     if v is not None and multiple:\n         # This is a workround to bug in lxml fixed 2.3.1\n         # fix https://github.com/lxml/lxml/commit/57f49eed82068a20da3db8f1b18ae00c1bab8b12#L1L1139\n-        selected_options = ele.xpath('.//option[@selected]')\n-        values = [(o.get('value') or o.text or '').strip() for o in selected_options]\n+        selected_options = ele.xpath(\".//option[@selected]\")\n+        values = [(o.get(\"value\") or o.text or \"\").strip() for o in selected_options]\n         return n, values\n     return n, v\n \n \n-def _get_clickable(clickdata: Optional[dict], form: FormElement) -> Optional[Tuple[str, str]]:\n+def _get_clickable(\n+    clickdata: Optional[dict], form: FormElement\n+) -> Optional[Tuple[str, str]]:\n     \"\"\"\n     Returns the clickable element specified in clickdata,\n     if the latter is given. If not, it returns the first\n     clickable element found\n     \"\"\"\n-    clickables = list(form.xpath(\n+    clickables = list(\n+        form.xpath(\n             'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n             '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\n-        namespaces={\"re\": \"http://exslt.org/regular-expressions\"}\n-    ))\n+            namespaces={\"re\": \"http://exslt.org/regular-expressions\"},\n+        )\n+    )\n     if not clickables:\n         return None\n \n     # If we don't have clickdata, we just use the first clickable element\n     if clickdata is None:\n         el = clickables[0]\n-        return (el.get('name'), el.get('value') or '')\n+        return (el.get(\"name\"), el.get(\"value\") or \"\")\n \n     # If clickdata is given, we compare it to the clickable elements to find a\n     # match. We first look to see if the number is specified in clickdata,\n     # because that uniquely identifies the element\n-    nr = clickdata.get('nr', None)\n+    nr = clickdata.get(\"nr\", None)\n     if nr is not None:\n         try:\n             el = list(form.inputs)[nr]\n         except IndexError:\n             pass\n         else:\n-            return (el.get('name'), el.get('value') or '')\n+            return (el.get(\"name\"), el.get(\"value\") or \"\")\n \n     # We didn't find it, so now we build an XPath expression out of the other\n     # arguments, because they can be used as such\n-    xpath = './/*' + ''.join(f'[@{k}=\"{v}\"]' for k, v in clickdata.items())\n+    xpath = \".//*\" + \"\".join(f'[@{k}=\"{v}\"]' for k, v in clickdata.items())\n     el = form.xpath(xpath)\n     if len(el) == 1:\n-        return (el[0].get('name'), el[0].get('value') or '')\n+        return (el[0].get(\"name\"), el[0].get(\"value\") or \"\")\n     if len(el) > 1:\n-        raise ValueError(f\"Multiple elements found ({el!r}) matching the \"\n-                         f\"criteria in clickdata: {clickdata!r}\")\n+        raise ValueError(\n+            f\"Multiple elements found ({el!r}) matching the \"\n+            f\"criteria in clickdata: {clickdata!r}\"\n+        )\n     else:\n-        raise ValueError(f'No clickable element matching clickdata: {clickdata!r}')\n+        raise ValueError(f\"No clickable element matching clickdata: {clickdata!r}\")\n\n@@ -20,37 +20,39 @@ class JsonRequest(Request):\n \n     def __init__(self, *args, dumps_kwargs: Optional[dict] = None, **kwargs) -> None:\n         dumps_kwargs = copy.deepcopy(dumps_kwargs) if dumps_kwargs is not None else {}\n-        dumps_kwargs.setdefault('sort_keys', True)\n+        dumps_kwargs.setdefault(\"sort_keys\", True)\n         self._dumps_kwargs = dumps_kwargs\n \n-        body_passed = kwargs.get('body', None) is not None\n-        data = kwargs.pop('data', None)\n+        body_passed = kwargs.get(\"body\", None) is not None\n+        data = kwargs.pop(\"data\", None)\n         data_passed = data is not None\n \n         if body_passed and data_passed:\n-            warnings.warn('Both body and data passed. data will be ignored')\n+            warnings.warn(\"Both body and data passed. data will be ignored\")\n         elif not body_passed and data_passed:\n-            kwargs['body'] = self._dumps(data)\n-            if 'method' not in kwargs:\n-                kwargs['method'] = 'POST'\n+            kwargs[\"body\"] = self._dumps(data)\n+            if \"method\" not in kwargs:\n+                kwargs[\"method\"] = \"POST\"\n \n         super().__init__(*args, **kwargs)\n-        self.headers.setdefault('Content-Type', 'application/json')\n-        self.headers.setdefault('Accept', 'application/json, text/javascript, */*; q=0.01')\n+        self.headers.setdefault(\"Content-Type\", \"application/json\")\n+        self.headers.setdefault(\n+            \"Accept\", \"application/json, text/javascript, */*; q=0.01\"\n+        )\n \n     @property\n     def dumps_kwargs(self) -> dict:\n         return self._dumps_kwargs\n \n     def replace(self, *args, **kwargs) -> Request:\n-        body_passed = kwargs.get('body', None) is not None\n-        data = kwargs.pop('data', None)\n+        body_passed = kwargs.get(\"body\", None) is not None\n+        data = kwargs.pop(\"data\", None)\n         data_passed = data is not None\n \n         if body_passed and data_passed:\n-            warnings.warn('Both body and data passed. data will be ignored')\n+            warnings.warn(\"Both body and data passed. data will be ignored\")\n         elif not body_passed and data_passed:\n-            kwargs['body'] = self._dumps(data)\n+            kwargs[\"body\"] = self._dumps(data)\n \n         return super().replace(*args, **kwargs)\n \n\n@@ -15,21 +15,20 @@ DUMPS_ARGS = get_func_args(xmlrpclib.dumps)\n \n \n class XmlRpcRequest(Request):\n-\n     def __init__(self, *args, encoding: Optional[str] = None, **kwargs):\n-        if 'body' not in kwargs and 'params' in kwargs:\n+        if \"body\" not in kwargs and \"params\" in kwargs:\n             kw = dict((k, kwargs.pop(k)) for k in DUMPS_ARGS if k in kwargs)\n-            kwargs['body'] = xmlrpclib.dumps(**kw)\n+            kwargs[\"body\"] = xmlrpclib.dumps(**kw)\n \n         # spec defines that requests must use POST method\n-        kwargs.setdefault('method', 'POST')\n+        kwargs.setdefault(\"method\", \"POST\")\n \n         # xmlrpc query multiples times over the same url\n-        kwargs.setdefault('dont_filter', True)\n+        kwargs.setdefault(\"dont_filter\", True)\n \n         # restore encoding\n         if encoding is not None:\n-            kwargs['encoding'] = encoding\n+            kwargs[\"encoding\"] = encoding\n \n         super().__init__(*args, **kwargs)\n-        self.headers.setdefault('Content-Type', 'text/xml')\n+        self.headers.setdefault(\"Content-Type\", \"text/xml\")\n\n@@ -21,7 +21,15 @@ class Response(object_ref):\n     \"\"\"\n \n     attributes: Tuple[str, ...] = (\n-        \"url\", \"status\", \"headers\", \"body\", \"flags\", \"request\", \"certificate\", \"ip_address\", \"protocol\",\n+        \"url\",\n+        \"status\",\n+        \"headers\",\n+        \"body\",\n+        \"flags\",\n+        \"request\",\n+        \"certificate\",\n+        \"ip_address\",\n+        \"protocol\",\n     )\n     \"\"\"A tuple of :class:`str` objects containing the name of all public\n     attributes of the class that are also keyword parameters of the\n@@ -79,26 +87,28 @@ class Response(object_ref):\n         if isinstance(url, str):\n             self._url = url\n         else:\n-            raise TypeError(f'{type(self).__name__} url must be str, '\n-                            f'got {type(url).__name__}')\n+            raise TypeError(\n+                f\"{type(self).__name__} url must be str, \" f\"got {type(url).__name__}\"\n+            )\n \n-    url = property(_get_url, obsolete_setter(_set_url, 'url'))\n+    url = property(_get_url, obsolete_setter(_set_url, \"url\"))\n \n     def _get_body(self):\n         return self._body\n \n     def _set_body(self, body):\n         if body is None:\n-            self._body = b''\n+            self._body = b\"\"\n         elif not isinstance(body, bytes):\n             raise TypeError(\n                 \"Response body must be bytes. \"\n                 \"If you want to pass unicode body use TextResponse \"\n-                \"or HtmlResponse.\")\n+                \"or HtmlResponse.\"\n+            )\n         else:\n             self._body = body\n \n-    body = property(_get_body, obsolete_setter(_set_body, 'body'))\n+    body = property(_get_body, obsolete_setter(_set_body, \"body\"))\n \n     def __repr__(self):\n         return f\"<{self.status} {self.url}>\"\n@@ -111,7 +121,7 @@ class Response(object_ref):\n         \"\"\"Create a new Response with the same attributes except for those given new values\"\"\"\n         for x in self.attributes:\n             kwargs.setdefault(x, getattr(self, x))\n-        cls = kwargs.pop('cls', self.__class__)\n+        cls = kwargs.pop(\"cls\", self.__class__)\n         return cls(*args, **kwargs)\n \n     def urljoin(self, url):\n@@ -138,9 +148,22 @@ class Response(object_ref):\n         \"\"\"\n         raise NotSupported(\"Response content isn't text\")\n \n-    def follow(self, url, callback=None, method='GET', headers=None, body=None,\n-               cookies=None, meta=None, encoding='utf-8', priority=0,\n-               dont_filter=False, errback=None, cb_kwargs=None, flags=None) -> Request:\n+    def follow(\n+        self,\n+        url,\n+        callback=None,\n+        method=\"GET\",\n+        headers=None,\n+        body=None,\n+        cookies=None,\n+        meta=None,\n+        encoding=\"utf-8\",\n+        priority=0,\n+        dont_filter=False,\n+        errback=None,\n+        cb_kwargs=None,\n+        flags=None,\n+    ) -> Request:\n         \"\"\"\n         Return a :class:`~.Request` instance to follow a link ``url``.\n         It accepts the same arguments as ``Request.__init__`` method,\n@@ -176,10 +199,22 @@ class Response(object_ref):\n             flags=flags,\n         )\n \n-    def follow_all(self, urls, callback=None, method='GET', headers=None, body=None,\n-                   cookies=None, meta=None, encoding='utf-8', priority=0,\n-                   dont_filter=False, errback=None, cb_kwargs=None,\n-                   flags=None) -> Generator[Request, None, None]:\n+    def follow_all(\n+        self,\n+        urls,\n+        callback=None,\n+        method=\"GET\",\n+        headers=None,\n+        body=None,\n+        cookies=None,\n+        meta=None,\n+        encoding=\"utf-8\",\n+        priority=0,\n+        dont_filter=False,\n+        errback=None,\n+        cb_kwargs=None,\n+        flags=None,\n+    ) -> Generator[Request, None, None]:\n         \"\"\"\n         .. versionadded:: 2.0\n \n@@ -192,7 +227,7 @@ class Response(object_ref):\n         method which supports selectors in addition to absolute/relative URLs\n         and Link objects.\n         \"\"\"\n-        if not hasattr(urls, '__iter__'):\n+        if not hasattr(urls, \"__iter__\"):\n             raise TypeError(\"'urls' argument must be an iterable\")\n         return (\n             self.follow(\n\n@@ -30,13 +30,13 @@ _NONE = object()\n \n class TextResponse(Response):\n \n-    _DEFAULT_ENCODING = 'ascii'\n+    _DEFAULT_ENCODING = \"ascii\"\n     _cached_decoded_json = _NONE\n \n     attributes: Tuple[str, ...] = Response.attributes + (\"encoding\",)\n \n     def __init__(self, *args, **kwargs):\n-        self._encoding = kwargs.pop('encoding', None)\n+        self._encoding = kwargs.pop(\"encoding\", None)\n         self._cached_benc = None\n         self._cached_ubody = None\n         self._cached_selector = None\n@@ -49,11 +49,13 @@ class TextResponse(Response):\n             super()._set_url(url)\n \n     def _set_body(self, body):\n-        self._body = b''  # used by encoding detection\n+        self._body = b\"\"  # used by encoding detection\n         if isinstance(body, str):\n             if self._encoding is None:\n-                raise TypeError('Cannot convert unicode body - '\n-                                f'{type(self).__name__} has no encoding')\n+                raise TypeError(\n+                    \"Cannot convert unicode body - \"\n+                    f\"{type(self).__name__} has no encoding\"\n+                )\n             self._body = body.encode(self._encoding)\n         else:\n             super()._set_body(body)\n@@ -87,7 +89,7 @@ class TextResponse(Response):\n         # _body_inferred_encoding is called\n         benc = self.encoding\n         if self._cached_ubody is None:\n-            charset = f'charset={benc}'\n+            charset = f\"charset={benc}\"\n             self._cached_ubody = html_to_unicode(charset, self.body)[1]\n         return self._cached_ubody\n \n@@ -98,21 +100,24 @@ class TextResponse(Response):\n \n     @memoizemethod_noargs\n     def _headers_encoding(self):\n-        content_type = self.headers.get(b'Content-Type', b'')\n+        content_type = self.headers.get(b\"Content-Type\", b\"\")\n         return http_content_type_encoding(to_unicode(content_type))\n \n     def _body_inferred_encoding(self):\n         if self._cached_benc is None:\n-            content_type = to_unicode(self.headers.get(b'Content-Type', b''))\n-            benc, ubody = html_to_unicode(content_type, self.body,\n+            content_type = to_unicode(self.headers.get(b\"Content-Type\", b\"\"))\n+            benc, ubody = html_to_unicode(\n+                content_type,\n+                self.body,\n                 auto_detect_fun=self._auto_detect_fun,\n-                                          default_encoding=self._DEFAULT_ENCODING)\n+                default_encoding=self._DEFAULT_ENCODING,\n+            )\n             self._cached_benc = benc\n             self._cached_ubody = ubody\n         return self._cached_benc\n \n     def _auto_detect_fun(self, text):\n-        for enc in (self._DEFAULT_ENCODING, 'utf-8', 'cp1252'):\n+        for enc in (self._DEFAULT_ENCODING, \"utf-8\", \"cp1252\"):\n             try:\n                 text.decode(enc)\n             except UnicodeError:\n@@ -130,6 +135,7 @@ class TextResponse(Response):\n     @property\n     def selector(self):\n         from scrapy.selector import Selector\n+\n         if self._cached_selector is None:\n             self._cached_selector = Selector(self)\n         return self._cached_selector\n@@ -140,9 +146,22 @@ class TextResponse(Response):\n     def css(self, query):\n         return self.selector.css(query)\n \n-    def follow(self, url, callback=None, method='GET', headers=None, body=None,\n-               cookies=None, meta=None, encoding=None, priority=0,\n-               dont_filter=False, errback=None, cb_kwargs=None, flags=None) -> Request:\n+    def follow(\n+        self,\n+        url,\n+        callback=None,\n+        method=\"GET\",\n+        headers=None,\n+        body=None,\n+        cookies=None,\n+        meta=None,\n+        encoding=None,\n+        priority=0,\n+        dont_filter=False,\n+        errback=None,\n+        cb_kwargs=None,\n+        flags=None,\n+    ) -> Request:\n         \"\"\"\n         Return a :class:`~.Request` instance to follow a link ``url``.\n         It accepts the same arguments as ``Request.__init__`` method,\n@@ -180,10 +199,24 @@ class TextResponse(Response):\n             flags=flags,\n         )\n \n-    def follow_all(self, urls=None, callback=None, method='GET', headers=None, body=None,\n-                   cookies=None, meta=None, encoding=None, priority=0,\n-                   dont_filter=False, errback=None, cb_kwargs=None, flags=None,\n-                   css=None, xpath=None) -> Generator[Request, None, None]:\n+    def follow_all(\n+        self,\n+        urls=None,\n+        callback=None,\n+        method=\"GET\",\n+        headers=None,\n+        body=None,\n+        cookies=None,\n+        meta=None,\n+        encoding=None,\n+        priority=0,\n+        dont_filter=False,\n+        errback=None,\n+        cb_kwargs=None,\n+        flags=None,\n+        css=None,\n+        xpath=None,\n+    ) -> Generator[Request, None, None]:\n         \"\"\"\n         A generator that produces :class:`~.Request` instances to follow all\n         links in ``urls``. It accepts the same arguments as the :class:`~.Request`'s\n@@ -251,12 +284,13 @@ def _url_from_selector(sel):\n     if isinstance(sel.root, str):\n         # e.g. ::attr(href) result\n         return strip_html5_whitespace(sel.root)\n-    if not hasattr(sel.root, 'tag'):\n+    if not hasattr(sel.root, \"tag\"):\n         raise _InvalidSelector(f\"Unsupported selector: {sel}\")\n-    if sel.root.tag not in ('a', 'link'):\n-        raise _InvalidSelector(\"Only <a> and <link> elements are supported; \"\n-                               f\"got <{sel.root.tag}>\")\n-    href = sel.root.get('href')\n+    if sel.root.tag not in (\"a\", \"link\"):\n+        raise _InvalidSelector(\n+            \"Only <a> and <link> elements are supported; \" f\"got <{sel.root.tag}>\"\n+        )\n+    href = sel.root.get(\"href\")\n     if href is None:\n         raise _InvalidSelector(f\"<{sel.root.tag}> element has no href attribute: {sel}\")\n     return strip_html5_whitespace(href)\n\n@@ -2,7 +2,6 @@ from zope.interface import Interface\n \n \n class ISpiderLoader(Interface):\n-\n     def from_settings(settings):\n         \"\"\"Return an instance of the class for the given settings\"\"\"\n \n\n@@ -24,11 +24,11 @@ class ItemMeta(ABCMeta):\n     \"\"\"\n \n     def __new__(mcs, class_name, bases, attrs):\n-        classcell = attrs.pop('__classcell__', None)\n-        new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n-        _class = super().__new__(mcs, 'x_' + class_name, new_bases, attrs)\n+        classcell = attrs.pop(\"__classcell__\", None)\n+        new_bases = tuple(base._class for base in bases if hasattr(base, \"_class\"))\n+        _class = super().__new__(mcs, \"x_\" + class_name, new_bases, attrs)\n \n-        fields = getattr(_class, 'fields', {})\n+        fields = getattr(_class, \"fields\", {})\n         new_attrs = {}\n         for n in dir(_class):\n             v = getattr(_class, n)\n@@ -37,10 +37,10 @@ class ItemMeta(ABCMeta):\n             elif n in attrs:\n                 new_attrs[n] = attrs[n]\n \n-        new_attrs['fields'] = fields\n-        new_attrs['_class'] = _class\n+        new_attrs[\"fields\"] = fields\n+        new_attrs[\"_class\"] = _class\n         if classcell is not None:\n-            new_attrs['__classcell__'] = classcell\n+            new_attrs[\"__classcell__\"] = classcell\n         return super().__new__(mcs, class_name, bases, new_attrs)\n \n \n@@ -93,7 +93,7 @@ class Item(MutableMapping, object_ref, metaclass=ItemMeta):\n         raise AttributeError(name)\n \n     def __setattr__(self, name, value):\n-        if not name.startswith('_'):\n+        if not name.startswith(\"_\"):\n             raise AttributeError(f\"Use item[{name!r}] = {value!r} to set field value\")\n         super().__setattr__(name, value)\n \n@@ -115,6 +115,5 @@ class Item(MutableMapping, object_ref, metaclass=ItemMeta):\n         return self.__class__(self)\n \n     def deepcopy(self):\n-        \"\"\"Return a :func:`~copy.deepcopy` of this item.\n-        \"\"\"\n+        \"\"\"Return a :func:`~copy.deepcopy` of this item.\"\"\"\n         return deepcopy(self)\n\n@@ -24,9 +24,9 @@ class Link:\n                     of the anchor tag.\n     \"\"\"\n \n-    __slots__ = ['url', 'text', 'fragment', 'nofollow']\n+    __slots__ = [\"url\", \"text\", \"fragment\", \"nofollow\"]\n \n-    def __init__(self, url, text='', fragment='', nofollow=False):\n+    def __init__(self, url, text=\"\", fragment=\"\", nofollow=False):\n         if not isinstance(url, str):\n             got = url.__class__.__name__\n             raise TypeError(f\"Link urls must be str objects, got {got}\")\n@@ -44,10 +44,12 @@ class Link:\n         )\n \n     def __hash__(self):\n-        return hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)\n+        return (\n+            hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)\n+        )\n \n     def __repr__(self):\n         return (\n-            f'Link(url={self.url!r}, text={self.text!r}, '\n-            f'fragment={self.fragment!r}, nofollow={self.nofollow!r})'\n+            f\"Link(url={self.url!r}, text={self.text!r}, \"\n+            f\"fragment={self.fragment!r}, nofollow={self.nofollow!r})\"\n         )\n\n@@ -10,25 +10,81 @@ import re\n # common file extensions that are not followed if they occur in links\n IGNORED_EXTENSIONS = [\n     # archives\n-    '7z', '7zip', 'bz2', 'rar', 'tar', 'tar.gz', 'xz', 'zip',\n-\n+    \"7z\",\n+    \"7zip\",\n+    \"bz2\",\n+    \"rar\",\n+    \"tar\",\n+    \"tar.gz\",\n+    \"xz\",\n+    \"zip\",\n     # images\n-    'mng', 'pct', 'bmp', 'gif', 'jpg', 'jpeg', 'png', 'pst', 'psp', 'tif',\n-    'tiff', 'ai', 'drw', 'dxf', 'eps', 'ps', 'svg', 'cdr', 'ico',\n-\n+    \"mng\",\n+    \"pct\",\n+    \"bmp\",\n+    \"gif\",\n+    \"jpg\",\n+    \"jpeg\",\n+    \"png\",\n+    \"pst\",\n+    \"psp\",\n+    \"tif\",\n+    \"tiff\",\n+    \"ai\",\n+    \"drw\",\n+    \"dxf\",\n+    \"eps\",\n+    \"ps\",\n+    \"svg\",\n+    \"cdr\",\n+    \"ico\",\n     # audio\n-    'mp3', 'wma', 'ogg', 'wav', 'ra', 'aac', 'mid', 'au', 'aiff',\n-\n+    \"mp3\",\n+    \"wma\",\n+    \"ogg\",\n+    \"wav\",\n+    \"ra\",\n+    \"aac\",\n+    \"mid\",\n+    \"au\",\n+    \"aiff\",\n     # video\n-    '3gp', 'asf', 'asx', 'avi', 'mov', 'mp4', 'mpg', 'qt', 'rm', 'swf', 'wmv',\n-    'm4a', 'm4v', 'flv', 'webm',\n-\n+    \"3gp\",\n+    \"asf\",\n+    \"asx\",\n+    \"avi\",\n+    \"mov\",\n+    \"mp4\",\n+    \"mpg\",\n+    \"qt\",\n+    \"rm\",\n+    \"swf\",\n+    \"wmv\",\n+    \"m4a\",\n+    \"m4v\",\n+    \"flv\",\n+    \"webm\",\n     # office suites\n-    'xls', 'xlsx', 'ppt', 'pptx', 'pps', 'doc', 'docx', 'odt', 'ods', 'odg',\n-    'odp',\n-\n+    \"xls\",\n+    \"xlsx\",\n+    \"ppt\",\n+    \"pptx\",\n+    \"pps\",\n+    \"doc\",\n+    \"docx\",\n+    \"odt\",\n+    \"ods\",\n+    \"odg\",\n+    \"odp\",\n     # other\n-    'css', 'pdf', 'exe', 'bin', 'rss', 'dmg', 'iso', 'apk'\n+    \"css\",\n+    \"pdf\",\n+    \"exe\",\n+    \"bin\",\n+    \"rss\",\n+    \"dmg\",\n+    \"iso\",\n+    \"apk\",\n ]\n \n \n@@ -40,7 +96,7 @@ def _matches(url, regexs):\n \n \n def _is_valid_url(url):\n-    return url.split('://', 1)[0] in {'http', 'https', 'file', 'ftp'}\n+    return url.split(\"://\", 1)[0] in {\"http\", \"https\", \"file\", \"ftp\"}\n \n \n # Top-level imports\n\n@@ -11,8 +11,13 @@ from w3lib.html import strip_html5_whitespace\n from w3lib.url import canonicalize_url, safe_url_string\n \n from scrapy.link import Link\n-from scrapy.linkextractors import (IGNORED_EXTENSIONS, _is_valid_url, _matches,\n-                                   _re_type, re)\n+from scrapy.linkextractors import (\n+    IGNORED_EXTENSIONS,\n+    _is_valid_url,\n+    _matches,\n+    _re_type,\n+    re,\n+)\n from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n from scrapy.utils.python import unique as unique_list\n from scrapy.utils.response import get_base_url\n@@ -26,8 +31,8 @@ _collect_string_content = etree.XPath(\"string()\")\n \n def _nons(tag):\n     if isinstance(tag, str):\n-        if tag[0] == '{' and tag[1:len(XHTML_NAMESPACE) + 1] == XHTML_NAMESPACE:\n-            return tag.split('}')[-1]\n+        if tag[0] == \"{\" and tag[1 : len(XHTML_NAMESPACE) + 1] == XHTML_NAMESPACE:\n+            return tag.split(\"}\")[-1]\n     return tag\n \n \n@@ -41,14 +46,22 @@ def _canonicalize_link_url(link):\n \n class LxmlParserLinkExtractor:\n     def __init__(\n-        self, tag=\"a\", attr=\"href\", process=None, unique=False, strip=True, canonicalized=False\n+        self,\n+        tag=\"a\",\n+        attr=\"href\",\n+        process=None,\n+        unique=False,\n+        strip=True,\n+        canonicalized=False,\n     ):\n         self.scan_tag = tag if callable(tag) else partial(operator.eq, tag)\n         self.scan_attr = attr if callable(attr) else partial(operator.eq, attr)\n         self.process_attr = process if callable(process) else _identity\n         self.unique = unique\n         self.strip = strip\n-        self.link_key = operator.attrgetter(\"url\") if canonicalized else _canonicalize_link_url\n+        self.link_key = (\n+            operator.attrgetter(\"url\") if canonicalized else _canonicalize_link_url\n+        )\n \n     def _iter_links(self, document):\n         for el in document.iter(etree.Element):\n@@ -78,14 +91,19 @@ class LxmlParserLinkExtractor:\n             url = safe_url_string(url, encoding=response_encoding)\n             # to fix relative links after process_value\n             url = urljoin(response_url, url)\n-            link = Link(url, _collect_string_content(el) or '',\n-                        nofollow=rel_has_nofollow(el.get('rel')))\n+            link = Link(\n+                url,\n+                _collect_string_content(el) or \"\",\n+                nofollow=rel_has_nofollow(el.get(\"rel\")),\n+            )\n             links.append(link)\n         return self._deduplicate_if_needed(links)\n \n     def extract_links(self, response):\n         base_url = get_base_url(response)\n-        return self._extract_links(response.selector, response.url, response.encoding, base_url)\n+        return self._extract_links(\n+            response.selector, response.url, response.encoding, base_url\n+        )\n \n     def _process_links(self, links):\n         \"\"\"Normalize and filter extracted links\n@@ -110,8 +128,8 @@ class LxmlLinkExtractor:\n         allow_domains=(),\n         deny_domains=(),\n         restrict_xpaths=(),\n-        tags=('a', 'area'),\n-        attrs=('href',),\n+        tags=(\"a\", \"area\"),\n+        attrs=(\"href\",),\n         canonicalize=False,\n         unique=True,\n         process_value=None,\n@@ -127,26 +145,31 @@ class LxmlLinkExtractor:\n             unique=unique,\n             process=process_value,\n             strip=strip,\n-            canonicalized=canonicalize\n+            canonicalized=canonicalize,\n         )\n-        self.allow_res = [x if isinstance(x, _re_type) else re.compile(x)\n-                          for x in arg_to_iter(allow)]\n-        self.deny_res = [x if isinstance(x, _re_type) else re.compile(x)\n-                         for x in arg_to_iter(deny)]\n+        self.allow_res = [\n+            x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(allow)\n+        ]\n+        self.deny_res = [\n+            x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(deny)\n+        ]\n \n         self.allow_domains = set(arg_to_iter(allow_domains))\n         self.deny_domains = set(arg_to_iter(deny_domains))\n \n         self.restrict_xpaths = tuple(arg_to_iter(restrict_xpaths))\n-        self.restrict_xpaths += tuple(map(self._csstranslator.css_to_xpath,\n-                                          arg_to_iter(restrict_css)))\n+        self.restrict_xpaths += tuple(\n+            map(self._csstranslator.css_to_xpath, arg_to_iter(restrict_css))\n+        )\n \n         if deny_extensions is None:\n             deny_extensions = IGNORED_EXTENSIONS\n         self.canonicalize = canonicalize\n-        self.deny_extensions = {'.' + e for e in arg_to_iter(deny_extensions)}\n-        self.restrict_text = [x if isinstance(x, _re_type) else re.compile(x)\n-                              for x in arg_to_iter(restrict_text)]\n+        self.deny_extensions = {\".\" + e for e in arg_to_iter(deny_extensions)}\n+        self.restrict_text = [\n+            x if isinstance(x, _re_type) else re.compile(x)\n+            for x in arg_to_iter(restrict_text)\n+        ]\n \n     def _link_allowed(self, link):\n         if not _is_valid_url(link.url):\n@@ -156,11 +179,15 @@ class LxmlLinkExtractor:\n         if self.deny_res and _matches(link.url, self.deny_res):\n             return False\n         parsed_url = urlparse(link.url)\n-        if self.allow_domains and not url_is_from_any_domain(parsed_url, self.allow_domains):\n+        if self.allow_domains and not url_is_from_any_domain(\n+            parsed_url, self.allow_domains\n+        ):\n             return False\n         if self.deny_domains and url_is_from_any_domain(parsed_url, self.deny_domains):\n             return False\n-        if self.deny_extensions and url_has_any_extension(parsed_url, self.deny_extensions):\n+        if self.deny_extensions and url_has_any_extension(\n+            parsed_url, self.deny_extensions\n+        ):\n             return False\n         if self.restrict_text and not _matches(link.text, self.restrict_text):\n             return False\n@@ -173,7 +200,11 @@ class LxmlLinkExtractor:\n         if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):\n             return False\n \n-        allowed = (regex.search(url) for regex in self.allow_res) if self.allow_res else [True]\n+        allowed = (\n+            (regex.search(url) for regex in self.allow_res)\n+            if self.allow_res\n+            else [True]\n+        )\n         denied = (regex.search(url) for regex in self.deny_res) if self.deny_res else []\n         return any(allowed) and not any(denied)\n \n@@ -200,9 +231,7 @@ class LxmlLinkExtractor:\n         base_url = get_base_url(response)\n         if self.restrict_xpaths:\n             docs = [\n-                subdoc\n-                for x in self.restrict_xpaths\n-                for subdoc in response.xpath(x)\n+                subdoc for x in self.restrict_xpaths for subdoc in response.xpath(x)\n             ]\n         else:\n             docs = [response.selector]\n\n@@ -15,7 +15,7 @@ def wrap_loader_context(function, context):\n         \"scrapy.loader.common.wrap_loader_context has moved to a new library.\"\n         \"Please update your reference to itemloaders.common.wrap_loader_context\",\n         ScrapyDeprecationWarning,\n-        stacklevel=2\n+        stacklevel=2,\n     )\n \n     return common.wrap_loader_context(function, context)\n\n@@ -8,14 +8,14 @@ from itemloaders import processors\n from scrapy.utils.deprecate import create_deprecated_class\n \n \n-MapCompose = create_deprecated_class('MapCompose', processors.MapCompose)\n+MapCompose = create_deprecated_class(\"MapCompose\", processors.MapCompose)\n \n-Compose = create_deprecated_class('Compose', processors.Compose)\n+Compose = create_deprecated_class(\"Compose\", processors.Compose)\n \n-TakeFirst = create_deprecated_class('TakeFirst', processors.TakeFirst)\n+TakeFirst = create_deprecated_class(\"TakeFirst\", processors.TakeFirst)\n \n-Identity = create_deprecated_class('Identity', processors.Identity)\n+Identity = create_deprecated_class(\"Identity\", processors.Identity)\n \n-SelectJmes = create_deprecated_class('SelectJmes', processors.SelectJmes)\n+SelectJmes = create_deprecated_class(\"SelectJmes\", processors.SelectJmes)\n \n-Join = create_deprecated_class('Join', processors.Join)\n+Join = create_deprecated_class(\"Join\", processors.Join)\n\n@@ -54,20 +54,20 @@ class LogFormatter:\n \n     def crawled(self, request, response, spider):\n         \"\"\"Logs a message when the crawler finds a webpage.\"\"\"\n-        request_flags = f' {str(request.flags)}' if request.flags else ''\n-        response_flags = f' {str(response.flags)}' if response.flags else ''\n+        request_flags = f\" {str(request.flags)}\" if request.flags else \"\"\n+        response_flags = f\" {str(response.flags)}\" if response.flags else \"\"\n         return {\n-            'level': logging.DEBUG,\n-            'msg': CRAWLEDMSG,\n-            'args': {\n-                'status': response.status,\n-                'request': request,\n-                'request_flags': request_flags,\n-                'referer': referer_str(request),\n-                'response_flags': response_flags,\n+            \"level\": logging.DEBUG,\n+            \"msg\": CRAWLEDMSG,\n+            \"args\": {\n+                \"status\": response.status,\n+                \"request\": request,\n+                \"request_flags\": request_flags,\n+                \"referer\": referer_str(request),\n+                \"response_flags\": response_flags,\n                 # backward compatibility with Scrapy logformatter below 1.4 version\n-                'flags': response_flags\n-            }\n+                \"flags\": response_flags,\n+            },\n         }\n \n     def scraped(self, item, response, spider):\n@@ -77,23 +77,23 @@ class LogFormatter:\n         else:\n             src = response\n         return {\n-            'level': logging.DEBUG,\n-            'msg': SCRAPEDMSG,\n-            'args': {\n-                'src': src,\n-                'item': item,\n-            }\n+            \"level\": logging.DEBUG,\n+            \"msg\": SCRAPEDMSG,\n+            \"args\": {\n+                \"src\": src,\n+                \"item\": item,\n+            },\n         }\n \n     def dropped(self, item, exception, response, spider):\n         \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n         return {\n-            'level': logging.WARNING,\n-            'msg': DROPPEDMSG,\n-            'args': {\n-                'exception': exception,\n-                'item': item,\n-            }\n+            \"level\": logging.WARNING,\n+            \"msg\": DROPPEDMSG,\n+            \"args\": {\n+                \"exception\": exception,\n+                \"item\": item,\n+            },\n         }\n \n     def item_error(self, item, exception, response, spider):\n@@ -103,11 +103,11 @@ class LogFormatter:\n         .. versionadded:: 2.0\n         \"\"\"\n         return {\n-            'level': logging.ERROR,\n-            'msg': ITEMERRORMSG,\n-            'args': {\n-                'item': item,\n-            }\n+            \"level\": logging.ERROR,\n+            \"msg\": ITEMERRORMSG,\n+            \"args\": {\n+                \"item\": item,\n+            },\n         }\n \n     def spider_error(self, failure, request, response, spider):\n@@ -116,12 +116,12 @@ class LogFormatter:\n         .. versionadded:: 2.0\n         \"\"\"\n         return {\n-            'level': logging.ERROR,\n-            'msg': SPIDERERRORMSG,\n-            'args': {\n-                'request': request,\n-                'referer': referer_str(request),\n-            }\n+            \"level\": logging.ERROR,\n+            \"msg\": SPIDERERRORMSG,\n+            \"args\": {\n+                \"request\": request,\n+                \"referer\": referer_str(request),\n+            },\n         }\n \n     def download_error(self, failure, request, spider, errmsg=None):\n@@ -130,16 +130,16 @@ class LogFormatter:\n \n         .. versionadded:: 2.0\n         \"\"\"\n-        args = {'request': request}\n+        args = {\"request\": request}\n         if errmsg:\n             msg = DOWNLOADERRORMSG_LONG\n-            args['errmsg'] = errmsg\n+            args[\"errmsg\"] = errmsg\n         else:\n             msg = DOWNLOADERRORMSG_SHORT\n         return {\n-            'level': logging.ERROR,\n-            'msg': msg,\n-            'args': args,\n+            \"level\": logging.ERROR,\n+            \"msg\": msg,\n+            \"args\": args,\n         }\n \n     @classmethod\n\n@@ -34,8 +34,15 @@ def _to_bytes_or_none(text):\n \n class MailSender:\n     def __init__(\n-        self, smtphost='localhost', mailfrom='scrapy@localhost', smtpuser=None,\n-        smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False\n+        self,\n+        smtphost=\"localhost\",\n+        mailfrom=\"scrapy@localhost\",\n+        smtpuser=None,\n+        smtppass=None,\n+        smtpport=25,\n+        smtptls=False,\n+        smtpssl=False,\n+        debug=False,\n     ):\n         self.smtphost = smtphost\n         self.smtpport = smtpport\n@@ -49,44 +56,57 @@ class MailSender:\n     @classmethod\n     def from_settings(cls, settings):\n         return cls(\n-            smtphost=settings['MAIL_HOST'],\n-            mailfrom=settings['MAIL_FROM'],\n-            smtpuser=settings['MAIL_USER'],\n-            smtppass=settings['MAIL_PASS'],\n-            smtpport=settings.getint('MAIL_PORT'),\n-            smtptls=settings.getbool('MAIL_TLS'),\n-            smtpssl=settings.getbool('MAIL_SSL'),\n+            smtphost=settings[\"MAIL_HOST\"],\n+            mailfrom=settings[\"MAIL_FROM\"],\n+            smtpuser=settings[\"MAIL_USER\"],\n+            smtppass=settings[\"MAIL_PASS\"],\n+            smtpport=settings.getint(\"MAIL_PORT\"),\n+            smtptls=settings.getbool(\"MAIL_TLS\"),\n+            smtpssl=settings.getbool(\"MAIL_SSL\"),\n         )\n \n-    def send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None, _callback=None):\n+    def send(\n+        self,\n+        to,\n+        subject,\n+        body,\n+        cc=None,\n+        attachs=(),\n+        mimetype=\"text/plain\",\n+        charset=None,\n+        _callback=None,\n+    ):\n         from twisted.internet import reactor\n+\n         if attachs:\n             msg = MIMEMultipart()\n         else:\n-            msg = MIMENonMultipart(*mimetype.split('/', 1))\n+            msg = MIMENonMultipart(*mimetype.split(\"/\", 1))\n \n         to = list(arg_to_iter(to))\n         cc = list(arg_to_iter(cc))\n \n-        msg['From'] = self.mailfrom\n-        msg['To'] = COMMASPACE.join(to)\n-        msg['Date'] = formatdate(localtime=True)\n-        msg['Subject'] = subject\n+        msg[\"From\"] = self.mailfrom\n+        msg[\"To\"] = COMMASPACE.join(to)\n+        msg[\"Date\"] = formatdate(localtime=True)\n+        msg[\"Subject\"] = subject\n         rcpts = to[:]\n         if cc:\n             rcpts.extend(cc)\n-            msg['Cc'] = COMMASPACE.join(cc)\n+            msg[\"Cc\"] = COMMASPACE.join(cc)\n \n         if charset:\n             msg.set_charset(charset)\n \n         if attachs:\n-            msg.attach(MIMEText(body, 'plain', charset or 'us-ascii'))\n+            msg.attach(MIMEText(body, \"plain\", charset or \"us-ascii\"))\n             for attach_name, mimetype, f in attachs:\n-                part = MIMEBase(*mimetype.split('/'))\n+                part = MIMEBase(*mimetype.split(\"/\"))\n                 part.set_payload(f.read())\n                 Encoders.encode_base64(part)\n-                part.add_header('Content-Disposition', 'attachment', filename=attach_name)\n+                part.add_header(\n+                    \"Content-Disposition\", \"attachment\", filename=attach_name\n+                )\n                 msg.attach(part)\n         else:\n             msg.set_payload(body)\n@@ -95,50 +115,79 @@ class MailSender:\n             _callback(to=to, subject=subject, body=body, cc=cc, attach=attachs, msg=msg)\n \n         if self.debug:\n-            logger.debug('Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s '\n+            logger.debug(\n+                \"Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s \"\n                 'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n-                         {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n-                          'mailattachs': len(attachs)})\n+                {\n+                    \"mailto\": to,\n+                    \"mailcc\": cc,\n+                    \"mailsubject\": subject,\n+                    \"mailattachs\": len(attachs),\n+                },\n+            )\n             return\n \n-        dfd = self._sendmail(rcpts, msg.as_string().encode(charset or 'utf-8'))\n+        dfd = self._sendmail(rcpts, msg.as_string().encode(charset or \"utf-8\"))\n         dfd.addCallbacks(\n             callback=self._sent_ok,\n             errback=self._sent_failed,\n             callbackArgs=[to, cc, subject, len(attachs)],\n             errbackArgs=[to, cc, subject, len(attachs)],\n         )\n-        reactor.addSystemEventTrigger('before', 'shutdown', lambda: dfd)\n+        reactor.addSystemEventTrigger(\"before\", \"shutdown\", lambda: dfd)\n         return dfd\n \n     def _sent_ok(self, result, to, cc, subject, nattachs):\n-        logger.info('Mail sent OK: To=%(mailto)s Cc=%(mailcc)s '\n+        logger.info(\n+            \"Mail sent OK: To=%(mailto)s Cc=%(mailcc)s \"\n             'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n-                    {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n-                     'mailattachs': nattachs})\n+            {\n+                \"mailto\": to,\n+                \"mailcc\": cc,\n+                \"mailsubject\": subject,\n+                \"mailattachs\": nattachs,\n+            },\n+        )\n \n     def _sent_failed(self, failure, to, cc, subject, nattachs):\n         errstr = str(failure.value)\n-        logger.error('Unable to send mail: To=%(mailto)s Cc=%(mailcc)s '\n+        logger.error(\n+            \"Unable to send mail: To=%(mailto)s Cc=%(mailcc)s \"\n             'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d'\n-                     '- %(mailerr)s',\n-                     {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n-                      'mailattachs': nattachs, 'mailerr': errstr})\n+            \"- %(mailerr)s\",\n+            {\n+                \"mailto\": to,\n+                \"mailcc\": cc,\n+                \"mailsubject\": subject,\n+                \"mailattachs\": nattachs,\n+                \"mailerr\": errstr,\n+            },\n+        )\n \n     def _sendmail(self, to_addrs, msg):\n         # Import twisted.mail here because it is not available in python3\n         from twisted.internet import reactor\n         from twisted.mail.smtp import ESMTPSenderFactory\n+\n         msg = BytesIO(msg)\n         d = defer.Deferred()\n         factory = ESMTPSenderFactory(\n-            self.smtpuser, self.smtppass, self.mailfrom, to_addrs, msg, d,\n-            heloFallback=True, requireAuthentication=False, requireTransportSecurity=self.smtptls,\n+            self.smtpuser,\n+            self.smtppass,\n+            self.mailfrom,\n+            to_addrs,\n+            msg,\n+            d,\n+            heloFallback=True,\n+            requireAuthentication=False,\n+            requireTransportSecurity=self.smtptls,\n         )\n         factory.noisy = False\n \n         if self.smtpssl:\n-            reactor.connectSSL(self.smtphost, self.smtpport, factory, ssl.ClientContextFactory())\n+            reactor.connectSSL(\n+                self.smtphost, self.smtpport, factory, ssl.ClientContextFactory()\n+            )\n         else:\n             reactor.connectTCP(self.smtphost, self.smtpport, factory)\n \n\n@@ -17,13 +17,15 @@ logger = logging.getLogger(__name__)\n class MiddlewareManager:\n     \"\"\"Base class for implementing middleware managers\"\"\"\n \n-    component_name = 'foo middleware'\n+    component_name = \"foo middleware\"\n \n     def __init__(self, *middlewares: Any) -> None:\n         self.middlewares = middlewares\n         # Only process_spider_output and process_spider_exception can be None.\n         # Only process_spider_output can be a tuple, and only until _async compatibility methods are removed.\n-        self.methods: Dict[str, Deque[Union[None, Callable, Tuple[Callable, Callable]]]] = defaultdict(deque)\n+        self.methods: Dict[\n+            str, Deque[Union[None, Callable, Tuple[Callable, Callable]]]\n+        ] = defaultdict(deque)\n         for mw in middlewares:\n             self._add_middleware(mw)\n \n@@ -44,15 +46,21 @@ class MiddlewareManager:\n                 enabled.append(clspath)\n             except NotConfigured as e:\n                 if e.args:\n-                    clsname = clspath.split('.')[-1]\n-                    logger.warning(\"Disabled %(clsname)s: %(eargs)s\",\n-                                   {'clsname': clsname, 'eargs': e.args[0]},\n-                                   extra={'crawler': crawler})\n+                    clsname = clspath.split(\".\")[-1]\n+                    logger.warning(\n+                        \"Disabled %(clsname)s: %(eargs)s\",\n+                        {\"clsname\": clsname, \"eargs\": e.args[0]},\n+                        extra={\"crawler\": crawler},\n+                    )\n \n-        logger.info(\"Enabled %(componentname)ss:\\n%(enabledlist)s\",\n-                    {'componentname': cls.component_name,\n-                     'enabledlist': pprint.pformat(enabled)},\n-                    extra={'crawler': crawler})\n+        logger.info(\n+            \"Enabled %(componentname)ss:\\n%(enabledlist)s\",\n+            {\n+                \"componentname\": cls.component_name,\n+                \"enabledlist\": pprint.pformat(enabled),\n+            },\n+            extra={\"crawler\": crawler},\n+        )\n         return cls(*middlewares)\n \n     @classmethod\n@@ -60,10 +68,10 @@ class MiddlewareManager:\n         return cls.from_settings(crawler.settings, crawler)\n \n     def _add_middleware(self, mw) -> None:\n-        if hasattr(mw, 'open_spider'):\n-            self.methods['open_spider'].append(mw.open_spider)\n-        if hasattr(mw, 'close_spider'):\n-            self.methods['close_spider'].appendleft(mw.close_spider)\n+        if hasattr(mw, \"open_spider\"):\n+            self.methods[\"open_spider\"].append(mw.open_spider)\n+        if hasattr(mw, \"close_spider\"):\n+            self.methods[\"close_spider\"].appendleft(mw.close_spider)\n \n     def _process_parallel(self, methodname: str, obj, *args) -> Deferred:\n         methods = cast(Iterable[Callable], self.methods[methodname])\n@@ -74,7 +82,7 @@ class MiddlewareManager:\n         return process_chain(methods, obj, *args)\n \n     def open_spider(self, spider: Spider) -> Deferred:\n-        return self._process_parallel('open_spider', spider)\n+        return self._process_parallel(\"open_spider\", spider)\n \n     def close_spider(self, spider: Spider) -> Deferred:\n-        return self._process_parallel('close_spider', spider)\n+        return self._process_parallel(\"close_spider\", spider)\n\n@@ -11,16 +11,18 @@ from scrapy.utils.defer import deferred_f_from_coro_f\n \n class ItemPipelineManager(MiddlewareManager):\n \n-    component_name = 'item pipeline'\n+    component_name = \"item pipeline\"\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return build_component_list(settings.getwithbase('ITEM_PIPELINES'))\n+        return build_component_list(settings.getwithbase(\"ITEM_PIPELINES\"))\n \n     def _add_middleware(self, pipe):\n         super()._add_middleware(pipe)\n-        if hasattr(pipe, 'process_item'):\n-            self.methods['process_item'].append(deferred_f_from_coro_f(pipe.process_item))\n+        if hasattr(pipe, \"process_item\"):\n+            self.methods[\"process_item\"].append(\n+                deferred_f_from_coro_f(pipe.process_item)\n+            )\n \n     def process_item(self, item, spider):\n-        return self._process_chain('process_item', item, spider)\n+        return self._process_chain(\"process_item\", item, spider)\n\n@@ -42,8 +42,8 @@ class FileException(Exception):\n \n class FSFilesStore:\n     def __init__(self, basedir: str):\n-        if '://' in basedir:\n-            basedir = basedir.split('://', 1)[1]\n+        if \"://\" in basedir:\n+            basedir = basedir.split(\"://\", 1)[1]\n         self.basedir = basedir\n         self._mkdir(Path(self.basedir))\n         self.created_directories: DefaultDict[str, Set[str]] = defaultdict(set)\n@@ -60,13 +60,13 @@ class FSFilesStore:\n         except os.error:\n             return {}\n \n-        with absolute_path.open('rb') as f:\n+        with absolute_path.open(\"rb\") as f:\n             checksum = md5sum(f)\n \n-        return {'last_modified': last_modified, 'checksum': checksum}\n+        return {\"last_modified\": last_modified, \"checksum\": checksum}\n \n     def _get_filesystem_path(self, path: str) -> Path:\n-        path_comps = path.split('/')\n+        path_comps = path.split(\"/\")\n         return Path(self.basedir, *path_comps)\n \n     def _mkdir(self, dirname: Path, domain: Optional[str] = None):\n@@ -86,49 +86,49 @@ class S3FilesStore:\n     AWS_USE_SSL = None\n     AWS_VERIFY = None\n \n-    POLICY = 'private'  # Overridden from settings.FILES_STORE_S3_ACL in FilesPipeline.from_settings\n+    POLICY = \"private\"  # Overridden from settings.FILES_STORE_S3_ACL in FilesPipeline.from_settings\n     HEADERS = {\n-        'Cache-Control': 'max-age=172800',\n+        \"Cache-Control\": \"max-age=172800\",\n     }\n \n     def __init__(self, uri):\n         if not is_botocore_available():\n-            raise NotConfigured('missing botocore library')\n+            raise NotConfigured(\"missing botocore library\")\n         import botocore.session\n+\n         session = botocore.session.get_session()\n         self.s3_client = session.create_client(\n-            's3',\n+            \"s3\",\n             aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n             aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n             aws_session_token=self.AWS_SESSION_TOKEN,\n             endpoint_url=self.AWS_ENDPOINT_URL,\n             region_name=self.AWS_REGION_NAME,\n             use_ssl=self.AWS_USE_SSL,\n-            verify=self.AWS_VERIFY\n+            verify=self.AWS_VERIFY,\n         )\n         if not uri.startswith(\"s3://\"):\n             raise ValueError(f\"Incorrect URI scheme in {uri}, expected 's3'\")\n-        self.bucket, self.prefix = uri[5:].split('/', 1)\n+        self.bucket, self.prefix = uri[5:].split(\"/\", 1)\n \n     def stat_file(self, path, info):\n         def _onsuccess(boto_key):\n-            checksum = boto_key['ETag'].strip('\"')\n-            last_modified = boto_key['LastModified']\n+            checksum = boto_key[\"ETag\"].strip('\"')\n+            last_modified = boto_key[\"LastModified\"]\n             modified_stamp = time.mktime(last_modified.timetuple())\n-            return {'checksum': checksum, 'last_modified': modified_stamp}\n+            return {\"checksum\": checksum, \"last_modified\": modified_stamp}\n \n         return self._get_boto_key(path).addCallback(_onsuccess)\n \n     def _get_boto_key(self, path):\n-        key_name = f'{self.prefix}{path}'\n+        key_name = f\"{self.prefix}{path}\"\n         return threads.deferToThread(\n-            self.s3_client.head_object,\n-            Bucket=self.bucket,\n-            Key=key_name)\n+            self.s3_client.head_object, Bucket=self.bucket, Key=key_name\n+        )\n \n     def persist_file(self, path, buf, info, meta=None, headers=None):\n         \"\"\"Upload file to S3 storage\"\"\"\n-        key_name = f'{self.prefix}{path}'\n+        key_name = f\"{self.prefix}{path}\"\n         buf.seek(0)\n         extra = self._headers_to_botocore_kwargs(self.HEADERS)\n         if headers:\n@@ -140,39 +140,41 @@ class S3FilesStore:\n             Body=buf,\n             Metadata={k: str(v) for k, v in (meta or {}).items()},\n             ACL=self.POLICY,\n-            **extra)\n+            **extra,\n+        )\n \n     def _headers_to_botocore_kwargs(self, headers):\n-        \"\"\" Convert headers to botocore keyword arguments.\n-        \"\"\"\n+        \"\"\"Convert headers to botocore keyword arguments.\"\"\"\n         # This is required while we need to support both boto and botocore.\n-        mapping = CaselessDict({\n-            'Content-Type': 'ContentType',\n-            'Cache-Control': 'CacheControl',\n-            'Content-Disposition': 'ContentDisposition',\n-            'Content-Encoding': 'ContentEncoding',\n-            'Content-Language': 'ContentLanguage',\n-            'Content-Length': 'ContentLength',\n-            'Content-MD5': 'ContentMD5',\n-            'Expires': 'Expires',\n-            'X-Amz-Grant-Full-Control': 'GrantFullControl',\n-            'X-Amz-Grant-Read': 'GrantRead',\n-            'X-Amz-Grant-Read-ACP': 'GrantReadACP',\n-            'X-Amz-Grant-Write-ACP': 'GrantWriteACP',\n-            'X-Amz-Object-Lock-Legal-Hold': 'ObjectLockLegalHoldStatus',\n-            'X-Amz-Object-Lock-Mode': 'ObjectLockMode',\n-            'X-Amz-Object-Lock-Retain-Until-Date': 'ObjectLockRetainUntilDate',\n-            'X-Amz-Request-Payer': 'RequestPayer',\n-            'X-Amz-Server-Side-Encryption': 'ServerSideEncryption',\n-            'X-Amz-Server-Side-Encryption-Aws-Kms-Key-Id': 'SSEKMSKeyId',\n-            'X-Amz-Server-Side-Encryption-Context': 'SSEKMSEncryptionContext',\n-            'X-Amz-Server-Side-Encryption-Customer-Algorithm': 'SSECustomerAlgorithm',\n-            'X-Amz-Server-Side-Encryption-Customer-Key': 'SSECustomerKey',\n-            'X-Amz-Server-Side-Encryption-Customer-Key-Md5': 'SSECustomerKeyMD5',\n-            'X-Amz-Storage-Class': 'StorageClass',\n-            'X-Amz-Tagging': 'Tagging',\n-            'X-Amz-Website-Redirect-Location': 'WebsiteRedirectLocation',\n-        })\n+        mapping = CaselessDict(\n+            {\n+                \"Content-Type\": \"ContentType\",\n+                \"Cache-Control\": \"CacheControl\",\n+                \"Content-Disposition\": \"ContentDisposition\",\n+                \"Content-Encoding\": \"ContentEncoding\",\n+                \"Content-Language\": \"ContentLanguage\",\n+                \"Content-Length\": \"ContentLength\",\n+                \"Content-MD5\": \"ContentMD5\",\n+                \"Expires\": \"Expires\",\n+                \"X-Amz-Grant-Full-Control\": \"GrantFullControl\",\n+                \"X-Amz-Grant-Read\": \"GrantRead\",\n+                \"X-Amz-Grant-Read-ACP\": \"GrantReadACP\",\n+                \"X-Amz-Grant-Write-ACP\": \"GrantWriteACP\",\n+                \"X-Amz-Object-Lock-Legal-Hold\": \"ObjectLockLegalHoldStatus\",\n+                \"X-Amz-Object-Lock-Mode\": \"ObjectLockMode\",\n+                \"X-Amz-Object-Lock-Retain-Until-Date\": \"ObjectLockRetainUntilDate\",\n+                \"X-Amz-Request-Payer\": \"RequestPayer\",\n+                \"X-Amz-Server-Side-Encryption\": \"ServerSideEncryption\",\n+                \"X-Amz-Server-Side-Encryption-Aws-Kms-Key-Id\": \"SSEKMSKeyId\",\n+                \"X-Amz-Server-Side-Encryption-Context\": \"SSEKMSEncryptionContext\",\n+                \"X-Amz-Server-Side-Encryption-Customer-Algorithm\": \"SSECustomerAlgorithm\",\n+                \"X-Amz-Server-Side-Encryption-Customer-Key\": \"SSECustomerKey\",\n+                \"X-Amz-Server-Side-Encryption-Customer-Key-Md5\": \"SSECustomerKeyMD5\",\n+                \"X-Amz-Storage-Class\": \"StorageClass\",\n+                \"X-Amz-Tagging\": \"Tagging\",\n+                \"X-Amz-Website-Redirect-Location\": \"WebsiteRedirectLocation\",\n+            }\n+        )\n         extra = {}\n         for key, value in headers.items():\n             try:\n@@ -188,7 +190,7 @@ class GCSFilesStore:\n \n     GCS_PROJECT_ID = None\n \n-    CACHE_CONTROL = 'max-age=172800'\n+    CACHE_CONTROL = \"max-age=172800\"\n \n     # The bucket's default object ACL will be applied to the object.\n     # Overridden from settings.FILES_STORE_GCS_ACL in FilesPipeline.from_settings.\n@@ -196,23 +198,24 @@ class GCSFilesStore:\n \n     def __init__(self, uri):\n         from google.cloud import storage\n+\n         client = storage.Client(project=self.GCS_PROJECT_ID)\n-        bucket, prefix = uri[5:].split('/', 1)\n+        bucket, prefix = uri[5:].split(\"/\", 1)\n         self.bucket = client.bucket(bucket)\n         self.prefix = prefix\n         permissions = self.bucket.test_iam_permissions(\n-            ['storage.objects.get', 'storage.objects.create']\n+            [\"storage.objects.get\", \"storage.objects.create\"]\n         )\n-        if 'storage.objects.get' not in permissions:\n+        if \"storage.objects.get\" not in permissions:\n             logger.warning(\n                 \"No 'storage.objects.get' permission for GSC bucket %(bucket)s. \"\n                 \"Checking if files are up to date will be impossible. Files will be downloaded every time.\",\n-                {'bucket': bucket}\n+                {\"bucket\": bucket},\n             )\n-        if 'storage.objects.create' not in permissions:\n+        if \"storage.objects.create\" not in permissions:\n             logger.error(\n                 \"No 'storage.objects.create' permission for GSC bucket %(bucket)s. Saving files will be impossible!\",\n-                {'bucket': bucket}\n+                {\"bucket\": bucket},\n             )\n \n     def stat_file(self, path, info):\n@@ -220,15 +223,18 @@ class GCSFilesStore:\n             if blob:\n                 checksum = blob.md5_hash\n                 last_modified = time.mktime(blob.updated.timetuple())\n-                return {'checksum': checksum, 'last_modified': last_modified}\n+                return {\"checksum\": checksum, \"last_modified\": last_modified}\n             return {}\n+\n         blob_path = self._get_blob_path(path)\n-        return threads.deferToThread(self.bucket.get_blob, blob_path).addCallback(_onsuccess)\n+        return threads.deferToThread(self.bucket.get_blob, blob_path).addCallback(\n+            _onsuccess\n+        )\n \n     def _get_content_type(self, headers):\n-        if headers and 'Content-Type' in headers:\n-            return headers['Content-Type']\n-        return 'application/octet-stream'\n+        if headers and \"Content-Type\" in headers:\n+            return headers[\"Content-Type\"]\n+        return \"application/octet-stream\"\n \n     def _get_blob_path(self, path):\n         return self.prefix + path\n@@ -242,7 +248,7 @@ class GCSFilesStore:\n             blob.upload_from_string,\n             data=buf.getvalue(),\n             content_type=self._get_content_type(headers),\n-            predefined_acl=self.POLICY\n+            predefined_acl=self.POLICY,\n         )\n \n \n@@ -261,14 +267,19 @@ class FTPFilesStore:\n         self.port = int(u.port or 21)\n         self.username = u.username or self.FTP_USERNAME\n         self.password = u.password or self.FTP_PASSWORD\n-        self.basedir = u.path.rstrip('/')\n+        self.basedir = u.path.rstrip(\"/\")\n \n     def persist_file(self, path, buf, info, meta=None, headers=None):\n-        path = f'{self.basedir}/{path}'\n+        path = f\"{self.basedir}/{path}\"\n         return threads.deferToThread(\n-            ftp_store_file, path=path, file=buf,\n-            host=self.host, port=self.port, username=self.username,\n-            password=self.password, use_active_mode=self.USE_ACTIVE_MODE\n+            ftp_store_file,\n+            path=path,\n+            file=buf,\n+            host=self.host,\n+            port=self.port,\n+            username=self.username,\n+            password=self.password,\n+            use_active_mode=self.USE_ACTIVE_MODE,\n         )\n \n     def stat_file(self, path, info):\n@@ -282,11 +293,12 @@ class FTPFilesStore:\n                 file_path = f\"{self.basedir}/{path}\"\n                 last_modified = float(ftp.voidcmd(f\"MDTM {file_path}\")[4:].strip())\n                 m = hashlib.md5()\n-                ftp.retrbinary(f'RETR {file_path}', m.update)\n-                return {'last_modified': last_modified, 'checksum': m.hexdigest()}\n+                ftp.retrbinary(f\"RETR {file_path}\", m.update)\n+                return {\"last_modified\": last_modified, \"checksum\": m.hexdigest()}\n             # The file doesn't exist\n             except Exception:\n                 return {}\n+\n         return threads.deferToThread(_stat_file, path)\n \n \n@@ -312,14 +324,14 @@ class FilesPipeline(MediaPipeline):\n     MEDIA_NAME = \"file\"\n     EXPIRES = 90\n     STORE_SCHEMES = {\n-        '': FSFilesStore,\n-        'file': FSFilesStore,\n-        's3': S3FilesStore,\n-        'gs': GCSFilesStore,\n-        'ftp': FTPFilesStore\n+        \"\": FSFilesStore,\n+        \"file\": FSFilesStore,\n+        \"s3\": S3FilesStore,\n+        \"gs\": GCSFilesStore,\n+        \"ftp\": FTPFilesStore,\n     }\n-    DEFAULT_FILES_URLS_FIELD = 'file_urls'\n-    DEFAULT_FILES_RESULT_FIELD = 'files'\n+    DEFAULT_FILES_URLS_FIELD = \"file_urls\"\n+    DEFAULT_FILES_RESULT_FIELD = \"files\"\n \n     def __init__(self, store_uri, download_func=None, settings=None):\n         if not store_uri:\n@@ -330,52 +342,50 @@ class FilesPipeline(MediaPipeline):\n \n         cls_name = \"FilesPipeline\"\n         self.store = self._get_store(store_uri)\n-        resolve = functools.partial(self._key_for_pipe,\n-                                    base_class_name=cls_name,\n-                                    settings=settings)\n-        self.expires = settings.getint(\n-            resolve('FILES_EXPIRES'), self.EXPIRES\n+        resolve = functools.partial(\n+            self._key_for_pipe, base_class_name=cls_name, settings=settings\n         )\n+        self.expires = settings.getint(resolve(\"FILES_EXPIRES\"), self.EXPIRES)\n         if not hasattr(self, \"FILES_URLS_FIELD\"):\n             self.FILES_URLS_FIELD = self.DEFAULT_FILES_URLS_FIELD\n         if not hasattr(self, \"FILES_RESULT_FIELD\"):\n             self.FILES_RESULT_FIELD = self.DEFAULT_FILES_RESULT_FIELD\n         self.files_urls_field = settings.get(\n-            resolve('FILES_URLS_FIELD'), self.FILES_URLS_FIELD\n+            resolve(\"FILES_URLS_FIELD\"), self.FILES_URLS_FIELD\n         )\n         self.files_result_field = settings.get(\n-            resolve('FILES_RESULT_FIELD'), self.FILES_RESULT_FIELD\n+            resolve(\"FILES_RESULT_FIELD\"), self.FILES_RESULT_FIELD\n         )\n \n         super().__init__(download_func=download_func, settings=settings)\n \n     @classmethod\n     def from_settings(cls, settings):\n-        s3store = cls.STORE_SCHEMES['s3']\n-        s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n-        s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n-        s3store.AWS_SESSION_TOKEN = settings['AWS_SESSION_TOKEN']\n-        s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']\n-        s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']\n-        s3store.AWS_USE_SSL = settings['AWS_USE_SSL']\n-        s3store.AWS_VERIFY = settings['AWS_VERIFY']\n-        s3store.POLICY = settings['FILES_STORE_S3_ACL']\n+        s3store = cls.STORE_SCHEMES[\"s3\"]\n+        s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n+        s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n+        s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n+        s3store.AWS_ENDPOINT_URL = settings[\"AWS_ENDPOINT_URL\"]\n+        s3store.AWS_REGION_NAME = settings[\"AWS_REGION_NAME\"]\n+        s3store.AWS_USE_SSL = settings[\"AWS_USE_SSL\"]\n+        s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n+        s3store.POLICY = settings[\"FILES_STORE_S3_ACL\"]\n \n-        gcs_store = cls.STORE_SCHEMES['gs']\n-        gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n-        gcs_store.POLICY = settings['FILES_STORE_GCS_ACL'] or None\n+        gcs_store = cls.STORE_SCHEMES[\"gs\"]\n+        gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n+        gcs_store.POLICY = settings[\"FILES_STORE_GCS_ACL\"] or None\n \n-        ftp_store = cls.STORE_SCHEMES['ftp']\n-        ftp_store.FTP_USERNAME = settings['FTP_USER']\n-        ftp_store.FTP_PASSWORD = settings['FTP_PASSWORD']\n-        ftp_store.USE_ACTIVE_MODE = settings.getbool('FEED_STORAGE_FTP_ACTIVE')\n+        ftp_store = cls.STORE_SCHEMES[\"ftp\"]\n+        ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n+        ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n+        ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n \n-        store_uri = settings['FILES_STORE']\n+        store_uri = settings[\"FILES_STORE\"]\n         return cls(store_uri, settings=settings)\n \n     def _get_store(self, uri: str):\n         if Path(uri).is_absolute():  # to support win32 paths like: C:\\\\some\\dir\n-            scheme = 'file'\n+            scheme = \"file\"\n         else:\n             scheme = urlparse(uri).scheme\n         store_cls = self.STORE_SCHEMES[scheme]\n@@ -386,7 +396,7 @@ class FilesPipeline(MediaPipeline):\n             if not result:\n                 return  # returning None force download\n \n-            last_modified = result.get('last_modified', None)\n+            last_modified = result.get(\"last_modified\", None)\n             if not last_modified:\n                 return  # returning None force download\n \n@@ -397,25 +407,30 @@ class FilesPipeline(MediaPipeline):\n \n             referer = referer_str(request)\n             logger.debug(\n-                'File (uptodate): Downloaded %(medianame)s from %(request)s '\n-                'referred in <%(referer)s>',\n-                {'medianame': self.MEDIA_NAME, 'request': request,\n-                 'referer': referer},\n-                extra={'spider': info.spider}\n+                \"File (uptodate): Downloaded %(medianame)s from %(request)s \"\n+                \"referred in <%(referer)s>\",\n+                {\"medianame\": self.MEDIA_NAME, \"request\": request, \"referer\": referer},\n+                extra={\"spider\": info.spider},\n             )\n-            self.inc_stats(info.spider, 'uptodate')\n+            self.inc_stats(info.spider, \"uptodate\")\n \n-            checksum = result.get('checksum', None)\n-            return {'url': request.url, 'path': path, 'checksum': checksum, 'status': 'uptodate'}\n+            checksum = result.get(\"checksum\", None)\n+            return {\n+                \"url\": request.url,\n+                \"path\": path,\n+                \"checksum\": checksum,\n+                \"status\": \"uptodate\",\n+            }\n \n         path = self.file_path(request, info=info, item=item)\n         dfd = defer.maybeDeferred(self.store.stat_file, path, info)\n         dfd.addCallbacks(_onsuccess, lambda _: None)\n         dfd.addErrback(\n-            lambda f:\n-            logger.error(self.__class__.__name__ + '.store.stat_file',\n+            lambda f: logger.error(\n+                self.__class__.__name__ + \".store.stat_file\",\n                 exc_info=failure_to_exc_info(f),\n-                         extra={'spider': info.spider})\n+                extra={\"spider\": info.spider},\n+            )\n         )\n         return dfd\n \n@@ -423,11 +438,15 @@ class FilesPipeline(MediaPipeline):\n         if not isinstance(failure.value, IgnoreRequest):\n             referer = referer_str(request)\n             logger.warning(\n-                'File (unknown-error): Error downloading %(medianame)s from '\n-                '%(request)s referred in <%(referer)s>: %(exception)s',\n-                {'medianame': self.MEDIA_NAME, 'request': request,\n-                 'referer': referer, 'exception': failure.value},\n-                extra={'spider': info.spider}\n+                \"File (unknown-error): Error downloading %(medianame)s from \"\n+                \"%(request)s referred in <%(referer)s>: %(exception)s\",\n+                {\n+                    \"medianame\": self.MEDIA_NAME,\n+                    \"request\": request,\n+                    \"referer\": referer,\n+                    \"exception\": failure.value,\n+                },\n+                extra={\"spider\": info.spider},\n             )\n \n         raise FileException\n@@ -437,29 +456,28 @@ class FilesPipeline(MediaPipeline):\n \n         if response.status != 200:\n             logger.warning(\n-                'File (code: %(status)s): Error downloading file from '\n-                '%(request)s referred in <%(referer)s>',\n-                {'status': response.status,\n-                 'request': request, 'referer': referer},\n-                extra={'spider': info.spider}\n+                \"File (code: %(status)s): Error downloading file from \"\n+                \"%(request)s referred in <%(referer)s>\",\n+                {\"status\": response.status, \"request\": request, \"referer\": referer},\n+                extra={\"spider\": info.spider},\n             )\n-            raise FileException('download-error')\n+            raise FileException(\"download-error\")\n \n         if not response.body:\n             logger.warning(\n-                'File (empty-content): Empty file from %(request)s referred '\n-                'in <%(referer)s>: no-content',\n-                {'request': request, 'referer': referer},\n-                extra={'spider': info.spider}\n+                \"File (empty-content): Empty file from %(request)s referred \"\n+                \"in <%(referer)s>: no-content\",\n+                {\"request\": request, \"referer\": referer},\n+                extra={\"spider\": info.spider},\n             )\n-            raise FileException('empty-content')\n+            raise FileException(\"empty-content\")\n \n-        status = 'cached' if 'cached' in response.flags else 'downloaded'\n+        status = \"cached\" if \"cached\" in response.flags else \"downloaded\"\n         logger.debug(\n-            'File (%(status)s): Downloaded file from %(request)s referred in '\n-            '<%(referer)s>',\n-            {'status': status, 'request': request, 'referer': referer},\n-            extra={'spider': info.spider}\n+            \"File (%(status)s): Downloaded file from %(request)s referred in \"\n+            \"<%(referer)s>\",\n+            {\"status\": status, \"request\": request, \"referer\": referer},\n+            extra={\"spider\": info.spider},\n         )\n         self.inc_stats(info.spider, status)\n \n@@ -468,26 +486,33 @@ class FilesPipeline(MediaPipeline):\n             checksum = self.file_downloaded(response, request, info, item=item)\n         except FileException as exc:\n             logger.warning(\n-                'File (error): Error processing file from %(request)s '\n-                'referred in <%(referer)s>: %(errormsg)s',\n-                {'request': request, 'referer': referer, 'errormsg': str(exc)},\n-                extra={'spider': info.spider}, exc_info=True\n+                \"File (error): Error processing file from %(request)s \"\n+                \"referred in <%(referer)s>: %(errormsg)s\",\n+                {\"request\": request, \"referer\": referer, \"errormsg\": str(exc)},\n+                extra={\"spider\": info.spider},\n+                exc_info=True,\n             )\n             raise\n         except Exception as exc:\n             logger.error(\n-                'File (unknown-error): Error processing file from %(request)s '\n-                'referred in <%(referer)s>',\n-                {'request': request, 'referer': referer},\n-                exc_info=True, extra={'spider': info.spider}\n+                \"File (unknown-error): Error processing file from %(request)s \"\n+                \"referred in <%(referer)s>\",\n+                {\"request\": request, \"referer\": referer},\n+                exc_info=True,\n+                extra={\"spider\": info.spider},\n             )\n             raise FileException(str(exc))\n \n-        return {'url': request.url, 'path': path, 'checksum': checksum, 'status': status}\n+        return {\n+            \"url\": request.url,\n+            \"path\": path,\n+            \"checksum\": checksum,\n+            \"status\": status,\n+        }\n \n     def inc_stats(self, spider, status):\n-        spider.crawler.stats.inc_value('file_count', spider=spider)\n-        spider.crawler.stats.inc_value(f'file_status_count/{status}', spider=spider)\n+        spider.crawler.stats.inc_value(\"file_count\", spider=spider)\n+        spider.crawler.stats.inc_value(f\"file_status_count/{status}\", spider=spider)\n \n     # Overridable Interface\n     def get_media_requests(self, item, info):\n@@ -513,8 +538,8 @@ class FilesPipeline(MediaPipeline):\n         # Handles empty and wild extensions by trying to guess the\n         # mime type then extension or default to empty string otherwise\n         if media_ext not in mimetypes.types_map:\n-            media_ext = ''\n+            media_ext = \"\"\n             media_type = mimetypes.guess_type(request.url)[0]\n             if media_type:\n                 media_ext = mimetypes.guess_extension(media_type)\n-        return f'full/{media_guid}{media_ext}'\n+        return f\"full/{media_guid}{media_ext}\"\n\n@@ -14,6 +14,7 @@ from itemadapter import ItemAdapter\n from scrapy.exceptions import DropItem, NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.pipelines.files import FileException, FilesPipeline\n+\n # TODO: from scrapy.pipelines.media import MediaPipeline\n from scrapy.settings import Settings\n from scrapy.utils.misc import md5sum\n@@ -24,7 +25,11 @@ class NoimagesDrop(DropItem):\n     \"\"\"Product with no images exception\"\"\"\n \n     def __init__(self, *args, **kwargs):\n-        warnings.warn(\"The NoimagesDrop class is deprecated\", category=ScrapyDeprecationWarning, stacklevel=2)\n+        warnings.warn(\n+            \"The NoimagesDrop class is deprecated\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n         super().__init__(*args, **kwargs)\n \n \n@@ -33,11 +38,9 @@ class ImageException(FileException):\n \n \n class ImagesPipeline(FilesPipeline):\n-    \"\"\"Abstract pipeline that implement the image thumbnail generation logic\n+    \"\"\"Abstract pipeline that implement the image thumbnail generation logic\"\"\"\n \n-    \"\"\"\n-\n-    MEDIA_NAME = 'image'\n+    MEDIA_NAME = \"image\"\n \n     # Uppercase attributes kept for backward compatibility with code that subclasses\n     # ImagesPipeline. They may be overridden by settings.\n@@ -45,16 +48,17 @@ class ImagesPipeline(FilesPipeline):\n     MIN_HEIGHT = 0\n     EXPIRES = 90\n     THUMBS = {}\n-    DEFAULT_IMAGES_URLS_FIELD = 'image_urls'\n-    DEFAULT_IMAGES_RESULT_FIELD = 'images'\n+    DEFAULT_IMAGES_URLS_FIELD = \"image_urls\"\n+    DEFAULT_IMAGES_RESULT_FIELD = \"images\"\n \n     def __init__(self, store_uri, download_func=None, settings=None):\n         try:\n             from PIL import Image\n+\n             self._Image = Image\n         except ImportError:\n             raise NotConfigured(\n-                'ImagesPipeline requires installing Pillow 4.0.0 or later'\n+                \"ImagesPipeline requires installing Pillow 4.0.0 or later\"\n             )\n \n         super().__init__(store_uri, settings=settings, download_func=download_func)\n@@ -62,12 +66,10 @@ class ImagesPipeline(FilesPipeline):\n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n \n-        resolve = functools.partial(self._key_for_pipe,\n-                                    base_class_name=\"ImagesPipeline\",\n-                                    settings=settings)\n-        self.expires = settings.getint(\n-            resolve(\"IMAGES_EXPIRES\"), self.EXPIRES\n+        resolve = functools.partial(\n+            self._key_for_pipe, base_class_name=\"ImagesPipeline\", settings=settings\n         )\n+        self.expires = settings.getint(resolve(\"IMAGES_EXPIRES\"), self.EXPIRES)\n \n         if not hasattr(self, \"IMAGES_RESULT_FIELD\"):\n             self.IMAGES_RESULT_FIELD = self.DEFAULT_IMAGES_RESULT_FIELD\n@@ -75,47 +77,39 @@ class ImagesPipeline(FilesPipeline):\n             self.IMAGES_URLS_FIELD = self.DEFAULT_IMAGES_URLS_FIELD\n \n         self.images_urls_field = settings.get(\n-            resolve('IMAGES_URLS_FIELD'),\n-            self.IMAGES_URLS_FIELD\n+            resolve(\"IMAGES_URLS_FIELD\"), self.IMAGES_URLS_FIELD\n         )\n         self.images_result_field = settings.get(\n-            resolve('IMAGES_RESULT_FIELD'),\n-            self.IMAGES_RESULT_FIELD\n-        )\n-        self.min_width = settings.getint(\n-            resolve('IMAGES_MIN_WIDTH'), self.MIN_WIDTH\n-        )\n-        self.min_height = settings.getint(\n-            resolve('IMAGES_MIN_HEIGHT'), self.MIN_HEIGHT\n-        )\n-        self.thumbs = settings.get(\n-            resolve('IMAGES_THUMBS'), self.THUMBS\n+            resolve(\"IMAGES_RESULT_FIELD\"), self.IMAGES_RESULT_FIELD\n         )\n+        self.min_width = settings.getint(resolve(\"IMAGES_MIN_WIDTH\"), self.MIN_WIDTH)\n+        self.min_height = settings.getint(resolve(\"IMAGES_MIN_HEIGHT\"), self.MIN_HEIGHT)\n+        self.thumbs = settings.get(resolve(\"IMAGES_THUMBS\"), self.THUMBS)\n \n         self._deprecated_convert_image = None\n \n     @classmethod\n     def from_settings(cls, settings):\n-        s3store = cls.STORE_SCHEMES['s3']\n-        s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n-        s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n-        s3store.AWS_SESSION_TOKEN = settings['AWS_SESSION_TOKEN']\n-        s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']\n-        s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']\n-        s3store.AWS_USE_SSL = settings['AWS_USE_SSL']\n-        s3store.AWS_VERIFY = settings['AWS_VERIFY']\n-        s3store.POLICY = settings['IMAGES_STORE_S3_ACL']\n+        s3store = cls.STORE_SCHEMES[\"s3\"]\n+        s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n+        s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n+        s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n+        s3store.AWS_ENDPOINT_URL = settings[\"AWS_ENDPOINT_URL\"]\n+        s3store.AWS_REGION_NAME = settings[\"AWS_REGION_NAME\"]\n+        s3store.AWS_USE_SSL = settings[\"AWS_USE_SSL\"]\n+        s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n+        s3store.POLICY = settings[\"IMAGES_STORE_S3_ACL\"]\n \n-        gcs_store = cls.STORE_SCHEMES['gs']\n-        gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n-        gcs_store.POLICY = settings['IMAGES_STORE_GCS_ACL'] or None\n+        gcs_store = cls.STORE_SCHEMES[\"gs\"]\n+        gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n+        gcs_store.POLICY = settings[\"IMAGES_STORE_GCS_ACL\"] or None\n \n-        ftp_store = cls.STORE_SCHEMES['ftp']\n-        ftp_store.FTP_USERNAME = settings['FTP_USER']\n-        ftp_store.FTP_PASSWORD = settings['FTP_PASSWORD']\n-        ftp_store.USE_ACTIVE_MODE = settings.getbool('FEED_STORAGE_FTP_ACTIVE')\n+        ftp_store = cls.STORE_SCHEMES[\"ftp\"]\n+        ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n+        ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n+        ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n \n-        store_uri = settings['IMAGES_STORE']\n+        store_uri = settings[\"IMAGES_STORE\"]\n         return cls(store_uri, settings=settings)\n \n     def file_downloaded(self, response, request, info, *, item=None):\n@@ -129,9 +123,12 @@ class ImagesPipeline(FilesPipeline):\n                 checksum = md5sum(buf)\n             width, height = image.size\n             self.store.persist_file(\n-                path, buf, info,\n-                meta={'width': width, 'height': height},\n-                headers={'Content-Type': 'image/jpeg'})\n+                path,\n+                buf,\n+                info,\n+                meta={\"width\": width, \"height\": height},\n+                headers={\"Content-Type\": \"image/jpeg\"},\n+            )\n         return checksum\n \n     def get_images(self, response, request, info, *, item=None):\n@@ -140,25 +137,35 @@ class ImagesPipeline(FilesPipeline):\n \n         width, height = orig_image.size\n         if width < self.min_width or height < self.min_height:\n-            raise ImageException(\"Image too small \"\n+            raise ImageException(\n+                \"Image too small \"\n                 f\"({width}x{height} < \"\n-                                 f\"{self.min_width}x{self.min_height})\")\n+                f\"{self.min_width}x{self.min_height})\"\n+            )\n \n         if self._deprecated_convert_image is None:\n-            self._deprecated_convert_image = 'response_body' not in get_func_args(self.convert_image)\n+            self._deprecated_convert_image = \"response_body\" not in get_func_args(\n+                self.convert_image\n+            )\n             if self._deprecated_convert_image:\n-                warnings.warn(f'{self.__class__.__name__}.convert_image() method overriden in a deprecated way, '\n-                              'overriden method does not accept response_body argument.',\n-                              category=ScrapyDeprecationWarning)\n+                warnings.warn(\n+                    f\"{self.__class__.__name__}.convert_image() method overriden in a deprecated way, \"\n+                    \"overriden method does not accept response_body argument.\",\n+                    category=ScrapyDeprecationWarning,\n+                )\n \n         if self._deprecated_convert_image:\n             image, buf = self.convert_image(orig_image)\n         else:\n-            image, buf = self.convert_image(orig_image, response_body=BytesIO(response.body))\n+            image, buf = self.convert_image(\n+                orig_image, response_body=BytesIO(response.body)\n+            )\n         yield path, image, buf\n \n         for thumb_id, size in self.thumbs.items():\n-            thumb_path = self.thumb_path(request, thumb_id, response=response, info=info, item=item)\n+            thumb_path = self.thumb_path(\n+                request, thumb_id, response=response, info=info, item=item\n+            )\n             if self._deprecated_convert_image:\n                 thumb_image, thumb_buf = self.convert_image(image, size)\n             else:\n@@ -167,21 +174,24 @@ class ImagesPipeline(FilesPipeline):\n \n     def convert_image(self, image, size=None, response_body=None):\n         if response_body is None:\n-            warnings.warn(f'{self.__class__.__name__}.convert_image() method called in a deprecated way, '\n-                          'method called without response_body argument.',\n-                          category=ScrapyDeprecationWarning, stacklevel=2)\n+            warnings.warn(\n+                f\"{self.__class__.__name__}.convert_image() method called in a deprecated way, \"\n+                \"method called without response_body argument.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n \n-        if image.format == 'PNG' and image.mode == 'RGBA':\n-            background = self._Image.new('RGBA', image.size, (255, 255, 255))\n+        if image.format == \"PNG\" and image.mode == \"RGBA\":\n+            background = self._Image.new(\"RGBA\", image.size, (255, 255, 255))\n             background.paste(image, image)\n-            image = background.convert('RGB')\n-        elif image.mode == 'P':\n+            image = background.convert(\"RGB\")\n+        elif image.mode == \"P\":\n             image = image.convert(\"RGBA\")\n-            background = self._Image.new('RGBA', image.size, (255, 255, 255))\n+            background = self._Image.new(\"RGBA\", image.size, (255, 255, 255))\n             background.paste(image, image)\n-            image = background.convert('RGB')\n-        elif image.mode != 'RGB':\n-            image = image.convert('RGB')\n+            image = background.convert(\"RGB\")\n+        elif image.mode != \"RGB\":\n+            image = image.convert(\"RGB\")\n \n         if size:\n             image = image.copy()\n@@ -193,11 +203,11 @@ class ImagesPipeline(FilesPipeline):\n             except AttributeError:\n                 resampling_filter = self._Image.ANTIALIAS\n             image.thumbnail(size, resampling_filter)\n-        elif response_body is not None and image.format == 'JPEG':\n+        elif response_body is not None and image.format == \"JPEG\":\n             return image, response_body\n \n         buf = BytesIO()\n-        image.save(buf, 'JPEG')\n+        image.save(buf, \"JPEG\")\n         return image, buf\n \n     def get_media_requests(self, item, info):\n@@ -211,8 +221,8 @@ class ImagesPipeline(FilesPipeline):\n \n     def file_path(self, request, response=None, info=None, *, item=None):\n         image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n-        return f'full/{image_guid}.jpg'\n+        return f\"full/{image_guid}.jpg\"\n \n     def thumb_path(self, request, thumb_id, response=None, info=None, *, item=None):\n         thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n-        return f'thumbs/{thumb_id}/{thumb_guid}.jpg'\n+        return f\"thumbs/{thumb_id}/{thumb_guid}.jpg\"\n\n@@ -34,12 +34,10 @@ class MediaPipeline:\n \n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n-        resolve = functools.partial(self._key_for_pipe,\n-                                    base_class_name=\"MediaPipeline\",\n-                                    settings=settings)\n-        self.allow_redirects = settings.getbool(\n-            resolve('MEDIA_ALLOW_REDIRECTS'), False\n+        resolve = functools.partial(\n+            self._key_for_pipe, base_class_name=\"MediaPipeline\", settings=settings\n         )\n+        self.allow_redirects = settings.getbool(resolve(\"MEDIA_ALLOW_REDIRECTS\"), False)\n         self._handle_statuses(self.allow_redirects)\n \n         # Check if deprecated methods are being used and make them compatible\n@@ -64,7 +62,8 @@ class MediaPipeline:\n         if (\n             not base_class_name\n             or class_name == base_class_name\n-            or settings and not settings.get(formatted_key)\n+            or settings\n+            and not settings.get(formatted_key)\n         ):\n             return key\n         return formatted_key\n@@ -113,16 +112,23 @@ class MediaPipeline:\n         dfd = mustbe_deferred(self.media_to_download, request, info, item=item)\n         dfd.addCallback(self._check_media_to_download, request, info, item=item)\n         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n-        dfd.addErrback(lambda f: logger.error(\n-            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})\n+        dfd.addErrback(\n+            lambda f: logger.error(\n+                f.value, exc_info=failure_to_exc_info(f), extra={\"spider\": info.spider}\n+            )\n         )\n         return dfd.addBoth(lambda _: wad)  # it must return wad at last\n \n     def _make_compatible(self):\n         \"\"\"Make overridable methods of MediaPipeline and subclasses backwards compatible\"\"\"\n         methods = [\n-            \"file_path\", \"thumb_path\", \"media_to_download\", \"media_downloaded\",\n-            \"file_downloaded\", \"image_downloaded\", \"get_images\"\n+            \"file_path\",\n+            \"thumb_path\",\n+            \"media_to_download\",\n+            \"media_downloaded\",\n+            \"file_downloaded\",\n+            \"image_downloaded\",\n+            \"get_images\",\n         ]\n \n         for method_name in methods:\n@@ -139,7 +145,7 @@ class MediaPipeline:\n             if self._expects_item[func.__name__]:\n                 return func(*args, **kwargs)\n \n-            kwargs.pop('item', None)\n+            kwargs.pop(\"item\", None)\n             return func(*args, **kwargs)\n \n         return wrapper\n@@ -148,19 +154,22 @@ class MediaPipeline:\n         sig = signature(func)\n         self._expects_item[func.__name__] = True\n \n-        if 'item' not in sig.parameters:\n+        if \"item\" not in sig.parameters:\n             old_params = str(sig)[1:-1]\n             new_params = old_params + \", *, item=None\"\n-            warn(f'{func.__name__}(self, {old_params}) is deprecated, '\n-                 f'please use {func.__name__}(self, {new_params})',\n-                 ScrapyDeprecationWarning, stacklevel=2)\n+            warn(\n+                f\"{func.__name__}(self, {old_params}) is deprecated, \"\n+                f\"please use {func.__name__}(self, {new_params})\",\n+                ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n             self._expects_item[func.__name__] = False\n \n     def _modify_media_request(self, request):\n         if self.handle_httpstatus_list:\n-            request.meta['handle_httpstatus_list'] = self.handle_httpstatus_list\n+            request.meta[\"handle_httpstatus_list\"] = self.handle_httpstatus_list\n         else:\n-            request.meta['handle_httpstatus_all'] = True\n+            request.meta[\"handle_httpstatus_all\"] = True\n \n     def _check_media_to_download(self, result, request, info, item):\n         if result is not None:\n@@ -169,14 +178,22 @@ class MediaPipeline:\n             # this ugly code was left only to support tests. TODO: remove\n             dfd = mustbe_deferred(self.download_func, request, info.spider)\n             dfd.addCallbacks(\n-                callback=self.media_downloaded, callbackArgs=(request, info), callbackKeywords={'item': item},\n-                errback=self.media_failed, errbackArgs=(request, info))\n+                callback=self.media_downloaded,\n+                callbackArgs=(request, info),\n+                callbackKeywords={\"item\": item},\n+                errback=self.media_failed,\n+                errbackArgs=(request, info),\n+            )\n         else:\n             self._modify_media_request(request)\n             dfd = self.crawler.engine.download(request)\n             dfd.addCallbacks(\n-                callback=self.media_downloaded, callbackArgs=(request, info), callbackKeywords={'item': item},\n-                errback=self.media_failed, errbackArgs=(request, info))\n+                callback=self.media_downloaded,\n+                callbackArgs=(request, info),\n+                callbackKeywords={\"item\": item},\n+                errback=self.media_failed,\n+                errbackArgs=(request, info),\n+            )\n         return dfd\n \n     def _cache_result_and_execute_waiters(self, result, fp, info):\n@@ -207,9 +224,9 @@ class MediaPipeline:\n             #\n             # This problem does not occur in Python 2.7 since we don't have\n             # Exception Chaining (https://www.python.org/dev/peps/pep-3134/).\n-            context = getattr(result.value, '__context__', None)\n+            context = getattr(result.value, \"__context__\", None)\n             if isinstance(context, StopIteration):\n-                setattr(result.value, '__context__', None)\n+                setattr(result.value, \"__context__\", None)\n \n         info.downloading.remove(fp)\n         info.downloaded[fp] = result  # cache result\n@@ -239,10 +256,10 @@ class MediaPipeline:\n             for ok, value in results:\n                 if not ok:\n                     logger.error(\n-                        '%(class)s found errors processing %(item)s',\n-                        {'class': self.__class__.__name__, 'item': item},\n+                        \"%(class)s found errors processing %(item)s\",\n+                        {\"class\": self.__class__.__name__, \"item\": item},\n                         exc_info=failure_to_exc_info(value),\n-                        extra={'spider': info.spider}\n+                        extra={\"spider\": info.spider},\n                     )\n         return item\n \n\n@@ -18,11 +18,11 @@ def _path_safe(text):\n     >>> _path_safe('some@symbol?').startswith('some_symbol_')\n     True\n     \"\"\"\n-    pathable_slot = \"\".join([c if c.isalnum() or c in '-._' else '_' for c in text])\n+    pathable_slot = \"\".join([c if c.isalnum() or c in \"-._\" else \"_\" for c in text])\n     # as we replace some letters we can get collision for different slots\n     # add we add unique part\n-    unique_slot = hashlib.md5(text.encode('utf8')).hexdigest()\n-    return '-'.join([pathable_slot, unique_slot])\n+    unique_slot = hashlib.md5(text.encode(\"utf8\")).hexdigest()\n+    return \"-\".join([pathable_slot, unique_slot])\n \n \n class ScrapyPriorityQueue:\n@@ -77,7 +77,7 @@ class ScrapyPriorityQueue:\n             self.downstream_queue_cls,\n             None,\n             self.crawler,\n-            self.key + '/' + str(key),\n+            self.key + \"/\" + str(key),\n         )\n \n     def priority(self, request):\n@@ -128,7 +128,6 @@ class ScrapyPriorityQueue:\n \n \n class DownloaderInterface:\n-\n     def __init__(self, crawler):\n         self.downloader = crawler.engine.downloader\n \n@@ -156,17 +155,21 @@ class DownloaderAwarePriorityQueue:\n         return cls(crawler, downstream_queue_cls, key, startprios)\n \n     def __init__(self, crawler, downstream_queue_cls, key, slot_startprios=()):\n-        if crawler.settings.getint('CONCURRENT_REQUESTS_PER_IP') != 0:\n-            raise ValueError(f'\"{self.__class__}\" does not support CONCURRENT_REQUESTS_PER_IP')\n+        if crawler.settings.getint(\"CONCURRENT_REQUESTS_PER_IP\") != 0:\n+            raise ValueError(\n+                f'\"{self.__class__}\" does not support CONCURRENT_REQUESTS_PER_IP'\n+            )\n \n         if slot_startprios and not isinstance(slot_startprios, dict):\n-            raise ValueError(\"DownloaderAwarePriorityQueue accepts \"\n+            raise ValueError(\n+                \"DownloaderAwarePriorityQueue accepts \"\n                 \"``slot_startprios`` as a dict; \"\n                 f\"{slot_startprios.__class__!r} instance \"\n                 \"is passed. Most likely, it means the state is\"\n                 \"created by an incompatible priority queue. \"\n                 \"Only a crawl started with the same priority \"\n-                             \"queue class can be resumed.\")\n+                \"queue class can be resumed.\"\n+            )\n \n         self._downloader_interface = DownloaderInterface(crawler)\n         self.downstream_queue_cls = downstream_queue_cls\n@@ -181,7 +184,7 @@ class DownloaderAwarePriorityQueue:\n         return ScrapyPriorityQueue(\n             self.crawler,\n             self.downstream_queue_cls,\n-            self.key + '/' + _path_safe(slot),\n+            self.key + \"/\" + _path_safe(slot),\n             startprios,\n         )\n \n\n@@ -1,6 +1,11 @@\n from twisted.internet import defer\n from twisted.internet.base import ThreadedResolver\n-from twisted.internet.interfaces import IHostResolution, IHostnameResolver, IResolutionReceiver, IResolverSimple\n+from twisted.internet.interfaces import (\n+    IHostResolution,\n+    IHostnameResolver,\n+    IResolutionReceiver,\n+    IResolverSimple,\n+)\n from zope.interface.declarations import implementer, provider\n \n from scrapy.utils.datatypes import LocalCache\n@@ -23,11 +28,11 @@ class CachingThreadedResolver(ThreadedResolver):\n \n     @classmethod\n     def from_crawler(cls, crawler, reactor):\n-        if crawler.settings.getbool('DNSCACHE_ENABLED'):\n-            cache_size = crawler.settings.getint('DNSCACHE_SIZE')\n+        if crawler.settings.getbool(\"DNSCACHE_ENABLED\"):\n+            cache_size = crawler.settings.getint(\"DNSCACHE_SIZE\")\n         else:\n             cache_size = 0\n-        return cls(reactor, cache_size, crawler.settings.getfloat('DNS_TIMEOUT'))\n+        return cls(reactor, cache_size, crawler.settings.getfloat(\"DNS_TIMEOUT\"))\n \n     def install_on_reactor(self):\n         self.reactor.installResolver(self)\n@@ -94,8 +99,8 @@ class CachingHostnameResolver:\n \n     @classmethod\n     def from_crawler(cls, crawler, reactor):\n-        if crawler.settings.getbool('DNSCACHE_ENABLED'):\n-            cache_size = crawler.settings.getint('DNSCACHE_SIZE')\n+        if crawler.settings.getbool(\"DNSCACHE_ENABLED\"):\n+            cache_size = crawler.settings.getint(\"DNSCACHE_SIZE\")\n         else:\n             cache_size = 0\n         return cls(reactor, cache_size)\n@@ -104,7 +109,12 @@ class CachingHostnameResolver:\n         self.reactor.installNameResolver(self)\n \n     def resolveHostName(\n-        self, resolutionReceiver, hostName, portNumber=0, addressTypes=None, transportSemantics=\"TCP\"\n+        self,\n+        resolutionReceiver,\n+        hostName,\n+        portNumber=0,\n+        addressTypes=None,\n+        transportSemantics=\"TCP\",\n     ):\n         try:\n             addresses = dnscache[hostName]\n\n@@ -14,26 +14,26 @@ from scrapy.utils.python import binary_is_text, to_bytes, to_unicode\n class ResponseTypes:\n \n     CLASSES = {\n-        'text/html': 'scrapy.http.HtmlResponse',\n-        'application/atom+xml': 'scrapy.http.XmlResponse',\n-        'application/rdf+xml': 'scrapy.http.XmlResponse',\n-        'application/rss+xml': 'scrapy.http.XmlResponse',\n-        'application/xhtml+xml': 'scrapy.http.HtmlResponse',\n-        'application/vnd.wap.xhtml+xml': 'scrapy.http.HtmlResponse',\n-        'application/xml': 'scrapy.http.XmlResponse',\n-        'application/json': 'scrapy.http.TextResponse',\n-        'application/x-json': 'scrapy.http.TextResponse',\n-        'application/json-amazonui-streaming': 'scrapy.http.TextResponse',\n-        'application/javascript': 'scrapy.http.TextResponse',\n-        'application/x-javascript': 'scrapy.http.TextResponse',\n-        'text/xml': 'scrapy.http.XmlResponse',\n-        'text/*': 'scrapy.http.TextResponse',\n+        \"text/html\": \"scrapy.http.HtmlResponse\",\n+        \"application/atom+xml\": \"scrapy.http.XmlResponse\",\n+        \"application/rdf+xml\": \"scrapy.http.XmlResponse\",\n+        \"application/rss+xml\": \"scrapy.http.XmlResponse\",\n+        \"application/xhtml+xml\": \"scrapy.http.HtmlResponse\",\n+        \"application/vnd.wap.xhtml+xml\": \"scrapy.http.HtmlResponse\",\n+        \"application/xml\": \"scrapy.http.XmlResponse\",\n+        \"application/json\": \"scrapy.http.TextResponse\",\n+        \"application/x-json\": \"scrapy.http.TextResponse\",\n+        \"application/json-amazonui-streaming\": \"scrapy.http.TextResponse\",\n+        \"application/javascript\": \"scrapy.http.TextResponse\",\n+        \"application/x-javascript\": \"scrapy.http.TextResponse\",\n+        \"text/xml\": \"scrapy.http.XmlResponse\",\n+        \"text/*\": \"scrapy.http.TextResponse\",\n     }\n \n     def __init__(self):\n         self.classes = {}\n         self.mimetypes = MimeTypes()\n-        mimedata = get_data('scrapy', 'mime.types').decode('utf8')\n+        mimedata = get_data(\"scrapy\", \"mime.types\").decode(\"utf8\")\n         self.mimetypes.readfp(StringIO(mimedata))\n         for mimetype, cls in self.CLASSES.items():\n             self.classes[mimetype] = load_object(cls)\n@@ -52,14 +52,17 @@ class ResponseTypes:\n         header\"\"\"\n         if content_encoding:\n             return Response\n-        mimetype = to_unicode(content_type).split(';')[0].strip().lower()\n+        mimetype = to_unicode(content_type).split(\";\")[0].strip().lower()\n         return self.from_mimetype(mimetype)\n \n     def from_content_disposition(self, content_disposition):\n         try:\n-            filename = to_unicode(\n-                content_disposition, encoding='latin-1', errors='replace'\n-            ).split(';')[1].split('=')[1].strip('\"\\'')\n+            filename = (\n+                to_unicode(content_disposition, encoding=\"latin-1\", errors=\"replace\")\n+                .split(\";\")[1]\n+                .split(\"=\")[1]\n+                .strip(\"\\\"'\")\n+            )\n             return self.from_filename(filename)\n         except IndexError:\n             return Response\n@@ -68,13 +71,13 @@ class ResponseTypes:\n         \"\"\"Return the most appropriate Response class by looking at the HTTP\n         headers\"\"\"\n         cls = Response\n-        if b'Content-Type' in headers:\n+        if b\"Content-Type\" in headers:\n             cls = self.from_content_type(\n-                content_type=headers[b'Content-Type'],\n-                content_encoding=headers.get(b'Content-Encoding')\n+                content_type=headers[b\"Content-Type\"],\n+                content_encoding=headers.get(b\"Content-Encoding\"),\n             )\n-        if cls is Response and b'Content-Disposition' in headers:\n-            cls = self.from_content_disposition(headers[b'Content-Disposition'])\n+        if cls is Response and b\"Content-Disposition\" in headers:\n+            cls = self.from_content_disposition(headers[b\"Content-Disposition\"])\n         return cls\n \n     def from_filename(self, filename):\n@@ -92,15 +95,15 @@ class ResponseTypes:\n         chunk = body[:5000]\n         chunk = to_bytes(chunk)\n         if not binary_is_text(chunk):\n-            return self.from_mimetype('application/octet-stream')\n+            return self.from_mimetype(\"application/octet-stream\")\n         lowercase_chunk = chunk.lower()\n         if b\"<html>\" in lowercase_chunk:\n-            return self.from_mimetype('text/html')\n+            return self.from_mimetype(\"text/html\")\n         if b\"<?xml\" in lowercase_chunk:\n-            return self.from_mimetype('text/xml')\n-        if b'<!doctype html>' in lowercase_chunk:\n-            return self.from_mimetype('text/html')\n-        return self.from_mimetype('text')\n+            return self.from_mimetype(\"text/xml\")\n+        if b\"<!doctype html>\" in lowercase_chunk:\n+            return self.from_mimetype(\"text/html\")\n+        return self.from_mimetype(\"text\")\n \n     def from_args(self, headers=None, url=None, filename=None, body=None):\n         \"\"\"Guess the most appropriate Response class based on\n\n@@ -12,7 +12,7 @@ def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):\n         if to_native_str_type:\n             robotstxt_body = to_unicode(robotstxt_body)\n         else:\n-            robotstxt_body = robotstxt_body.decode('utf-8')\n+            robotstxt_body = robotstxt_body.decode(\"utf-8\")\n     except UnicodeDecodeError:\n         # If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.\n         # Switch to 'allow all' state.\n@@ -20,9 +20,9 @@ def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):\n             \"Failure while parsing robots.txt. File either contains garbage or \"\n             \"is in an encoding other than UTF-8, treating it as an empty file.\",\n             exc_info=sys.exc_info(),\n-            extra={'spider': spider},\n+            extra={\"spider\": spider},\n         )\n-        robotstxt_body = ''\n+        robotstxt_body = \"\"\n     return robotstxt_body\n \n \n@@ -57,8 +57,11 @@ class RobotParser(metaclass=ABCMeta):\n class PythonRobotParser(RobotParser):\n     def __init__(self, robotstxt_body, spider):\n         from urllib.robotparser import RobotFileParser\n+\n         self.spider = spider\n-        robotstxt_body = decode_robotstxt(robotstxt_body, spider, to_native_str_type=True)\n+        robotstxt_body = decode_robotstxt(\n+            robotstxt_body, spider, to_native_str_type=True\n+        )\n         self.rp = RobotFileParser()\n         self.rp.parse(robotstxt_body.splitlines())\n \n@@ -77,8 +80,9 @@ class PythonRobotParser(RobotParser):\n class ReppyRobotParser(RobotParser):\n     def __init__(self, robotstxt_body, spider):\n         from reppy.robots import Robots\n+\n         self.spider = spider\n-        self.rp = Robots.parse('', robotstxt_body)\n+        self.rp = Robots.parse(\"\", robotstxt_body)\n \n     @classmethod\n     def from_crawler(cls, crawler, robotstxt_body):\n@@ -93,6 +97,7 @@ class ReppyRobotParser(RobotParser):\n class RerpRobotParser(RobotParser):\n     def __init__(self, robotstxt_body, spider):\n         from robotexclusionrulesparser import RobotExclusionRulesParser\n+\n         self.spider = spider\n         self.rp = RobotExclusionRulesParser()\n         robotstxt_body = decode_robotstxt(robotstxt_body, spider)\n@@ -113,6 +118,7 @@ class RerpRobotParser(RobotParser):\n class ProtegoRobotParser(RobotParser):\n     def __init__(self, robotstxt_body, spider):\n         from protego import Protego\n+\n         self.spider = spider\n         robotstxt_body = decode_robotstxt(robotstxt_body, spider)\n         self.rp = Protego.parse(robotstxt_body)\n\n@@ -8,19 +8,18 @@ from scrapy.utils.python import to_bytes\n from scrapy.http import HtmlResponse, XmlResponse\n \n \n-__all__ = ['Selector', 'SelectorList']\n+__all__ = [\"Selector\", \"SelectorList\"]\n \n \n def _st(response, st):\n     if st is None:\n-        return 'xml' if isinstance(response, XmlResponse) else 'html'\n+        return \"xml\" if isinstance(response, XmlResponse) else \"html\"\n     return st\n \n \n def _response_from_text(text, st):\n-    rt = XmlResponse if st == 'xml' else HtmlResponse\n-    return rt(url='about:blank', encoding='utf-8',\n-              body=to_bytes(text, 'utf-8'))\n+    rt = XmlResponse if st == \"xml\" else HtmlResponse\n+    return rt(url=\"about:blank\", encoding=\"utf-8\", body=to_bytes(text, \"utf-8\"))\n \n \n class SelectorList(_ParselSelector.selectorlist_cls, object_ref):\n@@ -61,13 +60,15 @@ class Selector(_ParselSelector, object_ref):\n     detection will occur.\n     \"\"\"\n \n-    __slots__ = ['response']\n+    __slots__ = [\"response\"]\n     selectorlist_cls = SelectorList\n \n     def __init__(self, response=None, text=None, type=None, root=None, **kwargs):\n         if response is not None and text is not None:\n-            raise ValueError(f'{self.__class__.__name__}.__init__() received '\n-                             'both response and text')\n+            raise ValueError(\n+                f\"{self.__class__.__name__}.__init__() received \"\n+                \"both response and text\"\n+            )\n \n         st = _st(response, type)\n \n@@ -76,7 +77,7 @@ class Selector(_ParselSelector, object_ref):\n \n         if response is not None:\n             text = response.text\n-            kwargs.setdefault('base_url', response.url)\n+            kwargs.setdefault(\"base_url\", response.url)\n \n         self.response = response\n         super().__init__(text=text, type=st, root=root, **kwargs)\n\n@@ -8,11 +8,11 @@ from scrapy.settings import default_settings\n \n \n SETTINGS_PRIORITIES = {\n-    'default': 0,\n-    'command': 10,\n-    'project': 20,\n-    'spider': 30,\n-    'cmdline': 40,\n+    \"default\": 0,\n+    \"command\": 10,\n+    \"project\": 20,\n+    \"spider\": 30,\n+    \"cmdline\": 40,\n }\n \n \n@@ -76,7 +76,7 @@ class BaseSettings(MutableMapping):\n     highest priority will be retrieved.\n     \"\"\"\n \n-    def __init__(self, values=None, priority='project'):\n+    def __init__(self, values=None, priority=\"project\"):\n         self.frozen = False\n         self.attributes = {}\n         if values:\n@@ -126,9 +126,11 @@ class BaseSettings(MutableMapping):\n                 return True\n             if got in (\"False\", \"false\"):\n                 return False\n-            raise ValueError(\"Supported values for boolean settings \"\n+            raise ValueError(\n+                \"Supported values for boolean settings \"\n                 \"are 0/1, True/False, '0'/'1', \"\n-                             \"'True'/'False' and 'true'/'false'\")\n+                \"'True'/'False' and 'true'/'false'\"\n+            )\n \n     def getint(self, name, default=0):\n         \"\"\"\n@@ -170,7 +172,7 @@ class BaseSettings(MutableMapping):\n         \"\"\"\n         value = self.get(name, default or [])\n         if isinstance(value, str):\n-            value = value.split(',')\n+            value = value.split(\",\")\n         return list(value)\n \n     def getdict(self, name, default=None):\n@@ -223,7 +225,7 @@ class BaseSettings(MutableMapping):\n             try:\n                 return json.loads(value)\n             except ValueError:\n-                return value.split(',')\n+                return value.split(\",\")\n         return copy.deepcopy(value)\n \n     def getwithbase(self, name):\n@@ -234,7 +236,7 @@ class BaseSettings(MutableMapping):\n         :type name: str\n         \"\"\"\n         compbs = BaseSettings()\n-        compbs.update(self[name + '_BASE'])\n+        compbs.update(self[name + \"_BASE\"])\n         compbs.update(self[name])\n         return compbs\n \n@@ -259,12 +261,12 @@ class BaseSettings(MutableMapping):\n         \"\"\"\n         if len(self) > 0:\n             return max(self.getpriority(name) for name in self)\n-        return get_settings_priority('default')\n+        return get_settings_priority(\"default\")\n \n     def __setitem__(self, name, value):\n         self.set(name, value)\n \n-    def set(self, name, value, priority='project'):\n+    def set(self, name, value, priority=\"project\"):\n         \"\"\"\n         Store a key/value attribute with a given priority.\n \n@@ -292,10 +294,10 @@ class BaseSettings(MutableMapping):\n         else:\n             self.attributes[name].set(value, priority)\n \n-    def setdict(self, values, priority='project'):\n+    def setdict(self, values, priority=\"project\"):\n         self.update(values, priority)\n \n-    def setmodule(self, module, priority='project'):\n+    def setmodule(self, module, priority=\"project\"):\n         \"\"\"\n         Store settings from a module with a given priority.\n \n@@ -317,7 +319,7 @@ class BaseSettings(MutableMapping):\n             if key.isupper():\n                 self.set(key, getattr(module, key), priority)\n \n-    def update(self, values, priority='project'):\n+    def update(self, values, priority=\"project\"):\n         \"\"\"\n         Store key/value pairs with a given priority.\n \n@@ -350,7 +352,7 @@ class BaseSettings(MutableMapping):\n                 for name, value in values.items():\n                     self.set(name, value, priority)\n \n-    def delete(self, name, priority='project'):\n+    def delete(self, name, priority=\"project\"):\n         self._assert_mutability()\n         priority = get_settings_priority(priority)\n         if priority >= self.getpriority(name):\n@@ -403,12 +405,17 @@ class BaseSettings(MutableMapping):\n         return len(self.attributes)\n \n     def _to_dict(self):\n-        return {self._get_key(k): (v._to_dict() if isinstance(v, BaseSettings) else v)\n-                for k, v in self.items()}\n+        return {\n+            self._get_key(k): (v._to_dict() if isinstance(v, BaseSettings) else v)\n+            for k, v in self.items()\n+        }\n \n     def _get_key(self, key_value):\n-        return (key_value if isinstance(key_value, (bool, float, int, str, type(None)))\n-                else str(key_value))\n+        return (\n+            key_value\n+            if isinstance(key_value, (bool, float, int, str, type(None)))\n+            else str(key_value)\n+        )\n \n     def copy_to_dict(self):\n         \"\"\"\n@@ -444,17 +451,17 @@ class Settings(BaseSettings):\n     described on :ref:`topics-settings-ref` already populated.\n     \"\"\"\n \n-    def __init__(self, values=None, priority='project'):\n+    def __init__(self, values=None, priority=\"project\"):\n         # Do not pass kwarg values here. We don't want to promote user-defined\n         # dicts, and we want to update, not replace, default dicts with the\n         # values given by the user\n         super().__init__()\n-        self.setmodule(default_settings, 'default')\n+        self.setmodule(default_settings, \"default\")\n         # Promote default dictionaries to BaseSettings instances for per-key\n         # priorities\n         for name, val in self.items():\n             if isinstance(val, dict):\n-                self.set(name, BaseSettings(val, 'default'), 'default')\n+                self.set(name, BaseSettings(val, \"default\"), \"default\")\n         self.update(values, priority)\n \n \n\n@@ -27,14 +27,14 @@ AUTOTHROTTLE_MAX_DELAY = 60.0\n AUTOTHROTTLE_START_DELAY = 5.0\n AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n \n-BOT_NAME = 'scrapybot'\n+BOT_NAME = \"scrapybot\"\n \n CLOSESPIDER_TIMEOUT = 0\n CLOSESPIDER_PAGECOUNT = 0\n CLOSESPIDER_ITEMCOUNT = 0\n CLOSESPIDER_ERRORCOUNT = 0\n \n-COMMANDS_MODULE = ''\n+COMMANDS_MODULE = \"\"\n \n COMPRESSION_ENABLED = True\n \n@@ -47,11 +47,11 @@ CONCURRENT_REQUESTS_PER_IP = 0\n COOKIES_ENABLED = True\n COOKIES_DEBUG = False\n \n-DEFAULT_ITEM_CLASS = 'scrapy.item.Item'\n+DEFAULT_ITEM_CLASS = \"scrapy.item.Item\"\n \n DEFAULT_REQUEST_HEADERS = {\n-    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n-    'Accept-Language': 'en',\n+    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n+    \"Accept-Language\": \"en\",\n }\n \n DEPTH_LIMIT = 0\n@@ -60,19 +60,19 @@ DEPTH_PRIORITY = 0\n \n DNSCACHE_ENABLED = True\n DNSCACHE_SIZE = 10000\n-DNS_RESOLVER = 'scrapy.resolver.CachingThreadedResolver'\n+DNS_RESOLVER = \"scrapy.resolver.CachingThreadedResolver\"\n DNS_TIMEOUT = 60\n \n DOWNLOAD_DELAY = 0\n \n DOWNLOAD_HANDLERS = {}\n DOWNLOAD_HANDLERS_BASE = {\n-    'data': 'scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler',\n-    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n-    'http': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',\n-    'https': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',\n-    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n-    'ftp': 'scrapy.core.downloader.handlers.ftp.FTPDownloadHandler',\n+    \"data\": \"scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler\",\n+    \"file\": \"scrapy.core.downloader.handlers.file.FileDownloadHandler\",\n+    \"http\": \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n+    \"https\": \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n+    \"s3\": \"scrapy.core.downloader.handlers.s3.S3DownloadHandler\",\n+    \"ftp\": \"scrapy.core.downloader.handlers.ftp.FTPDownloadHandler\",\n }\n \n DOWNLOAD_TIMEOUT = 180  # 3mins\n@@ -82,56 +82,60 @@ DOWNLOAD_WARNSIZE = 32 * 1024 * 1024    # 32m\n \n DOWNLOAD_FAIL_ON_DATALOSS = True\n \n-DOWNLOADER = 'scrapy.core.downloader.Downloader'\n+DOWNLOADER = \"scrapy.core.downloader.Downloader\"\n \n-DOWNLOADER_HTTPCLIENTFACTORY = 'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'\n-DOWNLOADER_CLIENTCONTEXTFACTORY = 'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'\n-DOWNLOADER_CLIENT_TLS_CIPHERS = 'DEFAULT'\n+DOWNLOADER_HTTPCLIENTFACTORY = (\n+    \"scrapy.core.downloader.webclient.ScrapyHTTPClientFactory\"\n+)\n+DOWNLOADER_CLIENTCONTEXTFACTORY = (\n+    \"scrapy.core.downloader.contextfactory.ScrapyClientContextFactory\"\n+)\n+DOWNLOADER_CLIENT_TLS_CIPHERS = \"DEFAULT\"\n # Use highest TLS/SSL protocol version supported by the platform, also allowing negotiation:\n-DOWNLOADER_CLIENT_TLS_METHOD = 'TLS'\n+DOWNLOADER_CLIENT_TLS_METHOD = \"TLS\"\n DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING = False\n \n DOWNLOADER_MIDDLEWARES = {}\n \n DOWNLOADER_MIDDLEWARES_BASE = {\n     # Engine side\n-    'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,\n-    'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,\n-    'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350,\n-    'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 400,\n-    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 500,\n-    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 550,\n-    'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560,\n-    'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580,\n-    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590,\n-    'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600,\n-    'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700,\n-    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,\n-    'scrapy.downloadermiddlewares.stats.DownloaderStats': 850,\n-    'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900,\n+    \"scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware\": 100,\n+    \"scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware\": 300,\n+    \"scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware\": 350,\n+    \"scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware\": 400,\n+    \"scrapy.downloadermiddlewares.useragent.UserAgentMiddleware\": 500,\n+    \"scrapy.downloadermiddlewares.retry.RetryMiddleware\": 550,\n+    \"scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware\": 560,\n+    \"scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware\": 580,\n+    \"scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\": 590,\n+    \"scrapy.downloadermiddlewares.redirect.RedirectMiddleware\": 600,\n+    \"scrapy.downloadermiddlewares.cookies.CookiesMiddleware\": 700,\n+    \"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\": 750,\n+    \"scrapy.downloadermiddlewares.stats.DownloaderStats\": 850,\n+    \"scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware\": 900,\n     # Downloader side\n }\n \n DOWNLOADER_STATS = True\n \n-DUPEFILTER_CLASS = 'scrapy.dupefilters.RFPDupeFilter'\n+DUPEFILTER_CLASS = \"scrapy.dupefilters.RFPDupeFilter\"\n \n-EDITOR = 'vi'\n-if sys.platform == 'win32':\n-    EDITOR = '%s -m idlelib.idle'\n+EDITOR = \"vi\"\n+if sys.platform == \"win32\":\n+    EDITOR = \"%s -m idlelib.idle\"\n \n EXTENSIONS = {}\n \n EXTENSIONS_BASE = {\n-    'scrapy.extensions.corestats.CoreStats': 0,\n-    'scrapy.extensions.telnet.TelnetConsole': 0,\n-    'scrapy.extensions.memusage.MemoryUsage': 0,\n-    'scrapy.extensions.memdebug.MemoryDebugger': 0,\n-    'scrapy.extensions.closespider.CloseSpider': 0,\n-    'scrapy.extensions.feedexport.FeedExporter': 0,\n-    'scrapy.extensions.logstats.LogStats': 0,\n-    'scrapy.extensions.spiderstate.SpiderState': 0,\n-    'scrapy.extensions.throttle.AutoThrottle': 0,\n+    \"scrapy.extensions.corestats.CoreStats\": 0,\n+    \"scrapy.extensions.telnet.TelnetConsole\": 0,\n+    \"scrapy.extensions.memusage.MemoryUsage\": 0,\n+    \"scrapy.extensions.memdebug.MemoryDebugger\": 0,\n+    \"scrapy.extensions.closespider.CloseSpider\": 0,\n+    \"scrapy.extensions.feedexport.FeedExporter\": 0,\n+    \"scrapy.extensions.logstats.LogStats\": 0,\n+    \"scrapy.extensions.spiderstate.SpiderState\": 0,\n+    \"scrapy.extensions.throttle.AutoThrottle\": 0,\n }\n \n FEED_TEMPDIR = None\n@@ -142,71 +146,71 @@ FEED_EXPORT_ENCODING = None\n FEED_EXPORT_FIELDS = None\n FEED_STORAGES = {}\n FEED_STORAGES_BASE = {\n-    '': 'scrapy.extensions.feedexport.FileFeedStorage',\n-    'file': 'scrapy.extensions.feedexport.FileFeedStorage',\n-    'ftp': 'scrapy.extensions.feedexport.FTPFeedStorage',\n-    'gs': 'scrapy.extensions.feedexport.GCSFeedStorage',\n-    's3': 'scrapy.extensions.feedexport.S3FeedStorage',\n-    'stdout': 'scrapy.extensions.feedexport.StdoutFeedStorage',\n+    \"\": \"scrapy.extensions.feedexport.FileFeedStorage\",\n+    \"file\": \"scrapy.extensions.feedexport.FileFeedStorage\",\n+    \"ftp\": \"scrapy.extensions.feedexport.FTPFeedStorage\",\n+    \"gs\": \"scrapy.extensions.feedexport.GCSFeedStorage\",\n+    \"s3\": \"scrapy.extensions.feedexport.S3FeedStorage\",\n+    \"stdout\": \"scrapy.extensions.feedexport.StdoutFeedStorage\",\n }\n FEED_EXPORT_BATCH_ITEM_COUNT = 0\n FEED_EXPORTERS = {}\n FEED_EXPORTERS_BASE = {\n-    'json': 'scrapy.exporters.JsonItemExporter',\n-    'jsonlines': 'scrapy.exporters.JsonLinesItemExporter',\n-    'jsonl': 'scrapy.exporters.JsonLinesItemExporter',\n-    'jl': 'scrapy.exporters.JsonLinesItemExporter',\n-    'csv': 'scrapy.exporters.CsvItemExporter',\n-    'xml': 'scrapy.exporters.XmlItemExporter',\n-    'marshal': 'scrapy.exporters.MarshalItemExporter',\n-    'pickle': 'scrapy.exporters.PickleItemExporter',\n+    \"json\": \"scrapy.exporters.JsonItemExporter\",\n+    \"jsonlines\": \"scrapy.exporters.JsonLinesItemExporter\",\n+    \"jsonl\": \"scrapy.exporters.JsonLinesItemExporter\",\n+    \"jl\": \"scrapy.exporters.JsonLinesItemExporter\",\n+    \"csv\": \"scrapy.exporters.CsvItemExporter\",\n+    \"xml\": \"scrapy.exporters.XmlItemExporter\",\n+    \"marshal\": \"scrapy.exporters.MarshalItemExporter\",\n+    \"pickle\": \"scrapy.exporters.PickleItemExporter\",\n }\n FEED_EXPORT_INDENT = 0\n \n FEED_STORAGE_FTP_ACTIVE = False\n-FEED_STORAGE_GCS_ACL = ''\n-FEED_STORAGE_S3_ACL = ''\n+FEED_STORAGE_GCS_ACL = \"\"\n+FEED_STORAGE_S3_ACL = \"\"\n \n-FILES_STORE_S3_ACL = 'private'\n-FILES_STORE_GCS_ACL = ''\n+FILES_STORE_S3_ACL = \"private\"\n+FILES_STORE_GCS_ACL = \"\"\n \n-FTP_USER = 'anonymous'\n-FTP_PASSWORD = 'guest'\n+FTP_USER = \"anonymous\"\n+FTP_PASSWORD = \"guest\"\n FTP_PASSIVE_MODE = True\n \n GCS_PROJECT_ID = None\n \n HTTPCACHE_ENABLED = False\n-HTTPCACHE_DIR = 'httpcache'\n+HTTPCACHE_DIR = \"httpcache\"\n HTTPCACHE_IGNORE_MISSING = False\n-HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n+HTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n HTTPCACHE_EXPIRATION_SECS = 0\n HTTPCACHE_ALWAYS_STORE = False\n HTTPCACHE_IGNORE_HTTP_CODES = []\n-HTTPCACHE_IGNORE_SCHEMES = ['file']\n+HTTPCACHE_IGNORE_SCHEMES = [\"file\"]\n HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS = []\n-HTTPCACHE_DBM_MODULE = 'dbm'\n-HTTPCACHE_POLICY = 'scrapy.extensions.httpcache.DummyPolicy'\n+HTTPCACHE_DBM_MODULE = \"dbm\"\n+HTTPCACHE_POLICY = \"scrapy.extensions.httpcache.DummyPolicy\"\n HTTPCACHE_GZIP = False\n \n HTTPPROXY_ENABLED = True\n-HTTPPROXY_AUTH_ENCODING = 'latin-1'\n+HTTPPROXY_AUTH_ENCODING = \"latin-1\"\n \n-IMAGES_STORE_S3_ACL = 'private'\n-IMAGES_STORE_GCS_ACL = ''\n+IMAGES_STORE_S3_ACL = \"private\"\n+IMAGES_STORE_GCS_ACL = \"\"\n \n-ITEM_PROCESSOR = 'scrapy.pipelines.ItemPipelineManager'\n+ITEM_PROCESSOR = \"scrapy.pipelines.ItemPipelineManager\"\n \n ITEM_PIPELINES = {}\n ITEM_PIPELINES_BASE = {}\n \n LOG_ENABLED = True\n-LOG_ENCODING = 'utf-8'\n-LOG_FORMATTER = 'scrapy.logformatter.LogFormatter'\n-LOG_FORMAT = '%(asctime)s [%(name)s] %(levelname)s: %(message)s'\n-LOG_DATEFORMAT = '%Y-%m-%d %H:%M:%S'\n+LOG_ENCODING = \"utf-8\"\n+LOG_FORMATTER = \"scrapy.logformatter.LogFormatter\"\n+LOG_FORMAT = \"%(asctime)s [%(name)s] %(levelname)s: %(message)s\"\n+LOG_DATEFORMAT = \"%Y-%m-%d %H:%M:%S\"\n LOG_STDOUT = False\n-LOG_LEVEL = 'DEBUG'\n+LOG_LEVEL = \"DEBUG\"\n LOG_FILE = None\n LOG_FILE_APPEND = True\n LOG_SHORT_NAMES = False\n@@ -215,9 +219,9 @@ SCHEDULER_DEBUG = False\n \n LOGSTATS_INTERVAL = 60.0\n \n-MAIL_HOST = 'localhost'\n+MAIL_HOST = \"localhost\"\n MAIL_PORT = 25\n-MAIL_FROM = 'scrapy@localhost'\n+MAIL_FROM = \"scrapy@localhost\"\n MAIL_PASS = None\n MAIL_USER = None\n \n@@ -234,7 +238,7 @@ METAREFRESH_ENABLED = True\n METAREFRESH_IGNORE_TAGS = []\n METAREFRESH_MAXDELAY = 100\n \n-NEWSPIDER_MODULE = ''\n+NEWSPIDER_MODULE = \"\"\n \n RANDOMIZE_DOWNLOAD_DELAY = True\n \n@@ -245,10 +249,10 @@ REDIRECT_MAX_TIMES = 20  # uses Firefox default setting\n REDIRECT_PRIORITY_ADJUST = +2\n \n REFERER_ENABLED = True\n-REFERRER_POLICY = 'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy'\n+REFERRER_POLICY = \"scrapy.spidermiddlewares.referer.DefaultReferrerPolicy\"\n \n-REQUEST_FINGERPRINTER_CLASS = 'scrapy.utils.request.RequestFingerprinter'\n-REQUEST_FINGERPRINTER_IMPLEMENTATION = '2.6'\n+REQUEST_FINGERPRINTER_CLASS = \"scrapy.utils.request.RequestFingerprinter\"\n+REQUEST_FINGERPRINTER_IMPLEMENTATION = \"2.6\"\n \n RETRY_ENABLED = True\n RETRY_TIMES = 2  # initial response + 2 retries = 3 requests\n@@ -256,39 +260,39 @@ RETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408, 429]\n RETRY_PRIORITY_ADJUST = -1\n \n ROBOTSTXT_OBEY = False\n-ROBOTSTXT_PARSER = 'scrapy.robotstxt.ProtegoRobotParser'\n+ROBOTSTXT_PARSER = \"scrapy.robotstxt.ProtegoRobotParser\"\n ROBOTSTXT_USER_AGENT = None\n \n-SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n-SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleLifoDiskQueue'\n-SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.LifoMemoryQueue'\n-SCHEDULER_PRIORITY_QUEUE = 'scrapy.pqueues.ScrapyPriorityQueue'\n+SCHEDULER = \"scrapy.core.scheduler.Scheduler\"\n+SCHEDULER_DISK_QUEUE = \"scrapy.squeues.PickleLifoDiskQueue\"\n+SCHEDULER_MEMORY_QUEUE = \"scrapy.squeues.LifoMemoryQueue\"\n+SCHEDULER_PRIORITY_QUEUE = \"scrapy.pqueues.ScrapyPriorityQueue\"\n \n SCRAPER_SLOT_MAX_ACTIVE_SIZE = 5000000\n \n-SPIDER_LOADER_CLASS = 'scrapy.spiderloader.SpiderLoader'\n+SPIDER_LOADER_CLASS = \"scrapy.spiderloader.SpiderLoader\"\n SPIDER_LOADER_WARN_ONLY = False\n \n SPIDER_MIDDLEWARES = {}\n \n SPIDER_MIDDLEWARES_BASE = {\n     # Engine side\n-    'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware': 50,\n-    'scrapy.spidermiddlewares.offsite.OffsiteMiddleware': 500,\n-    'scrapy.spidermiddlewares.referer.RefererMiddleware': 700,\n-    'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware': 800,\n-    'scrapy.spidermiddlewares.depth.DepthMiddleware': 900,\n+    \"scrapy.spidermiddlewares.httperror.HttpErrorMiddleware\": 50,\n+    \"scrapy.spidermiddlewares.offsite.OffsiteMiddleware\": 500,\n+    \"scrapy.spidermiddlewares.referer.RefererMiddleware\": 700,\n+    \"scrapy.spidermiddlewares.urllength.UrlLengthMiddleware\": 800,\n+    \"scrapy.spidermiddlewares.depth.DepthMiddleware\": 900,\n     # Spider side\n }\n \n SPIDER_MODULES = []\n \n-STATS_CLASS = 'scrapy.statscollectors.MemoryStatsCollector'\n+STATS_CLASS = \"scrapy.statscollectors.MemoryStatsCollector\"\n STATS_DUMP = True\n \n STATSMAILER_RCPTS = []\n \n-TEMPLATES_DIR = str((Path(__file__).parent / '..' / 'templates').resolve())\n+TEMPLATES_DIR = str((Path(__file__).parent / \"..\" / \"templates\").resolve())\n \n URLLENGTH_LIMIT = 2083\n \n@@ -296,16 +300,16 @@ USER_AGENT = f'Scrapy/{import_module(\"scrapy\").__version__} (+https://scrapy.org\n \n TELNETCONSOLE_ENABLED = 1\n TELNETCONSOLE_PORT = [6023, 6073]\n-TELNETCONSOLE_HOST = '127.0.0.1'\n-TELNETCONSOLE_USERNAME = 'scrapy'\n+TELNETCONSOLE_HOST = \"127.0.0.1\"\n+TELNETCONSOLE_USERNAME = \"scrapy\"\n TELNETCONSOLE_PASSWORD = None\n \n TWISTED_REACTOR = None\n \n SPIDER_CONTRACTS = {}\n SPIDER_CONTRACTS_BASE = {\n-    'scrapy.contracts.default.UrlContract': 1,\n-    'scrapy.contracts.default.CallbackKeywordArgumentsContract': 1,\n-    'scrapy.contracts.default.ReturnsContract': 2,\n-    'scrapy.contracts.default.ScrapesContract': 3,\n+    \"scrapy.contracts.default.UrlContract\": 1,\n+    \"scrapy.contracts.default.CallbackKeywordArgumentsContract\": 1,\n+    \"scrapy.contracts.default.ReturnsContract\": 2,\n+    \"scrapy.contracts.default.ScrapesContract\": 3,\n }\n\n@@ -30,7 +30,7 @@ class Shell:\n     def __init__(self, crawler, update_vars=None, code=None):\n         self.crawler = crawler\n         self.update_vars = update_vars or (lambda x: None)\n-        self.item_class = load_object(crawler.settings['DEFAULT_ITEM_CLASS'])\n+        self.item_class = load_object(crawler.settings[\"DEFAULT_ITEM_CLASS\"])\n         self.spider = None\n         self.inthread = not threadable.isInIOThread()\n         self.code = code\n@@ -61,19 +61,20 @@ class Shell:\n             shell = python\n             \"\"\"\n             cfg = get_config()\n-            section, option = 'settings', 'shell'\n-            env = os.environ.get('SCRAPY_PYTHON_SHELL')\n+            section, option = \"settings\", \"shell\"\n+            env = os.environ.get(\"SCRAPY_PYTHON_SHELL\")\n             shells = []\n             if env:\n-                shells += env.strip().lower().split(',')\n+                shells += env.strip().lower().split(\",\")\n             elif cfg.has_option(section, option):\n                 shells += [cfg.get(section, option).strip().lower()]\n             else:  # try all by default\n                 shells += DEFAULT_PYTHON_SHELLS.keys()\n             # always add standard shell as fallback\n-            shells += ['python']\n-            start_python_console(self.vars, shells=shells,\n-                                 banner=self.vars.pop('banner', ''))\n+            shells += [\"python\"]\n+            start_python_console(\n+                self.vars, shells=shells, banner=self.vars.pop(\"banner\", \"\")\n+            )\n \n     def _schedule(self, request, spider):\n         spider = self._open_spider(request, spider)\n@@ -96,19 +97,23 @@ class Shell:\n \n     def fetch(self, request_or_url, spider=None, redirect=True, **kwargs):\n         from twisted.internet import reactor\n+\n         if isinstance(request_or_url, Request):\n             request = request_or_url\n         else:\n             url = any_to_uri(request_or_url)\n             request = Request(url, dont_filter=True, **kwargs)\n             if redirect:\n-                request.meta['handle_httpstatus_list'] = SequenceExclude(range(300, 400))\n+                request.meta[\"handle_httpstatus_list\"] = SequenceExclude(\n+                    range(300, 400)\n+                )\n             else:\n-                request.meta['handle_httpstatus_all'] = True\n+                request.meta[\"handle_httpstatus_all\"] = True\n         response = None\n         try:\n             response, spider = threads.blockingCallFromThread(\n-                reactor, self._schedule, request, spider)\n+                reactor, self._schedule, request, spider\n+            )\n         except IgnoreRequest:\n             pass\n         self.populate_vars(response, request, spider)\n@@ -116,20 +121,20 @@ class Shell:\n     def populate_vars(self, response=None, request=None, spider=None):\n         import scrapy\n \n-        self.vars['scrapy'] = scrapy\n-        self.vars['crawler'] = self.crawler\n-        self.vars['item'] = self.item_class()\n-        self.vars['settings'] = self.crawler.settings\n-        self.vars['spider'] = spider\n-        self.vars['request'] = request\n-        self.vars['response'] = response\n+        self.vars[\"scrapy\"] = scrapy\n+        self.vars[\"crawler\"] = self.crawler\n+        self.vars[\"item\"] = self.item_class()\n+        self.vars[\"settings\"] = self.crawler.settings\n+        self.vars[\"spider\"] = spider\n+        self.vars[\"request\"] = request\n+        self.vars[\"response\"] = response\n         if self.inthread:\n-            self.vars['fetch'] = self.fetch\n-        self.vars['view'] = open_in_browser\n-        self.vars['shelp'] = self.print_help\n+            self.vars[\"fetch\"] = self.fetch\n+        self.vars[\"view\"] = open_in_browser\n+        self.vars[\"shelp\"] = self.print_help\n         self.update_vars(self.vars)\n         if not self.code:\n-            self.vars['banner'] = self.get_help()\n+            self.vars[\"banner\"] = self.get_help()\n \n     def print_help(self):\n         print(self.get_help())\n@@ -137,16 +142,22 @@ class Shell:\n     def get_help(self):\n         b = []\n         b.append(\"Available Scrapy objects:\")\n-        b.append(\"  scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\")\n+        b.append(\n+            \"  scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\"\n+        )\n         for k, v in sorted(self.vars.items()):\n             if self._is_relevant(v):\n                 b.append(f\"  {k:<10} {v}\")\n         b.append(\"Useful shortcuts:\")\n         if self.inthread:\n-            b.append(\"  fetch(url[, redirect=True]) \"\n-                     \"Fetch URL and update local objects (by default, redirects are followed)\")\n-            b.append(\"  fetch(req)                  \"\n-                     \"Fetch a scrapy.Request and update local objects \")\n+            b.append(\n+                \"  fetch(url[, redirect=True]) \"\n+                \"Fetch URL and update local objects (by default, redirects are followed)\"\n+            )\n+            b.append(\n+                \"  fetch(req)                  \"\n+                \"Fetch a scrapy.Request and update local objects \"\n+            )\n         b.append(\"  shelp()           Shell help (print this help)\")\n         b.append(\"  view(response)    View response in a browser\")\n \n\n@@ -3,7 +3,6 @@ from scrapy.utils import signal as _signal\n \n \n class SignalManager:\n-\n     def __init__(self, sender=dispatcher.Anonymous):\n         self.sender = sender\n \n@@ -21,7 +20,7 @@ class SignalManager:\n         :param signal: the signal to connect to\n         :type signal: object\n         \"\"\"\n-        kwargs.setdefault('sender', self.sender)\n+        kwargs.setdefault(\"sender\", self.sender)\n         return dispatcher.connect(receiver, signal, **kwargs)\n \n     def disconnect(self, receiver, signal, **kwargs):\n@@ -30,7 +29,7 @@ class SignalManager:\n         opposite effect of the :meth:`connect` method, and the arguments\n         are the same.\n         \"\"\"\n-        kwargs.setdefault('sender', self.sender)\n+        kwargs.setdefault(\"sender\", self.sender)\n         return dispatcher.disconnect(receiver, signal, **kwargs)\n \n     def send_catch_log(self, signal, **kwargs):\n@@ -40,7 +39,7 @@ class SignalManager:\n         The keyword arguments are passed to the signal handlers (connected\n         through the :meth:`connect` method).\n         \"\"\"\n-        kwargs.setdefault('sender', self.sender)\n+        kwargs.setdefault(\"sender\", self.sender)\n         return _signal.send_catch_log(signal, **kwargs)\n \n     def send_catch_log_deferred(self, signal, **kwargs):\n@@ -54,7 +53,7 @@ class SignalManager:\n         The keyword arguments are passed to the signal handlers (connected\n         through the :meth:`connect` method).\n         \"\"\"\n-        kwargs.setdefault('sender', self.sender)\n+        kwargs.setdefault(\"sender\", self.sender)\n         return _signal.send_catch_log_deferred(signal, **kwargs)\n \n     def disconnect_all(self, signal, **kwargs):\n@@ -64,5 +63,5 @@ class SignalManager:\n         :param signal: the signal to disconnect from\n         :type signal: object\n         \"\"\"\n-        kwargs.setdefault('sender', self.sender)\n+        kwargs.setdefault(\"sender\", self.sender)\n         return _signal.disconnect_all(signal, **kwargs)\n\n@@ -17,8 +17,8 @@ class SpiderLoader:\n     \"\"\"\n \n     def __init__(self, settings):\n-        self.spider_modules = settings.getlist('SPIDER_MODULES')\n-        self.warn_only = settings.getbool('SPIDER_LOADER_WARN_ONLY')\n+        self.spider_modules = settings.getlist(\"SPIDER_MODULES\")\n+        self.warn_only = settings.getbool(\"SPIDER_LOADER_WARN_ONLY\")\n         self._spiders = {}\n         self._found = defaultdict(list)\n         self._load_all_spiders()\n@@ -26,11 +26,13 @@ class SpiderLoader:\n     def _check_name_duplicates(self):\n         dupes = []\n         for name, locations in self._found.items():\n-            dupes.extend([\n+            dupes.extend(\n+                [\n                     f\"  {cls} named {name!r} (in {mod})\"\n                     for mod, cls in locations\n                     if len(locations) > 1\n-            ])\n+                ]\n+            )\n \n         if dupes:\n             dupes_string = \"\\n\\n\".join(dupes)\n@@ -81,8 +83,7 @@ class SpiderLoader:\n         Return the list of spider names that can handle the given request.\n         \"\"\"\n         return [\n-            name for name, cls in self._spiders.items()\n-            if cls.handles_request(request)\n+            name for name, cls in self._spiders.items() if cls.handles_request(request)\n         ]\n \n     def list(self):\n\n@@ -12,7 +12,6 @@ logger = logging.getLogger(__name__)\n \n \n class DepthMiddleware:\n-\n     def __init__(self, maxdepth, stats, verbose_stats=False, prio=1):\n         self.maxdepth = maxdepth\n         self.stats = stats\n@@ -22,9 +21,9 @@ class DepthMiddleware:\n     @classmethod\n     def from_crawler(cls, crawler):\n         settings = crawler.settings\n-        maxdepth = settings.getint('DEPTH_LIMIT')\n-        verbose = settings.getbool('DEPTH_STATS_VERBOSE')\n-        prio = settings.getint('DEPTH_PRIORITY')\n+        maxdepth = settings.getint(\"DEPTH_LIMIT\")\n+        verbose = settings.getbool(\"DEPTH_STATS_VERBOSE\")\n+        prio = settings.getint(\"DEPTH_PRIORITY\")\n         return cls(maxdepth, crawler.stats, verbose, prio)\n \n     def process_spider_output(self, response, result, spider):\n@@ -39,28 +38,26 @@ class DepthMiddleware:\n \n     def _init_depth(self, response, spider):\n         # base case (depth=0)\n-        if 'depth' not in response.meta:\n-            response.meta['depth'] = 0\n+        if \"depth\" not in response.meta:\n+            response.meta[\"depth\"] = 0\n             if self.verbose_stats:\n-                self.stats.inc_value('request_depth_count/0', spider=spider)\n+                self.stats.inc_value(\"request_depth_count/0\", spider=spider)\n \n     def _filter(self, request, response, spider):\n         if not isinstance(request, Request):\n             return True\n-        depth = response.meta['depth'] + 1\n-        request.meta['depth'] = depth\n+        depth = response.meta[\"depth\"] + 1\n+        request.meta[\"depth\"] = depth\n         if self.prio:\n             request.priority -= depth * self.prio\n         if self.maxdepth and depth > self.maxdepth:\n             logger.debug(\n                 \"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n-                {'maxdepth': self.maxdepth, 'requrl': request.url},\n-                extra={'spider': spider}\n+                {\"maxdepth\": self.maxdepth, \"requrl\": request.url},\n+                extra={\"spider\": spider},\n             )\n             return False\n         if self.verbose_stats:\n-            self.stats.inc_value(f'request_depth_count/{depth}',\n-                                 spider=spider)\n-        self.stats.max_value('request_depth_max', depth,\n-                             spider=spider)\n+            self.stats.inc_value(f\"request_depth_count/{depth}\", spider=spider)\n+        self.stats.max_value(\"request_depth_max\", depth, spider=spider)\n         return True\n\n@@ -19,39 +19,41 @@ class HttpError(IgnoreRequest):\n \n \n class HttpErrorMiddleware:\n-\n     @classmethod\n     def from_crawler(cls, crawler):\n         return cls(crawler.settings)\n \n     def __init__(self, settings):\n-        self.handle_httpstatus_all = settings.getbool('HTTPERROR_ALLOW_ALL')\n-        self.handle_httpstatus_list = settings.getlist('HTTPERROR_ALLOWED_CODES')\n+        self.handle_httpstatus_all = settings.getbool(\"HTTPERROR_ALLOW_ALL\")\n+        self.handle_httpstatus_list = settings.getlist(\"HTTPERROR_ALLOWED_CODES\")\n \n     def process_spider_input(self, response, spider):\n         if 200 <= response.status < 300:  # common case\n             return\n         meta = response.meta\n-        if meta.get('handle_httpstatus_all', False):\n+        if meta.get(\"handle_httpstatus_all\", False):\n             return\n-        if 'handle_httpstatus_list' in meta:\n-            allowed_statuses = meta['handle_httpstatus_list']\n+        if \"handle_httpstatus_list\" in meta:\n+            allowed_statuses = meta[\"handle_httpstatus_list\"]\n         elif self.handle_httpstatus_all:\n             return\n         else:\n-            allowed_statuses = getattr(spider, 'handle_httpstatus_list', self.handle_httpstatus_list)\n+            allowed_statuses = getattr(\n+                spider, \"handle_httpstatus_list\", self.handle_httpstatus_list\n+            )\n         if response.status in allowed_statuses:\n             return\n-        raise HttpError(response, 'Ignoring non-200 response')\n+        raise HttpError(response, \"Ignoring non-200 response\")\n \n     def process_spider_exception(self, response, exception, spider):\n         if isinstance(exception, HttpError):\n-            spider.crawler.stats.inc_value('httperror/response_ignored_count')\n+            spider.crawler.stats.inc_value(\"httperror/response_ignored_count\")\n             spider.crawler.stats.inc_value(\n-                f'httperror/response_ignored_status_count/{response.status}'\n+                f\"httperror/response_ignored_status_count/{response.status}\"\n             )\n             logger.info(\n                 \"Ignoring response %(response)r: HTTP status code is not handled or not allowed\",\n-                {'response': response}, extra={'spider': spider},\n+                {\"response\": response},\n+                extra={\"spider\": spider},\n             )\n             return []\n\n@@ -15,7 +15,6 @@ logger = logging.getLogger(__name__)\n \n \n class OffsiteMiddleware:\n-\n     def __init__(self, stats):\n         self.stats = stats\n \n@@ -43,22 +42,24 @@ class OffsiteMiddleware:\n             self.domains_seen.add(domain)\n             logger.debug(\n                 \"Filtered offsite request to %(domain)r: %(request)s\",\n-                {'domain': domain, 'request': request}, extra={'spider': spider})\n-            self.stats.inc_value('offsite/domains', spider=spider)\n-        self.stats.inc_value('offsite/filtered', spider=spider)\n+                {\"domain\": domain, \"request\": request},\n+                extra={\"spider\": spider},\n+            )\n+            self.stats.inc_value(\"offsite/domains\", spider=spider)\n+        self.stats.inc_value(\"offsite/filtered\", spider=spider)\n         return False\n \n     def should_follow(self, request, spider):\n         regex = self.host_regex\n         # hostname can be None for wrong urls (like javascript links)\n-        host = urlparse_cached(request).hostname or ''\n+        host = urlparse_cached(request).hostname or \"\"\n         return bool(regex.search(host))\n \n     def get_host_regex(self, spider):\n         \"\"\"Override this method to implement a different offsite policy\"\"\"\n-        allowed_domains = getattr(spider, 'allowed_domains', None)\n+        allowed_domains = getattr(spider, \"allowed_domains\", None)\n         if not allowed_domains:\n-            return re.compile('')  # allow all by default\n+            return re.compile(\"\")  # allow all by default\n         url_pattern = re.compile(r\"^https?://.*$\")\n         port_pattern = re.compile(r\":\\d+$\")\n         domains = []\n@@ -66,16 +67,20 @@ class OffsiteMiddleware:\n             if domain is None:\n                 continue\n             if url_pattern.match(domain):\n-                message = (\"allowed_domains accepts only domains, not URLs. \"\n-                           f\"Ignoring URL entry {domain} in allowed_domains.\")\n+                message = (\n+                    \"allowed_domains accepts only domains, not URLs. \"\n+                    f\"Ignoring URL entry {domain} in allowed_domains.\"\n+                )\n                 warnings.warn(message, URLWarning)\n             elif port_pattern.search(domain):\n-                message = (\"allowed_domains accepts only domains without ports. \"\n-                           f\"Ignoring entry {domain} in allowed_domains.\")\n+                message = (\n+                    \"allowed_domains accepts only domains without ports. \"\n+                    f\"Ignoring entry {domain} in allowed_domains.\"\n+                )\n                 warnings.warn(message, PortWarning)\n             else:\n                 domains.append(re.escape(domain))\n-        regex = fr'^(.*\\.)?({\"|\".join(domains)})$'\n+        regex = rf'^(.*\\.)?({\"|\".join(domains)})$'\n         return re.compile(regex)\n \n     def spider_opened(self, spider):\n\n@@ -16,7 +16,12 @@ from scrapy.utils.python import to_unicode\n from scrapy.utils.url import strip_url\n \n \n-LOCAL_SCHEMES = ('about', 'blob', 'data', 'filesystem',)\n+LOCAL_SCHEMES = (\n+    \"about\",\n+    \"blob\",\n+    \"data\",\n+    \"filesystem\",\n+)\n \n POLICY_NO_REFERRER = \"no-referrer\"\n POLICY_NO_REFERRER_WHEN_DOWNGRADE = \"no-referrer-when-downgrade\"\n@@ -61,11 +66,13 @@ class ReferrerPolicy:\n         \"\"\"\n         if not url:\n             return None\n-        return strip_url(url,\n+        return strip_url(\n+            url,\n             strip_credentials=True,\n             strip_fragment=True,\n             strip_default_port=True,\n-                         origin_only=origin_only)\n+            origin_only=origin_only,\n+        )\n \n     def origin(self, url):\n         \"\"\"Return serialized origin (scheme, host, path) for a request or response URL.\"\"\"\n@@ -74,12 +81,12 @@ class ReferrerPolicy:\n     def potentially_trustworthy(self, url):\n         # Note: this does not follow https://w3c.github.io/webappsec-secure-contexts/#is-url-trustworthy\n         parsed_url = urlparse(url)\n-        if parsed_url.scheme in ('data',):\n+        if parsed_url.scheme in (\"data\",):\n             return False\n         return self.tls_protected(url)\n \n     def tls_protected(self, url):\n-        return urlparse(url).scheme in ('https', 'ftps')\n+        return urlparse(url).scheme in (\"https\", \"ftps\")\n \n \n class NoReferrerPolicy(ReferrerPolicy):\n@@ -90,6 +97,7 @@ class NoReferrerPolicy(ReferrerPolicy):\n     is to be sent along with requests made from a particular request client to any origin.\n     The header will be omitted entirely.\n     \"\"\"\n+\n     name: str = POLICY_NO_REFERRER\n \n     def referrer(self, response_url, request_url):\n@@ -110,6 +118,7 @@ class NoReferrerWhenDowngradePolicy(ReferrerPolicy):\n \n     This is a user agent's default behavior, if no policy is otherwise specified.\n     \"\"\"\n+\n     name: str = POLICY_NO_REFERRER_WHEN_DOWNGRADE\n \n     def referrer(self, response_url, request_url):\n@@ -127,6 +136,7 @@ class SameOriginPolicy(ReferrerPolicy):\n     Cross-origin requests, on the other hand, will contain no referrer information.\n     A Referer HTTP header will not be sent.\n     \"\"\"\n+\n     name: str = POLICY_SAME_ORIGIN\n \n     def referrer(self, response_url, request_url):\n@@ -143,6 +153,7 @@ class OriginPolicy(ReferrerPolicy):\n     when making both same-origin requests and cross-origin requests\n     from a particular request client.\n     \"\"\"\n+\n     name: str = POLICY_ORIGIN\n \n     def referrer(self, response_url, request_url):\n@@ -162,11 +173,13 @@ class StrictOriginPolicy(ReferrerPolicy):\n     on the other hand, will contain no referrer information.\n     A Referer HTTP header will not be sent.\n     \"\"\"\n+\n     name: str = POLICY_STRICT_ORIGIN\n \n     def referrer(self, response_url, request_url):\n         if (\n-            self.tls_protected(response_url) and self.potentially_trustworthy(request_url)\n+            self.tls_protected(response_url)\n+            and self.potentially_trustworthy(request_url)\n             or not self.tls_protected(response_url)\n         ):\n             return self.origin_referrer(response_url)\n@@ -183,6 +196,7 @@ class OriginWhenCrossOriginPolicy(ReferrerPolicy):\n     is sent as referrer information when making cross-origin requests\n     from a particular request client.\n     \"\"\"\n+\n     name: str = POLICY_ORIGIN_WHEN_CROSS_ORIGIN\n \n     def referrer(self, response_url, request_url):\n@@ -209,6 +223,7 @@ class StrictOriginWhenCrossOriginPolicy(ReferrerPolicy):\n     on the other hand, will contain no referrer information.\n     A Referer HTTP header will not be sent.\n     \"\"\"\n+\n     name: str = POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN\n \n     def referrer(self, response_url, request_url):\n@@ -216,7 +231,8 @@ class StrictOriginWhenCrossOriginPolicy(ReferrerPolicy):\n         if origin == self.origin(request_url):\n             return self.stripped_referrer(response_url)\n         if (\n-            self.tls_protected(response_url) and self.potentially_trustworthy(request_url)\n+            self.tls_protected(response_url)\n+            and self.potentially_trustworthy(request_url)\n             or not self.tls_protected(response_url)\n         ):\n             return self.origin_referrer(response_url)\n@@ -235,6 +251,7 @@ class UnsafeUrlPolicy(ReferrerPolicy):\n     to insecure origins.\n     Carefully consider the impact of setting such a policy for potentially sensitive documents.\n     \"\"\"\n+\n     name: str = POLICY_UNSAFE_URL\n \n     def referrer(self, response_url, request_url):\n@@ -247,11 +264,14 @@ class DefaultReferrerPolicy(NoReferrerWhenDowngradePolicy):\n     with the addition that \"Referer\" is not sent if the parent request was\n     using ``file://`` or ``s3://`` scheme.\n     \"\"\"\n-    NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES + ('file', 's3')\n+\n+    NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES + (\"file\", \"s3\")\n     name: str = POLICY_SCRAPY_DEFAULT\n \n \n-_policy_classes = {p.name: p for p in (\n+_policy_classes = {\n+    p.name: p\n+    for p in (\n         NoReferrerPolicy,\n         NoReferrerWhenDowngradePolicy,\n         SameOriginPolicy,\n@@ -261,10 +281,11 @@ _policy_classes = {p.name: p for p in (\n         StrictOriginWhenCrossOriginPolicy,\n         UnsafeUrlPolicy,\n         DefaultReferrerPolicy,\n-)}\n+    )\n+}\n \n # Reference: https://www.w3.org/TR/referrer-policy/#referrer-policy-empty-string\n-_policy_classes[''] = NoReferrerWhenDowngradePolicy\n+_policy_classes[\"\"] = NoReferrerWhenDowngradePolicy\n \n \n def _load_policy_class(policy, warning_only=False):\n@@ -288,16 +309,14 @@ def _load_policy_class(policy, warning_only=False):\n \n \n class RefererMiddleware:\n-\n     def __init__(self, settings=None):\n         self.default_policy = DefaultReferrerPolicy\n         if settings is not None:\n-            self.default_policy = _load_policy_class(\n-                settings.get('REFERRER_POLICY'))\n+            self.default_policy = _load_policy_class(settings.get(\"REFERRER_POLICY\"))\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        if not crawler.settings.getbool('REFERER_ENABLED'):\n+        if not crawler.settings.getbool(\"REFERER_ENABLED\"):\n             raise NotConfigured\n         mw = cls(crawler.settings)\n \n@@ -319,12 +338,12 @@ class RefererMiddleware:\n           it is used if valid\n         - otherwise, the policy from settings is used.\n         \"\"\"\n-        policy_name = request.meta.get('referrer_policy')\n+        policy_name = request.meta.get(\"referrer_policy\")\n         if policy_name is None:\n             if isinstance(resp_or_url, Response):\n-                policy_header = resp_or_url.headers.get('Referrer-Policy')\n+                policy_header = resp_or_url.headers.get(\"Referrer-Policy\")\n                 if policy_header is not None:\n-                    policy_name = to_unicode(policy_header.decode('latin1'))\n+                    policy_name = to_unicode(policy_header.decode(\"latin1\"))\n         if policy_name is None:\n             return self.default_policy()\n \n@@ -342,14 +361,14 @@ class RefererMiddleware:\n         if isinstance(r, Request):\n             referrer = self.policy(response, r).referrer(response.url, r.url)\n             if referrer is not None:\n-                r.headers.setdefault('Referer', referrer)\n+                r.headers.setdefault(\"Referer\", referrer)\n         return r\n \n     def request_scheduled(self, request, spider):\n         # check redirected request to patch \"Referer\" header if necessary\n-        redirected_urls = request.meta.get('redirect_urls', [])\n+        redirected_urls = request.meta.get(\"redirect_urls\", [])\n         if redirected_urls:\n-            request_referrer = request.headers.get('Referer')\n+            request_referrer = request.headers.get(\"Referer\")\n             # we don't patch the referrer value if there is none\n             if request_referrer is not None:\n                 # the request's referrer header value acts as a surrogate\n@@ -359,9 +378,10 @@ class RefererMiddleware:\n                 #       the information is not available using this hook\n                 parent_url = safe_url_string(request_referrer)\n                 policy_referrer = self.policy(parent_url, request).referrer(\n-                    parent_url, request.url)\n+                    parent_url, request.url\n+                )\n                 if policy_referrer != request_referrer:\n                     if policy_referrer is None:\n-                        request.headers.pop('Referer')\n+                        request.headers.pop(\"Referer\")\n                     else:\n-                        request.headers['Referer'] = policy_referrer\n+                        request.headers[\"Referer\"] = policy_referrer\n\n@@ -13,13 +13,12 @@ logger = logging.getLogger(__name__)\n \n \n class UrlLengthMiddleware:\n-\n     def __init__(self, maxlength):\n         self.maxlength = maxlength\n \n     @classmethod\n     def from_settings(cls, settings):\n-        maxlength = settings.getint('URLLENGTH_LIMIT')\n+        maxlength = settings.getint(\"URLLENGTH_LIMIT\")\n         if not maxlength:\n             raise NotConfigured\n         return cls(maxlength)\n@@ -36,9 +35,11 @@ class UrlLengthMiddleware:\n         if isinstance(request, Request) and len(request.url) > self.maxlength:\n             logger.info(\n                 \"Ignoring link (url length > %(maxlength)d): %(url)s \",\n-                {'maxlength': self.maxlength, 'url': request.url},\n-                extra={'spider': spider}\n+                {\"maxlength\": self.maxlength, \"url\": request.url},\n+                extra={\"spider\": spider},\n+            )\n+            spider.crawler.stats.inc_value(\n+                \"urllength/request_ignored_count\", spider=spider\n             )\n-            spider.crawler.stats.inc_value('urllength/request_ignored_count', spider=spider)\n             return False\n         return True\n\n@@ -28,16 +28,16 @@ class Spider(object_ref):\n     def __init__(self, name=None, **kwargs):\n         if name is not None:\n             self.name = name\n-        elif not getattr(self, 'name', None):\n+        elif not getattr(self, \"name\", None):\n             raise ValueError(f\"{type(self).__name__} must have a name\")\n         self.__dict__.update(kwargs)\n-        if not hasattr(self, 'start_urls'):\n+        if not hasattr(self, \"start_urls\"):\n             self.start_urls = []\n \n     @property\n     def logger(self):\n         logger = logging.getLogger(self.name)\n-        return logging.LoggerAdapter(logger, {'spider': self})\n+        return logging.LoggerAdapter(logger, {\"spider\": self})\n \n     def log(self, message, level=logging.DEBUG, **kw):\n         \"\"\"Log the given message at the given log level\n@@ -60,11 +60,12 @@ class Spider(object_ref):\n         crawler.signals.connect(self.close, signals.spider_closed)\n \n     def start_requests(self):\n-        if not self.start_urls and hasattr(self, 'start_url'):\n+        if not self.start_urls and hasattr(self, \"start_url\"):\n             raise AttributeError(\n                 \"Crawling could not start: 'start_urls' not found \"\n                 \"or empty (but found 'start_url' attribute instead, \"\n-                \"did you miss an 's'?)\")\n+                \"did you miss an 's'?)\"\n+            )\n         for url in self.start_urls:\n             yield Request(url, dont_filter=True)\n \n@@ -72,11 +73,13 @@ class Spider(object_ref):\n         return self.parse(response, **kwargs)\n \n     def parse(self, response, **kwargs):\n-        raise NotImplementedError(f'{self.__class__.__name__}.parse callback is not defined')\n+        raise NotImplementedError(\n+            f\"{self.__class__.__name__}.parse callback is not defined\"\n+        )\n \n     @classmethod\n     def update_settings(cls, settings):\n-        settings.setdict(cls.custom_settings or {}, priority='spider')\n+        settings.setdict(cls.custom_settings or {}, priority=\"spider\")\n \n     @classmethod\n     def handles_request(cls, request):\n@@ -84,7 +87,7 @@ class Spider(object_ref):\n \n     @staticmethod\n     def close(spider, reason):\n-        closed = getattr(spider, 'closed', None)\n+        closed = getattr(spider, \"closed\", None)\n         if callable(closed):\n             return closed(reason)\n \n\n@@ -34,7 +34,6 @@ _default_link_extractor = LinkExtractor()\n \n \n class Rule:\n-\n     def __init__(\n         self,\n         link_extractor=None,\n@@ -95,19 +94,24 @@ class CrawlSpider(Spider):\n             return\n         seen = set()\n         for rule_index, rule in enumerate(self._rules):\n-            links = [lnk for lnk in rule.link_extractor.extract_links(response)\n-                     if lnk not in seen]\n+            links = [\n+                lnk\n+                for lnk in rule.link_extractor.extract_links(response)\n+                if lnk not in seen\n+            ]\n             for link in rule.process_links(links):\n                 seen.add(link)\n                 request = self._build_request(rule_index, link)\n                 yield rule.process_request(request, response)\n \n     def _callback(self, response, **cb_kwargs):\n-        rule = self._rules[response.meta['rule']]\n-        return self._parse_response(response, rule.callback, {**rule.cb_kwargs, **cb_kwargs}, rule.follow)\n+        rule = self._rules[response.meta[\"rule\"]]\n+        return self._parse_response(\n+            response, rule.callback, {**rule.cb_kwargs, **cb_kwargs}, rule.follow\n+        )\n \n     def _errback(self, failure):\n-        rule = self._rules[failure.request.meta['rule']]\n+        rule = self._rules[failure.request.meta[\"rule\"]]\n         return self._handle_failure(failure, rule.errback)\n \n     async def _parse_response(self, response, callback, cb_kwargs, follow=True):\n@@ -140,5 +144,7 @@ class CrawlSpider(Spider):\n     @classmethod\n     def from_crawler(cls, crawler, *args, **kwargs):\n         spider = super().from_crawler(crawler, *args, **kwargs)\n-        spider._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True)\n+        spider._follow_links = crawler.settings.getbool(\n+            \"CRAWLSPIDER_FOLLOW_LINKS\", True\n+        )\n         return spider\n\n@@ -21,8 +21,8 @@ class XMLFeedSpider(Spider):\n     use iternodes, since it's a faster and cleaner.\n     \"\"\"\n \n-    iterator = 'iternodes'\n-    itertag = 'item'\n+    iterator = \"iternodes\"\n+    itertag = \"item\"\n     namespaces = ()\n \n     def process_results(self, response, results):\n@@ -44,7 +44,7 @@ class XMLFeedSpider(Spider):\n \n     def parse_node(self, response, selector):\n         \"\"\"This method must be overridden with your custom spider functionality\"\"\"\n-        if hasattr(self, 'parse_item'):  # backward compatibility\n+        if hasattr(self, \"parse_item\"):  # backward compatibility\n             return self.parse_item(response, selector)\n         raise NotImplementedError\n \n@@ -62,22 +62,24 @@ class XMLFeedSpider(Spider):\n                 yield result_item\n \n     def _parse(self, response, **kwargs):\n-        if not hasattr(self, 'parse_node'):\n-            raise NotConfigured('You must define parse_node method in order to scrape this XML feed')\n+        if not hasattr(self, \"parse_node\"):\n+            raise NotConfigured(\n+                \"You must define parse_node method in order to scrape this XML feed\"\n+            )\n \n         response = self.adapt_response(response)\n-        if self.iterator == 'iternodes':\n+        if self.iterator == \"iternodes\":\n             nodes = self._iternodes(response)\n-        elif self.iterator == 'xml':\n-            selector = Selector(response, type='xml')\n+        elif self.iterator == \"xml\":\n+            selector = Selector(response, type=\"xml\")\n             self._register_namespaces(selector)\n-            nodes = selector.xpath(f'//{self.itertag}')\n-        elif self.iterator == 'html':\n-            selector = Selector(response, type='html')\n+            nodes = selector.xpath(f\"//{self.itertag}\")\n+        elif self.iterator == \"html\":\n+            selector = Selector(response, type=\"html\")\n             self._register_namespaces(selector)\n-            nodes = selector.xpath(f'//{self.itertag}')\n+            nodes = selector.xpath(f\"//{self.itertag}\")\n         else:\n-            raise NotSupported('Unsupported node iterator')\n+            raise NotSupported(\"Unsupported node iterator\")\n \n         return self.parse_nodes(response, nodes)\n \n@@ -100,8 +102,12 @@ class CSVFeedSpider(Spider):\n     and the file's headers.\n     \"\"\"\n \n-    delimiter = None  # When this is None, python's csv module's default delimiter is used\n-    quotechar = None  # When this is None, python's csv module's default quotechar is used\n+    delimiter = (\n+        None  # When this is None, python's csv module's default delimiter is used\n+    )\n+    quotechar = (\n+        None  # When this is None, python's csv module's default quotechar is used\n+    )\n     headers = None\n \n     def process_results(self, response, results):\n@@ -123,13 +129,17 @@ class CSVFeedSpider(Spider):\n         process_results methods for pre and post-processing purposes.\n         \"\"\"\n \n-        for row in csviter(response, self.delimiter, self.headers, quotechar=self.quotechar):\n+        for row in csviter(\n+            response, self.delimiter, self.headers, quotechar=self.quotechar\n+        ):\n             ret = iterate_spider_output(self.parse_row(response, row))\n             for result_item in self.process_results(response, ret):\n                 yield result_item\n \n     def _parse(self, response, **kwargs):\n-        if not hasattr(self, 'parse_row'):\n-            raise NotConfigured('You must define parse_row method in order to scrape this CSV feed')\n+        if not hasattr(self, \"parse_row\"):\n+            raise NotConfigured(\n+                \"You must define parse_row method in order to scrape this CSV feed\"\n+            )\n         response = self.adapt_response(response)\n         return self.parse_rows(response)\n\n@@ -13,7 +13,7 @@ class InitSpider(Spider):\n         \"\"\"This method must be set as the callback of your last initialization\n         request. See self.init_request() docstring for more info.\n         \"\"\"\n-        return self.__dict__.pop('_postinit_reqs')\n+        return self.__dict__.pop(\"_postinit_reqs\")\n \n     def init_request(self):\n         \"\"\"This function should return one initialization request, with the\n\n@@ -13,8 +13,8 @@ logger = logging.getLogger(__name__)\n class SitemapSpider(Spider):\n \n     sitemap_urls = ()\n-    sitemap_rules = [('', 'parse')]\n-    sitemap_follow = ['']\n+    sitemap_rules = [(\"\", \"parse\")]\n+    sitemap_follow = [\"\"]\n     sitemap_alternate_links = False\n \n     def __init__(self, *a, **kw):\n@@ -39,24 +39,27 @@ class SitemapSpider(Spider):\n             yield entry\n \n     def _parse_sitemap(self, response):\n-        if response.url.endswith('/robots.txt'):\n+        if response.url.endswith(\"/robots.txt\"):\n             for url in sitemap_urls_from_robots(response.text, base_url=response.url):\n                 yield Request(url, callback=self._parse_sitemap)\n         else:\n             body = self._get_sitemap_body(response)\n             if body is None:\n-                logger.warning(\"Ignoring invalid sitemap: %(response)s\",\n-                               {'response': response}, extra={'spider': self})\n+                logger.warning(\n+                    \"Ignoring invalid sitemap: %(response)s\",\n+                    {\"response\": response},\n+                    extra={\"spider\": self},\n+                )\n                 return\n \n             s = Sitemap(body)\n             it = self.sitemap_filter(s)\n \n-            if s.type == 'sitemapindex':\n+            if s.type == \"sitemapindex\":\n                 for loc in iterloc(it, self.sitemap_alternate_links):\n                     if any(x.search(loc) for x in self._follow):\n                         yield Request(loc, callback=self._parse_sitemap)\n-            elif s.type == 'urlset':\n+            elif s.type == \"urlset\":\n                 for loc in iterloc(it, self.sitemap_alternate_links):\n                     for r, c in self._cbs:\n                         if r.search(loc):\n@@ -80,7 +83,7 @@ class SitemapSpider(Spider):\n         # without actually being a .xml.gz file in the first place,\n         # merely XML gzip-compressed on the fly,\n         # in other word, here, we have plain XML\n-        if response.url.endswith('.xml') or response.url.endswith('.xml.gz'):\n+        if response.url.endswith(\".xml\") or response.url.endswith(\".xml.gz\"):\n             return response.body\n \n \n@@ -92,8 +95,8 @@ def regex(x):\n \n def iterloc(it, alt=False):\n     for d in it:\n-        yield d['loc']\n+        yield d[\"loc\"]\n \n         # Also consider alternate URLs (xhtml:link rel=\"alternate\")\n-        if alt and 'alternate' in d:\n-            yield from d['alternate']\n+        if alt and \"alternate\" in d:\n+            yield from d[\"alternate\"]\n\n@@ -15,9 +15,7 @@ from scrapy.utils.request import request_from_dict\n \n \n def _with_mkdir(queue_class):\n-\n     class DirectoriesCreated(queue_class):\n-\n         def __init__(self, path: Union[str, PathLike], *args, **kwargs):\n             dirname = Path(path).parent\n             if not dirname.exists():\n@@ -28,9 +26,7 @@ def _with_mkdir(queue_class):\n \n \n def _serializable_queue(queue_class, serialize, deserialize):\n-\n     class SerializableQueue(queue_class):\n-\n         def push(self, obj):\n             s = serialize(obj)\n             super().push(s)\n@@ -50,7 +46,9 @@ def _serializable_queue(queue_class, serialize, deserialize):\n             try:\n                 s = super().peek()\n             except AttributeError as ex:\n-                raise NotImplementedError(\"The underlying queue class does not implement 'peek'\") from ex\n+                raise NotImplementedError(\n+                    \"The underlying queue class does not implement 'peek'\"\n+                ) from ex\n             if s:\n                 return deserialize(s)\n \n@@ -58,9 +56,7 @@ def _serializable_queue(queue_class, serialize, deserialize):\n \n \n def _scrapy_serialization_queue(queue_class):\n-\n     class ScrapyRequestQueue(queue_class):\n-\n         def __init__(self, crawler, key):\n             self.spider = crawler.spider\n             super().__init__(key)\n@@ -95,7 +91,6 @@ def _scrapy_serialization_queue(queue_class):\n \n \n def _scrapy_non_serialization_queue(queue_class):\n-\n     class ScrapyRequestQueue(queue_class):\n         @classmethod\n         def from_crawler(cls, crawler, *args, **kwargs):\n@@ -111,7 +106,9 @@ def _scrapy_non_serialization_queue(queue_class):\n             try:\n                 s = super().peek()\n             except AttributeError as ex:\n-                raise NotImplementedError(\"The underlying queue class does not implement 'peek'\") from ex\n+                raise NotImplementedError(\n+                    \"The underlying queue class does not implement 'peek'\"\n+                ) from ex\n             return s\n \n     return ScrapyRequestQueue\n@@ -127,24 +124,16 @@ def _pickle_serialize(obj):\n \n \n _PickleFifoSerializationDiskQueue = _serializable_queue(\n-    _with_mkdir(queue.FifoDiskQueue),\n-    _pickle_serialize,\n-    pickle.loads\n+    _with_mkdir(queue.FifoDiskQueue), _pickle_serialize, pickle.loads\n )\n _PickleLifoSerializationDiskQueue = _serializable_queue(\n-    _with_mkdir(queue.LifoDiskQueue),\n-    _pickle_serialize,\n-    pickle.loads\n+    _with_mkdir(queue.LifoDiskQueue), _pickle_serialize, pickle.loads\n )\n _MarshalFifoSerializationDiskQueue = _serializable_queue(\n-    _with_mkdir(queue.FifoDiskQueue),\n-    marshal.dumps,\n-    marshal.loads\n+    _with_mkdir(queue.FifoDiskQueue), marshal.dumps, marshal.loads\n )\n _MarshalLifoSerializationDiskQueue = _serializable_queue(\n-    _with_mkdir(queue.LifoDiskQueue),\n-    marshal.dumps,\n-    marshal.loads\n+    _with_mkdir(queue.LifoDiskQueue), marshal.dumps, marshal.loads\n )\n \n # public queue classes\n\n@@ -8,9 +8,8 @@ logger = logging.getLogger(__name__)\n \n \n class StatsCollector:\n-\n     def __init__(self, crawler):\n-        self._dump = crawler.settings.getbool('STATS_DUMP')\n+        self._dump = crawler.settings.getbool(\"STATS_DUMP\")\n         self._stats = {}\n \n     def get_value(self, key, default=None, spider=None):\n@@ -43,8 +42,10 @@ class StatsCollector:\n \n     def close_spider(self, spider, reason):\n         if self._dump:\n-            logger.info(\"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats),\n-                        extra={'spider': spider})\n+            logger.info(\n+                \"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats),\n+                extra={\"spider\": spider},\n+            )\n         self._persist_stats(self._stats, spider)\n \n     def _persist_stats(self, stats, spider):\n@@ -52,7 +53,6 @@ class StatsCollector:\n \n \n class MemoryStatsCollector(StatsCollector):\n-\n     def __init__(self, crawler):\n         super().__init__(crawler)\n         self.spider_stats = {}\n@@ -62,7 +62,6 @@ class MemoryStatsCollector(StatsCollector):\n \n \n class DummyStatsCollector(StatsCollector):\n-\n     def get_value(self, key, default=None, spider=None):\n         return default\n \n\n@@ -13,26 +13,28 @@ class Root(Resource):\n         return self\n \n     def render(self, request):\n-        total = _getarg(request, b'total', 100, int)\n-        show = _getarg(request, b'show', 10, int)\n+        total = _getarg(request, b\"total\", 100, int)\n+        show = _getarg(request, b\"show\", 10, int)\n         nlist = [random.randint(1, total) for _ in range(show)]\n         request.write(b\"<html><head></head><body>\")\n         args = request.args.copy()\n         for nl in nlist:\n-            args['n'] = nl\n+            args[\"n\"] = nl\n             argstr = urlencode(args, doseq=True)\n-            request.write(f\"<a href='/follow?{argstr}'>follow {nl}</a><br>\"\n-                          .encode('utf8'))\n+            request.write(\n+                f\"<a href='/follow?{argstr}'>follow {nl}</a><br>\".encode(\"utf8\")\n+            )\n         request.write(b\"</body></html>\")\n-        return b''\n+        return b\"\"\n \n \n def _getarg(request, name, default=None, type=str):\n     return type(request.args[name][0]) if name in request.args else default\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     from twisted.internet import reactor\n+\n     root = Root()\n     factory = Site(root)\n     httpPort = reactor.listenTCP(8998, Site(root))\n@@ -40,5 +42,6 @@ if __name__ == '__main__':\n     def _print_listening():\n         httpHost = httpPort.getHost()\n         print(f\"Bench server at http://{httpHost.host}:{httpHost.port}\")\n+\n     reactor.callWhenRunning(_print_listening)\n     reactor.run()\n\n@@ -4,6 +4,7 @@\n def is_botocore_available():\n     try:\n         import botocore  # noqa: F401\n+\n         return True\n     except ImportError:\n         return False\n\n@@ -19,8 +19,10 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n \n     def _check_components(complist):\n         if len({convert(c) for c in complist}) != len(complist):\n-            raise ValueError(f'Some paths in {complist!r} convert to the same object, '\n-                             'please update your settings')\n+            raise ValueError(\n+                f\"Some paths in {complist!r} convert to the same object, \"\n+                \"please update your settings\"\n+            )\n \n     def _map_keys(compdict):\n         if isinstance(compdict, BaseSettings):\n@@ -28,9 +30,10 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n             for k, v in compdict.items():\n                 prio = compdict.getpriority(k)\n                 if compbs.getpriority(convert(k)) == prio:\n-                    raise ValueError(f'Some paths in {list(compdict.keys())!r} '\n-                                     'convert to the same '\n-                                     'object, please update your settings'\n+                    raise ValueError(\n+                        f\"Some paths in {list(compdict.keys())!r} \"\n+                        \"convert to the same \"\n+                        \"object, please update your settings\"\n                     )\n                 else:\n                     compbs.set(convert(k), v, priority=prio)\n@@ -42,8 +45,10 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n         \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n         for name, value in compdict.items():\n             if value is not None and not isinstance(value, numbers.Real):\n-                raise ValueError(f'Invalid value {value} for component {name}, '\n-                                 'please provide a real number or None instead')\n+                raise ValueError(\n+                    f\"Invalid value {value} for component {name}, \"\n+                    \"please provide a real number or None instead\"\n+                )\n \n     if isinstance(custom, (list, tuple)):\n         _check_components(custom)\n@@ -61,30 +66,33 @@ def arglist_to_dict(arglist):\n     \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n     dict\n     \"\"\"\n-    return dict(x.split('=', 1) for x in arglist)\n+    return dict(x.split(\"=\", 1) for x in arglist)\n \n \n-def closest_scrapy_cfg(path: Union[str, os.PathLike] = '.', prevpath: Optional[Union[str, os.PathLike]] = None) -> str:\n+def closest_scrapy_cfg(\n+    path: Union[str, os.PathLike] = \".\",\n+    prevpath: Optional[Union[str, os.PathLike]] = None,\n+) -> str:\n     \"\"\"Return the path to the closest scrapy.cfg file by traversing the current\n     directory and its parents\n     \"\"\"\n     if prevpath is not None and str(path) == str(prevpath):\n-        return ''\n+        return \"\"\n     path = Path(path).resolve()\n-    cfgfile = path / 'scrapy.cfg'\n+    cfgfile = path / \"scrapy.cfg\"\n     if cfgfile.exists():\n         return str(cfgfile)\n     return closest_scrapy_cfg(path.parent, path)\n \n \n-def init_env(project='default', set_syspath=True):\n+def init_env(project=\"default\", set_syspath=True):\n     \"\"\"Initialize environment to use command-line tool from inside a project\n     dir. This sets the Scrapy settings module and modifies the Python path to\n     be able to locate the project module.\n     \"\"\"\n     cfg = get_config()\n-    if cfg.has_option('settings', project):\n-        os.environ['SCRAPY_SETTINGS_MODULE'] = cfg.get('settings', project)\n+    if cfg.has_option(\"settings\", project):\n+        os.environ[\"SCRAPY_SETTINGS_MODULE\"] = cfg.get(\"settings\", project)\n     closest = closest_scrapy_cfg()\n     if closest:\n         projdir = str(Path(closest).parent)\n@@ -101,12 +109,14 @@ def get_config(use_closest=True):\n \n \n def get_sources(use_closest=True) -> List[str]:\n-    xdg_config_home = os.environ.get('XDG_CONFIG_HOME') or Path('~/.config').expanduser()\n+    xdg_config_home = (\n+        os.environ.get(\"XDG_CONFIG_HOME\") or Path(\"~/.config\").expanduser()\n+    )\n     sources = [\n-        '/etc/scrapy.cfg',\n-        r'c:\\scrapy\\scrapy.cfg',\n-        str(Path(xdg_config_home) / 'scrapy.cfg'),\n-        str(Path('~/.scrapy.cfg').expanduser()),\n+        \"/etc/scrapy.cfg\",\n+        r\"c:\\scrapy\\scrapy.cfg\",\n+        str(Path(xdg_config_home) / \"scrapy.cfg\"),\n+        str(Path(\"~/.scrapy.cfg\").expanduser()),\n     ]\n     if use_closest:\n         sources.append(closest_scrapy_cfg())\n@@ -115,7 +125,7 @@ def get_sources(use_closest=True) -> List[str]:\n \n def feed_complete_default_values_from_settings(feed, settings):\n     out = feed.copy()\n-    out.setdefault(\"batch_item_count\", settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT'))\n+    out.setdefault(\"batch_item_count\", settings.getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\"))\n     out.setdefault(\"encoding\", settings[\"FEED_EXPORT_ENCODING\"])\n     out.setdefault(\"fields\", settings.getdictorlist(\"FEED_EXPORT_FIELDS\") or None)\n     out.setdefault(\"store_empty\", settings.getbool(\"FEED_STORE_EMPTY\"))\n@@ -128,15 +138,19 @@ def feed_complete_default_values_from_settings(feed, settings):\n     return out\n \n \n-def feed_process_params_from_cli(settings, output: List[str], output_format=None,\n-                                 overwrite_output: Optional[List[str]] = None):\n+def feed_process_params_from_cli(\n+    settings,\n+    output: List[str],\n+    output_format=None,\n+    overwrite_output: Optional[List[str]] = None,\n+):\n     \"\"\"\n     Receives feed export params (from the 'crawl' or 'runspider' commands),\n     checks for inconsistencies in their quantities and returns a dictionary\n     suitable to be used as the FEEDS setting.\n     \"\"\"\n     valid_output_formats = without_none_values(\n-        settings.getwithbase('FEED_EXPORTERS')\n+        settings.getwithbase(\"FEED_EXPORTERS\")\n     ).keys()\n \n     def check_valid_format(output_format):\n@@ -179,28 +193,28 @@ def feed_process_params_from_cli(settings, output: List[str], output_format=None\n                 \"scrapy crawl quotes -O quotes.json:json\"\n             )\n             warnings.warn(message, ScrapyDeprecationWarning, stacklevel=2)\n-            return {output[0]: {'format': output_format}}\n+            return {output[0]: {\"format\": output_format}}\n         raise UsageError(\n-            'The -t command-line option cannot be used if multiple output '\n-            'URIs are specified'\n+            \"The -t command-line option cannot be used if multiple output \"\n+            \"URIs are specified\"\n         )\n \n     result: Dict[str, Dict[str, Any]] = {}\n     for element in output:\n         try:\n-            feed_uri, feed_format = element.rsplit(':', 1)\n+            feed_uri, feed_format = element.rsplit(\":\", 1)\n         except ValueError:\n             feed_uri = element\n-            feed_format = Path(element).suffix.replace('.', '')\n+            feed_format = Path(element).suffix.replace(\".\", \"\")\n         else:\n-            if feed_uri == '-':\n-                feed_uri = 'stdout:'\n+            if feed_uri == \"-\":\n+                feed_uri = \"stdout:\"\n         check_valid_format(feed_format)\n-        result[feed_uri] = {'format': feed_format}\n+        result[feed_uri] = {\"format\": feed_format}\n         if overwrite:\n-            result[feed_uri]['overwrite'] = True\n+            result[feed_uri][\"overwrite\"] = True\n \n     # FEEDS setting should take precedence over the matching CLI options\n-    result.update(settings.getdict('FEEDS'))\n+    result.update(settings.getdict(\"FEEDS\"))\n \n     return result\n\n@@ -2,7 +2,7 @@ from functools import wraps\n from collections import OrderedDict\n \n \n-def _embed_ipython_shell(namespace={}, banner=''):\n+def _embed_ipython_shell(namespace={}, banner=\"\"):\n     \"\"\"Start an IPython Shell\"\"\"\n     try:\n         from IPython.terminal.embed import InteractiveShellEmbed\n@@ -12,7 +12,7 @@ def _embed_ipython_shell(namespace={}, banner=''):\n         from IPython.frontend.terminal.ipapp import load_default_config\n \n     @wraps(_embed_ipython_shell)\n-    def wrapper(namespace=namespace, banner=''):\n+    def wrapper(namespace=namespace, banner=\"\"):\n         config = load_default_config()\n         # Always use .instance() to ensure _instance propagation to all parents\n         # this is needed for <TAB> completion works well for new imports\n@@ -20,55 +20,64 @@ def _embed_ipython_shell(namespace={}, banner=''):\n         # on repeated breaks like with inspect_response()\n         InteractiveShellEmbed.clear_instance()\n         shell = InteractiveShellEmbed.instance(\n-            banner1=banner, user_ns=namespace, config=config)\n+            banner1=banner, user_ns=namespace, config=config\n+        )\n         shell()\n+\n     return wrapper\n \n \n-def _embed_bpython_shell(namespace={}, banner=''):\n+def _embed_bpython_shell(namespace={}, banner=\"\"):\n     \"\"\"Start a bpython shell\"\"\"\n     import bpython\n \n     @wraps(_embed_bpython_shell)\n-    def wrapper(namespace=namespace, banner=''):\n+    def wrapper(namespace=namespace, banner=\"\"):\n         bpython.embed(locals_=namespace, banner=banner)\n+\n     return wrapper\n \n \n-def _embed_ptpython_shell(namespace={}, banner=''):\n+def _embed_ptpython_shell(namespace={}, banner=\"\"):\n     \"\"\"Start a ptpython shell\"\"\"\n     import ptpython.repl\n \n     @wraps(_embed_ptpython_shell)\n-    def wrapper(namespace=namespace, banner=''):\n+    def wrapper(namespace=namespace, banner=\"\"):\n         print(banner)\n         ptpython.repl.embed(locals=namespace)\n+\n     return wrapper\n \n \n-def _embed_standard_shell(namespace={}, banner=''):\n+def _embed_standard_shell(namespace={}, banner=\"\"):\n     \"\"\"Start a standard python shell\"\"\"\n     import code\n+\n     try:  # readline module is only available on unix systems\n         import readline\n     except ImportError:\n         pass\n     else:\n         import rlcompleter  # noqa: F401\n+\n         readline.parse_and_bind(\"tab:complete\")\n \n     @wraps(_embed_standard_shell)\n-    def wrapper(namespace=namespace, banner=''):\n+    def wrapper(namespace=namespace, banner=\"\"):\n         code.interact(banner=banner, local=namespace)\n+\n     return wrapper\n \n \n-DEFAULT_PYTHON_SHELLS = OrderedDict([\n-    ('ptpython', _embed_ptpython_shell),\n-    ('ipython', _embed_ipython_shell),\n-    ('bpython', _embed_bpython_shell),\n-    ('python', _embed_standard_shell),\n-])\n+DEFAULT_PYTHON_SHELLS = OrderedDict(\n+    [\n+        (\"ptpython\", _embed_ptpython_shell),\n+        (\"ipython\", _embed_ipython_shell),\n+        (\"bpython\", _embed_bpython_shell),\n+        (\"python\", _embed_standard_shell),\n+    ]\n+)\n \n \n def get_shell_embed_func(shells=None, known_shells=None):\n@@ -89,7 +98,7 @@ def get_shell_embed_func(shells=None, known_shells=None):\n                 continue\n \n \n-def start_python_console(namespace=None, banner='', shells=None):\n+def start_python_console(namespace=None, banner=\"\", shells=None):\n     \"\"\"Start Python console bound to the given namespace.\n     Readline support and tab completion will be used on Unix, if available.\n     \"\"\"\n\n@@ -9,52 +9,54 @@ from w3lib.http import basic_auth_header\n \n class CurlParser(argparse.ArgumentParser):\n     def error(self, message):\n-        error_msg = f'There was an error parsing the curl command: {message}'\n+        error_msg = f\"There was an error parsing the curl command: {message}\"\n         raise ValueError(error_msg)\n \n \n curl_parser = CurlParser()\n-curl_parser.add_argument('url')\n-curl_parser.add_argument('-H', '--header', dest='headers', action='append')\n-curl_parser.add_argument('-X', '--request', dest='method')\n-curl_parser.add_argument('-d', '--data', '--data-raw', dest='data')\n-curl_parser.add_argument('-u', '--user', dest='auth')\n+curl_parser.add_argument(\"url\")\n+curl_parser.add_argument(\"-H\", \"--header\", dest=\"headers\", action=\"append\")\n+curl_parser.add_argument(\"-X\", \"--request\", dest=\"method\")\n+curl_parser.add_argument(\"-d\", \"--data\", \"--data-raw\", dest=\"data\")\n+curl_parser.add_argument(\"-u\", \"--user\", dest=\"auth\")\n \n \n safe_to_ignore_arguments = [\n-    ['--compressed'],\n+    [\"--compressed\"],\n     # `--compressed` argument is not safe to ignore, but it's included here\n     # because the `HttpCompressionMiddleware` is enabled by default\n-    ['-s', '--silent'],\n-    ['-v', '--verbose'],\n-    ['-#', '--progress-bar']\n+    [\"-s\", \"--silent\"],\n+    [\"-v\", \"--verbose\"],\n+    [\"-#\", \"--progress-bar\"],\n ]\n \n for argument in safe_to_ignore_arguments:\n-    curl_parser.add_argument(*argument, action='store_true')\n+    curl_parser.add_argument(*argument, action=\"store_true\")\n \n \n def _parse_headers_and_cookies(parsed_args):\n     headers = []\n     cookies = {}\n     for header in parsed_args.headers or ():\n-        name, val = header.split(':', 1)\n+        name, val = header.split(\":\", 1)\n         name = name.strip()\n         val = val.strip()\n-        if name.title() == 'Cookie':\n+        if name.title() == \"Cookie\":\n             for name, morsel in SimpleCookie(val).items():\n                 cookies[name] = morsel.value\n         else:\n             headers.append((name, val))\n \n     if parsed_args.auth:\n-        user, password = parsed_args.auth.split(':', 1)\n-        headers.append(('Authorization', basic_auth_header(user, password)))\n+        user, password = parsed_args.auth.split(\":\", 1)\n+        headers.append((\"Authorization\", basic_auth_header(user, password)))\n \n     return headers, cookies\n \n \n-def curl_to_request_kwargs(curl_command: str, ignore_unknown_options: bool = True) -> dict:\n+def curl_to_request_kwargs(\n+    curl_command: str, ignore_unknown_options: bool = True\n+) -> dict:\n     \"\"\"Convert a cURL command syntax to Request kwargs.\n \n     :param str curl_command: string containing the curl command\n@@ -66,7 +68,7 @@ def curl_to_request_kwargs(curl_command: str, ignore_unknown_options: bool = Tru\n \n     curl_args = split(curl_command)\n \n-    if curl_args[0] != 'curl':\n+    if curl_args[0] != \"curl\":\n         raise ValueError('A curl command must start with \"curl\"')\n \n     parsed_args, argv = curl_parser.parse_known_args(curl_args[1:])\n@@ -84,23 +86,23 @@ def curl_to_request_kwargs(curl_command: str, ignore_unknown_options: bool = Tru\n     # needs the scheme to work\n     parsed_url = urlparse(url)\n     if not parsed_url.scheme:\n-        url = 'http://' + url\n+        url = \"http://\" + url\n \n-    method = parsed_args.method or 'GET'\n+    method = parsed_args.method or \"GET\"\n \n-    result = {'method': method.upper(), 'url': url}\n+    result = {\"method\": method.upper(), \"url\": url}\n \n     headers, cookies = _parse_headers_and_cookies(parsed_args)\n \n     if headers:\n-        result['headers'] = headers\n+        result[\"headers\"] = headers\n     if cookies:\n-        result['cookies'] = cookies\n+        result[\"cookies\"] = cookies\n     if parsed_args.data:\n-        result['body'] = parsed_args.data\n+        result[\"body\"] = parsed_args.data\n         if not parsed_args.method:\n             # if the \"data\" is specified but the \"method\" is not specified,\n             # the default method is 'POST'\n-            result['method'] = 'POST'\n+            result[\"method\"] = \"POST\"\n \n     return result\n\n@@ -30,10 +30,12 @@ class CaselessDict(dict):\n \n     def __contains__(self, key):\n         return dict.__contains__(self, self.normkey(key))\n+\n     has_key = __contains__\n \n     def __copy__(self):\n         return self.__class__(self)\n+\n     copy = __copy__\n \n     def normkey(self, key):\n\n@@ -19,6 +19,7 @@ def deprecated(use_instead=None):\n                 message += f\" Use {use_instead} instead.\"\n             warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n             return func(*args, **kwargs)\n+\n         return wrapped\n \n     if callable(use_instead):\n@@ -29,9 +30,11 @@ def deprecated(use_instead=None):\n \n def defers(func):\n     \"\"\"Decorator to make sure a function always returns a deferred\"\"\"\n+\n     @wraps(func)\n     def wrapped(*a, **kw):\n         return defer.maybeDeferred(func, *a, **kw)\n+\n     return wrapped\n \n \n@@ -39,7 +42,9 @@ def inthread(func):\n     \"\"\"Decorator to call a function in a thread and return a deferred with the\n     result\n     \"\"\"\n+\n     @wraps(func)\n     def wrapped(*a, **kw):\n         return threads.deferToThread(func, *a, **kw)\n+\n     return wrapped\n\n@@ -16,7 +16,7 @@ from typing import (\n     Iterator,\n     List,\n     Optional,\n-    Union\n+    Union,\n )\n \n from twisted.internet import defer\n@@ -26,7 +26,10 @@ from twisted.python import failure\n from twisted.python.failure import Failure\n \n from scrapy.exceptions import IgnoreRequest\n-from scrapy.utils.reactor import is_asyncio_reactor_installed, get_asyncio_event_loop_policy\n+from scrapy.utils.reactor import (\n+    is_asyncio_reactor_installed,\n+    get_asyncio_event_loop_policy,\n+)\n \n \n def defer_fail(_failure: Failure) -> Deferred:\n@@ -37,6 +40,7 @@ def defer_fail(_failure: Failure) -> Deferred:\n     before attending pending delayed calls, so do not set delay to zero.\n     \"\"\"\n     from twisted.internet import reactor\n+\n     d = Deferred()\n     reactor.callLater(0.1, d.errback, _failure)\n     return d\n@@ -50,6 +54,7 @@ def defer_succeed(result) -> Deferred:\n     before attending pending delayed calls, so do not set delay to zero.\n     \"\"\"\n     from twisted.internet import reactor\n+\n     d = Deferred()\n     reactor.callLater(0.1, d.callback, result)\n     return d\n@@ -80,7 +85,9 @@ def mustbe_deferred(f: Callable, *args, **kw) -> Deferred:\n         return defer_result(result)\n \n \n-def parallel(iterable: Iterable, count: int, callable: Callable, *args, **named) -> DeferredList:\n+def parallel(\n+    iterable: Iterable, count: int, callable: Callable, *args, **named\n+) -> DeferredList:\n     \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n     using no more than ``count`` concurrent calls.\n \n@@ -136,7 +143,14 @@ class _AsyncCooperatorAdapter(Iterator):\n     Cooperator/CooperativeTask and use it instead of this adapter to achieve the same\n     goal.\n     \"\"\"\n-    def __init__(self, aiterable: AsyncIterable, callable: Callable, *callable_args, **callable_kwargs):\n+\n+    def __init__(\n+        self,\n+        aiterable: AsyncIterable,\n+        callable: Callable,\n+        *callable_args,\n+        **callable_kwargs\n+    ):\n         self.aiterator = aiterable.__aiter__()\n         self.callable = callable\n         self.callable_args = callable_args\n@@ -186,7 +200,9 @@ class _AsyncCooperatorAdapter(Iterator):\n         return d\n \n \n-def parallel_async(async_iterable: AsyncIterable, count: int, callable: Callable, *args, **named) -> DeferredList:\n+def parallel_async(\n+    async_iterable: AsyncIterable, count: int, callable: Callable, *args, **named\n+) -> DeferredList:\n     \"\"\"Like parallel but for async iterators\"\"\"\n     coop = Cooperator()\n     work = _AsyncCooperatorAdapter(async_iterable, callable, *args, **named)\n@@ -203,14 +219,19 @@ def process_chain(callbacks: Iterable[Callable], input, *a, **kw) -> Deferred:\n     return d\n \n \n-def process_chain_both(callbacks: Iterable[Callable], errbacks: Iterable[Callable], input, *a, **kw) -> Deferred:\n+def process_chain_both(\n+    callbacks: Iterable[Callable], errbacks: Iterable[Callable], input, *a, **kw\n+) -> Deferred:\n     \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n     d = Deferred()\n     for cb, eb in zip(callbacks, errbacks):\n         d.addCallbacks(\n-            callback=cb, errback=eb,\n-            callbackArgs=a, callbackKeywords=kw,\n-            errbackArgs=a, errbackKeywords=kw,\n+            callback=cb,\n+            errback=eb,\n+            callbackArgs=a,\n+            callbackKeywords=kw,\n+            errbackArgs=a,\n+            errbackKeywords=kw,\n         )\n     if isinstance(input, failure.Failure):\n         d.errback(input)\n@@ -243,7 +264,9 @@ def iter_errback(iterable: Iterable, errback: Callable, *a, **kw) -> Generator:\n             errback(failure.Failure(), *a, **kw)\n \n \n-async def aiter_errback(aiterable: AsyncIterable, errback: Callable, *a, **kw) -> AsyncGenerator:\n+async def aiter_errback(\n+    aiterable: AsyncIterable, errback: Callable, *a, **kw\n+) -> AsyncGenerator:\n     \"\"\"Wraps an async iterable calling an errback if an error is caught while\n     iterating it. Similar to scrapy.utils.defer.iter_errback()\n     \"\"\"\n@@ -278,9 +301,11 @@ def deferred_f_from_coro_f(coro_f: Callable[..., Coroutine]) -> Callable:\n     The coroutine function will be called at the time when the wrapper is called. Wrapper args will be passed to it.\n     This is useful for callback chains, as callback functions are called with the previous callback result.\n     \"\"\"\n+\n     @wraps(coro_f)\n     def f(*coro_args, **coro_kwargs):\n         return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))\n+\n     return f\n \n \n\n@@ -6,13 +6,14 @@ from typing import List, Tuple\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n-def attribute(obj, oldattr, newattr, version='0.12'):\n+def attribute(obj, oldattr, newattr, version=\"0.12\"):\n     cname = obj.__class__.__name__\n     warnings.warn(\n         f\"{cname}.{oldattr} attribute is deprecated and will be no longer supported \"\n         f\"in Scrapy {version}, use {cname}.{newattr} attribute instead\",\n         ScrapyDeprecationWarning,\n-        stacklevel=3)\n+        stacklevel=3,\n+    )\n \n \n def create_deprecated_class(\n@@ -24,7 +25,7 @@ def create_deprecated_class(\n     old_class_path=None,\n     new_class_path=None,\n     subclass_warn_message=\"{cls} inherits from deprecated class {old}, please inherit from {new}.\",\n-    instance_warn_message=\"{cls} is deprecated, instantiate {new} instead.\"\n+    instance_warn_message=\"{cls} is deprecated, instantiate {new} instead.\",\n ):\n     \"\"\"\n     Return a \"deprecated\" class that causes its subclasses to issue a warning.\n@@ -67,11 +68,13 @@ def create_deprecated_class(\n             old = meta.deprecated_class\n             if old in bases and not (warn_once and meta.warned_on_subclass):\n                 meta.warned_on_subclass = True\n-                msg = subclass_warn_message.format(cls=_clspath(cls),\n+                msg = subclass_warn_message.format(\n+                    cls=_clspath(cls),\n                     old=_clspath(old, old_class_path),\n-                                                   new=_clspath(new_class, new_class_path))\n+                    new=_clspath(new_class, new_class_path),\n+                )\n                 if warn_once:\n-                    msg += ' (warning only on first subclass, there may be others)'\n+                    msg += \" (warning only on first subclass, there may be others)\"\n                 warnings.warn(msg, warn_category, stacklevel=2)\n             super().__init__(name, bases, clsdict_)\n \n@@ -79,8 +82,7 @@ def create_deprecated_class(\n         # and https://docs.python.org/reference/datamodel.html#customizing-instance-and-subclass-checks\n         # for implementation details\n         def __instancecheck__(cls, inst):\n-            return any(cls.__subclasscheck__(c)\n-                       for c in (type(inst), inst.__class__))\n+            return any(cls.__subclasscheck__(c) for c in (type(inst), inst.__class__))\n \n         def __subclasscheck__(cls, sub):\n             if cls is not DeprecatedClass.deprecated_class:\n@@ -93,14 +95,16 @@ def create_deprecated_class(\n             if not inspect.isclass(sub):\n                 raise TypeError(\"issubclass() arg 1 must be a class\")\n \n-            mro = getattr(sub, '__mro__', ())\n+            mro = getattr(sub, \"__mro__\", ())\n             return any(c in {cls, new_class} for c in mro)\n \n         def __call__(cls, *args, **kwargs):\n             old = DeprecatedClass.deprecated_class\n             if cls is old:\n-                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n-                                                   new=_clspath(new_class, new_class_path))\n+                msg = instance_warn_message.format(\n+                    cls=_clspath(cls, old_class_path),\n+                    new=_clspath(new_class, new_class_path),\n+                )\n                 warnings.warn(msg, warn_category, stacklevel=2)\n             return super().__call__(*args, **kwargs)\n \n@@ -124,7 +128,7 @@ def create_deprecated_class(\n def _clspath(cls, forced=None):\n     if forced is not None:\n         return forced\n-    return f'{cls.__module__}.{cls.__name__}'\n+    return f\"{cls.__module__}.{cls.__name__}\"\n \n \n DEPRECATION_RULES: List[Tuple[str, str]] = []\n@@ -135,8 +139,10 @@ def update_classpath(path):\n     for prefix, replacement in DEPRECATION_RULES:\n         if isinstance(path, str) and path.startswith(prefix):\n             new_path = path.replace(prefix, replacement, 1)\n-            warnings.warn(f\"`{path}` class is deprecated, use `{new_path}` instead\",\n-                          ScrapyDeprecationWarning)\n+            warnings.warn(\n+                f\"`{path}` class is deprecated, use `{new_path}` instead\",\n+                ScrapyDeprecationWarning,\n+            )\n             return new_path\n     return path\n \n\n@@ -37,11 +37,12 @@ def _colorize(text, colorize=True):\n     else:\n         from pygments.formatters import TerminalFormatter\n         from pygments.lexers import PythonLexer\n+\n         return highlight(text, PythonLexer(), TerminalFormatter())\n \n \n def pformat(obj, *args, **kwargs):\n-    return _colorize(pformat_(obj), kwargs.pop('colorize', True))\n+    return _colorize(pformat_(obj), kwargs.pop(\"colorize\", True))\n \n \n def pprint(obj, *args, **kwargs):\n\n@@ -18,8 +18,8 @@ def ftp_makedirs_cwd(ftp, path, first_call=True):\n \n \n def ftp_store_file(\n-        *, path, file, host, port,\n-        username, password, use_active_mode=False, overwrite=True):\n+    *, path, file, host, port, username, password, use_active_mode=False, overwrite=True\n+):\n     \"\"\"Opens a FTP connection with passed credentials,sets current directory\n     to the directory extracted from given path, then uploads the file to server\n     \"\"\"\n@@ -31,6 +31,6 @@ def ftp_store_file(\n         file.seek(0)\n         dirname, filename = posixpath.split(path)\n         ftp_makedirs_cwd(ftp, dirname)\n-        command = 'STOR' if overwrite else 'APPE'\n-        ftp.storbinary(f'{command} {filename}', file)\n+        command = \"STOR\" if overwrite else \"APPE\"\n+        ftp.storbinary(f\"{command} {filename}\", file)\n         file.close()\n\n@@ -10,7 +10,7 @@ def gunzip(data):\n     \"\"\"\n     f = GzipFile(fileobj=BytesIO(data))\n     output_list = []\n-    chunk = b'.'\n+    chunk = b\".\"\n     while chunk:\n         try:\n             chunk = f.read1(8196)\n@@ -20,15 +20,15 @@ def gunzip(data):\n             # see issue 87 about catching struct.error\n             # some pages are quite small so output_list is empty and f.extrabuf\n             # contains the whole page content\n-            if output_list or getattr(f, 'extrabuf', None):\n+            if output_list or getattr(f, \"extrabuf\", None):\n                 try:\n                     output_list.append(f.extrabuf[-f.extrasize :])\n                 finally:\n                     break\n             else:\n                 raise\n-    return b''.join(output_list)\n+    return b\"\".join(output_list)\n \n \n def gzip_magic_number(response):\n-    return response.body[:3] == b'\\x1f\\x8b\\x08'\n+    return response.body[:3] == b\"\\x1f\\x8b\\x08\"\n\n@@ -7,7 +7,9 @@ from weakref import WeakKeyDictionary\n from scrapy.http import Request, Response\n \n \n-_urlparse_cache: \"WeakKeyDictionary[Union[Request, Response], ParseResult]\" = WeakKeyDictionary()\n+_urlparse_cache: \"WeakKeyDictionary[Union[Request, Response], ParseResult]\" = (\n+    WeakKeyDictionary()\n+)\n \n \n def urlparse_cached(request_or_response: Union[Request, Response]) -> ParseResult:\n\n@@ -22,60 +22,62 @@ def xmliter(obj, nodename):\n     \"\"\"\n     nodename_patt = re.escape(nodename)\n \n-    DOCUMENT_HEADER_RE = re.compile(r'<\\?xml[^>]+>\\s*', re.S)\n-    HEADER_END_RE = re.compile(fr'<\\s*/{nodename_patt}\\s*>', re.S)\n-    END_TAG_RE = re.compile(r'<\\s*/([^\\s>]+)\\s*>', re.S)\n-    NAMESPACE_RE = re.compile(r'((xmlns[:A-Za-z]*)=[^>\\s]+)', re.S)\n+    DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.S)\n+    HEADER_END_RE = re.compile(rf\"<\\s*/{nodename_patt}\\s*>\", re.S)\n+    END_TAG_RE = re.compile(r\"<\\s*/([^\\s>]+)\\s*>\", re.S)\n+    NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]*)=[^>\\s]+)\", re.S)\n     text = _body_or_str(obj)\n \n     document_header = re.search(DOCUMENT_HEADER_RE, text)\n-    document_header = document_header.group().strip() if document_header else ''\n+    document_header = document_header.group().strip() if document_header else \"\"\n     header_end_idx = re_rsearch(HEADER_END_RE, text)\n-    header_end = text[header_end_idx[1]:].strip() if header_end_idx else ''\n+    header_end = text[header_end_idx[1] :].strip() if header_end_idx else \"\"\n     namespaces = {}\n     if header_end:\n         for tagname in reversed(re.findall(END_TAG_RE, header_end)):\n-            tag = re.search(fr'<\\s*{tagname}.*?xmlns[:=][^>]*>', text[:header_end_idx[1]], re.S)\n+            tag = re.search(\n+                rf\"<\\s*{tagname}.*?xmlns[:=][^>]*>\", text[: header_end_idx[1]], re.S\n+            )\n             if tag:\n-                namespaces.update(reversed(x) for x in re.findall(NAMESPACE_RE, tag.group()))\n+                namespaces.update(\n+                    reversed(x) for x in re.findall(NAMESPACE_RE, tag.group())\n+                )\n \n-    r = re.compile(fr'<{nodename_patt}[\\s>].*?</{nodename_patt}>', re.DOTALL)\n+    r = re.compile(rf\"<{nodename_patt}[\\s>].*?</{nodename_patt}>\", re.DOTALL)\n     for match in r.finditer(text):\n         nodetext = (\n             document_header\n             + match.group().replace(\n-                nodename,\n-                f'{nodename} {\" \".join(namespaces.values())}',\n-                1\n+                nodename, f'{nodename} {\" \".join(namespaces.values())}', 1\n             )\n             + header_end\n         )\n-        yield Selector(text=nodetext, type='xml')\n+        yield Selector(text=nodetext, type=\"xml\")\n \n \n-def xmliter_lxml(obj, nodename, namespace=None, prefix='x'):\n+def xmliter_lxml(obj, nodename, namespace=None, prefix=\"x\"):\n     from lxml import etree\n+\n     reader = _StreamReader(obj)\n-    tag = f'{{{namespace}}}{nodename}' if namespace else nodename\n+    tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename\n     iterable = etree.iterparse(reader, tag=tag, encoding=reader.encoding)\n-    selxpath = '//' + (f'{prefix}:{nodename}' if namespace else nodename)\n+    selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)\n     for _, node in iterable:\n-        nodetext = etree.tostring(node, encoding='unicode')\n+        nodetext = etree.tostring(node, encoding=\"unicode\")\n         node.clear()\n-        xs = Selector(text=nodetext, type='xml')\n+        xs = Selector(text=nodetext, type=\"xml\")\n         if namespace:\n             xs.register_namespace(prefix, namespace)\n         yield xs.xpath(selxpath)[0]\n \n \n class _StreamReader:\n-\n     def __init__(self, obj):\n         self._ptr = 0\n         if isinstance(obj, Response):\n             self._text, self.encoding = obj.body, obj.encoding\n         else:\n-            self._text, self.encoding = obj, 'utf-8'\n+            self._text, self.encoding = obj, \"utf-8\"\n         self._is_unicode = isinstance(self._text, str)\n \n     def read(self, n=65535):\n@@ -90,7 +92,7 @@ class _StreamReader:\n     def _read_unicode(self, n=65535):\n         s, e = self._ptr, self._ptr + n\n         self._ptr = e\n-        return self._text[s:e].encode('utf-8')\n+        return self._text[s:e].encode(\"utf-8\")\n \n \n def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n@@ -109,7 +111,7 @@ def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n     quotechar is the character used to enclosure fields on the given obj.\n     \"\"\"\n \n-    encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or 'utf-8'\n+    encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or \"utf-8\"\n \n     def row_to_unicode(row_):\n         return [to_unicode(field, encoding) for field in row_]\n@@ -133,10 +135,15 @@ def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n     for row in csv_r:\n         row = row_to_unicode(row)\n         if len(row) != len(headers):\n-            logger.warning(\"ignoring row %(csvlnum)d (length: %(csvrow)d, \"\n+            logger.warning(\n+                \"ignoring row %(csvlnum)d (length: %(csvrow)d, \"\n                 \"should be: %(csvheader)d)\",\n-                           {'csvlnum': csv_r.line_num, 'csvrow': len(row),\n-                            'csvheader': len(headers)})\n+                {\n+                    \"csvlnum\": csv_r.line_num,\n+                    \"csvrow\": len(row),\n+                    \"csvheader\": len(headers),\n+                },\n+            )\n             continue\n         yield dict(zip(headers, row))\n \n@@ -153,7 +160,7 @@ def _body_or_str(obj, unicode=True):\n             return obj.body\n         if isinstance(obj, TextResponse):\n             return obj.text\n-        return obj.body.decode('utf-8')\n+        return obj.body.decode(\"utf-8\")\n     if isinstance(obj, str):\n-        return obj if unicode else obj.encode('utf-8')\n-    return obj.decode('utf-8') if unicode else obj\n+        return obj if unicode else obj.encode(\"utf-8\")\n+    return obj.decode(\"utf-8\") if unicode else obj\n\n@@ -5,7 +5,7 @@ from scrapy.settings import BaseSettings\n \n \n def job_dir(settings: BaseSettings) -> Optional[str]:\n-    path = settings['JOBDIR']\n+    path = settings[\"JOBDIR\"]\n     if path and not Path(path).exists():\n         Path(path).mkdir(parents=True)\n     return path\n\n@@ -37,25 +37,25 @@ class TopLevelFormatter(logging.Filter):\n         self.loggers = loggers or []\n \n     def filter(self, record):\n-        if any(record.name.startswith(logger + '.') for logger in self.loggers):\n-            record.name = record.name.split('.', 1)[0]\n+        if any(record.name.startswith(logger + \".\") for logger in self.loggers):\n+            record.name = record.name.split(\".\", 1)[0]\n         return True\n \n \n DEFAULT_LOGGING = {\n-    'version': 1,\n-    'disable_existing_loggers': False,\n-    'loggers': {\n-        'hpack': {\n-            'level': 'ERROR',\n+    \"version\": 1,\n+    \"disable_existing_loggers\": False,\n+    \"loggers\": {\n+        \"hpack\": {\n+            \"level\": \"ERROR\",\n         },\n-        'scrapy': {\n-            'level': 'DEBUG',\n+        \"scrapy\": {\n+            \"level\": \"DEBUG\",\n+        },\n+        \"twisted\": {\n+            \"level\": \"ERROR\",\n         },\n-        'twisted': {\n-            'level': 'ERROR',\n     },\n-    }\n }\n \n \n@@ -87,7 +87,7 @@ def configure_logging(settings=None, install_root_handler=True):\n         # Route warnings through python logging\n         logging.captureWarnings(True)\n \n-    observer = twisted_log.PythonLoggingObserver('twisted')\n+    observer = twisted_log.PythonLoggingObserver(\"twisted\")\n     observer.start()\n \n     dictConfig(DEFAULT_LOGGING)\n@@ -95,8 +95,8 @@ def configure_logging(settings=None, install_root_handler=True):\n     if isinstance(settings, dict) or settings is None:\n         settings = Settings(settings)\n \n-    if settings.getbool('LOG_STDOUT'):\n-        sys.stdout = StreamLogger(logging.getLogger('stdout'))\n+    if settings.getbool(\"LOG_STDOUT\"):\n+        sys.stdout = StreamLogger(logging.getLogger(\"stdout\"))\n \n     if install_root_handler:\n         install_scrapy_root_handler(settings)\n@@ -105,8 +105,10 @@ def configure_logging(settings=None, install_root_handler=True):\n def install_scrapy_root_handler(settings):\n     global _scrapy_root_handler\n \n-    if (_scrapy_root_handler is not None\n-            and _scrapy_root_handler in logging.root.handlers):\n+    if (\n+        _scrapy_root_handler is not None\n+        and _scrapy_root_handler in logging.root.handlers\n+    ):\n         logging.root.removeHandler(_scrapy_root_handler)\n     logging.root.setLevel(logging.NOTSET)\n     _scrapy_root_handler = _get_handler(settings)\n@@ -122,42 +124,45 @@ _scrapy_root_handler = None\n \n def _get_handler(settings):\n     \"\"\"Return a log handler object according to settings\"\"\"\n-    filename = settings.get('LOG_FILE')\n+    filename = settings.get(\"LOG_FILE\")\n     if filename:\n-        mode = 'a' if settings.getbool('LOG_FILE_APPEND') else 'w'\n-        encoding = settings.get('LOG_ENCODING')\n+        mode = \"a\" if settings.getbool(\"LOG_FILE_APPEND\") else \"w\"\n+        encoding = settings.get(\"LOG_ENCODING\")\n         handler = logging.FileHandler(filename, mode=mode, encoding=encoding)\n-    elif settings.getbool('LOG_ENABLED'):\n+    elif settings.getbool(\"LOG_ENABLED\"):\n         handler = logging.StreamHandler()\n     else:\n         handler = logging.NullHandler()\n \n     formatter = logging.Formatter(\n-        fmt=settings.get('LOG_FORMAT'),\n-        datefmt=settings.get('LOG_DATEFORMAT')\n+        fmt=settings.get(\"LOG_FORMAT\"), datefmt=settings.get(\"LOG_DATEFORMAT\")\n     )\n     handler.setFormatter(formatter)\n-    handler.setLevel(settings.get('LOG_LEVEL'))\n-    if settings.getbool('LOG_SHORT_NAMES'):\n-        handler.addFilter(TopLevelFormatter(['scrapy']))\n+    handler.setLevel(settings.get(\"LOG_LEVEL\"))\n+    if settings.getbool(\"LOG_SHORT_NAMES\"):\n+        handler.addFilter(TopLevelFormatter([\"scrapy\"]))\n     return handler\n \n \n def log_scrapy_info(settings: Settings) -> None:\n-    logger.info(\"Scrapy %(version)s started (bot: %(bot)s)\",\n-                {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})\n+    logger.info(\n+        \"Scrapy %(version)s started (bot: %(bot)s)\",\n+        {\"version\": scrapy.__version__, \"bot\": settings[\"BOT_NAME\"]},\n+    )\n     versions = [\n         f\"{name} {version}\"\n         for name, version in scrapy_components_versions()\n         if name != \"Scrapy\"\n     ]\n-    logger.info(\"Versions: %(versions)s\", {'versions': \", \".join(versions)})\n+    logger.info(\"Versions: %(versions)s\", {\"versions\": \", \".join(versions)})\n \n \n def log_reactor_info() -> None:\n     from twisted.internet import reactor\n+\n     logger.debug(\"Using reactor: %s.%s\", reactor.__module__, reactor.__class__.__name__)\n     from twisted.internet import asyncioreactor\n+\n     if isinstance(reactor, asyncioreactor.AsyncioSelectorReactor):\n         logger.debug(\n             \"Using asyncio event loop: %s.%s\",\n@@ -172,10 +177,11 @@ class StreamLogger:\n     Taken from:\n         https://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/\n     \"\"\"\n+\n     def __init__(self, logger, log_level=logging.INFO):\n         self.logger = logger\n         self.log_level = log_level\n-        self.linebuf = ''\n+        self.linebuf = \"\"\n \n     def write(self, buf):\n         for line in buf.rstrip().splitlines():\n@@ -194,7 +200,7 @@ class LogCounterHandler(logging.Handler):\n         self.crawler = crawler\n \n     def emit(self, record):\n-        sname = f'log_count/{record.levelname}'\n+        sname = f\"log_count/{record.levelname}\"\n         self.crawler.stats.inc_value(sname)\n \n \n@@ -204,19 +210,20 @@ def logformatter_adapter(logkws):\n     and adapts it into a tuple of positional arguments for logger.log calls,\n     handling backward compatibility as well.\n     \"\"\"\n-    if not {'level', 'msg', 'args'} <= set(logkws):\n-        warnings.warn('Missing keys in LogFormatter method',\n-                      ScrapyDeprecationWarning)\n+    if not {\"level\", \"msg\", \"args\"} <= set(logkws):\n+        warnings.warn(\"Missing keys in LogFormatter method\", ScrapyDeprecationWarning)\n \n-    if 'format' in logkws:\n-        warnings.warn('`format` key in LogFormatter methods has been '\n-                      'deprecated, use `msg` instead',\n-                      ScrapyDeprecationWarning)\n+    if \"format\" in logkws:\n+        warnings.warn(\n+            \"`format` key in LogFormatter methods has been \"\n+            \"deprecated, use `msg` instead\",\n+            ScrapyDeprecationWarning,\n+        )\n \n-    level = logkws.get('level', logging.INFO)\n-    message = logkws.get('format', logkws.get('msg'))\n+    level = logkws.get(\"level\", logging.INFO)\n+    message = logkws.get(\"format\", logkws.get(\"msg\"))\n     # NOTE: This also handles 'args' being an empty dict, that case doesn't\n     # play well in logger.log calls\n-    args = logkws if not logkws.get('args') else logkws['args']\n+    args = logkws if not logkws.get(\"args\") else logkws[\"args\"]\n \n     return (level, message, args)\n\n@@ -30,7 +30,7 @@ def arg_to_iter(arg):\n     \"\"\"\n     if arg is None:\n         return []\n-    if not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, '__iter__'):\n+    if not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, \"__iter__\"):\n         return arg\n     return [arg]\n \n@@ -48,11 +48,12 @@ def load_object(path):\n     if not isinstance(path, str):\n         if callable(path):\n             return path\n-        raise TypeError(\"Unexpected argument type, expected string \"\n-                        f\"or object, got: {type(path)}\")\n+        raise TypeError(\n+            \"Unexpected argument type, expected string \" f\"or object, got: {type(path)}\"\n+        )\n \n     try:\n-        dot = path.rindex('.')\n+        dot = path.rindex(\".\")\n     except ValueError:\n         raise ValueError(f\"Error loading object '{path}': not a full path\")\n \n@@ -78,9 +79,9 @@ def walk_modules(path):\n     mods = []\n     mod = import_module(path)\n     mods.append(mod)\n-    if hasattr(mod, '__path__'):\n+    if hasattr(mod, \"__path__\"):\n         for _, subpath, ispkg in iter_modules(mod.__path__):\n-            fullpath = path + '.' + subpath\n+            fullpath = path + \".\" + subpath\n             if ispkg:\n                 mods += walk_modules(fullpath)\n             else:\n@@ -89,7 +90,7 @@ def walk_modules(path):\n     return mods\n \n \n-def extract_regex(regex, text, encoding='utf-8'):\n+def extract_regex(regex, text, encoding=\"utf-8\"):\n     \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n \n     * if the regex contains a named group called \"extract\" that will be returned\n@@ -99,22 +100,23 @@ def extract_regex(regex, text, encoding='utf-8'):\n     warnings.warn(\n         \"scrapy.utils.misc.extract_regex has moved to parsel.utils.extract_regex.\",\n         ScrapyDeprecationWarning,\n-        stacklevel=2\n+        stacklevel=2,\n     )\n \n     if isinstance(regex, str):\n         regex = re.compile(regex, re.UNICODE)\n \n     try:\n-        strings = [regex.search(text).group('extract')]   # named group\n+        strings = [regex.search(text).group(\"extract\")]  # named group\n     except Exception:\n         strings = regex.findall(text)  # full regex or numbered groups\n     strings = flatten(strings)\n \n     if isinstance(text, str):\n-        return [replace_entities(s, keep=['lt', 'amp']) for s in strings]\n-    return [replace_entities(to_unicode(s, encoding), keep=['lt', 'amp'])\n-            for s in strings]\n+        return [replace_entities(s, keep=[\"lt\", \"amp\"]) for s in strings]\n+    return [\n+        replace_entities(to_unicode(s, encoding), keep=[\"lt\", \"amp\"]) for s in strings\n+    ]\n \n \n def md5sum(file):\n@@ -136,7 +138,7 @@ def md5sum(file):\n \n def rel_has_nofollow(rel):\n     \"\"\"Return True if link rel attribute has nofollow type\"\"\"\n-    return rel is not None and 'nofollow' in rel.replace(',', ' ').split()\n+    return rel is not None and \"nofollow\" in rel.replace(\",\", \" \").split()\n \n \n def create_instance(objcls, settings, crawler, *args, **kwargs):\n@@ -160,15 +162,15 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n         if crawler is None:\n             raise ValueError(\"Specify at least one of settings and crawler.\")\n         settings = crawler.settings\n-    if crawler and hasattr(objcls, 'from_crawler'):\n+    if crawler and hasattr(objcls, \"from_crawler\"):\n         instance = objcls.from_crawler(crawler, *args, **kwargs)\n-        method_name = 'from_crawler'\n-    elif hasattr(objcls, 'from_settings'):\n+        method_name = \"from_crawler\"\n+    elif hasattr(objcls, \"from_settings\"):\n         instance = objcls.from_settings(settings, *args, **kwargs)\n-        method_name = 'from_settings'\n+        method_name = \"from_settings\"\n     else:\n         instance = objcls(*args, **kwargs)\n-        method_name = '__new__'\n+        method_name = \"__new__\"\n     if instance is None:\n         raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n     return instance\n@@ -221,7 +223,9 @@ def is_generator_with_return_value(callable):\n \n     def returns_none(return_node):\n         value = return_node.value\n-        return value is None or isinstance(value, ast.NameConstant) and value.value is None\n+        return (\n+            value is None or isinstance(value, ast.NameConstant) and value.value is None\n+        )\n \n     if inspect.isgeneratorfunction(callable):\n         func = callable\n@@ -256,8 +260,8 @@ def warn_on_generator_with_return_value(spider, callable):\n             warnings.warn(\n                 f'The \"{spider.__class__.__name__}.{callable.__name__}\" method is '\n                 'a generator and includes a \"return\" statement with a value '\n-                'different than None. This could lead to unexpected behaviour. Please see '\n-                'https://docs.python.org/3/reference/simple_stmts.html#the-return-statement '\n+                \"different than None. This could lead to unexpected behaviour. Please see \"\n+                \"https://docs.python.org/3/reference/simple_stmts.html#the-return-statement \"\n                 'for details about the semantics of the \"return\" statement within generators',\n                 stacklevel=2,\n             )\n@@ -265,9 +269,9 @@ def warn_on_generator_with_return_value(spider, callable):\n         callable_name = spider.__class__.__name__ + \".\" + callable.__name__\n         warnings.warn(\n             f'Unable to determine whether or not \"{callable_name}\" is a generator with a return value. '\n-            'This will not prevent your code from working, but it prevents Scrapy from detecting '\n+            \"This will not prevent your code from working, but it prevents Scrapy from detecting \"\n             f'potential issues in your implementation of \"{callable_name}\". Please, report this in the '\n-            'Scrapy issue tracker (https://github.com/scrapy/scrapy/issues), '\n+            \"Scrapy issue tracker (https://github.com/scrapy/scrapy/issues), \"\n             f'including the code of \"{callable_name}\"',\n             stacklevel=2,\n         )\n\n@@ -3,7 +3,7 @@ import signal\n \n signal_names = {}\n for signame in dir(signal):\n-    if signame.startswith('SIG') and not signame.startswith('SIG_'):\n+    if signame.startswith(\"SIG\") and not signame.startswith(\"SIG_\"):\n         signum = getattr(signal, signame)\n         if isinstance(signum, int):\n             signal_names[signum] = signame\n@@ -16,10 +16,11 @@ def install_shutdown_handlers(function, override_sigint=True):\n     (e.g.  Pdb)\n     \"\"\"\n     from twisted.internet import reactor\n+\n     reactor._handleSignals()\n     signal.signal(signal.SIGTERM, function)\n     if signal.getsignal(signal.SIGINT) == signal.default_int_handler or override_sigint:\n         signal.signal(signal.SIGINT, function)\n     # Catch Ctrl-Break in windows\n-    if hasattr(signal, 'SIGBREAK'):\n+    if hasattr(signal, \"SIGBREAK\"):\n         signal.signal(signal.SIGBREAK, function)\n\n@@ -9,23 +9,25 @@ from scrapy.settings import Settings\n from scrapy.exceptions import NotConfigured\n \n \n-ENVVAR = 'SCRAPY_SETTINGS_MODULE'\n-DATADIR_CFG_SECTION = 'datadir'\n+ENVVAR = \"SCRAPY_SETTINGS_MODULE\"\n+DATADIR_CFG_SECTION = \"datadir\"\n \n \n def inside_project():\n-    scrapy_module = os.environ.get('SCRAPY_SETTINGS_MODULE')\n+    scrapy_module = os.environ.get(\"SCRAPY_SETTINGS_MODULE\")\n     if scrapy_module is not None:\n         try:\n             import_module(scrapy_module)\n         except ImportError as exc:\n-            warnings.warn(f\"Cannot import scrapy settings module {scrapy_module}: {exc}\")\n+            warnings.warn(\n+                f\"Cannot import scrapy settings module {scrapy_module}: {exc}\"\n+            )\n         else:\n             return True\n     return bool(closest_scrapy_cfg())\n \n \n-def project_data_dir(project='default') -> str:\n+def project_data_dir(project=\"default\") -> str:\n     \"\"\"Return the current project data dir, creating it if it doesn't exist\"\"\"\n     if not inside_project():\n         raise NotConfigured(\"Not inside a project\")\n@@ -35,8 +37,10 @@ def project_data_dir(project='default') -> str:\n     else:\n         scrapy_cfg = closest_scrapy_cfg()\n         if not scrapy_cfg:\n-            raise NotConfigured(\"Unable to find scrapy.cfg file to infer project data dir\")\n-        d = (Path(scrapy_cfg).parent / '.scrapy').resolve()\n+            raise NotConfigured(\n+                \"Unable to find scrapy.cfg file to infer project data dir\"\n+            )\n+        d = (Path(scrapy_cfg).parent / \".scrapy\").resolve()\n     if not d.exists():\n         d.mkdir(parents=True)\n     return str(d)\n@@ -52,7 +56,7 @@ def data_path(path: str, createdir=False) -> str:\n         if inside_project():\n             path_obj = Path(project_data_dir(), path)\n         else:\n-            path_obj = Path('.scrapy', path)\n+            path_obj = Path(\".scrapy\", path)\n     if createdir and not path_obj.exists():\n         path_obj.mkdir(parents=True)\n     return str(path_obj)\n@@ -60,24 +64,27 @@ def data_path(path: str, createdir=False) -> str:\n \n def get_project_settings():\n     if ENVVAR not in os.environ:\n-        project = os.environ.get('SCRAPY_PROJECT', 'default')\n+        project = os.environ.get(\"SCRAPY_PROJECT\", \"default\")\n         init_env(project)\n \n     settings = Settings()\n     settings_module_path = os.environ.get(ENVVAR)\n     if settings_module_path:\n-        settings.setmodule(settings_module_path, priority='project')\n+        settings.setmodule(settings_module_path, priority=\"project\")\n \n     valid_envvars = {\n-        'CHECK',\n-        'PROJECT',\n-        'PYTHON_SHELL',\n-        'SETTINGS_MODULE',\n+        \"CHECK\",\n+        \"PROJECT\",\n+        \"PYTHON_SHELL\",\n+        \"SETTINGS_MODULE\",\n     }\n \n-    scrapy_envvars = {k[7:]: v for k, v in os.environ.items() if\n-                      k.startswith('SCRAPY_') and k.replace('SCRAPY_', '') in valid_envvars}\n+    scrapy_envvars = {\n+        k[7:]: v\n+        for k, v in os.environ.items()\n+        if k.startswith(\"SCRAPY_\") and k.replace(\"SCRAPY_\", \"\") in valid_envvars\n+    }\n \n-    settings.setdict(scrapy_envvars, priority='project')\n+    settings.setdict(scrapy_envvars, priority=\"project\")\n \n     return settings\n\n@@ -82,29 +82,32 @@ def unique(list_, key=lambda x: x):\n     return result\n \n \n-def to_unicode(text, encoding=None, errors='strict'):\n+def to_unicode(text, encoding=None, errors=\"strict\"):\n     \"\"\"Return the unicode representation of a bytes object ``text``. If\n     ``text`` is already an unicode object, return it as-is.\"\"\"\n     if isinstance(text, str):\n         return text\n     if not isinstance(text, (bytes, str)):\n-        raise TypeError('to_unicode must receive a bytes or str '\n-                        f'object, got {type(text).__name__}')\n+        raise TypeError(\n+            \"to_unicode must receive a bytes or str \"\n+            f\"object, got {type(text).__name__}\"\n+        )\n     if encoding is None:\n-        encoding = 'utf-8'\n+        encoding = \"utf-8\"\n     return text.decode(encoding, errors)\n \n \n-def to_bytes(text, encoding=None, errors='strict'):\n+def to_bytes(text, encoding=None, errors=\"strict\"):\n     \"\"\"Return the binary representation of ``text``. If ``text``\n     is already a bytes object, return it as-is.\"\"\"\n     if isinstance(text, bytes):\n         return text\n     if not isinstance(text, str):\n-        raise TypeError('to_bytes must receive a str or bytes '\n-                        f'object, got {type(text).__name__}')\n+        raise TypeError(\n+            \"to_bytes must receive a str or bytes \" f\"object, got {type(text).__name__}\"\n+        )\n     if encoding is None:\n-        encoding = 'utf-8'\n+        encoding = \"utf-8\"\n     return text.encode(encoding, errors)\n \n \n@@ -125,7 +128,7 @@ def re_rsearch(pattern, text, chunk_size=1024):\n     def _chunk_iter():\n         offset = len(text)\n         while True:\n-            offset -= (chunk_size * 1024)\n+            offset -= chunk_size * 1024\n             if offset <= 0:\n                 break\n             yield (text[offset:], offset)\n@@ -182,16 +185,19 @@ def get_func_args(func, stripself=False):\n     elif inspect.ismethoddescriptor(func):\n         return []\n     elif isinstance(func, partial):\n-        return [x for x in get_func_args(func.func)[len(func.args):]\n-                if not (func.keywords and x in func.keywords)]\n-    elif hasattr(func, '__call__'):\n+        return [\n+            x\n+            for x in get_func_args(func.func)[len(func.args) :]\n+            if not (func.keywords and x in func.keywords)\n+        ]\n+    elif hasattr(func, \"__call__\"):\n         if inspect.isroutine(func):\n             return []\n-        if getattr(func, '__name__', None) == '__call__':\n+        if getattr(func, \"__name__\", None) == \"__call__\":\n             return []\n         return get_func_args(func.__call__, True)\n     else:\n-        raise TypeError(f'{type(func)} is not callable')\n+        raise TypeError(f\"{type(func)} is not callable\")\n     if stripself:\n         func_args.pop(0)\n     return func_args\n@@ -221,10 +227,10 @@ def get_spec(func):\n \n     if inspect.isfunction(func) or inspect.ismethod(func):\n         spec = inspect.getfullargspec(func)\n-    elif hasattr(func, '__call__'):\n+    elif hasattr(func, \"__call__\"):\n         spec = inspect.getfullargspec(func.__call__)\n     else:\n-        raise TypeError(f'{type(func)} is not callable')\n+        raise TypeError(f\"{type(func)} is not callable\")\n \n     defaults = spec.defaults or []\n \n@@ -276,11 +282,14 @@ def global_object_name(obj):\n \n \n if hasattr(sys, \"pypy_version_info\"):\n+\n     def garbage_collect():\n         # Collecting weakreferences can take two collections on PyPy.\n         gc.collect()\n         gc.collect()\n+\n else:\n+\n     def garbage_collect():\n         gc.collect()\n \n\n@@ -10,11 +10,12 @@ from scrapy.utils.misc import load_object\n def listen_tcp(portrange, host, factory):\n     \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n     from twisted.internet import reactor\n+\n     if len(portrange) > 2:\n         raise ValueError(f\"invalid portrange: {portrange}\")\n     if not portrange:\n         return reactor.listenTCP(0, factory, interface=host)\n-    if not hasattr(portrange, '__iter__'):\n+    if not hasattr(portrange, \"__iter__\"):\n         return reactor.listenTCP(portrange, factory, interface=host)\n     if len(portrange) == 1:\n         return reactor.listenTCP(portrange[0], factory, interface=host)\n@@ -39,6 +40,7 @@ class CallLaterOnce:\n \n     def schedule(self, delay=0):\n         from twisted.internet import reactor\n+\n         if self._call is None:\n             self._call = reactor.callLater(delay, self)\n \n@@ -93,16 +95,20 @@ def verify_installed_reactor(reactor_path):\n     :mod:`~twisted.internet.reactor` does not match the specified import\n     path.\"\"\"\n     from twisted.internet import reactor\n+\n     reactor_class = load_object(reactor_path)\n     if not reactor.__class__ == reactor_class:\n-        msg = (\"The installed reactor \"\n+        msg = (\n+            \"The installed reactor \"\n             f\"({reactor.__module__}.{reactor.__class__.__name__}) does not \"\n-               f\"match the requested one ({reactor_path})\")\n+            f\"match the requested one ({reactor_path})\"\n+        )\n         raise Exception(msg)\n \n \n def verify_installed_asyncio_event_loop(loop_path):\n     from twisted.internet import reactor\n+\n     loop_class = load_object(loop_path)\n     if isinstance(reactor._asyncioEventloop, loop_class):\n         return\n@@ -121,4 +127,5 @@ def verify_installed_asyncio_event_loop(loop_path):\n \n def is_asyncio_reactor_installed():\n     from twisted.internet import reactor\n+\n     return isinstance(reactor, asyncioreactor.AsyncioSelectorReactor)\n\n@@ -7,16 +7,22 @@ from scrapy.utils.request import request_from_dict as _from_dict\n \n \n warnings.warn(\n-    (\"Module scrapy.utils.reqser is deprecated, please use request.to_dict method\"\n-     \" and/or scrapy.utils.request.request_from_dict instead\"),\n+    (\n+        \"Module scrapy.utils.reqser is deprecated, please use request.to_dict method\"\n+        \" and/or scrapy.utils.request.request_from_dict instead\"\n+    ),\n     category=ScrapyDeprecationWarning,\n     stacklevel=2,\n )\n \n \n-def request_to_dict(request: \"scrapy.Request\", spider: Optional[\"scrapy.Spider\"] = None) -> dict:\n+def request_to_dict(\n+    request: \"scrapy.Request\", spider: Optional[\"scrapy.Spider\"] = None\n+) -> dict:\n     return request.to_dict(spider=spider)\n \n \n-def request_from_dict(d: dict, spider: Optional[\"scrapy.Spider\"] = None) -> \"scrapy.Request\":\n+def request_from_dict(\n+    d: dict, spider: Optional[\"scrapy.Spider\"] = None\n+) -> \"scrapy.Request\":\n     return _from_dict(d, spider=spider)\n\n@@ -69,54 +69,54 @@ def request_fingerprint(\n     \"\"\"\n     if include_headers or keep_fragments:\n         message = (\n-            'Call to deprecated function '\n-            'scrapy.utils.request.request_fingerprint().\\n'\n-            '\\n'\n-            'If you are using this function in a Scrapy component because you '\n-            'need a non-default fingerprinting algorithm, and you are OK '\n-            'with that non-default fingerprinting algorithm being used by '\n-            'all Scrapy components and not just the one calling this '\n-            'function, use crawler.request_fingerprinter.fingerprint() '\n-            'instead in your Scrapy component (you can get the crawler '\n-            'object from the \\'from_crawler\\' class method), and use the '\n-            '\\'REQUEST_FINGERPRINTER_CLASS\\' setting to configure your '\n-            'non-default fingerprinting algorithm.\\n'\n-            '\\n'\n-            'Otherwise, consider using the '\n-            'scrapy.utils.request.fingerprint() function instead.\\n'\n-            '\\n'\n-            'If you switch to \\'fingerprint()\\', or assign the '\n-            '\\'REQUEST_FINGERPRINTER_CLASS\\' setting a class that uses '\n-            '\\'fingerprint()\\', the generated fingerprints will not only be '\n-            'bytes instead of a string, but they will also be different from '\n-            'those generated by \\'request_fingerprint()\\'. Before you switch, '\n-            'make sure that you understand the consequences of this (e.g. '\n-            'cache invalidation) and are OK with them; otherwise, consider '\n-            'implementing your own function which returns the same '\n-            'fingerprints as the deprecated \\'request_fingerprint()\\' function.'\n+            \"Call to deprecated function \"\n+            \"scrapy.utils.request.request_fingerprint().\\n\"\n+            \"\\n\"\n+            \"If you are using this function in a Scrapy component because you \"\n+            \"need a non-default fingerprinting algorithm, and you are OK \"\n+            \"with that non-default fingerprinting algorithm being used by \"\n+            \"all Scrapy components and not just the one calling this \"\n+            \"function, use crawler.request_fingerprinter.fingerprint() \"\n+            \"instead in your Scrapy component (you can get the crawler \"\n+            \"object from the 'from_crawler' class method), and use the \"\n+            \"'REQUEST_FINGERPRINTER_CLASS' setting to configure your \"\n+            \"non-default fingerprinting algorithm.\\n\"\n+            \"\\n\"\n+            \"Otherwise, consider using the \"\n+            \"scrapy.utils.request.fingerprint() function instead.\\n\"\n+            \"\\n\"\n+            \"If you switch to 'fingerprint()', or assign the \"\n+            \"'REQUEST_FINGERPRINTER_CLASS' setting a class that uses \"\n+            \"'fingerprint()', the generated fingerprints will not only be \"\n+            \"bytes instead of a string, but they will also be different from \"\n+            \"those generated by 'request_fingerprint()'. Before you switch, \"\n+            \"make sure that you understand the consequences of this (e.g. \"\n+            \"cache invalidation) and are OK with them; otherwise, consider \"\n+            \"implementing your own function which returns the same \"\n+            \"fingerprints as the deprecated 'request_fingerprint()' function.\"\n         )\n     else:\n         message = (\n-            'Call to deprecated function '\n-            'scrapy.utils.request.request_fingerprint().\\n'\n-            '\\n'\n-            'If you are using this function in a Scrapy component, and you '\n-            'are OK with users of your component changing the fingerprinting '\n-            'algorithm through settings, use '\n-            'crawler.request_fingerprinter.fingerprint() instead in your '\n-            'Scrapy component (you can get the crawler object from the '\n-            '\\'from_crawler\\' class method).\\n'\n-            '\\n'\n-            'Otherwise, consider using the '\n-            'scrapy.utils.request.fingerprint() function instead.\\n'\n-            '\\n'\n-            'Either way, the resulting fingerprints will be returned as '\n-            'bytes, not as a string, and they will also be different from '\n-            'those generated by \\'request_fingerprint()\\'. Before you switch, '\n-            'make sure that you understand the consequences of this (e.g. '\n-            'cache invalidation) and are OK with them; otherwise, consider '\n-            'implementing your own function which returns the same '\n-            'fingerprints as the deprecated \\'request_fingerprint()\\' function.'\n+            \"Call to deprecated function \"\n+            \"scrapy.utils.request.request_fingerprint().\\n\"\n+            \"\\n\"\n+            \"If you are using this function in a Scrapy component, and you \"\n+            \"are OK with users of your component changing the fingerprinting \"\n+            \"algorithm through settings, use \"\n+            \"crawler.request_fingerprinter.fingerprint() instead in your \"\n+            \"Scrapy component (you can get the crawler object from the \"\n+            \"'from_crawler' class method).\\n\"\n+            \"\\n\"\n+            \"Otherwise, consider using the \"\n+            \"scrapy.utils.request.fingerprint() function instead.\\n\"\n+            \"\\n\"\n+            \"Either way, the resulting fingerprints will be returned as \"\n+            \"bytes, not as a string, and they will also be different from \"\n+            \"those generated by 'request_fingerprint()'. Before you switch, \"\n+            \"make sure that you understand the consequences of this (e.g. \"\n+            \"cache invalidation) and are OK with them; otherwise, consider \"\n+            \"implementing your own function which returns the same \"\n+            \"fingerprints as the deprecated 'request_fingerprint()' function.\"\n         )\n     warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n     processed_include_headers: Optional[Tuple[bytes, ...]] = None\n@@ -129,8 +129,10 @@ def request_fingerprint(\n     if cache_key not in cache:\n         fp = hashlib.sha1()\n         fp.update(to_bytes(request.method))\n-        fp.update(to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments)))\n-        fp.update(request.body or b'')\n+        fp.update(\n+            to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments))\n+        )\n+        fp.update(request.body or b\"\")\n         if processed_include_headers:\n             for part in _serialize_headers(processed_include_headers, request):\n                 fp.update(part)\n@@ -203,10 +205,10 @@ def fingerprint(\n                         for header_value in request.headers.getlist(header)\n                     ]\n         fingerprint_data = {\n-            'method': to_unicode(request.method),\n-            'url': canonicalize_url(request.url, keep_fragments=keep_fragments),\n-            'body': (request.body or b'').hex(),\n-            'headers': headers,\n+            \"method\": to_unicode(request.method),\n+            \"url\": canonicalize_url(request.url, keep_fragments=keep_fragments),\n+            \"body\": (request.body or b\"\").hex(),\n+            \"headers\": headers,\n         }\n         fingerprint_json = json.dumps(fingerprint_data, sort_keys=True)\n         cache[cache_key] = hashlib.sha1(fingerprint_json.encode()).digest()\n@@ -233,35 +235,35 @@ class RequestFingerprinter:\n     def __init__(self, crawler=None):\n         if crawler:\n             implementation = crawler.settings.get(\n-                'REQUEST_FINGERPRINTER_IMPLEMENTATION'\n+                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\"\n             )\n         else:\n-            implementation = '2.6'\n-        if implementation == '2.6':\n+            implementation = \"2.6\"\n+        if implementation == \"2.6\":\n             message = (\n-                '\\'2.6\\' is a deprecated value for the '\n-                '\\'REQUEST_FINGERPRINTER_IMPLEMENTATION\\' setting.\\n'\n-                '\\n'\n-                'It is also the default value. In other words, it is normal '\n-                'to get this warning if you have not defined a value for the '\n-                '\\'REQUEST_FINGERPRINTER_IMPLEMENTATION\\' setting. This is so '\n-                'for backward compatibility reasons, but it will change in a '\n-                'future version of Scrapy.\\n'\n-                '\\n'\n-                'See the documentation of the '\n-                '\\'REQUEST_FINGERPRINTER_IMPLEMENTATION\\' setting for '\n-                'information on how to handle this deprecation.'\n+                \"'2.6' is a deprecated value for the \"\n+                \"'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\\n\"\n+                \"\\n\"\n+                \"It is also the default value. In other words, it is normal \"\n+                \"to get this warning if you have not defined a value for the \"\n+                \"'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so \"\n+                \"for backward compatibility reasons, but it will change in a \"\n+                \"future version of Scrapy.\\n\"\n+                \"\\n\"\n+                \"See the documentation of the \"\n+                \"'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for \"\n+                \"information on how to handle this deprecation.\"\n             )\n             warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n             self._fingerprint = _request_fingerprint_as_bytes\n-        elif implementation == '2.7':\n+        elif implementation == \"2.7\":\n             self._fingerprint = fingerprint\n         else:\n             raise ValueError(\n-                f'Got an invalid value on setting '\n-                f'\\'REQUEST_FINGERPRINTER_IMPLEMENTATION\\': '\n-                f'{implementation!r}. Valid values are \\'2.6\\' (deprecated) '\n-                f'and \\'2.7\\'.'\n+                f\"Got an invalid value on setting \"\n+                f\"'REQUEST_FINGERPRINTER_IMPLEMENTATION': \"\n+                f\"{implementation!r}. Valid values are '2.6' (deprecated) \"\n+                f\"and '2.7'.\"\n             )\n \n     def fingerprint(self, request: Request):\n@@ -276,7 +278,7 @@ def request_authenticate(\n     \"\"\"Authenticate the given request (in place) using the HTTP basic access\n     authentication mechanism (RFC 2617) and the given username and password\n     \"\"\"\n-    request.headers['Authorization'] = basic_auth_header(username, password)\n+    request.headers[\"Authorization\"] = basic_auth_header(username, password)\n \n \n def request_httprepr(request: Request) -> bytes:\n@@ -286,9 +288,9 @@ def request_httprepr(request: Request) -> bytes:\n     by Twisted).\n     \"\"\"\n     parsed = urlparse_cached(request)\n-    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n+    path = urlunparse((\"\", \"\", parsed.path or \"/\", parsed.params, parsed.query, \"\"))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n-    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n+    s += b\"Host: \" + to_bytes(parsed.hostname or b\"\") + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\"\n@@ -298,10 +300,10 @@ def request_httprepr(request: Request) -> bytes:\n \n def referer_str(request: Request) -> Optional[str]:\n     \"\"\"Return Referer HTTP header suitable for logging.\"\"\"\n-    referrer = request.headers.get('Referer')\n+    referrer = request.headers.get(\"Referer\")\n     if referrer is None:\n         return referrer\n-    return to_unicode(referrer, errors='replace')\n+    return to_unicode(referrer, errors=\"replace\")\n \n \n def request_from_dict(d: dict, *, spider: Optional[Spider] = None) -> Request:\n\n@@ -25,31 +25,35 @@ def get_base_url(response: \"scrapy.http.response.text.TextResponse\") -> str:\n     \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n     if response not in _baseurl_cache:\n         text = response.text[0:4096]\n-        _baseurl_cache[response] = html.get_base_url(text, response.url, response.encoding)\n+        _baseurl_cache[response] = html.get_base_url(\n+            text, response.url, response.encoding\n+        )\n     return _baseurl_cache[response]\n \n \n-_metaref_cache: \"WeakKeyDictionary[Response, Union[Tuple[None, None], Tuple[float, str]]]\" = WeakKeyDictionary()\n+_metaref_cache: \"WeakKeyDictionary[Response, Union[Tuple[None, None], Tuple[float, str]]]\" = (\n+    WeakKeyDictionary()\n+)\n \n \n def get_meta_refresh(\n     response: \"scrapy.http.response.text.TextResponse\",\n-    ignore_tags: Optional[Iterable[str]] = ('script', 'noscript'),\n+    ignore_tags: Optional[Iterable[str]] = (\"script\", \"noscript\"),\n ) -> Union[Tuple[None, None], Tuple[float, str]]:\n     \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n     if response not in _metaref_cache:\n         text = response.text[0:4096]\n         _metaref_cache[response] = html.get_meta_refresh(\n-            text, response.url, response.encoding, ignore_tags=ignore_tags)\n+            text, response.url, response.encoding, ignore_tags=ignore_tags\n+        )\n     return _metaref_cache[response]\n \n \n def response_status_message(status: Union[bytes, float, int, str]) -> str:\n-    \"\"\"Return status code plus status text descriptive message\n-    \"\"\"\n+    \"\"\"Return status code plus status text descriptive message\"\"\"\n     status_int = int(status)\n     message = http.RESPONSES.get(status_int, \"Unknown Status\")\n-    return f'{status_int} {to_unicode(message)}'\n+    return f\"{status_int} {to_unicode(message)}\"\n \n \n @deprecated\n@@ -62,7 +66,7 @@ def response_httprepr(response: Response) -> bytes:\n         b\"HTTP/1.1 \",\n         to_bytes(str(response.status)),\n         b\" \",\n-        to_bytes(http.RESPONSES.get(response.status, b'')),\n+        to_bytes(http.RESPONSES.get(response.status, b\"\")),\n         b\"\\r\\n\",\n     ]\n     if response.headers:\n@@ -72,26 +76,29 @@ def response_httprepr(response: Response) -> bytes:\n \n \n def open_in_browser(\n-    response: Union[\"scrapy.http.response.html.HtmlResponse\", \"scrapy.http.response.text.TextResponse\"],\n+    response: Union[\n+        \"scrapy.http.response.html.HtmlResponse\",\n+        \"scrapy.http.response.text.TextResponse\",\n+    ],\n     _openfunc: Callable[[str], Any] = webbrowser.open,\n ) -> Any:\n     \"\"\"Open the given response in a local web browser, populating the <base>\n     tag for external links to work\n     \"\"\"\n     from scrapy.http import HtmlResponse, TextResponse\n+\n     # XXX: this implementation is a bit dirty and could be improved\n     body = response.body\n     if isinstance(response, HtmlResponse):\n-        if b'<base' not in body:\n-            repl = fr'\\1<base href=\"{response.url}\">'\n+        if b\"<base\" not in body:\n+            repl = rf'\\1<base href=\"{response.url}\">'\n             body = re.sub(b\"<!--.*?-->\", b\"\", body, flags=re.DOTALL)\n             body = re.sub(rb\"(<head(?:>|\\s.*?>))\", to_bytes(repl), body)\n-        ext = '.html'\n+        ext = \".html\"\n     elif isinstance(response, TextResponse):\n-        ext = '.txt'\n+        ext = \".txt\"\n     else:\n-        raise TypeError(\"Unsupported response type: \"\n-                        f\"{response.__class__.__name__}\")\n+        raise TypeError(\"Unsupported response type: \" f\"{response.__class__.__name__}\")\n     fd, fname = tempfile.mkstemp(ext)\n     os.write(fd, body)\n     os.close(fd)\n\n@@ -5,7 +5,13 @@ import logging\n from twisted.internet.defer import DeferredList, Deferred\n from twisted.python.failure import Failure\n \n-from pydispatch.dispatcher import Anonymous, Any, disconnect, getAllReceivers, liveReceivers\n+from pydispatch.dispatcher import (\n+    Anonymous,\n+    Any,\n+    disconnect,\n+    getAllReceivers,\n+    liveReceivers,\n+)\n from pydispatch.robustapply import robustApply\n \n from scrapy.exceptions import StopDownload\n@@ -20,24 +26,36 @@ def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n     \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n     Failures instead of exceptions.\n     \"\"\"\n-    dont_log = named.pop('dont_log', ())\n-    dont_log = tuple(dont_log) if isinstance(dont_log, collections.abc.Sequence) else (dont_log,)\n+    dont_log = named.pop(\"dont_log\", ())\n+    dont_log = (\n+        tuple(dont_log)\n+        if isinstance(dont_log, collections.abc.Sequence)\n+        else (dont_log,)\n+    )\n     dont_log += (StopDownload,)\n-    spider = named.get('spider', None)\n+    spider = named.get(\"spider\", None)\n     responses = []\n     for receiver in liveReceivers(getAllReceivers(sender, signal)):\n         try:\n-            response = robustApply(receiver, signal=signal, sender=sender, *arguments, **named)\n+            response = robustApply(\n+                receiver, signal=signal, sender=sender, *arguments, **named\n+            )\n             if isinstance(response, Deferred):\n-                logger.error(\"Cannot return deferreds from signal handler: %(receiver)s\",\n-                             {'receiver': receiver}, extra={'spider': spider})\n+                logger.error(\n+                    \"Cannot return deferreds from signal handler: %(receiver)s\",\n+                    {\"receiver\": receiver},\n+                    extra={\"spider\": spider},\n+                )\n         except dont_log:\n             result = Failure()\n         except Exception:\n             result = Failure()\n-            logger.error(\"Error caught on signal handler: %(receiver)s\",\n-                         {'receiver': receiver},\n-                         exc_info=True, extra={'spider': spider})\n+            logger.error(\n+                \"Error caught on signal handler: %(receiver)s\",\n+                {\"receiver\": receiver},\n+                exc_info=True,\n+                extra={\"spider\": spider},\n+            )\n         else:\n             result = response\n         responses.append((receiver, result))\n@@ -49,19 +67,24 @@ def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n     Returns a deferred that gets fired once all signal handlers deferreds were\n     fired.\n     \"\"\"\n+\n     def logerror(failure, recv):\n         if dont_log is None or not isinstance(failure.value, dont_log):\n-            logger.error(\"Error caught on signal handler: %(receiver)s\",\n-                         {'receiver': recv},\n+            logger.error(\n+                \"Error caught on signal handler: %(receiver)s\",\n+                {\"receiver\": recv},\n                 exc_info=failure_to_exc_info(failure),\n-                         extra={'spider': spider})\n+                extra={\"spider\": spider},\n+            )\n         return failure\n \n-    dont_log = named.pop('dont_log', None)\n-    spider = named.get('spider', None)\n+    dont_log = named.pop(\"dont_log\", None)\n+    spider = named.get(\"spider\", None)\n     dfds = []\n     for receiver in liveReceivers(getAllReceivers(sender, signal)):\n-        d = maybeDeferred_coro(robustApply, receiver, signal=signal, sender=sender, *arguments, **named)\n+        d = maybeDeferred_coro(\n+            robustApply, receiver, signal=signal, sender=sender, *arguments, **named\n+        )\n         d.addErrback(logerror, receiver)\n         d.addBoth(lambda result: (receiver, result))\n         dfds.append(d)\n\n@@ -15,25 +15,27 @@ class Sitemap:\n     (type=sitemapindex) files\"\"\"\n \n     def __init__(self, xmltext):\n-        xmlp = lxml.etree.XMLParser(recover=True, remove_comments=True, resolve_entities=False)\n+        xmlp = lxml.etree.XMLParser(\n+            recover=True, remove_comments=True, resolve_entities=False\n+        )\n         self._root = lxml.etree.fromstring(xmltext, parser=xmlp)\n         rt = self._root.tag\n-        self.type = self._root.tag.split('}', 1)[1] if '}' in rt else rt\n+        self.type = self._root.tag.split(\"}\", 1)[1] if \"}\" in rt else rt\n \n     def __iter__(self):\n         for elem in self._root.getchildren():\n             d = {}\n             for el in elem.getchildren():\n                 tag = el.tag\n-                name = tag.split('}', 1)[1] if '}' in tag else tag\n+                name = tag.split(\"}\", 1)[1] if \"}\" in tag else tag\n \n-                if name == 'link':\n-                    if 'href' in el.attrib:\n-                        d.setdefault('alternate', []).append(el.get('href'))\n+                if name == \"link\":\n+                    if \"href\" in el.attrib:\n+                        d.setdefault(\"alternate\", []).append(el.get(\"href\"))\n                 else:\n-                    d[name] = el.text.strip() if el.text else ''\n+                    d[name] = el.text.strip() if el.text else \"\"\n \n-            if 'loc' in d:\n+            if \"loc\" in d:\n                 yield d\n \n \n@@ -42,6 +44,6 @@ def sitemap_urls_from_robots(robots_text, base_url=None):\n     robots.txt file\n     \"\"\"\n     for line in robots_text.splitlines():\n-        if line.lstrip().lower().startswith('sitemap:'):\n-            url = line.split(':', 1)[1].strip()\n+        if line.lstrip().lower().startswith(\"sitemap:\"):\n+            url = line.split(\":\", 1)[1].strip()\n             yield urljoin(base_url, url)\n\n@@ -32,13 +32,14 @@ def iter_spider_classes(module):\n             inspect.isclass(obj)\n             and issubclass(obj, Spider)\n             and obj.__module__ == module.__name__\n-            and getattr(obj, 'name', None)\n+            and getattr(obj, \"name\", None)\n         ):\n             yield obj\n \n \n-def spidercls_for_request(spider_loader, request, default_spidercls=None,\n-                          log_none=False, log_multiple=False):\n+def spidercls_for_request(\n+    spider_loader, request, default_spidercls=None, log_none=False, log_multiple=False\n+):\n     \"\"\"Return a spider class that handles the given Request.\n \n     This will look for the spiders that can handle the given request (using\n@@ -54,15 +55,18 @@ def spidercls_for_request(spider_loader, request, default_spidercls=None,\n         return spider_loader.load(snames[0])\n \n     if len(snames) > 1 and log_multiple:\n-        logger.error('More than one spider can handle: %(request)s - %(snames)s',\n-                     {'request': request, 'snames': ', '.join(snames)})\n+        logger.error(\n+            \"More than one spider can handle: %(request)s - %(snames)s\",\n+            {\"request\": request, \"snames\": \", \".join(snames)},\n+        )\n \n     if len(snames) == 0 and log_none:\n-        logger.error('Unable to find spider that handles: %(request)s',\n-                     {'request': request})\n+        logger.error(\n+            \"Unable to find spider that handles: %(request)s\", {\"request\": request}\n+        )\n \n     return default_spidercls\n \n \n class DefaultSpider(Spider):\n-    name = 'default'\n+    name = \"default\"\n\n@@ -6,7 +6,7 @@ from scrapy.utils.python import to_unicode\n \n # The OpenSSL symbol is present since 1.1.1 but it's not currently supported in any version of pyOpenSSL.\n # Using the binding directly, as this code does, requires cryptography 2.4.\n-SSL_OP_NO_TLSv1_3 = getattr(pyOpenSSLutil.lib, 'SSL_OP_NO_TLSv1_3', 0)\n+SSL_OP_NO_TLSv1_3 = getattr(pyOpenSSLutil.lib, \"SSL_OP_NO_TLSv1_3\", 0)\n \n \n def ffi_buf_to_string(buf):\n@@ -16,13 +16,17 @@ def ffi_buf_to_string(buf):\n def x509name_to_string(x509name):\n     # from OpenSSL.crypto.X509Name.__repr__\n     result_buffer = pyOpenSSLutil.ffi.new(\"char[]\", 512)\n-    pyOpenSSLutil.lib.X509_NAME_oneline(x509name._name, result_buffer, len(result_buffer))\n+    pyOpenSSLutil.lib.X509_NAME_oneline(\n+        x509name._name, result_buffer, len(result_buffer)\n+    )\n \n     return ffi_buf_to_string(result_buffer)\n \n \n def get_temp_key_info(ssl_object):\n-    if not hasattr(pyOpenSSLutil.lib, 'SSL_get_server_tmp_key'):  # requires OpenSSL 1.0.2\n+    if not hasattr(\n+        pyOpenSSLutil.lib, \"SSL_get_server_tmp_key\"\n+    ):  # requires OpenSSL 1.0.2\n         return None\n \n     # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()\n@@ -36,26 +40,28 @@ def get_temp_key_info(ssl_object):\n     key_info = []\n     key_type = pyOpenSSLutil.lib.EVP_PKEY_id(temp_key)\n     if key_type == pyOpenSSLutil.lib.EVP_PKEY_RSA:\n-        key_info.append('RSA')\n+        key_info.append(\"RSA\")\n     elif key_type == pyOpenSSLutil.lib.EVP_PKEY_DH:\n-        key_info.append('DH')\n+        key_info.append(\"DH\")\n     elif key_type == pyOpenSSLutil.lib.EVP_PKEY_EC:\n-        key_info.append('ECDH')\n+        key_info.append(\"ECDH\")\n         ec_key = pyOpenSSLutil.lib.EVP_PKEY_get1_EC_KEY(temp_key)\n         ec_key = pyOpenSSLutil.ffi.gc(ec_key, pyOpenSSLutil.lib.EC_KEY_free)\n-        nid = pyOpenSSLutil.lib.EC_GROUP_get_curve_name(pyOpenSSLutil.lib.EC_KEY_get0_group(ec_key))\n+        nid = pyOpenSSLutil.lib.EC_GROUP_get_curve_name(\n+            pyOpenSSLutil.lib.EC_KEY_get0_group(ec_key)\n+        )\n         cname = pyOpenSSLutil.lib.EC_curve_nid2nist(nid)\n         if cname == pyOpenSSLutil.ffi.NULL:\n             cname = pyOpenSSLutil.lib.OBJ_nid2sn(nid)\n         key_info.append(ffi_buf_to_string(cname))\n     else:\n         key_info.append(ffi_buf_to_string(pyOpenSSLutil.lib.OBJ_nid2sn(key_type)))\n-    key_info.append(f'{pyOpenSSLutil.lib.EVP_PKEY_bits(temp_key)} bits')\n-    return ', '.join(key_info)\n+    key_info.append(f\"{pyOpenSSLutil.lib.EVP_PKEY_bits(temp_key)} bits\")\n+    return \", \".join(key_info)\n \n \n def get_openssl_version():\n-    system_openssl = OpenSSL.SSL.SSLeay_version(\n-        OpenSSL.SSL.SSLEAY_VERSION\n-    ).decode('ascii', errors='replace')\n-    return f'{OpenSSL.version.__version__} ({system_openssl})'\n+    system_openssl = OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION).decode(\n+        \"ascii\", errors=\"replace\"\n+    )\n+    return f\"{OpenSSL.version.__version__} ({system_openssl})\"\n\n@@ -9,19 +9,19 @@ from typing import Union\n \n def render_templatefile(path: Union[str, PathLike], **kwargs):\n     path_obj = Path(path)\n-    raw = path_obj.read_text('utf8')\n+    raw = path_obj.read_text(\"utf8\")\n \n     content = string.Template(raw).substitute(**kwargs)\n \n-    render_path = path_obj.with_suffix('') if path_obj.suffix == '.tmpl' else path_obj\n+    render_path = path_obj.with_suffix(\"\") if path_obj.suffix == \".tmpl\" else path_obj\n \n-    if path_obj.suffix == '.tmpl':\n+    if path_obj.suffix == \".tmpl\":\n         path_obj.rename(render_path)\n \n-    render_path.write_text(content, 'utf8')\n+    render_path.write_text(content, \"utf8\")\n \n \n-CAMELCASE_INVALID_CHARS = re.compile(r'[^a-zA-Z\\d]')\n+CAMELCASE_INVALID_CHARS = re.compile(r\"[^a-zA-Z\\d]\")\n \n \n def string_camelcase(string):\n@@ -34,4 +34,4 @@ def string_camelcase(string):\n     'MissingImages'\n \n     \"\"\"\n-    return CAMELCASE_INVALID_CHARS.sub('', string.title())\n+    return CAMELCASE_INVALID_CHARS.sub(\"\", string.title())\n\n@@ -15,18 +15,19 @@ from scrapy.utils.boto import is_botocore_available\n \n \n def assert_gcs_environ():\n-    if 'GCS_PROJECT_ID' not in os.environ:\n+    if \"GCS_PROJECT_ID\" not in os.environ:\n         raise SkipTest(\"GCS_PROJECT_ID not found\")\n \n \n def skip_if_no_boto():\n     if not is_botocore_available():\n-        raise SkipTest('missing botocore library')\n+        raise SkipTest(\"missing botocore library\")\n \n \n def get_gcs_content_and_delete(bucket, path):\n     from google.cloud import storage\n-    client = storage.Client(project=os.environ.get('GCS_PROJECT_ID'))\n+\n+    client = storage.Client(project=os.environ.get(\"GCS_PROJECT_ID\"))\n     bucket = client.get_bucket(bucket)\n     blob = bucket.get_blob(path)\n     content = blob.download_as_string()\n@@ -36,9 +37,10 @@ def get_gcs_content_and_delete(bucket, path):\n \n \n def get_ftp_content_and_delete(\n-        path, host, port, username,\n-        password, use_active_mode=False):\n+    path, host, port, username, password, use_active_mode=False\n+):\n     from ftplib import FTP\n+\n     ftp = FTP()\n     ftp.connect(host, port)\n     ftp.login(username, password)\n@@ -48,7 +50,8 @@ def get_ftp_content_and_delete(\n \n     def buffer_data(data):\n         ftp_data.append(data)\n-    ftp.retrbinary(f'RETR {path}', buffer_data)\n+\n+    ftp.retrbinary(f\"RETR {path}\", buffer_data)\n     dirname, filename = split(path)\n     ftp.cwd(dirname)\n     ftp.delete(filename)\n@@ -66,7 +69,7 @@ def get_crawler(spidercls=None, settings_dict=None, prevent_warnings=True):\n     # Set by default settings that prevent deprecation warnings.\n     settings = {}\n     if prevent_warnings:\n-        settings['REQUEST_FINGERPRINTER_IMPLEMENTATION'] = '2.7'\n+        settings[\"REQUEST_FINGERPRINTER_IMPLEMENTATION\"] = \"2.7\"\n     settings.update(settings_dict or {})\n     runner = CrawlerRunner(settings)\n     return runner.create_crawler(spidercls or Spider)\n@@ -75,8 +78,8 @@ def get_crawler(spidercls=None, settings_dict=None, prevent_warnings=True):\n def get_pythonpath() -> str:\n     \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n     installation of Scrapy\"\"\"\n-    scrapy_path = import_module('scrapy').__path__[0]\n-    return str(Path(scrapy_path).parent) + os.pathsep + os.environ.get('PYTHONPATH', '')\n+    scrapy_path = import_module(\"scrapy\").__path__[0]\n+    return str(Path(scrapy_path).parent) + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n \n \n def get_testenv():\n@@ -84,7 +87,7 @@ def get_testenv():\n     this installation of Scrapy, instead of a system installed one.\n     \"\"\"\n     env = os.environ.copy()\n-    env['PYTHONPATH'] = get_pythonpath()\n+    env[\"PYTHONPATH\"] = get_pythonpath()\n     return env\n \n \n@@ -107,6 +110,7 @@ def mock_google_cloud_storage():\n     classes and set their proper return values.\n     \"\"\"\n     from google.cloud.storage import Client, Bucket, Blob\n+\n     client_mock = mock.create_autospec(Client)\n \n     bucket_mock = mock.create_autospec(Bucket)\n@@ -121,5 +125,6 @@ def mock_google_cloud_storage():\n def get_web_client_agent_req(url):\n     from twisted.internet import reactor\n     from twisted.web.client import Agent  # imports twisted.internet.reactor\n+\n     agent = Agent(reactor)\n-    return agent.request(b'GET', url.encode('utf-8'))\n+    return agent.request(b\"GET\", url.encode(\"utf-8\"))\n\n@@ -7,14 +7,15 @@ from twisted.internet import defer, protocol\n class ProcessTest:\n \n     command = None\n-    prefix = [sys.executable, '-m', 'scrapy.cmdline']\n+    prefix = [sys.executable, \"-m\", \"scrapy.cmdline\"]\n     cwd = os.getcwd()  # trial chdirs to temp dir\n \n     def execute(self, args, check_code=True, settings=None):\n         from twisted.internet import reactor\n+\n         env = os.environ.copy()\n         if settings is not None:\n-            env['SCRAPY_SETTINGS_MODULE'] = settings\n+            env[\"SCRAPY_SETTINGS_MODULE\"] = settings\n         cmd = self.prefix + [self.command] + list(args)\n         pp = TestProcessProtocol()\n         pp.deferred.addBoth(self._process_finished, cmd, check_code)\n@@ -32,11 +33,10 @@ class ProcessTest:\n \n \n class TestProcessProtocol(protocol.ProcessProtocol):\n-\n     def __init__(self):\n         self.deferred = defer.Deferred()\n-        self.out = b''\n-        self.err = b''\n+        self.out = b\"\"\n+        self.err = b\"\"\n         self.exitcode = None\n \n     def outReceived(self, data):\n\n@@ -4,9 +4,9 @@ from twisted.web import server, resource, static, util\n \n \n class SiteTest:\n-\n     def setUp(self):\n         from twisted.internet import reactor\n+\n         super().setUp()\n         self.site = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n         self.baseurl = f\"http://localhost:{self.site.getHost().port}/\"\n@@ -22,23 +22,34 @@ class SiteTest:\n class NoMetaRefreshRedirect(util.Redirect):\n     def render(self, request):\n         content = util.Redirect.render(self, request)\n-        return content.replace(b'http-equiv=\\\"refresh\\\"',\n-                               b'http-no-equiv=\\\"do-not-refresh-me\\\"')\n+        return content.replace(\n+            b'http-equiv=\"refresh\"', b'http-no-equiv=\"do-not-refresh-me\"'\n+        )\n \n \n def test_site():\n     r = resource.Resource()\n     r.putChild(b\"text\", static.Data(b\"Works\", \"text/plain\"))\n-    r.putChild(b\"html\", static.Data(b\"<body><p class='one'>Works</p><p class='two'>World</p></body>\", \"text/html\"))\n-    r.putChild(b\"enc-gb18030\", static.Data(b\"<p>gb18030 encoding</p>\", \"text/html; charset=gb18030\"))\n+    r.putChild(\n+        b\"html\",\n+        static.Data(\n+            b\"<body><p class='one'>Works</p><p class='two'>World</p></body>\",\n+            \"text/html\",\n+        ),\n+    )\n+    r.putChild(\n+        b\"enc-gb18030\",\n+        static.Data(b\"<p>gb18030 encoding</p>\", \"text/html; charset=gb18030\"),\n+    )\n     r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n     r.putChild(b\"redirect-no-meta-refresh\", NoMetaRefreshRedirect(b\"/redirected\"))\n     r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n     return server.Site(r)\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     from twisted.internet import reactor\n+\n     port = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n     print(f\"http://localhost:{port.getHost().port}/\")\n     reactor.run()\n\n@@ -35,8 +35,7 @@ def format_live_refs(ignore=NoneType):\n     \"\"\"Return a tabular representation of tracked objects\"\"\"\n     s = \"Live References\\n\\n\"\n     now = time()\n-    for cls, wdict in sorted(live_refs.items(),\n-                             key=lambda x: x[0].__name__):\n+    for cls, wdict in sorted(live_refs.items(), key=lambda x: x[0].__name__):\n         if not wdict:\n             continue\n         if issubclass(cls, ignore):\n\n@@ -21,12 +21,14 @@ def url_is_from_any_domain(url, domains):\n     if not host:\n         return False\n     domains = [d.lower() for d in domains]\n-    return any((host == d) or (host.endswith(f'.{d}')) for d in domains)\n+    return any((host == d) or (host.endswith(f\".{d}\")) for d in domains)\n \n \n def url_is_from_spider(url, spider):\n     \"\"\"Return True if the url belongs to the given spider\"\"\"\n-    return url_is_from_any_domain(url, [spider.name] + list(getattr(spider, 'allowed_domains', [])))\n+    return url_is_from_any_domain(\n+        url, [spider.name] + list(getattr(spider, \"allowed_domains\", []))\n+    )\n \n \n def url_has_any_extension(url, extensions):\n@@ -68,9 +70,9 @@ def escape_ajax(url):\n     'www.example.com/ajax.html'\n     \"\"\"\n     defrag, frag = urldefrag(url)\n-    if not frag.startswith('!'):\n+    if not frag.startswith(\"!\"):\n         return url\n-    return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])\n+    return add_or_replace_parameter(defrag, \"_escaped_fragment_\", frag[1:])\n \n \n def add_http_if_no_scheme(url):\n@@ -87,7 +89,7 @@ def add_http_if_no_scheme(url):\n def _is_posix_path(string):\n     return bool(\n         re.match(\n-            r'''\n+            r\"\"\"\n             ^                   # start with...\n             (\n                 \\.              # ...a single dot,\n@@ -99,7 +101,7 @@ def _is_posix_path(string):\n             )?      # optional match of \".\", \"..\" or \".blabla\"\n             /       # at least one \"/\" for a file path,\n             .       # and something after the \"/\"\n-            ''',\n+            \"\"\",\n             string,\n             flags=re.VERBOSE,\n         )\n@@ -109,13 +111,13 @@ def _is_posix_path(string):\n def _is_windows_path(string):\n     return bool(\n         re.match(\n-            r'''\n+            r\"\"\"\n             ^\n             (\n                 [a-z]:\\\\\n                 | \\\\\\\\\n             )\n-            ''',\n+            \"\"\",\n             string,\n             flags=re.IGNORECASE | re.VERBOSE,\n         )\n@@ -134,7 +136,13 @@ def guess_scheme(url):\n     return add_http_if_no_scheme(url)\n \n \n-def strip_url(url, strip_credentials=True, strip_default_port=True, origin_only=False, strip_fragment=True):\n+def strip_url(\n+    url,\n+    strip_credentials=True,\n+    strip_default_port=True,\n+    origin_only=False,\n+    strip_fragment=True,\n+):\n \n     \"\"\"Strip URL string from some of its components:\n \n@@ -148,18 +156,24 @@ def strip_url(url, strip_credentials=True, strip_default_port=True, origin_only=\n \n     parsed_url = urlparse(url)\n     netloc = parsed_url.netloc\n-    if (strip_credentials or origin_only) and (parsed_url.username or parsed_url.password):\n-        netloc = netloc.split('@')[-1]\n+    if (strip_credentials or origin_only) and (\n+        parsed_url.username or parsed_url.password\n+    ):\n+        netloc = netloc.split(\"@\")[-1]\n     if strip_default_port and parsed_url.port:\n-        if (parsed_url.scheme, parsed_url.port) in (('http', 80),\n-                                                    ('https', 443),\n-                                                    ('ftp', 21)):\n-            netloc = netloc.replace(f':{parsed_url.port}', '')\n-    return urlunparse((\n+        if (parsed_url.scheme, parsed_url.port) in (\n+            (\"http\", 80),\n+            (\"https\", 443),\n+            (\"ftp\", 21),\n+        ):\n+            netloc = netloc.replace(f\":{parsed_url.port}\", \"\")\n+    return urlunparse(\n+        (\n             parsed_url.scheme,\n             netloc,\n-        '/' if origin_only else parsed_url.path,\n-        '' if origin_only else parsed_url.params,\n-        '' if origin_only else parsed_url.query,\n-        '' if strip_fragment else parsed_url.fragment\n-    ))\n+            \"/\" if origin_only else parsed_url.path,\n+            \"\" if origin_only else parsed_url.params,\n+            \"\" if origin_only else parsed_url.query,\n+            \"\" if strip_fragment else parsed_url.fragment,\n+        )\n+    )\n\n@@ -3,7 +3,7 @@ from pkg_resources import parse_version\n from setuptools import setup, find_packages, __version__ as setuptools_version\n \n \n-version = (Path(__file__).parent / 'scrapy/VERSION').read_text('ascii').strip()\n+version = (Path(__file__).parent / \"scrapy/VERSION\").read_text(\"ascii\").strip()\n \n \n def has_environment_marker_platform_impl_support():\n@@ -14,82 +14,82 @@ def has_environment_marker_platform_impl_support():\n     it is 18.5, see:\n     https://setuptools.readthedocs.io/en/latest/history.html#id235\n     \"\"\"\n-    return parse_version(setuptools_version) >= parse_version('18.5')\n+    return parse_version(setuptools_version) >= parse_version(\"18.5\")\n \n \n install_requires = [\n-    'Twisted>=18.9.0',\n-    'cryptography>=3.3',\n-    'cssselect>=0.9.1',\n-    'itemloaders>=1.0.1',\n-    'parsel>=1.5.0',\n-    'pyOpenSSL>=21.0.0',\n-    'queuelib>=1.4.2',\n-    'service_identity>=18.1.0',\n-    'w3lib>=1.17.0',\n-    'zope.interface>=5.1.0',\n-    'protego>=0.1.15',\n-    'itemadapter>=0.1.0',\n-    'setuptools',\n-    'packaging',\n-    'tldextract',\n-    'lxml>=4.3.0',\n+    \"Twisted>=18.9.0\",\n+    \"cryptography>=3.3\",\n+    \"cssselect>=0.9.1\",\n+    \"itemloaders>=1.0.1\",\n+    \"parsel>=1.5.0\",\n+    \"pyOpenSSL>=21.0.0\",\n+    \"queuelib>=1.4.2\",\n+    \"service_identity>=18.1.0\",\n+    \"w3lib>=1.17.0\",\n+    \"zope.interface>=5.1.0\",\n+    \"protego>=0.1.15\",\n+    \"itemadapter>=0.1.0\",\n+    \"setuptools\",\n+    \"packaging\",\n+    \"tldextract\",\n+    \"lxml>=4.3.0\",\n ]\n extras_require = {}\n cpython_dependencies = [\n-    'PyDispatcher>=2.0.5',\n+    \"PyDispatcher>=2.0.5\",\n ]\n if has_environment_marker_platform_impl_support():\n-    extras_require[':platform_python_implementation == \"CPython\"'] = cpython_dependencies\n+    extras_require[\n+        ':platform_python_implementation == \"CPython\"'\n+    ] = cpython_dependencies\n     extras_require[':platform_python_implementation == \"PyPy\"'] = [\n-        'PyPyDispatcher>=2.1.0',\n+        \"PyPyDispatcher>=2.1.0\",\n     ]\n else:\n     install_requires.extend(cpython_dependencies)\n \n \n setup(\n-    name='Scrapy',\n+    name=\"Scrapy\",\n     version=version,\n-    url='https://scrapy.org',\n+    url=\"https://scrapy.org\",\n     project_urls={\n-        'Documentation': 'https://docs.scrapy.org/',\n-        'Source': 'https://github.com/scrapy/scrapy',\n-        'Tracker': 'https://github.com/scrapy/scrapy/issues',\n+        \"Documentation\": \"https://docs.scrapy.org/\",\n+        \"Source\": \"https://github.com/scrapy/scrapy\",\n+        \"Tracker\": \"https://github.com/scrapy/scrapy/issues\",\n     },\n-    description='A high-level Web Crawling and Web Scraping framework',\n-    long_description=open('README.rst', encoding=\"utf-8\").read(),\n-    author='Scrapy developers',\n-    maintainer='Pablo Hoffman',\n-    maintainer_email='pablo@pablohoffman.com',\n-    license='BSD',\n-    packages=find_packages(exclude=('tests', 'tests.*')),\n+    description=\"A high-level Web Crawling and Web Scraping framework\",\n+    long_description=open(\"README.rst\", encoding=\"utf-8\").read(),\n+    author=\"Scrapy developers\",\n+    maintainer=\"Pablo Hoffman\",\n+    maintainer_email=\"pablo@pablohoffman.com\",\n+    license=\"BSD\",\n+    packages=find_packages(exclude=(\"tests\", \"tests.*\")),\n     include_package_data=True,\n     zip_safe=False,\n-    entry_points={\n-        'console_scripts': ['scrapy = scrapy.cmdline:execute']\n-    },\n+    entry_points={\"console_scripts\": [\"scrapy = scrapy.cmdline:execute\"]},\n     classifiers=[\n-        'Framework :: Scrapy',\n-        'Development Status :: 5 - Production/Stable',\n-        'Environment :: Console',\n-        'Intended Audience :: Developers',\n-        'License :: OSI Approved :: BSD License',\n-        'Operating System :: OS Independent',\n-        'Programming Language :: Python',\n-        'Programming Language :: Python :: 3',\n-        'Programming Language :: Python :: 3.7',\n-        'Programming Language :: Python :: 3.8',\n-        'Programming Language :: Python :: 3.9',\n-        'Programming Language :: Python :: 3.10',\n-        'Programming Language :: Python :: 3.11',\n-        'Programming Language :: Python :: Implementation :: CPython',\n-        'Programming Language :: Python :: Implementation :: PyPy',\n-        'Topic :: Internet :: WWW/HTTP',\n-        'Topic :: Software Development :: Libraries :: Application Frameworks',\n-        'Topic :: Software Development :: Libraries :: Python Modules',\n+        \"Framework :: Scrapy\",\n+        \"Development Status :: 5 - Production/Stable\",\n+        \"Environment :: Console\",\n+        \"Intended Audience :: Developers\",\n+        \"License :: OSI Approved :: BSD License\",\n+        \"Operating System :: OS Independent\",\n+        \"Programming Language :: Python\",\n+        \"Programming Language :: Python :: 3\",\n+        \"Programming Language :: Python :: 3.7\",\n+        \"Programming Language :: Python :: 3.8\",\n+        \"Programming Language :: Python :: 3.9\",\n+        \"Programming Language :: Python :: 3.10\",\n+        \"Programming Language :: Python :: 3.11\",\n+        \"Programming Language :: Python :: Implementation :: CPython\",\n+        \"Programming Language :: Python :: Implementation :: PyPy\",\n+        \"Topic :: Internet :: WWW/HTTP\",\n+        \"Topic :: Software Development :: Libraries :: Application Frameworks\",\n+        \"Topic :: Software Development :: Libraries :: Python Modules\",\n     ],\n-    python_requires='>=3.7',\n+    python_requires=\">=3.7\",\n     install_requires=install_requires,\n     extras_require=extras_require,\n )\n\n@@ -3,15 +3,17 @@ from scrapy.crawler import CrawlerProcess\n \n \n class NoRequestsSpider(scrapy.Spider):\n-    name = 'no_request'\n+    name = \"no_request\"\n \n     def start_requests(self):\n         return []\n \n \n-process = CrawlerProcess(settings={\n+process = CrawlerProcess(\n+    settings={\n         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n-    \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\"\n-})\n+        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n+    }\n+)\n process.crawl(NoRequestsSpider)\n process.start()\n\n@@ -37,9 +37,11 @@ if __name__ == \"__main__\":\n     except IndexError:\n         ASYNCIO_EVENT_LOOP = None\n \n-    process = CrawlerProcess(settings={\n+    process = CrawlerProcess(\n+        settings={\n             \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n             \"ASYNCIO_EVENT_LOOP\": ASYNCIO_EVENT_LOOP,\n-    })\n+        }\n+    )\n     process.crawl(UrlSpider)\n     process.start()\n\n@@ -3,14 +3,16 @@ from scrapy.crawler import CrawlerProcess\n \n \n class NoRequestsSpider(scrapy.Spider):\n-    name = 'no_request'\n+    name = \"no_request\"\n \n     def start_requests(self):\n         return []\n \n \n-process = CrawlerProcess(settings={\n+process = CrawlerProcess(\n+    settings={\n         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n-})\n+    }\n+)\n process.crawl(NoRequestsSpider)\n process.start()\n\n@@ -2,6 +2,7 @@ import asyncio\n import sys\n \n from twisted.internet import asyncioreactor\n+\n if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n asyncioreactor.install(asyncio.get_event_loop())\n@@ -11,14 +12,16 @@ from scrapy.crawler import CrawlerProcess  # noqa: E402\n \n \n class NoRequestsSpider(scrapy.Spider):\n-    name = 'no_request'\n+    name = \"no_request\"\n \n     def start_requests(self):\n         return []\n \n \n-process = CrawlerProcess(settings={\n+process = CrawlerProcess(\n+    settings={\n         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n-})\n+    }\n+)\n process.crawl(NoRequestsSpider)\n process.start()\n\n@@ -2,6 +2,7 @@ import asyncio\n import sys\n \n from twisted.internet import asyncioreactor\n+\n if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n asyncioreactor.install(asyncio.get_event_loop())\n@@ -11,15 +12,17 @@ from scrapy.crawler import CrawlerProcess  # noqa: E402\n \n \n class NoRequestsSpider(scrapy.Spider):\n-    name = 'no_request'\n+    name = \"no_request\"\n \n     def start_requests(self):\n         return []\n \n \n-process = CrawlerProcess(settings={\n+process = CrawlerProcess(\n+    settings={\n         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n         \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n-})\n+    }\n+)\n process.crawl(NoRequestsSpider)\n process.start()\n\n@@ -4,6 +4,7 @@ import sys\n from uvloop import Loop\n \n from twisted.internet import asyncioreactor\n+\n if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n asyncio.set_event_loop(Loop())\n@@ -14,15 +15,17 @@ from scrapy.crawler import CrawlerProcess  # noqa: E402\n \n \n class NoRequestsSpider(scrapy.Spider):\n-    name = 'no_request'\n+    name = \"no_request\"\n \n     def start_requests(self):\n         return []\n \n \n-process = CrawlerProcess(settings={\n+process = CrawlerProcess(\n+    settings={\n         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n         \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n-})\n+    }\n+)\n process.crawl(NoRequestsSpider)\n process.start()\n\n@@ -8,6 +8,7 @@ class CachingHostnameResolverSpider(scrapy.Spider):\n     \"\"\"\n     Finishes in a finite amount of time (does not hang indefinitely in the DNS resolution)\n     \"\"\"\n+\n     name = \"caching_hostname_resolver_spider\"\n \n     def start_requests(self):\n@@ -15,16 +16,20 @@ class CachingHostnameResolverSpider(scrapy.Spider):\n \n     def parse(self, response):\n         for _ in range(10):\n-            yield scrapy.Request(response.url, dont_filter=True, callback=self.ignore_response)\n+            yield scrapy.Request(\n+                response.url, dont_filter=True, callback=self.ignore_response\n+            )\n \n     def ignore_response(self, response):\n         self.logger.info(repr(response.ip_address))\n \n \n if __name__ == \"__main__\":\n-    process = CrawlerProcess(settings={\n+    process = CrawlerProcess(\n+        settings={\n             \"RETRY_ENABLED\": False,\n             \"DNS_RESOLVER\": \"scrapy.resolver.CachingHostnameResolver\",\n-    })\n+        }\n+    )\n     process.crawl(CachingHostnameResolverSpider, url=sys.argv[1])\n     process.start()\n\n@@ -6,14 +6,17 @@ class CachingHostnameResolverSpider(scrapy.Spider):\n     \"\"\"\n     Finishes without a twisted.internet.error.DNSLookupError exception\n     \"\"\"\n+\n     name = \"caching_hostname_resolver_spider\"\n     start_urls = [\"http://[::1]\"]\n \n \n if __name__ == \"__main__\":\n-    process = CrawlerProcess(settings={\n+    process = CrawlerProcess(\n+        settings={\n             \"RETRY_ENABLED\": False,\n             \"DNS_RESOLVER\": \"scrapy.resolver.CachingHostnameResolver\",\n-    })\n+        }\n+    )\n     process.crawl(CachingHostnameResolverSpider)\n     process.start()\n\n@@ -7,6 +7,7 @@ class IPv6Spider(scrapy.Spider):\n     Raises a twisted.internet.error.DNSLookupError:\n     the default name resolver does not handle IPv6 addresses.\n     \"\"\"\n+\n     name = \"ipv6_spider\"\n     start_urls = [\"http://[::1]\"]\n \n\n@@ -3,7 +3,7 @@ from scrapy.crawler import CrawlerProcess\n \n \n class NoRequestsSpider(scrapy.Spider):\n-    name = 'no_request'\n+    name = \"no_request\"\n \n     def start_requests(self):\n         return []\n\n@@ -5,7 +5,7 @@ from scrapy.crawler import CrawlerProcess\n \n \n class NoRequestsSpider(scrapy.Spider):\n-    name = 'no_request'\n+    name = \"no_request\"\n \n     def start_requests(self):\n         return []\n\n@@ -5,15 +5,17 @@ from scrapy.crawler import CrawlerProcess\n \n \n class NoRequestsSpider(scrapy.Spider):\n-    name = 'no_request'\n+    name = \"no_request\"\n \n     def start_requests(self):\n         return []\n \n \n-process = CrawlerProcess(settings={\n+process = CrawlerProcess(\n+    settings={\n         \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n-})\n+    }\n+)\n \n process.crawl(NoRequestsSpider)\n process.start()\n\n@@ -7,7 +7,7 @@ selectreactor.install()\n \n \n class NoRequestsSpider(scrapy.Spider):\n-    name = 'no_request'\n+    name = \"no_request\"\n \n     def start_requests(self):\n         return []\n\n@@ -13,15 +13,17 @@ installReactor(reactor)\n \n \n class NoRequestsSpider(scrapy.Spider):\n-    name = 'no_request'\n+    name = \"no_request\"\n \n     def start_requests(self):\n         return []\n \n \n-process = CrawlerProcess(settings={\n+process = CrawlerProcess(\n+    settings={\n         \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n-})\n+    }\n+)\n \n process.crawl(NoRequestsSpider)\n process.start()\n\n@@ -7,15 +7,17 @@ selectreactor.install()\n \n \n class NoRequestsSpider(scrapy.Spider):\n-    name = 'no_request'\n+    name = \"no_request\"\n \n     def start_requests(self):\n         return []\n \n \n-process = CrawlerProcess(settings={\n+process = CrawlerProcess(\n+    settings={\n         \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n-})\n+    }\n+)\n \n process.crawl(NoRequestsSpider)\n process.start()\n\n@@ -3,7 +3,7 @@ from scrapy.crawler import CrawlerProcess\n \n \n class NoRequestsSpider(scrapy.Spider):\n-    name = 'no_request'\n+    name = \"no_request\"\n \n     def start_requests(self):\n         return []\n\n@@ -3,11 +3,13 @@ from scrapy.crawler import CrawlerProcess\n \n \n class AsyncioReactorSpider(scrapy.Spider):\n-    name = 'asyncio_reactor'\n+    name = \"asyncio_reactor\"\n \n \n-process = CrawlerProcess(settings={\n+process = CrawlerProcess(\n+    settings={\n         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n-})\n+    }\n+)\n process.crawl(AsyncioReactorSpider)\n process.start()\n\n@@ -3,7 +3,7 @@ from scrapy.crawler import CrawlerProcess\n \n \n class AsyncioReactorSpider(scrapy.Spider):\n-    name = 'asyncio_reactor'\n+    name = \"asyncio_reactor\"\n     custom_settings = {\n         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n     }\n\n@@ -3,14 +3,14 @@ from scrapy.crawler import CrawlerProcess\n \n \n class SelectReactorSpider(scrapy.Spider):\n-    name = 'select_reactor'\n+    name = \"select_reactor\"\n     custom_settings = {\n         \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n     }\n \n \n class AsyncioReactorSpider(scrapy.Spider):\n-    name = 'asyncio_reactor'\n+    name = \"asyncio_reactor\"\n     custom_settings = {\n         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n     }\n\n@@ -3,14 +3,14 @@ from scrapy.crawler import CrawlerProcess\n \n \n class AsyncioReactorSpider1(scrapy.Spider):\n-    name = 'asyncio_reactor1'\n+    name = \"asyncio_reactor1\"\n     custom_settings = {\n         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n     }\n \n \n class AsyncioReactorSpider2(scrapy.Spider):\n-    name = 'asyncio_reactor2'\n+    name = \"asyncio_reactor2\"\n     custom_settings = {\n         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n     }\n\n@@ -3,11 +3,13 @@ from scrapy.crawler import CrawlerProcess\n \n \n class PollReactorSpider(scrapy.Spider):\n-    name = 'poll_reactor'\n+    name = \"poll_reactor\"\n \n \n-process = CrawlerProcess(settings={\n+process = CrawlerProcess(\n+    settings={\n         \"TWISTED_REACTOR\": \"twisted.internet.pollreactor.PollReactor\",\n-})\n+    }\n+)\n process.crawl(PollReactorSpider)\n process.start()\n\n@@ -3,11 +3,13 @@ from scrapy.crawler import CrawlerProcess\n \n \n class SelectReactorSpider(scrapy.Spider):\n-    name = 'epoll_reactor'\n+    name = \"epoll_reactor\"\n \n \n-process = CrawlerProcess(settings={\n+process = CrawlerProcess(\n+    settings={\n         \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n-})\n+    }\n+)\n process.crawl(SelectReactorSpider)\n process.start()\n\n@@ -14,7 +14,7 @@ from tests.mockserver import MockServer, MockDNSServer\n # https://stackoverflow.com/a/32784190\n def createResolver(servers=None, resolvconf=None, hosts=None):\n     if hosts is None:\n-        hosts = b'/etc/hosts' if platform.getType() == 'posix' else r'c:\\windows\\hosts'\n+        hosts = b\"/etc/hosts\" if platform.getType() == \"posix\" else r\"c:\\windows\\hosts\"\n     theResolver = Resolver(resolvconf, servers)\n     hostResolver = hostsModule.Resolver(hosts)\n     chain = [hostResolver, cache.CacheResolver(), theResolver]\n\n@@ -11,24 +11,24 @@ from pathlib import Path\n # ignore system-wide proxies for tests\n # which would send requests to a totally unsuspecting server\n # (e.g. because urllib does not fully understand the proxy spec)\n-os.environ['http_proxy'] = ''\n-os.environ['https_proxy'] = ''\n-os.environ['ftp_proxy'] = ''\n+os.environ[\"http_proxy\"] = \"\"\n+os.environ[\"https_proxy\"] = \"\"\n+os.environ[\"ftp_proxy\"] = \"\"\n \n # Absolutize paths to coverage config and output file because tests that\n # spawn subprocesses also changes current working directory.\n _sourceroot = Path(__file__).resolve().parent.parent\n-if 'COV_CORE_CONFIG' in os.environ:\n-    os.environ['COVERAGE_FILE'] = str(_sourceroot / '.coverage')\n-    os.environ['COV_CORE_CONFIG'] = str(_sourceroot / os.environ['COV_CORE_CONFIG'])\n+if \"COV_CORE_CONFIG\" in os.environ:\n+    os.environ[\"COVERAGE_FILE\"] = str(_sourceroot / \".coverage\")\n+    os.environ[\"COV_CORE_CONFIG\"] = str(_sourceroot / os.environ[\"COV_CORE_CONFIG\"])\n \n-tests_datadir = str(Path(__file__).parent.resolve() / 'sample_data')\n+tests_datadir = str(Path(__file__).parent.resolve() / \"sample_data\")\n \n \n # In some environments accessing a non-existing host doesn't raise an\n # error. In such cases we're going to skip tests which rely on it.\n try:\n-    socket.getaddrinfo('non-existing-host', 80)\n+    socket.getaddrinfo(\"non-existing-host\", 80)\n     NON_EXISTING_RESOLVABLE = True\n except socket.gaierror:\n     NON_EXISTING_RESOLVABLE = False\n\n@@ -7,18 +7,18 @@ from pyftpdlib.servers import FTPServer\n \n def main():\n     parser = ArgumentParser()\n-    parser.add_argument('-d', '--directory')\n+    parser.add_argument(\"-d\", \"--directory\")\n     args = parser.parse_args()\n \n     authorizer = DummyAuthorizer()\n-    full_permissions = 'elradfmwMT'\n+    full_permissions = \"elradfmwMT\"\n     authorizer.add_anonymous(args.directory, perm=full_permissions)\n     handler = FTPHandler\n     handler.authorizer = authorizer\n-    address = ('127.0.0.1', 2121)\n+    address = (\"127.0.0.1\", 2121)\n     server = FTPServer(address, handler)\n     server.serve_forever()\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     main()\n\n@@ -29,7 +29,7 @@ def generate_keys():\n         key_size=2048,\n         backend=default_backend(),\n     )\n-    (folder / 'localhost.key').write_bytes(\n+    (folder / \"localhost.key\").write_bytes(\n         key.private_bytes(\n             encoding=Encoding.PEM,\n             format=PrivateFormat.TraditionalOpenSSL,\n@@ -58,4 +58,4 @@ def generate_keys():\n         )\n         .sign(key, SHA256(), default_backend())\n     )\n-    (folder / 'localhost.crt').write_bytes(cert.public_bytes(Encoding.PEM))\n+    (folder / \"localhost.crt\").write_bytes(cert.public_bytes(Encoding.PEM))\n\n@@ -4,6 +4,7 @@ import collections\n \n class DummyDB(dict):\n     \"\"\"Provide dummy DBM-like interface.\"\"\"\n+\n     def close(self):\n         pass\n \n@@ -14,7 +15,7 @@ error = KeyError\n _DATABASES = collections.defaultdict(DummyDB)\n \n \n-def open(file, flag='r', mode=0o666):\n+def open(file, flag=\"r\", mode=0o666):\n     \"\"\"Open or create a dummy database compatible.\n \n     Arguments ``flag`` and ``mode`` are ignored.\n\n@@ -111,7 +111,6 @@ class LeafResource(resource.Resource):\n \n \n class Follow(LeafResource):\n-\n     def render(self, request):\n         total = getarg(request, b\"total\", 100, type=int)\n         show = getarg(request, b\"show\", 1, type=int)\n@@ -140,13 +139,12 @@ class Follow(LeafResource):\n \n \n class Delay(LeafResource):\n-\n     def render_GET(self, request):\n         n = getarg(request, b\"n\", 1, type=float)\n         b = getarg(request, b\"b\", 1, type=int)\n         if b:\n             # send headers now and delay body\n-            request.write('')\n+            request.write(\"\")\n         self.deferRequest(request, n, self._delayedRender, request, n)\n         return NOT_DONE_YET\n \n@@ -156,7 +154,6 @@ class Delay(LeafResource):\n \n \n class Status(LeafResource):\n-\n     def render_GET(self, request):\n         n = getarg(request, b\"n\", 200, type=int)\n         request.setResponseCode(n)\n@@ -164,15 +161,15 @@ class Status(LeafResource):\n \n \n class Raw(LeafResource):\n-\n     def render_GET(self, request):\n         request.startedWriting = 1\n         self.deferRequest(request, 0, self._delayedRender, request)\n         return NOT_DONE_YET\n+\n     render_POST = render_GET\n \n     def _delayedRender(self, request):\n-        raw = getarg(request, b'raw', b'HTTP 1.1 200 OK\\n')\n+        raw = getarg(request, b\"raw\", b\"HTTP 1.1 200 OK\\n\")\n         request.startedWriting = 1\n         request.write(raw)\n         request.channel.transport.loseConnection()\n@@ -180,30 +177,29 @@ class Raw(LeafResource):\n \n \n class Echo(LeafResource):\n-\n     def render_GET(self, request):\n         output = {\n-            'headers': dict(\n+            \"headers\": dict(\n                 (to_unicode(k), [to_unicode(v) for v in vs])\n-                for k, vs in request.requestHeaders.getAllRawHeaders()),\n-            'body': to_unicode(request.content.read()),\n+                for k, vs in request.requestHeaders.getAllRawHeaders()\n+            ),\n+            \"body\": to_unicode(request.content.read()),\n         }\n         return to_bytes(json.dumps(output))\n+\n     render_POST = render_GET\n \n \n class RedirectTo(LeafResource):\n-\n     def render(self, request):\n-        goto = getarg(request, b'goto', b'/')\n+        goto = getarg(request, b\"goto\", b\"/\")\n         # we force the body content, otherwise Twisted redirectTo()\n         # returns HTML with <meta http-equiv=\"refresh\"\n         redirectTo(goto, request)\n-        return b'redirecting...'\n+        return b\"redirecting...\"\n \n \n class Partial(LeafResource):\n-\n     def render_GET(self, request):\n         request.setHeader(b\"Content-Length\", b\"1024\")\n         self.deferRequest(request, 0, self._delayedRender, request)\n@@ -215,13 +211,12 @@ class Partial(LeafResource):\n \n \n class Drop(Partial):\n-\n     def _delayedRender(self, request):\n         abort = getarg(request, b\"abort\", 0, type=int)\n         request.write(b\"this connection will be dropped\\n\")\n         tr = request.channel.transport\n         try:\n-            if abort and hasattr(tr, 'abortConnection'):\n+            if abort and hasattr(tr, \"abortConnection\"):\n                 tr.abortConnection()\n             else:\n                 tr.loseConnection()\n@@ -230,13 +225,11 @@ class Drop(Partial):\n \n \n class ArbitraryLengthPayloadResource(LeafResource):\n-\n     def render(self, request):\n         return request.content.read()\n \n \n class Root(resource.Resource):\n-\n     def __init__(self):\n         resource.Resource.__init__(self)\n         self.putChild(b\"status\", Status())\n@@ -247,11 +240,15 @@ class Root(resource.Resource):\n         self.putChild(b\"raw\", Raw())\n         self.putChild(b\"echo\", Echo())\n         self.putChild(b\"payload\", PayloadResource())\n-        self.putChild(b\"xpayload\", resource.EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]))\n+        self.putChild(\n+            b\"xpayload\",\n+            resource.EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]),\n+        )\n         self.putChild(b\"alpayload\", ArbitraryLengthPayloadResource())\n         try:\n             from tests import tests_datadir\n-            self.putChild(b\"files\", File(str(Path(tests_datadir, 'test_site/files/'))))\n+\n+            self.putChild(b\"files\", File(str(Path(tests_datadir, \"test_site/files/\"))))\n         except Exception:\n             pass\n         self.putChild(b\"redirect-to\", RedirectTo())\n@@ -260,16 +257,18 @@ class Root(resource.Resource):\n         return self\n \n     def render(self, request):\n-        return b'Scrapy mock HTTP server\\n'\n+        return b\"Scrapy mock HTTP server\\n\"\n \n \n class MockServer:\n-\n     def __enter__(self):\n-        self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver', '-t', 'http'],\n-                          stdout=PIPE, env=get_testenv())\n-        http_address = self.proc.stdout.readline().strip().decode('ascii')\n-        https_address = self.proc.stdout.readline().strip().decode('ascii')\n+        self.proc = Popen(\n+            [sys.executable, \"-u\", \"-m\", \"tests.mockserver\", \"-t\", \"http\"],\n+            stdout=PIPE,\n+            env=get_testenv(),\n+        )\n+        http_address = self.proc.stdout.readline().strip().decode(\"ascii\")\n+        https_address = self.proc.stdout.readline().strip().decode(\"ascii\")\n \n         self.http_address = http_address\n         self.https_address = https_address\n@@ -282,7 +281,7 @@ class MockServer:\n \n     def url(self, path, is_secure=False):\n         host = self.https_address if is_secure else self.http_address\n-        host = host.replace('0.0.0.0', '127.0.0.1')\n+        host = host.replace(\"0.0.0.0\", \"127.0.0.1\")\n         return host + path\n \n \n@@ -306,12 +305,16 @@ class MockDNSResolver:\n \n \n class MockDNSServer:\n-\n     def __enter__(self):\n-        self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver', '-t', 'dns'],\n-                          stdout=PIPE, env=get_testenv())\n-        self.host = '127.0.0.1'\n-        self.port = int(self.proc.stdout.readline().strip().decode('ascii').split(\":\")[1])\n+        self.proc = Popen(\n+            [sys.executable, \"-u\", \"-m\", \"tests.mockserver\", \"-t\", \"dns\"],\n+            stdout=PIPE,\n+            env=get_testenv(),\n+        )\n+        self.host = \"127.0.0.1\"\n+        self.port = int(\n+            self.proc.stdout.readline().strip().decode(\"ascii\").split(\":\")[1]\n+        )\n         return self\n \n     def __exit__(self, exc_type, exc_value, traceback):\n@@ -326,10 +329,13 @@ class MockFTPServer:\n \n     def __enter__(self):\n         self.path = Path(mkdtemp())\n-        self.proc = Popen([sys.executable, '-u', '-m', 'tests.ftpserver', '-d', str(self.path)],\n-                          stderr=PIPE, env=get_testenv())\n+        self.proc = Popen(\n+            [sys.executable, \"-u\", \"-m\", \"tests.ftpserver\", \"-d\", str(self.path)],\n+            stderr=PIPE,\n+            env=get_testenv(),\n+        )\n         for line in self.proc.stderr:\n-            if b'starting FTP server' in line:\n+            if b\"starting FTP server\" in line:\n                 break\n         return self\n \n@@ -339,10 +345,12 @@ class MockFTPServer:\n         self.proc.communicate()\n \n     def url(self, path):\n-        return 'ftp://127.0.0.1:2121/' + path\n+        return \"ftp://127.0.0.1:2121/\" + path\n \n \n-def ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.crt', cipher_string=None):\n+def ssl_context_factory(\n+    keyfile=\"keys/localhost.key\", certfile=\"keys/localhost.crt\", cipher_string=None\n+):\n     factory = ssl.DefaultOpenSSLContextFactory(\n         str(Path(__file__).parent / keyfile),\n         str(Path(__file__).parent / certfile),\n@@ -357,7 +365,9 @@ def ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.c\n \n if __name__ == \"__main__\":\n     parser = argparse.ArgumentParser()\n-    parser.add_argument(\"-t\", \"--type\", type=str, choices=(\"http\", \"dns\"), default=\"http\")\n+    parser.add_argument(\n+        \"-t\", \"--type\", type=str, choices=(\"http\", \"dns\"), default=\"http\"\n+    )\n     args = parser.parse_args()\n \n     if args.type == \"http\":\n@@ -370,8 +380,8 @@ if __name__ == \"__main__\":\n         def print_listening():\n             httpHost = httpPort.getHost()\n             httpsHost = httpsPort.getHost()\n-            httpAddress = f'http://{httpHost.host}:{httpHost.port}'\n-            httpsAddress = f'https://{httpsHost.host}:{httpsHost.port}'\n+            httpAddress = f\"http://{httpHost.host}:{httpHost.port}\"\n+            httpsAddress = f\"https://{httpsHost.host}:{httpsHost.port}\"\n             print(httpAddress)\n             print(httpsAddress)\n \n\n@@ -4,7 +4,6 @@ Some pipelines used for testing\n \n \n class ZeroDivisionErrorPipeline:\n-\n     def open_spider(self, spider):\n         1 / 0\n \n@@ -13,6 +12,5 @@ class ZeroDivisionErrorPipeline:\n \n \n class ProcessWithZeroDivisionErrorPipiline:\n-\n     def process_item(self, item, spider):\n         1 / 0\n\n@@ -26,26 +26,28 @@ class MockServerSpider(Spider):\n \n class MetaSpider(MockServerSpider):\n \n-    name = 'meta'\n+    name = \"meta\"\n \n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self.meta = {}\n \n     def closed(self, reason):\n-        self.meta['close_reason'] = reason\n+        self.meta[\"close_reason\"] = reason\n \n \n class FollowAllSpider(MetaSpider):\n \n-    name = 'follow'\n+    name = \"follow\"\n     link_extractor = LinkExtractor()\n \n-    def __init__(self, total=10, show=20, order=\"rand\", maxlatency=0.0, *args, **kwargs):\n+    def __init__(\n+        self, total=10, show=20, order=\"rand\", maxlatency=0.0, *args, **kwargs\n+    ):\n         super().__init__(*args, **kwargs)\n         self.urls_visited = []\n         self.times = []\n-        qargs = {'total': total, 'show': show, 'order': order, 'maxlatency': maxlatency}\n+        qargs = {\"total\": total, \"show\": show, \"order\": order, \"maxlatency\": maxlatency}\n         url = self.mockserver.url(f\"/follow?{urlencode(qargs, doseq=True)}\")\n         self.start_urls = [url]\n \n@@ -58,7 +60,7 @@ class FollowAllSpider(MetaSpider):\n \n class DelaySpider(MetaSpider):\n \n-    name = 'delay'\n+    name = \"delay\"\n \n     def __init__(self, n=1, b=0, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n@@ -80,7 +82,7 @@ class DelaySpider(MetaSpider):\n \n class SimpleSpider(MetaSpider):\n \n-    name = 'simple'\n+    name = \"simple\"\n \n     def __init__(self, url=\"http://localhost:8998\", *args, **kwargs):\n         super().__init__(*args, **kwargs)\n@@ -92,7 +94,7 @@ class SimpleSpider(MetaSpider):\n \n class AsyncDefSpider(SimpleSpider):\n \n-    name = 'asyncdef'\n+    name = \"asyncdef\"\n \n     async def parse(self, response):\n         await defer.succeed(42)\n@@ -101,7 +103,7 @@ class AsyncDefSpider(SimpleSpider):\n \n class AsyncDefAsyncioSpider(SimpleSpider):\n \n-    name = 'asyncdef_asyncio'\n+    name = \"asyncdef_asyncio\"\n \n     async def parse(self, response):\n         await asyncio.sleep(0.2)\n@@ -111,13 +113,13 @@ class AsyncDefAsyncioSpider(SimpleSpider):\n \n class AsyncDefAsyncioReturnSpider(SimpleSpider):\n \n-    name = 'asyncdef_asyncio_return'\n+    name = \"asyncdef_asyncio_return\"\n \n     async def parse(self, response):\n         await asyncio.sleep(0.2)\n         status = await get_from_asyncio_queue(response.status)\n         self.logger.info(f\"Got response {status}\")\n-        return [{'id': 1}, {'id': 2}]\n+        return [{\"id\": 1}, {\"id\": 2}]\n \n \n class AsyncDefAsyncioReturnSingleElementSpider(SimpleSpider):\n@@ -133,113 +135,119 @@ class AsyncDefAsyncioReturnSingleElementSpider(SimpleSpider):\n \n class AsyncDefAsyncioReqsReturnSpider(SimpleSpider):\n \n-    name = 'asyncdef_asyncio_reqs_return'\n+    name = \"asyncdef_asyncio_reqs_return\"\n \n     async def parse(self, response):\n         await asyncio.sleep(0.2)\n-        req_id = response.meta.get('req_id', 0)\n+        req_id = response.meta.get(\"req_id\", 0)\n         status = await get_from_asyncio_queue(response.status)\n         self.logger.info(f\"Got response {status}, req_id {req_id}\")\n         if req_id > 0:\n             return\n         reqs = []\n         for i in range(1, 3):\n-            req = Request(self.start_urls[0], dont_filter=True, meta={'req_id': i})\n+            req = Request(self.start_urls[0], dont_filter=True, meta={\"req_id\": i})\n             reqs.append(req)\n         return reqs\n \n \n class AsyncDefAsyncioGenExcSpider(SimpleSpider):\n-    name = 'asyncdef_asyncio_gen_exc'\n+    name = \"asyncdef_asyncio_gen_exc\"\n \n     async def parse(self, response):\n         for i in range(10):\n             await asyncio.sleep(0.1)\n-            yield {'foo': i}\n+            yield {\"foo\": i}\n             if i > 5:\n                 raise ValueError(\"Stopping the processing\")\n \n \n class AsyncDefDeferredDirectSpider(SimpleSpider):\n-    name = 'asyncdef_deferred_direct'\n+    name = \"asyncdef_deferred_direct\"\n \n     async def parse(self, response):\n         resp = await get_web_client_agent_req(self.mockserver.url(\"/status?n=200\"))\n-        yield {'code': resp.code}\n+        yield {\"code\": resp.code}\n \n \n class AsyncDefDeferredWrappedSpider(SimpleSpider):\n-    name = 'asyncdef_deferred_wrapped'\n+    name = \"asyncdef_deferred_wrapped\"\n \n     async def parse(self, response):\n-        resp = await deferred_to_future(get_web_client_agent_req(self.mockserver.url(\"/status?n=200\")))\n-        yield {'code': resp.code}\n+        resp = await deferred_to_future(\n+            get_web_client_agent_req(self.mockserver.url(\"/status?n=200\"))\n+        )\n+        yield {\"code\": resp.code}\n \n \n class AsyncDefDeferredMaybeWrappedSpider(SimpleSpider):\n-    name = 'asyncdef_deferred_wrapped'\n+    name = \"asyncdef_deferred_wrapped\"\n \n     async def parse(self, response):\n-        resp = await maybe_deferred_to_future(get_web_client_agent_req(self.mockserver.url(\"/status?n=200\")))\n-        yield {'code': resp.code}\n+        resp = await maybe_deferred_to_future(\n+            get_web_client_agent_req(self.mockserver.url(\"/status?n=200\"))\n+        )\n+        yield {\"code\": resp.code}\n \n \n class AsyncDefAsyncioGenSpider(SimpleSpider):\n \n-    name = 'asyncdef_asyncio_gen'\n+    name = \"asyncdef_asyncio_gen\"\n \n     async def parse(self, response):\n         await asyncio.sleep(0.2)\n-        yield {'foo': 42}\n+        yield {\"foo\": 42}\n         self.logger.info(f\"Got response {response.status}\")\n \n \n class AsyncDefAsyncioGenLoopSpider(SimpleSpider):\n \n-    name = 'asyncdef_asyncio_gen_loop'\n+    name = \"asyncdef_asyncio_gen_loop\"\n \n     async def parse(self, response):\n         for i in range(10):\n             await asyncio.sleep(0.1)\n-            yield {'foo': i}\n+            yield {\"foo\": i}\n         self.logger.info(f\"Got response {response.status}\")\n \n \n class AsyncDefAsyncioGenComplexSpider(SimpleSpider):\n \n-    name = 'asyncdef_asyncio_gen_complex'\n+    name = \"asyncdef_asyncio_gen_complex\"\n     initial_reqs = 4\n     following_reqs = 3\n     depth = 2\n \n     def _get_req(self, index, cb=None):\n-        return Request(self.mockserver.url(f\"/status?n=200&request={index}\"),\n-                       meta={'index': index},\n+        return Request(\n+            self.mockserver.url(f\"/status?n=200&request={index}\"),\n+            meta={\"index\": index},\n             dont_filter=True,\n-                       callback=cb)\n+            callback=cb,\n+        )\n \n     def start_requests(self):\n         for i in range(1, self.initial_reqs + 1):\n             yield self._get_req(i)\n \n     async def parse(self, response):\n-        index = response.meta['index']\n-        yield {'index': index}\n+        index = response.meta[\"index\"]\n+        yield {\"index\": index}\n         if index < 10**self.depth:\n             for new_index in range(10 * index, 10 * index + self.following_reqs):\n                 yield self._get_req(new_index)\n         yield self._get_req(index, cb=self.parse2)\n         await asyncio.sleep(0.1)\n-        yield {'index': index + 5}\n+        yield {\"index\": index + 5}\n \n     async def parse2(self, response):\n         await asyncio.sleep(0.1)\n-        yield {'index2': response.meta['index']}\n+        yield {\"index2\": response.meta[\"index\"]}\n \n \n class ItemSpider(FollowAllSpider):\n \n-    name = 'item'\n+    name = \"item\"\n \n     def parse(self, response):\n         for request in super().parse(response):\n@@ -254,11 +262,11 @@ class DefaultError(Exception):\n \n class ErrorSpider(FollowAllSpider):\n \n-    name = 'error'\n+    name = \"error\"\n     exception_cls = DefaultError\n \n     def raise_exception(self):\n-        raise self.exception_cls('Expected exception')\n+        raise self.exception_cls(\"Expected exception\")\n \n     def parse(self, response):\n         for request in super().parse(response):\n@@ -280,16 +288,18 @@ class BrokenStartRequestsSpider(FollowAllSpider):\n             1 / 0\n \n         for s in range(100):\n-            qargs = {'total': 10, 'seed': s}\n+            qargs = {\"total\": 10, \"seed\": s}\n             url = self.mockserver.url(f\"/follow?{urlencode(qargs, doseq=True)}\")\n-            yield Request(url, meta={'seed': s})\n+            yield Request(url, meta={\"seed\": s})\n             if self.fail_yielding:\n                 2 / 0\n \n-        assert self.seedsseen, 'All start requests consumed before any download happened'\n+        assert (\n+            self.seedsseen\n+        ), \"All start requests consumed before any download happened\"\n \n     def parse(self, response):\n-        self.seedsseen.append(response.meta.get('seed'))\n+        self.seedsseen.append(response.meta.get(\"seed\"))\n         for req in super().parse(response):\n             yield req\n \n@@ -307,21 +317,21 @@ class SingleRequestSpider(MetaSpider):\n             yield Request(self.seed, callback=self.parse, errback=self.on_error)\n \n     def parse(self, response):\n-        self.meta.setdefault('responses', []).append(response)\n+        self.meta.setdefault(\"responses\", []).append(response)\n         if callable(self.callback_func):\n             return self.callback_func(response)\n-        if 'next' in response.meta:\n-            return response.meta['next']\n+        if \"next\" in response.meta:\n+            return response.meta[\"next\"]\n \n     def on_error(self, failure):\n-        self.meta['failure'] = failure\n+        self.meta[\"failure\"] = failure\n         if callable(self.errback_func):\n             return self.errback_func(failure)\n \n \n class DuplicateStartRequestsSpider(MockServerSpider):\n     dont_filter = True\n-    name = 'duplicatestartrequests'\n+    name = \"duplicatestartrequests\"\n     distinct_urls = 2\n     dupe_factor = 3\n \n@@ -343,13 +353,12 @@ class CrawlSpiderWithParseMethod(MockServerSpider, CrawlSpider):\n     \"\"\"\n     A CrawlSpider which overrides the 'parse' method\n     \"\"\"\n-    name = 'crawl_spider_with_parse_method'\n+\n+    name = \"crawl_spider_with_parse_method\"\n     custom_settings: dict = {\n-        'RETRY_HTTP_CODES': [],  # no need to retry\n+        \"RETRY_HTTP_CODES\": [],  # no need to retry\n     }\n-    rules = (\n-        Rule(LinkExtractor(), callback='parse', follow=True),\n-    )\n+    rules = (Rule(LinkExtractor(), callback=\"parse\", follow=True),)\n \n     def start_requests(self):\n         test_body = b\"\"\"\n@@ -365,39 +374,45 @@ class CrawlSpiderWithParseMethod(MockServerSpider, CrawlSpider):\n         yield Request(url, method=\"POST\", body=test_body)\n \n     def parse(self, response, foo=None):\n-        self.logger.info('[parse] status %i (foo: %s)', response.status, foo)\n-        yield Request(self.mockserver.url(\"/status?n=202\"), self.parse, cb_kwargs={\"foo\": \"bar\"})\n+        self.logger.info(\"[parse] status %i (foo: %s)\", response.status, foo)\n+        yield Request(\n+            self.mockserver.url(\"/status?n=202\"), self.parse, cb_kwargs={\"foo\": \"bar\"}\n+        )\n \n \n class CrawlSpiderWithAsyncCallback(CrawlSpiderWithParseMethod):\n     \"\"\"A CrawlSpider with an async def callback\"\"\"\n-    name = 'crawl_spider_with_async_callback'\n-    rules = (\n-        Rule(LinkExtractor(), callback='parse_async', follow=True),\n-    )\n+\n+    name = \"crawl_spider_with_async_callback\"\n+    rules = (Rule(LinkExtractor(), callback=\"parse_async\", follow=True),)\n \n     async def parse_async(self, response, foo=None):\n-        self.logger.info('[parse_async] status %i (foo: %s)', response.status, foo)\n-        return Request(self.mockserver.url(\"/status?n=202\"), self.parse_async, cb_kwargs={\"foo\": \"bar\"})\n+        self.logger.info(\"[parse_async] status %i (foo: %s)\", response.status, foo)\n+        return Request(\n+            self.mockserver.url(\"/status?n=202\"),\n+            self.parse_async,\n+            cb_kwargs={\"foo\": \"bar\"},\n+        )\n \n \n class CrawlSpiderWithAsyncGeneratorCallback(CrawlSpiderWithParseMethod):\n     \"\"\"A CrawlSpider with an async generator callback\"\"\"\n-    name = 'crawl_spider_with_async_generator_callback'\n-    rules = (\n-        Rule(LinkExtractor(), callback='parse_async_gen', follow=True),\n-    )\n+\n+    name = \"crawl_spider_with_async_generator_callback\"\n+    rules = (Rule(LinkExtractor(), callback=\"parse_async_gen\", follow=True),)\n \n     async def parse_async_gen(self, response, foo=None):\n-        self.logger.info('[parse_async_gen] status %i (foo: %s)', response.status, foo)\n-        yield Request(self.mockserver.url(\"/status?n=202\"), self.parse_async_gen, cb_kwargs={\"foo\": \"bar\"})\n+        self.logger.info(\"[parse_async_gen] status %i (foo: %s)\", response.status, foo)\n+        yield Request(\n+            self.mockserver.url(\"/status?n=202\"),\n+            self.parse_async_gen,\n+            cb_kwargs={\"foo\": \"bar\"},\n+        )\n \n \n class CrawlSpiderWithErrback(CrawlSpiderWithParseMethod):\n-    name = 'crawl_spider_with_errback'\n-    rules = (\n-        Rule(LinkExtractor(), callback='parse', errback='errback', follow=True),\n-    )\n+    name = \"crawl_spider_with_errback\"\n+    rules = (Rule(LinkExtractor(), callback=\"parse\", errback=\"errback\", follow=True),)\n \n     def start_requests(self):\n         test_body = b\"\"\"\n@@ -416,13 +431,18 @@ class CrawlSpiderWithErrback(CrawlSpiderWithParseMethod):\n         yield Request(url, method=\"POST\", body=test_body)\n \n     def errback(self, failure):\n-        self.logger.info('[errback] status %i', failure.value.response.status)\n+        self.logger.info(\"[errback] status %i\", failure.value.response.status)\n \n \n class CrawlSpiderWithProcessRequestCallbackKeywordArguments(CrawlSpiderWithParseMethod):\n-    name = 'crawl_spider_with_process_request_cb_kwargs'\n+    name = \"crawl_spider_with_process_request_cb_kwargs\"\n     rules = (\n-        Rule(LinkExtractor(), callback='parse', follow=True, process_request=\"process_request\"),\n+        Rule(\n+            LinkExtractor(),\n+            callback=\"parse\",\n+            follow=True,\n+            process_request=\"process_request\",\n+        ),\n     )\n \n     def process_request(self, request, response):\n@@ -457,14 +477,12 @@ class BytesReceivedCallbackSpider(MetaSpider):\n \n \n class BytesReceivedErrbackSpider(BytesReceivedCallbackSpider):\n-\n     def bytes_received(self, data, request, spider):\n         self.meta[\"bytes_received\"] = data\n         raise StopDownload(fail=True)\n \n \n class HeadersReceivedCallbackSpider(MetaSpider):\n-\n     @classmethod\n     def from_crawler(cls, crawler, *args, **kwargs):\n         spider = super().from_crawler(crawler, *args, **kwargs)\n@@ -486,7 +504,6 @@ class HeadersReceivedCallbackSpider(MetaSpider):\n \n \n class HeadersReceivedErrbackSpider(HeadersReceivedCallbackSpider):\n-\n     def headers_received(self, headers, body_length, request, spider):\n         self.meta[\"headers_received\"] = headers\n         raise StopDownload(fail=True)\n\n@@ -6,7 +6,6 @@ from tests.mockserver import MockServer\n \n \n class TestCloseSpider(TestCase):\n-\n     def setUp(self):\n         self.mockserver = MockServer()\n         self.mockserver.__enter__()\n@@ -17,40 +16,40 @@ class TestCloseSpider(TestCase):\n     @defer.inlineCallbacks\n     def test_closespider_itemcount(self):\n         close_on = 5\n-        crawler = get_crawler(ItemSpider, {'CLOSESPIDER_ITEMCOUNT': close_on})\n+        crawler = get_crawler(ItemSpider, {\"CLOSESPIDER_ITEMCOUNT\": close_on})\n         yield crawler.crawl(mockserver=self.mockserver)\n-        reason = crawler.spider.meta['close_reason']\n-        self.assertEqual(reason, 'closespider_itemcount')\n-        itemcount = crawler.stats.get_value('item_scraped_count')\n+        reason = crawler.spider.meta[\"close_reason\"]\n+        self.assertEqual(reason, \"closespider_itemcount\")\n+        itemcount = crawler.stats.get_value(\"item_scraped_count\")\n         self.assertTrue(itemcount >= close_on)\n \n     @defer.inlineCallbacks\n     def test_closespider_pagecount(self):\n         close_on = 5\n-        crawler = get_crawler(FollowAllSpider, {'CLOSESPIDER_PAGECOUNT': close_on})\n+        crawler = get_crawler(FollowAllSpider, {\"CLOSESPIDER_PAGECOUNT\": close_on})\n         yield crawler.crawl(mockserver=self.mockserver)\n-        reason = crawler.spider.meta['close_reason']\n-        self.assertEqual(reason, 'closespider_pagecount')\n-        pagecount = crawler.stats.get_value('response_received_count')\n+        reason = crawler.spider.meta[\"close_reason\"]\n+        self.assertEqual(reason, \"closespider_pagecount\")\n+        pagecount = crawler.stats.get_value(\"response_received_count\")\n         self.assertTrue(pagecount >= close_on)\n \n     @defer.inlineCallbacks\n     def test_closespider_errorcount(self):\n         close_on = 5\n-        crawler = get_crawler(ErrorSpider, {'CLOSESPIDER_ERRORCOUNT': close_on})\n+        crawler = get_crawler(ErrorSpider, {\"CLOSESPIDER_ERRORCOUNT\": close_on})\n         yield crawler.crawl(total=1000000, mockserver=self.mockserver)\n-        reason = crawler.spider.meta['close_reason']\n-        self.assertEqual(reason, 'closespider_errorcount')\n-        key = f'spider_exceptions/{crawler.spider.exception_cls.__name__}'\n+        reason = crawler.spider.meta[\"close_reason\"]\n+        self.assertEqual(reason, \"closespider_errorcount\")\n+        key = f\"spider_exceptions/{crawler.spider.exception_cls.__name__}\"\n         errorcount = crawler.stats.get_value(key)\n         self.assertTrue(errorcount >= close_on)\n \n     @defer.inlineCallbacks\n     def test_closespider_timeout(self):\n         close_on = 0.1\n-        crawler = get_crawler(FollowAllSpider, {'CLOSESPIDER_TIMEOUT': close_on})\n+        crawler = get_crawler(FollowAllSpider, {\"CLOSESPIDER_TIMEOUT\": close_on})\n         yield crawler.crawl(total=1000000, mockserver=self.mockserver)\n-        reason = crawler.spider.meta['close_reason']\n-        self.assertEqual(reason, 'closespider_timeout')\n-        total_seconds = crawler.stats.get_value('elapsed_time_seconds')\n+        reason = crawler.spider.meta[\"close_reason\"]\n+        self.assertEqual(reason, \"closespider_timeout\")\n+        total_seconds = crawler.stats.get_value(\"elapsed_time_seconds\")\n         self.assertTrue(total_seconds >= close_on)\n\n@@ -12,47 +12,52 @@ from scrapy.utils.test import get_testenv\n \n \n class CmdlineTest(unittest.TestCase):\n-\n     def setUp(self):\n         self.env = get_testenv()\n-        self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'\n+        self.env[\"SCRAPY_SETTINGS_MODULE\"] = \"tests.test_cmdline.settings\"\n \n     def _execute(self, *new_args, **kwargs):\n-        encoding = getattr(sys.stdout, 'encoding') or 'utf-8'\n-        args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n+        encoding = getattr(sys.stdout, \"encoding\") or \"utf-8\"\n+        args = (sys.executable, \"-m\", \"scrapy.cmdline\") + new_args\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n         comm = proc.communicate()[0].strip()\n         return comm.decode(encoding)\n \n     def test_default_settings(self):\n-        self.assertEqual(self._execute('settings', '--get', 'TEST1'), 'default')\n+        self.assertEqual(self._execute(\"settings\", \"--get\", \"TEST1\"), \"default\")\n \n     def test_override_settings_using_set_arg(self):\n-        self.assertEqual(self._execute('settings', '--get', 'TEST1', '-s',\n-                                       'TEST1=override'), 'override')\n+        self.assertEqual(\n+            self._execute(\"settings\", \"--get\", \"TEST1\", \"-s\", \"TEST1=override\"),\n+            \"override\",\n+        )\n \n     def test_profiling(self):\n         path = Path(tempfile.mkdtemp())\n-        filename = path / 'res.prof'\n+        filename = path / \"res.prof\"\n         try:\n-            self._execute('version', '--profile', str(filename))\n+            self._execute(\"version\", \"--profile\", str(filename))\n             self.assertTrue(filename.exists())\n             out = StringIO()\n             stats = pstats.Stats(str(filename), stream=out)\n             stats.print_stats()\n             out.seek(0)\n             stats = out.read()\n-            self.assertIn(str(Path('scrapy', 'commands', 'version.py')),\n-                          stats)\n-            self.assertIn('tottime', stats)\n+            self.assertIn(str(Path(\"scrapy\", \"commands\", \"version.py\")), stats)\n+            self.assertIn(\"tottime\", stats)\n         finally:\n             shutil.rmtree(path)\n \n     def test_override_dict_settings(self):\n         EXT_PATH = \"tests.test_cmdline.extensions.DummyExtension\"\n         EXTENSIONS = {EXT_PATH: 200}\n-        settingsstr = self._execute('settings', '--get', 'EXTENSIONS', '-s',\n-                                    'EXTENSIONS=' + json.dumps(EXTENSIONS))\n+        settingsstr = self._execute(\n+            \"settings\",\n+            \"--get\",\n+            \"EXTENSIONS\",\n+            \"-s\",\n+            \"EXTENSIONS=\" + json.dumps(EXTENSIONS),\n+        )\n         # XXX: There's gotta be a smarter way to do this...\n         self.assertNotIn(\"...\", settingsstr)\n         for char in (\"'\", \"<\", \">\"):\n@@ -62,5 +67,7 @@ class CmdlineTest(unittest.TestCase):\n         self.assertEqual(200, settingsdict[EXT_PATH])\n \n     def test_pathlib_path_as_feeds_key(self):\n-        self.assertEqual(self._execute('settings', '--get', 'FEEDS'),\n-                         json.dumps({\"items.csv\": {\"format\": \"csv\", \"fields\": [\"price\", \"name\"]}}))\n+        self.assertEqual(\n+            self._execute(\"settings\", \"--get\", \"FEEDS\"),\n+            json.dumps({\"items.csv\": {\"format\": \"csv\", \"fields\": [\"price\", \"name\"]}}),\n+        )\n\n@@ -2,9 +2,8 @@\n \n \n class TestExtension:\n-\n     def __init__(self, settings):\n-        settings.set('TEST1', f\"{settings['TEST1']} + started\")\n+        settings.set(\"TEST1\", f\"{settings['TEST1']} + started\")\n \n     @classmethod\n     def from_crawler(cls, crawler):\n\n@@ -1,14 +1,14 @@\n from pathlib import Path\n \n EXTENSIONS = {\n-    'tests.test_cmdline.extensions.TestExtension': 0,\n+    \"tests.test_cmdline.extensions.TestExtension\": 0,\n }\n \n-TEST1 = 'default'\n+TEST1 = \"default\"\n \n FEEDS = {\n-    Path('items.csv'): {\n-        'format': 'csv',\n-        'fields': ['price', 'name'],\n+    Path(\"items.csv\"): {\n+        \"format\": \"csv\",\n+        \"fields\": [\"price\", \"name\"],\n     },\n }\n\n@@ -5,16 +5,15 @@ from subprocess import Popen, PIPE\n \n \n class CmdlineCrawlPipelineTest(unittest.TestCase):\n-\n     def _execute(self, spname):\n-        args = (sys.executable, '-m', 'scrapy.cmdline', 'crawl', spname)\n+        args = (sys.executable, \"-m\", \"scrapy.cmdline\", \"crawl\", spname)\n         cwd = Path(__file__).resolve().parent\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, cwd=cwd)\n         proc.communicate()\n         return proc.returncode\n \n     def test_open_spider_normally_in_pipeline(self):\n-        self.assertEqual(self._execute('normal'), 0)\n+        self.assertEqual(self._execute(\"normal\"), 0)\n \n     def test_exception_at_open_spider_in_pipeline(self):\n-        self.assertEqual(self._execute('exception'), 1)\n+        self.assertEqual(self._execute(\"exception\"), 1)\n\n@@ -1,5 +1,4 @@\n class TestSpiderPipeline:\n-\n     def open_spider(self, spider):\n         pass\n \n@@ -8,9 +7,8 @@ class TestSpiderPipeline:\n \n \n class TestSpiderExceptionPipeline:\n-\n     def open_spider(self, spider):\n-        raise Exception('exception')\n+        raise Exception(\"exception\")\n \n     def process_item(self, item, spider):\n         return item\n\n@@ -1,2 +1,2 @@\n-BOT_NAME = 'test_spider'\n-SPIDER_MODULES = ['test_spider.spiders']\n+BOT_NAME = \"test_spider\"\n+SPIDER_MODULES = [\"test_spider.spiders\"]\n\n@@ -2,12 +2,10 @@ import scrapy\n \n \n class ExceptionSpider(scrapy.Spider):\n-    name = 'exception'\n+    name = \"exception\"\n \n     custom_settings = {\n-        'ITEM_PIPELINES': {\n-            'test_spider.pipelines.TestSpiderExceptionPipeline': 300\n-        }\n+        \"ITEM_PIPELINES\": {\"test_spider.pipelines.TestSpiderExceptionPipeline\": 300}\n     }\n \n     def parse(self, response):\n\n@@ -2,12 +2,10 @@ import scrapy\n \n \n class NormalSpider(scrapy.Spider):\n-    name = 'normal'\n+    name = \"normal\"\n \n     custom_settings = {\n-        'ITEM_PIPELINES': {\n-            'test_spider.pipelines.TestSpiderPipeline': 300\n-        }\n+        \"ITEM_PIPELINES\": {\"test_spider.pipelines.TestSpiderPipeline\": 300}\n     }\n \n     def parse(self, response):\n\n@@ -3,15 +3,16 @@ from tests.test_commands import CommandTest\n \n class CheckCommandTest(CommandTest):\n \n-    command = 'check'\n+    command = \"check\"\n \n     def setUp(self):\n         super().setUp()\n-        self.spider_name = 'check_spider'\n-        self.spider = (self.proj_mod_path / 'spiders' / 'checkspider.py').resolve()\n+        self.spider_name = \"check_spider\"\n+        self.spider = (self.proj_mod_path / \"spiders\" / \"checkspider.py\").resolve()\n \n     def _write_contract(self, contracts, parse_def):\n-        self.spider.write_text(f\"\"\"\n+        self.spider.write_text(\n+            f\"\"\"\n import scrapy\n \n class CheckSpider(scrapy.Spider):\n@@ -24,13 +25,15 @@ class CheckSpider(scrapy.Spider):\n         {contracts}\n         \\\"\\\"\\\"\n         {parse_def}\n-        \"\"\", encoding=\"utf-8\")\n+        \"\"\",\n+            encoding=\"utf-8\",\n+        )\n \n-    def _test_contract(self, contracts='', parse_def='pass'):\n+    def _test_contract(self, contracts=\"\", parse_def=\"pass\"):\n         self._write_contract(contracts, parse_def)\n-        p, out, err = self.proc('check')\n-        self.assertNotIn('F', out)\n-        self.assertIn('OK', err)\n+        p, out, err = self.proc(\"check\")\n+        self.assertNotIn(\"F\", out)\n+        self.assertIn(\"OK\", err)\n         self.assertEqual(p.returncode, 0)\n \n     def test_check_returns_requests_contract(self):\n\n@@ -7,28 +7,30 @@ from scrapy.utils.testproc import ProcessTest\n \n class FetchTest(ProcessTest, SiteTest, unittest.TestCase):\n \n-    command = 'fetch'\n+    command = \"fetch\"\n \n     @defer.inlineCallbacks\n     def test_output(self):\n-        _, out, _ = yield self.execute([self.url('/text')])\n-        self.assertEqual(out.strip(), b'Works')\n+        _, out, _ = yield self.execute([self.url(\"/text\")])\n+        self.assertEqual(out.strip(), b\"Works\")\n \n     @defer.inlineCallbacks\n     def test_redirect_default(self):\n-        _, out, _ = yield self.execute([self.url('/redirect')])\n-        self.assertEqual(out.strip(), b'Redirected here')\n+        _, out, _ = yield self.execute([self.url(\"/redirect\")])\n+        self.assertEqual(out.strip(), b\"Redirected here\")\n \n     @defer.inlineCallbacks\n     def test_redirect_disabled(self):\n-        _, out, err = yield self.execute(['--no-redirect', self.url('/redirect-no-meta-refresh')])\n+        _, out, err = yield self.execute(\n+            [\"--no-redirect\", self.url(\"/redirect-no-meta-refresh\")]\n+        )\n         err = err.strip()\n-        self.assertIn(b'downloader/response_status_count/302', err, err)\n-        self.assertNotIn(b'downloader/response_status_count/200', err, err)\n+        self.assertIn(b\"downloader/response_status_count/302\", err, err)\n+        self.assertNotIn(b\"downloader/response_status_count/200\", err, err)\n \n     @defer.inlineCallbacks\n     def test_headers(self):\n-        _, out, _ = yield self.execute([self.url('/text'), '--headers'])\n-        out = out.replace(b'\\r', b'')  # required on win32\n-        assert b'Server: TwistedWeb' in out, out\n-        assert b'Content-Type: text/plain' in out\n+        _, out, _ = yield self.execute([self.url(\"/text\"), \"--headers\"])\n+        out = out.replace(b\"\\r\", b\"\")  # required on win32\n+        assert b\"Server: TwistedWeb\" in out, out\n+        assert b\"Content-Type: text/plain\" in out\n\n@@ -14,16 +14,17 @@ from tests.test_commands import CommandTest\n def _textmode(bstr):\n     \"\"\"Normalize input the same as writing to a file\n     and reading from it in text mode\"\"\"\n-    return to_unicode(bstr).replace(os.linesep, '\\n')\n+    return to_unicode(bstr).replace(os.linesep, \"\\n\")\n \n \n class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n-    command = 'parse'\n+    command = \"parse\"\n \n     def setUp(self):\n         super().setUp()\n-        self.spider_name = 'parse_spider'\n-        (self.proj_mod_path / 'spiders' / 'myspider.py').write_text(f\"\"\"\n+        self.spider_name = \"parse_spider\"\n+        (self.proj_mod_path / \"spiders\" / \"myspider.py\").write_text(\n+            f\"\"\"\n import scrapy\n from scrapy.linkextractors import LinkExtractor\n from scrapy.spiders import CrawlSpider, Rule\n@@ -92,9 +93,12 @@ class MyBadCrawlSpider(CrawlSpider):\n \n     def parse(self, response):\n         return [scrapy.Item(), dict(foo='bar')]\n-\"\"\", encoding=\"utf-8\")\n+\"\"\",\n+            encoding=\"utf-8\",\n+        )\n \n-        (self.proj_mod_path / 'pipelines.py').write_text(\"\"\"\n+        (self.proj_mod_path / \"pipelines.py\").write_text(\n+            \"\"\"\n import logging\n \n class MyPipeline:\n@@ -103,91 +107,141 @@ class MyPipeline:\n     def process_item(self, item, spider):\n         logging.info('It Works!')\n         return item\n-\"\"\", encoding=\"utf-8\")\n+\"\"\",\n+            encoding=\"utf-8\",\n+        )\n \n-        with (self.proj_mod_path / 'settings.py').open(\"a\", encoding=\"utf-8\") as f:\n-            f.write(f\"\"\"\n+        with (self.proj_mod_path / \"settings.py\").open(\"a\", encoding=\"utf-8\") as f:\n+            f.write(\n+                f\"\"\"\n ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n-\"\"\")\n+\"\"\"\n+            )\n \n     @defer.inlineCallbacks\n     def test_spider_arguments(self):\n-        _, _, stderr = yield self.execute(['--spider', self.spider_name,\n-                                           '-a', 'test_arg=1',\n-                                           '-c', 'parse',\n-                                           '--verbose',\n-                                           self.url('/html')])\n+        _, _, stderr = yield self.execute(\n+            [\n+                \"--spider\",\n+                self.spider_name,\n+                \"-a\",\n+                \"test_arg=1\",\n+                \"-c\",\n+                \"parse\",\n+                \"--verbose\",\n+                self.url(\"/html\"),\n+            ]\n+        )\n         self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n     @defer.inlineCallbacks\n     def test_request_with_meta(self):\n         raw_json_string = '{\"foo\" : \"baz\"}'\n-        _, _, stderr = yield self.execute(['--spider', self.spider_name,\n-                                           '--meta', raw_json_string,\n-                                           '-c', 'parse_request_with_meta',\n-                                           '--verbose',\n-                                           self.url('/html')])\n+        _, _, stderr = yield self.execute(\n+            [\n+                \"--spider\",\n+                self.spider_name,\n+                \"--meta\",\n+                raw_json_string,\n+                \"-c\",\n+                \"parse_request_with_meta\",\n+                \"--verbose\",\n+                self.url(\"/html\"),\n+            ]\n+        )\n         self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n-        _, _, stderr = yield self.execute(['--spider', self.spider_name,\n-                                           '-m', raw_json_string,\n-                                           '-c', 'parse_request_with_meta',\n-                                           '--verbose',\n-                                           self.url('/html')])\n+        _, _, stderr = yield self.execute(\n+            [\n+                \"--spider\",\n+                self.spider_name,\n+                \"-m\",\n+                raw_json_string,\n+                \"-c\",\n+                \"parse_request_with_meta\",\n+                \"--verbose\",\n+                self.url(\"/html\"),\n+            ]\n+        )\n         self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n     @defer.inlineCallbacks\n     def test_request_with_cb_kwargs(self):\n         raw_json_string = '{\"foo\" : \"bar\", \"key\": \"value\"}'\n-        _, _, stderr = yield self.execute(['--spider', self.spider_name,\n-                                           '--cbkwargs', raw_json_string,\n-                                           '-c', 'parse_request_with_cb_kwargs',\n-                                           '--verbose',\n-                                           self.url('/html')])\n+        _, _, stderr = yield self.execute(\n+            [\n+                \"--spider\",\n+                self.spider_name,\n+                \"--cbkwargs\",\n+                raw_json_string,\n+                \"-c\",\n+                \"parse_request_with_cb_kwargs\",\n+                \"--verbose\",\n+                self.url(\"/html\"),\n+            ]\n+        )\n         self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n     @defer.inlineCallbacks\n     def test_request_without_meta(self):\n-        _, _, stderr = yield self.execute(['--spider', self.spider_name,\n-                                           '-c', 'parse_request_without_meta',\n-                                           '--nolinks',\n-                                           self.url('/html')])\n+        _, _, stderr = yield self.execute(\n+            [\n+                \"--spider\",\n+                self.spider_name,\n+                \"-c\",\n+                \"parse_request_without_meta\",\n+                \"--nolinks\",\n+                self.url(\"/html\"),\n+            ]\n+        )\n         self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n     @defer.inlineCallbacks\n     def test_pipelines(self):\n-        _, _, stderr = yield self.execute(['--spider', self.spider_name,\n-                                           '--pipelines',\n-                                           '-c', 'parse',\n-                                           '--verbose',\n-                                           self.url('/html')])\n+        _, _, stderr = yield self.execute(\n+            [\n+                \"--spider\",\n+                self.spider_name,\n+                \"--pipelines\",\n+                \"-c\",\n+                \"parse\",\n+                \"--verbose\",\n+                self.url(\"/html\"),\n+            ]\n+        )\n         self.assertIn(\"INFO: It Works!\", _textmode(stderr))\n \n     @defer.inlineCallbacks\n     def test_asyncio_parse_items(self):\n         status, out, stderr = yield self.execute(\n-            ['--spider', 'asyncdef' + self.spider_name, '-c', 'parse', self.url('/html')]\n+            [\n+                \"--spider\",\n+                \"asyncdef\" + self.spider_name,\n+                \"-c\",\n+                \"parse\",\n+                self.url(\"/html\"),\n+            ]\n         )\n         self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", _textmode(out))\n \n     @defer.inlineCallbacks\n     def test_parse_items(self):\n         status, out, stderr = yield self.execute(\n-            ['--spider', self.spider_name, '-c', 'parse', self.url('/html')]\n+            [\"--spider\", self.spider_name, \"-c\", \"parse\", self.url(\"/html\")]\n         )\n         self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", _textmode(out))\n \n     @defer.inlineCallbacks\n     def test_parse_items_no_callback_passed(self):\n         status, out, stderr = yield self.execute(\n-            ['--spider', self.spider_name, self.url('/html')]\n+            [\"--spider\", self.spider_name, self.url(\"/html\")]\n         )\n         self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", _textmode(out))\n \n     @defer.inlineCallbacks\n     def test_wrong_callback_passed(self):\n         status, out, stderr = yield self.execute(\n-            ['--spider', self.spider_name, '-c', 'dummy', self.url('/html')]\n+            [\"--spider\", self.spider_name, \"-c\", \"dummy\", self.url(\"/html\")]\n         )\n         self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n         self.assertIn(\"\"\"Cannot find callback\"\"\", _textmode(stderr))\n@@ -196,7 +250,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n     def test_crawlspider_matching_rule_callback_set(self):\n         \"\"\"If a rule matches the URL, use it's defined callback.\"\"\"\n         status, out, stderr = yield self.execute(\n-            ['--spider', 'goodcrawl' + self.spider_name, '-r', self.url('/html')]\n+            [\"--spider\", \"goodcrawl\" + self.spider_name, \"-r\", self.url(\"/html\")]\n         )\n         self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", _textmode(out))\n \n@@ -204,7 +258,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n     def test_crawlspider_matching_rule_default_callback(self):\n         \"\"\"If a rule match but it has no callback set, use the 'parse' callback.\"\"\"\n         status, out, stderr = yield self.execute(\n-            ['--spider', 'goodcrawl' + self.spider_name, '-r', self.url('/text')]\n+            [\"--spider\", \"goodcrawl\" + self.spider_name, \"-r\", self.url(\"/text\")]\n         )\n         self.assertIn(\"\"\"[{}, {'nomatch': 'default'}]\"\"\", _textmode(out))\n \n@@ -212,7 +266,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n     def test_spider_with_no_rules_attribute(self):\n         \"\"\"Using -r with a spider with no rule should not produce items.\"\"\"\n         status, out, stderr = yield self.execute(\n-            ['--spider', self.spider_name, '-r', self.url('/html')]\n+            [\"--spider\", self.spider_name, \"-r\", self.url(\"/html\")]\n         )\n         self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n         self.assertIn(\"\"\"No CrawlSpider rules found\"\"\", _textmode(stderr))\n@@ -220,7 +274,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n     @defer.inlineCallbacks\n     def test_crawlspider_missing_callback(self):\n         status, out, stderr = yield self.execute(\n-            ['--spider', 'badcrawl' + self.spider_name, '-r', self.url('/html')]\n+            [\"--spider\", \"badcrawl\" + self.spider_name, \"-r\", self.url(\"/html\")]\n         )\n         self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n \n@@ -228,14 +282,14 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n     def test_crawlspider_no_matching_rule(self):\n         \"\"\"The requested URL has no matching rule, so no items should be scraped\"\"\"\n         status, out, stderr = yield self.execute(\n-            ['--spider', 'badcrawl' + self.spider_name, '-r', self.url('/enc-gb18030')]\n+            [\"--spider\", \"badcrawl\" + self.spider_name, \"-r\", self.url(\"/enc-gb18030\")]\n         )\n         self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n         self.assertIn(\"\"\"Cannot find a rule that matches\"\"\", _textmode(stderr))\n \n     @defer.inlineCallbacks\n     def test_crawlspider_not_exists_with_not_matched_url(self):\n-        status, out, stderr = yield self.execute([self.url('/invalid_url')])\n+        status, out, stderr = yield self.execute([self.url(\"/invalid_url\")])\n         self.assertEqual(status, 0)\n \n     @defer.inlineCallbacks\n@@ -243,14 +297,19 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         \"\"\"Checks if a file was created successfully having\n         correct format containing correct data in it.\n         \"\"\"\n-        file_name = 'data.json'\n+        file_name = \"data.json\"\n         file_path = Path(self.proj_path, file_name)\n-        yield self.execute([\n-            '--spider', self.spider_name,\n-            '-c', 'parse',\n-            '-o', file_name,\n-            self.url('/html')\n-        ])\n+        yield self.execute(\n+            [\n+                \"--spider\",\n+                self.spider_name,\n+                \"-c\",\n+                \"parse\",\n+                \"-o\",\n+                file_name,\n+                self.url(\"/html\"),\n+            ]\n+        )\n \n         self.assertTrue(file_path.exists())\n         self.assertTrue(file_path.is_file())\n@@ -262,12 +321,14 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         command = parse.Command()\n         command.settings = Settings()\n         parser = argparse.ArgumentParser(\n-            prog='scrapy', formatter_class=argparse.HelpFormatter,\n-            conflict_handler='resolve', prefix_chars='-'\n+            prog=\"scrapy\",\n+            formatter_class=argparse.HelpFormatter,\n+            conflict_handler=\"resolve\",\n+            prefix_chars=\"-\",\n         )\n         command.add_options(parser)\n         namespace = parser.parse_args(\n-            ['--verbose', '--nolinks', '-d', '2', '--spider', self.spider_name]\n+            [\"--verbose\", \"--nolinks\", \"-d\", \"2\", \"--spider\", self.spider_name]\n         )\n         self.assertTrue(namespace.nolinks)\n         self.assertEqual(namespace.depth, 2)\n\n@@ -11,107 +11,118 @@ from tests import tests_datadir, NON_EXISTING_RESOLVABLE\n \n class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n \n-    command = 'shell'\n+    command = \"shell\"\n \n     @defer.inlineCallbacks\n     def test_empty(self):\n-        _, out, _ = yield self.execute(['-c', 'item'])\n-        assert b'{}' in out\n+        _, out, _ = yield self.execute([\"-c\", \"item\"])\n+        assert b\"{}\" in out\n \n     @defer.inlineCallbacks\n     def test_response_body(self):\n-        _, out, _ = yield self.execute([self.url('/text'), '-c', 'response.body'])\n-        assert b'Works' in out\n+        _, out, _ = yield self.execute([self.url(\"/text\"), \"-c\", \"response.body\"])\n+        assert b\"Works\" in out\n \n     @defer.inlineCallbacks\n     def test_response_type_text(self):\n-        _, out, _ = yield self.execute([self.url('/text'), '-c', 'type(response)'])\n-        assert b'TextResponse' in out\n+        _, out, _ = yield self.execute([self.url(\"/text\"), \"-c\", \"type(response)\"])\n+        assert b\"TextResponse\" in out\n \n     @defer.inlineCallbacks\n     def test_response_type_html(self):\n-        _, out, _ = yield self.execute([self.url('/html'), '-c', 'type(response)'])\n-        assert b'HtmlResponse' in out\n+        _, out, _ = yield self.execute([self.url(\"/html\"), \"-c\", \"type(response)\"])\n+        assert b\"HtmlResponse\" in out\n \n     @defer.inlineCallbacks\n     def test_response_selector_html(self):\n-        xpath = 'response.xpath(\"//p[@class=\\'one\\']/text()\").get()'\n-        _, out, _ = yield self.execute([self.url('/html'), '-c', xpath])\n-        self.assertEqual(out.strip(), b'Works')\n+        xpath = \"response.xpath(\\\"//p[@class='one']/text()\\\").get()\"\n+        _, out, _ = yield self.execute([self.url(\"/html\"), \"-c\", xpath])\n+        self.assertEqual(out.strip(), b\"Works\")\n \n     @defer.inlineCallbacks\n     def test_response_encoding_gb18030(self):\n-        _, out, _ = yield self.execute([self.url('/enc-gb18030'), '-c', 'response.encoding'])\n-        self.assertEqual(out.strip(), b'gb18030')\n+        _, out, _ = yield self.execute(\n+            [self.url(\"/enc-gb18030\"), \"-c\", \"response.encoding\"]\n+        )\n+        self.assertEqual(out.strip(), b\"gb18030\")\n \n     @defer.inlineCallbacks\n     def test_redirect(self):\n-        _, out, _ = yield self.execute([self.url('/redirect'), '-c', 'response.url'])\n-        assert out.strip().endswith(b'/redirected')\n+        _, out, _ = yield self.execute([self.url(\"/redirect\"), \"-c\", \"response.url\"])\n+        assert out.strip().endswith(b\"/redirected\")\n \n     @defer.inlineCallbacks\n     def test_redirect_follow_302(self):\n-        _, out, _ = yield self.execute([self.url('/redirect-no-meta-refresh'), '-c', 'response.status'])\n-        assert out.strip().endswith(b'200')\n+        _, out, _ = yield self.execute(\n+            [self.url(\"/redirect-no-meta-refresh\"), \"-c\", \"response.status\"]\n+        )\n+        assert out.strip().endswith(b\"200\")\n \n     @defer.inlineCallbacks\n     def test_redirect_not_follow_302(self):\n         _, out, _ = yield self.execute(\n-            ['--no-redirect', self.url('/redirect-no-meta-refresh'), '-c', 'response.status']\n+            [\n+                \"--no-redirect\",\n+                self.url(\"/redirect-no-meta-refresh\"),\n+                \"-c\",\n+                \"response.status\",\n+            ]\n         )\n-        assert out.strip().endswith(b'302')\n+        assert out.strip().endswith(b\"302\")\n \n     @defer.inlineCallbacks\n     def test_fetch_redirect_follow_302(self):\n         \"\"\"Test that calling ``fetch(url)`` follows HTTP redirects by default.\"\"\"\n-        url = self.url('/redirect-no-meta-refresh')\n+        url = self.url(\"/redirect-no-meta-refresh\")\n         code = f\"fetch('{url}')\"\n-        errcode, out, errout = yield self.execute(['-c', code])\n+        errcode, out, errout = yield self.execute([\"-c\", code])\n         self.assertEqual(errcode, 0, out)\n-        assert b'Redirecting (302)' in errout\n-        assert b'Crawled (200)' in errout\n+        assert b\"Redirecting (302)\" in errout\n+        assert b\"Crawled (200)\" in errout\n \n     @defer.inlineCallbacks\n     def test_fetch_redirect_not_follow_302(self):\n         \"\"\"Test that calling ``fetch(url, redirect=False)`` disables automatic redirects.\"\"\"\n-        url = self.url('/redirect-no-meta-refresh')\n+        url = self.url(\"/redirect-no-meta-refresh\")\n         code = f\"fetch('{url}', redirect=False)\"\n-        errcode, out, errout = yield self.execute(['-c', code])\n+        errcode, out, errout = yield self.execute([\"-c\", code])\n         self.assertEqual(errcode, 0, out)\n-        assert b'Crawled (302)' in errout\n+        assert b\"Crawled (302)\" in errout\n \n     @defer.inlineCallbacks\n     def test_request_replace(self):\n-        url = self.url('/text')\n+        url = self.url(\"/text\")\n         code = f\"fetch('{url}') or fetch(response.request.replace(method='POST'))\"\n-        errcode, out, _ = yield self.execute(['-c', code])\n+        errcode, out, _ = yield self.execute([\"-c\", code])\n         self.assertEqual(errcode, 0, out)\n \n     @defer.inlineCallbacks\n     def test_scrapy_import(self):\n-        url = self.url('/text')\n+        url = self.url(\"/text\")\n         code = f\"fetch(scrapy.Request('{url}'))\"\n-        errcode, out, _ = yield self.execute(['-c', code])\n+        errcode, out, _ = yield self.execute([\"-c\", code])\n         self.assertEqual(errcode, 0, out)\n \n     @defer.inlineCallbacks\n     def test_local_file(self):\n-        filepath = Path(tests_datadir, 'test_site', 'index.html')\n-        _, out, _ = yield self.execute([str(filepath), '-c', 'item'])\n-        assert b'{}' in out\n+        filepath = Path(tests_datadir, \"test_site\", \"index.html\")\n+        _, out, _ = yield self.execute([str(filepath), \"-c\", \"item\"])\n+        assert b\"{}\" in out\n \n     @defer.inlineCallbacks\n     def test_local_nofile(self):\n-        filepath = 'file:///tests/sample_data/test_site/nothinghere.html'\n-        errcode, out, err = yield self.execute([filepath, '-c', 'item'], check_code=False)\n+        filepath = \"file:///tests/sample_data/test_site/nothinghere.html\"\n+        errcode, out, err = yield self.execute(\n+            [filepath, \"-c\", \"item\"], check_code=False\n+        )\n         self.assertEqual(errcode, 1, out or err)\n-        self.assertIn(b'No such file or directory', err)\n+        self.assertIn(b\"No such file or directory\", err)\n \n     @defer.inlineCallbacks\n     def test_dns_failures(self):\n         if NON_EXISTING_RESOLVABLE:\n             raise unittest.SkipTest(\"Non-existing hosts are resolvable\")\n-        url = 'www.somedomainthatdoesntexi.st'\n-        errcode, out, err = yield self.execute([url, '-c', 'item'], check_code=False)\n+        url = \"www.somedomainthatdoesntexi.st\"\n+        errcode, out, err = yield self.execute([url, \"-c\", \"item\"], check_code=False)\n         self.assertEqual(errcode, 1, out or err)\n-        self.assertIn(b'DNS lookup failed', err)\n+        self.assertIn(b\"DNS lookup failed\", err)\n\n@@ -8,11 +8,11 @@ from scrapy.utils.testproc import ProcessTest\n \n class VersionTest(ProcessTest, unittest.TestCase):\n \n-    command = 'version'\n+    command = \"version\"\n \n     @defer.inlineCallbacks\n     def test_output(self):\n-        encoding = getattr(sys.stdout, 'encoding') or 'utf-8'\n+        encoding = getattr(sys.stdout, \"encoding\") or \"utf-8\"\n         _, out, _ = yield self.execute([])\n         self.assertEqual(\n             out.strip().decode(encoding),\n@@ -21,13 +21,25 @@ class VersionTest(ProcessTest, unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_verbose_output(self):\n-        encoding = getattr(sys.stdout, 'encoding') or 'utf-8'\n-        _, out, _ = yield self.execute(['-v'])\n+        encoding = getattr(sys.stdout, \"encoding\") or \"utf-8\"\n+        _, out, _ = yield self.execute([\"-v\"])\n         headers = [\n             line.partition(\":\")[0].strip()\n             for line in out.strip().decode(encoding).splitlines()\n         ]\n-        self.assertEqual(headers, ['Scrapy', 'lxml', 'libxml2',\n-                                   'cssselect', 'parsel', 'w3lib',\n-                                   'Twisted', 'Python', 'pyOpenSSL',\n-                                   'cryptography', 'Platform'])\n+        self.assertEqual(\n+            headers,\n+            [\n+                \"Scrapy\",\n+                \"lxml\",\n+                \"libxml2\",\n+                \"cssselect\",\n+                \"parsel\",\n+                \"w3lib\",\n+                \"Twisted\",\n+                \"Python\",\n+                \"pyOpenSSL\",\n+                \"cryptography\",\n+                \"Platform\",\n+            ],\n+        )\n\n@@ -33,35 +33,46 @@ from tests.test_crawler import ExceptionSpider, NoRequestsSpider\n \n \n class CommandSettings(unittest.TestCase):\n-\n     def setUp(self):\n         self.command = ScrapyCommand()\n         self.command.settings = Settings()\n-        self.parser = argparse.ArgumentParser(formatter_class=ScrapyHelpFormatter,\n-                                              conflict_handler='resolve')\n+        self.parser = argparse.ArgumentParser(\n+            formatter_class=ScrapyHelpFormatter, conflict_handler=\"resolve\"\n+        )\n         self.command.add_options(self.parser)\n \n     def test_settings_json_string(self):\n         feeds_json = '{\"data.json\": {\"format\": \"json\"}, \"data.xml\": {\"format\": \"xml\"}}'\n-        opts, args = self.parser.parse_known_args(args=['-s', f'FEEDS={feeds_json}', 'spider.py'])\n+        opts, args = self.parser.parse_known_args(\n+            args=[\"-s\", f\"FEEDS={feeds_json}\", \"spider.py\"]\n+        )\n         self.command.process_options(args, opts)\n-        self.assertIsInstance(self.command.settings['FEEDS'], scrapy.settings.BaseSettings)\n-        self.assertEqual(dict(self.command.settings['FEEDS']), json.loads(feeds_json))\n+        self.assertIsInstance(\n+            self.command.settings[\"FEEDS\"], scrapy.settings.BaseSettings\n+        )\n+        self.assertEqual(dict(self.command.settings[\"FEEDS\"]), json.loads(feeds_json))\n \n     def test_help_formatter(self):\n-        formatter = ScrapyHelpFormatter(prog='scrapy')\n-        part_strings = ['usage: scrapy genspider [options] <name> <domain>\\n\\n',\n-                        '\\n', 'optional arguments:\\n', '\\n', 'Global Options:\\n']\n+        formatter = ScrapyHelpFormatter(prog=\"scrapy\")\n+        part_strings = [\n+            \"usage: scrapy genspider [options] <name> <domain>\\n\\n\",\n+            \"\\n\",\n+            \"optional arguments:\\n\",\n+            \"\\n\",\n+            \"Global Options:\\n\",\n+        ]\n         self.assertEqual(\n             formatter._join_parts(part_strings),\n-            ('Usage\\n=====\\n  scrapy genspider [options] <name> <domain>\\n\\n\\n'\n-             'Optional Arguments\\n==================\\n\\n'\n-             'Global Options\\n--------------\\n')\n+            (\n+                \"Usage\\n=====\\n  scrapy genspider [options] <name> <domain>\\n\\n\\n\"\n+                \"Optional Arguments\\n==================\\n\\n\"\n+                \"Global Options\\n--------------\\n\"\n+            ),\n         )\n \n \n class ProjectTest(unittest.TestCase):\n-    project_name = 'testproject'\n+    project_name = \"testproject\"\n \n     def setUp(self):\n         self.temp_path = mkdtemp()\n@@ -75,15 +86,16 @@ class ProjectTest(unittest.TestCase):\n \n     def call(self, *new_args, **kwargs):\n         with tempfile.TemporaryFile() as out:\n-            args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n-            return subprocess.call(args, stdout=out, stderr=out, cwd=self.cwd,\n-                                   env=self.env, **kwargs)\n+            args = (sys.executable, \"-m\", \"scrapy.cmdline\") + new_args\n+            return subprocess.call(\n+                args, stdout=out, stderr=out, cwd=self.cwd, env=self.env, **kwargs\n+            )\n \n     def proc(self, *new_args, **popen_kwargs):\n-        args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n+        args = (sys.executable, \"-m\", \"scrapy.cmdline\") + new_args\n         p = subprocess.Popen(\n             args,\n-            cwd=popen_kwargs.pop('cwd', self.cwd),\n+            cwd=popen_kwargs.pop(\"cwd\", self.cwd),\n             env=self.env,\n             stdout=subprocess.PIPE,\n             stderr=subprocess.PIPE,\n@@ -93,7 +105,7 @@ class ProjectTest(unittest.TestCase):\n         def kill_proc():\n             p.kill()\n             p.communicate()\n-            assert False, 'Command took too much time to complete'\n+            assert False, \"Command took too much time to complete\"\n \n         timer = Timer(15, kill_proc)\n         try:\n@@ -104,7 +116,9 @@ class ProjectTest(unittest.TestCase):\n \n         return p, to_unicode(stdout), to_unicode(stderr)\n \n-    def find_in_file(self, filename: Union[str, os.PathLike], regex) -> Optional[re.Match]:\n+    def find_in_file(\n+        self, filename: Union[str, os.PathLike], regex\n+    ) -> Optional[re.Match]:\n         \"\"\"Find first pattern occurrence in file\"\"\"\n         pattern = re.compile(regex)\n         with Path(filename).open(\"r\", encoding=\"utf-8\") as f:\n@@ -116,68 +130,75 @@ class ProjectTest(unittest.TestCase):\n \n \n class StartprojectTest(ProjectTest):\n-\n     def test_startproject(self):\n-        p, out, err = self.proc('startproject', self.project_name)\n+        p, out, err = self.proc(\"startproject\", self.project_name)\n         print(out)\n         print(err, file=sys.stderr)\n         self.assertEqual(p.returncode, 0)\n \n-        assert Path(self.proj_path, 'scrapy.cfg').exists()\n-        assert Path(self.proj_path, 'testproject').exists()\n-        assert Path(self.proj_mod_path, '__init__.py').exists()\n-        assert Path(self.proj_mod_path, 'items.py').exists()\n-        assert Path(self.proj_mod_path, 'pipelines.py').exists()\n-        assert Path(self.proj_mod_path, 'settings.py').exists()\n-        assert Path(self.proj_mod_path, 'spiders', '__init__.py').exists()\n+        assert Path(self.proj_path, \"scrapy.cfg\").exists()\n+        assert Path(self.proj_path, \"testproject\").exists()\n+        assert Path(self.proj_mod_path, \"__init__.py\").exists()\n+        assert Path(self.proj_mod_path, \"items.py\").exists()\n+        assert Path(self.proj_mod_path, \"pipelines.py\").exists()\n+        assert Path(self.proj_mod_path, \"settings.py\").exists()\n+        assert Path(self.proj_mod_path, \"spiders\", \"__init__.py\").exists()\n \n-        self.assertEqual(1, self.call('startproject', self.project_name))\n-        self.assertEqual(1, self.call('startproject', 'wrong---project---name'))\n-        self.assertEqual(1, self.call('startproject', 'sys'))\n+        self.assertEqual(1, self.call(\"startproject\", self.project_name))\n+        self.assertEqual(1, self.call(\"startproject\", \"wrong---project---name\"))\n+        self.assertEqual(1, self.call(\"startproject\", \"sys\"))\n \n     def test_startproject_with_project_dir(self):\n         project_dir = mkdtemp()\n-        self.assertEqual(0, self.call('startproject', self.project_name, project_dir))\n+        self.assertEqual(0, self.call(\"startproject\", self.project_name, project_dir))\n \n-        assert Path(project_dir, 'scrapy.cfg').exists()\n-        assert Path(project_dir, 'testproject').exists()\n-        assert Path(project_dir, self.project_name, '__init__.py').exists()\n-        assert Path(project_dir, self.project_name, 'items.py').exists()\n-        assert Path(project_dir, self.project_name, 'pipelines.py').exists()\n-        assert Path(project_dir, self.project_name, 'settings.py').exists()\n-        assert Path(project_dir, self.project_name, 'spiders', '__init__.py').exists()\n+        assert Path(project_dir, \"scrapy.cfg\").exists()\n+        assert Path(project_dir, \"testproject\").exists()\n+        assert Path(project_dir, self.project_name, \"__init__.py\").exists()\n+        assert Path(project_dir, self.project_name, \"items.py\").exists()\n+        assert Path(project_dir, self.project_name, \"pipelines.py\").exists()\n+        assert Path(project_dir, self.project_name, \"settings.py\").exists()\n+        assert Path(project_dir, self.project_name, \"spiders\", \"__init__.py\").exists()\n \n-        self.assertEqual(0, self.call('startproject', self.project_name, project_dir + '2'))\n+        self.assertEqual(\n+            0, self.call(\"startproject\", self.project_name, project_dir + \"2\")\n+        )\n \n-        self.assertEqual(1, self.call('startproject', self.project_name, project_dir))\n-        self.assertEqual(1, self.call('startproject', self.project_name + '2', project_dir))\n-        self.assertEqual(1, self.call('startproject', 'wrong---project---name'))\n-        self.assertEqual(1, self.call('startproject', 'sys'))\n-        self.assertEqual(2, self.call('startproject'))\n-        self.assertEqual(2, self.call('startproject', self.project_name, project_dir, 'another_params'))\n+        self.assertEqual(1, self.call(\"startproject\", self.project_name, project_dir))\n+        self.assertEqual(\n+            1, self.call(\"startproject\", self.project_name + \"2\", project_dir)\n+        )\n+        self.assertEqual(1, self.call(\"startproject\", \"wrong---project---name\"))\n+        self.assertEqual(1, self.call(\"startproject\", \"sys\"))\n+        self.assertEqual(2, self.call(\"startproject\"))\n+        self.assertEqual(\n+            2,\n+            self.call(\"startproject\", self.project_name, project_dir, \"another_params\"),\n+        )\n \n     def test_existing_project_dir(self):\n         project_dir = mkdtemp()\n-        project_name = self.project_name + '_existing'\n+        project_name = self.project_name + \"_existing\"\n         project_path = Path(project_dir, project_name)\n         project_path.mkdir()\n \n-        p, out, err = self.proc('startproject', project_name, cwd=project_dir)\n+        p, out, err = self.proc(\"startproject\", project_name, cwd=project_dir)\n         print(out)\n         print(err, file=sys.stderr)\n         self.assertEqual(p.returncode, 0)\n \n-        assert Path(project_path, 'scrapy.cfg').exists()\n+        assert Path(project_path, \"scrapy.cfg\").exists()\n         assert Path(project_path, project_name).exists()\n-        assert Path(project_path, project_name, '__init__.py').exists()\n-        assert Path(project_path, project_name, 'items.py').exists()\n-        assert Path(project_path, project_name, 'pipelines.py').exists()\n-        assert Path(project_path, project_name, 'settings.py').exists()\n-        assert Path(project_path, project_name, 'spiders', '__init__.py').exists()\n+        assert Path(project_path, project_name, \"__init__.py\").exists()\n+        assert Path(project_path, project_name, \"items.py\").exists()\n+        assert Path(project_path, project_name, \"pipelines.py\").exists()\n+        assert Path(project_path, project_name, \"settings.py\").exists()\n+        assert Path(project_path, project_name, \"spiders\", \"__init__.py\").exists()\n \n \n-def get_permissions_dict(path: Union[str, os.PathLike], renamings=None, ignore=None) -> Dict[str, str]:\n-\n+def get_permissions_dict(\n+    path: Union[str, os.PathLike], renamings=None, ignore=None\n+) -> Dict[str, str]:\n     def get_permissions(path: Path) -> str:\n         return oct(path.stat().st_mode)\n \n@@ -185,7 +206,7 @@ def get_permissions_dict(path: Union[str, os.PathLike], renamings=None, ignore=N\n \n     renamings = renamings or tuple()\n     permissions_dict = {\n-        '.': get_permissions(path_obj),\n+        \".\": get_permissions(path_obj),\n     }\n     for root, dirs, files in os.walk(path_obj):\n         nodes = list(chain(dirs, files))\n@@ -196,10 +217,7 @@ def get_permissions_dict(path: Union[str, os.PathLike], renamings=None, ignore=N\n             absolute_path = Path(root, node)\n             relative_path = str(absolute_path.relative_to(path))\n             for search_string, replacement in renamings:\n-                relative_path = relative_path.replace(\n-                    search_string,\n-                    replacement\n-                )\n+                relative_path = relative_path.replace(search_string, replacement)\n             permissions = get_permissions(absolute_path)\n             permissions_dict[relative_path] = permissions\n     return permissions_dict\n@@ -211,31 +229,33 @@ class StartprojectTemplatesTest(ProjectTest):\n \n     def setUp(self):\n         super().setUp()\n-        self.tmpl = str(Path(self.temp_path, 'templates'))\n-        self.tmpl_proj = str(Path(self.tmpl, 'project'))\n+        self.tmpl = str(Path(self.temp_path, \"templates\"))\n+        self.tmpl_proj = str(Path(self.tmpl, \"project\"))\n \n     def test_startproject_template_override(self):\n-        copytree(Path(scrapy.__path__[0], 'templates'), self.tmpl)\n-        Path(self.tmpl_proj, 'root_template').write_bytes(b\"\")\n-        assert Path(self.tmpl_proj, 'root_template').exists()\n+        copytree(Path(scrapy.__path__[0], \"templates\"), self.tmpl)\n+        Path(self.tmpl_proj, \"root_template\").write_bytes(b\"\")\n+        assert Path(self.tmpl_proj, \"root_template\").exists()\n \n-        args = ['--set', f'TEMPLATES_DIR={self.tmpl}']\n-        p, out, err = self.proc('startproject', self.project_name, *args)\n-        self.assertIn(f\"New Scrapy project '{self.project_name}', \"\n-                      \"using template directory\", out)\n+        args = [\"--set\", f\"TEMPLATES_DIR={self.tmpl}\"]\n+        p, out, err = self.proc(\"startproject\", self.project_name, *args)\n+        self.assertIn(\n+            f\"New Scrapy project '{self.project_name}', \" \"using template directory\",\n+            out,\n+        )\n         self.assertIn(self.tmpl_proj, out)\n-        assert Path(self.proj_path, 'root_template').exists()\n+        assert Path(self.proj_path, \"root_template\").exists()\n \n     def test_startproject_permissions_from_writable(self):\n         \"\"\"Check that generated files have the right permissions when the\n         template folder has the same permissions as in the project, i.e.\n         everything is writable.\"\"\"\n         scrapy_path = scrapy.__path__[0]\n-        project_template = Path(scrapy_path, 'templates', 'project')\n-        project_name = 'startproject1'\n+        project_template = Path(scrapy_path, \"templates\", \"project\")\n+        project_name = \"startproject1\"\n         renamings = (\n-            ('module', project_name),\n-            ('.tmpl', ''),\n+            (\"module\", project_name),\n+            (\".tmpl\", \"\"),\n         )\n         expected_permissions = get_permissions_dict(\n             project_template,\n@@ -247,9 +267,9 @@ class StartprojectTemplatesTest(ProjectTest):\n         process = subprocess.Popen(\n             (\n                 sys.executable,\n-                '-m',\n-                'scrapy.cmdline',\n-                'startproject',\n+                \"-m\",\n+                \"scrapy.cmdline\",\n+                \"startproject\",\n                 project_name,\n             ),\n             cwd=destination,\n@@ -270,12 +290,12 @@ class StartprojectTemplatesTest(ProjectTest):\n         See https://github.com/scrapy/scrapy/pull/4604\n         \"\"\"\n         scrapy_path = scrapy.__path__[0]\n-        templates_dir = Path(scrapy_path, 'templates')\n-        project_template = Path(templates_dir, 'project')\n-        project_name = 'startproject2'\n+        templates_dir = Path(scrapy_path, \"templates\")\n+        project_template = Path(templates_dir, \"project\")\n+        project_name = \"startproject2\"\n         renamings = (\n-            ('module', project_name),\n-            ('.tmpl', ''),\n+            (\"module\", project_name),\n+            (\".tmpl\", \"\"),\n         )\n         expected_permissions = get_permissions_dict(\n             project_template,\n@@ -287,7 +307,7 @@ class StartprojectTemplatesTest(ProjectTest):\n             current_permissions = path.stat().st_mode\n             path.chmod(current_permissions & ~ANYONE_WRITE_PERMISSION)\n \n-        read_only_templates_dir = str(Path(mkdtemp()) / 'templates')\n+        read_only_templates_dir = str(Path(mkdtemp()) / \"templates\")\n         copytree(templates_dir, read_only_templates_dir)\n \n         for root, dirs, files in os.walk(read_only_templates_dir):\n@@ -298,12 +318,12 @@ class StartprojectTemplatesTest(ProjectTest):\n         process = subprocess.Popen(\n             (\n                 sys.executable,\n-                '-m',\n-                'scrapy.cmdline',\n-                'startproject',\n+                \"-m\",\n+                \"scrapy.cmdline\",\n+                \"startproject\",\n                 project_name,\n-                '--set',\n-                f'TEMPLATES_DIR={read_only_templates_dir}',\n+                \"--set\",\n+                f\"TEMPLATES_DIR={read_only_templates_dir}\",\n             ),\n             cwd=destination,\n             env=self.env,\n@@ -319,11 +339,11 @@ class StartprojectTemplatesTest(ProjectTest):\n         \"\"\"Check that pre-existing folders and files in the destination folder\n         do not see their permissions modified.\"\"\"\n         scrapy_path = scrapy.__path__[0]\n-        project_template = Path(scrapy_path, 'templates', 'project')\n-        project_name = 'startproject3'\n+        project_template = Path(scrapy_path, \"templates\", \"project\")\n+        project_name = \"startproject3\"\n         renamings = (\n-            ('module', project_name),\n-            ('.tmpl', ''),\n+            (\"module\", project_name),\n+            (\".tmpl\", \"\"),\n         )\n         expected_permissions = get_permissions_dict(\n             project_template,\n@@ -336,15 +356,20 @@ class StartprojectTemplatesTest(ProjectTest):\n \n         existing_nodes = {\n             oct(permissions)[2:] + extension: permissions\n-            for extension in ('', '.d')\n+            for extension in (\"\", \".d\")\n             for permissions in (\n-                0o444, 0o555, 0o644, 0o666, 0o755, 0o777,\n+                0o444,\n+                0o555,\n+                0o644,\n+                0o666,\n+                0o755,\n+                0o777,\n             )\n         }\n         project_dir.mkdir()\n         for node, permissions in existing_nodes.items():\n             path = project_dir / node\n-            if node.endswith('.d'):\n+            if node.endswith(\".d\"):\n                 path.mkdir(mode=permissions)\n             else:\n                 path.touch(mode=permissions)\n@@ -353,11 +378,11 @@ class StartprojectTemplatesTest(ProjectTest):\n         process = subprocess.Popen(\n             (\n                 sys.executable,\n-                '-m',\n-                'scrapy.cmdline',\n-                'startproject',\n+                \"-m\",\n+                \"scrapy.cmdline\",\n+                \"startproject\",\n                 project_name,\n-                '.',\n+                \".\",\n             ),\n             cwd=project_dir,\n             env=self.env,\n@@ -372,6 +397,7 @@ class StartprojectTemplatesTest(ProjectTest):\n         \"\"\"Check that generated files have the right permissions when the\n         system uses a umask value that causes new files to have different\n         permissions than those from the template folder.\"\"\"\n+\n         @contextmanager\n         def umask(new_mask):\n             cur_mask = os.umask(new_mask)\n@@ -379,15 +405,11 @@ class StartprojectTemplatesTest(ProjectTest):\n             os.umask(cur_mask)\n \n         scrapy_path = scrapy.__path__[0]\n-        project_template = Path(\n-            scrapy_path,\n-            'templates',\n-            'project'\n-        )\n-        project_name = 'umaskproject'\n+        project_template = Path(scrapy_path, \"templates\", \"project\")\n+        project_name = \"umaskproject\"\n         renamings = (\n-            ('module', project_name),\n-            ('.tmpl', ''),\n+            (\"module\", project_name),\n+            (\".tmpl\", \"\"),\n         )\n         expected_permissions = get_permissions_dict(\n             project_template,\n@@ -400,9 +422,9 @@ class StartprojectTemplatesTest(ProjectTest):\n             process = subprocess.Popen(\n                 (\n                     sys.executable,\n-                    '-m',\n-                    'scrapy.cmdline',\n-                    'startproject',\n+                    \"-m\",\n+                    \"scrapy.cmdline\",\n+                    \"startproject\",\n                     project_name,\n                 ),\n                 cwd=destination,\n@@ -417,67 +439,74 @@ class StartprojectTemplatesTest(ProjectTest):\n \n \n class CommandTest(ProjectTest):\n-\n     def setUp(self):\n         super().setUp()\n-        self.call('startproject', self.project_name)\n+        self.call(\"startproject\", self.project_name)\n         self.cwd = Path(self.temp_path, self.project_name)\n-        self.env['SCRAPY_SETTINGS_MODULE'] = f'{self.project_name}.settings'\n+        self.env[\"SCRAPY_SETTINGS_MODULE\"] = f\"{self.project_name}.settings\"\n \n \n class GenspiderCommandTest(CommandTest):\n-\n     def test_arguments(self):\n         # only pass one argument. spider script shouldn't be created\n-        self.assertEqual(2, self.call('genspider', 'test_name'))\n-        assert not Path(self.proj_mod_path, 'spiders', 'test_name.py').exists()\n+        self.assertEqual(2, self.call(\"genspider\", \"test_name\"))\n+        assert not Path(self.proj_mod_path, \"spiders\", \"test_name.py\").exists()\n         # pass two arguments <name> <domain>. spider script should be created\n-        self.assertEqual(0, self.call('genspider', 'test_name', 'test.com'))\n-        assert Path(self.proj_mod_path, 'spiders', 'test_name.py').exists()\n+        self.assertEqual(0, self.call(\"genspider\", \"test_name\", \"test.com\"))\n+        assert Path(self.proj_mod_path, \"spiders\", \"test_name.py\").exists()\n \n-    def test_template(self, tplname='crawl'):\n-        args = [f'--template={tplname}'] if tplname else []\n-        spname = 'test_spider'\n+    def test_template(self, tplname=\"crawl\"):\n+        args = [f\"--template={tplname}\"] if tplname else []\n+        spname = \"test_spider\"\n         spmodule = f\"{self.project_name}.spiders.{spname}\"\n-        p, out, err = self.proc('genspider', spname, 'test.com', *args)\n-        self.assertIn(f\"Created spider {spname!r} using template {tplname!r} in module:{os.linesep}  {spmodule}\", out)\n-        self.assertTrue(Path(self.proj_mod_path, 'spiders', 'test_spider.py').exists())\n-        modify_time_before = Path(self.proj_mod_path, 'spiders', 'test_spider.py').stat().st_mtime\n-        p, out, err = self.proc('genspider', spname, 'test.com', *args)\n+        p, out, err = self.proc(\"genspider\", spname, \"test.com\", *args)\n+        self.assertIn(\n+            f\"Created spider {spname!r} using template {tplname!r} in module:{os.linesep}  {spmodule}\",\n+            out,\n+        )\n+        self.assertTrue(Path(self.proj_mod_path, \"spiders\", \"test_spider.py\").exists())\n+        modify_time_before = (\n+            Path(self.proj_mod_path, \"spiders\", \"test_spider.py\").stat().st_mtime\n+        )\n+        p, out, err = self.proc(\"genspider\", spname, \"test.com\", *args)\n         self.assertIn(f\"Spider {spname!r} already exists in module\", out)\n-        modify_time_after = Path(self.proj_mod_path, 'spiders', 'test_spider.py').stat().st_mtime\n+        modify_time_after = (\n+            Path(self.proj_mod_path, \"spiders\", \"test_spider.py\").stat().st_mtime\n+        )\n         self.assertEqual(modify_time_after, modify_time_before)\n \n     def test_template_basic(self):\n-        self.test_template('basic')\n+        self.test_template(\"basic\")\n \n     def test_template_csvfeed(self):\n-        self.test_template('csvfeed')\n+        self.test_template(\"csvfeed\")\n \n     def test_template_xmlfeed(self):\n-        self.test_template('xmlfeed')\n+        self.test_template(\"xmlfeed\")\n \n     def test_list(self):\n-        self.assertEqual(0, self.call('genspider', '--list'))\n+        self.assertEqual(0, self.call(\"genspider\", \"--list\"))\n \n     def test_dump(self):\n-        self.assertEqual(0, self.call('genspider', '--dump=basic'))\n-        self.assertEqual(0, self.call('genspider', '-d', 'basic'))\n+        self.assertEqual(0, self.call(\"genspider\", \"--dump=basic\"))\n+        self.assertEqual(0, self.call(\"genspider\", \"-d\", \"basic\"))\n \n     def test_same_name_as_project(self):\n-        self.assertEqual(2, self.call('genspider', self.project_name))\n-        assert not Path(self.proj_mod_path, 'spiders', f'{self.project_name}.py').exists()\n+        self.assertEqual(2, self.call(\"genspider\", self.project_name))\n+        assert not Path(\n+            self.proj_mod_path, \"spiders\", f\"{self.project_name}.py\"\n+        ).exists()\n \n     def test_same_filename_as_existing_spider(self, force=False):\n-        file_name = 'example'\n-        file_path = Path(self.proj_mod_path, 'spiders', f'{file_name}.py')\n-        self.assertEqual(0, self.call('genspider', file_name, 'example.com'))\n+        file_name = \"example\"\n+        file_path = Path(self.proj_mod_path, \"spiders\", f\"{file_name}.py\")\n+        self.assertEqual(0, self.call(\"genspider\", file_name, \"example.com\"))\n         assert file_path.exists()\n \n         # change name of spider but not its file name\n-        with file_path.open('r+', encoding=\"utf-8\") as spider_file:\n+        with file_path.open(\"r+\", encoding=\"utf-8\") as spider_file:\n             file_data = spider_file.read()\n-            file_data = file_data.replace(\"name = \\'example\\'\", \"name = \\'renamed\\'\")\n+            file_data = file_data.replace(\"name = 'example'\", \"name = 'renamed'\")\n             spider_file.seek(0)\n             spider_file.write(file_data)\n             spider_file.truncate()\n@@ -485,14 +514,16 @@ class GenspiderCommandTest(CommandTest):\n         file_contents_before = file_data\n \n         if force:\n-            p, out, err = self.proc('genspider', '--force', file_name, 'example.com')\n-            self.assertIn(f\"Created spider {file_name!r} using template \\'basic\\' in module\", out)\n+            p, out, err = self.proc(\"genspider\", \"--force\", file_name, \"example.com\")\n+            self.assertIn(\n+                f\"Created spider {file_name!r} using template 'basic' in module\", out\n+            )\n             modify_time_after = file_path.stat().st_mtime\n             self.assertNotEqual(modify_time_after, modify_time_before)\n             file_contents_after = file_path.read_text(encoding=\"utf-8\")\n             self.assertNotEqual(file_contents_after, file_contents_before)\n         else:\n-            p, out, err = self.proc('genspider', file_name, 'example.com')\n+            p, out, err = self.proc(\"genspider\", file_name, \"example.com\")\n             self.assertIn(f\"{file_path.resolve()} already exists\", out)\n             modify_time_after = file_path.stat().st_mtime\n             self.assertEqual(modify_time_after, modify_time_before)\n@@ -502,53 +533,63 @@ class GenspiderCommandTest(CommandTest):\n     def test_same_filename_as_existing_spider_force(self):\n         self.test_same_filename_as_existing_spider(force=True)\n \n-    def test_url(self, url='test.com', domain=\"test.com\"):\n-        self.assertEqual(0, self.call('genspider', '--force', 'test_name', url))\n-        self.assertEqual(domain,\n-                         self.find_in_file(Path(self.proj_mod_path,\n-                                                'spiders', 'test_name.py'),\n-                                           r'allowed_domains\\s*=\\s*\\[\\'(.+)\\'\\]').group(1))\n-        self.assertEqual(f'http://{domain}/',\n-                         self.find_in_file(Path(self.proj_mod_path,\n-                                                'spiders', 'test_name.py'),\n-                                           r'start_urls\\s*=\\s*\\[\\'(.+)\\'\\]').group(1))\n+    def test_url(self, url=\"test.com\", domain=\"test.com\"):\n+        self.assertEqual(0, self.call(\"genspider\", \"--force\", \"test_name\", url))\n+        self.assertEqual(\n+            domain,\n+            self.find_in_file(\n+                Path(self.proj_mod_path, \"spiders\", \"test_name.py\"),\n+                r\"allowed_domains\\s*=\\s*\\[\\'(.+)\\'\\]\",\n+            ).group(1),\n+        )\n+        self.assertEqual(\n+            f\"http://{domain}/\",\n+            self.find_in_file(\n+                Path(self.proj_mod_path, \"spiders\", \"test_name.py\"),\n+                r\"start_urls\\s*=\\s*\\[\\'(.+)\\'\\]\",\n+            ).group(1),\n+        )\n \n     def test_url_schema(self):\n-        self.test_url('http://test.com', 'test.com')\n+        self.test_url(\"http://test.com\", \"test.com\")\n \n     def test_url_path(self):\n-        self.test_url('test.com/some/other/page', 'test.com')\n+        self.test_url(\"test.com/some/other/page\", \"test.com\")\n \n     def test_url_schema_path(self):\n-        self.test_url('https://test.com/some/other/page', 'test.com')\n+        self.test_url(\"https://test.com/some/other/page\", \"test.com\")\n \n \n class GenspiderStandaloneCommandTest(ProjectTest):\n-\n     def test_generate_standalone_spider(self):\n-        self.call('genspider', 'example', 'example.com')\n-        assert Path(self.temp_path, 'example.py').exists()\n+        self.call(\"genspider\", \"example\", \"example.com\")\n+        assert Path(self.temp_path, \"example.py\").exists()\n \n     def test_same_name_as_existing_file(self, force=False):\n-        file_name = 'example'\n-        file_path = Path(self.temp_path, file_name + '.py')\n-        p, out, err = self.proc('genspider', file_name, 'example.com')\n-        self.assertIn(f\"Created spider {file_name!r} using template \\'basic\\' \", out)\n+        file_name = \"example\"\n+        file_path = Path(self.temp_path, file_name + \".py\")\n+        p, out, err = self.proc(\"genspider\", file_name, \"example.com\")\n+        self.assertIn(f\"Created spider {file_name!r} using template 'basic' \", out)\n         assert file_path.exists()\n         modify_time_before = file_path.stat().st_mtime\n         file_contents_before = file_path.read_text(encoding=\"utf-8\")\n \n         if force:\n             # use different template to ensure contents were changed\n-            p, out, err = self.proc('genspider', '--force', '-t', 'crawl', file_name, 'example.com')\n-            self.assertIn(f\"Created spider {file_name!r} using template \\'crawl\\' \", out)\n+            p, out, err = self.proc(\n+                \"genspider\", \"--force\", \"-t\", \"crawl\", file_name, \"example.com\"\n+            )\n+            self.assertIn(f\"Created spider {file_name!r} using template 'crawl' \", out)\n             modify_time_after = file_path.stat().st_mtime\n             self.assertNotEqual(modify_time_after, modify_time_before)\n             file_contents_after = file_path.read_text(encoding=\"utf-8\")\n             self.assertNotEqual(file_contents_after, file_contents_before)\n         else:\n-            p, out, err = self.proc('genspider', file_name, 'example.com')\n-            self.assertIn(f\"{Path(self.temp_path, file_name + '.py').resolve()} already exists\", out)\n+            p, out, err = self.proc(\"genspider\", file_name, \"example.com\")\n+            self.assertIn(\n+                f\"{Path(self.temp_path, file_name + '.py').resolve()} already exists\",\n+                out,\n+            )\n             modify_time_after = file_path.stat().st_mtime\n             self.assertEqual(modify_time_after, modify_time_before)\n             file_contents_after = file_path.read_text(encoding=\"utf-8\")\n@@ -559,14 +600,13 @@ class GenspiderStandaloneCommandTest(ProjectTest):\n \n \n class MiscCommandsTest(CommandTest):\n-\n     def test_list(self):\n-        self.assertEqual(0, self.call('list'))\n+        self.assertEqual(0, self.call(\"list\"))\n \n \n class RunSpiderCommandTest(CommandTest):\n \n-    spider_filename = 'myspider.py'\n+    spider_filename = \"myspider.py\"\n \n     debug_log_spider = \"\"\"\n import scrapy\n@@ -604,7 +644,7 @@ class BadSpider(scrapy.Spider):\n \n     def runspider(self, code, name=None, args=()):\n         with self._create_file(code, name) as fname:\n-            return self.proc('runspider', fname, *args)\n+            return self.proc(\"runspider\", fname, *args)\n \n     def get_log(self, code, name=None, args=()):\n         p, stdout, stderr = self.runspider(code, name, args=args)\n@@ -618,18 +658,21 @@ class BadSpider(scrapy.Spider):\n         self.assertIn(\"INFO: Spider closed (finished)\", log)\n \n     def test_run_fail_spider(self):\n-        proc, _, _ = self.runspider(\"import scrapy\\n\" + inspect.getsource(ExceptionSpider))\n+        proc, _, _ = self.runspider(\n+            \"import scrapy\\n\" + inspect.getsource(ExceptionSpider)\n+        )\n         ret = proc.returncode\n         self.assertNotEqual(ret, 0)\n \n     def test_run_good_spider(self):\n-        proc, _, _ = self.runspider(\"import scrapy\\n\" + inspect.getsource(NoRequestsSpider))\n+        proc, _, _ = self.runspider(\n+            \"import scrapy\\n\" + inspect.getsource(NoRequestsSpider)\n+        )\n         ret = proc.returncode\n         self.assertEqual(ret, 0)\n \n     def test_runspider_log_level(self):\n-        log = self.get_log(self.debug_log_spider,\n-                           args=('-s', 'LOG_LEVEL=INFO'))\n+        log = self.get_log(self.debug_log_spider, args=(\"-s\", \"LOG_LEVEL=INFO\"))\n         self.assertNotIn(\"DEBUG: It Works!\", log)\n         self.assertIn(\"INFO: Spider opened\", log)\n \n@@ -649,19 +692,17 @@ class MySpider(scrapy.Spider):\n     def parse(self, response):\n         return {'test': 'value'}\n \"\"\"\n-        log = self.get_log(dnscache_spider, args=('-s', 'DNSCACHE_ENABLED=False'))\n+        log = self.get_log(dnscache_spider, args=(\"-s\", \"DNSCACHE_ENABLED=False\"))\n         self.assertNotIn(\"DNSLookupError\", log)\n         self.assertIn(\"INFO: Spider opened\", log)\n \n     def test_runspider_log_short_names(self):\n-        log1 = self.get_log(self.debug_log_spider,\n-                            args=('-s', 'LOG_SHORT_NAMES=1'))\n+        log1 = self.get_log(self.debug_log_spider, args=(\"-s\", \"LOG_SHORT_NAMES=1\"))\n         self.assertIn(\"[myspider] DEBUG: It Works!\", log1)\n         self.assertIn(\"[scrapy]\", log1)\n         self.assertNotIn(\"[scrapy.core.engine]\", log1)\n \n-        log2 = self.get_log(self.debug_log_spider,\n-                            args=('-s', 'LOG_SHORT_NAMES=0'))\n+        log2 = self.get_log(self.debug_log_spider, args=(\"-s\", \"LOG_SHORT_NAMES=0\"))\n         self.assertIn(\"[myspider] DEBUG: It Works!\", log2)\n         self.assertNotIn(\"[scrapy]\", log2)\n         self.assertIn(\"[scrapy.core.engine]\", log2)\n@@ -671,57 +712,89 @@ class MySpider(scrapy.Spider):\n         self.assertIn(\"No spider found in file\", log)\n \n     def test_runspider_file_not_found(self):\n-        _, _, log = self.proc('runspider', 'some_non_existent_file')\n+        _, _, log = self.proc(\"runspider\", \"some_non_existent_file\")\n         self.assertIn(\"File not found: some_non_existent_file\", log)\n \n     def test_runspider_unable_to_load(self):\n-        log = self.get_log('', name='myspider.txt')\n-        self.assertIn('Unable to load', log)\n+        log = self.get_log(\"\", name=\"myspider.txt\")\n+        self.assertIn(\"Unable to load\", log)\n \n     def test_start_requests_errors(self):\n-        log = self.get_log(self.badspider, name='badspider.py')\n+        log = self.get_log(self.badspider, name=\"badspider.py\")\n         self.assertIn(\"start_requests\", log)\n         self.assertIn(\"badspider.py\", log)\n \n     def test_asyncio_enabled_true(self):\n-        log = self.get_log(self.debug_log_spider, args=[\n-            '-s', 'TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor'\n-        ])\n-        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        log = self.get_log(\n+            self.debug_log_spider,\n+            args=[\n+                \"-s\",\n+                \"TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+            ],\n+        )\n+        self.assertIn(\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        )\n \n     def test_asyncio_enabled_default(self):\n         log = self.get_log(self.debug_log_spider, args=[])\n-        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        self.assertIn(\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        )\n \n     def test_asyncio_enabled_false(self):\n-        log = self.get_log(self.debug_log_spider, args=[\n-            '-s', 'TWISTED_REACTOR=twisted.internet.selectreactor.SelectReactor'\n-        ])\n-        self.assertIn(\"Using reactor: twisted.internet.selectreactor.SelectReactor\", log)\n-        self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        log = self.get_log(\n+            self.debug_log_spider,\n+            args=[\"-s\", \"TWISTED_REACTOR=twisted.internet.selectreactor.SelectReactor\"],\n+        )\n+        self.assertIn(\n+            \"Using reactor: twisted.internet.selectreactor.SelectReactor\", log\n+        )\n+        self.assertNotIn(\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        )\n \n-    @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')\n-    @mark.skipif(platform.system() == 'Windows', reason='uvloop does not support Windows')\n-    @mark.skipif(twisted_version == Version('twisted', 21, 2, 0), reason='https://twistedmatrix.com/trac/ticket/10106')\n+    @mark.skipif(\n+        sys.implementation.name == \"pypy\",\n+        reason=\"uvloop does not support pypy properly\",\n+    )\n+    @mark.skipif(\n+        platform.system() == \"Windows\", reason=\"uvloop does not support Windows\"\n+    )\n+    @mark.skipif(\n+        twisted_version == Version(\"twisted\", 21, 2, 0),\n+        reason=\"https://twistedmatrix.com/trac/ticket/10106\",\n+    )\n     def test_custom_asyncio_loop_enabled_true(self):\n-        log = self.get_log(self.debug_log_spider, args=[\n-            '-s',\n-            'TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor',\n-            '-s',\n-            'ASYNCIO_EVENT_LOOP=uvloop.Loop',\n-        ])\n+        log = self.get_log(\n+            self.debug_log_spider,\n+            args=[\n+                \"-s\",\n+                \"TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+                \"-s\",\n+                \"ASYNCIO_EVENT_LOOP=uvloop.Loop\",\n+            ],\n+        )\n         self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n \n     def test_custom_asyncio_loop_enabled_false(self):\n-        log = self.get_log(self.debug_log_spider, args=[\n-            '-s', 'TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor'\n-        ])\n+        log = self.get_log(\n+            self.debug_log_spider,\n+            args=[\n+                \"-s\",\n+                \"TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+            ],\n+        )\n         import asyncio\n-        if sys.platform != 'win32':\n+\n+        if sys.platform != \"win32\":\n             loop = asyncio.new_event_loop()\n         else:\n             loop = asyncio.SelectorEventLoop()\n-        self.assertIn(f\"Using asyncio event loop: {loop.__module__}.{loop.__class__.__name__}\", log)\n+        self.assertIn(\n+            f\"Using asyncio event loop: {loop.__module__}.{loop.__class__.__name__}\",\n+            log,\n+        )\n \n     def test_output(self):\n         spider_code = \"\"\"\n@@ -734,9 +807,11 @@ class MySpider(scrapy.Spider):\n         self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))\n         return []\n \"\"\"\n-        args = ['-o', 'example.json']\n+        args = [\"-o\", \"example.json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn(\"[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}\", log)\n+        self.assertIn(\n+            \"[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}\", log\n+        )\n \n     def test_overwrite_output(self):\n         spider_code = \"\"\"\n@@ -755,9 +830,12 @@ class MySpider(scrapy.Spider):\n         return []\n \"\"\"\n         Path(self.cwd, \"example.json\").write_text(\"not empty\", encoding=\"utf-8\")\n-        args = ['-O', 'example.json']\n+        args = [\"-O\", \"example.json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn('[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}', log)\n+        self.assertIn(\n+            '[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}',\n+            log,\n+        )\n         with Path(self.cwd, \"example.json\").open(encoding=\"utf-8\") as f2:\n             first_line = f2.readline()\n         self.assertNotEqual(first_line, \"not empty\")\n@@ -772,9 +850,11 @@ class MySpider(scrapy.Spider):\n     def start_requests(self):\n         return []\n \"\"\"\n-        args = ['-o', 'example1.json', '-O', 'example2.json']\n+        args = [\"-o\", \"example1.json\", \"-O\", \"example2.json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn(\"error: Please use only one of -o/--output and -O/--overwrite-output\", log)\n+        self.assertIn(\n+            \"error: Please use only one of -o/--output and -O/--overwrite-output\", log\n+        )\n \n     def test_output_stdout(self):\n         spider_code = \"\"\"\n@@ -787,21 +867,21 @@ class MySpider(scrapy.Spider):\n         self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))\n         return []\n \"\"\"\n-        args = ['-o', '-:json']\n+        args = [\"-o\", \"-:json\"]\n         log = self.get_log(spider_code, args=args)\n         self.assertIn(\"[myspider] DEBUG: FEEDS: {'stdout:': {'format': 'json'}}\", log)\n \n \n-@skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n+@skipIf(platform.system() != \"Windows\", \"Windows required for .pyw files\")\n class WindowsRunSpiderCommandTest(RunSpiderCommandTest):\n \n-    spider_filename = 'myspider.pyw'\n+    spider_filename = \"myspider.pyw\"\n \n     def setUp(self):\n         super().setUp()\n \n     def test_start_requests_errors(self):\n-        log = self.get_log(self.badspider, name='badspider.pyw')\n+        log = self.get_log(self.badspider, name=\"badspider.pyw\")\n         self.assertIn(\"start_requests\", log)\n         self.assertIn(\"badspider.pyw\", log)\n \n@@ -834,34 +914,37 @@ class WindowsRunSpiderCommandTest(RunSpiderCommandTest):\n \n \n class BenchCommandTest(CommandTest):\n-\n     def test_run(self):\n-        _, _, log = self.proc('bench', '-s', 'LOGSTATS_INTERVAL=0.001',\n-                              '-s', 'CLOSESPIDER_TIMEOUT=0.01')\n-        self.assertIn('INFO: Crawled', log)\n-        self.assertNotIn('Unhandled Error', log)\n+        _, _, log = self.proc(\n+            \"bench\", \"-s\", \"LOGSTATS_INTERVAL=0.001\", \"-s\", \"CLOSESPIDER_TIMEOUT=0.01\"\n+        )\n+        self.assertIn(\"INFO: Crawled\", log)\n+        self.assertNotIn(\"Unhandled Error\", log)\n \n \n class ViewCommandTest(CommandTest):\n-\n     def test_methods(self):\n         command = view.Command()\n         command.settings = Settings()\n-        parser = argparse.ArgumentParser(prog='scrapy', prefix_chars='-',\n+        parser = argparse.ArgumentParser(\n+            prog=\"scrapy\",\n+            prefix_chars=\"-\",\n             formatter_class=ScrapyHelpFormatter,\n-                                         conflict_handler='resolve')\n+            conflict_handler=\"resolve\",\n+        )\n         command.add_options(parser)\n-        self.assertEqual(command.short_desc(),\n-                         \"Open URL in browser, as seen by Scrapy\")\n-        self.assertIn(\"URL using the Scrapy downloader and show its\",\n-                      command.long_desc())\n+        self.assertEqual(command.short_desc(), \"Open URL in browser, as seen by Scrapy\")\n+        self.assertIn(\n+            \"URL using the Scrapy downloader and show its\", command.long_desc()\n+        )\n \n \n class CrawlCommandTest(CommandTest):\n-\n     def crawl(self, code, args=()):\n-        Path(self.proj_mod_path, 'spiders', 'myspider.py').write_text(code, encoding=\"utf-8\")\n-        return self.proc('crawl', 'myspider', *args)\n+        Path(self.proj_mod_path, \"spiders\", \"myspider.py\").write_text(\n+            code, encoding=\"utf-8\"\n+        )\n+        return self.proc(\"crawl\", \"myspider\", *args)\n \n     def get_log(self, code, args=()):\n         _, _, stderr = self.crawl(code, args=args)\n@@ -892,9 +975,11 @@ class MySpider(scrapy.Spider):\n         self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))\n         return []\n \"\"\"\n-        args = ['-o', 'example.json']\n+        args = [\"-o\", \"example.json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn(\"[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}\", log)\n+        self.assertIn(\n+            \"[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}\", log\n+        )\n \n     def test_overwrite_output(self):\n         spider_code = \"\"\"\n@@ -913,9 +998,12 @@ class MySpider(scrapy.Spider):\n         return []\n \"\"\"\n         Path(self.cwd, \"example.json\").write_text(\"not empty\", encoding=\"utf-8\")\n-        args = ['-O', 'example.json']\n+        args = [\"-O\", \"example.json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn('[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}', log)\n+        self.assertIn(\n+            '[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}',\n+            log,\n+        )\n         with Path(self.cwd, \"example.json\").open(encoding=\"utf-8\") as f2:\n             first_line = f2.readline()\n         self.assertNotEqual(first_line, \"not empty\")\n@@ -930,18 +1018,32 @@ class MySpider(scrapy.Spider):\n     def start_requests(self):\n         return []\n \"\"\"\n-        args = ['-o', 'example1.json', '-O', 'example2.json']\n+        args = [\"-o\", \"example1.json\", \"-O\", \"example2.json\"]\n         log = self.get_log(spider_code, args=args)\n-        self.assertIn(\"error: Please use only one of -o/--output and -O/--overwrite-output\", log)\n+        self.assertIn(\n+            \"error: Please use only one of -o/--output and -O/--overwrite-output\", log\n+        )\n \n \n class HelpMessageTest(CommandTest):\n-\n     def setUp(self):\n         super().setUp()\n-        self.commands = [\"parse\", \"startproject\", \"view\", \"crawl\", \"edit\",\n-                         \"list\", \"fetch\", \"settings\", \"shell\", \"runspider\",\n-                         \"version\", \"genspider\", \"check\", \"bench\"]\n+        self.commands = [\n+            \"parse\",\n+            \"startproject\",\n+            \"view\",\n+            \"crawl\",\n+            \"edit\",\n+            \"list\",\n+            \"fetch\",\n+            \"settings\",\n+            \"shell\",\n+            \"runspider\",\n+            \"version\",\n+            \"genspider\",\n+            \"check\",\n+            \"bench\",\n+        ]\n \n     def test_help_messages(self):\n         for command in self.commands:\n\n@@ -26,42 +26,42 @@ class TestItem(Item):\n \n \n class ResponseMock:\n-    url = 'http://scrapy.org'\n+    url = \"http://scrapy.org\"\n \n \n class CustomSuccessContract(Contract):\n-    name = 'custom_success_contract'\n+    name = \"custom_success_contract\"\n \n     def adjust_request_args(self, args):\n-        args['url'] = 'http://scrapy.org'\n+        args[\"url\"] = \"http://scrapy.org\"\n         return args\n \n \n class CustomFailContract(Contract):\n-    name = 'custom_fail_contract'\n+    name = \"custom_fail_contract\"\n \n     def adjust_request_args(self, args):\n-        raise TypeError('Error in adjust_request_args')\n+        raise TypeError(\"Error in adjust_request_args\")\n \n \n class CustomFormContract(Contract):\n-    name = 'custom_form'\n+    name = \"custom_form\"\n     request_cls = FormRequest\n \n     def adjust_request_args(self, args):\n-        args['formdata'] = {'name': 'scrapy'}\n+        args[\"formdata\"] = {\"name\": \"scrapy\"}\n         return args\n \n \n class TestSpider(Spider):\n-    name = 'demo_spider'\n+    name = \"demo_spider\"\n \n     def returns_request(self, response):\n         \"\"\"method which returns request\n         @url http://scrapy.org\n         @returns requests 1\n         \"\"\"\n-        return Request('http://scrapy.org', callback=self.returns_item)\n+        return Request(\"http://scrapy.org\", callback=self.returns_item)\n \n     def returns_item(self, response):\n         \"\"\"method which returns item\n@@ -120,7 +120,7 @@ class TestSpider(Spider):\n         @url http://scrapy.org\n         @returns items 0 0\n         \"\"\"\n-        return {'url': response.url}\n+        return {\"url\": response.url}\n \n     def scrapes_item_ok(self, response):\n         \"\"\"returns item with name and url\n@@ -128,7 +128,7 @@ class TestSpider(Spider):\n         @returns items 1 1\n         @scrapes name url\n         \"\"\"\n-        return TestItem(name='test', url=response.url)\n+        return TestItem(name=\"test\", url=response.url)\n \n     def scrapes_dict_item_ok(self, response):\n         \"\"\"returns item with name and url\n@@ -136,7 +136,7 @@ class TestSpider(Spider):\n         @returns items 1 1\n         @scrapes name url\n         \"\"\"\n-        return {'name': 'test', 'url': response.url}\n+        return {\"name\": \"test\", \"url\": response.url}\n \n     def scrapes_item_fail(self, response):\n         \"\"\"returns item with no name\n@@ -152,7 +152,7 @@ class TestSpider(Spider):\n         @returns items 1 1\n         @scrapes name url\n         \"\"\"\n-        return {'url': response.url}\n+        return {\"url\": response.url}\n \n     def scrapes_multiple_missing_fields(self, response):\n         \"\"\"returns item with no name\n@@ -177,7 +177,7 @@ class TestSpider(Spider):\n \n \n class CustomContractSuccessSpider(Spider):\n-    name = 'custom_contract_success_spider'\n+    name = \"custom_contract_success_spider\"\n \n     def parse(self, response):\n         \"\"\"\n@@ -187,7 +187,7 @@ class CustomContractSuccessSpider(Spider):\n \n \n class CustomContractFailSpider(Spider):\n-    name = 'custom_contract_fail_spider'\n+    name = \"custom_contract_fail_spider\"\n \n     def parse(self, response):\n         \"\"\"\n@@ -197,7 +197,7 @@ class CustomContractFailSpider(Spider):\n \n \n class InheritsTestSpider(TestSpider):\n-    name = 'inherits_demo_spider'\n+    name = \"inherits_demo_spider\"\n \n \n class ContractsManagerTest(unittest.TestCase):\n@@ -234,7 +234,8 @@ class ContractsManagerTest(unittest.TestCase):\n         self.assertEqual(len(contracts), 2)\n         self.assertEqual(\n             frozenset(type(x) for x in contracts),\n-            frozenset([UrlContract, ReturnsContract]))\n+            frozenset([UrlContract, ReturnsContract]),\n+        )\n \n         # returns request for valid method\n         request = self.conman.from_method(spider.returns_request, self.results)\n@@ -251,26 +252,40 @@ class ContractsManagerTest(unittest.TestCase):\n         # extract contracts correctly\n         contracts = self.conman.extract_contracts(spider.returns_request_cb_kwargs)\n         self.assertEqual(len(contracts), 3)\n-        self.assertEqual(frozenset(type(x) for x in contracts),\n-                         frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]))\n+        self.assertEqual(\n+            frozenset(type(x) for x in contracts),\n+            frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]),\n+        )\n \n         contracts = self.conman.extract_contracts(spider.returns_item_cb_kwargs)\n         self.assertEqual(len(contracts), 3)\n-        self.assertEqual(frozenset(type(x) for x in contracts),\n-                         frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]))\n+        self.assertEqual(\n+            frozenset(type(x) for x in contracts),\n+            frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]),\n+        )\n \n-        contracts = self.conman.extract_contracts(spider.returns_item_cb_kwargs_error_unexpected_keyword)\n+        contracts = self.conman.extract_contracts(\n+            spider.returns_item_cb_kwargs_error_unexpected_keyword\n+        )\n         self.assertEqual(len(contracts), 3)\n-        self.assertEqual(frozenset(type(x) for x in contracts),\n-                         frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]))\n+        self.assertEqual(\n+            frozenset(type(x) for x in contracts),\n+            frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]),\n+        )\n \n-        contracts = self.conman.extract_contracts(spider.returns_item_cb_kwargs_error_missing_argument)\n+        contracts = self.conman.extract_contracts(\n+            spider.returns_item_cb_kwargs_error_missing_argument\n+        )\n         self.assertEqual(len(contracts), 2)\n-        self.assertEqual(frozenset(type(x) for x in contracts),\n-                         frozenset([UrlContract, ReturnsContract]))\n+        self.assertEqual(\n+            frozenset(type(x) for x in contracts),\n+            frozenset([UrlContract, ReturnsContract]),\n+        )\n \n         # returns_request\n-        request = self.conman.from_method(spider.returns_request_cb_kwargs, self.results)\n+        request = self.conman.from_method(\n+            spider.returns_request_cb_kwargs, self.results\n+        )\n         request.callback(response, **request.cb_kwargs)\n         self.should_succeed()\n \n@@ -280,12 +295,16 @@ class ContractsManagerTest(unittest.TestCase):\n         self.should_succeed()\n \n         # returns_item (error, callback doesn't take keyword arguments)\n-        request = self.conman.from_method(spider.returns_item_cb_kwargs_error_unexpected_keyword, self.results)\n+        request = self.conman.from_method(\n+            spider.returns_item_cb_kwargs_error_unexpected_keyword, self.results\n+        )\n         request.callback(response, **request.cb_kwargs)\n         self.should_error()\n \n         # returns_item (error, contract doesn't provide keyword arguments)\n-        request = self.conman.from_method(spider.returns_item_cb_kwargs_error_missing_argument, self.results)\n+        request = self.conman.from_method(\n+            spider.returns_item_cb_kwargs_error_missing_argument, self.results\n+        )\n         request.callback(response, **request.cb_kwargs)\n         self.should_error()\n \n@@ -343,10 +362,12 @@ class ContractsManagerTest(unittest.TestCase):\n         self.should_fail()\n \n         # scrapes_multiple_missing_fields\n-        request = self.conman.from_method(spider.scrapes_multiple_missing_fields, self.results)\n+        request = self.conman.from_method(\n+            spider.scrapes_multiple_missing_fields, self.results\n+        )\n         request.callback(response)\n         self.should_fail()\n-        message = 'ContractFail: Missing fields: name, url'\n+        message = \"ContractFail: Missing fields: name, url\"\n         assert message in self.results.failures[-1][-1]\n \n     def test_custom_contracts(self):\n@@ -361,7 +382,7 @@ class ContractsManagerTest(unittest.TestCase):\n         response = ResponseMock()\n \n         try:\n-            raise HttpError(response, 'Ignoring non-200 response')\n+            raise HttpError(response, \"Ignoring non-200 response\")\n         except HttpError:\n             failure_mock = failure.Failure()\n \n@@ -373,9 +394,8 @@ class ContractsManagerTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_same_url(self):\n-\n         class TestSameUrlSpider(Spider):\n-            name = 'test_same_url'\n+            name = \"test_same_url\"\n \n             def __init__(self, *args, **kwargs):\n                 super().__init__(*args, **kwargs)\n@@ -406,7 +426,7 @@ class ContractsManagerTest(unittest.TestCase):\n     def test_form_contract(self):\n         spider = TestSpider()\n         request = self.conman.from_method(spider.custom_form, self.results)\n-        self.assertEqual(request.method, 'POST')\n+        self.assertEqual(request.method, \"POST\")\n         self.assertIsInstance(request, FormRequest)\n \n     def test_inherited_contracts(self):\n\n@@ -4,7 +4,8 @@ from scrapy.core.downloader import Slot\n \n \n class SlotTest(unittest.TestCase):\n-\n     def test_repr(self):\n         slot = Slot(concurrency=8, delay=0.1, randomize_delay=True)\n-        self.assertEqual(repr(slot), 'Slot(concurrency=8, delay=0.10, randomize_delay=True)')\n+        self.assertEqual(\n+            repr(slot), \"Slot(concurrency=8, delay=0.10, randomize_delay=True)\"\n+        )\n\n@@ -53,7 +53,6 @@ from tests.spiders import (\n \n \n class CrawlTestCase(TestCase):\n-\n     def setUp(self):\n         self.mockserver = MockServer()\n         self.mockserver.__enter__()\n@@ -82,17 +81,17 @@ class CrawlTestCase(TestCase):\n             mockserver=self.mockserver,\n             total=total,\n         )\n-        tolerance = (1 - (0.6 if randomize else 0.2))\n+        tolerance = 1 - (0.6 if randomize else 0.2)\n \n-        settings = {\"DOWNLOAD_DELAY\": delay,\n-                    'RANDOMIZE_DOWNLOAD_DELAY': randomize}\n+        settings = {\"DOWNLOAD_DELAY\": delay, \"RANDOMIZE_DOWNLOAD_DELAY\": randomize}\n         crawler = get_crawler(FollowAllSpider, settings)\n         yield crawler.crawl(**crawl_kwargs)\n         times = crawler.spider.times\n         total_time = times[-1] - times[0]\n         average = total_time / (len(times) - 1)\n-        self.assertTrue(average > delay * tolerance,\n-                        f\"download delay too small: {average}\")\n+        self.assertTrue(\n+            average > delay * tolerance, f\"download delay too small: {average}\"\n+        )\n \n         # Ensure that the same test parameters would cause a failure if no\n         # download delay is set. Otherwise, it means we are using a combination\n@@ -104,8 +103,9 @@ class CrawlTestCase(TestCase):\n         times = crawler.spider.times\n         total_time = times[-1] - times[0]\n         average = total_time / (len(times) - 1)\n-        self.assertFalse(average > delay / tolerance,\n-                         \"test total or delay values are too small\")\n+        self.assertFalse(\n+            average > delay / tolerance, \"test total or delay values are too small\"\n+        )\n \n     @defer.inlineCallbacks\n     def test_timeout_success(self):\n@@ -134,14 +134,18 @@ class CrawlTestCase(TestCase):\n     def test_retry_503(self):\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=503\"), mockserver=self.mockserver)\n+            yield crawler.crawl(\n+                self.mockserver.url(\"/status?n=503\"), mockserver=self.mockserver\n+            )\n         self._assert_retried(log)\n \n     @defer.inlineCallbacks\n     def test_retry_conn_failed(self):\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(\"http://localhost:65432/status?n=503\", mockserver=self.mockserver)\n+            yield crawler.crawl(\n+                \"http://localhost:65432/status?n=503\", mockserver=self.mockserver\n+            )\n         self._assert_retried(log)\n \n     @defer.inlineCallbacks\n@@ -151,12 +155,14 @@ class CrawlTestCase(TestCase):\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n             # try to fetch the homepage of a non-existent domain\n-            yield crawler.crawl(\"http://dns.resolution.invalid./\", mockserver=self.mockserver)\n+            yield crawler.crawl(\n+                \"http://dns.resolution.invalid./\", mockserver=self.mockserver\n+            )\n         self._assert_retried(log)\n \n     @defer.inlineCallbacks\n     def test_start_requests_bug_before_yield(self):\n-        with LogCapture('scrapy', level=logging.ERROR) as log:\n+        with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n             crawler = get_crawler(BrokenStartRequestsSpider)\n             yield crawler.crawl(fail_before_yield=1, mockserver=self.mockserver)\n \n@@ -167,7 +173,7 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_start_requests_bug_yielding(self):\n-        with LogCapture('scrapy', level=logging.ERROR) as log:\n+        with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n             crawler = get_crawler(BrokenStartRequestsSpider)\n             yield crawler.crawl(fail_yielding=1, mockserver=self.mockserver)\n \n@@ -183,16 +189,24 @@ class CrawlTestCase(TestCase):\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertTrue(\n             crawler.spider.seedsseen.index(None) < crawler.spider.seedsseen.index(99),\n-            crawler.spider.seedsseen)\n+            crawler.spider.seedsseen,\n+        )\n \n     @defer.inlineCallbacks\n     def test_start_requests_dupes(self):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n         crawler = get_crawler(DuplicateStartRequestsSpider, settings)\n-        yield crawler.crawl(dont_filter=True, distinct_urls=2, dupe_factor=3, mockserver=self.mockserver)\n+        yield crawler.crawl(\n+            dont_filter=True, distinct_urls=2, dupe_factor=3, mockserver=self.mockserver\n+        )\n         self.assertEqual(crawler.spider.visited, 6)\n \n-        yield crawler.crawl(dont_filter=False, distinct_urls=3, dupe_factor=4, mockserver=self.mockserver)\n+        yield crawler.crawl(\n+            dont_filter=False,\n+            distinct_urls=3,\n+            dupe_factor=4,\n+            mockserver=self.mockserver,\n+        )\n         self.assertEqual(crawler.spider.visited, 3)\n \n     @defer.inlineCallbacks\n@@ -200,7 +214,10 @@ class CrawlTestCase(TestCase):\n         # Completeness of responses without Content-Length or Transfer-Encoding\n         # can not be determined, we treat them as valid but flagged as \"partial\"\n         from urllib.parse import urlencode\n-        query = urlencode({'raw': '''\\\n+\n+        query = urlencode(\n+            {\n+                \"raw\": \"\"\"\\\n HTTP/1.1 200 OK\n Server: Apache-Coyote/1.1\n X-Powered-By: Servlet 2.4; JBoss-4.2.3.GA (build: SVNTag=JBoss_4_2_3_GA date=200807181417)/JBossWeb-2.0\n@@ -216,10 +233,14 @@ Connection: close\n \n foo body\n with multiples lines\n-'''})\n+\"\"\"\n+            }\n+        )\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(f\"/raw?{query}\"), mockserver=self.mockserver)\n+            yield crawler.crawl(\n+                self.mockserver.url(f\"/raw?{query}\"), mockserver=self.mockserver\n+            )\n         self.assertEqual(str(log).count(\"Got response 200\"), 1)\n \n     @defer.inlineCallbacks\n@@ -227,7 +248,9 @@ with multiples lines\n         # connection lost after receiving data\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(\"/drop?abort=0\"), mockserver=self.mockserver)\n+            yield crawler.crawl(\n+                self.mockserver.url(\"/drop?abort=0\"), mockserver=self.mockserver\n+            )\n         self._assert_retried(log)\n \n     @defer.inlineCallbacks\n@@ -235,7 +258,9 @@ with multiples lines\n         # connection lost before receiving data\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(\"/drop?abort=1\"), mockserver=self.mockserver)\n+            yield crawler.crawl(\n+                self.mockserver.url(\"/drop?abort=1\"), mockserver=self.mockserver\n+            )\n         self._assert_retried(log)\n \n     def _assert_retried(self, log):\n@@ -245,56 +270,62 @@ with multiples lines\n     @defer.inlineCallbacks\n     def test_referer_header(self):\n         \"\"\"Referer header is set by RefererMiddleware unless it is already set\"\"\"\n-        req0 = Request(self.mockserver.url('/echo?headers=1&body=0'), dont_filter=1)\n+        req0 = Request(self.mockserver.url(\"/echo?headers=1&body=0\"), dont_filter=1)\n         req1 = req0.replace()\n-        req2 = req0.replace(headers={'Referer': None})\n-        req3 = req0.replace(headers={'Referer': 'http://example.com'})\n-        req0.meta['next'] = req1\n-        req1.meta['next'] = req2\n-        req2.meta['next'] = req3\n+        req2 = req0.replace(headers={\"Referer\": None})\n+        req3 = req0.replace(headers={\"Referer\": \"http://example.com\"})\n+        req0.meta[\"next\"] = req1\n+        req1.meta[\"next\"] = req2\n+        req2.meta[\"next\"] = req3\n         crawler = get_crawler(SingleRequestSpider)\n         yield crawler.crawl(seed=req0, mockserver=self.mockserver)\n         # basic asserts in case of weird communication errors\n-        self.assertIn('responses', crawler.spider.meta)\n-        self.assertNotIn('failures', crawler.spider.meta)\n+        self.assertIn(\"responses\", crawler.spider.meta)\n+        self.assertNotIn(\"failures\", crawler.spider.meta)\n         # start requests doesn't set Referer header\n-        echo0 = json.loads(to_unicode(crawler.spider.meta['responses'][2].body))\n-        self.assertNotIn('Referer', echo0['headers'])\n+        echo0 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][2].body))\n+        self.assertNotIn(\"Referer\", echo0[\"headers\"])\n         # following request sets Referer to start request url\n-        echo1 = json.loads(to_unicode(crawler.spider.meta['responses'][1].body))\n-        self.assertEqual(echo1['headers'].get('Referer'), [req0.url])\n+        echo1 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][1].body))\n+        self.assertEqual(echo1[\"headers\"].get(\"Referer\"), [req0.url])\n         # next request avoids Referer header\n-        echo2 = json.loads(to_unicode(crawler.spider.meta['responses'][2].body))\n-        self.assertNotIn('Referer', echo2['headers'])\n+        echo2 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][2].body))\n+        self.assertNotIn(\"Referer\", echo2[\"headers\"])\n         # last request explicitly sets a Referer header\n-        echo3 = json.loads(to_unicode(crawler.spider.meta['responses'][3].body))\n-        self.assertEqual(echo3['headers'].get('Referer'), ['http://example.com'])\n+        echo3 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][3].body))\n+        self.assertEqual(echo3[\"headers\"].get(\"Referer\"), [\"http://example.com\"])\n \n     @defer.inlineCallbacks\n     def test_engine_status(self):\n         from scrapy.utils.engine import get_engine_status\n+\n         est = []\n \n         def cb(response):\n             est.append(get_engine_status(crawler.engine))\n \n         crawler = get_crawler(SingleRequestSpider)\n-        yield crawler.crawl(seed=self.mockserver.url('/'), callback_func=cb, mockserver=self.mockserver)\n+        yield crawler.crawl(\n+            seed=self.mockserver.url(\"/\"), callback_func=cb, mockserver=self.mockserver\n+        )\n         self.assertEqual(len(est), 1, est)\n         s = dict(est[0])\n-        self.assertEqual(s['engine.spider.name'], crawler.spider.name)\n-        self.assertEqual(s['len(engine.scraper.slot.active)'], 1)\n+        self.assertEqual(s[\"engine.spider.name\"], crawler.spider.name)\n+        self.assertEqual(s[\"len(engine.scraper.slot.active)\"], 1)\n \n     @defer.inlineCallbacks\n     def test_format_engine_status(self):\n         from scrapy.utils.engine import format_engine_status\n+\n         est = []\n \n         def cb(response):\n             est.append(format_engine_status(crawler.engine))\n \n         crawler = get_crawler(SingleRequestSpider)\n-        yield crawler.crawl(seed=self.mockserver.url('/'), callback_func=cb, mockserver=self.mockserver)\n+        yield crawler.crawl(\n+            seed=self.mockserver.url(\"/\"), callback_func=cb, mockserver=self.mockserver\n+        )\n         self.assertEqual(len(est), 1, est)\n         est = est[0].split(\"\\n\")[2:-2]  # remove header & footer\n         # convert to dict\n@@ -304,8 +335,8 @@ with multiples lines\n         it = iter(est)\n         s = dict(zip(it, it))\n \n-        self.assertEqual(s['engine.spider.name'], crawler.spider.name)\n-        self.assertEqual(s['len(engine.scraper.slot.active)'], '1')\n+        self.assertEqual(s[\"engine.spider.name\"], crawler.spider.name)\n+        self.assertEqual(s[\"len(engine.scraper.slot.active)\"], \"1\")\n \n     @defer.inlineCallbacks\n     def test_graceful_crawl_error_handling(self):\n@@ -337,8 +368,11 @@ with multiples lines\n         }\n         crawler = get_crawler(SimpleSpider, settings)\n         yield self.assertFailure(\n-            crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver),\n-            ZeroDivisionError)\n+            crawler.crawl(\n+                self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n+            ),\n+            ZeroDivisionError,\n+        )\n         self.assertFalse(crawler.crawling)\n \n     @defer.inlineCallbacks\n@@ -346,14 +380,26 @@ with multiples lines\n         crawler = get_crawler(SimpleSpider)\n         runner = CrawlerRunner()\n         with LogCapture() as log:\n-            yield runner.crawl(crawler, self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+            yield runner.crawl(\n+                crawler,\n+                self.mockserver.url(\"/status?n=200\"),\n+                mockserver=self.mockserver,\n+            )\n         self.assertIn(\"Got response 200\", str(log))\n \n     @defer.inlineCallbacks\n     def test_crawl_multiple(self):\n-        runner = CrawlerRunner({'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'})\n-        runner.crawl(SimpleSpider, self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n-        runner.crawl(SimpleSpider, self.mockserver.url(\"/status?n=503\"), mockserver=self.mockserver)\n+        runner = CrawlerRunner({\"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\"})\n+        runner.crawl(\n+            SimpleSpider,\n+            self.mockserver.url(\"/status?n=200\"),\n+            mockserver=self.mockserver,\n+        )\n+        runner.crawl(\n+            SimpleSpider,\n+            self.mockserver.url(\"/status?n=503\"),\n+            mockserver=self.mockserver,\n+        )\n \n         with LogCapture() as log:\n             yield runner.join()\n@@ -363,7 +409,6 @@ with multiples lines\n \n \n class CrawlSpiderTestCase(TestCase):\n-\n     def setUp(self):\n         self.mockserver = MockServer()\n         self.mockserver.__enter__()\n@@ -381,7 +426,9 @@ class CrawlSpiderTestCase(TestCase):\n         crawler = get_crawler(spider_cls)\n         crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n         with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+            yield crawler.crawl(\n+                self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n+            )\n         return log, items, crawler.stats\n \n     @defer.inlineCallbacks\n@@ -441,17 +488,24 @@ class CrawlSpiderTestCase(TestCase):\n     def test_async_def_parse(self):\n         crawler = get_crawler(AsyncDefSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+            yield crawler.crawl(\n+                self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n+            )\n         self.assertIn(\"Got response 200\", str(log))\n \n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_async_def_asyncio_parse(self):\n-        crawler = get_crawler(AsyncDefAsyncioSpider, {\n+        crawler = get_crawler(\n+            AsyncDefAsyncioSpider,\n+            {\n                 \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n-        })\n+            },\n+        )\n         with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+            yield crawler.crawl(\n+                self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n+            )\n         self.assertIn(\"Got response 200\", str(log))\n \n     @mark.only_asyncio()\n@@ -459,8 +513,8 @@ class CrawlSpiderTestCase(TestCase):\n     def test_async_def_asyncio_parse_items_list(self):\n         log, items, _ = yield self._run_spider(AsyncDefAsyncioReturnSpider)\n         self.assertIn(\"Got response 200\", str(log))\n-        self.assertIn({'id': 1}, items)\n-        self.assertIn({'id': 2}, items)\n+        self.assertIn({\"id\": 1}, items)\n+        self.assertIn({\"id\": 2}, items)\n \n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n@@ -473,7 +527,9 @@ class CrawlSpiderTestCase(TestCase):\n         crawler = get_crawler(AsyncDefAsyncioReturnSingleElementSpider)\n         crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n         with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+            yield crawler.crawl(\n+                self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n+            )\n         self.assertIn(\"Got response 200\", str(log))\n         self.assertIn({\"foo\": 42}, items)\n \n@@ -482,7 +538,7 @@ class CrawlSpiderTestCase(TestCase):\n     def test_async_def_asyncgen_parse(self):\n         log, _, stats = yield self._run_spider(AsyncDefAsyncioGenSpider)\n         self.assertIn(\"Got response 200\", str(log))\n-        itemcount = stats.get_value('item_scraped_count')\n+        itemcount = stats.get_value(\"item_scraped_count\")\n         self.assertEqual(itemcount, 1)\n \n     @mark.only_asyncio()\n@@ -490,10 +546,10 @@ class CrawlSpiderTestCase(TestCase):\n     def test_async_def_asyncgen_parse_loop(self):\n         log, items, stats = yield self._run_spider(AsyncDefAsyncioGenLoopSpider)\n         self.assertIn(\"Got response 200\", str(log))\n-        itemcount = stats.get_value('item_scraped_count')\n+        itemcount = stats.get_value(\"item_scraped_count\")\n         self.assertEqual(itemcount, 10)\n         for i in range(10):\n-            self.assertIn({'foo': i}, items)\n+            self.assertIn({\"foo\": i}, items)\n \n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n@@ -502,22 +558,22 @@ class CrawlSpiderTestCase(TestCase):\n         log = str(log)\n         self.assertIn(\"Spider error processing\", log)\n         self.assertIn(\"ValueError\", log)\n-        itemcount = stats.get_value('item_scraped_count')\n+        itemcount = stats.get_value(\"item_scraped_count\")\n         self.assertEqual(itemcount, 7)\n         for i in range(7):\n-            self.assertIn({'foo': i}, items)\n+            self.assertIn({\"foo\": i}, items)\n \n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse_complex(self):\n         _, items, stats = yield self._run_spider(AsyncDefAsyncioGenComplexSpider)\n-        itemcount = stats.get_value('item_scraped_count')\n+        itemcount = stats.get_value(\"item_scraped_count\")\n         self.assertEqual(itemcount, 156)\n         # some random items\n         for i in [1, 4, 21, 22, 207, 311]:\n-            self.assertIn({'index': i}, items)\n+            self.assertIn({\"index\": i}, items)\n         for i in [10, 30, 122]:\n-            self.assertIn({'index2': i}, items)\n+            self.assertIn({\"index2\": i}, items)\n \n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n@@ -530,32 +586,32 @@ class CrawlSpiderTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_async_def_deferred_direct(self):\n         _, items, _ = yield self._run_spider(AsyncDefDeferredDirectSpider)\n-        self.assertEqual(items, [{'code': 200}])\n+        self.assertEqual(items, [{\"code\": 200}])\n \n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_async_def_deferred_wrapped(self):\n         log, items, _ = yield self._run_spider(AsyncDefDeferredWrappedSpider)\n-        self.assertEqual(items, [{'code': 200}])\n+        self.assertEqual(items, [{\"code\": 200}])\n \n     @defer.inlineCallbacks\n     def test_async_def_deferred_maybe_wrapped(self):\n         _, items, _ = yield self._run_spider(AsyncDefDeferredMaybeWrappedSpider)\n-        self.assertEqual(items, [{'code': 200}])\n+        self.assertEqual(items, [{\"code\": 200}])\n \n     @defer.inlineCallbacks\n     def test_response_ssl_certificate_none(self):\n         crawler = get_crawler(SingleRequestSpider)\n         url = self.mockserver.url(\"/echo?body=test\", is_secure=False)\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n-        self.assertIsNone(crawler.spider.meta['responses'][0].certificate)\n+        self.assertIsNone(crawler.spider.meta[\"responses\"][0].certificate)\n \n     @defer.inlineCallbacks\n     def test_response_ssl_certificate(self):\n         crawler = get_crawler(SingleRequestSpider)\n         url = self.mockserver.url(\"/echo?body=test\", is_secure=True)\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n-        cert = crawler.spider.meta['responses'][0].certificate\n+        cert = crawler.spider.meta[\"responses\"][0].certificate\n         self.assertIsInstance(cert, Certificate)\n         self.assertEqual(cert.getSubject().commonName, b\"localhost\")\n         self.assertEqual(cert.getIssuer().commonName, b\"localhost\")\n@@ -566,7 +622,7 @@ class CrawlSpiderTestCase(TestCase):\n         crawler = get_crawler(SingleRequestSpider)\n         url = self.mockserver.url(\"/status?n=200\", is_secure=True)\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n-        cert = crawler.spider.meta['responses'][0].certificate\n+        cert = crawler.spider.meta[\"responses\"][0].certificate\n         self.assertIsInstance(cert, Certificate)\n         self.assertEqual(cert.getSubject().commonName, b\"localhost\")\n         self.assertEqual(cert.getIssuer().commonName, b\"localhost\")\n@@ -574,18 +630,18 @@ class CrawlSpiderTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_dns_server_ip_address_none(self):\n         crawler = get_crawler(SingleRequestSpider)\n-        url = self.mockserver.url('/status?n=200')\n+        url = self.mockserver.url(\"/status?n=200\")\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n-        ip_address = crawler.spider.meta['responses'][0].ip_address\n+        ip_address = crawler.spider.meta[\"responses\"][0].ip_address\n         self.assertIsNone(ip_address)\n \n     @defer.inlineCallbacks\n     def test_dns_server_ip_address(self):\n         crawler = get_crawler(SingleRequestSpider)\n-        url = self.mockserver.url('/echo?body=test')\n-        expected_netloc, _ = urlparse(url).netloc.split(':')\n+        url = self.mockserver.url(\"/echo?body=test\")\n+        expected_netloc, _ = urlparse(url).netloc.split(\":\")\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n-        ip_address = crawler.spider.meta['responses'][0].ip_address\n+        ip_address = crawler.spider.meta[\"responses\"][0].ip_address\n         self.assertIsInstance(ip_address, IPv4Address)\n         self.assertEqual(str(ip_address), gethostbyname(expected_netloc))\n \n@@ -595,8 +651,14 @@ class CrawlSpiderTestCase(TestCase):\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertIsNone(crawler.spider.meta.get(\"failure\"))\n         self.assertIsInstance(crawler.spider.meta[\"response\"], Response)\n-        self.assertEqual(crawler.spider.meta[\"response\"].body, crawler.spider.meta.get(\"bytes_received\"))\n-        self.assertLess(len(crawler.spider.meta[\"response\"].body), crawler.spider.full_response_length)\n+        self.assertEqual(\n+            crawler.spider.meta[\"response\"].body,\n+            crawler.spider.meta.get(\"bytes_received\"),\n+        )\n+        self.assertLess(\n+            len(crawler.spider.meta[\"response\"].body),\n+            crawler.spider.full_response_length,\n+        )\n \n     @defer.inlineCallbacks\n     def test_bytes_received_stop_download_errback(self):\n@@ -608,10 +670,12 @@ class CrawlSpiderTestCase(TestCase):\n         self.assertIsInstance(crawler.spider.meta[\"failure\"].value.response, Response)\n         self.assertEqual(\n             crawler.spider.meta[\"failure\"].value.response.body,\n-            crawler.spider.meta.get(\"bytes_received\"))\n+            crawler.spider.meta.get(\"bytes_received\"),\n+        )\n         self.assertLess(\n             len(crawler.spider.meta[\"failure\"].value.response.body),\n-            crawler.spider.full_response_length)\n+            crawler.spider.full_response_length,\n+        )\n \n     @defer.inlineCallbacks\n     def test_headers_received_stop_download_callback(self):\n@@ -619,7 +683,10 @@ class CrawlSpiderTestCase(TestCase):\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertIsNone(crawler.spider.meta.get(\"failure\"))\n         self.assertIsInstance(crawler.spider.meta[\"response\"], Response)\n-        self.assertEqual(crawler.spider.meta[\"response\"].headers, crawler.spider.meta.get(\"headers_received\"))\n+        self.assertEqual(\n+            crawler.spider.meta[\"response\"].headers,\n+            crawler.spider.meta.get(\"headers_received\"),\n+        )\n \n     @defer.inlineCallbacks\n     def test_headers_received_stop_download_errback(self):\n@@ -631,4 +698,5 @@ class CrawlSpiderTestCase(TestCase):\n         self.assertIsInstance(crawler.spider.meta[\"failure\"].value.response, Response)\n         self.assertEqual(\n             crawler.spider.meta[\"failure\"].value.response.headers,\n-            crawler.spider.meta.get(\"headers_received\"))\n+            crawler.spider.meta.get(\"headers_received\"),\n+        )\n\n@@ -31,44 +31,42 @@ from tests.mockserver import MockServer\n \n \n class BaseCrawlerTest(unittest.TestCase):\n-\n     def assertOptionIsDefault(self, settings, key):\n         self.assertIsInstance(settings, Settings)\n         self.assertEqual(settings[key], getattr(default_settings, key))\n \n \n class CrawlerTestCase(BaseCrawlerTest):\n-\n     def test_populate_spidercls_settings(self):\n-        spider_settings = {'TEST1': 'spider', 'TEST2': 'spider'}\n-        project_settings = {'TEST1': 'project', 'TEST3': 'project'}\n+        spider_settings = {\"TEST1\": \"spider\", \"TEST2\": \"spider\"}\n+        project_settings = {\"TEST1\": \"project\", \"TEST3\": \"project\"}\n \n         class CustomSettingsSpider(DefaultSpider):\n             custom_settings = spider_settings\n \n         settings = Settings()\n-        settings.setdict(project_settings, priority='project')\n+        settings.setdict(project_settings, priority=\"project\")\n         with warnings.catch_warnings():\n             warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n             crawler = Crawler(CustomSettingsSpider, settings)\n \n-        self.assertEqual(crawler.settings.get('TEST1'), 'spider')\n-        self.assertEqual(crawler.settings.get('TEST2'), 'spider')\n-        self.assertEqual(crawler.settings.get('TEST3'), 'project')\n+        self.assertEqual(crawler.settings.get(\"TEST1\"), \"spider\")\n+        self.assertEqual(crawler.settings.get(\"TEST2\"), \"spider\")\n+        self.assertEqual(crawler.settings.get(\"TEST3\"), \"project\")\n \n         self.assertFalse(settings.frozen)\n         self.assertTrue(crawler.settings.frozen)\n \n     def test_crawler_accepts_dict(self):\n-        crawler = get_crawler(DefaultSpider, {'foo': 'bar'})\n-        self.assertEqual(crawler.settings['foo'], 'bar')\n-        self.assertOptionIsDefault(crawler.settings, 'RETRY_ENABLED')\n+        crawler = get_crawler(DefaultSpider, {\"foo\": \"bar\"})\n+        self.assertEqual(crawler.settings[\"foo\"], \"bar\")\n+        self.assertOptionIsDefault(crawler.settings, \"RETRY_ENABLED\")\n \n     def test_crawler_accepts_None(self):\n         with warnings.catch_warnings():\n             warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n             crawler = Crawler(DefaultSpider)\n-        self.assertOptionIsDefault(crawler.settings, 'RETRY_ENABLED')\n+        self.assertOptionIsDefault(crawler.settings, \"RETRY_ENABLED\")\n \n     def test_crawler_rejects_spider_objects(self):\n         with raises(ValueError):\n@@ -78,10 +76,8 @@ class CrawlerTestCase(BaseCrawlerTest):\n class SpiderSettingsTestCase(unittest.TestCase):\n     def test_spider_custom_settings(self):\n         class MySpider(scrapy.Spider):\n-            name = 'spider'\n-            custom_settings = {\n-                'AUTOTHROTTLE_ENABLED': True\n-            }\n+            name = \"spider\"\n+            custom_settings = {\"AUTOTHROTTLE_ENABLED\": True}\n \n         crawler = get_crawler(MySpider)\n         enabled_exts = [e.__class__ for e in crawler.extensions.middlewares]\n@@ -95,73 +91,71 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n             logging.root.removeHandler(handler)\n \n         class MySpider(scrapy.Spider):\n-            name = 'spider'\n+            name = \"spider\"\n \n         get_crawler(MySpider)\n         assert get_scrapy_root_handler() is None\n \n     def test_spider_custom_settings_log_level(self):\n         log_file = Path(self.mktemp())\n-        log_file.write_text('previous message\\n', encoding='utf-8')\n+        log_file.write_text(\"previous message\\n\", encoding=\"utf-8\")\n \n         class MySpider(scrapy.Spider):\n-            name = 'spider'\n+            name = \"spider\"\n             custom_settings = {\n-                'LOG_LEVEL': 'INFO',\n-                'LOG_FILE': str(log_file),\n+                \"LOG_LEVEL\": \"INFO\",\n+                \"LOG_FILE\": str(log_file),\n                 # settings to avoid extra warnings\n-                'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n-                'TELNETCONSOLE_ENABLED': telnet.TWISTED_CONCH_AVAILABLE,\n+                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n+                \"TELNETCONSOLE_ENABLED\": telnet.TWISTED_CONCH_AVAILABLE,\n             }\n \n         configure_logging()\n         self.assertEqual(get_scrapy_root_handler().level, logging.DEBUG)\n         crawler = get_crawler(MySpider)\n         self.assertEqual(get_scrapy_root_handler().level, logging.INFO)\n-        info_count = crawler.stats.get_value('log_count/INFO')\n-        logging.debug('debug message')\n-        logging.info('info message')\n-        logging.warning('warning message')\n-        logging.error('error message')\n+        info_count = crawler.stats.get_value(\"log_count/INFO\")\n+        logging.debug(\"debug message\")\n+        logging.info(\"info message\")\n+        logging.warning(\"warning message\")\n+        logging.error(\"error message\")\n \n-        logged = log_file.read_text(encoding='utf-8')\n+        logged = log_file.read_text(encoding=\"utf-8\")\n \n-        self.assertIn('previous message', logged)\n-        self.assertNotIn('debug message', logged)\n-        self.assertIn('info message', logged)\n-        self.assertIn('warning message', logged)\n-        self.assertIn('error message', logged)\n-        self.assertEqual(crawler.stats.get_value('log_count/ERROR'), 1)\n-        self.assertEqual(crawler.stats.get_value('log_count/WARNING'), 1)\n-        self.assertEqual(\n-            crawler.stats.get_value('log_count/INFO') - info_count, 1)\n-        self.assertEqual(crawler.stats.get_value('log_count/DEBUG', 0), 0)\n+        self.assertIn(\"previous message\", logged)\n+        self.assertNotIn(\"debug message\", logged)\n+        self.assertIn(\"info message\", logged)\n+        self.assertIn(\"warning message\", logged)\n+        self.assertIn(\"error message\", logged)\n+        self.assertEqual(crawler.stats.get_value(\"log_count/ERROR\"), 1)\n+        self.assertEqual(crawler.stats.get_value(\"log_count/WARNING\"), 1)\n+        self.assertEqual(crawler.stats.get_value(\"log_count/INFO\") - info_count, 1)\n+        self.assertEqual(crawler.stats.get_value(\"log_count/DEBUG\", 0), 0)\n \n     def test_spider_custom_settings_log_append(self):\n         log_file = Path(self.mktemp())\n-        log_file.write_text('previous message\\n', encoding='utf-8')\n+        log_file.write_text(\"previous message\\n\", encoding=\"utf-8\")\n \n         class MySpider(scrapy.Spider):\n-            name = 'spider'\n+            name = \"spider\"\n             custom_settings = {\n-                'LOG_FILE': str(log_file),\n-                'LOG_FILE_APPEND': False,\n+                \"LOG_FILE\": str(log_file),\n+                \"LOG_FILE_APPEND\": False,\n                 # disable telnet if not available to avoid an extra warning\n-                'TELNETCONSOLE_ENABLED': telnet.TWISTED_CONCH_AVAILABLE,\n+                \"TELNETCONSOLE_ENABLED\": telnet.TWISTED_CONCH_AVAILABLE,\n             }\n \n         configure_logging()\n         get_crawler(MySpider)\n-        logging.debug('debug message')\n+        logging.debug(\"debug message\")\n \n-        logged = log_file.read_text(encoding='utf-8')\n+        logged = log_file.read_text(encoding=\"utf-8\")\n \n-        self.assertNotIn('previous message', logged)\n-        self.assertIn('debug message', logged)\n+        self.assertNotIn(\"previous message\", logged)\n+        self.assertIn(\"debug message\", logged)\n \n \n class SpiderLoaderWithWrongInterface:\n-\n     def unneeded_method(self):\n         pass\n \n@@ -171,11 +165,12 @@ class CustomSpiderLoader(SpiderLoader):\n \n \n class CrawlerRunnerTestCase(BaseCrawlerTest):\n-\n     def test_spider_manager_verify_interface(self):\n-        settings = Settings({\n-            'SPIDER_LOADER_CLASS': SpiderLoaderWithWrongInterface,\n-        })\n+        settings = Settings(\n+            {\n+                \"SPIDER_LOADER_CLASS\": SpiderLoaderWithWrongInterface,\n+            }\n+        )\n         with warnings.catch_warnings(record=True) as w:\n             self.assertRaises(AttributeError, CrawlerRunner, settings)\n             self.assertEqual(len(w), 1)\n@@ -183,13 +178,13 @@ class CrawlerRunnerTestCase(BaseCrawlerTest):\n             self.assertIn(\"scrapy.interfaces.ISpiderLoader\", str(w[0].message))\n \n     def test_crawler_runner_accepts_dict(self):\n-        runner = CrawlerRunner({'foo': 'bar'})\n-        self.assertEqual(runner.settings['foo'], 'bar')\n-        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')\n+        runner = CrawlerRunner({\"foo\": \"bar\"})\n+        self.assertEqual(runner.settings[\"foo\"], \"bar\")\n+        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n \n     def test_crawler_runner_accepts_None(self):\n         runner = CrawlerRunner()\n-        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')\n+        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n \n     def test_deprecated_attribute_spiders(self):\n         with warnings.catch_warnings(record=True) as w:\n@@ -198,41 +193,40 @@ class CrawlerRunnerTestCase(BaseCrawlerTest):\n             self.assertEqual(len(w), 1)\n             self.assertIn(\"CrawlerRunner.spiders\", str(w[0].message))\n             self.assertIn(\"CrawlerRunner.spider_loader\", str(w[0].message))\n-            sl_cls = load_object(runner.settings['SPIDER_LOADER_CLASS'])\n+            sl_cls = load_object(runner.settings[\"SPIDER_LOADER_CLASS\"])\n             self.assertIsInstance(spiders, sl_cls)\n \n \n class CrawlerProcessTest(BaseCrawlerTest):\n     def test_crawler_process_accepts_dict(self):\n-        runner = CrawlerProcess({'foo': 'bar'})\n-        self.assertEqual(runner.settings['foo'], 'bar')\n-        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')\n+        runner = CrawlerProcess({\"foo\": \"bar\"})\n+        self.assertEqual(runner.settings[\"foo\"], \"bar\")\n+        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n \n     def test_crawler_process_accepts_None(self):\n         runner = CrawlerProcess()\n-        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')\n+        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n \n \n class ExceptionSpider(scrapy.Spider):\n-    name = 'exception'\n+    name = \"exception\"\n \n     @classmethod\n     def from_crawler(cls, crawler, *args, **kwargs):\n-        raise ValueError('Exception in from_crawler method')\n+        raise ValueError(\"Exception in from_crawler method\")\n \n \n class NoRequestsSpider(scrapy.Spider):\n-    name = 'no_request'\n+    name = \"no_request\"\n \n     def start_requests(self):\n         return []\n \n \n-@mark.usefixtures('reactor_pytest')\n+@mark.usefixtures(\"reactor_pytest\")\n class CrawlerRunnerHasSpider(unittest.TestCase):\n-\n     def _runner(self):\n-        return CrawlerRunner({'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'})\n+        return CrawlerRunner({\"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\"})\n \n     @defer.inlineCallbacks\n     def test_crawler_runner_bootstrap_successful(self):\n@@ -256,7 +250,7 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n         except ValueError:\n             pass\n         else:\n-            self.fail('Exception should be raised from spider')\n+            self.fail(\"Exception should be raised from spider\")\n \n         self.assertEqual(runner.bootstrap_failed, True)\n \n@@ -269,7 +263,7 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n         except ValueError:\n             pass\n         else:\n-            self.fail('Exception should be raised from spider')\n+            self.fail(\"Exception should be raised from spider\")\n \n         yield runner.crawl(NoRequestsSpider)\n \n@@ -277,18 +271,22 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_crawler_runner_asyncio_enabled_true(self):\n-        if self.reactor_pytest == 'asyncio':\n-            CrawlerRunner(settings={\n+        if self.reactor_pytest == \"asyncio\":\n+            CrawlerRunner(\n+                settings={\n                     \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n                     \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n-            })\n+                }\n+            )\n         else:\n             msg = r\"The installed reactor \\(.*?\\) does not match the requested one \\(.*?\\)\"\n             with self.assertRaisesRegex(Exception, msg):\n-                runner = CrawlerRunner(settings={\n+                runner = CrawlerRunner(\n+                    settings={\n                         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n                         \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n-                })\n+                    }\n+                )\n                 yield runner.crawl(NoRequestsSpider)\n \n \n@@ -298,35 +296,42 @@ class ScriptRunnerMixin:\n     def run_script(self, script_name: str, *script_args):\n         script_path = self.script_dir / script_name\n         args = [sys.executable, str(script_path)] + list(script_args)\n-        p = subprocess.Popen(args, env=get_testenv(),\n-                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n+        p = subprocess.Popen(\n+            args, env=get_testenv(), stdout=subprocess.PIPE, stderr=subprocess.PIPE\n+        )\n         stdout, stderr = p.communicate()\n-        return stderr.decode('utf-8')\n+        return stderr.decode(\"utf-8\")\n \n \n class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n-    script_dir = Path(__file__).parent.resolve() / 'CrawlerProcess'\n+    script_dir = Path(__file__).parent.resolve() / \"CrawlerProcess\"\n \n     def test_simple(self):\n-        log = self.run_script('simple.py')\n-        self.assertIn('Spider closed (finished)', log)\n-        self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        log = self.run_script(\"simple.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertNotIn(\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        )\n \n     def test_multi(self):\n-        log = self.run_script('multi.py')\n-        self.assertIn('Spider closed (finished)', log)\n-        self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        log = self.run_script(\"multi.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertNotIn(\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        )\n         self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n \n     def test_reactor_default(self):\n-        log = self.run_script('reactor_default.py')\n-        self.assertIn('Spider closed (finished)', log)\n-        self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        log = self.run_script(\"reactor_default.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertNotIn(\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        )\n         self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n \n     def test_reactor_default_twisted_reactor_select(self):\n-        log = self.run_script('reactor_default_twisted_reactor_select.py')\n-        if platform.system() in ['Windows', 'Darwin']:\n+        log = self.run_script(\"reactor_default_twisted_reactor_select.py\")\n+        if platform.system() in [\"Windows\", \"Darwin\"]:\n             # The goal of this test function is to test that, when a reactor is\n             # installed (the default one here) and a different reactor is\n             # configured (select here), an error raises.\n@@ -337,9 +342,9 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n             # If that ever becomes the case on more platforms (i.e. if Linux\n             # also starts using the select reactor by default in a future\n             # version of Twisted), then we will need to rethink this test.\n-            self.assertIn('Spider closed (finished)', log)\n+            self.assertIn(\"Spider closed (finished)\", log)\n         else:\n-            self.assertNotIn('Spider closed (finished)', log)\n+            self.assertNotIn(\"Spider closed (finished)\", log)\n             self.assertIn(\n                 (\n                     \"does not match the requested one \"\n@@ -349,18 +354,18 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n             )\n \n     def test_reactor_select(self):\n-        log = self.run_script('reactor_select.py')\n-        self.assertIn('Spider closed (finished)', log)\n+        log = self.run_script(\"reactor_select.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n         self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n \n     def test_reactor_select_twisted_reactor_select(self):\n-        log = self.run_script('reactor_select_twisted_reactor_select.py')\n-        self.assertIn('Spider closed (finished)', log)\n+        log = self.run_script(\"reactor_select_twisted_reactor_select.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n         self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n \n     def test_reactor_select_subclass_twisted_reactor_select(self):\n-        log = self.run_script('reactor_select_subclass_twisted_reactor_select.py')\n-        self.assertNotIn('Spider closed (finished)', log)\n+        log = self.run_script(\"reactor_select_subclass_twisted_reactor_select.py\")\n+        self.assertNotIn(\"Spider closed (finished)\", log)\n         self.assertIn(\n             (\n                 \"does not match the requested one \"\n@@ -370,24 +375,34 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         )\n \n     def test_asyncio_enabled_no_reactor(self):\n-        log = self.run_script('asyncio_enabled_no_reactor.py')\n-        self.assertIn('Spider closed (finished)', log)\n-        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        log = self.run_script(\"asyncio_enabled_no_reactor.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertIn(\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        )\n \n     def test_asyncio_enabled_reactor(self):\n-        log = self.run_script('asyncio_enabled_reactor.py')\n-        self.assertIn('Spider closed (finished)', log)\n-        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        log = self.run_script(\"asyncio_enabled_reactor.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertIn(\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        )\n \n-    @mark.skipif(parse_version(w3lib_version) >= parse_version(\"2.0.0\"),\n-                 reason='w3lib 2.0.0 and later do not allow invalid domains.')\n+    @mark.skipif(\n+        parse_version(w3lib_version) >= parse_version(\"2.0.0\"),\n+        reason=\"w3lib 2.0.0 and later do not allow invalid domains.\",\n+    )\n     def test_ipv6_default_name_resolver(self):\n-        log = self.run_script('default_name_resolver.py')\n-        self.assertIn('Spider closed (finished)', log)\n-        self.assertIn(\"'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,\", log)\n+        log = self.run_script(\"default_name_resolver.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertIn(\n+            \"'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,\",\n+            log,\n+        )\n         self.assertIn(\n             \"twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ::1.\",\n-            log)\n+            log,\n+        )\n \n     def test_caching_hostname_resolver_ipv6(self):\n         log = self.run_script(\"caching_hostname_resolver_ipv6.py\")\n@@ -406,9 +421,13 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n     def test_twisted_reactor_select(self):\n         log = self.run_script(\"twisted_reactor_select.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\"Using reactor: twisted.internet.selectreactor.SelectReactor\", log)\n+        self.assertIn(\n+            \"Using reactor: twisted.internet.selectreactor.SelectReactor\", log\n+        )\n \n-    @mark.skipif(platform.system() == 'Windows', reason=\"PollReactor is not supported on Windows\")\n+    @mark.skipif(\n+        platform.system() == \"Windows\", reason=\"PollReactor is not supported on Windows\"\n+    )\n     def test_twisted_reactor_poll(self):\n         log = self.run_script(\"twisted_reactor_poll.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n@@ -417,54 +436,103 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n     def test_twisted_reactor_asyncio(self):\n         log = self.run_script(\"twisted_reactor_asyncio.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        self.assertIn(\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        )\n \n     def test_twisted_reactor_asyncio_custom_settings(self):\n         log = self.run_script(\"twisted_reactor_custom_settings.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        self.assertIn(\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        )\n \n     def test_twisted_reactor_asyncio_custom_settings_same(self):\n         log = self.run_script(\"twisted_reactor_custom_settings_same.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        self.assertIn(\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        )\n \n     def test_twisted_reactor_asyncio_custom_settings_conflict(self):\n         log = self.run_script(\"twisted_reactor_custom_settings_conflict.py\")\n-        self.assertIn(\"Using reactor: twisted.internet.selectreactor.SelectReactor\", log)\n-        self.assertIn(\"(twisted.internet.selectreactor.SelectReactor) does not match the requested one\", log)\n+        self.assertIn(\n+            \"Using reactor: twisted.internet.selectreactor.SelectReactor\", log\n+        )\n+        self.assertIn(\n+            \"(twisted.internet.selectreactor.SelectReactor) does not match the requested one\",\n+            log,\n+        )\n \n-    @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')\n-    @mark.skipif(platform.system() == 'Windows', reason='uvloop does not support Windows')\n-    @mark.skipif(twisted_version == Version('twisted', 21, 2, 0), reason='https://twistedmatrix.com/trac/ticket/10106')\n+    @mark.skipif(\n+        sys.implementation.name == \"pypy\",\n+        reason=\"uvloop does not support pypy properly\",\n+    )\n+    @mark.skipif(\n+        platform.system() == \"Windows\", reason=\"uvloop does not support Windows\"\n+    )\n+    @mark.skipif(\n+        twisted_version == Version(\"twisted\", 21, 2, 0),\n+        reason=\"https://twistedmatrix.com/trac/ticket/10106\",\n+    )\n     def test_custom_loop_asyncio(self):\n         log = self.run_script(\"asyncio_custom_loop.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        self.assertIn(\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        )\n         self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n \n-    @mark.skipif(sys.implementation.name == \"pypy\", reason=\"uvloop does not support pypy properly\")\n-    @mark.skipif(platform.system() == \"Windows\", reason=\"uvloop does not support Windows\")\n-    @mark.skipif(twisted_version == Version('twisted', 21, 2, 0), reason='https://twistedmatrix.com/trac/ticket/10106')\n+    @mark.skipif(\n+        sys.implementation.name == \"pypy\",\n+        reason=\"uvloop does not support pypy properly\",\n+    )\n+    @mark.skipif(\n+        platform.system() == \"Windows\", reason=\"uvloop does not support Windows\"\n+    )\n+    @mark.skipif(\n+        twisted_version == Version(\"twisted\", 21, 2, 0),\n+        reason=\"https://twistedmatrix.com/trac/ticket/10106\",\n+    )\n     def test_custom_loop_asyncio_deferred_signal(self):\n         log = self.run_script(\"asyncio_deferred_signal.py\", \"uvloop.Loop\")\n         self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        self.assertIn(\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        )\n         self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n         self.assertIn(\"async pipeline opened!\", log)\n \n-    @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')\n-    @mark.skipif(platform.system() == 'Windows', reason='uvloop does not support Windows')\n-    @mark.skipif(twisted_version == Version('twisted', 21, 2, 0), reason='https://twistedmatrix.com/trac/ticket/10106')\n+    @mark.skipif(\n+        sys.implementation.name == \"pypy\",\n+        reason=\"uvloop does not support pypy properly\",\n+    )\n+    @mark.skipif(\n+        platform.system() == \"Windows\", reason=\"uvloop does not support Windows\"\n+    )\n+    @mark.skipif(\n+        twisted_version == Version(\"twisted\", 21, 2, 0),\n+        reason=\"https://twistedmatrix.com/trac/ticket/10106\",\n+    )\n     def test_asyncio_enabled_reactor_same_loop(self):\n         log = self.run_script(\"asyncio_enabled_reactor_same_loop.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        self.assertIn(\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        )\n         self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n \n-    @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')\n-    @mark.skipif(platform.system() == 'Windows', reason='uvloop does not support Windows')\n-    @mark.skipif(twisted_version == Version('twisted', 21, 2, 0), reason='https://twistedmatrix.com/trac/ticket/10106')\n+    @mark.skipif(\n+        sys.implementation.name == \"pypy\",\n+        reason=\"uvloop does not support pypy properly\",\n+    )\n+    @mark.skipif(\n+        platform.system() == \"Windows\", reason=\"uvloop does not support Windows\"\n+    )\n+    @mark.skipif(\n+        twisted_version == Version(\"twisted\", 21, 2, 0),\n+        reason=\"https://twistedmatrix.com/trac/ticket/10106\",\n+    )\n     def test_asyncio_enabled_reactor_different_loop(self):\n         log = self.run_script(\"asyncio_enabled_reactor_different_loop.py\")\n         self.assertNotIn(\"Spider closed (finished)\", log)\n@@ -479,13 +547,15 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n     def test_default_loop_asyncio_deferred_signal(self):\n         log = self.run_script(\"asyncio_deferred_signal.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        self.assertIn(\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n+        )\n         self.assertNotIn(\"Using asyncio event loop: uvloop.Loop\", log)\n         self.assertIn(\"async pipeline opened!\", log)\n \n \n class CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):\n-    script_dir = Path(__file__).parent.resolve() / 'CrawlerRunner'\n+    script_dir = Path(__file__).parent.resolve() / \"CrawlerRunner\"\n \n     def test_response_ip_address(self):\n         log = self.run_script(\"ip_address.py\")\n\n@@ -9,15 +9,14 @@ from twisted.trial import unittest\n \n \n class ScrapyUtilsTest(unittest.TestCase):\n-\n     def test_required_openssl_version(self):\n         try:\n-            module = import_module('OpenSSL')\n+            module = import_module(\"OpenSSL\")\n         except ImportError:\n             raise unittest.SkipTest(\"OpenSSL is not available\")\n \n-        if hasattr(module, '__version__'):\n-            installed_version = [int(x) for x in module.__version__.split('.')[:2]]\n+        if hasattr(module, \"__version__\"):\n+            installed_version = [int(x) for x in module.__version__.split(\".\")[:2]]\n             assert installed_version >= [0, 6], \"OpenSSL >= 0.6 required\"\n \n     def test_pinned_twisted_version(self):\n@@ -27,20 +26,17 @@ class ScrapyUtilsTest(unittest.TestCase):\n \n         See https://github.com/scrapy/scrapy/pull/4814#issuecomment-706230011\n         \"\"\"\n-        if not os.environ.get('_SCRAPY_PINNED', None):\n-            self.skipTest('Not in a pinned environment')\n+        if not os.environ.get(\"_SCRAPY_PINNED\", None):\n+            self.skipTest(\"Not in a pinned environment\")\n \n-        tox_config_file_path = Path(__file__).parent / '..' / 'tox.ini'\n+        tox_config_file_path = Path(__file__).parent / \"..\" / \"tox.ini\"\n         config_parser = ConfigParser()\n         config_parser.read(tox_config_file_path)\n-        pattern = r'Twisted\\[http2\\]==([\\d.]+)'\n-        match = re.search(pattern, config_parser['pinned']['deps'])\n+        pattern = r\"Twisted\\[http2\\]==([\\d.]+)\"\n+        match = re.search(pattern, config_parser[\"pinned\"][\"deps\"])\n         pinned_twisted_version_string = match[1]\n \n-        self.assertEqual(\n-            twisted_version.short(),\n-            pinned_twisted_version_string\n-        )\n+        self.assertEqual(twisted_version.short(), pinned_twisted_version_string)\n \n \n if __name__ == \"__main__\":\n\n@@ -66,50 +66,48 @@ class OffDH:\n \n \n class LoadTestCase(unittest.TestCase):\n-\n     def test_enabled_handler(self):\n-        handlers = {'scheme': DummyDH}\n-        crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})\n+        handlers = {\"scheme\": DummyDH}\n+        crawler = get_crawler(settings_dict={\"DOWNLOAD_HANDLERS\": handlers})\n         dh = DownloadHandlers(crawler)\n-        self.assertIn('scheme', dh._schemes)\n-        self.assertIn('scheme', dh._handlers)\n-        self.assertNotIn('scheme', dh._notconfigured)\n+        self.assertIn(\"scheme\", dh._schemes)\n+        self.assertIn(\"scheme\", dh._handlers)\n+        self.assertNotIn(\"scheme\", dh._notconfigured)\n \n     def test_not_configured_handler(self):\n-        handlers = {'scheme': OffDH}\n-        crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})\n+        handlers = {\"scheme\": OffDH}\n+        crawler = get_crawler(settings_dict={\"DOWNLOAD_HANDLERS\": handlers})\n         dh = DownloadHandlers(crawler)\n-        self.assertIn('scheme', dh._schemes)\n-        self.assertNotIn('scheme', dh._handlers)\n-        self.assertIn('scheme', dh._notconfigured)\n+        self.assertIn(\"scheme\", dh._schemes)\n+        self.assertNotIn(\"scheme\", dh._handlers)\n+        self.assertIn(\"scheme\", dh._notconfigured)\n \n     def test_disabled_handler(self):\n-        handlers = {'scheme': None}\n-        crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})\n+        handlers = {\"scheme\": None}\n+        crawler = get_crawler(settings_dict={\"DOWNLOAD_HANDLERS\": handlers})\n         dh = DownloadHandlers(crawler)\n-        self.assertNotIn('scheme', dh._schemes)\n+        self.assertNotIn(\"scheme\", dh._schemes)\n         for scheme in handlers:  # force load handlers\n             dh._get_handler(scheme)\n-        self.assertNotIn('scheme', dh._handlers)\n-        self.assertIn('scheme', dh._notconfigured)\n+        self.assertNotIn(\"scheme\", dh._handlers)\n+        self.assertIn(\"scheme\", dh._notconfigured)\n \n     def test_lazy_handlers(self):\n-        handlers = {'scheme': DummyLazyDH}\n-        crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})\n+        handlers = {\"scheme\": DummyLazyDH}\n+        crawler = get_crawler(settings_dict={\"DOWNLOAD_HANDLERS\": handlers})\n         dh = DownloadHandlers(crawler)\n-        self.assertIn('scheme', dh._schemes)\n-        self.assertNotIn('scheme', dh._handlers)\n+        self.assertIn(\"scheme\", dh._schemes)\n+        self.assertNotIn(\"scheme\", dh._handlers)\n         for scheme in handlers:  # force load lazy handler\n             dh._get_handler(scheme)\n-        self.assertIn('scheme', dh._handlers)\n-        self.assertNotIn('scheme', dh._notconfigured)\n+        self.assertIn(\"scheme\", dh._handlers)\n+        self.assertNotIn(\"scheme\", dh._notconfigured)\n \n \n class FileTestCase(unittest.TestCase):\n-\n     def setUp(self):\n         # add a special char to check that they are handled correctly\n-        self.tmpname = Path(self.mktemp() + '^')\n+        self.tmpname = Path(self.mktemp() + \"^\")\n         Path(self.tmpname).write_text(\"0123456789\", encoding=\"utf-8\")\n         handler = create_instance(FileDownloadHandler, None, get_crawler())\n         self.download_request = handler.download_request\n@@ -121,16 +119,16 @@ class FileTestCase(unittest.TestCase):\n         def _test(response):\n             self.assertEqual(response.url, request.url)\n             self.assertEqual(response.status, 200)\n-            self.assertEqual(response.body, b'0123456789')\n+            self.assertEqual(response.body, b\"0123456789\")\n             self.assertEqual(response.protocol, None)\n \n         request = Request(path_to_file_uri(str(self.tmpname)))\n-        assert request.url.upper().endswith('%5E')\n-        return self.download_request(request, Spider('foo')).addCallback(_test)\n+        assert request.url.upper().endswith(\"%5E\")\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n \n     def test_non_existent(self):\n-        request = Request(f'file://{self.mktemp()}')\n-        d = self.download_request(request, Spider('foo'))\n+        request = Request(f\"file://{self.mktemp()}\")\n+        d = self.download_request(request, Spider(\"foo\"))\n         return self.assertFailure(d, IOError)\n \n \n@@ -145,7 +143,6 @@ class ContentLengthHeaderResource(resource.Resource):\n \n \n class ChunkedResource(resource.Resource):\n-\n     def render(self, request):\n         def response():\n             request.write(b\"chunked \")\n@@ -157,7 +154,6 @@ class ChunkedResource(resource.Resource):\n \n \n class BrokenChunkedResource(resource.Resource):\n-\n     def render(self, request):\n         def response():\n             request.write(b\"chunked \")\n@@ -171,7 +167,6 @@ class BrokenChunkedResource(resource.Resource):\n \n \n class BrokenDownloadResource(resource.Resource):\n-\n     def render(self, request):\n         def response():\n             request.setHeader(b\"Content-Length\", b\"20\")\n@@ -185,7 +180,7 @@ class BrokenDownloadResource(resource.Resource):\n def closeConnection(request):\n     # We have to force a disconnection for HTTP/1.1 clients. Otherwise\n     # client keeps the connection open waiting for more data.\n-    if hasattr(request.channel, 'loseConnection'):  # twisted >=16.3.0\n+    if hasattr(request.channel, \"loseConnection\"):  # twisted >=16.3.0\n         request.channel.loseConnection()\n     else:\n         request.channel.transport.loseConnection()\n@@ -215,12 +210,12 @@ class LargeChunkedFileResource(resource.Resource):\n \n \n class HttpTestCase(unittest.TestCase):\n-    scheme = 'http'\n+    scheme = \"http\"\n     download_handler_cls: Type = HTTPDownloadHandler\n \n     # only used for HTTPS tests\n-    keyfile = 'keys/localhost.key'\n-    certfile = 'keys/localhost.crt'\n+    keyfile = \"keys/localhost.key\"\n+    certfile = \"keys/localhost.crt\"\n \n     def setUp(self):\n         self.tmpname = Path(self.mktemp())\n@@ -242,23 +237,28 @@ class HttpTestCase(unittest.TestCase):\n         r.putChild(b\"echo\", Echo())\n         self.site = server.Site(r, timeout=None)\n         self.wrapper = WrappingFactory(self.site)\n-        self.host = 'localhost'\n-        if self.scheme == 'https':\n+        self.host = \"localhost\"\n+        if self.scheme == \"https\":\n             # Using WrappingFactory do not enable HTTP/2 failing all the\n             # tests with H2DownloadHandler\n             self.port = reactor.listenSSL(\n-                0, self.site, ssl_context_factory(self.keyfile, self.certfile),\n-                interface=self.host)\n+                0,\n+                self.site,\n+                ssl_context_factory(self.keyfile, self.certfile),\n+                interface=self.host,\n+            )\n         else:\n             self.port = reactor.listenTCP(0, self.wrapper, interface=self.host)\n         self.portno = self.port.getHost().port\n-        self.download_handler = create_instance(self.download_handler_cls, None, get_crawler())\n+        self.download_handler = create_instance(\n+            self.download_handler_cls, None, get_crawler()\n+        )\n         self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n     def tearDown(self):\n         yield self.port.stopListening()\n-        if hasattr(self.download_handler, 'close'):\n+        if hasattr(self.download_handler, \"close\"):\n             yield self.download_handler.close()\n         shutil.rmtree(self.tmpname)\n \n@@ -266,29 +266,29 @@ class HttpTestCase(unittest.TestCase):\n         return f\"{self.scheme}://{self.host}:{self.portno}/{path}\"\n \n     def test_download(self):\n-        request = Request(self.getURL('file'))\n-        d = self.download_request(request, Spider('foo'))\n+        request = Request(self.getURL(\"file\"))\n+        d = self.download_request(request, Spider(\"foo\"))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEqual, b\"0123456789\")\n         return d\n \n     def test_download_head(self):\n-        request = Request(self.getURL('file'), method='HEAD')\n-        d = self.download_request(request, Spider('foo'))\n+        request = Request(self.getURL(\"file\"), method=\"HEAD\")\n+        d = self.download_request(request, Spider(\"foo\"))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b'')\n+        d.addCallback(self.assertEqual, b\"\")\n         return d\n \n     def test_redirect_status(self):\n-        request = Request(self.getURL('redirect'))\n-        d = self.download_request(request, Spider('foo'))\n+        request = Request(self.getURL(\"redirect\"))\n+        d = self.download_request(request, Spider(\"foo\"))\n         d.addCallback(lambda r: r.status)\n         d.addCallback(self.assertEqual, 302)\n         return d\n \n     def test_redirect_status_head(self):\n-        request = Request(self.getURL('redirect'), method='HEAD')\n-        d = self.download_request(request, Spider('foo'))\n+        request = Request(self.getURL(\"redirect\"), method=\"HEAD\")\n+        d = self.download_request(request, Spider(\"foo\"))\n         d.addCallback(lambda r: r.status)\n         d.addCallback(self.assertEqual, 302)\n         return d\n@@ -302,9 +302,9 @@ class HttpTestCase(unittest.TestCase):\n             )\n \n         # client connects but no data is received\n-        spider = Spider('foo')\n-        meta = {'download_timeout': 0.5}\n-        request = Request(self.getURL('wait'), meta=meta)\n+        spider = Spider(\"foo\")\n+        meta = {\"download_timeout\": 0.5}\n+        request = Request(self.getURL(\"wait\"), meta=meta)\n         d = self.download_request(request, spider)\n         yield self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n \n@@ -316,34 +316,33 @@ class HttpTestCase(unittest.TestCase):\n                 \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n             )\n         # client connects, server send headers and some body bytes but hangs\n-        spider = Spider('foo')\n-        meta = {'download_timeout': 0.5}\n-        request = Request(self.getURL('hang-after-headers'), meta=meta)\n+        spider = Spider(\"foo\")\n+        meta = {\"download_timeout\": 0.5}\n+        request = Request(self.getURL(\"hang-after-headers\"), meta=meta)\n         d = self.download_request(request, spider)\n         yield self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n \n     def test_host_header_not_in_request_headers(self):\n         def _test(response):\n-            self.assertEqual(\n-                response.body, to_bytes(f'{self.host}:{self.portno}'))\n+            self.assertEqual(response.body, to_bytes(f\"{self.host}:{self.portno}\"))\n             self.assertEqual(request.headers, {})\n \n-        request = Request(self.getURL('host'))\n-        return self.download_request(request, Spider('foo')).addCallback(_test)\n+        request = Request(self.getURL(\"host\"))\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n \n     def test_host_header_seted_in_request_headers(self):\n-        host = self.host + ':' + str(self.portno)\n+        host = self.host + \":\" + str(self.portno)\n \n         def _test(response):\n             self.assertEqual(response.body, host.encode())\n-            self.assertEqual(request.headers.get('Host'), host.encode())\n+            self.assertEqual(request.headers.get(\"Host\"), host.encode())\n \n-        request = Request(self.getURL('host'), headers={'Host': host})\n-        return self.download_request(request, Spider('foo')).addCallback(_test)\n+        request = Request(self.getURL(\"host\"), headers={\"Host\": host})\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n \n-        d = self.download_request(request, Spider('foo'))\n+        d = self.download_request(request, Spider(\"foo\"))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b'localhost')\n+        d.addCallback(self.assertEqual, b\"localhost\")\n         return d\n \n     def test_content_length_zero_bodyless_post_request_headers(self):\n@@ -359,26 +358,27 @@ class HttpTestCase(unittest.TestCase):\n         \"\"\"\n \n         def _test(response):\n-            self.assertEqual(response.body, b'0')\n+            self.assertEqual(response.body, b\"0\")\n \n-        request = Request(self.getURL('contentlength'), method='POST')\n-        return self.download_request(request, Spider('foo')).addCallback(_test)\n+        request = Request(self.getURL(\"contentlength\"), method=\"POST\")\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n \n     def test_content_length_zero_bodyless_post_only_one(self):\n         def _test(response):\n             import json\n-            headers = Headers(json.loads(response.text)['headers'])\n-            contentlengths = headers.getlist('Content-Length')\n+\n+            headers = Headers(json.loads(response.text)[\"headers\"])\n+            contentlengths = headers.getlist(\"Content-Length\")\n             self.assertEqual(len(contentlengths), 1)\n             self.assertEqual(contentlengths, [b\"0\"])\n \n-        request = Request(self.getURL('echo'), method='POST')\n-        return self.download_request(request, Spider('foo')).addCallback(_test)\n+        request = Request(self.getURL(\"echo\"), method=\"POST\")\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n \n     def test_payload(self):\n-        body = b'1' * 100  # PayloadResource requires body length to be 100\n-        request = Request(self.getURL('payload'), method='POST', body=body)\n-        d = self.download_request(request, Spider('foo'))\n+        body = b\"1\" * 100  # PayloadResource requires body length to be 100\n+        request = Request(self.getURL(\"payload\"), method=\"POST\", body=body)\n+        d = self.download_request(request, Spider(\"foo\"))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEqual, body)\n         return d\n@@ -386,8 +386,8 @@ class HttpTestCase(unittest.TestCase):\n     def test_response_header_content_length(self):\n         request = Request(self.getURL(\"file\"), method=b\"GET\")\n         d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.headers[b'content-length'])\n-        d.addCallback(self.assertEqual, b'159')\n+        d.addCallback(lambda r: r.headers[b\"content-length\"])\n+        d.addCallback(self.assertEqual, b\"159\")\n         return d\n \n     def _test_response_class(self, filename, body, response_class):\n@@ -395,14 +395,14 @@ class HttpTestCase(unittest.TestCase):\n             self.assertEqual(type(response), response_class)\n \n         request = Request(self.getURL(filename), body=body)\n-        return self.download_request(request, Spider('foo')).addCallback(_test)\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n \n     def test_response_class_from_url(self):\n-        return self._test_response_class('foo.html', b'', HtmlResponse)\n+        return self._test_response_class(\"foo.html\", b\"\", HtmlResponse)\n \n     def test_response_class_from_body(self):\n         return self._test_response_class(\n-            'foo',\n+            \"foo\",\n             b\"<!DOCTYPE html>\\n<title>.</title>\",\n             HtmlResponse,\n         )\n@@ -410,6 +410,7 @@ class HttpTestCase(unittest.TestCase):\n \n class Http10TestCase(HttpTestCase):\n     \"\"\"HTTP 1.0 test case\"\"\"\n+\n     download_handler_cls: Type = HTTP10DownloadHandler\n \n     def test_protocol(self):\n@@ -421,16 +422,17 @@ class Http10TestCase(HttpTestCase):\n \n \n class Https10TestCase(Http10TestCase):\n-    scheme = 'https'\n+    scheme = \"https\"\n \n \n class Http11TestCase(HttpTestCase):\n     \"\"\"HTTP 1.1 test case\"\"\"\n+\n     download_handler_cls: Type = HTTP11DownloadHandler\n \n     def test_download_without_maxsize_limit(self):\n-        request = Request(self.getURL('file'))\n-        d = self.download_request(request, Spider('foo'))\n+        request = Request(self.getURL(\"file\"))\n+        d = self.download_request(request, Spider(\"foo\"))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEqual, b\"0123456789\")\n         return d\n@@ -439,39 +441,39 @@ class Http11TestCase(HttpTestCase):\n         \"\"\"Tests choosing of correct response type\n         in case of Content-Type is empty but body contains text.\n         \"\"\"\n-        body = b'Some plain text\\ndata with tabs\\t and null bytes\\0'\n+        body = b\"Some plain text\\ndata with tabs\\t and null bytes\\0\"\n \n         def _test_type(response):\n             self.assertEqual(type(response), TextResponse)\n \n-        request = Request(self.getURL('nocontenttype'), body=body)\n-        d = self.download_request(request, Spider('foo'))\n+        request = Request(self.getURL(\"nocontenttype\"), body=body)\n+        d = self.download_request(request, Spider(\"foo\"))\n         d.addCallback(_test_type)\n         return d\n \n     @defer.inlineCallbacks\n     def test_download_with_maxsize(self):\n-        request = Request(self.getURL('file'))\n+        request = Request(self.getURL(\"file\"))\n \n         # 10 is minimal size for this request and the limit is only counted on\n         # response body. (regardless of headers)\n-        d = self.download_request(request, Spider('foo', download_maxsize=10))\n+        d = self.download_request(request, Spider(\"foo\", download_maxsize=10))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEqual, b\"0123456789\")\n         yield d\n \n-        d = self.download_request(request, Spider('foo', download_maxsize=9))\n+        d = self.download_request(request, Spider(\"foo\", download_maxsize=9))\n         yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n \n     @defer.inlineCallbacks\n     def test_download_with_maxsize_very_large_file(self):\n-        with mock.patch('scrapy.core.downloader.handlers.http11.logger') as logger:\n-            request = Request(self.getURL('largechunkedfile'))\n+        with mock.patch(\"scrapy.core.downloader.handlers.http11.logger\") as logger:\n+            request = Request(self.getURL(\"largechunkedfile\"))\n \n             def check(logger):\n                 logger.warning.assert_called_once_with(mock.ANY, mock.ANY)\n \n-            d = self.download_request(request, Spider('foo', download_maxsize=1500))\n+            d = self.download_request(request, Spider(\"foo\", download_maxsize=1500))\n             yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n \n             # As the error message is logged in the dataReceived callback, we\n@@ -479,39 +481,39 @@ class Http11TestCase(HttpTestCase):\n             # after closing the connection.\n             d = defer.Deferred()\n             d.addCallback(check)\n-            reactor.callLater(.1, d.callback, logger)\n+            reactor.callLater(0.1, d.callback, logger)\n             yield d\n \n     @defer.inlineCallbacks\n     def test_download_with_maxsize_per_req(self):\n-        meta = {'download_maxsize': 2}\n-        request = Request(self.getURL('file'), meta=meta)\n-        d = self.download_request(request, Spider('foo'))\n+        meta = {\"download_maxsize\": 2}\n+        request = Request(self.getURL(\"file\"), meta=meta)\n+        d = self.download_request(request, Spider(\"foo\"))\n         yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n \n     @defer.inlineCallbacks\n     def test_download_with_small_maxsize_per_spider(self):\n-        request = Request(self.getURL('file'))\n-        d = self.download_request(request, Spider('foo', download_maxsize=2))\n+        request = Request(self.getURL(\"file\"))\n+        d = self.download_request(request, Spider(\"foo\", download_maxsize=2))\n         yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n \n     def test_download_with_large_maxsize_per_spider(self):\n-        request = Request(self.getURL('file'))\n-        d = self.download_request(request, Spider('foo', download_maxsize=100))\n+        request = Request(self.getURL(\"file\"))\n+        d = self.download_request(request, Spider(\"foo\", download_maxsize=100))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEqual, b\"0123456789\")\n         return d\n \n     def test_download_chunked_content(self):\n-        request = Request(self.getURL('chunked'))\n-        d = self.download_request(request, Spider('foo'))\n+        request = Request(self.getURL(\"chunked\"))\n+        d = self.download_request(request, Spider(\"foo\"))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEqual, b\"chunked content\\n\")\n         return d\n \n-    def test_download_broken_content_cause_data_loss(self, url='broken'):\n+    def test_download_broken_content_cause_data_loss(self, url=\"broken\"):\n         request = Request(self.getURL(url))\n-        d = self.download_request(request, Spider('foo'))\n+        d = self.download_request(request, Spider(\"foo\"))\n \n         def checkDataLoss(failure):\n             if failure.check(ResponseFailed):\n@@ -524,29 +526,31 @@ class Http11TestCase(HttpTestCase):\n         return d\n \n     def test_download_broken_chunked_content_cause_data_loss(self):\n-        return self.test_download_broken_content_cause_data_loss('broken-chunked')\n+        return self.test_download_broken_content_cause_data_loss(\"broken-chunked\")\n \n-    def test_download_broken_content_allow_data_loss(self, url='broken'):\n-        request = Request(self.getURL(url), meta={'download_fail_on_dataloss': False})\n-        d = self.download_request(request, Spider('foo'))\n+    def test_download_broken_content_allow_data_loss(self, url=\"broken\"):\n+        request = Request(self.getURL(url), meta={\"download_fail_on_dataloss\": False})\n+        d = self.download_request(request, Spider(\"foo\"))\n         d.addCallback(lambda r: r.flags)\n-        d.addCallback(self.assertEqual, ['dataloss'])\n+        d.addCallback(self.assertEqual, [\"dataloss\"])\n         return d\n \n     def test_download_broken_chunked_content_allow_data_loss(self):\n-        return self.test_download_broken_content_allow_data_loss('broken-chunked')\n+        return self.test_download_broken_content_allow_data_loss(\"broken-chunked\")\n \n-    def test_download_broken_content_allow_data_loss_via_setting(self, url='broken'):\n-        crawler = get_crawler(settings_dict={'DOWNLOAD_FAIL_ON_DATALOSS': False})\n+    def test_download_broken_content_allow_data_loss_via_setting(self, url=\"broken\"):\n+        crawler = get_crawler(settings_dict={\"DOWNLOAD_FAIL_ON_DATALOSS\": False})\n         download_handler = create_instance(self.download_handler_cls, None, crawler)\n         request = Request(self.getURL(url))\n-        d = download_handler.download_request(request, Spider('foo'))\n+        d = download_handler.download_request(request, Spider(\"foo\"))\n         d.addCallback(lambda r: r.flags)\n-        d.addCallback(self.assertEqual, ['dataloss'])\n+        d.addCallback(self.assertEqual, [\"dataloss\"])\n         return d\n \n     def test_download_broken_chunked_content_allow_data_loss_via_setting(self):\n-        return self.test_download_broken_content_allow_data_loss_via_setting('broken-chunked')\n+        return self.test_download_broken_content_allow_data_loss_via_setting(\n+            \"broken-chunked\"\n+        )\n \n     def test_protocol(self):\n         request = Request(self.getURL(\"host\"), method=\"GET\")\n@@ -557,7 +561,7 @@ class Http11TestCase(HttpTestCase):\n \n \n class Https11TestCase(Http11TestCase):\n-    scheme = 'https'\n+    scheme = \"https\"\n \n     tls_log_message = (\n         'SSL connection certificate: issuer \"/C=IE/O=Scrapy/CN=localhost\", '\n@@ -566,30 +570,34 @@ class Https11TestCase(Http11TestCase):\n \n     @defer.inlineCallbacks\n     def test_tls_logging(self):\n-        crawler = get_crawler(settings_dict={'DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING': True})\n+        crawler = get_crawler(\n+            settings_dict={\"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\": True}\n+        )\n         download_handler = create_instance(self.download_handler_cls, None, crawler)\n         try:\n             with LogCapture() as log_capture:\n-                request = Request(self.getURL('file'))\n-                d = download_handler.download_request(request, Spider('foo'))\n+                request = Request(self.getURL(\"file\"))\n+                d = download_handler.download_request(request, Spider(\"foo\"))\n                 d.addCallback(lambda r: r.body)\n                 d.addCallback(self.assertEqual, b\"0123456789\")\n                 yield d\n-                log_capture.check_present(('scrapy.core.downloader.tls', 'DEBUG', self.tls_log_message))\n+                log_capture.check_present(\n+                    (\"scrapy.core.downloader.tls\", \"DEBUG\", self.tls_log_message)\n+                )\n         finally:\n             yield download_handler.close()\n \n \n class Https11WrongHostnameTestCase(Http11TestCase):\n-    scheme = 'https'\n+    scheme = \"https\"\n \n     # above tests use a server certificate for \"localhost\",\n     # client connection to \"localhost\" too.\n     # here we test that even if the server certificate is for another domain,\n     # \"www.example.com\" in this case,\n     # the tests still pass\n-    keyfile = 'keys/example-com.key.pem'\n-    certfile = 'keys/example-com.cert.pem'\n+    keyfile = \"keys/example-com.key.pem\"\n+    certfile = \"keys/example-com.cert.pem\"\n \n \n class Https11InvalidDNSId(Https11TestCase):\n@@ -597,14 +605,14 @@ class Https11InvalidDNSId(Https11TestCase):\n \n     def setUp(self):\n         super().setUp()\n-        self.host = '127.0.0.1'\n+        self.host = \"127.0.0.1\"\n \n \n class Https11InvalidDNSPattern(Https11TestCase):\n     \"\"\"Connect to HTTPS hosts where the certificate are issued to an ip instead of a domain.\"\"\"\n \n-    keyfile = 'keys/localhost.ip.key'\n-    certfile = 'keys/localhost.ip.crt'\n+    keyfile = \"keys/localhost.ip.key\"\n+    certfile = \"keys/localhost.ip.crt\"\n \n     def setUp(self):\n         try:\n@@ -619,11 +627,11 @@ class Https11InvalidDNSPattern(Https11TestCase):\n \n \n class Https11CustomCiphers(unittest.TestCase):\n-    scheme = 'https'\n+    scheme = \"https\"\n     download_handler_cls: Type = HTTP11DownloadHandler\n \n-    keyfile = 'keys/localhost.key'\n-    certfile = 'keys/localhost.crt'\n+    keyfile = \"keys/localhost.key\"\n+    certfile = \"keys/localhost.crt\"\n \n     def setUp(self):\n         self.tmpname = Path(self.mktemp())\n@@ -631,19 +639,28 @@ class Https11CustomCiphers(unittest.TestCase):\n         (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n         r = static.File(str(self.tmpname))\n         self.site = server.Site(r, timeout=None)\n-        self.host = 'localhost'\n+        self.host = \"localhost\"\n         self.port = reactor.listenSSL(\n-            0, self.site, ssl_context_factory(self.keyfile, self.certfile, cipher_string='CAMELLIA256-SHA'),\n-            interface=self.host)\n+            0,\n+            self.site,\n+            ssl_context_factory(\n+                self.keyfile, self.certfile, cipher_string=\"CAMELLIA256-SHA\"\n+            ),\n+            interface=self.host,\n+        )\n         self.portno = self.port.getHost().port\n-        crawler = get_crawler(settings_dict={'DOWNLOADER_CLIENT_TLS_CIPHERS': 'CAMELLIA256-SHA'})\n-        self.download_handler = create_instance(self.download_handler_cls, None, crawler)\n+        crawler = get_crawler(\n+            settings_dict={\"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"CAMELLIA256-SHA\"}\n+        )\n+        self.download_handler = create_instance(\n+            self.download_handler_cls, None, crawler\n+        )\n         self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n     def tearDown(self):\n         yield self.port.stopListening()\n-        if hasattr(self.download_handler, 'close'):\n+        if hasattr(self.download_handler, \"close\"):\n             yield self.download_handler.close()\n         shutil.rmtree(self.tmpname)\n \n@@ -651,8 +668,8 @@ class Https11CustomCiphers(unittest.TestCase):\n         return f\"{self.scheme}://{self.host}:{self.portno}/{path}\"\n \n     def test_download(self):\n-        request = Request(self.getURL('file'))\n-        d = self.download_request(request, Spider('foo'))\n+        request = Request(self.getURL(\"file\"))\n+        d = self.download_request(request, Spider(\"foo\"))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEqual, b\"0123456789\")\n         return d\n@@ -660,6 +677,7 @@ class Https11CustomCiphers(unittest.TestCase):\n \n class Http11MockServerTestCase(unittest.TestCase):\n     \"\"\"HTTP 1.1 test case with MockServer\"\"\"\n+\n     settings_dict: Optional[dict] = None\n \n     def setUp(self):\n@@ -674,40 +692,48 @@ class Http11MockServerTestCase(unittest.TestCase):\n         crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n         # http://localhost:8998/partial set Content-Length to 1024, use download_maxsize= 1000 to avoid\n         # download it\n-        yield crawler.crawl(seed=Request(url=self.mockserver.url('/partial'), meta={'download_maxsize': 1000}))\n-        failure = crawler.spider.meta['failure']\n+        yield crawler.crawl(\n+            seed=Request(\n+                url=self.mockserver.url(\"/partial\"), meta={\"download_maxsize\": 1000}\n+            )\n+        )\n+        failure = crawler.spider.meta[\"failure\"]\n         self.assertIsInstance(failure.value, defer.CancelledError)\n \n     @defer.inlineCallbacks\n     def test_download(self):\n         crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n-        yield crawler.crawl(seed=Request(url=self.mockserver.url('')))\n-        failure = crawler.spider.meta.get('failure')\n+        yield crawler.crawl(seed=Request(url=self.mockserver.url(\"\")))\n+        failure = crawler.spider.meta.get(\"failure\")\n         self.assertTrue(failure is None)\n-        reason = crawler.spider.meta['close_reason']\n-        self.assertTrue(reason, 'finished')\n+        reason = crawler.spider.meta[\"close_reason\"]\n+        self.assertTrue(reason, \"finished\")\n \n     @defer.inlineCallbacks\n     def test_download_gzip_response(self):\n         crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n-        body = b'1' * 100  # PayloadResource requires body length to be 100\n-        request = Request(self.mockserver.url('/payload'), method='POST',\n-                          body=body, meta={'download_maxsize': 50})\n+        body = b\"1\" * 100  # PayloadResource requires body length to be 100\n+        request = Request(\n+            self.mockserver.url(\"/payload\"),\n+            method=\"POST\",\n+            body=body,\n+            meta={\"download_maxsize\": 50},\n+        )\n         yield crawler.crawl(seed=request)\n-        failure = crawler.spider.meta['failure']\n+        failure = crawler.spider.meta[\"failure\"]\n         # download_maxsize < 100, hence the CancelledError\n         self.assertIsInstance(failure.value, defer.CancelledError)\n \n         # See issue https://twistedmatrix.com/trac/ticket/8175\n         raise unittest.SkipTest(\"xpayload fails on PY3\")\n-        request.headers.setdefault(b'Accept-Encoding', b'gzip,deflate')\n-        request = request.replace(url=self.mockserver.url('/xpayload'))\n+        request.headers.setdefault(b\"Accept-Encoding\", b\"gzip,deflate\")\n+        request = request.replace(url=self.mockserver.url(\"/xpayload\"))\n         yield crawler.crawl(seed=request)\n         # download_maxsize = 50 is enough for the gzipped response\n-        failure = crawler.spider.meta.get('failure')\n+        failure = crawler.spider.meta.get(\"failure\")\n         self.assertTrue(failure is None)\n-        reason = crawler.spider.meta['close_reason']\n-        self.assertTrue(reason, 'finished')\n+        reason = crawler.spider.meta[\"close_reason\"]\n+        self.assertTrue(reason, \"finished\")\n \n \n class UriResource(resource.Resource):\n@@ -720,27 +746,29 @@ class UriResource(resource.Resource):\n         # Note: this is an ugly hack for CONNECT request timeout test.\n         #       Returning some data here fail SSL/TLS handshake\n         # ToDo: implement proper HTTPS proxy tests, not faking them.\n-        if request.method != b'CONNECT':\n+        if request.method != b\"CONNECT\":\n             return request.uri\n-        return b''\n+        return b\"\"\n \n \n class HttpProxyTestCase(unittest.TestCase):\n     download_handler_cls: Type = HTTPDownloadHandler\n-    expected_http_proxy_request_body = b'http://example.com'\n+    expected_http_proxy_request_body = b\"http://example.com\"\n \n     def setUp(self):\n         site = server.Site(UriResource(), timeout=None)\n         wrapper = WrappingFactory(site)\n-        self.port = reactor.listenTCP(0, wrapper, interface='127.0.0.1')\n+        self.port = reactor.listenTCP(0, wrapper, interface=\"127.0.0.1\")\n         self.portno = self.port.getHost().port\n-        self.download_handler = create_instance(self.download_handler_cls, None, get_crawler())\n+        self.download_handler = create_instance(\n+            self.download_handler_cls, None, get_crawler()\n+        )\n         self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n     def tearDown(self):\n         yield self.port.stopListening()\n-        if hasattr(self.download_handler, 'close'):\n+        if hasattr(self.download_handler, \"close\"):\n             yield self.download_handler.close()\n \n     def getURL(self, path):\n@@ -752,25 +780,25 @@ class HttpProxyTestCase(unittest.TestCase):\n             self.assertEqual(response.url, request.url)\n             self.assertEqual(response.body, self.expected_http_proxy_request_body)\n \n-        http_proxy = self.getURL('')\n-        request = Request('http://example.com', meta={'proxy': http_proxy})\n-        return self.download_request(request, Spider('foo')).addCallback(_test)\n+        http_proxy = self.getURL(\"\")\n+        request = Request(\"http://example.com\", meta={\"proxy\": http_proxy})\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n \n     def test_download_without_proxy(self):\n         def _test(response):\n             self.assertEqual(response.status, 200)\n             self.assertEqual(response.url, request.url)\n-            self.assertEqual(response.body, b'/path/to/resource')\n+            self.assertEqual(response.body, b\"/path/to/resource\")\n \n-        request = Request(self.getURL('path/to/resource'))\n-        return self.download_request(request, Spider('foo')).addCallback(_test)\n+        request = Request(self.getURL(\"path/to/resource\"))\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n \n \n class Http10ProxyTestCase(HttpProxyTestCase):\n     download_handler_cls: Type = HTTP10DownloadHandler\n \n     def test_download_with_proxy_https_noconnect(self):\n-        raise unittest.SkipTest('noconnect is not supported in HTTP10DownloadHandler')\n+        raise unittest.SkipTest(\"noconnect is not supported in HTTP10DownloadHandler\")\n \n \n class Http11ProxyTestCase(HttpProxyTestCase):\n@@ -781,11 +809,10 @@ class Http11ProxyTestCase(HttpProxyTestCase):\n         \"\"\"Test TunnelingTCP4ClientEndpoint\"\"\"\n         if NON_EXISTING_RESOLVABLE:\n             raise SkipTest(\"Non-existing hosts are resolvable\")\n-        http_proxy = self.getURL('')\n-        domain = 'https://no-such-domain.nosuch'\n-        request = Request(\n-            domain, meta={'proxy': http_proxy, 'download_timeout': 0.2})\n-        d = self.download_request(request, Spider('foo'))\n+        http_proxy = self.getURL(\"\")\n+        domain = \"https://no-such-domain.nosuch\"\n+        request = Request(domain, meta={\"proxy\": http_proxy, \"download_timeout\": 0.2})\n+        d = self.download_request(request, Spider(\"foo\"))\n         timeout = yield self.assertFailure(d, error.TimeoutError)\n         self.assertIn(domain, timeout.osError)\n \n@@ -795,13 +822,12 @@ class Http11ProxyTestCase(HttpProxyTestCase):\n             self.assertEqual(response.url, request.url)\n             self.assertEqual(response.body, self.expected_http_proxy_request_body)\n \n-        http_proxy = self.getURL('').replace('http://', '')\n-        request = Request('http://example.com', meta={'proxy': http_proxy})\n-        return self.download_request(request, Spider('foo')).addCallback(_test)\n+        http_proxy = self.getURL(\"\").replace(\"http://\", \"\")\n+        request = Request(\"http://example.com\", meta={\"proxy\": http_proxy})\n+        return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n \n \n class HttpDownloadHandlerMock:\n-\n     def __init__(self, *args, **kwargs):\n         pass\n \n@@ -810,7 +836,6 @@ class HttpDownloadHandlerMock:\n \n \n class S3AnonTestCase(unittest.TestCase):\n-\n     def setUp(self):\n         skip_if_no_boto()\n         crawler = get_crawler()\n@@ -822,15 +847,14 @@ class S3AnonTestCase(unittest.TestCase):\n             # anon=True, # implicit\n         )\n         self.download_request = self.s3reqh.download_request\n-        self.spider = Spider('foo')\n+        self.spider = Spider(\"foo\")\n \n     def test_anon_request(self):\n-        req = Request('s3://aws-publicdatasets/')\n+        req = Request(\"s3://aws-publicdatasets/\")\n         httpreq = self.download_request(req, self.spider)\n-        self.assertEqual(hasattr(self.s3reqh, 'anon'), True)\n+        self.assertEqual(hasattr(self.s3reqh, \"anon\"), True)\n         self.assertEqual(self.s3reqh.anon, True)\n-        self.assertEqual(\n-            httpreq.url, 'http://aws-publicdatasets.s3.amazonaws.com/')\n+        self.assertEqual(httpreq.url, \"http://aws-publicdatasets.s3.amazonaws.com/\")\n \n \n class S3TestCase(unittest.TestCase):\n@@ -840,8 +864,8 @@ class S3TestCase(unittest.TestCase):\n     # http://s3.amazonaws.com/awsdocs/S3/20060301/s3-dg-20060301.pdf\n     # and the tests described here are the examples from that manual\n \n-    AWS_ACCESS_KEY_ID = '0PN5J17HBGZHT7JJ3X82'\n-    AWS_SECRET_ACCESS_KEY = 'uV3F3YluFJax1cknvbcGwgjvx4QpvB+leU8dUj2o'\n+    AWS_ACCESS_KEY_ID = \"0PN5J17HBGZHT7JJ3X82\"\n+    AWS_SECRET_ACCESS_KEY = \"uV3F3YluFJax1cknvbcGwgjvx4QpvB+leU8dUj2o\"\n \n     def setUp(self):\n         skip_if_no_boto()\n@@ -855,7 +879,7 @@ class S3TestCase(unittest.TestCase):\n             httpdownloadhandler=HttpDownloadHandlerMock,\n         )\n         self.download_request = s3reqh.download_request\n-        self.spider = Spider('foo')\n+        self.spider = Spider(\"foo\")\n \n     @contextlib.contextmanager\n     def _mocked_date(self, date):\n@@ -867,7 +891,7 @@ class S3TestCase(unittest.TestCase):\n             # We need to mock botocore.auth.formatdate, because otherwise\n             # botocore overrides Date header with current date and time\n             # and Authorization header is different each time\n-            with mock.patch('botocore.auth.formatdate') as mock_formatdate:\n+            with mock.patch(\"botocore.auth.formatdate\") as mock_formatdate:\n                 mock_formatdate.return_value = date\n                 yield\n \n@@ -887,90 +911,105 @@ class S3TestCase(unittest.TestCase):\n \n     def test_request_signing1(self):\n         # gets an object from the johnsmith bucket.\n-        date = 'Tue, 27 Mar 2007 19:36:42 +0000'\n-        req = Request('s3://johnsmith/photos/puppy.jpg', headers={'Date': date})\n+        date = \"Tue, 27 Mar 2007 19:36:42 +0000\"\n+        req = Request(\"s3://johnsmith/photos/puppy.jpg\", headers={\"Date\": date})\n         with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n-        self.assertEqual(httpreq.headers['Authorization'],\n-                         b'AWS 0PN5J17HBGZHT7JJ3X82:xXjDGYUmKxnwqr5KXNPGldn5LbA=')\n+        self.assertEqual(\n+            httpreq.headers[\"Authorization\"],\n+            b\"AWS 0PN5J17HBGZHT7JJ3X82:xXjDGYUmKxnwqr5KXNPGldn5LbA=\",\n+        )\n \n     def test_request_signing2(self):\n         # puts an object into the johnsmith bucket.\n-        date = 'Tue, 27 Mar 2007 21:15:45 +0000'\n+        date = \"Tue, 27 Mar 2007 21:15:45 +0000\"\n         req = Request(\n-            's3://johnsmith/photos/puppy.jpg',\n-            method='PUT',\n+            \"s3://johnsmith/photos/puppy.jpg\",\n+            method=\"PUT\",\n             headers={\n-                'Content-Type': 'image/jpeg',\n-                'Date': date,\n-                'Content-Length': '94328',\n+                \"Content-Type\": \"image/jpeg\",\n+                \"Date\": date,\n+                \"Content-Length\": \"94328\",\n             },\n         )\n         with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n-        self.assertEqual(httpreq.headers['Authorization'],\n-                         b'AWS 0PN5J17HBGZHT7JJ3X82:hcicpDDvL9SsO6AkvxqmIWkmOuQ=')\n+        self.assertEqual(\n+            httpreq.headers[\"Authorization\"],\n+            b\"AWS 0PN5J17HBGZHT7JJ3X82:hcicpDDvL9SsO6AkvxqmIWkmOuQ=\",\n+        )\n \n     def test_request_signing3(self):\n         # lists the content of the johnsmith bucket.\n-        date = 'Tue, 27 Mar 2007 19:42:41 +0000'\n+        date = \"Tue, 27 Mar 2007 19:42:41 +0000\"\n         req = Request(\n-            's3://johnsmith/?prefix=photos&max-keys=50&marker=puppy',\n-            method='GET', headers={\n-                'User-Agent': 'Mozilla/5.0',\n-                'Date': date,\n-            })\n-        with self._mocked_date(date):\n-            httpreq = self.download_request(req, self.spider)\n-        self.assertEqual(httpreq.headers['Authorization'],\n-                         b'AWS 0PN5J17HBGZHT7JJ3X82:jsRt/rhG+Vtp88HrYL706QhE4w4=')\n-\n-    def test_request_signing4(self):\n-        # fetches the access control policy sub-resource for the 'johnsmith' bucket.\n-        date = 'Tue, 27 Mar 2007 19:44:46 +0000'\n-        req = Request('s3://johnsmith/?acl', method='GET', headers={'Date': date})\n-        with self._mocked_date(date):\n-            httpreq = self.download_request(req, self.spider)\n-        self.assertEqual(httpreq.headers['Authorization'],\n-                         b'AWS 0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=')\n-\n-    def test_request_signing6(self):\n-        # uploads an object to a CNAME style virtual hosted bucket with metadata.\n-        date = 'Tue, 27 Mar 2007 21:06:08 +0000'\n-        req = Request(\n-            's3://static.johnsmith.net:8080/db-backup.dat.gz',\n-            method='PUT', headers={\n-                'User-Agent': 'curl/7.15.5',\n-                'Host': 'static.johnsmith.net:8080',\n-                'Date': date,\n-                'x-amz-acl': 'public-read',\n-                'content-type': 'application/x-download',\n-                'Content-MD5': '4gJE4saaMU4BqNR0kLY+lw==',\n-                'X-Amz-Meta-ReviewedBy': 'joe@johnsmith.net,jane@johnsmith.net',\n-                'X-Amz-Meta-FileChecksum': '0x02661779',\n-                'X-Amz-Meta-ChecksumAlgorithm': 'crc32',\n-                'Content-Disposition': 'attachment; filename=database.dat',\n-                'Content-Encoding': 'gzip',\n-                'Content-Length': '5913339',\n-            })\n-        with self._mocked_date(date):\n-            httpreq = self.download_request(req, self.spider)\n-        self.assertEqual(httpreq.headers['Authorization'],\n-                         b'AWS 0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=')\n-\n-    def test_request_signing7(self):\n-        # ensure that spaces are quoted properly before signing\n-        date = 'Tue, 27 Mar 2007 19:42:41 +0000'\n-        req = Request(\n-            \"s3://johnsmith/photos/my puppy.jpg?response-content-disposition=my puppy.jpg\",\n-            method='GET',\n-            headers={'Date': date},\n+            \"s3://johnsmith/?prefix=photos&max-keys=50&marker=puppy\",\n+            method=\"GET\",\n+            headers={\n+                \"User-Agent\": \"Mozilla/5.0\",\n+                \"Date\": date,\n+            },\n         )\n         with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n         self.assertEqual(\n-            httpreq.headers['Authorization'],\n-            b'AWS 0PN5J17HBGZHT7JJ3X82:+CfvG8EZ3YccOrRVMXNaK2eKZmM=')\n+            httpreq.headers[\"Authorization\"],\n+            b\"AWS 0PN5J17HBGZHT7JJ3X82:jsRt/rhG+Vtp88HrYL706QhE4w4=\",\n+        )\n+\n+    def test_request_signing4(self):\n+        # fetches the access control policy sub-resource for the 'johnsmith' bucket.\n+        date = \"Tue, 27 Mar 2007 19:44:46 +0000\"\n+        req = Request(\"s3://johnsmith/?acl\", method=\"GET\", headers={\"Date\": date})\n+        with self._mocked_date(date):\n+            httpreq = self.download_request(req, self.spider)\n+        self.assertEqual(\n+            httpreq.headers[\"Authorization\"],\n+            b\"AWS 0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=\",\n+        )\n+\n+    def test_request_signing6(self):\n+        # uploads an object to a CNAME style virtual hosted bucket with metadata.\n+        date = \"Tue, 27 Mar 2007 21:06:08 +0000\"\n+        req = Request(\n+            \"s3://static.johnsmith.net:8080/db-backup.dat.gz\",\n+            method=\"PUT\",\n+            headers={\n+                \"User-Agent\": \"curl/7.15.5\",\n+                \"Host\": \"static.johnsmith.net:8080\",\n+                \"Date\": date,\n+                \"x-amz-acl\": \"public-read\",\n+                \"content-type\": \"application/x-download\",\n+                \"Content-MD5\": \"4gJE4saaMU4BqNR0kLY+lw==\",\n+                \"X-Amz-Meta-ReviewedBy\": \"joe@johnsmith.net,jane@johnsmith.net\",\n+                \"X-Amz-Meta-FileChecksum\": \"0x02661779\",\n+                \"X-Amz-Meta-ChecksumAlgorithm\": \"crc32\",\n+                \"Content-Disposition\": \"attachment; filename=database.dat\",\n+                \"Content-Encoding\": \"gzip\",\n+                \"Content-Length\": \"5913339\",\n+            },\n+        )\n+        with self._mocked_date(date):\n+            httpreq = self.download_request(req, self.spider)\n+        self.assertEqual(\n+            httpreq.headers[\"Authorization\"],\n+            b\"AWS 0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=\",\n+        )\n+\n+    def test_request_signing7(self):\n+        # ensure that spaces are quoted properly before signing\n+        date = \"Tue, 27 Mar 2007 19:42:41 +0000\"\n+        req = Request(\n+            \"s3://johnsmith/photos/my puppy.jpg?response-content-disposition=my puppy.jpg\",\n+            method=\"GET\",\n+            headers={\"Date\": date},\n+        )\n+        with self._mocked_date(date):\n+            httpreq = self.download_request(req, self.spider)\n+        self.assertEqual(\n+            httpreq.headers[\"Authorization\"],\n+            b\"AWS 0PN5J17HBGZHT7JJ3X82:+CfvG8EZ3YccOrRVMXNaK2eKZmM=\",\n+        )\n \n \n class BaseFTPTestCase(unittest.TestCase):\n@@ -979,9 +1018,9 @@ class BaseFTPTestCase(unittest.TestCase):\n     req_meta = {\"ftp_user\": username, \"ftp_password\": password}\n \n     test_files = (\n-        ('file.txt', b\"I have the power!\"),\n-        ('file with spaces.txt', b\"Moooooooooo power!\"),\n-        ('html-file-without-extension', b\"<!DOCTYPE html>\\n<title>.</title>\"),\n+        (\"file.txt\", b\"I have the power!\"),\n+        (\"file with spaces.txt\", b\"Moooooooooo power!\"),\n+        (\"html-file-without-extension\", b\"<!DOCTYPE html>\\n<title>.</title>\"),\n     )\n \n     def setUp(self):\n@@ -997,7 +1036,9 @@ class BaseFTPTestCase(unittest.TestCase):\n             (userdir / filename).write_bytes(content)\n \n         # setup server\n-        realm = FTPRealm(anonymousRoot=str(self.directory), userHome=str(self.directory))\n+        realm = FTPRealm(\n+            anonymousRoot=str(self.directory), userHome=str(self.directory)\n+        )\n         p = portal.Portal(realm)\n         users_checker = checkers.InMemoryUsernamePasswordDatabaseDontUse()\n         users_checker.addUser(self.username, self.password)\n@@ -1006,7 +1047,9 @@ class BaseFTPTestCase(unittest.TestCase):\n         self.port = reactor.listenTCP(0, self.factory, interface=\"127.0.0.1\")\n         self.portNum = self.port.getHost().port\n         crawler = get_crawler()\n-        self.download_handler = create_instance(FTPDownloadHandler, crawler.settings, crawler)\n+        self.download_handler = create_instance(\n+            FTPDownloadHandler, crawler.settings, crawler\n+        )\n         self.addCleanup(self.port.stopListening)\n \n     def tearDown(self):\n@@ -1025,34 +1068,37 @@ class BaseFTPTestCase(unittest.TestCase):\n         return deferred\n \n     def test_ftp_download_success(self):\n-        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/file.txt\",\n-                          meta=self.req_meta)\n-        d = self.download_handler.download_request(request, None)\n-\n-        def _test(r):\n-            self.assertEqual(r.status, 200)\n-            self.assertEqual(r.body, b'I have the power!')\n-            self.assertEqual(r.headers, {b'Local Filename': [b''], b'Size': [b'17']})\n-            self.assertIsNone(r.protocol)\n-        return self._add_test_callbacks(d, _test)\n-\n-    def test_ftp_download_path_with_spaces(self):\n         request = Request(\n-            url=f\"ftp://127.0.0.1:{self.portNum}/file with spaces.txt\",\n-            meta=self.req_meta\n+            url=f\"ftp://127.0.0.1:{self.portNum}/file.txt\", meta=self.req_meta\n         )\n         d = self.download_handler.download_request(request, None)\n \n         def _test(r):\n             self.assertEqual(r.status, 200)\n-            self.assertEqual(r.body, b'Moooooooooo power!')\n-            self.assertEqual(r.headers, {b'Local Filename': [b''], b'Size': [b'18']})\n+            self.assertEqual(r.body, b\"I have the power!\")\n+            self.assertEqual(r.headers, {b\"Local Filename\": [b\"\"], b\"Size\": [b\"17\"]})\n+            self.assertIsNone(r.protocol)\n+\n+        return self._add_test_callbacks(d, _test)\n+\n+    def test_ftp_download_path_with_spaces(self):\n+        request = Request(\n+            url=f\"ftp://127.0.0.1:{self.portNum}/file with spaces.txt\",\n+            meta=self.req_meta,\n+        )\n+        d = self.download_handler.download_request(request, None)\n+\n+        def _test(r):\n+            self.assertEqual(r.status, 200)\n+            self.assertEqual(r.body, b\"Moooooooooo power!\")\n+            self.assertEqual(r.headers, {b\"Local Filename\": [b\"\"], b\"Size\": [b\"18\"]})\n \n         return self._add_test_callbacks(d, _test)\n \n     def test_ftp_download_notexist(self):\n-        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/notexist.txt\",\n-                          meta=self.req_meta)\n+        request = Request(\n+            url=f\"ftp://127.0.0.1:{self.portNum}/notexist.txt\", meta=self.req_meta\n+        )\n         d = self.download_handler.download_request(request, None)\n \n         def _test(r):\n@@ -1067,14 +1113,14 @@ class BaseFTPTestCase(unittest.TestCase):\n         os.close(f)\n         meta = {\"ftp_local_filename\": fname_bytes}\n         meta.update(self.req_meta)\n-        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/file.txt\",\n-                          meta=meta)\n+        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/file.txt\", meta=meta)\n         d = self.download_handler.download_request(request, None)\n \n         def _test(r):\n             self.assertEqual(r.body, fname_bytes)\n-            self.assertEqual(r.headers, {b'Local Filename': [fname_bytes],\n-                                         b'Size': [b'17']})\n+            self.assertEqual(\n+                r.headers, {b\"Local Filename\": [fname_bytes], b\"Size\": [b\"17\"]}\n+            )\n             self.assertTrue(local_fname.exists())\n             self.assertEqual(local_fname.read_bytes(), b\"I have the power!\")\n             local_fname.unlink()\n@@ -1087,24 +1133,23 @@ class BaseFTPTestCase(unittest.TestCase):\n         os.close(f)\n         meta = {}\n         meta.update(self.req_meta)\n-        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/{filename}\",\n-                          meta=meta)\n+        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/{filename}\", meta=meta)\n         d = self.download_handler.download_request(request, None)\n \n         def _test(r):\n             self.assertEqual(type(r), response_class)\n             local_fname.unlink()\n+\n         return self._add_test_callbacks(d, _test)\n \n     def test_response_class_from_url(self):\n-        return self._test_response_class('file.txt', TextResponse)\n+        return self._test_response_class(\"file.txt\", TextResponse)\n \n     def test_response_class_from_body(self):\n-        return self._test_response_class('html-file-without-extension', HtmlResponse)\n+        return self._test_response_class(\"html-file-without-extension\", HtmlResponse)\n \n \n class FTPTestCase(BaseFTPTestCase):\n-\n     def test_invalid_credentials(self):\n         if self.reactor_pytest == \"asyncio\" and sys.platform == \"win32\":\n             raise unittest.SkipTest(\n@@ -1113,9 +1158,8 @@ class FTPTestCase(BaseFTPTestCase):\n         from twisted.protocols.ftp import ConnectionLost\n \n         meta = dict(self.req_meta)\n-        meta.update({\"ftp_password\": 'invalid'})\n-        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/file.txt\",\n-                          meta=meta)\n+        meta.update({\"ftp_password\": \"invalid\"})\n+        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/file.txt\", meta=meta)\n         d = self.download_handler.download_request(request, None)\n \n         def _test(r):\n@@ -1142,15 +1186,15 @@ class AnonymousFTPTestCase(BaseFTPTestCase):\n         # setup server for anonymous access\n         realm = FTPRealm(anonymousRoot=str(self.directory))\n         p = portal.Portal(realm)\n-        p.registerChecker(checkers.AllowAnonymousAccess(),\n-                          credentials.IAnonymous)\n+        p.registerChecker(checkers.AllowAnonymousAccess(), credentials.IAnonymous)\n \n-        self.factory = FTPFactory(portal=p,\n-                                  userAnonymous=self.username)\n+        self.factory = FTPFactory(portal=p, userAnonymous=self.username)\n         self.port = reactor.listenTCP(0, self.factory, interface=\"127.0.0.1\")\n         self.portNum = self.port.getHost().port\n         crawler = get_crawler()\n-        self.download_handler = create_instance(FTPDownloadHandler, crawler.settings, crawler)\n+        self.download_handler = create_instance(\n+            FTPDownloadHandler, crawler.settings, crawler\n+        )\n         self.addCleanup(self.port.stopListening)\n \n     def tearDown(self):\n@@ -1158,12 +1202,13 @@ class AnonymousFTPTestCase(BaseFTPTestCase):\n \n \n class DataURITestCase(unittest.TestCase):\n-\n     def setUp(self):\n         crawler = get_crawler()\n-        self.download_handler = create_instance(DataURIDownloadHandler, crawler.settings, crawler)\n+        self.download_handler = create_instance(\n+            DataURIDownloadHandler, crawler.settings, crawler\n+        )\n         self.download_request = self.download_handler.download_request\n-        self.spider = Spider('foo')\n+        self.spider = Spider(\"foo\")\n \n     def test_response_attrs(self):\n         uri = \"data:,A%20brief%20note\"\n@@ -1177,7 +1222,7 @@ class DataURITestCase(unittest.TestCase):\n \n     def test_default_mediatype_encoding(self):\n         def _test(response):\n-            self.assertEqual(response.text, 'A brief note')\n+            self.assertEqual(response.text, \"A brief note\")\n             self.assertEqual(type(response), responsetypes.from_mimetype(\"text/plain\"))\n             self.assertEqual(response.encoding, \"US-ASCII\")\n \n@@ -1186,7 +1231,7 @@ class DataURITestCase(unittest.TestCase):\n \n     def test_default_mediatype(self):\n         def _test(response):\n-            self.assertEqual(response.text, '\\u038e\\u03a3\\u038e')\n+            self.assertEqual(response.text, \"\\u038e\\u03a3\\u038e\")\n             self.assertEqual(type(response), responsetypes.from_mimetype(\"text/plain\"))\n             self.assertEqual(response.encoding, \"iso-8859-7\")\n \n@@ -1195,8 +1240,8 @@ class DataURITestCase(unittest.TestCase):\n \n     def test_text_charset(self):\n         def _test(response):\n-            self.assertEqual(response.text, '\\u038e\\u03a3\\u038e')\n-            self.assertEqual(response.body, b'\\xbe\\xd3\\xbe')\n+            self.assertEqual(response.text, \"\\u038e\\u03a3\\u038e\")\n+            self.assertEqual(response.body, b\"\\xbe\\xd3\\xbe\")\n             self.assertEqual(response.encoding, \"iso-8859-7\")\n \n         request = Request(\"data:text/plain;charset=iso-8859-7,%be%d3%be\")\n@@ -1204,20 +1249,22 @@ class DataURITestCase(unittest.TestCase):\n \n     def test_mediatype_parameters(self):\n         def _test(response):\n-            self.assertEqual(response.text, '\\u038e\\u03a3\\u038e')\n+            self.assertEqual(response.text, \"\\u038e\\u03a3\\u038e\")\n             self.assertEqual(type(response), responsetypes.from_mimetype(\"text/plain\"))\n             self.assertEqual(response.encoding, \"utf-8\")\n \n-        request = Request('data:text/plain;foo=%22foo;bar%5C%22%22;'\n-                          'charset=utf-8;bar=%22foo;%5C%22 foo ;/,%22'\n-                          ',%CE%8E%CE%A3%CE%8E')\n+        request = Request(\n+            \"data:text/plain;foo=%22foo;bar%5C%22%22;\"\n+            \"charset=utf-8;bar=%22foo;%5C%22 foo ;/,%22\"\n+            \",%CE%8E%CE%A3%CE%8E\"\n+        )\n         return self.download_request(request, self.spider).addCallback(_test)\n \n     def test_base64(self):\n         def _test(response):\n-            self.assertEqual(response.text, 'Hello, world.')\n+            self.assertEqual(response.text, \"Hello, world.\")\n \n-        request = Request('data:text/plain;base64,SGVsbG8sIHdvcmxkLg%3D%3D')\n+        request = Request(\"data:text/plain;base64,SGVsbG8sIHdvcmxkLg%3D%3D\")\n         return self.download_request(request, self.spider).addCallback(_test)\n \n     def test_protocol(self):\n\n@@ -15,21 +15,24 @@ from scrapy.utils.misc import create_instance\n from scrapy.utils.test import get_crawler\n from tests.mockserver import ssl_context_factory\n from tests.test_downloader_handlers import (\n-    Https11TestCase, Https11CustomCiphers,\n-    Http11MockServerTestCase, Http11ProxyTestCase,\n-    UriResource\n+    Https11TestCase,\n+    Https11CustomCiphers,\n+    Http11MockServerTestCase,\n+    Http11ProxyTestCase,\n+    UriResource,\n )\n \n \n @skipIf(not H2_ENABLED, \"HTTP/2 support in Twisted is not enabled\")\n class Https2TestCase(Https11TestCase):\n \n-    scheme = 'https'\n+    scheme = \"https\"\n     HTTP2_DATALOSS_SKIP_REASON = \"Content-Length mismatch raises InvalidBodyLengthError\"\n \n     @classmethod\n     def setUpClass(cls):\n         from scrapy.core.downloader.handlers.http2 import H2DownloadHandler\n+\n         cls.download_handler_cls = H2DownloadHandler\n \n     def test_protocol(self):\n@@ -41,13 +44,13 @@ class Https2TestCase(Https11TestCase):\n \n     @defer.inlineCallbacks\n     def test_download_with_maxsize_very_large_file(self):\n-        with mock.patch('scrapy.core.http2.stream.logger') as logger:\n-            request = Request(self.getURL('largechunkedfile'))\n+        with mock.patch(\"scrapy.core.http2.stream.logger\") as logger:\n+            request = Request(self.getURL(\"largechunkedfile\"))\n \n             def check(logger):\n                 logger.error.assert_called_once_with(mock.ANY)\n \n-            d = self.download_request(request, Spider('foo', download_maxsize=1500))\n+            d = self.download_request(request, Spider(\"foo\", download_maxsize=1500))\n             yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n \n             # As the error message is logged in the dataReceived callback, we\n@@ -55,7 +58,7 @@ class Https2TestCase(Https11TestCase):\n             # after closing the connection.\n             d = defer.Deferred()\n             d.addCallback(check)\n-            reactor.callLater(.1, d.callback, logger)\n+            reactor.callLater(0.1, d.callback, logger)\n             yield d\n \n     @defer.inlineCallbacks\n@@ -64,88 +67,86 @@ class Https2TestCase(Https11TestCase):\n         d = self.download_request(request, Spider(\"foo\"))\n         yield self.assertFailure(d, SchemeNotSupported)\n \n-    def test_download_broken_content_cause_data_loss(self, url='broken'):\n+    def test_download_broken_content_cause_data_loss(self, url=\"broken\"):\n         raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n \n     def test_download_broken_chunked_content_cause_data_loss(self):\n         raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n \n-    def test_download_broken_content_allow_data_loss(self, url='broken'):\n+    def test_download_broken_content_allow_data_loss(self, url=\"broken\"):\n         raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n \n     def test_download_broken_chunked_content_allow_data_loss(self):\n         raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n \n-    def test_download_broken_content_allow_data_loss_via_setting(self, url='broken'):\n+    def test_download_broken_content_allow_data_loss_via_setting(self, url=\"broken\"):\n         raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n \n     def test_download_broken_chunked_content_allow_data_loss_via_setting(self):\n         raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n \n     def test_concurrent_requests_same_domain(self):\n-        spider = Spider('foo')\n+        spider = Spider(\"foo\")\n \n-        request1 = Request(self.getURL('file'))\n+        request1 = Request(self.getURL(\"file\"))\n         d1 = self.download_request(request1, spider)\n         d1.addCallback(lambda r: r.body)\n         d1.addCallback(self.assertEqual, b\"0123456789\")\n \n-        request2 = Request(self.getURL('echo'), method='POST')\n+        request2 = Request(self.getURL(\"echo\"), method=\"POST\")\n         d2 = self.download_request(request2, spider)\n-        d2.addCallback(lambda r: r.headers['Content-Length'])\n+        d2.addCallback(lambda r: r.headers[\"Content-Length\"])\n         d2.addCallback(self.assertEqual, b\"79\")\n \n         return defer.DeferredList([d1, d2])\n \n     @mark.xfail(reason=\"https://github.com/python-hyper/h2/issues/1247\")\n     def test_connect_request(self):\n-        request = Request(self.getURL('file'), method='CONNECT')\n-        d = self.download_request(request, Spider('foo'))\n+        request = Request(self.getURL(\"file\"), method=\"CONNECT\")\n+        d = self.download_request(request, Spider(\"foo\"))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b'')\n+        d.addCallback(self.assertEqual, b\"\")\n         return d\n \n     def test_custom_content_length_good(self):\n-        request = Request(self.getURL('contentlength'))\n+        request = Request(self.getURL(\"contentlength\"))\n         custom_content_length = str(len(request.body))\n-        request.headers['Content-Length'] = custom_content_length\n-        d = self.download_request(request, Spider('foo'))\n+        request.headers[\"Content-Length\"] = custom_content_length\n+        d = self.download_request(request, Spider(\"foo\"))\n         d.addCallback(lambda r: r.text)\n         d.addCallback(self.assertEqual, custom_content_length)\n         return d\n \n     def test_custom_content_length_bad(self):\n-        request = Request(self.getURL('contentlength'))\n+        request = Request(self.getURL(\"contentlength\"))\n         actual_content_length = str(len(request.body))\n         bad_content_length = str(len(request.body) + 1)\n-        request.headers['Content-Length'] = bad_content_length\n+        request.headers[\"Content-Length\"] = bad_content_length\n         log = LogCapture()\n-        d = self.download_request(request, Spider('foo'))\n+        d = self.download_request(request, Spider(\"foo\"))\n         d.addCallback(lambda r: r.text)\n         d.addCallback(self.assertEqual, actual_content_length)\n         d.addCallback(\n             lambda _: log.check_present(\n                 (\n-                    'scrapy.core.http2.stream',\n-                    'WARNING',\n-                    f'Ignoring bad Content-Length header '\n-                    f'{bad_content_length!r} of request {request}, sending '\n-                    f'{actual_content_length!r} instead',\n+                    \"scrapy.core.http2.stream\",\n+                    \"WARNING\",\n+                    f\"Ignoring bad Content-Length header \"\n+                    f\"{bad_content_length!r} of request {request}, sending \"\n+                    f\"{actual_content_length!r} instead\",\n                 )\n             )\n         )\n-        d.addCallback(\n-            lambda _: log.uninstall()\n-        )\n+        d.addCallback(lambda _: log.uninstall())\n         return d\n \n     def test_duplicate_header(self):\n-        request = Request(self.getURL('echo'))\n-        header, value1, value2 = 'Custom-Header', 'foo', 'bar'\n+        request = Request(self.getURL(\"echo\"))\n+        header, value1, value2 = \"Custom-Header\", \"foo\", \"bar\"\n         request.headers.appendlist(header, value1)\n         request.headers.appendlist(header, value2)\n-        d = self.download_request(request, Spider('foo'))\n-        d.addCallback(lambda r: json.loads(r.text)['headers'][header])\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: json.loads(r.text)[\"headers\"][header])\n         d.addCallback(self.assertEqual, [value1, value2])\n         return d\n \n@@ -163,8 +164,8 @@ class Https2WrongHostnameTestCase(Https2TestCase):\n     # here we test that even if the server certificate is for another domain,\n     # \"www.example.com\" in this case,\n     # the tests still pass\n-    keyfile = 'keys/example-com.key.pem'\n-    certfile = 'keys/example-com.cert.pem'\n+    keyfile = \"keys/example-com.key.pem\"\n+    certfile = \"keys/example-com.cert.pem\"\n \n \n class Https2InvalidDNSId(Https2TestCase):\n@@ -172,14 +173,14 @@ class Https2InvalidDNSId(Https2TestCase):\n \n     def setUp(self):\n         super().setUp()\n-        self.host = '127.0.0.1'\n+        self.host = \"127.0.0.1\"\n \n \n class Https2InvalidDNSPattern(Https2TestCase):\n     \"\"\"Connect to HTTPS hosts where the certificate are issued to an ip instead of a domain.\"\"\"\n \n-    keyfile = 'keys/localhost.ip.key'\n-    certfile = 'keys/localhost.ip.crt'\n+    keyfile = \"keys/localhost.ip.key\"\n+    certfile = \"keys/localhost.ip.crt\"\n \n     def setUp(self):\n         try:\n@@ -195,19 +196,21 @@ class Https2InvalidDNSPattern(Https2TestCase):\n \n @skipIf(not H2_ENABLED, \"HTTP/2 support in Twisted is not enabled\")\n class Https2CustomCiphers(Https11CustomCiphers):\n-    scheme = 'https'\n+    scheme = \"https\"\n \n     @classmethod\n     def setUpClass(cls):\n         from scrapy.core.downloader.handlers.http2 import H2DownloadHandler\n+\n         cls.download_handler_cls = H2DownloadHandler\n \n \n class Http2MockServerTestCase(Http11MockServerTestCase):\n     \"\"\"HTTP 2.0 test case with MockServer\"\"\"\n+\n     settings_dict = {\n-        'DOWNLOAD_HANDLERS': {\n-            'https': 'scrapy.core.downloader.handlers.http2.H2DownloadHandler'\n+        \"DOWNLOAD_HANDLERS\": {\n+            \"https\": \"scrapy.core.downloader.handlers.http2.H2DownloadHandler\"\n         }\n     }\n \n@@ -215,28 +218,32 @@ class Http2MockServerTestCase(Http11MockServerTestCase):\n @skipIf(not H2_ENABLED, \"HTTP/2 support in Twisted is not enabled\")\n class Https2ProxyTestCase(Http11ProxyTestCase):\n     # only used for HTTPS tests\n-    keyfile = 'keys/localhost.key'\n-    certfile = 'keys/localhost.crt'\n+    keyfile = \"keys/localhost.key\"\n+    certfile = \"keys/localhost.crt\"\n \n-    scheme = 'https'\n-    host = '127.0.0.1'\n+    scheme = \"https\"\n+    host = \"127.0.0.1\"\n \n-    expected_http_proxy_request_body = b'/'\n+    expected_http_proxy_request_body = b\"/\"\n \n     @classmethod\n     def setUpClass(cls):\n         from scrapy.core.downloader.handlers.http2 import H2DownloadHandler\n+\n         cls.download_handler_cls = H2DownloadHandler\n \n     def setUp(self):\n         site = server.Site(UriResource(), timeout=None)\n         self.port = reactor.listenSSL(\n-            0, site,\n+            0,\n+            site,\n             ssl_context_factory(self.keyfile, self.certfile),\n-            interface=self.host\n+            interface=self.host,\n         )\n         self.portno = self.port.getHost().port\n-        self.download_handler = create_instance(self.download_handler_cls, None, get_crawler())\n+        self.download_handler = create_instance(\n+            self.download_handler_cls, None, get_crawler()\n+        )\n         self.download_request = self.download_handler.download_request\n \n     def getURL(self, path):\n\n@@ -21,14 +21,14 @@ class ManagerTestCase(TestCase):\n \n     def setUp(self):\n         self.crawler = get_crawler(Spider, self.settings_dict)\n-        self.spider = self.crawler._create_spider('foo')\n+        self.spider = self.crawler._create_spider(\"foo\")\n         self.mwman = DownloaderMiddlewareManager.from_crawler(self.crawler)\n         # some mw depends on stats collector\n         self.crawler.stats.open_spider(self.spider)\n         return self.mwman.open_spider(self.spider)\n \n     def tearDown(self):\n-        self.crawler.stats.close_spider(self.spider, '')\n+        self.crawler.stats.close_spider(self.spider, \"\")\n         return self.mwman.close_spider(self.spider)\n \n     def _download(self, request, response=None):\n@@ -57,7 +57,7 @@ class DefaultsTest(ManagerTestCase):\n     \"\"\"Tests default behavior with default settings\"\"\"\n \n     def test_request_response(self):\n-        req = Request('http://example.com/index.html')\n+        req = Request(\"http://example.com/index.html\")\n         resp = Response(req.url, status=200)\n         ret = self._download(req, resp)\n         self.assertTrue(isinstance(ret, Response), \"Non-response returned\")\n@@ -74,29 +74,41 @@ class DefaultsTest(ManagerTestCase):\n             exceptions.IOError: Not a gzipped file\n \n         \"\"\"\n-        req = Request('http://example.com')\n-        body = b'<p>You are being redirected</p>'\n-        resp = Response(req.url, status=302, body=body, headers={\n-            'Content-Length': str(len(body)),\n-            'Content-Type': 'text/html',\n-            'Content-Encoding': 'gzip',\n-            'Location': 'http://example.com/login',\n-        })\n+        req = Request(\"http://example.com\")\n+        body = b\"<p>You are being redirected</p>\"\n+        resp = Response(\n+            req.url,\n+            status=302,\n+            body=body,\n+            headers={\n+                \"Content-Length\": str(len(body)),\n+                \"Content-Type\": \"text/html\",\n+                \"Content-Encoding\": \"gzip\",\n+                \"Location\": \"http://example.com/login\",\n+            },\n+        )\n         ret = self._download(request=req, response=resp)\n-        self.assertTrue(isinstance(ret, Request),\n-                        f\"Not redirected: {ret!r}\")\n-        self.assertEqual(to_bytes(ret.url), resp.headers['Location'],\n-                         \"Not redirected to location header\")\n+        self.assertTrue(isinstance(ret, Request), f\"Not redirected: {ret!r}\")\n+        self.assertEqual(\n+            to_bytes(ret.url),\n+            resp.headers[\"Location\"],\n+            \"Not redirected to location header\",\n+        )\n \n     def test_200_and_invalid_gzipped_body_must_fail(self):\n-        req = Request('http://example.com')\n-        body = b'<p>You are being redirected</p>'\n-        resp = Response(req.url, status=200, body=body, headers={\n-            'Content-Length': str(len(body)),\n-            'Content-Type': 'text/html',\n-            'Content-Encoding': 'gzip',\n-            'Location': 'http://example.com/login',\n-        })\n+        req = Request(\"http://example.com\")\n+        body = b\"<p>You are being redirected</p>\"\n+        resp = Response(\n+            req.url,\n+            status=200,\n+            body=body,\n+            headers={\n+                \"Content-Length\": str(len(body)),\n+                \"Content-Type\": \"text/html\",\n+                \"Content-Encoding\": \"gzip\",\n+                \"Location\": \"http://example.com/login\",\n+            },\n+        )\n         self.assertRaises(IOError, self._download, request=req, response=resp)\n \n \n@@ -104,7 +116,7 @@ class ResponseFromProcessRequestTest(ManagerTestCase):\n     \"\"\"Tests middleware returning a response from process_request.\"\"\"\n \n     def test_download_func_not_called(self):\n-        resp = Response('http://example.com/index.html')\n+        resp = Response(\"http://example.com/index.html\")\n \n         class ResponseMiddleware:\n             def process_request(self, request, spider):\n@@ -112,7 +124,7 @@ class ResponseFromProcessRequestTest(ManagerTestCase):\n \n         self.mwman._add_middleware(ResponseMiddleware())\n \n-        req = Request('http://example.com/index.html')\n+        req = Request(\"http://example.com/index.html\")\n         download_func = mock.MagicMock()\n         dfd = self.mwman.download(download_func, req, self.spider)\n         results = []\n@@ -127,7 +139,7 @@ class ProcessRequestInvalidOutput(ManagerTestCase):\n     \"\"\"Invalid return value for process_request method should raise an exception\"\"\"\n \n     def test_invalid_process_request(self):\n-        req = Request('http://example.com/index.html')\n+        req = Request(\"http://example.com/index.html\")\n \n         class InvalidProcessRequestMiddleware:\n             def process_request(self, request, spider):\n@@ -146,7 +158,7 @@ class ProcessResponseInvalidOutput(ManagerTestCase):\n     \"\"\"Invalid return value for process_response method should raise an exception\"\"\"\n \n     def test_invalid_process_response(self):\n-        req = Request('http://example.com/index.html')\n+        req = Request(\"http://example.com/index.html\")\n \n         class InvalidProcessResponseMiddleware:\n             def process_response(self, request, response, spider):\n@@ -165,7 +177,7 @@ class ProcessExceptionInvalidOutput(ManagerTestCase):\n     \"\"\"Invalid return value for process_exception method should raise an exception\"\"\"\n \n     def test_invalid_process_exception(self):\n-        req = Request('http://example.com/index.html')\n+        req = Request(\"http://example.com/index.html\")\n \n         class InvalidProcessExceptionMiddleware:\n             def process_request(self, request, spider):\n@@ -187,7 +199,7 @@ class MiddlewareUsingDeferreds(ManagerTestCase):\n     \"\"\"Middlewares using Deferreds should work\"\"\"\n \n     def test_deferred(self):\n-        resp = Response('http://example.com/index.html')\n+        resp = Response(\"http://example.com/index.html\")\n \n         class DeferredMiddleware:\n             def cb(self, result):\n@@ -200,7 +212,7 @@ class MiddlewareUsingDeferreds(ManagerTestCase):\n                 return d\n \n         self.mwman._add_middleware(DeferredMiddleware())\n-        req = Request('http://example.com/index.html')\n+        req = Request(\"http://example.com/index.html\")\n         download_func = mock.MagicMock()\n         dfd = self.mwman.download(download_func, req, self.spider)\n         results = []\n@@ -211,12 +223,12 @@ class MiddlewareUsingDeferreds(ManagerTestCase):\n         self.assertFalse(download_func.called)\n \n \n-@mark.usefixtures('reactor_pytest')\n+@mark.usefixtures(\"reactor_pytest\")\n class MiddlewareUsingCoro(ManagerTestCase):\n     \"\"\"Middlewares using asyncio coroutines should work\"\"\"\n \n     def test_asyncdef(self):\n-        resp = Response('http://example.com/index.html')\n+        resp = Response(\"http://example.com/index.html\")\n \n         class CoroMiddleware:\n             async def process_request(self, request, spider):\n@@ -224,7 +236,7 @@ class MiddlewareUsingCoro(ManagerTestCase):\n                 return resp\n \n         self.mwman._add_middleware(CoroMiddleware())\n-        req = Request('http://example.com/index.html')\n+        req = Request(\"http://example.com/index.html\")\n         download_func = mock.MagicMock()\n         dfd = self.mwman.download(download_func, req, self.spider)\n         results = []\n@@ -236,7 +248,7 @@ class MiddlewareUsingCoro(ManagerTestCase):\n \n     @mark.only_asyncio()\n     def test_asyncdef_asyncio(self):\n-        resp = Response('http://example.com/index.html')\n+        resp = Response(\"http://example.com/index.html\")\n \n         class CoroMiddleware:\n             async def process_request(self, request, spider):\n@@ -245,7 +257,7 @@ class MiddlewareUsingCoro(ManagerTestCase):\n                 return result\n \n         self.mwman._add_middleware(CoroMiddleware())\n-        req = Request('http://example.com/index.html')\n+        req = Request(\"http://example.com/index.html\")\n         download_func = mock.MagicMock()\n         dfd = self.mwman.download(download_func, req, self.spider)\n         results = []\n\n@@ -6,13 +6,13 @@ from scrapy.http import Request, HtmlResponse, Response\n from scrapy.utils.test import get_crawler\n \n \n-__doctests__ = ['scrapy.downloadermiddlewares.ajaxcrawl']\n+__doctests__ = [\"scrapy.downloadermiddlewares.ajaxcrawl\"]\n \n \n class AjaxCrawlMiddlewareTest(unittest.TestCase):\n     def setUp(self):\n-        crawler = get_crawler(Spider, {'AJAXCRAWL_ENABLED': True})\n-        self.spider = crawler._create_spider('foo')\n+        crawler = get_crawler(Spider, {\"AJAXCRAWL_ENABLED\": True})\n+        self.spider = crawler._create_spider(\"foo\")\n         self.mw = AjaxCrawlMiddleware.from_crawler(crawler)\n \n     def _ajaxcrawlable_body(self):\n@@ -24,37 +24,41 @@ class AjaxCrawlMiddlewareTest(unittest.TestCase):\n         return req, resp\n \n     def test_non_get(self):\n-        req, resp = self._req_resp('http://example.com/', {'method': 'HEAD'})\n+        req, resp = self._req_resp(\"http://example.com/\", {\"method\": \"HEAD\"})\n         resp2 = self.mw.process_response(req, resp, self.spider)\n         self.assertEqual(resp, resp2)\n \n     def test_binary_response(self):\n-        req = Request('http://example.com/')\n-        resp = Response('http://example.com/', body=b'foobar\\x00\\x01\\x02', request=req)\n+        req = Request(\"http://example.com/\")\n+        resp = Response(\"http://example.com/\", body=b\"foobar\\x00\\x01\\x02\", request=req)\n         resp2 = self.mw.process_response(req, resp, self.spider)\n         self.assertIs(resp, resp2)\n \n     def test_ajaxcrawl(self):\n         req, resp = self._req_resp(\n-            'http://example.com/',\n-            {'meta': {'foo': 'bar'}},\n-            {'body': self._ajaxcrawlable_body()}\n+            \"http://example.com/\",\n+            {\"meta\": {\"foo\": \"bar\"}},\n+            {\"body\": self._ajaxcrawlable_body()},\n         )\n         req2 = self.mw.process_response(req, resp, self.spider)\n-        self.assertEqual(req2.url, 'http://example.com/?_escaped_fragment_=')\n-        self.assertEqual(req2.meta['foo'], 'bar')\n+        self.assertEqual(req2.url, \"http://example.com/?_escaped_fragment_=\")\n+        self.assertEqual(req2.meta[\"foo\"], \"bar\")\n \n     def test_ajaxcrawl_loop(self):\n-        req, resp = self._req_resp('http://example.com/', {}, {'body': self._ajaxcrawlable_body()})\n+        req, resp = self._req_resp(\n+            \"http://example.com/\", {}, {\"body\": self._ajaxcrawlable_body()}\n+        )\n         req2 = self.mw.process_response(req, resp, self.spider)\n         resp2 = HtmlResponse(req2.url, body=resp.body, request=req2)\n         resp3 = self.mw.process_response(req2, resp2, self.spider)\n \n         assert isinstance(resp3, HtmlResponse), (resp3.__class__, resp3)\n-        self.assertEqual(resp3.request.url, 'http://example.com/?_escaped_fragment_=')\n+        self.assertEqual(resp3.request.url, \"http://example.com/?_escaped_fragment_=\")\n         assert resp3 is resp2\n \n     def test_noncrawlable_body(self):\n-        req, resp = self._req_resp('http://example.com/', {}, {'body': b'<html></html>'})\n+        req, resp = self._req_resp(\n+            \"http://example.com/\", {}, {\"body\": b\"<html></html>\"}\n+        )\n         resp2 = self.mw.process_response(req, resp, self.spider)\n         self.assertIs(resp, resp2)\n\n@@ -48,24 +48,18 @@ def _cookies_to_set_cookie_list(cookies):\n         return []\n     if isinstance(cookies, dict):\n         cookies = ({\"name\": k, \"value\": v} for k, v in cookies.items())\n-    return filter(\n-        None,\n-        (\n-            _cookie_to_set_cookie_value(cookie)\n-            for cookie in cookies\n-        )\n-    )\n+    return filter(None, (_cookie_to_set_cookie_value(cookie) for cookie in cookies))\n \n \n class CookiesMiddlewareTest(TestCase):\n-\n     def assertCookieValEqual(self, first, second, msg=None):\n         def split_cookies(cookies):\n             return sorted([s.strip() for s in to_bytes(cookies).split(b\";\")])\n+\n         return self.assertEqual(split_cookies(first), split_cookies(second), msg=msg)\n \n     def setUp(self):\n-        self.spider = Spider('foo')\n+        self.spider = Spider(\"foo\")\n         self.mw = CookiesMiddleware()\n         self.redirect_middleware = RedirectMiddleware(settings=Settings())\n \n@@ -74,346 +68,391 @@ class CookiesMiddlewareTest(TestCase):\n         del self.redirect_middleware\n \n     def test_basic(self):\n-        req = Request('http://scrapytest.org/')\n+        req = Request(\"http://scrapytest.org/\")\n         assert self.mw.process_request(req, self.spider) is None\n-        assert 'Cookie' not in req.headers\n+        assert \"Cookie\" not in req.headers\n \n-        headers = {'Set-Cookie': 'C1=value1; path=/'}\n-        res = Response('http://scrapytest.org/', headers=headers)\n+        headers = {\"Set-Cookie\": \"C1=value1; path=/\"}\n+        res = Response(\"http://scrapytest.org/\", headers=headers)\n         assert self.mw.process_response(req, res, self.spider) is res\n \n-        req2 = Request('http://scrapytest.org/sub1/')\n+        req2 = Request(\"http://scrapytest.org/sub1/\")\n         assert self.mw.process_request(req2, self.spider) is None\n-        self.assertEqual(req2.headers.get('Cookie'), b\"C1=value1\")\n+        self.assertEqual(req2.headers.get(\"Cookie\"), b\"C1=value1\")\n \n     def test_setting_false_cookies_enabled(self):\n         self.assertRaises(\n             NotConfigured,\n             CookiesMiddleware.from_crawler,\n-            get_crawler(settings_dict={'COOKIES_ENABLED': False})\n+            get_crawler(settings_dict={\"COOKIES_ENABLED\": False}),\n         )\n \n     def test_setting_default_cookies_enabled(self):\n         self.assertIsInstance(\n-            CookiesMiddleware.from_crawler(get_crawler()),\n-            CookiesMiddleware\n+            CookiesMiddleware.from_crawler(get_crawler()), CookiesMiddleware\n         )\n \n     def test_setting_true_cookies_enabled(self):\n         self.assertIsInstance(\n             CookiesMiddleware.from_crawler(\n-                get_crawler(settings_dict={'COOKIES_ENABLED': True})\n+                get_crawler(settings_dict={\"COOKIES_ENABLED\": True})\n             ),\n-            CookiesMiddleware\n+            CookiesMiddleware,\n         )\n \n     def test_setting_enabled_cookies_debug(self):\n-        crawler = get_crawler(settings_dict={'COOKIES_DEBUG': True})\n+        crawler = get_crawler(settings_dict={\"COOKIES_DEBUG\": True})\n         mw = CookiesMiddleware.from_crawler(crawler)\n         with LogCapture(\n-            'scrapy.downloadermiddlewares.cookies',\n+            \"scrapy.downloadermiddlewares.cookies\",\n             propagate=False,\n             level=logging.DEBUG,\n         ) as log:\n-            req = Request('http://scrapytest.org/')\n-            res = Response('http://scrapytest.org/', headers={'Set-Cookie': 'C1=value1; path=/'})\n+            req = Request(\"http://scrapytest.org/\")\n+            res = Response(\n+                \"http://scrapytest.org/\", headers={\"Set-Cookie\": \"C1=value1; path=/\"}\n+            )\n             mw.process_response(req, res, crawler.spider)\n-            req2 = Request('http://scrapytest.org/sub1/')\n+            req2 = Request(\"http://scrapytest.org/sub1/\")\n             mw.process_request(req2, crawler.spider)\n \n             log.check(\n-                ('scrapy.downloadermiddlewares.cookies',\n-                 'DEBUG',\n-                 'Received cookies from: <200 http://scrapytest.org/>\\n'\n-                 'Set-Cookie: C1=value1; path=/\\n'),\n-                ('scrapy.downloadermiddlewares.cookies',\n-                 'DEBUG',\n-                 'Sending cookies to: <GET http://scrapytest.org/sub1/>\\n'\n-                 'Cookie: C1=value1\\n'),\n+                (\n+                    \"scrapy.downloadermiddlewares.cookies\",\n+                    \"DEBUG\",\n+                    \"Received cookies from: <200 http://scrapytest.org/>\\n\"\n+                    \"Set-Cookie: C1=value1; path=/\\n\",\n+                ),\n+                (\n+                    \"scrapy.downloadermiddlewares.cookies\",\n+                    \"DEBUG\",\n+                    \"Sending cookies to: <GET http://scrapytest.org/sub1/>\\n\"\n+                    \"Cookie: C1=value1\\n\",\n+                ),\n             )\n \n     def test_setting_disabled_cookies_debug(self):\n-        crawler = get_crawler(settings_dict={'COOKIES_DEBUG': False})\n+        crawler = get_crawler(settings_dict={\"COOKIES_DEBUG\": False})\n         mw = CookiesMiddleware.from_crawler(crawler)\n         with LogCapture(\n-            'scrapy.downloadermiddlewares.cookies',\n+            \"scrapy.downloadermiddlewares.cookies\",\n             propagate=False,\n             level=logging.DEBUG,\n         ) as log:\n-            req = Request('http://scrapytest.org/')\n-            res = Response('http://scrapytest.org/', headers={'Set-Cookie': 'C1=value1; path=/'})\n+            req = Request(\"http://scrapytest.org/\")\n+            res = Response(\n+                \"http://scrapytest.org/\", headers={\"Set-Cookie\": \"C1=value1; path=/\"}\n+            )\n             mw.process_response(req, res, crawler.spider)\n-            req2 = Request('http://scrapytest.org/sub1/')\n+            req2 = Request(\"http://scrapytest.org/sub1/\")\n             mw.process_request(req2, crawler.spider)\n \n             log.check()\n \n     def test_do_not_break_on_non_utf8_header(self):\n-        req = Request('http://scrapytest.org/')\n+        req = Request(\"http://scrapytest.org/\")\n         assert self.mw.process_request(req, self.spider) is None\n-        assert 'Cookie' not in req.headers\n+        assert \"Cookie\" not in req.headers\n \n-        headers = {'Set-Cookie': b'C1=in\\xa3valid; path=/', 'Other': b'ignore\\xa3me'}\n-        res = Response('http://scrapytest.org/', headers=headers)\n+        headers = {\"Set-Cookie\": b\"C1=in\\xa3valid; path=/\", \"Other\": b\"ignore\\xa3me\"}\n+        res = Response(\"http://scrapytest.org/\", headers=headers)\n         assert self.mw.process_response(req, res, self.spider) is res\n \n-        req2 = Request('http://scrapytest.org/sub1/')\n+        req2 = Request(\"http://scrapytest.org/sub1/\")\n         assert self.mw.process_request(req2, self.spider) is None\n-        self.assertIn('Cookie', req2.headers)\n+        self.assertIn(\"Cookie\", req2.headers)\n \n     def test_dont_merge_cookies(self):\n         # merge some cookies into jar\n-        headers = {'Set-Cookie': 'C1=value1; path=/'}\n-        req = Request('http://scrapytest.org/')\n-        res = Response('http://scrapytest.org/', headers=headers)\n+        headers = {\"Set-Cookie\": \"C1=value1; path=/\"}\n+        req = Request(\"http://scrapytest.org/\")\n+        res = Response(\"http://scrapytest.org/\", headers=headers)\n         assert self.mw.process_response(req, res, self.spider) is res\n \n         # test Cookie header is not seted to request\n-        req = Request('http://scrapytest.org/dontmerge', meta={'dont_merge_cookies': 1})\n+        req = Request(\"http://scrapytest.org/dontmerge\", meta={\"dont_merge_cookies\": 1})\n         assert self.mw.process_request(req, self.spider) is None\n-        assert 'Cookie' not in req.headers\n+        assert \"Cookie\" not in req.headers\n \n         # check that returned cookies are not merged back to jar\n         res = Response(\n-            'http://scrapytest.org/dontmerge',\n-            headers={'Set-Cookie': 'dont=mergeme; path=/'},\n+            \"http://scrapytest.org/dontmerge\",\n+            headers={\"Set-Cookie\": \"dont=mergeme; path=/\"},\n         )\n         assert self.mw.process_response(req, res, self.spider) is res\n \n         # check that cookies are merged back\n-        req = Request('http://scrapytest.org/mergeme')\n+        req = Request(\"http://scrapytest.org/mergeme\")\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers.get('Cookie'), b'C1=value1')\n+        self.assertEqual(req.headers.get(\"Cookie\"), b\"C1=value1\")\n \n         # check that cookies are merged when dont_merge_cookies is passed as 0\n-        req = Request('http://scrapytest.org/mergeme', meta={'dont_merge_cookies': 0})\n+        req = Request(\"http://scrapytest.org/mergeme\", meta={\"dont_merge_cookies\": 0})\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers.get('Cookie'), b'C1=value1')\n+        self.assertEqual(req.headers.get(\"Cookie\"), b\"C1=value1\")\n \n     def test_complex_cookies(self):\n         # merge some cookies into jar\n         cookies = [\n-            {'name': 'C1', 'value': 'value1', 'path': '/foo', 'domain': 'scrapytest.org'},\n-            {'name': 'C2', 'value': 'value2', 'path': '/bar', 'domain': 'scrapytest.org'},\n-            {'name': 'C3', 'value': 'value3', 'path': '/foo', 'domain': 'scrapytest.org'},\n-            {'name': 'C4', 'value': 'value4', 'path': '/foo', 'domain': 'scrapy.org'},\n+            {\n+                \"name\": \"C1\",\n+                \"value\": \"value1\",\n+                \"path\": \"/foo\",\n+                \"domain\": \"scrapytest.org\",\n+            },\n+            {\n+                \"name\": \"C2\",\n+                \"value\": \"value2\",\n+                \"path\": \"/bar\",\n+                \"domain\": \"scrapytest.org\",\n+            },\n+            {\n+                \"name\": \"C3\",\n+                \"value\": \"value3\",\n+                \"path\": \"/foo\",\n+                \"domain\": \"scrapytest.org\",\n+            },\n+            {\"name\": \"C4\", \"value\": \"value4\", \"path\": \"/foo\", \"domain\": \"scrapy.org\"},\n         ]\n \n-        req = Request('http://scrapytest.org/', cookies=cookies)\n+        req = Request(\"http://scrapytest.org/\", cookies=cookies)\n         self.mw.process_request(req, self.spider)\n \n         # embed C1 and C3 for scrapytest.org/foo\n-        req = Request('http://scrapytest.org/foo')\n+        req = Request(\"http://scrapytest.org/foo\")\n         self.mw.process_request(req, self.spider)\n-        assert req.headers.get('Cookie') in (b'C1=value1; C3=value3', b'C3=value3; C1=value1')\n+        assert req.headers.get(\"Cookie\") in (\n+            b\"C1=value1; C3=value3\",\n+            b\"C3=value3; C1=value1\",\n+        )\n \n         # embed C2 for scrapytest.org/bar\n-        req = Request('http://scrapytest.org/bar')\n+        req = Request(\"http://scrapytest.org/bar\")\n         self.mw.process_request(req, self.spider)\n-        self.assertEqual(req.headers.get('Cookie'), b'C2=value2')\n+        self.assertEqual(req.headers.get(\"Cookie\"), b\"C2=value2\")\n \n         # embed nothing for scrapytest.org/baz\n-        req = Request('http://scrapytest.org/baz')\n+        req = Request(\"http://scrapytest.org/baz\")\n         self.mw.process_request(req, self.spider)\n-        assert 'Cookie' not in req.headers\n+        assert \"Cookie\" not in req.headers\n \n     def test_merge_request_cookies(self):\n-        req = Request('http://scrapytest.org/', cookies={'galleta': 'salada'})\n+        req = Request(\"http://scrapytest.org/\", cookies={\"galleta\": \"salada\"})\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers.get('Cookie'), b'galleta=salada')\n+        self.assertEqual(req.headers.get(\"Cookie\"), b\"galleta=salada\")\n \n-        headers = {'Set-Cookie': 'C1=value1; path=/'}\n-        res = Response('http://scrapytest.org/', headers=headers)\n+        headers = {\"Set-Cookie\": \"C1=value1; path=/\"}\n+        res = Response(\"http://scrapytest.org/\", headers=headers)\n         assert self.mw.process_response(req, res, self.spider) is res\n \n-        req2 = Request('http://scrapytest.org/sub1/')\n+        req2 = Request(\"http://scrapytest.org/sub1/\")\n         assert self.mw.process_request(req2, self.spider) is None\n \n-        self.assertCookieValEqual(req2.headers.get('Cookie'), b\"C1=value1; galleta=salada\")\n+        self.assertCookieValEqual(\n+            req2.headers.get(\"Cookie\"), b\"C1=value1; galleta=salada\"\n+        )\n \n     def test_cookiejar_key(self):\n         req = Request(\n-            'http://scrapytest.org/',\n-            cookies={'galleta': 'salada'},\n-            meta={'cookiejar': \"store1\"},\n+            \"http://scrapytest.org/\",\n+            cookies={\"galleta\": \"salada\"},\n+            meta={\"cookiejar\": \"store1\"},\n         )\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers.get('Cookie'), b'galleta=salada')\n+        self.assertEqual(req.headers.get(\"Cookie\"), b\"galleta=salada\")\n \n-        headers = {'Set-Cookie': 'C1=value1; path=/'}\n-        res = Response('http://scrapytest.org/', headers=headers, request=req)\n+        headers = {\"Set-Cookie\": \"C1=value1; path=/\"}\n+        res = Response(\"http://scrapytest.org/\", headers=headers, request=req)\n         assert self.mw.process_response(req, res, self.spider) is res\n \n-        req2 = Request('http://scrapytest.org/', meta=res.meta)\n+        req2 = Request(\"http://scrapytest.org/\", meta=res.meta)\n         assert self.mw.process_request(req2, self.spider) is None\n-        self.assertCookieValEqual(req2.headers.get('Cookie'), b'C1=value1; galleta=salada')\n+        self.assertCookieValEqual(\n+            req2.headers.get(\"Cookie\"), b\"C1=value1; galleta=salada\"\n+        )\n \n         req3 = Request(\n-            'http://scrapytest.org/',\n-            cookies={'galleta': 'dulce'},\n-            meta={'cookiejar': \"store2\"},\n+            \"http://scrapytest.org/\",\n+            cookies={\"galleta\": \"dulce\"},\n+            meta={\"cookiejar\": \"store2\"},\n         )\n         assert self.mw.process_request(req3, self.spider) is None\n-        self.assertEqual(req3.headers.get('Cookie'), b'galleta=dulce')\n+        self.assertEqual(req3.headers.get(\"Cookie\"), b\"galleta=dulce\")\n \n-        headers = {'Set-Cookie': 'C2=value2; path=/'}\n-        res2 = Response('http://scrapytest.org/', headers=headers, request=req3)\n+        headers = {\"Set-Cookie\": \"C2=value2; path=/\"}\n+        res2 = Response(\"http://scrapytest.org/\", headers=headers, request=req3)\n         assert self.mw.process_response(req3, res2, self.spider) is res2\n \n-        req4 = Request('http://scrapytest.org/', meta=res2.meta)\n+        req4 = Request(\"http://scrapytest.org/\", meta=res2.meta)\n         assert self.mw.process_request(req4, self.spider) is None\n-        self.assertCookieValEqual(req4.headers.get('Cookie'), b'C2=value2; galleta=dulce')\n+        self.assertCookieValEqual(\n+            req4.headers.get(\"Cookie\"), b\"C2=value2; galleta=dulce\"\n+        )\n \n         # cookies from hosts with port\n-        req5_1 = Request('http://scrapytest.org:1104/')\n+        req5_1 = Request(\"http://scrapytest.org:1104/\")\n         assert self.mw.process_request(req5_1, self.spider) is None\n \n-        headers = {'Set-Cookie': 'C1=value1; path=/'}\n-        res5_1 = Response('http://scrapytest.org:1104/', headers=headers, request=req5_1)\n+        headers = {\"Set-Cookie\": \"C1=value1; path=/\"}\n+        res5_1 = Response(\n+            \"http://scrapytest.org:1104/\", headers=headers, request=req5_1\n+        )\n         assert self.mw.process_response(req5_1, res5_1, self.spider) is res5_1\n \n-        req5_2 = Request('http://scrapytest.org:1104/some-redirected-path')\n+        req5_2 = Request(\"http://scrapytest.org:1104/some-redirected-path\")\n         assert self.mw.process_request(req5_2, self.spider) is None\n-        self.assertEqual(req5_2.headers.get('Cookie'), b'C1=value1')\n+        self.assertEqual(req5_2.headers.get(\"Cookie\"), b\"C1=value1\")\n \n-        req5_3 = Request('http://scrapytest.org/some-redirected-path')\n+        req5_3 = Request(\"http://scrapytest.org/some-redirected-path\")\n         assert self.mw.process_request(req5_3, self.spider) is None\n-        self.assertEqual(req5_3.headers.get('Cookie'), b'C1=value1')\n+        self.assertEqual(req5_3.headers.get(\"Cookie\"), b\"C1=value1\")\n \n         # skip cookie retrieval for not http request\n-        req6 = Request('file:///scrapy/sometempfile')\n+        req6 = Request(\"file:///scrapy/sometempfile\")\n         assert self.mw.process_request(req6, self.spider) is None\n-        self.assertEqual(req6.headers.get('Cookie'), None)\n+        self.assertEqual(req6.headers.get(\"Cookie\"), None)\n \n     def test_local_domain(self):\n-        request = Request(\"http://example-host/\", cookies={'currencyCookie': 'USD'})\n+        request = Request(\"http://example-host/\", cookies={\"currencyCookie\": \"USD\"})\n         assert self.mw.process_request(request, self.spider) is None\n-        self.assertIn('Cookie', request.headers)\n-        self.assertEqual(b'currencyCookie=USD', request.headers['Cookie'])\n+        self.assertIn(\"Cookie\", request.headers)\n+        self.assertEqual(b\"currencyCookie=USD\", request.headers[\"Cookie\"])\n \n     @pytest.mark.xfail(reason=\"Cookie header is not currently being processed\")\n     def test_keep_cookie_from_default_request_headers_middleware(self):\n-        DEFAULT_REQUEST_HEADERS = dict(Cookie='default=value; asdf=qwerty')\n+        DEFAULT_REQUEST_HEADERS = dict(Cookie=\"default=value; asdf=qwerty\")\n         mw_default_headers = DefaultHeadersMiddleware(DEFAULT_REQUEST_HEADERS.items())\n         # overwrite with values from 'cookies' request argument\n-        req1 = Request('http://example.org', cookies={'default': 'something'})\n+        req1 = Request(\"http://example.org\", cookies={\"default\": \"something\"})\n         assert mw_default_headers.process_request(req1, self.spider) is None\n         assert self.mw.process_request(req1, self.spider) is None\n-        self.assertCookieValEqual(req1.headers['Cookie'], b'default=something; asdf=qwerty')\n+        self.assertCookieValEqual(\n+            req1.headers[\"Cookie\"], b\"default=something; asdf=qwerty\"\n+        )\n         # keep both\n-        req2 = Request('http://example.com', cookies={'a': 'b'})\n+        req2 = Request(\"http://example.com\", cookies={\"a\": \"b\"})\n         assert mw_default_headers.process_request(req2, self.spider) is None\n         assert self.mw.process_request(req2, self.spider) is None\n-        self.assertCookieValEqual(req2.headers['Cookie'], b'default=value; a=b; asdf=qwerty')\n+        self.assertCookieValEqual(\n+            req2.headers[\"Cookie\"], b\"default=value; a=b; asdf=qwerty\"\n+        )\n \n     @pytest.mark.xfail(reason=\"Cookie header is not currently being processed\")\n     def test_keep_cookie_header(self):\n         # keep only cookies from 'Cookie' request header\n-        req1 = Request('http://scrapytest.org', headers={'Cookie': 'a=b; c=d'})\n+        req1 = Request(\"http://scrapytest.org\", headers={\"Cookie\": \"a=b; c=d\"})\n         assert self.mw.process_request(req1, self.spider) is None\n-        self.assertCookieValEqual(req1.headers['Cookie'], 'a=b; c=d')\n+        self.assertCookieValEqual(req1.headers[\"Cookie\"], \"a=b; c=d\")\n         # keep cookies from both 'Cookie' request header and 'cookies' keyword\n-        req2 = Request('http://scrapytest.org', headers={'Cookie': 'a=b; c=d'}, cookies={'e': 'f'})\n+        req2 = Request(\n+            \"http://scrapytest.org\", headers={\"Cookie\": \"a=b; c=d\"}, cookies={\"e\": \"f\"}\n+        )\n         assert self.mw.process_request(req2, self.spider) is None\n-        self.assertCookieValEqual(req2.headers['Cookie'], 'a=b; c=d; e=f')\n+        self.assertCookieValEqual(req2.headers[\"Cookie\"], \"a=b; c=d; e=f\")\n         # overwrite values from 'Cookie' request header with 'cookies' keyword\n         req3 = Request(\n-            'http://scrapytest.org',\n-            headers={'Cookie': 'a=b; c=d'},\n-            cookies={'a': 'new', 'e': 'f'},\n+            \"http://scrapytest.org\",\n+            headers={\"Cookie\": \"a=b; c=d\"},\n+            cookies={\"a\": \"new\", \"e\": \"f\"},\n         )\n         assert self.mw.process_request(req3, self.spider) is None\n-        self.assertCookieValEqual(req3.headers['Cookie'], 'a=new; c=d; e=f')\n+        self.assertCookieValEqual(req3.headers[\"Cookie\"], \"a=new; c=d; e=f\")\n \n     def test_request_cookies_encoding(self):\n         # 1) UTF8-encoded bytes\n-        req1 = Request('http://example.org', cookies={'a': 'á'.encode('utf8')})\n+        req1 = Request(\"http://example.org\", cookies={\"a\": \"á\".encode(\"utf8\")})\n         assert self.mw.process_request(req1, self.spider) is None\n-        self.assertCookieValEqual(req1.headers['Cookie'], b'a=\\xc3\\xa1')\n+        self.assertCookieValEqual(req1.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n \n         # 2) Non UTF8-encoded bytes\n-        req2 = Request('http://example.org', cookies={'a': 'á'.encode('latin1')})\n+        req2 = Request(\"http://example.org\", cookies={\"a\": \"á\".encode(\"latin1\")})\n         assert self.mw.process_request(req2, self.spider) is None\n-        self.assertCookieValEqual(req2.headers['Cookie'], b'a=\\xc3\\xa1')\n+        self.assertCookieValEqual(req2.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n \n         # 3) String\n-        req3 = Request('http://example.org', cookies={'a': 'á'})\n+        req3 = Request(\"http://example.org\", cookies={\"a\": \"á\"})\n         assert self.mw.process_request(req3, self.spider) is None\n-        self.assertCookieValEqual(req3.headers['Cookie'], b'a=\\xc3\\xa1')\n+        self.assertCookieValEqual(req3.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n \n     @pytest.mark.xfail(reason=\"Cookie header is not currently being processed\")\n     def test_request_headers_cookie_encoding(self):\n         # 1) UTF8-encoded bytes\n-        req1 = Request('http://example.org', headers={'Cookie': 'a=á'.encode('utf8')})\n+        req1 = Request(\"http://example.org\", headers={\"Cookie\": \"a=á\".encode(\"utf8\")})\n         assert self.mw.process_request(req1, self.spider) is None\n-        self.assertCookieValEqual(req1.headers['Cookie'], b'a=\\xc3\\xa1')\n+        self.assertCookieValEqual(req1.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n \n         # 2) Non UTF8-encoded bytes\n-        req2 = Request('http://example.org', headers={'Cookie': 'a=á'.encode('latin1')})\n+        req2 = Request(\"http://example.org\", headers={\"Cookie\": \"a=á\".encode(\"latin1\")})\n         assert self.mw.process_request(req2, self.spider) is None\n-        self.assertCookieValEqual(req2.headers['Cookie'], b'a=\\xc3\\xa1')\n+        self.assertCookieValEqual(req2.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n \n         # 3) String\n-        req3 = Request('http://example.org', headers={'Cookie': 'a=á'})\n+        req3 = Request(\"http://example.org\", headers={\"Cookie\": \"a=á\"})\n         assert self.mw.process_request(req3, self.spider) is None\n-        self.assertCookieValEqual(req3.headers['Cookie'], b'a=\\xc3\\xa1')\n+        self.assertCookieValEqual(req3.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n \n     def test_invalid_cookies(self):\n         \"\"\"\n         Invalid cookies are logged as warnings and discarded\n         \"\"\"\n         with LogCapture(\n-            'scrapy.downloadermiddlewares.cookies',\n+            \"scrapy.downloadermiddlewares.cookies\",\n             propagate=False,\n             level=logging.INFO,\n         ) as lc:\n-            cookies1 = [{'value': 'bar'}, {'name': 'key', 'value': 'value1'}]\n-            req1 = Request('http://example.org/1', cookies=cookies1)\n+            cookies1 = [{\"value\": \"bar\"}, {\"name\": \"key\", \"value\": \"value1\"}]\n+            req1 = Request(\"http://example.org/1\", cookies=cookies1)\n             assert self.mw.process_request(req1, self.spider) is None\n-            cookies2 = [{'name': 'foo'}, {'name': 'key', 'value': 'value2'}]\n-            req2 = Request('http://example.org/2', cookies=cookies2)\n+            cookies2 = [{\"name\": \"foo\"}, {\"name\": \"key\", \"value\": \"value2\"}]\n+            req2 = Request(\"http://example.org/2\", cookies=cookies2)\n             assert self.mw.process_request(req2, self.spider) is None\n-            cookies3 = [{'name': 'foo', 'value': None}, {'name': 'key', 'value': ''}]\n-            req3 = Request('http://example.org/3', cookies=cookies3)\n+            cookies3 = [{\"name\": \"foo\", \"value\": None}, {\"name\": \"key\", \"value\": \"\"}]\n+            req3 = Request(\"http://example.org/3\", cookies=cookies3)\n             assert self.mw.process_request(req3, self.spider) is None\n             lc.check(\n-                (\"scrapy.downloadermiddlewares.cookies\",\n+                (\n+                    \"scrapy.downloadermiddlewares.cookies\",\n                     \"WARNING\",\n                     \"Invalid cookie found in request <GET http://example.org/1>:\"\n-                 \" {'value': 'bar'} ('name' is missing)\"),\n-                (\"scrapy.downloadermiddlewares.cookies\",\n+                    \" {'value': 'bar'} ('name' is missing)\",\n+                ),\n+                (\n+                    \"scrapy.downloadermiddlewares.cookies\",\n                     \"WARNING\",\n                     \"Invalid cookie found in request <GET http://example.org/2>:\"\n-                 \" {'name': 'foo'} ('value' is missing)\"),\n-                (\"scrapy.downloadermiddlewares.cookies\",\n+                    \" {'name': 'foo'} ('value' is missing)\",\n+                ),\n+                (\n+                    \"scrapy.downloadermiddlewares.cookies\",\n                     \"WARNING\",\n                     \"Invalid cookie found in request <GET http://example.org/3>:\"\n-                 \" {'name': 'foo', 'value': None} ('value' is missing)\"),\n+                    \" {'name': 'foo', 'value': None} ('value' is missing)\",\n+                ),\n             )\n-        self.assertCookieValEqual(req1.headers['Cookie'], 'key=value1')\n-        self.assertCookieValEqual(req2.headers['Cookie'], 'key=value2')\n-        self.assertCookieValEqual(req3.headers['Cookie'], 'key=')\n+        self.assertCookieValEqual(req1.headers[\"Cookie\"], \"key=value1\")\n+        self.assertCookieValEqual(req2.headers[\"Cookie\"], \"key=value2\")\n+        self.assertCookieValEqual(req3.headers[\"Cookie\"], \"key=\")\n \n     def test_primitive_type_cookies(self):\n         # Boolean\n-        req1 = Request('http://example.org', cookies={'a': True})\n+        req1 = Request(\"http://example.org\", cookies={\"a\": True})\n         assert self.mw.process_request(req1, self.spider) is None\n-        self.assertCookieValEqual(req1.headers['Cookie'], b'a=True')\n+        self.assertCookieValEqual(req1.headers[\"Cookie\"], b\"a=True\")\n \n         # Float\n-        req2 = Request('http://example.org', cookies={'a': 9.5})\n+        req2 = Request(\"http://example.org\", cookies={\"a\": 9.5})\n         assert self.mw.process_request(req2, self.spider) is None\n-        self.assertCookieValEqual(req2.headers['Cookie'], b'a=9.5')\n+        self.assertCookieValEqual(req2.headers[\"Cookie\"], b\"a=9.5\")\n \n         # Integer\n-        req3 = Request('http://example.org', cookies={'a': 10})\n+        req3 = Request(\"http://example.org\", cookies={\"a\": 10})\n         assert self.mw.process_request(req3, self.spider) is None\n-        self.assertCookieValEqual(req3.headers['Cookie'], b'a=10')\n+        self.assertCookieValEqual(req3.headers[\"Cookie\"], b\"a=10\")\n \n         # String\n-        req4 = Request('http://example.org', cookies={'a': 'b'})\n+        req4 = Request(\"http://example.org\", cookies={\"a\": \"b\"})\n         assert self.mw.process_request(req4, self.spider) is None\n-        self.assertCookieValEqual(req4.headers['Cookie'], b'a=b')\n+        self.assertCookieValEqual(req4.headers[\"Cookie\"], b\"a=b\")\n \n     def _test_cookie_redirect(\n         self,\n@@ -423,22 +462,22 @@ class CookiesMiddlewareTest(TestCase):\n         cookies1,\n         cookies2,\n     ):\n-        input_cookies = {'a': 'b'}\n+        input_cookies = {\"a\": \"b\"}\n \n         if not isinstance(source, dict):\n-            source = {'url': source}\n+            source = {\"url\": source}\n         if not isinstance(target, dict):\n-            target = {'url': target}\n-        target.setdefault('status', 301)\n+            target = {\"url\": target}\n+        target.setdefault(\"status\", 301)\n \n         request1 = Request(cookies=input_cookies, **source)\n         self.mw.process_request(request1, self.spider)\n-        cookies = request1.headers.get('Cookie')\n+        cookies = request1.headers.get(\"Cookie\")\n         self.assertEqual(cookies, b\"a=b\" if cookies1 else None)\n \n         response = Response(\n             headers={\n-                'Location': target['url'],\n+                \"Location\": target[\"url\"],\n             },\n             **target,\n         )\n@@ -455,37 +494,37 @@ class CookiesMiddlewareTest(TestCase):\n         self.assertIsInstance(request2, Request)\n \n         self.mw.process_request(request2, self.spider)\n-        cookies = request2.headers.get('Cookie')\n+        cookies = request2.headers.get(\"Cookie\")\n         self.assertEqual(cookies, b\"a=b\" if cookies2 else None)\n \n     def test_cookie_redirect_same_domain(self):\n         self._test_cookie_redirect(\n-            'https://toscrape.com',\n-            'https://toscrape.com',\n+            \"https://toscrape.com\",\n+            \"https://toscrape.com\",\n             cookies1=True,\n             cookies2=True,\n         )\n \n     def test_cookie_redirect_same_domain_forcing_get(self):\n         self._test_cookie_redirect(\n-            'https://toscrape.com',\n-            {'url': 'https://toscrape.com', 'status': 302},\n+            \"https://toscrape.com\",\n+            {\"url\": \"https://toscrape.com\", \"status\": 302},\n             cookies1=True,\n             cookies2=True,\n         )\n \n     def test_cookie_redirect_different_domain(self):\n         self._test_cookie_redirect(\n-            'https://toscrape.com',\n-            'https://example.com',\n+            \"https://toscrape.com\",\n+            \"https://example.com\",\n             cookies1=True,\n             cookies2=False,\n         )\n \n     def test_cookie_redirect_different_domain_forcing_get(self):\n         self._test_cookie_redirect(\n-            'https://toscrape.com',\n-            {'url': 'https://example.com', 'status': 302},\n+            \"https://toscrape.com\",\n+            {\"url\": \"https://example.com\", \"status\": 302},\n             cookies1=True,\n             cookies2=False,\n         )\n@@ -514,16 +553,16 @@ class CookiesMiddlewareTest(TestCase):\n                   because the middleware empties the header from every request.\n         \"\"\"\n         if not isinstance(source, dict):\n-            source = {'url': source}\n+            source = {\"url\": source}\n         if not isinstance(target, dict):\n-            target = {'url': target}\n-        target.setdefault('status', 301)\n+            target = {\"url\": target}\n+        target.setdefault(\"status\", 301)\n \n-        request1 = Request(headers={'Cookie': b'a=b'}, **source)\n+        request1 = Request(headers={\"Cookie\": b\"a=b\"}, **source)\n \n         response = Response(\n             headers={\n-                'Location': target['url'],\n+                \"Location\": target[\"url\"],\n             },\n             **target,\n         )\n@@ -535,34 +574,34 @@ class CookiesMiddlewareTest(TestCase):\n         )\n         self.assertIsInstance(request2, Request)\n \n-        cookies = request2.headers.get('Cookie')\n+        cookies = request2.headers.get(\"Cookie\")\n         self.assertEqual(cookies, b\"a=b\" if cookies2 else None)\n \n     def test_cookie_header_redirect_same_domain(self):\n         self._test_cookie_header_redirect(\n-            'https://toscrape.com',\n-            'https://toscrape.com',\n+            \"https://toscrape.com\",\n+            \"https://toscrape.com\",\n             cookies2=True,\n         )\n \n     def test_cookie_header_redirect_same_domain_forcing_get(self):\n         self._test_cookie_header_redirect(\n-            'https://toscrape.com',\n-            {'url': 'https://toscrape.com', 'status': 302},\n+            \"https://toscrape.com\",\n+            {\"url\": \"https://toscrape.com\", \"status\": 302},\n             cookies2=True,\n         )\n \n     def test_cookie_header_redirect_different_domain(self):\n         self._test_cookie_header_redirect(\n-            'https://toscrape.com',\n-            'https://example.com',\n+            \"https://toscrape.com\",\n+            \"https://example.com\",\n             cookies2=False,\n         )\n \n     def test_cookie_header_redirect_different_domain_forcing_get(self):\n         self._test_cookie_header_redirect(\n-            'https://toscrape.com',\n-            {'url': 'https://example.com', 'status': 302},\n+            \"https://toscrape.com\",\n+            {\"url\": \"https://example.com\", \"status\": 302},\n             cookies2=False,\n         )\n \n@@ -577,54 +616,54 @@ class CookiesMiddlewareTest(TestCase):\n     ):\n         input_cookies = [\n             {\n-                'name': 'a',\n-                'value': 'b',\n-                'domain': domain,\n+                \"name\": \"a\",\n+                \"value\": \"b\",\n+                \"domain\": domain,\n             }\n         ]\n \n         request1 = Request(url1, cookies=input_cookies)\n         self.mw.process_request(request1, self.spider)\n-        cookies = request1.headers.get('Cookie')\n+        cookies = request1.headers.get(\"Cookie\")\n         self.assertEqual(cookies, b\"a=b\" if cookies1 else None)\n \n         request2 = Request(url2)\n         self.mw.process_request(request2, self.spider)\n-        cookies = request2.headers.get('Cookie')\n+        cookies = request2.headers.get(\"Cookie\")\n         self.assertEqual(cookies, b\"a=b\" if cookies2 else None)\n \n     def test_user_set_cookie_domain_suffix_private(self):\n         self._test_user_set_cookie_domain_followup(\n-            'https://books.toscrape.com',\n-            'https://quotes.toscrape.com',\n-            'toscrape.com',\n+            \"https://books.toscrape.com\",\n+            \"https://quotes.toscrape.com\",\n+            \"toscrape.com\",\n             cookies1=True,\n             cookies2=True,\n         )\n \n     def test_user_set_cookie_domain_suffix_public_period(self):\n         self._test_user_set_cookie_domain_followup(\n-            'https://foo.co.uk',\n-            'https://bar.co.uk',\n-            'co.uk',\n+            \"https://foo.co.uk\",\n+            \"https://bar.co.uk\",\n+            \"co.uk\",\n             cookies1=False,\n             cookies2=False,\n         )\n \n     def test_user_set_cookie_domain_suffix_public_private(self):\n         self._test_user_set_cookie_domain_followup(\n-            'https://foo.blogspot.com',\n-            'https://bar.blogspot.com',\n-            'blogspot.com',\n+            \"https://foo.blogspot.com\",\n+            \"https://bar.blogspot.com\",\n+            \"blogspot.com\",\n             cookies1=False,\n             cookies2=False,\n         )\n \n     def test_user_set_cookie_domain_public_period(self):\n         self._test_user_set_cookie_domain_followup(\n-            'https://co.uk',\n-            'https://co.uk',\n-            'co.uk',\n+            \"https://co.uk\",\n+            \"https://co.uk\",\n+            \"co.uk\",\n             cookies1=True,\n             cookies2=True,\n         )\n@@ -642,14 +681,14 @@ class CookiesMiddlewareTest(TestCase):\n \n         input_cookies = [\n             {\n-                'name': 'a',\n-                'value': 'b',\n-                'domain': domain,\n+                \"name\": \"a\",\n+                \"value\": \"b\",\n+                \"domain\": domain,\n             }\n         ]\n \n         headers = {\n-            'Set-Cookie': _cookies_to_set_cookie_list(input_cookies),\n+            \"Set-Cookie\": _cookies_to_set_cookie_list(input_cookies),\n         }\n         response = Response(url1, status=200, headers=headers)\n         self.assertEqual(\n@@ -659,37 +698,37 @@ class CookiesMiddlewareTest(TestCase):\n \n         request2 = Request(url2)\n         self.mw.process_request(request2, self.spider)\n-        actual_cookies = request2.headers.get('Cookie')\n+        actual_cookies = request2.headers.get(\"Cookie\")\n         self.assertEqual(actual_cookies, b\"a=b\" if cookies else None)\n \n     def test_server_set_cookie_domain_suffix_private(self):\n         self._test_server_set_cookie_domain_followup(\n-            'https://books.toscrape.com',\n-            'https://quotes.toscrape.com',\n-            'toscrape.com',\n+            \"https://books.toscrape.com\",\n+            \"https://quotes.toscrape.com\",\n+            \"toscrape.com\",\n             cookies=True,\n         )\n \n     def test_server_set_cookie_domain_suffix_public_period(self):\n         self._test_server_set_cookie_domain_followup(\n-            'https://foo.co.uk',\n-            'https://bar.co.uk',\n-            'co.uk',\n+            \"https://foo.co.uk\",\n+            \"https://bar.co.uk\",\n+            \"co.uk\",\n             cookies=False,\n         )\n \n     def test_server_set_cookie_domain_suffix_public_private(self):\n         self._test_server_set_cookie_domain_followup(\n-            'https://foo.blogspot.com',\n-            'https://bar.blogspot.com',\n-            'blogspot.com',\n+            \"https://foo.blogspot.com\",\n+            \"https://bar.blogspot.com\",\n+            \"blogspot.com\",\n             cookies=False,\n         )\n \n     def test_server_set_cookie_domain_public_period(self):\n         self._test_server_set_cookie_domain_followup(\n-            'https://co.uk',\n-            'https://co.uk',\n-            'co.uk',\n+            \"https://co.uk\",\n+            \"https://co.uk\",\n+            \"co.uk\",\n             cookies=True,\n         )\n\n@@ -7,39 +7,39 @@ from tests import get_testdata\n \n \n def _test_data(formats):\n-    uncompressed_body = get_testdata('compressed', 'feed-sample1.xml')\n+    uncompressed_body = get_testdata(\"compressed\", \"feed-sample1.xml\")\n     test_responses = {}\n     for format in formats:\n-        body = get_testdata('compressed', 'feed-sample1.' + format)\n-        test_responses[format] = Response('http://foo.com/bar', body=body)\n+        body = get_testdata(\"compressed\", \"feed-sample1.\" + format)\n+        test_responses[format] = Response(\"http://foo.com/bar\", body=body)\n     return uncompressed_body, test_responses\n \n \n class DecompressionMiddlewareTest(TestCase):\n \n-    test_formats = ['tar', 'xml.bz2', 'xml.gz', 'zip']\n+    test_formats = [\"tar\", \"xml.bz2\", \"xml.gz\", \"zip\"]\n     uncompressed_body, test_responses = _test_data(test_formats)\n \n     def setUp(self):\n         self.mw = DecompressionMiddleware()\n-        self.spider = Spider('foo')\n+        self.spider = Spider(\"foo\")\n \n     def test_known_compression_formats(self):\n         for fmt in self.test_formats:\n             rsp = self.test_responses[fmt]\n             new = self.mw.process_response(None, rsp, self.spider)\n-            error_msg = f'Failed {fmt}, response type {type(new).__name__}'\n+            error_msg = f\"Failed {fmt}, response type {type(new).__name__}\"\n             assert isinstance(new, XmlResponse), error_msg\n             assert_samelines(self, new.body, self.uncompressed_body, fmt)\n \n     def test_plain_response(self):\n-        rsp = Response(url='http://test.com', body=self.uncompressed_body)\n+        rsp = Response(url=\"http://test.com\", body=self.uncompressed_body)\n         new = self.mw.process_response(None, rsp, self.spider)\n         assert new is rsp\n         assert_samelines(self, new.body, rsp.body)\n \n     def test_empty_response(self):\n-        rsp = Response(url='http://test.com', body=b'')\n+        rsp = Response(url=\"http://test.com\", body=b\"\")\n         new = self.mw.process_response(None, rsp, self.spider)\n         assert new is rsp\n         assert not rsp.body\n@@ -49,5 +49,5 @@ class DecompressionMiddlewareTest(TestCase):\n         del self.mw\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     main()\n\n@@ -8,27 +8,26 @@ from scrapy.utils.python import to_bytes\n \n \n class TestDefaultHeadersMiddleware(TestCase):\n-\n     def get_defaults_spider_mw(self):\n         crawler = get_crawler(Spider)\n-        spider = crawler._create_spider('foo')\n+        spider = crawler._create_spider(\"foo\")\n         defaults = {\n             to_bytes(k): [to_bytes(v)]\n-            for k, v in crawler.settings.get('DEFAULT_REQUEST_HEADERS').items()\n+            for k, v in crawler.settings.get(\"DEFAULT_REQUEST_HEADERS\").items()\n         }\n         return defaults, spider, DefaultHeadersMiddleware.from_crawler(crawler)\n \n     def test_process_request(self):\n         defaults, spider, mw = self.get_defaults_spider_mw()\n-        req = Request('http://www.scrapytest.org')\n+        req = Request(\"http://www.scrapytest.org\")\n         mw.process_request(req, spider)\n         self.assertEqual(req.headers, defaults)\n \n     def test_update_headers(self):\n         defaults, spider, mw = self.get_defaults_spider_mw()\n-        headers = {'Accept-Language': ['es'], 'Test-Header': ['test']}\n-        bytes_headers = {b'Accept-Language': [b'es'], b'Test-Header': [b'test']}\n-        req = Request('http://www.scrapytest.org', headers=headers)\n+        headers = {\"Accept-Language\": [\"es\"], \"Test-Header\": [\"test\"]}\n+        bytes_headers = {b\"Accept-Language\": [b\"es\"], b\"Test-Header\": [b\"test\"]}\n+        req = Request(\"http://www.scrapytest.org\", headers=headers)\n         self.assertEqual(req.headers, bytes_headers)\n \n         mw.process_request(req, spider)\n\n@@ -7,36 +7,35 @@ from scrapy.utils.test import get_crawler\n \n \n class DownloadTimeoutMiddlewareTest(unittest.TestCase):\n-\n     def get_request_spider_mw(self, settings=None):\n         crawler = get_crawler(Spider, settings)\n-        spider = crawler._create_spider('foo')\n-        request = Request('http://scrapytest.org/')\n+        spider = crawler._create_spider(\"foo\")\n+        request = Request(\"http://scrapytest.org/\")\n         return request, spider, DownloadTimeoutMiddleware.from_crawler(crawler)\n \n     def test_default_download_timeout(self):\n         req, spider, mw = self.get_request_spider_mw()\n         mw.spider_opened(spider)\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta.get('download_timeout'), 180)\n+        self.assertEqual(req.meta.get(\"download_timeout\"), 180)\n \n     def test_string_download_timeout(self):\n-        req, spider, mw = self.get_request_spider_mw({'DOWNLOAD_TIMEOUT': '20.1'})\n+        req, spider, mw = self.get_request_spider_mw({\"DOWNLOAD_TIMEOUT\": \"20.1\"})\n         mw.spider_opened(spider)\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta.get('download_timeout'), 20.1)\n+        self.assertEqual(req.meta.get(\"download_timeout\"), 20.1)\n \n     def test_spider_has_download_timeout(self):\n         req, spider, mw = self.get_request_spider_mw()\n         spider.download_timeout = 2\n         mw.spider_opened(spider)\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta.get('download_timeout'), 2)\n+        self.assertEqual(req.meta.get(\"download_timeout\"), 2)\n \n     def test_request_has_download_timeout(self):\n         req, spider, mw = self.get_request_spider_mw()\n         spider.download_timeout = 2\n         mw.spider_opened(spider)\n-        req.meta['download_timeout'] = 1\n+        req.meta[\"download_timeout\"] = 1\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta.get('download_timeout'), 1)\n+        self.assertEqual(req.meta.get(\"download_timeout\"), 1)\n\n@@ -10,108 +10,106 @@ from scrapy.spiders import Spider\n \n \n class TestSpiderLegacy(Spider):\n-    http_user = 'foo'\n-    http_pass = 'bar'\n+    http_user = \"foo\"\n+    http_pass = \"bar\"\n \n \n class TestSpider(Spider):\n-    http_user = 'foo'\n-    http_pass = 'bar'\n-    http_auth_domain = 'example.com'\n+    http_user = \"foo\"\n+    http_pass = \"bar\"\n+    http_auth_domain = \"example.com\"\n \n \n class TestSpiderAny(Spider):\n-    http_user = 'foo'\n-    http_pass = 'bar'\n+    http_user = \"foo\"\n+    http_pass = \"bar\"\n     http_auth_domain = None\n \n \n class HttpAuthMiddlewareLegacyTest(unittest.TestCase):\n-\n     def setUp(self):\n-        self.spider = TestSpiderLegacy('foo')\n+        self.spider = TestSpiderLegacy(\"foo\")\n \n     def test_auth(self):\n-        with pytest.warns(ScrapyDeprecationWarning,\n-                          match=\"Using HttpAuthMiddleware without http_auth_domain is deprecated\"):\n+        with pytest.warns(\n+            ScrapyDeprecationWarning,\n+            match=\"Using HttpAuthMiddleware without http_auth_domain is deprecated\",\n+        ):\n             mw = HttpAuthMiddleware()\n             mw.spider_opened(self.spider)\n \n         # initial request, sets the domain and sends the header\n-        req = Request('http://example.com/')\n+        req = Request(\"http://example.com/\")\n         assert mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))\n+        self.assertEqual(req.headers[\"Authorization\"], basic_auth_header(\"foo\", \"bar\"))\n \n         # subsequent request to the same domain, should send the header\n-        req = Request('http://example.com/')\n+        req = Request(\"http://example.com/\")\n         assert mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))\n+        self.assertEqual(req.headers[\"Authorization\"], basic_auth_header(\"foo\", \"bar\"))\n \n         # subsequent request to a different domain, shouldn't send the header\n-        req = Request('http://example-noauth.com/')\n+        req = Request(\"http://example-noauth.com/\")\n         assert mw.process_request(req, self.spider) is None\n-        self.assertNotIn('Authorization', req.headers)\n+        self.assertNotIn(\"Authorization\", req.headers)\n \n     def test_auth_already_set(self):\n-        with pytest.warns(ScrapyDeprecationWarning,\n-                          match=\"Using HttpAuthMiddleware without http_auth_domain is deprecated\"):\n+        with pytest.warns(\n+            ScrapyDeprecationWarning,\n+            match=\"Using HttpAuthMiddleware without http_auth_domain is deprecated\",\n+        ):\n             mw = HttpAuthMiddleware()\n             mw.spider_opened(self.spider)\n-        req = Request('http://example.com/',\n-                      headers=dict(Authorization='Digest 123'))\n+        req = Request(\"http://example.com/\", headers=dict(Authorization=\"Digest 123\"))\n         assert mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers['Authorization'], b'Digest 123')\n+        self.assertEqual(req.headers[\"Authorization\"], b\"Digest 123\")\n \n \n class HttpAuthMiddlewareTest(unittest.TestCase):\n-\n     def setUp(self):\n         self.mw = HttpAuthMiddleware()\n-        self.spider = TestSpider('foo')\n+        self.spider = TestSpider(\"foo\")\n         self.mw.spider_opened(self.spider)\n \n     def tearDown(self):\n         del self.mw\n \n     def test_no_auth(self):\n-        req = Request('http://example-noauth.com/')\n+        req = Request(\"http://example-noauth.com/\")\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertNotIn('Authorization', req.headers)\n+        self.assertNotIn(\"Authorization\", req.headers)\n \n     def test_auth_domain(self):\n-        req = Request('http://example.com/')\n+        req = Request(\"http://example.com/\")\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))\n+        self.assertEqual(req.headers[\"Authorization\"], basic_auth_header(\"foo\", \"bar\"))\n \n     def test_auth_subdomain(self):\n-        req = Request('http://foo.example.com/')\n+        req = Request(\"http://foo.example.com/\")\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))\n+        self.assertEqual(req.headers[\"Authorization\"], basic_auth_header(\"foo\", \"bar\"))\n \n     def test_auth_already_set(self):\n-        req = Request('http://example.com/',\n-                      headers=dict(Authorization='Digest 123'))\n+        req = Request(\"http://example.com/\", headers=dict(Authorization=\"Digest 123\"))\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers['Authorization'], b'Digest 123')\n+        self.assertEqual(req.headers[\"Authorization\"], b\"Digest 123\")\n \n \n class HttpAuthAnyMiddlewareTest(unittest.TestCase):\n-\n     def setUp(self):\n         self.mw = HttpAuthMiddleware()\n-        self.spider = TestSpiderAny('foo')\n+        self.spider = TestSpiderAny(\"foo\")\n         self.mw.spider_opened(self.spider)\n \n     def tearDown(self):\n         del self.mw\n \n     def test_auth(self):\n-        req = Request('http://example.com/')\n+        req = Request(\"http://example.com/\")\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))\n+        self.assertEqual(req.headers[\"Authorization\"], basic_auth_header(\"foo\", \"bar\"))\n \n     def test_auth_already_set(self):\n-        req = Request('http://example.com/',\n-                      headers=dict(Authorization='Digest 123'))\n+        req = Request(\"http://example.com/\", headers=dict(Authorization=\"Digest 123\"))\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers['Authorization'], b'Digest 123')\n+        self.assertEqual(req.headers[\"Authorization\"], b\"Digest 123\")\n\n@@ -15,36 +15,37 @@ from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware\n \n class _BaseTest(unittest.TestCase):\n \n-    storage_class = 'scrapy.extensions.httpcache.DbmCacheStorage'\n-    policy_class = 'scrapy.extensions.httpcache.RFC2616Policy'\n+    storage_class = \"scrapy.extensions.httpcache.DbmCacheStorage\"\n+    policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n \n     def setUp(self):\n         self.yesterday = email.utils.formatdate(time.time() - 86400)\n         self.today = email.utils.formatdate()\n         self.tomorrow = email.utils.formatdate(time.time() + 86400)\n         self.crawler = get_crawler(Spider)\n-        self.spider = self.crawler._create_spider('example.com')\n+        self.spider = self.crawler._create_spider(\"example.com\")\n         self.tmpdir = tempfile.mkdtemp()\n-        self.request = Request('http://www.example.com',\n-                               headers={'User-Agent': 'test'})\n-        self.response = Response('http://www.example.com',\n-                                 headers={'Content-Type': 'text/html'},\n-                                 body=b'test body',\n-                                 status=202)\n+        self.request = Request(\"http://www.example.com\", headers={\"User-Agent\": \"test\"})\n+        self.response = Response(\n+            \"http://www.example.com\",\n+            headers={\"Content-Type\": \"text/html\"},\n+            body=b\"test body\",\n+            status=202,\n+        )\n         self.crawler.stats.open_spider(self.spider)\n \n     def tearDown(self):\n-        self.crawler.stats.close_spider(self.spider, '')\n+        self.crawler.stats.close_spider(self.spider, \"\")\n         shutil.rmtree(self.tmpdir)\n \n     def _get_settings(self, **new_settings):\n         settings = {\n-            'HTTPCACHE_ENABLED': True,\n-            'HTTPCACHE_DIR': self.tmpdir,\n-            'HTTPCACHE_EXPIRATION_SECS': 1,\n-            'HTTPCACHE_IGNORE_HTTP_CODES': [],\n-            'HTTPCACHE_POLICY': self.policy_class,\n-            'HTTPCACHE_STORAGE': self.storage_class,\n+            \"HTTPCACHE_ENABLED\": True,\n+            \"HTTPCACHE_DIR\": self.tmpdir,\n+            \"HTTPCACHE_EXPIRATION_SECS\": 1,\n+            \"HTTPCACHE_IGNORE_HTTP_CODES\": [],\n+            \"HTTPCACHE_POLICY\": self.policy_class,\n+            \"HTTPCACHE_STORAGE\": self.storage_class,\n         }\n         settings.update(new_settings)\n         return Settings(settings)\n@@ -82,26 +83,32 @@ class _BaseTest(unittest.TestCase):\n \n     def assertEqualRequestButWithCacheValidators(self, request1, request2):\n         self.assertEqual(request1.url, request2.url)\n-        assert b'If-None-Match' not in request1.headers\n-        assert b'If-Modified-Since' not in request1.headers\n-        assert any(h in request2.headers for h in (b'If-None-Match', b'If-Modified-Since'))\n+        assert b\"If-None-Match\" not in request1.headers\n+        assert b\"If-Modified-Since\" not in request1.headers\n+        assert any(\n+            h in request2.headers for h in (b\"If-None-Match\", b\"If-Modified-Since\")\n+        )\n         self.assertEqual(request1.body, request2.body)\n \n     def test_dont_cache(self):\n         with self._middleware() as mw:\n-            self.request.meta['dont_cache'] = True\n+            self.request.meta[\"dont_cache\"] = True\n             mw.process_response(self.request, self.response, self.spider)\n-            self.assertEqual(mw.storage.retrieve_response(self.spider, self.request), None)\n+            self.assertEqual(\n+                mw.storage.retrieve_response(self.spider, self.request), None\n+            )\n \n         with self._middleware() as mw:\n-            self.request.meta['dont_cache'] = False\n+            self.request.meta[\"dont_cache\"] = False\n             mw.process_response(self.request, self.response, self.spider)\n             if mw.policy.should_cache_response(self.response, self.request):\n-                self.assertIsInstance(mw.storage.retrieve_response(self.spider, self.request), self.response.__class__)\n+                self.assertIsInstance(\n+                    mw.storage.retrieve_response(self.spider, self.request),\n+                    self.response.__class__,\n+                )\n \n \n class DefaultStorageTest(_BaseTest):\n-\n     def test_storage(self):\n         with self._storage() as storage:\n             request2 = self.request.copy()\n@@ -128,8 +135,8 @@ class DefaultStorageTest(_BaseTest):\n         with self._storage() as storage:\n             assert storage.retrieve_response(self.spider, self.request) is None\n             response = Response(\n-                'http://www.example.com',\n-                body=b'<!DOCTYPE html>\\n<title>.</title>',\n+                \"http://www.example.com\",\n+                body=b\"<!DOCTYPE html>\\n<title>.</title>\",\n                 status=202,\n             )\n             storage.store_response(self.spider, self.request, response)\n@@ -140,15 +147,15 @@ class DefaultStorageTest(_BaseTest):\n \n class DbmStorageTest(DefaultStorageTest):\n \n-    storage_class = 'scrapy.extensions.httpcache.DbmCacheStorage'\n+    storage_class = \"scrapy.extensions.httpcache.DbmCacheStorage\"\n \n \n class DbmStorageWithCustomDbmModuleTest(DbmStorageTest):\n \n-    dbm_module = 'tests.mocks.dummydbm'\n+    dbm_module = \"tests.mocks.dummydbm\"\n \n     def _get_settings(self, **new_settings):\n-        new_settings.setdefault('HTTPCACHE_DBM_MODULE', self.dbm_module)\n+        new_settings.setdefault(\"HTTPCACHE_DBM_MODULE\", self.dbm_module)\n         return super()._get_settings(**new_settings)\n \n     def test_custom_dbm_module_loaded(self):\n@@ -159,19 +166,18 @@ class DbmStorageWithCustomDbmModuleTest(DbmStorageTest):\n \n class FilesystemStorageTest(DefaultStorageTest):\n \n-    storage_class = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n+    storage_class = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n \n \n class FilesystemStorageGzipTest(FilesystemStorageTest):\n-\n     def _get_settings(self, **new_settings):\n-        new_settings.setdefault('HTTPCACHE_GZIP', True)\n+        new_settings.setdefault(\"HTTPCACHE_GZIP\", True)\n         return super()._get_settings(**new_settings)\n \n \n class DummyPolicyTest(_BaseTest):\n \n-    policy_class = 'scrapy.extensions.httpcache.DummyPolicy'\n+    policy_class = \"scrapy.extensions.httpcache.DummyPolicy\"\n \n     def test_middleware(self):\n         with self._middleware() as mw:\n@@ -180,31 +186,33 @@ class DummyPolicyTest(_BaseTest):\n             response = mw.process_request(self.request, self.spider)\n             assert isinstance(response, HtmlResponse)\n             self.assertEqualResponse(self.response, response)\n-            assert 'cached' in response.flags\n+            assert \"cached\" in response.flags\n \n     def test_different_request_response_urls(self):\n         with self._middleware() as mw:\n-            req = Request('http://host.com/path')\n-            res = Response('http://host2.net/test.html')\n+            req = Request(\"http://host.com/path\")\n+            res = Response(\"http://host2.net/test.html\")\n             assert mw.process_request(req, self.spider) is None\n             mw.process_response(req, res, self.spider)\n             cached = mw.process_request(req, self.spider)\n             assert isinstance(cached, Response)\n             self.assertEqualResponse(res, cached)\n-            assert 'cached' in cached.flags\n+            assert \"cached\" in cached.flags\n \n     def test_middleware_ignore_missing(self):\n         with self._middleware(HTTPCACHE_IGNORE_MISSING=True) as mw:\n-            self.assertRaises(IgnoreRequest, mw.process_request, self.request, self.spider)\n+            self.assertRaises(\n+                IgnoreRequest, mw.process_request, self.request, self.spider\n+            )\n             mw.process_response(self.request, self.response, self.spider)\n             response = mw.process_request(self.request, self.spider)\n             assert isinstance(response, HtmlResponse)\n             self.assertEqualResponse(self.response, response)\n-            assert 'cached' in response.flags\n+            assert \"cached\" in response.flags\n \n     def test_middleware_ignore_schemes(self):\n         # http responses are cached by default\n-        req, res = Request('http://test.com/'), Response('http://test.com/')\n+        req, res = Request(\"http://test.com/\"), Response(\"http://test.com/\")\n         with self._middleware() as mw:\n             assert mw.process_request(req, self.spider) is None\n             mw.process_response(req, res, self.spider)\n@@ -212,10 +220,10 @@ class DummyPolicyTest(_BaseTest):\n             cached = mw.process_request(req, self.spider)\n             assert isinstance(cached, Response), type(cached)\n             self.assertEqualResponse(res, cached)\n-            assert 'cached' in cached.flags\n+            assert \"cached\" in cached.flags\n \n         # file response is not cached by default\n-        req, res = Request('file:///tmp/t.txt'), Response('file:///tmp/t.txt')\n+        req, res = Request(\"file:///tmp/t.txt\"), Response(\"file:///tmp/t.txt\")\n         with self._middleware() as mw:\n             assert mw.process_request(req, self.spider) is None\n             mw.process_response(req, res, self.spider)\n@@ -224,7 +232,7 @@ class DummyPolicyTest(_BaseTest):\n             assert mw.process_request(req, self.spider) is None\n \n         # s3 scheme response is cached by default\n-        req, res = Request('s3://bucket/key'), Response('http://bucket/key')\n+        req, res = Request(\"s3://bucket/key\"), Response(\"http://bucket/key\")\n         with self._middleware() as mw:\n             assert mw.process_request(req, self.spider) is None\n             mw.process_response(req, res, self.spider)\n@@ -232,11 +240,11 @@ class DummyPolicyTest(_BaseTest):\n             cached = mw.process_request(req, self.spider)\n             assert isinstance(cached, Response), type(cached)\n             self.assertEqualResponse(res, cached)\n-            assert 'cached' in cached.flags\n+            assert \"cached\" in cached.flags\n \n         # ignore s3 scheme\n-        req, res = Request('s3://bucket/key2'), Response('http://bucket/key2')\n-        with self._middleware(HTTPCACHE_IGNORE_SCHEMES=['s3']) as mw:\n+        req, res = Request(\"s3://bucket/key2\"), Response(\"http://bucket/key2\")\n+        with self._middleware(HTTPCACHE_IGNORE_SCHEMES=[\"s3\"]) as mw:\n             assert mw.process_request(req, self.spider) is None\n             mw.process_response(req, res, self.spider)\n \n@@ -258,12 +266,12 @@ class DummyPolicyTest(_BaseTest):\n             response = mw.process_request(self.request, self.spider)\n             assert isinstance(response, HtmlResponse)\n             self.assertEqualResponse(self.response, response)\n-            assert 'cached' in response.flags\n+            assert \"cached\" in response.flags\n \n \n class RFC2616PolicyTest(DefaultStorageTest):\n \n-    policy_class = 'scrapy.extensions.httpcache.RFC2616Policy'\n+    policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n \n     def _process_requestresponse(self, mw, request, response):\n         result = None\n@@ -276,17 +284,18 @@ class RFC2616PolicyTest(DefaultStorageTest):\n             assert isinstance(result, Response)\n             return result\n         except Exception:\n-            print('Request', request)\n-            print('Response', response)\n-            print('Result', result)\n+            print(\"Request\", request)\n+            print(\"Response\", response)\n+            print(\"Result\", result)\n             raise\n \n     def test_request_cacheability(self):\n-        res0 = Response(self.request.url, status=200,\n-                        headers={'Expires': self.tomorrow})\n-        req0 = Request('http://example.com')\n-        req1 = req0.replace(headers={'Cache-Control': 'no-store'})\n-        req2 = req0.replace(headers={'Cache-Control': 'no-cache'})\n+        res0 = Response(\n+            self.request.url, status=200, headers={\"Expires\": self.tomorrow}\n+        )\n+        req0 = Request(\"http://example.com\")\n+        req1 = req0.replace(headers={\"Cache-Control\": \"no-store\"})\n+        req2 = req0.replace(headers={\"Cache-Control\": \"no-cache\"})\n         with self._middleware() as mw:\n             # response for a request with no-store must not be cached\n             res1 = self._process_requestresponse(mw, req1, res0)\n@@ -294,227 +303,276 @@ class RFC2616PolicyTest(DefaultStorageTest):\n             assert mw.storage.retrieve_response(self.spider, req1) is None\n             # Re-do request without no-store and expect it to be cached\n             res2 = self._process_requestresponse(mw, req0, res0)\n-            assert 'cached' not in res2.flags\n+            assert \"cached\" not in res2.flags\n             res3 = mw.process_request(req0, self.spider)\n-            assert 'cached' in res3.flags\n+            assert \"cached\" in res3.flags\n             self.assertEqualResponse(res2, res3)\n             # request with no-cache directive must not return cached response\n             # but it allows new response to be stored\n-            res0b = res0.replace(body=b'foo')\n+            res0b = res0.replace(body=b\"foo\")\n             res4 = self._process_requestresponse(mw, req2, res0b)\n             self.assertEqualResponse(res4, res0b)\n-            assert 'cached' not in res4.flags\n+            assert \"cached\" not in res4.flags\n             res5 = self._process_requestresponse(mw, req0, None)\n             self.assertEqualResponse(res5, res0b)\n-            assert 'cached' in res5.flags\n+            assert \"cached\" in res5.flags\n \n     def test_response_cacheability(self):\n         responses = [\n             # 304 is not cacheable no matter what servers sends\n             (False, 304, {}),\n-            (False, 304, {'Last-Modified': self.yesterday}),\n-            (False, 304, {'Expires': self.tomorrow}),\n-            (False, 304, {'Etag': 'bar'}),\n-            (False, 304, {'Cache-Control': 'max-age=3600'}),\n+            (False, 304, {\"Last-Modified\": self.yesterday}),\n+            (False, 304, {\"Expires\": self.tomorrow}),\n+            (False, 304, {\"Etag\": \"bar\"}),\n+            (False, 304, {\"Cache-Control\": \"max-age=3600\"}),\n             # Always obey no-store cache control\n-            (False, 200, {'Cache-Control': 'no-store'}),\n-            (False, 200, {'Cache-Control': 'no-store, max-age=300'}),  # invalid\n-            (False, 200, {'Cache-Control': 'no-store', 'Expires': self.tomorrow}),  # invalid\n+            (False, 200, {\"Cache-Control\": \"no-store\"}),\n+            (False, 200, {\"Cache-Control\": \"no-store, max-age=300\"}),  # invalid\n+            (\n+                False,\n+                200,\n+                {\"Cache-Control\": \"no-store\", \"Expires\": self.tomorrow},\n+            ),  # invalid\n             # Ignore responses missing expiration and/or validation headers\n             (False, 200, {}),\n             (False, 302, {}),\n             (False, 307, {}),\n             (False, 404, {}),\n             # Cache responses with expiration and/or validation headers\n-            (True, 200, {'Last-Modified': self.yesterday}),\n-            (True, 203, {'Last-Modified': self.yesterday}),\n-            (True, 300, {'Last-Modified': self.yesterday}),\n-            (True, 301, {'Last-Modified': self.yesterday}),\n-            (True, 308, {'Last-Modified': self.yesterday}),\n-            (True, 401, {'Last-Modified': self.yesterday}),\n-            (True, 404, {'Cache-Control': 'public, max-age=600'}),\n-            (True, 302, {'Expires': self.tomorrow}),\n-            (True, 200, {'Etag': 'foo'}),\n+            (True, 200, {\"Last-Modified\": self.yesterday}),\n+            (True, 203, {\"Last-Modified\": self.yesterday}),\n+            (True, 300, {\"Last-Modified\": self.yesterday}),\n+            (True, 301, {\"Last-Modified\": self.yesterday}),\n+            (True, 308, {\"Last-Modified\": self.yesterday}),\n+            (True, 401, {\"Last-Modified\": self.yesterday}),\n+            (True, 404, {\"Cache-Control\": \"public, max-age=600\"}),\n+            (True, 302, {\"Expires\": self.tomorrow}),\n+            (True, 200, {\"Etag\": \"foo\"}),\n         ]\n         with self._middleware() as mw:\n             for idx, (shouldcache, status, headers) in enumerate(responses):\n-                req0 = Request(f'http://example-{idx}.com')\n+                req0 = Request(f\"http://example-{idx}.com\")\n                 res0 = Response(req0.url, status=status, headers=headers)\n                 res1 = self._process_requestresponse(mw, req0, res0)\n                 res304 = res0.replace(status=304)\n-                res2 = self._process_requestresponse(mw, req0, res304 if shouldcache else res0)\n+                res2 = self._process_requestresponse(\n+                    mw, req0, res304 if shouldcache else res0\n+                )\n                 self.assertEqualResponse(res1, res0)\n                 self.assertEqualResponse(res2, res0)\n                 resc = mw.storage.retrieve_response(self.spider, req0)\n                 if shouldcache:\n                     self.assertEqualResponse(resc, res1)\n-                    assert 'cached' in res2.flags and res2.status != 304\n+                    assert \"cached\" in res2.flags and res2.status != 304\n                 else:\n                     self.assertFalse(resc)\n-                    assert 'cached' not in res2.flags\n+                    assert \"cached\" not in res2.flags\n \n         # cache unconditionally unless response contains no-store or is a 304\n         with self._middleware(HTTPCACHE_ALWAYS_STORE=True) as mw:\n             for idx, (_, status, headers) in enumerate(responses):\n-                shouldcache = 'no-store' not in headers.get('Cache-Control', '') and status != 304\n-                req0 = Request(f'http://example2-{idx}.com')\n+                shouldcache = (\n+                    \"no-store\" not in headers.get(\"Cache-Control\", \"\") and status != 304\n+                )\n+                req0 = Request(f\"http://example2-{idx}.com\")\n                 res0 = Response(req0.url, status=status, headers=headers)\n                 res1 = self._process_requestresponse(mw, req0, res0)\n                 res304 = res0.replace(status=304)\n-                res2 = self._process_requestresponse(mw, req0, res304 if shouldcache else res0)\n+                res2 = self._process_requestresponse(\n+                    mw, req0, res304 if shouldcache else res0\n+                )\n                 self.assertEqualResponse(res1, res0)\n                 self.assertEqualResponse(res2, res0)\n                 resc = mw.storage.retrieve_response(self.spider, req0)\n                 if shouldcache:\n                     self.assertEqualResponse(resc, res1)\n-                    assert 'cached' in res2.flags and res2.status != 304\n+                    assert \"cached\" in res2.flags and res2.status != 304\n                 else:\n                     self.assertFalse(resc)\n-                    assert 'cached' not in res2.flags\n+                    assert \"cached\" not in res2.flags\n \n     def test_cached_and_fresh(self):\n         sampledata = [\n-            (200, {'Date': self.yesterday, 'Expires': self.tomorrow}),\n-            (200, {'Date': self.yesterday, 'Cache-Control': 'max-age=86405'}),\n-            (200, {'Age': '299', 'Cache-Control': 'max-age=300'}),\n+            (200, {\"Date\": self.yesterday, \"Expires\": self.tomorrow}),\n+            (200, {\"Date\": self.yesterday, \"Cache-Control\": \"max-age=86405\"}),\n+            (200, {\"Age\": \"299\", \"Cache-Control\": \"max-age=300\"}),\n             # Obey max-age if present over any others\n-            (200, {'Date': self.today,\n-                   'Age': '86405',\n-                   'Cache-Control': 'max-age=' + str(86400 * 3),\n-                   'Expires': self.yesterday,\n-                   'Last-Modified': self.yesterday,\n-                   }),\n+            (\n+                200,\n+                {\n+                    \"Date\": self.today,\n+                    \"Age\": \"86405\",\n+                    \"Cache-Control\": \"max-age=\" + str(86400 * 3),\n+                    \"Expires\": self.yesterday,\n+                    \"Last-Modified\": self.yesterday,\n+                },\n+            ),\n             # obey Expires if max-age is not present\n-            (200, {'Date': self.yesterday,\n-                   'Age': '86400',\n-                   'Cache-Control': 'public',\n-                   'Expires': self.tomorrow,\n-                   'Last-Modified': self.yesterday,\n-                   }),\n+            (\n+                200,\n+                {\n+                    \"Date\": self.yesterday,\n+                    \"Age\": \"86400\",\n+                    \"Cache-Control\": \"public\",\n+                    \"Expires\": self.tomorrow,\n+                    \"Last-Modified\": self.yesterday,\n+                },\n+            ),\n             # Default missing Date header to right now\n-            (200, {'Expires': self.tomorrow}),\n+            (200, {\"Expires\": self.tomorrow}),\n             # Firefox - Expires if age is greater than 10% of (Date - Last-Modified)\n-            (200, {'Date': self.today, 'Last-Modified': self.yesterday, 'Age': str(86400 / 10 - 1)}),\n+            (\n+                200,\n+                {\n+                    \"Date\": self.today,\n+                    \"Last-Modified\": self.yesterday,\n+                    \"Age\": str(86400 / 10 - 1),\n+                },\n+            ),\n             # Firefox - Set one year maxage to permanent redirects missing expiration info\n-            (300, {}), (301, {}), (308, {}),\n+            (300, {}),\n+            (301, {}),\n+            (308, {}),\n         ]\n         with self._middleware() as mw:\n             for idx, (status, headers) in enumerate(sampledata):\n-                req0 = Request(f'http://example-{idx}.com')\n+                req0 = Request(f\"http://example-{idx}.com\")\n                 res0 = Response(req0.url, status=status, headers=headers)\n                 # cache fresh response\n                 res1 = self._process_requestresponse(mw, req0, res0)\n                 self.assertEqualResponse(res1, res0)\n-                assert 'cached' not in res1.flags\n+                assert \"cached\" not in res1.flags\n                 # return fresh cached response without network interaction\n                 res2 = self._process_requestresponse(mw, req0, None)\n                 self.assertEqualResponse(res1, res2)\n-                assert 'cached' in res2.flags\n+                assert \"cached\" in res2.flags\n                 # validate cached response if request max-age set as 0\n-                req1 = req0.replace(headers={'Cache-Control': 'max-age=0'})\n+                req1 = req0.replace(headers={\"Cache-Control\": \"max-age=0\"})\n                 res304 = res0.replace(status=304)\n                 assert mw.process_request(req1, self.spider) is None\n                 res3 = self._process_requestresponse(mw, req1, res304)\n                 self.assertEqualResponse(res1, res3)\n-                assert 'cached' in res3.flags\n+                assert \"cached\" in res3.flags\n \n     def test_cached_and_stale(self):\n         sampledata = [\n-            (200, {'Date': self.today, 'Expires': self.yesterday}),\n-            (200, {'Date': self.today, 'Expires': self.yesterday, 'Last-Modified': self.yesterday}),\n-            (200, {'Expires': self.yesterday}),\n-            (200, {'Expires': self.yesterday, 'ETag': 'foo'}),\n-            (200, {'Expires': self.yesterday, 'Last-Modified': self.yesterday}),\n-            (200, {'Expires': self.tomorrow, 'Age': '86405'}),\n-            (200, {'Cache-Control': 'max-age=86400', 'Age': '86405'}),\n+            (200, {\"Date\": self.today, \"Expires\": self.yesterday}),\n+            (\n+                200,\n+                {\n+                    \"Date\": self.today,\n+                    \"Expires\": self.yesterday,\n+                    \"Last-Modified\": self.yesterday,\n+                },\n+            ),\n+            (200, {\"Expires\": self.yesterday}),\n+            (200, {\"Expires\": self.yesterday, \"ETag\": \"foo\"}),\n+            (200, {\"Expires\": self.yesterday, \"Last-Modified\": self.yesterday}),\n+            (200, {\"Expires\": self.tomorrow, \"Age\": \"86405\"}),\n+            (200, {\"Cache-Control\": \"max-age=86400\", \"Age\": \"86405\"}),\n             # no-cache forces expiration, also revalidation if validators exists\n-            (200, {'Cache-Control': 'no-cache'}),\n-            (200, {'Cache-Control': 'no-cache', 'ETag': 'foo'}),\n-            (200, {'Cache-Control': 'no-cache', 'Last-Modified': self.yesterday}),\n-            (200, {'Cache-Control': 'no-cache,must-revalidate', 'Last-Modified': self.yesterday}),\n-            (200, {'Cache-Control': 'must-revalidate', 'Expires': self.yesterday, 'Last-Modified': self.yesterday}),\n-            (200, {'Cache-Control': 'max-age=86400,must-revalidate', 'Age': '86405'}),\n+            (200, {\"Cache-Control\": \"no-cache\"}),\n+            (200, {\"Cache-Control\": \"no-cache\", \"ETag\": \"foo\"}),\n+            (200, {\"Cache-Control\": \"no-cache\", \"Last-Modified\": self.yesterday}),\n+            (\n+                200,\n+                {\n+                    \"Cache-Control\": \"no-cache,must-revalidate\",\n+                    \"Last-Modified\": self.yesterday,\n+                },\n+            ),\n+            (\n+                200,\n+                {\n+                    \"Cache-Control\": \"must-revalidate\",\n+                    \"Expires\": self.yesterday,\n+                    \"Last-Modified\": self.yesterday,\n+                },\n+            ),\n+            (200, {\"Cache-Control\": \"max-age=86400,must-revalidate\", \"Age\": \"86405\"}),\n         ]\n         with self._middleware() as mw:\n             for idx, (status, headers) in enumerate(sampledata):\n-                req0 = Request(f'http://example-{idx}.com')\n+                req0 = Request(f\"http://example-{idx}.com\")\n                 res0a = Response(req0.url, status=status, headers=headers)\n                 # cache expired response\n                 res1 = self._process_requestresponse(mw, req0, res0a)\n                 self.assertEqualResponse(res1, res0a)\n-                assert 'cached' not in res1.flags\n+                assert \"cached\" not in res1.flags\n                 # Same request but as cached response is stale a new response must\n                 # be returned\n-                res0b = res0a.replace(body=b'bar')\n+                res0b = res0a.replace(body=b\"bar\")\n                 res2 = self._process_requestresponse(mw, req0, res0b)\n                 self.assertEqualResponse(res2, res0b)\n-                assert 'cached' not in res2.flags\n-                cc = headers.get('Cache-Control', '')\n+                assert \"cached\" not in res2.flags\n+                cc = headers.get(\"Cache-Control\", \"\")\n                 # Previous response expired too, subsequent request to same\n                 # resource must revalidate and succeed on 304 if validators\n                 # are present\n-                if 'ETag' in headers or 'Last-Modified' in headers:\n+                if \"ETag\" in headers or \"Last-Modified\" in headers:\n                     res0c = res0b.replace(status=304)\n                     res3 = self._process_requestresponse(mw, req0, res0c)\n                     self.assertEqualResponse(res3, res0b)\n-                    assert 'cached' in res3.flags\n+                    assert \"cached\" in res3.flags\n                     # get cached response on server errors unless must-revalidate\n                     # in cached response\n                     res0d = res0b.replace(status=500)\n                     res4 = self._process_requestresponse(mw, req0, res0d)\n-                    if 'must-revalidate' in cc:\n-                        assert 'cached' not in res4.flags\n+                    if \"must-revalidate\" in cc:\n+                        assert \"cached\" not in res4.flags\n                         self.assertEqualResponse(res4, res0d)\n                     else:\n-                        assert 'cached' in res4.flags\n+                        assert \"cached\" in res4.flags\n                         self.assertEqualResponse(res4, res0b)\n                 # Requests with max-stale can fetch expired cached responses\n                 # unless cached response has must-revalidate\n-                req1 = req0.replace(headers={'Cache-Control': 'max-stale'})\n+                req1 = req0.replace(headers={\"Cache-Control\": \"max-stale\"})\n                 res5 = self._process_requestresponse(mw, req1, res0b)\n                 self.assertEqualResponse(res5, res0b)\n-                if 'no-cache' in cc or 'must-revalidate' in cc:\n-                    assert 'cached' not in res5.flags\n+                if \"no-cache\" in cc or \"must-revalidate\" in cc:\n+                    assert \"cached\" not in res5.flags\n                 else:\n-                    assert 'cached' in res5.flags\n+                    assert \"cached\" in res5.flags\n \n     def test_process_exception(self):\n         with self._middleware() as mw:\n-            res0 = Response(self.request.url, headers={'Expires': self.yesterday})\n+            res0 = Response(self.request.url, headers={\"Expires\": self.yesterday})\n             req0 = Request(self.request.url)\n             self._process_requestresponse(mw, req0, res0)\n             for e in mw.DOWNLOAD_EXCEPTIONS:\n                 # Simulate encountering an error on download attempts\n                 assert mw.process_request(req0, self.spider) is None\n-                res1 = mw.process_exception(req0, e('foo'), self.spider)\n+                res1 = mw.process_exception(req0, e(\"foo\"), self.spider)\n                 # Use cached response as recovery\n-                assert 'cached' in res1.flags\n+                assert \"cached\" in res1.flags\n                 self.assertEqualResponse(res0, res1)\n             # Do not use cached response for unhandled exceptions\n             mw.process_request(req0, self.spider)\n-            assert mw.process_exception(req0, Exception('foo'), self.spider) is None\n+            assert mw.process_exception(req0, Exception(\"foo\"), self.spider) is None\n \n     def test_ignore_response_cache_controls(self):\n         sampledata = [\n-            (200, {'Date': self.yesterday, 'Expires': self.tomorrow}),\n-            (200, {'Date': self.yesterday, 'Cache-Control': 'no-store,max-age=86405'}),\n-            (200, {'Age': '299', 'Cache-Control': 'max-age=300,no-cache'}),\n-            (300, {'Cache-Control': 'no-cache'}),\n-            (200, {'Expires': self.tomorrow, 'Cache-Control': 'no-store'}),\n+            (200, {\"Date\": self.yesterday, \"Expires\": self.tomorrow}),\n+            (200, {\"Date\": self.yesterday, \"Cache-Control\": \"no-store,max-age=86405\"}),\n+            (200, {\"Age\": \"299\", \"Cache-Control\": \"max-age=300,no-cache\"}),\n+            (300, {\"Cache-Control\": \"no-cache\"}),\n+            (200, {\"Expires\": self.tomorrow, \"Cache-Control\": \"no-store\"}),\n         ]\n-        with self._middleware(HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS=['no-cache', 'no-store']) as mw:\n+        with self._middleware(\n+            HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS=[\"no-cache\", \"no-store\"]\n+        ) as mw:\n             for idx, (status, headers) in enumerate(sampledata):\n-                req0 = Request(f'http://example-{idx}.com')\n+                req0 = Request(f\"http://example-{idx}.com\")\n                 res0 = Response(req0.url, status=status, headers=headers)\n                 # cache fresh response\n                 res1 = self._process_requestresponse(mw, req0, res0)\n                 self.assertEqualResponse(res1, res0)\n-                assert 'cached' not in res1.flags\n+                assert \"cached\" not in res1.flags\n                 # return fresh cached response without network interaction\n                 res2 = self._process_requestresponse(mw, req0, None)\n                 self.assertEqualResponse(res1, res2)\n-                assert 'cached' in res2.flags\n+                assert \"cached\" in res2.flags\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -7,35 +7,40 @@ from warnings import catch_warnings\n from w3lib.encoding import resolve_encoding\n from scrapy.spiders import Spider\n from scrapy.http import Response, Request, HtmlResponse\n-from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware, ACCEPTED_ENCODINGS\n+from scrapy.downloadermiddlewares.httpcompression import (\n+    HttpCompressionMiddleware,\n+    ACCEPTED_ENCODINGS,\n+)\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.gz import gunzip\n from scrapy.utils.test import get_crawler\n from tests import tests_datadir\n \n-SAMPLEDIR = Path(tests_datadir, 'compressed')\n+SAMPLEDIR = Path(tests_datadir, \"compressed\")\n \n FORMAT = {\n-    'gzip': ('html-gzip.bin', 'gzip'),\n-    'x-gzip': ('html-gzip.bin', 'gzip'),\n-    'rawdeflate': ('html-rawdeflate.bin', 'deflate'),\n-    'zlibdeflate': ('html-zlibdeflate.bin', 'deflate'),\n-    'br': ('html-br.bin', 'br'),\n+    \"gzip\": (\"html-gzip.bin\", \"gzip\"),\n+    \"x-gzip\": (\"html-gzip.bin\", \"gzip\"),\n+    \"rawdeflate\": (\"html-rawdeflate.bin\", \"deflate\"),\n+    \"zlibdeflate\": (\"html-zlibdeflate.bin\", \"deflate\"),\n+    \"br\": (\"html-br.bin\", \"br\"),\n     # $ zstd raw.html --content-size -o html-zstd-static-content-size.bin\n-    'zstd-static-content-size': ('html-zstd-static-content-size.bin', 'zstd'),\n+    \"zstd-static-content-size\": (\"html-zstd-static-content-size.bin\", \"zstd\"),\n     # $ zstd raw.html --no-content-size -o html-zstd-static-no-content-size.bin\n-    'zstd-static-no-content-size': ('html-zstd-static-no-content-size.bin', 'zstd'),\n+    \"zstd-static-no-content-size\": (\"html-zstd-static-no-content-size.bin\", \"zstd\"),\n     # $ cat raw.html | zstd -o html-zstd-streaming-no-content-size.bin\n-    'zstd-streaming-no-content-size': ('html-zstd-streaming-no-content-size.bin', 'zstd'),\n+    \"zstd-streaming-no-content-size\": (\n+        \"html-zstd-streaming-no-content-size.bin\",\n+        \"zstd\",\n+    ),\n }\n \n \n class HttpCompressionTest(TestCase):\n-\n     def setUp(self):\n         self.crawler = get_crawler(Spider)\n-        self.spider = self.crawler._create_spider('scrapytest.org')\n+        self.spider = self.crawler._create_spider(\"scrapytest.org\")\n         self.mw = HttpCompressionMiddleware.from_crawler(self.crawler)\n         self.crawler.stats.open_spider(self.spider)\n \n@@ -48,90 +53,93 @@ class HttpCompressionTest(TestCase):\n         body = (SAMPLEDIR / samplefile).read_bytes()\n \n         headers = {\n-            'Server': 'Yaws/1.49 Yet Another Web Server',\n-            'Date': 'Sun, 08 Mar 2009 00:41:03 GMT',\n-            'Content-Length': len(body),\n-            'Content-Type': 'text/html',\n-            'Content-Encoding': contentencoding,\n+            \"Server\": \"Yaws/1.49 Yet Another Web Server\",\n+            \"Date\": \"Sun, 08 Mar 2009 00:41:03 GMT\",\n+            \"Content-Length\": len(body),\n+            \"Content-Type\": \"text/html\",\n+            \"Content-Encoding\": contentencoding,\n         }\n \n-        response = Response('http://scrapytest.org/', body=body, headers=headers)\n-        response.request = Request('http://scrapytest.org', headers={'Accept-Encoding': 'gzip, deflate'})\n+        response = Response(\"http://scrapytest.org/\", body=body, headers=headers)\n+        response.request = Request(\n+            \"http://scrapytest.org\", headers={\"Accept-Encoding\": \"gzip, deflate\"}\n+        )\n         return response\n \n     def assertStatsEqual(self, key, value):\n         self.assertEqual(\n             self.crawler.stats.get_value(key, spider=self.spider),\n             value,\n-            str(self.crawler.stats.get_stats(self.spider))\n+            str(self.crawler.stats.get_stats(self.spider)),\n         )\n \n     def test_setting_false_compression_enabled(self):\n         self.assertRaises(\n             NotConfigured,\n             HttpCompressionMiddleware.from_crawler,\n-            get_crawler(settings_dict={'COMPRESSION_ENABLED': False})\n+            get_crawler(settings_dict={\"COMPRESSION_ENABLED\": False}),\n         )\n \n     def test_setting_default_compression_enabled(self):\n         self.assertIsInstance(\n             HttpCompressionMiddleware.from_crawler(get_crawler()),\n-            HttpCompressionMiddleware\n+            HttpCompressionMiddleware,\n         )\n \n     def test_setting_true_compression_enabled(self):\n         self.assertIsInstance(\n             HttpCompressionMiddleware.from_crawler(\n-                get_crawler(settings_dict={'COMPRESSION_ENABLED': True})\n+                get_crawler(settings_dict={\"COMPRESSION_ENABLED\": True})\n             ),\n-            HttpCompressionMiddleware\n+            HttpCompressionMiddleware,\n         )\n \n     def test_process_request(self):\n-        request = Request('http://scrapytest.org')\n-        assert 'Accept-Encoding' not in request.headers\n+        request = Request(\"http://scrapytest.org\")\n+        assert \"Accept-Encoding\" not in request.headers\n         self.mw.process_request(request, self.spider)\n-        self.assertEqual(request.headers.get('Accept-Encoding'),\n-                         b', '.join(ACCEPTED_ENCODINGS))\n+        self.assertEqual(\n+            request.headers.get(\"Accept-Encoding\"), b\", \".join(ACCEPTED_ENCODINGS)\n+        )\n \n     def test_process_response_gzip(self):\n-        response = self._getresponse('gzip')\n+        response = self._getresponse(\"gzip\")\n         request = response.request\n \n-        self.assertEqual(response.headers['Content-Encoding'], b'gzip')\n+        self.assertEqual(response.headers[\"Content-Encoding\"], b\"gzip\")\n         newresponse = self.mw.process_response(request, response, self.spider)\n         assert newresponse is not response\n-        assert newresponse.body.startswith(b'<!DOCTYPE')\n-        assert 'Content-Encoding' not in newresponse.headers\n-        self.assertStatsEqual('httpcompression/response_count', 1)\n-        self.assertStatsEqual('httpcompression/response_bytes', 74837)\n+        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n+        assert \"Content-Encoding\" not in newresponse.headers\n+        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n+        self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n \n     def test_process_response_gzip_no_stats(self):\n         mw = HttpCompressionMiddleware()\n-        response = self._getresponse('gzip')\n+        response = self._getresponse(\"gzip\")\n         request = response.request\n \n-        self.assertEqual(response.headers['Content-Encoding'], b'gzip')\n+        self.assertEqual(response.headers[\"Content-Encoding\"], b\"gzip\")\n         newresponse = mw.process_response(request, response, self.spider)\n         self.assertEqual(mw.stats, None)\n         assert newresponse is not response\n-        assert newresponse.body.startswith(b'<!DOCTYPE')\n-        assert 'Content-Encoding' not in newresponse.headers\n+        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n+        assert \"Content-Encoding\" not in newresponse.headers\n \n     def test_process_response_br(self):\n         try:\n             import brotli  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n-        response = self._getresponse('br')\n+        response = self._getresponse(\"br\")\n         request = response.request\n-        self.assertEqual(response.headers['Content-Encoding'], b'br')\n+        self.assertEqual(response.headers[\"Content-Encoding\"], b\"br\")\n         newresponse = self.mw.process_response(request, response, self.spider)\n         assert newresponse is not response\n         assert newresponse.body.startswith(b\"<!DOCTYPE\")\n-        assert 'Content-Encoding' not in newresponse.headers\n-        self.assertStatsEqual('httpcompression/response_count', 1)\n-        self.assertStatsEqual('httpcompression/response_bytes', 74837)\n+        assert \"Content-Encoding\" not in newresponse.headers\n+        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n+        self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n \n     def test_process_response_zstd(self):\n         try:\n@@ -140,11 +148,11 @@ class HttpCompressionTest(TestCase):\n             raise SkipTest(\"no zstd support (zstandard)\")\n         raw_content = None\n         for check_key in FORMAT:\n-            if not check_key.startswith('zstd-'):\n+            if not check_key.startswith(\"zstd-\"):\n                 continue\n             response = self._getresponse(check_key)\n             request = response.request\n-            self.assertEqual(response.headers['Content-Encoding'], b'zstd')\n+            self.assertEqual(response.headers[\"Content-Encoding\"], b\"zstd\")\n             newresponse = self.mw.process_response(request, response, self.spider)\n             if raw_content is None:\n                 raw_content = newresponse.body\n@@ -152,145 +160,159 @@ class HttpCompressionTest(TestCase):\n                 assert raw_content == newresponse.body\n             assert newresponse is not response\n             assert newresponse.body.startswith(b\"<!DOCTYPE\")\n-            assert 'Content-Encoding' not in newresponse.headers\n+            assert \"Content-Encoding\" not in newresponse.headers\n \n     def test_process_response_rawdeflate(self):\n-        response = self._getresponse('rawdeflate')\n+        response = self._getresponse(\"rawdeflate\")\n         request = response.request\n \n-        self.assertEqual(response.headers['Content-Encoding'], b'deflate')\n+        self.assertEqual(response.headers[\"Content-Encoding\"], b\"deflate\")\n         newresponse = self.mw.process_response(request, response, self.spider)\n         assert newresponse is not response\n-        assert newresponse.body.startswith(b'<!DOCTYPE')\n-        assert 'Content-Encoding' not in newresponse.headers\n-        self.assertStatsEqual('httpcompression/response_count', 1)\n-        self.assertStatsEqual('httpcompression/response_bytes', 74840)\n+        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n+        assert \"Content-Encoding\" not in newresponse.headers\n+        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n+        self.assertStatsEqual(\"httpcompression/response_bytes\", 74840)\n \n     def test_process_response_zlibdelate(self):\n-        response = self._getresponse('zlibdeflate')\n+        response = self._getresponse(\"zlibdeflate\")\n         request = response.request\n \n-        self.assertEqual(response.headers['Content-Encoding'], b'deflate')\n+        self.assertEqual(response.headers[\"Content-Encoding\"], b\"deflate\")\n         newresponse = self.mw.process_response(request, response, self.spider)\n         assert newresponse is not response\n-        assert newresponse.body.startswith(b'<!DOCTYPE')\n-        assert 'Content-Encoding' not in newresponse.headers\n-        self.assertStatsEqual('httpcompression/response_count', 1)\n-        self.assertStatsEqual('httpcompression/response_bytes', 74840)\n+        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n+        assert \"Content-Encoding\" not in newresponse.headers\n+        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n+        self.assertStatsEqual(\"httpcompression/response_bytes\", 74840)\n \n     def test_process_response_plain(self):\n-        response = Response('http://scrapytest.org', body=b'<!DOCTYPE...')\n-        request = Request('http://scrapytest.org')\n+        response = Response(\"http://scrapytest.org\", body=b\"<!DOCTYPE...\")\n+        request = Request(\"http://scrapytest.org\")\n \n-        assert not response.headers.get('Content-Encoding')\n+        assert not response.headers.get(\"Content-Encoding\")\n         newresponse = self.mw.process_response(request, response, self.spider)\n         assert newresponse is response\n-        assert newresponse.body.startswith(b'<!DOCTYPE')\n-        self.assertStatsEqual('httpcompression/response_count', None)\n-        self.assertStatsEqual('httpcompression/response_bytes', None)\n+        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n+        self.assertStatsEqual(\"httpcompression/response_count\", None)\n+        self.assertStatsEqual(\"httpcompression/response_bytes\", None)\n \n     def test_multipleencodings(self):\n-        response = self._getresponse('gzip')\n-        response.headers['Content-Encoding'] = ['uuencode', 'gzip']\n+        response = self._getresponse(\"gzip\")\n+        response.headers[\"Content-Encoding\"] = [\"uuencode\", \"gzip\"]\n         request = response.request\n         newresponse = self.mw.process_response(request, response, self.spider)\n         assert newresponse is not response\n-        self.assertEqual(newresponse.headers.getlist('Content-Encoding'), [b'uuencode'])\n+        self.assertEqual(newresponse.headers.getlist(\"Content-Encoding\"), [b\"uuencode\"])\n \n     def test_process_response_encoding_inside_body(self):\n         headers = {\n-            'Content-Type': 'text/html',\n-            'Content-Encoding': 'gzip',\n+            \"Content-Type\": \"text/html\",\n+            \"Content-Encoding\": \"gzip\",\n         }\n         f = BytesIO()\n-        plainbody = (b'<html><head><title>Some page</title>'\n-                     b'<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">')\n-        zf = GzipFile(fileobj=f, mode='wb')\n+        plainbody = (\n+            b\"<html><head><title>Some page</title>\"\n+            b'<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">'\n+        )\n+        zf = GzipFile(fileobj=f, mode=\"wb\")\n         zf.write(plainbody)\n         zf.close()\n-        response = Response(\"http;//www.example.com/\", headers=headers, body=f.getvalue())\n+        response = Response(\n+            \"http;//www.example.com/\", headers=headers, body=f.getvalue()\n+        )\n         request = Request(\"http://www.example.com/\")\n \n         newresponse = self.mw.process_response(request, response, self.spider)\n         assert isinstance(newresponse, HtmlResponse)\n         self.assertEqual(newresponse.body, plainbody)\n-        self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))\n-        self.assertStatsEqual('httpcompression/response_count', 1)\n-        self.assertStatsEqual('httpcompression/response_bytes', 104)\n+        self.assertEqual(newresponse.encoding, resolve_encoding(\"gb2312\"))\n+        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n+        self.assertStatsEqual(\"httpcompression/response_bytes\", 104)\n \n     def test_process_response_force_recalculate_encoding(self):\n         headers = {\n-            'Content-Type': 'text/html',\n-            'Content-Encoding': 'gzip',\n+            \"Content-Type\": \"text/html\",\n+            \"Content-Encoding\": \"gzip\",\n         }\n         f = BytesIO()\n-        plainbody = (b'<html><head><title>Some page</title>'\n-                     b'<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">')\n-        zf = GzipFile(fileobj=f, mode='wb')\n+        plainbody = (\n+            b\"<html><head><title>Some page</title>\"\n+            b'<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">'\n+        )\n+        zf = GzipFile(fileobj=f, mode=\"wb\")\n         zf.write(plainbody)\n         zf.close()\n-        response = HtmlResponse(\"http;//www.example.com/page.html\", headers=headers, body=f.getvalue())\n+        response = HtmlResponse(\n+            \"http;//www.example.com/page.html\", headers=headers, body=f.getvalue()\n+        )\n         request = Request(\"http://www.example.com/\")\n \n         newresponse = self.mw.process_response(request, response, self.spider)\n         assert isinstance(newresponse, HtmlResponse)\n         self.assertEqual(newresponse.body, plainbody)\n-        self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))\n-        self.assertStatsEqual('httpcompression/response_count', 1)\n-        self.assertStatsEqual('httpcompression/response_bytes', 104)\n+        self.assertEqual(newresponse.encoding, resolve_encoding(\"gb2312\"))\n+        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n+        self.assertStatsEqual(\"httpcompression/response_bytes\", 104)\n \n     def test_process_response_no_content_type_header(self):\n         headers = {\n-            'Content-Encoding': 'identity',\n+            \"Content-Encoding\": \"identity\",\n         }\n-        plainbody = (b'<html><head><title>Some page</title>'\n-                     b'<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">')\n-        respcls = responsetypes.from_args(url=\"http://www.example.com/index\", headers=headers, body=plainbody)\n-        response = respcls(\"http://www.example.com/index\", headers=headers, body=plainbody)\n+        plainbody = (\n+            b\"<html><head><title>Some page</title>\"\n+            b'<meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">'\n+        )\n+        respcls = responsetypes.from_args(\n+            url=\"http://www.example.com/index\", headers=headers, body=plainbody\n+        )\n+        response = respcls(\n+            \"http://www.example.com/index\", headers=headers, body=plainbody\n+        )\n         request = Request(\"http://www.example.com/index\")\n \n         newresponse = self.mw.process_response(request, response, self.spider)\n         assert isinstance(newresponse, respcls)\n         self.assertEqual(newresponse.body, plainbody)\n-        self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))\n-        self.assertStatsEqual('httpcompression/response_count', 1)\n-        self.assertStatsEqual('httpcompression/response_bytes', 104)\n+        self.assertEqual(newresponse.encoding, resolve_encoding(\"gb2312\"))\n+        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n+        self.assertStatsEqual(\"httpcompression/response_bytes\", 104)\n \n     def test_process_response_gzipped_contenttype(self):\n-        response = self._getresponse('gzip')\n-        response.headers['Content-Type'] = 'application/gzip'\n+        response = self._getresponse(\"gzip\")\n+        response.headers[\"Content-Type\"] = \"application/gzip\"\n         request = response.request\n \n         newresponse = self.mw.process_response(request, response, self.spider)\n         self.assertIsNot(newresponse, response)\n-        self.assertTrue(newresponse.body.startswith(b'<!DOCTYPE'))\n-        self.assertNotIn('Content-Encoding', newresponse.headers)\n-        self.assertStatsEqual('httpcompression/response_count', 1)\n-        self.assertStatsEqual('httpcompression/response_bytes', 74837)\n+        self.assertTrue(newresponse.body.startswith(b\"<!DOCTYPE\"))\n+        self.assertNotIn(\"Content-Encoding\", newresponse.headers)\n+        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n+        self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n \n     def test_process_response_gzip_app_octetstream_contenttype(self):\n-        response = self._getresponse('gzip')\n-        response.headers['Content-Type'] = 'application/octet-stream'\n+        response = self._getresponse(\"gzip\")\n+        response.headers[\"Content-Type\"] = \"application/octet-stream\"\n         request = response.request\n \n         newresponse = self.mw.process_response(request, response, self.spider)\n         self.assertIsNot(newresponse, response)\n-        self.assertTrue(newresponse.body.startswith(b'<!DOCTYPE'))\n-        self.assertNotIn('Content-Encoding', newresponse.headers)\n-        self.assertStatsEqual('httpcompression/response_count', 1)\n-        self.assertStatsEqual('httpcompression/response_bytes', 74837)\n+        self.assertTrue(newresponse.body.startswith(b\"<!DOCTYPE\"))\n+        self.assertNotIn(\"Content-Encoding\", newresponse.headers)\n+        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n+        self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n \n     def test_process_response_gzip_binary_octetstream_contenttype(self):\n-        response = self._getresponse('x-gzip')\n-        response.headers['Content-Type'] = 'binary/octet-stream'\n+        response = self._getresponse(\"x-gzip\")\n+        response.headers[\"Content-Type\"] = \"binary/octet-stream\"\n         request = response.request\n \n         newresponse = self.mw.process_response(request, response, self.spider)\n         self.assertIsNot(newresponse, response)\n-        self.assertTrue(newresponse.body.startswith(b'<!DOCTYPE'))\n-        self.assertNotIn('Content-Encoding', newresponse.headers)\n-        self.assertStatsEqual('httpcompression/response_count', 1)\n-        self.assertStatsEqual('httpcompression/response_bytes', 74837)\n+        self.assertTrue(newresponse.body.startswith(b\"<!DOCTYPE\"))\n+        self.assertNotIn(\"Content-Encoding\", newresponse.headers)\n+        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n+        self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n \n     def test_process_response_gzipped_gzip_file(self):\n         \"\"\"Test that a gzip Content-Encoded .gz file is gunzipped\n@@ -298,8 +320,8 @@ class HttpCompressionTest(TestCase):\n         to upper layers.\n         \"\"\"\n         headers = {\n-            'Content-Type': 'application/gzip',\n-            'Content-Encoding': 'gzip',\n+            \"Content-Type\": \"application/gzip\",\n+            \"Content-Encoding\": \"gzip\",\n         }\n         # build a gzipped file (here, a sitemap)\n         f = BytesIO()\n@@ -318,42 +340,42 @@ class HttpCompressionTest(TestCase):\n     <priority>0.8</priority>\n   </url>\n </urlset>\"\"\"\n-        gz_file = GzipFile(fileobj=f, mode='wb')\n+        gz_file = GzipFile(fileobj=f, mode=\"wb\")\n         gz_file.write(plainbody)\n         gz_file.close()\n \n         # build a gzipped response body containing this gzipped file\n         r = BytesIO()\n-        gz_resp = GzipFile(fileobj=r, mode='wb')\n+        gz_resp = GzipFile(fileobj=r, mode=\"wb\")\n         gz_resp.write(f.getvalue())\n         gz_resp.close()\n \n-        response = Response(\"http;//www.example.com/\", headers=headers, body=r.getvalue())\n+        response = Response(\n+            \"http;//www.example.com/\", headers=headers, body=r.getvalue()\n+        )\n         request = Request(\"http://www.example.com/\")\n \n         newresponse = self.mw.process_response(request, response, self.spider)\n         self.assertEqual(gunzip(newresponse.body), plainbody)\n-        self.assertStatsEqual('httpcompression/response_count', 1)\n-        self.assertStatsEqual('httpcompression/response_bytes', 230)\n+        self.assertStatsEqual(\"httpcompression/response_count\", 1)\n+        self.assertStatsEqual(\"httpcompression/response_bytes\", 230)\n \n     def test_process_response_head_request_no_decode_required(self):\n-        response = self._getresponse('gzip')\n-        response.headers['Content-Type'] = 'application/gzip'\n+        response = self._getresponse(\"gzip\")\n+        response.headers[\"Content-Type\"] = \"application/gzip\"\n         request = response.request\n-        request.method = 'HEAD'\n+        request.method = \"HEAD\"\n         response = response.replace(body=None)\n         newresponse = self.mw.process_response(request, response, self.spider)\n         self.assertIs(newresponse, response)\n-        self.assertEqual(response.body, b'')\n-        self.assertStatsEqual('httpcompression/response_count', None)\n-        self.assertStatsEqual('httpcompression/response_bytes', None)\n+        self.assertEqual(response.body, b\"\")\n+        self.assertStatsEqual(\"httpcompression/response_count\", None)\n+        self.assertStatsEqual(\"httpcompression/response_bytes\", None)\n \n \n class HttpCompressionSubclassTest(TestCase):\n-\n     def test_init_missing_stats(self):\n         class HttpCompressionMiddlewareSubclass(HttpCompressionMiddleware):\n-\n             def __init__(self):\n                 super().__init__()\n \n@@ -361,7 +383,8 @@ class HttpCompressionSubclassTest(TestCase):\n         with catch_warnings(record=True) as caught_warnings:\n             HttpCompressionMiddlewareSubclass.from_crawler(crawler)\n         messages = tuple(\n-            str(warning.message) for warning in caught_warnings\n+            str(warning.message)\n+            for warning in caught_warnings\n             if warning.category is ScrapyDeprecationWarning\n         )\n         self.assertEqual(\n@@ -372,5 +395,5 @@ class HttpCompressionSubclassTest(TestCase):\n                     \"their '__init__' method to support a 'stats' parameter \"\n                     \"or reimplement the 'from_crawler' method.\"\n                 ),\n-            )\n+            ),\n         )\n\n@@ -9,7 +9,7 @@ from scrapy.http import Request\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n-spider = Spider('foo')\n+spider = Spider(\"foo\")\n \n \n class TestHttpProxyMiddleware(TestCase):\n@@ -23,184 +23,197 @@ class TestHttpProxyMiddleware(TestCase):\n         os.environ = self._oldenv\n \n     def test_not_enabled(self):\n-        crawler = get_crawler(Spider, {'HTTPPROXY_ENABLED': False})\n+        crawler = get_crawler(Spider, {\"HTTPPROXY_ENABLED\": False})\n         with pytest.raises(NotConfigured):\n             HttpProxyMiddleware.from_crawler(crawler)\n \n     def test_no_environment_proxies(self):\n-        os.environ = {'dummy_proxy': 'reset_env_and_do_not_raise'}\n+        os.environ = {\"dummy_proxy\": \"reset_env_and_do_not_raise\"}\n         mw = HttpProxyMiddleware()\n \n-        for url in ('http://e.com', 'https://e.com', 'file:///tmp/a'):\n+        for url in (\"http://e.com\", \"https://e.com\", \"file:///tmp/a\"):\n             req = Request(url)\n             assert mw.process_request(req, spider) is None\n             self.assertEqual(req.url, url)\n             self.assertEqual(req.meta, {})\n \n     def test_environment_proxies(self):\n-        os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'\n-        os.environ['https_proxy'] = https_proxy = 'http://proxy.for.https:8080'\n-        os.environ.pop('file_proxy', None)\n+        os.environ[\"http_proxy\"] = http_proxy = \"https://proxy.for.http:3128\"\n+        os.environ[\"https_proxy\"] = https_proxy = \"http://proxy.for.https:8080\"\n+        os.environ.pop(\"file_proxy\", None)\n         mw = HttpProxyMiddleware()\n \n         for url, proxy in [\n-            ('http://e.com', http_proxy),\n-            ('https://e.com', https_proxy),\n-            ('file://tmp/a', None),\n+            (\"http://e.com\", http_proxy),\n+            (\"https://e.com\", https_proxy),\n+            (\"file://tmp/a\", None),\n         ]:\n             req = Request(url)\n             assert mw.process_request(req, spider) is None\n             self.assertEqual(req.url, url)\n-            self.assertEqual(req.meta.get('proxy'), proxy)\n+            self.assertEqual(req.meta.get(\"proxy\"), proxy)\n \n     def test_proxy_precedence_meta(self):\n-        os.environ['http_proxy'] = 'https://proxy.com'\n+        os.environ[\"http_proxy\"] = \"https://proxy.com\"\n         mw = HttpProxyMiddleware()\n-        req = Request('http://scrapytest.org', meta={'proxy': 'https://new.proxy:3128'})\n+        req = Request(\"http://scrapytest.org\", meta={\"proxy\": \"https://new.proxy:3128\"})\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta, {'proxy': 'https://new.proxy:3128'})\n+        self.assertEqual(req.meta, {\"proxy\": \"https://new.proxy:3128\"})\n \n     def test_proxy_auth(self):\n-        os.environ['http_proxy'] = 'https://user:pass@proxy:3128'\n+        os.environ[\"http_proxy\"] = \"https://user:pass@proxy:3128\"\n         mw = HttpProxyMiddleware()\n-        req = Request('http://scrapytest.org')\n+        req = Request(\"http://scrapytest.org\")\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')\n-        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjpwYXNz')\n+        self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n+        self.assertEqual(req.headers.get(\"Proxy-Authorization\"), b\"Basic dXNlcjpwYXNz\")\n         # proxy from request.meta\n-        req = Request('http://scrapytest.org', meta={'proxy': 'https://username:password@proxy:3128'})\n+        req = Request(\n+            \"http://scrapytest.org\",\n+            meta={\"proxy\": \"https://username:password@proxy:3128\"},\n+        )\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')\n-        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcm5hbWU6cGFzc3dvcmQ=')\n+        self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n+        self.assertEqual(\n+            req.headers.get(\"Proxy-Authorization\"), b\"Basic dXNlcm5hbWU6cGFzc3dvcmQ=\"\n+        )\n \n     def test_proxy_auth_empty_passwd(self):\n-        os.environ['http_proxy'] = 'https://user:@proxy:3128'\n+        os.environ[\"http_proxy\"] = \"https://user:@proxy:3128\"\n         mw = HttpProxyMiddleware()\n-        req = Request('http://scrapytest.org')\n+        req = Request(\"http://scrapytest.org\")\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')\n-        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjo=')\n+        self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n+        self.assertEqual(req.headers.get(\"Proxy-Authorization\"), b\"Basic dXNlcjo=\")\n         # proxy from request.meta\n-        req = Request('http://scrapytest.org', meta={'proxy': 'https://username:@proxy:3128'})\n+        req = Request(\n+            \"http://scrapytest.org\", meta={\"proxy\": \"https://username:@proxy:3128\"}\n+        )\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')\n-        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcm5hbWU6')\n+        self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n+        self.assertEqual(req.headers.get(\"Proxy-Authorization\"), b\"Basic dXNlcm5hbWU6\")\n \n     def test_proxy_auth_encoding(self):\n         # utf-8 encoding\n-        os.environ['http_proxy'] = 'https://m\\u00E1n:pass@proxy:3128'\n-        mw = HttpProxyMiddleware(auth_encoding='utf-8')\n-        req = Request('http://scrapytest.org')\n+        os.environ[\"http_proxy\"] = \"https://m\\u00E1n:pass@proxy:3128\"\n+        mw = HttpProxyMiddleware(auth_encoding=\"utf-8\")\n+        req = Request(\"http://scrapytest.org\")\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')\n-        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic bcOhbjpwYXNz')\n+        self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n+        self.assertEqual(req.headers.get(\"Proxy-Authorization\"), b\"Basic bcOhbjpwYXNz\")\n \n         # proxy from request.meta\n-        req = Request('http://scrapytest.org', meta={'proxy': 'https://\\u00FCser:pass@proxy:3128'})\n+        req = Request(\n+            \"http://scrapytest.org\", meta={\"proxy\": \"https://\\u00FCser:pass@proxy:3128\"}\n+        )\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')\n-        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic w7xzZXI6cGFzcw==')\n+        self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n+        self.assertEqual(\n+            req.headers.get(\"Proxy-Authorization\"), b\"Basic w7xzZXI6cGFzcw==\"\n+        )\n \n         # default latin-1 encoding\n-        mw = HttpProxyMiddleware(auth_encoding='latin-1')\n-        req = Request('http://scrapytest.org')\n+        mw = HttpProxyMiddleware(auth_encoding=\"latin-1\")\n+        req = Request(\"http://scrapytest.org\")\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')\n-        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic beFuOnBhc3M=')\n+        self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n+        self.assertEqual(req.headers.get(\"Proxy-Authorization\"), b\"Basic beFuOnBhc3M=\")\n \n         # proxy from request.meta, latin-1 encoding\n-        req = Request('http://scrapytest.org', meta={'proxy': 'https://\\u00FCser:pass@proxy:3128'})\n+        req = Request(\n+            \"http://scrapytest.org\", meta={\"proxy\": \"https://\\u00FCser:pass@proxy:3128\"}\n+        )\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')\n-        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic /HNlcjpwYXNz')\n+        self.assertEqual(req.meta[\"proxy\"], \"https://proxy:3128\")\n+        self.assertEqual(req.headers.get(\"Proxy-Authorization\"), b\"Basic /HNlcjpwYXNz\")\n \n     def test_proxy_already_seted(self):\n-        os.environ['http_proxy'] = 'https://proxy.for.http:3128'\n+        os.environ[\"http_proxy\"] = \"https://proxy.for.http:3128\"\n         mw = HttpProxyMiddleware()\n-        req = Request('http://noproxy.com', meta={'proxy': None})\n+        req = Request(\"http://noproxy.com\", meta={\"proxy\": None})\n         assert mw.process_request(req, spider) is None\n-        assert 'proxy' in req.meta and req.meta['proxy'] is None\n+        assert \"proxy\" in req.meta and req.meta[\"proxy\"] is None\n \n     def test_no_proxy(self):\n-        os.environ['http_proxy'] = 'https://proxy.for.http:3128'\n+        os.environ[\"http_proxy\"] = \"https://proxy.for.http:3128\"\n         mw = HttpProxyMiddleware()\n \n-        os.environ['no_proxy'] = '*'\n-        req = Request('http://noproxy.com')\n+        os.environ[\"no_proxy\"] = \"*\"\n+        req = Request(\"http://noproxy.com\")\n         assert mw.process_request(req, spider) is None\n-        assert 'proxy' not in req.meta\n+        assert \"proxy\" not in req.meta\n \n-        os.environ['no_proxy'] = 'other.com'\n-        req = Request('http://noproxy.com')\n+        os.environ[\"no_proxy\"] = \"other.com\"\n+        req = Request(\"http://noproxy.com\")\n         assert mw.process_request(req, spider) is None\n-        assert 'proxy' in req.meta\n+        assert \"proxy\" in req.meta\n \n-        os.environ['no_proxy'] = 'other.com,noproxy.com'\n-        req = Request('http://noproxy.com')\n+        os.environ[\"no_proxy\"] = \"other.com,noproxy.com\"\n+        req = Request(\"http://noproxy.com\")\n         assert mw.process_request(req, spider) is None\n-        assert 'proxy' not in req.meta\n+        assert \"proxy\" not in req.meta\n \n         # proxy from meta['proxy'] takes precedence\n-        os.environ['no_proxy'] = '*'\n-        req = Request('http://noproxy.com', meta={'proxy': 'http://proxy.com'})\n+        os.environ[\"no_proxy\"] = \"*\"\n+        req = Request(\"http://noproxy.com\", meta={\"proxy\": \"http://proxy.com\"})\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta, {'proxy': 'http://proxy.com'})\n+        self.assertEqual(req.meta, {\"proxy\": \"http://proxy.com\"})\n \n     def test_no_proxy_invalid_values(self):\n-        os.environ['no_proxy'] = '/var/run/docker.sock'\n+        os.environ[\"no_proxy\"] = \"/var/run/docker.sock\"\n         mw = HttpProxyMiddleware()\n         # '/var/run/docker.sock' may be used by the user for\n         # no_proxy value but is not parseable and should be skipped\n-        assert 'no' not in mw.proxies\n+        assert \"no\" not in mw.proxies\n \n     def test_add_proxy_without_credentials(self):\n         middleware = HttpProxyMiddleware()\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         assert middleware.process_request(request, spider) is None\n-        request.meta['proxy'] = 'https://example.com'\n+        request.meta[\"proxy\"] = \"https://example.com\"\n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta['proxy'], 'https://example.com')\n-        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n+        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n \n     def test_add_proxy_with_credentials(self):\n         middleware = HttpProxyMiddleware()\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         assert middleware.process_request(request, spider) is None\n-        request.meta['proxy'] = 'https://user1:password1@example.com'\n+        request.meta[\"proxy\"] = \"https://user1:password1@example.com\"\n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n         encoded_credentials = middleware._basic_auth_header(\n-            'user1',\n-            'password1',\n+            \"user1\",\n+            \"password1\",\n         )\n         self.assertEqual(\n-            request.headers['Proxy-Authorization'],\n-            b'Basic ' + encoded_credentials,\n+            request.headers[\"Proxy-Authorization\"],\n+            b\"Basic \" + encoded_credentials,\n         )\n \n     def test_remove_proxy_without_credentials(self):\n         middleware = HttpProxyMiddleware()\n         request = Request(\n-            'https://example.com',\n-            meta={'proxy': 'https://example.com'},\n+            \"https://example.com\",\n+            meta={\"proxy\": \"https://example.com\"},\n         )\n         assert middleware.process_request(request, spider) is None\n-        request.meta['proxy'] = None\n+        request.meta[\"proxy\"] = None\n         assert middleware.process_request(request, spider) is None\n-        self.assertIsNone(request.meta['proxy'])\n-        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+        self.assertIsNone(request.meta[\"proxy\"])\n+        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n \n     def test_remove_proxy_with_credentials(self):\n         middleware = HttpProxyMiddleware()\n         request = Request(\n-            'https://example.com',\n-            meta={'proxy': 'https://user1:password1@example.com'},\n+            \"https://example.com\",\n+            meta={\"proxy\": \"https://user1:password1@example.com\"},\n         )\n         assert middleware.process_request(request, spider) is None\n-        request.meta['proxy'] = None\n+        request.meta[\"proxy\"] = None\n         assert middleware.process_request(request, spider) is None\n-        self.assertIsNone(request.meta['proxy'])\n-        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+        self.assertIsNone(request.meta[\"proxy\"])\n+        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n \n     def test_add_credentials(self):\n         \"\"\"If the proxy request meta switches to a proxy URL with the same\n@@ -208,21 +221,21 @@ class TestHttpProxyMiddleware(TestCase):\n         credentials must be used.\"\"\"\n         middleware = HttpProxyMiddleware()\n         request = Request(\n-            'https://example.com',\n-            meta={'proxy': 'https://example.com'},\n+            \"https://example.com\",\n+            meta={\"proxy\": \"https://example.com\"},\n         )\n         assert middleware.process_request(request, spider) is None\n \n-        request.meta['proxy'] = 'https://user1:password1@example.com'\n+        request.meta[\"proxy\"] = \"https://user1:password1@example.com\"\n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n         encoded_credentials = middleware._basic_auth_header(\n-            'user1',\n-            'password1',\n+            \"user1\",\n+            \"password1\",\n         )\n         self.assertEqual(\n-            request.headers['Proxy-Authorization'],\n-            b'Basic ' + encoded_credentials,\n+            request.headers[\"Proxy-Authorization\"],\n+            b\"Basic \" + encoded_credentials,\n         )\n \n     def test_change_credentials(self):\n@@ -230,20 +243,20 @@ class TestHttpProxyMiddleware(TestCase):\n         credentials, those new credentials must be used.\"\"\"\n         middleware = HttpProxyMiddleware()\n         request = Request(\n-            'https://example.com',\n-            meta={'proxy': 'https://user1:password1@example.com'},\n+            \"https://example.com\",\n+            meta={\"proxy\": \"https://user1:password1@example.com\"},\n         )\n         assert middleware.process_request(request, spider) is None\n-        request.meta['proxy'] = 'https://user2:password2@example.com'\n+        request.meta[\"proxy\"] = \"https://user2:password2@example.com\"\n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n         encoded_credentials = middleware._basic_auth_header(\n-            'user2',\n-            'password2',\n+            \"user2\",\n+            \"password2\",\n         )\n         self.assertEqual(\n-            request.headers['Proxy-Authorization'],\n-            b'Basic ' + encoded_credentials,\n+            request.headers[\"Proxy-Authorization\"],\n+            b\"Basic \" + encoded_credentials,\n         )\n \n     def test_remove_credentials(self):\n@@ -256,93 +269,93 @@ class TestHttpProxyMiddleware(TestCase):\n         \"\"\"\n         middleware = HttpProxyMiddleware()\n         request = Request(\n-            'https://example.com',\n-            meta={'proxy': 'https://user1:password1@example.com'},\n+            \"https://example.com\",\n+            meta={\"proxy\": \"https://user1:password1@example.com\"},\n         )\n         assert middleware.process_request(request, spider) is None\n \n-        request.meta['proxy'] = 'https://example.com'\n+        request.meta[\"proxy\"] = \"https://example.com\"\n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n         encoded_credentials = middleware._basic_auth_header(\n-            'user1',\n-            'password1',\n+            \"user1\",\n+            \"password1\",\n         )\n         self.assertEqual(\n-            request.headers['Proxy-Authorization'],\n-            b'Basic ' + encoded_credentials,\n+            request.headers[\"Proxy-Authorization\"],\n+            b\"Basic \" + encoded_credentials,\n         )\n \n-        request.meta['proxy'] = 'https://example.com'\n-        del request.headers[b'Proxy-Authorization']\n+        request.meta[\"proxy\"] = \"https://example.com\"\n+        del request.headers[b\"Proxy-Authorization\"]\n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta['proxy'], 'https://example.com')\n-        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n+        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n \n     def test_change_proxy_add_credentials(self):\n         middleware = HttpProxyMiddleware()\n         request = Request(\n-            'https://example.com',\n-            meta={'proxy': 'https://example.com'},\n+            \"https://example.com\",\n+            meta={\"proxy\": \"https://example.com\"},\n         )\n         assert middleware.process_request(request, spider) is None\n \n-        request.meta['proxy'] = 'https://user1:password1@example.org'\n+        request.meta[\"proxy\"] = \"https://user1:password1@example.org\"\n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta['proxy'], 'https://example.org')\n+        self.assertEqual(request.meta[\"proxy\"], \"https://example.org\")\n         encoded_credentials = middleware._basic_auth_header(\n-            'user1',\n-            'password1',\n+            \"user1\",\n+            \"password1\",\n         )\n         self.assertEqual(\n-            request.headers['Proxy-Authorization'],\n-            b'Basic ' + encoded_credentials,\n+            request.headers[\"Proxy-Authorization\"],\n+            b\"Basic \" + encoded_credentials,\n         )\n \n     def test_change_proxy_keep_credentials(self):\n         middleware = HttpProxyMiddleware()\n         request = Request(\n-            'https://example.com',\n-            meta={'proxy': 'https://user1:password1@example.com'},\n+            \"https://example.com\",\n+            meta={\"proxy\": \"https://user1:password1@example.com\"},\n         )\n         assert middleware.process_request(request, spider) is None\n \n-        request.meta['proxy'] = 'https://user1:password1@example.org'\n+        request.meta[\"proxy\"] = \"https://user1:password1@example.org\"\n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta['proxy'], 'https://example.org')\n+        self.assertEqual(request.meta[\"proxy\"], \"https://example.org\")\n         encoded_credentials = middleware._basic_auth_header(\n-            'user1',\n-            'password1',\n+            \"user1\",\n+            \"password1\",\n         )\n         self.assertEqual(\n-            request.headers['Proxy-Authorization'],\n-            b'Basic ' + encoded_credentials,\n+            request.headers[\"Proxy-Authorization\"],\n+            b\"Basic \" + encoded_credentials,\n         )\n \n         # Make sure, indirectly, that _auth_proxy is updated.\n-        request.meta['proxy'] = 'https://example.com'\n+        request.meta[\"proxy\"] = \"https://example.com\"\n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta['proxy'], 'https://example.com')\n-        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n+        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n \n     def test_change_proxy_change_credentials(self):\n         middleware = HttpProxyMiddleware()\n         request = Request(\n-            'https://example.com',\n-            meta={'proxy': 'https://user1:password1@example.com'},\n+            \"https://example.com\",\n+            meta={\"proxy\": \"https://user1:password1@example.com\"},\n         )\n         assert middleware.process_request(request, spider) is None\n \n-        request.meta['proxy'] = 'https://user2:password2@example.org'\n+        request.meta[\"proxy\"] = \"https://user2:password2@example.org\"\n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta['proxy'], 'https://example.org')\n+        self.assertEqual(request.meta[\"proxy\"], \"https://example.org\")\n         encoded_credentials = middleware._basic_auth_header(\n-            'user2',\n-            'password2',\n+            \"user2\",\n+            \"password2\",\n         )\n         self.assertEqual(\n-            request.headers['Proxy-Authorization'],\n-            b'Basic ' + encoded_credentials,\n+            request.headers[\"Proxy-Authorization\"],\n+            b\"Basic \" + encoded_credentials,\n         )\n \n     def test_change_proxy_remove_credentials(self):\n@@ -350,14 +363,14 @@ class TestHttpProxyMiddleware(TestCase):\n         proxy and no credentials, no credentials must be used.\"\"\"\n         middleware = HttpProxyMiddleware()\n         request = Request(\n-            'https://example.com',\n-            meta={'proxy': 'https://user1:password1@example.com'},\n+            \"https://example.com\",\n+            meta={\"proxy\": \"https://user1:password1@example.com\"},\n         )\n         assert middleware.process_request(request, spider) is None\n-        request.meta['proxy'] = 'https://example.org'\n+        request.meta[\"proxy\"] = \"https://example.org\"\n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta, {'proxy': 'https://example.org'})\n-        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+        self.assertEqual(request.meta, {\"proxy\": \"https://example.org\"})\n+        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n \n     def test_change_proxy_remove_credentials_preremoved_header(self):\n         \"\"\"Corner case of proxy switch with credentials removal where the\n@@ -368,36 +381,36 @@ class TestHttpProxyMiddleware(TestCase):\n         \"\"\"\n         middleware = HttpProxyMiddleware()\n         request = Request(\n-            'https://example.com',\n-            meta={'proxy': 'https://user1:password1@example.com'},\n+            \"https://example.com\",\n+            meta={\"proxy\": \"https://user1:password1@example.com\"},\n         )\n         assert middleware.process_request(request, spider) is None\n-        request.meta['proxy'] = 'https://example.org'\n-        del request.headers[b'Proxy-Authorization']\n+        request.meta[\"proxy\"] = \"https://example.org\"\n+        del request.headers[b\"Proxy-Authorization\"]\n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta, {'proxy': 'https://example.org'})\n-        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+        self.assertEqual(request.meta, {\"proxy\": \"https://example.org\"})\n+        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n \n     def test_proxy_authentication_header_undefined_proxy(self):\n         middleware = HttpProxyMiddleware()\n         request = Request(\n-            'https://example.com',\n-            headers={'Proxy-Authorization': 'Basic foo'},\n+            \"https://example.com\",\n+            headers={\"Proxy-Authorization\": \"Basic foo\"},\n         )\n         assert middleware.process_request(request, spider) is None\n-        self.assertNotIn('proxy', request.meta)\n-        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+        self.assertNotIn(\"proxy\", request.meta)\n+        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n \n     def test_proxy_authentication_header_disabled_proxy(self):\n         middleware = HttpProxyMiddleware()\n         request = Request(\n-            'https://example.com',\n-            headers={'Proxy-Authorization': 'Basic foo'},\n-            meta={'proxy': None},\n+            \"https://example.com\",\n+            headers={\"Proxy-Authorization\": \"Basic foo\"},\n+            meta={\"proxy\": None},\n         )\n         assert middleware.process_request(request, spider) is None\n-        self.assertIsNone(request.meta['proxy'])\n-        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+        self.assertIsNone(request.meta[\"proxy\"])\n+        self.assertNotIn(b\"Proxy-Authorization\", request.headers)\n \n     def test_proxy_authentication_header_proxy_without_credentials(self):\n         \"\"\"As long as the proxy URL in request metadata remains the same, the\n@@ -405,59 +418,59 @@ class TestHttpProxyMiddleware(TestCase):\n         changed.\"\"\"\n         middleware = HttpProxyMiddleware()\n         request = Request(\n-            'https://example.com',\n-            headers={'Proxy-Authorization': 'Basic foo'},\n-            meta={'proxy': 'https://example.com'},\n+            \"https://example.com\",\n+            headers={\"Proxy-Authorization\": \"Basic foo\"},\n+            meta={\"proxy\": \"https://example.com\"},\n         )\n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta['proxy'], 'https://example.com')\n-        self.assertEqual(request.headers['Proxy-Authorization'], b'Basic foo')\n+        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n+        self.assertEqual(request.headers[\"Proxy-Authorization\"], b\"Basic foo\")\n \n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta['proxy'], 'https://example.com')\n-        self.assertEqual(request.headers['Proxy-Authorization'], b'Basic foo')\n+        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n+        self.assertEqual(request.headers[\"Proxy-Authorization\"], b\"Basic foo\")\n \n-        request.headers['Proxy-Authorization'] = b'Basic bar'\n+        request.headers[\"Proxy-Authorization\"] = b\"Basic bar\"\n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta['proxy'], 'https://example.com')\n-        self.assertEqual(request.headers['Proxy-Authorization'], b'Basic bar')\n+        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n+        self.assertEqual(request.headers[\"Proxy-Authorization\"], b\"Basic bar\")\n \n     def test_proxy_authentication_header_proxy_with_same_credentials(self):\n         middleware = HttpProxyMiddleware()\n         encoded_credentials = middleware._basic_auth_header(\n-            'user1',\n-            'password1',\n+            \"user1\",\n+            \"password1\",\n         )\n         request = Request(\n-            'https://example.com',\n-            headers={'Proxy-Authorization': b'Basic ' + encoded_credentials},\n-            meta={'proxy': 'https://user1:password1@example.com'},\n+            \"https://example.com\",\n+            headers={\"Proxy-Authorization\": b\"Basic \" + encoded_credentials},\n+            meta={\"proxy\": \"https://user1:password1@example.com\"},\n         )\n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n         self.assertEqual(\n-            request.headers['Proxy-Authorization'],\n-            b'Basic ' + encoded_credentials,\n+            request.headers[\"Proxy-Authorization\"],\n+            b\"Basic \" + encoded_credentials,\n         )\n \n     def test_proxy_authentication_header_proxy_with_different_credentials(self):\n         middleware = HttpProxyMiddleware()\n         encoded_credentials1 = middleware._basic_auth_header(\n-            'user1',\n-            'password1',\n+            \"user1\",\n+            \"password1\",\n         )\n         request = Request(\n-            'https://example.com',\n-            headers={'Proxy-Authorization': b'Basic ' + encoded_credentials1},\n-            meta={'proxy': 'https://user2:password2@example.com'},\n+            \"https://example.com\",\n+            headers={\"Proxy-Authorization\": b\"Basic \" + encoded_credentials1},\n+            meta={\"proxy\": \"https://user2:password2@example.com\"},\n         )\n         assert middleware.process_request(request, spider) is None\n-        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        self.assertEqual(request.meta[\"proxy\"], \"https://example.com\")\n         encoded_credentials2 = middleware._basic_auth_header(\n-            'user2',\n-            'password2',\n+            \"user2\",\n+            \"password2\",\n         )\n         self.assertEqual(\n-            request.headers['Proxy-Authorization'],\n-            b'Basic ' + encoded_credentials2,\n+            request.headers[\"Proxy-Authorization\"],\n+            b\"Basic \" + encoded_credentials2,\n         )\n\n@@ -1,6 +1,9 @@\n import unittest\n \n-from scrapy.downloadermiddlewares.redirect import RedirectMiddleware, MetaRefreshMiddleware\n+from scrapy.downloadermiddlewares.redirect import (\n+    RedirectMiddleware,\n+    MetaRefreshMiddleware,\n+)\n from scrapy.spiders import Spider\n from scrapy.exceptions import IgnoreRequest\n from scrapy.http import Request, Response, HtmlResponse\n@@ -8,24 +11,25 @@ from scrapy.utils.test import get_crawler\n \n \n class RedirectMiddlewareTest(unittest.TestCase):\n-\n     def setUp(self):\n         self.crawler = get_crawler(Spider)\n-        self.spider = self.crawler._create_spider('foo')\n+        self.spider = self.crawler._create_spider(\"foo\")\n         self.mw = RedirectMiddleware.from_crawler(self.crawler)\n \n     def test_priority_adjust(self):\n-        req = Request('http://a.com')\n-        rsp = Response('http://a.com', headers={'Location': 'http://a.com/redirected'}, status=301)\n+        req = Request(\"http://a.com\")\n+        rsp = Response(\n+            \"http://a.com\", headers={\"Location\": \"http://a.com/redirected\"}, status=301\n+        )\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert req2.priority > req.priority\n \n     def test_redirect_3xx_permanent(self):\n         def _test(method, status=301):\n-            url = f'http://www.example.com/{status}'\n-            url2 = 'http://www.example.com/redirected'\n+            url = f\"http://www.example.com/{status}\"\n+            url2 = \"http://www.example.com/redirected\"\n             req = Request(url, method=method)\n-            rsp = Response(url, headers={'Location': url2}, status=status)\n+            rsp = Response(url, headers={\"Location\": url2}, status=status)\n \n             req2 = self.mw.process_response(req, rsp, self.spider)\n             assert isinstance(req2, Request)\n@@ -33,33 +37,33 @@ class RedirectMiddlewareTest(unittest.TestCase):\n             self.assertEqual(req2.method, method)\n \n             # response without Location header but with status code is 3XX should be ignored\n-            del rsp.headers['Location']\n+            del rsp.headers[\"Location\"]\n             assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n-        _test('GET')\n-        _test('POST')\n-        _test('HEAD')\n+        _test(\"GET\")\n+        _test(\"POST\")\n+        _test(\"HEAD\")\n \n-        _test('GET', status=307)\n-        _test('POST', status=307)\n-        _test('HEAD', status=307)\n+        _test(\"GET\", status=307)\n+        _test(\"POST\", status=307)\n+        _test(\"HEAD\", status=307)\n \n-        _test('GET', status=308)\n-        _test('POST', status=308)\n-        _test('HEAD', status=308)\n+        _test(\"GET\", status=308)\n+        _test(\"POST\", status=308)\n+        _test(\"HEAD\", status=308)\n \n     def test_dont_redirect(self):\n-        url = 'http://www.example.com/301'\n-        url2 = 'http://www.example.com/redirected'\n-        req = Request(url, meta={'dont_redirect': True})\n-        rsp = Response(url, headers={'Location': url2}, status=301)\n+        url = \"http://www.example.com/301\"\n+        url2 = \"http://www.example.com/redirected\"\n+        req = Request(url, meta={\"dont_redirect\": True})\n+        rsp = Response(url, headers={\"Location\": url2}, status=301)\n \n         r = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(r, Response)\n         assert r is rsp\n \n         # Test that it redirects when dont_redirect is False\n-        req = Request(url, meta={'dont_redirect': False})\n+        req = Request(url, meta={\"dont_redirect\": False})\n         rsp = Response(url2, status=200)\n \n         r = self.mw.process_response(req, rsp, self.spider)\n@@ -67,247 +71,313 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         assert r is rsp\n \n     def test_redirect_302(self):\n-        url = 'http://www.example.com/302'\n-        url2 = 'http://www.example.com/redirected2'\n-        req = Request(url, method='POST', body='test',\n-                      headers={'Content-Type': 'text/plain', 'Content-length': '4'})\n-        rsp = Response(url, headers={'Location': url2}, status=302)\n+        url = \"http://www.example.com/302\"\n+        url2 = \"http://www.example.com/redirected2\"\n+        req = Request(\n+            url,\n+            method=\"POST\",\n+            body=\"test\",\n+            headers={\"Content-Type\": \"text/plain\", \"Content-length\": \"4\"},\n+        )\n+        rsp = Response(url, headers={\"Location\": url2}, status=302)\n \n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req2, Request)\n         self.assertEqual(req2.url, url2)\n-        self.assertEqual(req2.method, 'GET')\n-        assert 'Content-Type' not in req2.headers, \"Content-Type header must not be present in redirected request\"\n-        assert 'Content-Length' not in req2.headers, \"Content-Length header must not be present in redirected request\"\n+        self.assertEqual(req2.method, \"GET\")\n+        assert (\n+            \"Content-Type\" not in req2.headers\n+        ), \"Content-Type header must not be present in redirected request\"\n+        assert (\n+            \"Content-Length\" not in req2.headers\n+        ), \"Content-Length header must not be present in redirected request\"\n         assert not req2.body, f\"Redirected body must be empty, not '{req2.body}'\"\n \n         # response without Location header but with status code is 3XX should be ignored\n-        del rsp.headers['Location']\n+        del rsp.headers[\"Location\"]\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n     def test_redirect_302_head(self):\n-        url = 'http://www.example.com/302'\n-        url2 = 'http://www.example.com/redirected2'\n-        req = Request(url, method='HEAD')\n-        rsp = Response(url, headers={'Location': url2}, status=302)\n+        url = \"http://www.example.com/302\"\n+        url2 = \"http://www.example.com/redirected2\"\n+        req = Request(url, method=\"HEAD\")\n+        rsp = Response(url, headers={\"Location\": url2}, status=302)\n \n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req2, Request)\n         self.assertEqual(req2.url, url2)\n-        self.assertEqual(req2.method, 'HEAD')\n+        self.assertEqual(req2.method, \"HEAD\")\n \n         # response without Location header but with status code is 3XX should be ignored\n-        del rsp.headers['Location']\n+        del rsp.headers[\"Location\"]\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n     def test_redirect_302_relative(self):\n-        url = 'http://www.example.com/302'\n-        url2 = '///i8n.example2.com/302'\n-        url3 = 'http://i8n.example2.com/302'\n-        req = Request(url, method='HEAD')\n-        rsp = Response(url, headers={'Location': url2}, status=302)\n+        url = \"http://www.example.com/302\"\n+        url2 = \"///i8n.example2.com/302\"\n+        url3 = \"http://i8n.example2.com/302\"\n+        req = Request(url, method=\"HEAD\")\n+        rsp = Response(url, headers={\"Location\": url2}, status=302)\n \n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req2, Request)\n         self.assertEqual(req2.url, url3)\n-        self.assertEqual(req2.method, 'HEAD')\n+        self.assertEqual(req2.method, \"HEAD\")\n \n         # response without Location header but with status code is 3XX should be ignored\n-        del rsp.headers['Location']\n+        del rsp.headers[\"Location\"]\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n     def test_max_redirect_times(self):\n         self.mw.max_redirect_times = 1\n-        req = Request('http://scrapytest.org/302')\n-        rsp = Response('http://scrapytest.org/302', headers={'Location': '/redirected'}, status=302)\n+        req = Request(\"http://scrapytest.org/302\")\n+        rsp = Response(\n+            \"http://scrapytest.org/302\", headers={\"Location\": \"/redirected\"}, status=302\n+        )\n \n         req = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req, Request)\n-        assert 'redirect_times' in req.meta\n-        self.assertEqual(req.meta['redirect_times'], 1)\n-        self.assertRaises(IgnoreRequest, self.mw.process_response, req, rsp, self.spider)\n+        assert \"redirect_times\" in req.meta\n+        self.assertEqual(req.meta[\"redirect_times\"], 1)\n+        self.assertRaises(\n+            IgnoreRequest, self.mw.process_response, req, rsp, self.spider\n+        )\n \n     def test_ttl(self):\n         self.mw.max_redirect_times = 100\n-        req = Request('http://scrapytest.org/302', meta={'redirect_ttl': 1})\n-        rsp = Response('http://www.scrapytest.org/302', headers={'Location': '/redirected'}, status=302)\n+        req = Request(\"http://scrapytest.org/302\", meta={\"redirect_ttl\": 1})\n+        rsp = Response(\n+            \"http://www.scrapytest.org/302\",\n+            headers={\"Location\": \"/redirected\"},\n+            status=302,\n+        )\n \n         req = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req, Request)\n-        self.assertRaises(IgnoreRequest, self.mw.process_response, req, rsp, self.spider)\n+        self.assertRaises(\n+            IgnoreRequest, self.mw.process_response, req, rsp, self.spider\n+        )\n \n     def test_redirect_urls(self):\n-        req1 = Request('http://scrapytest.org/first')\n-        rsp1 = Response('http://scrapytest.org/first', headers={'Location': '/redirected'}, status=302)\n+        req1 = Request(\"http://scrapytest.org/first\")\n+        rsp1 = Response(\n+            \"http://scrapytest.org/first\",\n+            headers={\"Location\": \"/redirected\"},\n+            status=302,\n+        )\n         req2 = self.mw.process_response(req1, rsp1, self.spider)\n-        rsp2 = Response('http://scrapytest.org/redirected', headers={'Location': '/redirected2'}, status=302)\n+        rsp2 = Response(\n+            \"http://scrapytest.org/redirected\",\n+            headers={\"Location\": \"/redirected2\"},\n+            status=302,\n+        )\n         req3 = self.mw.process_response(req2, rsp2, self.spider)\n \n-        self.assertEqual(req2.url, 'http://scrapytest.org/redirected')\n-        self.assertEqual(req2.meta['redirect_urls'], ['http://scrapytest.org/first'])\n-        self.assertEqual(req3.url, 'http://scrapytest.org/redirected2')\n+        self.assertEqual(req2.url, \"http://scrapytest.org/redirected\")\n+        self.assertEqual(req2.meta[\"redirect_urls\"], [\"http://scrapytest.org/first\"])\n+        self.assertEqual(req3.url, \"http://scrapytest.org/redirected2\")\n         self.assertEqual(\n-            req3.meta['redirect_urls'],\n-            ['http://scrapytest.org/first', 'http://scrapytest.org/redirected']\n+            req3.meta[\"redirect_urls\"],\n+            [\"http://scrapytest.org/first\", \"http://scrapytest.org/redirected\"],\n         )\n \n     def test_redirect_reasons(self):\n-        req1 = Request('http://scrapytest.org/first')\n-        rsp1 = Response('http://scrapytest.org/first', headers={'Location': '/redirected1'}, status=301)\n+        req1 = Request(\"http://scrapytest.org/first\")\n+        rsp1 = Response(\n+            \"http://scrapytest.org/first\",\n+            headers={\"Location\": \"/redirected1\"},\n+            status=301,\n+        )\n         req2 = self.mw.process_response(req1, rsp1, self.spider)\n-        rsp2 = Response('http://scrapytest.org/redirected1', headers={'Location': '/redirected2'}, status=301)\n+        rsp2 = Response(\n+            \"http://scrapytest.org/redirected1\",\n+            headers={\"Location\": \"/redirected2\"},\n+            status=301,\n+        )\n         req3 = self.mw.process_response(req2, rsp2, self.spider)\n \n-        self.assertEqual(req2.meta['redirect_reasons'], [301])\n-        self.assertEqual(req3.meta['redirect_reasons'], [301, 301])\n+        self.assertEqual(req2.meta[\"redirect_reasons\"], [301])\n+        self.assertEqual(req3.meta[\"redirect_reasons\"], [301, 301])\n \n     def test_spider_handling(self):\n-        smartspider = self.crawler._create_spider('smarty')\n+        smartspider = self.crawler._create_spider(\"smarty\")\n         smartspider.handle_httpstatus_list = [404, 301, 302]\n-        url = 'http://www.example.com/301'\n-        url2 = 'http://www.example.com/redirected'\n+        url = \"http://www.example.com/301\"\n+        url2 = \"http://www.example.com/redirected\"\n         req = Request(url)\n-        rsp = Response(url, headers={'Location': url2}, status=301)\n+        rsp = Response(url, headers={\"Location\": url2}, status=301)\n         r = self.mw.process_response(req, rsp, smartspider)\n         self.assertIs(r, rsp)\n \n     def test_request_meta_handling(self):\n-        url = 'http://www.example.com/301'\n-        url2 = 'http://www.example.com/redirected'\n+        url = \"http://www.example.com/301\"\n+        url2 = \"http://www.example.com/redirected\"\n \n         def _test_passthrough(req):\n-            rsp = Response(url, headers={'Location': url2}, status=301, request=req)\n+            rsp = Response(url, headers={\"Location\": url2}, status=301, request=req)\n             r = self.mw.process_response(req, rsp, self.spider)\n             self.assertIs(r, rsp)\n-        _test_passthrough(Request(url, meta={'handle_httpstatus_list': [404, 301, 302]}))\n-        _test_passthrough(Request(url, meta={'handle_httpstatus_all': True}))\n+\n+        _test_passthrough(\n+            Request(url, meta={\"handle_httpstatus_list\": [404, 301, 302]})\n+        )\n+        _test_passthrough(Request(url, meta={\"handle_httpstatus_all\": True}))\n \n     def test_latin1_location(self):\n-        req = Request('http://scrapytest.org/first')\n-        latin1_location = '/ação'.encode('latin1')  # HTTP historically supports latin1\n-        resp = Response('http://scrapytest.org/first', headers={'Location': latin1_location}, status=302)\n+        req = Request(\"http://scrapytest.org/first\")\n+        latin1_location = \"/ação\".encode(\"latin1\")  # HTTP historically supports latin1\n+        resp = Response(\n+            \"http://scrapytest.org/first\",\n+            headers={\"Location\": latin1_location},\n+            status=302,\n+        )\n         req_result = self.mw.process_response(req, resp, self.spider)\n-        perc_encoded_utf8_url = 'http://scrapytest.org/a%E7%E3o'\n+        perc_encoded_utf8_url = \"http://scrapytest.org/a%E7%E3o\"\n         self.assertEqual(perc_encoded_utf8_url, req_result.url)\n \n     def test_utf8_location(self):\n-        req = Request('http://scrapytest.org/first')\n-        utf8_location = '/ação'.encode('utf-8')  # header using UTF-8 encoding\n-        resp = Response('http://scrapytest.org/first', headers={'Location': utf8_location}, status=302)\n+        req = Request(\"http://scrapytest.org/first\")\n+        utf8_location = \"/ação\".encode(\"utf-8\")  # header using UTF-8 encoding\n+        resp = Response(\n+            \"http://scrapytest.org/first\",\n+            headers={\"Location\": utf8_location},\n+            status=302,\n+        )\n         req_result = self.mw.process_response(req, resp, self.spider)\n-        perc_encoded_utf8_url = 'http://scrapytest.org/a%C3%A7%C3%A3o'\n+        perc_encoded_utf8_url = \"http://scrapytest.org/a%C3%A7%C3%A3o\"\n         self.assertEqual(perc_encoded_utf8_url, req_result.url)\n \n \n class MetaRefreshMiddlewareTest(unittest.TestCase):\n-\n     def setUp(self):\n         crawler = get_crawler(Spider)\n-        self.spider = crawler._create_spider('foo')\n+        self.spider = crawler._create_spider(\"foo\")\n         self.mw = MetaRefreshMiddleware.from_crawler(crawler)\n \n-    def _body(self, interval=5, url='http://example.org/newpage'):\n+    def _body(self, interval=5, url=\"http://example.org/newpage\"):\n         html = f\"\"\"<html><head><meta http-equiv=\"refresh\" content=\"{interval};url={url}\"/></head></html>\"\"\"\n-        return html.encode('utf-8')\n+        return html.encode(\"utf-8\")\n \n     def test_priority_adjust(self):\n-        req = Request('http://a.com')\n+        req = Request(\"http://a.com\")\n         rsp = HtmlResponse(req.url, body=self._body())\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert req2.priority > req.priority\n \n     def test_meta_refresh(self):\n-        req = Request(url='http://example.org')\n+        req = Request(url=\"http://example.org\")\n         rsp = HtmlResponse(req.url, body=self._body())\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req2, Request)\n-        self.assertEqual(req2.url, 'http://example.org/newpage')\n+        self.assertEqual(req2.url, \"http://example.org/newpage\")\n \n     def test_meta_refresh_with_high_interval(self):\n         # meta-refresh with high intervals don't trigger redirects\n-        req = Request(url='http://example.org')\n-        rsp = HtmlResponse(url='http://example.org',\n-                           body=self._body(interval=1000),\n-                           encoding='utf-8')\n+        req = Request(url=\"http://example.org\")\n+        rsp = HtmlResponse(\n+            url=\"http://example.org\", body=self._body(interval=1000), encoding=\"utf-8\"\n+        )\n         rsp2 = self.mw.process_response(req, rsp, self.spider)\n         assert rsp is rsp2\n \n     def test_meta_refresh_trough_posted_request(self):\n-        req = Request(url='http://example.org', method='POST', body='test',\n-                      headers={'Content-Type': 'text/plain', 'Content-length': '4'})\n+        req = Request(\n+            url=\"http://example.org\",\n+            method=\"POST\",\n+            body=\"test\",\n+            headers={\"Content-Type\": \"text/plain\", \"Content-length\": \"4\"},\n+        )\n         rsp = HtmlResponse(req.url, body=self._body())\n         req2 = self.mw.process_response(req, rsp, self.spider)\n \n         assert isinstance(req2, Request)\n-        self.assertEqual(req2.url, 'http://example.org/newpage')\n-        self.assertEqual(req2.method, 'GET')\n-        assert 'Content-Type' not in req2.headers, \"Content-Type header must not be present in redirected request\"\n-        assert 'Content-Length' not in req2.headers, \"Content-Length header must not be present in redirected request\"\n+        self.assertEqual(req2.url, \"http://example.org/newpage\")\n+        self.assertEqual(req2.method, \"GET\")\n+        assert (\n+            \"Content-Type\" not in req2.headers\n+        ), \"Content-Type header must not be present in redirected request\"\n+        assert (\n+            \"Content-Length\" not in req2.headers\n+        ), \"Content-Length header must not be present in redirected request\"\n         assert not req2.body, f\"Redirected body must be empty, not '{req2.body}'\"\n \n     def test_max_redirect_times(self):\n         self.mw.max_redirect_times = 1\n-        req = Request('http://scrapytest.org/max')\n+        req = Request(\"http://scrapytest.org/max\")\n         rsp = HtmlResponse(req.url, body=self._body())\n \n         req = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req, Request)\n-        assert 'redirect_times' in req.meta\n-        self.assertEqual(req.meta['redirect_times'], 1)\n-        self.assertRaises(IgnoreRequest, self.mw.process_response, req, rsp, self.spider)\n+        assert \"redirect_times\" in req.meta\n+        self.assertEqual(req.meta[\"redirect_times\"], 1)\n+        self.assertRaises(\n+            IgnoreRequest, self.mw.process_response, req, rsp, self.spider\n+        )\n \n     def test_ttl(self):\n         self.mw.max_redirect_times = 100\n-        req = Request('http://scrapytest.org/302', meta={'redirect_ttl': 1})\n+        req = Request(\"http://scrapytest.org/302\", meta={\"redirect_ttl\": 1})\n         rsp = HtmlResponse(req.url, body=self._body())\n \n         req = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req, Request)\n-        self.assertRaises(IgnoreRequest, self.mw.process_response, req, rsp, self.spider)\n+        self.assertRaises(\n+            IgnoreRequest, self.mw.process_response, req, rsp, self.spider\n+        )\n \n     def test_redirect_urls(self):\n-        req1 = Request('http://scrapytest.org/first')\n-        rsp1 = HtmlResponse(req1.url, body=self._body(url='/redirected'))\n+        req1 = Request(\"http://scrapytest.org/first\")\n+        rsp1 = HtmlResponse(req1.url, body=self._body(url=\"/redirected\"))\n         req2 = self.mw.process_response(req1, rsp1, self.spider)\n         assert isinstance(req2, Request), req2\n-        rsp2 = HtmlResponse(req2.url, body=self._body(url='/redirected2'))\n+        rsp2 = HtmlResponse(req2.url, body=self._body(url=\"/redirected2\"))\n         req3 = self.mw.process_response(req2, rsp2, self.spider)\n         assert isinstance(req3, Request), req3\n-        self.assertEqual(req2.url, 'http://scrapytest.org/redirected')\n-        self.assertEqual(req2.meta['redirect_urls'], ['http://scrapytest.org/first'])\n-        self.assertEqual(req3.url, 'http://scrapytest.org/redirected2')\n+        self.assertEqual(req2.url, \"http://scrapytest.org/redirected\")\n+        self.assertEqual(req2.meta[\"redirect_urls\"], [\"http://scrapytest.org/first\"])\n+        self.assertEqual(req3.url, \"http://scrapytest.org/redirected2\")\n         self.assertEqual(\n-            req3.meta['redirect_urls'],\n-            ['http://scrapytest.org/first', 'http://scrapytest.org/redirected']\n+            req3.meta[\"redirect_urls\"],\n+            [\"http://scrapytest.org/first\", \"http://scrapytest.org/redirected\"],\n         )\n \n     def test_redirect_reasons(self):\n-        req1 = Request('http://scrapytest.org/first')\n-        rsp1 = HtmlResponse('http://scrapytest.org/first', body=self._body(url='/redirected'))\n+        req1 = Request(\"http://scrapytest.org/first\")\n+        rsp1 = HtmlResponse(\n+            \"http://scrapytest.org/first\", body=self._body(url=\"/redirected\")\n+        )\n         req2 = self.mw.process_response(req1, rsp1, self.spider)\n-        rsp2 = HtmlResponse('http://scrapytest.org/redirected', body=self._body(url='/redirected1'))\n+        rsp2 = HtmlResponse(\n+            \"http://scrapytest.org/redirected\", body=self._body(url=\"/redirected1\")\n+        )\n         req3 = self.mw.process_response(req2, rsp2, self.spider)\n \n-        self.assertEqual(req2.meta['redirect_reasons'], ['meta refresh'])\n-        self.assertEqual(req3.meta['redirect_reasons'], ['meta refresh', 'meta refresh'])\n+        self.assertEqual(req2.meta[\"redirect_reasons\"], [\"meta refresh\"])\n+        self.assertEqual(\n+            req3.meta[\"redirect_reasons\"], [\"meta refresh\", \"meta refresh\"]\n+        )\n \n     def test_ignore_tags_default(self):\n-        req = Request(url='http://example.org')\n-        body = ('''<noscript><meta http-equiv=\"refresh\" '''\n-                '''content=\"0;URL='http://example.org/newpage'\"></noscript>''')\n+        req = Request(url=\"http://example.org\")\n+        body = (\n+            \"\"\"<noscript><meta http-equiv=\"refresh\" \"\"\"\n+            \"\"\"content=\"0;URL='http://example.org/newpage'\"></noscript>\"\"\"\n+        )\n         rsp = HtmlResponse(req.url, body=body.encode())\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req2, Request)\n-        self.assertEqual(req2.url, 'http://example.org/newpage')\n+        self.assertEqual(req2.url, \"http://example.org/newpage\")\n \n     def test_ignore_tags_1_x_list(self):\n         \"\"\"Test that Scrapy 1.x behavior remains possible\"\"\"\n-        settings = {'METAREFRESH_IGNORE_TAGS': ['script', 'noscript']}\n+        settings = {\"METAREFRESH_IGNORE_TAGS\": [\"script\", \"noscript\"]}\n         crawler = get_crawler(Spider, settings)\n         mw = MetaRefreshMiddleware.from_crawler(crawler)\n-        req = Request(url='http://example.org')\n-        body = ('''<noscript><meta http-equiv=\"refresh\" '''\n-                '''content=\"0;URL='http://example.org/newpage'\"></noscript>''')\n+        req = Request(url=\"http://example.org\")\n+        body = (\n+            \"\"\"<noscript><meta http-equiv=\"refresh\" \"\"\"\n+            \"\"\"content=\"0;URL='http://example.org/newpage'\"></noscript>\"\"\"\n+        )\n         rsp = HtmlResponse(req.url, body=body.encode())\n         response = mw.process_response(req, rsp, self.spider)\n         assert isinstance(response, Response)\n\n@@ -22,65 +22,68 @@ from scrapy.utils.test import get_crawler\n class RetryTest(unittest.TestCase):\n     def setUp(self):\n         self.crawler = get_crawler(Spider)\n-        self.spider = self.crawler._create_spider('foo')\n+        self.spider = self.crawler._create_spider(\"foo\")\n         self.mw = RetryMiddleware.from_crawler(self.crawler)\n         self.mw.max_retry_times = 2\n \n     def test_priority_adjust(self):\n-        req = Request('http://www.scrapytest.org/503')\n-        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n+        req = Request(\"http://www.scrapytest.org/503\")\n+        rsp = Response(\"http://www.scrapytest.org/503\", body=b\"\", status=503)\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert req2.priority < req.priority\n \n     def test_404(self):\n-        req = Request('http://www.scrapytest.org/404')\n-        rsp = Response('http://www.scrapytest.org/404', body=b'', status=404)\n+        req = Request(\"http://www.scrapytest.org/404\")\n+        rsp = Response(\"http://www.scrapytest.org/404\", body=b\"\", status=404)\n \n         # dont retry 404s\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n     def test_dont_retry(self):\n-        req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})\n-        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n+        req = Request(\"http://www.scrapytest.org/503\", meta={\"dont_retry\": True})\n+        rsp = Response(\"http://www.scrapytest.org/503\", body=b\"\", status=503)\n \n         # first retry\n         r = self.mw.process_response(req, rsp, self.spider)\n         assert r is rsp\n \n         # Test retry when dont_retry set to False\n-        req = Request('http://www.scrapytest.org/503', meta={'dont_retry': False})\n-        rsp = Response('http://www.scrapytest.org/503')\n+        req = Request(\"http://www.scrapytest.org/503\", meta={\"dont_retry\": False})\n+        rsp = Response(\"http://www.scrapytest.org/503\")\n \n         # first retry\n         r = self.mw.process_response(req, rsp, self.spider)\n         assert r is rsp\n \n     def test_dont_retry_exc(self):\n-        req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})\n+        req = Request(\"http://www.scrapytest.org/503\", meta={\"dont_retry\": True})\n \n         r = self.mw.process_exception(req, DNSLookupError(), self.spider)\n         assert r is None\n \n     def test_503(self):\n-        req = Request('http://www.scrapytest.org/503')\n-        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n+        req = Request(\"http://www.scrapytest.org/503\")\n+        rsp = Response(\"http://www.scrapytest.org/503\", body=b\"\", status=503)\n \n         # first retry\n         req = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req, Request)\n-        self.assertEqual(req.meta['retry_times'], 1)\n+        self.assertEqual(req.meta[\"retry_times\"], 1)\n \n         # second retry\n         req = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req, Request)\n-        self.assertEqual(req.meta['retry_times'], 2)\n+        self.assertEqual(req.meta[\"retry_times\"], 2)\n \n         # discard it\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n-        assert self.crawler.stats.get_value('retry/max_reached') == 1\n-        assert self.crawler.stats.get_value('retry/reason_count/503 Service Unavailable') == 2\n-        assert self.crawler.stats.get_value('retry/count') == 2\n+        assert self.crawler.stats.get_value(\"retry/max_reached\") == 1\n+        assert (\n+            self.crawler.stats.get_value(\"retry/reason_count/503 Service Unavailable\")\n+            == 2\n+        )\n+        assert self.crawler.stats.get_value(\"retry/count\") == 2\n \n     def test_twistederrors(self):\n         exceptions = [\n@@ -96,24 +99,27 @@ class RetryTest(unittest.TestCase):\n         ]\n \n         for exc in exceptions:\n-            req = Request(f'http://www.scrapytest.org/{exc.__name__}')\n-            self._test_retry_exception(req, exc('foo'))\n+            req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n+            self._test_retry_exception(req, exc(\"foo\"))\n \n         stats = self.crawler.stats\n-        assert stats.get_value('retry/max_reached') == len(exceptions)\n-        assert stats.get_value('retry/count') == len(exceptions) * 2\n-        assert stats.get_value('retry/reason_count/twisted.internet.defer.TimeoutError') == 2\n+        assert stats.get_value(\"retry/max_reached\") == len(exceptions)\n+        assert stats.get_value(\"retry/count\") == len(exceptions) * 2\n+        assert (\n+            stats.get_value(\"retry/reason_count/twisted.internet.defer.TimeoutError\")\n+            == 2\n+        )\n \n     def _test_retry_exception(self, req, exception):\n         # first retry\n         req = self.mw.process_exception(req, exception, self.spider)\n         assert isinstance(req, Request)\n-        self.assertEqual(req.meta['retry_times'], 1)\n+        self.assertEqual(req.meta[\"retry_times\"], 1)\n \n         # second retry\n         req = self.mw.process_exception(req, exception, self.spider)\n         assert isinstance(req, Request)\n-        self.assertEqual(req.meta['retry_times'], 2)\n+        self.assertEqual(req.meta[\"retry_times\"], 2)\n \n         # discard it\n         req = self.mw.process_exception(req, exception, self.spider)\n@@ -122,22 +128,22 @@ class RetryTest(unittest.TestCase):\n \n class MaxRetryTimesTest(unittest.TestCase):\n \n-    invalid_url = 'http://www.scrapytest.org/invalid_url'\n+    invalid_url = \"http://www.scrapytest.org/invalid_url\"\n \n     def get_spider_and_middleware(self, settings=None):\n         crawler = get_crawler(Spider, settings or {})\n-        spider = crawler._create_spider('foo')\n+        spider = crawler._create_spider(\"foo\")\n         middleware = RetryMiddleware.from_crawler(crawler)\n         return spider, middleware\n \n     def test_with_settings_zero(self):\n         max_retry_times = 0\n-        settings = {'RETRY_TIMES': max_retry_times}\n+        settings = {\"RETRY_TIMES\": max_retry_times}\n         spider, middleware = self.get_spider_and_middleware(settings)\n         req = Request(self.invalid_url)\n         self._test_retry(\n             req,\n-            DNSLookupError('foo'),\n+            DNSLookupError(\"foo\"),\n             max_retry_times,\n             spider=spider,\n             middleware=middleware,\n@@ -146,11 +152,11 @@ class MaxRetryTimesTest(unittest.TestCase):\n     def test_with_metakey_zero(self):\n         max_retry_times = 0\n         spider, middleware = self.get_spider_and_middleware()\n-        meta = {'max_retry_times': max_retry_times}\n+        meta = {\"max_retry_times\": max_retry_times}\n         req = Request(self.invalid_url, meta=meta)\n         self._test_retry(\n             req,\n-            DNSLookupError('foo'),\n+            DNSLookupError(\"foo\"),\n             max_retry_times,\n             spider=spider,\n             middleware=middleware,\n@@ -158,12 +164,12 @@ class MaxRetryTimesTest(unittest.TestCase):\n \n     def test_without_metakey(self):\n         max_retry_times = 5\n-        settings = {'RETRY_TIMES': max_retry_times}\n+        settings = {\"RETRY_TIMES\": max_retry_times}\n         spider, middleware = self.get_spider_and_middleware(settings)\n         req = Request(self.invalid_url)\n         self._test_retry(\n             req,\n-            DNSLookupError('foo'),\n+            DNSLookupError(\"foo\"),\n             max_retry_times,\n             spider=spider,\n             middleware=middleware,\n@@ -173,22 +179,22 @@ class MaxRetryTimesTest(unittest.TestCase):\n         meta_max_retry_times = 3\n         middleware_max_retry_times = 2\n \n-        req1 = Request(self.invalid_url, meta={'max_retry_times': meta_max_retry_times})\n+        req1 = Request(self.invalid_url, meta={\"max_retry_times\": meta_max_retry_times})\n         req2 = Request(self.invalid_url)\n \n-        settings = {'RETRY_TIMES': middleware_max_retry_times}\n+        settings = {\"RETRY_TIMES\": middleware_max_retry_times}\n         spider, middleware = self.get_spider_and_middleware(settings)\n \n         self._test_retry(\n             req1,\n-            DNSLookupError('foo'),\n+            DNSLookupError(\"foo\"),\n             meta_max_retry_times,\n             spider=spider,\n             middleware=middleware,\n         )\n         self._test_retry(\n             req2,\n-            DNSLookupError('foo'),\n+            DNSLookupError(\"foo\"),\n             middleware_max_retry_times,\n             spider=spider,\n             middleware=middleware,\n@@ -198,22 +204,22 @@ class MaxRetryTimesTest(unittest.TestCase):\n         meta_max_retry_times = 4\n         middleware_max_retry_times = 5\n \n-        req1 = Request(self.invalid_url, meta={'max_retry_times': meta_max_retry_times})\n+        req1 = Request(self.invalid_url, meta={\"max_retry_times\": meta_max_retry_times})\n         req2 = Request(self.invalid_url)\n \n-        settings = {'RETRY_TIMES': middleware_max_retry_times}\n+        settings = {\"RETRY_TIMES\": middleware_max_retry_times}\n         spider, middleware = self.get_spider_and_middleware(settings)\n \n         self._test_retry(\n             req1,\n-            DNSLookupError('foo'),\n+            DNSLookupError(\"foo\"),\n             meta_max_retry_times,\n             spider=spider,\n             middleware=middleware,\n         )\n         self._test_retry(\n             req2,\n-            DNSLookupError('foo'),\n+            DNSLookupError(\"foo\"),\n             middleware_max_retry_times,\n             spider=spider,\n             middleware=middleware,\n@@ -223,13 +229,13 @@ class MaxRetryTimesTest(unittest.TestCase):\n         max_retry_times = 4\n         spider, middleware = self.get_spider_and_middleware()\n         meta = {\n-            'max_retry_times': max_retry_times,\n-            'dont_retry': True,\n+            \"max_retry_times\": max_retry_times,\n+            \"dont_retry\": True,\n         }\n         req = Request(self.invalid_url, meta=meta)\n         self._test_retry(\n             req,\n-            DNSLookupError('foo'),\n+            DNSLookupError(\"foo\"),\n             0,\n             spider=spider,\n             middleware=middleware,\n@@ -256,13 +262,12 @@ class MaxRetryTimesTest(unittest.TestCase):\n \n \n class GetRetryRequestTest(unittest.TestCase):\n-\n     def get_spider(self, settings=None):\n         crawler = get_crawler(Spider, settings or {})\n-        return crawler._create_spider('foo')\n+        return crawler._create_spider(\"foo\")\n \n     def test_basic_usage(self):\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         spider = self.get_spider()\n         with LogCapture() as log:\n             new_request = get_retry_request(\n@@ -273,10 +278,10 @@ class GetRetryRequestTest(unittest.TestCase):\n         self.assertNotEqual(new_request, request)\n         self.assertEqual(new_request.dont_filter, True)\n         expected_retry_times = 1\n-        self.assertEqual(new_request.meta['retry_times'], expected_retry_times)\n+        self.assertEqual(new_request.meta[\"retry_times\"], expected_retry_times)\n         self.assertEqual(new_request.priority, -1)\n         expected_reason = \"unspecified\"\n-        for stat in ('retry/count', f'retry/reason_count/{expected_reason}'):\n+        for stat in (\"retry/count\", f\"retry/reason_count/{expected_reason}\"):\n             self.assertEqual(spider.crawler.stats.get_value(stat), 1)\n         log.check_present(\n             (\n@@ -288,7 +293,7 @@ class GetRetryRequestTest(unittest.TestCase):\n         )\n \n     def test_max_retries_reached(self):\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         spider = self.get_spider()\n         max_retry_times = 0\n         with LogCapture() as log:\n@@ -298,10 +303,7 @@ class GetRetryRequestTest(unittest.TestCase):\n                 max_retry_times=max_retry_times,\n             )\n         self.assertEqual(new_request, None)\n-        self.assertEqual(\n-            spider.crawler.stats.get_value('retry/max_reached'),\n-            1\n-        )\n+        self.assertEqual(spider.crawler.stats.get_value(\"retry/max_reached\"), 1)\n         failure_count = max_retry_times + 1\n         expected_reason = \"unspecified\"\n         log.check_present(\n@@ -314,7 +316,7 @@ class GetRetryRequestTest(unittest.TestCase):\n         )\n \n     def test_one_retry(self):\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         spider = self.get_spider()\n         with LogCapture() as log:\n             new_request = get_retry_request(\n@@ -326,10 +328,10 @@ class GetRetryRequestTest(unittest.TestCase):\n         self.assertNotEqual(new_request, request)\n         self.assertEqual(new_request.dont_filter, True)\n         expected_retry_times = 1\n-        self.assertEqual(new_request.meta['retry_times'], expected_retry_times)\n+        self.assertEqual(new_request.meta[\"retry_times\"], expected_retry_times)\n         self.assertEqual(new_request.priority, -1)\n         expected_reason = \"unspecified\"\n-        for stat in ('retry/count', f'retry/reason_count/{expected_reason}'):\n+        for stat in (\"retry/count\", f\"retry/reason_count/{expected_reason}\"):\n             self.assertEqual(spider.crawler.stats.get_value(stat), 1)\n         log.check_present(\n             (\n@@ -342,7 +344,7 @@ class GetRetryRequestTest(unittest.TestCase):\n \n     def test_two_retries(self):\n         spider = self.get_spider()\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         new_request = request\n         max_retry_times = 2\n         for index in range(max_retry_times):\n@@ -356,10 +358,10 @@ class GetRetryRequestTest(unittest.TestCase):\n             self.assertNotEqual(new_request, request)\n             self.assertEqual(new_request.dont_filter, True)\n             expected_retry_times = index + 1\n-            self.assertEqual(new_request.meta['retry_times'], expected_retry_times)\n+            self.assertEqual(new_request.meta[\"retry_times\"], expected_retry_times)\n             self.assertEqual(new_request.priority, -expected_retry_times)\n             expected_reason = \"unspecified\"\n-            for stat in ('retry/count', f'retry/reason_count/{expected_reason}'):\n+            for stat in (\"retry/count\", f\"retry/reason_count/{expected_reason}\"):\n                 value = spider.crawler.stats.get_value(stat)\n                 self.assertEqual(value, expected_retry_times)\n             log.check_present(\n@@ -378,10 +380,7 @@ class GetRetryRequestTest(unittest.TestCase):\n                 max_retry_times=max_retry_times,\n             )\n         self.assertEqual(new_request, None)\n-        self.assertEqual(\n-            spider.crawler.stats.get_value('retry/max_reached'),\n-            1\n-        )\n+        self.assertEqual(spider.crawler.stats.get_value(\"retry/max_reached\"), 1)\n         failure_count = max_retry_times + 1\n         expected_reason = \"unspecified\"\n         log.check_present(\n@@ -394,14 +393,14 @@ class GetRetryRequestTest(unittest.TestCase):\n         )\n \n     def test_no_spider(self):\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         with self.assertRaises(TypeError):\n             get_retry_request(request)  # pylint: disable=missing-kwoa\n \n     def test_max_retry_times_setting(self):\n         max_retry_times = 0\n-        spider = self.get_spider({'RETRY_TIMES': max_retry_times})\n-        request = Request('https://example.com')\n+        spider = self.get_spider({\"RETRY_TIMES\": max_retry_times})\n+        request = Request(\"https://example.com\")\n         new_request = get_retry_request(\n             request,\n             spider=spider,\n@@ -410,9 +409,9 @@ class GetRetryRequestTest(unittest.TestCase):\n \n     def test_max_retry_times_meta(self):\n         max_retry_times = 0\n-        spider = self.get_spider({'RETRY_TIMES': max_retry_times + 1})\n-        meta = {'max_retry_times': max_retry_times}\n-        request = Request('https://example.com', meta=meta)\n+        spider = self.get_spider({\"RETRY_TIMES\": max_retry_times + 1})\n+        meta = {\"max_retry_times\": max_retry_times}\n+        request = Request(\"https://example.com\", meta=meta)\n         new_request = get_retry_request(\n             request,\n             spider=spider,\n@@ -421,9 +420,9 @@ class GetRetryRequestTest(unittest.TestCase):\n \n     def test_max_retry_times_argument(self):\n         max_retry_times = 0\n-        spider = self.get_spider({'RETRY_TIMES': max_retry_times + 1})\n-        meta = {'max_retry_times': max_retry_times + 1}\n-        request = Request('https://example.com', meta=meta)\n+        spider = self.get_spider({\"RETRY_TIMES\": max_retry_times + 1})\n+        meta = {\"max_retry_times\": max_retry_times + 1}\n+        request = Request(\"https://example.com\", meta=meta)\n         new_request = get_retry_request(\n             request,\n             spider=spider,\n@@ -433,8 +432,8 @@ class GetRetryRequestTest(unittest.TestCase):\n \n     def test_priority_adjust_setting(self):\n         priority_adjust = 1\n-        spider = self.get_spider({'RETRY_PRIORITY_ADJUST': priority_adjust})\n-        request = Request('https://example.com')\n+        spider = self.get_spider({\"RETRY_PRIORITY_ADJUST\": priority_adjust})\n+        request = Request(\"https://example.com\")\n         new_request = get_retry_request(\n             request,\n             spider=spider,\n@@ -443,8 +442,8 @@ class GetRetryRequestTest(unittest.TestCase):\n \n     def test_priority_adjust_argument(self):\n         priority_adjust = 1\n-        spider = self.get_spider({'RETRY_PRIORITY_ADJUST': priority_adjust + 1})\n-        request = Request('https://example.com')\n+        spider = self.get_spider({\"RETRY_PRIORITY_ADJUST\": priority_adjust + 1})\n+        request = Request(\"https://example.com\")\n         new_request = get_retry_request(\n             request,\n             spider=spider,\n@@ -453,9 +452,9 @@ class GetRetryRequestTest(unittest.TestCase):\n         self.assertEqual(new_request.priority, priority_adjust)\n \n     def test_log_extra_retry_success(self):\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         spider = self.get_spider()\n-        with LogCapture(attributes=('spider',)) as log:\n+        with LogCapture(attributes=(\"spider\",)) as log:\n             get_retry_request(\n                 request,\n                 spider=spider,\n@@ -463,9 +462,9 @@ class GetRetryRequestTest(unittest.TestCase):\n         log.check_present(spider)\n \n     def test_log_extra_retries_exceeded(self):\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         spider = self.get_spider()\n-        with LogCapture(attributes=('spider',)) as log:\n+        with LogCapture(attributes=(\"spider\",)) as log:\n             get_retry_request(\n                 request,\n                 spider=spider,\n@@ -474,9 +473,9 @@ class GetRetryRequestTest(unittest.TestCase):\n         log.check_present(spider)\n \n     def test_reason_string(self):\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         spider = self.get_spider()\n-        expected_reason = 'because'\n+        expected_reason = \"because\"\n         with LogCapture() as log:\n             get_retry_request(\n                 request,\n@@ -484,7 +483,7 @@ class GetRetryRequestTest(unittest.TestCase):\n                 reason=expected_reason,\n             )\n         expected_retry_times = 1\n-        for stat in ('retry/count', f'retry/reason_count/{expected_reason}'):\n+        for stat in (\"retry/count\", f\"retry/reason_count/{expected_reason}\"):\n             self.assertEqual(spider.crawler.stats.get_value(stat), 1)\n         log.check_present(\n             (\n@@ -496,10 +495,10 @@ class GetRetryRequestTest(unittest.TestCase):\n         )\n \n     def test_reason_builtin_exception(self):\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         spider = self.get_spider()\n         expected_reason = NotImplementedError()\n-        expected_reason_string = 'builtins.NotImplementedError'\n+        expected_reason_string = \"builtins.NotImplementedError\"\n         with LogCapture() as log:\n             get_retry_request(\n                 request,\n@@ -508,7 +507,7 @@ class GetRetryRequestTest(unittest.TestCase):\n             )\n         expected_retry_times = 1\n         stat = spider.crawler.stats.get_value(\n-            f'retry/reason_count/{expected_reason_string}'\n+            f\"retry/reason_count/{expected_reason_string}\"\n         )\n         self.assertEqual(stat, 1)\n         log.check_present(\n@@ -521,10 +520,10 @@ class GetRetryRequestTest(unittest.TestCase):\n         )\n \n     def test_reason_builtin_exception_class(self):\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         spider = self.get_spider()\n         expected_reason = NotImplementedError\n-        expected_reason_string = 'builtins.NotImplementedError'\n+        expected_reason_string = \"builtins.NotImplementedError\"\n         with LogCapture() as log:\n             get_retry_request(\n                 request,\n@@ -533,7 +532,7 @@ class GetRetryRequestTest(unittest.TestCase):\n             )\n         expected_retry_times = 1\n         stat = spider.crawler.stats.get_value(\n-            f'retry/reason_count/{expected_reason_string}'\n+            f\"retry/reason_count/{expected_reason_string}\"\n         )\n         self.assertEqual(stat, 1)\n         log.check_present(\n@@ -546,10 +545,10 @@ class GetRetryRequestTest(unittest.TestCase):\n         )\n \n     def test_reason_custom_exception(self):\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         spider = self.get_spider()\n         expected_reason = IgnoreRequest()\n-        expected_reason_string = 'scrapy.exceptions.IgnoreRequest'\n+        expected_reason_string = \"scrapy.exceptions.IgnoreRequest\"\n         with LogCapture() as log:\n             get_retry_request(\n                 request,\n@@ -558,7 +557,7 @@ class GetRetryRequestTest(unittest.TestCase):\n             )\n         expected_retry_times = 1\n         stat = spider.crawler.stats.get_value(\n-            f'retry/reason_count/{expected_reason_string}'\n+            f\"retry/reason_count/{expected_reason_string}\"\n         )\n         self.assertEqual(stat, 1)\n         log.check_present(\n@@ -571,10 +570,10 @@ class GetRetryRequestTest(unittest.TestCase):\n         )\n \n     def test_reason_custom_exception_class(self):\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         spider = self.get_spider()\n         expected_reason = IgnoreRequest\n-        expected_reason_string = 'scrapy.exceptions.IgnoreRequest'\n+        expected_reason_string = \"scrapy.exceptions.IgnoreRequest\"\n         with LogCapture() as log:\n             get_retry_request(\n                 request,\n@@ -583,7 +582,7 @@ class GetRetryRequestTest(unittest.TestCase):\n             )\n         expected_retry_times = 1\n         stat = spider.crawler.stats.get_value(\n-            f'retry/reason_count/{expected_reason_string}'\n+            f\"retry/reason_count/{expected_reason_string}\"\n         )\n         self.assertEqual(stat, 1)\n         log.check_present(\n@@ -626,7 +625,10 @@ class GetRetryRequestTest(unittest.TestCase):\n             reason=expected_reason,\n             stats_base_key=stats_key,\n         )\n-        for stat in (f\"{stats_key}/count\", f\"{stats_key}/reason_count/{expected_reason}\"):\n+        for stat in (\n+            f\"{stats_key}/count\",\n+            f\"{stats_key}/reason_count/{expected_reason}\",\n+        ):\n             self.assertEqual(spider.crawler.stats.get_value(stat), 1)\n \n \n\n@@ -4,8 +4,10 @@ from twisted.internet import reactor, error\n from twisted.internet.defer import Deferred, DeferredList, maybeDeferred\n from twisted.python import failure\n from twisted.trial import unittest\n-from scrapy.downloadermiddlewares.robotstxt import (RobotsTxtMiddleware,\n-                                                    logger as mw_module_logger)\n+from scrapy.downloadermiddlewares.robotstxt import (\n+    RobotsTxtMiddleware,\n+    logger as mw_module_logger,\n+)\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Request, Response, TextResponse\n from scrapy.settings import Settings\n@@ -13,7 +15,6 @@ from tests.test_robotstxt_interface import rerp_available, reppy_available\n \n \n class RobotsTxtMiddlewareTest(unittest.TestCase):\n-\n     def setUp(self):\n         self.crawler = mock.MagicMock()\n         self.crawler.settings = Settings()\n@@ -24,12 +25,12 @@ class RobotsTxtMiddlewareTest(unittest.TestCase):\n \n     def test_robotstxt_settings(self):\n         self.crawler.settings = Settings()\n-        self.crawler.settings.set('USER_AGENT', 'CustomAgent')\n+        self.crawler.settings.set(\"USER_AGENT\", \"CustomAgent\")\n         self.assertRaises(NotConfigured, RobotsTxtMiddleware, self.crawler)\n \n     def _get_successful_crawler(self):\n         crawler = self.crawler\n-        crawler.settings.set('ROBOTSTXT_OBEY', True)\n+        crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n         ROBOTS = \"\"\"\n User-Agent: *\n Disallow: /admin/\n@@ -39,138 +40,179 @@ Disallow: /wiki/K%C3%A4ytt%C3%A4j%C3%A4:\n Disallow: /wiki/Käyttäjä:\n User-Agent: UnicödeBöt\n Disallow: /some/randome/page.html\n-\"\"\".encode('utf-8')\n-        response = TextResponse('http://site.local/robots.txt', body=ROBOTS)\n+\"\"\".encode(\n+            \"utf-8\"\n+        )\n+        response = TextResponse(\"http://site.local/robots.txt\", body=ROBOTS)\n \n         def return_response(request):\n             deferred = Deferred()\n             reactor.callFromThread(deferred.callback, response)\n             return deferred\n+\n         crawler.engine.download.side_effect = return_response\n         return crawler\n \n     def test_robotstxt(self):\n         middleware = RobotsTxtMiddleware(self._get_successful_crawler())\n-        return DeferredList([\n-            self.assertNotIgnored(Request('http://site.local/allowed'), middleware),\n-            self.assertIgnored(Request('http://site.local/admin/main'), middleware),\n-            self.assertIgnored(Request('http://site.local/static/'), middleware),\n-            self.assertIgnored(Request('http://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:'), middleware),\n-            self.assertIgnored(Request('http://site.local/wiki/Käyttäjä:'), middleware)\n-        ], fireOnOneErrback=True)\n+        return DeferredList(\n+            [\n+                self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware),\n+                self.assertIgnored(Request(\"http://site.local/admin/main\"), middleware),\n+                self.assertIgnored(Request(\"http://site.local/static/\"), middleware),\n+                self.assertIgnored(\n+                    Request(\"http://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:\"),\n+                    middleware,\n+                ),\n+                self.assertIgnored(\n+                    Request(\"http://site.local/wiki/Käyttäjä:\"), middleware\n+                ),\n+            ],\n+            fireOnOneErrback=True,\n+        )\n \n     def test_robotstxt_ready_parser(self):\n         middleware = RobotsTxtMiddleware(self._get_successful_crawler())\n-        d = self.assertNotIgnored(Request('http://site.local/allowed'), middleware)\n-        d.addCallback(lambda _: self.assertNotIgnored(Request('http://site.local/allowed'), middleware))\n+        d = self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n+        d.addCallback(\n+            lambda _: self.assertNotIgnored(\n+                Request(\"http://site.local/allowed\"), middleware\n+            )\n+        )\n         return d\n \n     def test_robotstxt_meta(self):\n         middleware = RobotsTxtMiddleware(self._get_successful_crawler())\n-        meta = {'dont_obey_robotstxt': True}\n-        return DeferredList([\n-            self.assertNotIgnored(Request('http://site.local/allowed', meta=meta), middleware),\n-            self.assertNotIgnored(Request('http://site.local/admin/main', meta=meta), middleware),\n-            self.assertNotIgnored(Request('http://site.local/static/', meta=meta), middleware)\n-        ], fireOnOneErrback=True)\n+        meta = {\"dont_obey_robotstxt\": True}\n+        return DeferredList(\n+            [\n+                self.assertNotIgnored(\n+                    Request(\"http://site.local/allowed\", meta=meta), middleware\n+                ),\n+                self.assertNotIgnored(\n+                    Request(\"http://site.local/admin/main\", meta=meta), middleware\n+                ),\n+                self.assertNotIgnored(\n+                    Request(\"http://site.local/static/\", meta=meta), middleware\n+                ),\n+            ],\n+            fireOnOneErrback=True,\n+        )\n \n     def _get_garbage_crawler(self):\n         crawler = self.crawler\n-        crawler.settings.set('ROBOTSTXT_OBEY', True)\n-        response = Response('http://site.local/robots.txt', body=b'GIF89a\\xd3\\x00\\xfe\\x00\\xa2')\n+        crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n+        response = Response(\n+            \"http://site.local/robots.txt\", body=b\"GIF89a\\xd3\\x00\\xfe\\x00\\xa2\"\n+        )\n \n         def return_response(request):\n             deferred = Deferred()\n             reactor.callFromThread(deferred.callback, response)\n             return deferred\n+\n         crawler.engine.download.side_effect = return_response\n         return crawler\n \n     def test_robotstxt_garbage(self):\n         # garbage response should be discarded, equal 'allow all'\n         middleware = RobotsTxtMiddleware(self._get_garbage_crawler())\n-        deferred = DeferredList([\n-            self.assertNotIgnored(Request('http://site.local'), middleware),\n-            self.assertNotIgnored(Request('http://site.local/allowed'), middleware),\n-            self.assertNotIgnored(Request('http://site.local/admin/main'), middleware),\n-            self.assertNotIgnored(Request('http://site.local/static/'), middleware)\n-        ], fireOnOneErrback=True)\n+        deferred = DeferredList(\n+            [\n+                self.assertNotIgnored(Request(\"http://site.local\"), middleware),\n+                self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware),\n+                self.assertNotIgnored(\n+                    Request(\"http://site.local/admin/main\"), middleware\n+                ),\n+                self.assertNotIgnored(Request(\"http://site.local/static/\"), middleware),\n+            ],\n+            fireOnOneErrback=True,\n+        )\n         return deferred\n \n     def _get_emptybody_crawler(self):\n         crawler = self.crawler\n-        crawler.settings.set('ROBOTSTXT_OBEY', True)\n-        response = Response('http://site.local/robots.txt')\n+        crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n+        response = Response(\"http://site.local/robots.txt\")\n \n         def return_response(request):\n             deferred = Deferred()\n             reactor.callFromThread(deferred.callback, response)\n             return deferred\n+\n         crawler.engine.download.side_effect = return_response\n         return crawler\n \n     def test_robotstxt_empty_response(self):\n         # empty response should equal 'allow all'\n         middleware = RobotsTxtMiddleware(self._get_emptybody_crawler())\n-        return DeferredList([\n-            self.assertNotIgnored(Request('http://site.local/allowed'), middleware),\n-            self.assertNotIgnored(Request('http://site.local/admin/main'), middleware),\n-            self.assertNotIgnored(Request('http://site.local/static/'), middleware)\n-        ], fireOnOneErrback=True)\n+        return DeferredList(\n+            [\n+                self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware),\n+                self.assertNotIgnored(\n+                    Request(\"http://site.local/admin/main\"), middleware\n+                ),\n+                self.assertNotIgnored(Request(\"http://site.local/static/\"), middleware),\n+            ],\n+            fireOnOneErrback=True,\n+        )\n \n     def test_robotstxt_error(self):\n-        self.crawler.settings.set('ROBOTSTXT_OBEY', True)\n-        err = error.DNSLookupError('Robotstxt address not found')\n+        self.crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n+        err = error.DNSLookupError(\"Robotstxt address not found\")\n \n         def return_failure(request):\n             deferred = Deferred()\n             reactor.callFromThread(deferred.errback, failure.Failure(err))\n             return deferred\n+\n         self.crawler.engine.download.side_effect = return_failure\n \n         middleware = RobotsTxtMiddleware(self.crawler)\n         middleware._logerror = mock.MagicMock(side_effect=middleware._logerror)\n-        deferred = middleware.process_request(Request('http://site.local'), None)\n+        deferred = middleware.process_request(Request(\"http://site.local\"), None)\n         deferred.addCallback(lambda _: self.assertTrue(middleware._logerror.called))\n         return deferred\n \n     def test_robotstxt_immediate_error(self):\n-        self.crawler.settings.set('ROBOTSTXT_OBEY', True)\n-        err = error.DNSLookupError('Robotstxt address not found')\n+        self.crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n+        err = error.DNSLookupError(\"Robotstxt address not found\")\n \n         def immediate_failure(request):\n             deferred = Deferred()\n             deferred.errback(failure.Failure(err))\n             return deferred\n+\n         self.crawler.engine.download.side_effect = immediate_failure\n \n         middleware = RobotsTxtMiddleware(self.crawler)\n-        return self.assertNotIgnored(Request('http://site.local'), middleware)\n+        return self.assertNotIgnored(Request(\"http://site.local\"), middleware)\n \n     def test_ignore_robotstxt_request(self):\n-        self.crawler.settings.set('ROBOTSTXT_OBEY', True)\n+        self.crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n \n         def ignore_request(request):\n             deferred = Deferred()\n             reactor.callFromThread(deferred.errback, failure.Failure(IgnoreRequest()))\n             return deferred\n+\n         self.crawler.engine.download.side_effect = ignore_request\n \n         middleware = RobotsTxtMiddleware(self.crawler)\n         mw_module_logger.error = mock.MagicMock()\n \n-        d = self.assertNotIgnored(Request('http://site.local/allowed'), middleware)\n+        d = self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n         d.addCallback(lambda _: self.assertFalse(mw_module_logger.error.called))\n         return d\n \n     def test_robotstxt_user_agent_setting(self):\n         crawler = self._get_successful_crawler()\n-        crawler.settings.set('ROBOTSTXT_USER_AGENT', 'Examplebot')\n-        crawler.settings.set('USER_AGENT', 'Mozilla/5.0 (X11; Linux x86_64)')\n+        crawler.settings.set(\"ROBOTSTXT_USER_AGENT\", \"Examplebot\")\n+        crawler.settings.set(\"USER_AGENT\", \"Mozilla/5.0 (X11; Linux x86_64)\")\n         middleware = RobotsTxtMiddleware(crawler)\n         rp = mock.MagicMock(return_value=True)\n-        middleware.process_request_2(rp, Request('http://site.local/allowed'), None)\n-        rp.allowed.assert_called_once_with('http://site.local/allowed', 'Examplebot')\n+        middleware.process_request_2(rp, Request(\"http://site.local/allowed\"), None)\n+        rp.allowed.assert_called_once_with(\"http://site.local/allowed\", \"Examplebot\")\n \n     def assertNotIgnored(self, request, middleware):\n         spider = None  # not actually used\n@@ -180,8 +222,9 @@ Disallow: /some/randome/page.html\n \n     def assertIgnored(self, request, middleware):\n         spider = None  # not actually used\n-        return self.assertFailure(maybeDeferred(middleware.process_request, request, spider),\n-                                  IgnoreRequest)\n+        return self.assertFailure(\n+            maybeDeferred(middleware.process_request, request, spider), IgnoreRequest\n+        )\n \n \n class RobotsTxtMiddlewareWithRerpTest(RobotsTxtMiddlewareTest):\n@@ -190,7 +233,9 @@ class RobotsTxtMiddlewareWithRerpTest(RobotsTxtMiddlewareTest):\n \n     def setUp(self):\n         super().setUp()\n-        self.crawler.settings.set('ROBOTSTXT_PARSER', 'scrapy.robotstxt.RerpRobotParser')\n+        self.crawler.settings.set(\n+            \"ROBOTSTXT_PARSER\", \"scrapy.robotstxt.RerpRobotParser\"\n+        )\n \n \n class RobotsTxtMiddlewareWithReppyTest(RobotsTxtMiddlewareTest):\n@@ -199,4 +244,6 @@ class RobotsTxtMiddlewareWithReppyTest(RobotsTxtMiddlewareTest):\n \n     def setUp(self):\n         super().setUp()\n-        self.crawler.settings.set('ROBOTSTXT_PARSER', 'scrapy.robotstxt.ReppyRobotParser')\n+        self.crawler.settings.set(\n+            \"ROBOTSTXT_PARSER\", \"scrapy.robotstxt.ReppyRobotParser\"\n+        )\n\n@@ -15,59 +15,57 @@ class MyException(Exception):\n \n \n class TestDownloaderStats(TestCase):\n-\n     def setUp(self):\n         self.crawler = get_crawler(Spider)\n-        self.spider = self.crawler._create_spider('scrapytest.org')\n+        self.spider = self.crawler._create_spider(\"scrapytest.org\")\n         self.mw = DownloaderStats(self.crawler.stats)\n \n         self.crawler.stats.open_spider(self.spider)\n \n-        self.req = Request('http://scrapytest.org')\n-        self.res = Response('scrapytest.org', status=400)\n+        self.req = Request(\"http://scrapytest.org\")\n+        self.res = Response(\"scrapytest.org\", status=400)\n \n     def assertStatsEqual(self, key, value):\n         self.assertEqual(\n             self.crawler.stats.get_value(key, spider=self.spider),\n             value,\n-            str(self.crawler.stats.get_stats(self.spider))\n+            str(self.crawler.stats.get_stats(self.spider)),\n         )\n \n     def test_process_request(self):\n         self.mw.process_request(self.req, self.spider)\n-        self.assertStatsEqual('downloader/request_count', 1)\n+        self.assertStatsEqual(\"downloader/request_count\", 1)\n \n     def test_process_response(self):\n         self.mw.process_response(self.req, self.res, self.spider)\n-        self.assertStatsEqual('downloader/response_count', 1)\n+        self.assertStatsEqual(\"downloader/response_count\", 1)\n \n     def test_response_len(self):\n-        body = (b'', b'not_empty')  # empty/notempty body\n-        headers = ({}, {'lang': 'en'}, {'lang': 'en', 'User-Agent': 'scrapy'})  # 0 headers, 1h and 2h\n+        body = (b\"\", b\"not_empty\")  # empty/notempty body\n+        headers = (\n+            {},\n+            {\"lang\": \"en\"},\n+            {\"lang\": \"en\", \"User-Agent\": \"scrapy\"},\n+        )  # 0 headers, 1h and 2h\n         test_responses = [  # form test responses with all combinations of body/headers\n-            Response(\n-                url='scrapytest.org',\n-                status=200,\n-                body=r[0],\n-                headers=r[1]\n-            )\n+            Response(url=\"scrapytest.org\", status=200, body=r[0], headers=r[1])\n             for r in product(body, headers)\n         ]\n         for test_response in test_responses:\n-            self.crawler.stats.set_value('downloader/response_bytes', 0)\n+            self.crawler.stats.set_value(\"downloader/response_bytes\", 0)\n             self.mw.process_response(self.req, test_response, self.spider)\n             with warnings.catch_warnings():\n                 warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n                 resp_size = len(response_httprepr(test_response))\n-            self.assertStatsEqual('downloader/response_bytes', resp_size)\n+            self.assertStatsEqual(\"downloader/response_bytes\", resp_size)\n \n     def test_process_exception(self):\n         self.mw.process_exception(self.req, MyException(), self.spider)\n-        self.assertStatsEqual('downloader/exception_count', 1)\n+        self.assertStatsEqual(\"downloader/exception_count\", 1)\n         self.assertStatsEqual(\n-            'downloader/exception_type_count/tests.test_downloadermiddleware_stats.MyException',\n-            1\n+            \"downloader/exception_type_count/tests.test_downloadermiddleware_stats.MyException\",\n+            1,\n         )\n \n     def tearDown(self):\n-        self.crawler.stats.close_spider(self.spider, '')\n+        self.crawler.stats.close_spider(self.spider, \"\")\n\n@@ -7,48 +7,48 @@ from scrapy.utils.test import get_crawler\n \n \n class UserAgentMiddlewareTest(TestCase):\n-\n     def get_spider_and_mw(self, default_useragent):\n-        crawler = get_crawler(Spider, {'USER_AGENT': default_useragent})\n-        spider = crawler._create_spider('foo')\n+        crawler = get_crawler(Spider, {\"USER_AGENT\": default_useragent})\n+        spider = crawler._create_spider(\"foo\")\n         return spider, UserAgentMiddleware.from_crawler(crawler)\n \n     def test_default_agent(self):\n-        spider, mw = self.get_spider_and_mw('default_useragent')\n-        req = Request('http://scrapytest.org/')\n+        spider, mw = self.get_spider_and_mw(\"default_useragent\")\n+        req = Request(\"http://scrapytest.org/\")\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.headers['User-Agent'], b'default_useragent')\n+        self.assertEqual(req.headers[\"User-Agent\"], b\"default_useragent\")\n \n     def test_remove_agent(self):\n         # settings UESR_AGENT to None should remove the user agent\n-        spider, mw = self.get_spider_and_mw('default_useragent')\n+        spider, mw = self.get_spider_and_mw(\"default_useragent\")\n         spider.user_agent = None\n         mw.spider_opened(spider)\n-        req = Request('http://scrapytest.org/')\n+        req = Request(\"http://scrapytest.org/\")\n         assert mw.process_request(req, spider) is None\n-        assert req.headers.get('User-Agent') is None\n+        assert req.headers.get(\"User-Agent\") is None\n \n     def test_spider_agent(self):\n-        spider, mw = self.get_spider_and_mw('default_useragent')\n-        spider.user_agent = 'spider_useragent'\n+        spider, mw = self.get_spider_and_mw(\"default_useragent\")\n+        spider.user_agent = \"spider_useragent\"\n         mw.spider_opened(spider)\n-        req = Request('http://scrapytest.org/')\n+        req = Request(\"http://scrapytest.org/\")\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.headers['User-Agent'], b'spider_useragent')\n+        self.assertEqual(req.headers[\"User-Agent\"], b\"spider_useragent\")\n \n     def test_header_agent(self):\n-        spider, mw = self.get_spider_and_mw('default_useragent')\n-        spider.user_agent = 'spider_useragent'\n+        spider, mw = self.get_spider_and_mw(\"default_useragent\")\n+        spider.user_agent = \"spider_useragent\"\n         mw.spider_opened(spider)\n-        req = Request('http://scrapytest.org/',\n-                      headers={'User-Agent': 'header_useragent'})\n+        req = Request(\n+            \"http://scrapytest.org/\", headers={\"User-Agent\": \"header_useragent\"}\n+        )\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.headers['User-Agent'], b'header_useragent')\n+        self.assertEqual(req.headers[\"User-Agent\"], b\"header_useragent\")\n \n     def test_no_agent(self):\n         spider, mw = self.get_spider_and_mw(None)\n         spider.user_agent = None\n         mw.spider_opened(spider)\n-        req = Request('http://scrapytest.org/')\n+        req = Request(\"http://scrapytest.org/\")\n         assert mw.process_request(req, spider) is None\n-        assert 'User-Agent' not in req.headers\n+        assert \"User-Agent\" not in req.headers\n\n@@ -25,59 +25,62 @@ def _get_dupefilter(*, crawler=None, settings=None, open=True):\n \n \n class FromCrawlerRFPDupeFilter(RFPDupeFilter):\n-\n     @classmethod\n     def from_crawler(cls, crawler):\n         df = super().from_crawler(crawler)\n-        df.method = 'from_crawler'\n+        df.method = \"from_crawler\"\n         return df\n \n \n class FromSettingsRFPDupeFilter(RFPDupeFilter):\n-\n     @classmethod\n     def from_settings(cls, settings, *, fingerprinter=None):\n         df = super().from_settings(settings, fingerprinter=fingerprinter)\n-        df.method = 'from_settings'\n+        df.method = \"from_settings\"\n         return df\n \n \n class DirectDupeFilter:\n-    method = 'n/a'\n+    method = \"n/a\"\n \n \n class RFPDupeFilterTest(unittest.TestCase):\n-\n     def test_df_from_crawler_scheduler(self):\n-        settings = {'DUPEFILTER_DEBUG': True,\n-                    'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter,\n-                    'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}\n+        settings = {\n+            \"DUPEFILTER_DEBUG\": True,\n+            \"DUPEFILTER_CLASS\": FromCrawlerRFPDupeFilter,\n+            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n+        }\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         self.assertTrue(scheduler.df.debug)\n-        self.assertEqual(scheduler.df.method, 'from_crawler')\n+        self.assertEqual(scheduler.df.method, \"from_crawler\")\n \n     def test_df_from_settings_scheduler(self):\n-        settings = {'DUPEFILTER_DEBUG': True,\n-                    'DUPEFILTER_CLASS': FromSettingsRFPDupeFilter,\n-                    'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}\n+        settings = {\n+            \"DUPEFILTER_DEBUG\": True,\n+            \"DUPEFILTER_CLASS\": FromSettingsRFPDupeFilter,\n+            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n+        }\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         self.assertTrue(scheduler.df.debug)\n-        self.assertEqual(scheduler.df.method, 'from_settings')\n+        self.assertEqual(scheduler.df.method, \"from_settings\")\n \n     def test_df_direct_scheduler(self):\n-        settings = {'DUPEFILTER_CLASS': DirectDupeFilter,\n-                    'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}\n+        settings = {\n+            \"DUPEFILTER_CLASS\": DirectDupeFilter,\n+            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n+        }\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n-        self.assertEqual(scheduler.df.method, 'n/a')\n+        self.assertEqual(scheduler.df.method, \"n/a\")\n \n     def test_filter(self):\n         dupefilter = _get_dupefilter()\n-        r1 = Request('http://scrapytest.org/1')\n-        r2 = Request('http://scrapytest.org/2')\n-        r3 = Request('http://scrapytest.org/2')\n+        r1 = Request(\"http://scrapytest.org/1\")\n+        r2 = Request(\"http://scrapytest.org/2\")\n+        r3 = Request(\"http://scrapytest.org/2\")\n \n         assert not dupefilter.request_seen(r1)\n         assert dupefilter.request_seen(r1)\n@@ -85,23 +88,23 @@ class RFPDupeFilterTest(unittest.TestCase):\n         assert not dupefilter.request_seen(r2)\n         assert dupefilter.request_seen(r3)\n \n-        dupefilter.close('finished')\n+        dupefilter.close(\"finished\")\n \n     def test_dupefilter_path(self):\n-        r1 = Request('http://scrapytest.org/1')\n-        r2 = Request('http://scrapytest.org/2')\n+        r1 = Request(\"http://scrapytest.org/1\")\n+        r2 = Request(\"http://scrapytest.org/2\")\n \n         path = tempfile.mkdtemp()\n         try:\n-            df = _get_dupefilter(settings={'JOBDIR': path}, open=False)\n+            df = _get_dupefilter(settings={\"JOBDIR\": path}, open=False)\n             try:\n                 df.open()\n                 assert not df.request_seen(r1)\n                 assert df.request_seen(r1)\n             finally:\n-                df.close('finished')\n+                df.close(\"finished\")\n \n-            df2 = _get_dupefilter(settings={'JOBDIR': path}, open=False)\n+            df2 = _get_dupefilter(settings={\"JOBDIR\": path}, open=False)\n             assert df != df2\n             try:\n                 df2.open()\n@@ -109,7 +112,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n                 assert not df2.request_seen(r2)\n                 assert df2.request_seen(r2)\n             finally:\n-                df2.close('finished')\n+                df2.close(\"finished\")\n         finally:\n             shutil.rmtree(path)\n \n@@ -119,147 +122,156 @@ class RFPDupeFilterTest(unittest.TestCase):\n \n         \"\"\"\n         dupefilter = _get_dupefilter()\n-        r1 = Request('http://scrapytest.org/index.html')\n-        r2 = Request('http://scrapytest.org/INDEX.html')\n+        r1 = Request(\"http://scrapytest.org/index.html\")\n+        r2 = Request(\"http://scrapytest.org/INDEX.html\")\n \n         assert not dupefilter.request_seen(r1)\n         assert not dupefilter.request_seen(r2)\n \n-        dupefilter.close('finished')\n+        dupefilter.close(\"finished\")\n \n         class RequestFingerprinter:\n-\n             def fingerprint(self, request):\n                 fp = hashlib.sha1()\n                 fp.update(to_bytes(request.url.lower()))\n                 return fp.digest()\n \n-        settings = {'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter}\n+        settings = {\"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter}\n         case_insensitive_dupefilter = _get_dupefilter(settings=settings)\n \n         assert not case_insensitive_dupefilter.request_seen(r1)\n         assert case_insensitive_dupefilter.request_seen(r2)\n \n-        case_insensitive_dupefilter.close('finished')\n+        case_insensitive_dupefilter.close(\"finished\")\n \n     def test_seenreq_newlines(self):\n         \"\"\"Checks against adding duplicate \\r to\n         line endings on Windows platforms.\"\"\"\n \n-        r1 = Request('http://scrapytest.org/1')\n+        r1 = Request(\"http://scrapytest.org/1\")\n \n         path = tempfile.mkdtemp()\n-        crawler = get_crawler(settings_dict={'JOBDIR': path})\n+        crawler = get_crawler(settings_dict={\"JOBDIR\": path})\n         try:\n             scheduler = Scheduler.from_crawler(crawler)\n             df = scheduler.df\n             df.open()\n             df.request_seen(r1)\n-            df.close('finished')\n+            df.close(\"finished\")\n \n-            with Path(path, 'requests.seen').open('rb') as seen_file:\n+            with Path(path, \"requests.seen\").open(\"rb\") as seen_file:\n                 line = next(seen_file).decode()\n-                assert not line.endswith('\\r\\r\\n')\n-                if sys.platform == 'win32':\n-                    assert line.endswith('\\r\\n')\n+                assert not line.endswith(\"\\r\\r\\n\")\n+                if sys.platform == \"win32\":\n+                    assert line.endswith(\"\\r\\n\")\n                 else:\n-                    assert line.endswith('\\n')\n+                    assert line.endswith(\"\\n\")\n \n         finally:\n             shutil.rmtree(path)\n \n     def test_log(self):\n         with LogCapture() as log:\n-            settings = {'DUPEFILTER_DEBUG': False,\n-                        'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter,\n-                        'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}\n+            settings = {\n+                \"DUPEFILTER_DEBUG\": False,\n+                \"DUPEFILTER_CLASS\": FromCrawlerRFPDupeFilter,\n+                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n+            }\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n             spider = SimpleSpider.from_crawler(crawler)\n             dupefilter = _get_dupefilter(crawler=crawler)\n \n-            r1 = Request('http://scrapytest.org/index.html')\n-            r2 = Request('http://scrapytest.org/index.html')\n+            r1 = Request(\"http://scrapytest.org/index.html\")\n+            r2 = Request(\"http://scrapytest.org/index.html\")\n \n             dupefilter.log(r1, spider)\n             dupefilter.log(r2, spider)\n \n-            assert crawler.stats.get_value('dupefilter/filtered') == 2\n+            assert crawler.stats.get_value(\"dupefilter/filtered\") == 2\n             log.check_present(\n                 (\n-                    'scrapy.dupefilters',\n-                    'DEBUG',\n-                    'Filtered duplicate request: <GET http://scrapytest.org/index.html> - no more'\n-                    ' duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)'\n+                    \"scrapy.dupefilters\",\n+                    \"DEBUG\",\n+                    \"Filtered duplicate request: <GET http://scrapytest.org/index.html> - no more\"\n+                    \" duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\",\n                 )\n             )\n \n-            dupefilter.close('finished')\n+            dupefilter.close(\"finished\")\n \n     def test_log_debug(self):\n         with LogCapture() as log:\n-            settings = {'DUPEFILTER_DEBUG': True,\n-                        'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter,\n-                        'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}\n+            settings = {\n+                \"DUPEFILTER_DEBUG\": True,\n+                \"DUPEFILTER_CLASS\": FromCrawlerRFPDupeFilter,\n+                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n+            }\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n             spider = SimpleSpider.from_crawler(crawler)\n             dupefilter = _get_dupefilter(crawler=crawler)\n \n-            r1 = Request('http://scrapytest.org/index.html')\n-            r2 = Request('http://scrapytest.org/index.html',\n-                         headers={'Referer': 'http://scrapytest.org/INDEX.html'})\n+            r1 = Request(\"http://scrapytest.org/index.html\")\n+            r2 = Request(\n+                \"http://scrapytest.org/index.html\",\n+                headers={\"Referer\": \"http://scrapytest.org/INDEX.html\"},\n+            )\n \n             dupefilter.log(r1, spider)\n             dupefilter.log(r2, spider)\n \n-            assert crawler.stats.get_value('dupefilter/filtered') == 2\n+            assert crawler.stats.get_value(\"dupefilter/filtered\") == 2\n             log.check_present(\n                 (\n-                    'scrapy.dupefilters',\n-                    'DEBUG',\n-                    'Filtered duplicate request: <GET http://scrapytest.org/index.html> (referer: None)'\n+                    \"scrapy.dupefilters\",\n+                    \"DEBUG\",\n+                    \"Filtered duplicate request: <GET http://scrapytest.org/index.html> (referer: None)\",\n                 )\n             )\n             log.check_present(\n                 (\n-                    'scrapy.dupefilters',\n-                    'DEBUG',\n-                    'Filtered duplicate request: <GET http://scrapytest.org/index.html>'\n-                    ' (referer: http://scrapytest.org/INDEX.html)'\n+                    \"scrapy.dupefilters\",\n+                    \"DEBUG\",\n+                    \"Filtered duplicate request: <GET http://scrapytest.org/index.html>\"\n+                    \" (referer: http://scrapytest.org/INDEX.html)\",\n                 )\n             )\n \n-            dupefilter.close('finished')\n+            dupefilter.close(\"finished\")\n \n     def test_log_debug_default_dupefilter(self):\n         with LogCapture() as log:\n-            settings = {'DUPEFILTER_DEBUG': True,\n-                        'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}\n+            settings = {\n+                \"DUPEFILTER_DEBUG\": True,\n+                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n+            }\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n             spider = SimpleSpider.from_crawler(crawler)\n             dupefilter = _get_dupefilter(crawler=crawler)\n \n-            r1 = Request('http://scrapytest.org/index.html')\n-            r2 = Request('http://scrapytest.org/index.html',\n-                         headers={'Referer': 'http://scrapytest.org/INDEX.html'})\n+            r1 = Request(\"http://scrapytest.org/index.html\")\n+            r2 = Request(\n+                \"http://scrapytest.org/index.html\",\n+                headers={\"Referer\": \"http://scrapytest.org/INDEX.html\"},\n+            )\n \n             dupefilter.log(r1, spider)\n             dupefilter.log(r2, spider)\n \n-            assert crawler.stats.get_value('dupefilter/filtered') == 2\n+            assert crawler.stats.get_value(\"dupefilter/filtered\") == 2\n             log.check_present(\n                 (\n-                    'scrapy.dupefilters',\n-                    'DEBUG',\n-                    'Filtered duplicate request: <GET http://scrapytest.org/index.html> (referer: None)'\n+                    \"scrapy.dupefilters\",\n+                    \"DEBUG\",\n+                    \"Filtered duplicate request: <GET http://scrapytest.org/index.html> (referer: None)\",\n                 )\n             )\n             log.check_present(\n                 (\n-                    'scrapy.dupefilters',\n-                    'DEBUG',\n-                    'Filtered duplicate request: <GET http://scrapytest.org/index.html>'\n-                    ' (referer: http://scrapytest.org/INDEX.html)'\n+                    \"scrapy.dupefilters\",\n+                    \"DEBUG\",\n+                    \"Filtered duplicate request: <GET http://scrapytest.org/index.html>\"\n+                    \" (referer: http://scrapytest.org/INDEX.html)\",\n                 )\n             )\n \n-            dupefilter.close('finished')\n+            dupefilter.close(\"finished\")\n\n@@ -81,11 +81,11 @@ class TestSpider(Spider):\n         adapter = ItemAdapter(self.item_cls())\n         m = self.name_re.search(response.text)\n         if m:\n-            adapter['name'] = m.group(1)\n-        adapter['url'] = response.url\n+            adapter[\"name\"] = m.group(1)\n+        adapter[\"url\"] = response.url\n         m = self.price_re.search(response.text)\n         if m:\n-            adapter['price'] = m.group(1)\n+            adapter[\"price\"] = m.group(1)\n         return adapter.item\n \n \n@@ -136,8 +136,10 @@ def start_test_site(debug=False):\n \n     port = reactor.listenTCP(0, server.Site(r), interface=\"127.0.0.1\")\n     if debug:\n-        print(f\"Test server running at http://localhost:{port.getHost().port}/ \"\n-              \"- hit Ctrl-C to finish.\")\n+        print(\n+            f\"Test server running at http://localhost:{port.getHost().port}/ \"\n+            \"- hit Ctrl-C to finish.\"\n+        )\n     return port\n \n \n@@ -169,7 +171,7 @@ class CrawlerRun:\n         ]\n \n         for name, signal in vars(signals).items():\n-            if not name.startswith('_'):\n+            if not name.startswith(\"_\"):\n                 dispatcher.connect(self.record_signal, signal)\n \n         self.crawler = get_crawler(self.spider_class)\n@@ -179,8 +181,12 @@ class CrawlerRun:\n         self.crawler.signals.connect(self.bytes_received, signals.bytes_received)\n         self.crawler.signals.connect(self.request_scheduled, signals.request_scheduled)\n         self.crawler.signals.connect(self.request_dropped, signals.request_dropped)\n-        self.crawler.signals.connect(self.request_reached, signals.request_reached_downloader)\n-        self.crawler.signals.connect(self.response_downloaded, signals.response_downloaded)\n+        self.crawler.signals.connect(\n+            self.request_reached, signals.request_reached_downloader\n+        )\n+        self.crawler.signals.connect(\n+            self.response_downloaded, signals.response_downloaded\n+        )\n         self.crawler.crawl(start_urls=start_urls)\n         self.spider = self.crawler.spider\n \n@@ -191,7 +197,7 @@ class CrawlerRun:\n     def stop(self):\n         self.port.stopListening()  # FIXME: wait for this Deferred\n         for name, signal in vars(signals).items():\n-            if not name.startswith('_'):\n+            if not name.startswith(\"_\"):\n                 disconnect_all(signal)\n         self.deferred.callback(None)\n         return self.crawler.stop()\n@@ -230,8 +236,8 @@ class CrawlerRun:\n     def record_signal(self, *args, **kwargs):\n         \"\"\"Record a signal and its parameters\"\"\"\n         signalargs = kwargs.copy()\n-        sig = signalargs.pop('signal')\n-        signalargs.pop('sender', None)\n+        sig = signalargs.pop(\"signal\")\n+        signalargs.pop(\"sender\", None)\n         self.signals_caught[sig] = signalargs\n \n \n@@ -239,7 +245,12 @@ class EngineTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_crawler(self):\n \n-        for spider in (TestSpider, DictItemsSpider, AttrsItemsSpider, DataClassItemsSpider):\n+        for spider in (\n+            TestSpider,\n+            DictItemsSpider,\n+            AttrsItemsSpider,\n+            DataClassItemsSpider,\n+        ):\n             run = CrawlerRun(spider)\n             yield run.run()\n             self._assert_visited_urls(run)\n@@ -266,20 +277,30 @@ class EngineTest(unittest.TestCase):\n     def test_crawler_change_close_reason_on_idle(self):\n         run = CrawlerRun(ChangeCloseReasonSpider)\n         yield run.run()\n-        self.assertEqual({'spider': run.spider, 'reason': 'custom_reason'},\n-                         run.signals_caught[signals.spider_closed])\n+        self.assertEqual(\n+            {\"spider\": run.spider, \"reason\": \"custom_reason\"},\n+            run.signals_caught[signals.spider_closed],\n+        )\n \n     def _assert_visited_urls(self, run: CrawlerRun):\n-        must_be_visited = [\"/\", \"/redirect\", \"/redirected\",\n-                           \"/item1.html\", \"/item2.html\", \"/item999.html\"]\n+        must_be_visited = [\n+            \"/\",\n+            \"/redirect\",\n+            \"/redirected\",\n+            \"/item1.html\",\n+            \"/item2.html\",\n+            \"/item999.html\",\n+        ]\n         urls_visited = {rp[0].url for rp in run.respplug}\n         urls_expected = {run.geturl(p) for p in must_be_visited}\n-        assert urls_expected <= urls_visited, f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n+        assert (\n+            urls_expected <= urls_visited\n+        ), f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n \n     def _assert_scheduled_requests(self, run: CrawlerRun, count=None):\n         self.assertEqual(count, len(run.reqplug))\n \n-        paths_expected = ['/item999.html', '/item2.html', '/item1.html']\n+        paths_expected = [\"/item999.html\", \"/item2.html\", \"/item1.html\"]\n \n         urls_requested = {rq[0].url for rq in run.reqplug}\n         urls_expected = {run.geturl(p) for p in paths_expected}\n@@ -287,10 +308,10 @@ class EngineTest(unittest.TestCase):\n         scheduled_requests_count = len(run.reqplug)\n         dropped_requests_count = len(run.reqdropped)\n         responses_count = len(run.respplug)\n-        self.assertEqual(scheduled_requests_count,\n-                         dropped_requests_count + responses_count)\n-        self.assertEqual(len(run.reqreached),\n-                         responses_count)\n+        self.assertEqual(\n+            scheduled_requests_count, dropped_requests_count + responses_count\n+        )\n+        self.assertEqual(len(run.reqreached), responses_count)\n \n     def _assert_dropped_requests(self, run: CrawlerRun):\n         self.assertEqual(len(run.reqdropped), 1)\n@@ -301,9 +322,9 @@ class EngineTest(unittest.TestCase):\n         self.assertEqual(count, len(run.reqreached))\n \n         for response, _ in run.respplug:\n-            if run.getpath(response.url) == '/item999.html':\n+            if run.getpath(response.url) == \"/item999.html\":\n                 self.assertEqual(404, response.status)\n-            if run.getpath(response.url) == '/redirect':\n+            if run.getpath(response.url) == \"/redirect\":\n                 self.assertEqual(302, response.status)\n \n     def _assert_items_error(self, run: CrawlerRun):\n@@ -312,25 +333,25 @@ class EngineTest(unittest.TestCase):\n             self.assertEqual(failure.value.__class__, ZeroDivisionError)\n             self.assertEqual(spider, run.spider)\n \n-            self.assertEqual(item['url'], response.url)\n-            if 'item1.html' in item['url']:\n-                self.assertEqual('Item 1 name', item['name'])\n-                self.assertEqual('100', item['price'])\n-            if 'item2.html' in item['url']:\n-                self.assertEqual('Item 2 name', item['name'])\n-                self.assertEqual('200', item['price'])\n+            self.assertEqual(item[\"url\"], response.url)\n+            if \"item1.html\" in item[\"url\"]:\n+                self.assertEqual(\"Item 1 name\", item[\"name\"])\n+                self.assertEqual(\"100\", item[\"price\"])\n+            if \"item2.html\" in item[\"url\"]:\n+                self.assertEqual(\"Item 2 name\", item[\"name\"])\n+                self.assertEqual(\"200\", item[\"price\"])\n \n     def _assert_scraped_items(self, run: CrawlerRun):\n         self.assertEqual(2, len(run.itemresp))\n         for item, response in run.itemresp:\n             item = ItemAdapter(item)\n-            self.assertEqual(item['url'], response.url)\n-            if 'item1.html' in item['url']:\n-                self.assertEqual('Item 1 name', item['name'])\n-                self.assertEqual('100', item['price'])\n-            if 'item2.html' in item['url']:\n-                self.assertEqual('Item 2 name', item['name'])\n-                self.assertEqual('200', item['price'])\n+            self.assertEqual(item[\"url\"], response.url)\n+            if \"item1.html\" in item[\"url\"]:\n+                self.assertEqual(\"Item 1 name\", item[\"name\"])\n+                self.assertEqual(\"100\", item[\"price\"])\n+            if \"item2.html\" in item[\"url\"]:\n+                self.assertEqual(\"Item 2 name\", item[\"name\"])\n+                self.assertEqual(\"200\", item[\"price\"])\n \n     def _assert_headers_received(self, run: CrawlerRun):\n         for headers in run.headers.values():\n@@ -351,17 +372,17 @@ class EngineTest(unittest.TestCase):\n                 self.assertEqual(joined_data, get_testdata(\"test_site\", \"item2.html\"))\n             elif run.getpath(request.url) == \"/redirected\":\n                 self.assertEqual(joined_data, b\"Redirected here\")\n-            elif run.getpath(request.url) == '/redirect':\n+            elif run.getpath(request.url) == \"/redirect\":\n                 self.assertEqual(\n                     joined_data,\n                     b\"\\n<html>\\n\"\n                     b\"    <head>\\n\"\n-                    b\"        <meta http-equiv=\\\"refresh\\\" content=\\\"0;URL=/redirected\\\">\\n\"\n+                    b'        <meta http-equiv=\"refresh\" content=\"0;URL=/redirected\">\\n'\n                     b\"    </head>\\n\"\n-                    b\"    <body bgcolor=\\\"#FFFFFF\\\" text=\\\"#000000\\\">\\n\"\n-                    b\"    <a href=\\\"/redirected\\\">click here</a>\\n\"\n+                    b'    <body bgcolor=\"#FFFFFF\" text=\"#000000\">\\n'\n+                    b'    <a href=\"/redirected\">click here</a>\\n'\n                     b\"    </body>\\n\"\n-                    b\"</html>\\n\"\n+                    b\"</html>\\n\",\n                 )\n             elif run.getpath(request.url) == \"/tem999.html\":\n                 self.assertEqual(\n@@ -372,7 +393,7 @@ class EngineTest(unittest.TestCase):\n                     b\"    <h1>No Such Resource</h1>\\n\"\n                     b\"    <p>File not found.</p>\\n\"\n                     b\"  </body>\\n\"\n-                    b\"</html>\\n\"\n+                    b\"</html>\\n\",\n                 )\n             elif run.getpath(request.url) == \"/numbers\":\n                 # signal was fired multiple times\n@@ -389,12 +410,16 @@ class EngineTest(unittest.TestCase):\n         assert signals.spider_closed in run.signals_caught\n         assert signals.headers_received in run.signals_caught\n \n-        self.assertEqual({'spider': run.spider},\n-                         run.signals_caught[signals.spider_opened])\n-        self.assertEqual({'spider': run.spider},\n-                         run.signals_caught[signals.spider_idle])\n-        self.assertEqual({'spider': run.spider, 'reason': 'finished'},\n-                         run.signals_caught[signals.spider_closed])\n+        self.assertEqual(\n+            {\"spider\": run.spider}, run.signals_caught[signals.spider_opened]\n+        )\n+        self.assertEqual(\n+            {\"spider\": run.spider}, run.signals_caught[signals.spider_idle]\n+        )\n+        self.assertEqual(\n+            {\"spider\": run.spider, \"reason\": \"finished\"},\n+            run.signals_caught[signals.spider_closed],\n+        )\n \n     @defer.inlineCallbacks\n     def test_close_downloader(self):\n@@ -415,9 +440,11 @@ class EngineTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_close_spiders_downloader(self):\n-        with pytest.warns(ScrapyDeprecationWarning,\n+        with pytest.warns(\n+            ScrapyDeprecationWarning,\n             match=\"ExecutionEngine.open_spiders is deprecated, \"\n-                                \"please use ExecutionEngine.spider instead\"):\n+            \"please use ExecutionEngine.spider instead\",\n+        ):\n             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n             yield e.open_spider(TestSpider(), [])\n             self.assertEqual(len(e.open_spiders), 1)\n@@ -426,9 +453,11 @@ class EngineTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_close_engine_spiders_downloader(self):\n-        with pytest.warns(ScrapyDeprecationWarning,\n+        with pytest.warns(\n+            ScrapyDeprecationWarning,\n             match=\"ExecutionEngine.open_spiders is deprecated, \"\n-                                \"please use ExecutionEngine.spider instead\"):\n+            \"please use ExecutionEngine.spider instead\",\n+        ):\n             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n             yield e.open_spider(TestSpider(), [])\n             e.start()\n@@ -439,9 +468,11 @@ class EngineTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_crawl_deprecated_spider_arg(self):\n-        with pytest.warns(ScrapyDeprecationWarning,\n+        with pytest.warns(\n+            ScrapyDeprecationWarning,\n             match=\"Passing a 'spider' argument to \"\n-                                \"ExecutionEngine.crawl is deprecated\"):\n+            \"ExecutionEngine.crawl is deprecated\",\n+        ):\n             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n             spider = TestSpider()\n             yield e.open_spider(spider, [])\n@@ -451,9 +482,11 @@ class EngineTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_download_deprecated_spider_arg(self):\n-        with pytest.warns(ScrapyDeprecationWarning,\n+        with pytest.warns(\n+            ScrapyDeprecationWarning,\n             match=\"Passing a 'spider' argument to \"\n-                                \"ExecutionEngine.download is deprecated\"):\n+            \"ExecutionEngine.download is deprecated\",\n+        ):\n             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n             spider = TestSpider()\n             yield e.open_spider(spider, [])\n@@ -463,9 +496,11 @@ class EngineTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_deprecated_schedule(self):\n-        with pytest.warns(ScrapyDeprecationWarning,\n+        with pytest.warns(\n+            ScrapyDeprecationWarning,\n             match=\"ExecutionEngine.schedule is deprecated, please use \"\n-                                \"ExecutionEngine.crawl or ExecutionEngine.download instead\"):\n+            \"ExecutionEngine.crawl or ExecutionEngine.download instead\",\n+        ):\n             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n             spider = TestSpider()\n             yield e.open_spider(spider, [])\n@@ -475,8 +510,9 @@ class EngineTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_deprecated_has_capacity(self):\n-        with pytest.warns(ScrapyDeprecationWarning,\n-                          match=\"ExecutionEngine.has_capacity is deprecated\"):\n+        with pytest.warns(\n+            ScrapyDeprecationWarning, match=\"ExecutionEngine.has_capacity is deprecated\"\n+        ):\n             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n             self.assertTrue(e.has_capacity())\n             spider = TestSpider()\n@@ -489,14 +525,14 @@ class EngineTest(unittest.TestCase):\n     def test_short_timeout(self):\n         args = (\n             sys.executable,\n-            '-m',\n-            'scrapy.cmdline',\n-            'fetch',\n-            '-s',\n-            'CLOSESPIDER_TIMEOUT=0.001',\n-            '-s',\n-            'LOG_LEVEL=DEBUG',\n-            'http://toscrape.com',\n+            \"-m\",\n+            \"scrapy.cmdline\",\n+            \"fetch\",\n+            \"-s\",\n+            \"CLOSESPIDER_TIMEOUT=0.001\",\n+            \"-s\",\n+            \"LOG_LEVEL=DEBUG\",\n+            \"http://toscrape.com\",\n         )\n         p = subprocess.Popen(\n             args,\n@@ -506,7 +542,7 @@ class EngineTest(unittest.TestCase):\n         def kill_proc():\n             p.kill()\n             p.communicate()\n-            assert False, 'Command took too much time to complete'\n+            assert False, \"Command took too much time to complete\"\n \n         timer = Timer(15, kill_proc)\n         try:\n@@ -515,10 +551,10 @@ class EngineTest(unittest.TestCase):\n         finally:\n             timer.cancel()\n \n-        self.assertNotIn(b'Traceback', stderr)\n+        self.assertNotIn(b\"Traceback\", stderr)\n \n \n if __name__ == \"__main__\":\n-    if len(sys.argv) > 1 and sys.argv[1] == 'runserver':\n+    if len(sys.argv) > 1 and sys.argv[1] == \"runserver\":\n         start_test_site(debug=True)\n         reactor.run()\n\n@@ -22,22 +22,39 @@ class BytesReceivedCrawlerRun(CrawlerRun):\n class BytesReceivedEngineTest(EngineTest):\n     @defer.inlineCallbacks\n     def test_crawler(self):\n-        for spider in (TestSpider, DictItemsSpider, AttrsItemsSpider, DataClassItemsSpider):\n+        for spider in (\n+            TestSpider,\n+            DictItemsSpider,\n+            AttrsItemsSpider,\n+            DataClassItemsSpider,\n+        ):\n             run = BytesReceivedCrawlerRun(spider)\n             with LogCapture() as log:\n                 yield run.run()\n-                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n+                log.check_present(\n+                    (\n+                        \"scrapy.core.downloader.handlers.http11\",\n                         \"DEBUG\",\n                         f\"Download stopped for <GET http://localhost:{run.portno}/redirected> \"\n-                                   \"from signal handler BytesReceivedCrawlerRun.bytes_received\"))\n-                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n+                        \"from signal handler BytesReceivedCrawlerRun.bytes_received\",\n+                    )\n+                )\n+                log.check_present(\n+                    (\n+                        \"scrapy.core.downloader.handlers.http11\",\n                         \"DEBUG\",\n                         f\"Download stopped for <GET http://localhost:{run.portno}/> \"\n-                                   \"from signal handler BytesReceivedCrawlerRun.bytes_received\"))\n-                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n+                        \"from signal handler BytesReceivedCrawlerRun.bytes_received\",\n+                    )\n+                )\n+                log.check_present(\n+                    (\n+                        \"scrapy.core.downloader.handlers.http11\",\n                         \"DEBUG\",\n                         f\"Download stopped for <GET http://localhost:{run.portno}/numbers> \"\n-                                   \"from signal handler BytesReceivedCrawlerRun.bytes_received\"))\n+                        \"from signal handler BytesReceivedCrawlerRun.bytes_received\",\n+                    )\n+                )\n             self._assert_visited_urls(run)\n             self._assert_scheduled_requests(run, count=9)\n             self._assert_downloaded_responses(run, count=9)\n\n@@ -22,22 +22,39 @@ class HeadersReceivedCrawlerRun(CrawlerRun):\n class HeadersReceivedEngineTest(EngineTest):\n     @defer.inlineCallbacks\n     def test_crawler(self):\n-        for spider in (TestSpider, DictItemsSpider, AttrsItemsSpider, DataClassItemsSpider):\n+        for spider in (\n+            TestSpider,\n+            DictItemsSpider,\n+            AttrsItemsSpider,\n+            DataClassItemsSpider,\n+        ):\n             run = HeadersReceivedCrawlerRun(spider)\n             with LogCapture() as log:\n                 yield run.run()\n-                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n+                log.check_present(\n+                    (\n+                        \"scrapy.core.downloader.handlers.http11\",\n                         \"DEBUG\",\n                         f\"Download stopped for <GET http://localhost:{run.portno}/redirected> from\"\n-                                   \" signal handler HeadersReceivedCrawlerRun.headers_received\"))\n-                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n+                        \" signal handler HeadersReceivedCrawlerRun.headers_received\",\n+                    )\n+                )\n+                log.check_present(\n+                    (\n+                        \"scrapy.core.downloader.handlers.http11\",\n                         \"DEBUG\",\n                         f\"Download stopped for <GET http://localhost:{run.portno}/> from signal\"\n-                                   \" handler HeadersReceivedCrawlerRun.headers_received\"))\n-                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n+                        \" handler HeadersReceivedCrawlerRun.headers_received\",\n+                    )\n+                )\n+                log.check_present(\n+                    (\n+                        \"scrapy.core.downloader.handlers.http11\",\n                         \"DEBUG\",\n                         f\"Download stopped for <GET http://localhost:{run.portno}/numbers> from\"\n-                                   \" signal handler HeadersReceivedCrawlerRun.headers_received\"))\n+                        \" signal handler HeadersReceivedCrawlerRun.headers_received\",\n+                    )\n+                )\n             self._assert_visited_urls(run)\n             self._assert_downloaded_responses(run, count=6)\n             self._assert_signals_caught(run)\n@@ -51,4 +68,6 @@ class HeadersReceivedEngineTest(EngineTest):\n         must_be_visited = [\"/\", \"/redirect\", \"/redirected\"]\n         urls_visited = {rp[0].url for rp in run.respplug}\n         urls_expected = {run.geturl(p) for p in must_be_visited}\n-        assert urls_expected <= urls_visited, f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n+        assert (\n+            urls_expected <= urls_visited\n+        ), f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n\n@@ -16,9 +16,15 @@ from scrapy.item import Item, Field\n from scrapy.utils.python import to_unicode\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.exporters import (\n-    BaseItemExporter, PprintItemExporter, PickleItemExporter, CsvItemExporter,\n-    XmlItemExporter, JsonLinesItemExporter, JsonItemExporter,\n-    PythonItemExporter, MarshalItemExporter\n+    BaseItemExporter,\n+    PprintItemExporter,\n+    PickleItemExporter,\n+    CsvItemExporter,\n+    XmlItemExporter,\n+    JsonLinesItemExporter,\n+    JsonItemExporter,\n+    PythonItemExporter,\n+    MarshalItemExporter,\n )\n \n \n@@ -54,7 +60,7 @@ class BaseItemExporterTest(unittest.TestCase):\n     custom_field_item_class = CustomFieldItem\n \n     def setUp(self):\n-        self.i = self.item_class(name='John\\xa3', age='22')\n+        self.i = self.item_class(name=\"John\\xa3\", age=\"22\")\n         self.output = BytesIO()\n         self.ie = self._get_exporter()\n \n@@ -71,10 +77,10 @@ class BaseItemExporterTest(unittest.TestCase):\n \n     def _get_nonstring_types_item(self):\n         return {\n-            'boolean': False,\n-            'number': 22,\n-            'time': datetime(2015, 1, 1, 1, 1, 1),\n-            'float': 3.14,\n+            \"boolean\": False,\n+            \"number\": 22,\n+            \"time\": datetime(2015, 1, 1, 1, 1, 1),\n+            \"float\": 3.14,\n         }\n \n     def assertItemExportWorks(self, item):\n@@ -95,35 +101,36 @@ class BaseItemExporterTest(unittest.TestCase):\n \n     def test_serialize_field(self):\n         a = ItemAdapter(self.i)\n-        res = self.ie.serialize_field(a.get_field_meta('name'), 'name', a['name'])\n-        self.assertEqual(res, 'John\\xa3')\n+        res = self.ie.serialize_field(a.get_field_meta(\"name\"), \"name\", a[\"name\"])\n+        self.assertEqual(res, \"John\\xa3\")\n \n-        res = self.ie.serialize_field(a.get_field_meta('age'), 'age', a['age'])\n-        self.assertEqual(res, '22')\n+        res = self.ie.serialize_field(a.get_field_meta(\"age\"), \"age\", a[\"age\"])\n+        self.assertEqual(res, \"22\")\n \n     def test_fields_to_export(self):\n-        ie = self._get_exporter(fields_to_export=['name'])\n-        self.assertEqual(list(ie._get_serialized_fields(self.i)), [('name', 'John\\xa3')])\n+        ie = self._get_exporter(fields_to_export=[\"name\"])\n+        self.assertEqual(\n+            list(ie._get_serialized_fields(self.i)), [(\"name\", \"John\\xa3\")]\n+        )\n \n-        ie = self._get_exporter(fields_to_export=['name'], encoding='latin-1')\n+        ie = self._get_exporter(fields_to_export=[\"name\"], encoding=\"latin-1\")\n         _, name = list(ie._get_serialized_fields(self.i))[0]\n         assert isinstance(name, str)\n-        self.assertEqual(name, 'John\\xa3')\n+        self.assertEqual(name, \"John\\xa3\")\n \n-        ie = self._get_exporter(\n-            fields_to_export={'name': '名稱'}\n-        )\n-        self.assertEqual(\n-            list(ie._get_serialized_fields(self.i)),\n-            [('名稱', 'John\\xa3')]\n-        )\n+        ie = self._get_exporter(fields_to_export={\"name\": \"名稱\"})\n+        self.assertEqual(list(ie._get_serialized_fields(self.i)), [(\"名稱\", \"John\\xa3\")])\n \n     def test_field_custom_serializer(self):\n-        i = self.custom_field_item_class(name='John\\xa3', age='22')\n+        i = self.custom_field_item_class(name=\"John\\xa3\", age=\"22\")\n         a = ItemAdapter(i)\n         ie = self._get_exporter()\n-        self.assertEqual(ie.serialize_field(a.get_field_meta('name'), 'name', a['name']), 'John\\xa3')\n-        self.assertEqual(ie.serialize_field(a.get_field_meta('age'), 'age', a['age']), '24')\n+        self.assertEqual(\n+            ie.serialize_field(a.get_field_meta(\"name\"), \"name\", a[\"name\"]), \"John\\xa3\"\n+        )\n+        self.assertEqual(\n+            ie.serialize_field(a.get_field_meta(\"age\"), \"age\", a[\"age\"]), \"24\"\n+        )\n \n \n class BaseItemExporterDataclassTest(BaseItemExporterTest):\n@@ -137,54 +144,63 @@ class PythonItemExporterTest(BaseItemExporterTest):\n \n     def test_invalid_option(self):\n         with self.assertRaisesRegex(TypeError, \"Unexpected options: invalid_option\"):\n-            PythonItemExporter(invalid_option='something')\n+            PythonItemExporter(invalid_option=\"something\")\n \n     def test_nested_item(self):\n-        i1 = self.item_class(name='Joseph', age='22')\n-        i2 = dict(name='Maria', age=i1)\n-        i3 = self.item_class(name='Jesus', age=i2)\n+        i1 = self.item_class(name=\"Joseph\", age=\"22\")\n+        i2 = dict(name=\"Maria\", age=i1)\n+        i3 = self.item_class(name=\"Jesus\", age=i2)\n         ie = self._get_exporter()\n         exported = ie.export_item(i3)\n         self.assertEqual(type(exported), dict)\n         self.assertEqual(\n             exported,\n-            {'age': {'age': {'age': '22', 'name': 'Joseph'}, 'name': 'Maria'}, 'name': 'Jesus'}\n+            {\n+                \"age\": {\"age\": {\"age\": \"22\", \"name\": \"Joseph\"}, \"name\": \"Maria\"},\n+                \"name\": \"Jesus\",\n+            },\n         )\n-        self.assertEqual(type(exported['age']), dict)\n-        self.assertEqual(type(exported['age']['age']), dict)\n+        self.assertEqual(type(exported[\"age\"]), dict)\n+        self.assertEqual(type(exported[\"age\"][\"age\"]), dict)\n \n     def test_export_list(self):\n-        i1 = self.item_class(name='Joseph', age='22')\n-        i2 = self.item_class(name='Maria', age=[i1])\n-        i3 = self.item_class(name='Jesus', age=[i2])\n+        i1 = self.item_class(name=\"Joseph\", age=\"22\")\n+        i2 = self.item_class(name=\"Maria\", age=[i1])\n+        i3 = self.item_class(name=\"Jesus\", age=[i2])\n         ie = self._get_exporter()\n         exported = ie.export_item(i3)\n         self.assertEqual(\n             exported,\n-            {'age': [{'age': [{'age': '22', 'name': 'Joseph'}], 'name': 'Maria'}], 'name': 'Jesus'}\n+            {\n+                \"age\": [{\"age\": [{\"age\": \"22\", \"name\": \"Joseph\"}], \"name\": \"Maria\"}],\n+                \"name\": \"Jesus\",\n+            },\n         )\n-        self.assertEqual(type(exported['age'][0]), dict)\n-        self.assertEqual(type(exported['age'][0]['age'][0]), dict)\n+        self.assertEqual(type(exported[\"age\"][0]), dict)\n+        self.assertEqual(type(exported[\"age\"][0][\"age\"][0]), dict)\n \n     def test_export_item_dict_list(self):\n-        i1 = self.item_class(name='Joseph', age='22')\n-        i2 = dict(name='Maria', age=[i1])\n-        i3 = self.item_class(name='Jesus', age=[i2])\n+        i1 = self.item_class(name=\"Joseph\", age=\"22\")\n+        i2 = dict(name=\"Maria\", age=[i1])\n+        i3 = self.item_class(name=\"Jesus\", age=[i2])\n         ie = self._get_exporter()\n         exported = ie.export_item(i3)\n         self.assertEqual(\n             exported,\n-            {'age': [{'age': [{'age': '22', 'name': 'Joseph'}], 'name': 'Maria'}], 'name': 'Jesus'}\n+            {\n+                \"age\": [{\"age\": [{\"age\": \"22\", \"name\": \"Joseph\"}], \"name\": \"Maria\"}],\n+                \"name\": \"Jesus\",\n+            },\n         )\n-        self.assertEqual(type(exported['age'][0]), dict)\n-        self.assertEqual(type(exported['age'][0]['age'][0]), dict)\n+        self.assertEqual(type(exported[\"age\"][0]), dict)\n+        self.assertEqual(type(exported[\"age\"][0][\"age\"][0]), dict)\n \n     def test_export_binary(self):\n         with catch_warnings():\n-            filterwarnings('ignore', category=ScrapyDeprecationWarning)\n+            filterwarnings(\"ignore\", category=ScrapyDeprecationWarning)\n             exporter = PythonItemExporter(binary=True)\n-            value = self.item_class(name='John\\xa3', age='22')\n-            expected = {b'name': b'John\\xc2\\xa3', b'age': b'22'}\n+            value = self.item_class(name=\"John\\xa3\", age=\"22\")\n+            expected = {b\"name\": b\"John\\xc2\\xa3\", b\"age\": b\"22\"}\n             self.assertEqual(expected, exporter.export_item(value))\n \n     def test_nonstring_types_item(self):\n@@ -200,7 +216,6 @@ class PythonItemExporterDataclassTest(PythonItemExporterTest):\n \n \n class PprintItemExporterTest(BaseItemExporterTest):\n-\n     def _get_exporter(self, **kwargs):\n         return PprintItemExporter(self.output, **kwargs)\n \n@@ -214,7 +229,6 @@ class PprintItemExporterDataclassTest(PprintItemExporterTest):\n \n \n class PickleItemExporterTest(BaseItemExporterTest):\n-\n     def _get_exporter(self, **kwargs):\n         return PickleItemExporter(self.output, **kwargs)\n \n@@ -222,8 +236,8 @@ class PickleItemExporterTest(BaseItemExporterTest):\n         self._assert_expected_item(pickle.loads(self.output.getvalue()))\n \n     def test_export_multiple_items(self):\n-        i1 = self.item_class(name='hello', age='world')\n-        i2 = self.item_class(name='bye', age='world')\n+        i1 = self.item_class(name=\"hello\", age=\"world\")\n+        i2 = self.item_class(name=\"bye\", age=\"world\")\n         f = BytesIO()\n         ie = PickleItemExporter(f)\n         ie.start_exporting()\n@@ -250,7 +264,6 @@ class PickleItemExporterDataclassTest(PickleItemExporterTest):\n \n \n class MarshalItemExporterTest(BaseItemExporterTest):\n-\n     def _get_exporter(self, **kwargs):\n         self.output = tempfile.TemporaryFile()\n         return MarshalItemExporter(self.output, **kwargs)\n@@ -261,7 +274,7 @@ class MarshalItemExporterTest(BaseItemExporterTest):\n \n     def test_nonstring_types_item(self):\n         item = self._get_nonstring_types_item()\n-        item.pop('time')  # datetime is not marshallable\n+        item.pop(\"time\")  # datetime is not marshallable\n         fp = tempfile.TemporaryFile()\n         ie = MarshalItemExporter(fp)\n         ie.start_exporting()\n@@ -287,11 +300,14 @@ class CsvItemExporterTest(BaseItemExporterTest):\n                 sorted(re.split(r\"(,|\\s+)\", line))\n                 for line in to_unicode(csv).splitlines(True)\n             ]\n+\n         return self.assertEqual(split_csv(first), split_csv(second), msg=msg)\n \n     def _check_output(self):\n         self.output.seek(0)\n-        self.assertCsvEqual(to_unicode(self.output.read()), 'age,name\\r\\n22,John\\xa3\\r\\n')\n+        self.assertCsvEqual(\n+            to_unicode(self.output.read()), \"age,name\\r\\n22,John\\xa3\\r\\n\"\n+        )\n \n     def assertExportResult(self, item, expected, **kwargs):\n         fp = BytesIO()\n@@ -305,21 +321,21 @@ class CsvItemExporterTest(BaseItemExporterTest):\n         self.assertExportResult(\n             item=self.i,\n             fields_to_export=ItemAdapter(self.i).field_names(),\n-            expected=b'age,name\\r\\n22,John\\xc2\\xa3\\r\\n',\n+            expected=b\"age,name\\r\\n22,John\\xc2\\xa3\\r\\n\",\n         )\n \n     def test_header_export_all_dict(self):\n         self.assertExportResult(\n             item=ItemAdapter(self.i).asdict(),\n-            expected=b'age,name\\r\\n22,John\\xc2\\xa3\\r\\n',\n+            expected=b\"age,name\\r\\n22,John\\xc2\\xa3\\r\\n\",\n         )\n \n     def test_header_export_single_field(self):\n         for item in [self.i, ItemAdapter(self.i).asdict()]:\n             self.assertExportResult(\n                 item=item,\n-                fields_to_export=['age'],\n-                expected=b'age\\r\\n22\\r\\n',\n+                fields_to_export=[\"age\"],\n+                expected=b\"age\\r\\n22\\r\\n\",\n             )\n \n     def test_header_export_two_items(self):\n@@ -330,15 +346,16 @@ class CsvItemExporterTest(BaseItemExporterTest):\n             ie.export_item(item)\n             ie.export_item(item)\n             ie.finish_exporting()\n-            self.assertCsvEqual(output.getvalue(),\n-                                b'age,name\\r\\n22,John\\xc2\\xa3\\r\\n22,John\\xc2\\xa3\\r\\n')\n+            self.assertCsvEqual(\n+                output.getvalue(), b\"age,name\\r\\n22,John\\xc2\\xa3\\r\\n22,John\\xc2\\xa3\\r\\n\"\n+            )\n \n     def test_header_no_header_line(self):\n         for item in [self.i, ItemAdapter(self.i).asdict()]:\n             self.assertExportResult(\n                 item=item,\n                 include_headers_line=False,\n-                expected=b'22,John\\xc2\\xa3\\r\\n',\n+                expected=b\"22,John\\xc2\\xa3\\r\\n\",\n             )\n \n     def test_join_multivalue(self):\n@@ -348,14 +365,14 @@ class CsvItemExporterTest(BaseItemExporterTest):\n \n         for cls in TestItem2, dict:\n             self.assertExportResult(\n-                item=cls(name='John', friends=['Mary', 'Paul']),\n+                item=cls(name=\"John\", friends=[\"Mary\", \"Paul\"]),\n                 include_headers_line=False,\n                 expected='\"Mary,Paul\",John\\r\\n',\n             )\n \n     def test_join_multivalue_not_strings(self):\n         self.assertExportResult(\n-            item=dict(name='John', friends=[4, 8]),\n+            item=dict(name=\"John\", friends=[4, 8]),\n             include_headers_line=False,\n             expected='\"[4, 8]\",John\\r\\n',\n         )\n@@ -364,24 +381,24 @@ class CsvItemExporterTest(BaseItemExporterTest):\n         self.assertExportResult(\n             item=self._get_nonstring_types_item(),\n             include_headers_line=False,\n-            expected='22,False,3.14,2015-01-01 01:01:01\\r\\n'\n+            expected=\"22,False,3.14,2015-01-01 01:01:01\\r\\n\",\n         )\n \n     def test_errors_default(self):\n         with self.assertRaises(UnicodeEncodeError):\n             self.assertExportResult(\n-                item=dict(text='W\\u0275\\u200Brd'),\n+                item=dict(text=\"W\\u0275\\u200Brd\"),\n                 expected=None,\n-                encoding='windows-1251',\n+                encoding=\"windows-1251\",\n             )\n \n     def test_errors_xmlcharrefreplace(self):\n         self.assertExportResult(\n-            item=dict(text='W\\u0275\\u200Brd'),\n+            item=dict(text=\"W\\u0275\\u200Brd\"),\n             include_headers_line=False,\n-            expected='W&#629;&#8203;rd\\r\\n',\n-            encoding='windows-1251',\n-            errors='xmlcharrefreplace',\n+            expected=\"W&#629;&#8203;rd\\r\\n\",\n+            encoding=\"windows-1251\",\n+            errors=\"xmlcharrefreplace\",\n         )\n \n \n@@ -391,7 +408,6 @@ class CsvItemExporterDataclassTest(CsvItemExporterTest):\n \n \n class XmlItemExporterTest(BaseItemExporterTest):\n-\n     def _get_exporter(self, **kwargs):\n         return XmlItemExporter(self.output, **kwargs)\n \n@@ -405,6 +421,7 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         def xmlsplit(xmlcontent):\n             doc = lxml.etree.fromstring(xmlcontent)\n             return xmltuple(doc)\n+\n         return self.assertEqual(xmlsplit(first), xmlsplit(second), msg)\n \n     def assertExportResult(self, item, expected_value):\n@@ -418,13 +435,13 @@ class XmlItemExporterTest(BaseItemExporterTest):\n     def _check_output(self):\n         expected_value = (\n             b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n-            b'<items><item><age>22</age><name>John\\xc2\\xa3</name></item></items>'\n+            b\"<items><item><age>22</age><name>John\\xc2\\xa3</name></item></items>\"\n         )\n         self.assertXmlEquivalent(self.output.getvalue(), expected_value)\n \n     def test_multivalued_fields(self):\n         self.assertExportResult(\n-            self.item_class(name=['John\\xa3', 'Doe'], age=[1, 2, 3]),\n+            self.item_class(name=[\"John\\xa3\", \"Doe\"], age=[1, 2, 3]),\n             b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n\n             <items>\n                 <item>\n@@ -432,13 +449,13 @@ class XmlItemExporterTest(BaseItemExporterTest):\n                     <age><value>1</value><value>2</value><value>3</value></age>\n                 </item>\n             </items>\n-            \"\"\"\n+            \"\"\",\n         )\n \n     def test_nested_item(self):\n-        i1 = dict(name='foo\\xa3hoo', age='22')\n-        i2 = dict(name='bar', age=i1)\n-        i3 = self.item_class(name='buz', age=i2)\n+        i1 = dict(name=\"foo\\xa3hoo\", age=\"22\")\n+        i2 = dict(name=\"bar\", age=i1)\n+        i3 = self.item_class(name=\"buz\", age=i2)\n \n         self.assertExportResult(\n             i3,\n@@ -455,13 +472,13 @@ class XmlItemExporterTest(BaseItemExporterTest):\n                         <name>buz</name>\n                     </item>\n                 </items>\n-            \"\"\"\n+            \"\"\",\n         )\n \n     def test_nested_list_item(self):\n-        i1 = dict(name='foo')\n-        i2 = dict(name='bar', v2={\"egg\": [\"spam\"]})\n-        i3 = self.item_class(name='buz', age=[i1, i2])\n+        i1 = dict(name=\"foo\")\n+        i2 = dict(name=\"bar\", v2={\"egg\": [\"spam\"]})\n+        i3 = self.item_class(name=\"buz\", age=[i1, i2])\n \n         self.assertExportResult(\n             i3,\n@@ -475,7 +492,7 @@ class XmlItemExporterTest(BaseItemExporterTest):\n                         <name>buz</name>\n                     </item>\n                 </items>\n-            \"\"\"\n+            \"\"\",\n         )\n \n     def test_nonstring_types_item(self):\n@@ -491,7 +508,7 @@ class XmlItemExporterTest(BaseItemExporterTest):\n                        <time>2015-01-01 01:01:01</time>\n                    </item>\n                 </items>\n-            \"\"\"\n+            \"\"\",\n         )\n \n \n@@ -503,7 +520,10 @@ class XmlItemExporterDataclassTest(XmlItemExporterTest):\n \n class JsonLinesItemExporterTest(BaseItemExporterTest):\n \n-    _expected_nested = {'name': 'Jesus', 'age': {'name': 'Maria', 'age': {'name': 'Joseph', 'age': '22'}}}\n+    _expected_nested = {\n+        \"name\": \"Jesus\",\n+        \"age\": {\"name\": \"Maria\", \"age\": {\"name\": \"Joseph\", \"age\": \"22\"}},\n+    }\n \n     def _get_exporter(self, **kwargs):\n         return JsonLinesItemExporter(self.output, **kwargs)\n@@ -513,9 +533,9 @@ class JsonLinesItemExporterTest(BaseItemExporterTest):\n         self.assertEqual(exported, ItemAdapter(self.i).asdict())\n \n     def test_nested_item(self):\n-        i1 = self.item_class(name='Joseph', age='22')\n-        i2 = dict(name='Maria', age=i1)\n-        i3 = self.item_class(name='Jesus', age=i2)\n+        i1 = self.item_class(name=\"Joseph\", age=\"22\")\n+        i2 = dict(name=\"Maria\", age=i1)\n+        i3 = self.item_class(name=\"Jesus\", age=i2)\n         self.ie.start_exporting()\n         self.ie.export_item(i3)\n         self.ie.finish_exporting()\n@@ -534,7 +554,7 @@ class JsonLinesItemExporterTest(BaseItemExporterTest):\n         self.ie.export_item(item)\n         self.ie.finish_exporting()\n         exported = json.loads(to_unicode(self.output.getvalue()))\n-        item['time'] = str(item['time'])\n+        item[\"time\"] = str(item[\"time\"])\n         self.assertEqual(exported, item)\n \n \n@@ -561,7 +581,9 @@ class JsonItemExporterTest(JsonLinesItemExporterTest):\n         self.ie.export_item(item)\n         self.ie.finish_exporting()\n         exported = json.loads(to_unicode(self.output.getvalue()))\n-        self.assertEqual(exported, [ItemAdapter(item).asdict(), ItemAdapter(item).asdict()])\n+        self.assertEqual(\n+            exported, [ItemAdapter(item).asdict(), ItemAdapter(item).asdict()]\n+        )\n \n     def test_two_items(self):\n         self.assertTwoItemsExported(self.i)\n@@ -570,25 +592,28 @@ class JsonItemExporterTest(JsonLinesItemExporterTest):\n         self.assertTwoItemsExported(ItemAdapter(self.i).asdict())\n \n     def test_nested_item(self):\n-        i1 = self.item_class(name='Joseph\\xa3', age='22')\n-        i2 = self.item_class(name='Maria', age=i1)\n-        i3 = self.item_class(name='Jesus', age=i2)\n+        i1 = self.item_class(name=\"Joseph\\xa3\", age=\"22\")\n+        i2 = self.item_class(name=\"Maria\", age=i1)\n+        i3 = self.item_class(name=\"Jesus\", age=i2)\n         self.ie.start_exporting()\n         self.ie.export_item(i3)\n         self.ie.finish_exporting()\n         exported = json.loads(to_unicode(self.output.getvalue()))\n-        expected = {'name': 'Jesus', 'age': {'name': 'Maria', 'age': ItemAdapter(i1).asdict()}}\n+        expected = {\n+            \"name\": \"Jesus\",\n+            \"age\": {\"name\": \"Maria\", \"age\": ItemAdapter(i1).asdict()},\n+        }\n         self.assertEqual(exported, [expected])\n \n     def test_nested_dict_item(self):\n-        i1 = dict(name='Joseph\\xa3', age='22')\n-        i2 = self.item_class(name='Maria', age=i1)\n-        i3 = dict(name='Jesus', age=i2)\n+        i1 = dict(name=\"Joseph\\xa3\", age=\"22\")\n+        i2 = self.item_class(name=\"Maria\", age=i1)\n+        i3 = dict(name=\"Jesus\", age=i2)\n         self.ie.start_exporting()\n         self.ie.export_item(i3)\n         self.ie.finish_exporting()\n         exported = json.loads(to_unicode(self.output.getvalue()))\n-        expected = {'name': 'Jesus', 'age': {'name': 'Maria', 'age': i1}}\n+        expected = {\"name\": \"Jesus\", \"age\": {\"name\": \"Maria\", \"age\": i1}}\n         self.assertEqual(exported, [expected])\n \n     def test_nonstring_types_item(self):\n@@ -597,7 +622,7 @@ class JsonItemExporterTest(JsonLinesItemExporterTest):\n         self.ie.export_item(item)\n         self.ie.finish_exporting()\n         exported = json.loads(to_unicode(self.output.getvalue()))\n-        item['time'] = str(item['time'])\n+        item[\"time\"] = str(item[\"time\"])\n         self.assertEqual(exported, [item])\n \n \n@@ -618,20 +643,24 @@ class CustomExporterItemTest(unittest.TestCase):\n     def test_exporter_custom_serializer(self):\n         class CustomItemExporter(BaseItemExporter):\n             def serialize_field(self, field, name, value):\n-                if name == 'age':\n+                if name == \"age\":\n                     return str(int(value) + 1)\n                 return super().serialize_field(field, name, value)\n \n-        i = self.item_class(name='John', age='22')\n+        i = self.item_class(name=\"John\", age=\"22\")\n         a = ItemAdapter(i)\n         ie = CustomItemExporter()\n \n-        self.assertEqual(ie.serialize_field(a.get_field_meta('name'), 'name', a['name']), 'John')\n-        self.assertEqual(ie.serialize_field(a.get_field_meta('age'), 'age', a['age']), '23')\n+        self.assertEqual(\n+            ie.serialize_field(a.get_field_meta(\"name\"), \"name\", a[\"name\"]), \"John\"\n+        )\n+        self.assertEqual(\n+            ie.serialize_field(a.get_field_meta(\"age\"), \"age\", a[\"age\"]), \"23\"\n+        )\n \n-        i2 = {'name': 'John', 'age': '22'}\n-        self.assertEqual(ie.serialize_field({}, 'name', i2['name']), 'John')\n-        self.assertEqual(ie.serialize_field({}, 'age', i2['age']), '23')\n+        i2 = {\"name\": \"John\", \"age\": \"22\"}\n+        self.assertEqual(ie.serialize_field({}, \"name\", i2[\"name\"]), \"John\")\n+        self.assertEqual(ie.serialize_field({}, \"age\", i2[\"age\"]), \"23\")\n \n \n class CustomExporterDataclassTest(CustomExporterItemTest):\n@@ -639,5 +668,5 @@ class CustomExporterDataclassTest(CustomExporterItemTest):\n     item_class = TestDataClass\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -24,7 +24,7 @@ class TelnetExtensionTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_bad_credentials(self):\n         console, portal = self._get_console_and_portal()\n-        creds = credentials.UsernamePassword(b'username', b'password')\n+        creds = credentials.UsernamePassword(b\"username\", b\"password\")\n         d = portal.login(creds, None, ITelnetProtocol)\n         yield self.assertFailure(d, ValueError)\n         console.stop_listening()\n@@ -33,8 +33,7 @@ class TelnetExtensionTest(unittest.TestCase):\n     def test_good_credentials(self):\n         console, portal = self._get_console_and_portal()\n         creds = credentials.UsernamePassword(\n-            console.username.encode('utf8'),\n-            console.password.encode('utf8')\n+            console.username.encode(\"utf8\"), console.password.encode(\"utf8\")\n         )\n         d = portal.login(creds, None, ITelnetProtocol)\n         yield d\n@@ -43,11 +42,11 @@ class TelnetExtensionTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_custom_credentials(self):\n         settings = {\n-            'TELNETCONSOLE_USERNAME': 'user',\n-            'TELNETCONSOLE_PASSWORD': 'pass',\n+            \"TELNETCONSOLE_USERNAME\": \"user\",\n+            \"TELNETCONSOLE_PASSWORD\": \"pass\",\n         }\n         console, portal = self._get_console_and_portal(settings=settings)\n-        creds = credentials.UsernamePassword(b'user', b'pass')\n+        creds = credentials.UsernamePassword(b\"user\", b\"pass\")\n         d = portal.login(creds, None, ITelnetProtocol)\n         yield d\n         console.stop_listening()\n\n@@ -57,29 +57,28 @@ from tests.spiders import ItemSpider\n \n \n def path_to_url(path):\n-    return urljoin('file:', pathname2url(str(path)))\n+    return urljoin(\"file:\", pathname2url(str(path)))\n \n \n def printf_escape(string):\n-    return string.replace('%', '%%')\n+    return string.replace(\"%\", \"%%\")\n \n \n def build_url(path: Union[str, PathLike]) -> str:\n     path_str = str(path)\n-    if path_str[0] != '/':\n-        path_str = '/' + path_str\n-    return urljoin('file:', path_str)\n+    if path_str[0] != \"/\":\n+        path_str = \"/\" + path_str\n+    return urljoin(\"file:\", path_str)\n \n \n class FileFeedStorageTest(unittest.TestCase):\n-\n     def test_store_file_uri(self):\n         path = Path(self.mktemp()).resolve()\n         uri = path_to_file_uri(str(path))\n         return self._assert_stores(FileFeedStorage(uri), path)\n \n     def test_store_file_uri_makedirs(self):\n-        path = Path(self.mktemp()).resolve() / 'more' / 'paths' / 'file.txt'\n+        path = Path(self.mktemp()).resolve() / \"more\" / \"paths\" / \"file.txt\"\n         uri = path_to_file_uri(str(path))\n         return self._assert_stores(FileFeedStorage(uri), path)\n \n@@ -112,8 +111,7 @@ class FileFeedStorageTest(unittest.TestCase):\n     def test_overwrite(self):\n         path = self._store({\"overwrite\": True})\n         return self._assert_stores(\n-            FileFeedStorage(str(path), feed_options={\"overwrite\": True}),\n-            path\n+            FileFeedStorage(str(path), feed_options={\"overwrite\": True}), path\n         )\n \n     @defer.inlineCallbacks\n@@ -130,10 +128,9 @@ class FileFeedStorageTest(unittest.TestCase):\n \n \n class FTPFeedStorageTest(unittest.TestCase):\n-\n     def get_test_spider(self, settings=None):\n         class TestSpider(scrapy.Spider):\n-            name = 'test_spider'\n+            name = \"test_spider\"\n \n         crawler = get_crawler(settings_dict=settings)\n         spider = TestSpider.from_crawler(crawler)\n@@ -162,9 +159,9 @@ class FTPFeedStorageTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_append(self):\n         with MockFTPServer() as ftp_server:\n-            filename = 'file'\n+            filename = \"file\"\n             url = ftp_server.url(filename)\n-            feed_options = {'overwrite': False}\n+            feed_options = {\"overwrite\": False}\n             yield self._store(url, b\"foo\", feed_options=feed_options)\n             yield self._store(url, b\"bar\", feed_options=feed_options)\n             self._assert_stored(ftp_server.path / filename, b\"foobar\")\n@@ -172,7 +169,7 @@ class FTPFeedStorageTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_overwrite(self):\n         with MockFTPServer() as ftp_server:\n-            filename = 'file'\n+            filename = \"file\"\n             url = ftp_server.url(filename)\n             yield self._store(url, b\"foo\")\n             yield self._store(url, b\"bar\")\n@@ -181,10 +178,10 @@ class FTPFeedStorageTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_append_active_mode(self):\n         with MockFTPServer() as ftp_server:\n-            settings = {'FEED_STORAGE_FTP_ACTIVE': True}\n-            filename = 'file'\n+            settings = {\"FEED_STORAGE_FTP_ACTIVE\": True}\n+            filename = \"file\"\n             url = ftp_server.url(filename)\n-            feed_options = {'overwrite': False}\n+            feed_options = {\"overwrite\": False}\n             yield self._store(url, b\"foo\", feed_options=feed_options, settings=settings)\n             yield self._store(url, b\"bar\", feed_options=feed_options, settings=settings)\n             self._assert_stored(ftp_server.path / filename, b\"foobar\")\n@@ -192,8 +189,8 @@ class FTPFeedStorageTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_overwrite_active_mode(self):\n         with MockFTPServer() as ftp_server:\n-            settings = {'FEED_STORAGE_FTP_ACTIVE': True}\n-            filename = 'file'\n+            settings = {\"FEED_STORAGE_FTP_ACTIVE\": True}\n+            filename = \"file\"\n             url = ftp_server.url(filename)\n             yield self._store(url, b\"foo\", settings=settings)\n             yield self._store(url, b\"bar\", settings=settings)\n@@ -201,16 +198,15 @@ class FTPFeedStorageTest(unittest.TestCase):\n \n     def test_uri_auth_quote(self):\n         # RFC3986: 3.2.1. User Information\n-        pw_quoted = quote(string.punctuation, safe='')\n-        st = FTPFeedStorage(f'ftp://foo:{pw_quoted}@example.com/some_path', {})\n+        pw_quoted = quote(string.punctuation, safe=\"\")\n+        st = FTPFeedStorage(f\"ftp://foo:{pw_quoted}@example.com/some_path\", {})\n         self.assertEqual(st.password, string.punctuation)\n \n \n class BlockingFeedStorageTest(unittest.TestCase):\n-\n     def get_test_spider(self, settings=None):\n         class TestSpider(scrapy.Spider):\n-            name = 'test_spider'\n+            name = \"test_spider\"\n \n         crawler = get_crawler(settings_dict=settings)\n         spider = TestSpider.from_crawler(crawler)\n@@ -227,7 +223,7 @@ class BlockingFeedStorageTest(unittest.TestCase):\n         b = BlockingFeedStorage()\n \n         tests_path = Path(__file__).resolve().parent\n-        spider = self.get_test_spider({'FEED_TEMPDIR': str(tests_path)})\n+        spider = self.get_test_spider({\"FEED_TEMPDIR\": str(tests_path)})\n         tmp = b.open(spider)\n         tmp_path = Path(tmp.name).parent\n         self.assertEqual(tmp_path, tests_path)\n@@ -236,66 +232,72 @@ class BlockingFeedStorageTest(unittest.TestCase):\n         b = BlockingFeedStorage()\n \n         tests_path = Path(__file__).resolve().parent\n-        invalid_path = tests_path / 'invalid_path'\n-        spider = self.get_test_spider({'FEED_TEMPDIR': str(invalid_path)})\n+        invalid_path = tests_path / \"invalid_path\"\n+        spider = self.get_test_spider({\"FEED_TEMPDIR\": str(invalid_path)})\n \n         self.assertRaises(OSError, b.open, spider=spider)\n \n \n class S3FeedStorageTest(unittest.TestCase):\n-\n     def test_parse_credentials(self):\n         skip_if_no_boto()\n-        aws_credentials = {'AWS_ACCESS_KEY_ID': 'settings_key',\n-                           'AWS_SECRET_ACCESS_KEY': 'settings_secret',\n-                           'AWS_SESSION_TOKEN': 'settings_token'}\n+        aws_credentials = {\n+            \"AWS_ACCESS_KEY_ID\": \"settings_key\",\n+            \"AWS_SECRET_ACCESS_KEY\": \"settings_secret\",\n+            \"AWS_SESSION_TOKEN\": \"settings_token\",\n+        }\n         crawler = get_crawler(settings_dict=aws_credentials)\n         # Instantiate with crawler\n         storage = S3FeedStorage.from_crawler(\n             crawler,\n-            's3://mybucket/export.csv',\n+            \"s3://mybucket/export.csv\",\n         )\n-        self.assertEqual(storage.access_key, 'settings_key')\n-        self.assertEqual(storage.secret_key, 'settings_secret')\n-        self.assertEqual(storage.session_token, 'settings_token')\n+        self.assertEqual(storage.access_key, \"settings_key\")\n+        self.assertEqual(storage.secret_key, \"settings_secret\")\n+        self.assertEqual(storage.session_token, \"settings_token\")\n         # Instantiate directly\n-        storage = S3FeedStorage('s3://mybucket/export.csv',\n-                                aws_credentials['AWS_ACCESS_KEY_ID'],\n-                                aws_credentials['AWS_SECRET_ACCESS_KEY'],\n-                                session_token=aws_credentials['AWS_SESSION_TOKEN'])\n-        self.assertEqual(storage.access_key, 'settings_key')\n-        self.assertEqual(storage.secret_key, 'settings_secret')\n-        self.assertEqual(storage.session_token, 'settings_token')\n+        storage = S3FeedStorage(\n+            \"s3://mybucket/export.csv\",\n+            aws_credentials[\"AWS_ACCESS_KEY_ID\"],\n+            aws_credentials[\"AWS_SECRET_ACCESS_KEY\"],\n+            session_token=aws_credentials[\"AWS_SESSION_TOKEN\"],\n+        )\n+        self.assertEqual(storage.access_key, \"settings_key\")\n+        self.assertEqual(storage.secret_key, \"settings_secret\")\n+        self.assertEqual(storage.session_token, \"settings_token\")\n         # URI priority > settings priority\n-        storage = S3FeedStorage('s3://uri_key:uri_secret@mybucket/export.csv',\n-                                aws_credentials['AWS_ACCESS_KEY_ID'],\n-                                aws_credentials['AWS_SECRET_ACCESS_KEY'])\n-        self.assertEqual(storage.access_key, 'uri_key')\n-        self.assertEqual(storage.secret_key, 'uri_secret')\n+        storage = S3FeedStorage(\n+            \"s3://uri_key:uri_secret@mybucket/export.csv\",\n+            aws_credentials[\"AWS_ACCESS_KEY_ID\"],\n+            aws_credentials[\"AWS_SECRET_ACCESS_KEY\"],\n+        )\n+        self.assertEqual(storage.access_key, \"uri_key\")\n+        self.assertEqual(storage.secret_key, \"uri_secret\")\n \n     @defer.inlineCallbacks\n     def test_store(self):\n         skip_if_no_boto()\n \n         settings = {\n-            'AWS_ACCESS_KEY_ID': 'access_key',\n-            'AWS_SECRET_ACCESS_KEY': 'secret_key',\n+            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n+            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n         }\n         crawler = get_crawler(settings_dict=settings)\n-        bucket = 'mybucket'\n-        key = 'export.csv'\n-        storage = S3FeedStorage.from_crawler(crawler, f's3://{bucket}/{key}')\n+        bucket = \"mybucket\"\n+        key = \"export.csv\"\n+        storage = S3FeedStorage.from_crawler(crawler, f\"s3://{bucket}/{key}\")\n         verifyObject(IFeedStorage, storage)\n \n         file = mock.MagicMock()\n         from botocore.stub import Stubber\n+\n         with Stubber(storage.s3_client) as stub:\n             stub.add_response(\n-                'put_object',\n+                \"put_object\",\n                 expected_params={\n-                    'Body': file,\n-                    'Bucket': bucket,\n-                    'Key': key,\n+                    \"Body\": file,\n+                    \"Bucket\": bucket,\n+                    \"Key\": key,\n                 },\n                 service_response={},\n             )\n@@ -309,172 +311,154 @@ class S3FeedStorageTest(unittest.TestCase):\n                     mock.call.seek(0),\n                     # The call to read does not happen with Stubber\n                     mock.call.close(),\n-                ]\n+                ],\n             )\n \n     def test_init_without_acl(self):\n-        storage = S3FeedStorage(\n-            's3://mybucket/export.csv',\n-            'access_key',\n-            'secret_key'\n-        )\n-        self.assertEqual(storage.access_key, 'access_key')\n-        self.assertEqual(storage.secret_key, 'secret_key')\n+        storage = S3FeedStorage(\"s3://mybucket/export.csv\", \"access_key\", \"secret_key\")\n+        self.assertEqual(storage.access_key, \"access_key\")\n+        self.assertEqual(storage.secret_key, \"secret_key\")\n         self.assertEqual(storage.acl, None)\n \n     def test_init_with_acl(self):\n         storage = S3FeedStorage(\n-            's3://mybucket/export.csv',\n-            'access_key',\n-            'secret_key',\n-            'custom-acl'\n+            \"s3://mybucket/export.csv\", \"access_key\", \"secret_key\", \"custom-acl\"\n         )\n-        self.assertEqual(storage.access_key, 'access_key')\n-        self.assertEqual(storage.secret_key, 'secret_key')\n-        self.assertEqual(storage.acl, 'custom-acl')\n+        self.assertEqual(storage.access_key, \"access_key\")\n+        self.assertEqual(storage.secret_key, \"secret_key\")\n+        self.assertEqual(storage.acl, \"custom-acl\")\n \n     def test_init_with_endpoint_url(self):\n         storage = S3FeedStorage(\n-            's3://mybucket/export.csv',\n-            'access_key',\n-            'secret_key',\n-            endpoint_url='https://example.com'\n+            \"s3://mybucket/export.csv\",\n+            \"access_key\",\n+            \"secret_key\",\n+            endpoint_url=\"https://example.com\",\n         )\n-        self.assertEqual(storage.access_key, 'access_key')\n-        self.assertEqual(storage.secret_key, 'secret_key')\n-        self.assertEqual(storage.endpoint_url, 'https://example.com')\n+        self.assertEqual(storage.access_key, \"access_key\")\n+        self.assertEqual(storage.secret_key, \"secret_key\")\n+        self.assertEqual(storage.endpoint_url, \"https://example.com\")\n \n     def test_from_crawler_without_acl(self):\n         settings = {\n-            'AWS_ACCESS_KEY_ID': 'access_key',\n-            'AWS_SECRET_ACCESS_KEY': 'secret_key',\n+            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n+            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n         }\n         crawler = get_crawler(settings_dict=settings)\n         storage = S3FeedStorage.from_crawler(\n             crawler,\n-            's3://mybucket/export.csv',\n+            \"s3://mybucket/export.csv\",\n         )\n-        self.assertEqual(storage.access_key, 'access_key')\n-        self.assertEqual(storage.secret_key, 'secret_key')\n+        self.assertEqual(storage.access_key, \"access_key\")\n+        self.assertEqual(storage.secret_key, \"secret_key\")\n         self.assertEqual(storage.acl, None)\n \n     def test_without_endpoint_url(self):\n         settings = {\n-            'AWS_ACCESS_KEY_ID': 'access_key',\n-            'AWS_SECRET_ACCESS_KEY': 'secret_key',\n+            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n+            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n         }\n         crawler = get_crawler(settings_dict=settings)\n         storage = S3FeedStorage.from_crawler(\n             crawler,\n-            's3://mybucket/export.csv',\n+            \"s3://mybucket/export.csv\",\n         )\n-        self.assertEqual(storage.access_key, 'access_key')\n-        self.assertEqual(storage.secret_key, 'secret_key')\n+        self.assertEqual(storage.access_key, \"access_key\")\n+        self.assertEqual(storage.secret_key, \"secret_key\")\n         self.assertEqual(storage.endpoint_url, None)\n \n     def test_from_crawler_with_acl(self):\n         settings = {\n-            'AWS_ACCESS_KEY_ID': 'access_key',\n-            'AWS_SECRET_ACCESS_KEY': 'secret_key',\n-            'FEED_STORAGE_S3_ACL': 'custom-acl',\n+            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n+            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n+            \"FEED_STORAGE_S3_ACL\": \"custom-acl\",\n         }\n         crawler = get_crawler(settings_dict=settings)\n         storage = S3FeedStorage.from_crawler(\n             crawler,\n-            's3://mybucket/export.csv',\n+            \"s3://mybucket/export.csv\",\n         )\n-        self.assertEqual(storage.access_key, 'access_key')\n-        self.assertEqual(storage.secret_key, 'secret_key')\n-        self.assertEqual(storage.acl, 'custom-acl')\n+        self.assertEqual(storage.access_key, \"access_key\")\n+        self.assertEqual(storage.secret_key, \"secret_key\")\n+        self.assertEqual(storage.acl, \"custom-acl\")\n \n     def test_from_crawler_with_endpoint_url(self):\n         settings = {\n-            'AWS_ACCESS_KEY_ID': 'access_key',\n-            'AWS_SECRET_ACCESS_KEY': 'secret_key',\n-            'AWS_ENDPOINT_URL': 'https://example.com',\n+            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n+            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n+            \"AWS_ENDPOINT_URL\": \"https://example.com\",\n         }\n         crawler = get_crawler(settings_dict=settings)\n-        storage = S3FeedStorage.from_crawler(\n-            crawler,\n-            's3://mybucket/export.csv'\n-        )\n-        self.assertEqual(storage.access_key, 'access_key')\n-        self.assertEqual(storage.secret_key, 'secret_key')\n-        self.assertEqual(storage.endpoint_url, 'https://example.com')\n+        storage = S3FeedStorage.from_crawler(crawler, \"s3://mybucket/export.csv\")\n+        self.assertEqual(storage.access_key, \"access_key\")\n+        self.assertEqual(storage.secret_key, \"secret_key\")\n+        self.assertEqual(storage.endpoint_url, \"https://example.com\")\n \n     @defer.inlineCallbacks\n     def test_store_botocore_without_acl(self):\n         skip_if_no_boto()\n         storage = S3FeedStorage(\n-            's3://mybucket/export.csv',\n-            'access_key',\n-            'secret_key',\n+            \"s3://mybucket/export.csv\",\n+            \"access_key\",\n+            \"secret_key\",\n         )\n-        self.assertEqual(storage.access_key, 'access_key')\n-        self.assertEqual(storage.secret_key, 'secret_key')\n+        self.assertEqual(storage.access_key, \"access_key\")\n+        self.assertEqual(storage.secret_key, \"secret_key\")\n         self.assertEqual(storage.acl, None)\n \n         storage.s3_client = mock.MagicMock()\n-        yield storage.store(BytesIO(b'test file'))\n-        self.assertNotIn('ACL', storage.s3_client.put_object.call_args[1])\n+        yield storage.store(BytesIO(b\"test file\"))\n+        self.assertNotIn(\"ACL\", storage.s3_client.put_object.call_args[1])\n \n     @defer.inlineCallbacks\n     def test_store_botocore_with_acl(self):\n         skip_if_no_boto()\n         storage = S3FeedStorage(\n-            's3://mybucket/export.csv',\n-            'access_key',\n-            'secret_key',\n-            'custom-acl'\n+            \"s3://mybucket/export.csv\", \"access_key\", \"secret_key\", \"custom-acl\"\n         )\n-        self.assertEqual(storage.access_key, 'access_key')\n-        self.assertEqual(storage.secret_key, 'secret_key')\n-        self.assertEqual(storage.acl, 'custom-acl')\n+        self.assertEqual(storage.access_key, \"access_key\")\n+        self.assertEqual(storage.secret_key, \"secret_key\")\n+        self.assertEqual(storage.acl, \"custom-acl\")\n \n         storage.s3_client = mock.MagicMock()\n-        yield storage.store(BytesIO(b'test file'))\n+        yield storage.store(BytesIO(b\"test file\"))\n         self.assertEqual(\n-            storage.s3_client.put_object.call_args[1].get('ACL'),\n-            'custom-acl'\n+            storage.s3_client.put_object.call_args[1].get(\"ACL\"), \"custom-acl\"\n         )\n \n     def test_overwrite_default(self):\n         with LogCapture() as log:\n             S3FeedStorage(\n-                's3://mybucket/export.csv',\n-                'access_key',\n-                'secret_key',\n-                'custom-acl'\n+                \"s3://mybucket/export.csv\", \"access_key\", \"secret_key\", \"custom-acl\"\n             )\n-        self.assertNotIn('S3 does not support appending to files', str(log))\n+        self.assertNotIn(\"S3 does not support appending to files\", str(log))\n \n     def test_overwrite_false(self):\n         with LogCapture() as log:\n             S3FeedStorage(\n-                's3://mybucket/export.csv',\n-                'access_key',\n-                'secret_key',\n-                'custom-acl',\n-                feed_options={'overwrite': False},\n+                \"s3://mybucket/export.csv\",\n+                \"access_key\",\n+                \"secret_key\",\n+                \"custom-acl\",\n+                feed_options={\"overwrite\": False},\n             )\n-        self.assertIn('S3 does not support appending to files', str(log))\n+        self.assertIn(\"S3 does not support appending to files\", str(log))\n \n \n class GCSFeedStorageTest(unittest.TestCase):\n-\n     def test_parse_settings(self):\n         try:\n             from google.cloud.storage import Client  # noqa\n         except ImportError:\n             raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n \n-        settings = {'GCS_PROJECT_ID': '123', 'FEED_STORAGE_GCS_ACL': 'publicRead'}\n+        settings = {\"GCS_PROJECT_ID\": \"123\", \"FEED_STORAGE_GCS_ACL\": \"publicRead\"}\n         crawler = get_crawler(settings_dict=settings)\n-        storage = GCSFeedStorage.from_crawler(crawler, 'gs://mybucket/export.csv')\n-        assert storage.project_id == '123'\n-        assert storage.acl == 'publicRead'\n-        assert storage.bucket_name == 'mybucket'\n-        assert storage.blob_name == 'export.csv'\n+        storage = GCSFeedStorage.from_crawler(crawler, \"gs://mybucket/export.csv\")\n+        assert storage.project_id == \"123\"\n+        assert storage.acl == \"publicRead\"\n+        assert storage.bucket_name == \"mybucket\"\n+        assert storage.blob_name == \"export.csv\"\n \n     def test_parse_empty_acl(self):\n         try:\n@@ -482,14 +466,14 @@ class GCSFeedStorageTest(unittest.TestCase):\n         except ImportError:\n             raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n \n-        settings = {'GCS_PROJECT_ID': '123', 'FEED_STORAGE_GCS_ACL': ''}\n+        settings = {\"GCS_PROJECT_ID\": \"123\", \"FEED_STORAGE_GCS_ACL\": \"\"}\n         crawler = get_crawler(settings_dict=settings)\n-        storage = GCSFeedStorage.from_crawler(crawler, 'gs://mybucket/export.csv')\n+        storage = GCSFeedStorage.from_crawler(crawler, \"gs://mybucket/export.csv\")\n         assert storage.acl is None\n \n-        settings = {'GCS_PROJECT_ID': '123', 'FEED_STORAGE_GCS_ACL': None}\n+        settings = {\"GCS_PROJECT_ID\": \"123\", \"FEED_STORAGE_GCS_ACL\": None}\n         crawler = get_crawler(settings_dict=settings)\n-        storage = GCSFeedStorage.from_crawler(crawler, 'gs://mybucket/export.csv')\n+        storage = GCSFeedStorage.from_crawler(crawler, \"gs://mybucket/export.csv\")\n         assert storage.acl is None\n \n     @defer.inlineCallbacks\n@@ -499,11 +483,11 @@ class GCSFeedStorageTest(unittest.TestCase):\n         except ImportError:\n             raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n \n-        uri = 'gs://mybucket/export.csv'\n-        project_id = 'myproject-123'\n-        acl = 'publicRead'\n+        uri = \"gs://mybucket/export.csv\"\n+        project_id = \"myproject-123\"\n+        acl = \"publicRead\"\n         (client_mock, bucket_mock, blob_mock) = mock_google_cloud_storage()\n-        with mock.patch('google.cloud.storage.Client') as m:\n+        with mock.patch(\"google.cloud.storage.Client\") as m:\n             m.return_value = client_mock\n \n             f = mock.Mock()\n@@ -512,17 +496,16 @@ class GCSFeedStorageTest(unittest.TestCase):\n \n             f.seek.assert_called_once_with(0)\n             m.assert_called_once_with(project=project_id)\n-            client_mock.get_bucket.assert_called_once_with('mybucket')\n-            bucket_mock.blob.assert_called_once_with('export.csv')\n+            client_mock.get_bucket.assert_called_once_with(\"mybucket\")\n+            bucket_mock.blob.assert_called_once_with(\"export.csv\")\n             blob_mock.upload_from_file.assert_called_once_with(f, predefined_acl=acl)\n \n \n class StdoutFeedStorageTest(unittest.TestCase):\n-\n     @defer.inlineCallbacks\n     def test_store(self):\n         out = BytesIO()\n-        storage = StdoutFeedStorage('stdout:', _stdout=out)\n+        storage = StdoutFeedStorage(\"stdout:\", _stdout=out)\n         file = storage.open(scrapy.Spider(\"default\"))\n         file.write(b\"content\")\n         yield storage.store(file)\n@@ -530,13 +513,17 @@ class StdoutFeedStorageTest(unittest.TestCase):\n \n     def test_overwrite_default(self):\n         with LogCapture() as log:\n-            StdoutFeedStorage('stdout:')\n-        self.assertNotIn('Standard output (stdout) storage does not support overwriting', str(log))\n+            StdoutFeedStorage(\"stdout:\")\n+        self.assertNotIn(\n+            \"Standard output (stdout) storage does not support overwriting\", str(log)\n+        )\n \n     def test_overwrite_true(self):\n         with LogCapture() as log:\n-            StdoutFeedStorage('stdout:', feed_options={'overwrite': True})\n-        self.assertIn('Standard output (stdout) storage does not support overwriting', str(log))\n+            StdoutFeedStorage(\"stdout:\", feed_options={\"overwrite\": True})\n+        self.assertIn(\n+            \"Standard output (stdout) storage does not support overwriting\", str(log)\n+        )\n \n \n class FromCrawlerMixin:\n@@ -553,7 +540,6 @@ class FromCrawlerCsvItemExporter(CsvItemExporter, FromCrawlerMixin):\n \n \n class FromCrawlerFileFeedStorage(FileFeedStorage, FromCrawlerMixin):\n-\n     @classmethod\n     def from_crawler(cls, crawler, *args, feed_options=None, **kwargs):\n         cls.init_with_crawler = True\n@@ -561,7 +547,6 @@ class FromCrawlerFileFeedStorage(FileFeedStorage, FromCrawlerMixin):\n \n \n class DummyBlockingFeedStorage(BlockingFeedStorage):\n-\n     def __init__(self, uri, *args, feed_options=None):\n         self.path = Path(file_uri_to_path(uri))\n \n@@ -569,16 +554,15 @@ class DummyBlockingFeedStorage(BlockingFeedStorage):\n         dirname = self.path.parent\n         if dirname and not dirname.exists():\n             dirname.mkdir(parents=True)\n-        with self.path.open('ab') as output_file:\n+        with self.path.open(\"ab\") as output_file:\n             output_file.write(file.read())\n \n         file.close()\n \n \n class FailingBlockingFeedStorage(DummyBlockingFeedStorage):\n-\n     def _store_in_thread(self, file):\n-        raise OSError('Cannot store')\n+        raise OSError(\"Cannot store\")\n \n \n @implementer(IFeedStorage)\n@@ -593,10 +577,10 @@ class LogOnStoreFileStorage:\n         self.logger = getLogger()\n \n     def open(self, spider):\n-        return tempfile.NamedTemporaryFile(prefix='feed-')\n+        return tempfile.NamedTemporaryFile(prefix=\"feed-\")\n \n     def store(self, file):\n-        self.logger.info('Storage.store is called')\n+        self.logger.info(\"Storage.store is called\")\n         file.close()\n \n \n@@ -612,9 +596,9 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n         foo = scrapy.Field()\n         hello = scrapy.Field()\n \n-    def _random_temp_filename(self, inter_dir='') -> Path:\n+    def _random_temp_filename(self, inter_dir=\"\") -> Path:\n         chars = [random.choice(ascii_letters + digits) for _ in range(15)]\n-        filename = ''.join(chars)\n+        filename = \"\".join(chars)\n         return Path(self.temp_dir, inter_dir, filename)\n \n     def setUp(self):\n@@ -630,7 +614,7 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n         \"\"\"\n \n         class TestSpider(scrapy.Spider):\n-            name = 'testspider'\n+            name = \"testspider\"\n \n             def parse(self, response):\n                 for item in items:\n@@ -646,7 +630,7 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n         \"\"\"\n \n         class TestSpider(scrapy.Spider):\n-            name = 'testspider'\n+            name = \"testspider\"\n \n             def parse(self, response):\n                 pass\n@@ -687,8 +671,8 @@ class FeedExportTest(FeedExportTestBase):\n     def run_and_export(self, spider_cls, settings):\n         \"\"\"Run spider with specified settings; return exported data.\"\"\"\n \n-        FEEDS = settings.get('FEEDS') or {}\n-        settings['FEEDS'] = {\n+        FEEDS = settings.get(\"FEEDS\") or {}\n+        settings[\"FEEDS\"] = {\n             printf_escape(path_to_url(file_path)): feed_options\n             for file_path, feed_options in FEEDS.items()\n         }\n@@ -696,7 +680,7 @@ class FeedExportTest(FeedExportTestBase):\n         content = {}\n         try:\n             with MockServer() as s:\n-                spider_cls.start_urls = [s.url('/')]\n+                spider_cls.start_urls = [s.url(\"/\")]\n                 crawler = get_crawler(spider_cls, settings)\n                 yield crawler.crawl()\n \n@@ -704,7 +688,7 @@ class FeedExportTest(FeedExportTestBase):\n                 if not Path(file_path).exists():\n                     continue\n \n-                content[feed_options['format']] = Path(file_path).read_bytes()\n+                content[feed_options[\"format\"]] = Path(file_path).read_bytes()\n \n         finally:\n             for file_path in FEEDS.keys():\n@@ -718,88 +702,102 @@ class FeedExportTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def assertExportedCsv(self, items, header, rows, settings=None):\n         settings = settings or {}\n-        settings.update({\n-            'FEEDS': {\n-                self._random_temp_filename(): {'format': 'csv'},\n+        settings.update(\n+            {\n+                \"FEEDS\": {\n+                    self._random_temp_filename(): {\"format\": \"csv\"},\n                 },\n-        })\n+            }\n+        )\n         data = yield self.exported_data(items, settings)\n-        reader = csv.DictReader(to_unicode(data['csv']).splitlines())\n+        reader = csv.DictReader(to_unicode(data[\"csv\"]).splitlines())\n         self.assertEqual(reader.fieldnames, list(header))\n         self.assertEqual(rows, list(reader))\n \n     @defer.inlineCallbacks\n     def assertExportedJsonLines(self, items, rows, settings=None):\n         settings = settings or {}\n-        settings.update({\n-            'FEEDS': {\n-                self._random_temp_filename(): {'format': 'jl'},\n+        settings.update(\n+            {\n+                \"FEEDS\": {\n+                    self._random_temp_filename(): {\"format\": \"jl\"},\n                 },\n-        })\n+            }\n+        )\n         data = yield self.exported_data(items, settings)\n-        parsed = [json.loads(to_unicode(line)) for line in data['jl'].splitlines()]\n+        parsed = [json.loads(to_unicode(line)) for line in data[\"jl\"].splitlines()]\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         self.assertEqual(rows, parsed)\n \n     @defer.inlineCallbacks\n     def assertExportedXml(self, items, rows, settings=None):\n         settings = settings or {}\n-        settings.update({\n-            'FEEDS': {\n-                self._random_temp_filename(): {'format': 'xml'},\n+        settings.update(\n+            {\n+                \"FEEDS\": {\n+                    self._random_temp_filename(): {\"format\": \"xml\"},\n                 },\n-        })\n+            }\n+        )\n         data = yield self.exported_data(items, settings)\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n-        root = lxml.etree.fromstring(data['xml'])\n-        got_rows = [{e.tag: e.text for e in it} for it in root.findall('item')]\n+        root = lxml.etree.fromstring(data[\"xml\"])\n+        got_rows = [{e.tag: e.text for e in it} for it in root.findall(\"item\")]\n         self.assertEqual(rows, got_rows)\n \n     @defer.inlineCallbacks\n     def assertExportedMultiple(self, items, rows, settings=None):\n         settings = settings or {}\n-        settings.update({\n-            'FEEDS': {\n-                self._random_temp_filename(): {'format': 'xml'},\n-                self._random_temp_filename(): {'format': 'json'},\n+        settings.update(\n+            {\n+                \"FEEDS\": {\n+                    self._random_temp_filename(): {\"format\": \"xml\"},\n+                    self._random_temp_filename(): {\"format\": \"json\"},\n                 },\n-        })\n+            }\n+        )\n         data = yield self.exported_data(items, settings)\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         # XML\n-        root = lxml.etree.fromstring(data['xml'])\n-        xml_rows = [{e.tag: e.text for e in it} for it in root.findall('item')]\n+        root = lxml.etree.fromstring(data[\"xml\"])\n+        xml_rows = [{e.tag: e.text for e in it} for it in root.findall(\"item\")]\n         self.assertEqual(rows, xml_rows)\n         # JSON\n-        json_rows = json.loads(to_unicode(data['json']))\n+        json_rows = json.loads(to_unicode(data[\"json\"]))\n         self.assertEqual(rows, json_rows)\n \n     @defer.inlineCallbacks\n     def assertExportedPickle(self, items, rows, settings=None):\n         settings = settings or {}\n-        settings.update({\n-            'FEEDS': {\n-                self._random_temp_filename(): {'format': 'pickle'},\n+        settings.update(\n+            {\n+                \"FEEDS\": {\n+                    self._random_temp_filename(): {\"format\": \"pickle\"},\n                 },\n-        })\n+            }\n+        )\n         data = yield self.exported_data(items, settings)\n         expected = [{k: v for k, v in row.items() if v} for row in rows]\n         import pickle\n-        result = self._load_until_eof(data['pickle'], load_func=pickle.load)\n+\n+        result = self._load_until_eof(data[\"pickle\"], load_func=pickle.load)\n         self.assertEqual(expected, result)\n \n     @defer.inlineCallbacks\n     def assertExportedMarshal(self, items, rows, settings=None):\n         settings = settings or {}\n-        settings.update({\n-            'FEEDS': {\n-                self._random_temp_filename(): {'format': 'marshal'},\n+        settings.update(\n+            {\n+                \"FEEDS\": {\n+                    self._random_temp_filename(): {\"format\": \"marshal\"},\n                 },\n-        })\n+            }\n+        )\n         data = yield self.exported_data(items, settings)\n         expected = [{k: v for k, v in row.items() if v} for row in rows]\n         import marshal\n-        result = self._load_until_eof(data['marshal'], load_func=marshal.load)\n+\n+        result = self._load_until_eof(data[\"marshal\"], load_func=marshal.load)\n         self.assertEqual(expected, result)\n \n     @defer.inlineCallbacks\n@@ -814,8 +812,12 @@ class FeedExportTest(FeedExportTestBase):\n         crawler = get_crawler(ItemSpider, settings)\n         with MockServer() as mockserver:\n             yield crawler.crawl(mockserver=mockserver)\n-        self.assertIn(\"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats())\n-        self.assertEqual(crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\"), 1)\n+        self.assertIn(\n+            \"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats()\n+        )\n+        self.assertEqual(\n+            crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\"), 1\n+        )\n \n     @defer.inlineCallbacks\n     def test_stats_file_failed(self):\n@@ -832,17 +834,22 @@ class FeedExportTest(FeedExportTestBase):\n             stack.enter_context(\n                 mock.patch(\n                     \"scrapy.extensions.feedexport.FileFeedStorage.store\",\n-                    side_effect=KeyError(\"foo\"))\n+                    side_effect=KeyError(\"foo\"),\n+                )\n             )\n             yield crawler.crawl(mockserver=mockserver)\n-        self.assertIn(\"feedexport/failed_count/FileFeedStorage\", crawler.stats.get_stats())\n-        self.assertEqual(crawler.stats.get_value(\"feedexport/failed_count/FileFeedStorage\"), 1)\n+        self.assertIn(\n+            \"feedexport/failed_count/FileFeedStorage\", crawler.stats.get_stats()\n+        )\n+        self.assertEqual(\n+            crawler.stats.get_value(\"feedexport/failed_count/FileFeedStorage\"), 1\n+        )\n \n     @defer.inlineCallbacks\n     def test_stats_multiple_file(self):\n         settings = {\n-            'AWS_ACCESS_KEY_ID': 'access_key',\n-            'AWS_SECRET_ACCESS_KEY': 'secret_key',\n+            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n+            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n             \"FEEDS\": {\n                 printf_escape(path_to_url(str(self._random_temp_filename()))): {\n                     \"format\": \"json\",\n@@ -852,60 +859,72 @@ class FeedExportTest(FeedExportTestBase):\n                 },\n                 \"stdout:\": {\n                     \"format\": \"xml\",\n-                }\n+                },\n             },\n         }\n         crawler = get_crawler(ItemSpider, settings)\n         with MockServer() as mockserver, mock.patch.object(S3FeedStorage, \"store\"):\n             yield crawler.crawl(mockserver=mockserver)\n-        self.assertIn(\"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats())\n-        self.assertIn(\"feedexport/success_count/S3FeedStorage\", crawler.stats.get_stats())\n-        self.assertIn(\"feedexport/success_count/StdoutFeedStorage\", crawler.stats.get_stats())\n-        self.assertEqual(crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\"), 1)\n-        self.assertEqual(crawler.stats.get_value(\"feedexport/success_count/S3FeedStorage\"), 1)\n-        self.assertEqual(crawler.stats.get_value(\"feedexport/success_count/StdoutFeedStorage\"), 1)\n+        self.assertIn(\n+            \"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats()\n+        )\n+        self.assertIn(\n+            \"feedexport/success_count/S3FeedStorage\", crawler.stats.get_stats()\n+        )\n+        self.assertIn(\n+            \"feedexport/success_count/StdoutFeedStorage\", crawler.stats.get_stats()\n+        )\n+        self.assertEqual(\n+            crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\"), 1\n+        )\n+        self.assertEqual(\n+            crawler.stats.get_value(\"feedexport/success_count/S3FeedStorage\"), 1\n+        )\n+        self.assertEqual(\n+            crawler.stats.get_value(\"feedexport/success_count/StdoutFeedStorage\"), 1\n+        )\n \n     @defer.inlineCallbacks\n     def test_export_items(self):\n         # feed exporters use field names from Item\n         items = [\n-            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n-            self.MyItem({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),\n+            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n+            self.MyItem({\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"}),\n         ]\n         rows = [\n-            {'egg': 'spam1', 'foo': 'bar1', 'baz': ''},\n-            {'egg': 'spam2', 'foo': 'bar2', 'baz': 'quux2'}\n+            {\"egg\": \"spam1\", \"foo\": \"bar1\", \"baz\": \"\"},\n+            {\"egg\": \"spam2\", \"foo\": \"bar2\", \"baz\": \"quux2\"},\n         ]\n         header = self.MyItem.fields.keys()\n         yield self.assertExported(items, header, rows)\n \n     @defer.inlineCallbacks\n     def test_export_no_items_not_store_empty(self):\n-        for fmt in ('json', 'jsonlines', 'xml', 'csv'):\n+        for fmt in (\"json\", \"jsonlines\", \"xml\", \"csv\"):\n             settings = {\n-                'FEEDS': {\n-                    self._random_temp_filename(): {'format': fmt},\n+                \"FEEDS\": {\n+                    self._random_temp_filename(): {\"format\": fmt},\n                 },\n             }\n             data = yield self.exported_no_data(settings)\n-            self.assertEqual(b'', data[fmt])\n+            self.assertEqual(b\"\", data[fmt])\n \n     @defer.inlineCallbacks\n     def test_export_no_items_store_empty(self):\n         formats = (\n-            ('json', b'[]'),\n-            ('jsonlines', b''),\n-            ('xml', b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items></items>'),\n-            ('csv', b''),\n+            (\"json\", b\"[]\"),\n+            (\"jsonlines\", b\"\"),\n+            (\"xml\", b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items></items>'),\n+            (\"csv\", b\"\"),\n         )\n \n         for fmt, expctd in formats:\n             settings = {\n-                'FEEDS': {\n-                    self._random_temp_filename(): {'format': fmt},\n+                \"FEEDS\": {\n+                    self._random_temp_filename(): {\"format\": fmt},\n                 },\n-                'FEED_STORE_EMPTY': True,\n-                'FEED_EXPORT_INDENT': None,\n+                \"FEED_STORE_EMPTY\": True,\n+                \"FEED_EXPORT_INDENT\": None,\n             }\n             data = yield self.exported_no_data(settings)\n             self.assertEqual(expctd, data[fmt])\n@@ -914,39 +933,39 @@ class FeedExportTest(FeedExportTestBase):\n     def test_export_no_items_multiple_feeds(self):\n         \"\"\"Make sure that `storage.store` is called for every feed.\"\"\"\n         settings = {\n-            'FEEDS': {\n-                self._random_temp_filename(): {'format': 'json'},\n-                self._random_temp_filename(): {'format': 'xml'},\n-                self._random_temp_filename(): {'format': 'csv'},\n+            \"FEEDS\": {\n+                self._random_temp_filename(): {\"format\": \"json\"},\n+                self._random_temp_filename(): {\"format\": \"xml\"},\n+                self._random_temp_filename(): {\"format\": \"csv\"},\n             },\n-            'FEED_STORAGES': {'file': LogOnStoreFileStorage},\n-            'FEED_STORE_EMPTY': False\n+            \"FEED_STORAGES\": {\"file\": LogOnStoreFileStorage},\n+            \"FEED_STORE_EMPTY\": False,\n         }\n \n         with LogCapture() as log:\n             yield self.exported_no_data(settings)\n \n         print(log)\n-        self.assertEqual(str(log).count('Storage.store is called'), 3)\n+        self.assertEqual(str(log).count(\"Storage.store is called\"), 3)\n \n     @defer.inlineCallbacks\n     def test_export_multiple_item_classes(self):\n \n         items = [\n-            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n-            self.MyItem2({'hello': 'world2', 'foo': 'bar2'}),\n-            self.MyItem({'foo': 'bar3', 'egg': 'spam3', 'baz': 'quux3'}),\n-            {'hello': 'world4', 'egg': 'spam4'},\n+            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n+            self.MyItem2({\"hello\": \"world2\", \"foo\": \"bar2\"}),\n+            self.MyItem({\"foo\": \"bar3\", \"egg\": \"spam3\", \"baz\": \"quux3\"}),\n+            {\"hello\": \"world4\", \"egg\": \"spam4\"},\n         ]\n \n         # by default, Scrapy uses fields of the first Item for CSV and\n         # all fields for JSON Lines\n         header = self.MyItem.fields.keys()\n         rows_csv = [\n-            {'egg': 'spam1', 'foo': 'bar1', 'baz': ''},\n-            {'egg': '', 'foo': 'bar2', 'baz': ''},\n-            {'egg': 'spam3', 'foo': 'bar3', 'baz': 'quux3'},\n-            {'egg': 'spam4', 'foo': '', 'baz': ''},\n+            {\"egg\": \"spam1\", \"foo\": \"bar1\", \"baz\": \"\"},\n+            {\"egg\": \"\", \"foo\": \"bar2\", \"baz\": \"\"},\n+            {\"egg\": \"spam3\", \"foo\": \"bar3\", \"baz\": \"quux3\"},\n+            {\"egg\": \"spam4\", \"foo\": \"\", \"baz\": \"\"},\n         ]\n         rows_jl = [dict(row) for row in items]\n         yield self.assertExportedCsv(items, header, rows_csv)\n@@ -955,106 +974,103 @@ class FeedExportTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_export_items_empty_field_list(self):\n         # FEED_EXPORT_FIELDS==[] means the same as default None\n-        items = [{'foo': 'bar'}]\n+        items = [{\"foo\": \"bar\"}]\n         header = [\"foo\"]\n-        rows = [{'foo': 'bar'}]\n-        settings = {'FEED_EXPORT_FIELDS': []}\n+        rows = [{\"foo\": \"bar\"}]\n+        settings = {\"FEED_EXPORT_FIELDS\": []}\n         yield self.assertExportedCsv(items, header, rows)\n         yield self.assertExportedJsonLines(items, rows, settings)\n \n     @defer.inlineCallbacks\n     def test_export_items_field_list(self):\n-        items = [{'foo': 'bar'}]\n+        items = [{\"foo\": \"bar\"}]\n         header = [\"foo\", \"baz\"]\n-        rows = [{'foo': 'bar', 'baz': ''}]\n-        settings = {'FEED_EXPORT_FIELDS': header}\n+        rows = [{\"foo\": \"bar\", \"baz\": \"\"}]\n+        settings = {\"FEED_EXPORT_FIELDS\": header}\n         yield self.assertExported(items, header, rows, settings=settings)\n \n     @defer.inlineCallbacks\n     def test_export_items_comma_separated_field_list(self):\n-        items = [{'foo': 'bar'}]\n+        items = [{\"foo\": \"bar\"}]\n         header = [\"foo\", \"baz\"]\n-        rows = [{'foo': 'bar', 'baz': ''}]\n-        settings = {'FEED_EXPORT_FIELDS': \",\".join(header)}\n+        rows = [{\"foo\": \"bar\", \"baz\": \"\"}]\n+        settings = {\"FEED_EXPORT_FIELDS\": \",\".join(header)}\n         yield self.assertExported(items, header, rows, settings=settings)\n \n     @defer.inlineCallbacks\n     def test_export_items_json_field_list(self):\n-        items = [{'foo': 'bar'}]\n+        items = [{\"foo\": \"bar\"}]\n         header = [\"foo\", \"baz\"]\n-        rows = [{'foo': 'bar', 'baz': ''}]\n-        settings = {'FEED_EXPORT_FIELDS': json.dumps(header)}\n+        rows = [{\"foo\": \"bar\", \"baz\": \"\"}]\n+        settings = {\"FEED_EXPORT_FIELDS\": json.dumps(header)}\n         yield self.assertExported(items, header, rows, settings=settings)\n \n     @defer.inlineCallbacks\n     def test_export_items_field_names(self):\n-        items = [{'foo': 'bar'}]\n-        header = {'foo': 'Foo'}\n-        rows = [{'Foo': 'bar'}]\n-        settings = {'FEED_EXPORT_FIELDS': header}\n-        yield self.assertExported(items, list(header.values()), rows,\n-                                  settings=settings)\n+        items = [{\"foo\": \"bar\"}]\n+        header = {\"foo\": \"Foo\"}\n+        rows = [{\"Foo\": \"bar\"}]\n+        settings = {\"FEED_EXPORT_FIELDS\": header}\n+        yield self.assertExported(items, list(header.values()), rows, settings=settings)\n \n     @defer.inlineCallbacks\n     def test_export_items_dict_field_names(self):\n-        items = [{'foo': 'bar'}]\n+        items = [{\"foo\": \"bar\"}]\n         header = {\n-            'baz': 'Baz',\n-            'foo': 'Foo',\n+            \"baz\": \"Baz\",\n+            \"foo\": \"Foo\",\n         }\n-        rows = [{'Baz': '', 'Foo': 'bar'}]\n-        settings = {'FEED_EXPORT_FIELDS': header}\n-        yield self.assertExported(items, ['Baz', 'Foo'], rows,\n-                                  settings=settings)\n+        rows = [{\"Baz\": \"\", \"Foo\": \"bar\"}]\n+        settings = {\"FEED_EXPORT_FIELDS\": header}\n+        yield self.assertExported(items, [\"Baz\", \"Foo\"], rows, settings=settings)\n \n     @defer.inlineCallbacks\n     def test_export_items_json_field_names(self):\n-        items = [{'foo': 'bar'}]\n-        header = {'foo': 'Foo'}\n-        rows = [{'Foo': 'bar'}]\n-        settings = {'FEED_EXPORT_FIELDS': json.dumps(header)}\n-        yield self.assertExported(items, list(header.values()), rows,\n-                                  settings=settings)\n+        items = [{\"foo\": \"bar\"}]\n+        header = {\"foo\": \"Foo\"}\n+        rows = [{\"Foo\": \"bar\"}]\n+        settings = {\"FEED_EXPORT_FIELDS\": json.dumps(header)}\n+        yield self.assertExported(items, list(header.values()), rows, settings=settings)\n \n     @defer.inlineCallbacks\n     def test_export_based_on_item_classes(self):\n         items = [\n-            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n-            self.MyItem2({'hello': 'world2', 'foo': 'bar2'}),\n-            {'hello': 'world3', 'egg': 'spam3'},\n+            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n+            self.MyItem2({\"hello\": \"world2\", \"foo\": \"bar2\"}),\n+            {\"hello\": \"world3\", \"egg\": \"spam3\"},\n         ]\n \n         formats = {\n-            'csv': b'baz,egg,foo\\r\\n,spam1,bar1\\r\\n',\n-            'json': b'[\\n{\"hello\": \"world2\", \"foo\": \"bar2\"}\\n]',\n-            'jsonlines': (\n+            \"csv\": b\"baz,egg,foo\\r\\n,spam1,bar1\\r\\n\",\n+            \"json\": b'[\\n{\"hello\": \"world2\", \"foo\": \"bar2\"}\\n]',\n+            \"jsonlines\": (\n                 b'{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n'\n                 b'{\"hello\": \"world2\", \"foo\": \"bar2\"}\\n'\n             ),\n-            'xml': (\n+            \"xml\": (\n                 b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items>\\n<item>'\n-                b'<foo>bar1</foo><egg>spam1</egg></item>\\n<item><hello>'\n-                b'world2</hello><foo>bar2</foo></item>\\n<item><hello>world3'\n-                b'</hello><egg>spam3</egg></item>\\n</items>'\n+                b\"<foo>bar1</foo><egg>spam1</egg></item>\\n<item><hello>\"\n+                b\"world2</hello><foo>bar2</foo></item>\\n<item><hello>world3\"\n+                b\"</hello><egg>spam3</egg></item>\\n</items>\"\n             ),\n         }\n \n         settings = {\n-            'FEEDS': {\n+            \"FEEDS\": {\n                 self._random_temp_filename(): {\n-                    'format': 'csv',\n-                    'item_classes': [self.MyItem],\n+                    \"format\": \"csv\",\n+                    \"item_classes\": [self.MyItem],\n                 },\n                 self._random_temp_filename(): {\n-                    'format': 'json',\n-                    'item_classes': [self.MyItem2],\n+                    \"format\": \"json\",\n+                    \"item_classes\": [self.MyItem2],\n                 },\n                 self._random_temp_filename(): {\n-                    'format': 'jsonlines',\n-                    'item_classes': [self.MyItem, self.MyItem2],\n+                    \"format\": \"jsonlines\",\n+                    \"item_classes\": [self.MyItem, self.MyItem2],\n                 },\n                 self._random_temp_filename(): {\n-                    'format': 'xml',\n+                    \"format\": \"xml\",\n                 },\n             },\n         }\n@@ -1066,9 +1082,9 @@ class FeedExportTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_export_based_on_custom_filters(self):\n         items = [\n-            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n-            self.MyItem2({'hello': 'world2', 'foo': 'bar2'}),\n-            {'hello': 'world3', 'egg': 'spam3'},\n+            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n+            self.MyItem2({\"hello\": \"world2\", \"foo\": \"bar2\"}),\n+            {\"hello\": \"world3\", \"egg\": \"spam3\"},\n         ]\n \n         MyItem = self.MyItem\n@@ -1082,40 +1098,40 @@ class FeedExportTest(FeedExportTestBase):\n \n         class CustomFilter2(scrapy.extensions.feedexport.ItemFilter):\n             def accepts(self, item):\n-                if 'foo' not in item.fields:\n+                if \"foo\" not in item.fields:\n                     return False\n                 return True\n \n         class CustomFilter3(scrapy.extensions.feedexport.ItemFilter):\n             def accepts(self, item):\n-                if isinstance(item, tuple(self.item_classes)) and item['foo'] == \"bar1\":\n+                if isinstance(item, tuple(self.item_classes)) and item[\"foo\"] == \"bar1\":\n                     return True\n                 return False\n \n         formats = {\n-            'json': b'[\\n{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n]',\n-            'xml': (\n+            \"json\": b'[\\n{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n]',\n+            \"xml\": (\n                 b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items>\\n<item>'\n-                b'<foo>bar1</foo><egg>spam1</egg></item>\\n<item><hello>'\n-                b'world2</hello><foo>bar2</foo></item>\\n</items>'\n+                b\"<foo>bar1</foo><egg>spam1</egg></item>\\n<item><hello>\"\n+                b\"world2</hello><foo>bar2</foo></item>\\n</items>\"\n             ),\n-            'jsonlines': b'{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n',\n+            \"jsonlines\": b'{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n',\n         }\n \n         settings = {\n-            'FEEDS': {\n+            \"FEEDS\": {\n                 self._random_temp_filename(): {\n-                    'format': 'json',\n-                    'item_filter': CustomFilter1,\n+                    \"format\": \"json\",\n+                    \"item_filter\": CustomFilter1,\n                 },\n                 self._random_temp_filename(): {\n-                    'format': 'xml',\n-                    'item_filter': CustomFilter2,\n+                    \"format\": \"xml\",\n+                    \"item_filter\": CustomFilter2,\n                 },\n                 self._random_temp_filename(): {\n-                    'format': 'jsonlines',\n-                    'item_classes': [self.MyItem, self.MyItem2],\n-                    'item_filter': CustomFilter3,\n+                    \"format\": \"jsonlines\",\n+                    \"item_classes\": [self.MyItem, self.MyItem2],\n+                    \"item_filter\": CustomFilter3,\n                 },\n             },\n         }\n@@ -1129,15 +1145,12 @@ class FeedExportTest(FeedExportTestBase):\n         # When dicts are used, only keys from the first row are used as\n         # a header for CSV, and all fields are used for JSON Lines.\n         items = [\n-            {'foo': 'bar', 'egg': 'spam'},\n-            {'foo': 'bar', 'egg': 'spam', 'baz': 'quux'},\n-        ]\n-        rows_csv = [\n-            {'egg': 'spam', 'foo': 'bar'},\n-            {'egg': 'spam', 'foo': 'bar'}\n+            {\"foo\": \"bar\", \"egg\": \"spam\"},\n+            {\"foo\": \"bar\", \"egg\": \"spam\", \"baz\": \"quux\"},\n         ]\n+        rows_csv = [{\"egg\": \"spam\", \"foo\": \"bar\"}, {\"egg\": \"spam\", \"foo\": \"bar\"}]\n         rows_jl = items\n-        yield self.assertExportedCsv(items, ['foo', 'egg'], rows_csv)\n+        yield self.assertExportedCsv(items, [\"foo\", \"egg\"], rows_csv)\n         yield self.assertExportedJsonLines(items, rows_jl)\n \n     @defer.inlineCallbacks\n@@ -1147,105 +1160,102 @@ class FeedExportTest(FeedExportTestBase):\n \n         for item_cls in [self.MyItem, dict]:\n             items = [\n-                item_cls({'foo': 'bar1', 'egg': 'spam1'}),\n-                item_cls({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),\n+                item_cls({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n+                item_cls({\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"}),\n             ]\n \n             # export all columns\n-            settings = {'FEED_EXPORT_FIELDS': 'foo,baz,egg'}\n+            settings = {\"FEED_EXPORT_FIELDS\": \"foo,baz,egg\"}\n             rows = [\n-                {'egg': 'spam1', 'foo': 'bar1', 'baz': ''},\n-                {'egg': 'spam2', 'foo': 'bar2', 'baz': 'quux2'}\n+                {\"egg\": \"spam1\", \"foo\": \"bar1\", \"baz\": \"\"},\n+                {\"egg\": \"spam2\", \"foo\": \"bar2\", \"baz\": \"quux2\"},\n             ]\n-            yield self.assertExported(items, ['foo', 'baz', 'egg'], rows,\n-                                      settings=settings)\n+            yield self.assertExported(\n+                items, [\"foo\", \"baz\", \"egg\"], rows, settings=settings\n+            )\n \n             # export a subset of columns\n-            settings = {'FEED_EXPORT_FIELDS': 'egg,baz'}\n-            rows = [\n-                {'egg': 'spam1', 'baz': ''},\n-                {'egg': 'spam2', 'baz': 'quux2'}\n-            ]\n-            yield self.assertExported(items, ['egg', 'baz'], rows,\n-                                      settings=settings)\n+            settings = {\"FEED_EXPORT_FIELDS\": \"egg,baz\"}\n+            rows = [{\"egg\": \"spam1\", \"baz\": \"\"}, {\"egg\": \"spam2\", \"baz\": \"quux2\"}]\n+            yield self.assertExported(items, [\"egg\", \"baz\"], rows, settings=settings)\n \n     @defer.inlineCallbacks\n     def test_export_encoding(self):\n-        items = [dict({'foo': 'Test\\xd6'})]\n+        items = [dict({\"foo\": \"Test\\xd6\"})]\n \n         formats = {\n-            'json': '[{\"foo\": \"Test\\\\u00d6\"}]'.encode('utf-8'),\n-            'jsonlines': '{\"foo\": \"Test\\\\u00d6\"}\\n'.encode('utf-8'),\n-            'xml': (\n+            \"json\": '[{\"foo\": \"Test\\\\u00d6\"}]'.encode(\"utf-8\"),\n+            \"jsonlines\": '{\"foo\": \"Test\\\\u00d6\"}\\n'.encode(\"utf-8\"),\n+            \"xml\": (\n                 '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n-                '<items><item><foo>Test\\xd6</foo></item></items>'\n-            ).encode('utf-8'),\n-            'csv': 'foo\\r\\nTest\\xd6\\r\\n'.encode('utf-8'),\n+                \"<items><item><foo>Test\\xd6</foo></item></items>\"\n+            ).encode(\"utf-8\"),\n+            \"csv\": \"foo\\r\\nTest\\xd6\\r\\n\".encode(\"utf-8\"),\n         }\n \n         for fmt, expected in formats.items():\n             settings = {\n-                'FEEDS': {\n-                    self._random_temp_filename(): {'format': fmt},\n+                \"FEEDS\": {\n+                    self._random_temp_filename(): {\"format\": fmt},\n                 },\n-                'FEED_EXPORT_INDENT': None,\n+                \"FEED_EXPORT_INDENT\": None,\n             }\n             data = yield self.exported_data(items, settings)\n             self.assertEqual(expected, data[fmt])\n \n         formats = {\n-            'json': '[{\"foo\": \"Test\\xd6\"}]'.encode('latin-1'),\n-            'jsonlines': '{\"foo\": \"Test\\xd6\"}\\n'.encode('latin-1'),\n-            'xml': (\n+            \"json\": '[{\"foo\": \"Test\\xd6\"}]'.encode(\"latin-1\"),\n+            \"jsonlines\": '{\"foo\": \"Test\\xd6\"}\\n'.encode(\"latin-1\"),\n+            \"xml\": (\n                 '<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n-                '<items><item><foo>Test\\xd6</foo></item></items>'\n-            ).encode('latin-1'),\n-            'csv': 'foo\\r\\nTest\\xd6\\r\\n'.encode('latin-1'),\n+                \"<items><item><foo>Test\\xd6</foo></item></items>\"\n+            ).encode(\"latin-1\"),\n+            \"csv\": \"foo\\r\\nTest\\xd6\\r\\n\".encode(\"latin-1\"),\n         }\n \n         for fmt, expected in formats.items():\n             settings = {\n-                'FEEDS': {\n-                    self._random_temp_filename(): {'format': fmt},\n+                \"FEEDS\": {\n+                    self._random_temp_filename(): {\"format\": fmt},\n                 },\n-                'FEED_EXPORT_INDENT': None,\n-                'FEED_EXPORT_ENCODING': 'latin-1',\n+                \"FEED_EXPORT_INDENT\": None,\n+                \"FEED_EXPORT_ENCODING\": \"latin-1\",\n             }\n             data = yield self.exported_data(items, settings)\n             self.assertEqual(expected, data[fmt])\n \n     @defer.inlineCallbacks\n     def test_export_multiple_configs(self):\n-        items = [dict({'foo': 'FOO', 'bar': 'BAR'})]\n+        items = [dict({\"foo\": \"FOO\", \"bar\": \"BAR\"})]\n \n         formats = {\n-            'json': '[\\n{\"bar\": \"BAR\"}\\n]'.encode('utf-8'),\n-            'xml': (\n+            \"json\": '[\\n{\"bar\": \"BAR\"}\\n]'.encode(\"utf-8\"),\n+            \"xml\": (\n                 '<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n-                '<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>'\n-            ).encode('latin-1'),\n-            'csv': 'bar,foo\\r\\nBAR,FOO\\r\\n'.encode('utf-8'),\n+                \"<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>\"\n+            ).encode(\"latin-1\"),\n+            \"csv\": \"bar,foo\\r\\nBAR,FOO\\r\\n\".encode(\"utf-8\"),\n         }\n \n         settings = {\n-            'FEEDS': {\n+            \"FEEDS\": {\n                 self._random_temp_filename(): {\n-                    'format': 'json',\n-                    'indent': 0,\n-                    'fields': ['bar'],\n-                    'encoding': 'utf-8',\n+                    \"format\": \"json\",\n+                    \"indent\": 0,\n+                    \"fields\": [\"bar\"],\n+                    \"encoding\": \"utf-8\",\n                 },\n                 self._random_temp_filename(): {\n-                    'format': 'xml',\n-                    'indent': 2,\n-                    'fields': ['foo'],\n-                    'encoding': 'latin-1',\n+                    \"format\": \"xml\",\n+                    \"indent\": 2,\n+                    \"fields\": [\"foo\"],\n+                    \"encoding\": \"latin-1\",\n                 },\n                 self._random_temp_filename(): {\n-                    'format': 'csv',\n-                    'indent': None,\n-                    'fields': ['bar', 'foo'],\n-                    'encoding': 'utf-8',\n+                    \"format\": \"csv\",\n+                    \"indent\": None,\n+                    \"fields\": [\"bar\", \"foo\"],\n+                    \"encoding\": \"utf-8\",\n                 },\n             },\n         }\n@@ -1257,37 +1267,37 @@ class FeedExportTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_export_indentation(self):\n         items = [\n-            {'foo': ['bar']},\n-            {'key': 'value'},\n+            {\"foo\": [\"bar\"]},\n+            {\"key\": \"value\"},\n         ]\n \n         test_cases = [\n             # JSON\n             {\n-                'format': 'json',\n-                'indent': None,\n-                'expected': b'[{\"foo\": [\"bar\"]},{\"key\": \"value\"}]',\n+                \"format\": \"json\",\n+                \"indent\": None,\n+                \"expected\": b'[{\"foo\": [\"bar\"]},{\"key\": \"value\"}]',\n             },\n             {\n-                'format': 'json',\n-                'indent': -1,\n-                'expected': b\"\"\"[\n+                \"format\": \"json\",\n+                \"indent\": -1,\n+                \"expected\": b\"\"\"[\n {\"foo\": [\"bar\"]},\n {\"key\": \"value\"}\n ]\"\"\",\n             },\n             {\n-                'format': 'json',\n-                'indent': 0,\n-                'expected': b\"\"\"[\n+                \"format\": \"json\",\n+                \"indent\": 0,\n+                \"expected\": b\"\"\"[\n {\"foo\": [\"bar\"]},\n {\"key\": \"value\"}\n ]\"\"\",\n             },\n             {\n-                'format': 'json',\n-                'indent': 2,\n-                'expected': b\"\"\"[\n+                \"format\": \"json\",\n+                \"indent\": 2,\n+                \"expected\": b\"\"\"[\n {\n   \"foo\": [\n     \"bar\"\n@@ -1299,9 +1309,9 @@ class FeedExportTest(FeedExportTestBase):\n ]\"\"\",\n             },\n             {\n-                'format': 'json',\n-                'indent': 4,\n-                'expected': b\"\"\"[\n+                \"format\": \"json\",\n+                \"indent\": 4,\n+                \"expected\": b\"\"\"[\n {\n     \"foo\": [\n         \"bar\"\n@@ -1313,9 +1323,9 @@ class FeedExportTest(FeedExportTestBase):\n ]\"\"\",\n             },\n             {\n-                'format': 'json',\n-                'indent': 5,\n-                'expected': b\"\"\"[\n+                \"format\": \"json\",\n+                \"indent\": 5,\n+                \"expected\": b\"\"\"[\n {\n      \"foo\": [\n           \"bar\"\n@@ -1326,36 +1336,35 @@ class FeedExportTest(FeedExportTestBase):\n }\n ]\"\"\",\n             },\n-\n             # XML\n             {\n-                'format': 'xml',\n-                'indent': None,\n-                'expected': b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+                \"format\": \"xml\",\n+                \"indent\": None,\n+                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items><item><foo><value>bar</value></foo></item><item><key>value</key></item></items>\"\"\",\n             },\n             {\n-                'format': 'xml',\n-                'indent': -1,\n-                'expected': b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+                \"format\": \"xml\",\n+                \"indent\": -1,\n+                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items>\n <item><foo><value>bar</value></foo></item>\n <item><key>value</key></item>\n </items>\"\"\",\n             },\n             {\n-                'format': 'xml',\n-                'indent': 0,\n-                'expected': b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+                \"format\": \"xml\",\n+                \"indent\": 0,\n+                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items>\n <item><foo><value>bar</value></foo></item>\n <item><key>value</key></item>\n </items>\"\"\",\n             },\n             {\n-                'format': 'xml',\n-                'indent': 2,\n-                'expected': b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+                \"format\": \"xml\",\n+                \"indent\": 2,\n+                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items>\n   <item>\n     <foo>\n@@ -1368,9 +1377,9 @@ class FeedExportTest(FeedExportTestBase):\n </items>\"\"\",\n             },\n             {\n-                'format': 'xml',\n-                'indent': 4,\n-                'expected': b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+                \"format\": \"xml\",\n+                \"indent\": 4,\n+                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items>\n     <item>\n         <foo>\n@@ -1383,9 +1392,9 @@ class FeedExportTest(FeedExportTestBase):\n </items>\"\"\",\n             },\n             {\n-                'format': 'xml',\n-                'indent': 5,\n-                'expected': b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+                \"format\": \"xml\",\n+                \"indent\": 5,\n+                \"expected\": b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items>\n      <item>\n           <foo>\n@@ -1401,23 +1410,23 @@ class FeedExportTest(FeedExportTestBase):\n \n         for row in test_cases:\n             settings = {\n-                'FEEDS': {\n+                \"FEEDS\": {\n                     self._random_temp_filename(): {\n-                        'format': row['format'],\n-                        'indent': row['indent'],\n+                        \"format\": row[\"format\"],\n+                        \"indent\": row[\"indent\"],\n                     },\n                 },\n             }\n             data = yield self.exported_data(items, settings)\n-            self.assertEqual(row['expected'], data[row['format']])\n+            self.assertEqual(row[\"expected\"], data[row[\"format\"]])\n \n     @defer.inlineCallbacks\n     def test_init_exporters_storages_with_crawler(self):\n         settings = {\n-            'FEED_EXPORTERS': {'csv': FromCrawlerCsvItemExporter},\n-            'FEED_STORAGES': {'file': FromCrawlerFileFeedStorage},\n-            'FEEDS': {\n-                self._random_temp_filename(): {'format': 'csv'},\n+            \"FEED_EXPORTERS\": {\"csv\": FromCrawlerCsvItemExporter},\n+            \"FEED_STORAGES\": {\"file\": FromCrawlerFileFeedStorage},\n+            \"FEEDS\": {\n+                self._random_temp_filename(): {\"format\": \"csv\"},\n             },\n         }\n         yield self.exported_data(items=[], settings=settings)\n@@ -1427,105 +1436,103 @@ class FeedExportTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_str_uri(self):\n         settings = {\n-            'FEED_STORE_EMPTY': True,\n-            'FEEDS': {\n-                str(self._random_temp_filename()): {'format': 'csv'}\n-            },\n+            \"FEED_STORE_EMPTY\": True,\n+            \"FEEDS\": {str(self._random_temp_filename()): {\"format\": \"csv\"}},\n         }\n         data = yield self.exported_no_data(settings)\n-        self.assertEqual(data['csv'], b'')\n+        self.assertEqual(data[\"csv\"], b\"\")\n \n     @defer.inlineCallbacks\n     def test_multiple_feeds_success_logs_blocking_feed_storage(self):\n         settings = {\n-            'FEEDS': {\n-                self._random_temp_filename(): {'format': 'json'},\n-                self._random_temp_filename(): {'format': 'xml'},\n-                self._random_temp_filename(): {'format': 'csv'},\n+            \"FEEDS\": {\n+                self._random_temp_filename(): {\"format\": \"json\"},\n+                self._random_temp_filename(): {\"format\": \"xml\"},\n+                self._random_temp_filename(): {\"format\": \"csv\"},\n             },\n-            'FEED_STORAGES': {'file': DummyBlockingFeedStorage},\n+            \"FEED_STORAGES\": {\"file\": DummyBlockingFeedStorage},\n         }\n         items = [\n-            {'foo': 'bar1', 'baz': ''},\n-            {'foo': 'bar2', 'baz': 'quux'},\n+            {\"foo\": \"bar1\", \"baz\": \"\"},\n+            {\"foo\": \"bar2\", \"baz\": \"quux\"},\n         ]\n         with LogCapture() as log:\n             yield self.exported_data(items, settings)\n \n         print(log)\n-        for fmt in ['json', 'xml', 'csv']:\n-            self.assertIn(f'Stored {fmt} feed (2 items)', str(log))\n+        for fmt in [\"json\", \"xml\", \"csv\"]:\n+            self.assertIn(f\"Stored {fmt} feed (2 items)\", str(log))\n \n     @defer.inlineCallbacks\n     def test_multiple_feeds_failing_logs_blocking_feed_storage(self):\n         settings = {\n-            'FEEDS': {\n-                self._random_temp_filename(): {'format': 'json'},\n-                self._random_temp_filename(): {'format': 'xml'},\n-                self._random_temp_filename(): {'format': 'csv'},\n+            \"FEEDS\": {\n+                self._random_temp_filename(): {\"format\": \"json\"},\n+                self._random_temp_filename(): {\"format\": \"xml\"},\n+                self._random_temp_filename(): {\"format\": \"csv\"},\n             },\n-            'FEED_STORAGES': {'file': FailingBlockingFeedStorage},\n+            \"FEED_STORAGES\": {\"file\": FailingBlockingFeedStorage},\n         }\n         items = [\n-            {'foo': 'bar1', 'baz': ''},\n-            {'foo': 'bar2', 'baz': 'quux'},\n+            {\"foo\": \"bar1\", \"baz\": \"\"},\n+            {\"foo\": \"bar2\", \"baz\": \"quux\"},\n         ]\n         with LogCapture() as log:\n             yield self.exported_data(items, settings)\n \n         print(log)\n-        for fmt in ['json', 'xml', 'csv']:\n-            self.assertIn(f'Error storing {fmt} feed (2 items)', str(log))\n+        for fmt in [\"json\", \"xml\", \"csv\"]:\n+            self.assertIn(f\"Error storing {fmt} feed (2 items)\", str(log))\n \n     @defer.inlineCallbacks\n     def test_extend_kwargs(self):\n-        items = [{'foo': 'FOO', 'bar': 'BAR'}]\n+        items = [{\"foo\": \"FOO\", \"bar\": \"BAR\"}]\n \n-        expected_with_title_csv = 'foo,bar\\r\\nFOO,BAR\\r\\n'.encode('utf-8')\n-        expected_without_title_csv = 'FOO,BAR\\r\\n'.encode('utf-8')\n+        expected_with_title_csv = \"foo,bar\\r\\nFOO,BAR\\r\\n\".encode(\"utf-8\")\n+        expected_without_title_csv = \"FOO,BAR\\r\\n\".encode(\"utf-8\")\n         test_cases = [\n             # with title\n             {\n-                'options': {\n-                    'format': 'csv',\n-                    'item_export_kwargs': {'include_headers_line': True},\n+                \"options\": {\n+                    \"format\": \"csv\",\n+                    \"item_export_kwargs\": {\"include_headers_line\": True},\n                 },\n-                'expected': expected_with_title_csv,\n+                \"expected\": expected_with_title_csv,\n             },\n             # without title\n             {\n-                'options': {\n-                    'format': 'csv',\n-                    'item_export_kwargs': {'include_headers_line': False},\n+                \"options\": {\n+                    \"format\": \"csv\",\n+                    \"item_export_kwargs\": {\"include_headers_line\": False},\n                 },\n-                'expected': expected_without_title_csv,\n+                \"expected\": expected_without_title_csv,\n             },\n         ]\n \n         for row in test_cases:\n-            feed_options = row['options']\n+            feed_options = row[\"options\"]\n             settings = {\n-                'FEEDS': {\n+                \"FEEDS\": {\n                     self._random_temp_filename(): feed_options,\n                 },\n-                'FEED_EXPORT_INDENT': None,\n+                \"FEED_EXPORT_INDENT\": None,\n             }\n \n             data = yield self.exported_data(items, settings)\n-            self.assertEqual(row['expected'], data[feed_options['format']])\n+            self.assertEqual(row[\"expected\"], data[feed_options[\"format\"]])\n \n \n class FeedPostProcessedExportsTest(FeedExportTestBase):\n     __test__ = True\n \n-    items = [{'foo': 'bar'}]\n-    expected = b'foo\\r\\nbar\\r\\n'\n+    items = [{\"foo\": \"bar\"}]\n+    expected = b\"foo\\r\\nbar\\r\\n\"\n \n     class MyPlugin1:\n         def __init__(self, file, feed_options):\n             self.file = file\n             self.feed_options = feed_options\n-            self.char = self.feed_options.get('plugin1_char', b'')\n+            self.char = self.feed_options.get(\"plugin1_char\", b\"\")\n \n         def write(self, data):\n             written_count = self.file.write(data)\n@@ -1542,8 +1549,8 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n     def run_and_export(self, spider_cls, settings):\n         \"\"\"Run spider with specified settings; return exported data with filename.\"\"\"\n \n-        FEEDS = settings.get('FEEDS') or {}\n-        settings['FEEDS'] = {\n+        FEEDS = settings.get(\"FEEDS\") or {}\n+        settings[\"FEEDS\"] = {\n             printf_escape(path_to_url(file_path)): feed_options\n             for file_path, feed_options in FEEDS.items()\n         }\n@@ -1551,7 +1558,7 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n         content = {}\n         try:\n             with MockServer() as s:\n-                spider_cls.start_urls = [s.url('/')]\n+                spider_cls.start_urls = [s.url(\"/\")]\n                 crawler = get_crawler(spider_cls, settings)\n                 yield crawler.crawl()\n \n@@ -1570,10 +1577,15 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n \n         return content\n \n-    def get_gzip_compressed(self, data, compresslevel=9, mtime=0, filename=''):\n+    def get_gzip_compressed(self, data, compresslevel=9, mtime=0, filename=\"\"):\n         data_stream = BytesIO()\n-        gzipf = gzip.GzipFile(fileobj=data_stream, filename=filename, mtime=mtime,\n-                              compresslevel=compresslevel, mode=\"wb\")\n+        gzipf = gzip.GzipFile(\n+            fileobj=data_stream,\n+            filename=filename,\n+            mtime=mtime,\n+            compresslevel=compresslevel,\n+            mode=\"wb\",\n+        )\n         gzipf.write(data)\n         gzipf.close()\n         data_stream.seek(0)\n@@ -1582,13 +1594,13 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_gzip_plugin(self):\n \n-        filename = self._named_tempfile('gzip_file')\n+        filename = self._named_tempfile(\"gzip_file\")\n \n         settings = {\n-            'FEEDS': {\n+            \"FEEDS\": {\n                 filename: {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n                 },\n             },\n         }\n@@ -1603,25 +1615,29 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n     def test_gzip_plugin_compresslevel(self):\n \n         filename_to_compressed = {\n-            self._named_tempfile('compresslevel_0'): self.get_gzip_compressed(self.expected, compresslevel=0),\n-            self._named_tempfile('compresslevel_9'): self.get_gzip_compressed(self.expected, compresslevel=9),\n+            self._named_tempfile(\"compresslevel_0\"): self.get_gzip_compressed(\n+                self.expected, compresslevel=0\n+            ),\n+            self._named_tempfile(\"compresslevel_9\"): self.get_gzip_compressed(\n+                self.expected, compresslevel=9\n+            ),\n         }\n \n         settings = {\n-            'FEEDS': {\n-                self._named_tempfile('compresslevel_0'): {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],\n-                    'gzip_compresslevel': 0,\n-                    'gzip_mtime': 0,\n-                    'gzip_filename': \"\",\n+            \"FEEDS\": {\n+                self._named_tempfile(\"compresslevel_0\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n+                    \"gzip_compresslevel\": 0,\n+                    \"gzip_mtime\": 0,\n+                    \"gzip_filename\": \"\",\n                 },\n-                self._named_tempfile('compresslevel_9'): {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],\n-                    'gzip_compresslevel': 9,\n-                    'gzip_mtime': 0,\n-                    'gzip_filename': \"\",\n+                self._named_tempfile(\"compresslevel_9\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n+                    \"gzip_compresslevel\": 9,\n+                    \"gzip_mtime\": 0,\n+                    \"gzip_filename\": \"\",\n                 },\n             },\n         }\n@@ -1636,23 +1652,27 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_gzip_plugin_mtime(self):\n         filename_to_compressed = {\n-            self._named_tempfile('mtime_123'): self.get_gzip_compressed(self.expected, mtime=123),\n-            self._named_tempfile('mtime_123456789'): self.get_gzip_compressed(self.expected, mtime=123456789),\n+            self._named_tempfile(\"mtime_123\"): self.get_gzip_compressed(\n+                self.expected, mtime=123\n+            ),\n+            self._named_tempfile(\"mtime_123456789\"): self.get_gzip_compressed(\n+                self.expected, mtime=123456789\n+            ),\n         }\n \n         settings = {\n-            'FEEDS': {\n-                self._named_tempfile('mtime_123'): {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],\n-                    'gzip_mtime': 123,\n-                    'gzip_filename': \"\",\n+            \"FEEDS\": {\n+                self._named_tempfile(\"mtime_123\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n+                    \"gzip_mtime\": 123,\n+                    \"gzip_filename\": \"\",\n                 },\n-                self._named_tempfile('mtime_123456789'): {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],\n-                    'gzip_mtime': 123456789,\n-                    'gzip_filename': \"\",\n+                self._named_tempfile(\"mtime_123456789\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n+                    \"gzip_mtime\": 123456789,\n+                    \"gzip_filename\": \"\",\n                 },\n             },\n         }\n@@ -1667,23 +1687,27 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_gzip_plugin_filename(self):\n         filename_to_compressed = {\n-            self._named_tempfile('filename_FILE1'): self.get_gzip_compressed(self.expected, filename=\"FILE1\"),\n-            self._named_tempfile('filename_FILE2'): self.get_gzip_compressed(self.expected, filename=\"FILE2\"),\n+            self._named_tempfile(\"filename_FILE1\"): self.get_gzip_compressed(\n+                self.expected, filename=\"FILE1\"\n+            ),\n+            self._named_tempfile(\"filename_FILE2\"): self.get_gzip_compressed(\n+                self.expected, filename=\"FILE2\"\n+            ),\n         }\n \n         settings = {\n-            'FEEDS': {\n-                self._named_tempfile('filename_FILE1'): {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],\n-                    'gzip_mtime': 0,\n-                    'gzip_filename': \"FILE1\",\n+            \"FEEDS\": {\n+                self._named_tempfile(\"filename_FILE1\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n+                    \"gzip_mtime\": 0,\n+                    \"gzip_filename\": \"FILE1\",\n                 },\n-                self._named_tempfile('filename_FILE2'): {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],\n-                    'gzip_mtime': 0,\n-                    'gzip_filename': \"FILE2\",\n+                self._named_tempfile(\"filename_FILE2\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.GzipPlugin\"],\n+                    \"gzip_mtime\": 0,\n+                    \"gzip_filename\": \"FILE2\",\n                 },\n             },\n         }\n@@ -1698,13 +1722,13 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_lzma_plugin(self):\n \n-        filename = self._named_tempfile('lzma_file')\n+        filename = self._named_tempfile(\"lzma_file\")\n \n         settings = {\n-            'FEEDS': {\n+            \"FEEDS\": {\n                 filename: {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n                 },\n             },\n         }\n@@ -1719,21 +1743,25 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n     def test_lzma_plugin_format(self):\n \n         filename_to_compressed = {\n-            self._named_tempfile('format_FORMAT_XZ'): lzma.compress(self.expected, format=lzma.FORMAT_XZ),\n-            self._named_tempfile('format_FORMAT_ALONE'): lzma.compress(self.expected, format=lzma.FORMAT_ALONE),\n+            self._named_tempfile(\"format_FORMAT_XZ\"): lzma.compress(\n+                self.expected, format=lzma.FORMAT_XZ\n+            ),\n+            self._named_tempfile(\"format_FORMAT_ALONE\"): lzma.compress(\n+                self.expected, format=lzma.FORMAT_ALONE\n+            ),\n         }\n \n         settings = {\n-            'FEEDS': {\n-                self._named_tempfile('format_FORMAT_XZ'): {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],\n-                    'lzma_format': lzma.FORMAT_XZ,\n+            \"FEEDS\": {\n+                self._named_tempfile(\"format_FORMAT_XZ\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n+                    \"lzma_format\": lzma.FORMAT_XZ,\n                 },\n-                self._named_tempfile('format_FORMAT_ALONE'): {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],\n-                    'lzma_format': lzma.FORMAT_ALONE,\n+                self._named_tempfile(\"format_FORMAT_ALONE\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n+                    \"lzma_format\": lzma.FORMAT_ALONE,\n                 },\n             },\n         }\n@@ -1749,21 +1777,25 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n     def test_lzma_plugin_check(self):\n \n         filename_to_compressed = {\n-            self._named_tempfile('check_CHECK_NONE'): lzma.compress(self.expected, check=lzma.CHECK_NONE),\n-            self._named_tempfile('check_CHECK_CRC256'): lzma.compress(self.expected, check=lzma.CHECK_SHA256),\n+            self._named_tempfile(\"check_CHECK_NONE\"): lzma.compress(\n+                self.expected, check=lzma.CHECK_NONE\n+            ),\n+            self._named_tempfile(\"check_CHECK_CRC256\"): lzma.compress(\n+                self.expected, check=lzma.CHECK_SHA256\n+            ),\n         }\n \n         settings = {\n-            'FEEDS': {\n-                self._named_tempfile('check_CHECK_NONE'): {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],\n-                    'lzma_check': lzma.CHECK_NONE,\n+            \"FEEDS\": {\n+                self._named_tempfile(\"check_CHECK_NONE\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n+                    \"lzma_check\": lzma.CHECK_NONE,\n                 },\n-                self._named_tempfile('check_CHECK_CRC256'): {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],\n-                    'lzma_check': lzma.CHECK_SHA256,\n+                self._named_tempfile(\"check_CHECK_CRC256\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n+                    \"lzma_check\": lzma.CHECK_SHA256,\n                 },\n             },\n         }\n@@ -1779,21 +1811,25 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n     def test_lzma_plugin_preset(self):\n \n         filename_to_compressed = {\n-            self._named_tempfile('preset_PRESET_0'): lzma.compress(self.expected, preset=0),\n-            self._named_tempfile('preset_PRESET_9'): lzma.compress(self.expected, preset=9),\n+            self._named_tempfile(\"preset_PRESET_0\"): lzma.compress(\n+                self.expected, preset=0\n+            ),\n+            self._named_tempfile(\"preset_PRESET_9\"): lzma.compress(\n+                self.expected, preset=9\n+            ),\n         }\n \n         settings = {\n-            'FEEDS': {\n-                self._named_tempfile('preset_PRESET_0'): {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],\n-                    'lzma_preset': 0,\n+            \"FEEDS\": {\n+                self._named_tempfile(\"preset_PRESET_0\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n+                    \"lzma_preset\": 0,\n                 },\n-                self._named_tempfile('preset_PRESET_9'): {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],\n-                    'lzma_preset': 9,\n+                self._named_tempfile(\"preset_PRESET_9\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n+                    \"lzma_preset\": 9,\n                 },\n             },\n         }\n@@ -1811,16 +1847,16 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n             # https://foss.heptapod.net/pypy/pypy/-/issues/3527\n             raise unittest.SkipTest(\"lzma filters doesn't work in PyPy\")\n \n-        filters = [{'id': lzma.FILTER_LZMA2}]\n+        filters = [{\"id\": lzma.FILTER_LZMA2}]\n         compressed = lzma.compress(self.expected, filters=filters)\n-        filename = self._named_tempfile('filters')\n+        filename = self._named_tempfile(\"filters\")\n \n         settings = {\n-            'FEEDS': {\n+            \"FEEDS\": {\n                 filename: {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],\n-                    'lzma_filters': filters,\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.LZMAPlugin\"],\n+                    \"lzma_filters\": filters,\n                 },\n             },\n         }\n@@ -1833,13 +1869,13 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_bz2_plugin(self):\n \n-        filename = self._named_tempfile('bz2_file')\n+        filename = self._named_tempfile(\"bz2_file\")\n \n         settings = {\n-            'FEEDS': {\n+            \"FEEDS\": {\n                 filename: {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.Bz2Plugin'],\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.Bz2Plugin\"],\n                 },\n             },\n         }\n@@ -1854,21 +1890,25 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n     def test_bz2_plugin_compresslevel(self):\n \n         filename_to_compressed = {\n-            self._named_tempfile('compresslevel_1'): bz2.compress(self.expected, compresslevel=1),\n-            self._named_tempfile('compresslevel_9'): bz2.compress(self.expected, compresslevel=9),\n+            self._named_tempfile(\"compresslevel_1\"): bz2.compress(\n+                self.expected, compresslevel=1\n+            ),\n+            self._named_tempfile(\"compresslevel_9\"): bz2.compress(\n+                self.expected, compresslevel=9\n+            ),\n         }\n \n         settings = {\n-            'FEEDS': {\n-                self._named_tempfile('compresslevel_1'): {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.Bz2Plugin'],\n-                    'bz2_compresslevel': 1,\n+            \"FEEDS\": {\n+                self._named_tempfile(\"compresslevel_1\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.Bz2Plugin\"],\n+                    \"bz2_compresslevel\": 1,\n                 },\n-                self._named_tempfile('compresslevel_9'): {\n-                    'format': 'csv',\n-                    'postprocessing': ['scrapy.extensions.postprocessing.Bz2Plugin'],\n-                    'bz2_compresslevel': 9,\n+                self._named_tempfile(\"compresslevel_9\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\"scrapy.extensions.postprocessing.Bz2Plugin\"],\n+                    \"bz2_compresslevel\": 9,\n                 },\n             },\n         }\n@@ -1882,13 +1922,13 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_custom_plugin(self):\n-        filename = self._named_tempfile('csv_file')\n+        filename = self._named_tempfile(\"csv_file\")\n \n         settings = {\n-            'FEEDS': {\n+            \"FEEDS\": {\n                 filename: {\n-                    'format': 'csv',\n-                    'postprocessing': [self.MyPlugin1],\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [self.MyPlugin1],\n                 },\n             },\n         }\n@@ -1899,15 +1939,15 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_custom_plugin_with_parameter(self):\n \n-        expected = b'foo\\r\\n\\nbar\\r\\n\\n'\n-        filename = self._named_tempfile('newline')\n+        expected = b\"foo\\r\\n\\nbar\\r\\n\\n\"\n+        filename = self._named_tempfile(\"newline\")\n \n         settings = {\n-            'FEEDS': {\n+            \"FEEDS\": {\n                 filename: {\n-                    'format': 'csv',\n-                    'postprocessing': [self.MyPlugin1],\n-                    'plugin1_char': b'\\n'\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [self.MyPlugin1],\n+                    \"plugin1_char\": b\"\\n\",\n                 },\n             },\n         }\n@@ -1918,30 +1958,39 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_custom_plugin_with_compression(self):\n \n-        expected = b'foo\\r\\n\\nbar\\r\\n\\n'\n+        expected = b\"foo\\r\\n\\nbar\\r\\n\\n\"\n \n         filename_to_decompressor = {\n-            self._named_tempfile('bz2'): bz2.decompress,\n-            self._named_tempfile('lzma'): lzma.decompress,\n-            self._named_tempfile('gzip'): gzip.decompress,\n+            self._named_tempfile(\"bz2\"): bz2.decompress,\n+            self._named_tempfile(\"lzma\"): lzma.decompress,\n+            self._named_tempfile(\"gzip\"): gzip.decompress,\n         }\n \n         settings = {\n-            'FEEDS': {\n-                self._named_tempfile('bz2'): {\n-                    'format': 'csv',\n-                    'postprocessing': [self.MyPlugin1, 'scrapy.extensions.postprocessing.Bz2Plugin'],\n-                    'plugin1_char': b'\\n',\n+            \"FEEDS\": {\n+                self._named_tempfile(\"bz2\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\n+                        self.MyPlugin1,\n+                        \"scrapy.extensions.postprocessing.Bz2Plugin\",\n+                    ],\n+                    \"plugin1_char\": b\"\\n\",\n                 },\n-                self._named_tempfile('lzma'): {\n-                    'format': 'csv',\n-                    'postprocessing': [self.MyPlugin1, 'scrapy.extensions.postprocessing.LZMAPlugin'],\n-                    'plugin1_char': b'\\n',\n+                self._named_tempfile(\"lzma\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\n+                        self.MyPlugin1,\n+                        \"scrapy.extensions.postprocessing.LZMAPlugin\",\n+                    ],\n+                    \"plugin1_char\": b\"\\n\",\n                 },\n-                self._named_tempfile('gzip'): {\n-                    'format': 'csv',\n-                    'postprocessing': [self.MyPlugin1, 'scrapy.extensions.postprocessing.GzipPlugin'],\n-                    'plugin1_char': b'\\n',\n+                self._named_tempfile(\"gzip\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [\n+                        self.MyPlugin1,\n+                        \"scrapy.extensions.postprocessing.GzipPlugin\",\n+                    ],\n+                    \"plugin1_char\": b\"\\n\",\n                 },\n             },\n         }\n@@ -1956,40 +2005,41 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n     def test_exports_compatibility_with_postproc(self):\n         import marshal\n         import pickle\n+\n         filename_to_expected = {\n-            self._named_tempfile('csv'): b'foo\\r\\nbar\\r\\n',\n-            self._named_tempfile('json'): b'[\\n{\"foo\": \"bar\"}\\n]',\n-            self._named_tempfile('jsonlines'): b'{\"foo\": \"bar\"}\\n',\n-            self._named_tempfile('xml'): b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n-                                         b'<items>\\n<item><foo>bar</foo></item>\\n</items>',\n+            self._named_tempfile(\"csv\"): b\"foo\\r\\nbar\\r\\n\",\n+            self._named_tempfile(\"json\"): b'[\\n{\"foo\": \"bar\"}\\n]',\n+            self._named_tempfile(\"jsonlines\"): b'{\"foo\": \"bar\"}\\n',\n+            self._named_tempfile(\"xml\"): b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n+            b\"<items>\\n<item><foo>bar</foo></item>\\n</items>\",\n         }\n \n         settings = {\n-            'FEEDS': {\n-                self._named_tempfile('csv'): {\n-                    'format': 'csv',\n-                    'postprocessing': [self.MyPlugin1],\n+            \"FEEDS\": {\n+                self._named_tempfile(\"csv\"): {\n+                    \"format\": \"csv\",\n+                    \"postprocessing\": [self.MyPlugin1],\n                     # empty plugin to activate postprocessing.PostProcessingManager\n                 },\n-                self._named_tempfile('json'): {\n-                    'format': 'json',\n-                    'postprocessing': [self.MyPlugin1],\n+                self._named_tempfile(\"json\"): {\n+                    \"format\": \"json\",\n+                    \"postprocessing\": [self.MyPlugin1],\n                 },\n-                self._named_tempfile('jsonlines'): {\n-                    'format': 'jsonlines',\n-                    'postprocessing': [self.MyPlugin1],\n+                self._named_tempfile(\"jsonlines\"): {\n+                    \"format\": \"jsonlines\",\n+                    \"postprocessing\": [self.MyPlugin1],\n                 },\n-                self._named_tempfile('xml'): {\n-                    'format': 'xml',\n-                    'postprocessing': [self.MyPlugin1],\n+                self._named_tempfile(\"xml\"): {\n+                    \"format\": \"xml\",\n+                    \"postprocessing\": [self.MyPlugin1],\n                 },\n-                self._named_tempfile('marshal'): {\n-                    'format': 'marshal',\n-                    'postprocessing': [self.MyPlugin1],\n+                self._named_tempfile(\"marshal\"): {\n+                    \"format\": \"marshal\",\n+                    \"postprocessing\": [self.MyPlugin1],\n                 },\n-                self._named_tempfile('pickle'): {\n-                    'format': 'pickle',\n-                    'postprocessing': [self.MyPlugin1],\n+                self._named_tempfile(\"pickle\"): {\n+                    \"format\": \"pickle\",\n+                    \"postprocessing\": [self.MyPlugin1],\n                 },\n             },\n         }\n@@ -1997,9 +2047,9 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n         data = yield self.exported_data(self.items, settings)\n \n         for filename, result in data.items():\n-            if 'pickle' in filename:\n+            if \"pickle\" in filename:\n                 expected, result = self.items[0], pickle.loads(result)\n-            elif 'marshal' in filename:\n+            elif \"marshal\" in filename:\n                 expected, result = self.items[0], marshal.loads(result)\n             else:\n                 expected = filename_to_expected[filename]\n@@ -2008,28 +2058,27 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n \n class BatchDeliveriesTest(FeedExportTestBase):\n     __test__ = True\n-    _file_mark = '_%(batch_time)s_#%(batch_id)02d_'\n+    _file_mark = \"_%(batch_time)s_#%(batch_id)02d_\"\n \n     @defer.inlineCallbacks\n     def run_and_export(self, spider_cls, settings):\n         \"\"\"Run spider with specified settings; return exported data.\"\"\"\n \n-        FEEDS = settings.get('FEEDS') or {}\n-        settings['FEEDS'] = {\n-            build_url(file_path): feed\n-            for file_path, feed in FEEDS.items()\n+        FEEDS = settings.get(\"FEEDS\") or {}\n+        settings[\"FEEDS\"] = {\n+            build_url(file_path): feed for file_path, feed in FEEDS.items()\n         }\n         content = defaultdict(list)\n         try:\n             with MockServer() as s:\n-                spider_cls.start_urls = [s.url('/')]\n+                spider_cls.start_urls = [s.url(\"/\")]\n                 crawler = get_crawler(spider_cls, settings)\n                 yield crawler.crawl()\n \n             for path, feed in FEEDS.items():\n                 dir_name = Path(path).parent\n                 for file in sorted(dir_name.iterdir()):\n-                    content[feed['format']].append(file.read_bytes())\n+                    content[feed[\"format\"]].append(file.read_bytes())\n         finally:\n             self.tearDown()\n         defer.returnValue(content)\n@@ -2037,30 +2086,40 @@ class BatchDeliveriesTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def assertExportedJsonLines(self, items, rows, settings=None):\n         settings = settings or {}\n-        settings.update({\n-            'FEEDS': {\n-                self._random_temp_filename() / 'jl' / self._file_mark: {'format': 'jl'},\n+        settings.update(\n+            {\n+                \"FEEDS\": {\n+                    self._random_temp_filename()\n+                    / \"jl\"\n+                    / self._file_mark: {\"format\": \"jl\"},\n                 },\n-        })\n-        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n+            }\n+        )\n+        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n-        for batch in data['jl']:\n-            got_batch = [json.loads(to_unicode(batch_item)) for batch_item in batch.splitlines()]\n+        for batch in data[\"jl\"]:\n+            got_batch = [\n+                json.loads(to_unicode(batch_item)) for batch_item in batch.splitlines()\n+            ]\n             expected_batch, rows = rows[:batch_size], rows[batch_size:]\n             self.assertEqual(expected_batch, got_batch)\n \n     @defer.inlineCallbacks\n     def assertExportedCsv(self, items, header, rows, settings=None):\n         settings = settings or {}\n-        settings.update({\n-            'FEEDS': {\n-                self._random_temp_filename() / 'csv' / self._file_mark: {'format': 'csv'},\n+        settings.update(\n+            {\n+                \"FEEDS\": {\n+                    self._random_temp_filename()\n+                    / \"csv\"\n+                    / self._file_mark: {\"format\": \"csv\"},\n                 },\n-        })\n-        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n+            }\n+        )\n+        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n         data = yield self.exported_data(items, settings)\n-        for batch in data['csv']:\n+        for batch in data[\"csv\"]:\n             got_batch = csv.DictReader(to_unicode(batch).splitlines())\n             self.assertEqual(list(header), got_batch.fieldnames)\n             expected_batch, rows = rows[:batch_size], rows[batch_size:]\n@@ -2069,59 +2128,74 @@ class BatchDeliveriesTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def assertExportedXml(self, items, rows, settings=None):\n         settings = settings or {}\n-        settings.update({\n-            'FEEDS': {\n-                self._random_temp_filename() / 'xml' / self._file_mark: {'format': 'xml'},\n+        settings.update(\n+            {\n+                \"FEEDS\": {\n+                    self._random_temp_filename()\n+                    / \"xml\"\n+                    / self._file_mark: {\"format\": \"xml\"},\n                 },\n-        })\n-        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n+            }\n+        )\n+        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n-        for batch in data['xml']:\n+        for batch in data[\"xml\"]:\n             root = lxml.etree.fromstring(batch)\n-            got_batch = [{e.tag: e.text for e in it} for it in root.findall('item')]\n+            got_batch = [{e.tag: e.text for e in it} for it in root.findall(\"item\")]\n             expected_batch, rows = rows[:batch_size], rows[batch_size:]\n             self.assertEqual(expected_batch, got_batch)\n \n     @defer.inlineCallbacks\n     def assertExportedMultiple(self, items, rows, settings=None):\n         settings = settings or {}\n-        settings.update({\n-            'FEEDS': {\n-                self._random_temp_filename() / 'xml' / self._file_mark: {'format': 'xml'},\n-                self._random_temp_filename() / 'json' / self._file_mark: {'format': 'json'},\n+        settings.update(\n+            {\n+                \"FEEDS\": {\n+                    self._random_temp_filename()\n+                    / \"xml\"\n+                    / self._file_mark: {\"format\": \"xml\"},\n+                    self._random_temp_filename()\n+                    / \"json\"\n+                    / self._file_mark: {\"format\": \"json\"},\n                 },\n-        })\n-        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n+            }\n+        )\n+        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         # XML\n         xml_rows = rows.copy()\n-        for batch in data['xml']:\n+        for batch in data[\"xml\"]:\n             root = lxml.etree.fromstring(batch)\n-            got_batch = [{e.tag: e.text for e in it} for it in root.findall('item')]\n+            got_batch = [{e.tag: e.text for e in it} for it in root.findall(\"item\")]\n             expected_batch, xml_rows = xml_rows[:batch_size], xml_rows[batch_size:]\n             self.assertEqual(expected_batch, got_batch)\n         # JSON\n         json_rows = rows.copy()\n-        for batch in data['json']:\n-            got_batch = json.loads(batch.decode('utf-8'))\n+        for batch in data[\"json\"]:\n+            got_batch = json.loads(batch.decode(\"utf-8\"))\n             expected_batch, json_rows = json_rows[:batch_size], json_rows[batch_size:]\n             self.assertEqual(expected_batch, got_batch)\n \n     @defer.inlineCallbacks\n     def assertExportedPickle(self, items, rows, settings=None):\n         settings = settings or {}\n-        settings.update({\n-            'FEEDS': {\n-                self._random_temp_filename() / 'pickle' / self._file_mark: {'format': 'pickle'},\n+        settings.update(\n+            {\n+                \"FEEDS\": {\n+                    self._random_temp_filename()\n+                    / \"pickle\"\n+                    / self._file_mark: {\"format\": \"pickle\"},\n                 },\n-        })\n-        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n+            }\n+        )\n+        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         import pickle\n-        for batch in data['pickle']:\n+\n+        for batch in data[\"pickle\"]:\n             got_batch = self._load_until_eof(batch, load_func=pickle.load)\n             expected_batch, rows = rows[:batch_size], rows[batch_size:]\n             self.assertEqual(expected_batch, got_batch)\n@@ -2129,16 +2203,21 @@ class BatchDeliveriesTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def assertExportedMarshal(self, items, rows, settings=None):\n         settings = settings or {}\n-        settings.update({\n-            'FEEDS': {\n-                self._random_temp_filename() / 'marshal' / self._file_mark: {'format': 'marshal'},\n+        settings.update(\n+            {\n+                \"FEEDS\": {\n+                    self._random_temp_filename()\n+                    / \"marshal\"\n+                    / self._file_mark: {\"format\": \"marshal\"},\n                 },\n-        })\n-        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n+            }\n+        )\n+        batch_size = Settings(settings).getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\")\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         import marshal\n-        for batch in data['marshal']:\n+\n+        for batch in data[\"marshal\"]:\n             got_batch = self._load_until_eof(batch, load_func=marshal.load)\n             expected_batch, rows = rows[:batch_size], rows[batch_size:]\n             self.assertEqual(expected_batch, got_batch)\n@@ -2147,62 +2226,64 @@ class BatchDeliveriesTest(FeedExportTestBase):\n     def test_export_items(self):\n         \"\"\"Test partial deliveries in all supported formats\"\"\"\n         items = [\n-            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n-            self.MyItem({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),\n-            self.MyItem({'foo': 'bar3', 'baz': 'quux3'}),\n+            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n+            self.MyItem({\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"}),\n+            self.MyItem({\"foo\": \"bar3\", \"baz\": \"quux3\"}),\n         ]\n         rows = [\n-            {'egg': 'spam1', 'foo': 'bar1', 'baz': ''},\n-            {'egg': 'spam2', 'foo': 'bar2', 'baz': 'quux2'},\n-            {'foo': 'bar3', 'baz': 'quux3', 'egg': ''}\n+            {\"egg\": \"spam1\", \"foo\": \"bar1\", \"baz\": \"\"},\n+            {\"egg\": \"spam2\", \"foo\": \"bar2\", \"baz\": \"quux2\"},\n+            {\"foo\": \"bar3\", \"baz\": \"quux3\", \"egg\": \"\"},\n         ]\n-        settings = {\n-            'FEED_EXPORT_BATCH_ITEM_COUNT': 2\n-        }\n+        settings = {\"FEED_EXPORT_BATCH_ITEM_COUNT\": 2}\n         header = self.MyItem.fields.keys()\n         yield self.assertExported(items, header, rows, settings=settings)\n \n     def test_wrong_path(self):\n         \"\"\"If path is without %(batch_time)s and %(batch_id) an exception must be raised\"\"\"\n         settings = {\n-            'FEEDS': {\n-                self._random_temp_filename(): {'format': 'xml'},\n+            \"FEEDS\": {\n+                self._random_temp_filename(): {\"format\": \"xml\"},\n             },\n-            'FEED_EXPORT_BATCH_ITEM_COUNT': 1\n+            \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n         }\n         crawler = get_crawler(settings_dict=settings)\n         self.assertRaises(NotConfigured, FeedExporter, crawler)\n \n     @defer.inlineCallbacks\n     def test_export_no_items_not_store_empty(self):\n-        for fmt in ('json', 'jsonlines', 'xml', 'csv'):\n+        for fmt in (\"json\", \"jsonlines\", \"xml\", \"csv\"):\n             settings = {\n-                'FEEDS': {\n-                    self._random_temp_filename() / fmt / self._file_mark: {'format': fmt},\n+                \"FEEDS\": {\n+                    self._random_temp_filename()\n+                    / fmt\n+                    / self._file_mark: {\"format\": fmt},\n                 },\n-                'FEED_EXPORT_BATCH_ITEM_COUNT': 1\n+                \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n             }\n             data = yield self.exported_no_data(settings)\n             data = dict(data)\n-            self.assertEqual(b'', data[fmt][0])\n+            self.assertEqual(b\"\", data[fmt][0])\n \n     @defer.inlineCallbacks\n     def test_export_no_items_store_empty(self):\n         formats = (\n-            ('json', b'[]'),\n-            ('jsonlines', b''),\n-            ('xml', b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items></items>'),\n-            ('csv', b''),\n+            (\"json\", b\"[]\"),\n+            (\"jsonlines\", b\"\"),\n+            (\"xml\", b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items></items>'),\n+            (\"csv\", b\"\"),\n         )\n \n         for fmt, expctd in formats:\n             settings = {\n-                'FEEDS': {\n-                    self._random_temp_filename() / fmt / self._file_mark: {'format': fmt},\n+                \"FEEDS\": {\n+                    self._random_temp_filename()\n+                    / fmt\n+                    / self._file_mark: {\"format\": fmt},\n                 },\n-                'FEED_STORE_EMPTY': True,\n-                'FEED_EXPORT_INDENT': None,\n-                'FEED_EXPORT_BATCH_ITEM_COUNT': 1,\n+                \"FEED_STORE_EMPTY\": True,\n+                \"FEED_EXPORT_INDENT\": None,\n+                \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n             }\n             data = yield self.exported_no_data(settings)\n             data = dict(data)\n@@ -2210,47 +2291,60 @@ class BatchDeliveriesTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_export_multiple_configs(self):\n-        items = [dict({'foo': 'FOO', 'bar': 'BAR'}), dict({'foo': 'FOO1', 'bar': 'BAR1'})]\n+        items = [\n+            dict({\"foo\": \"FOO\", \"bar\": \"BAR\"}),\n+            dict({\"foo\": \"FOO1\", \"bar\": \"BAR1\"}),\n+        ]\n \n         formats = {\n-            'json': ['[\\n{\"bar\": \"BAR\"}\\n]'.encode('utf-8'),\n-                     '[\\n{\"bar\": \"BAR1\"}\\n]'.encode('utf-8')],\n-            'xml': [\n-                (\n-                    '<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n-                    '<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>'\n-                ).encode('latin-1'),\n-                (\n-                    '<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n-                    '<items>\\n  <item>\\n    <foo>FOO1</foo>\\n  </item>\\n</items>'\n-                ).encode('latin-1')\n+            \"json\": [\n+                '[\\n{\"bar\": \"BAR\"}\\n]'.encode(\"utf-8\"),\n+                '[\\n{\"bar\": \"BAR1\"}\\n]'.encode(\"utf-8\"),\n+            ],\n+            \"xml\": [\n+                (\n+                    '<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n+                    \"<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>\"\n+                ).encode(\"latin-1\"),\n+                (\n+                    '<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n+                    \"<items>\\n  <item>\\n    <foo>FOO1</foo>\\n  </item>\\n</items>\"\n+                ).encode(\"latin-1\"),\n+            ],\n+            \"csv\": [\n+                \"foo,bar\\r\\nFOO,BAR\\r\\n\".encode(\"utf-8\"),\n+                \"foo,bar\\r\\nFOO1,BAR1\\r\\n\".encode(\"utf-8\"),\n             ],\n-            'csv': ['foo,bar\\r\\nFOO,BAR\\r\\n'.encode('utf-8'),\n-                    'foo,bar\\r\\nFOO1,BAR1\\r\\n'.encode('utf-8')],\n         }\n \n         settings = {\n-            'FEEDS': {\n-                self._random_temp_filename() / 'json' / self._file_mark: {\n-                    'format': 'json',\n-                    'indent': 0,\n-                    'fields': ['bar'],\n-                    'encoding': 'utf-8',\n+            \"FEEDS\": {\n+                self._random_temp_filename()\n+                / \"json\"\n+                / self._file_mark: {\n+                    \"format\": \"json\",\n+                    \"indent\": 0,\n+                    \"fields\": [\"bar\"],\n+                    \"encoding\": \"utf-8\",\n                 },\n-                self._random_temp_filename() / 'xml' / self._file_mark: {\n-                    'format': 'xml',\n-                    'indent': 2,\n-                    'fields': ['foo'],\n-                    'encoding': 'latin-1',\n+                self._random_temp_filename()\n+                / \"xml\"\n+                / self._file_mark: {\n+                    \"format\": \"xml\",\n+                    \"indent\": 2,\n+                    \"fields\": [\"foo\"],\n+                    \"encoding\": \"latin-1\",\n                 },\n-                self._random_temp_filename() / 'csv' / self._file_mark: {\n-                    'format': 'csv',\n-                    'indent': None,\n-                    'fields': ['foo', 'bar'],\n-                    'encoding': 'utf-8',\n+                self._random_temp_filename()\n+                / \"csv\"\n+                / self._file_mark: {\n+                    \"format\": \"csv\",\n+                    \"indent\": None,\n+                    \"fields\": [\"foo\", \"bar\"],\n+                    \"encoding\": \"utf-8\",\n                 },\n             },\n-            'FEED_EXPORT_BATCH_ITEM_COUNT': 1,\n+            \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n         }\n         data = yield self.exported_data(items, settings)\n         for fmt, expected in formats.items():\n@@ -2259,18 +2353,22 @@ class BatchDeliveriesTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_batch_item_count_feeds_setting(self):\n-        items = [dict({'foo': 'FOO'}), dict({'foo': 'FOO1'})]\n+        items = [dict({\"foo\": \"FOO\"}), dict({\"foo\": \"FOO1\"})]\n         formats = {\n-            'json': ['[{\"foo\": \"FOO\"}]'.encode('utf-8'),\n-                     '[{\"foo\": \"FOO1\"}]'.encode('utf-8')],\n+            \"json\": [\n+                '[{\"foo\": \"FOO\"}]'.encode(\"utf-8\"),\n+                '[{\"foo\": \"FOO1\"}]'.encode(\"utf-8\"),\n+            ],\n         }\n         settings = {\n-            'FEEDS': {\n-                self._random_temp_filename() / 'json' / self._file_mark: {\n-                    'format': 'json',\n-                    'indent': None,\n-                    'encoding': 'utf-8',\n-                    'batch_item_count': 1,\n+            \"FEEDS\": {\n+                self._random_temp_filename()\n+                / \"json\"\n+                / self._file_mark: {\n+                    \"format\": \"json\",\n+                    \"indent\": None,\n+                    \"encoding\": \"utf-8\",\n+                    \"batch_item_count\": 1,\n                 },\n             },\n         }\n@@ -2279,7 +2377,9 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             for expected_batch, got_batch in zip(expected, data[fmt]):\n                 self.assertEqual(expected_batch, got_batch)\n \n-    @pytest.mark.skipif(sys.platform == 'win32', reason='Odd behaviour on file creation/output')\n+    @pytest.mark.skipif(\n+        sys.platform == \"win32\", reason=\"Odd behaviour on file creation/output\"\n+    )\n     @defer.inlineCallbacks\n     def test_batch_path_differ(self):\n         \"\"\"\n@@ -2287,26 +2387,29 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         So %(batch_time)s replaced with the current date.\n         \"\"\"\n         items = [\n-            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n-            self.MyItem({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),\n-            self.MyItem({'foo': 'bar3', 'baz': 'quux3'}),\n+            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n+            self.MyItem({\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"}),\n+            self.MyItem({\"foo\": \"bar3\", \"baz\": \"quux3\"}),\n         ]\n         settings = {\n-            'FEEDS': {\n-                self._random_temp_filename() / '%(batch_time)s': {\n-                    'format': 'json',\n+            \"FEEDS\": {\n+                self._random_temp_filename()\n+                / \"%(batch_time)s\": {\n+                    \"format\": \"json\",\n                 },\n             },\n-            'FEED_EXPORT_BATCH_ITEM_COUNT': 1,\n+            \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n         }\n         data = yield self.exported_data(items, settings)\n-        self.assertEqual(len(items), len([_ for _ in data['json'] if _]))\n+        self.assertEqual(len(items), len([_ for _ in data[\"json\"] if _]))\n \n     @defer.inlineCallbacks\n     def test_stats_batch_file_success(self):\n         settings = {\n             \"FEEDS\": {\n-                build_url(str(self._random_temp_filename() / \"json\" / self._file_mark)): {\n+                build_url(\n+                    str(self._random_temp_filename() / \"json\" / self._file_mark)\n+                ): {\n                     \"format\": \"json\",\n                 }\n             },\n@@ -2315,18 +2418,22 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         crawler = get_crawler(ItemSpider, settings)\n         with MockServer() as mockserver:\n             yield crawler.crawl(total=2, mockserver=mockserver)\n-        self.assertIn(\"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats())\n-        self.assertEqual(crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\"), 12)\n+        self.assertIn(\n+            \"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats()\n+        )\n+        self.assertEqual(\n+            crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\"), 12\n+        )\n \n     @defer.inlineCallbacks\n     def test_s3_export(self):\n         skip_if_no_boto()\n \n-        bucket = 'mybucket'\n+        bucket = \"mybucket\"\n         items = [\n-            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n-            self.MyItem({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),\n-            self.MyItem({'foo': 'bar3', 'baz': 'quux3'}),\n+            self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n+            self.MyItem({\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"}),\n+            self.MyItem({\"foo\": \"bar3\", \"baz\": \"quux3\"}),\n         ]\n \n         class CustomS3FeedStorage(S3FeedStorage):\n@@ -2335,33 +2442,34 @@ class BatchDeliveriesTest(FeedExportTestBase):\n \n             def open(self, *args, **kwargs):\n                 from botocore.stub import ANY, Stubber\n+\n                 stub = Stubber(self.s3_client)\n                 stub.activate()\n                 CustomS3FeedStorage.stubs.append(stub)\n                 stub.add_response(\n-                    'put_object',\n+                    \"put_object\",\n                     expected_params={\n-                        'Body': ANY,\n-                        'Bucket': bucket,\n-                        'Key': ANY,\n+                        \"Body\": ANY,\n+                        \"Bucket\": bucket,\n+                        \"Key\": ANY,\n                     },\n                     service_response={},\n                 )\n                 return super().open(*args, **kwargs)\n \n-        key = 'export.csv'\n-        uri = f's3://{bucket}/{key}/%(batch_time)s.json'\n+        key = \"export.csv\"\n+        uri = f\"s3://{bucket}/{key}/%(batch_time)s.json\"\n         batch_item_count = 1\n         settings = {\n-            'AWS_ACCESS_KEY_ID': 'access_key',\n-            'AWS_SECRET_ACCESS_KEY': 'secret_key',\n-            'FEED_EXPORT_BATCH_ITEM_COUNT': batch_item_count,\n-            'FEED_STORAGES': {\n-                's3': CustomS3FeedStorage,\n+            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n+            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n+            \"FEED_EXPORT_BATCH_ITEM_COUNT\": batch_item_count,\n+            \"FEED_STORAGES\": {\n+                \"s3\": CustomS3FeedStorage,\n             },\n-            'FEEDS': {\n+            \"FEEDS\": {\n                 uri: {\n-                    'format': 'json',\n+                    \"format\": \"json\",\n                 },\n             },\n         }\n@@ -2370,14 +2478,14 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         verifyObject(IFeedStorage, storage)\n \n         class TestSpider(scrapy.Spider):\n-            name = 'testspider'\n+            name = \"testspider\"\n \n             def parse(self, response):\n                 for item in items:\n                     yield item\n \n         with MockServer() as server:\n-            TestSpider.start_urls = [server.url('/')]\n+            TestSpider.start_urls = [server.url(\"/\")]\n             crawler = get_crawler(TestSpider, settings)\n             yield crawler.crawl()\n \n@@ -2387,11 +2495,10 @@ class BatchDeliveriesTest(FeedExportTestBase):\n \n \n class FeedExportInitTest(unittest.TestCase):\n-\n     def test_unsupported_storage(self):\n         settings = {\n-            'FEEDS': {\n-                'unsupported://uri': {},\n+            \"FEEDS\": {\n+                \"unsupported://uri\": {},\n             },\n         }\n         crawler = get_crawler(settings_dict=settings)\n@@ -2400,9 +2507,9 @@ class FeedExportInitTest(unittest.TestCase):\n \n     def test_unsupported_format(self):\n         settings = {\n-            'FEEDS': {\n-                'file://path': {\n-                    'format': 'unsupported_format',\n+            \"FEEDS\": {\n+                \"file://path\": {\n+                    \"format\": \"unsupported_format\",\n                 },\n             },\n         }\n@@ -2412,7 +2519,6 @@ class FeedExportInitTest(unittest.TestCase):\n \n \n class StdoutFeedStorageWithoutFeedOptions(StdoutFeedStorage):\n-\n     def __init__(self, uri):\n         super().__init__(uri)\n \n@@ -2424,25 +2530,26 @@ class StdoutFeedStoragePreFeedOptionsTest(unittest.TestCase):\n \n     def test_init(self):\n         settings_dict = {\n-            'FEED_URI': 'file:///tmp/foobar',\n-            'FEED_STORAGES': {\n-                'file': StdoutFeedStorageWithoutFeedOptions\n-            },\n+            \"FEED_URI\": \"file:///tmp/foobar\",\n+            \"FEED_STORAGES\": {\"file\": StdoutFeedStorageWithoutFeedOptions},\n         }\n-        with pytest.warns(ScrapyDeprecationWarning,\n-                          match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\"):\n+        with pytest.warns(\n+            ScrapyDeprecationWarning,\n+            match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\",\n+        ):\n             crawler = get_crawler(settings_dict=settings_dict)\n             feed_exporter = FeedExporter.from_crawler(crawler)\n \n         spider = scrapy.Spider(\"default\")\n-        with pytest.warns(ScrapyDeprecationWarning,\n+        with pytest.warns(\n+            ScrapyDeprecationWarning,\n             match=\"StdoutFeedStorageWithoutFeedOptions does not support \"\n-                                \"the 'feed_options' keyword argument.\"):\n+            \"the 'feed_options' keyword argument.\",\n+        ):\n             feed_exporter.open_spider(spider)\n \n \n class FileFeedStorageWithoutFeedOptions(FileFeedStorage):\n-\n     def __init__(self, uri):\n         super().__init__(uri)\n \n@@ -2457,31 +2564,31 @@ class FileFeedStoragePreFeedOptionsTest(unittest.TestCase):\n     def test_init(self):\n         with tempfile.NamedTemporaryFile() as temp:\n             settings_dict = {\n-                'FEED_URI': f'file:///{temp.name}',\n-                'FEED_STORAGES': {\n-                    'file': FileFeedStorageWithoutFeedOptions\n-                },\n+                \"FEED_URI\": f\"file:///{temp.name}\",\n+                \"FEED_STORAGES\": {\"file\": FileFeedStorageWithoutFeedOptions},\n             }\n-            with pytest.warns(ScrapyDeprecationWarning,\n-                              match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\"):\n+            with pytest.warns(\n+                ScrapyDeprecationWarning,\n+                match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\",\n+            ):\n                 crawler = get_crawler(settings_dict=settings_dict)\n                 feed_exporter = FeedExporter.from_crawler(crawler)\n         spider = scrapy.Spider(\"default\")\n \n-        with pytest.warns(ScrapyDeprecationWarning,\n+        with pytest.warns(\n+            ScrapyDeprecationWarning,\n             match=\"FileFeedStorageWithoutFeedOptions does not support \"\n-                                \"the 'feed_options' keyword argument.\"):\n+            \"the 'feed_options' keyword argument.\",\n+        ):\n             feed_exporter.open_spider(spider)\n \n \n class S3FeedStorageWithoutFeedOptions(S3FeedStorage):\n-\n     def __init__(self, uri, access_key, secret_key, acl, endpoint_url, **kwargs):\n         super().__init__(uri, access_key, secret_key, acl, endpoint_url, **kwargs)\n \n \n class S3FeedStorageWithoutFeedOptionsWithFromCrawler(S3FeedStorage):\n-\n     @classmethod\n     def from_crawler(cls, crawler, uri):\n         return super().from_crawler(crawler, uri)\n@@ -2496,53 +2603,55 @@ class S3FeedStoragePreFeedOptionsTest(unittest.TestCase):\n \n     def test_init(self):\n         settings_dict = {\n-            'FEED_URI': 'file:///tmp/foobar',\n-            'FEED_STORAGES': {\n-                'file': S3FeedStorageWithoutFeedOptions\n-            },\n+            \"FEED_URI\": \"file:///tmp/foobar\",\n+            \"FEED_STORAGES\": {\"file\": S3FeedStorageWithoutFeedOptions},\n         }\n-        with pytest.warns(ScrapyDeprecationWarning,\n-                          match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\"):\n+        with pytest.warns(\n+            ScrapyDeprecationWarning,\n+            match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\",\n+        ):\n             crawler = get_crawler(settings_dict=settings_dict)\n             feed_exporter = FeedExporter.from_crawler(crawler)\n \n         spider = scrapy.Spider(\"default\")\n         spider.crawler = crawler\n \n-        with pytest.warns(ScrapyDeprecationWarning,\n+        with pytest.warns(\n+            ScrapyDeprecationWarning,\n             match=\"S3FeedStorageWithoutFeedOptions does not support \"\n@@ -9,7 +9,12 @@ from unittest import mock, skipIf\n from urllib.parse import urlencode\n \n from twisted.internet import reactor\n-from twisted.internet.defer import CancelledError, Deferred, DeferredList, inlineCallbacks\n+from twisted.internet.defer import (\n+    CancelledError,\n+    Deferred,\n+    DeferredList,\n+    inlineCallbacks,\n+)\n from twisted.internet.endpoints import SSL4ClientEndpoint, SSL4ServerEndpoint\n from twisted.internet.error import TimeoutError\n from twisted.internet.ssl import optionsForClientTLS, PrivateCertificate, Certificate\n@@ -27,22 +32,19 @@ from tests.mockserver import ssl_context_factory, LeafResource, Status\n \n \n def generate_random_string(size):\n-    return ''.join(random.choices(\n-        string.ascii_uppercase + string.digits,\n-        k=size\n-    ))\n+    return \"\".join(random.choices(string.ascii_uppercase + string.digits, k=size))\n \n \n def make_html_body(val):\n-    response = f'''<html>\n+    response = f\"\"\"<html>\n <h1>Hello from HTTP2<h1>\n <p>{val}</p>\n-</html>'''\n-    return bytes(response, 'utf-8')\n+</html>\"\"\"\n+    return bytes(response, \"utf-8\")\n \n \n class DummySpider(Spider):\n-    name = 'dummy'\n+    name = \"dummy\"\n     start_urls: list = []\n \n     def parse(self, response):\n@@ -62,22 +64,22 @@ class Data:\n     HTML_SMALL = make_html_body(STR_SMALL)\n     HTML_LARGE = make_html_body(STR_LARGE)\n \n-    JSON_SMALL = {'data': STR_SMALL}\n-    JSON_LARGE = {'data': STR_LARGE}\n+    JSON_SMALL = {\"data\": STR_SMALL}\n+    JSON_LARGE = {\"data\": STR_LARGE}\n \n-    DATALOSS = b'Dataloss Content'\n-    NO_CONTENT_LENGTH = b'This response do not have any content-length header'\n+    DATALOSS = b\"Dataloss Content\"\n+    NO_CONTENT_LENGTH = b\"This response do not have any content-length header\"\n \n \n class GetDataHtmlSmall(LeafResource):\n     def render_GET(self, request: TxRequest):\n-        request.setHeader('Content-Type', 'text/html; charset=UTF-8')\n+        request.setHeader(\"Content-Type\", \"text/html; charset=UTF-8\")\n         return Data.HTML_SMALL\n \n \n class GetDataHtmlLarge(LeafResource):\n     def render_GET(self, request: TxRequest):\n-        request.setHeader('Content-Type', 'text/html; charset=UTF-8')\n+        request.setHeader(\"Content-Type\", \"text/html; charset=UTF-8\")\n         return Data.HTML_LARGE\n \n \n@@ -85,16 +87,16 @@ class PostDataJsonMixin:\n     @staticmethod\n     def make_response(request: TxRequest, extra_data: str):\n         response = {\n-            'request-headers': {},\n-            'request-body': json.loads(request.content.read()),\n-            'extra-data': extra_data\n+            \"request-headers\": {},\n+            \"request-body\": json.loads(request.content.read()),\n+            \"extra-data\": extra_data,\n         }\n         for k, v in request.requestHeaders.getAllRawHeaders():\n-            response['request-headers'][str(k, 'utf-8')] = str(v[0], 'utf-8')\n+            response[\"request-headers\"][str(k, \"utf-8\")] = str(v[0], \"utf-8\")\n \n-        response_bytes = bytes(json.dumps(response), 'utf-8')\n-        request.setHeader('Content-Type', 'application/json; charset=UTF-8')\n-        request.setHeader('Content-Encoding', 'UTF-8')\n+        response_bytes = bytes(json.dumps(response), \"utf-8\")\n+        request.setHeader(\"Content-Type\", \"application/json; charset=UTF-8\")\n+        request.setHeader(\"Content-Encoding\", \"UTF-8\")\n         return response_bytes\n \n \n@@ -109,7 +111,6 @@ class PostDataJsonLarge(LeafResource, PostDataJsonMixin):\n \n \n class Dataloss(LeafResource):\n-\n     def render_GET(self, request: TxRequest):\n         request.setHeader(b\"Content-Length\", b\"1024\")\n         self.deferRequest(request, 0, self._delayed_render, request)\n@@ -123,7 +124,7 @@ class Dataloss(LeafResource):\n \n class NoContentLengthHeader(LeafResource):\n     def render_GET(self, request: TxRequest):\n-        request.requestHeaders.removeHeader('Content-Length')\n+        request.requestHeaders.removeHeader(\"Content-Length\")\n         self.deferRequest(request, 0, self._delayed_render, request)\n         return NOT_DONE_YET\n \n@@ -140,57 +141,61 @@ class TimeoutResponse(LeafResource):\n \n class QueryParams(LeafResource):\n     def render_GET(self, request: TxRequest):\n-        request.setHeader('Content-Type', 'application/json; charset=UTF-8')\n-        request.setHeader('Content-Encoding', 'UTF-8')\n+        request.setHeader(\"Content-Type\", \"application/json; charset=UTF-8\")\n+        request.setHeader(\"Content-Encoding\", \"UTF-8\")\n \n         query_params = {}\n         for k, v in request.args.items():\n-            query_params[str(k, 'utf-8')] = str(v[0], 'utf-8')\n+            query_params[str(k, \"utf-8\")] = str(v[0], \"utf-8\")\n \n-        return bytes(json.dumps(query_params), 'utf-8')\n+        return bytes(json.dumps(query_params), \"utf-8\")\n \n \n class RequestHeaders(LeafResource):\n     \"\"\"Sends all the headers received as a response\"\"\"\n \n     def render_GET(self, request: TxRequest):\n-        request.setHeader('Content-Type', 'application/json; charset=UTF-8')\n-        request.setHeader('Content-Encoding', 'UTF-8')\n+        request.setHeader(\"Content-Type\", \"application/json; charset=UTF-8\")\n+        request.setHeader(\"Content-Encoding\", \"UTF-8\")\n         headers = {}\n         for k, v in request.requestHeaders.getAllRawHeaders():\n-            headers[str(k, 'utf-8')] = str(v[0], 'utf-8')\n+            headers[str(k, \"utf-8\")] = str(v[0], \"utf-8\")\n \n-        return bytes(json.dumps(headers), 'utf-8')\n+        return bytes(json.dumps(headers), \"utf-8\")\n \n \n-def get_client_certificate(key_file: Path, certificate_file: Path) -> PrivateCertificate:\n-    pem = key_file.read_text(encoding=\"utf-8\") + certificate_file.read_text(encoding=\"utf-8\")\n+def get_client_certificate(\n+    key_file: Path, certificate_file: Path\n+) -> PrivateCertificate:\n+    pem = key_file.read_text(encoding=\"utf-8\") + certificate_file.read_text(\n+        encoding=\"utf-8\"\n+    )\n \n     return PrivateCertificate.loadPEM(pem)\n \n \n @skipIf(not H2_ENABLED, \"HTTP/2 support in Twisted is not enabled\")\n class Https2ClientProtocolTestCase(TestCase):\n-    scheme = 'https'\n-    key_file = Path(__file__).parent / 'keys' / 'localhost.key'\n-    certificate_file = Path(__file__).parent / 'keys' / 'localhost.crt'\n+    scheme = \"https\"\n+    key_file = Path(__file__).parent / \"keys\" / \"localhost.key\"\n+    certificate_file = Path(__file__).parent / \"keys\" / \"localhost.crt\"\n \n     def _init_resource(self):\n         self.temp_directory = self.mktemp()\n         Path(self.temp_directory).mkdir()\n         r = File(self.temp_directory)\n-        r.putChild(b'get-data-html-small', GetDataHtmlSmall())\n-        r.putChild(b'get-data-html-large', GetDataHtmlLarge())\n+        r.putChild(b\"get-data-html-small\", GetDataHtmlSmall())\n+        r.putChild(b\"get-data-html-large\", GetDataHtmlLarge())\n \n-        r.putChild(b'post-data-json-small', PostDataJsonSmall())\n-        r.putChild(b'post-data-json-large', PostDataJsonLarge())\n+        r.putChild(b\"post-data-json-small\", PostDataJsonSmall())\n+        r.putChild(b\"post-data-json-large\", PostDataJsonLarge())\n \n-        r.putChild(b'dataloss', Dataloss())\n-        r.putChild(b'no-content-length-header', NoContentLengthHeader())\n-        r.putChild(b'status', Status())\n-        r.putChild(b'query-params', QueryParams())\n-        r.putChild(b'timeout', TimeoutResponse())\n-        r.putChild(b'request-headers', RequestHeaders())\n+        r.putChild(b\"dataloss\", Dataloss())\n+        r.putChild(b\"no-content-length-header\", NoContentLengthHeader())\n+        r.putChild(b\"status\", Status())\n+        r.putChild(b\"query-params\", QueryParams())\n+        r.putChild(b\"timeout\", TimeoutResponse())\n+        r.putChild(b\"request-headers\", RequestHeaders())\n         return r\n \n     @inlineCallbacks\n@@ -200,26 +205,35 @@ class Https2ClientProtocolTestCase(TestCase):\n         self.site = Site(root, timeout=None)\n \n         # Start server for testing\n-        self.hostname = 'localhost'\n-        context_factory = ssl_context_factory(str(self.key_file), str(self.certificate_file))\n+        self.hostname = \"localhost\"\n+        context_factory = ssl_context_factory(\n+            str(self.key_file), str(self.certificate_file)\n+        )\n \n-        server_endpoint = SSL4ServerEndpoint(reactor, 0, context_factory, interface=self.hostname)\n+        server_endpoint = SSL4ServerEndpoint(\n+            reactor, 0, context_factory, interface=self.hostname\n+        )\n         self.server = yield server_endpoint.listen(self.site)\n         self.port_number = self.server.getHost().port\n \n         # Connect H2 client with server\n-        self.client_certificate = get_client_certificate(self.key_file, self.certificate_file)\n+        self.client_certificate = get_client_certificate(\n+            self.key_file, self.certificate_file\n+        )\n         client_options = optionsForClientTLS(\n             hostname=self.hostname,\n             trustRoot=self.client_certificate,\n-            acceptableProtocols=[b'h2']\n+            acceptableProtocols=[b\"h2\"],\n         )\n-        uri = URI.fromBytes(bytes(self.get_url('/'), 'utf-8'))\n+        uri = URI.fromBytes(bytes(self.get_url(\"/\"), \"utf-8\"))\n \n         self.conn_closed_deferred = Deferred()\n         from scrapy.core.http2.protocol import H2ClientFactory\n+\n         h2_client_factory = H2ClientFactory(uri, Settings(), self.conn_closed_deferred)\n-        client_endpoint = SSL4ClientEndpoint(reactor, self.hostname, self.port_number, client_options)\n+        client_endpoint = SSL4ClientEndpoint(\n+            reactor, self.hostname, self.port_number, client_options\n+        )\n         self.client = yield client_endpoint.connect(h2_client_factory)\n \n     @inlineCallbacks\n@@ -236,8 +250,8 @@ class Https2ClientProtocolTestCase(TestCase):\n         :param path: Should have / at the starting compulsorily if not empty\n         :return: Complete url\n         \"\"\"\n-        assert len(path) > 0 and (path[0] == '/' or path[0] == '&')\n-        return f'{self.scheme}://{self.hostname}:{self.port_number}{path}'\n+        assert len(path) > 0 and (path[0] == \"/\" or path[0] == \"&\")\n+        return f\"{self.scheme}://{self.hostname}:{self.port_number}{path}\"\n \n     def make_request(self, request: Request) -> Deferred:\n         return self.client.request(request, DummySpider())\n@@ -251,18 +265,13 @@ class Https2ClientProtocolTestCase(TestCase):\n \n         return DeferredList(d_list, fireOnOneErrback=True)\n \n-    def _check_GET(\n-        self,\n-        request: Request,\n-        expected_body,\n-        expected_status\n-    ):\n+    def _check_GET(self, request: Request, expected_body, expected_status):\n         def check_response(response: Response):\n             self.assertEqual(response.status, expected_status)\n             self.assertEqual(response.body, expected_body)\n             self.assertEqual(response.request, request)\n \n-            content_length = int(response.headers.get('Content-Length'))\n+            content_length = int(response.headers.get(\"Content-Length\"))\n             self.assertEqual(len(response.body), content_length)\n \n         d = self.make_request(request)\n@@ -271,11 +280,11 @@ class Https2ClientProtocolTestCase(TestCase):\n         return d\n \n     def test_GET_small_body(self):\n-        request = Request(self.get_url('/get-data-html-small'))\n+        request = Request(self.get_url(\"/get-data-html-small\"))\n         return self._check_GET(request, Data.HTML_SMALL, 200)\n \n     def test_GET_large_body(self):\n-        request = Request(self.get_url('/get-data-html-large'))\n+        request = Request(self.get_url(\"/get-data-html-large\"))\n         return self._check_GET(request, Data.HTML_LARGE, 200)\n \n     def _check_GET_x10(self, *args, **kwargs):\n@@ -286,16 +295,12 @@ class Https2ClientProtocolTestCase(TestCase):\n \n     def test_GET_small_body_x10(self):\n         return self._check_GET_x10(\n-            Request(self.get_url('/get-data-html-small')),\n-            Data.HTML_SMALL,\n-            200\n+            Request(self.get_url(\"/get-data-html-small\")), Data.HTML_SMALL, 200\n         )\n \n     def test_GET_large_body_x10(self):\n         return self._check_GET_x10(\n-            Request(self.get_url('/get-data-html-large')),\n-            Data.HTML_LARGE,\n-            200\n+            Request(self.get_url(\"/get-data-html-large\")), Data.HTML_LARGE, 200\n         )\n \n     def _check_POST_json(\n@@ -303,7 +308,7 @@ class Https2ClientProtocolTestCase(TestCase):\n         request: Request,\n         expected_request_body,\n         expected_extra_data,\n-        expected_status: int\n+        expected_status: int,\n     ):\n         d = self.make_request(request)\n \n@@ -311,50 +316,48 @@ class Https2ClientProtocolTestCase(TestCase):\n             self.assertEqual(response.status, expected_status)\n             self.assertEqual(response.request, request)\n \n-            content_length = int(response.headers.get('Content-Length'))\n+            content_length = int(response.headers.get(\"Content-Length\"))\n             self.assertEqual(len(response.body), content_length)\n \n             # Parse the body\n-            content_encoding = str(response.headers[b'Content-Encoding'], 'utf-8')\n+            content_encoding = str(response.headers[b\"Content-Encoding\"], \"utf-8\")\n             body = json.loads(str(response.body, content_encoding))\n-            self.assertIn('request-body', body)\n-            self.assertIn('extra-data', body)\n-            self.assertIn('request-headers', body)\n+            self.assertIn(\"request-body\", body)\n+            self.assertIn(\"extra-data\", body)\n+            self.assertIn(\"request-headers\", body)\n \n-            request_body = body['request-body']\n+            request_body = body[\"request-body\"]\n             self.assertEqual(request_body, expected_request_body)\n \n-            extra_data = body['extra-data']\n+            extra_data = body[\"extra-data\"]\n             self.assertEqual(extra_data, expected_extra_data)\n \n             # Check if headers were sent successfully\n-            request_headers = body['request-headers']\n+            request_headers = body[\"request-headers\"]\n             for k, v in request.headers.items():\n-                k_str = str(k, 'utf-8')\n+                k_str = str(k, \"utf-8\")\n                 self.assertIn(k_str, request_headers)\n-                self.assertEqual(request_headers[k_str], str(v[0], 'utf-8'))\n+                self.assertEqual(request_headers[k_str], str(v[0], \"utf-8\"))\n \n         d.addCallback(assert_response)\n         d.addErrback(self.fail)\n         return d\n \n     def test_POST_small_json(self):\n-        request = JsonRequest(url=self.get_url('/post-data-json-small'), method='POST', data=Data.JSON_SMALL)\n-        return self._check_POST_json(\n-            request,\n-            Data.JSON_SMALL,\n-            Data.EXTRA_SMALL,\n-            200\n+        request = JsonRequest(\n+            url=self.get_url(\"/post-data-json-small\"),\n+            method=\"POST\",\n+            data=Data.JSON_SMALL,\n         )\n+        return self._check_POST_json(request, Data.JSON_SMALL, Data.EXTRA_SMALL, 200)\n \n     def test_POST_large_json(self):\n-        request = JsonRequest(url=self.get_url('/post-data-json-large'), method='POST', data=Data.JSON_LARGE)\n-        return self._check_POST_json(\n-            request,\n-            Data.JSON_LARGE,\n-            Data.EXTRA_LARGE,\n-            200\n+        request = JsonRequest(\n+            url=self.get_url(\"/post-data-json-large\"),\n+            method=\"POST\",\n+            data=Data.JSON_LARGE,\n         )\n+        return self._check_POST_json(request, Data.JSON_LARGE, Data.EXTRA_LARGE, 200)\n \n     def _check_POST_json_x10(self, *args, **kwargs):\n         def get_deferred():\n@@ -363,32 +366,36 @@ class Https2ClientProtocolTestCase(TestCase):\n         return self._check_repeat(get_deferred, 10)\n \n     def test_POST_small_json_x10(self):\n-        request = JsonRequest(url=self.get_url('/post-data-json-small'), method='POST', data=Data.JSON_SMALL)\n+        request = JsonRequest(\n+            url=self.get_url(\"/post-data-json-small\"),\n+            method=\"POST\",\n+            data=Data.JSON_SMALL,\n+        )\n         return self._check_POST_json_x10(\n-            request,\n-            Data.JSON_SMALL,\n-            Data.EXTRA_SMALL,\n-            200\n+            request, Data.JSON_SMALL, Data.EXTRA_SMALL, 200\n         )\n \n     def test_POST_large_json_x10(self):\n-        request = JsonRequest(url=self.get_url('/post-data-json-large'), method='POST', data=Data.JSON_LARGE)\n+        request = JsonRequest(\n+            url=self.get_url(\"/post-data-json-large\"),\n+            method=\"POST\",\n+            data=Data.JSON_LARGE,\n+        )\n         return self._check_POST_json_x10(\n-            request,\n-            Data.JSON_LARGE,\n-            Data.EXTRA_LARGE,\n-            200\n+            request, Data.JSON_LARGE, Data.EXTRA_LARGE, 200\n         )\n \n     @inlineCallbacks\n     def test_invalid_negotiated_protocol(self):\n-        with mock.patch(\"scrapy.core.http2.protocol.PROTOCOL_NAME\", return_value=b\"not-h2\"):\n-            request = Request(url=self.get_url('/status?n=200'))\n+        with mock.patch(\n+            \"scrapy.core.http2.protocol.PROTOCOL_NAME\", return_value=b\"not-h2\"\n+        ):\n+            request = Request(url=self.get_url(\"/status?n=200\"))\n             with self.assertRaises(ResponseFailed):\n                 yield self.make_request(request)\n \n     def test_cancel_request(self):\n-        request = Request(url=self.get_url('/get-data-html-large'))\n+        request = Request(url=self.get_url(\"/get-data-html-large\"))\n \n         def assert_response(response: Response):\n             self.assertEqual(response.status, 499)\n@@ -402,13 +409,15 @@ class Https2ClientProtocolTestCase(TestCase):\n         return d\n \n     def test_download_maxsize_exceeded(self):\n-        request = Request(url=self.get_url('/get-data-html-large'), meta={'download_maxsize': 1000})\n+        request = Request(\n+            url=self.get_url(\"/get-data-html-large\"), meta={\"download_maxsize\": 1000}\n+        )\n \n         def assert_cancelled_error(failure):\n             self.assertIsInstance(failure.value, CancelledError)\n             error_pattern = re.compile(\n-                rf'Cancelling download of {request.url}: received response '\n-                rf'size \\(\\d*\\) larger than download max size \\(1000\\)'\n+                rf\"Cancelling download of {request.url}: received response \"\n+                rf\"size \\(\\d*\\) larger than download max size \\(1000\\)\"\n             )\n             self.assertEqual(len(re.findall(error_pattern, str(failure.value))), 1)\n \n@@ -420,15 +429,18 @@ class Https2ClientProtocolTestCase(TestCase):\n     def test_received_dataloss_response(self):\n         \"\"\"In case when value of Header Content-Length != len(Received Data)\n         ProtocolError is raised\"\"\"\n-        request = Request(url=self.get_url('/dataloss'))\n+        request = Request(url=self.get_url(\"/dataloss\"))\n \n         def assert_failure(failure: Failure):\n             self.assertTrue(len(failure.value.reasons) > 0)\n             from h2.exceptions import InvalidBodyLengthError\n-            self.assertTrue(any(\n+\n+            self.assertTrue(\n+                any(\n                     isinstance(error, InvalidBodyLengthError)\n                     for error in failure.value.reasons\n-            ))\n+                )\n+            )\n \n         d = self.make_request(request)\n         d.addCallback(self.fail)\n@@ -436,13 +448,13 @@ class Https2ClientProtocolTestCase(TestCase):\n         return d\n \n     def test_missing_content_length_header(self):\n-        request = Request(url=self.get_url('/no-content-length-header'))\n+        request = Request(url=self.get_url(\"/no-content-length-header\"))\n \n         def assert_content_length(response: Response):\n             self.assertEqual(response.status, 200)\n             self.assertEqual(response.body, Data.NO_CONTENT_LENGTH)\n             self.assertEqual(response.request, request)\n-            self.assertNotIn('Content-Length', response.headers)\n+            self.assertNotIn(\"Content-Length\", response.headers)\n \n         d = self.make_request(request)\n         d.addCallback(assert_content_length)\n@@ -450,40 +462,39 @@ class Https2ClientProtocolTestCase(TestCase):\n         return d\n \n     @inlineCallbacks\n-    def _check_log_warnsize(\n-        self,\n-        request,\n-        warn_pattern,\n-        expected_body\n-    ):\n-        with self.assertLogs('scrapy.core.http2.stream', level='WARNING') as cm:\n+    def _check_log_warnsize(self, request, warn_pattern, expected_body):\n+        with self.assertLogs(\"scrapy.core.http2.stream\", level=\"WARNING\") as cm:\n             response = yield self.make_request(request)\n             self.assertEqual(response.status, 200)\n             self.assertEqual(response.request, request)\n             self.assertEqual(response.body, expected_body)\n \n             # Check the warning is raised only once for this request\n-            self.assertEqual(sum(\n-                len(re.findall(warn_pattern, log))\n-                for log in cm.output\n-            ), 1)\n+            self.assertEqual(\n+                sum(len(re.findall(warn_pattern, log)) for log in cm.output), 1\n+            )\n \n     @inlineCallbacks\n     def test_log_expected_warnsize(self):\n-        request = Request(url=self.get_url('/get-data-html-large'), meta={'download_warnsize': 1000})\n+        request = Request(\n+            url=self.get_url(\"/get-data-html-large\"), meta={\"download_warnsize\": 1000}\n+        )\n         warn_pattern = re.compile(\n-            rf'Expected response size \\(\\d*\\) larger than '\n-            rf'download warn size \\(1000\\) in request {request}'\n+            rf\"Expected response size \\(\\d*\\) larger than \"\n+            rf\"download warn size \\(1000\\) in request {request}\"\n         )\n \n         yield self._check_log_warnsize(request, warn_pattern, Data.HTML_LARGE)\n \n     @inlineCallbacks\n     def test_log_received_warnsize(self):\n-        request = Request(url=self.get_url('/no-content-length-header'), meta={'download_warnsize': 10})\n+        request = Request(\n+            url=self.get_url(\"/no-content-length-header\"),\n+            meta={\"download_warnsize\": 10},\n+        )\n         warn_pattern = re.compile(\n-            rf'Received more \\(\\d*\\) bytes than download '\n-            rf'warn size \\(10\\) in request {request}'\n+            rf\"Received more \\(\\d*\\) bytes than download \"\n+            rf\"warn size \\(10\\) in request {request}\"\n         )\n \n         yield self._check_log_warnsize(request, warn_pattern, Data.NO_CONTENT_LENGTH)\n@@ -495,9 +506,7 @@ class Https2ClientProtocolTestCase(TestCase):\n \n         def get_deferred():\n             return self._check_GET(\n-                Request(self.get_url('/get-data-html-small')),\n-                Data.HTML_SMALL,\n-                200\n+                Request(self.get_url(\"/get-data-html-small\")), Data.HTML_SMALL, 200\n             )\n \n         return self._check_repeat(get_deferred, 500)\n@@ -511,20 +520,20 @@ class Https2ClientProtocolTestCase(TestCase):\n         def assert_inactive_stream(failure):\n             self.assertIsNotNone(failure.check(ResponseFailed))\n             from scrapy.core.http2.stream import InactiveStreamClosed\n-            self.assertTrue(any(\n-                isinstance(e, InactiveStreamClosed)\n-                for e in failure.value.reasons\n-            ))\n+\n+            self.assertTrue(\n+                any(isinstance(e, InactiveStreamClosed) for e in failure.value.reasons)\n+            )\n \n         # Send 100 request (we do not check the result)\n         for _ in range(100):\n-            d = self.make_request(Request(self.get_url('/get-data-html-small')))\n+            d = self.make_request(Request(self.get_url(\"/get-data-html-small\")))\n             d.addBoth(lambda _: None)\n             d_list.append(d)\n \n         # Now send 10 extra request and save the response deferred in a list\n         for _ in range(10):\n-            d = self.make_request(Request(self.get_url('/get-data-html-small')))\n+            d = self.make_request(Request(self.get_url(\"/get-data-html-small\")))\n             d.addCallback(self.fail)\n             d.addErrback(assert_inactive_stream)\n             d_list.append(d)\n@@ -537,19 +546,19 @@ class Https2ClientProtocolTestCase(TestCase):\n \n     def test_invalid_request_type(self):\n         with self.assertRaises(TypeError):\n-            self.make_request('https://InvalidDataTypePassed.com')\n+            self.make_request(\"https://InvalidDataTypePassed.com\")\n \n     def test_query_parameters(self):\n         params = {\n-            'a': generate_random_string(20),\n-            'b': generate_random_string(20),\n-            'c': generate_random_string(20),\n-            'd': generate_random_string(20)\n+            \"a\": generate_random_string(20),\n+            \"b\": generate_random_string(20),\n+            \"c\": generate_random_string(20),\n+            \"d\": generate_random_string(20),\n         }\n-        request = Request(self.get_url(f'/query-params?{urlencode(params)}'))\n+        request = Request(self.get_url(f\"/query-params?{urlencode(params)}\"))\n \n         def assert_query_params(response: Response):\n-            content_encoding = str(response.headers[b'Content-Encoding'], 'utf-8')\n+            content_encoding = str(response.headers[b\"Content-Encoding\"], \"utf-8\")\n             data = json.loads(str(response.body, content_encoding))\n             self.assertEqual(data, params)\n \n@@ -565,7 +574,7 @@ class Https2ClientProtocolTestCase(TestCase):\n \n         d_list = []\n         for status in [200, 404]:\n-            request = Request(self.get_url(f'/status?n={status}'))\n+            request = Request(self.get_url(f\"/status?n={status}\"))\n             d = self.make_request(request)\n             d.addCallback(assert_response_status, status)\n             d.addErrback(self.fail)\n@@ -574,17 +583,23 @@ class Https2ClientProtocolTestCase(TestCase):\n         return DeferredList(d_list, fireOnOneErrback=True)\n \n     def test_response_has_correct_certificate_ip_address(self):\n-        request = Request(self.get_url('/status?n=200'))\n+        request = Request(self.get_url(\"/status?n=200\"))\n \n         def assert_metadata(response: Response):\n             self.assertEqual(response.request, request)\n             self.assertIsInstance(response.certificate, Certificate)\n             self.assertIsNotNone(response.certificate.original)\n-            self.assertEqual(response.certificate.getIssuer(), self.client_certificate.getIssuer())\n-            self.assertTrue(response.certificate.getPublicKey().matches(self.client_certificate.getPublicKey()))\n+            self.assertEqual(\n+                response.certificate.getIssuer(), self.client_certificate.getIssuer()\n+            )\n+            self.assertTrue(\n+                response.certificate.getPublicKey().matches(\n+                    self.client_certificate.getPublicKey()\n+                )\n+            )\n \n             self.assertIsInstance(response.ip_address, IPv4Address)\n-            self.assertEqual(str(response.ip_address), '127.0.0.1')\n+            self.assertEqual(str(response.ip_address), \"127.0.0.1\")\n \n         d = self.make_request(request)\n         d.addCallback(assert_metadata)\n@@ -597,10 +612,11 @@ class Https2ClientProtocolTestCase(TestCase):\n \n         def assert_invalid_hostname(failure: Failure):\n             from scrapy.core.http2.stream import InvalidHostname\n+\n             self.assertIsNotNone(failure.check(InvalidHostname))\n             error_msg = str(failure.value)\n-            self.assertIn('localhost', error_msg)\n-            self.assertIn('127.0.0.1', error_msg)\n+            self.assertIn(\"localhost\", error_msg)\n+            self.assertIn(\"127.0.0.1\", error_msg)\n             self.assertIn(str(request), error_msg)\n \n         d = self.make_request(request)\n@@ -609,24 +625,24 @@ class Https2ClientProtocolTestCase(TestCase):\n         return d\n \n     def test_invalid_hostname(self):\n-        return self._check_invalid_netloc('https://notlocalhost.notlocalhostdomain')\n+        return self._check_invalid_netloc(\"https://notlocalhost.notlocalhostdomain\")\n \n     def test_invalid_host_port(self):\n         port = self.port_number + 1\n-        return self._check_invalid_netloc(f'https://127.0.0.1:{port}')\n+        return self._check_invalid_netloc(f\"https://127.0.0.1:{port}\")\n \n     def test_connection_stays_with_invalid_requests(self):\n         d_list = [\n             self.test_invalid_hostname(),\n             self.test_invalid_host_port(),\n             self.test_GET_small_body(),\n-            self.test_POST_small_json()\n+            self.test_POST_small_json(),\n         ]\n \n         return DeferredList(d_list, fireOnOneErrback=True)\n \n     def test_connection_timeout(self):\n-        request = Request(self.get_url('/timeout'))\n+        request = Request(self.get_url(\"/timeout\"))\n         d = self.make_request(request)\n \n         # Update the timer to 1s to test connection timeout\n@@ -635,8 +651,12 @@ class Https2ClientProtocolTestCase(TestCase):\n         def assert_timeout_error(failure: Failure):\n             for err in failure.value.reasons:\n                 from scrapy.core.http2.protocol import H2ClientProtocol\n+\n                 if isinstance(err, TimeoutError):\n-                    self.assertIn(f\"Connection was IDLE for more than {H2ClientProtocol.IDLE_TIMEOUT}s\", str(err))\n+                    self.assertIn(\n+                        f\"Connection was IDLE for more than {H2ClientProtocol.IDLE_TIMEOUT}s\",\n+                        str(err),\n+                    )\n                     break\n             else:\n                 self.fail()\n@@ -646,20 +666,20 @@ class Https2ClientProtocolTestCase(TestCase):\n         return d\n \n     def test_request_headers_received(self):\n-        request = Request(self.get_url('/request-headers'), headers={\n-            'header-1': 'header value 1',\n-            'header-2': 'header value 2'\n-        })\n+        request = Request(\n+            self.get_url(\"/request-headers\"),\n+            headers={\"header-1\": \"header value 1\", \"header-2\": \"header value 2\"},\n+        )\n         d = self.make_request(request)\n \n         def assert_request_headers(response: Response):\n             self.assertEqual(response.status, 200)\n             self.assertEqual(response.request, request)\n \n-            response_headers = json.loads(str(response.body, 'utf-8'))\n+            response_headers = json.loads(str(response.body, \"utf-8\"))\n             self.assertIsInstance(response_headers, dict)\n             for k, v in request.headers.items():\n-                k, v = str(k, 'utf-8'), str(v[0], 'utf-8')\n+                k, v = str(k, \"utf-8\"), str(v[0], \"utf-8\")\n                 self.assertIn(k, response_headers)\n                 self.assertEqual(v, response_headers[k])\n \n\n@@ -6,10 +6,10 @@ from scrapy.http.cookies import WrappedRequest, WrappedResponse\n \n \n class WrappedRequestTest(TestCase):\n-\n     def setUp(self):\n-        self.request = Request(\"http://www.example.com/page.html\",\n-                               headers={\"Content-Type\": \"text/html\"})\n+        self.request = Request(\n+            \"http://www.example.com/page.html\", headers={\"Content-Type\": \"text/html\"}\n+        )\n         self.wrapped = WrappedRequest(self.request)\n \n     def test_get_full_url(self):\n@@ -29,35 +29,34 @@ class WrappedRequestTest(TestCase):\n         self.assertFalse(self.wrapped.unverifiable)\n \n     def test_is_unverifiable2(self):\n-        self.request.meta['is_unverifiable'] = True\n+        self.request.meta[\"is_unverifiable\"] = True\n         self.assertTrue(self.wrapped.is_unverifiable())\n         self.assertTrue(self.wrapped.unverifiable)\n \n     def test_get_origin_req_host(self):\n-        self.assertEqual(self.wrapped.origin_req_host, 'www.example.com')\n+        self.assertEqual(self.wrapped.origin_req_host, \"www.example.com\")\n \n     def test_has_header(self):\n-        self.assertTrue(self.wrapped.has_header('content-type'))\n-        self.assertFalse(self.wrapped.has_header('xxxxx'))\n+        self.assertTrue(self.wrapped.has_header(\"content-type\"))\n+        self.assertFalse(self.wrapped.has_header(\"xxxxx\"))\n \n     def test_get_header(self):\n-        self.assertEqual(self.wrapped.get_header('content-type'), 'text/html')\n-        self.assertEqual(self.wrapped.get_header('xxxxx', 'def'), 'def')\n+        self.assertEqual(self.wrapped.get_header(\"content-type\"), \"text/html\")\n+        self.assertEqual(self.wrapped.get_header(\"xxxxx\", \"def\"), \"def\")\n \n     def test_header_items(self):\n-        self.assertEqual(self.wrapped.header_items(),\n-                         [('Content-Type', ['text/html'])])\n+        self.assertEqual(self.wrapped.header_items(), [(\"Content-Type\", [\"text/html\"])])\n \n     def test_add_unredirected_header(self):\n-        self.wrapped.add_unredirected_header('hello', 'world')\n-        self.assertEqual(self.request.headers['hello'], b'world')\n+        self.wrapped.add_unredirected_header(\"hello\", \"world\")\n+        self.assertEqual(self.request.headers[\"hello\"], b\"world\")\n \n \n class WrappedResponseTest(TestCase):\n-\n     def setUp(self):\n-        self.response = Response(\"http://www.example.com/page.html\",\n-                                 headers={\"Content-TYpe\": \"text/html\"})\n+        self.response = Response(\n+            \"http://www.example.com/page.html\", headers={\"Content-TYpe\": \"text/html\"}\n+        )\n         self.wrapped = WrappedResponse(self.response)\n \n     def test_info(self):\n@@ -65,4 +64,4 @@ class WrappedResponseTest(TestCase):\n \n     def test_get_all(self):\n         # get_all result must be native string\n-        self.assertEqual(self.wrapped.get_all('content-type'), ['text/html'])\n+        self.assertEqual(self.wrapped.get_all(\"content-type\"), [\"text/html\"])\n\n@@ -5,38 +5,39 @@ from scrapy.http import Headers\n \n \n class HeadersTest(unittest.TestCase):\n-\n     def assertSortedEqual(self, first, second, msg=None):\n         return self.assertEqual(sorted(first), sorted(second), msg)\n \n     def test_basics(self):\n-        h = Headers({'Content-Type': 'text/html', 'Content-Length': 1234})\n-        assert h['Content-Type']\n-        assert h['Content-Length']\n+        h = Headers({\"Content-Type\": \"text/html\", \"Content-Length\": 1234})\n+        assert h[\"Content-Type\"]\n+        assert h[\"Content-Length\"]\n \n-        self.assertRaises(KeyError, h.__getitem__, 'Accept')\n-        self.assertEqual(h.get('Accept'), None)\n-        self.assertEqual(h.getlist('Accept'), [])\n+        self.assertRaises(KeyError, h.__getitem__, \"Accept\")\n+        self.assertEqual(h.get(\"Accept\"), None)\n+        self.assertEqual(h.getlist(\"Accept\"), [])\n \n-        self.assertEqual(h.get('Accept', '*/*'), b'*/*')\n-        self.assertEqual(h.getlist('Accept', '*/*'), [b'*/*'])\n-        self.assertEqual(h.getlist('Accept', ['text/html', 'images/jpeg']),\n-                         [b'text/html', b'images/jpeg'])\n+        self.assertEqual(h.get(\"Accept\", \"*/*\"), b\"*/*\")\n+        self.assertEqual(h.getlist(\"Accept\", \"*/*\"), [b\"*/*\"])\n+        self.assertEqual(\n+            h.getlist(\"Accept\", [\"text/html\", \"images/jpeg\"]),\n+            [b\"text/html\", b\"images/jpeg\"],\n+        )\n \n     def test_single_value(self):\n         h = Headers()\n-        h['Content-Type'] = 'text/html'\n-        self.assertEqual(h['Content-Type'], b'text/html')\n-        self.assertEqual(h.get('Content-Type'), b'text/html')\n-        self.assertEqual(h.getlist('Content-Type'), [b'text/html'])\n+        h[\"Content-Type\"] = \"text/html\"\n+        self.assertEqual(h[\"Content-Type\"], b\"text/html\")\n+        self.assertEqual(h.get(\"Content-Type\"), b\"text/html\")\n+        self.assertEqual(h.getlist(\"Content-Type\"), [b\"text/html\"])\n \n     def test_multivalue(self):\n         h = Headers()\n-        h['X-Forwarded-For'] = hlist = ['ip1', 'ip2']\n-        self.assertEqual(h['X-Forwarded-For'], b'ip2')\n-        self.assertEqual(h.get('X-Forwarded-For'), b'ip2')\n-        self.assertEqual(h.getlist('X-Forwarded-For'), [b'ip1', b'ip2'])\n-        assert h.getlist('X-Forwarded-For') is not hlist\n+        h[\"X-Forwarded-For\"] = hlist = [\"ip1\", \"ip2\"]\n+        self.assertEqual(h[\"X-Forwarded-For\"], b\"ip2\")\n+        self.assertEqual(h.get(\"X-Forwarded-For\"), b\"ip2\")\n+        self.assertEqual(h.getlist(\"X-Forwarded-For\"), [b\"ip1\", b\"ip2\"])\n+        assert h.getlist(\"X-Forwarded-For\") is not hlist\n \n     def test_multivalue_for_one_header(self):\n         h = Headers(((\"a\", \"b\"), (\"a\", \"c\")))\n@@ -45,117 +46,121 @@ class HeadersTest(unittest.TestCase):\n         self.assertEqual(h.getlist(\"a\"), [b\"b\", b\"c\"])\n \n     def test_encode_utf8(self):\n-        h = Headers({'key': '\\xa3'}, encoding='utf-8')\n+        h = Headers({\"key\": \"\\xa3\"}, encoding=\"utf-8\")\n         key, val = dict(h).popitem()\n         assert isinstance(key, bytes), key\n         assert isinstance(val[0], bytes), val[0]\n-        self.assertEqual(val[0], b'\\xc2\\xa3')\n+        self.assertEqual(val[0], b\"\\xc2\\xa3\")\n \n     def test_encode_latin1(self):\n-        h = Headers({'key': '\\xa3'}, encoding='latin1')\n+        h = Headers({\"key\": \"\\xa3\"}, encoding=\"latin1\")\n         key, val = dict(h).popitem()\n-        self.assertEqual(val[0], b'\\xa3')\n+        self.assertEqual(val[0], b\"\\xa3\")\n \n     def test_encode_multiple(self):\n-        h = Headers({'key': ['\\xa3']}, encoding='utf-8')\n+        h = Headers({\"key\": [\"\\xa3\"]}, encoding=\"utf-8\")\n         key, val = dict(h).popitem()\n-        self.assertEqual(val[0], b'\\xc2\\xa3')\n+        self.assertEqual(val[0], b\"\\xc2\\xa3\")\n \n     def test_delete_and_contains(self):\n         h = Headers()\n-        h['Content-Type'] = 'text/html'\n-        assert 'Content-Type' in h\n-        del h['Content-Type']\n-        assert 'Content-Type' not in h\n+        h[\"Content-Type\"] = \"text/html\"\n+        assert \"Content-Type\" in h\n+        del h[\"Content-Type\"]\n+        assert \"Content-Type\" not in h\n \n     def test_setdefault(self):\n         h = Headers()\n-        hlist = ['ip1', 'ip2']\n-        olist = h.setdefault('X-Forwarded-For', hlist)\n-        assert h.getlist('X-Forwarded-For') is not hlist\n-        assert h.getlist('X-Forwarded-For') is olist\n+        hlist = [\"ip1\", \"ip2\"]\n+        olist = h.setdefault(\"X-Forwarded-For\", hlist)\n+        assert h.getlist(\"X-Forwarded-For\") is not hlist\n+        assert h.getlist(\"X-Forwarded-For\") is olist\n \n         h = Headers()\n-        olist = h.setdefault('X-Forwarded-For', 'ip1')\n-        self.assertEqual(h.getlist('X-Forwarded-For'), [b'ip1'])\n-        assert h.getlist('X-Forwarded-For') is olist\n+        olist = h.setdefault(\"X-Forwarded-For\", \"ip1\")\n+        self.assertEqual(h.getlist(\"X-Forwarded-For\"), [b\"ip1\"])\n+        assert h.getlist(\"X-Forwarded-For\") is olist\n \n     def test_iterables(self):\n-        idict = {'Content-Type': 'text/html', 'X-Forwarded-For': ['ip1', 'ip2']}\n+        idict = {\"Content-Type\": \"text/html\", \"X-Forwarded-For\": [\"ip1\", \"ip2\"]}\n \n         h = Headers(idict)\n-        self.assertDictEqual(dict(h),\n-                             {b'Content-Type': [b'text/html'],\n-                              b'X-Forwarded-For': [b'ip1', b'ip2']})\n-        self.assertSortedEqual(h.keys(),\n-                               [b'X-Forwarded-For', b'Content-Type'])\n-        self.assertSortedEqual(h.items(),\n-                               [(b'X-Forwarded-For', [b'ip1', b'ip2']),\n-                                (b'Content-Type', [b'text/html'])])\n-        self.assertSortedEqual(h.values(), [b'ip2', b'text/html'])\n+        self.assertDictEqual(\n+            dict(h),\n+            {b\"Content-Type\": [b\"text/html\"], b\"X-Forwarded-For\": [b\"ip1\", b\"ip2\"]},\n+        )\n+        self.assertSortedEqual(h.keys(), [b\"X-Forwarded-For\", b\"Content-Type\"])\n+        self.assertSortedEqual(\n+            h.items(),\n+            [(b\"X-Forwarded-For\", [b\"ip1\", b\"ip2\"]), (b\"Content-Type\", [b\"text/html\"])],\n+        )\n+        self.assertSortedEqual(h.values(), [b\"ip2\", b\"text/html\"])\n \n     def test_update(self):\n         h = Headers()\n-        h.update({'Content-Type': 'text/html',\n-                  'X-Forwarded-For': ['ip1', 'ip2']})\n-        self.assertEqual(h.getlist('Content-Type'), [b'text/html'])\n-        self.assertEqual(h.getlist('X-Forwarded-For'), [b'ip1', b'ip2'])\n+        h.update({\"Content-Type\": \"text/html\", \"X-Forwarded-For\": [\"ip1\", \"ip2\"]})\n+        self.assertEqual(h.getlist(\"Content-Type\"), [b\"text/html\"])\n+        self.assertEqual(h.getlist(\"X-Forwarded-For\"), [b\"ip1\", b\"ip2\"])\n \n     def test_copy(self):\n-        h1 = Headers({'header1': ['value1', 'value2']})\n+        h1 = Headers({\"header1\": [\"value1\", \"value2\"]})\n         h2 = copy.copy(h1)\n         self.assertEqual(h1, h2)\n-        self.assertEqual(h1.getlist('header1'), h2.getlist('header1'))\n-        assert h1.getlist('header1') is not h2.getlist('header1')\n+        self.assertEqual(h1.getlist(\"header1\"), h2.getlist(\"header1\"))\n+        assert h1.getlist(\"header1\") is not h2.getlist(\"header1\")\n         assert isinstance(h2, Headers)\n \n     def test_appendlist(self):\n-        h1 = Headers({'header1': 'value1'})\n-        h1.appendlist('header1', 'value3')\n-        self.assertEqual(h1.getlist('header1'), [b'value1', b'value3'])\n+        h1 = Headers({\"header1\": \"value1\"})\n+        h1.appendlist(\"header1\", \"value3\")\n+        self.assertEqual(h1.getlist(\"header1\"), [b\"value1\", b\"value3\"])\n \n         h1 = Headers()\n-        h1.appendlist('header1', 'value1')\n-        h1.appendlist('header1', 'value3')\n-        self.assertEqual(h1.getlist('header1'), [b'value1', b'value3'])\n+        h1.appendlist(\"header1\", \"value1\")\n+        h1.appendlist(\"header1\", \"value3\")\n+        self.assertEqual(h1.getlist(\"header1\"), [b\"value1\", b\"value3\"])\n \n     def test_setlist(self):\n-        h1 = Headers({'header1': 'value1'})\n-        self.assertEqual(h1.getlist('header1'), [b'value1'])\n-        h1.setlist('header1', [b'value2', b'value3'])\n-        self.assertEqual(h1.getlist('header1'), [b'value2', b'value3'])\n+        h1 = Headers({\"header1\": \"value1\"})\n+        self.assertEqual(h1.getlist(\"header1\"), [b\"value1\"])\n+        h1.setlist(\"header1\", [b\"value2\", b\"value3\"])\n+        self.assertEqual(h1.getlist(\"header1\"), [b\"value2\", b\"value3\"])\n \n     def test_setlistdefault(self):\n-        h1 = Headers({'header1': 'value1'})\n-        h1.setlistdefault('header1', ['value2', 'value3'])\n-        h1.setlistdefault('header2', ['value2', 'value3'])\n-        self.assertEqual(h1.getlist('header1'), [b'value1'])\n-        self.assertEqual(h1.getlist('header2'), [b'value2', b'value3'])\n+        h1 = Headers({\"header1\": \"value1\"})\n+        h1.setlistdefault(\"header1\", [\"value2\", \"value3\"])\n+        h1.setlistdefault(\"header2\", [\"value2\", \"value3\"])\n+        self.assertEqual(h1.getlist(\"header1\"), [b\"value1\"])\n+        self.assertEqual(h1.getlist(\"header2\"), [b\"value2\", b\"value3\"])\n \n     def test_none_value(self):\n         h1 = Headers()\n-        h1['foo'] = 'bar'\n-        h1['foo'] = None\n-        h1.setdefault('foo', 'bar')\n-        self.assertEqual(h1.get('foo'), None)\n-        self.assertEqual(h1.getlist('foo'), [])\n+        h1[\"foo\"] = \"bar\"\n+        h1[\"foo\"] = None\n+        h1.setdefault(\"foo\", \"bar\")\n+        self.assertEqual(h1.get(\"foo\"), None)\n+        self.assertEqual(h1.getlist(\"foo\"), [])\n \n     def test_int_value(self):\n-        h1 = Headers({'hey': 5})\n-        h1['foo'] = 1\n-        h1.setdefault('bar', 2)\n-        h1.setlist('buz', [1, 'dos', 3])\n-        self.assertEqual(h1.getlist('foo'), [b'1'])\n-        self.assertEqual(h1.getlist('bar'), [b'2'])\n-        self.assertEqual(h1.getlist('buz'), [b'1', b'dos', b'3'])\n-        self.assertEqual(h1.getlist('hey'), [b'5'])\n+        h1 = Headers({\"hey\": 5})\n+        h1[\"foo\"] = 1\n+        h1.setdefault(\"bar\", 2)\n+        h1.setlist(\"buz\", [1, \"dos\", 3])\n+        self.assertEqual(h1.getlist(\"foo\"), [b\"1\"])\n+        self.assertEqual(h1.getlist(\"bar\"), [b\"2\"])\n+        self.assertEqual(h1.getlist(\"buz\"), [b\"1\", b\"dos\", b\"3\"])\n+        self.assertEqual(h1.getlist(\"hey\"), [b\"5\"])\n \n     def test_invalid_value(self):\n-        self.assertRaisesRegex(TypeError, 'Unsupported value type',\n-                               Headers, {'foo': object()})\n-        self.assertRaisesRegex(TypeError, 'Unsupported value type',\n-                               Headers().__setitem__, 'foo', object())\n-        self.assertRaisesRegex(TypeError, 'Unsupported value type',\n-                               Headers().setdefault, 'foo', object())\n-        self.assertRaisesRegex(TypeError, 'Unsupported value type',\n-                               Headers().setlist, 'foo', [object()])\n+        self.assertRaisesRegex(\n+            TypeError, \"Unsupported value type\", Headers, {\"foo\": object()}\n+        )\n+        self.assertRaisesRegex(\n+            TypeError, \"Unsupported value type\", Headers().__setitem__, \"foo\", object()\n+        )\n+        self.assertRaisesRegex(\n+            TypeError, \"Unsupported value type\", Headers().setdefault, \"foo\", object()\n+        )\n+        self.assertRaisesRegex(\n+            TypeError, \"Unsupported value type\", Headers().setlist, \"foo\", [object()]\n+        )\n\n@@ -6,14 +6,21 @@ import warnings\n from unittest import mock\n from urllib.parse import parse_qs, unquote_to_bytes, urlparse\n \n-from scrapy.http import Request, FormRequest, XmlRpcRequest, JsonRequest, Headers, HtmlResponse\n+from scrapy.http import (\n+    Request,\n+    FormRequest,\n+    XmlRpcRequest,\n+    JsonRequest,\n+    Headers,\n+    HtmlResponse,\n+)\n from scrapy.utils.python import to_bytes, to_unicode\n \n \n class RequestTest(unittest.TestCase):\n \n     request_class = Request\n-    default_method = 'GET'\n+    default_method = \"GET\"\n     default_headers = {}\n     default_meta = {}\n \n@@ -23,7 +30,7 @@ class RequestTest(unittest.TestCase):\n \n         # url argument must be basestring\n         self.assertRaises(TypeError, self.request_class, 123)\n-        r = self.request_class('http://www.example.com')\n+        r = self.request_class(\"http://www.example.com\")\n \n         r = self.request_class(\"http://www.example.com\")\n         assert isinstance(r.url, str)\n@@ -36,7 +43,9 @@ class RequestTest(unittest.TestCase):\n \n         meta = {\"lala\": \"lolo\"}\n         headers = {b\"caca\": b\"coco\"}\n-        r = self.request_class(\"http://www.example.com\", meta=meta, headers=headers, body=\"a body\")\n+        r = self.request_class(\n+            \"http://www.example.com\", meta=meta, headers=headers, body=\"a body\"\n+        )\n \n         assert r.meta is not meta\n         self.assertEqual(r.meta, meta)\n@@ -45,22 +54,22 @@ class RequestTest(unittest.TestCase):\n \n     def test_url_scheme(self):\n         # This test passes by not raising any (ValueError) exception\n-        self.request_class('http://example.org')\n-        self.request_class('https://example.org')\n-        self.request_class('s3://example.org')\n-        self.request_class('ftp://example.org')\n-        self.request_class('about:config')\n-        self.request_class('data:,Hello%2C%20World!')\n+        self.request_class(\"http://example.org\")\n+        self.request_class(\"https://example.org\")\n+        self.request_class(\"s3://example.org\")\n+        self.request_class(\"ftp://example.org\")\n+        self.request_class(\"about:config\")\n+        self.request_class(\"data:,Hello%2C%20World!\")\n \n     def test_url_no_scheme(self):\n-        self.assertRaises(ValueError, self.request_class, 'foo')\n-        self.assertRaises(ValueError, self.request_class, '/foo/')\n-        self.assertRaises(ValueError, self.request_class, '/foo:bar')\n+        self.assertRaises(ValueError, self.request_class, \"foo\")\n+        self.assertRaises(ValueError, self.request_class, \"/foo/\")\n+        self.assertRaises(ValueError, self.request_class, \"/foo:bar\")\n \n     def test_headers(self):\n         # Different ways of setting headers attribute\n-        url = 'http://www.scrapy.org'\n-        headers = {b'Accept': 'gzip', b'Custom-Header': 'nothing to tell you'}\n+        url = \"http://www.scrapy.org\"\n+        headers = {b\"Accept\": \"gzip\", b\"Custom-Header\": \"nothing to tell you\"}\n         r = self.request_class(url=url, headers=headers)\n         p = self.request_class(url=url, headers=r.headers)\n \n@@ -69,15 +78,15 @@ class RequestTest(unittest.TestCase):\n         self.assertFalse(p.headers is r.headers)\n \n         # headers must not be unicode\n-        h = Headers({'key1': 'val1', 'key2': 'val2'})\n-        h['newkey'] = 'newval'\n+        h = Headers({\"key1\": \"val1\", \"key2\": \"val2\"})\n+        h[\"newkey\"] = \"newval\"\n         for k, v in h.items():\n             self.assertIsInstance(k, bytes)\n             for s in v:\n                 self.assertIsInstance(s, bytes)\n \n     def test_eq(self):\n-        url = 'http://www.scrapy.org'\n+        url = \"http://www.scrapy.org\"\n         r1 = self.request_class(url=url)\n         r2 = self.request_class(url=url)\n         self.assertNotEqual(r1, r2)\n@@ -115,13 +124,17 @@ class RequestTest(unittest.TestCase):\n         self.assertEqual(r1.url, \"http://www.scrapy.org/price/%C2%A3?unit=%C2%B5\")\n \n         # should be same as above\n-        r2 = self.request_class(url=\"http://www.scrapy.org/price/£?unit=µ\", encoding=\"utf-8\")\n+        r2 = self.request_class(\n+            url=\"http://www.scrapy.org/price/£?unit=µ\", encoding=\"utf-8\"\n+        )\n         self.assertEqual(r2.url, \"http://www.scrapy.org/price/%C2%A3?unit=%C2%B5\")\n \n     def test_url_encoding_query_latin1(self):\n         # encoding is used for encoding query-string before percent-escaping;\n         # path is still UTF-8 encoded before percent-escaping\n-        r3 = self.request_class(url=\"http://www.scrapy.org/price/µ?currency=£\", encoding=\"latin1\")\n+        r3 = self.request_class(\n+            url=\"http://www.scrapy.org/price/µ?currency=£\", encoding=\"latin1\"\n+        )\n         self.assertEqual(r3.url, \"http://www.scrapy.org/price/%C2%B5?currency=%A3\")\n \n     def test_url_encoding_nonutf8_untouched(self):\n@@ -154,27 +167,35 @@ class RequestTest(unittest.TestCase):\n \n     def test_body(self):\n         r1 = self.request_class(url=\"http://www.example.com/\")\n-        assert r1.body == b''\n+        assert r1.body == b\"\"\n \n         r2 = self.request_class(url=\"http://www.example.com/\", body=b\"\")\n         assert isinstance(r2.body, bytes)\n-        self.assertEqual(r2.encoding, 'utf-8')  # default encoding\n+        self.assertEqual(r2.encoding, \"utf-8\")  # default encoding\n \n-        r3 = self.request_class(url=\"http://www.example.com/\", body=\"Price: \\xa3100\", encoding='utf-8')\n+        r3 = self.request_class(\n+            url=\"http://www.example.com/\", body=\"Price: \\xa3100\", encoding=\"utf-8\"\n+        )\n         assert isinstance(r3.body, bytes)\n         self.assertEqual(r3.body, b\"Price: \\xc2\\xa3100\")\n \n-        r4 = self.request_class(url=\"http://www.example.com/\", body=\"Price: \\xa3100\", encoding='latin1')\n+        r4 = self.request_class(\n+            url=\"http://www.example.com/\", body=\"Price: \\xa3100\", encoding=\"latin1\"\n+        )\n         assert isinstance(r4.body, bytes)\n         self.assertEqual(r4.body, b\"Price: \\xa3100\")\n \n     def test_ajax_url(self):\n         # ascii url\n         r = self.request_class(url=\"http://www.example.com/ajax.html#!key=value\")\n-        self.assertEqual(r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\")\n+        self.assertEqual(\n+            r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\"\n+        )\n         # unicode url\n         r = self.request_class(url=\"http://www.example.com/ajax.html#!key=value\")\n-        self.assertEqual(r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\")\n+        self.assertEqual(\n+            r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\"\n+        )\n \n     def test_copy(self):\n         \"\"\"Test Request copy\"\"\"\n@@ -182,10 +203,14 @@ class RequestTest(unittest.TestCase):\n         def somecallback():\n             pass\n \n-        r1 = self.request_class(\"http://www.example.com\", flags=['f1', 'f2'],\n-                                callback=somecallback, errback=somecallback)\n-        r1.meta['foo'] = 'bar'\n-        r1.cb_kwargs['key'] = 'value'\n+        r1 = self.request_class(\n+            \"http://www.example.com\",\n+            flags=[\"f1\", \"f2\"],\n+            callback=somecallback,\n+            errback=somecallback,\n+        )\n+        r1.meta[\"foo\"] = \"bar\"\n+        r1.cb_kwargs[\"key\"] = \"value\"\n         r2 = r1.copy()\n \n         # make sure copy does not propagate callbacks\n@@ -199,7 +224,9 @@ class RequestTest(unittest.TestCase):\n         self.assertEqual(r1.flags, r2.flags)\n \n         # make sure cb_kwargs dict is shallow copied\n-        assert r1.cb_kwargs is not r2.cb_kwargs, \"cb_kwargs must be a shallow copy, not identical\"\n+        assert (\n+            r1.cb_kwargs is not r2.cb_kwargs\n+        ), \"cb_kwargs must be a shallow copy, not identical\"\n         self.assertEqual(r1.cb_kwargs, r2.cb_kwargs)\n \n         # make sure meta dict is shallow copied\n@@ -207,7 +234,9 @@ class RequestTest(unittest.TestCase):\n         self.assertEqual(r1.meta, r2.meta)\n \n         # make sure headers attribute is shallow copied\n-        assert r1.headers is not r2.headers, \"headers must be a shallow copy, not identical\"\n+        assert (\n+            r1.headers is not r2.headers\n+        ), \"headers must be a shallow copy, not identical\"\n         self.assertEqual(r1.headers, r2.headers)\n         self.assertEqual(r1.encoding, r2.encoding)\n         self.assertEqual(r1.dont_filter, r2.dont_filter)\n@@ -220,27 +249,31 @@ class RequestTest(unittest.TestCase):\n         class CustomRequest(self.request_class):\n             pass\n \n-        r1 = CustomRequest('http://www.example.com')\n+        r1 = CustomRequest(\"http://www.example.com\")\n         r2 = r1.copy()\n \n         assert isinstance(r2, CustomRequest)\n \n     def test_replace(self):\n         \"\"\"Test Request.replace() method\"\"\"\n-        r1 = self.request_class(\"http://www.example.com\", method='GET')\n+        r1 = self.request_class(\"http://www.example.com\", method=\"GET\")\n         hdrs = Headers(r1.headers)\n-        hdrs[b'key'] = b'value'\n+        hdrs[b\"key\"] = b\"value\"\n         r2 = r1.replace(method=\"POST\", body=\"New body\", headers=hdrs)\n         self.assertEqual(r1.url, r2.url)\n         self.assertEqual((r1.method, r2.method), (\"GET\", \"POST\"))\n-        self.assertEqual((r1.body, r2.body), (b'', b\"New body\"))\n+        self.assertEqual((r1.body, r2.body), (b\"\", b\"New body\"))\n         self.assertEqual((r1.headers, r2.headers), (self.default_headers, hdrs))\n \n         # Empty attributes (which may fail if not compared properly)\n-        r3 = self.request_class(\"http://www.example.com\", meta={'a': 1}, dont_filter=True)\n-        r4 = r3.replace(url=\"http://www.example.com/2\", body=b'', meta={}, dont_filter=False)\n+        r3 = self.request_class(\n+            \"http://www.example.com\", meta={\"a\": 1}, dont_filter=True\n+        )\n+        r4 = r3.replace(\n+            url=\"http://www.example.com/2\", body=b\"\", meta={}, dont_filter=False\n+        )\n         self.assertEqual(r4.url, \"http://www.example.com/2\")\n-        self.assertEqual(r4.body, b'')\n+        self.assertEqual(r4.body, b\"\")\n         self.assertEqual(r4.meta, {})\n         assert r4.dont_filter is False\n \n@@ -250,27 +283,27 @@ class RequestTest(unittest.TestCase):\n \n     def test_immutable_attributes(self):\n         r = self.request_class(\"http://example.com\")\n-        self.assertRaises(AttributeError, setattr, r, 'url', 'http://example2.com')\n-        self.assertRaises(AttributeError, setattr, r, 'body', 'xxx')\n+        self.assertRaises(AttributeError, setattr, r, \"url\", \"http://example2.com\")\n+        self.assertRaises(AttributeError, setattr, r, \"body\", \"xxx\")\n \n     def test_callback_and_errback(self):\n         def a_function():\n             pass\n \n-        r1 = self.request_class('http://example.com')\n+        r1 = self.request_class(\"http://example.com\")\n         self.assertIsNone(r1.callback)\n         self.assertIsNone(r1.errback)\n \n-        r2 = self.request_class('http://example.com', callback=a_function)\n+        r2 = self.request_class(\"http://example.com\", callback=a_function)\n         self.assertIs(r2.callback, a_function)\n         self.assertIsNone(r2.errback)\n \n-        r3 = self.request_class('http://example.com', errback=a_function)\n+        r3 = self.request_class(\"http://example.com\", errback=a_function)\n         self.assertIsNone(r3.callback)\n         self.assertIs(r3.errback, a_function)\n \n         r4 = self.request_class(\n-            url='http://example.com',\n+            url=\"http://example.com\",\n             callback=a_function,\n             errback=a_function,\n         )\n@@ -279,14 +312,14 @@ class RequestTest(unittest.TestCase):\n \n     def test_callback_and_errback_type(self):\n         with self.assertRaises(TypeError):\n-            self.request_class('http://example.com', callback='a_function')\n+            self.request_class(\"http://example.com\", callback=\"a_function\")\n         with self.assertRaises(TypeError):\n-            self.request_class('http://example.com', errback='a_function')\n+            self.request_class(\"http://example.com\", errback=\"a_function\")\n         with self.assertRaises(TypeError):\n             self.request_class(\n-                url='http://example.com',\n-                callback='a_function',\n-                errback='a_function',\n+                url=\"http://example.com\",\n+                callback=\"a_function\",\n+                errback=\"a_function\",\n             )\n \n     def test_from_curl(self):\n@@ -311,37 +344,48 @@ class RequestTest(unittest.TestCase):\n         r = self.request_class.from_curl(curl_command)\n         self.assertEqual(r.method, \"POST\")\n         self.assertEqual(r.url, \"http://httpbin.org/post\")\n-        self.assertEqual(r.body,\n+        self.assertEqual(\n+            r.body,\n             b\"custname=John+Smith&custtel=500&custemail=jsmith%40\"\n             b\"example.org&size=small&topping=cheese&topping=onion\"\n-                         b\"&delivery=12%3A15&comments=\")\n-        self.assertEqual(r.cookies, {\n-            '_gauges_unique_year': '1',\n-            '_gauges_unique': '1',\n-            '_gauges_unique_month': '1',\n-            '_gauges_unique_hour': '1',\n-            '_gauges_unique_day': '1'\n-        })\n-        self.assertEqual(r.headers, {\n-            b'Origin': [b'http://httpbin.org'],\n-            b'Accept-Encoding': [b'gzip, deflate'],\n-            b'Accept-Language': [b'en-US,en;q=0.9,ru;q=0.8,es;q=0.7'],\n-            b'Upgrade-Insecure-Requests': [b'1'],\n-            b'User-Agent': [b'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.'\n-                            b'36 (KHTML, like Gecko) Ubuntu Chromium/62.0.3202'\n-                            b'.75 Chrome/62.0.3202.75 Safari/537.36'],\n-            b'Content-Type': [b'application /x-www-form-urlencoded'],\n-            b'Accept': [b'text/html,application/xhtml+xml,application/xml;q=0.'\n-                        b'9,image/webp,image/apng,*/*;q=0.8'],\n-            b'Cache-Control': [b'max-age=0'],\n-            b'Referer': [b'http://httpbin.org/forms/post'],\n-            b'Connection': [b'keep-alive']})\n+            b\"&delivery=12%3A15&comments=\",\n+        )\n+        self.assertEqual(\n+            r.cookies,\n+            {\n+                \"_gauges_unique_year\": \"1\",\n+                \"_gauges_unique\": \"1\",\n+                \"_gauges_unique_month\": \"1\",\n+                \"_gauges_unique_hour\": \"1\",\n+                \"_gauges_unique_day\": \"1\",\n+            },\n+        )\n+        self.assertEqual(\n+            r.headers,\n+            {\n+                b\"Origin\": [b\"http://httpbin.org\"],\n+                b\"Accept-Encoding\": [b\"gzip, deflate\"],\n+                b\"Accept-Language\": [b\"en-US,en;q=0.9,ru;q=0.8,es;q=0.7\"],\n+                b\"Upgrade-Insecure-Requests\": [b\"1\"],\n+                b\"User-Agent\": [\n+                    b\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.\"\n+                    b\"36 (KHTML, like Gecko) Ubuntu Chromium/62.0.3202\"\n+                    b\".75 Chrome/62.0.3202.75 Safari/537.36\"\n+                ],\n+                b\"Content-Type\": [b\"application /x-www-form-urlencoded\"],\n+                b\"Accept\": [\n+                    b\"text/html,application/xhtml+xml,application/xml;q=0.\"\n+                    b\"9,image/webp,image/apng,*/*;q=0.8\"\n+                ],\n+                b\"Cache-Control\": [b\"max-age=0\"],\n+                b\"Referer\": [b\"http://httpbin.org/forms/post\"],\n+                b\"Connection\": [b\"keep-alive\"],\n+            },\n+        )\n \n     def test_from_curl_with_kwargs(self):\n         r = self.request_class.from_curl(\n-            'curl -X PATCH \"http://example.org\"',\n-            method=\"POST\",\n-            meta={'key': 'value'}\n+            'curl -X PATCH \"http://example.org\"', method=\"POST\", meta={\"key\": \"value\"}\n         )\n         self.assertEqual(r.method, \"POST\")\n         self.assertEqual(r.meta, {\"key\": \"value\"})\n@@ -349,7 +393,7 @@ class RequestTest(unittest.TestCase):\n     def test_from_curl_ignore_unknown_options(self):\n         # By default: it works and ignores the unknown options: --foo and -z\n         with warnings.catch_warnings():  # avoid warning when executing tests\n-            warnings.simplefilter('ignore')\n+            warnings.simplefilter(\"ignore\")\n             r = self.request_class.from_curl(\n                 'curl -X DELETE \"http://example.org\" --foo -z',\n             )\n@@ -377,68 +421,85 @@ class FormRequestTest(RequestTest):\n \n     def test_empty_formdata(self):\n         r1 = self.request_class(\"http://www.example.com\", formdata={})\n-        self.assertEqual(r1.body, b'')\n+        self.assertEqual(r1.body, b\"\")\n \n     def test_formdata_overrides_querystring(self):\n-        data = (('a', 'one'), ('a', 'two'), ('b', '2'))\n-        url = self.request_class('http://www.example.com/?a=0&b=1&c=3#fragment',\n-                                 method='GET', formdata=data).url.split('#')[0]\n-        fs = _qs(self.request_class(url, method='GET', formdata=data))\n-        self.assertEqual(set(fs[b'a']), {b'one', b'two'})\n-        self.assertEqual(fs[b'b'], [b'2'])\n-        self.assertIsNone(fs.get(b'c'))\n+        data = ((\"a\", \"one\"), (\"a\", \"two\"), (\"b\", \"2\"))\n+        url = self.request_class(\n+            \"http://www.example.com/?a=0&b=1&c=3#fragment\", method=\"GET\", formdata=data\n+        ).url.split(\"#\")[0]\n+        fs = _qs(self.request_class(url, method=\"GET\", formdata=data))\n+        self.assertEqual(set(fs[b\"a\"]), {b\"one\", b\"two\"})\n+        self.assertEqual(fs[b\"b\"], [b\"2\"])\n+        self.assertIsNone(fs.get(b\"c\"))\n \n-        data = {'a': '1', 'b': '2'}\n-        fs = _qs(self.request_class('http://www.example.com/', method='GET', formdata=data))\n-        self.assertEqual(fs[b'a'], [b'1'])\n-        self.assertEqual(fs[b'b'], [b'2'])\n+        data = {\"a\": \"1\", \"b\": \"2\"}\n+        fs = _qs(\n+            self.request_class(\"http://www.example.com/\", method=\"GET\", formdata=data)\n+        )\n+        self.assertEqual(fs[b\"a\"], [b\"1\"])\n+        self.assertEqual(fs[b\"b\"], [b\"2\"])\n \n     def test_default_encoding_bytes(self):\n         # using default encoding (utf-8)\n-        data = {b'one': b'two', b'price': b'\\xc2\\xa3 100'}\n+        data = {b\"one\": b\"two\", b\"price\": b\"\\xc2\\xa3 100\"}\n         r2 = self.request_class(\"http://www.example.com\", formdata=data)\n-        self.assertEqual(r2.method, 'POST')\n-        self.assertEqual(r2.encoding, 'utf-8')\n-        self.assertQueryEqual(r2.body, b'price=%C2%A3+100&one=two')\n-        self.assertEqual(r2.headers[b'Content-Type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(r2.method, \"POST\")\n+        self.assertEqual(r2.encoding, \"utf-8\")\n+        self.assertQueryEqual(r2.body, b\"price=%C2%A3+100&one=two\")\n+        self.assertEqual(\n+            r2.headers[b\"Content-Type\"], b\"application/x-www-form-urlencoded\"\n+        )\n \n     def test_default_encoding_textual_data(self):\n         # using default encoding (utf-8)\n-        data = {'µ one': 'two', 'price': '£ 100'}\n+        data = {\"µ one\": \"two\", \"price\": \"£ 100\"}\n         r2 = self.request_class(\"http://www.example.com\", formdata=data)\n-        self.assertEqual(r2.method, 'POST')\n-        self.assertEqual(r2.encoding, 'utf-8')\n-        self.assertQueryEqual(r2.body, b'price=%C2%A3+100&%C2%B5+one=two')\n-        self.assertEqual(r2.headers[b'Content-Type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(r2.method, \"POST\")\n+        self.assertEqual(r2.encoding, \"utf-8\")\n+        self.assertQueryEqual(r2.body, b\"price=%C2%A3+100&%C2%B5+one=two\")\n+        self.assertEqual(\n+            r2.headers[b\"Content-Type\"], b\"application/x-www-form-urlencoded\"\n+        )\n \n     def test_default_encoding_mixed_data(self):\n         # using default encoding (utf-8)\n-        data = {'\\u00b5one': b'two', b'price\\xc2\\xa3': '\\u00a3 100'}\n+        data = {\"\\u00b5one\": b\"two\", b\"price\\xc2\\xa3\": \"\\u00a3 100\"}\n         r2 = self.request_class(\"http://www.example.com\", formdata=data)\n-        self.assertEqual(r2.method, 'POST')\n-        self.assertEqual(r2.encoding, 'utf-8')\n-        self.assertQueryEqual(r2.body, b'%C2%B5one=two&price%C2%A3=%C2%A3+100')\n-        self.assertEqual(r2.headers[b'Content-Type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(r2.method, \"POST\")\n+        self.assertEqual(r2.encoding, \"utf-8\")\n+        self.assertQueryEqual(r2.body, b\"%C2%B5one=two&price%C2%A3=%C2%A3+100\")\n+        self.assertEqual(\n+            r2.headers[b\"Content-Type\"], b\"application/x-www-form-urlencoded\"\n+        )\n \n     def test_custom_encoding_bytes(self):\n-        data = {b'\\xb5 one': b'two', b'price': b'\\xa3 100'}\n-        r2 = self.request_class(\"http://www.example.com\", formdata=data, encoding='latin1')\n-        self.assertEqual(r2.method, 'POST')\n-        self.assertEqual(r2.encoding, 'latin1')\n-        self.assertQueryEqual(r2.body, b'price=%A3+100&%B5+one=two')\n-        self.assertEqual(r2.headers[b'Content-Type'], b'application/x-www-form-urlencoded')\n+        data = {b\"\\xb5 one\": b\"two\", b\"price\": b\"\\xa3 100\"}\n+        r2 = self.request_class(\n+            \"http://www.example.com\", formdata=data, encoding=\"latin1\"\n+        )\n+        self.assertEqual(r2.method, \"POST\")\n+        self.assertEqual(r2.encoding, \"latin1\")\n+        self.assertQueryEqual(r2.body, b\"price=%A3+100&%B5+one=two\")\n+        self.assertEqual(\n+            r2.headers[b\"Content-Type\"], b\"application/x-www-form-urlencoded\"\n+        )\n \n     def test_custom_encoding_textual_data(self):\n-        data = {'price': '£ 100'}\n-        r3 = self.request_class(\"http://www.example.com\", formdata=data, encoding='latin1')\n-        self.assertEqual(r3.encoding, 'latin1')\n-        self.assertEqual(r3.body, b'price=%A3+100')\n+        data = {\"price\": \"£ 100\"}\n+        r3 = self.request_class(\n+            \"http://www.example.com\", formdata=data, encoding=\"latin1\"\n+        )\n+        self.assertEqual(r3.encoding, \"latin1\")\n+        self.assertEqual(r3.body, b\"price=%A3+100\")\n \n     def test_multi_key_values(self):\n         # using multiples values for a single key\n-        data = {'price': '\\xa3 100', 'colours': ['red', 'blue', 'green']}\n+        data = {\"price\": \"\\xa3 100\", \"colours\": [\"red\", \"blue\", \"green\"]}\n         r3 = self.request_class(\"http://www.example.com\", formdata=data)\n-        self.assertQueryEqual(r3.body, b'colours=red&colours=blue&colours=green&price=%C2%A3+100')\n+        self.assertQueryEqual(\n+            r3.body, b\"colours=red&colours=blue&colours=green&price=%C2%A3+100\"\n+        )\n \n     def test_from_response_post(self):\n         response = _buildresponse(\n@@ -447,17 +508,22 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"test\" value=\"val2\">\n             <input type=\"hidden\" name=\"test2\" value=\"xxx\">\n             </form>\"\"\",\n-            url=\"http://www.example.com/this/list.html\")\n-        req = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})\n+            url=\"http://www.example.com/this/list.html\",\n+        )\n+        req = self.request_class.from_response(\n+            response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n+        )\n \n-        self.assertEqual(req.method, 'POST')\n-        self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(req.method, \"POST\")\n+        self.assertEqual(\n+            req.headers[b\"Content-type\"], b\"application/x-www-form-urlencoded\"\n+        )\n         self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n         fs = _qs(req)\n-        self.assertEqual(set(fs[b'test']), {b'val1', b'val2'})\n-        self.assertEqual(set(fs[b'one']), {b'two', b'three'})\n-        self.assertEqual(fs[b'test2'], [b'xxx'])\n-        self.assertEqual(fs[b'six'], [b'seven'])\n+        self.assertEqual(set(fs[b\"test\"]), {b\"val1\", b\"val2\"})\n+        self.assertEqual(set(fs[b\"one\"]), {b\"two\", b\"three\"})\n+        self.assertEqual(fs[b\"test2\"], [b\"xxx\"])\n+        self.assertEqual(fs[b\"six\"], [b\"seven\"])\n \n     def test_from_response_post_nonascii_bytes_utf8(self):\n         response = _buildresponse(\n@@ -466,17 +532,22 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"test \\xc2\\xa3\" value=\"val2\">\n             <input type=\"hidden\" name=\"test2\" value=\"xxx \\xc2\\xb5\">\n             </form>\"\"\",\n-            url=\"http://www.example.com/this/list.html\")\n-        req = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})\n+            url=\"http://www.example.com/this/list.html\",\n+        )\n+        req = self.request_class.from_response(\n+            response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n+        )\n \n-        self.assertEqual(req.method, 'POST')\n-        self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(req.method, \"POST\")\n+        self.assertEqual(\n+            req.headers[b\"Content-type\"], b\"application/x-www-form-urlencoded\"\n+        )\n         self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n         fs = _qs(req, to_unicode=True)\n-        self.assertEqual(set(fs['test £']), {'val1', 'val2'})\n-        self.assertEqual(set(fs['one']), {'two', 'three'})\n-        self.assertEqual(fs['test2'], ['xxx µ'])\n-        self.assertEqual(fs['six'], ['seven'])\n+        self.assertEqual(set(fs[\"test £\"]), {\"val1\", \"val2\"})\n+        self.assertEqual(set(fs[\"one\"]), {\"two\", \"three\"})\n+        self.assertEqual(fs[\"test2\"], [\"xxx µ\"])\n+        self.assertEqual(fs[\"six\"], [\"seven\"])\n \n     def test_from_response_post_nonascii_bytes_latin1(self):\n         response = _buildresponse(\n@@ -486,18 +557,22 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"test2\" value=\"xxx \\xb5\">\n             </form>\"\"\",\n             url=\"http://www.example.com/this/list.html\",\n-            encoding='latin1',\n+            encoding=\"latin1\",\n+        )\n+        req = self.request_class.from_response(\n+            response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n         )\n-        req = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})\n \n-        self.assertEqual(req.method, 'POST')\n-        self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(req.method, \"POST\")\n+        self.assertEqual(\n+            req.headers[b\"Content-type\"], b\"application/x-www-form-urlencoded\"\n+        )\n         self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n-        fs = _qs(req, to_unicode=True, encoding='latin1')\n-        self.assertEqual(set(fs['test £']), {'val1', 'val2'})\n-        self.assertEqual(set(fs['one']), {'two', 'three'})\n-        self.assertEqual(fs['test2'], ['xxx µ'])\n-        self.assertEqual(fs['six'], ['seven'])\n+        fs = _qs(req, to_unicode=True, encoding=\"latin1\")\n+        self.assertEqual(set(fs[\"test £\"]), {\"val1\", \"val2\"})\n+        self.assertEqual(set(fs[\"one\"]), {\"two\", \"three\"})\n+        self.assertEqual(fs[\"test2\"], [\"xxx µ\"])\n+        self.assertEqual(fs[\"six\"], [\"seven\"])\n \n     def test_from_response_post_nonascii_unicode(self):\n         response = _buildresponse(\n@@ -506,42 +581,46 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"test £\" value=\"val2\">\n             <input type=\"hidden\" name=\"test2\" value=\"xxx µ\">\n             </form>\"\"\",\n-            url=\"http://www.example.com/this/list.html\")\n-        req = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})\n+            url=\"http://www.example.com/this/list.html\",\n+        )\n+        req = self.request_class.from_response(\n+            response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n+        )\n \n-        self.assertEqual(req.method, 'POST')\n-        self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(req.method, \"POST\")\n+        self.assertEqual(\n+            req.headers[b\"Content-type\"], b\"application/x-www-form-urlencoded\"\n+        )\n         self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n         fs = _qs(req, to_unicode=True)\n-        self.assertEqual(set(fs['test £']), {'val1', 'val2'})\n-        self.assertEqual(set(fs['one']), {'two', 'three'})\n-        self.assertEqual(fs['test2'], ['xxx µ'])\n-        self.assertEqual(fs['six'], ['seven'])\n+        self.assertEqual(set(fs[\"test £\"]), {\"val1\", \"val2\"})\n+        self.assertEqual(set(fs[\"one\"]), {\"two\", \"three\"})\n+        self.assertEqual(fs[\"test2\"], [\"xxx µ\"])\n+        self.assertEqual(fs[\"six\"], [\"seven\"])\n \n     def test_from_response_duplicate_form_key(self):\n-        response = _buildresponse(\n-            '<form></form>',\n-            url='http://www.example.com')\n+        response = _buildresponse(\"<form></form>\", url=\"http://www.example.com\")\n         req = self.request_class.from_response(\n             response=response,\n-            method='GET',\n-            formdata=(('foo', 'bar'), ('foo', 'baz')),\n+            method=\"GET\",\n+            formdata=((\"foo\", \"bar\"), (\"foo\", \"baz\")),\n         )\n-        self.assertEqual(urlparse(req.url).hostname, 'www.example.com')\n-        self.assertEqual(urlparse(req.url).query, 'foo=bar&foo=baz')\n+        self.assertEqual(urlparse(req.url).hostname, \"www.example.com\")\n+        self.assertEqual(urlparse(req.url).query, \"foo=bar&foo=baz\")\n \n     def test_from_response_override_duplicate_form_key(self):\n         response = _buildresponse(\n             \"\"\"<form action=\"get.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"one\" value=\"1\">\n             <input type=\"hidden\" name=\"two\" value=\"3\">\n-            </form>\"\"\")\n+            </form>\"\"\"\n+        )\n         req = self.request_class.from_response(\n-            response,\n-            formdata=(('two', '2'), ('two', '4')))\n+            response, formdata=((\"two\", \"2\"), (\"two\", \"4\"))\n+        )\n         fs = _qs(req)\n-        self.assertEqual(fs[b'one'], [b'1'])\n-        self.assertEqual(fs[b'two'], [b'2', b'4'])\n+        self.assertEqual(fs[b\"one\"], [b\"1\"])\n+        self.assertEqual(fs[b\"two\"], [b\"2\", b\"4\"])\n \n     def test_from_response_extra_headers(self):\n         response = _buildresponse(\n@@ -549,15 +628,18 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"test\" value=\"val1\">\n             <input type=\"hidden\" name=\"test\" value=\"val2\">\n             <input type=\"hidden\" name=\"test2\" value=\"xxx\">\n-            </form>\"\"\")\n+            </form>\"\"\"\n+        )\n         req = self.request_class.from_response(\n             response=response,\n-            formdata={'one': ['two', 'three'], 'six': 'seven'},\n+            formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"},\n             headers={\"Accept-Encoding\": \"gzip,deflate\"},\n         )\n-        self.assertEqual(req.method, 'POST')\n-        self.assertEqual(req.headers['Content-type'], b'application/x-www-form-urlencoded')\n-        self.assertEqual(req.headers['Accept-Encoding'], b'gzip,deflate')\n+        self.assertEqual(req.method, \"POST\")\n+        self.assertEqual(\n+            req.headers[\"Content-type\"], b\"application/x-www-form-urlencoded\"\n+        )\n+        self.assertEqual(req.headers[\"Accept-Encoding\"], b\"gzip,deflate\")\n \n     def test_from_response_get(self):\n         response = _buildresponse(\n@@ -566,60 +648,67 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"test\" value=\"val2\">\n             <input type=\"hidden\" name=\"test2\" value=\"xxx\">\n             </form>\"\"\",\n-            url=\"http://www.example.com/this/list.html\")\n-        r1 = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})\n-        self.assertEqual(r1.method, 'GET')\n+            url=\"http://www.example.com/this/list.html\",\n+        )\n+        r1 = self.request_class.from_response(\n+            response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n+        )\n+        self.assertEqual(r1.method, \"GET\")\n         self.assertEqual(urlparse(r1.url).hostname, \"www.example.com\")\n         self.assertEqual(urlparse(r1.url).path, \"/this/get.php\")\n         fs = _qs(r1)\n-        self.assertEqual(set(fs[b'test']), {b'val1', b'val2'})\n-        self.assertEqual(set(fs[b'one']), {b'two', b'three'})\n-        self.assertEqual(fs[b'test2'], [b'xxx'])\n-        self.assertEqual(fs[b'six'], [b'seven'])\n+        self.assertEqual(set(fs[b\"test\"]), {b\"val1\", b\"val2\"})\n+        self.assertEqual(set(fs[b\"one\"]), {b\"two\", b\"three\"})\n+        self.assertEqual(fs[b\"test2\"], [b\"xxx\"])\n+        self.assertEqual(fs[b\"six\"], [b\"seven\"])\n \n     def test_from_response_override_params(self):\n         response = _buildresponse(\n             \"\"\"<form action=\"get.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"one\" value=\"1\">\n             <input type=\"hidden\" name=\"two\" value=\"3\">\n-            </form>\"\"\")\n-        req = self.request_class.from_response(response, formdata={'two': '2'})\n+            </form>\"\"\"\n+        )\n+        req = self.request_class.from_response(response, formdata={\"two\": \"2\"})\n         fs = _qs(req)\n-        self.assertEqual(fs[b'one'], [b'1'])\n-        self.assertEqual(fs[b'two'], [b'2'])\n+        self.assertEqual(fs[b\"one\"], [b\"1\"])\n+        self.assertEqual(fs[b\"two\"], [b\"2\"])\n \n     def test_from_response_drop_params(self):\n         response = _buildresponse(\n             \"\"\"<form action=\"get.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"one\" value=\"1\">\n             <input type=\"hidden\" name=\"two\" value=\"3\">\n-            </form>\"\"\")\n-        req = self.request_class.from_response(response, formdata={'two': None})\n+            </form>\"\"\"\n+        )\n+        req = self.request_class.from_response(response, formdata={\"two\": None})\n         fs = _qs(req)\n-        self.assertEqual(fs[b'one'], [b'1'])\n-        self.assertNotIn(b'two', fs)\n+        self.assertEqual(fs[b\"one\"], [b\"1\"])\n+        self.assertNotIn(b\"two\", fs)\n \n     def test_from_response_override_method(self):\n         response = _buildresponse(\n-            '''<html><body>\n+            \"\"\"<html><body>\n             <form action=\"/app\"></form>\n-            </body></html>''')\n+            </body></html>\"\"\"\n+        )\n         request = FormRequest.from_response(response)\n-        self.assertEqual(request.method, 'GET')\n-        request = FormRequest.from_response(response, method='POST')\n-        self.assertEqual(request.method, 'POST')\n+        self.assertEqual(request.method, \"GET\")\n+        request = FormRequest.from_response(response, method=\"POST\")\n+        self.assertEqual(request.method, \"POST\")\n \n     def test_from_response_override_url(self):\n         response = _buildresponse(\n-            '''<html><body>\n+            \"\"\"<html><body>\n             <form action=\"/app\"></form>\n-            </body></html>''')\n+            </body></html>\"\"\"\n+        )\n         request = FormRequest.from_response(response)\n-        self.assertEqual(request.url, 'http://example.com/app')\n-        request = FormRequest.from_response(response, url='http://foo.bar/absolute')\n-        self.assertEqual(request.url, 'http://foo.bar/absolute')\n-        request = FormRequest.from_response(response, url='/relative')\n-        self.assertEqual(request.url, 'http://example.com/relative')\n+        self.assertEqual(request.url, \"http://example.com/app\")\n+        request = FormRequest.from_response(response, url=\"http://foo.bar/absolute\")\n+        self.assertEqual(request.url, \"http://foo.bar/absolute\")\n+        request = FormRequest.from_response(response, url=\"/relative\")\n+        self.assertEqual(request.url, \"http://example.com/relative\")\n \n     def test_from_response_case_insensitive(self):\n         response = _buildresponse(\n@@ -627,12 +716,13 @@ class FormRequestTest(RequestTest):\n             <input type=\"SuBmIt\" name=\"clickable1\" value=\"clicked1\">\n             <input type=\"iMaGe\" name=\"i1\" src=\"http://my.image.org/1.jpg\">\n             <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n-            </form>\"\"\")\n+            </form>\"\"\"\n+        )\n         req = self.request_class.from_response(response)\n         fs = _qs(req)\n-        self.assertEqual(fs[b'clickable1'], [b'clicked1'])\n-        self.assertFalse(b'i1' in fs, fs)  # xpath in _get_inputs()\n-        self.assertFalse(b'clickable2' in fs, fs)  # xpath in _get_clickable()\n+        self.assertEqual(fs[b\"clickable1\"], [b\"clicked1\"])\n+        self.assertFalse(b\"i1\" in fs, fs)  # xpath in _get_inputs()\n+        self.assertFalse(b\"clickable2\" in fs, fs)  # xpath in _get_clickable()\n \n     def test_from_response_submit_first_clickable(self):\n         response = _buildresponse(\n@@ -641,13 +731,14 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"one\" value=\"1\">\n             <input type=\"hidden\" name=\"two\" value=\"3\">\n             <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n-            </form>\"\"\")\n-        req = self.request_class.from_response(response, formdata={'two': '2'})\n+            </form>\"\"\"\n+        )\n+        req = self.request_class.from_response(response, formdata={\"two\": \"2\"})\n         fs = _qs(req)\n-        self.assertEqual(fs[b'clickable1'], [b'clicked1'])\n-        self.assertFalse(b'clickable2' in fs, fs)\n-        self.assertEqual(fs[b'one'], [b'1'])\n-        self.assertEqual(fs[b'two'], [b'2'])\n+        self.assertEqual(fs[b\"clickable1\"], [b\"clicked1\"])\n+        self.assertFalse(b\"clickable2\" in fs, fs)\n+        self.assertEqual(fs[b\"one\"], [b\"1\"])\n+        self.assertEqual(fs[b\"two\"], [b\"2\"])\n \n     def test_from_response_submit_not_first_clickable(self):\n         response = _buildresponse(\n@@ -656,15 +747,16 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"one\" value=\"1\">\n             <input type=\"hidden\" name=\"two\" value=\"3\">\n             <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n-            </form>\"\"\")\n+            </form>\"\"\"\n+        )\n         req = self.request_class.from_response(\n-            response, formdata={'two': '2'}, clickdata={'name': 'clickable2'}\n+            response, formdata={\"two\": \"2\"}, clickdata={\"name\": \"clickable2\"}\n         )\n         fs = _qs(req)\n-        self.assertEqual(fs[b'clickable2'], [b'clicked2'])\n-        self.assertFalse(b'clickable1' in fs, fs)\n-        self.assertEqual(fs[b'one'], [b'1'])\n-        self.assertEqual(fs[b'two'], [b'2'])\n+        self.assertEqual(fs[b\"clickable2\"], [b\"clicked2\"])\n+        self.assertFalse(b\"clickable1\" in fs, fs)\n+        self.assertEqual(fs[b\"one\"], [b\"1\"])\n+        self.assertEqual(fs[b\"two\"], [b\"2\"])\n \n     def test_from_response_dont_submit_image_as_input(self):\n         response = _buildresponse(\n@@ -672,10 +764,11 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"i1\" value=\"i1v\">\n             <input type=\"image\" name=\"i2\" src=\"http://my.image.org/1.jpg\">\n             <input type=\"submit\" name=\"i3\" value=\"i3v\">\n-            </form>\"\"\")\n+            </form>\"\"\"\n+        )\n         req = self.request_class.from_response(response, dont_click=True)\n         fs = _qs(req)\n-        self.assertEqual(fs, {b'i1': [b'i1v']})\n+        self.assertEqual(fs, {b\"i1\": [b\"i1v\"]})\n \n     def test_from_response_dont_submit_reset_as_input(self):\n         response = _buildresponse(\n@@ -684,20 +777,22 @@ class FormRequestTest(RequestTest):\n             <input type=\"text\" name=\"i2\" value=\"i2v\">\n             <input type=\"reset\" name=\"resetme\">\n             <input type=\"submit\" name=\"i3\" value=\"i3v\">\n-            </form>\"\"\")\n+            </form>\"\"\"\n+        )\n         req = self.request_class.from_response(response, dont_click=True)\n         fs = _qs(req)\n-        self.assertEqual(fs, {b'i1': [b'i1v'], b'i2': [b'i2v']})\n+        self.assertEqual(fs, {b\"i1\": [b\"i1v\"], b\"i2\": [b\"i2v\"]})\n \n     def test_from_response_clickdata_does_not_ignore_image(self):\n         response = _buildresponse(\n             \"\"\"<form>\n             <input type=\"text\" name=\"i1\" value=\"i1v\">\n             <input id=\"image\" name=\"i2\" type=\"image\" value=\"i2v\" alt=\"Login\" src=\"http://my.image.org/1.jpg\">\n-            </form>\"\"\")\n+            </form>\"\"\"\n+        )\n         req = self.request_class.from_response(response)\n         fs = _qs(req)\n-        self.assertEqual(fs, {b'i1': [b'i1v'], b'i2': [b'i2v']})\n+        self.assertEqual(fs, {b\"i1\": [b\"i1v\"], b\"i2\": [b\"i2v\"]})\n \n     def test_from_response_multiple_clickdata(self):\n         response = _buildresponse(\n@@ -706,14 +801,15 @@ class FormRequestTest(RequestTest):\n             <input type=\"submit\" name=\"clickable\" value=\"clicked2\">\n             <input type=\"hidden\" name=\"one\" value=\"clicked1\">\n             <input type=\"hidden\" name=\"two\" value=\"clicked2\">\n-            </form>\"\"\")\n+            </form>\"\"\"\n+        )\n         req = self.request_class.from_response(\n-            response, clickdata={'name': 'clickable', 'value': 'clicked2'}\n+            response, clickdata={\"name\": \"clickable\", \"value\": \"clicked2\"}\n         )\n         fs = _qs(req)\n-        self.assertEqual(fs[b'clickable'], [b'clicked2'])\n-        self.assertEqual(fs[b'one'], [b'clicked1'])\n-        self.assertEqual(fs[b'two'], [b'clicked2'])\n+        self.assertEqual(fs[b\"clickable\"], [b\"clicked2\"])\n+        self.assertEqual(fs[b\"one\"], [b\"clicked1\"])\n+        self.assertEqual(fs[b\"two\"], [b\"clicked2\"])\n \n     def test_from_response_unicode_clickdata(self):\n         response = _buildresponse(\n@@ -722,12 +818,13 @@ class FormRequestTest(RequestTest):\n             <input type=\"submit\" name=\"price in \\u20ac\" value=\"\\u20ac 2000\">\n             <input type=\"hidden\" name=\"poundsign\" value=\"\\u00a3\">\n             <input type=\"hidden\" name=\"eurosign\" value=\"\\u20ac\">\n-            </form>\"\"\")\n+            </form>\"\"\"\n+        )\n         req = self.request_class.from_response(\n-            response, clickdata={'name': 'price in \\u00a3'}\n+            response, clickdata={\"name\": \"price in \\u00a3\"}\n         )\n         fs = _qs(req, to_unicode=True)\n-        self.assertTrue(fs['price in \\u00a3'])\n+        self.assertTrue(fs[\"price in \\u00a3\"])\n \n     def test_from_response_unicode_clickdata_latin1(self):\n         response = _buildresponse(\n@@ -737,12 +834,13 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"poundsign\" value=\"\\u00a3\">\n             <input type=\"hidden\" name=\"yensign\" value=\"\\u00a5\">\n             </form>\"\"\",\n-            encoding='latin1')\n-        req = self.request_class.from_response(\n-            response, clickdata={'name': 'price in \\u00a5'}\n+            encoding=\"latin1\",\n         )\n-        fs = _qs(req, to_unicode=True, encoding='latin1')\n-        self.assertTrue(fs['price in \\u00a5'])\n+        req = self.request_class.from_response(\n+            response, clickdata={\"name\": \"price in \\u00a5\"}\n+        )\n+        fs = _qs(req, to_unicode=True, encoding=\"latin1\")\n+        self.assertTrue(fs[\"price in \\u00a5\"])\n \n     def test_from_response_multiple_forms_clickdata(self):\n         response = _buildresponse(\n@@ -754,22 +852,25 @@ class FormRequestTest(RequestTest):\n             <input type=\"submit\" name=\"clickable\" value=\"clicked2\">\n             <input type=\"hidden\" name=\"field2\" value=\"value2\">\n             </form>\n-            \"\"\")\n+            \"\"\"\n+        )\n         req = self.request_class.from_response(\n-            response, formname='form2', clickdata={'name': 'clickable'}\n+            response, formname=\"form2\", clickdata={\"name\": \"clickable\"}\n         )\n         fs = _qs(req)\n-        self.assertEqual(fs[b'clickable'], [b'clicked2'])\n-        self.assertEqual(fs[b'field2'], [b'value2'])\n-        self.assertFalse(b'field1' in fs, fs)\n+        self.assertEqual(fs[b\"clickable\"], [b\"clicked2\"])\n+        self.assertEqual(fs[b\"field2\"], [b\"value2\"])\n+        self.assertFalse(b\"field1\" in fs, fs)\n \n     def test_from_response_override_clickable(self):\n-        response = _buildresponse('''<form><input type=\"submit\" name=\"clickme\" value=\"one\"> </form>''')\n+        response = _buildresponse(\n+            \"\"\"<form><input type=\"submit\" name=\"clickme\" value=\"one\"> </form>\"\"\"\n+        )\n         req = self.request_class.from_response(\n-            response, formdata={'clickme': 'two'}, clickdata={'name': 'clickme'}\n+            response, formdata={\"clickme\": \"two\"}, clickdata={\"name\": \"clickme\"}\n         )\n         fs = _qs(req)\n-        self.assertEqual(fs[b'clickme'], [b'two'])\n+        self.assertEqual(fs[b\"clickme\"], [b\"two\"])\n \n     def test_from_response_dont_click(self):\n         response = _buildresponse(\n@@ -778,11 +879,12 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"one\" value=\"1\">\n             <input type=\"hidden\" name=\"two\" value=\"3\">\n             <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n-            </form>\"\"\")\n+            </form>\"\"\"\n+        )\n         r1 = self.request_class.from_response(response, dont_click=True)\n         fs = _qs(r1)\n-        self.assertFalse(b'clickable1' in fs, fs)\n-        self.assertFalse(b'clickable2' in fs, fs)\n+        self.assertFalse(b\"clickable1\" in fs, fs)\n+        self.assertFalse(b\"clickable2\" in fs, fs)\n \n     def test_from_response_ambiguous_clickdata(self):\n         response = _buildresponse(\n@@ -792,17 +894,27 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"one\" value=\"1\">\n             <input type=\"hidden\" name=\"two\" value=\"3\">\n             <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n-            </form>\"\"\")\n-        self.assertRaises(ValueError, self.request_class.from_response,\n-                          response, clickdata={'type': 'submit'})\n+            </form>\"\"\"\n+        )\n+        self.assertRaises(\n+            ValueError,\n+            self.request_class.from_response,\n+            response,\n+            clickdata={\"type\": \"submit\"},\n+        )\n \n     def test_from_response_non_matching_clickdata(self):\n         response = _buildresponse(\n             \"\"\"<form>\n             <input type=\"submit\" name=\"clickable\" value=\"clicked\">\n-            </form>\"\"\")\n-        self.assertRaises(ValueError, self.request_class.from_response,\n-                          response, clickdata={'nonexistent': 'notme'})\n+            </form>\"\"\"\n+        )\n+        self.assertRaises(\n+            ValueError,\n+            self.request_class.from_response,\n+            response,\n+            clickdata={\"nonexistent\": \"notme\"},\n+        )\n \n     def test_from_response_nr_index_clickdata(self):\n         response = _buildresponse(\n@@ -810,32 +922,37 @@ class FormRequestTest(RequestTest):\n             <input type=\"submit\" name=\"clickable1\" value=\"clicked1\">\n             <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n             </form>\n-            \"\"\")\n-        req = self.request_class.from_response(response, clickdata={'nr': 1})\n+            \"\"\"\n+        )\n+        req = self.request_class.from_response(response, clickdata={\"nr\": 1})\n         fs = _qs(req)\n-        self.assertIn(b'clickable2', fs)\n-        self.assertNotIn(b'clickable1', fs)\n+        self.assertIn(b\"clickable2\", fs)\n+        self.assertNotIn(b\"clickable1\", fs)\n \n     def test_from_response_invalid_nr_index_clickdata(self):\n         response = _buildresponse(\n             \"\"\"<form>\n             <input type=\"submit\" name=\"clickable\" value=\"clicked\">\n             </form>\n-            \"\"\")\n-        self.assertRaises(ValueError, self.request_class.from_response,\n-                          response, clickdata={'nr': 1})\n+            \"\"\"\n+        )\n+        self.assertRaises(\n+            ValueError, self.request_class.from_response, response, clickdata={\"nr\": 1}\n+        )\n \n     def test_from_response_errors_noform(self):\n         response = _buildresponse(\"\"\"<html></html>\"\"\")\n         self.assertRaises(ValueError, self.request_class.from_response, response)\n \n     def test_from_response_invalid_html5(self):\n-        response = _buildresponse(\"\"\"<!DOCTYPE html><body></html><form>\"\"\"\n+        response = _buildresponse(\n+            \"\"\"<!DOCTYPE html><body></html><form>\"\"\"\n             \"\"\"<input type=\"text\" name=\"foo\" value=\"xxx\">\"\"\"\n-                                  \"\"\"</form></body></html>\"\"\")\n-        req = self.request_class.from_response(response, formdata={'bar': 'buz'})\n+            \"\"\"</form></body></html>\"\"\"\n+        )\n+        req = self.request_class.from_response(response, formdata={\"bar\": \"buz\"})\n         fs = _qs(req)\n-        self.assertEqual(fs, {b'foo': [b'xxx'], b'bar': [b'buz']})\n+        self.assertEqual(fs, {b\"foo\": [b\"xxx\"], b\"bar\": [b\"buz\"]})\n \n     def test_from_response_errors_formnumber(self):\n         response = _buildresponse(\n@@ -843,20 +960,26 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"test\" value=\"val1\">\n             <input type=\"hidden\" name=\"test\" value=\"val2\">\n             <input type=\"hidden\" name=\"test2\" value=\"xxx\">\n-            </form>\"\"\")\n-        self.assertRaises(IndexError, self.request_class.from_response, response, formnumber=1)\n+            </form>\"\"\"\n+        )\n+        self.assertRaises(\n+            IndexError, self.request_class.from_response, response, formnumber=1\n+        )\n \n     def test_from_response_noformname(self):\n         response = _buildresponse(\n             \"\"\"<form action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"one\" value=\"1\">\n             <input type=\"hidden\" name=\"two\" value=\"2\">\n-            </form>\"\"\")\n-        r1 = self.request_class.from_response(response, formdata={'two': '3'})\n-        self.assertEqual(r1.method, 'POST')\n-        self.assertEqual(r1.headers['Content-type'], b'application/x-www-form-urlencoded')\n+            </form>\"\"\"\n+        )\n+        r1 = self.request_class.from_response(response, formdata={\"two\": \"3\"})\n+        self.assertEqual(r1.method, \"POST\")\n+        self.assertEqual(\n+            r1.headers[\"Content-type\"], b\"application/x-www-form-urlencoded\"\n+        )\n         fs = _qs(r1)\n-        self.assertEqual(fs, {b'one': [b'1'], b'two': [b'3']})\n+        self.assertEqual(fs, {b\"one\": [b\"1\"], b\"two\": [b\"3\"]})\n \n     def test_from_response_formname_exists(self):\n         response = _buildresponse(\n@@ -867,11 +990,12 @@ class FormRequestTest(RequestTest):\n             <form name=\"form2\" action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"three\" value=\"3\">\n             <input type=\"hidden\" name=\"four\" value=\"4\">\n-            </form>\"\"\")\n+            </form>\"\"\"\n+        )\n         r1 = self.request_class.from_response(response, formname=\"form2\")\n-        self.assertEqual(r1.method, 'POST')\n+        self.assertEqual(r1.method, \"POST\")\n         fs = _qs(r1)\n-        self.assertEqual(fs, {b'four': [b'4'], b'three': [b'3']})\n+        self.assertEqual(fs, {b\"four\": [b\"4\"], b\"three\": [b\"3\"]})\n \n     def test_from_response_formname_notexist(self):\n         response = _buildresponse(\n@@ -880,11 +1004,12 @@ class FormRequestTest(RequestTest):\n             </form>\n             <form name=\"form2\" action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"two\" value=\"2\">\n-            </form>\"\"\")\n+            </form>\"\"\"\n+        )\n         r1 = self.request_class.from_response(response, formname=\"form3\")\n-        self.assertEqual(r1.method, 'POST')\n+        self.assertEqual(r1.method, \"POST\")\n         fs = _qs(r1)\n-        self.assertEqual(fs, {b'one': [b'1']})\n+        self.assertEqual(fs, {b\"one\": [b\"1\"]})\n \n     def test_from_response_formname_errors_formnumber(self):\n         response = _buildresponse(\n@@ -893,9 +1018,15 @@ class FormRequestTest(RequestTest):\n             </form>\n             <form name=\"form2\" action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"two\" value=\"2\">\n-            </form>\"\"\")\n-        self.assertRaises(IndexError, self.request_class.from_response,\n-                          response, formname=\"form3\", formnumber=2)\n+            </form>\"\"\"\n+        )\n+        self.assertRaises(\n+            IndexError,\n+            self.request_class.from_response,\n+            response,\n+            formname=\"form3\",\n+            formnumber=2,\n+        )\n \n     def test_from_response_formid_exists(self):\n         response = _buildresponse(\n@@ -906,11 +1037,12 @@ class FormRequestTest(RequestTest):\n             <form id=\"form2\" action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"three\" value=\"3\">\n             <input type=\"hidden\" name=\"four\" value=\"4\">\n-            </form>\"\"\")\n+            </form>\"\"\"\n+        )\n         r1 = self.request_class.from_response(response, formid=\"form2\")\n-        self.assertEqual(r1.method, 'POST')\n+        self.assertEqual(r1.method, \"POST\")\n         fs = _qs(r1)\n-        self.assertEqual(fs, {b'four': [b'4'], b'three': [b'3']})\n+        self.assertEqual(fs, {b\"four\": [b\"4\"], b\"three\": [b\"3\"]})\n \n     def test_from_response_formname_notexists_fallback_formid(self):\n         response = _buildresponse(\n@@ -921,11 +1053,14 @@ class FormRequestTest(RequestTest):\n             <form id=\"form2\" name=\"form2\" action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"three\" value=\"3\">\n             <input type=\"hidden\" name=\"four\" value=\"4\">\n-            </form>\"\"\")\n-        r1 = self.request_class.from_response(response, formname=\"form3\", formid=\"form2\")\n-        self.assertEqual(r1.method, 'POST')\n+            </form>\"\"\"\n+        )\n+        r1 = self.request_class.from_response(\n+            response, formname=\"form3\", formid=\"form2\"\n+        )\n+        self.assertEqual(r1.method, \"POST\")\n         fs = _qs(r1)\n-        self.assertEqual(fs, {b'four': [b'4'], b'three': [b'3']})\n+        self.assertEqual(fs, {b\"four\": [b\"4\"], b\"three\": [b\"3\"]})\n \n     def test_from_response_formid_notexist(self):\n         response = _buildresponse(\n@@ -934,11 +1069,12 @@ class FormRequestTest(RequestTest):\n             </form>\n             <form id=\"form2\" action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"two\" value=\"2\">\n-            </form>\"\"\")\n+            </form>\"\"\"\n+        )\n         r1 = self.request_class.from_response(response, formid=\"form3\")\n-        self.assertEqual(r1.method, 'POST')\n+        self.assertEqual(r1.method, \"POST\")\n         fs = _qs(r1)\n-        self.assertEqual(fs, {b'one': [b'1']})\n+        self.assertEqual(fs, {b\"one\": [b\"1\"]})\n \n     def test_from_response_formid_errors_formnumber(self):\n         response = _buildresponse(\n@@ -947,13 +1083,19 @@ class FormRequestTest(RequestTest):\n             </form>\n             <form id=\"form2\" name=\"form2\" action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"two\" value=\"2\">\n-            </form>\"\"\")\n-        self.assertRaises(IndexError, self.request_class.from_response,\n-                          response, formid=\"form3\", formnumber=2)\n+            </form>\"\"\"\n+        )\n+        self.assertRaises(\n+            IndexError,\n+            self.request_class.from_response,\n+            response,\n+            formid=\"form3\",\n+            formnumber=2,\n+        )\n \n     def test_from_response_select(self):\n         res = _buildresponse(\n-            '''<form>\n+            \"\"\"<form>\n             <select name=\"i1\">\n                 <option value=\"i1v1\">option 1</option>\n                 <option value=\"i1v2\" selected>option 2</option>\n@@ -977,14 +1119,15 @@ class FormRequestTest(RequestTest):\n             </select>\n             <select name=\"i6\"></select>\n             <select name=\"i7\"/>\n-            </form>''')\n+            </form>\"\"\"\n+        )\n         req = self.request_class.from_response(res)\n         fs = _qs(req, to_unicode=True)\n-        self.assertEqual(fs, {'i1': ['i1v2'], 'i2': ['i2v1'], 'i4': ['i4v2', 'i4v3']})\n+        self.assertEqual(fs, {\"i1\": [\"i1v2\"], \"i2\": [\"i2v1\"], \"i4\": [\"i4v2\", \"i4v3\"]})\n \n     def test_from_response_radio(self):\n         res = _buildresponse(\n-            '''<form>\n+            \"\"\"<form>\n             <input type=\"radio\" name=\"i1\" value=\"i1v1\">\n             <input type=\"radio\" name=\"i1\" value=\"iv2\" checked>\n             <input type=\"radio\" name=\"i2\" checked>\n@@ -993,14 +1136,15 @@ class FormRequestTest(RequestTest):\n             <input type=\"radio\" name=\"i3\">\n             <input type=\"radio\" value=\"i4v1\">\n             <input type=\"radio\">\n-            </form>''')\n+            </form>\"\"\"\n+        )\n         req = self.request_class.from_response(res)\n         fs = _qs(req)\n-        self.assertEqual(fs, {b'i1': [b'iv2'], b'i2': [b'on']})\n+        self.assertEqual(fs, {b\"i1\": [b\"iv2\"], b\"i2\": [b\"on\"]})\n \n     def test_from_response_checkbox(self):\n         res = _buildresponse(\n-            '''<form>\n+            \"\"\"<form>\n             <input type=\"checkbox\" name=\"i1\" value=\"i1v1\">\n             <input type=\"checkbox\" name=\"i1\" value=\"iv2\" checked>\n             <input type=\"checkbox\" name=\"i2\" checked>\n@@ -1009,51 +1153,55 @@ class FormRequestTest(RequestTest):\n             <input type=\"checkbox\" name=\"i3\">\n             <input type=\"checkbox\" value=\"i4v1\">\n             <input type=\"checkbox\">\n-            </form>''')\n+            </form>\"\"\"\n+        )\n         req = self.request_class.from_response(res)\n         fs = _qs(req)\n-        self.assertEqual(fs, {b'i1': [b'iv2'], b'i2': [b'on']})\n+        self.assertEqual(fs, {b\"i1\": [b\"iv2\"], b\"i2\": [b\"on\"]})\n \n     def test_from_response_input_text(self):\n         res = _buildresponse(\n-            '''<form>\n+            \"\"\"<form>\n             <input type=\"text\" name=\"i1\" value=\"i1v1\">\n             <input type=\"text\" name=\"i2\">\n             <input type=\"text\" value=\"i3v1\">\n             <input type=\"text\">\n             <input name=\"i4\" value=\"i4v1\">\n-            </form>''')\n+            </form>\"\"\"\n+        )\n         req = self.request_class.from_response(res)\n         fs = _qs(req)\n-        self.assertEqual(fs, {b'i1': [b'i1v1'], b'i2': [b''], b'i4': [b'i4v1']})\n+        self.assertEqual(fs, {b\"i1\": [b\"i1v1\"], b\"i2\": [b\"\"], b\"i4\": [b\"i4v1\"]})\n \n     def test_from_response_input_hidden(self):\n         res = _buildresponse(\n-            '''<form>\n+            \"\"\"<form>\n             <input type=\"hidden\" name=\"i1\" value=\"i1v1\">\n             <input type=\"hidden\" name=\"i2\">\n             <input type=\"hidden\" value=\"i3v1\">\n             <input type=\"hidden\">\n-            </form>''')\n+            </form>\"\"\"\n+        )\n         req = self.request_class.from_response(res)\n         fs = _qs(req)\n-        self.assertEqual(fs, {b'i1': [b'i1v1'], b'i2': [b'']})\n+        self.assertEqual(fs, {b\"i1\": [b\"i1v1\"], b\"i2\": [b\"\"]})\n \n     def test_from_response_input_textarea(self):\n         res = _buildresponse(\n-            '''<form>\n+            \"\"\"<form>\n             <textarea name=\"i1\">i1v</textarea>\n             <textarea name=\"i2\"></textarea>\n             <textarea name=\"i3\"/>\n             <textarea>i4v</textarea>\n-            </form>''')\n+            </form>\"\"\"\n+        )\n         req = self.request_class.from_response(res)\n         fs = _qs(req)\n-        self.assertEqual(fs, {b'i1': [b'i1v'], b'i2': [b''], b'i3': [b'']})\n+        self.assertEqual(fs, {b\"i1\": [b\"i1v\"], b\"i2\": [b\"\"], b\"i3\": [b\"\"]})\n \n     def test_from_response_descendants(self):\n         res = _buildresponse(\n-            '''<form>\n+            \"\"\"<form>\n             <div>\n               <fieldset>\n                 <input type=\"text\" name=\"i1\">\n@@ -1067,10 +1215,11 @@ class FormRequestTest(RequestTest):\n               <input type=\"hidden\" name=\"h1\" value=\"h1v\">\n               </div>\n             <input type=\"hidden\" name=\"h2\" value=\"h2v\">\n-            </form>''')\n+            </form>\"\"\"\n+        )\n         req = self.request_class.from_response(res)\n         fs = _qs(req)\n-        self.assertEqual(set(fs), {b'h2', b'i2', b'i1', b'i3', b'h1', b'i5', b'i4'})\n+        self.assertEqual(set(fs), {b\"h2\", b\"i2\", b\"i1\", b\"i3\", b\"h1\", b\"i5\", b\"i4\"})\n \n     def test_from_response_xpath(self):\n         response = _buildresponse(\n@@ -1081,28 +1230,43 @@ class FormRequestTest(RequestTest):\n             <form action=\"post2.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"three\" value=\"3\">\n             <input type=\"hidden\" name=\"four\" value=\"4\">\n-            </form>\"\"\")\n-        r1 = self.request_class.from_response(response, formxpath=\"//form[@action='post.php']\")\n+            </form>\"\"\"\n+        )\n+        r1 = self.request_class.from_response(\n+            response, formxpath=\"//form[@action='post.php']\"\n+        )\n         fs = _qs(r1)\n-        self.assertEqual(fs[b'one'], [b'1'])\n+        self.assertEqual(fs[b\"one\"], [b\"1\"])\n \n-        r1 = self.request_class.from_response(response, formxpath=\"//form/input[@name='four']\")\n+        r1 = self.request_class.from_response(\n+            response, formxpath=\"//form/input[@name='four']\"\n+        )\n         fs = _qs(r1)\n-        self.assertEqual(fs[b'three'], [b'3'])\n+        self.assertEqual(fs[b\"three\"], [b\"3\"])\n \n-        self.assertRaises(ValueError, self.request_class.from_response,\n-                          response, formxpath=\"//form/input[@name='abc']\")\n+        self.assertRaises(\n+            ValueError,\n+            self.request_class.from_response,\n+            response,\n+            formxpath=\"//form/input[@name='abc']\",\n+        )\n \n     def test_from_response_unicode_xpath(self):\n         response = _buildresponse(b'<form name=\"\\xd1\\x8a\"></form>')\n-        r = self.request_class.from_response(response, formxpath=\"//form[@name='\\u044a']\")\n+        r = self.request_class.from_response(\n+            response, formxpath=\"//form[@name='\\u044a']\"\n+        )\n         fs = _qs(r)\n         self.assertEqual(fs, {})\n \n         xpath = \"//form[@name='\\u03b1']\"\n-        self.assertRaisesRegex(ValueError, re.escape(xpath),\n+        self.assertRaisesRegex(\n+            ValueError,\n+            re.escape(xpath),\n             self.request_class.from_response,\n-                               response, formxpath=xpath)\n+            response,\n+            formxpath=xpath,\n+        )\n \n     def test_from_response_button_submit(self):\n         response = _buildresponse(\n@@ -1111,15 +1275,18 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"test2\" value=\"val2\">\n             <button type=\"submit\" name=\"button1\" value=\"submit1\">Submit</button>\n             </form>\"\"\",\n-            url=\"http://www.example.com/this/list.html\")\n+            url=\"http://www.example.com/this/list.html\",\n+        )\n         req = self.request_class.from_response(response)\n-        self.assertEqual(req.method, 'POST')\n-        self.assertEqual(req.headers['Content-type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(req.method, \"POST\")\n+        self.assertEqual(\n+            req.headers[\"Content-type\"], b\"application/x-www-form-urlencoded\"\n+        )\n         self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n         fs = _qs(req)\n-        self.assertEqual(fs[b'test1'], [b'val1'])\n-        self.assertEqual(fs[b'test2'], [b'val2'])\n-        self.assertEqual(fs[b'button1'], [b'submit1'])\n+        self.assertEqual(fs[b\"test1\"], [b\"val1\"])\n+        self.assertEqual(fs[b\"test2\"], [b\"val2\"])\n+        self.assertEqual(fs[b\"button1\"], [b\"submit1\"])\n \n     def test_from_response_button_notype(self):\n         response = _buildresponse(\n@@ -1128,15 +1295,18 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"test2\" value=\"val2\">\n             <button name=\"button1\" value=\"submit1\">Submit</button>\n             </form>\"\"\",\n-            url=\"http://www.example.com/this/list.html\")\n+            url=\"http://www.example.com/this/list.html\",\n+        )\n         req = self.request_class.from_response(response)\n-        self.assertEqual(req.method, 'POST')\n-        self.assertEqual(req.headers['Content-type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(req.method, \"POST\")\n+        self.assertEqual(\n+            req.headers[\"Content-type\"], b\"application/x-www-form-urlencoded\"\n+        )\n         self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n         fs = _qs(req)\n-        self.assertEqual(fs[b'test1'], [b'val1'])\n-        self.assertEqual(fs[b'test2'], [b'val2'])\n-        self.assertEqual(fs[b'button1'], [b'submit1'])\n+        self.assertEqual(fs[b\"test1\"], [b\"val1\"])\n+        self.assertEqual(fs[b\"test2\"], [b\"val2\"])\n+        self.assertEqual(fs[b\"button1\"], [b\"submit1\"])\n \n     def test_from_response_submit_novalue(self):\n         response = _buildresponse(\n@@ -1145,15 +1315,18 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"test2\" value=\"val2\">\n             <input type=\"submit\" name=\"button1\">Submit</button>\n             </form>\"\"\",\n-            url=\"http://www.example.com/this/list.html\")\n+            url=\"http://www.example.com/this/list.html\",\n+        )\n         req = self.request_class.from_response(response)\n-        self.assertEqual(req.method, 'POST')\n-        self.assertEqual(req.headers['Content-type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(req.method, \"POST\")\n+        self.assertEqual(\n+            req.headers[\"Content-type\"], b\"application/x-www-form-urlencoded\"\n+        )\n         self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n         fs = _qs(req)\n-        self.assertEqual(fs[b'test1'], [b'val1'])\n-        self.assertEqual(fs[b'test2'], [b'val2'])\n-        self.assertEqual(fs[b'button1'], [b''])\n+        self.assertEqual(fs[b\"test1\"], [b\"val1\"])\n+        self.assertEqual(fs[b\"test2\"], [b\"val2\"])\n+        self.assertEqual(fs[b\"button1\"], [b\"\"])\n \n     def test_from_response_button_novalue(self):\n         response = _buildresponse(\n@@ -1162,15 +1335,18 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"test2\" value=\"val2\">\n             <button type=\"submit\" name=\"button1\">Submit</button>\n             </form>\"\"\",\n-            url=\"http://www.example.com/this/list.html\")\n+            url=\"http://www.example.com/this/list.html\",\n+        )\n         req = self.request_class.from_response(response)\n-        self.assertEqual(req.method, 'POST')\n-        self.assertEqual(req.headers['Content-type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(req.method, \"POST\")\n+        self.assertEqual(\n+            req.headers[\"Content-type\"], b\"application/x-www-form-urlencoded\"\n+        )\n         self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n         fs = _qs(req)\n-        self.assertEqual(fs[b'test1'], [b'val1'])\n-        self.assertEqual(fs[b'test2'], [b'val2'])\n-        self.assertEqual(fs[b'button1'], [b''])\n+        self.assertEqual(fs[b\"test1\"], [b\"val1\"])\n+        self.assertEqual(fs[b\"test2\"], [b\"val2\"])\n+        self.assertEqual(fs[b\"button1\"], [b\"\"])\n \n     def test_html_base_form_action(self):\n         response = _buildresponse(\n@@ -1185,15 +1361,15 @@ class FormRequestTest(RequestTest):\n                 </body>\n             </html>\n             \"\"\",\n-            url='http://a.com/'\n+            url=\"http://a.com/\",\n         )\n         req = self.request_class.from_response(response)\n-        self.assertEqual(req.url, 'http://b.com/test_form')\n+        self.assertEqual(req.url, \"http://b.com/test_form\")\n \n     def test_spaces_in_action(self):\n         resp = _buildresponse('<body><form action=\" path\\n\"></form></body>')\n         req = self.request_class.from_response(resp)\n-        self.assertEqual(req.url, 'http://example.com/path')\n+        self.assertEqual(req.url, \"http://example.com/path\")\n \n     def test_from_response_css(self):\n         response = _buildresponse(\n@@ -1204,44 +1380,53 @@ class FormRequestTest(RequestTest):\n             <form action=\"post2.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"three\" value=\"3\">\n             <input type=\"hidden\" name=\"four\" value=\"4\">\n-            </form>\"\"\")\n-        r1 = self.request_class.from_response(response, formcss=\"form[action='post.php']\")\n+            </form>\"\"\"\n+        )\n+        r1 = self.request_class.from_response(\n+            response, formcss=\"form[action='post.php']\"\n+        )\n         fs = _qs(r1)\n-        self.assertEqual(fs[b'one'], [b'1'])\n+        self.assertEqual(fs[b\"one\"], [b\"1\"])\n \n         r1 = self.request_class.from_response(response, formcss=\"input[name='four']\")\n         fs = _qs(r1)\n-        self.assertEqual(fs[b'three'], [b'3'])\n+        self.assertEqual(fs[b\"three\"], [b\"3\"])\n \n-        self.assertRaises(ValueError, self.request_class.from_response,\n-                          response, formcss=\"input[name='abc']\")\n+        self.assertRaises(\n+            ValueError,\n+            self.request_class.from_response,\n+            response,\n+            formcss=\"input[name='abc']\",\n+        )\n \n     def test_from_response_valid_form_methods(self):\n-        form_methods = [[method, method] for method in self.request_class.valid_form_methods]\n-        form_methods.append(['UNKNOWN', 'GET'])\n+        form_methods = [\n+            [method, method] for method in self.request_class.valid_form_methods\n+        ]\n+        form_methods.append([\"UNKNOWN\", \"GET\"])\n \n         for method, expected in form_methods:\n             response = _buildresponse(\n                 f'<form action=\"post.php\" method=\"{method}\">'\n                 '<input type=\"hidden\" name=\"one\" value=\"1\">'\n-                '</form>'\n+                \"</form>\"\n             )\n             r = self.request_class.from_response(response)\n             self.assertEqual(r.method, expected)\n \n \n def _buildresponse(body, **kwargs):\n-    kwargs.setdefault('body', body)\n-    kwargs.setdefault('url', 'http://example.com')\n-    kwargs.setdefault('encoding', 'utf-8')\n+    kwargs.setdefault(\"body\", body)\n+    kwargs.setdefault(\"url\", \"http://example.com\")\n+    kwargs.setdefault(\"encoding\", \"utf-8\")\n     return HtmlResponse(**kwargs)\n \n \n-def _qs(req, encoding='utf-8', to_unicode=False):\n-    if req.method == 'POST':\n+def _qs(req, encoding=\"utf-8\", to_unicode=False):\n+    if req.method == \"POST\":\n         qs = req.body\n     else:\n-        qs = req.url.partition('?')[2]\n+        qs = req.url.partition(\"?\")[2]\n     uqs = unquote_to_bytes(qs)\n     if to_unicode:\n         uqs = uqs.decode(encoding)\n@@ -1251,38 +1436,41 @@ def _qs(req, encoding='utf-8', to_unicode=False):\n class XmlRpcRequestTest(RequestTest):\n \n     request_class = XmlRpcRequest\n-    default_method = 'POST'\n-    default_headers = {b'Content-Type': [b'text/xml']}\n+    default_method = \"POST\"\n+    default_headers = {b\"Content-Type\": [b\"text/xml\"]}\n \n     def _test_request(self, **kwargs):\n-        r = self.request_class('http://scrapytest.org/rpc2', **kwargs)\n-        self.assertEqual(r.headers[b'Content-Type'], b'text/xml')\n-        self.assertEqual(r.body,\n-                         to_bytes(xmlrpc.client.dumps(**kwargs),\n-                                  encoding=kwargs.get('encoding', 'utf-8')))\n-        self.assertEqual(r.method, 'POST')\n-        self.assertEqual(r.encoding, kwargs.get('encoding', 'utf-8'))\n+        r = self.request_class(\"http://scrapytest.org/rpc2\", **kwargs)\n+        self.assertEqual(r.headers[b\"Content-Type\"], b\"text/xml\")\n+        self.assertEqual(\n+            r.body,\n+            to_bytes(\n+                xmlrpc.client.dumps(**kwargs), encoding=kwargs.get(\"encoding\", \"utf-8\")\n+            ),\n+        )\n+        self.assertEqual(r.method, \"POST\")\n+        self.assertEqual(r.encoding, kwargs.get(\"encoding\", \"utf-8\"))\n         self.assertTrue(r.dont_filter, True)\n \n     def test_xmlrpc_dumps(self):\n-        self._test_request(params=('value',))\n-        self._test_request(params=('username', 'password'), methodname='login')\n-        self._test_request(params=('response', ), methodresponse='login')\n-        self._test_request(params=('pas£',), encoding='utf-8')\n+        self._test_request(params=(\"value\",))\n+        self._test_request(params=(\"username\", \"password\"), methodname=\"login\")\n+        self._test_request(params=(\"response\",), methodresponse=\"login\")\n+        self._test_request(params=(\"pas£\",), encoding=\"utf-8\")\n         self._test_request(params=(None,), allow_none=1)\n         self.assertRaises(TypeError, self._test_request)\n         self.assertRaises(TypeError, self._test_request, params=(None,))\n \n     def test_latin1(self):\n-        self._test_request(params=('pas£',), encoding='latin1')\n+        self._test_request(params=(\"pas£\",), encoding=\"latin1\")\n \n \n class JsonRequestTest(RequestTest):\n     request_class = JsonRequest\n-    default_method = 'GET'\n+    default_method = \"GET\"\n     default_headers = {\n-        b'Content-Type': [b'application/json'],\n-        b'Accept': [b'application/json, text/javascript, */*; q=0.01'],\n+        b\"Content-Type\": [b\"application/json\"],\n+        b\"Accept\": [b\"application/json, text/javascript, */*; q=0.01\"],\n     }\n \n     def setUp(self):\n@@ -1291,14 +1479,14 @@ class JsonRequestTest(RequestTest):\n \n     def test_data(self):\n         r1 = self.request_class(url=\"http://www.example.com/\")\n-        self.assertEqual(r1.body, b'')\n+        self.assertEqual(r1.body, b\"\")\n \n-        body = b'body'\n+        body = b\"body\"\n         r2 = self.request_class(url=\"http://www.example.com/\", body=body)\n         self.assertEqual(r2.body, body)\n \n         data = {\n-            'name': 'value',\n+            \"name\": \"value\",\n         }\n         r3 = self.request_class(url=\"http://www.example.com/\", data=data)\n         self.assertEqual(r3.body, to_bytes(json.dumps(data)))\n@@ -1310,97 +1498,99 @@ class JsonRequestTest(RequestTest):\n     def test_data_method(self):\n         # data is not passed\n         r1 = self.request_class(url=\"http://www.example.com/\")\n-        self.assertEqual(r1.method, 'GET')\n+        self.assertEqual(r1.method, \"GET\")\n \n-        body = b'body'\n+        body = b\"body\"\n         r2 = self.request_class(url=\"http://www.example.com/\", body=body)\n-        self.assertEqual(r2.method, 'GET')\n+        self.assertEqual(r2.method, \"GET\")\n \n         data = {\n-            'name': 'value',\n+            \"name\": \"value\",\n         }\n         r3 = self.request_class(url=\"http://www.example.com/\", data=data)\n-        self.assertEqual(r3.method, 'POST')\n+        self.assertEqual(r3.method, \"POST\")\n \n         # method passed explicitly\n-        r4 = self.request_class(url=\"http://www.example.com/\", data=data, method='GET')\n-        self.assertEqual(r4.method, 'GET')\n+        r4 = self.request_class(url=\"http://www.example.com/\", data=data, method=\"GET\")\n+        self.assertEqual(r4.method, \"GET\")\n \n         r5 = self.request_class(url=\"http://www.example.com/\", data=[])\n-        self.assertEqual(r5.method, 'POST')\n+        self.assertEqual(r5.method, \"POST\")\n \n     def test_body_data(self):\n         \"\"\"passing both body and data should result a warning\"\"\"\n-        body = b'body'\n+        body = b\"body\"\n         data = {\n-            'name': 'value',\n+            \"name\": \"value\",\n         }\n         with warnings.catch_warnings(record=True) as _warnings:\n             r5 = self.request_class(url=\"http://www.example.com/\", body=body, data=data)\n             self.assertEqual(r5.body, body)\n-            self.assertEqual(r5.method, 'GET')\n+            self.assertEqual(r5.method, \"GET\")\n             self.assertEqual(len(_warnings), 1)\n-            self.assertIn('data will be ignored', str(_warnings[0].message))\n+            self.assertIn(\"data will be ignored\", str(_warnings[0].message))\n \n     def test_empty_body_data(self):\n         \"\"\"passing any body value and data should result a warning\"\"\"\n         data = {\n-            'name': 'value',\n+            \"name\": \"value\",\n         }\n         with warnings.catch_warnings(record=True) as _warnings:\n-            r6 = self.request_class(url=\"http://www.example.com/\", body=b'', data=data)\n-            self.assertEqual(r6.body, b'')\n-            self.assertEqual(r6.method, 'GET')\n+            r6 = self.request_class(url=\"http://www.example.com/\", body=b\"\", data=data)\n+            self.assertEqual(r6.body, b\"\")\n+            self.assertEqual(r6.method, \"GET\")\n             self.assertEqual(len(_warnings), 1)\n-            self.assertIn('data will be ignored', str(_warnings[0].message))\n+            self.assertIn(\"data will be ignored\", str(_warnings[0].message))\n \n     def test_body_none_data(self):\n         data = {\n-            'name': 'value',\n+            \"name\": \"value\",\n         }\n         with warnings.catch_warnings(record=True) as _warnings:\n             r7 = self.request_class(url=\"http://www.example.com/\", body=None, data=data)\n             self.assertEqual(r7.body, to_bytes(json.dumps(data)))\n-            self.assertEqual(r7.method, 'POST')\n+            self.assertEqual(r7.method, \"POST\")\n             self.assertEqual(len(_warnings), 0)\n \n     def test_body_data_none(self):\n         with warnings.catch_warnings(record=True) as _warnings:\n             r8 = self.request_class(url=\"http://www.example.com/\", body=None, data=None)\n-            self.assertEqual(r8.method, 'GET')\n+            self.assertEqual(r8.method, \"GET\")\n             self.assertEqual(len(_warnings), 0)\n \n     def test_dumps_sort_keys(self):\n         \"\"\"Test that sort_keys=True is passed to json.dumps by default\"\"\"\n         data = {\n-            'name': 'value',\n+            \"name\": \"value\",\n         }\n-        with mock.patch('json.dumps', return_value=b'') as mock_dumps:\n+        with mock.patch(\"json.dumps\", return_value=b\"\") as mock_dumps:\n             self.request_class(url=\"http://www.example.com/\", data=data)\n             kwargs = mock_dumps.call_args[1]\n-            self.assertEqual(kwargs['sort_keys'], True)\n+            self.assertEqual(kwargs[\"sort_keys\"], True)\n \n     def test_dumps_kwargs(self):\n         \"\"\"Test that dumps_kwargs are passed to json.dumps\"\"\"\n         data = {\n-            'name': 'value',\n+            \"name\": \"value\",\n         }\n         dumps_kwargs = {\n-            'ensure_ascii': True,\n-            'allow_nan': True,\n+            \"ensure_ascii\": True,\n+            \"allow_nan\": True,\n         }\n-        with mock.patch('json.dumps', return_value=b'') as mock_dumps:\n-            self.request_class(url=\"http://www.example.com/\", data=data, dumps_kwargs=dumps_kwargs)\n+        with mock.patch(\"json.dumps\", return_value=b\"\") as mock_dumps:\n+            self.request_class(\n+                url=\"http://www.example.com/\", data=data, dumps_kwargs=dumps_kwargs\n+            )\n             kwargs = mock_dumps.call_args[1]\n-            self.assertEqual(kwargs['ensure_ascii'], True)\n-            self.assertEqual(kwargs['allow_nan'], True)\n+            self.assertEqual(kwargs[\"ensure_ascii\"], True)\n+            self.assertEqual(kwargs[\"allow_nan\"], True)\n \n     def test_replace_data(self):\n         data1 = {\n-            'name1': 'value1',\n+            \"name1\": \"value1\",\n         }\n         data2 = {\n-            'name2': 'value2',\n+            \"name2\": \"value2\",\n         }\n         r1 = self.request_class(url=\"http://www.example.com/\", data=data1)\n         r2 = r1.replace(data=data2)\n@@ -1409,35 +1599,37 @@ class JsonRequestTest(RequestTest):\n     def test_replace_sort_keys(self):\n         \"\"\"Test that replace provides sort_keys=True to json.dumps\"\"\"\n         data1 = {\n-            'name1': 'value1',\n+            \"name1\": \"value1\",\n         }\n         data2 = {\n-            'name2': 'value2',\n+            \"name2\": \"value2\",\n         }\n         r1 = self.request_class(url=\"http://www.example.com/\", data=data1)\n-        with mock.patch('json.dumps', return_value=b'') as mock_dumps:\n+        with mock.patch(\"json.dumps\", return_value=b\"\") as mock_dumps:\n             r1.replace(data=data2)\n             kwargs = mock_dumps.call_args[1]\n-            self.assertEqual(kwargs['sort_keys'], True)\n+            self.assertEqual(kwargs[\"sort_keys\"], True)\n \n     def test_replace_dumps_kwargs(self):\n         \"\"\"Test that dumps_kwargs are provided to json.dumps when replace is called\"\"\"\n         data1 = {\n-            'name1': 'value1',\n+            \"name1\": \"value1\",\n         }\n         data2 = {\n-            'name2': 'value2',\n+            \"name2\": \"value2\",\n         }\n         dumps_kwargs = {\n-            'ensure_ascii': True,\n-            'allow_nan': True,\n+            \"ensure_ascii\": True,\n+            \"allow_nan\": True,\n         }\n-        r1 = self.request_class(url=\"http://www.example.com/\", data=data1, dumps_kwargs=dumps_kwargs)\n-        with mock.patch('json.dumps', return_value=b'') as mock_dumps:\n+        r1 = self.request_class(\n+            url=\"http://www.example.com/\", data=data1, dumps_kwargs=dumps_kwargs\n+        )\n+        with mock.patch(\"json.dumps\", return_value=b\"\") as mock_dumps:\n             r1.replace(data=data2)\n             kwargs = mock_dumps.call_args[1]\n-            self.assertEqual(kwargs['ensure_ascii'], True)\n-            self.assertEqual(kwargs['allow_nan'], True)\n+            self.assertEqual(kwargs[\"ensure_ascii\"], True)\n+            self.assertEqual(kwargs[\"allow_nan\"], True)\n \n     def tearDown(self):\n         warnings.resetwarnings()\n\n@@ -4,8 +4,14 @@ from unittest import mock\n \n from w3lib.encoding import resolve_encoding\n \n-from scrapy.http import (Request, Response, TextResponse, HtmlResponse,\n-                         XmlResponse, Headers)\n+from scrapy.http import (\n+    Request,\n+    Response,\n+    TextResponse,\n+    HtmlResponse,\n+    XmlResponse,\n+    Headers,\n+)\n from scrapy.selector import Selector\n from scrapy.utils.python import to_unicode\n from scrapy.exceptions import NotSupported\n@@ -20,15 +26,30 @@ class BaseResponseTest(unittest.TestCase):\n     def test_init(self):\n         # Response requires url in the constructor\n         self.assertRaises(Exception, self.response_class)\n-        self.assertTrue(isinstance(self.response_class('http://example.com/'), self.response_class))\n+        self.assertTrue(\n+            isinstance(self.response_class(\"http://example.com/\"), self.response_class)\n+        )\n         self.assertRaises(TypeError, self.response_class, b\"http://example.com\")\n         # body can be str or None\n-        self.assertTrue(isinstance(self.response_class('http://example.com/', body=b''), self.response_class))\n-        self.assertTrue(isinstance(self.response_class('http://example.com/', body=b'body'), self.response_class))\n+        self.assertTrue(\n+            isinstance(\n+                self.response_class(\"http://example.com/\", body=b\"\"),\n+                self.response_class,\n+            )\n+        )\n+        self.assertTrue(\n+            isinstance(\n+                self.response_class(\"http://example.com/\", body=b\"body\"),\n+                self.response_class,\n+            )\n+        )\n         # test presence of all optional parameters\n         self.assertTrue(\n             isinstance(\n-                self.response_class('http://example.com/', body=b'', headers={}, status=200), self.response_class\n+                self.response_class(\n+                    \"http://example.com/\", body=b\"\", headers={}, status=200\n+                ),\n+                self.response_class,\n             )\n         )\n \n@@ -49,15 +70,17 @@ class BaseResponseTest(unittest.TestCase):\n \n         r = self.response_class(\"http://www.example.com\", status=301)\n         self.assertEqual(r.status, 301)\n-        r = self.response_class(\"http://www.example.com\", status='301')\n+        r = self.response_class(\"http://www.example.com\", status=\"301\")\n         self.assertEqual(r.status, 301)\n-        self.assertRaises(ValueError, self.response_class, \"http://example.com\", status='lala200')\n+        self.assertRaises(\n+            ValueError, self.response_class, \"http://example.com\", status=\"lala200\"\n+        )\n \n     def test_copy(self):\n         \"\"\"Test Response copy\"\"\"\n \n         r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\")\n-        r1.flags.append('cached')\n+        r1.flags.append(\"cached\")\n         r2 = r1.copy()\n \n         self.assertEqual(r1.status, r2.status)\n@@ -68,29 +91,37 @@ class BaseResponseTest(unittest.TestCase):\n         self.assertEqual(r1.flags, r2.flags)\n \n         # make sure headers attribute is shallow copied\n-        assert r1.headers is not r2.headers, \"headers must be a shallow copy, not identical\"\n+        assert (\n+            r1.headers is not r2.headers\n+        ), \"headers must be a shallow copy, not identical\"\n         self.assertEqual(r1.headers, r2.headers)\n \n     def test_copy_meta(self):\n         req = Request(\"http://www.example.com\")\n-        req.meta['foo'] = 'bar'\n-        r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\", request=req)\n+        req.meta[\"foo\"] = \"bar\"\n+        r1 = self.response_class(\n+            \"http://www.example.com\", body=b\"Some body\", request=req\n+        )\n         assert r1.meta is req.meta\n \n     def test_copy_cb_kwargs(self):\n         req = Request(\"http://www.example.com\")\n-        req.cb_kwargs['foo'] = 'bar'\n-        r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\", request=req)\n+        req.cb_kwargs[\"foo\"] = \"bar\"\n+        r1 = self.response_class(\n+            \"http://www.example.com\", body=b\"Some body\", request=req\n+        )\n         assert r1.cb_kwargs is req.cb_kwargs\n \n     def test_unavailable_meta(self):\n         r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\")\n-        with self.assertRaisesRegex(AttributeError, r'Response\\.meta not available'):\n+        with self.assertRaisesRegex(AttributeError, r\"Response\\.meta not available\"):\n             r1.meta\n \n     def test_unavailable_cb_kwargs(self):\n         r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\")\n-        with self.assertRaisesRegex(AttributeError, r'Response\\.cb_kwargs not available'):\n+        with self.assertRaisesRegex(\n+            AttributeError, r\"Response\\.cb_kwargs not available\"\n+        ):\n             r1.cb_kwargs\n \n     def test_copy_inherited_classes(self):\n@@ -99,7 +130,7 @@ class BaseResponseTest(unittest.TestCase):\n         class CustomResponse(self.response_class):\n             pass\n \n-        r1 = CustomResponse('http://www.example.com')\n+        r1 = CustomResponse(\"http://www.example.com\")\n         r2 = r1.copy()\n \n         assert isinstance(r2, CustomResponse)\n@@ -109,16 +140,16 @@ class BaseResponseTest(unittest.TestCase):\n         hdrs = Headers({\"key\": \"value\"})\n         r1 = self.response_class(\"http://www.example.com\")\n         r2 = r1.replace(status=301, body=b\"New body\", headers=hdrs)\n-        assert r1.body == b''\n+        assert r1.body == b\"\"\n         self.assertEqual(r1.url, r2.url)\n         self.assertEqual((r1.status, r2.status), (200, 301))\n-        self.assertEqual((r1.body, r2.body), (b'', b\"New body\"))\n+        self.assertEqual((r1.body, r2.body), (b\"\", b\"New body\"))\n         self.assertEqual((r1.headers, r2.headers), ({}, hdrs))\n \n         # Empty attributes (which may fail if not compared properly)\n-        r3 = self.response_class(\"http://www.example.com\", flags=['cached'])\n-        r4 = r3.replace(body=b'', flags=[])\n-        self.assertEqual(r4.body, b'')\n+        r3 = self.response_class(\"http://www.example.com\", flags=[\"cached\"])\n+        r4 = r3.replace(body=b\"\", flags=[])\n+        self.assertEqual(r4.body, b\"\")\n         self.assertEqual(r4.flags, [])\n \n     def _assert_response_values(self, response, encoding, body):\n@@ -140,81 +171,84 @@ class BaseResponseTest(unittest.TestCase):\n \n     def test_immutable_attributes(self):\n         r = self.response_class(\"http://example.com\")\n-        self.assertRaises(AttributeError, setattr, r, 'url', 'http://example2.com')\n-        self.assertRaises(AttributeError, setattr, r, 'body', 'xxx')\n+        self.assertRaises(AttributeError, setattr, r, \"url\", \"http://example2.com\")\n+        self.assertRaises(AttributeError, setattr, r, \"body\", \"xxx\")\n \n     def test_urljoin(self):\n         \"\"\"Test urljoin shortcut (only for existence, since behavior equals urljoin)\"\"\"\n-        joined = self.response_class('http://www.example.com').urljoin('/test')\n-        absolute = 'http://www.example.com/test'\n+        joined = self.response_class(\"http://www.example.com\").urljoin(\"/test\")\n+        absolute = \"http://www.example.com/test\"\n         self.assertEqual(joined, absolute)\n \n     def test_shortcut_attributes(self):\n-        r = self.response_class(\"http://example.com\", body=b'hello')\n+        r = self.response_class(\"http://example.com\", body=b\"hello\")\n         if self.response_class == Response:\n             msg = \"Response content isn't text\"\n-            self.assertRaisesRegex(AttributeError, msg, getattr, r, 'text')\n-            self.assertRaisesRegex(NotSupported, msg, r.css, 'body')\n-            self.assertRaisesRegex(NotSupported, msg, r.xpath, '//body')\n+            self.assertRaisesRegex(AttributeError, msg, getattr, r, \"text\")\n+            self.assertRaisesRegex(NotSupported, msg, r.css, \"body\")\n+            self.assertRaisesRegex(NotSupported, msg, r.xpath, \"//body\")\n         else:\n             r.text\n-            r.css('body')\n-            r.xpath('//body')\n+            r.css(\"body\")\n+            r.xpath(\"//body\")\n \n     # Response.follow\n \n     def test_follow_url_absolute(self):\n-        self._assert_followed_url('http://foo.example.com',\n-                                  'http://foo.example.com')\n+        self._assert_followed_url(\"http://foo.example.com\", \"http://foo.example.com\")\n \n     def test_follow_url_relative(self):\n-        self._assert_followed_url('foo',\n-                                  'http://example.com/foo')\n+        self._assert_followed_url(\"foo\", \"http://example.com/foo\")\n \n     def test_follow_link(self):\n-        self._assert_followed_url(Link('http://example.com/foo'),\n-                                  'http://example.com/foo')\n+        self._assert_followed_url(\n+            Link(\"http://example.com/foo\"), \"http://example.com/foo\"\n+        )\n \n     def test_follow_None_url(self):\n         r = self.response_class(\"http://example.com\")\n         self.assertRaises(ValueError, r.follow, None)\n \n     def test_follow_whitespace_url(self):\n-        self._assert_followed_url('foo ',\n-                                  'http://example.com/foo%20')\n+        self._assert_followed_url(\"foo \", \"http://example.com/foo%20\")\n \n     def test_follow_whitespace_link(self):\n-        self._assert_followed_url(Link('http://example.com/foo '),\n-                                  'http://example.com/foo%20')\n+        self._assert_followed_url(\n+            Link(\"http://example.com/foo \"), \"http://example.com/foo%20\"\n+        )\n \n     def test_follow_flags(self):\n-        res = self.response_class('http://example.com/')\n-        fol = res.follow('http://example.com/', flags=['cached', 'allowed'])\n-        self.assertEqual(fol.flags, ['cached', 'allowed'])\n+        res = self.response_class(\"http://example.com/\")\n+        fol = res.follow(\"http://example.com/\", flags=[\"cached\", \"allowed\"])\n+        self.assertEqual(fol.flags, [\"cached\", \"allowed\"])\n \n     # Response.follow_all\n \n     def test_follow_all_absolute(self):\n-        url_list = ['http://example.org', 'http://www.example.org',\n-                    'http://example.com', 'http://www.example.com']\n+        url_list = [\n+            \"http://example.org\",\n+            \"http://www.example.org\",\n+            \"http://example.com\",\n+            \"http://www.example.com\",\n+        ]\n         self._assert_followed_all_urls(url_list, url_list)\n \n     def test_follow_all_relative(self):\n-        relative = ['foo', 'bar', 'foo/bar', 'bar/foo']\n+        relative = [\"foo\", \"bar\", \"foo/bar\", \"bar/foo\"]\n         absolute = [\n-            'http://example.com/foo',\n-            'http://example.com/bar',\n-            'http://example.com/foo/bar',\n-            'http://example.com/bar/foo',\n+            \"http://example.com/foo\",\n+            \"http://example.com/bar\",\n+            \"http://example.com/foo/bar\",\n+            \"http://example.com/bar/foo\",\n         ]\n         self._assert_followed_all_urls(relative, absolute)\n \n     def test_follow_all_links(self):\n         absolute = [\n-            'http://example.com/foo',\n-            'http://example.com/bar',\n-            'http://example.com/foo/bar',\n-            'http://example.com/bar/foo',\n+            \"http://example.com/foo\",\n+            \"http://example.com/bar\",\n+            \"http://example.com/foo/bar\",\n+            \"http://example.com/bar/foo\",\n         ]\n         links = map(Link, absolute)\n         self._assert_followed_all_urls(links, absolute)\n@@ -241,36 +275,36 @@ class BaseResponseTest(unittest.TestCase):\n                 list(r.follow_all(urls=[None]))\n \n     def test_follow_all_whitespace(self):\n-        relative = ['foo ', 'bar ', 'foo/bar ', 'bar/foo ']\n+        relative = [\"foo \", \"bar \", \"foo/bar \", \"bar/foo \"]\n         absolute = [\n-            'http://example.com/foo%20',\n-            'http://example.com/bar%20',\n-            'http://example.com/foo/bar%20',\n-            'http://example.com/bar/foo%20',\n+            \"http://example.com/foo%20\",\n+            \"http://example.com/bar%20\",\n+            \"http://example.com/foo/bar%20\",\n+            \"http://example.com/bar/foo%20\",\n         ]\n         self._assert_followed_all_urls(relative, absolute)\n \n     def test_follow_all_whitespace_links(self):\n         absolute = [\n-            'http://example.com/foo ',\n-            'http://example.com/bar ',\n-            'http://example.com/foo/bar ',\n-            'http://example.com/bar/foo ',\n+            \"http://example.com/foo \",\n+            \"http://example.com/bar \",\n+            \"http://example.com/foo/bar \",\n+            \"http://example.com/bar/foo \",\n         ]\n         links = map(Link, absolute)\n-        expected = [u.replace(' ', '%20') for u in absolute]\n+        expected = [u.replace(\" \", \"%20\") for u in absolute]\n         self._assert_followed_all_urls(links, expected)\n \n     def test_follow_all_flags(self):\n-        re = self.response_class('http://www.example.com/')\n+        re = self.response_class(\"http://www.example.com/\")\n         urls = [\n-            'http://www.example.com/',\n-            'http://www.example.com/2',\n-            'http://www.example.com/foo',\n+            \"http://www.example.com/\",\n+            \"http://www.example.com/2\",\n+            \"http://www.example.com/foo\",\n         ]\n-        fol = re.follow_all(urls, flags=['cached', 'allowed'])\n+        fol = re.follow_all(urls, flags=[\"cached\", \"allowed\"])\n         for req in fol:\n-            self.assertEqual(req.flags, ['cached', 'allowed'])\n+            self.assertEqual(req.flags, [\"cached\", \"allowed\"])\n \n     def _assert_followed_url(self, follow_obj, target_url, response=None):\n         if response is None:\n@@ -288,13 +322,13 @@ class BaseResponseTest(unittest.TestCase):\n             yield req\n \n     def _links_response(self):\n-        body = get_testdata('link_extractor', 'linkextractor.html')\n-        resp = self.response_class('http://example.com/index', body=body)\n+        body = get_testdata(\"link_extractor\", \"linkextractor.html\")\n+        resp = self.response_class(\"http://example.com/index\", body=body)\n         return resp\n \n     def _links_response_no_href(self):\n-        body = get_testdata('link_extractor', 'linkextractor_no_href.html')\n-        resp = self.response_class('http://example.com/index', body=body)\n+        body = get_testdata(\"link_extractor\", \"linkextractor_no_href.html\")\n+        resp = self.response_class(\"http://example.com/index\", body=body)\n         return resp\n \n \n@@ -304,7 +338,9 @@ class TextResponseTest(BaseResponseTest):\n \n     def test_replace(self):\n         super().test_replace()\n-        r1 = self.response_class(\"http://www.example.com\", body=\"hello\", encoding=\"cp852\")\n+        r1 = self.response_class(\n+            \"http://www.example.com\", body=\"hello\", encoding=\"cp852\"\n+        )\n         r2 = r1.replace(url=\"http://www.example.com/other\")\n         r3 = r1.replace(url=\"http://www.example.com/other\", encoding=\"latin1\")\n \n@@ -320,52 +356,89 @@ class TextResponseTest(BaseResponseTest):\n         self._assert_response_encoding(resp, self.response_class._DEFAULT_ENCODING)\n \n         # make sure urls are converted to str\n-        resp = self.response_class(url=\"http://www.example.com/\", encoding='utf-8')\n+        resp = self.response_class(url=\"http://www.example.com/\", encoding=\"utf-8\")\n         assert isinstance(resp.url, str)\n \n-        resp = self.response_class(url=\"http://www.example.com/price/\\xa3\", encoding='utf-8')\n-        self.assertEqual(resp.url, to_unicode(b'http://www.example.com/price/\\xc2\\xa3'))\n-        resp = self.response_class(url=\"http://www.example.com/price/\\xa3\", encoding='latin-1')\n-        self.assertEqual(resp.url, 'http://www.example.com/price/\\xa3')\n-        resp = self.response_class(\"http://www.example.com/price/\\xa3\",\n-                                   headers={\"Content-type\": [\"text/html; charset=utf-8\"]})\n-        self.assertEqual(resp.url, to_unicode(b'http://www.example.com/price/\\xc2\\xa3'))\n-        resp = self.response_class(\"http://www.example.com/price/\\xa3\",\n-                                   headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]})\n-        self.assertEqual(resp.url, 'http://www.example.com/price/\\xa3')\n+        resp = self.response_class(\n+            url=\"http://www.example.com/price/\\xa3\", encoding=\"utf-8\"\n+        )\n+        self.assertEqual(resp.url, to_unicode(b\"http://www.example.com/price/\\xc2\\xa3\"))\n+        resp = self.response_class(\n+            url=\"http://www.example.com/price/\\xa3\", encoding=\"latin-1\"\n+        )\n+        self.assertEqual(resp.url, \"http://www.example.com/price/\\xa3\")\n+        resp = self.response_class(\n+            \"http://www.example.com/price/\\xa3\",\n+            headers={\"Content-type\": [\"text/html; charset=utf-8\"]},\n+        )\n+        self.assertEqual(resp.url, to_unicode(b\"http://www.example.com/price/\\xc2\\xa3\"))\n+        resp = self.response_class(\n+            \"http://www.example.com/price/\\xa3\",\n+            headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]},\n+        )\n+        self.assertEqual(resp.url, \"http://www.example.com/price/\\xa3\")\n \n     def test_unicode_body(self):\n-        unicode_string = ('\\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 '\n-                          '\\u0442\\u0435\\u043a\\u0441\\u0442')\n-        self.assertRaises(TypeError, self.response_class, 'http://www.example.com', body='unicode body')\n+        unicode_string = (\n+            \"\\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \"\n+            \"\\u0442\\u0435\\u043a\\u0441\\u0442\"\n+        )\n+        self.assertRaises(\n+            TypeError,\n+            self.response_class,\n+            \"http://www.example.com\",\n+            body=\"unicode body\",\n+        )\n \n-        original_string = unicode_string.encode('cp1251')\n-        r1 = self.response_class('http://www.example.com', body=original_string, encoding='cp1251')\n+        original_string = unicode_string.encode(\"cp1251\")\n+        r1 = self.response_class(\n+            \"http://www.example.com\", body=original_string, encoding=\"cp1251\"\n+        )\n \n         # check response.text\n         self.assertTrue(isinstance(r1.text, str))\n         self.assertEqual(r1.text, unicode_string)\n \n     def test_encoding(self):\n-        r1 = self.response_class(\"http://www.example.com\", body=b\"\\xc2\\xa3\",\n-                                 headers={\"Content-type\": [\"text/html; charset=utf-8\"]})\n-        r2 = self.response_class(\"http://www.example.com\", encoding='utf-8', body=\"\\xa3\")\n-        r3 = self.response_class(\"http://www.example.com\", body=b\"\\xa3\",\n-                                 headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]})\n+        r1 = self.response_class(\n+            \"http://www.example.com\",\n+            body=b\"\\xc2\\xa3\",\n+            headers={\"Content-type\": [\"text/html; charset=utf-8\"]},\n+        )\n+        r2 = self.response_class(\n+            \"http://www.example.com\", encoding=\"utf-8\", body=\"\\xa3\"\n+        )\n+        r3 = self.response_class(\n+            \"http://www.example.com\",\n+            body=b\"\\xa3\",\n+            headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]},\n+        )\n         r4 = self.response_class(\"http://www.example.com\", body=b\"\\xa2\\xa3\")\n-        r5 = self.response_class(\"http://www.example.com\", body=b\"\\xc2\\xa3\",\n-                                 headers={\"Content-type\": [\"text/html; charset=None\"]})\n-        r6 = self.response_class(\"http://www.example.com\", body=b\"\\xa8D\",\n-                                 headers={\"Content-type\": [\"text/html; charset=gb2312\"]})\n-        r7 = self.response_class(\"http://www.example.com\", body=b\"\\xa8D\",\n-                                 headers={\"Content-type\": [\"text/html; charset=gbk\"]})\n-        r8 = self.response_class(\"http://www.example.com\", body=codecs.BOM_UTF8 + b\"\\xc2\\xa3\",\n-                                 headers={\"Content-type\": [\"text/html; charset=cp1251\"]})\n+        r5 = self.response_class(\n+            \"http://www.example.com\",\n+            body=b\"\\xc2\\xa3\",\n+            headers={\"Content-type\": [\"text/html; charset=None\"]},\n+        )\n+        r6 = self.response_class(\n+            \"http://www.example.com\",\n+            body=b\"\\xa8D\",\n+            headers={\"Content-type\": [\"text/html; charset=gb2312\"]},\n+        )\n+        r7 = self.response_class(\n+            \"http://www.example.com\",\n+            body=b\"\\xa8D\",\n+            headers={\"Content-type\": [\"text/html; charset=gbk\"]},\n+        )\n+        r8 = self.response_class(\n+            \"http://www.example.com\",\n+            body=codecs.BOM_UTF8 + b\"\\xc2\\xa3\",\n+            headers={\"Content-type\": [\"text/html; charset=cp1251\"]},\n+        )\n \n         self.assertEqual(r1._headers_encoding(), \"utf-8\")\n         self.assertEqual(r2._headers_encoding(), None)\n-        self.assertEqual(r2._declared_encoding(), 'utf-8')\n-        self._assert_response_encoding(r2, 'utf-8')\n+        self.assertEqual(r2._declared_encoding(), \"utf-8\")\n+        self._assert_response_encoding(r2, \"utf-8\")\n         self.assertEqual(r3._headers_encoding(), \"cp1252\")\n         self.assertEqual(r3._declared_encoding(), \"cp1252\")\n         self.assertEqual(r4._headers_encoding(), None)\n@@ -374,81 +447,96 @@ class TextResponseTest(BaseResponseTest):\n         self.assertEqual(r8._declared_encoding(), \"utf-8\")\n         self._assert_response_encoding(r5, \"utf-8\")\n         self._assert_response_encoding(r8, \"utf-8\")\n-        assert r4._body_inferred_encoding() is not None and r4._body_inferred_encoding() != 'ascii'\n-        self._assert_response_values(r1, 'utf-8', \"\\xa3\")\n-        self._assert_response_values(r2, 'utf-8', \"\\xa3\")\n-        self._assert_response_values(r3, 'iso-8859-1', \"\\xa3\")\n-        self._assert_response_values(r6, 'gb18030', \"\\u2015\")\n-        self._assert_response_values(r7, 'gb18030', \"\\u2015\")\n+        assert (\n+            r4._body_inferred_encoding() is not None\n+            and r4._body_inferred_encoding() != \"ascii\"\n+        )\n+        self._assert_response_values(r1, \"utf-8\", \"\\xa3\")\n+        self._assert_response_values(r2, \"utf-8\", \"\\xa3\")\n+        self._assert_response_values(r3, \"iso-8859-1\", \"\\xa3\")\n+        self._assert_response_values(r6, \"gb18030\", \"\\u2015\")\n+        self._assert_response_values(r7, \"gb18030\", \"\\u2015\")\n \n         # TextResponse (and subclasses) must be passed a encoding when instantiating with unicode bodies\n-        self.assertRaises(TypeError, self.response_class, \"http://www.example.com\", body=\"\\xa3\")\n+        self.assertRaises(\n+            TypeError, self.response_class, \"http://www.example.com\", body=\"\\xa3\"\n+        )\n \n     def test_declared_encoding_invalid(self):\n         \"\"\"Check that unknown declared encodings are ignored\"\"\"\n-        r = self.response_class(\"http://www.example.com\",\n+        r = self.response_class(\n+            \"http://www.example.com\",\n             headers={\"Content-type\": [\"text/html; charset=UNKNOWN\"]},\n-                                body=b\"\\xc2\\xa3\")\n+            body=b\"\\xc2\\xa3\",\n+        )\n         self.assertEqual(r._declared_encoding(), None)\n-        self._assert_response_values(r, 'utf-8', \"\\xa3\")\n+        self._assert_response_values(r, \"utf-8\", \"\\xa3\")\n \n     def test_utf16(self):\n         \"\"\"Test utf-16 because UnicodeDammit is known to have problems with\"\"\"\n-        r = self.response_class(\"http://www.example.com\",\n-                                body=b'\\xff\\xfeh\\x00i\\x00',\n-                                encoding='utf-16')\n-        self._assert_response_values(r, 'utf-16', \"hi\")\n+        r = self.response_class(\n+            \"http://www.example.com\", body=b\"\\xff\\xfeh\\x00i\\x00\", encoding=\"utf-16\"\n+        )\n+        self._assert_response_values(r, \"utf-16\", \"hi\")\n \n     def test_invalid_utf8_encoded_body_with_valid_utf8_BOM(self):\n-        r6 = self.response_class(\"http://www.example.com\",\n+        r6 = self.response_class(\n+            \"http://www.example.com\",\n             headers={\"Content-type\": [\"text/html; charset=utf-8\"]},\n-                                 body=b\"\\xef\\xbb\\xbfWORD\\xe3\\xab\")\n-        self.assertEqual(r6.encoding, 'utf-8')\n-        self.assertIn(r6.text, {\n-            'WORD\\ufffd\\ufffd',  # w3lib < 1.19.0\n-            'WORD\\ufffd',        # w3lib >= 1.19.0\n-        })\n+            body=b\"\\xef\\xbb\\xbfWORD\\xe3\\xab\",\n+        )\n+        self.assertEqual(r6.encoding, \"utf-8\")\n+        self.assertIn(\n+            r6.text,\n+            {\n+                \"WORD\\ufffd\\ufffd\",  # w3lib < 1.19.0\n+                \"WORD\\ufffd\",  # w3lib >= 1.19.0\n+            },\n+        )\n \n     def test_bom_is_removed_from_body(self):\n         # Inferring encoding from body also cache decoded body as sideeffect,\n         # this test tries to ensure that calling response.encoding and\n         # response.text in indistint order doesn't affect final\n         # values for encoding and decoded body.\n-        url = 'http://example.com'\n+        url = \"http://example.com\"\n         body = b\"\\xef\\xbb\\xbfWORD\"\n         headers = {\"Content-type\": [\"text/html; charset=utf-8\"]}\n \n         # Test response without content-type and BOM encoding\n         response = self.response_class(url, body=body)\n-        self.assertEqual(response.encoding, 'utf-8')\n-        self.assertEqual(response.text, 'WORD')\n+        self.assertEqual(response.encoding, \"utf-8\")\n+        self.assertEqual(response.text, \"WORD\")\n         response = self.response_class(url, body=body)\n-        self.assertEqual(response.text, 'WORD')\n-        self.assertEqual(response.encoding, 'utf-8')\n+        self.assertEqual(response.text, \"WORD\")\n+        self.assertEqual(response.encoding, \"utf-8\")\n \n         # Body caching sideeffect isn't triggered when encoding is declared in\n         # content-type header but BOM still need to be removed from decoded\n         # body\n         response = self.response_class(url, headers=headers, body=body)\n-        self.assertEqual(response.encoding, 'utf-8')\n-        self.assertEqual(response.text, 'WORD')\n+        self.assertEqual(response.encoding, \"utf-8\")\n+        self.assertEqual(response.text, \"WORD\")\n         response = self.response_class(url, headers=headers, body=body)\n-        self.assertEqual(response.text, 'WORD')\n-        self.assertEqual(response.encoding, 'utf-8')\n+        self.assertEqual(response.text, \"WORD\")\n+        self.assertEqual(response.encoding, \"utf-8\")\n \n     def test_replace_wrong_encoding(self):\n         \"\"\"Test invalid chars are replaced properly\"\"\"\n-        r = self.response_class(\"http://www.example.com\", encoding='utf-8', body=b'PREFIX\\xe3\\xabSUFFIX')\n+        r = self.response_class(\n+            \"http://www.example.com\", encoding=\"utf-8\", body=b\"PREFIX\\xe3\\xabSUFFIX\"\n+        )\n         # XXX: Policy for replacing invalid chars may suffer minor variations\n         # but it should always contain the unicode replacement char ('\\ufffd')\n-        assert '\\ufffd' in r.text, repr(r.text)\n-        assert 'PREFIX' in r.text, repr(r.text)\n-        assert 'SUFFIX' in r.text, repr(r.text)\n+        assert \"\\ufffd\" in r.text, repr(r.text)\n+        assert \"PREFIX\" in r.text, repr(r.text)\n+        assert \"SUFFIX\" in r.text, repr(r.text)\n \n         # Do not destroy html tags due to encoding bugs\n-        r = self.response_class(\"http://example.com\", encoding='utf-8',\n-                                body=b'\\xf0<span>value</span>')\n-        assert '<span>value</span>' in r.text, repr(r.text)\n+        r = self.response_class(\n+            \"http://example.com\", encoding=\"utf-8\", body=b\"\\xf0<span>value</span>\"\n+        )\n+        assert \"<span>value</span>\" in r.text, repr(r.text)\n \n         # FIXME: This test should pass once we stop using BeautifulSoup's UnicodeDammit in TextResponse\n         # r = self.response_class(\"http://www.example.com\", body=b'PREFIX\\xe3\\xabSUFFIX')\n@@ -459,22 +547,15 @@ class TextResponseTest(BaseResponseTest):\n         response = self.response_class(\"http://www.example.com\", body=body)\n \n         self.assertIsInstance(response.selector, Selector)\n-        self.assertEqual(response.selector.type, 'html')\n+        self.assertEqual(response.selector.type, \"html\")\n         self.assertIs(response.selector, response.selector)  # property is cached\n         self.assertIs(response.selector.response, response)\n \n         self.assertEqual(\n-            response.selector.xpath(\"//title/text()\").getall(),\n-            ['Some page']\n-        )\n-        self.assertEqual(\n-            response.selector.css(\"title::text\").getall(),\n-            ['Some page']\n-        )\n-        self.assertEqual(\n-            response.selector.re(\"Some (.*)</title>\"),\n-            ['page']\n+            response.selector.xpath(\"//title/text()\").getall(), [\"Some page\"]\n         )\n+        self.assertEqual(response.selector.css(\"title::text\").getall(), [\"Some page\"])\n+        self.assertEqual(response.selector.re(\"Some (.*)</title>\"), [\"page\"])\n \n     def test_selector_shortcuts(self):\n         body = b\"<html><head><title>Some page</title><body></body></html>\"\n@@ -490,151 +571,163 @@ class TextResponseTest(BaseResponseTest):\n         )\n \n     def test_selector_shortcuts_kwargs(self):\n-        body = b\"<html><head><title>Some page</title><body><p class=\\\"content\\\">A nice paragraph.</p></body></html>\"\n+        body = b'<html><head><title>Some page</title><body><p class=\"content\">A nice paragraph.</p></body></html>'\n         response = self.response_class(\"http://www.example.com\", body=body)\n \n         self.assertEqual(\n-            response.xpath(\"normalize-space(//p[@class=$pclass])\", pclass=\"content\").getall(),\n-            response.xpath(\"normalize-space(//p[@class=\\\"content\\\"])\").getall(),\n+            response.xpath(\n+                \"normalize-space(//p[@class=$pclass])\", pclass=\"content\"\n+            ).getall(),\n+            response.xpath('normalize-space(//p[@class=\"content\"])').getall(),\n         )\n         self.assertEqual(\n             response.xpath(\n                 \"//title[count(following::p[@class=$pclass])=$pcount]/text()\",\n-                pclass=\"content\", pcount=1,\n+                pclass=\"content\",\n+                pcount=1,\n+            ).getall(),\n+            response.xpath(\n+                '//title[count(following::p[@class=\"content\"])=1]/text()'\n             ).getall(),\n-            response.xpath(\"//title[count(following::p[@class=\\\"content\\\"])=1]/text()\").getall(),\n         )\n \n     def test_urljoin_with_base_url(self):\n         \"\"\"Test urljoin shortcut which also evaluates base-url through get_base_url().\"\"\"\n         body = b'<html><body><base href=\"https://example.net\"></body></html>'\n-        joined = self.response_class('http://www.example.com', body=body).urljoin('/test')\n-        absolute = 'https://example.net/test'\n+        joined = self.response_class(\"http://www.example.com\", body=body).urljoin(\n+            \"/test\"\n+        )\n+        absolute = \"https://example.net/test\"\n         self.assertEqual(joined, absolute)\n \n         body = b'<html><body><base href=\"/elsewhere\"></body></html>'\n-        joined = self.response_class('http://www.example.com', body=body).urljoin('test')\n-        absolute = 'http://www.example.com/test'\n+        joined = self.response_class(\"http://www.example.com\", body=body).urljoin(\n+            \"test\"\n+        )\n+        absolute = \"http://www.example.com/test\"\n         self.assertEqual(joined, absolute)\n \n         body = b'<html><body><base href=\"/elsewhere/\"></body></html>'\n-        joined = self.response_class('http://www.example.com', body=body).urljoin('test')\n-        absolute = 'http://www.example.com/elsewhere/test'\n+        joined = self.response_class(\"http://www.example.com\", body=body).urljoin(\n+            \"test\"\n+        )\n+        absolute = \"http://www.example.com/elsewhere/test\"\n         self.assertEqual(joined, absolute)\n \n     def test_follow_selector(self):\n         resp = self._links_response()\n         urls = [\n-            'http://example.com/sample2.html',\n-            'http://example.com/sample3.html',\n-            'http://example.com/sample3.html',\n-            'http://example.com/sample3.html#foo',\n-            'http://www.google.com/something',\n-            'http://example.com/innertag.html'\n+            \"http://example.com/sample2.html\",\n+            \"http://example.com/sample3.html\",\n+            \"http://example.com/sample3.html\",\n+            \"http://example.com/sample3.html#foo\",\n+            \"http://www.google.com/something\",\n+            \"http://example.com/innertag.html\",\n         ]\n \n         # select <a> elements\n-        for sellist in [resp.css('a'), resp.xpath('//a')]:\n+        for sellist in [resp.css(\"a\"), resp.xpath(\"//a\")]:\n             for sel, url in zip(sellist, urls):\n                 self._assert_followed_url(sel, url, response=resp)\n \n         # select <link> elements\n         self._assert_followed_url(\n-            Selector(text='<link href=\"foo\"></link>').css('link')[0],\n-            'http://example.com/foo',\n-            response=resp\n+            Selector(text='<link href=\"foo\"></link>').css(\"link\")[0],\n+            \"http://example.com/foo\",\n+            response=resp,\n         )\n \n         # href attributes should work\n-        for sellist in [resp.css('a::attr(href)'), resp.xpath('//a/@href')]:\n+        for sellist in [resp.css(\"a::attr(href)\"), resp.xpath(\"//a/@href\")]:\n             for sel, url in zip(sellist, urls):\n                 self._assert_followed_url(sel, url, response=resp)\n \n         # non-a elements are not supported\n-        self.assertRaises(ValueError, resp.follow, resp.css('div')[0])\n+        self.assertRaises(ValueError, resp.follow, resp.css(\"div\")[0])\n \n     def test_follow_selector_list(self):\n         resp = self._links_response()\n-        self.assertRaisesRegex(ValueError, 'SelectorList',\n-                               resp.follow, resp.css('a'))\n+        self.assertRaisesRegex(ValueError, \"SelectorList\", resp.follow, resp.css(\"a\"))\n \n     def test_follow_selector_invalid(self):\n         resp = self._links_response()\n-        self.assertRaisesRegex(ValueError, 'Unsupported',\n-                               resp.follow, resp.xpath('count(//div)')[0])\n+        self.assertRaisesRegex(\n+            ValueError, \"Unsupported\", resp.follow, resp.xpath(\"count(//div)\")[0]\n+        )\n \n     def test_follow_selector_attribute(self):\n         resp = self._links_response()\n-        for src in resp.css('img::attr(src)'):\n-            self._assert_followed_url(src, 'http://example.com/sample2.jpg')\n+        for src in resp.css(\"img::attr(src)\"):\n+            self._assert_followed_url(src, \"http://example.com/sample2.jpg\")\n \n     def test_follow_selector_no_href(self):\n         resp = self.response_class(\n-            url='http://example.com',\n-            body=b'<html><body><a name=123>click me</a></body></html>',\n+            url=\"http://example.com\",\n+            body=b\"<html><body><a name=123>click me</a></body></html>\",\n         )\n-        self.assertRaisesRegex(ValueError, 'no href',\n-                               resp.follow, resp.css('a')[0])\n+        self.assertRaisesRegex(ValueError, \"no href\", resp.follow, resp.css(\"a\")[0])\n \n     def test_follow_whitespace_selector(self):\n         resp = self.response_class(\n-            'http://example.com',\n-            body=b'''<html><body><a href=\" foo\\n\">click me</a></body></html>'''\n+            \"http://example.com\",\n+            body=b\"\"\"<html><body><a href=\" foo\\n\">click me</a></body></html>\"\"\",\n         )\n         self._assert_followed_url(\n-            resp.css('a')[0],\n-            'http://example.com/foo',\n-            response=resp)\n+            resp.css(\"a\")[0], \"http://example.com/foo\", response=resp\n+        )\n         self._assert_followed_url(\n-            resp.css('a::attr(href)')[0],\n-            'http://example.com/foo',\n-            response=resp)\n+            resp.css(\"a::attr(href)\")[0], \"http://example.com/foo\", response=resp\n+        )\n \n     def test_follow_encoding(self):\n         resp1 = self.response_class(\n-            'http://example.com',\n-            encoding='utf8',\n-            body='<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode('utf8')\n+            \"http://example.com\",\n+            encoding=\"utf8\",\n+            body='<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode(\n+                \"utf8\"\n+            ),\n         )\n         req = self._assert_followed_url(\n-            resp1.css('a')[0],\n-            'http://example.com/foo?%D0%BF%D1%80%D0%B8%D0%B2%D0%B5%D1%82',\n+            resp1.css(\"a\")[0],\n+            \"http://example.com/foo?%D0%BF%D1%80%D0%B8%D0%B2%D0%B5%D1%82\",\n             response=resp1,\n         )\n-        self.assertEqual(req.encoding, 'utf8')\n+        self.assertEqual(req.encoding, \"utf8\")\n \n         resp2 = self.response_class(\n-            'http://example.com',\n-            encoding='cp1251',\n-            body='<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode('cp1251')\n+            \"http://example.com\",\n+            encoding=\"cp1251\",\n+            body='<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode(\n+                \"cp1251\"\n+            ),\n         )\n         req = self._assert_followed_url(\n-            resp2.css('a')[0],\n-            'http://example.com/foo?%EF%F0%E8%E2%E5%F2',\n+            resp2.css(\"a\")[0],\n+            \"http://example.com/foo?%EF%F0%E8%E2%E5%F2\",\n             response=resp2,\n         )\n-        self.assertEqual(req.encoding, 'cp1251')\n+        self.assertEqual(req.encoding, \"cp1251\")\n \n     def test_follow_flags(self):\n-        res = self.response_class('http://example.com/')\n-        fol = res.follow('http://example.com/', flags=['cached', 'allowed'])\n-        self.assertEqual(fol.flags, ['cached', 'allowed'])\n+        res = self.response_class(\"http://example.com/\")\n+        fol = res.follow(\"http://example.com/\", flags=[\"cached\", \"allowed\"])\n+        self.assertEqual(fol.flags, [\"cached\", \"allowed\"])\n \n     def test_follow_all_flags(self):\n-        re = self.response_class('http://www.example.com/')\n+        re = self.response_class(\"http://www.example.com/\")\n         urls = [\n-            'http://www.example.com/',\n-            'http://www.example.com/2',\n-            'http://www.example.com/foo',\n+            \"http://www.example.com/\",\n+            \"http://www.example.com/2\",\n+            \"http://www.example.com/foo\",\n         ]\n-        fol = re.follow_all(urls, flags=['cached', 'allowed'])\n+        fol = re.follow_all(urls, flags=[\"cached\", \"allowed\"])\n         for req in fol:\n-            self.assertEqual(req.flags, ['cached', 'allowed'])\n+            self.assertEqual(req.flags, [\"cached\", \"allowed\"])\n \n     def test_follow_all_css(self):\n         expected = [\n-            'http://example.com/sample3.html',\n-            'http://example.com/innertag.html',\n+            \"http://example.com/sample3.html\",\n+            \"http://example.com/innertag.html\",\n         ]\n         response = self._links_response()\n         extracted = [r.url for r in response.follow_all(css='a[href*=\"example.com\"]')]\n@@ -642,20 +735,20 @@ class TextResponseTest(BaseResponseTest):\n \n     def test_follow_all_css_skip_invalid(self):\n         expected = [\n-            'http://example.com/page/1/',\n-            'http://example.com/page/3/',\n-            'http://example.com/page/4/',\n+            \"http://example.com/page/1/\",\n+            \"http://example.com/page/3/\",\n+            \"http://example.com/page/4/\",\n         ]\n         response = self._links_response_no_href()\n-        extracted1 = [r.url for r in response.follow_all(css='.pagination a')]\n+        extracted1 = [r.url for r in response.follow_all(css=\".pagination a\")]\n         self.assertEqual(expected, extracted1)\n-        extracted2 = [r.url for r in response.follow_all(response.css('.pagination a'))]\n+        extracted2 = [r.url for r in response.follow_all(response.css(\".pagination a\"))]\n         self.assertEqual(expected, extracted2)\n \n     def test_follow_all_xpath(self):\n         expected = [\n-            'http://example.com/sample3.html',\n-            'http://example.com/innertag.html',\n+            \"http://example.com/sample3.html\",\n+            \"http://example.com/innertag.html\",\n         ]\n         response = self._links_response()\n         extracted = response.follow_all(xpath='//a[contains(@href, \"example.com\")]')\n@@ -663,25 +756,33 @@ class TextResponseTest(BaseResponseTest):\n \n     def test_follow_all_xpath_skip_invalid(self):\n         expected = [\n-            'http://example.com/page/1/',\n-            'http://example.com/page/3/',\n-            'http://example.com/page/4/',\n+            \"http://example.com/page/1/\",\n+            \"http://example.com/page/3/\",\n+            \"http://example.com/page/4/\",\n         ]\n         response = self._links_response_no_href()\n-        extracted1 = [r.url for r in response.follow_all(xpath='//div[@id=\"pagination\"]/a')]\n+        extracted1 = [\n+            r.url for r in response.follow_all(xpath='//div[@id=\"pagination\"]/a')\n+        ]\n         self.assertEqual(expected, extracted1)\n-        extracted2 = [r.url for r in response.follow_all(response.xpath('//div[@id=\"pagination\"]/a'))]\n+        extracted2 = [\n+            r.url\n+            for r in response.follow_all(response.xpath('//div[@id=\"pagination\"]/a'))\n+        ]\n         self.assertEqual(expected, extracted2)\n \n     def test_follow_all_too_many_arguments(self):\n         response = self._links_response()\n         with self.assertRaises(ValueError):\n-            response.follow_all(css='a[href*=\"example.com\"]', xpath='//a[contains(@href, \"example.com\")]')\n+            response.follow_all(\n+                css='a[href*=\"example.com\"]',\n+                xpath='//a[contains(@href, \"example.com\")]',\n+            )\n \n     def test_json_response(self):\n         json_body = b\"\"\"{\"ip\": \"109.187.217.200\"}\"\"\"\n         json_response = self.response_class(\"http://www.example.com\", body=json_body)\n-        self.assertEqual(json_response.json(), {'ip': '109.187.217.200'})\n+        self.assertEqual(json_response.json(), {\"ip\": \"109.187.217.200\"})\n \n         text_body = b\"\"\"<html><body>text</body></html>\"\"\"\n         text_response = self.response_class(\"http://www.example.com\", body=text_body)\n@@ -691,9 +792,11 @@ class TextResponseTest(BaseResponseTest):\n     def test_cache_json_response(self):\n         json_valid_bodies = [b\"\"\"{\"ip\": \"109.187.217.200\"}\"\"\", b\"\"\"null\"\"\"]\n         for json_body in json_valid_bodies:\n-            json_response = self.response_class(\"http://www.example.com\", body=json_body)\n+            json_response = self.response_class(\n+                \"http://www.example.com\", body=json_body\n+            )\n \n-            with mock.patch('json.loads') as mock_json:\n+            with mock.patch(\"json.loads\") as mock_json:\n                 for _ in range(2):\n                     json_response.json()\n                 mock_json.assert_called_once_with(json_body.decode())\n@@ -710,33 +813,36 @@ class HtmlResponseTest(TextResponseTest):\n         </head><body>Price: \\xa3100</body></html>'\n         \"\"\"\n         r1 = self.response_class(\"http://www.example.com\", body=body)\n-        self._assert_response_values(r1, 'iso-8859-1', body)\n+        self._assert_response_values(r1, \"iso-8859-1\", body)\n \n         body = b\"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\n         <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\">\n         Price: \\xa3100\n         \"\"\"\n         r2 = self.response_class(\"http://www.example.com\", body=body)\n-        self._assert_response_values(r2, 'iso-8859-1', body)\n+        self._assert_response_values(r2, \"iso-8859-1\", body)\n \n         # for conflicting declarations headers must take precedence\n         body = b\"\"\"<html><head><title>Some page</title>\n         <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n         </head><body>Price: \\xa3100</body></html>'\n         \"\"\"\n-        r3 = self.response_class(\"http://www.example.com\", body=body,\n-                                 headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]})\n-        self._assert_response_values(r3, 'iso-8859-1', body)\n+        r3 = self.response_class(\n+            \"http://www.example.com\",\n+            body=body,\n+            headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]},\n+        )\n+        self._assert_response_values(r3, \"iso-8859-1\", body)\n \n         # make sure replace() preserves the encoding of the original response\n         body = b\"New body \\xa3\"\n         r4 = r3.replace(body=body)\n-        self._assert_response_values(r4, 'iso-8859-1', body)\n+        self._assert_response_values(r4, \"iso-8859-1\", body)\n \n     def test_html5_meta_charset(self):\n         body = b\"\"\"<html><head><meta charset=\"gb2312\" /><title>Some page</title><body>bla bla</body>\"\"\"\n         r1 = self.response_class(\"http://www.example.com\", body=body)\n-        self._assert_response_values(r1, 'gb2312', body)\n+        self._assert_response_values(r1, \"gb2312\", body)\n \n \n class XmlResponseTest(TextResponseTest):\n@@ -750,14 +856,14 @@ class XmlResponseTest(TextResponseTest):\n \n         body = b\"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n         r2 = self.response_class(\"http://www.example.com\", body=body)\n-        self._assert_response_values(r2, 'iso-8859-1', body)\n+        self._assert_response_values(r2, \"iso-8859-1\", body)\n \n         # make sure replace() preserves the explicit encoding passed in the __init__ method\n         body = b\"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n-        r3 = self.response_class(\"http://www.example.com\", body=body, encoding='utf-8')\n+        r3 = self.response_class(\"http://www.example.com\", body=body, encoding=\"utf-8\")\n         body2 = b\"New body\"\n         r4 = r3.replace(body=body2)\n-        self._assert_response_values(r4, 'utf-8', body2)\n+        self._assert_response_values(r4, \"utf-8\", body2)\n \n     def test_replace_encoding(self):\n         # make sure replace() keeps the previous encoding unless overridden explicitly\n@@ -765,24 +871,21 @@ class XmlResponseTest(TextResponseTest):\n         body2 = b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><xml></xml>\"\"\"\n         r5 = self.response_class(\"http://www.example.com\", body=body)\n         r6 = r5.replace(body=body2)\n-        r7 = r5.replace(body=body2, encoding='utf-8')\n-        self._assert_response_values(r5, 'iso-8859-1', body)\n-        self._assert_response_values(r6, 'iso-8859-1', body2)\n-        self._assert_response_values(r7, 'utf-8', body2)\n+        r7 = r5.replace(body=body2, encoding=\"utf-8\")\n+        self._assert_response_values(r5, \"iso-8859-1\", body)\n+        self._assert_response_values(r6, \"iso-8859-1\", body2)\n+        self._assert_response_values(r7, \"utf-8\", body2)\n \n     def test_selector(self):\n         body = b'<?xml version=\"1.0\" encoding=\"utf-8\"?><xml><elem>value</elem></xml>'\n         response = self.response_class(\"http://www.example.com\", body=body)\n \n         self.assertIsInstance(response.selector, Selector)\n-        self.assertEqual(response.selector.type, 'xml')\n+        self.assertEqual(response.selector.type, \"xml\")\n         self.assertIs(response.selector, response.selector)  # property is cached\n         self.assertIs(response.selector.response, response)\n \n-        self.assertEqual(\n-            response.selector.xpath(\"//elem/text()\").getall(),\n-            ['value']\n-        )\n+        self.assertEqual(response.selector.xpath(\"//elem/text()\").getall(), [\"value\"])\n \n     def test_selector_shortcuts(self):\n         body = b'<?xml version=\"1.0\" encoding=\"utf-8\"?><xml><elem>value</elem></xml>'\n@@ -794,20 +897,26 @@ class XmlResponseTest(TextResponseTest):\n         )\n \n     def test_selector_shortcuts_kwargs(self):\n-        body = b'''<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+        body = b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n         <xml xmlns:somens=\"http://scrapy.org\">\n         <somens:elem>value</somens:elem>\n-        </xml>'''\n+        </xml>\"\"\"\n         response = self.response_class(\"http://www.example.com\", body=body)\n \n         self.assertEqual(\n-            response.xpath(\"//s:elem/text()\", namespaces={'s': 'http://scrapy.org'}).getall(),\n-            response.selector.xpath(\"//s:elem/text()\", namespaces={'s': 'http://scrapy.org'}).getall(),\n+            response.xpath(\n+                \"//s:elem/text()\", namespaces={\"s\": \"http://scrapy.org\"}\n+            ).getall(),\n+            response.selector.xpath(\n+                \"//s:elem/text()\", namespaces={\"s\": \"http://scrapy.org\"}\n+            ).getall(),\n         )\n \n-        response.selector.register_namespace('s2', 'http://scrapy.org')\n+        response.selector.register_namespace(\"s2\", \"http://scrapy.org\")\n         self.assertEqual(\n-            response.xpath(\"//s1:elem/text()\", namespaces={'s1': 'http://scrapy.org'}).getall(),\n+            response.xpath(\n+                \"//s1:elem/text()\", namespaces={\"s1\": \"http://scrapy.org\"}\n+            ).getall(),\n             response.selector.xpath(\"//s2:elem/text()\").getall(),\n         )\n \n@@ -827,7 +936,9 @@ class CustomResponseTest(TextResponseTest):\n \n     def test_copy(self):\n         super().test_copy()\n-        r1 = self.response_class(url=\"https://example.org\", status=200, foo=\"foo\", bar=\"bar\", lost=\"lost\")\n+        r1 = self.response_class(\n+            url=\"https://example.org\", status=200, foo=\"foo\", bar=\"bar\", lost=\"lost\"\n+        )\n         r2 = r1.copy()\n         self.assertIsInstance(r2, self.response_class)\n         self.assertEqual(r1.foo, r2.foo)\n@@ -837,7 +948,9 @@ class CustomResponseTest(TextResponseTest):\n \n     def test_replace(self):\n         super().test_replace()\n-        r1 = self.response_class(url=\"https://example.org\", status=200, foo=\"foo\", bar=\"bar\", lost=\"lost\")\n+        r1 = self.response_class(\n+            url=\"https://example.org\", status=200, foo=\"foo\", bar=\"bar\", lost=\"lost\"\n+        )\n \n         r2 = r1.replace(foo=\"new-foo\", bar=\"new-bar\", lost=\"new-lost\")\n         self.assertIsInstance(r2, self.response_class)\n@@ -868,4 +981,8 @@ class CustomResponseTest(TextResponseTest):\n \n         with self.assertRaises(TypeError) as ctx:\n             r1.replace(unknown=\"unknown\")\n-        self.assertTrue(str(ctx.exception).endswith(\"__init__() got an unexpected keyword argument 'unknown'\"))\n+        self.assertTrue(\n+            str(ctx.exception).endswith(\n+                \"__init__() got an unexpected keyword argument 'unknown'\"\n+            )\n+        )\n\n@@ -5,7 +5,6 @@ from scrapy.item import ABCMeta, Field, Item, ItemMeta\n \n \n class ItemTest(unittest.TestCase):\n-\n     def assertSortedEqual(self, first, second, msg=None):\n         return self.assertEqual(sorted(first), sorted(second), msg)\n \n@@ -14,35 +13,34 @@ class ItemTest(unittest.TestCase):\n             name = Field()\n \n         i = TestItem()\n-        i['name'] = 'name'\n-        self.assertEqual(i['name'], 'name')\n+        i[\"name\"] = \"name\"\n+        self.assertEqual(i[\"name\"], \"name\")\n \n     def test_init(self):\n         class TestItem(Item):\n             name = Field()\n \n         i = TestItem()\n-        self.assertRaises(KeyError, i.__getitem__, 'name')\n+        self.assertRaises(KeyError, i.__getitem__, \"name\")\n \n-        i2 = TestItem(name='john doe')\n-        self.assertEqual(i2['name'], 'john doe')\n+        i2 = TestItem(name=\"john doe\")\n+        self.assertEqual(i2[\"name\"], \"john doe\")\n \n-        i3 = TestItem({'name': 'john doe'})\n-        self.assertEqual(i3['name'], 'john doe')\n+        i3 = TestItem({\"name\": \"john doe\"})\n+        self.assertEqual(i3[\"name\"], \"john doe\")\n \n         i4 = TestItem(i3)\n-        self.assertEqual(i4['name'], 'john doe')\n+        self.assertEqual(i4[\"name\"], \"john doe\")\n \n-        self.assertRaises(KeyError, TestItem, {'name': 'john doe',\n-                                               'other': 'foo'})\n+        self.assertRaises(KeyError, TestItem, {\"name\": \"john doe\", \"other\": \"foo\"})\n \n     def test_invalid_field(self):\n         class TestItem(Item):\n             pass\n \n         i = TestItem()\n-        self.assertRaises(KeyError, i.__setitem__, 'field', 'text')\n-        self.assertRaises(KeyError, i.__getitem__, 'field')\n+        self.assertRaises(KeyError, i.__setitem__, \"field\", \"text\")\n+        self.assertRaises(KeyError, i.__getitem__, \"field\")\n \n     def test_repr(self):\n         class TestItem(Item):\n@@ -50,55 +48,54 @@ class ItemTest(unittest.TestCase):\n             number = Field()\n \n         i = TestItem()\n-        i['name'] = 'John Doe'\n-        i['number'] = 123\n+        i[\"name\"] = \"John Doe\"\n+        i[\"number\"] = 123\n         itemrepr = repr(i)\n \n-        self.assertEqual(itemrepr,\n-                         \"{'name': 'John Doe', 'number': 123}\")\n+        self.assertEqual(itemrepr, \"{'name': 'John Doe', 'number': 123}\")\n \n         i2 = eval(itemrepr)\n-        self.assertEqual(i2['name'], 'John Doe')\n-        self.assertEqual(i2['number'], 123)\n+        self.assertEqual(i2[\"name\"], \"John Doe\")\n+        self.assertEqual(i2[\"number\"], 123)\n \n     def test_private_attr(self):\n         class TestItem(Item):\n             name = Field()\n \n         i = TestItem()\n-        i._private = 'test'\n-        self.assertEqual(i._private, 'test')\n+        i._private = \"test\"\n+        self.assertEqual(i._private, \"test\")\n \n     def test_raise_getattr(self):\n         class TestItem(Item):\n             name = Field()\n \n         i = TestItem()\n-        self.assertRaises(AttributeError, getattr, i, 'name')\n+        self.assertRaises(AttributeError, getattr, i, \"name\")\n \n     def test_raise_setattr(self):\n         class TestItem(Item):\n             name = Field()\n \n         i = TestItem()\n-        self.assertRaises(AttributeError, setattr, i, 'name', 'john')\n+        self.assertRaises(AttributeError, setattr, i, \"name\", \"john\")\n \n     def test_custom_methods(self):\n         class TestItem(Item):\n             name = Field()\n \n             def get_name(self):\n-                return self['name']\n+                return self[\"name\"]\n \n             def change_name(self, name):\n-                self['name'] = name\n+                self[\"name\"] = name\n \n         i = TestItem()\n         self.assertRaises(KeyError, i.get_name)\n-        i['name'] = 'lala'\n-        self.assertEqual(i.get_name(), 'lala')\n-        i.change_name('other')\n-        self.assertEqual(i.get_name(), 'other')\n+        i[\"name\"] = \"lala\"\n+        self.assertEqual(i.get_name(), \"lala\")\n+        i.change_name(\"other\")\n+        self.assertEqual(i.get_name(), \"other\")\n \n     def test_metaclass(self):\n         class TestItem(Item):\n@@ -107,22 +104,22 @@ class ItemTest(unittest.TestCase):\n             values = Field()\n \n         i = TestItem()\n-        i['name'] = 'John'\n-        self.assertEqual(list(i.keys()), ['name'])\n-        self.assertEqual(list(i.values()), ['John'])\n+        i[\"name\"] = \"John\"\n+        self.assertEqual(list(i.keys()), [\"name\"])\n+        self.assertEqual(list(i.values()), [\"John\"])\n \n-        i['keys'] = 'Keys'\n-        i['values'] = 'Values'\n-        self.assertSortedEqual(list(i.keys()), ['keys', 'values', 'name'])\n-        self.assertSortedEqual(list(i.values()), ['Keys', 'Values', 'John'])\n+        i[\"keys\"] = \"Keys\"\n+        i[\"values\"] = \"Values\"\n+        self.assertSortedEqual(list(i.keys()), [\"keys\", \"values\", \"name\"])\n+        self.assertSortedEqual(list(i.values()), [\"Keys\", \"Values\", \"John\"])\n \n     def test_metaclass_with_fields_attribute(self):\n         class TestItem(Item):\n-            fields = {'new': Field(default='X')}\n+            fields = {\"new\": Field(default=\"X\")}\n \n-        item = TestItem(new='New')\n-        self.assertSortedEqual(list(item.keys()), ['new'])\n-        self.assertSortedEqual(list(item.values()), ['New'])\n+        item = TestItem(new=\"New\")\n+        self.assertSortedEqual(list(item.keys()), [\"new\"])\n+        self.assertSortedEqual(list(item.values()), [\"New\"])\n \n     def test_metaclass_inheritance(self):\n         class ParentItem(Item):\n@@ -134,151 +131,163 @@ class ItemTest(unittest.TestCase):\n             keys = Field()\n \n         i = TestItem()\n-        i['keys'] = 3\n-        self.assertEqual(list(i.keys()), ['keys'])\n+        i[\"keys\"] = 3\n+        self.assertEqual(list(i.keys()), [\"keys\"])\n         self.assertEqual(list(i.values()), [3])\n \n     def test_metaclass_multiple_inheritance_simple(self):\n         class A(Item):\n-            fields = {'load': Field(default='A')}\n-            save = Field(default='A')\n+            fields = {\"load\": Field(default=\"A\")}\n+            save = Field(default=\"A\")\n \n         class B(A):\n             pass\n \n         class C(Item):\n-            fields = {'load': Field(default='C')}\n-            save = Field(default='C')\n+            fields = {\"load\": Field(default=\"C\")}\n+            save = Field(default=\"C\")\n \n         class D(B, C):\n             pass\n \n-        item = D(save='X', load='Y')\n-        self.assertEqual(item['save'], 'X')\n-        self.assertEqual(item['load'], 'Y')\n-        self.assertEqual(D.fields, {'load': {'default': 'A'}, 'save': {'default': 'A'}})\n+        item = D(save=\"X\", load=\"Y\")\n+        self.assertEqual(item[\"save\"], \"X\")\n+        self.assertEqual(item[\"load\"], \"Y\")\n+        self.assertEqual(D.fields, {\"load\": {\"default\": \"A\"}, \"save\": {\"default\": \"A\"}})\n \n         # D class inverted\n         class E(C, B):\n             pass\n \n-        self.assertEqual(E(save='X')['save'], 'X')\n-        self.assertEqual(E(load='X')['load'], 'X')\n-        self.assertEqual(E.fields, {'load': {'default': 'C'}, 'save': {'default': 'C'}})\n+        self.assertEqual(E(save=\"X\")[\"save\"], \"X\")\n+        self.assertEqual(E(load=\"X\")[\"load\"], \"X\")\n+        self.assertEqual(E.fields, {\"load\": {\"default\": \"C\"}, \"save\": {\"default\": \"C\"}})\n \n     def test_metaclass_multiple_inheritance_diamond(self):\n         class A(Item):\n-            fields = {'update': Field(default='A')}\n-            save = Field(default='A')\n-            load = Field(default='A')\n+            fields = {\"update\": Field(default=\"A\")}\n+            save = Field(default=\"A\")\n+            load = Field(default=\"A\")\n \n         class B(A):\n             pass\n \n         class C(A):\n-            fields = {'update': Field(default='C')}\n-            save = Field(default='C')\n+            fields = {\"update\": Field(default=\"C\")}\n+            save = Field(default=\"C\")\n \n         class D(B, C):\n-            fields = {'update': Field(default='D')}\n-            load = Field(default='D')\n+            fields = {\"update\": Field(default=\"D\")}\n+            load = Field(default=\"D\")\n \n-        self.assertEqual(D(save='X')['save'], 'X')\n-        self.assertEqual(D(load='X')['load'], 'X')\n+        self.assertEqual(D(save=\"X\")[\"save\"], \"X\")\n+        self.assertEqual(D(load=\"X\")[\"load\"], \"X\")\n         self.assertEqual(\n             D.fields,\n-            {'save': {'default': 'C'}, 'load': {'default': 'D'}, 'update': {'default': 'D'}})\n+            {\n+                \"save\": {\"default\": \"C\"},\n+                \"load\": {\"default\": \"D\"},\n+                \"update\": {\"default\": \"D\"},\n+            },\n+        )\n \n         # D class inverted\n         class E(C, B):\n-            load = Field(default='E')\n+            load = Field(default=\"E\")\n \n-        self.assertEqual(E(save='X')['save'], 'X')\n-        self.assertEqual(E(load='X')['load'], 'X')\n+        self.assertEqual(E(save=\"X\")[\"save\"], \"X\")\n+        self.assertEqual(E(load=\"X\")[\"load\"], \"X\")\n         self.assertEqual(\n             E.fields,\n-            {'save': {'default': 'C'}, 'load': {'default': 'E'}, 'update': {'default': 'C'}})\n+            {\n+                \"save\": {\"default\": \"C\"},\n+                \"load\": {\"default\": \"E\"},\n+                \"update\": {\"default\": \"C\"},\n+            },\n+        )\n \n     def test_metaclass_multiple_inheritance_without_metaclass(self):\n         class A(Item):\n-            fields = {'load': Field(default='A')}\n-            save = Field(default='A')\n+            fields = {\"load\": Field(default=\"A\")}\n+            save = Field(default=\"A\")\n \n         class B(A):\n             pass\n \n         class C:\n-            fields = {'load': Field(default='C')}\n-            not_allowed = Field(default='not_allowed')\n-            save = Field(default='C')\n+            fields = {\"load\": Field(default=\"C\")}\n+            not_allowed = Field(default=\"not_allowed\")\n+            save = Field(default=\"C\")\n \n         class D(B, C):\n             pass\n \n-        self.assertRaises(KeyError, D, not_allowed='value')\n-        self.assertEqual(D(save='X')['save'], 'X')\n-        self.assertEqual(D.fields, {'save': {'default': 'A'}, 'load': {'default': 'A'}})\n+        self.assertRaises(KeyError, D, not_allowed=\"value\")\n+        self.assertEqual(D(save=\"X\")[\"save\"], \"X\")\n+        self.assertEqual(D.fields, {\"save\": {\"default\": \"A\"}, \"load\": {\"default\": \"A\"}})\n \n         # D class inverted\n         class E(C, B):\n             pass\n \n-        self.assertRaises(KeyError, E, not_allowed='value')\n-        self.assertEqual(E(save='X')['save'], 'X')\n-        self.assertEqual(E.fields, {'save': {'default': 'A'}, 'load': {'default': 'A'}})\n+        self.assertRaises(KeyError, E, not_allowed=\"value\")\n+        self.assertEqual(E(save=\"X\")[\"save\"], \"X\")\n+        self.assertEqual(E.fields, {\"save\": {\"default\": \"A\"}, \"load\": {\"default\": \"A\"}})\n \n     def test_to_dict(self):\n         class TestItem(Item):\n             name = Field()\n \n         i = TestItem()\n-        i['name'] = 'John'\n-        self.assertEqual(dict(i), {'name': 'John'})\n+        i[\"name\"] = \"John\"\n+        self.assertEqual(dict(i), {\"name\": \"John\"})\n \n     def test_copy(self):\n         class TestItem(Item):\n             name = Field()\n-        item = TestItem({'name': 'lower'})\n+\n+        item = TestItem({\"name\": \"lower\"})\n         copied_item = item.copy()\n         self.assertNotEqual(id(item), id(copied_item))\n-        copied_item['name'] = copied_item['name'].upper()\n-        self.assertNotEqual(item['name'], copied_item['name'])\n+        copied_item[\"name\"] = copied_item[\"name\"].upper()\n+        self.assertNotEqual(item[\"name\"], copied_item[\"name\"])\n \n     def test_deepcopy(self):\n         class TestItem(Item):\n             tags = Field()\n-        item = TestItem({'tags': ['tag1']})\n+\n+        item = TestItem({\"tags\": [\"tag1\"]})\n         copied_item = item.deepcopy()\n-        item['tags'].append('tag2')\n-        assert item['tags'] != copied_item['tags']\n+        item[\"tags\"].append(\"tag2\")\n+        assert item[\"tags\"] != copied_item[\"tags\"]\n \n \n class ItemMetaTest(unittest.TestCase):\n-\n     def test_new_method_propagates_classcell(self):\n         new_mock = mock.Mock(side_effect=ABCMeta.__new__)\n         base = ItemMeta.__bases__[0]\n \n-        with mock.patch.object(base, '__new__', new_mock):\n+        with mock.patch.object(base, \"__new__\", new_mock):\n \n             class MyItem(Item):\n                 def f(self):\n                     # For rationale of this see:\n                     # https://github.com/python/cpython/blob/ee1a81b77444c6715cbe610e951c655b6adab88b/Lib/test/test_super.py#L222\n-                    return __class__  # noqa  https://github.com/scrapy/scrapy/issues/2836\n+                    return (\n+                        __class__  # noqa  https://github.com/scrapy/scrapy/issues/2836\n+                    )\n \n             MyItem()\n \n         (first_call, second_call) = new_mock.call_args_list[-2:]\n \n         mcs, class_name, bases, attrs = first_call[0]\n-        assert '__classcell__' not in attrs\n+        assert \"__classcell__\" not in attrs\n         mcs, class_name, bases, attrs = second_call[0]\n-        assert '__classcell__' in attrs\n+        assert \"__classcell__\" in attrs\n \n \n class ItemMetaClassCellRegression(unittest.TestCase):\n-\n     def test_item_meta_classcell_regression(self):\n         class MyItem(Item, metaclass=ItemMeta):\n             def __init__(self, *args, **kwargs):\n\n@@ -4,7 +4,6 @@ from scrapy.link import Link\n \n \n class LinkTest(unittest.TestCase):\n-\n     def _assert_same_links(self, link1, link2):\n         self.assertEqual(link1, link2)\n         self.assertEqual(hash(link1), hash(link2))\n@@ -30,16 +29,26 @@ class LinkTest(unittest.TestCase):\n         self._assert_different_links(l4, l5)\n         self._assert_same_links(l4, l6)\n \n-        l7 = Link(\"http://www.example.com\", text=\"test\", fragment='something', nofollow=False)\n-        l8 = Link(\"http://www.example.com\", text=\"test\", fragment='something', nofollow=False)\n-        l9 = Link(\"http://www.example.com\", text=\"test\", fragment='something', nofollow=True)\n-        l10 = Link(\"http://www.example.com\", text=\"test\", fragment='other', nofollow=False)\n+        l7 = Link(\n+            \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=False\n+        )\n+        l8 = Link(\n+            \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=False\n+        )\n+        l9 = Link(\n+            \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=True\n+        )\n+        l10 = Link(\n+            \"http://www.example.com\", text=\"test\", fragment=\"other\", nofollow=False\n+        )\n         self._assert_same_links(l7, l8)\n         self._assert_different_links(l7, l9)\n         self._assert_different_links(l7, l10)\n \n     def test_repr(self):\n-        l1 = Link(\"http://www.example.com\", text=\"test\", fragment='something', nofollow=True)\n+        l1 = Link(\n+            \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=True\n+        )\n         l2 = eval(repr(l1))\n         self._assert_same_links(l1, l2)\n \n\n@@ -14,108 +14,163 @@ class Base:\n         extractor_cls = None\n \n         def setUp(self):\n-            body = get_testdata('link_extractor', 'linkextractor.html')\n-            self.response = HtmlResponse(url='http://example.com/index', body=body)\n+            body = get_testdata(\"link_extractor\", \"linkextractor.html\")\n+            self.response = HtmlResponse(url=\"http://example.com/index\", body=body)\n \n         def test_urls_type(self):\n-            ''' Test that the resulting urls are str objects '''\n+            \"\"\"Test that the resulting urls are str objects\"\"\"\n             lx = self.extractor_cls()\n-            self.assertTrue(all(isinstance(link.url, str)\n-                                for link in lx.extract_links(self.response)))\n+            self.assertTrue(\n+                all(\n+                    isinstance(link.url, str)\n+                    for link in lx.extract_links(self.response)\n+                )\n+            )\n \n         def test_extract_all_links(self):\n             lx = self.extractor_cls()\n-            page4_url = 'http://example.com/page%204.html'\n+            page4_url = \"http://example.com/page%204.html\"\n \n-            self.assertEqual([link for link in lx.extract_links(self.response)], [\n-                Link(url='http://example.com/sample1.html', text=''),\n-                Link(url='http://example.com/sample2.html', text='sample 2'),\n-                Link(url='http://example.com/sample3.html', text='sample 3 text'),\n-                Link(url='http://example.com/sample3.html#foo', text='sample 3 repetition with fragment'),\n-                Link(url='http://www.google.com/something', text=''),\n-                Link(url='http://example.com/innertag.html', text='inner tag'),\n-                Link(url=page4_url, text='href with whitespaces'),\n-            ])\n+            self.assertEqual(\n+                [link for link in lx.extract_links(self.response)],\n+                [\n+                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n+                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n+                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n+                    Link(\n+                        url=\"http://example.com/sample3.html#foo\",\n+                        text=\"sample 3 repetition with fragment\",\n+                    ),\n+                    Link(url=\"http://www.google.com/something\", text=\"\"),\n+                    Link(url=\"http://example.com/innertag.html\", text=\"inner tag\"),\n+                    Link(url=page4_url, text=\"href with whitespaces\"),\n+                ],\n+            )\n \n         def test_extract_filter_allow(self):\n-            lx = self.extractor_cls(allow=('sample', ))\n-            self.assertEqual([link for link in lx.extract_links(self.response)], [\n-                Link(url='http://example.com/sample1.html', text=''),\n-                Link(url='http://example.com/sample2.html', text='sample 2'),\n-                Link(url='http://example.com/sample3.html', text='sample 3 text'),\n-                Link(url='http://example.com/sample3.html#foo', text='sample 3 repetition with fragment')\n-            ])\n+            lx = self.extractor_cls(allow=(\"sample\",))\n+            self.assertEqual(\n+                [link for link in lx.extract_links(self.response)],\n+                [\n+                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n+                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n+                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n+                    Link(\n+                        url=\"http://example.com/sample3.html#foo\",\n+                        text=\"sample 3 repetition with fragment\",\n+                    ),\n+                ],\n+            )\n \n         def test_extract_filter_allow_with_duplicates(self):\n-            lx = self.extractor_cls(allow=('sample', ), unique=False)\n-            self.assertEqual([link for link in lx.extract_links(self.response)], [\n-                Link(url='http://example.com/sample1.html', text=''),\n-                Link(url='http://example.com/sample2.html', text='sample 2'),\n-                Link(url='http://example.com/sample3.html', text='sample 3 text'),\n-                Link(url='http://example.com/sample3.html', text='sample 3 repetition'),\n-                Link(url='http://example.com/sample3.html#foo', text='sample 3 repetition with fragment')\n-            ])\n+            lx = self.extractor_cls(allow=(\"sample\",), unique=False)\n+            self.assertEqual(\n+                [link for link in lx.extract_links(self.response)],\n+                [\n+                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n+                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n+                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n+                    Link(\n+                        url=\"http://example.com/sample3.html\",\n+                        text=\"sample 3 repetition\",\n+                    ),\n+                    Link(\n+                        url=\"http://example.com/sample3.html#foo\",\n+                        text=\"sample 3 repetition with fragment\",\n+                    ),\n+                ],\n+            )\n \n         def test_extract_filter_allow_with_duplicates_canonicalize(self):\n-            lx = self.extractor_cls(allow=('sample', ), unique=False,\n-                                    canonicalize=True)\n-            self.assertEqual([link for link in lx.extract_links(self.response)], [\n-                Link(url='http://example.com/sample1.html', text=''),\n-                Link(url='http://example.com/sample2.html', text='sample 2'),\n-                Link(url='http://example.com/sample3.html', text='sample 3 text'),\n-                Link(url='http://example.com/sample3.html', text='sample 3 repetition'),\n-                Link(url='http://example.com/sample3.html', text='sample 3 repetition with fragment')\n-            ])\n+            lx = self.extractor_cls(allow=(\"sample\",), unique=False, canonicalize=True)\n+            self.assertEqual(\n+                [link for link in lx.extract_links(self.response)],\n+                [\n+                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n+                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n+                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n+                    Link(\n+                        url=\"http://example.com/sample3.html\",\n+                        text=\"sample 3 repetition\",\n+                    ),\n+                    Link(\n+                        url=\"http://example.com/sample3.html\",\n+                        text=\"sample 3 repetition with fragment\",\n+                    ),\n+                ],\n+            )\n \n         def test_extract_filter_allow_no_duplicates_canonicalize(self):\n-            lx = self.extractor_cls(allow=('sample',), unique=True,\n-                                    canonicalize=True)\n-            self.assertEqual([link for link in lx.extract_links(self.response)], [\n-                Link(url='http://example.com/sample1.html', text=''),\n-                Link(url='http://example.com/sample2.html', text='sample 2'),\n-                Link(url='http://example.com/sample3.html', text='sample 3 text'),\n-            ])\n+            lx = self.extractor_cls(allow=(\"sample\",), unique=True, canonicalize=True)\n+            self.assertEqual(\n+                [link for link in lx.extract_links(self.response)],\n+                [\n+                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n+                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n+                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n+                ],\n+            )\n \n         def test_extract_filter_allow_and_deny(self):\n-            lx = self.extractor_cls(allow=('sample', ), deny=('3', ))\n-            self.assertEqual([link for link in lx.extract_links(self.response)], [\n-                Link(url='http://example.com/sample1.html', text=''),\n-                Link(url='http://example.com/sample2.html', text='sample 2'),\n-            ])\n+            lx = self.extractor_cls(allow=(\"sample\",), deny=(\"3\",))\n+            self.assertEqual(\n+                [link for link in lx.extract_links(self.response)],\n+                [\n+                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n+                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n+                ],\n+            )\n \n         def test_extract_filter_allowed_domains(self):\n-            lx = self.extractor_cls(allow_domains=('google.com', ))\n-            self.assertEqual([link for link in lx.extract_links(self.response)], [\n-                Link(url='http://www.google.com/something', text=''),\n-            ])\n+            lx = self.extractor_cls(allow_domains=(\"google.com\",))\n+            self.assertEqual(\n+                [link for link in lx.extract_links(self.response)],\n+                [\n+                    Link(url=\"http://www.google.com/something\", text=\"\"),\n+                ],\n+            )\n \n         def test_extraction_using_single_values(self):\n-            '''Test the extractor's behaviour among different situations'''\n+            \"\"\"Test the extractor's behaviour among different situations\"\"\"\n \n-            lx = self.extractor_cls(allow='sample')\n-            self.assertEqual([link for link in lx.extract_links(self.response)], [\n-                Link(url='http://example.com/sample1.html', text=''),\n-                Link(url='http://example.com/sample2.html', text='sample 2'),\n-                Link(url='http://example.com/sample3.html', text='sample 3 text'),\n-                Link(url='http://example.com/sample3.html#foo',\n-                     text='sample 3 repetition with fragment')\n-            ])\n+            lx = self.extractor_cls(allow=\"sample\")\n+            self.assertEqual(\n+                [link for link in lx.extract_links(self.response)],\n+                [\n+                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n+                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n+                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n+                    Link(\n+                        url=\"http://example.com/sample3.html#foo\",\n+                        text=\"sample 3 repetition with fragment\",\n+                    ),\n+                ],\n+            )\n \n-            lx = self.extractor_cls(allow='sample', deny='3')\n-            self.assertEqual([link for link in lx.extract_links(self.response)], [\n-                Link(url='http://example.com/sample1.html', text=''),\n-                Link(url='http://example.com/sample2.html', text='sample 2'),\n-            ])\n+            lx = self.extractor_cls(allow=\"sample\", deny=\"3\")\n+            self.assertEqual(\n+                [link for link in lx.extract_links(self.response)],\n+                [\n+                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n+                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n+                ],\n+            )\n \n-            lx = self.extractor_cls(allow_domains='google.com')\n-            self.assertEqual([link for link in lx.extract_links(self.response)], [\n-                Link(url='http://www.google.com/something', text=''),\n-            ])\n+            lx = self.extractor_cls(allow_domains=\"google.com\")\n+            self.assertEqual(\n+                [link for link in lx.extract_links(self.response)],\n+                [\n+                    Link(url=\"http://www.google.com/something\", text=\"\"),\n+                ],\n+            )\n \n-            lx = self.extractor_cls(deny_domains='example.com')\n-            self.assertEqual([link for link in lx.extract_links(self.response)], [\n-                Link(url='http://www.google.com/something', text=''),\n-            ])\n+            lx = self.extractor_cls(deny_domains=\"example.com\")\n+            self.assertEqual(\n+                [link for link in lx.extract_links(self.response)],\n+                [\n+                    Link(url=\"http://www.google.com/something\", text=\"\"),\n+                ],\n+            )\n \n         def test_nofollow(self):\n             '''Test the extractor's behaviour for links with rel=\"nofollow\"'''\n@@ -141,48 +196,68 @@ class Base:\n             response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html)\n \n             lx = self.extractor_cls()\n-            self.assertEqual(lx.extract_links(response), [\n-                Link(url='http://example.org/about.html', text='About us'),\n-                Link(url='http://example.org/follow.html', text='Follow this link'),\n-                Link(url='http://example.org/nofollow.html', text='Dont follow this one', nofollow=True),\n-                Link(url='http://example.org/nofollow2.html', text='Choose to follow or not'),\n-                Link(url='http://google.com/something', text='External link not to follow', nofollow=True),\n-            ])\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [\n+                    Link(url=\"http://example.org/about.html\", text=\"About us\"),\n+                    Link(url=\"http://example.org/follow.html\", text=\"Follow this link\"),\n+                    Link(\n+                        url=\"http://example.org/nofollow.html\",\n+                        text=\"Dont follow this one\",\n+                        nofollow=True,\n+                    ),\n+                    Link(\n+                        url=\"http://example.org/nofollow2.html\",\n+                        text=\"Choose to follow or not\",\n+                    ),\n+                    Link(\n+                        url=\"http://google.com/something\",\n+                        text=\"External link not to follow\",\n+                        nofollow=True,\n+                    ),\n+                ],\n+            )\n \n         def test_matches(self):\n-            url1 = 'http://lotsofstuff.com/stuff1/index'\n-            url2 = 'http://evenmorestuff.com/uglystuff/index'\n+            url1 = \"http://lotsofstuff.com/stuff1/index\"\n+            url2 = \"http://evenmorestuff.com/uglystuff/index\"\n \n-            lx = self.extractor_cls(allow=(r'stuff1', ))\n+            lx = self.extractor_cls(allow=(r\"stuff1\",))\n             self.assertEqual(lx.matches(url1), True)\n             self.assertEqual(lx.matches(url2), False)\n \n-            lx = self.extractor_cls(deny=(r'uglystuff', ))\n+            lx = self.extractor_cls(deny=(r\"uglystuff\",))\n             self.assertEqual(lx.matches(url1), True)\n             self.assertEqual(lx.matches(url2), False)\n \n-            lx = self.extractor_cls(allow_domains=('evenmorestuff.com', ))\n+            lx = self.extractor_cls(allow_domains=(\"evenmorestuff.com\",))\n             self.assertEqual(lx.matches(url1), False)\n             self.assertEqual(lx.matches(url2), True)\n \n-            lx = self.extractor_cls(deny_domains=('lotsofstuff.com', ))\n+            lx = self.extractor_cls(deny_domains=(\"lotsofstuff.com\",))\n             self.assertEqual(lx.matches(url1), False)\n             self.assertEqual(lx.matches(url2), True)\n \n-            lx = self.extractor_cls(allow=['blah1'], deny=['blah2'],\n-                                    allow_domains=['blah1.com'],\n-                                    deny_domains=['blah2.com'])\n-            self.assertEqual(lx.matches('http://blah1.com/blah1'), True)\n-            self.assertEqual(lx.matches('http://blah1.com/blah2'), False)\n-            self.assertEqual(lx.matches('http://blah2.com/blah1'), False)\n-            self.assertEqual(lx.matches('http://blah2.com/blah2'), False)\n+            lx = self.extractor_cls(\n+                allow=[\"blah1\"],\n+                deny=[\"blah2\"],\n+                allow_domains=[\"blah1.com\"],\n+                deny_domains=[\"blah2.com\"],\n+            )\n+            self.assertEqual(lx.matches(\"http://blah1.com/blah1\"), True)\n+            self.assertEqual(lx.matches(\"http://blah1.com/blah2\"), False)\n+            self.assertEqual(lx.matches(\"http://blah2.com/blah1\"), False)\n+            self.assertEqual(lx.matches(\"http://blah2.com/blah2\"), False)\n \n         def test_restrict_xpaths(self):\n             lx = self.extractor_cls(restrict_xpaths=('//div[@id=\"subwrapper\"]',))\n-            self.assertEqual([link for link in lx.extract_links(self.response)], [\n-                Link(url='http://example.com/sample1.html', text=''),\n-                Link(url='http://example.com/sample2.html', text='sample 2'),\n-            ])\n+            self.assertEqual(\n+                [link for link in lx.extract_links(self.response)],\n+                [\n+                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n+                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n+                ],\n+            )\n \n         def test_restrict_xpaths_encoding(self):\n             \"\"\"Test restrict_xpaths with encodings\"\"\"\n@@ -195,84 +270,143 @@ class Base:\n             <p><a href=\"/nofollow.html\">This shouldn't be followed</a></p>\n             </div>\n             </body></html>\"\"\"\n-            response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html, encoding='windows-1252')\n+            response = HtmlResponse(\n+                \"http://example.org/somepage/index.html\",\n+                body=html,\n+                encoding=\"windows-1252\",\n+            )\n \n             lx = self.extractor_cls(restrict_xpaths=\"//div[@class='links']\")\n-            self.assertEqual(lx.extract_links(response),\n-                             [Link(url='http://example.org/about.html', text='About us\\xa3')])\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [Link(url=\"http://example.org/about.html\", text=\"About us\\xa3\")],\n+            )\n \n         def test_restrict_xpaths_with_html_entities(self):\n             html = b'<html><body><p><a href=\"/&hearts;/you?c=&euro;\">text</a></p></body></html>'\n-            response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html, encoding='iso8859-15')\n-            links = self.extractor_cls(restrict_xpaths='//p').extract_links(response)\n-            self.assertEqual(links,\n-                             [Link(url='http://example.org/%E2%99%A5/you?c=%A4', text='text')])\n+            response = HtmlResponse(\n+                \"http://example.org/somepage/index.html\",\n+                body=html,\n+                encoding=\"iso8859-15\",\n+            )\n+            links = self.extractor_cls(restrict_xpaths=\"//p\").extract_links(response)\n+            self.assertEqual(\n+                links, [Link(url=\"http://example.org/%E2%99%A5/you?c=%A4\", text=\"text\")]\n+            )\n \n         def test_restrict_xpaths_concat_in_handle_data(self):\n             \"\"\"html entities cause SGMLParser to call handle_data hook twice\"\"\"\n             body = b\"\"\"<html><body><div><a href=\"/foo\">&gt;\\xbe\\xa9&lt;\\xb6\\xab</a></body></html>\"\"\"\n-            response = HtmlResponse(\"http://example.org\", body=body, encoding='gb18030')\n+            response = HtmlResponse(\"http://example.org\", body=body, encoding=\"gb18030\")\n             lx = self.extractor_cls(restrict_xpaths=\"//div\")\n-            self.assertEqual(lx.extract_links(response),\n-                             [Link(url='http://example.org/foo', text='>\\u4eac<\\u4e1c',\n-                                   fragment='', nofollow=False)])\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [\n+                    Link(\n+                        url=\"http://example.org/foo\",\n+                        text=\">\\u4eac<\\u4e1c\",\n+                        fragment=\"\",\n+                        nofollow=False,\n+                    )\n+                ],\n+            )\n \n         def test_restrict_css(self):\n-            lx = self.extractor_cls(restrict_css=('#subwrapper a',))\n-            self.assertEqual(lx.extract_links(self.response), [\n-                Link(url='http://example.com/sample2.html', text='sample 2')\n-            ])\n+            lx = self.extractor_cls(restrict_css=(\"#subwrapper a\",))\n+            self.assertEqual(\n+                lx.extract_links(self.response),\n+                [Link(url=\"http://example.com/sample2.html\", text=\"sample 2\")],\n+            )\n \n         def test_restrict_css_and_restrict_xpaths_together(self):\n-            lx = self.extractor_cls(restrict_xpaths=('//div[@id=\"subwrapper\"]', ),\n-                                    restrict_css=('#subwrapper + a', ))\n-            self.assertEqual([link for link in lx.extract_links(self.response)], [\n-                Link(url='http://example.com/sample1.html', text=''),\n-                Link(url='http://example.com/sample2.html', text='sample 2'),\n-                Link(url='http://example.com/sample3.html', text='sample 3 text'),\n-            ])\n+            lx = self.extractor_cls(\n+                restrict_xpaths=('//div[@id=\"subwrapper\"]',),\n+                restrict_css=(\"#subwrapper + a\",),\n+            )\n+            self.assertEqual(\n+                [link for link in lx.extract_links(self.response)],\n+                [\n+                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n+                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n+                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n+                ],\n+            )\n \n         def test_area_tag_with_unicode_present(self):\n             body = b\"\"\"<html><body>\\xbe\\xa9<map><area href=\"http://example.org/foo\" /></map></body></html>\"\"\"\n-            response = HtmlResponse(\"http://example.org\", body=body, encoding='utf-8')\n+            response = HtmlResponse(\"http://example.org\", body=body, encoding=\"utf-8\")\n             lx = self.extractor_cls()\n             lx.extract_links(response)\n             lx.extract_links(response)\n             lx.extract_links(response)\n-            self.assertEqual(lx.extract_links(response),\n-                             [Link(url='http://example.org/foo', text='',\n-                                   fragment='', nofollow=False)])\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [\n+                    Link(\n+                        url=\"http://example.org/foo\",\n+                        text=\"\",\n+                        fragment=\"\",\n+                        nofollow=False,\n+                    )\n+                ],\n+            )\n \n         def test_encoded_url(self):\n             body = b\"\"\"<html><body><div><a href=\"?page=2\">BinB</a></body></html>\"\"\"\n-            response = HtmlResponse(\"http://known.fm/AC%2FDC/\", body=body, encoding='utf8')\n+            response = HtmlResponse(\n+                \"http://known.fm/AC%2FDC/\", body=body, encoding=\"utf8\"\n+            )\n             lx = self.extractor_cls()\n-            self.assertEqual(lx.extract_links(response), [\n-                Link(url='http://known.fm/AC%2FDC/?page=2', text='BinB', fragment='', nofollow=False),\n-            ])\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [\n+                    Link(\n+                        url=\"http://known.fm/AC%2FDC/?page=2\",\n+                        text=\"BinB\",\n+                        fragment=\"\",\n+                        nofollow=False,\n+                    ),\n+                ],\n+            )\n \n         def test_encoded_url_in_restricted_xpath(self):\n             body = b\"\"\"<html><body><div><a href=\"?page=2\">BinB</a></body></html>\"\"\"\n-            response = HtmlResponse(\"http://known.fm/AC%2FDC/\", body=body, encoding='utf8')\n+            response = HtmlResponse(\n+                \"http://known.fm/AC%2FDC/\", body=body, encoding=\"utf8\"\n+            )\n             lx = self.extractor_cls(restrict_xpaths=\"//div\")\n-            self.assertEqual(lx.extract_links(response), [\n-                Link(url='http://known.fm/AC%2FDC/?page=2', text='BinB', fragment='', nofollow=False),\n-            ])\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [\n+                    Link(\n+                        url=\"http://known.fm/AC%2FDC/?page=2\",\n+                        text=\"BinB\",\n+                        fragment=\"\",\n+                        nofollow=False,\n+                    ),\n+                ],\n+            )\n \n         def test_ignored_extensions(self):\n             # jpg is ignored by default\n             html = b\"\"\"<a href=\"page.html\">asd</a> and <a href=\"photo.jpg\">\"\"\"\n             response = HtmlResponse(\"http://example.org/\", body=html)\n             lx = self.extractor_cls()\n-            self.assertEqual(lx.extract_links(response), [\n-                Link(url='http://example.org/page.html', text='asd'),\n-            ])\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [\n+                    Link(url=\"http://example.org/page.html\", text=\"asd\"),\n+                ],\n+            )\n \n             # override denied extensions\n-            lx = self.extractor_cls(deny_extensions=['html'])\n-            self.assertEqual(lx.extract_links(response), [\n-                Link(url='http://example.org/photo.jpg'),\n-            ])\n+            lx = self.extractor_cls(deny_extensions=[\"html\"])\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [\n+                    Link(url=\"http://example.org/photo.jpg\"),\n+                ],\n+            )\n \n         def test_process_value(self):\n             \"\"\"Test restrict_xpaths with encodings\"\"\"\n@@ -280,7 +414,11 @@ class Base:\n <a href=\"javascript:goToPage('../other/page.html','photo','width=600,height=540,scrollbars'); return false\">Text</a>\n <a href=\"/about.html\">About us</a>\n             \"\"\"\n-            response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html, encoding='windows-1252')\n+            response = HtmlResponse(\n+                \"http://example.org/somepage/index.html\",\n+                body=html,\n+                encoding=\"windows-1252\",\n+            )\n \n             def process_value(value):\n                 m = re.search(r\"javascript:goToPage\\('(.*?)'\", value)\n@@ -288,8 +426,10 @@ class Base:\n                     return m.group(1)\n \n             lx = self.extractor_cls(process_value=process_value)\n-            self.assertEqual(lx.extract_links(response),\n-                             [Link(url='http://example.org/other/page.html', text='Text')])\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [Link(url=\"http://example.org/other/page.html\", text=\"Text\")],\n+            )\n \n         def test_base_url_with_restrict_xpaths(self):\n             html = b\"\"\"<html><head><title>Page title<title><base href=\"http://otherdomain.com/base/\" />\n@@ -297,34 +437,50 @@ class Base:\n             </body></html>\"\"\"\n             response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html)\n             lx = self.extractor_cls(restrict_xpaths=\"//p\")\n-            self.assertEqual(lx.extract_links(response),\n-                             [Link(url='http://otherdomain.com/base/item/12.html', text='Item 12')])\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [Link(url=\"http://otherdomain.com/base/item/12.html\", text=\"Item 12\")],\n+            )\n \n         def test_attrs(self):\n             lx = self.extractor_cls(attrs=\"href\")\n-            page4_url = 'http://example.com/page%204.html'\n+            page4_url = \"http://example.com/page%204.html\"\n \n-            self.assertEqual(lx.extract_links(self.response), [\n-                Link(url='http://example.com/sample1.html', text=''),\n-                Link(url='http://example.com/sample2.html', text='sample 2'),\n-                Link(url='http://example.com/sample3.html', text='sample 3 text'),\n-                Link(url='http://example.com/sample3.html#foo', text='sample 3 repetition with fragment'),\n-                Link(url='http://www.google.com/something', text=''),\n-                Link(url='http://example.com/innertag.html', text='inner tag'),\n-                Link(url=page4_url, text='href with whitespaces'),\n-            ])\n+            self.assertEqual(\n+                lx.extract_links(self.response),\n+                [\n+                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n+                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n+                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n+                    Link(\n+                        url=\"http://example.com/sample3.html#foo\",\n+                        text=\"sample 3 repetition with fragment\",\n+                    ),\n+                    Link(url=\"http://www.google.com/something\", text=\"\"),\n+                    Link(url=\"http://example.com/innertag.html\", text=\"inner tag\"),\n+                    Link(url=page4_url, text=\"href with whitespaces\"),\n+                ],\n+            )\n \n-            lx = self.extractor_cls(attrs=(\"href\", \"src\"), tags=(\"a\", \"area\", \"img\"), deny_extensions=())\n-            self.assertEqual(lx.extract_links(self.response), [\n-                Link(url='http://example.com/sample1.html', text=''),\n-                Link(url='http://example.com/sample2.html', text='sample 2'),\n-                Link(url='http://example.com/sample2.jpg', text=''),\n-                Link(url='http://example.com/sample3.html', text='sample 3 text'),\n-                Link(url='http://example.com/sample3.html#foo', text='sample 3 repetition with fragment'),\n-                Link(url='http://www.google.com/something', text=''),\n-                Link(url='http://example.com/innertag.html', text='inner tag'),\n-                Link(url=page4_url, text='href with whitespaces'),\n-            ])\n+            lx = self.extractor_cls(\n+                attrs=(\"href\", \"src\"), tags=(\"a\", \"area\", \"img\"), deny_extensions=()\n+            )\n+            self.assertEqual(\n+                lx.extract_links(self.response),\n+                [\n+                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n+                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n+                    Link(url=\"http://example.com/sample2.jpg\", text=\"\"),\n+                    Link(url=\"http://example.com/sample3.html\", text=\"sample 3 text\"),\n+                    Link(\n+                        url=\"http://example.com/sample3.html#foo\",\n+                        text=\"sample 3 repetition with fragment\",\n+                    ),\n+                    Link(url=\"http://www.google.com/something\", text=\"\"),\n+                    Link(url=\"http://example.com/innertag.html\", text=\"inner tag\"),\n+                    Link(url=page4_url, text=\"href with whitespaces\"),\n+                ],\n+            )\n \n             lx = self.extractor_cls(attrs=None)\n             self.assertEqual(lx.extract_links(self.response), [])\n@@ -340,26 +496,40 @@ class Base:\n             self.assertEqual(lx.extract_links(response), [])\n \n             lx = self.extractor_cls()\n-            self.assertEqual(lx.extract_links(response), [\n-                Link(url='http://example.com/sample1.html', text=''),\n-                Link(url='http://example.com/sample2.html', text='sample 2'),\n-            ])\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [\n+                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n+                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n+                ],\n+            )\n \n             lx = self.extractor_cls(tags=\"area\")\n-            self.assertEqual(lx.extract_links(response), [\n-                Link(url='http://example.com/sample1.html', text=''),\n-            ])\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [\n+                    Link(url=\"http://example.com/sample1.html\", text=\"\"),\n+                ],\n+            )\n \n             lx = self.extractor_cls(tags=\"a\")\n-            self.assertEqual(lx.extract_links(response), [\n-                Link(url='http://example.com/sample2.html', text='sample 2'),\n-            ])\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [\n+                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n+                ],\n+            )\n \n-            lx = self.extractor_cls(tags=(\"a\", \"img\"), attrs=(\"href\", \"src\"), deny_extensions=())\n-            self.assertEqual(lx.extract_links(response), [\n-                Link(url='http://example.com/sample2.html', text='sample 2'),\n-                Link(url='http://example.com/sample2.jpg', text=''),\n-            ])\n+            lx = self.extractor_cls(\n+                tags=(\"a\", \"img\"), attrs=(\"href\", \"src\"), deny_extensions=()\n+            )\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [\n+                    Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n+                    Link(url=\"http://example.com/sample2.jpg\", text=\"\"),\n+                ],\n+            )\n \n         def test_tags_attrs(self):\n             html = b\"\"\"\n@@ -370,17 +540,43 @@ class Base:\n             \"\"\"\n             response = HtmlResponse(\"http://example.com/index.html\", body=html)\n \n-            lx = self.extractor_cls(tags='div', attrs='data-url')\n-            self.assertEqual(lx.extract_links(response), [\n-                Link(url='http://example.com/get?id=1', text='Item 1', fragment='', nofollow=False),\n-                Link(url='http://example.com/get?id=2', text='Item 2', fragment='', nofollow=False)\n-            ])\n+            lx = self.extractor_cls(tags=\"div\", attrs=\"data-url\")\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [\n+                    Link(\n+                        url=\"http://example.com/get?id=1\",\n+                        text=\"Item 1\",\n+                        fragment=\"\",\n+                        nofollow=False,\n+                    ),\n+                    Link(\n+                        url=\"http://example.com/get?id=2\",\n+                        text=\"Item 2\",\n+                        fragment=\"\",\n+                        nofollow=False,\n+                    ),\n+                ],\n+            )\n \n-            lx = self.extractor_cls(tags=('div',), attrs=('data-url',))\n-            self.assertEqual(lx.extract_links(response), [\n-                Link(url='http://example.com/get?id=1', text='Item 1', fragment='', nofollow=False),\n-                Link(url='http://example.com/get?id=2', text='Item 2', fragment='', nofollow=False)\n-            ])\n+            lx = self.extractor_cls(tags=(\"div\",), attrs=(\"data-url\",))\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [\n+                    Link(\n+                        url=\"http://example.com/get?id=1\",\n+                        text=\"Item 1\",\n+                        fragment=\"\",\n+                        nofollow=False,\n+                    ),\n+                    Link(\n+                        url=\"http://example.com/get?id=2\",\n+                        text=\"Item 2\",\n+                        fragment=\"\",\n+                        nofollow=False,\n+                    ),\n+                ],\n+            )\n \n         def test_xhtml(self):\n             xhtml = b\"\"\"\n@@ -417,14 +613,36 @@ class Base:\n             self.assertEqual(\n                 lx.extract_links(response),\n                 [\n-                    Link(url='http://example.com/about.html', text='About us', fragment='', nofollow=False),\n-                    Link(url='http://example.com/follow.html', text='Follow this link', fragment='', nofollow=False),\n-                    Link(url='http://example.com/nofollow.html', text='Dont follow this one',\n-                         fragment='', nofollow=True),\n-                    Link(url='http://example.com/nofollow2.html', text='Choose to follow or not',\n-                         fragment='', nofollow=False),\n-                    Link(url='http://google.com/something', text='External link not to follow', nofollow=True),\n-                ]\n+                    Link(\n+                        url=\"http://example.com/about.html\",\n+                        text=\"About us\",\n+                        fragment=\"\",\n+                        nofollow=False,\n+                    ),\n+                    Link(\n+                        url=\"http://example.com/follow.html\",\n+                        text=\"Follow this link\",\n+                        fragment=\"\",\n+                        nofollow=False,\n+                    ),\n+                    Link(\n+                        url=\"http://example.com/nofollow.html\",\n+                        text=\"Dont follow this one\",\n+                        fragment=\"\",\n+                        nofollow=True,\n+                    ),\n+                    Link(\n+                        url=\"http://example.com/nofollow2.html\",\n+                        text=\"Choose to follow or not\",\n+                        fragment=\"\",\n+                        nofollow=False,\n+                    ),\n+                    Link(\n+                        url=\"http://google.com/something\",\n+                        text=\"External link not to follow\",\n+                        nofollow=True,\n+                    ),\n+                ],\n             )\n \n             response = XmlResponse(\"http://example.com/index.xhtml\", body=xhtml)\n@@ -433,14 +651,36 @@ class Base:\n             self.assertEqual(\n                 lx.extract_links(response),\n                 [\n-                    Link(url='http://example.com/about.html', text='About us', fragment='', nofollow=False),\n-                    Link(url='http://example.com/follow.html', text='Follow this link', fragment='', nofollow=False),\n-                    Link(url='http://example.com/nofollow.html', text='Dont follow this one',\n-                         fragment='', nofollow=True),\n-                    Link(url='http://example.com/nofollow2.html', text='Choose to follow or not',\n-                         fragment='', nofollow=False),\n-                    Link(url='http://google.com/something', text='External link not to follow', nofollow=True),\n-                ]\n+                    Link(\n+                        url=\"http://example.com/about.html\",\n+                        text=\"About us\",\n+                        fragment=\"\",\n+                        nofollow=False,\n+                    ),\n+                    Link(\n+                        url=\"http://example.com/follow.html\",\n+                        text=\"Follow this link\",\n+                        fragment=\"\",\n+                        nofollow=False,\n+                    ),\n+                    Link(\n+                        url=\"http://example.com/nofollow.html\",\n+                        text=\"Dont follow this one\",\n+                        fragment=\"\",\n+                        nofollow=True,\n+                    ),\n+                    Link(\n+                        url=\"http://example.com/nofollow2.html\",\n+                        text=\"Choose to follow or not\",\n+                        fragment=\"\",\n+                        nofollow=False,\n+                    ),\n+                    Link(\n+                        url=\"http://google.com/something\",\n+                        text=\"External link not to follow\",\n+                        nofollow=True,\n+                    ),\n+                ],\n             )\n \n         def test_link_wrong_href(self):\n@@ -451,21 +691,42 @@ class Base:\n             \"\"\"\n             response = HtmlResponse(\"http://example.org/index.html\", body=html)\n             lx = self.extractor_cls()\n-            self.assertEqual([link for link in lx.extract_links(response)], [\n-                Link(url='http://example.org/item1.html', text='Item 1', nofollow=False),\n-                Link(url='http://example.org/item3.html', text='Item 3', nofollow=False),\n-            ])\n+            self.assertEqual(\n+                [link for link in lx.extract_links(response)],\n+                [\n+                    Link(\n+                        url=\"http://example.org/item1.html\",\n+                        text=\"Item 1\",\n+                        nofollow=False,\n+                    ),\n+                    Link(\n+                        url=\"http://example.org/item3.html\",\n+                        text=\"Item 3\",\n+                        nofollow=False,\n+                    ),\n+                ],\n+            )\n \n         def test_ftp_links(self):\n             body = b\"\"\"\n             <html><body>\n             <div><a href=\"ftp://www.external.com/\">An Item</a></div>\n             </body></html>\"\"\"\n-            response = HtmlResponse(\"http://www.example.com/index.html\", body=body, encoding='utf8')\n+            response = HtmlResponse(\n+                \"http://www.example.com/index.html\", body=body, encoding=\"utf8\"\n+            )\n             lx = self.extractor_cls()\n-            self.assertEqual(lx.extract_links(response), [\n-                Link(url='ftp://www.external.com/', text='An Item', fragment='', nofollow=False),\n-            ])\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [\n+                    Link(\n+                        url=\"ftp://www.external.com/\",\n+                        text=\"An Item\",\n+                        fragment=\"\",\n+                        nofollow=False,\n+                    ),\n+                ],\n+            )\n \n         def test_pickle_extractor(self):\n             lx = self.extractor_cls()\n@@ -483,10 +744,17 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n         \"\"\"\n         response = HtmlResponse(\"http://example.org/index.html\", body=html)\n         lx = self.extractor_cls()\n-        self.assertEqual([link for link in lx.extract_links(response)], [\n-            Link(url='http://example.org/item1.html', text='Item 1', nofollow=False),\n-            Link(url='http://example.org/item3.html', text='Item 3', nofollow=False),\n-        ])\n+        self.assertEqual(\n+            [link for link in lx.extract_links(response)],\n+            [\n+                Link(\n+                    url=\"http://example.org/item1.html\", text=\"Item 1\", nofollow=False\n+                ),\n+                Link(\n+                    url=\"http://example.org/item3.html\", text=\"Item 3\", nofollow=False\n+                ),\n+            ],\n+        )\n \n     def test_link_restrict_text(self):\n         html = b\"\"\"\n@@ -496,21 +764,46 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n         \"\"\"\n         response = HtmlResponse(\"http://example.org/index.html\", body=html)\n         # Simple text inclusion test\n-        lx = self.extractor_cls(restrict_text='dog')\n-        self.assertEqual([link for link in lx.extract_links(response)], [\n-            Link(url='http://example.org/item2.html', text='Pic of a dog', nofollow=False),\n-        ])\n+        lx = self.extractor_cls(restrict_text=\"dog\")\n+        self.assertEqual(\n+            [link for link in lx.extract_links(response)],\n+            [\n+                Link(\n+                    url=\"http://example.org/item2.html\",\n+                    text=\"Pic of a dog\",\n+                    nofollow=False,\n+                ),\n+            ],\n+        )\n         # Unique regex test\n-        lx = self.extractor_cls(restrict_text=r'of.*dog')\n-        self.assertEqual([link for link in lx.extract_links(response)], [\n-            Link(url='http://example.org/item2.html', text='Pic of a dog', nofollow=False),\n-        ])\n+        lx = self.extractor_cls(restrict_text=r\"of.*dog\")\n+        self.assertEqual(\n+            [link for link in lx.extract_links(response)],\n+            [\n+                Link(\n+                    url=\"http://example.org/item2.html\",\n+                    text=\"Pic of a dog\",\n+                    nofollow=False,\n+                ),\n+            ],\n+        )\n         # Multiple regex test\n-        lx = self.extractor_cls(restrict_text=[r'of.*dog', r'of.*cat'])\n-        self.assertEqual([link for link in lx.extract_links(response)], [\n-            Link(url='http://example.org/item1.html', text='Pic of a cat', nofollow=False),\n-            Link(url='http://example.org/item2.html', text='Pic of a dog', nofollow=False),\n-        ])\n+        lx = self.extractor_cls(restrict_text=[r\"of.*dog\", r\"of.*cat\"])\n+        self.assertEqual(\n+            [link for link in lx.extract_links(response)],\n+            [\n+                Link(\n+                    url=\"http://example.org/item1.html\",\n+                    text=\"Pic of a cat\",\n+                    nofollow=False,\n+                ),\n+                Link(\n+                    url=\"http://example.org/item2.html\",\n+                    text=\"Pic of a dog\",\n+                    nofollow=False,\n+                ),\n+            ],\n+        )\n \n     def test_restrict_xpaths_with_html_entities(self):\n         super().test_restrict_xpaths_with_html_entities()\n\n@@ -59,32 +59,31 @@ class DefaultedItemLoader(NameItemLoader):\n \n # test processors\n def processor_with_args(value, other=None, loader_context=None):\n-    if 'key' in loader_context:\n-        return loader_context['key']\n+    if \"key\" in loader_context:\n+        return loader_context[\"key\"]\n     return value\n \n \n class BasicItemLoaderTest(unittest.TestCase):\n-\n     def test_add_value_on_unknown_field(self):\n         il = TestItemLoader()\n-        self.assertRaises(KeyError, il.add_value, 'wrong_field', ['lala', 'lolo'])\n+        self.assertRaises(KeyError, il.add_value, \"wrong_field\", [\"lala\", \"lolo\"])\n \n     def test_load_item_using_default_loader(self):\n         i = TestItem()\n-        i['summary'] = 'lala'\n+        i[\"summary\"] = \"lala\"\n         il = ItemLoader(item=i)\n-        il.add_value('name', 'marta')\n+        il.add_value(\"name\", \"marta\")\n         item = il.load_item()\n         assert item is i\n-        self.assertEqual(item['summary'], ['lala'])\n-        self.assertEqual(item['name'], ['marta'])\n+        self.assertEqual(item[\"summary\"], [\"lala\"])\n+        self.assertEqual(item[\"name\"], [\"marta\"])\n \n     def test_load_item_using_custom_loader(self):\n         il = TestItemLoader()\n-        il.add_value('name', 'marta')\n+        il.add_value(\"name\", \"marta\")\n         item = il.load_item()\n-        self.assertEqual(item['name'], ['Marta'])\n+        self.assertEqual(item[\"name\"], [\"Marta\"])\n \n \n class InitializationTestMixin:\n@@ -93,85 +92,94 @@ class InitializationTestMixin:\n \n     def test_keep_single_value(self):\n         \"\"\"Loaded item should contain values from the initial item\"\"\"\n-        input_item = self.item_class(name='foo')\n+        input_item = self.item_class(name=\"foo\")\n         il = ItemLoader(item=input_item)\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo']})\n+        self.assertEqual(ItemAdapter(loaded_item).asdict(), {\"name\": [\"foo\"]})\n \n     def test_keep_list(self):\n         \"\"\"Loaded item should contain values from the initial item\"\"\"\n-        input_item = self.item_class(name=['foo', 'bar'])\n+        input_item = self.item_class(name=[\"foo\", \"bar\"])\n         il = ItemLoader(item=input_item)\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo', 'bar']})\n+        self.assertEqual(ItemAdapter(loaded_item).asdict(), {\"name\": [\"foo\", \"bar\"]})\n \n     def test_add_value_singlevalue_singlevalue(self):\n         \"\"\"Values added after initialization should be appended\"\"\"\n-        input_item = self.item_class(name='foo')\n+        input_item = self.item_class(name=\"foo\")\n         il = ItemLoader(item=input_item)\n-        il.add_value('name', 'bar')\n+        il.add_value(\"name\", \"bar\")\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo', 'bar']})\n+        self.assertEqual(ItemAdapter(loaded_item).asdict(), {\"name\": [\"foo\", \"bar\"]})\n \n     def test_add_value_singlevalue_list(self):\n         \"\"\"Values added after initialization should be appended\"\"\"\n-        input_item = self.item_class(name='foo')\n+        input_item = self.item_class(name=\"foo\")\n         il = ItemLoader(item=input_item)\n-        il.add_value('name', ['item', 'loader'])\n+        il.add_value(\"name\", [\"item\", \"loader\"])\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo', 'item', 'loader']})\n+        self.assertEqual(\n+            ItemAdapter(loaded_item).asdict(), {\"name\": [\"foo\", \"item\", \"loader\"]}\n+        )\n \n     def test_add_value_list_singlevalue(self):\n         \"\"\"Values added after initialization should be appended\"\"\"\n-        input_item = self.item_class(name=['foo', 'bar'])\n+        input_item = self.item_class(name=[\"foo\", \"bar\"])\n         il = ItemLoader(item=input_item)\n-        il.add_value('name', 'qwerty')\n+        il.add_value(\"name\", \"qwerty\")\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo', 'bar', 'qwerty']})\n+        self.assertEqual(\n+            ItemAdapter(loaded_item).asdict(), {\"name\": [\"foo\", \"bar\", \"qwerty\"]}\n+        )\n \n     def test_add_value_list_list(self):\n         \"\"\"Values added after initialization should be appended\"\"\"\n-        input_item = self.item_class(name=['foo', 'bar'])\n+        input_item = self.item_class(name=[\"foo\", \"bar\"])\n         il = ItemLoader(item=input_item)\n-        il.add_value('name', ['item', 'loader'])\n+        il.add_value(\"name\", [\"item\", \"loader\"])\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo', 'bar', 'item', 'loader']})\n+        self.assertEqual(\n+            ItemAdapter(loaded_item).asdict(),\n+            {\"name\": [\"foo\", \"bar\", \"item\", \"loader\"]},\n+        )\n \n     def test_get_output_value_singlevalue(self):\n         \"\"\"Getting output value must not remove value from item\"\"\"\n-        input_item = self.item_class(name='foo')\n+        input_item = self.item_class(name=\"foo\")\n         il = ItemLoader(item=input_item)\n-        self.assertEqual(il.get_output_value('name'), ['foo'])\n+        self.assertEqual(il.get_output_value(\"name\"), [\"foo\"])\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(ItemAdapter(loaded_item).asdict(), dict({'name': ['foo']}))\n+        self.assertEqual(ItemAdapter(loaded_item).asdict(), dict({\"name\": [\"foo\"]}))\n \n     def test_get_output_value_list(self):\n         \"\"\"Getting output value must not remove value from item\"\"\"\n-        input_item = self.item_class(name=['foo', 'bar'])\n+        input_item = self.item_class(name=[\"foo\", \"bar\"])\n         il = ItemLoader(item=input_item)\n-        self.assertEqual(il.get_output_value('name'), ['foo', 'bar'])\n+        self.assertEqual(il.get_output_value(\"name\"), [\"foo\", \"bar\"])\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(ItemAdapter(loaded_item).asdict(), dict({'name': ['foo', 'bar']}))\n+        self.assertEqual(\n+            ItemAdapter(loaded_item).asdict(), dict({\"name\": [\"foo\", \"bar\"]})\n+        )\n \n     def test_values_single(self):\n         \"\"\"Values from initial item must be added to loader._values\"\"\"\n-        input_item = self.item_class(name='foo')\n+        input_item = self.item_class(name=\"foo\")\n         il = ItemLoader(item=input_item)\n-        self.assertEqual(il._values.get('name'), ['foo'])\n+        self.assertEqual(il._values.get(\"name\"), [\"foo\"])\n \n     def test_values_list(self):\n         \"\"\"Values from initial item must be added to loader._values\"\"\"\n-        input_item = self.item_class(name=['foo', 'bar'])\n+        input_item = self.item_class(name=[\"foo\", \"bar\"])\n         il = ItemLoader(item=input_item)\n-        self.assertEqual(il._values.get('name'), ['foo', 'bar'])\n+        self.assertEqual(il._values.get(\"name\"), [\"foo\", \"bar\"])\n \n \n class InitializationFromDictTest(InitializationTestMixin, unittest.TestCase):\n@@ -207,42 +215,52 @@ class NoInputReprocessingFromItemTest(unittest.TestCase):\n     \"\"\"\n     Loaders initialized from loaded items must not reprocess fields (Item instances)\n     \"\"\"\n+\n     def test_avoid_reprocessing_with_initial_values_single(self):\n-        il = NoInputReprocessingItemLoader(item=NoInputReprocessingItem(title='foo'))\n+        il = NoInputReprocessingItemLoader(item=NoInputReprocessingItem(title=\"foo\"))\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, {'title': 'foo'})\n-        self.assertEqual(NoInputReprocessingItemLoader(item=il_loaded).load_item(), {'title': 'foo'})\n+        self.assertEqual(il_loaded, {\"title\": \"foo\"})\n+        self.assertEqual(\n+            NoInputReprocessingItemLoader(item=il_loaded).load_item(), {\"title\": \"foo\"}\n+        )\n \n     def test_avoid_reprocessing_with_initial_values_list(self):\n-        il = NoInputReprocessingItemLoader(item=NoInputReprocessingItem(title=['foo', 'bar']))\n+        il = NoInputReprocessingItemLoader(\n+            item=NoInputReprocessingItem(title=[\"foo\", \"bar\"])\n+        )\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, {'title': 'foo'})\n-        self.assertEqual(NoInputReprocessingItemLoader(item=il_loaded).load_item(), {'title': 'foo'})\n+        self.assertEqual(il_loaded, {\"title\": \"foo\"})\n+        self.assertEqual(\n+            NoInputReprocessingItemLoader(item=il_loaded).load_item(), {\"title\": \"foo\"}\n+        )\n \n     def test_avoid_reprocessing_without_initial_values_single(self):\n         il = NoInputReprocessingItemLoader()\n-        il.add_value('title', 'FOO')\n+        il.add_value(\"title\", \"FOO\")\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, {'title': 'FOO'})\n-        self.assertEqual(NoInputReprocessingItemLoader(item=il_loaded).load_item(), {'title': 'FOO'})\n+        self.assertEqual(il_loaded, {\"title\": \"FOO\"})\n+        self.assertEqual(\n+            NoInputReprocessingItemLoader(item=il_loaded).load_item(), {\"title\": \"FOO\"}\n+        )\n \n     def test_avoid_reprocessing_without_initial_values_list(self):\n         il = NoInputReprocessingItemLoader()\n-        il.add_value('title', ['foo', 'bar'])\n+        il.add_value(\"title\", [\"foo\", \"bar\"])\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, {'title': 'FOO'})\n-        self.assertEqual(NoInputReprocessingItemLoader(item=il_loaded).load_item(), {'title': 'FOO'})\n+        self.assertEqual(il_loaded, {\"title\": \"FOO\"})\n+        self.assertEqual(\n+            NoInputReprocessingItemLoader(item=il_loaded).load_item(), {\"title\": \"FOO\"}\n+        )\n \n \n class TestOutputProcessorItem(unittest.TestCase):\n     def test_output_processor(self):\n-\n         class TempItem(Item):\n             temp = Field()\n \n             def __init__(self, *args, **kwargs):\n                 super().__init__(self, *args, **kwargs)\n-                self.setdefault('temp', 0.3)\n+                self.setdefault(\"temp\", 0.3)\n \n         class TempLoader(ItemLoader):\n             default_item_class = TempItem\n@@ -252,11 +270,14 @@ class TestOutputProcessorItem(unittest.TestCase):\n         loader = TempLoader()\n         item = loader.load_item()\n         self.assertIsInstance(item, TempItem)\n-        self.assertEqual(dict(item), {'temp': 0.3})\n+        self.assertEqual(dict(item), {\"temp\": 0.3})\n \n \n class SelectortemLoaderTest(unittest.TestCase):\n-    response = HtmlResponse(url=\"\", encoding='utf-8', body=b\"\"\"\n+    response = HtmlResponse(\n+        url=\"\",\n+        encoding=\"utf-8\",\n+        body=b\"\"\"\n     <html>\n     <body>\n     <div id=\"id\">marta</div>\n@@ -265,7 +286,8 @@ class SelectortemLoaderTest(unittest.TestCase):\n     <img src=\"/images/logo.png\" width=\"244\" height=\"65\" alt=\"Scrapy\">\n     </body>\n     </html>\n-    \"\"\")\n+    \"\"\",\n+    )\n \n     def test_init_method(self):\n         l = TestItemLoader()\n@@ -273,28 +295,28 @@ class SelectortemLoaderTest(unittest.TestCase):\n \n     def test_init_method_errors(self):\n         l = TestItemLoader()\n-        self.assertRaises(RuntimeError, l.add_xpath, 'url', '//a/@href')\n-        self.assertRaises(RuntimeError, l.replace_xpath, 'url', '//a/@href')\n-        self.assertRaises(RuntimeError, l.get_xpath, '//a/@href')\n-        self.assertRaises(RuntimeError, l.add_css, 'name', '#name::text')\n-        self.assertRaises(RuntimeError, l.replace_css, 'name', '#name::text')\n-        self.assertRaises(RuntimeError, l.get_css, '#name::text')\n+        self.assertRaises(RuntimeError, l.add_xpath, \"url\", \"//a/@href\")\n+        self.assertRaises(RuntimeError, l.replace_xpath, \"url\", \"//a/@href\")\n+        self.assertRaises(RuntimeError, l.get_xpath, \"//a/@href\")\n+        self.assertRaises(RuntimeError, l.add_css, \"name\", \"#name::text\")\n+        self.assertRaises(RuntimeError, l.replace_css, \"name\", \"#name::text\")\n+        self.assertRaises(RuntimeError, l.get_css, \"#name::text\")\n \n     def test_init_method_with_selector(self):\n         sel = Selector(text=\"<html><body><div>marta</div></body></html>\")\n         l = TestItemLoader(selector=sel)\n         self.assertIs(l.selector, sel)\n \n-        l.add_xpath('name', '//div/text()')\n-        self.assertEqual(l.get_output_value('name'), ['Marta'])\n+        l.add_xpath(\"name\", \"//div/text()\")\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n \n     def test_init_method_with_selector_css(self):\n         sel = Selector(text=\"<html><body><div>marta</div></body></html>\")\n         l = TestItemLoader(selector=sel)\n         self.assertIs(l.selector, sel)\n \n-        l.add_css('name', 'div::text')\n-        self.assertEqual(l.get_output_value('name'), ['Marta'])\n+        l.add_css(\"name\", \"div::text\")\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n \n     def test_init_method_with_base_response(self):\n         \"\"\"Selector should be None after initialization\"\"\"\n@@ -306,122 +328,131 @@ class SelectortemLoaderTest(unittest.TestCase):\n         l = TestItemLoader(response=self.response)\n         self.assertTrue(l.selector)\n \n-        l.add_xpath('name', '//div/text()')\n-        self.assertEqual(l.get_output_value('name'), ['Marta'])\n+        l.add_xpath(\"name\", \"//div/text()\")\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n \n     def test_init_method_with_response_css(self):\n         l = TestItemLoader(response=self.response)\n         self.assertTrue(l.selector)\n \n-        l.add_css('name', 'div::text')\n-        self.assertEqual(l.get_output_value('name'), ['Marta'])\n+        l.add_css(\"name\", \"div::text\")\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n \n-        l.add_css('url', 'a::attr(href)')\n-        self.assertEqual(l.get_output_value('url'), ['http://www.scrapy.org'])\n+        l.add_css(\"url\", \"a::attr(href)\")\n+        self.assertEqual(l.get_output_value(\"url\"), [\"http://www.scrapy.org\"])\n \n         # combining/accumulating CSS selectors and XPath expressions\n-        l.add_xpath('name', '//div/text()')\n-        self.assertEqual(l.get_output_value('name'), ['Marta', 'Marta'])\n+        l.add_xpath(\"name\", \"//div/text()\")\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\", \"Marta\"])\n \n-        l.add_xpath('url', '//img/@src')\n-        self.assertEqual(l.get_output_value('url'), ['http://www.scrapy.org', '/images/logo.png'])\n+        l.add_xpath(\"url\", \"//img/@src\")\n+        self.assertEqual(\n+            l.get_output_value(\"url\"), [\"http://www.scrapy.org\", \"/images/logo.png\"]\n+        )\n \n     def test_add_xpath_re(self):\n         l = TestItemLoader(response=self.response)\n-        l.add_xpath('name', '//div/text()', re='ma')\n-        self.assertEqual(l.get_output_value('name'), ['Ma'])\n+        l.add_xpath(\"name\", \"//div/text()\", re=\"ma\")\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Ma\"])\n \n     def test_replace_xpath(self):\n         l = TestItemLoader(response=self.response)\n         self.assertTrue(l.selector)\n-        l.add_xpath('name', '//div/text()')\n-        self.assertEqual(l.get_output_value('name'), ['Marta'])\n-        l.replace_xpath('name', '//p/text()')\n-        self.assertEqual(l.get_output_value('name'), ['Paragraph'])\n+        l.add_xpath(\"name\", \"//div/text()\")\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n+        l.replace_xpath(\"name\", \"//p/text()\")\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Paragraph\"])\n \n-        l.replace_xpath('name', ['//p/text()', '//div/text()'])\n-        self.assertEqual(l.get_output_value('name'), ['Paragraph', 'Marta'])\n+        l.replace_xpath(\"name\", [\"//p/text()\", \"//div/text()\"])\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Paragraph\", \"Marta\"])\n \n     def test_get_xpath(self):\n         l = TestItemLoader(response=self.response)\n-        self.assertEqual(l.get_xpath('//p/text()'), ['paragraph'])\n-        self.assertEqual(l.get_xpath('//p/text()', TakeFirst()), 'paragraph')\n-        self.assertEqual(l.get_xpath('//p/text()', TakeFirst(), re='pa'), 'pa')\n+        self.assertEqual(l.get_xpath(\"//p/text()\"), [\"paragraph\"])\n+        self.assertEqual(l.get_xpath(\"//p/text()\", TakeFirst()), \"paragraph\")\n+        self.assertEqual(l.get_xpath(\"//p/text()\", TakeFirst(), re=\"pa\"), \"pa\")\n \n-        self.assertEqual(l.get_xpath(['//p/text()', '//div/text()']), ['paragraph', 'marta'])\n+        self.assertEqual(\n+            l.get_xpath([\"//p/text()\", \"//div/text()\"]), [\"paragraph\", \"marta\"]\n+        )\n \n     def test_replace_xpath_multi_fields(self):\n         l = TestItemLoader(response=self.response)\n-        l.add_xpath(None, '//div/text()', TakeFirst(), lambda x: {'name': x})\n-        self.assertEqual(l.get_output_value('name'), ['Marta'])\n-        l.replace_xpath(None, '//p/text()', TakeFirst(), lambda x: {'name': x})\n-        self.assertEqual(l.get_output_value('name'), ['Paragraph'])\n+        l.add_xpath(None, \"//div/text()\", TakeFirst(), lambda x: {\"name\": x})\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n+        l.replace_xpath(None, \"//p/text()\", TakeFirst(), lambda x: {\"name\": x})\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Paragraph\"])\n \n     def test_replace_xpath_re(self):\n         l = TestItemLoader(response=self.response)\n         self.assertTrue(l.selector)\n-        l.add_xpath('name', '//div/text()')\n-        self.assertEqual(l.get_output_value('name'), ['Marta'])\n-        l.replace_xpath('name', '//div/text()', re='ma')\n-        self.assertEqual(l.get_output_value('name'), ['Ma'])\n+        l.add_xpath(\"name\", \"//div/text()\")\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n+        l.replace_xpath(\"name\", \"//div/text()\", re=\"ma\")\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Ma\"])\n \n     def test_add_css_re(self):\n         l = TestItemLoader(response=self.response)\n-        l.add_css('name', 'div::text', re='ma')\n-        self.assertEqual(l.get_output_value('name'), ['Ma'])\n+        l.add_css(\"name\", \"div::text\", re=\"ma\")\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Ma\"])\n \n-        l.add_css('url', 'a::attr(href)', re='http://(.+)')\n-        self.assertEqual(l.get_output_value('url'), ['www.scrapy.org'])\n+        l.add_css(\"url\", \"a::attr(href)\", re=\"http://(.+)\")\n+        self.assertEqual(l.get_output_value(\"url\"), [\"www.scrapy.org\"])\n \n     def test_replace_css(self):\n         l = TestItemLoader(response=self.response)\n         self.assertTrue(l.selector)\n-        l.add_css('name', 'div::text')\n-        self.assertEqual(l.get_output_value('name'), ['Marta'])\n-        l.replace_css('name', 'p::text')\n-        self.assertEqual(l.get_output_value('name'), ['Paragraph'])\n+        l.add_css(\"name\", \"div::text\")\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n+        l.replace_css(\"name\", \"p::text\")\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Paragraph\"])\n \n-        l.replace_css('name', ['p::text', 'div::text'])\n-        self.assertEqual(l.get_output_value('name'), ['Paragraph', 'Marta'])\n+        l.replace_css(\"name\", [\"p::text\", \"div::text\"])\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Paragraph\", \"Marta\"])\n \n-        l.add_css('url', 'a::attr(href)', re='http://(.+)')\n-        self.assertEqual(l.get_output_value('url'), ['www.scrapy.org'])\n-        l.replace_css('url', 'img::attr(src)')\n-        self.assertEqual(l.get_output_value('url'), ['/images/logo.png'])\n+        l.add_css(\"url\", \"a::attr(href)\", re=\"http://(.+)\")\n+        self.assertEqual(l.get_output_value(\"url\"), [\"www.scrapy.org\"])\n+        l.replace_css(\"url\", \"img::attr(src)\")\n+        self.assertEqual(l.get_output_value(\"url\"), [\"/images/logo.png\"])\n \n     def test_get_css(self):\n         l = TestItemLoader(response=self.response)\n-        self.assertEqual(l.get_css('p::text'), ['paragraph'])\n-        self.assertEqual(l.get_css('p::text', TakeFirst()), 'paragraph')\n-        self.assertEqual(l.get_css('p::text', TakeFirst(), re='pa'), 'pa')\n+        self.assertEqual(l.get_css(\"p::text\"), [\"paragraph\"])\n+        self.assertEqual(l.get_css(\"p::text\", TakeFirst()), \"paragraph\")\n+        self.assertEqual(l.get_css(\"p::text\", TakeFirst(), re=\"pa\"), \"pa\")\n \n-        self.assertEqual(l.get_css(['p::text', 'div::text']), ['paragraph', 'marta'])\n-        self.assertEqual(l.get_css(['a::attr(href)', 'img::attr(src)']),\n-                         ['http://www.scrapy.org', '/images/logo.png'])\n+        self.assertEqual(l.get_css([\"p::text\", \"div::text\"]), [\"paragraph\", \"marta\"])\n+        self.assertEqual(\n+            l.get_css([\"a::attr(href)\", \"img::attr(src)\"]),\n+            [\"http://www.scrapy.org\", \"/images/logo.png\"],\n+        )\n \n     def test_replace_css_multi_fields(self):\n         l = TestItemLoader(response=self.response)\n-        l.add_css(None, 'div::text', TakeFirst(), lambda x: {'name': x})\n-        self.assertEqual(l.get_output_value('name'), ['Marta'])\n-        l.replace_css(None, 'p::text', TakeFirst(), lambda x: {'name': x})\n-        self.assertEqual(l.get_output_value('name'), ['Paragraph'])\n+        l.add_css(None, \"div::text\", TakeFirst(), lambda x: {\"name\": x})\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n+        l.replace_css(None, \"p::text\", TakeFirst(), lambda x: {\"name\": x})\n+        self.assertEqual(l.get_output_value(\"name\"), [\"Paragraph\"])\n \n-        l.add_css(None, 'a::attr(href)', TakeFirst(), lambda x: {'url': x})\n-        self.assertEqual(l.get_output_value('url'), ['http://www.scrapy.org'])\n-        l.replace_css(None, 'img::attr(src)', TakeFirst(), lambda x: {'url': x})\n-        self.assertEqual(l.get_output_value('url'), ['/images/logo.png'])\n+        l.add_css(None, \"a::attr(href)\", TakeFirst(), lambda x: {\"url\": x})\n+        self.assertEqual(l.get_output_value(\"url\"), [\"http://www.scrapy.org\"])\n+        l.replace_css(None, \"img::attr(src)\", TakeFirst(), lambda x: {\"url\": x})\n+        self.assertEqual(l.get_output_value(\"url\"), [\"/images/logo.png\"])\n \n     def test_replace_css_re(self):\n         l = TestItemLoader(response=self.response)\n         self.assertTrue(l.selector)\n-        l.add_css('url', 'a::attr(href)')\n-        self.assertEqual(l.get_output_value('url'), ['http://www.scrapy.org'])\n-        l.replace_css('url', 'a::attr(href)', re=r'http://www\\.(.+)')\n-        self.assertEqual(l.get_output_value('url'), ['scrapy.org'])\n+        l.add_css(\"url\", \"a::attr(href)\")\n+        self.assertEqual(l.get_output_value(\"url\"), [\"http://www.scrapy.org\"])\n+        l.replace_css(\"url\", \"a::attr(href)\", re=r\"http://www\\.(.+)\")\n+        self.assertEqual(l.get_output_value(\"url\"), [\"scrapy.org\"])\n \n \n class SubselectorLoaderTest(unittest.TestCase):\n-    response = HtmlResponse(url=\"\", encoding='utf-8', body=b\"\"\"\n+    response = HtmlResponse(\n+        url=\"\",\n+        encoding=\"utf-8\",\n+        body=b\"\"\"\n     <html>\n     <body>\n     <header>\n@@ -434,76 +465,88 @@ class SubselectorLoaderTest(unittest.TestCase):\n     </footer>\n     </body>\n     </html>\n-    \"\"\")\n+    \"\"\",\n+    )\n \n     def test_nested_xpath(self):\n         l = NestedItemLoader(response=self.response)\n \n         nl = l.nested_xpath(\"//header\")\n-        nl.add_xpath('name', 'div/text()')\n-        nl.add_css('name_div', '#id')\n-        nl.add_value('name_value', nl.selector.xpath('div[@id = \"id\"]/text()').getall())\n+        nl.add_xpath(\"name\", \"div/text()\")\n+        nl.add_css(\"name_div\", \"#id\")\n+        nl.add_value(\"name_value\", nl.selector.xpath('div[@id = \"id\"]/text()').getall())\n \n-        self.assertEqual(l.get_output_value('name'), ['marta'])\n-        self.assertEqual(l.get_output_value('name_div'), ['<div id=\"id\">marta</div>'])\n-        self.assertEqual(l.get_output_value('name_value'), ['marta'])\n+        self.assertEqual(l.get_output_value(\"name\"), [\"marta\"])\n+        self.assertEqual(l.get_output_value(\"name_div\"), ['<div id=\"id\">marta</div>'])\n+        self.assertEqual(l.get_output_value(\"name_value\"), [\"marta\"])\n \n-        self.assertEqual(l.get_output_value('name'), nl.get_output_value('name'))\n-        self.assertEqual(l.get_output_value('name_div'), nl.get_output_value('name_div'))\n-        self.assertEqual(l.get_output_value('name_value'), nl.get_output_value('name_value'))\n+        self.assertEqual(l.get_output_value(\"name\"), nl.get_output_value(\"name\"))\n+        self.assertEqual(\n+            l.get_output_value(\"name_div\"), nl.get_output_value(\"name_div\")\n+        )\n+        self.assertEqual(\n+            l.get_output_value(\"name_value\"), nl.get_output_value(\"name_value\")\n+        )\n \n     def test_nested_css(self):\n         l = NestedItemLoader(response=self.response)\n         nl = l.nested_css(\"header\")\n-        nl.add_xpath('name', 'div/text()')\n-        nl.add_css('name_div', '#id')\n-        nl.add_value('name_value', nl.selector.xpath('div[@id = \"id\"]/text()').getall())\n+        nl.add_xpath(\"name\", \"div/text()\")\n+        nl.add_css(\"name_div\", \"#id\")\n+        nl.add_value(\"name_value\", nl.selector.xpath('div[@id = \"id\"]/text()').getall())\n \n-        self.assertEqual(l.get_output_value('name'), ['marta'])\n-        self.assertEqual(l.get_output_value('name_div'), ['<div id=\"id\">marta</div>'])\n-        self.assertEqual(l.get_output_value('name_value'), ['marta'])\n+        self.assertEqual(l.get_output_value(\"name\"), [\"marta\"])\n+        self.assertEqual(l.get_output_value(\"name_div\"), ['<div id=\"id\">marta</div>'])\n+        self.assertEqual(l.get_output_value(\"name_value\"), [\"marta\"])\n \n-        self.assertEqual(l.get_output_value('name'), nl.get_output_value('name'))\n-        self.assertEqual(l.get_output_value('name_div'), nl.get_output_value('name_div'))\n-        self.assertEqual(l.get_output_value('name_value'), nl.get_output_value('name_value'))\n+        self.assertEqual(l.get_output_value(\"name\"), nl.get_output_value(\"name\"))\n+        self.assertEqual(\n+            l.get_output_value(\"name_div\"), nl.get_output_value(\"name_div\")\n+        )\n+        self.assertEqual(\n+            l.get_output_value(\"name_value\"), nl.get_output_value(\"name_value\")\n+        )\n \n     def test_nested_replace(self):\n         l = NestedItemLoader(response=self.response)\n-        nl1 = l.nested_xpath('//footer')\n-        nl2 = nl1.nested_xpath('a')\n+        nl1 = l.nested_xpath(\"//footer\")\n+        nl2 = nl1.nested_xpath(\"a\")\n \n-        l.add_xpath('url', '//footer/a/@href')\n-        self.assertEqual(l.get_output_value('url'), ['http://www.scrapy.org'])\n-        nl1.replace_xpath('url', 'img/@src')\n-        self.assertEqual(l.get_output_value('url'), ['/images/logo.png'])\n-        nl2.replace_xpath('url', '@href')\n-        self.assertEqual(l.get_output_value('url'), ['http://www.scrapy.org'])\n+        l.add_xpath(\"url\", \"//footer/a/@href\")\n+        self.assertEqual(l.get_output_value(\"url\"), [\"http://www.scrapy.org\"])\n+        nl1.replace_xpath(\"url\", \"img/@src\")\n+        self.assertEqual(l.get_output_value(\"url\"), [\"/images/logo.png\"])\n+        nl2.replace_xpath(\"url\", \"@href\")\n+        self.assertEqual(l.get_output_value(\"url\"), [\"http://www.scrapy.org\"])\n \n     def test_nested_ordering(self):\n         l = NestedItemLoader(response=self.response)\n-        nl1 = l.nested_xpath('//footer')\n-        nl2 = nl1.nested_xpath('a')\n+        nl1 = l.nested_xpath(\"//footer\")\n+        nl2 = nl1.nested_xpath(\"a\")\n \n-        nl1.add_xpath('url', 'img/@src')\n-        l.add_xpath('url', '//footer/a/@href')\n-        nl2.add_xpath('url', 'text()')\n-        l.add_xpath('url', '//footer/a/@href')\n+        nl1.add_xpath(\"url\", \"img/@src\")\n+        l.add_xpath(\"url\", \"//footer/a/@href\")\n+        nl2.add_xpath(\"url\", \"text()\")\n+        l.add_xpath(\"url\", \"//footer/a/@href\")\n \n-        self.assertEqual(l.get_output_value('url'), [\n-            '/images/logo.png',\n-            'http://www.scrapy.org',\n-            'homepage',\n-            'http://www.scrapy.org',\n-        ])\n+        self.assertEqual(\n+            l.get_output_value(\"url\"),\n+            [\n+                \"/images/logo.png\",\n+                \"http://www.scrapy.org\",\n+                \"homepage\",\n+                \"http://www.scrapy.org\",\n+            ],\n+        )\n \n     def test_nested_load_item(self):\n         l = NestedItemLoader(response=self.response)\n-        nl1 = l.nested_xpath('//footer')\n-        nl2 = nl1.nested_xpath('img')\n+        nl1 = l.nested_xpath(\"//footer\")\n+        nl2 = nl1.nested_xpath(\"img\")\n \n-        l.add_xpath('name', '//header/div/text()')\n-        nl1.add_xpath('url', 'a/@href')\n-        nl2.add_xpath('image', '@src')\n+        l.add_xpath(\"name\", \"//header/div/text()\")\n+        nl1.add_xpath(\"url\", \"a/@href\")\n+        nl2.add_xpath(\"image\", \"@src\")\n \n         item = l.load_item()\n \n@@ -511,13 +554,14 @@ class SubselectorLoaderTest(unittest.TestCase):\n         assert item is nl1.item\n         assert item is nl2.item\n \n-        self.assertEqual(item['name'], ['marta'])\n-        self.assertEqual(item['url'], ['http://www.scrapy.org'])\n-        self.assertEqual(item['image'], ['/images/logo.png'])\n+        self.assertEqual(item[\"name\"], [\"marta\"])\n+        self.assertEqual(item[\"url\"], [\"http://www.scrapy.org\"])\n+        self.assertEqual(item[\"image\"], [\"/images/logo.png\"])\n \n \n # Functions as processors\n \n+\n def function_processor_strip(iterable):\n     return [x.strip() for x in iterable]\n \n@@ -538,15 +582,11 @@ class FunctionProcessorItemLoader(ItemLoader):\n \n \n class FunctionProcessorTestCase(unittest.TestCase):\n-\n     def test_processor_defined_in_item(self):\n         lo = FunctionProcessorItemLoader()\n-        lo.add_value('foo', '  bar  ')\n-        lo.add_value('foo', ['  asdf  ', '  qwerty  '])\n-        self.assertEqual(\n-            dict(lo.load_item()),\n-            {'foo': ['BAR', 'ASDF', 'QWERTY']}\n-        )\n+        lo.add_value(\"foo\", \"  bar  \")\n+        lo.add_value(\"foo\", [\"  asdf  \", \"  qwerty  \"])\n+        self.assertEqual(dict(lo.load_item()), {\"foo\": [\"BAR\", \"ASDF\", \"QWERTY\"]})\n \n \n if __name__ == \"__main__\":\n\n@@ -7,8 +7,14 @@ import unittest\n import warnings\n from functools import partial\n \n-from itemloaders.processors import (Compose, Identity, Join,\n-                                    MapCompose, SelectJmes, TakeFirst)\n+from itemloaders.processors import (\n+    Compose,\n+    Identity,\n+    Join,\n+    MapCompose,\n+    SelectJmes,\n+    TakeFirst,\n+)\n \n from scrapy.item import Item, Field\n from scrapy.loader import ItemLoader\n@@ -42,28 +48,27 @@ class DefaultedItemLoader(NameItemLoader):\n \n # test processors\n def processor_with_args(value, other=None, loader_context=None):\n-    if 'key' in loader_context:\n-        return loader_context['key']\n+    if \"key\" in loader_context:\n+        return loader_context[\"key\"]\n     return value\n \n \n class BasicItemLoaderTest(unittest.TestCase):\n-\n     def test_load_item_using_default_loader(self):\n         i = TestItem()\n-        i['summary'] = 'lala'\n+        i[\"summary\"] = \"lala\"\n         il = ItemLoader(item=i)\n-        il.add_value('name', 'marta')\n+        il.add_value(\"name\", \"marta\")\n         item = il.load_item()\n         assert item is i\n-        self.assertEqual(item['summary'], ['lala'])\n-        self.assertEqual(item['name'], ['marta'])\n+        self.assertEqual(item[\"summary\"], [\"lala\"])\n+        self.assertEqual(item[\"name\"], [\"marta\"])\n \n     def test_load_item_using_custom_loader(self):\n         il = TestItemLoader()\n-        il.add_value('name', 'marta')\n+        il.add_value(\"name\", \"marta\")\n         item = il.load_item()\n-        self.assertEqual(item['name'], ['Marta'])\n+        self.assertEqual(item[\"name\"], [\"Marta\"])\n \n     def test_load_item_ignore_none_field_values(self):\n         def validate_sku(value):\n@@ -76,206 +81,223 @@ class BasicItemLoaderTest(unittest.TestCase):\n             price_out = Compose(TakeFirst(), float)\n             sku_out = Compose(TakeFirst(), validate_sku)\n \n-        valid_fragment = 'SKU: 1234'\n-        invalid_fragment = 'SKU: not available'\n-        sku_re = 'SKU: (.+)'\n+        valid_fragment = \"SKU: 1234\"\n+        invalid_fragment = \"SKU: not available\"\n+        sku_re = \"SKU: (.+)\"\n \n         il = MyLoader(item={})\n         # Should not return \"sku: None\".\n-        il.add_value('sku', [invalid_fragment], re=sku_re)\n+        il.add_value(\"sku\", [invalid_fragment], re=sku_re)\n         # Should not ignore empty values.\n-        il.add_value('name', '')\n-        il.add_value('price', ['0'])\n-        self.assertEqual(il.load_item(), {\n-            'name': '',\n-            'price': 0.0,\n-        })\n+        il.add_value(\"name\", \"\")\n+        il.add_value(\"price\", [\"0\"])\n+        self.assertEqual(\n+            il.load_item(),\n+            {\n+                \"name\": \"\",\n+                \"price\": 0.0,\n+            },\n+        )\n \n-        il.replace_value('sku', [valid_fragment], re=sku_re)\n-        self.assertEqual(il.load_item()['sku'], '1234')\n+        il.replace_value(\"sku\", [valid_fragment], re=sku_re)\n+        self.assertEqual(il.load_item()[\"sku\"], \"1234\")\n \n     def test_self_referencing_loader(self):\n         class MyLoader(ItemLoader):\n             url_out = TakeFirst()\n \n             def img_url_out(self, values):\n-                return (self.get_output_value('url') or '') + values[0]\n+                return (self.get_output_value(\"url\") or \"\") + values[0]\n \n         il = MyLoader(item={})\n-        il.add_value('url', 'http://example.com/')\n-        il.add_value('img_url', '1234.png')\n-        self.assertEqual(il.load_item(), {\n-            'url': 'http://example.com/',\n-            'img_url': 'http://example.com/1234.png',\n-        })\n+        il.add_value(\"url\", \"http://example.com/\")\n+        il.add_value(\"img_url\", \"1234.png\")\n+        self.assertEqual(\n+            il.load_item(),\n+            {\n+                \"url\": \"http://example.com/\",\n+                \"img_url\": \"http://example.com/1234.png\",\n+            },\n+        )\n \n         il = MyLoader(item={})\n-        il.add_value('img_url', '1234.png')\n-        self.assertEqual(il.load_item(), {\n-            'img_url': '1234.png',\n-        })\n+        il.add_value(\"img_url\", \"1234.png\")\n+        self.assertEqual(\n+            il.load_item(),\n+            {\n+                \"img_url\": \"1234.png\",\n+            },\n+        )\n \n     def test_add_value(self):\n         il = TestItemLoader()\n-        il.add_value('name', 'marta')\n-        self.assertEqual(il.get_collected_values('name'), ['Marta'])\n-        self.assertEqual(il.get_output_value('name'), ['Marta'])\n-        il.add_value('name', 'pepe')\n-        self.assertEqual(il.get_collected_values('name'), ['Marta', 'Pepe'])\n-        self.assertEqual(il.get_output_value('name'), ['Marta', 'Pepe'])\n+        il.add_value(\"name\", \"marta\")\n+        self.assertEqual(il.get_collected_values(\"name\"), [\"Marta\"])\n+        self.assertEqual(il.get_output_value(\"name\"), [\"Marta\"])\n+        il.add_value(\"name\", \"pepe\")\n+        self.assertEqual(il.get_collected_values(\"name\"), [\"Marta\", \"Pepe\"])\n+        self.assertEqual(il.get_output_value(\"name\"), [\"Marta\", \"Pepe\"])\n \n         # test add object value\n-        il.add_value('summary', {'key': 1})\n-        self.assertEqual(il.get_collected_values('summary'), [{'key': 1}])\n+        il.add_value(\"summary\", {\"key\": 1})\n+        self.assertEqual(il.get_collected_values(\"summary\"), [{\"key\": 1}])\n \n-        il.add_value(None, 'Jim', lambda x: {'name': x})\n-        self.assertEqual(il.get_collected_values('name'), ['Marta', 'Pepe', 'Jim'])\n+        il.add_value(None, \"Jim\", lambda x: {\"name\": x})\n+        self.assertEqual(il.get_collected_values(\"name\"), [\"Marta\", \"Pepe\", \"Jim\"])\n \n     def test_add_zero(self):\n         il = NameItemLoader()\n-        il.add_value('name', 0)\n-        self.assertEqual(il.get_collected_values('name'), [0])\n+        il.add_value(\"name\", 0)\n+        self.assertEqual(il.get_collected_values(\"name\"), [0])\n \n     def test_replace_value(self):\n         il = TestItemLoader()\n-        il.replace_value('name', 'marta')\n-        self.assertEqual(il.get_collected_values('name'), ['Marta'])\n-        self.assertEqual(il.get_output_value('name'), ['Marta'])\n-        il.replace_value('name', 'pepe')\n-        self.assertEqual(il.get_collected_values('name'), ['Pepe'])\n-        self.assertEqual(il.get_output_value('name'), ['Pepe'])\n+        il.replace_value(\"name\", \"marta\")\n+        self.assertEqual(il.get_collected_values(\"name\"), [\"Marta\"])\n+        self.assertEqual(il.get_output_value(\"name\"), [\"Marta\"])\n+        il.replace_value(\"name\", \"pepe\")\n+        self.assertEqual(il.get_collected_values(\"name\"), [\"Pepe\"])\n+        self.assertEqual(il.get_output_value(\"name\"), [\"Pepe\"])\n \n-        il.replace_value(None, 'Jim', lambda x: {'name': x})\n-        self.assertEqual(il.get_collected_values('name'), ['Jim'])\n+        il.replace_value(None, \"Jim\", lambda x: {\"name\": x})\n+        self.assertEqual(il.get_collected_values(\"name\"), [\"Jim\"])\n \n     def test_get_value(self):\n         il = NameItemLoader()\n-        self.assertEqual('FOO', il.get_value(['foo', 'bar'], TakeFirst(), str.upper))\n-        self.assertEqual(['foo', 'bar'], il.get_value(['name:foo', 'name:bar'], re='name:(.*)$'))\n-        self.assertEqual('foo', il.get_value(['name:foo', 'name:bar'], TakeFirst(), re='name:(.*)$'))\n+        self.assertEqual(\"FOO\", il.get_value([\"foo\", \"bar\"], TakeFirst(), str.upper))\n+        self.assertEqual(\n+            [\"foo\", \"bar\"], il.get_value([\"name:foo\", \"name:bar\"], re=\"name:(.*)$\")\n+        )\n+        self.assertEqual(\n+            \"foo\", il.get_value([\"name:foo\", \"name:bar\"], TakeFirst(), re=\"name:(.*)$\")\n+        )\n \n-        il.add_value('name', ['name:foo', 'name:bar'], TakeFirst(), re='name:(.*)$')\n-        self.assertEqual(['foo'], il.get_collected_values('name'))\n-        il.replace_value('name', 'name:bar', re='name:(.*)$')\n-        self.assertEqual(['bar'], il.get_collected_values('name'))\n+        il.add_value(\"name\", [\"name:foo\", \"name:bar\"], TakeFirst(), re=\"name:(.*)$\")\n+        self.assertEqual([\"foo\"], il.get_collected_values(\"name\"))\n+        il.replace_value(\"name\", \"name:bar\", re=\"name:(.*)$\")\n+        self.assertEqual([\"bar\"], il.get_collected_values(\"name\"))\n \n     def test_iter_on_input_processor_input(self):\n         class NameFirstItemLoader(NameItemLoader):\n             name_in = TakeFirst()\n \n         il = NameFirstItemLoader()\n-        il.add_value('name', 'marta')\n-        self.assertEqual(il.get_collected_values('name'), ['marta'])\n+        il.add_value(\"name\", \"marta\")\n+        self.assertEqual(il.get_collected_values(\"name\"), [\"marta\"])\n         il = NameFirstItemLoader()\n-        il.add_value('name', ['marta', 'jose'])\n-        self.assertEqual(il.get_collected_values('name'), ['marta'])\n+        il.add_value(\"name\", [\"marta\", \"jose\"])\n+        self.assertEqual(il.get_collected_values(\"name\"), [\"marta\"])\n \n         il = NameFirstItemLoader()\n-        il.replace_value('name', 'marta')\n-        self.assertEqual(il.get_collected_values('name'), ['marta'])\n+        il.replace_value(\"name\", \"marta\")\n+        self.assertEqual(il.get_collected_values(\"name\"), [\"marta\"])\n         il = NameFirstItemLoader()\n-        il.replace_value('name', ['marta', 'jose'])\n-        self.assertEqual(il.get_collected_values('name'), ['marta'])\n+        il.replace_value(\"name\", [\"marta\", \"jose\"])\n+        self.assertEqual(il.get_collected_values(\"name\"), [\"marta\"])\n \n         il = NameFirstItemLoader()\n-        il.add_value('name', 'marta')\n-        il.add_value('name', ['jose', 'pedro'])\n-        self.assertEqual(il.get_collected_values('name'), ['marta', 'jose'])\n+        il.add_value(\"name\", \"marta\")\n+        il.add_value(\"name\", [\"jose\", \"pedro\"])\n+        self.assertEqual(il.get_collected_values(\"name\"), [\"marta\", \"jose\"])\n \n     def test_map_compose_filter(self):\n         def filter_world(x):\n-            return None if x == 'world' else x\n+            return None if x == \"world\" else x\n \n         proc = MapCompose(filter_world, str.upper)\n-        self.assertEqual(proc(['hello', 'world', 'this', 'is', 'scrapy']),\n-                         ['HELLO', 'THIS', 'IS', 'SCRAPY'])\n+        self.assertEqual(\n+            proc([\"hello\", \"world\", \"this\", \"is\", \"scrapy\"]),\n+            [\"HELLO\", \"THIS\", \"IS\", \"SCRAPY\"],\n+        )\n \n     def test_map_compose_filter_multil(self):\n         class TestItemLoader(NameItemLoader):\n             name_in = MapCompose(lambda v: v.title(), lambda v: v[:-1])\n \n         il = TestItemLoader()\n-        il.add_value('name', 'marta')\n-        self.assertEqual(il.get_output_value('name'), ['Mart'])\n+        il.add_value(\"name\", \"marta\")\n+        self.assertEqual(il.get_output_value(\"name\"), [\"Mart\"])\n         item = il.load_item()\n-        self.assertEqual(item['name'], ['Mart'])\n+        self.assertEqual(item[\"name\"], [\"Mart\"])\n \n     def test_default_input_processor(self):\n         il = DefaultedItemLoader()\n-        il.add_value('name', 'marta')\n-        self.assertEqual(il.get_output_value('name'), ['mart'])\n+        il.add_value(\"name\", \"marta\")\n+        self.assertEqual(il.get_output_value(\"name\"), [\"mart\"])\n \n     def test_inherited_default_input_processor(self):\n         class InheritDefaultedItemLoader(DefaultedItemLoader):\n             pass\n \n         il = InheritDefaultedItemLoader()\n-        il.add_value('name', 'marta')\n-        self.assertEqual(il.get_output_value('name'), ['mart'])\n+        il.add_value(\"name\", \"marta\")\n+        self.assertEqual(il.get_output_value(\"name\"), [\"mart\"])\n \n     def test_input_processor_inheritance(self):\n         class ChildItemLoader(TestItemLoader):\n             url_in = MapCompose(lambda v: v.lower())\n \n         il = ChildItemLoader()\n-        il.add_value('url', 'HTTP://scrapy.ORG')\n-        self.assertEqual(il.get_output_value('url'), ['http://scrapy.org'])\n-        il.add_value('name', 'marta')\n-        self.assertEqual(il.get_output_value('name'), ['Marta'])\n+        il.add_value(\"url\", \"HTTP://scrapy.ORG\")\n+        self.assertEqual(il.get_output_value(\"url\"), [\"http://scrapy.org\"])\n+        il.add_value(\"name\", \"marta\")\n+        self.assertEqual(il.get_output_value(\"name\"), [\"Marta\"])\n \n         class ChildChildItemLoader(ChildItemLoader):\n             url_in = MapCompose(lambda v: v.upper())\n             summary_in = MapCompose(lambda v: v)\n \n         il = ChildChildItemLoader()\n-        il.add_value('url', 'http://scrapy.org')\n-        self.assertEqual(il.get_output_value('url'), ['HTTP://SCRAPY.ORG'])\n-        il.add_value('name', 'marta')\n-        self.assertEqual(il.get_output_value('name'), ['Marta'])\n+        il.add_value(\"url\", \"http://scrapy.org\")\n+        self.assertEqual(il.get_output_value(\"url\"), [\"HTTP://SCRAPY.ORG\"])\n+        il.add_value(\"name\", \"marta\")\n+        self.assertEqual(il.get_output_value(\"name\"), [\"Marta\"])\n \n     def test_empty_map_compose(self):\n         class IdentityDefaultedItemLoader(DefaultedItemLoader):\n             name_in = MapCompose()\n \n         il = IdentityDefaultedItemLoader()\n-        il.add_value('name', 'marta')\n-        self.assertEqual(il.get_output_value('name'), ['marta'])\n+        il.add_value(\"name\", \"marta\")\n+        self.assertEqual(il.get_output_value(\"name\"), [\"marta\"])\n \n     def test_identity_input_processor(self):\n         class IdentityDefaultedItemLoader(DefaultedItemLoader):\n             name_in = Identity()\n \n         il = IdentityDefaultedItemLoader()\n-        il.add_value('name', 'marta')\n-        self.assertEqual(il.get_output_value('name'), ['marta'])\n+        il.add_value(\"name\", \"marta\")\n+        self.assertEqual(il.get_output_value(\"name\"), [\"marta\"])\n \n     def test_extend_custom_input_processors(self):\n         class ChildItemLoader(TestItemLoader):\n             name_in = MapCompose(TestItemLoader.name_in, str.swapcase)\n \n         il = ChildItemLoader()\n-        il.add_value('name', 'marta')\n-        self.assertEqual(il.get_output_value('name'), ['mARTA'])\n+        il.add_value(\"name\", \"marta\")\n+        self.assertEqual(il.get_output_value(\"name\"), [\"mARTA\"])\n \n     def test_extend_default_input_processors(self):\n         class ChildDefaultedItemLoader(DefaultedItemLoader):\n-            name_in = MapCompose(DefaultedItemLoader.default_input_processor, str.swapcase)\n+            name_in = MapCompose(\n+                DefaultedItemLoader.default_input_processor, str.swapcase\n+            )\n \n         il = ChildDefaultedItemLoader()\n-        il.add_value('name', 'marta')\n-        self.assertEqual(il.get_output_value('name'), ['MART'])\n+        il.add_value(\"name\", \"marta\")\n+        self.assertEqual(il.get_output_value(\"name\"), [\"MART\"])\n \n     def test_output_processor_using_function(self):\n         il = TestItemLoader()\n-        il.add_value('name', ['mar', 'ta'])\n-        self.assertEqual(il.get_output_value('name'), ['Mar', 'Ta'])\n+        il.add_value(\"name\", [\"mar\", \"ta\"])\n+        self.assertEqual(il.get_output_value(\"name\"), [\"Mar\", \"Ta\"])\n \n         class TakeFirstItemLoader(TestItemLoader):\n             name_out = \" \".join\n \n         il = TakeFirstItemLoader()\n-        il.add_value('name', ['mar', 'ta'])\n-        self.assertEqual(il.get_output_value('name'), 'Mar Ta')\n+        il.add_value(\"name\", [\"mar\", \"ta\"])\n+        self.assertEqual(il.get_output_value(\"name\"), \"Mar Ta\")\n \n     def test_output_processor_error(self):\n         class TestItemLoader(ItemLoader):\n@@ -283,9 +305,9 @@ class BasicItemLoaderTest(unittest.TestCase):\n             name_out = MapCompose(float)\n \n         il = TestItemLoader()\n-        il.add_value('name', ['$10'])\n+        il.add_value(\"name\", [\"$10\"])\n         try:\n-            float('$10')\n+            float(\"$10\")\n         except Exception as e:\n             expected_exc_str = str(e)\n \n@@ -296,118 +318,118 @@ class BasicItemLoaderTest(unittest.TestCase):\n             exc = e\n         assert isinstance(exc, ValueError)\n         s = str(exc)\n-        assert 'name' in s, s\n-        assert '$10' in s, s\n-        assert 'ValueError' in s, s\n+        assert \"name\" in s, s\n+        assert \"$10\" in s, s\n+        assert \"ValueError\" in s, s\n         assert expected_exc_str in s, s\n \n     def test_output_processor_using_classes(self):\n         il = TestItemLoader()\n-        il.add_value('name', ['mar', 'ta'])\n-        self.assertEqual(il.get_output_value('name'), ['Mar', 'Ta'])\n+        il.add_value(\"name\", [\"mar\", \"ta\"])\n+        self.assertEqual(il.get_output_value(\"name\"), [\"Mar\", \"Ta\"])\n \n         class TakeFirstItemLoader(TestItemLoader):\n             name_out = Join()\n \n         il = TakeFirstItemLoader()\n-        il.add_value('name', ['mar', 'ta'])\n-        self.assertEqual(il.get_output_value('name'), 'Mar Ta')\n+        il.add_value(\"name\", [\"mar\", \"ta\"])\n+        self.assertEqual(il.get_output_value(\"name\"), \"Mar Ta\")\n \n         class TakeFirstItemLoader(TestItemLoader):\n             name_out = Join(\"<br>\")\n \n         il = TakeFirstItemLoader()\n-        il.add_value('name', ['mar', 'ta'])\n-        self.assertEqual(il.get_output_value('name'), 'Mar<br>Ta')\n+        il.add_value(\"name\", [\"mar\", \"ta\"])\n+        self.assertEqual(il.get_output_value(\"name\"), \"Mar<br>Ta\")\n \n     def test_default_output_processor(self):\n         il = TestItemLoader()\n-        il.add_value('name', ['mar', 'ta'])\n-        self.assertEqual(il.get_output_value('name'), ['Mar', 'Ta'])\n+        il.add_value(\"name\", [\"mar\", \"ta\"])\n+        self.assertEqual(il.get_output_value(\"name\"), [\"Mar\", \"Ta\"])\n \n         class LalaItemLoader(TestItemLoader):\n             default_output_processor = Identity()\n \n         il = LalaItemLoader()\n-        il.add_value('name', ['mar', 'ta'])\n-        self.assertEqual(il.get_output_value('name'), ['Mar', 'Ta'])\n+        il.add_value(\"name\", [\"mar\", \"ta\"])\n+        self.assertEqual(il.get_output_value(\"name\"), [\"Mar\", \"Ta\"])\n \n     def test_loader_context_on_declaration(self):\n         class ChildItemLoader(TestItemLoader):\n-            url_in = MapCompose(processor_with_args, key='val')\n+            url_in = MapCompose(processor_with_args, key=\"val\")\n \n         il = ChildItemLoader()\n-        il.add_value('url', 'text')\n-        self.assertEqual(il.get_output_value('url'), ['val'])\n-        il.replace_value('url', 'text2')\n-        self.assertEqual(il.get_output_value('url'), ['val'])\n+        il.add_value(\"url\", \"text\")\n+        self.assertEqual(il.get_output_value(\"url\"), [\"val\"])\n+        il.replace_value(\"url\", \"text2\")\n+        self.assertEqual(il.get_output_value(\"url\"), [\"val\"])\n \n     def test_loader_context_on_instantiation(self):\n         class ChildItemLoader(TestItemLoader):\n             url_in = MapCompose(processor_with_args)\n \n-        il = ChildItemLoader(key='val')\n-        il.add_value('url', 'text')\n-        self.assertEqual(il.get_output_value('url'), ['val'])\n-        il.replace_value('url', 'text2')\n-        self.assertEqual(il.get_output_value('url'), ['val'])\n+        il = ChildItemLoader(key=\"val\")\n+        il.add_value(\"url\", \"text\")\n+        self.assertEqual(il.get_output_value(\"url\"), [\"val\"])\n+        il.replace_value(\"url\", \"text2\")\n+        self.assertEqual(il.get_output_value(\"url\"), [\"val\"])\n \n     def test_loader_context_on_assign(self):\n         class ChildItemLoader(TestItemLoader):\n             url_in = MapCompose(processor_with_args)\n \n         il = ChildItemLoader()\n-        il.context['key'] = 'val'\n-        il.add_value('url', 'text')\n-        self.assertEqual(il.get_output_value('url'), ['val'])\n-        il.replace_value('url', 'text2')\n-        self.assertEqual(il.get_output_value('url'), ['val'])\n+        il.context[\"key\"] = \"val\"\n+        il.add_value(\"url\", \"text\")\n+        self.assertEqual(il.get_output_value(\"url\"), [\"val\"])\n+        il.replace_value(\"url\", \"text2\")\n+        self.assertEqual(il.get_output_value(\"url\"), [\"val\"])\n \n     def test_item_passed_to_input_processor_functions(self):\n         def processor(value, loader_context):\n-            return loader_context['item']['name']\n+            return loader_context[\"item\"][\"name\"]\n \n         class ChildItemLoader(TestItemLoader):\n             url_in = MapCompose(processor)\n \n-        it = TestItem(name='marta')\n+        it = TestItem(name=\"marta\")\n         il = ChildItemLoader(item=it)\n-        il.add_value('url', 'text')\n-        self.assertEqual(il.get_output_value('url'), ['marta'])\n-        il.replace_value('url', 'text2')\n-        self.assertEqual(il.get_output_value('url'), ['marta'])\n+        il.add_value(\"url\", \"text\")\n+        self.assertEqual(il.get_output_value(\"url\"), [\"marta\"])\n+        il.replace_value(\"url\", \"text2\")\n+        self.assertEqual(il.get_output_value(\"url\"), [\"marta\"])\n \n     def test_compose_processor(self):\n         class TestItemLoader(NameItemLoader):\n             name_out = Compose(lambda v: v[0], lambda v: v.title(), lambda v: v[:-1])\n \n         il = TestItemLoader()\n-        il.add_value('name', ['marta', 'other'])\n-        self.assertEqual(il.get_output_value('name'), 'Mart')\n+        il.add_value(\"name\", [\"marta\", \"other\"])\n+        self.assertEqual(il.get_output_value(\"name\"), \"Mart\")\n         item = il.load_item()\n-        self.assertEqual(item['name'], 'Mart')\n+        self.assertEqual(item[\"name\"], \"Mart\")\n \n     def test_partial_processor(self):\n         def join(values, sep=None, loader_context=None, ignored=None):\n             if sep is not None:\n                 return sep.join(values)\n-            if loader_context and 'sep' in loader_context:\n-                return loader_context['sep'].join(values)\n-            return ''.join(values)\n+            if loader_context and \"sep\" in loader_context:\n+                return loader_context[\"sep\"].join(values)\n+            return \"\".join(values)\n \n         class TestItemLoader(NameItemLoader):\n-            name_out = Compose(partial(join, sep='+'))\n-            url_out = Compose(partial(join, loader_context={'sep': '.'}))\n-            summary_out = Compose(partial(join, ignored='foo'))\n+            name_out = Compose(partial(join, sep=\"+\"))\n+            url_out = Compose(partial(join, loader_context={\"sep\": \".\"}))\n+            summary_out = Compose(partial(join, ignored=\"foo\"))\n \n         il = TestItemLoader()\n-        il.add_value('name', ['rabbit', 'hole'])\n-        il.add_value('url', ['rabbit', 'hole'])\n-        il.add_value('summary', ['rabbit', 'hole'])\n+        il.add_value(\"name\", [\"rabbit\", \"hole\"])\n+        il.add_value(\"url\", [\"rabbit\", \"hole\"])\n+        il.add_value(\"summary\", [\"rabbit\", \"hole\"])\n         item = il.load_item()\n-        self.assertEqual(item['name'], 'rabbit+hole')\n-        self.assertEqual(item['url'], 'rabbit.hole')\n-        self.assertEqual(item['summary'], 'rabbithole')\n+        self.assertEqual(item[\"name\"], \"rabbit+hole\")\n+        self.assertEqual(item[\"url\"], \"rabbit.hole\")\n+        self.assertEqual(item[\"summary\"], \"rabbithole\")\n \n     def test_error_input_processor(self):\n         class TestItem(Item):\n@@ -418,8 +440,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n             name_in = MapCompose(float)\n \n         il = TestItemLoader()\n-        self.assertRaises(ValueError, il.add_value, 'name',\n-                          ['marta', 'other'])\n+        self.assertRaises(ValueError, il.add_value, \"name\", [\"marta\", \"other\"])\n \n     def test_error_output_processor(self):\n         class TestItem(Item):\n@@ -430,7 +451,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n             name_out = Compose(Join(), float)\n \n         il = TestItemLoader()\n-        il.add_value('name', 'marta')\n+        il.add_value(\"name\", \"marta\")\n         with self.assertRaises(ValueError):\n             il.load_item()\n \n@@ -442,8 +463,9 @@ class BasicItemLoaderTest(unittest.TestCase):\n             default_item_class = TestItem\n \n         il = TestItemLoader()\n-        self.assertRaises(ValueError, il.add_value, 'name',\n-                          ['marta', 'other'], Compose(float))\n+        self.assertRaises(\n+            ValueError, il.add_value, \"name\", [\"marta\", \"other\"], Compose(float)\n+        )\n \n \n class InitializationFromDictTest(unittest.TestCase):\n@@ -452,85 +474,85 @@ class InitializationFromDictTest(unittest.TestCase):\n \n     def test_keep_single_value(self):\n         \"\"\"Loaded item should contain values from the initial item\"\"\"\n-        input_item = self.item_class(name='foo')\n+        input_item = self.item_class(name=\"foo\")\n         il = ItemLoader(item=input_item)\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(dict(loaded_item), {'name': ['foo']})\n+        self.assertEqual(dict(loaded_item), {\"name\": [\"foo\"]})\n \n     def test_keep_list(self):\n         \"\"\"Loaded item should contain values from the initial item\"\"\"\n-        input_item = self.item_class(name=['foo', 'bar'])\n+        input_item = self.item_class(name=[\"foo\", \"bar\"])\n         il = ItemLoader(item=input_item)\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(dict(loaded_item), {'name': ['foo', 'bar']})\n+        self.assertEqual(dict(loaded_item), {\"name\": [\"foo\", \"bar\"]})\n \n     def test_add_value_singlevalue_singlevalue(self):\n         \"\"\"Values added after initialization should be appended\"\"\"\n-        input_item = self.item_class(name='foo')\n+        input_item = self.item_class(name=\"foo\")\n         il = ItemLoader(item=input_item)\n-        il.add_value('name', 'bar')\n+        il.add_value(\"name\", \"bar\")\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(dict(loaded_item), {'name': ['foo', 'bar']})\n+        self.assertEqual(dict(loaded_item), {\"name\": [\"foo\", \"bar\"]})\n \n     def test_add_value_singlevalue_list(self):\n         \"\"\"Values added after initialization should be appended\"\"\"\n-        input_item = self.item_class(name='foo')\n+        input_item = self.item_class(name=\"foo\")\n         il = ItemLoader(item=input_item)\n-        il.add_value('name', ['item', 'loader'])\n+        il.add_value(\"name\", [\"item\", \"loader\"])\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(dict(loaded_item), {'name': ['foo', 'item', 'loader']})\n+        self.assertEqual(dict(loaded_item), {\"name\": [\"foo\", \"item\", \"loader\"]})\n \n     def test_add_value_list_singlevalue(self):\n         \"\"\"Values added after initialization should be appended\"\"\"\n-        input_item = self.item_class(name=['foo', 'bar'])\n+        input_item = self.item_class(name=[\"foo\", \"bar\"])\n         il = ItemLoader(item=input_item)\n-        il.add_value('name', 'qwerty')\n+        il.add_value(\"name\", \"qwerty\")\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(dict(loaded_item), {'name': ['foo', 'bar', 'qwerty']})\n+        self.assertEqual(dict(loaded_item), {\"name\": [\"foo\", \"bar\", \"qwerty\"]})\n \n     def test_add_value_list_list(self):\n         \"\"\"Values added after initialization should be appended\"\"\"\n-        input_item = self.item_class(name=['foo', 'bar'])\n+        input_item = self.item_class(name=[\"foo\", \"bar\"])\n         il = ItemLoader(item=input_item)\n-        il.add_value('name', ['item', 'loader'])\n+        il.add_value(\"name\", [\"item\", \"loader\"])\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(dict(loaded_item), {'name': ['foo', 'bar', 'item', 'loader']})\n+        self.assertEqual(dict(loaded_item), {\"name\": [\"foo\", \"bar\", \"item\", \"loader\"]})\n \n     def test_get_output_value_singlevalue(self):\n         \"\"\"Getting output value must not remove value from item\"\"\"\n-        input_item = self.item_class(name='foo')\n+        input_item = self.item_class(name=\"foo\")\n         il = ItemLoader(item=input_item)\n-        self.assertEqual(il.get_output_value('name'), ['foo'])\n+        self.assertEqual(il.get_output_value(\"name\"), [\"foo\"])\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(loaded_item, dict({'name': ['foo']}))\n+        self.assertEqual(loaded_item, dict({\"name\": [\"foo\"]}))\n \n     def test_get_output_value_list(self):\n         \"\"\"Getting output value must not remove value from item\"\"\"\n-        input_item = self.item_class(name=['foo', 'bar'])\n+        input_item = self.item_class(name=[\"foo\", \"bar\"])\n         il = ItemLoader(item=input_item)\n-        self.assertEqual(il.get_output_value('name'), ['foo', 'bar'])\n+        self.assertEqual(il.get_output_value(\"name\"), [\"foo\", \"bar\"])\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(loaded_item, dict({'name': ['foo', 'bar']}))\n+        self.assertEqual(loaded_item, dict({\"name\": [\"foo\", \"bar\"]}))\n \n     def test_values_single(self):\n         \"\"\"Values from initial item must be added to loader._values\"\"\"\n-        input_item = self.item_class(name='foo')\n+        input_item = self.item_class(name=\"foo\")\n         il = ItemLoader(item=input_item)\n-        self.assertEqual(il._values.get('name'), ['foo'])\n+        self.assertEqual(il._values.get(\"name\"), [\"foo\"])\n \n     def test_values_list(self):\n         \"\"\"Values from initial item must be added to loader._values\"\"\"\n-        input_item = self.item_class(name=['foo', 'bar'])\n+        input_item = self.item_class(name=[\"foo\", \"bar\"])\n         il = ItemLoader(item=input_item)\n-        self.assertEqual(il._values.get('name'), ['foo', 'bar'])\n+        self.assertEqual(il._values.get(\"name\"), [\"foo\", \"bar\"])\n \n \n class BaseNoInputReprocessingLoader(ItemLoader):\n@@ -546,40 +568,48 @@ class NoInputReprocessingFromDictTest(unittest.TestCase):\n     \"\"\"\n     Loaders initialized from loaded items must not reprocess fields (dict instances)\n     \"\"\"\n+\n     def test_avoid_reprocessing_with_initial_values_single(self):\n-        il = NoInputReprocessingDictLoader(item=dict(title='foo'))\n+        il = NoInputReprocessingDictLoader(item=dict(title=\"foo\"))\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, dict(title='foo'))\n-        self.assertEqual(NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title='foo'))\n+        self.assertEqual(il_loaded, dict(title=\"foo\"))\n+        self.assertEqual(\n+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title=\"foo\")\n+        )\n \n     def test_avoid_reprocessing_with_initial_values_list(self):\n-        il = NoInputReprocessingDictLoader(item=dict(title=['foo', 'bar']))\n+        il = NoInputReprocessingDictLoader(item=dict(title=[\"foo\", \"bar\"]))\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, dict(title='foo'))\n-        self.assertEqual(NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title='foo'))\n+        self.assertEqual(il_loaded, dict(title=\"foo\"))\n+        self.assertEqual(\n+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title=\"foo\")\n+        )\n \n     def test_avoid_reprocessing_without_initial_values_single(self):\n         il = NoInputReprocessingDictLoader()\n-        il.add_value('title', 'foo')\n+        il.add_value(\"title\", \"foo\")\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, dict(title='FOO'))\n-        self.assertEqual(NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title='FOO'))\n+        self.assertEqual(il_loaded, dict(title=\"FOO\"))\n+        self.assertEqual(\n+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title=\"FOO\")\n+        )\n \n     def test_avoid_reprocessing_without_initial_values_list(self):\n         il = NoInputReprocessingDictLoader()\n-        il.add_value('title', ['foo', 'bar'])\n+        il.add_value(\"title\", [\"foo\", \"bar\"])\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, dict(title='FOO'))\n-        self.assertEqual(NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title='FOO'))\n+        self.assertEqual(il_loaded, dict(title=\"FOO\"))\n+        self.assertEqual(\n+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title=\"FOO\")\n+        )\n \n \n class TestOutputProcessorDict(unittest.TestCase):\n     def test_output_processor(self):\n-\n         class TempDict(dict):\n             def __init__(self, *args, **kwargs):\n                 super().__init__(self, *args, **kwargs)\n-                self.setdefault('temp', 0.3)\n+                self.setdefault(\"temp\", 0.3)\n \n         class TempLoader(ItemLoader):\n             default_item_class = TempDict\n@@ -589,64 +619,67 @@ class TestOutputProcessorDict(unittest.TestCase):\n         loader = TempLoader()\n         item = loader.load_item()\n         self.assertIsInstance(item, TempDict)\n-        self.assertEqual(dict(item), {'temp': 0.3})\n+        self.assertEqual(dict(item), {\"temp\": 0.3})\n \n \n class ProcessorsTest(unittest.TestCase):\n-\n     def test_take_first(self):\n         proc = TakeFirst()\n-        self.assertEqual(proc([None, '', 'hello', 'world']), 'hello')\n-        self.assertEqual(proc([None, '', 0, 'hello', 'world']), 0)\n+        self.assertEqual(proc([None, \"\", \"hello\", \"world\"]), \"hello\")\n+        self.assertEqual(proc([None, \"\", 0, \"hello\", \"world\"]), 0)\n \n     def test_identity(self):\n         proc = Identity()\n-        self.assertEqual(proc([None, '', 'hello', 'world']),\n-                         [None, '', 'hello', 'world'])\n+        self.assertEqual(\n+            proc([None, \"\", \"hello\", \"world\"]), [None, \"\", \"hello\", \"world\"]\n+        )\n \n     def test_join(self):\n         proc = Join()\n-        self.assertRaises(TypeError, proc, [None, '', 'hello', 'world'])\n-        self.assertEqual(proc(['', 'hello', 'world']), ' hello world')\n-        self.assertEqual(proc(['hello', 'world']), 'hello world')\n-        self.assertIsInstance(proc(['hello', 'world']), str)\n+        self.assertRaises(TypeError, proc, [None, \"\", \"hello\", \"world\"])\n+        self.assertEqual(proc([\"\", \"hello\", \"world\"]), \" hello world\")\n+        self.assertEqual(proc([\"hello\", \"world\"]), \"hello world\")\n+        self.assertIsInstance(proc([\"hello\", \"world\"]), str)\n \n     def test_compose(self):\n         proc = Compose(lambda v: v[0], str.upper)\n-        self.assertEqual(proc(['hello', 'world']), 'HELLO')\n+        self.assertEqual(proc([\"hello\", \"world\"]), \"HELLO\")\n         proc = Compose(str.upper)\n         self.assertEqual(proc(None), None)\n         proc = Compose(str.upper, stop_on_none=False)\n         self.assertRaises(ValueError, proc, None)\n         proc = Compose(str.upper, lambda x: x + 1)\n-        self.assertRaises(ValueError, proc, 'hello')\n+        self.assertRaises(ValueError, proc, \"hello\")\n \n     def test_mapcompose(self):\n         def filter_world(x):\n-            return None if x == 'world' else x\n+            return None if x == \"world\" else x\n+\n         proc = MapCompose(filter_world, str.upper)\n-        self.assertEqual(proc(['hello', 'world', 'this', 'is', 'scrapy']),\n-                         ['HELLO', 'THIS', 'IS', 'SCRAPY'])\n+        self.assertEqual(\n+            proc([\"hello\", \"world\", \"this\", \"is\", \"scrapy\"]),\n+            [\"HELLO\", \"THIS\", \"IS\", \"SCRAPY\"],\n+        )\n         proc = MapCompose(filter_world, str.upper)\n         self.assertEqual(proc(None), [])\n         proc = MapCompose(filter_world, str.upper)\n         self.assertRaises(ValueError, proc, [1])\n         proc = MapCompose(filter_world, lambda x: x + 1)\n-        self.assertRaises(ValueError, proc, 'hello')\n+        self.assertRaises(ValueError, proc, \"hello\")\n \n \n class SelectJmesTestCase(unittest.TestCase):\n     test_list_equals = {\n-        'simple': ('foo.bar', {\"foo\": {\"bar\": \"baz\"}}, \"baz\"),\n-        'invalid': ('foo.bar.baz', {\"foo\": {\"bar\": \"baz\"}}, None),\n-        'top_level': ('foo', {\"foo\": {\"bar\": \"baz\"}}, {\"bar\": \"baz\"}),\n-        'double_vs_single_quote_string': ('foo.bar', {\"foo\": {\"bar\": \"baz\"}}, \"baz\"),\n-        'dict': (\n-            'foo.bar[*].name',\n+        \"simple\": (\"foo.bar\", {\"foo\": {\"bar\": \"baz\"}}, \"baz\"),\n+        \"invalid\": (\"foo.bar.baz\", {\"foo\": {\"bar\": \"baz\"}}, None),\n+        \"top_level\": (\"foo\", {\"foo\": {\"bar\": \"baz\"}}, {\"bar\": \"baz\"}),\n+        \"double_vs_single_quote_string\": (\"foo.bar\", {\"foo\": {\"bar\": \"baz\"}}, \"baz\"),\n+        \"dict\": (\n+            \"foo.bar[*].name\",\n             {\"foo\": {\"bar\": [{\"name\": \"one\"}, {\"name\": \"two\"}]}},\n-            ['one', 'two']\n+            [\"one\", \"two\"],\n         ),\n-        'list': ('[1]', [1, 2], 2)\n+        \"list\": (\"[1]\", [1, 2], 2),\n     }\n \n     def test_output(self):\n@@ -654,14 +687,13 @@ class SelectJmesTestCase(unittest.TestCase):\n             expr, test_list, expected = self.test_list_equals[tl]\n             test = SelectJmes(expr)(test_list)\n             self.assertEqual(\n-                test,\n-                expected,\n-                msg=f'test \"{tl}\" got {test} expected {expected}'\n+                test, expected, msg=f'test \"{tl}\" got {test} expected {expected}'\n             )\n \n \n # Functions as processors\n \n+\n def function_processor_strip(iterable):\n     return [x.strip() for x in iterable]\n \n@@ -684,19 +716,14 @@ class FunctionProcessorDictLoader(ItemLoader):\n \n \n class FunctionProcessorTestCase(unittest.TestCase):\n-\n     def test_processor_defined_in_item_loader(self):\n         lo = FunctionProcessorDictLoader()\n-        lo.add_value('foo', '  bar  ')\n-        lo.add_value('foo', ['  asdf  ', '  qwerty  '])\n-        self.assertEqual(\n-            dict(lo.load_item()),\n-            {'foo': ['BAR', 'ASDF', 'QWERTY']}\n-        )\n+        lo.add_value(\"foo\", \"  bar  \")\n+        lo.add_value(\"foo\", [\"  asdf  \", \"  qwerty  \"])\n+        self.assertEqual(dict(lo.load_item()), {\"foo\": [\"BAR\", \"ASDF\", \"QWERTY\"]})\n \n \n class DeprecatedUtilityFunctionsTestCase(unittest.TestCase):\n-\n     def test_deprecated_wrap_loader_context(self):\n         def function(*args):\n             return None\n@@ -709,7 +736,7 @@ class DeprecatedUtilityFunctionsTestCase(unittest.TestCase):\n \n     def test_deprecated_extract_regex(self):\n         with warnings.catch_warnings(record=True) as w:\n-            extract_regex(r'\\w+', 'this is a test')\n+            extract_regex(r\"\\w+\", \"this is a test\")\n \n             assert len(w) == 1\n             assert issubclass(w[0].category, ScrapyDeprecationWarning)\n\n@@ -24,67 +24,74 @@ class CustomItem(Item):\n \n \n class LogFormatterTestCase(unittest.TestCase):\n-\n     def setUp(self):\n         self.formatter = LogFormatter()\n-        self.spider = Spider('default')\n+        self.spider = Spider(\"default\")\n \n     def test_crawled_with_referer(self):\n         req = Request(\"http://www.example.com\")\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n-        logline = logkws['msg'] % logkws['args']\n-        self.assertEqual(logline, \"Crawled (200) <GET http://www.example.com> (referer: None)\")\n+        logline = logkws[\"msg\"] % logkws[\"args\"]\n+        self.assertEqual(\n+            logline, \"Crawled (200) <GET http://www.example.com> (referer: None)\"\n+        )\n \n     def test_crawled_without_referer(self):\n-        req = Request(\"http://www.example.com\", headers={'referer': 'http://example.com'})\n-        res = Response(\"http://www.example.com\", flags=['cached'])\n+        req = Request(\n+            \"http://www.example.com\", headers={\"referer\": \"http://example.com\"}\n+        )\n+        res = Response(\"http://www.example.com\", flags=[\"cached\"])\n         logkws = self.formatter.crawled(req, res, self.spider)\n-        logline = logkws['msg'] % logkws['args']\n+        logline = logkws[\"msg\"] % logkws[\"args\"]\n         self.assertEqual(\n             logline,\n-            \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\")\n+            \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\",\n+        )\n \n     def test_flags_in_request(self):\n-        req = Request(\"http://www.example.com\", flags=['test', 'flag'])\n+        req = Request(\"http://www.example.com\", flags=[\"test\", \"flag\"])\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n-        logline = logkws['msg'] % logkws['args']\n+        logline = logkws[\"msg\"] % logkws[\"args\"]\n         self.assertEqual(\n             logline,\n-            \"Crawled (200) <GET http://www.example.com> ['test', 'flag'] (referer: None)\")\n+            \"Crawled (200) <GET http://www.example.com> ['test', 'flag'] (referer: None)\",\n+        )\n \n     def test_dropped(self):\n         item = {}\n         exception = Exception(\"\\u2018\")\n         response = Response(\"http://www.example.com\")\n         logkws = self.formatter.dropped(item, exception, response, self.spider)\n-        logline = logkws['msg'] % logkws['args']\n+        logline = logkws[\"msg\"] % logkws[\"args\"]\n         lines = logline.splitlines()\n         assert all(isinstance(x, str) for x in lines)\n-        self.assertEqual(lines, [\"Dropped: \\u2018\", '{}'])\n+        self.assertEqual(lines, [\"Dropped: \\u2018\", \"{}\"])\n \n     def test_item_error(self):\n         # In practice, the complete traceback is shown by passing the\n         # 'exc_info' argument to the logging function\n-        item = {'key': 'value'}\n+        item = {\"key\": \"value\"}\n         exception = Exception()\n         response = Response(\"http://www.example.com\")\n         logkws = self.formatter.item_error(item, exception, response, self.spider)\n-        logline = logkws['msg'] % logkws['args']\n+        logline = logkws[\"msg\"] % logkws[\"args\"]\n         self.assertEqual(logline, \"Error processing {'key': 'value'}\")\n \n     def test_spider_error(self):\n         # In practice, the complete traceback is shown by passing the\n         # 'exc_info' argument to the logging function\n         failure = Failure(Exception())\n-        request = Request(\"http://www.example.com\", headers={'Referer': 'http://example.org'})\n+        request = Request(\n+            \"http://www.example.com\", headers={\"Referer\": \"http://example.org\"}\n+        )\n         response = Response(\"http://www.example.com\", request=request)\n         logkws = self.formatter.spider_error(failure, request, response, self.spider)\n-        logline = logkws['msg'] % logkws['args']\n+        logline = logkws[\"msg\"] % logkws[\"args\"]\n         self.assertEqual(\n             logline,\n-            \"Spider error processing <GET http://www.example.com> (referer: http://example.org)\"\n+            \"Spider error processing <GET http://www.example.com> (referer: http://example.org)\",\n         )\n \n     def test_download_error_short(self):\n@@ -93,7 +100,7 @@ class LogFormatterTestCase(unittest.TestCase):\n         failure = Failure(Exception())\n         request = Request(\"http://www.example.com\")\n         logkws = self.formatter.download_error(failure, request, self.spider)\n-        logline = logkws['msg'] % logkws['args']\n+        logline = logkws[\"msg\"] % logkws[\"args\"]\n         self.assertEqual(logline, \"Error downloading <GET http://www.example.com>\")\n \n     def test_download_error_long(self):\n@@ -101,67 +108,77 @@ class LogFormatterTestCase(unittest.TestCase):\n         # 'exc_info' argument to the logging function\n         failure = Failure(Exception())\n         request = Request(\"http://www.example.com\")\n-        logkws = self.formatter.download_error(failure, request, self.spider, \"Some message\")\n-        logline = logkws['msg'] % logkws['args']\n-        self.assertEqual(logline, \"Error downloading <GET http://www.example.com>: Some message\")\n+        logkws = self.formatter.download_error(\n+            failure, request, self.spider, \"Some message\"\n+        )\n+        logline = logkws[\"msg\"] % logkws[\"args\"]\n+        self.assertEqual(\n+            logline, \"Error downloading <GET http://www.example.com>: Some message\"\n+        )\n \n     def test_scraped(self):\n         item = CustomItem()\n-        item['name'] = '\\xa3'\n+        item[\"name\"] = \"\\xa3\"\n         response = Response(\"http://www.example.com\")\n         logkws = self.formatter.scraped(item, response, self.spider)\n-        logline = logkws['msg'] % logkws['args']\n+        logline = logkws[\"msg\"] % logkws[\"args\"]\n         lines = logline.splitlines()\n         assert all(isinstance(x, str) for x in lines)\n-        self.assertEqual(lines, [\"Scraped from <200 http://www.example.com>\", 'name: \\xa3'])\n+        self.assertEqual(\n+            lines, [\"Scraped from <200 http://www.example.com>\", \"name: \\xa3\"]\n+        )\n \n \n class LogFormatterSubclass(LogFormatter):\n     def crawled(self, request, response, spider):\n         kwargs = super().crawled(request, response, spider)\n-        CRAWLEDMSG = (\n-            \"Crawled (%(status)s) %(request)s (referer: %(referer)s) %(flags)s\"\n-        )\n-        log_args = kwargs['args']\n-        log_args['flags'] = str(request.flags)\n+        CRAWLEDMSG = \"Crawled (%(status)s) %(request)s (referer: %(referer)s) %(flags)s\"\n+        log_args = kwargs[\"args\"]\n+        log_args[\"flags\"] = str(request.flags)\n         return {\n-            'level': kwargs['level'],\n-            'msg': CRAWLEDMSG,\n-            'args': log_args,\n+            \"level\": kwargs[\"level\"],\n+            \"msg\": CRAWLEDMSG,\n+            \"args\": log_args,\n         }\n \n \n class LogformatterSubclassTest(LogFormatterTestCase):\n     def setUp(self):\n         self.formatter = LogFormatterSubclass()\n-        self.spider = Spider('default')\n+        self.spider = Spider(\"default\")\n \n     def test_crawled_with_referer(self):\n         req = Request(\"http://www.example.com\")\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n-        logline = logkws['msg'] % logkws['args']\n+        logline = logkws[\"msg\"] % logkws[\"args\"]\n         self.assertEqual(\n-            logline,\n-            \"Crawled (200) <GET http://www.example.com> (referer: None) []\")\n+            logline, \"Crawled (200) <GET http://www.example.com> (referer: None) []\"\n+        )\n \n     def test_crawled_without_referer(self):\n-        req = Request(\"http://www.example.com\", headers={'referer': 'http://example.com'}, flags=['cached'])\n+        req = Request(\n+            \"http://www.example.com\",\n+            headers={\"referer\": \"http://example.com\"},\n+            flags=[\"cached\"],\n+        )\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n-        logline = logkws['msg'] % logkws['args']\n+        logline = logkws[\"msg\"] % logkws[\"args\"]\n         self.assertEqual(\n             logline,\n-            \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\")\n+            \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\",\n+        )\n \n     def test_flags_in_request(self):\n-        req = Request(\"http://www.example.com\", flags=['test', 'flag'])\n+        req = Request(\"http://www.example.com\", flags=[\"test\", \"flag\"])\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n-        logline = logkws['msg'] % logkws['args']\n+        logline = logkws[\"msg\"] % logkws[\"args\"]\n         self.assertEqual(\n             logline,\n-            \"Crawled (200) <GET http://www.example.com> (referer: None) ['test', 'flag']\")\n+            \"Crawled (200) <GET http://www.example.com> (referer: None) ['test', 'flag']\",\n+        )\n \n \n class SkipMessagesLogFormatter(LogFormatter):\n@@ -191,8 +208,8 @@ class ShowOrSkipMessagesTestCase(TwistedTestCase):\n         self.mockserver = MockServer()\n         self.mockserver.__enter__()\n         self.base_settings = {\n-            'LOG_LEVEL': 'DEBUG',\n-            'ITEM_PIPELINES': {\n+            \"LOG_LEVEL\": \"DEBUG\",\n+            \"ITEM_PIPELINES\": {\n                 DropSomeItemsPipeline: 300,\n             },\n         }\n@@ -212,7 +229,7 @@ class ShowOrSkipMessagesTestCase(TwistedTestCase):\n     @defer.inlineCallbacks\n     def test_skip_messages(self):\n         settings = self.base_settings.copy()\n-        settings['LOG_FORMATTER'] = SkipMessagesLogFormatter\n+        settings[\"LOG_FORMATTER\"] = SkipMessagesLogFormatter\n         crawler = get_crawler(ItemSpider, settings)\n         with LogCapture() as lc:\n             yield crawler.crawl(mockserver=self.mockserver)\n\n@@ -8,118 +8,144 @@ from scrapy.mail import MailSender\n \n \n class MailSenderTest(unittest.TestCase):\n-\n     def test_send(self):\n         mailsender = MailSender(debug=True)\n-        mailsender.send(to=['test@scrapy.org'], subject='subject', body='body',\n-                        _callback=self._catch_mail_sent)\n+        mailsender.send(\n+            to=[\"test@scrapy.org\"],\n+            subject=\"subject\",\n+            body=\"body\",\n+            _callback=self._catch_mail_sent,\n+        )\n \n         assert self.catched_msg\n \n-        self.assertEqual(self.catched_msg['to'], ['test@scrapy.org'])\n-        self.assertEqual(self.catched_msg['subject'], 'subject')\n-        self.assertEqual(self.catched_msg['body'], 'body')\n+        self.assertEqual(self.catched_msg[\"to\"], [\"test@scrapy.org\"])\n+        self.assertEqual(self.catched_msg[\"subject\"], \"subject\")\n+        self.assertEqual(self.catched_msg[\"body\"], \"body\")\n \n-        msg = self.catched_msg['msg']\n-        self.assertEqual(msg['to'], 'test@scrapy.org')\n-        self.assertEqual(msg['subject'], 'subject')\n-        self.assertEqual(msg.get_payload(), 'body')\n-        self.assertEqual(msg.get('Content-Type'), 'text/plain')\n+        msg = self.catched_msg[\"msg\"]\n+        self.assertEqual(msg[\"to\"], \"test@scrapy.org\")\n+        self.assertEqual(msg[\"subject\"], \"subject\")\n+        self.assertEqual(msg.get_payload(), \"body\")\n+        self.assertEqual(msg.get(\"Content-Type\"), \"text/plain\")\n \n     def test_send_single_values_to_and_cc(self):\n         mailsender = MailSender(debug=True)\n-        mailsender.send(to='test@scrapy.org', subject='subject', body='body',\n-                        cc='test@scrapy.org', _callback=self._catch_mail_sent)\n+        mailsender.send(\n+            to=\"test@scrapy.org\",\n+            subject=\"subject\",\n+            body=\"body\",\n+            cc=\"test@scrapy.org\",\n+            _callback=self._catch_mail_sent,\n+        )\n \n     def test_send_html(self):\n         mailsender = MailSender(debug=True)\n-        mailsender.send(to=['test@scrapy.org'], subject='subject',\n-                        body='<p>body</p>', mimetype='text/html',\n-                        _callback=self._catch_mail_sent)\n+        mailsender.send(\n+            to=[\"test@scrapy.org\"],\n+            subject=\"subject\",\n+            body=\"<p>body</p>\",\n+            mimetype=\"text/html\",\n+            _callback=self._catch_mail_sent,\n+        )\n \n-        msg = self.catched_msg['msg']\n-        self.assertEqual(msg.get_payload(), '<p>body</p>')\n-        self.assertEqual(msg.get('Content-Type'), 'text/html')\n+        msg = self.catched_msg[\"msg\"]\n+        self.assertEqual(msg.get_payload(), \"<p>body</p>\")\n+        self.assertEqual(msg.get(\"Content-Type\"), \"text/html\")\n \n     def test_send_attach(self):\n         attach = BytesIO()\n-        attach.write(b'content')\n+        attach.write(b\"content\")\n         attach.seek(0)\n-        attachs = [('attachment', 'text/plain', attach)]\n+        attachs = [(\"attachment\", \"text/plain\", attach)]\n \n         mailsender = MailSender(debug=True)\n-        mailsender.send(to=['test@scrapy.org'], subject='subject', body='body',\n-                        attachs=attachs, _callback=self._catch_mail_sent)\n+        mailsender.send(\n+            to=[\"test@scrapy.org\"],\n+            subject=\"subject\",\n+            body=\"body\",\n+            attachs=attachs,\n+            _callback=self._catch_mail_sent,\n+        )\n \n         assert self.catched_msg\n-        self.assertEqual(self.catched_msg['to'], ['test@scrapy.org'])\n-        self.assertEqual(self.catched_msg['subject'], 'subject')\n-        self.assertEqual(self.catched_msg['body'], 'body')\n+        self.assertEqual(self.catched_msg[\"to\"], [\"test@scrapy.org\"])\n+        self.assertEqual(self.catched_msg[\"subject\"], \"subject\")\n+        self.assertEqual(self.catched_msg[\"body\"], \"body\")\n \n-        msg = self.catched_msg['msg']\n-        self.assertEqual(msg['to'], 'test@scrapy.org')\n-        self.assertEqual(msg['subject'], 'subject')\n+        msg = self.catched_msg[\"msg\"]\n+        self.assertEqual(msg[\"to\"], \"test@scrapy.org\")\n+        self.assertEqual(msg[\"subject\"], \"subject\")\n \n         payload = msg.get_payload()\n         assert isinstance(payload, list)\n         self.assertEqual(len(payload), 2)\n \n         text, attach = payload\n-        self.assertEqual(text.get_payload(decode=True), b'body')\n-        self.assertEqual(text.get_charset(), Charset('us-ascii'))\n-        self.assertEqual(attach.get_payload(decode=True), b'content')\n+        self.assertEqual(text.get_payload(decode=True), b\"body\")\n+        self.assertEqual(text.get_charset(), Charset(\"us-ascii\"))\n+        self.assertEqual(attach.get_payload(decode=True), b\"content\")\n \n     def _catch_mail_sent(self, **kwargs):\n         self.catched_msg = dict(**kwargs)\n \n     def test_send_utf8(self):\n-        subject = 'sübjèçt'\n-        body = 'bödÿ-àéïöñß'\n+        subject = \"sübjèçt\"\n+        body = \"bödÿ-àéïöñß\"\n         mailsender = MailSender(debug=True)\n-        mailsender.send(to=['test@scrapy.org'], subject=subject, body=body,\n-                        charset='utf-8', _callback=self._catch_mail_sent)\n+        mailsender.send(\n+            to=[\"test@scrapy.org\"],\n+            subject=subject,\n+            body=body,\n+            charset=\"utf-8\",\n+            _callback=self._catch_mail_sent,\n+        )\n \n         assert self.catched_msg\n-        self.assertEqual(self.catched_msg['subject'], subject)\n-        self.assertEqual(self.catched_msg['body'], body)\n+        self.assertEqual(self.catched_msg[\"subject\"], subject)\n+        self.assertEqual(self.catched_msg[\"body\"], body)\n \n-        msg = self.catched_msg['msg']\n-        self.assertEqual(msg['subject'], subject)\n+        msg = self.catched_msg[\"msg\"]\n+        self.assertEqual(msg[\"subject\"], subject)\n         self.assertEqual(msg.get_payload(), body)\n-        self.assertEqual(msg.get_charset(), Charset('utf-8'))\n-        self.assertEqual(msg.get('Content-Type'), 'text/plain; charset=\"utf-8\"')\n+        self.assertEqual(msg.get_charset(), Charset(\"utf-8\"))\n+        self.assertEqual(msg.get(\"Content-Type\"), 'text/plain; charset=\"utf-8\"')\n \n     def test_send_attach_utf8(self):\n-        subject = 'sübjèçt'\n-        body = 'bödÿ-àéïöñß'\n+        subject = \"sübjèçt\"\n+        body = \"bödÿ-àéïöñß\"\n         attach = BytesIO()\n-        attach.write(body.encode('utf-8'))\n+        attach.write(body.encode(\"utf-8\"))\n         attach.seek(0)\n-        attachs = [('attachment', 'text/plain', attach)]\n+        attachs = [(\"attachment\", \"text/plain\", attach)]\n \n         mailsender = MailSender(debug=True)\n-        mailsender.send(to=['test@scrapy.org'], subject=subject, body=body,\n-                        attachs=attachs, charset='utf-8',\n-                        _callback=self._catch_mail_sent)\n+        mailsender.send(\n+            to=[\"test@scrapy.org\"],\n+            subject=subject,\n+            body=body,\n+            attachs=attachs,\n+            charset=\"utf-8\",\n+            _callback=self._catch_mail_sent,\n+        )\n \n         assert self.catched_msg\n-        self.assertEqual(self.catched_msg['subject'], subject)\n-        self.assertEqual(self.catched_msg['body'], body)\n+        self.assertEqual(self.catched_msg[\"subject\"], subject)\n+        self.assertEqual(self.catched_msg[\"body\"], body)\n \n-        msg = self.catched_msg['msg']\n-        self.assertEqual(msg['subject'], subject)\n-        self.assertEqual(msg.get_charset(), Charset('utf-8'))\n-        self.assertEqual(msg.get('Content-Type'),\n-                         'multipart/mixed; charset=\"utf-8\"')\n+        msg = self.catched_msg[\"msg\"]\n+        self.assertEqual(msg[\"subject\"], subject)\n+        self.assertEqual(msg.get_charset(), Charset(\"utf-8\"))\n+        self.assertEqual(msg.get(\"Content-Type\"), 'multipart/mixed; charset=\"utf-8\"')\n \n         payload = msg.get_payload()\n         assert isinstance(payload, list)\n         self.assertEqual(len(payload), 2)\n \n         text, attach = payload\n-        self.assertEqual(text.get_payload(decode=True).decode('utf-8'), body)\n-        self.assertEqual(text.get_charset(), Charset('utf-8'))\n-        self.assertEqual(attach.get_payload(decode=True).decode('utf-8'), body)\n+        self.assertEqual(text.get_payload(decode=True).decode(\"utf-8\"), body)\n+        self.assertEqual(text.get_charset(), Charset(\"utf-8\"))\n+        self.assertEqual(attach.get_payload(decode=True).decode(\"utf-8\"), body)\n \n \n if __name__ == \"__main__\":\n\n@@ -6,7 +6,6 @@ from scrapy.middleware import MiddlewareManager\n \n \n class M1:\n-\n     def open_spider(self, spider):\n         pass\n \n@@ -18,7 +17,6 @@ class M1:\n \n \n class M2:\n-\n     def open_spider(self, spider):\n         pass\n \n@@ -29,13 +27,11 @@ class M2:\n \n \n class M3:\n-\n     def process(self, response, request, spider):\n         pass\n \n \n class MOff:\n-\n     def open_spider(self, spider):\n         pass\n \n@@ -47,37 +43,39 @@ class MOff:\n \n \n class TestMiddlewareManager(MiddlewareManager):\n-\n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n         return [M1, MOff, M3]\n \n     def _add_middleware(self, mw):\n         super()._add_middleware(mw)\n-        if hasattr(mw, 'process'):\n-            self.methods['process'].append(mw.process)\n+        if hasattr(mw, \"process\"):\n+            self.methods[\"process\"].append(mw.process)\n \n \n class MiddlewareManagerTest(unittest.TestCase):\n-\n     def test_init(self):\n         m1, m2, m3 = M1(), M2(), M3()\n         mwman = TestMiddlewareManager(m1, m2, m3)\n-        self.assertEqual(list(mwman.methods['open_spider']), [m1.open_spider, m2.open_spider])\n-        self.assertEqual(list(mwman.methods['close_spider']), [m2.close_spider, m1.close_spider])\n-        self.assertEqual(list(mwman.methods['process']), [m1.process, m3.process])\n+        self.assertEqual(\n+            list(mwman.methods[\"open_spider\"]), [m1.open_spider, m2.open_spider]\n+        )\n+        self.assertEqual(\n+            list(mwman.methods[\"close_spider\"]), [m2.close_spider, m1.close_spider]\n+        )\n+        self.assertEqual(list(mwman.methods[\"process\"]), [m1.process, m3.process])\n \n     def test_methods(self):\n         mwman = TestMiddlewareManager(M1(), M2(), M3())\n         self.assertEqual(\n-            [x.__self__.__class__ for x in mwman.methods['open_spider']],\n-            [M1, M2])\n+            [x.__self__.__class__ for x in mwman.methods[\"open_spider\"]], [M1, M2]\n+        )\n         self.assertEqual(\n-            [x.__self__.__class__ for x in mwman.methods['close_spider']],\n-            [M2, M1])\n+            [x.__self__.__class__ for x in mwman.methods[\"close_spider\"]], [M2, M1]\n+        )\n         self.assertEqual(\n-            [x.__self__.__class__ for x in mwman.methods['process']],\n-            [M1, M3])\n+            [x.__self__.__class__ for x in mwman.methods[\"process\"]], [M1, M3]\n+        )\n \n     def test_enabled(self):\n         m1, m2, m3 = M1(), M2(), M3()\n\n@@ -13,7 +13,7 @@ from tests.spiders import SimpleSpider\n \n \n class MediaDownloadSpider(SimpleSpider):\n-    name = 'mediadownload'\n+    name = \"mediadownload\"\n \n     def _process_url(self, url):\n         return url\n@@ -27,34 +27,38 @@ class MediaDownloadSpider(SimpleSpider):\n                 self._process_url(response.urljoin(href))\n                 for href in response.xpath(\n                     '//table[thead/tr/th=\"Filename\"]/tbody//a/@href'\n-                ).getall()],\n+                ).getall()\n+            ],\n         }\n         yield item\n \n \n class BrokenLinksMediaDownloadSpider(MediaDownloadSpider):\n-    name = 'brokenmedia'\n+    name = \"brokenmedia\"\n \n     def _process_url(self, url):\n-        return url + '.foo'\n+        return url + \".foo\"\n \n \n class RedirectedMediaDownloadSpider(MediaDownloadSpider):\n-    name = 'redirectedmedia'\n+    name = \"redirectedmedia\"\n \n     def _process_url(self, url):\n-        return add_or_replace_parameter(self.mockserver.url('/redirect-to'), 'goto', url)\n+        return add_or_replace_parameter(\n+            self.mockserver.url(\"/redirect-to\"), \"goto\", url\n+        )\n \n \n class FileDownloadCrawlTestCase(TestCase):\n-    pipeline_class = 'scrapy.pipelines.files.FilesPipeline'\n-    store_setting_key = 'FILES_STORE'\n-    media_key = 'files'\n-    media_urls_key = 'file_urls'\n+    pipeline_class = \"scrapy.pipelines.files.FilesPipeline\"\n+    store_setting_key = \"FILES_STORE\"\n+    media_key = \"files\"\n+    media_urls_key = \"file_urls\"\n     expected_checksums = {\n-        '5547178b89448faf0015a13f904c936e',\n-        'c2281c83670e31d8aaab7cb642b824db',\n-        'ed3f6538dc15d4d9179dae57319edc5f'}\n+        \"5547178b89448faf0015a13f904c936e\",\n+        \"c2281c83670e31d8aaab7cb642b824db\",\n+        \"ed3f6538dc15d4d9179dae57319edc5f\",\n+    }\n \n     def setUp(self):\n         self.mockserver = MockServer()\n@@ -64,8 +68,8 @@ class FileDownloadCrawlTestCase(TestCase):\n         self.tmpmediastore = Path(self.mktemp())\n         self.tmpmediastore.mkdir()\n         self.settings = {\n-            'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n-            'ITEM_PIPELINES': {self.pipeline_class: 1},\n+            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n+            \"ITEM_PIPELINES\": {self.pipeline_class: 1},\n             self.store_setting_key: str(self.tmpmediastore),\n         }\n         self.runner = CrawlerRunner(self.settings)\n@@ -91,27 +95,25 @@ class FileDownloadCrawlTestCase(TestCase):\n         self.assertIn(self.media_key, items[0])\n \n         # check that logs show the expected number of successful file downloads\n-        file_dl_success = 'File (downloaded): Downloaded file from'\n+        file_dl_success = \"File (downloaded): Downloaded file from\"\n         self.assertEqual(logs.count(file_dl_success), 3)\n \n         # check that the images/files status is `downloaded`\n         for item in items:\n             for i in item[self.media_key]:\n-                self.assertEqual(i['status'], 'downloaded')\n+                self.assertEqual(i[\"status\"], \"downloaded\")\n \n         # check that the images/files checksums are what we know they should be\n         if self.expected_checksums is not None:\n             checksums = set(\n-                i['checksum']\n-                for item in items\n-                for i in item[self.media_key]\n+                i[\"checksum\"] for item in items for i in item[self.media_key]\n             )\n             self.assertEqual(checksums, self.expected_checksums)\n \n         # check that the image files where actually written to the media store\n         for item in items:\n             for i in item[self.media_key]:\n-                self.assertTrue((self.tmpmediastore / i['path']).exists())\n+                self.assertTrue((self.tmpmediastore / i[\"path\"]).exists())\n \n     def _assert_files_download_failure(self, crawler, items, code, logs):\n \n@@ -121,13 +123,19 @@ class FileDownloadCrawlTestCase(TestCase):\n         self.assertFalse(items[0][self.media_key])\n \n         # check that there was 1 successful fetch and 3 other responses with non-200 code\n-        self.assertEqual(crawler.stats.get_value('downloader/request_method_count/GET'), 4)\n-        self.assertEqual(crawler.stats.get_value('downloader/response_count'), 4)\n-        self.assertEqual(crawler.stats.get_value('downloader/response_status_count/200'), 1)\n-        self.assertEqual(crawler.stats.get_value(f'downloader/response_status_count/{code}'), 3)\n+        self.assertEqual(\n+            crawler.stats.get_value(\"downloader/request_method_count/GET\"), 4\n+        )\n+        self.assertEqual(crawler.stats.get_value(\"downloader/response_count\"), 4)\n+        self.assertEqual(\n+            crawler.stats.get_value(\"downloader/response_status_count/200\"), 1\n+        )\n+        self.assertEqual(\n+            crawler.stats.get_value(f\"downloader/response_status_count/{code}\"), 3\n+        )\n \n         # check that logs do show the failure on the file downloads\n-        file_dl_failure = f'File (code: {code}): Error downloading file from'\n+        file_dl_failure = f\"File (code: {code}): Error downloading file from\"\n         self.assertEqual(logs.count(file_dl_failure), 3)\n \n         # check that no files were written to the media store\n@@ -140,7 +148,8 @@ class FileDownloadCrawlTestCase(TestCase):\n             yield crawler.crawl(\n                 self.mockserver.url(\"/files/images/\"),\n                 media_key=self.media_key,\n-                media_urls_key=self.media_urls_key)\n+                media_urls_key=self.media_urls_key,\n+            )\n         self._assert_files_downloaded(self.items, str(log))\n \n     @defer.inlineCallbacks\n@@ -150,7 +159,8 @@ class FileDownloadCrawlTestCase(TestCase):\n             yield crawler.crawl(\n                 self.mockserver.url(\"/files/images/\"),\n                 media_key=self.media_key,\n-                media_urls_key=self.media_urls_key)\n+                media_urls_key=self.media_urls_key,\n+            )\n         self._assert_files_download_failure(crawler, self.items, 404, str(log))\n \n     @defer.inlineCallbacks\n@@ -161,13 +171,14 @@ class FileDownloadCrawlTestCase(TestCase):\n                 self.mockserver.url(\"/files/images/\"),\n                 media_key=self.media_key,\n                 media_urls_key=self.media_urls_key,\n-                mockserver=self.mockserver)\n+                mockserver=self.mockserver,\n+            )\n         self._assert_files_download_failure(crawler, self.items, 302, str(log))\n \n     @defer.inlineCallbacks\n     def test_download_media_redirected_allowed(self):\n         settings = dict(self.settings)\n-        settings.update({'MEDIA_ALLOW_REDIRECTS': True})\n+        settings.update({\"MEDIA_ALLOW_REDIRECTS\": True})\n         runner = CrawlerRunner(settings)\n         crawler = self._create_crawler(RedirectedMediaDownloadSpider, runner=runner)\n         with LogCapture() as log:\n@@ -175,15 +186,20 @@ class FileDownloadCrawlTestCase(TestCase):\n                 self.mockserver.url(\"/files/images/\"),\n                 media_key=self.media_key,\n                 media_urls_key=self.media_urls_key,\n-                mockserver=self.mockserver)\n+                mockserver=self.mockserver,\n+            )\n         self._assert_files_downloaded(self.items, str(log))\n-        self.assertEqual(crawler.stats.get_value('downloader/response_status_count/302'), 3)\n+        self.assertEqual(\n+            crawler.stats.get_value(\"downloader/response_status_count/302\"), 3\n+        )\n \n \n try:\n     from PIL import Image  # noqa: imported just to check for the import error\n except ImportError:\n-    skip_pillow = 'Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow'\n+    skip_pillow = (\n+        \"Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow\"\n+    )\n else:\n     skip_pillow = None\n \n@@ -192,10 +208,10 @@ class ImageDownloadCrawlTestCase(FileDownloadCrawlTestCase):\n \n     skip = skip_pillow\n \n-    pipeline_class = 'scrapy.pipelines.images.ImagesPipeline'\n-    store_setting_key = 'IMAGES_STORE'\n-    media_key = 'images'\n-    media_urls_key = 'image_urls'\n+    pipeline_class = \"scrapy.pipelines.images.ImagesPipeline\"\n+    store_setting_key = \"IMAGES_STORE\"\n+    media_key = \"images\"\n+    media_urls_key = \"image_urls\"\n \n     # somehow checksums for images are different for Python 3.3\n     expected_checksums = None\n\n@@ -35,15 +35,14 @@ from scrapy.utils.test import (\n \n \n def _mocked_download_func(request, info):\n-    response = request.meta.get('response')\n+    response = request.meta.get(\"response\")\n     return response() if callable(response) else response\n \n \n class FilesPipelineTestCase(unittest.TestCase):\n-\n     def setUp(self):\n         self.tempdir = mkdtemp()\n-        settings_dict = {'FILES_STORE': self.tempdir}\n+        settings_dict = {\"FILES_STORE\": self.tempdir}\n         crawler = get_crawler(spidercls=None, settings_dict=settings_dict)\n         self.pipeline = FilesPipeline.from_crawler(crawler)\n         self.pipeline.download_func = _mocked_download_func\n@@ -56,41 +55,70 @@ class FilesPipelineTestCase(unittest.TestCase):\n         file_path = self.pipeline.file_path\n         self.assertEqual(\n             file_path(Request(\"https://dev.mydeco.com/mydeco.pdf\")),\n-            'full/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf')\n+            \"full/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf\",\n+        )\n         self.assertEqual(\n-            file_path(Request(\"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt\")),\n-            'full/4ce274dd83db0368bafd7e406f382ae088e39219.txt')\n+            file_path(\n+                Request(\n+                    \"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt\"\n+                )\n+            ),\n+            \"full/4ce274dd83db0368bafd7e406f382ae088e39219.txt\",\n+        )\n         self.assertEqual(\n-            file_path(Request(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc\")),\n-            'full/94ccc495a17b9ac5d40e3eabf3afcb8c2c9b9e1a.doc')\n+            file_path(\n+                Request(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc\")\n+            ),\n+            \"full/94ccc495a17b9ac5d40e3eabf3afcb8c2c9b9e1a.doc\",\n+        )\n         self.assertEqual(\n-            file_path(Request(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\")),\n-            'full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg')\n+            file_path(\n+                Request(\n+                    \"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\"\n+                )\n+            ),\n+            \"full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg\",\n+        )\n         self.assertEqual(\n             file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532/\")),\n-            'full/97ee6f8a46cbbb418ea91502fd24176865cf39b2')\n+            \"full/97ee6f8a46cbbb418ea91502fd24176865cf39b2\",\n+        )\n         self.assertEqual(\n             file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\")),\n-            'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1')\n+            \"full/244e0dd7d96a3b7b01f54eded250c9e272577aa1\",\n+        )\n         self.assertEqual(\n-            file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\"),\n+            file_path(\n+                Request(\"http://www.dorma.co.uk/images/product_details/2532\"),\n                 response=Response(\"http://www.dorma.co.uk/images/product_details/2532\"),\n-                      info=object()),\n-            'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1')\n+                info=object(),\n+            ),\n+            \"full/244e0dd7d96a3b7b01f54eded250c9e272577aa1\",\n+        )\n         self.assertEqual(\n-            file_path(Request(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg.bohaha\")),\n-            'full/76c00cef2ef669ae65052661f68d451162829507')\n+            file_path(\n+                Request(\n+                    \"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg.bohaha\"\n+                )\n+            ),\n+            \"full/76c00cef2ef669ae65052661f68d451162829507\",\n+        )\n         self.assertEqual(\n-            file_path(Request(\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAR0AAACxCAMAAADOHZloAAACClBMVEX/\\\n-                                    //+F0tzCwMK76ZKQ21AMqr7oAAC96JvD5aWM2kvZ78J0N7fmAAC46Y4Ap7y\")),\n-            'full/178059cbeba2e34120a67f2dc1afc3ecc09b61cb.png')\n+            file_path(\n+                Request(\n+                    \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAR0AAACxCAMAAADOHZloAAACClBMVEX/\\\n+                                    //+F0tzCwMK76ZKQ21AMqr7oAAC96JvD5aWM2kvZ78J0N7fmAAC46Y4Ap7y\"\n+                )\n+            ),\n+            \"full/178059cbeba2e34120a67f2dc1afc3ecc09b61cb.png\",\n+        )\n \n     def test_fs_store(self):\n         assert isinstance(self.pipeline.store, FSFilesStore)\n         self.assertEqual(self.pipeline.store.basedir, self.tempdir)\n \n-        path = 'some/image/key.jpg'\n-        fullpath = Path(self.tempdir, 'some', 'image', 'key.jpg')\n+        path = \"some/image/key.jpg\"\n+        fullpath = Path(self.tempdir, \"some\", \"image\", \"key.jpg\")\n         self.assertEqual(self.pipeline.store._get_filesystem_path(path), fullpath)\n \n     @defer.inlineCallbacks\n@@ -98,18 +126,24 @@ class FilesPipelineTestCase(unittest.TestCase):\n         item_url = \"http://example.com/file.pdf\"\n         item = _create_item_with_files(item_url)\n         patchers = [\n-            mock.patch.object(FilesPipeline, 'inc_stats', return_value=True),\n-            mock.patch.object(FSFilesStore, 'stat_file', return_value={\n-                'checksum': 'abc', 'last_modified': time.time()}),\n-            mock.patch.object(FilesPipeline, 'get_media_requests',\n-                              return_value=[_prepare_request_object(item_url)])\n+            mock.patch.object(FilesPipeline, \"inc_stats\", return_value=True),\n+            mock.patch.object(\n+                FSFilesStore,\n+                \"stat_file\",\n+                return_value={\"checksum\": \"abc\", \"last_modified\": time.time()},\n+            ),\n+            mock.patch.object(\n+                FilesPipeline,\n+                \"get_media_requests\",\n+                return_value=[_prepare_request_object(item_url)],\n+            ),\n         ]\n         for p in patchers:\n             p.start()\n \n         result = yield self.pipeline.process_item(item, None)\n-        self.assertEqual(result['files'][0]['checksum'], 'abc')\n-        self.assertEqual(result['files'][0]['status'], 'uptodate')\n+        self.assertEqual(result[\"files\"][0][\"checksum\"], \"abc\")\n+        self.assertEqual(result[\"files\"][0][\"status\"], \"uptodate\")\n \n         for p in patchers:\n             p.stop()\n@@ -119,19 +153,28 @@ class FilesPipelineTestCase(unittest.TestCase):\n         item_url = \"http://example.com/file2.pdf\"\n         item = _create_item_with_files(item_url)\n         patchers = [\n-            mock.patch.object(FSFilesStore, 'stat_file', return_value={\n-                'checksum': 'abc',\n-                'last_modified': time.time() - (self.pipeline.expires * 60 * 60 * 24 * 2)}),\n-            mock.patch.object(FilesPipeline, 'get_media_requests',\n-                              return_value=[_prepare_request_object(item_url)]),\n-            mock.patch.object(FilesPipeline, 'inc_stats', return_value=True)\n+            mock.patch.object(\n+                FSFilesStore,\n+                \"stat_file\",\n+                return_value={\n+                    \"checksum\": \"abc\",\n+                    \"last_modified\": time.time()\n+                    - (self.pipeline.expires * 60 * 60 * 24 * 2),\n+                },\n+            ),\n+            mock.patch.object(\n+                FilesPipeline,\n+                \"get_media_requests\",\n+                return_value=[_prepare_request_object(item_url)],\n+            ),\n+            mock.patch.object(FilesPipeline, \"inc_stats\", return_value=True),\n         ]\n         for p in patchers:\n             p.start()\n \n         result = yield self.pipeline.process_item(item, None)\n-        self.assertNotEqual(result['files'][0]['checksum'], 'abc')\n-        self.assertEqual(result['files'][0]['status'], 'downloaded')\n+        self.assertNotEqual(result[\"files\"][0][\"checksum\"], \"abc\")\n+        self.assertEqual(result[\"files\"][0][\"status\"], \"downloaded\")\n \n         for p in patchers:\n             p.stop()\n@@ -141,19 +184,28 @@ class FilesPipelineTestCase(unittest.TestCase):\n         item_url = \"http://example.com/file3.pdf\"\n         item = _create_item_with_files(item_url)\n         patchers = [\n-            mock.patch.object(FilesPipeline, 'inc_stats', return_value=True),\n-            mock.patch.object(FSFilesStore, 'stat_file', return_value={\n-                'checksum': 'abc',\n-                'last_modified': time.time() - (self.pipeline.expires * 60 * 60 * 24 * 2)}),\n-            mock.patch.object(FilesPipeline, 'get_media_requests',\n-                              return_value=[_prepare_request_object(item_url, flags=['cached'])])\n+            mock.patch.object(FilesPipeline, \"inc_stats\", return_value=True),\n+            mock.patch.object(\n+                FSFilesStore,\n+                \"stat_file\",\n+                return_value={\n+                    \"checksum\": \"abc\",\n+                    \"last_modified\": time.time()\n+                    - (self.pipeline.expires * 60 * 60 * 24 * 2),\n+                },\n+            ),\n+            mock.patch.object(\n+                FilesPipeline,\n+                \"get_media_requests\",\n+                return_value=[_prepare_request_object(item_url, flags=[\"cached\"])],\n+            ),\n         ]\n         for p in patchers:\n             p.start()\n \n         result = yield self.pipeline.process_item(item, None)\n-        self.assertNotEqual(result['files'][0]['checksum'], 'abc')\n-        self.assertEqual(result['files'][0]['status'], 'cached')\n+        self.assertNotEqual(result[\"files\"][0][\"checksum\"], \"abc\")\n+        self.assertEqual(result[\"files\"][0][\"status\"], \"cached\")\n \n         for p in patchers:\n             p.stop()\n@@ -162,48 +214,58 @@ class FilesPipelineTestCase(unittest.TestCase):\n         \"\"\"\n         Custom file path based on item data, overriding default implementation\n         \"\"\"\n+\n         class CustomFilesPipeline(FilesPipeline):\n             def file_path(self, request, response=None, info=None, item=None):\n                 return f'full/{item.get(\"path\")}'\n \n-        file_path = CustomFilesPipeline.from_settings(Settings({'FILES_STORE': self.tempdir})).file_path\n-        item = dict(path='path-to-store-file')\n+        file_path = CustomFilesPipeline.from_settings(\n+            Settings({\"FILES_STORE\": self.tempdir})\n+        ).file_path\n+        item = dict(path=\"path-to-store-file\")\n         request = Request(\"http://example.com\")\n-        self.assertEqual(file_path(request, item=item), 'full/path-to-store-file')\n+        self.assertEqual(file_path(request, item=item), \"full/path-to-store-file\")\n \n \n class FilesPipelineTestCaseFieldsMixin:\n-\n     def test_item_fields_default(self):\n-        url = 'http://www.example.com/files/1.txt'\n-        item = self.item_class(name='item1', file_urls=[url])\n-        pipeline = FilesPipeline.from_settings(Settings({'FILES_STORE': 's3://example/files/'}))\n+        url = \"http://www.example.com/files/1.txt\"\n+        item = self.item_class(name=\"item1\", file_urls=[url])\n+        pipeline = FilesPipeline.from_settings(\n+            Settings({\"FILES_STORE\": \"s3://example/files/\"})\n+        )\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n-        results = [(True, {'url': url})]\n+        results = [(True, {\"url\": url})]\n         item = pipeline.item_completed(results, item, None)\n         files = ItemAdapter(item).get(\"files\")\n         self.assertEqual(files, [results[0][1]])\n         self.assertIsInstance(item, self.item_class)\n \n     def test_item_fields_override_settings(self):\n-        url = 'http://www.example.com/files/1.txt'\n-        item = self.item_class(name='item1', custom_file_urls=[url])\n-        pipeline = FilesPipeline.from_settings(Settings({\n-            'FILES_STORE': 's3://example/files/',\n-            'FILES_URLS_FIELD': 'custom_file_urls',\n-            'FILES_RESULT_FIELD': 'custom_files'\n-        }))\n+        url = \"http://www.example.com/files/1.txt\"\n+        item = self.item_class(name=\"item1\", custom_file_urls=[url])\n+        pipeline = FilesPipeline.from_settings(\n+            Settings(\n+                {\n+                    \"FILES_STORE\": \"s3://example/files/\",\n+                    \"FILES_URLS_FIELD\": \"custom_file_urls\",\n+                    \"FILES_RESULT_FIELD\": \"custom_files\",\n+                }\n+            )\n+        )\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n-        results = [(True, {'url': url})]\n+        results = [(True, {\"url\": url})]\n         item = pipeline.item_completed(results, item, None)\n         custom_files = ItemAdapter(item).get(\"custom_files\")\n         self.assertEqual(custom_files, [results[0][1]])\n         self.assertIsInstance(item, self.item_class)\n \n \n-class FilesPipelineTestCaseFieldsDict(FilesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+class FilesPipelineTestCaseFieldsDict(\n+    FilesPipelineTestCaseFieldsMixin, unittest.TestCase\n+):\n     item_class = dict\n \n \n@@ -217,7 +279,9 @@ class FilesPipelineTestItem(Item):\n     custom_files = Field()\n \n \n-class FilesPipelineTestCaseFieldsItem(FilesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+class FilesPipelineTestCaseFieldsItem(\n+    FilesPipelineTestCaseFieldsMixin, unittest.TestCase\n+):\n     item_class = FilesPipelineTestItem\n \n \n@@ -232,7 +296,9 @@ class FilesPipelineTestDataClass:\n     custom_files: list = dataclasses.field(default_factory=list)\n \n \n-class FilesPipelineTestCaseFieldsDataClass(FilesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+class FilesPipelineTestCaseFieldsDataClass(\n+    FilesPipelineTestCaseFieldsMixin, unittest.TestCase\n+):\n     item_class = FilesPipelineTestDataClass\n \n \n@@ -247,7 +313,9 @@ class FilesPipelineTestAttrsItem:\n     custom_files = attr.ib(default=lambda: [])\n \n \n-class FilesPipelineTestCaseFieldsAttrsItem(FilesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+class FilesPipelineTestCaseFieldsAttrsItem(\n+    FilesPipelineTestCaseFieldsMixin, unittest.TestCase\n+):\n     item_class = FilesPipelineTestAttrsItem\n \n \n@@ -255,12 +323,12 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n     default_cls_settings = {\n         \"EXPIRES\": 90,\n         \"FILES_URLS_FIELD\": \"file_urls\",\n-        \"FILES_RESULT_FIELD\": \"files\"\n+        \"FILES_RESULT_FIELD\": \"files\",\n     }\n     file_cls_attr_settings_map = {\n         (\"EXPIRES\", \"FILES_EXPIRES\", \"expires\"),\n         (\"FILES_URLS_FIELD\", \"FILES_URLS_FIELD\", \"files_urls_field\"),\n-        (\"FILES_RESULT_FIELD\", \"FILES_RESULT_FIELD\", \"files_result_field\")\n+        (\"FILES_RESULT_FIELD\", \"FILES_RESULT_FIELD\", \"files_result_field\"),\n     }\n \n     def setUp(self):\n@@ -270,7 +338,6 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n         rmtree(self.tempdir)\n \n     def _generate_fake_settings(self, prefix=None):\n-\n         def random_string():\n             return \"\".join([chr(random.randint(97, 123)) for _ in range(10)])\n \n@@ -278,15 +345,17 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n             \"FILES_EXPIRES\": random.randint(100, 1000),\n             \"FILES_URLS_FIELD\": random_string(),\n             \"FILES_RESULT_FIELD\": random_string(),\n-            \"FILES_STORE\": self.tempdir\n+            \"FILES_STORE\": self.tempdir,\n         }\n         if not prefix:\n             return settings\n \n-        return {prefix.upper() + \"_\" + k if k != \"FILES_STORE\" else k: v for k, v in settings.items()}\n+        return {\n+            prefix.upper() + \"_\" + k if k != \"FILES_STORE\" else k: v\n+            for k, v in settings.items()\n+        }\n \n     def _generate_fake_pipeline(self):\n-\n         class UserDefinedFilePipeline(FilesPipeline):\n             EXPIRES = 1001\n             FILES_URLS_FIELD = \"alfa\"\n@@ -339,10 +408,13 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n         If there are no settings for subclass and no subclass attributes, pipeline should use\n         attributes of base class.\n         \"\"\"\n+\n         class UserDefinedFilesPipeline(FilesPipeline):\n             pass\n \n-        user_pipeline = UserDefinedFilesPipeline.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n+        user_pipeline = UserDefinedFilesPipeline.from_settings(\n+            Settings({\"FILES_STORE\": self.tempdir})\n+        )\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n             custom_value = self.default_cls_settings.get(pipe_attr.upper())\n@@ -353,6 +425,7 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n         If there are custom settings for subclass and NO class attributes, pipeline should use custom\n         settings.\n         \"\"\"\n+\n         class UserDefinedFilesPipeline(FilesPipeline):\n             pass\n \n@@ -374,7 +447,11 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n         prefix = pipeline_cls.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n         user_pipeline = pipeline_cls.from_settings(Settings(settings))\n-        for pipe_cls_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n+        for (\n+            pipe_cls_attr,\n+            settings_attr,\n+            pipe_inst_attr,\n+        ) in self.file_cls_attr_settings_map:\n             custom_value = settings.get(prefix + \"_\" + settings_attr)\n             self.assertNotEqual(custom_value, self.default_cls_settings[pipe_cls_attr])\n             self.assertEqual(getattr(user_pipeline, pipe_inst_attr), custom_value)\n@@ -384,7 +461,9 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n             DEFAULT_FILES_RESULT_FIELD = \"this\"\n             DEFAULT_FILES_URLS_FIELD = \"that\"\n \n-        pipeline = UserDefinedFilesPipeline.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n+        pipeline = UserDefinedFilesPipeline.from_settings(\n+            Settings({\"FILES_STORE\": self.tempdir})\n+        )\n         self.assertEqual(pipeline.files_result_field, \"this\")\n         self.assertEqual(pipeline.files_urls_field, \"that\")\n \n@@ -402,37 +481,36 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n \n         for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n             expected_value = settings.get(settings_attr)\n-            self.assertEqual(getattr(pipeline_cls, pipe_inst_attr),\n-                             expected_value)\n+            self.assertEqual(getattr(pipeline_cls, pipe_inst_attr), expected_value)\n \n \n class TestS3FilesStore(unittest.TestCase):\n-\n     @defer.inlineCallbacks\n     def test_persist(self):\n         skip_if_no_boto()\n \n-        bucket = 'mybucket'\n-        key = 'export.csv'\n-        uri = f's3://{bucket}/{key}'\n+        bucket = \"mybucket\"\n+        key = \"export.csv\"\n+        uri = f\"s3://{bucket}/{key}\"\n         buffer = mock.MagicMock()\n-        meta = {'foo': 'bar'}\n-        path = ''\n-        content_type = 'image/png'\n+        meta = {\"foo\": \"bar\"}\n+        path = \"\"\n+        content_type = \"image/png\"\n \n         store = S3FilesStore(uri)\n         from botocore.stub import Stubber\n+\n         with Stubber(store.s3_client) as stub:\n             stub.add_response(\n-                'put_object',\n+                \"put_object\",\n                 expected_params={\n-                    'ACL': S3FilesStore.POLICY,\n-                    'Body': buffer,\n-                    'Bucket': bucket,\n-                    'CacheControl': S3FilesStore.HEADERS['Cache-Control'],\n-                    'ContentType': content_type,\n-                    'Key': key,\n-                    'Metadata': meta,\n+                    \"ACL\": S3FilesStore.POLICY,\n+                    \"Body\": buffer,\n+                    \"Bucket\": bucket,\n+                    \"CacheControl\": S3FilesStore.HEADERS[\"Cache-Control\"],\n+                    \"ContentType\": content_type,\n+                    \"Key\": key,\n+                    \"Metadata\": meta,\n                 },\n                 service_response={},\n             )\n@@ -442,7 +520,7 @@ class TestS3FilesStore(unittest.TestCase):\n                 buffer,\n                 info=None,\n                 meta=meta,\n-                headers={'Content-Type': content_type},\n+                headers={\"Content-Type\": content_type},\n             )\n \n             stub.assert_no_pending_responses()\n@@ -451,40 +529,41 @@ class TestS3FilesStore(unittest.TestCase):\n                 [\n                     mock.call.seek(0),\n                     # The call to read does not happen with Stubber\n-                ]\n+                ],\n             )\n \n     @defer.inlineCallbacks\n     def test_stat(self):\n         skip_if_no_boto()\n \n-        bucket = 'mybucket'\n-        key = 'export.csv'\n-        uri = f's3://{bucket}/{key}'\n-        checksum = '3187896a9657a28163abb31667df64c8'\n+        bucket = \"mybucket\"\n+        key = \"export.csv\"\n+        uri = f\"s3://{bucket}/{key}\"\n+        checksum = \"3187896a9657a28163abb31667df64c8\"\n         last_modified = datetime(2019, 12, 1)\n \n         store = S3FilesStore(uri)\n         from botocore.stub import Stubber\n+\n         with Stubber(store.s3_client) as stub:\n             stub.add_response(\n-                'head_object',\n+                \"head_object\",\n                 expected_params={\n-                    'Bucket': bucket,\n-                    'Key': key,\n+                    \"Bucket\": bucket,\n+                    \"Key\": key,\n                 },\n                 service_response={\n-                    'ETag': f'\"{checksum}\"',\n-                    'LastModified': last_modified,\n+                    \"ETag\": f'\"{checksum}\"',\n+                    \"LastModified\": last_modified,\n                 },\n             )\n \n-            file_stats = yield store.stat_file('', info=None)\n+            file_stats = yield store.stat_file(\"\", info=None)\n             self.assertEqual(\n                 file_stats,\n                 {\n-                    'checksum': checksum,\n-                    'last_modified': last_modified.timestamp(),\n+                    \"checksum\": checksum,\n+                    \"last_modified\": last_modified.timestamp(),\n                 },\n             )\n \n@@ -495,27 +574,27 @@ class TestGCSFilesStore(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_persist(self):\n         assert_gcs_environ()\n-        uri = os.environ.get('GCS_TEST_FILE_URI')\n+        uri = os.environ.get(\"GCS_TEST_FILE_URI\")\n         if not uri:\n             raise unittest.SkipTest(\"No GCS URI available for testing\")\n         data = b\"TestGCSFilesStore: \\xe2\\x98\\x83\"\n         buf = BytesIO(data)\n-        meta = {'foo': 'bar'}\n-        path = 'full/filename'\n+        meta = {\"foo\": \"bar\"}\n+        path = \"full/filename\"\n         store = GCSFilesStore(uri)\n-        store.POLICY = 'authenticatedRead'\n-        expected_policy = {'role': 'READER', 'entity': 'allAuthenticatedUsers'}\n+        store.POLICY = \"authenticatedRead\"\n+        expected_policy = {\"role\": \"READER\", \"entity\": \"allAuthenticatedUsers\"}\n         yield store.persist_file(path, buf, info=None, meta=meta, headers=None)\n         s = yield store.stat_file(path, info=None)\n-        self.assertIn('last_modified', s)\n-        self.assertIn('checksum', s)\n-        self.assertEqual(s['checksum'], 'zc2oVgXkbQr2EQdSdw3OPA==')\n+        self.assertIn(\"last_modified\", s)\n+        self.assertIn(\"checksum\", s)\n+        self.assertEqual(s[\"checksum\"], \"zc2oVgXkbQr2EQdSdw3OPA==\")\n         u = urlparse(uri)\n         content, acl, blob = get_gcs_content_and_delete(u.hostname, u.path[1:] + path)\n         self.assertEqual(content, data)\n-        self.assertEqual(blob.metadata, {'foo': 'bar'})\n+        self.assertEqual(blob.metadata, {\"foo\": \"bar\"})\n         self.assertEqual(blob.cache_control, GCSFilesStore.CACHE_CONTROL)\n-        self.assertEqual(blob.content_type, 'application/octet-stream')\n+        self.assertEqual(blob.content_type, \"application/octet-stream\")\n         self.assertIn(expected_policy, acl)\n \n     @defer.inlineCallbacks\n@@ -529,13 +608,15 @@ class TestGCSFilesStore(unittest.TestCase):\n         except ModuleNotFoundError:\n             raise unittest.SkipTest(\"google-cloud-storage is not installed\")\n         else:\n-            with mock.patch('google.cloud.storage') as _:\n-                with mock.patch('scrapy.pipelines.files.time') as _:\n-                    uri = 'gs://my_bucket/my_prefix/'\n+            with mock.patch(\"google.cloud.storage\") as _:\n+                with mock.patch(\"scrapy.pipelines.files.time\") as _:\n+                    uri = \"gs://my_bucket/my_prefix/\"\n                     store = GCSFilesStore(uri)\n                     store.bucket = mock.Mock()\n-                    path = 'full/my_data.txt'\n-                    yield store.persist_file(path, mock.Mock(), info=None, meta=None, headers=None)\n+                    path = \"full/my_data.txt\"\n+                    yield store.persist_file(\n+                        path, mock.Mock(), info=None, meta=None, headers=None\n+                    )\n                     yield store.stat_file(path, info=None)\n                     expected_blob_path = store.prefix + path\n                     store.bucket.blob.assert_called_with(expected_blob_path)\n@@ -545,25 +626,30 @@ class TestGCSFilesStore(unittest.TestCase):\n class TestFTPFileStore(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_persist(self):\n-        uri = os.environ.get('FTP_TEST_FILE_URI')\n+        uri = os.environ.get(\"FTP_TEST_FILE_URI\")\n         if not uri:\n             raise unittest.SkipTest(\"No FTP URI available for testing\")\n         data = b\"TestFTPFilesStore: \\xe2\\x98\\x83\"\n         buf = BytesIO(data)\n-        meta = {'foo': 'bar'}\n-        path = 'full/filename'\n+        meta = {\"foo\": \"bar\"}\n+        path = \"full/filename\"\n         store = FTPFilesStore(uri)\n         empty_dict = yield store.stat_file(path, info=None)\n         self.assertEqual(empty_dict, {})\n         yield store.persist_file(path, buf, info=None, meta=meta, headers=None)\n         stat = yield store.stat_file(path, info=None)\n-        self.assertIn('last_modified', stat)\n-        self.assertIn('checksum', stat)\n-        self.assertEqual(stat['checksum'], 'd113d66b2ec7258724a268bd88eef6b6')\n-        path = f'{store.basedir}/{path}'\n+        self.assertIn(\"last_modified\", stat)\n+        self.assertIn(\"checksum\", stat)\n+        self.assertEqual(stat[\"checksum\"], \"d113d66b2ec7258724a268bd88eef6b6\")\n+        path = f\"{store.basedir}/{path}\"\n         content = get_ftp_content_and_delete(\n-            path, store.host, store.port,\n-            store.username, store.password, store.USE_ACTIVE_MODE)\n+            path,\n+            store.host,\n+            store.port,\n+            store.username,\n+            store.password,\n+            store.USE_ACTIVE_MODE,\n+        )\n         self.assertEqual(data.decode(), content)\n \n \n@@ -574,14 +660,15 @@ class ItemWithFiles(Item):\n \n def _create_item_with_files(*files):\n     item = ItemWithFiles()\n-    item['file_urls'] = files\n+    item[\"file_urls\"] = files\n     return item\n \n \n def _prepare_request_object(item_url, flags=None):\n     return Request(\n         item_url,\n-        meta={'response': Response(item_url, status=200, body=b'data', flags=flags)})\n+        meta={\"response\": Response(item_url, status=200, body=b\"data\", flags=flags)},\n+    )\n \n \n if __name__ == \"__main__\":\n\n@@ -22,17 +22,19 @@ from scrapy.utils.python import to_bytes\n try:\n     from PIL import Image\n except ImportError:\n-    skip_pillow = 'Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow'\n+    skip_pillow = (\n+        \"Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow\"\n+    )\n else:\n-    encoders = {'jpeg_encoder', 'jpeg_decoder'}\n+    encoders = {\"jpeg_encoder\", \"jpeg_decoder\"}\n     if not encoders.issubset(set(Image.core.__dict__)):\n-        skip_pillow = 'Missing JPEG encoders'\n+        skip_pillow = \"Missing JPEG encoders\"\n     else:\n         skip_pillow = None\n \n \n def _mocked_download_func(request, info):\n-    response = request.meta.get('response')\n+    response = request.meta.get(\"response\")\n     return response() if callable(response) else response\n \n \n@@ -42,7 +44,9 @@ class ImagesPipelineTestCase(unittest.TestCase):\n \n     def setUp(self):\n         self.tempdir = mkdtemp()\n-        self.pipeline = ImagesPipeline(self.tempdir, download_func=_mocked_download_func)\n+        self.pipeline = ImagesPipeline(\n+            self.tempdir, download_func=_mocked_download_func\n+        )\n \n     def tearDown(self):\n         rmtree(self.tempdir)\n@@ -51,43 +55,75 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         file_path = self.pipeline.file_path\n         self.assertEqual(\n             file_path(Request(\"https://dev.mydeco.com/mydeco.gif\")),\n-            'full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n+            \"full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg\",\n+        )\n         self.assertEqual(\n-            file_path(Request(\"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.jpg\")),\n-            'full/0ffcd85d563bca45e2f90becd0ca737bc58a00b2.jpg')\n+            file_path(\n+                Request(\n+                    \"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.jpg\"\n+                )\n+            ),\n+            \"full/0ffcd85d563bca45e2f90becd0ca737bc58a00b2.jpg\",\n+        )\n         self.assertEqual(\n-            file_path(Request(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.gif\")),\n-            'full/b250e3a74fff2e4703e310048a5b13eba79379d2.jpg')\n+            file_path(\n+                Request(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.gif\")\n+            ),\n+            \"full/b250e3a74fff2e4703e310048a5b13eba79379d2.jpg\",\n+        )\n         self.assertEqual(\n-            file_path(Request(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\")),\n-            'full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg')\n+            file_path(\n+                Request(\n+                    \"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\"\n+                )\n+            ),\n+            \"full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg\",\n+        )\n         self.assertEqual(\n             file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532/\")),\n-            'full/97ee6f8a46cbbb418ea91502fd24176865cf39b2.jpg')\n+            \"full/97ee6f8a46cbbb418ea91502fd24176865cf39b2.jpg\",\n+        )\n         self.assertEqual(\n             file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\")),\n-            'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1.jpg')\n+            \"full/244e0dd7d96a3b7b01f54eded250c9e272577aa1.jpg\",\n+        )\n         self.assertEqual(\n-            file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\"),\n+            file_path(\n+                Request(\"http://www.dorma.co.uk/images/product_details/2532\"),\n                 response=Response(\"http://www.dorma.co.uk/images/product_details/2532\"),\n-                      info=object()),\n-            'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1.jpg')\n+                info=object(),\n+            ),\n+            \"full/244e0dd7d96a3b7b01f54eded250c9e272577aa1.jpg\",\n+        )\n \n     def test_thumbnail_name(self):\n         thumb_path = self.pipeline.thumb_path\n-        name = '50'\n-        self.assertEqual(thumb_path(Request(\"file:///tmp/foo.jpg\"), name),\n-                         'thumbs/50/38a86208c36e59d4404db9e37ce04be863ef0335.jpg')\n-        self.assertEqual(thumb_path(Request(\"file://foo.png\"), name),\n-                         'thumbs/50/e55b765eba0ec7348e50a1df496040449071b96a.jpg')\n-        self.assertEqual(thumb_path(Request(\"file:///tmp/foo\"), name),\n-                         'thumbs/50/0329ad83ebb8e93ea7c7906d46e9ed55f7349a50.jpg')\n-        self.assertEqual(thumb_path(Request(\"file:///tmp/some.name/foo\"), name),\n-                         'thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg')\n-        self.assertEqual(thumb_path(Request(\"file:///tmp/some.name/foo\"), name,\n+        name = \"50\"\n+        self.assertEqual(\n+            thumb_path(Request(\"file:///tmp/foo.jpg\"), name),\n+            \"thumbs/50/38a86208c36e59d4404db9e37ce04be863ef0335.jpg\",\n+        )\n+        self.assertEqual(\n+            thumb_path(Request(\"file://foo.png\"), name),\n+            \"thumbs/50/e55b765eba0ec7348e50a1df496040449071b96a.jpg\",\n+        )\n+        self.assertEqual(\n+            thumb_path(Request(\"file:///tmp/foo\"), name),\n+            \"thumbs/50/0329ad83ebb8e93ea7c7906d46e9ed55f7349a50.jpg\",\n+        )\n+        self.assertEqual(\n+            thumb_path(Request(\"file:///tmp/some.name/foo\"), name),\n+            \"thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg\",\n+        )\n+        self.assertEqual(\n+            thumb_path(\n+                Request(\"file:///tmp/some.name/foo\"),\n+                name,\n                 response=Response(\"file:///tmp/some.name/foo\"),\n-                                    info=object()),\n-                         'thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg')\n+                info=object(),\n+            ),\n+            \"thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg\",\n+        )\n \n     def test_thumbnail_name_from_item(self):\n         \"\"\"\n@@ -95,23 +131,27 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         \"\"\"\n \n         class CustomImagesPipeline(ImagesPipeline):\n-            def thumb_path(self, request, thumb_id, response=None, info=None, item=None):\n+            def thumb_path(\n+                self, request, thumb_id, response=None, info=None, item=None\n+            ):\n                 return f\"thumb/{thumb_id}/{item.get('path')}\"\n \n-        thumb_path = CustomImagesPipeline.from_settings(Settings(\n-            {'IMAGES_STORE': self.tempdir}\n-        )).thumb_path\n-        item = dict(path='path-to-store-file')\n+        thumb_path = CustomImagesPipeline.from_settings(\n+            Settings({\"IMAGES_STORE\": self.tempdir})\n+        ).thumb_path\n+        item = dict(path=\"path-to-store-file\")\n         request = Request(\"http://example.com\")\n-        self.assertEqual(thumb_path(request, 'small', item=item), 'thumb/small/path-to-store-file')\n+        self.assertEqual(\n+            thumb_path(request, \"small\", item=item), \"thumb/small/path-to-store-file\"\n+        )\n \n     def test_get_images_exception(self):\n         self.pipeline.min_width = 100\n         self.pipeline.min_height = 100\n \n-        _, buf1 = _create_image('JPEG', 'RGB', (50, 50), (0, 0, 0))\n-        _, buf2 = _create_image('JPEG', 'RGB', (150, 50), (0, 0, 0))\n-        _, buf3 = _create_image('JPEG', 'RGB', (50, 150), (0, 0, 0))\n+        _, buf1 = _create_image(\"JPEG\", \"RGB\", (50, 50), (0, 0, 0))\n+        _, buf2 = _create_image(\"JPEG\", \"RGB\", (150, 50), (0, 0, 0))\n+        _, buf3 = _create_image(\"JPEG\", \"RGB\", (50, 150), (0, 0, 0))\n \n         resp1 = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf1.getvalue())\n         resp2 = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf2.getvalue())\n@@ -128,121 +168,154 @@ class ImagesPipelineTestCase(unittest.TestCase):\n     def test_get_images_new(self):\n         self.pipeline.min_width = 0\n         self.pipeline.min_height = 0\n-        self.pipeline.thumbs = {'small': (20, 20)}\n+        self.pipeline.thumbs = {\"small\": (20, 20)}\n \n-        orig_im, buf = _create_image('JPEG', 'RGB', (50, 50), (0, 0, 0))\n-        orig_thumb, orig_thumb_buf = _create_image('JPEG', 'RGB', (20, 20), (0, 0, 0))\n+        orig_im, buf = _create_image(\"JPEG\", \"RGB\", (50, 50), (0, 0, 0))\n+        orig_thumb, orig_thumb_buf = _create_image(\"JPEG\", \"RGB\", (20, 20), (0, 0, 0))\n         resp = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf.getvalue())\n         req = Request(url=\"https://dev.mydeco.com/mydeco.gif\")\n \n-        get_images_gen = self.pipeline.get_images(response=resp, request=req, info=object())\n+        get_images_gen = self.pipeline.get_images(\n+            response=resp, request=req, info=object()\n+        )\n \n         path, new_im, new_buf = next(get_images_gen)\n-        self.assertEqual(path, 'full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n+        self.assertEqual(path, \"full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg\")\n         self.assertEqual(orig_im, new_im)\n         self.assertEqual(buf.getvalue(), new_buf.getvalue())\n \n         thumb_path, thumb_img, thumb_buf = next(get_images_gen)\n-        self.assertEqual(thumb_path, 'thumbs/small/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n+        self.assertEqual(\n+            thumb_path, \"thumbs/small/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg\"\n+        )\n         self.assertEqual(thumb_img, thumb_img)\n         self.assertEqual(orig_thumb_buf.getvalue(), thumb_buf.getvalue())\n \n     def test_get_images_old(self):\n-        self.pipeline.thumbs = {'small': (20, 20)}\n-        orig_im, buf = _create_image('JPEG', 'RGB', (50, 50), (0, 0, 0))\n+        self.pipeline.thumbs = {\"small\": (20, 20)}\n+        orig_im, buf = _create_image(\"JPEG\", \"RGB\", (50, 50), (0, 0, 0))\n         resp = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf.getvalue())\n         req = Request(url=\"https://dev.mydeco.com/mydeco.gif\")\n \n         def overridden_convert_image(image, size=None):\n-            im, buf = _create_image('JPEG', 'RGB', (50, 50), (0, 0, 0))\n+            im, buf = _create_image(\"JPEG\", \"RGB\", (50, 50), (0, 0, 0))\n             return im, buf\n \n-        with patch.object(self.pipeline, 'convert_image', overridden_convert_image):\n+        with patch.object(self.pipeline, \"convert_image\", overridden_convert_image):\n             with warnings.catch_warnings(record=True) as w:\n-                warnings.simplefilter('always')\n-                get_images_gen = self.pipeline.get_images(response=resp, request=req, info=object())\n+                warnings.simplefilter(\"always\")\n+                get_images_gen = self.pipeline.get_images(\n+                    response=resp, request=req, info=object()\n+                )\n                 path, new_im, new_buf = next(get_images_gen)\n-                self.assertEqual(path, 'full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n+                self.assertEqual(\n+                    path, \"full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg\"\n+                )\n                 self.assertEqual(orig_im.mode, new_im.mode)\n                 self.assertEqual(orig_im.getcolors(), new_im.getcolors())\n                 self.assertEqual(buf.getvalue(), new_buf.getvalue())\n \n                 thumb_path, thumb_img, thumb_buf = next(get_images_gen)\n-                self.assertEqual(thumb_path, 'thumbs/small/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n+                self.assertEqual(\n+                    thumb_path,\n+                    \"thumbs/small/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg\",\n+                )\n                 self.assertEqual(orig_im.mode, thumb_img.mode)\n                 self.assertEqual(orig_im.getcolors(), thumb_img.getcolors())\n                 self.assertEqual(buf.getvalue(), thumb_buf.getvalue())\n \n-                expected_warning_msg = ('.convert_image() method overriden in a deprecated way, '\n-                                        'overriden method does not accept response_body argument.')\n-                self.assertEqual(len([warning for warning in w if expected_warning_msg in str(warning.message)]), 1)\n+                expected_warning_msg = (\n+                    \".convert_image() method overriden in a deprecated way, \"\n+                    \"overriden method does not accept response_body argument.\"\n+                )\n+                self.assertEqual(\n+                    len(\n+                        [\n+                            warning\n+                            for warning in w\n+                            if expected_warning_msg in str(warning.message)\n+                        ]\n+                    ),\n+                    1,\n+                )\n \n     def test_convert_image_old(self):\n         # tests for old API\n         with warnings.catch_warnings(record=True) as w:\n-            warnings.simplefilter('always')\n+            warnings.simplefilter(\"always\")\n             SIZE = (100, 100)\n             # straigh forward case: RGB and JPEG\n             COLOUR = (0, 127, 255)\n-            im, _ = _create_image('JPEG', 'RGB', SIZE, COLOUR)\n+            im, _ = _create_image(\"JPEG\", \"RGB\", SIZE, COLOUR)\n             converted, _ = self.pipeline.convert_image(im)\n-            self.assertEqual(converted.mode, 'RGB')\n+            self.assertEqual(converted.mode, \"RGB\")\n             self.assertEqual(converted.getcolors(), [(10000, COLOUR)])\n \n             # check that thumbnail keep image ratio\n             thumbnail, _ = self.pipeline.convert_image(converted, size=(10, 25))\n-            self.assertEqual(thumbnail.mode, 'RGB')\n+            self.assertEqual(thumbnail.mode, \"RGB\")\n             self.assertEqual(thumbnail.size, (10, 10))\n \n             # transparency case: RGBA and PNG\n             COLOUR = (0, 127, 255, 50)\n-            im, _ = _create_image('PNG', 'RGBA', SIZE, COLOUR)\n+            im, _ = _create_image(\"PNG\", \"RGBA\", SIZE, COLOUR)\n             converted, _ = self.pipeline.convert_image(im)\n-            self.assertEqual(converted.mode, 'RGB')\n+            self.assertEqual(converted.mode, \"RGB\")\n             self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n \n             # transparency case with palette: P and PNG\n             COLOUR = (0, 127, 255, 50)\n-            im, _ = _create_image('PNG', 'RGBA', SIZE, COLOUR)\n-            im = im.convert('P')\n+            im, _ = _create_image(\"PNG\", \"RGBA\", SIZE, COLOUR)\n+            im = im.convert(\"P\")\n             converted, _ = self.pipeline.convert_image(im)\n-            self.assertEqual(converted.mode, 'RGB')\n+            self.assertEqual(converted.mode, \"RGB\")\n             self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n \n             # ensure that we recieved deprecation warnings\n-            expected_warning_msg = '.convert_image() method called in a deprecated way'\n-            self.assertTrue(len([warning for warning in w if expected_warning_msg in str(warning.message)]) == 4)\n+            expected_warning_msg = \".convert_image() method called in a deprecated way\"\n+            self.assertTrue(\n+                len(\n+                    [\n+                        warning\n+                        for warning in w\n+                        if expected_warning_msg in str(warning.message)\n+                    ]\n+                )\n+                == 4\n+            )\n \n     def test_convert_image_new(self):\n         # tests for new API\n         SIZE = (100, 100)\n         # straigh forward case: RGB and JPEG\n         COLOUR = (0, 127, 255)\n-        im, buf = _create_image('JPEG', 'RGB', SIZE, COLOUR)\n+        im, buf = _create_image(\"JPEG\", \"RGB\", SIZE, COLOUR)\n         converted, converted_buf = self.pipeline.convert_image(im, response_body=buf)\n-        self.assertEqual(converted.mode, 'RGB')\n+        self.assertEqual(converted.mode, \"RGB\")\n         self.assertEqual(converted.getcolors(), [(10000, COLOUR)])\n         # check that we don't convert JPEGs again\n         self.assertEqual(converted_buf, buf)\n \n         # check that thumbnail keep image ratio\n-        thumbnail, _ = self.pipeline.convert_image(converted, size=(10, 25), response_body=converted_buf)\n-        self.assertEqual(thumbnail.mode, 'RGB')\n+        thumbnail, _ = self.pipeline.convert_image(\n+            converted, size=(10, 25), response_body=converted_buf\n+        )\n+        self.assertEqual(thumbnail.mode, \"RGB\")\n         self.assertEqual(thumbnail.size, (10, 10))\n \n         # transparency case: RGBA and PNG\n         COLOUR = (0, 127, 255, 50)\n-        im, buf = _create_image('PNG', 'RGBA', SIZE, COLOUR)\n+        im, buf = _create_image(\"PNG\", \"RGBA\", SIZE, COLOUR)\n         converted, _ = self.pipeline.convert_image(im, response_body=buf)\n-        self.assertEqual(converted.mode, 'RGB')\n+        self.assertEqual(converted.mode, \"RGB\")\n         self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n \n         # transparency case with palette: P and PNG\n         COLOUR = (0, 127, 255, 50)\n-        im, buf = _create_image('PNG', 'RGBA', SIZE, COLOUR)\n-        im = im.convert('P')\n+        im, buf = _create_image(\"PNG\", \"RGBA\", SIZE, COLOUR)\n+        im = im.convert(\"P\")\n         converted, _ = self.pipeline.convert_image(im, response_body=buf)\n-        self.assertEqual(converted.mode, 'RGB')\n+        self.assertEqual(converted.mode, \"RGB\")\n         self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n \n \n@@ -252,11 +325,11 @@ class DeprecatedImagesPipeline(ImagesPipeline):\n \n     def image_key(self, url):\n         image_guid = hashlib.sha1(to_bytes(url)).hexdigest()\n-        return f'empty/{image_guid}.jpg'\n+        return f\"empty/{image_guid}.jpg\"\n \n     def thumb_key(self, url, thumb_id):\n         thumb_guid = hashlib.sha1(to_bytes(url)).hexdigest()\n-        return f'thumbsup/{thumb_id}/{thumb_guid}.jpg'\n+        return f\"thumbsup/{thumb_id}/{thumb_guid}.jpg\"\n \n \n class ImagesPipelineTestCaseFieldsMixin:\n@@ -264,35 +337,43 @@ class ImagesPipelineTestCaseFieldsMixin:\n     skip = skip_pillow\n \n     def test_item_fields_default(self):\n-        url = 'http://www.example.com/images/1.jpg'\n-        item = self.item_class(name='item1', image_urls=[url])\n-        pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': 's3://example/images/'}))\n+        url = \"http://www.example.com/images/1.jpg\"\n+        item = self.item_class(name=\"item1\", image_urls=[url])\n+        pipeline = ImagesPipeline.from_settings(\n+            Settings({\"IMAGES_STORE\": \"s3://example/images/\"})\n+        )\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n-        results = [(True, {'url': url})]\n+        results = [(True, {\"url\": url})]\n         item = pipeline.item_completed(results, item, None)\n         images = ItemAdapter(item).get(\"images\")\n         self.assertEqual(images, [results[0][1]])\n         self.assertIsInstance(item, self.item_class)\n \n     def test_item_fields_override_settings(self):\n-        url = 'http://www.example.com/images/1.jpg'\n-        item = self.item_class(name='item1', custom_image_urls=[url])\n-        pipeline = ImagesPipeline.from_settings(Settings({\n-            'IMAGES_STORE': 's3://example/images/',\n-            'IMAGES_URLS_FIELD': 'custom_image_urls',\n-            'IMAGES_RESULT_FIELD': 'custom_images'\n-        }))\n+        url = \"http://www.example.com/images/1.jpg\"\n+        item = self.item_class(name=\"item1\", custom_image_urls=[url])\n+        pipeline = ImagesPipeline.from_settings(\n+            Settings(\n+                {\n+                    \"IMAGES_STORE\": \"s3://example/images/\",\n+                    \"IMAGES_URLS_FIELD\": \"custom_image_urls\",\n+                    \"IMAGES_RESULT_FIELD\": \"custom_images\",\n+                }\n+            )\n+        )\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n-        results = [(True, {'url': url})]\n+        results = [(True, {\"url\": url})]\n         item = pipeline.item_completed(results, item, None)\n         custom_images = ItemAdapter(item).get(\"custom_images\")\n         self.assertEqual(custom_images, [results[0][1]])\n         self.assertIsInstance(item, self.item_class)\n \n \n-class ImagesPipelineTestCaseFieldsDict(ImagesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+class ImagesPipelineTestCaseFieldsDict(\n+    ImagesPipelineTestCaseFieldsMixin, unittest.TestCase\n+):\n     item_class = dict\n \n \n@@ -306,7 +387,9 @@ class ImagesPipelineTestItem(Item):\n     custom_images = Field()\n \n \n-class ImagesPipelineTestCaseFieldsItem(ImagesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+class ImagesPipelineTestCaseFieldsItem(\n+    ImagesPipelineTestCaseFieldsMixin, unittest.TestCase\n+):\n     item_class = ImagesPipelineTestItem\n \n \n@@ -321,7 +404,9 @@ class ImagesPipelineTestDataClass:\n     custom_images: list = dataclasses.field(default_factory=list)\n \n \n-class ImagesPipelineTestCaseFieldsDataClass(ImagesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+class ImagesPipelineTestCaseFieldsDataClass(\n+    ImagesPipelineTestCaseFieldsMixin, unittest.TestCase\n+):\n     item_class = ImagesPipelineTestDataClass\n \n \n@@ -336,7 +421,9 @@ class ImagesPipelineTestAttrsItem:\n     custom_images = attr.ib(default=lambda: [])\n \n \n-class ImagesPipelineTestCaseFieldsAttrsItem(ImagesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+class ImagesPipelineTestCaseFieldsAttrsItem(\n+    ImagesPipelineTestCaseFieldsMixin, unittest.TestCase\n+):\n     item_class = ImagesPipelineTestAttrsItem\n \n \n@@ -351,7 +438,7 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n         (\"MIN_HEIGHT\", \"IMAGES_MIN_HEIGHT\"),\n         (\"IMAGES_URLS_FIELD\", \"IMAGES_URLS_FIELD\"),\n         (\"IMAGES_RESULT_FIELD\", \"IMAGES_RESULT_FIELD\"),\n-        (\"THUMBS\", \"IMAGES_THUMBS\")\n+        (\"THUMBS\", \"IMAGES_THUMBS\"),\n     ]\n \n     # This should match what is defined in ImagesPipeline.\n@@ -360,8 +447,8 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n         MIN_HEIGHT=0,\n         EXPIRES=90,\n         THUMBS={},\n-        IMAGES_URLS_FIELD='image_urls',\n-        IMAGES_RESULT_FIELD='images'\n+        IMAGES_URLS_FIELD=\"image_urls\",\n+        IMAGES_RESULT_FIELD=\"images\",\n     )\n \n     def setUp(self):\n@@ -387,26 +474,30 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n             \"IMAGES_MIN_WIDTH\": random.randint(1, 1000),\n             \"IMAGES_MIN_HEIGHT\": random.randint(1, 1000),\n             \"IMAGES_THUMBS\": {\n-                'small': (random.randint(1, 1000), random.randint(1, 1000)),\n-                'big': (random.randint(1, 1000), random.randint(1, 1000))\n-            }\n+                \"small\": (random.randint(1, 1000), random.randint(1, 1000)),\n+                \"big\": (random.randint(1, 1000), random.randint(1, 1000)),\n+            },\n         }\n         if not prefix:\n             return settings\n \n-        return {prefix.upper() + \"_\" + k if k != \"IMAGES_STORE\" else k: v for k, v in settings.items()}\n+        return {\n+            prefix.upper() + \"_\" + k if k != \"IMAGES_STORE\" else k: v\n+            for k, v in settings.items()\n+        }\n \n     def _generate_fake_pipeline_subclass(self):\n         \"\"\"\n         :return: ImagePipeline class will all uppercase attributes set.\n         \"\"\"\n+\n         class UserDefinedImagePipeline(ImagesPipeline):\n             # Values should be in different range than fake_settings.\n             MIN_WIDTH = random.randint(1000, 2000)\n             MIN_HEIGHT = random.randint(1000, 2000)\n             THUMBS = {\n-                'small': (random.randint(1000, 2000), random.randint(1000, 2000)),\n-                'big': (random.randint(1000, 2000), random.randint(1000, 2000))\n+                \"small\": (random.randint(1000, 2000), random.randint(1000, 2000)),\n+                \"big\": (random.randint(1000, 2000), random.randint(1000, 2000)),\n             }\n             EXPIRES = random.randint(1000, 2000)\n             IMAGES_URLS_FIELD = \"field_one\"\n@@ -427,7 +518,9 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n             expected_default_value = self.default_pipeline_settings.get(pipe_attr)\n             custom_value = custom_settings.get(settings_attr)\n             self.assertNotEqual(expected_default_value, custom_value)\n-            self.assertEqual(getattr(default_sts_pipe, pipe_attr.lower()), expected_default_value)\n+            self.assertEqual(\n+                getattr(default_sts_pipe, pipe_attr.lower()), expected_default_value\n+            )\n             self.assertEqual(getattr(user_sts_pipe, pipe_attr.lower()), custom_value)\n \n     def test_subclass_attrs_preserved_default_settings(self):\n@@ -464,10 +557,13 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n         If there are no settings for subclass and no subclass attributes, pipeline should use\n         attributes of base class.\n         \"\"\"\n+\n         class UserDefinedImagePipeline(ImagesPipeline):\n             pass\n \n-        user_pipeline = UserDefinedImagePipeline.from_settings(Settings({\"IMAGES_STORE\": self.tempdir}))\n+        user_pipeline = UserDefinedImagePipeline.from_settings(\n+            Settings({\"IMAGES_STORE\": self.tempdir})\n+        )\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n             custom_value = self.default_pipeline_settings.get(pipe_attr.upper())\n@@ -478,6 +574,7 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n         If there are custom settings for subclass and NO class attributes, pipeline should use custom\n         settings.\n         \"\"\"\n+\n         class UserDefinedImagePipeline(ImagesPipeline):\n             pass\n \n@@ -508,7 +605,10 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n         class UserDefinedImagePipeline(ImagesPipeline):\n             DEFAULT_IMAGES_URLS_FIELD = \"something\"\n             DEFAULT_IMAGES_RESULT_FIELD = \"something_else\"\n-        pipeline = UserDefinedImagePipeline.from_settings(Settings({\"IMAGES_STORE\": self.tempdir}))\n+\n+        pipeline = UserDefinedImagePipeline.from_settings(\n+            Settings({\"IMAGES_STORE\": self.tempdir})\n+        )\n         self.assertEqual(pipeline.images_result_field, \"something_else\")\n         self.assertEqual(pipeline.images_urls_field, \"something\")\n \n@@ -526,12 +626,10 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n \n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             expected_value = settings.get(settings_attr)\n-            self.assertEqual(getattr(pipeline_cls, pipe_attr.lower()),\n-                             expected_value)\n+            self.assertEqual(getattr(pipeline_cls, pipe_attr.lower()), expected_value)\n \n \n class NoimagesDropTestCase(unittest.TestCase):\n-\n     def test_deprecation_warning(self):\n         arg = str()\n         with warnings.catch_warnings(record=True) as w:\n@@ -539,8 +637,10 @@ class NoimagesDropTestCase(unittest.TestCase):\n             self.assertEqual(len(w), 1)\n             self.assertEqual(w[0].category, ScrapyDeprecationWarning)\n         with warnings.catch_warnings(record=True) as w:\n+\n             class SubclassedNoimagesDrop(NoimagesDrop):\n                 pass\n+\n             SubclassedNoimagesDrop(arg)\n             self.assertEqual(len(w), 1)\n             self.assertEqual(w[0].category, ScrapyDeprecationWarning)\n\n@@ -23,13 +23,15 @@ from scrapy.utils.test import get_crawler\n try:\n     from PIL import Image  # noqa: imported just to check for the import error\n except ImportError:\n-    skip_pillow: Optional[str] = 'Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow'\n+    skip_pillow: Optional[\n+        str\n+    ] = \"Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow\"\n else:\n     skip_pillow = None\n \n \n def _mocked_download_func(request, info):\n-    response = request.meta.get('response')\n+    response = request.meta.get(\"response\")\n     return response() if callable(response) else response\n \n \n@@ -40,7 +42,7 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n \n     def setUp(self):\n         spider_cls = Spider\n-        self.spider = spider_cls('media.com')\n+        self.spider = spider_cls(\"media.com\")\n         crawler = get_crawler(spider_cls, self.settings)\n         self.pipe = self.pipeline_class.from_crawler(crawler)\n         self.pipe.download_func = _mocked_download_func\n@@ -50,29 +52,29 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n \n     def tearDown(self):\n         for name, signal in vars(signals).items():\n-            if not name.startswith('_'):\n+            if not name.startswith(\"_\"):\n                 disconnect_all(signal)\n \n     def test_default_media_to_download(self):\n-        request = Request('http://url')\n+        request = Request(\"http://url\")\n         assert self.pipe.media_to_download(request, self.info) is None\n \n     def test_default_get_media_requests(self):\n-        item = dict(name='name')\n+        item = dict(name=\"name\")\n         assert self.pipe.get_media_requests(item, self.info) is None\n \n     def test_default_media_downloaded(self):\n-        request = Request('http://url')\n-        response = Response('http://url', body=b'')\n+        request = Request(\"http://url\")\n+        response = Response(\"http://url\", body=b\"\")\n         assert self.pipe.media_downloaded(response, request, self.info) is response\n \n     def test_default_media_failed(self):\n-        request = Request('http://url')\n+        request = Request(\"http://url\")\n         fail = Failure(Exception())\n         assert self.pipe.media_failed(fail, request, self.info) is fail\n \n     def test_default_item_completed(self):\n-        item = dict(name='name')\n+        item = dict(name=\"name\")\n         assert self.pipe.item_completed([], item, self.info) is item\n \n         # Check that failures are logged by default\n@@ -85,7 +87,7 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         assert new_item is item\n         assert len(log.records) == 1\n         record = log.records[0]\n-        assert record.levelname == 'ERROR'\n+        assert record.levelname == \"ERROR\"\n         self.assertTupleEqual(record.exc_info, failure_to_exc_info(fail))\n \n         # disable failure logging and check again\n@@ -97,14 +99,14 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n \n     @inlineCallbacks\n     def test_default_process_item(self):\n-        item = dict(name='name')\n+        item = dict(name=\"name\")\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n \n     def test_modify_media_request(self):\n-        request = Request('http://url')\n+        request = Request(\"http://url\")\n         self.pipe._modify_media_request(request)\n-        assert request.meta == {'handle_httpstatus_all': True}\n+        assert request.meta == {\"handle_httpstatus_all\": True}\n \n     def test_should_remove_req_res_references_before_caching_the_results(self):\n         \"\"\"Regression test case to prevent a memory leak in the Media Pipeline.\n@@ -134,8 +136,8 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         Chaining (https://www.python.org/dev/peps/pep-3134/).\n         \"\"\"\n         # Create sample pair of Request and Response objects\n-        request = Request('http://url')\n-        response = Response('http://url', body=b'', request=request)\n+        request = Request(\"http://url\")\n+        response = Response(\"http://url\", body=b\"\", request=request)\n \n         # Simulate the Media Pipeline behavior to produce a Twisted Failure\n         try:\n@@ -146,7 +148,7 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n             try:\n                 # Simulate the media_downloaded callback raising a FileException\n                 # This usually happens when the status code is not 200 OK\n-                raise FileException('download-error')\n+                raise FileException(\"download-error\")\n             except Exception as exc:\n                 file_exc = exc\n                 # Simulate Twisted capturing the FileException\n@@ -171,42 +173,41 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         # ... encapsulating the original FileException ...\n         self.assertEqual(info.downloaded[fp].value, file_exc)\n         # ... but it should not store the StopIteration exception on its context\n-        context = getattr(info.downloaded[fp].value, '__context__', None)\n+        context = getattr(info.downloaded[fp].value, \"__context__\", None)\n         self.assertIsNone(context)\n \n \n class MockedMediaPipeline(MediaPipeline):\n-\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self._mockcalled = []\n \n     def download(self, request, info):\n-        self._mockcalled.append('download')\n+        self._mockcalled.append(\"download\")\n         return super().download(request, info)\n \n     def media_to_download(self, request, info, *, item=None):\n-        self._mockcalled.append('media_to_download')\n-        if 'result' in request.meta:\n-            return request.meta.get('result')\n+        self._mockcalled.append(\"media_to_download\")\n+        if \"result\" in request.meta:\n+            return request.meta.get(\"result\")\n         return super().media_to_download(request, info)\n \n     def get_media_requests(self, item, info):\n-        self._mockcalled.append('get_media_requests')\n-        return item.get('requests')\n+        self._mockcalled.append(\"get_media_requests\")\n+        return item.get(\"requests\")\n \n     def media_downloaded(self, response, request, info, *, item=None):\n-        self._mockcalled.append('media_downloaded')\n+        self._mockcalled.append(\"media_downloaded\")\n         return super().media_downloaded(response, request, info)\n \n     def media_failed(self, failure, request, info):\n-        self._mockcalled.append('media_failed')\n+        self._mockcalled.append(\"media_failed\")\n         return super().media_failed(failure, request, info)\n \n     def item_completed(self, results, item, info):\n-        self._mockcalled.append('item_completed')\n+        self._mockcalled.append(\"item_completed\")\n         item = super().item_completed(results, item, info)\n-        item['results'] = results\n+        item[\"results\"] = results\n         return item\n \n \n@@ -215,72 +216,94 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n     pipeline_class = MockedMediaPipeline\n \n     def _callback(self, result):\n-        self.pipe._mockcalled.append('request_callback')\n+        self.pipe._mockcalled.append(\"request_callback\")\n         return result\n \n     def _errback(self, result):\n-        self.pipe._mockcalled.append('request_errback')\n+        self.pipe._mockcalled.append(\"request_errback\")\n         return result\n \n     @inlineCallbacks\n     def test_result_succeed(self):\n-        rsp = Response('http://url1')\n-        req = Request('http://url1', meta=dict(response=rsp),\n-                      callback=self._callback, errback=self._errback)\n+        rsp = Response(\"http://url1\")\n+        req = Request(\n+            \"http://url1\",\n+            meta=dict(response=rsp),\n+            callback=self._callback,\n+            errback=self._errback,\n+        )\n         item = dict(requests=req)\n         new_item = yield self.pipe.process_item(item, self.spider)\n-        self.assertEqual(new_item['results'], [(True, rsp)])\n+        self.assertEqual(new_item[\"results\"], [(True, rsp)])\n         self.assertEqual(\n             self.pipe._mockcalled,\n-            ['get_media_requests', 'media_to_download', 'media_downloaded', 'request_callback', 'item_completed'])\n+            [\n+                \"get_media_requests\",\n+                \"media_to_download\",\n+                \"media_downloaded\",\n+                \"request_callback\",\n+                \"item_completed\",\n+            ],\n+        )\n \n     @inlineCallbacks\n     def test_result_failure(self):\n         self.pipe.LOG_FAILED_RESULTS = False\n         fail = Failure(Exception())\n-        req = Request('http://url1', meta=dict(response=fail),\n-                      callback=self._callback, errback=self._errback)\n+        req = Request(\n+            \"http://url1\",\n+            meta=dict(response=fail),\n+            callback=self._callback,\n+            errback=self._errback,\n+        )\n         item = dict(requests=req)\n         new_item = yield self.pipe.process_item(item, self.spider)\n-        self.assertEqual(new_item['results'], [(False, fail)])\n+        self.assertEqual(new_item[\"results\"], [(False, fail)])\n         self.assertEqual(\n             self.pipe._mockcalled,\n-            ['get_media_requests', 'media_to_download', 'media_failed', 'request_errback', 'item_completed'])\n+            [\n+                \"get_media_requests\",\n+                \"media_to_download\",\n+                \"media_failed\",\n+                \"request_errback\",\n+                \"item_completed\",\n+            ],\n+        )\n \n     @inlineCallbacks\n     def test_mix_of_success_and_failure(self):\n         self.pipe.LOG_FAILED_RESULTS = False\n-        rsp1 = Response('http://url1')\n-        req1 = Request('http://url1', meta=dict(response=rsp1))\n+        rsp1 = Response(\"http://url1\")\n+        req1 = Request(\"http://url1\", meta=dict(response=rsp1))\n         fail = Failure(Exception())\n-        req2 = Request('http://url2', meta=dict(response=fail))\n+        req2 = Request(\"http://url2\", meta=dict(response=fail))\n         item = dict(requests=[req1, req2])\n         new_item = yield self.pipe.process_item(item, self.spider)\n-        self.assertEqual(new_item['results'], [(True, rsp1), (False, fail)])\n+        self.assertEqual(new_item[\"results\"], [(True, rsp1), (False, fail)])\n         m = self.pipe._mockcalled\n         # only once\n-        self.assertEqual(m[0], 'get_media_requests')  # first hook called\n-        self.assertEqual(m.count('get_media_requests'), 1)\n-        self.assertEqual(m.count('item_completed'), 1)\n-        self.assertEqual(m[-1], 'item_completed')  # last hook called\n+        self.assertEqual(m[0], \"get_media_requests\")  # first hook called\n+        self.assertEqual(m.count(\"get_media_requests\"), 1)\n+        self.assertEqual(m.count(\"item_completed\"), 1)\n+        self.assertEqual(m[-1], \"item_completed\")  # last hook called\n         # twice, one per request\n-        self.assertEqual(m.count('media_to_download'), 2)\n+        self.assertEqual(m.count(\"media_to_download\"), 2)\n         # one to handle success and other for failure\n-        self.assertEqual(m.count('media_downloaded'), 1)\n-        self.assertEqual(m.count('media_failed'), 1)\n+        self.assertEqual(m.count(\"media_downloaded\"), 1)\n+        self.assertEqual(m.count(\"media_failed\"), 1)\n \n     @inlineCallbacks\n     def test_get_media_requests(self):\n         # returns single Request (without callback)\n-        req = Request('http://url')\n+        req = Request(\"http://url\")\n         item = dict(requests=req)  # pass a single item\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n         self.assertIn(self.fingerprint(req), self.info.downloaded)\n \n         # returns iterable of Requests\n-        req1 = Request('http://url1')\n-        req2 = Request('http://url2')\n+        req1 = Request(\"http://url1\")\n+        req2 = Request(\"http://url2\")\n         item = dict(requests=iter([req1, req2]))\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n@@ -289,30 +312,34 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n \n     @inlineCallbacks\n     def test_results_are_cached_across_multiple_items(self):\n-        rsp1 = Response('http://url1')\n-        req1 = Request('http://url1', meta=dict(response=rsp1))\n+        rsp1 = Response(\"http://url1\")\n+        req1 = Request(\"http://url1\", meta=dict(response=rsp1))\n         item = dict(requests=req1)\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n-        self.assertEqual(new_item['results'], [(True, rsp1)])\n+        self.assertEqual(new_item[\"results\"], [(True, rsp1)])\n \n         # rsp2 is ignored, rsp1 must be in results because request fingerprints are the same\n-        req2 = Request(req1.url, meta=dict(response=Response('http://donot.download.me')))\n+        req2 = Request(\n+            req1.url, meta=dict(response=Response(\"http://donot.download.me\"))\n+        )\n         item = dict(requests=req2)\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n         self.assertEqual(self.fingerprint(req1), self.fingerprint(req2))\n-        self.assertEqual(new_item['results'], [(True, rsp1)])\n+        self.assertEqual(new_item[\"results\"], [(True, rsp1)])\n \n     @inlineCallbacks\n     def test_results_are_cached_for_requests_of_single_item(self):\n-        rsp1 = Response('http://url1')\n-        req1 = Request('http://url1', meta=dict(response=rsp1))\n-        req2 = Request(req1.url, meta=dict(response=Response('http://donot.download.me')))\n+        rsp1 = Response(\"http://url1\")\n+        req1 = Request(\"http://url1\", meta=dict(response=rsp1))\n+        req2 = Request(\n+            req1.url, meta=dict(response=Response(\"http://donot.download.me\"))\n+        )\n         item = dict(requests=[req1, req2])\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n-        self.assertEqual(new_item['results'], [(True, rsp1), (True, rsp1)])\n+        self.assertEqual(new_item[\"results\"], [(True, rsp1), (True, rsp1)])\n \n     @inlineCallbacks\n     def test_wait_if_request_is_downloading(self):\n@@ -324,78 +351,80 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n             self.assertEqual(len(self.info.waiting[fp]), 2)\n             return response\n \n-        rsp1 = Response('http://url')\n+        rsp1 = Response(\"http://url\")\n \n         def rsp1_func():\n             dfd = Deferred().addCallback(_check_downloading)\n-            reactor.callLater(.1, dfd.callback, rsp1)\n+            reactor.callLater(0.1, dfd.callback, rsp1)\n             return dfd\n \n         def rsp2_func():\n-            self.fail('it must cache rsp1 result and must not try to redownload')\n+            self.fail(\"it must cache rsp1 result and must not try to redownload\")\n \n-        req1 = Request('http://url', meta=dict(response=rsp1_func))\n+        req1 = Request(\"http://url\", meta=dict(response=rsp1_func))\n         req2 = Request(req1.url, meta=dict(response=rsp2_func))\n         item = dict(requests=[req1, req2])\n         new_item = yield self.pipe.process_item(item, self.spider)\n-        self.assertEqual(new_item['results'], [(True, rsp1), (True, rsp1)])\n+        self.assertEqual(new_item[\"results\"], [(True, rsp1), (True, rsp1)])\n \n     @inlineCallbacks\n     def test_use_media_to_download_result(self):\n-        req = Request('http://url', meta=dict(result='ITSME', response=self.fail))\n+        req = Request(\"http://url\", meta=dict(result=\"ITSME\", response=self.fail))\n         item = dict(requests=req)\n         new_item = yield self.pipe.process_item(item, self.spider)\n-        self.assertEqual(new_item['results'], [(True, 'ITSME')])\n+        self.assertEqual(new_item[\"results\"], [(True, \"ITSME\")])\n         self.assertEqual(\n             self.pipe._mockcalled,\n-            ['get_media_requests', 'media_to_download', 'item_completed'])\n+            [\"get_media_requests\", \"media_to_download\", \"item_completed\"],\n+        )\n \n \n class MockedMediaPipelineDeprecatedMethods(ImagesPipeline):\n-\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self._mockcalled = []\n \n     def get_media_requests(self, item, info):\n-        item_url = item['image_urls'][0]\n+        item_url = item[\"image_urls\"][0]\n         output_img = io.BytesIO()\n-        img = Image.new('RGB', (60, 30), color='red')\n-        img.save(output_img, format='JPEG')\n+        img = Image.new(\"RGB\", (60, 30), color=\"red\")\n+        img.save(output_img, format=\"JPEG\")\n         return Request(\n             item_url,\n-            meta={'response': Response(item_url, status=200, body=output_img.getvalue())}\n+            meta={\n+                \"response\": Response(item_url, status=200, body=output_img.getvalue())\n+            },\n         )\n \n     def inc_stats(self, *args, **kwargs):\n         return True\n \n     def media_to_download(self, request, info):\n-        self._mockcalled.append('media_to_download')\n+        self._mockcalled.append(\"media_to_download\")\n         return super().media_to_download(request, info)\n \n     def media_downloaded(self, response, request, info):\n-        self._mockcalled.append('media_downloaded')\n+        self._mockcalled.append(\"media_downloaded\")\n         return super().media_downloaded(response, request, info)\n \n     def file_downloaded(self, response, request, info):\n-        self._mockcalled.append('file_downloaded')\n+        self._mockcalled.append(\"file_downloaded\")\n         return super().file_downloaded(response, request, info)\n \n     def file_path(self, request, response=None, info=None):\n-        self._mockcalled.append('file_path')\n+        self._mockcalled.append(\"file_path\")\n         return super().file_path(request, response, info)\n \n     def thumb_path(self, request, thumb_id, response=None, info=None):\n-        self._mockcalled.append('thumb_path')\n+        self._mockcalled.append(\"thumb_path\")\n         return super().thumb_path(request, thumb_id, response, info)\n \n     def get_images(self, response, request, info):\n-        self._mockcalled.append('get_images')\n+        self._mockcalled.append(\"get_images\")\n         return super().get_images(response, request, info)\n \n     def image_downloaded(self, response, request, info):\n-        self._mockcalled.append('image_downloaded')\n+        self._mockcalled.append(\"image_downloaded\")\n         return super().image_downloaded(response, request, info)\n \n \n@@ -404,20 +433,23 @@ class MediaPipelineDeprecatedMethodsTestCase(unittest.TestCase):\n \n     def setUp(self):\n         settings_dict = {\n-            'IMAGES_STORE': 'store-uri',\n-            'IMAGES_THUMBS': {'small': (50, 50)},\n+            \"IMAGES_STORE\": \"store-uri\",\n+            \"IMAGES_THUMBS\": {\"small\": (50, 50)},\n         }\n         crawler = get_crawler(spidercls=None, settings_dict=settings_dict)\n         self.pipe = MockedMediaPipelineDeprecatedMethods.from_crawler(crawler)\n         self.pipe.download_func = _mocked_download_func\n         self.pipe.open_spider(None)\n-        self.item = dict(image_urls=['http://picsum.photos/id/1014/200/300'], images=[])\n+        self.item = dict(image_urls=[\"http://picsum.photos/id/1014/200/300\"], images=[])\n \n     def _assert_method_called_with_warnings(self, method, message, warnings):\n         self.assertIn(method, self.pipe._mockcalled)\n         warningShown = False\n         for warning in warnings:\n-            if warning['message'] == message and warning['category'] == ScrapyDeprecationWarning:\n+            if (\n+                warning[\"message\"] == message\n+                and warning[\"category\"] == ScrapyDeprecationWarning\n+            ):\n                 warningShown = True\n         self.assertTrue(warningShown)\n \n@@ -426,83 +458,81 @@ class MediaPipelineDeprecatedMethodsTestCase(unittest.TestCase):\n         yield self.pipe.process_item(self.item, None)\n         warnings = self.flushWarnings([MediaPipeline._compatible])\n         message = (\n-            'media_to_download(self, request, info) is deprecated, '\n-            'please use media_to_download(self, request, info, *, item=None)'\n+            \"media_to_download(self, request, info) is deprecated, \"\n+            \"please use media_to_download(self, request, info, *, item=None)\"\n         )\n-        self._assert_method_called_with_warnings('media_to_download', message, warnings)\n+        self._assert_method_called_with_warnings(\"media_to_download\", message, warnings)\n \n     @inlineCallbacks\n     def test_media_downloaded_called(self):\n         yield self.pipe.process_item(self.item, None)\n         warnings = self.flushWarnings([MediaPipeline._compatible])\n         message = (\n-            'media_downloaded(self, response, request, info) is deprecated, '\n-            'please use media_downloaded(self, response, request, info, *, item=None)'\n+            \"media_downloaded(self, response, request, info) is deprecated, \"\n+            \"please use media_downloaded(self, response, request, info, *, item=None)\"\n         )\n-        self._assert_method_called_with_warnings('media_downloaded', message, warnings)\n+        self._assert_method_called_with_warnings(\"media_downloaded\", message, warnings)\n \n     @inlineCallbacks\n     def test_file_downloaded_called(self):\n         yield self.pipe.process_item(self.item, None)\n         warnings = self.flushWarnings([MediaPipeline._compatible])\n         message = (\n-            'file_downloaded(self, response, request, info) is deprecated, '\n-            'please use file_downloaded(self, response, request, info, *, item=None)'\n+            \"file_downloaded(self, response, request, info) is deprecated, \"\n+            \"please use file_downloaded(self, response, request, info, *, item=None)\"\n         )\n-        self._assert_method_called_with_warnings('file_downloaded', message, warnings)\n+        self._assert_method_called_with_warnings(\"file_downloaded\", message, warnings)\n \n     @inlineCallbacks\n     def test_file_path_called(self):\n         yield self.pipe.process_item(self.item, None)\n         warnings = self.flushWarnings([MediaPipeline._compatible])\n         message = (\n-            'file_path(self, request, response=None, info=None) is deprecated, '\n-            'please use file_path(self, request, response=None, info=None, *, item=None)'\n+            \"file_path(self, request, response=None, info=None) is deprecated, \"\n+            \"please use file_path(self, request, response=None, info=None, *, item=None)\"\n         )\n-        self._assert_method_called_with_warnings('file_path', message, warnings)\n+        self._assert_method_called_with_warnings(\"file_path\", message, warnings)\n \n     @inlineCallbacks\n     def test_thumb_path_called(self):\n         yield self.pipe.process_item(self.item, None)\n         warnings = self.flushWarnings([MediaPipeline._compatible])\n         message = (\n-            'thumb_path(self, request, thumb_id, response=None, info=None) is deprecated, '\n-            'please use thumb_path(self, request, thumb_id, response=None, info=None, *, item=None)'\n+            \"thumb_path(self, request, thumb_id, response=None, info=None) is deprecated, \"\n+            \"please use thumb_path(self, request, thumb_id, response=None, info=None, *, item=None)\"\n         )\n-        self._assert_method_called_with_warnings('thumb_path', message, warnings)\n+        self._assert_method_called_with_warnings(\"thumb_path\", message, warnings)\n \n     @inlineCallbacks\n     def test_get_images_called(self):\n         yield self.pipe.process_item(self.item, None)\n         warnings = self.flushWarnings([MediaPipeline._compatible])\n         message = (\n-            'get_images(self, response, request, info) is deprecated, '\n-            'please use get_images(self, response, request, info, *, item=None)'\n+            \"get_images(self, response, request, info) is deprecated, \"\n+            \"please use get_images(self, response, request, info, *, item=None)\"\n         )\n-        self._assert_method_called_with_warnings('get_images', message, warnings)\n+        self._assert_method_called_with_warnings(\"get_images\", message, warnings)\n \n     @inlineCallbacks\n     def test_image_downloaded_called(self):\n         yield self.pipe.process_item(self.item, None)\n         warnings = self.flushWarnings([MediaPipeline._compatible])\n         message = (\n-            'image_downloaded(self, response, request, info) is deprecated, '\n-            'please use image_downloaded(self, response, request, info, *, item=None)'\n+            \"image_downloaded(self, response, request, info) is deprecated, \"\n+            \"please use image_downloaded(self, response, request, info, *, item=None)\"\n         )\n-        self._assert_method_called_with_warnings('image_downloaded', message, warnings)\n+        self._assert_method_called_with_warnings(\"image_downloaded\", message, warnings)\n \n \n class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n-\n     def _assert_request_no3xx(self, pipeline_class, settings):\n         pipe = pipeline_class(settings=Settings(settings))\n-        request = Request('http://url')\n+        request = Request(\"http://url\")\n         pipe._modify_media_request(request)\n \n-        self.assertIn('handle_httpstatus_list', request.meta)\n+        self.assertIn(\"handle_httpstatus_list\", request.meta)\n         for status, check in [\n             (200, True),\n-\n             # These are the status codes we want\n             # the downloader to handle itself\n             (301, False),\n@@ -510,41 +540,29 @@ class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n             (302, False),\n             (307, False),\n             (308, False),\n-\n             # we still want to get 4xx and 5xx\n             (400, True),\n             (404, True),\n-                (500, True)]:\n+            (500, True),\n+        ]:\n             if check:\n-                self.assertIn(status, request.meta['handle_httpstatus_list'])\n+                self.assertIn(status, request.meta[\"handle_httpstatus_list\"])\n             else:\n-                self.assertNotIn(status, request.meta['handle_httpstatus_list'])\n+                self.assertNotIn(status, request.meta[\"handle_httpstatus_list\"])\n \n     def test_standard_setting(self):\n-        self._assert_request_no3xx(\n-            MediaPipeline,\n-            {\n-                'MEDIA_ALLOW_REDIRECTS': True\n-            })\n+        self._assert_request_no3xx(MediaPipeline, {\"MEDIA_ALLOW_REDIRECTS\": True})\n \n     def test_subclass_standard_setting(self):\n-\n         class UserDefinedPipeline(MediaPipeline):\n             pass\n \n-        self._assert_request_no3xx(\n-            UserDefinedPipeline,\n-            {\n-                'MEDIA_ALLOW_REDIRECTS': True\n-            })\n+        self._assert_request_no3xx(UserDefinedPipeline, {\"MEDIA_ALLOW_REDIRECTS\": True})\n \n     def test_subclass_specific_setting(self):\n-\n         class UserDefinedPipeline(MediaPipeline):\n             pass\n \n         self._assert_request_no3xx(\n-            UserDefinedPipeline,\n-            {\n-                'USERDEFINEDPIPELINE_MEDIA_ALLOW_REDIRECTS': True\n-            })\n+            UserDefinedPipeline, {\"USERDEFINEDPIPELINE_MEDIA_ALLOW_REDIRECTS\": True}\n+        )\n\n@@ -14,13 +14,13 @@ from tests.mockserver import MockServer\n \n class SimplePipeline:\n     def process_item(self, item, spider):\n-        item['pipeline_passed'] = True\n+        item[\"pipeline_passed\"] = True\n         return item\n \n \n class DeferredPipeline:\n     def cb(self, item):\n-        item['pipeline_passed'] = True\n+        item[\"pipeline_passed\"] = True\n         return item\n \n     def process_item(self, item, spider):\n@@ -34,9 +34,10 @@ class AsyncDefPipeline:\n     async def process_item(self, item, spider):\n         d = Deferred()\n         from twisted.internet import reactor\n+\n         reactor.callLater(0, d.callback, None)\n         await maybe_deferred_to_future(d)\n-        item['pipeline_passed'] = True\n+        item[\"pipeline_passed\"] = True\n         return item\n \n \n@@ -44,10 +45,11 @@ class AsyncDefAsyncioPipeline:\n     async def process_item(self, item, spider):\n         d = Deferred()\n         from twisted.internet import reactor\n+\n         reactor.callLater(0, d.callback, None)\n         await deferred_to_future(d)\n         await asyncio.sleep(0.2)\n-        item['pipeline_passed'] = await get_from_asyncio_queue(True)\n+        item[\"pipeline_passed\"] = await get_from_asyncio_queue(True)\n         return item\n \n \n@@ -55,23 +57,24 @@ class AsyncDefNotAsyncioPipeline:\n     async def process_item(self, item, spider):\n         d1 = Deferred()\n         from twisted.internet import reactor\n+\n         reactor.callLater(0, d1.callback, None)\n         await d1\n         d2 = Deferred()\n         reactor.callLater(0, d2.callback, None)\n         await maybe_deferred_to_future(d2)\n-        item['pipeline_passed'] = True\n+        item[\"pipeline_passed\"] = True\n         return item\n \n \n class ItemSpider(Spider):\n-    name = 'itemspider'\n+    name = \"itemspider\"\n \n     def start_requests(self):\n-        yield Request(self.mockserver.url('/status?n=200'))\n+        yield Request(self.mockserver.url(\"/status?n=200\"))\n \n     def parse(self, response):\n-        return {'field': 42}\n+        return {\"field\": 42}\n \n \n class PipelineTestCase(unittest.TestCase):\n@@ -84,12 +87,12 @@ class PipelineTestCase(unittest.TestCase):\n \n     def _on_item_scraped(self, item):\n         self.assertIsInstance(item, dict)\n-        self.assertTrue(item.get('pipeline_passed'))\n+        self.assertTrue(item.get(\"pipeline_passed\"))\n         self.items.append(item)\n \n     def _create_crawler(self, pipeline_class):\n         settings = {\n-            'ITEM_PIPELINES': {pipeline_class: 1},\n+            \"ITEM_PIPELINES\": {pipeline_class: 1},\n         }\n         crawler = get_crawler(ItemSpider, settings)\n         crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n\n@@ -19,7 +19,9 @@ class PriorityQueueTest(unittest.TestCase):\n \n     def test_queue_push_pop_one(self):\n         temp_dir = tempfile.mkdtemp()\n-        queue = ScrapyPriorityQueue.from_crawler(self.crawler, FifoMemoryQueue, temp_dir)\n+        queue = ScrapyPriorityQueue.from_crawler(\n+            self.crawler, FifoMemoryQueue, temp_dir\n+        )\n         self.assertIsNone(queue.pop())\n         self.assertEqual(len(queue), 0)\n         req1 = Request(\"https://example.org/1\", priority=1)\n@@ -35,9 +37,14 @@ class PriorityQueueTest(unittest.TestCase):\n         if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n             raise unittest.SkipTest(\"queuelib.queue.FifoMemoryQueue.peek is defined\")\n         temp_dir = tempfile.mkdtemp()\n-        queue = ScrapyPriorityQueue.from_crawler(self.crawler, FifoMemoryQueue, temp_dir)\n+        queue = ScrapyPriorityQueue.from_crawler(\n+            self.crawler, FifoMemoryQueue, temp_dir\n+        )\n         queue.push(Request(\"https://example.org\"))\n-        with self.assertRaises(NotImplementedError, msg=\"The underlying queue class does not implement 'peek'\"):\n+        with self.assertRaises(\n+            NotImplementedError,\n+            msg=\"The underlying queue class does not implement 'peek'\",\n+        ):\n             queue.peek()\n         queue.close()\n \n@@ -45,7 +52,9 @@ class PriorityQueueTest(unittest.TestCase):\n         if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n             raise unittest.SkipTest(\"queuelib.queue.FifoMemoryQueue.peek is undefined\")\n         temp_dir = tempfile.mkdtemp()\n-        queue = ScrapyPriorityQueue.from_crawler(self.crawler, FifoMemoryQueue, temp_dir)\n+        queue = ScrapyPriorityQueue.from_crawler(\n+            self.crawler, FifoMemoryQueue, temp_dir\n+        )\n         self.assertEqual(len(queue), 0)\n         self.assertIsNone(queue.peek())\n         req1 = Request(\"https://example.org/1\")\n@@ -67,7 +76,9 @@ class PriorityQueueTest(unittest.TestCase):\n \n     def test_queue_push_pop_priorities(self):\n         temp_dir = tempfile.mkdtemp()\n-        queue = ScrapyPriorityQueue.from_crawler(self.crawler, FifoMemoryQueue, temp_dir, [-1, -2, -3])\n+        queue = ScrapyPriorityQueue.from_crawler(\n+            self.crawler, FifoMemoryQueue, temp_dir, [-1, -2, -3]\n+        )\n         self.assertIsNone(queue.pop())\n         self.assertEqual(len(queue), 0)\n         req1 = Request(\"https://example.org/1\", priority=1)\n@@ -119,7 +130,10 @@ class DownloaderAwarePriorityQueueTest(unittest.TestCase):\n         if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n             raise unittest.SkipTest(\"queuelib.queue.FifoMemoryQueue.peek is defined\")\n         self.queue.push(Request(\"https://example.org\"))\n-        with self.assertRaises(NotImplementedError, msg=\"The underlying queue class does not implement 'peek'\"):\n+        with self.assertRaises(\n+            NotImplementedError,\n+            msg=\"The underlying queue class does not implement 'peek'\",\n+        ):\n             self.queue.peek()\n \n     def test_peek(self):\n\n@@ -17,30 +17,40 @@ from tests.spiders import SimpleSpider, SingleRequestSpider\n \n \n class MitmProxy:\n-    auth_user = 'scrapy'\n-    auth_pass = 'scrapy'\n+    auth_user = \"scrapy\"\n+    auth_pass = \"scrapy\"\n \n     def start(self):\n         from scrapy.utils.test import get_testenv\n+\n         script = \"\"\"\n import sys\n from mitmproxy.tools.main import mitmdump\n sys.argv[0] = \"mitmdump\"\n sys.exit(mitmdump())\n         \"\"\"\n-        cert_path = Path(__file__).parent.resolve() / 'keys' / 'mitmproxy-ca.pem'\n-        self.proc = Popen([sys.executable,\n-                           '-c', script,\n-                           '--listen-host', '127.0.0.1',\n-                           '--listen-port', '0',\n-                           '--proxyauth', f'{self.auth_user}:{self.auth_pass}',\n-                           '--certs', str(cert_path),\n-                           '--ssl-insecure',\n+        cert_path = Path(__file__).parent.resolve() / \"keys\" / \"mitmproxy-ca.pem\"\n+        self.proc = Popen(\n+            [\n+                sys.executable,\n+                \"-c\",\n+                script,\n+                \"--listen-host\",\n+                \"127.0.0.1\",\n+                \"--listen-port\",\n+                \"0\",\n+                \"--proxyauth\",\n+                f\"{self.auth_user}:{self.auth_pass}\",\n+                \"--certs\",\n+                str(cert_path),\n+                \"--ssl-insecure\",\n             ],\n-                          stdout=PIPE, env=get_testenv())\n-        line = self.proc.stdout.readline().decode('utf-8')\n-        host_port = re.search(r'listening at http://([^:]+:\\d+)', line).group(1)\n-        address = f'http://{self.auth_user}:{self.auth_pass}@{host_port}'\n+            stdout=PIPE,\n+            env=get_testenv(),\n+        )\n+        line = self.proc.stdout.readline().decode(\"utf-8\")\n+        host_port = re.search(r\"listening at http://([^:]+:\\d+)\", line).group(1)\n+        address = f\"http://{self.auth_user}:{self.auth_pass}@{host_port}\"\n         return address\n \n     def stop(self):\n@@ -50,17 +60,16 @@ sys.exit(mitmdump())\n \n def _wrong_credentials(proxy_url):\n     bad_auth_proxy = list(urlsplit(proxy_url))\n-    bad_auth_proxy[1] = bad_auth_proxy[1].replace('scrapy:scrapy@', 'wrong:wronger@')\n+    bad_auth_proxy[1] = bad_auth_proxy[1].replace(\"scrapy:scrapy@\", \"wrong:wronger@\")\n     return urlunsplit(bad_auth_proxy)\n \n \n class ProxyConnectTestCase(TestCase):\n-\n     def setUp(self):\n         try:\n             import mitmproxy  # noqa: F401\n         except ImportError:\n-            self.skipTest('mitmproxy is not installed')\n+            self.skipTest(\"mitmproxy is not installed\")\n \n         self.mockserver = MockServer()\n         self.mockserver.__enter__()\n@@ -68,8 +77,8 @@ class ProxyConnectTestCase(TestCase):\n \n         self._proxy = MitmProxy()\n         proxy_url = self._proxy.start()\n-        os.environ['https_proxy'] = proxy_url\n-        os.environ['http_proxy'] = proxy_url\n+        os.environ[\"https_proxy\"] = proxy_url\n+        os.environ[\"http_proxy\"] = proxy_url\n \n     def tearDown(self):\n         self.mockserver.__exit__(None, None, None)\n@@ -85,7 +94,7 @@ class ProxyConnectTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_https_tunnel_auth_error(self):\n-        os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy'])\n+        os.environ[\"https_proxy\"] = _wrong_credentials(os.environ[\"https_proxy\"])\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n@@ -100,13 +109,13 @@ class ProxyConnectTestCase(TestCase):\n         with LogCapture() as log:\n             yield crawler.crawl(seed=request)\n         self._assert_got_response_code(200, log)\n-        echo = json.loads(crawler.spider.meta['responses'][0].text)\n-        self.assertTrue('Proxy-Authorization' not in echo['headers'])\n+        echo = json.loads(crawler.spider.meta[\"responses\"][0].text)\n+        self.assertTrue(\"Proxy-Authorization\" not in echo[\"headers\"])\n \n     def _assert_got_response_code(self, code, log):\n         print(log)\n-        self.assertEqual(str(log).count(f'Crawled ({code})'), 1)\n+        self.assertEqual(str(log).count(f\"Crawled ({code})\"), 1)\n \n     def _assert_got_tunnel_error(self, log):\n         print(log)\n-        self.assertIn('TunnelError', str(log))\n+        self.assertIn(\"TunnelError\", str(log))\n\n@@ -60,7 +60,6 @@ class AlternativeCallbacksMiddleware:\n \n \n class CrawlTestCase(TestCase):\n-\n     def setUp(self):\n         self.mockserver = MockServer()\n         self.mockserver.__enter__()\n@@ -90,11 +89,14 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_downloader_middleware_raise_exception(self):\n         url = self.mockserver.url(\"/status?n=200\")\n-        crawler = get_crawler(SingleRequestSpider, {\n+        crawler = get_crawler(\n+            SingleRequestSpider,\n+            {\n                 \"DOWNLOADER_MIDDLEWARES\": {\n                     RaiseExceptionRequestMiddleware: 590,\n                 },\n-        })\n+            },\n+        )\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         failure = crawler.spider.meta[\"failure\"]\n         self.assertEqual(failure.request.url, url)\n@@ -116,11 +118,14 @@ class CrawlTestCase(TestCase):\n             signal_params[\"request\"] = request\n \n         url = self.mockserver.url(\"/status?n=200\")\n-        crawler = get_crawler(SingleRequestSpider, {\n+        crawler = get_crawler(\n+            SingleRequestSpider,\n+            {\n                 \"DOWNLOADER_MIDDLEWARES\": {\n                     ProcessResponseMiddleware: 595,\n                 }\n-        })\n+            },\n+        )\n         crawler.signals.connect(signal_handler, signal=signals.response_received)\n \n         with LogCapture() as log:\n@@ -133,7 +138,11 @@ class CrawlTestCase(TestCase):\n         self.assertEqual(signal_params[\"request\"].url, OVERRIDEN_URL)\n \n         log.check_present(\n-            (\"scrapy.core.engine\", \"DEBUG\", f\"Crawled (200) <GET {OVERRIDEN_URL}> (referer: None)\"),\n+            (\n+                \"scrapy.core.engine\",\n+                \"DEBUG\",\n+                f\"Crawled (200) <GET {OVERRIDEN_URL}> (referer: None)\",\n+            ),\n         )\n \n     @defer.inlineCallbacks\n@@ -145,12 +154,15 @@ class CrawlTestCase(TestCase):\n         The spider callback should receive the overridden response.request\n         \"\"\"\n         url = self.mockserver.url(\"/status?n=200\")\n-        crawler = get_crawler(SingleRequestSpider, {\n+        crawler = get_crawler(\n+            SingleRequestSpider,\n+            {\n                 \"DOWNLOADER_MIDDLEWARES\": {\n                     RaiseExceptionRequestMiddleware: 590,\n                     CatchExceptionOverrideRequestMiddleware: 595,\n                 },\n-        })\n+            },\n+        )\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         response = crawler.spider.meta[\"responses\"][0]\n         self.assertEqual(response.body, b\"Caught ZeroDivisionError\")\n@@ -165,12 +177,15 @@ class CrawlTestCase(TestCase):\n         The spider callback should receive the original response.request\n         \"\"\"\n         url = self.mockserver.url(\"/status?n=200\")\n-        crawler = get_crawler(SingleRequestSpider, {\n+        crawler = get_crawler(\n+            SingleRequestSpider,\n+            {\n                 \"DOWNLOADER_MIDDLEWARES\": {\n                     RaiseExceptionRequestMiddleware: 590,\n                     CatchExceptionDoNotOverrideRequestMiddleware: 595,\n                 },\n-        })\n+            },\n+        )\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         response = crawler.spider.meta[\"responses\"][0]\n         self.assertEqual(response.body, b\"Caught ZeroDivisionError\")\n@@ -182,16 +197,23 @@ class CrawlTestCase(TestCase):\n         Downloader middleware which returns a response with a\n         specific 'request' attribute, with an alternative callback\n         \"\"\"\n-        crawler = get_crawler(AlternativeCallbacksSpider, {\n+        crawler = get_crawler(\n+            AlternativeCallbacksSpider,\n+            {\n                 \"DOWNLOADER_MIDDLEWARES\": {\n                     AlternativeCallbacksMiddleware: 595,\n                 }\n-        })\n+            },\n+        )\n \n         with LogCapture() as log:\n             url = self.mockserver.url(\"/status?n=200\")\n             yield crawler.crawl(seed=url, mockserver=self.mockserver)\n \n         log.check_present(\n-            (\"alternative_callbacks_spider\", \"INFO\", \"alt_callback was invoked with foo=bar\"),\n+            (\n+                \"alternative_callbacks_spider\",\n+                \"INFO\",\n+                \"alt_callback was invoked with foo=bar\",\n+            ),\n         )\n\n@@ -12,14 +12,15 @@ class InjectArgumentsDownloaderMiddleware:\n     \"\"\"\n     Make sure downloader middlewares are able to update the keyword arguments\n     \"\"\"\n+\n     def process_request(self, request, spider):\n-        if request.callback.__name__ == 'parse_downloader_mw':\n-            request.cb_kwargs['from_process_request'] = True\n+        if request.callback.__name__ == \"parse_downloader_mw\":\n+            request.cb_kwargs[\"from_process_request\"] = True\n         return None\n \n     def process_response(self, request, response, spider):\n-        if request.callback.__name__ == 'parse_downloader_mw':\n-            request.cb_kwargs['from_process_response'] = True\n+        if request.callback.__name__ == \"parse_downloader_mw\":\n+            request.cb_kwargs[\"from_process_response\"] = True\n         return response\n \n \n@@ -27,32 +28,36 @@ class InjectArgumentsSpiderMiddleware:\n     \"\"\"\n     Make sure spider middlewares are able to update the keyword arguments\n     \"\"\"\n+\n     def process_start_requests(self, start_requests, spider):\n         for request in start_requests:\n-            if request.callback.__name__ == 'parse_spider_mw':\n-                request.cb_kwargs['from_process_start_requests'] = True\n+            if request.callback.__name__ == \"parse_spider_mw\":\n+                request.cb_kwargs[\"from_process_start_requests\"] = True\n             yield request\n \n     def process_spider_input(self, response, spider):\n         request = response.request\n-        if request.callback.__name__ == 'parse_spider_mw':\n-            request.cb_kwargs['from_process_spider_input'] = True\n+        if request.callback.__name__ == \"parse_spider_mw\":\n+            request.cb_kwargs[\"from_process_spider_input\"] = True\n         return None\n \n     def process_spider_output(self, response, result, spider):\n         for element in result:\n-            if isinstance(element, Request) and element.callback.__name__ == 'parse_spider_mw_2':\n-                element.cb_kwargs['from_process_spider_output'] = True\n+            if (\n+                isinstance(element, Request)\n+                and element.callback.__name__ == \"parse_spider_mw_2\"\n+            ):\n+                element.cb_kwargs[\"from_process_spider_output\"] = True\n             yield element\n \n \n class KeywordArgumentsSpider(MockServerSpider):\n-    name = 'kwargs'\n+    name = \"kwargs\"\n     custom_settings = {\n-        'DOWNLOADER_MIDDLEWARES': {\n+        \"DOWNLOADER_MIDDLEWARES\": {\n             InjectArgumentsDownloaderMiddleware: 750,\n         },\n-        'SPIDER_MIDDLEWARES': {\n+        \"SPIDER_MIDDLEWARES\": {\n             InjectArgumentsSpiderMiddleware: 750,\n         },\n     }\n@@ -60,50 +65,61 @@ class KeywordArgumentsSpider(MockServerSpider):\n     checks = []\n \n     def start_requests(self):\n-        data = {'key': 'value', 'number': 123, 'callback': 'some_callback'}\n-        yield Request(self.mockserver.url('/first'), self.parse_first, cb_kwargs=data)\n-        yield Request(self.mockserver.url('/general_with'), self.parse_general, cb_kwargs=data)\n-        yield Request(self.mockserver.url('/general_without'), self.parse_general)\n-        yield Request(self.mockserver.url('/no_kwargs'), self.parse_no_kwargs)\n-        yield Request(self.mockserver.url('/default'), self.parse_default, cb_kwargs=data)\n-        yield Request(self.mockserver.url('/takes_less'), self.parse_takes_less, cb_kwargs=data)\n-        yield Request(self.mockserver.url('/takes_more'), self.parse_takes_more, cb_kwargs=data)\n-        yield Request(self.mockserver.url('/downloader_mw'), self.parse_downloader_mw)\n-        yield Request(self.mockserver.url('/spider_mw'), self.parse_spider_mw)\n+        data = {\"key\": \"value\", \"number\": 123, \"callback\": \"some_callback\"}\n+        yield Request(self.mockserver.url(\"/first\"), self.parse_first, cb_kwargs=data)\n+        yield Request(\n+            self.mockserver.url(\"/general_with\"), self.parse_general, cb_kwargs=data\n+        )\n+        yield Request(self.mockserver.url(\"/general_without\"), self.parse_general)\n+        yield Request(self.mockserver.url(\"/no_kwargs\"), self.parse_no_kwargs)\n+        yield Request(\n+            self.mockserver.url(\"/default\"), self.parse_default, cb_kwargs=data\n+        )\n+        yield Request(\n+            self.mockserver.url(\"/takes_less\"), self.parse_takes_less, cb_kwargs=data\n+        )\n+        yield Request(\n+            self.mockserver.url(\"/takes_more\"), self.parse_takes_more, cb_kwargs=data\n+        )\n+        yield Request(self.mockserver.url(\"/downloader_mw\"), self.parse_downloader_mw)\n+        yield Request(self.mockserver.url(\"/spider_mw\"), self.parse_spider_mw)\n \n     def parse_first(self, response, key, number):\n-        self.checks.append(key == 'value')\n+        self.checks.append(key == \"value\")\n         self.checks.append(number == 123)\n-        self.crawler.stats.inc_value('boolean_checks', 2)\n+        self.crawler.stats.inc_value(\"boolean_checks\", 2)\n         yield response.follow(\n-            self.mockserver.url('/two'),\n+            self.mockserver.url(\"/two\"),\n             self.parse_second,\n-            cb_kwargs={'new_key': 'new_value'})\n+            cb_kwargs={\"new_key\": \"new_value\"},\n+        )\n \n     def parse_second(self, response, new_key):\n-        self.checks.append(new_key == 'new_value')\n-        self.crawler.stats.inc_value('boolean_checks')\n+        self.checks.append(new_key == \"new_value\")\n+        self.crawler.stats.inc_value(\"boolean_checks\")\n \n     def parse_general(self, response, **kwargs):\n-        if response.url.endswith('/general_with'):\n-            self.checks.append(kwargs['key'] == 'value')\n-            self.checks.append(kwargs['number'] == 123)\n-            self.checks.append(kwargs['callback'] == 'some_callback')\n-            self.crawler.stats.inc_value('boolean_checks', 3)\n-        elif response.url.endswith('/general_without'):\n-            self.checks.append(kwargs == {})  # pylint: disable=use-implicit-booleaness-not-comparison\n-            self.crawler.stats.inc_value('boolean_checks')\n+        if response.url.endswith(\"/general_with\"):\n+            self.checks.append(kwargs[\"key\"] == \"value\")\n+            self.checks.append(kwargs[\"number\"] == 123)\n+            self.checks.append(kwargs[\"callback\"] == \"some_callback\")\n+            self.crawler.stats.inc_value(\"boolean_checks\", 3)\n+        elif response.url.endswith(\"/general_without\"):\n+            self.checks.append(\n+                kwargs == {}\n+            )  # pylint: disable=use-implicit-booleaness-not-comparison\n+            self.crawler.stats.inc_value(\"boolean_checks\")\n \n     def parse_no_kwargs(self, response):\n-        self.checks.append(response.url.endswith('/no_kwargs'))\n-        self.crawler.stats.inc_value('boolean_checks')\n+        self.checks.append(response.url.endswith(\"/no_kwargs\"))\n+        self.crawler.stats.inc_value(\"boolean_checks\")\n \n     def parse_default(self, response, key, number=None, default=99):\n-        self.checks.append(response.url.endswith('/default'))\n-        self.checks.append(key == 'value')\n+        self.checks.append(response.url.endswith(\"/default\"))\n+        self.checks.append(key == \"value\")\n         self.checks.append(number == 123)\n         self.checks.append(default == 99)\n-        self.crawler.stats.inc_value('boolean_checks', 4)\n+        self.crawler.stats.inc_value(\"boolean_checks\", 4)\n \n     def parse_takes_less(self, response, key, callback):\n         \"\"\"\n@@ -117,20 +133,24 @@ class KeywordArgumentsSpider(MockServerSpider):\n         TypeError: parse_takes_more() missing 1 required positional argument: 'other'\n         \"\"\"\n \n-    def parse_downloader_mw(self, response, from_process_request, from_process_response):\n+    def parse_downloader_mw(\n+        self, response, from_process_request, from_process_response\n+    ):\n         self.checks.append(bool(from_process_request))\n         self.checks.append(bool(from_process_response))\n-        self.crawler.stats.inc_value('boolean_checks', 2)\n+        self.crawler.stats.inc_value(\"boolean_checks\", 2)\n \n-    def parse_spider_mw(self, response, from_process_spider_input, from_process_start_requests):\n+    def parse_spider_mw(\n+        self, response, from_process_spider_input, from_process_start_requests\n+    ):\n         self.checks.append(bool(from_process_spider_input))\n         self.checks.append(bool(from_process_start_requests))\n-        self.crawler.stats.inc_value('boolean_checks', 2)\n-        return Request(self.mockserver.url('/spider_mw_2'), self.parse_spider_mw_2)\n+        self.crawler.stats.inc_value(\"boolean_checks\", 2)\n+        return Request(self.mockserver.url(\"/spider_mw_2\"), self.parse_spider_mw_2)\n \n     def parse_spider_mw_2(self, response, from_process_spider_output):\n         self.checks.append(bool(from_process_spider_output))\n-        self.crawler.stats.inc_value('boolean_checks', 1)\n+        self.crawler.stats.inc_value(\"boolean_checks\", 1)\n \n \n class CallbackKeywordArgumentsTestCase(TestCase):\n@@ -150,24 +170,26 @@ class CallbackKeywordArgumentsTestCase(TestCase):\n         with LogCapture() as log:\n             yield crawler.crawl(mockserver=self.mockserver)\n         self.assertTrue(all(crawler.spider.checks))\n-        self.assertEqual(len(crawler.spider.checks), crawler.stats.get_value('boolean_checks'))\n+        self.assertEqual(\n+            len(crawler.spider.checks), crawler.stats.get_value(\"boolean_checks\")\n+        )\n         # check exceptions for argument mismatch\n         exceptions = {}\n         for line in log.records:\n-            for key in ('takes_less', 'takes_more'):\n+            for key in (\"takes_less\", \"takes_more\"):\n                 if key in line.getMessage():\n                     exceptions[key] = line\n-        self.assertEqual(exceptions['takes_less'].exc_info[0], TypeError)\n+        self.assertEqual(exceptions[\"takes_less\"].exc_info[0], TypeError)\n         self.assertTrue(\n-            str(exceptions['takes_less'].exc_info[1]).endswith(\n+            str(exceptions[\"takes_less\"].exc_info[1]).endswith(\n                 \"parse_takes_less() got an unexpected keyword argument 'number'\"\n             ),\n-            msg=\"Exception message: \" + str(exceptions['takes_less'].exc_info[1]),\n+            msg=\"Exception message: \" + str(exceptions[\"takes_less\"].exc_info[1]),\n         )\n-        self.assertEqual(exceptions['takes_more'].exc_info[0], TypeError)\n+        self.assertEqual(exceptions[\"takes_more\"].exc_info[0], TypeError)\n         self.assertTrue(\n-            str(exceptions['takes_more'].exc_info[1]).endswith(\n+            str(exceptions[\"takes_more\"].exc_info[1]).endswith(\n                 \"parse_takes_more() missing 1 required positional argument: 'other'\"\n             ),\n-            msg=\"Exception message: \" + str(exceptions['takes_more'].exc_info[1]),\n+            msg=\"Exception message: \" + str(exceptions[\"takes_more\"].exc_info[1]),\n         )\n\n@@ -14,7 +14,6 @@ class CustomRequest(Request):\n \n \n class RequestSerializationTest(unittest.TestCase):\n-\n     def setUp(self):\n         self.spider = TestSpider()\n \n@@ -29,13 +28,13 @@ class RequestSerializationTest(unittest.TestCase):\n             errback=self.spider.handle_error,\n             method=\"POST\",\n             body=b\"some body\",\n-            headers={'content-encoding': 'text/html; charset=latin-1'},\n-            cookies={'currency': 'руб'},\n-            encoding='latin-1',\n+            headers={\"content-encoding\": \"text/html; charset=latin-1\"},\n+            cookies={\"currency\": \"руб\"},\n+            encoding=\"latin-1\",\n             priority=20,\n-            meta={'a': 'b'},\n-            cb_kwargs={'k': 'v'},\n-            flags=['testFlag'],\n+            meta={\"a\": \"b\"},\n+            cb_kwargs={\"k\": \"v\"},\n+            flags=[\"testFlag\"],\n         )\n         self._assert_serializes_ok(r, spider=self.spider)\n \n@@ -80,46 +79,57 @@ class RequestSerializationTest(unittest.TestCase):\n         self._assert_serializes_ok(r3, spider=self.spider)\n \n     def test_callback_serialization(self):\n-        r = Request(\"http://www.example.com\", callback=self.spider.parse_item,\n-                    errback=self.spider.handle_error)\n+        r = Request(\n+            \"http://www.example.com\",\n+            callback=self.spider.parse_item,\n+            errback=self.spider.handle_error,\n+        )\n         self._assert_serializes_ok(r, spider=self.spider)\n \n     def test_reference_callback_serialization(self):\n-        r = Request(\"http://www.example.com\",\n+        r = Request(\n+            \"http://www.example.com\",\n             callback=self.spider.parse_item_reference,\n-                    errback=self.spider.handle_error_reference)\n+            errback=self.spider.handle_error_reference,\n+        )\n         self._assert_serializes_ok(r, spider=self.spider)\n         request_dict = r.to_dict(spider=self.spider)\n-        self.assertEqual(request_dict['callback'], 'parse_item_reference')\n-        self.assertEqual(request_dict['errback'], 'handle_error_reference')\n+        self.assertEqual(request_dict[\"callback\"], \"parse_item_reference\")\n+        self.assertEqual(request_dict[\"errback\"], \"handle_error_reference\")\n \n     def test_private_reference_callback_serialization(self):\n-        r = Request(\"http://www.example.com\",\n+        r = Request(\n+            \"http://www.example.com\",\n             callback=self.spider._TestSpider__parse_item_reference,\n-                    errback=self.spider._TestSpider__handle_error_reference)\n+            errback=self.spider._TestSpider__handle_error_reference,\n+        )\n         self._assert_serializes_ok(r, spider=self.spider)\n         request_dict = r.to_dict(spider=self.spider)\n-        self.assertEqual(request_dict['callback'],\n-                         '_TestSpider__parse_item_reference')\n-        self.assertEqual(request_dict['errback'],\n-                         '_TestSpider__handle_error_reference')\n+        self.assertEqual(request_dict[\"callback\"], \"_TestSpider__parse_item_reference\")\n+        self.assertEqual(request_dict[\"errback\"], \"_TestSpider__handle_error_reference\")\n \n     def test_private_callback_serialization(self):\n-        r = Request(\"http://www.example.com\",\n+        r = Request(\n+            \"http://www.example.com\",\n             callback=self.spider._TestSpider__parse_item_private,\n-                    errback=self.spider.handle_error)\n+            errback=self.spider.handle_error,\n+        )\n         self._assert_serializes_ok(r, spider=self.spider)\n \n     def test_mixin_private_callback_serialization(self):\n-        r = Request(\"http://www.example.com\",\n+        r = Request(\n+            \"http://www.example.com\",\n             callback=self.spider._TestSpiderMixin__mixin_callback,\n-                    errback=self.spider.handle_error)\n+            errback=self.spider.handle_error,\n+        )\n         self._assert_serializes_ok(r, spider=self.spider)\n \n     def test_delegated_callback_serialization(self):\n-        r = Request(\"http://www.example.com\",\n+        r = Request(\n+            \"http://www.example.com\",\n             callback=self.spider.delegated_callback,\n-                    errback=self.spider.handle_error)\n+            errback=self.spider.handle_error,\n+        )\n         self._assert_serializes_ok(r, spider=self.spider)\n \n     def test_unserializable_callback1(self):\n@@ -134,14 +144,14 @@ class RequestSerializationTest(unittest.TestCase):\n         \"\"\"Parser method is removed or replaced dynamically.\"\"\"\n \n         class MySpider(Spider):\n-            name = 'my_spider'\n+            name = \"my_spider\"\n \n             def parse(self, response):\n                 pass\n \n         spider = MySpider()\n         r = Request(\"http://www.example.com\", callback=spider.parse)\n-        setattr(spider, 'parse', None)\n+        setattr(spider, \"parse\", None)\n         self.assertRaises(ValueError, r.to_dict, spider=spider)\n \n     def test_callback_not_available(self):\n@@ -157,9 +167,14 @@ class DeprecatedMethodsRequestSerializationTest(RequestSerializationTest):\n         with warnings.catch_warnings(record=True) as caught:\n             warnings.simplefilter(\"always\")\n             with suppress(KeyError):\n-                del sys.modules[\"scrapy.utils.reqser\"]  # delete module to reset the deprecation warning\n+                del sys.modules[\n+                    \"scrapy.utils.reqser\"\n+                ]  # delete module to reset the deprecation warning\n \n-            from scrapy.utils.reqser import request_from_dict as _from_dict, request_to_dict as _to_dict\n+            from scrapy.utils.reqser import (\n+                request_from_dict as _from_dict,\n+                request_to_dict as _to_dict,\n+            )\n \n             request_copy = _from_dict(_to_dict(request, spider), spider)\n             self._assert_same_request(request, request_copy)\n@@ -200,7 +215,7 @@ def private_handle_error(failure):\n \n \n class TestSpider(Spider, TestSpiderMixin):\n-    name = 'test'\n+    name = \"test\"\n     parse_item_reference = parse_item\n     handle_error_reference = handle_error\n     __parse_item_reference = private_parse_item\n\n@@ -7,12 +7,11 @@ from tests.mockserver import MockServer\n \n \n class SignalCatcherSpider(Spider):\n-    name = 'signal_catcher'\n+    name = \"signal_catcher\"\n \n     def __init__(self, crawler, url, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n-        crawler.signals.connect(self.on_request_left,\n-                                signal=request_left_downloader)\n+        crawler.signals.connect(self.on_request_left, signal=request_left_downloader)\n         self.caught_times = 0\n         self.start_urls = [url]\n \n@@ -26,7 +25,6 @@ class SignalCatcherSpider(Spider):\n \n \n class TestCatching(TestCase):\n-\n     def setUp(self):\n         self.mockserver = MockServer()\n         self.mockserver.__enter__()\n@@ -42,8 +40,7 @@ class TestCatching(TestCase):\n \n     @defer.inlineCallbacks\n     def test_timeout(self):\n-        crawler = get_crawler(SignalCatcherSpider,\n-                              {'DOWNLOAD_TIMEOUT': 0.1})\n+        crawler = get_crawler(SignalCatcherSpider, {\"DOWNLOAD_TIMEOUT\": 0.1})\n         yield crawler.crawl(self.mockserver.url(\"/delay?n=0.2\"))\n         self.assertEqual(crawler.spider.caught_times, 1)\n \n@@ -56,5 +53,5 @@ class TestCatching(TestCase):\n     @defer.inlineCallbacks\n     def test_noconnect(self):\n         crawler = get_crawler(SignalCatcherSpider)\n-        yield crawler.crawl('http://thereisdefinetelynosuchdomain.com')\n+        yield crawler.crawl(\"http://thereisdefinetelynosuchdomain.com\")\n         self.assertEqual(crawler.spider.caught_times, 1)\n\n@@ -5,15 +5,14 @@ from scrapy.http import Response, TextResponse, XmlResponse, HtmlResponse, Heade\n \n \n class ResponseTypesTest(unittest.TestCase):\n-\n     def test_from_filename(self):\n         mappings = [\n-            ('data.bin', Response),\n-            ('file.txt', TextResponse),\n-            ('file.xml.gz', Response),\n-            ('file.xml', XmlResponse),\n-            ('file.html', HtmlResponse),\n-            ('file.unknownext', Response),\n+            (\"data.bin\", Response),\n+            (\"file.txt\", TextResponse),\n+            (\"file.xml.gz\", Response),\n+            (\"file.xml\", XmlResponse),\n+            (\"file.html\", HtmlResponse),\n+            (\"file.unknownext\", Response),\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_filename(source)\n@@ -22,13 +21,12 @@ class ResponseTypesTest(unittest.TestCase):\n     def test_from_content_disposition(self):\n         mappings = [\n             (b'attachment; filename=\"data.xml\"', XmlResponse),\n-            (b'attachment; filename=data.xml', XmlResponse),\n-            ('attachment;filename=data£.tar.gz'.encode('utf-8'), Response),\n-            ('attachment;filename=dataµ.tar.gz'.encode('latin-1'), Response),\n-            ('attachment;filename=data高.doc'.encode('gbk'), Response),\n-            ('attachment;filename=دورهdata.html'.encode('cp720'), HtmlResponse),\n-            ('attachment;filename=日本語版Wikipedia.xml'.encode('iso2022_jp'), XmlResponse),\n-\n+            (b\"attachment; filename=data.xml\", XmlResponse),\n+            (\"attachment;filename=data£.tar.gz\".encode(\"utf-8\"), Response),\n+            (\"attachment;filename=dataµ.tar.gz\".encode(\"latin-1\"), Response),\n+            (\"attachment;filename=data高.doc\".encode(\"gbk\"), Response),\n+            (\"attachment;filename=دورهdata.html\".encode(\"cp720\"), HtmlResponse),\n+            (\"attachment;filename=日本語版Wikipedia.xml\".encode(\"iso2022_jp\"), XmlResponse),\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_content_disposition(source)\n@@ -36,14 +34,14 @@ class ResponseTypesTest(unittest.TestCase):\n \n     def test_from_content_type(self):\n         mappings = [\n-            ('text/html; charset=UTF-8', HtmlResponse),\n-            ('text/xml; charset=UTF-8', XmlResponse),\n-            ('application/xhtml+xml; charset=UTF-8', HtmlResponse),\n-            ('application/vnd.wap.xhtml+xml; charset=utf-8', HtmlResponse),\n-            ('application/xml; charset=UTF-8', XmlResponse),\n-            ('application/octet-stream', Response),\n-            ('application/x-json; encoding=UTF8;charset=UTF-8', TextResponse),\n-            ('application/json-amazonui-streaming;charset=UTF-8', TextResponse),\n+            (\"text/html; charset=UTF-8\", HtmlResponse),\n+            (\"text/xml; charset=UTF-8\", XmlResponse),\n+            (\"application/xhtml+xml; charset=UTF-8\", HtmlResponse),\n+            (\"application/vnd.wap.xhtml+xml; charset=utf-8\", HtmlResponse),\n+            (\"application/xml; charset=UTF-8\", XmlResponse),\n+            (\"application/octet-stream\", Response),\n+            (\"application/x-json; encoding=UTF8;charset=UTF-8\", TextResponse),\n+            (\"application/json-amazonui-streaming;charset=UTF-8\", TextResponse),\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_content_type(source)\n@@ -51,11 +49,11 @@ class ResponseTypesTest(unittest.TestCase):\n \n     def test_from_body(self):\n         mappings = [\n-            (b'\\x03\\x02\\xdf\\xdd\\x23', Response),\n-            (b'Some plain text\\ndata with tabs\\t and null bytes\\0', TextResponse),\n-            (b'<html><head><title>Hello</title></head>', HtmlResponse),\n+            (b\"\\x03\\x02\\xdf\\xdd\\x23\", Response),\n+            (b\"Some plain text\\ndata with tabs\\t and null bytes\\0\", TextResponse),\n+            (b\"<html><head><title>Hello</title></head>\", HtmlResponse),\n             # https://codersblock.com/blog/the-smallest-valid-html5-page/\n-            (b'<!DOCTYPE html>\\n<title>.</title>', HtmlResponse),\n+            (b\"<!DOCTYPE html>\\n<title>.</title>\", HtmlResponse),\n             (b'<?xml version=\"1.0\" encoding=\"utf-8\"', XmlResponse),\n         ]\n         for source, cls in mappings:\n@@ -64,10 +62,21 @@ class ResponseTypesTest(unittest.TestCase):\n \n     def test_from_headers(self):\n         mappings = [\n-            ({'Content-Type': ['text/html; charset=utf-8']}, HtmlResponse),\n-            ({'Content-Type': ['text/html; charset=utf-8'], 'Content-Encoding': ['gzip']}, Response),\n-            ({'Content-Type': ['application/octet-stream'],\n-              'Content-Disposition': ['attachment; filename=data.txt']}, TextResponse),\n+            ({\"Content-Type\": [\"text/html; charset=utf-8\"]}, HtmlResponse),\n+            (\n+                {\n+                    \"Content-Type\": [\"text/html; charset=utf-8\"],\n+                    \"Content-Encoding\": [\"gzip\"],\n+                },\n+                Response,\n+            ),\n+            (\n+                {\n+                    \"Content-Type\": [\"application/octet-stream\"],\n+                    \"Content-Disposition\": [\"attachment; filename=data.txt\"],\n+                },\n+                TextResponse,\n+            ),\n         ]\n         for source, cls in mappings:\n             source = Headers(source)\n@@ -77,14 +86,24 @@ class ResponseTypesTest(unittest.TestCase):\n     def test_from_args(self):\n         # TODO: add more tests that check precedence between the different arguments\n         mappings = [\n-            ({'url': 'http://www.example.com/data.csv'}, TextResponse),\n+            ({\"url\": \"http://www.example.com/data.csv\"}, TextResponse),\n             # headers takes precedence over url\n-            ({'headers': Headers({'Content-Type': ['text/html; charset=utf-8']}),\n-              'url': 'http://www.example.com/item/'}, HtmlResponse),\n-            ({'headers': Headers({'Content-Disposition': ['attachment; filename=\"data.xml.gz\"']}),\n-              'url': 'http://www.example.com/page/'}, Response),\n-\n-\n+            (\n+                {\n+                    \"headers\": Headers({\"Content-Type\": [\"text/html; charset=utf-8\"]}),\n+                    \"url\": \"http://www.example.com/item/\",\n+                },\n+                HtmlResponse,\n+            ),\n+            (\n+                {\n+                    \"headers\": Headers(\n+                        {\"Content-Disposition\": ['attachment; filename=\"data.xml.gz\"']}\n+                    ),\n+                    \"url\": \"http://www.example.com/page/\",\n+                },\n+                Response,\n+            ),\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_args(**source)\n@@ -92,7 +111,9 @@ class ResponseTypesTest(unittest.TestCase):\n \n     def test_custom_mime_types_loaded(self):\n         # check that mime.types files shipped with scrapy are loaded\n-        self.assertEqual(responsetypes.mimetypes.guess_type('x.scrapytest')[0], 'x-scrapy/test')\n+        self.assertEqual(\n+            responsetypes.mimetypes.guess_type(\"x.scrapytest\")[0], \"x-scrapy/test\"\n+        )\n \n \n if __name__ == \"__main__\":\n\n@@ -34,11 +34,15 @@ class BaseRobotParserTest:\n         self.parser_cls = parser_cls\n \n     def test_allowed(self):\n-        robotstxt_robotstxt_body = (\"User-agent: * \\n\"\n+        robotstxt_robotstxt_body = (\n+            \"User-agent: * \\n\"\n             \"Disallow: /disallowed \\n\"\n             \"Allow: /allowed \\n\"\n-                                    \"Crawl-delay: 10\".encode('utf-8'))\n-        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)\n+            \"Crawl-delay: 10\".encode(\"utf-8\")\n+        )\n+        rp = self.parser_cls.from_crawler(\n+            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n+        )\n         self.assertTrue(rp.allowed(\"https://www.site.local/allowed\", \"*\"))\n         self.assertFalse(rp.allowed(\"https://www.site.local/disallowed\", \"*\"))\n \n@@ -49,35 +53,49 @@ class BaseRobotParserTest:\n                                 User-agent: second\n                                 Allow: /*allowed\n                                 Disallow: /\n-                                \"\"\".encode('utf-8')\n-        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)\n+                                \"\"\".encode(\n+            \"utf-8\"\n+        )\n+        rp = self.parser_cls.from_crawler(\n+            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n+        )\n \n         self.assertTrue(rp.allowed(\"https://www.site.local/disallowed\", \"first\"))\n-        self.assertFalse(rp.allowed(\"https://www.site.local/disallowed/xyz/end\", \"first\"))\n-        self.assertFalse(rp.allowed(\"https://www.site.local/disallowed/abc/end\", \"first\"))\n-        self.assertTrue(rp.allowed(\"https://www.site.local/disallowed/xyz/endinglater\", \"first\"))\n+        self.assertFalse(\n+            rp.allowed(\"https://www.site.local/disallowed/xyz/end\", \"first\")\n+        )\n+        self.assertFalse(\n+            rp.allowed(\"https://www.site.local/disallowed/abc/end\", \"first\")\n+        )\n+        self.assertTrue(\n+            rp.allowed(\"https://www.site.local/disallowed/xyz/endinglater\", \"first\")\n+        )\n \n         self.assertTrue(rp.allowed(\"https://www.site.local/allowed\", \"second\"))\n         self.assertTrue(rp.allowed(\"https://www.site.local/is_still_allowed\", \"second\"))\n         self.assertTrue(rp.allowed(\"https://www.site.local/is_allowed_too\", \"second\"))\n \n     def test_length_based_precedence(self):\n-        robotstxt_robotstxt_body = (\"User-agent: * \\n\"\n-                                    \"Disallow: / \\n\"\n-                                    \"Allow: /page\".encode('utf-8'))\n-        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)\n+        robotstxt_robotstxt_body = (\n+            \"User-agent: * \\n\" \"Disallow: / \\n\" \"Allow: /page\".encode(\"utf-8\")\n+        )\n+        rp = self.parser_cls.from_crawler(\n+            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n+        )\n         self.assertTrue(rp.allowed(\"https://www.site.local/page\", \"*\"))\n \n     def test_order_based_precedence(self):\n-        robotstxt_robotstxt_body = (\"User-agent: * \\n\"\n-                                    \"Disallow: / \\n\"\n-                                    \"Allow: /page\".encode('utf-8'))\n-        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)\n+        robotstxt_robotstxt_body = (\n+            \"User-agent: * \\n\" \"Disallow: / \\n\" \"Allow: /page\".encode(\"utf-8\")\n+        )\n+        rp = self.parser_cls.from_crawler(\n+            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n+        )\n         self.assertFalse(rp.allowed(\"https://www.site.local/page\", \"*\"))\n \n     def test_empty_response(self):\n         \"\"\"empty response should equal 'allow all'\"\"\"\n-        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=b'')\n+        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=b\"\")\n         self.assertTrue(rp.allowed(\"https://site.local/\", \"*\"))\n         self.assertTrue(rp.allowed(\"https://site.local/\", \"chrome\"))\n         self.assertTrue(rp.allowed(\"https://site.local/index.html\", \"*\"))\n@@ -85,8 +103,10 @@ class BaseRobotParserTest:\n \n     def test_garbage_response(self):\n         \"\"\"garbage response should be discarded, equal 'allow all'\"\"\"\n-        robotstxt_robotstxt_body = b'GIF89a\\xd3\\x00\\xfe\\x00\\xa2'\n-        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)\n+        robotstxt_robotstxt_body = b\"GIF89a\\xd3\\x00\\xfe\\x00\\xa2\"\n+        rp = self.parser_cls.from_crawler(\n+            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n+        )\n         self.assertTrue(rp.allowed(\"https://site.local/\", \"*\"))\n         self.assertTrue(rp.allowed(\"https://site.local/\", \"chrome\"))\n         self.assertTrue(rp.allowed(\"https://site.local/index.html\", \"*\"))\n@@ -102,25 +122,36 @@ class BaseRobotParserTest:\n         Disallow: /wiki/Käyttäjä:\n \n         User-Agent: UnicödeBöt\n-        Disallow: /some/randome/page.html\"\"\".encode('utf-8')\n-        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)\n+        Disallow: /some/randome/page.html\"\"\".encode(\n+            \"utf-8\"\n+        )\n+        rp = self.parser_cls.from_crawler(\n+            crawler=None, robotstxt_body=robotstxt_robotstxt_body\n+        )\n         self.assertTrue(rp.allowed(\"https://site.local/\", \"*\"))\n         self.assertFalse(rp.allowed(\"https://site.local/admin/\", \"*\"))\n         self.assertFalse(rp.allowed(\"https://site.local/static/\", \"*\"))\n         self.assertTrue(rp.allowed(\"https://site.local/admin/\", \"UnicödeBöt\"))\n-        self.assertFalse(rp.allowed(\"https://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:\", \"*\"))\n+        self.assertFalse(\n+            rp.allowed(\"https://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:\", \"*\")\n+        )\n         self.assertFalse(rp.allowed(\"https://site.local/wiki/Käyttäjä:\", \"*\"))\n         self.assertTrue(rp.allowed(\"https://site.local/some/randome/page.html\", \"*\"))\n-        self.assertFalse(rp.allowed(\"https://site.local/some/randome/page.html\", \"UnicödeBöt\"))\n+        self.assertFalse(\n+            rp.allowed(\"https://site.local/some/randome/page.html\", \"UnicödeBöt\")\n+        )\n \n \n class PythonRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n     def setUp(self):\n         from scrapy.robotstxt import PythonRobotParser\n+\n         super()._setUp(PythonRobotParser)\n \n     def test_length_based_precedence(self):\n-        raise unittest.SkipTest(\"RobotFileParser does not support length based directives precedence.\")\n+        raise unittest.SkipTest(\n+            \"RobotFileParser does not support length based directives precedence.\"\n+        )\n \n     def test_allowed_wildcards(self):\n         raise unittest.SkipTest(\"RobotFileParser does not support wildcards.\")\n@@ -132,10 +163,13 @@ class ReppyRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n \n     def setUp(self):\n         from scrapy.robotstxt import ReppyRobotParser\n+\n         super()._setUp(ReppyRobotParser)\n \n     def test_order_based_precedence(self):\n-        raise unittest.SkipTest(\"Reppy does not support order based directives precedence.\")\n+        raise unittest.SkipTest(\n+            \"Reppy does not support order based directives precedence.\"\n+        )\n \n \n class RerpRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n@@ -144,10 +178,13 @@ class RerpRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n \n     def setUp(self):\n         from scrapy.robotstxt import RerpRobotParser\n+\n         super()._setUp(RerpRobotParser)\n \n     def test_length_based_precedence(self):\n-        raise unittest.SkipTest(\"Rerp does not support length based directives precedence.\")\n+        raise unittest.SkipTest(\n+            \"Rerp does not support length based directives precedence.\"\n+        )\n \n \n class ProtegoRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n@@ -156,7 +193,10 @@ class ProtegoRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n \n     def setUp(self):\n         from scrapy.robotstxt import ProtegoRobotParser\n+\n         super()._setUp(ProtegoRobotParser)\n \n     def test_order_based_precedence(self):\n-        raise unittest.SkipTest(\"Protego does not support order based directives precedence.\")\n+        raise unittest.SkipTest(\n+            \"Protego does not support order based directives precedence.\"\n+        )\n\n@@ -16,8 +16,8 @@ from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n \n \n-MockEngine = collections.namedtuple('MockEngine', ['downloader'])\n-MockSlot = collections.namedtuple('MockSlot', ['active'])\n+MockEngine = collections.namedtuple(\"MockEngine\", [\"downloader\"])\n+MockSlot = collections.namedtuple(\"MockSlot\", [\"active\"])\n \n \n class MockDownloader:\n@@ -28,7 +28,7 @@ class MockDownloader:\n         if Downloader.DOWNLOAD_SLOT in request.meta:\n             return request.meta[Downloader.DOWNLOAD_SLOT]\n \n-        return urlparse_cached(request).hostname or ''\n+        return urlparse_cached(request).hostname or \"\"\n \n     def increment(self, slot_key):\n         slot = self.slots.setdefault(slot_key, MockSlot(active=[]))\n@@ -47,12 +47,12 @@ class MockCrawler(Crawler):\n \n         settings = dict(\n             SCHEDULER_DEBUG=False,\n-            SCHEDULER_DISK_QUEUE='scrapy.squeues.PickleLifoDiskQueue',\n-            SCHEDULER_MEMORY_QUEUE='scrapy.squeues.LifoMemoryQueue',\n+            SCHEDULER_DISK_QUEUE=\"scrapy.squeues.PickleLifoDiskQueue\",\n+            SCHEDULER_MEMORY_QUEUE=\"scrapy.squeues.LifoMemoryQueue\",\n             SCHEDULER_PRIORITY_QUEUE=priority_queue_cls,\n             JOBDIR=jobdir,\n-            DUPEFILTER_CLASS='scrapy.dupefilters.BaseDupeFilter',\n-            REQUEST_FINGERPRINTER_IMPLEMENTATION='2.7',\n+            DUPEFILTER_CLASS=\"scrapy.dupefilters.BaseDupeFilter\",\n+            REQUEST_FINGERPRINTER_IMPLEMENTATION=\"2.7\",\n         )\n         super().__init__(Spider, settings)\n         self.engine = MockEngine(downloader=MockDownloader())\n@@ -65,11 +65,11 @@ class SchedulerHandler:\n     def create_scheduler(self):\n         self.mock_crawler = MockCrawler(self.priority_queue_cls, self.jobdir)\n         self.scheduler = Scheduler.from_crawler(self.mock_crawler)\n-        self.spider = Spider(name='spider')\n+        self.spider = Spider(name=\"spider\")\n         self.scheduler.open(self.spider)\n \n     def close_scheduler(self):\n-        self.scheduler.close('finished')\n+        self.scheduler.close(\"finished\")\n         self.mock_crawler.stop()\n         self.mock_crawler.engine.downloader.close()\n \n@@ -80,11 +80,13 @@ class SchedulerHandler:\n         self.close_scheduler()\n \n \n-_PRIORITIES = [(\"http://foo.com/a\", -2),\n+_PRIORITIES = [\n+    (\"http://foo.com/a\", -2),\n     (\"http://foo.com/d\", 1),\n     (\"http://foo.com/b\", -1),\n     (\"http://foo.com/c\", 0),\n-               (\"http://foo.com/e\", 2)]\n+    (\"http://foo.com/e\", 2),\n+]\n \n \n _URLS = {\"http://foo.com/a\", \"http://foo.com/b\", \"http://foo.com/c\"}\n@@ -119,12 +121,12 @@ class BaseSchedulerInMemoryTester(SchedulerHandler):\n         while self.scheduler.has_pending_requests():\n             priorities.append(self.scheduler.next_request().priority)\n \n-        self.assertEqual(priorities,\n-                         sorted([x[1] for x in _PRIORITIES], key=lambda x: -x))\n+        self.assertEqual(\n+            priorities, sorted([x[1] for x in _PRIORITIES], key=lambda x: -x)\n+        )\n \n \n class BaseSchedulerOnDiskTester(SchedulerHandler):\n-\n     def setUp(self):\n         self.jobdir = tempfile.mkdtemp()\n         self.create_scheduler()\n@@ -172,28 +174,30 @@ class BaseSchedulerOnDiskTester(SchedulerHandler):\n         while self.scheduler.has_pending_requests():\n             priorities.append(self.scheduler.next_request().priority)\n \n-        self.assertEqual(priorities,\n-                         sorted([x[1] for x in _PRIORITIES], key=lambda x: -x))\n+        self.assertEqual(\n+            priorities, sorted([x[1] for x in _PRIORITIES], key=lambda x: -x)\n+        )\n \n \n class TestSchedulerInMemory(BaseSchedulerInMemoryTester, unittest.TestCase):\n-    priority_queue_cls = 'scrapy.pqueues.ScrapyPriorityQueue'\n+    priority_queue_cls = \"scrapy.pqueues.ScrapyPriorityQueue\"\n \n \n class TestSchedulerOnDisk(BaseSchedulerOnDiskTester, unittest.TestCase):\n-    priority_queue_cls = 'scrapy.pqueues.ScrapyPriorityQueue'\n+    priority_queue_cls = \"scrapy.pqueues.ScrapyPriorityQueue\"\n \n \n-_URLS_WITH_SLOTS = [(\"http://foo.com/a\", 'a'),\n-                    (\"http://foo.com/b\", 'a'),\n-                    (\"http://foo.com/c\", 'b'),\n-                    (\"http://foo.com/d\", 'b'),\n-                    (\"http://foo.com/e\", 'c'),\n-                    (\"http://foo.com/f\", 'c')]\n+_URLS_WITH_SLOTS = [\n+    (\"http://foo.com/a\", \"a\"),\n+    (\"http://foo.com/b\", \"a\"),\n+    (\"http://foo.com/c\", \"b\"),\n+    (\"http://foo.com/d\", \"b\"),\n+    (\"http://foo.com/e\", \"c\"),\n+    (\"http://foo.com/f\", \"c\"),\n+]\n \n \n class TestMigration(unittest.TestCase):\n-\n     def setUp(self):\n         self.tmpdir = tempfile.mkdtemp()\n \n@@ -202,7 +206,7 @@ class TestMigration(unittest.TestCase):\n \n     def _migration(self, tmp_dir):\n         prev_scheduler_handler = SchedulerHandler()\n-        prev_scheduler_handler.priority_queue_cls = 'scrapy.pqueues.ScrapyPriorityQueue'\n+        prev_scheduler_handler.priority_queue_cls = \"scrapy.pqueues.ScrapyPriorityQueue\"\n         prev_scheduler_handler.jobdir = tmp_dir\n \n         prev_scheduler_handler.create_scheduler()\n@@ -211,7 +215,9 @@ class TestMigration(unittest.TestCase):\n         prev_scheduler_handler.close_scheduler()\n \n         next_scheduler_handler = SchedulerHandler()\n-        next_scheduler_handler.priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n+        next_scheduler_handler.priority_queue_cls = (\n+            \"scrapy.pqueues.DownloaderAwarePriorityQueue\"\n+        )\n         next_scheduler_handler.jobdir = tmp_dir\n \n         next_scheduler_handler.create_scheduler()\n@@ -247,7 +253,7 @@ def _is_scheduling_fair(enqueued_slots, dequeued_slots):\n \n \n class DownloaderAwareSchedulerTestMixin:\n-    priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n+    priority_queue_cls = \"scrapy.pqueues.DownloaderAwarePriorityQueue\"\n     reopen = False\n \n     def test_logic(self):\n@@ -276,28 +282,28 @@ class DownloaderAwareSchedulerTestMixin:\n             slot = downloader._get_slot_key(request, None)\n             downloader.decrement(slot)\n \n-        self.assertTrue(_is_scheduling_fair(list(s for u, s in _URLS_WITH_SLOTS),\n-                                            dequeued_slots))\n+        self.assertTrue(\n+            _is_scheduling_fair(list(s for u, s in _URLS_WITH_SLOTS), dequeued_slots)\n+        )\n         self.assertEqual(sum(len(s.active) for s in downloader.slots.values()), 0)\n \n \n-class TestSchedulerWithDownloaderAwareInMemory(DownloaderAwareSchedulerTestMixin,\n-                                               BaseSchedulerInMemoryTester,\n-                                               unittest.TestCase):\n+class TestSchedulerWithDownloaderAwareInMemory(\n+    DownloaderAwareSchedulerTestMixin, BaseSchedulerInMemoryTester, unittest.TestCase\n+):\n     pass\n \n \n-class TestSchedulerWithDownloaderAwareOnDisk(DownloaderAwareSchedulerTestMixin,\n-                                             BaseSchedulerOnDiskTester,\n-                                             unittest.TestCase):\n+class TestSchedulerWithDownloaderAwareOnDisk(\n+    DownloaderAwareSchedulerTestMixin, BaseSchedulerOnDiskTester, unittest.TestCase\n+):\n     reopen = True\n \n \n class StartUrlsSpider(Spider):\n-\n     def __init__(self, start_urls):\n         self.start_urls = start_urls\n-        super().__init__(name='StartUrlsSpider')\n+        super().__init__(name=\"StartUrlsSpider\")\n \n     def parse(self, response):\n         pass\n@@ -308,8 +314,8 @@ class TestIntegrationWithDownloaderAwareInMemory(TestCase):\n         self.crawler = get_crawler(\n             spidercls=StartUrlsSpider,\n             settings_dict={\n-                'SCHEDULER_PRIORITY_QUEUE': 'scrapy.pqueues.DownloaderAwarePriorityQueue',\n-                'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n+                \"SCHEDULER_PRIORITY_QUEUE\": \"scrapy.pqueues.DownloaderAwarePriorityQueue\",\n+                \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n             },\n         )\n \n@@ -324,20 +330,21 @@ class TestIntegrationWithDownloaderAwareInMemory(TestCase):\n             url = mockserver.url(\"/status?n=200\", is_secure=False)\n             start_urls = [url] * 6\n             yield self.crawler.crawl(start_urls)\n-            self.assertEqual(self.crawler.stats.get_value('downloader/response_count'),\n-                             len(start_urls))\n+            self.assertEqual(\n+                self.crawler.stats.get_value(\"downloader/response_count\"),\n+                len(start_urls),\n+            )\n \n \n class TestIncompatibility(unittest.TestCase):\n-\n     def _incompatible(self):\n         settings = dict(\n-            SCHEDULER_PRIORITY_QUEUE='scrapy.pqueues.DownloaderAwarePriorityQueue',\n+            SCHEDULER_PRIORITY_QUEUE=\"scrapy.pqueues.DownloaderAwarePriorityQueue\",\n             CONCURRENT_REQUESTS_PER_IP=1,\n         )\n         crawler = get_crawler(Spider, settings)\n         scheduler = Scheduler.from_crawler(crawler)\n-        spider = Spider(name='spider')\n+        spider = Spider(name=\"spider\")\n         scheduler.open(spider)\n \n     def test_incompatibility(self):\n\n@@ -74,7 +74,11 @@ class BaseSchedulerTest(TestCase, InterfaceCheckMixin):\n         self.assertIsNone(self.scheduler.open(Spider(\"foo\")))\n         self.assertIsNone(self.scheduler.close(\"finished\"))\n         self.assertRaises(NotImplementedError, self.scheduler.has_pending_requests)\n-        self.assertRaises(NotImplementedError, self.scheduler.enqueue_request, Request(\"https://example.org\"))\n+        self.assertRaises(\n+            NotImplementedError,\n+            self.scheduler.enqueue_request,\n+            Request(\"https://example.org\"),\n+        )\n         self.assertRaises(NotImplementedError, self.scheduler.next_request)\n \n \n\n@@ -7,89 +7,104 @@ from scrapy.selector import Selector\n \n \n class SelectorTestCase(unittest.TestCase):\n-\n     def test_simple_selection(self):\n         \"\"\"Simple selector tests\"\"\"\n         body = b\"<p><input name='a'value='1'/><input name='b'value='2'/></p>\"\n-        response = TextResponse(url=\"http://example.com\", body=body, encoding='utf-8')\n+        response = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n         sel = Selector(response)\n \n-        xl = sel.xpath('//input')\n+        xl = sel.xpath(\"//input\")\n         self.assertEqual(2, len(xl))\n         for x in xl:\n             assert isinstance(x, Selector)\n \n         self.assertEqual(\n-            sel.xpath('//input').getall(),\n-            [x.get() for x in sel.xpath('//input')]\n+            sel.xpath(\"//input\").getall(), [x.get() for x in sel.xpath(\"//input\")]\n         )\n         self.assertEqual(\n-            [x.get() for x in sel.xpath(\"//input[@name='a']/@name\")],\n-            ['a']\n+            [x.get() for x in sel.xpath(\"//input[@name='a']/@name\")], [\"a\"]\n         )\n         self.assertEqual(\n-            [x.get() for x in sel.xpath(\"number(concat(//input[@name='a']/@value, //input[@name='b']/@value))\")],\n-            ['12.0']\n+            [\n+                x.get()\n+                for x in sel.xpath(\n+                    \"number(concat(//input[@name='a']/@value, //input[@name='b']/@value))\"\n                 )\n-        self.assertEqual(\n-            sel.xpath(\"concat('xpath', 'rules')\").getall(),\n-            ['xpathrules']\n+            ],\n+            [\"12.0\"],\n         )\n+        self.assertEqual(sel.xpath(\"concat('xpath', 'rules')\").getall(), [\"xpathrules\"])\n         self.assertEqual(\n-            [x.get() for x in sel.xpath(\"concat(//input[@name='a']/@value, //input[@name='b']/@value)\")],\n-            ['12']\n+            [\n+                x.get()\n+                for x in sel.xpath(\n+                    \"concat(//input[@name='a']/@value, //input[@name='b']/@value)\"\n+                )\n+            ],\n+            [\"12\"],\n         )\n \n     def test_root_base_url(self):\n         body = b'<html><form action=\"/path\"><input name=\"a\" /></form></html>'\n         url = \"http://example.com\"\n-        response = TextResponse(url=url, body=body, encoding='utf-8')\n+        response = TextResponse(url=url, body=body, encoding=\"utf-8\")\n         sel = Selector(response)\n         self.assertEqual(url, sel.root.base)\n \n     def test_flavor_detection(self):\n         text = b'<div><img src=\"a.jpg\"><p>Hello</div>'\n-        sel = Selector(XmlResponse('http://example.com', body=text, encoding='utf-8'))\n-        self.assertEqual(sel.type, 'xml')\n-        self.assertEqual(sel.xpath(\"//div\").getall(),\n-                         ['<div><img src=\"a.jpg\"><p>Hello</p></img></div>'])\n+        sel = Selector(XmlResponse(\"http://example.com\", body=text, encoding=\"utf-8\"))\n+        self.assertEqual(sel.type, \"xml\")\n+        self.assertEqual(\n+            sel.xpath(\"//div\").getall(),\n+            ['<div><img src=\"a.jpg\"><p>Hello</p></img></div>'],\n+        )\n \n-        sel = Selector(HtmlResponse('http://example.com', body=text, encoding='utf-8'))\n-        self.assertEqual(sel.type, 'html')\n-        self.assertEqual(sel.xpath(\"//div\").getall(),\n-                         ['<div><img src=\"a.jpg\"><p>Hello</p></div>'])\n+        sel = Selector(HtmlResponse(\"http://example.com\", body=text, encoding=\"utf-8\"))\n+        self.assertEqual(sel.type, \"html\")\n+        self.assertEqual(\n+            sel.xpath(\"//div\").getall(), ['<div><img src=\"a.jpg\"><p>Hello</p></div>']\n+        )\n \n     def test_http_header_encoding_precedence(self):\n         # '\\xa3'     = pound symbol in unicode\n         # '\\xc2\\xa3' = pound symbol in utf-8\n         # '\\xa3'     = pound symbol in latin-1 (iso-8859-1)\n \n-        meta = '<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">'\n-        head = '<head>' + meta + '</head>'\n+        meta = (\n+            '<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">'\n+        )\n+        head = \"<head>\" + meta + \"</head>\"\n         body_content = '<span id=\"blank\">\\xa3</span>'\n-        body = '<body>' + body_content + '</body>'\n-        html = '<html>' + head + body + '</html>'\n-        encoding = 'utf-8'\n+        body = \"<body>\" + body_content + \"</body>\"\n+        html = \"<html>\" + head + body + \"</html>\"\n+        encoding = \"utf-8\"\n         html_utf8 = html.encode(encoding)\n \n-        headers = {'Content-Type': ['text/html; charset=utf-8']}\n-        response = HtmlResponse(url=\"http://example.com\", headers=headers, body=html_utf8)\n+        headers = {\"Content-Type\": [\"text/html; charset=utf-8\"]}\n+        response = HtmlResponse(\n+            url=\"http://example.com\", headers=headers, body=html_utf8\n+        )\n         x = Selector(response)\n-        self.assertEqual(x.xpath(\"//span[@id='blank']/text()\").getall(), ['\\xa3'])\n+        self.assertEqual(x.xpath(\"//span[@id='blank']/text()\").getall(), [\"\\xa3\"])\n \n     def test_badly_encoded_body(self):\n         # \\xe9 alone isn't valid utf8 sequence\n-        r1 = TextResponse('http://www.example.com',\n-                          body=b'<html><p>an Jos\\xe9 de</p><html>',\n-                          encoding='utf-8')\n-        Selector(r1).xpath('//text()').getall()\n+        r1 = TextResponse(\n+            \"http://www.example.com\",\n+            body=b\"<html><p>an Jos\\xe9 de</p><html>\",\n+            encoding=\"utf-8\",\n+        )\n+        Selector(r1).xpath(\"//text()\").getall()\n \n     def test_weakref_slots(self):\n         \"\"\"Check that classes are using slots and are weak-referenceable\"\"\"\n-        x = Selector(text='')\n+        x = Selector(text=\"\")\n         weakref.ref(x)\n-        assert not hasattr(x, '__dict__'), f\"{x.__class__.__name__} does not use __slots__\"\n+        assert not hasattr(\n+            x, \"__dict__\"\n+        ), f\"{x.__class__.__name__} does not use __slots__\"\n \n     def test_selector_bad_args(self):\n-        with self.assertRaisesRegex(ValueError, 'received both response and text'):\n-            Selector(TextResponse(url='http://example.com', body=b''), text='')\n+        with self.assertRaisesRegex(ValueError, \"received both response and text\"):\n+            Selector(TextResponse(url=\"http://example.com\", body=b\"\"), text=\"\")\n\n@@ -1,13 +1,17 @@\n import unittest\n from unittest import mock\n \n-from scrapy.settings import (BaseSettings, Settings, SettingsAttribute,\n-                             SETTINGS_PRIORITIES, get_settings_priority)\n+from scrapy.settings import (\n+    BaseSettings,\n+    Settings,\n+    SettingsAttribute,\n+    SETTINGS_PRIORITIES,\n+    get_settings_priority,\n+)\n from . import default_settings\n \n \n class SettingsGlobalFuncsTest(unittest.TestCase):\n-\n     def test_get_settings_priority(self):\n         for prio_str, prio_num in SETTINGS_PRIORITIES.items():\n             self.assertEqual(get_settings_priority(prio_str), prio_num)\n@@ -15,127 +19,130 @@ class SettingsGlobalFuncsTest(unittest.TestCase):\n \n \n class SettingsAttributeTest(unittest.TestCase):\n-\n     def setUp(self):\n-        self.attribute = SettingsAttribute('value', 10)\n+        self.attribute = SettingsAttribute(\"value\", 10)\n \n     def test_set_greater_priority(self):\n-        self.attribute.set('value2', 20)\n-        self.assertEqual(self.attribute.value, 'value2')\n+        self.attribute.set(\"value2\", 20)\n+        self.assertEqual(self.attribute.value, \"value2\")\n         self.assertEqual(self.attribute.priority, 20)\n \n     def test_set_equal_priority(self):\n-        self.attribute.set('value2', 10)\n-        self.assertEqual(self.attribute.value, 'value2')\n+        self.attribute.set(\"value2\", 10)\n+        self.assertEqual(self.attribute.value, \"value2\")\n         self.assertEqual(self.attribute.priority, 10)\n \n     def test_set_less_priority(self):\n-        self.attribute.set('value2', 0)\n-        self.assertEqual(self.attribute.value, 'value')\n+        self.attribute.set(\"value2\", 0)\n+        self.assertEqual(self.attribute.value, \"value\")\n         self.assertEqual(self.attribute.priority, 10)\n \n     def test_overwrite_basesettings(self):\n-        original_dict = {'one': 10, 'two': 20}\n+        original_dict = {\"one\": 10, \"two\": 20}\n         original_settings = BaseSettings(original_dict, 0)\n         attribute = SettingsAttribute(original_settings, 0)\n \n-        new_dict = {'three': 11, 'four': 21}\n+        new_dict = {\"three\": 11, \"four\": 21}\n         attribute.set(new_dict, 10)\n         self.assertIsInstance(attribute.value, BaseSettings)\n         self.assertCountEqual(attribute.value, new_dict)\n         self.assertCountEqual(original_settings, original_dict)\n \n-        new_settings = BaseSettings({'five': 12}, 0)\n+        new_settings = BaseSettings({\"five\": 12}, 0)\n         attribute.set(new_settings, 0)  # Insufficient priority\n         self.assertCountEqual(attribute.value, new_dict)\n         attribute.set(new_settings, 10)\n         self.assertCountEqual(attribute.value, new_settings)\n \n     def test_repr(self):\n-        self.assertEqual(repr(self.attribute),\n-                         \"<SettingsAttribute value='value' priority=10>\")\n+        self.assertEqual(\n+            repr(self.attribute), \"<SettingsAttribute value='value' priority=10>\"\n+        )\n \n \n class BaseSettingsTest(unittest.TestCase):\n-\n     def setUp(self):\n         self.settings = BaseSettings()\n \n     def test_set_new_attribute(self):\n-        self.settings.set('TEST_OPTION', 'value', 0)\n-        self.assertIn('TEST_OPTION', self.settings.attributes)\n+        self.settings.set(\"TEST_OPTION\", \"value\", 0)\n+        self.assertIn(\"TEST_OPTION\", self.settings.attributes)\n \n-        attr = self.settings.attributes['TEST_OPTION']\n+        attr = self.settings.attributes[\"TEST_OPTION\"]\n         self.assertIsInstance(attr, SettingsAttribute)\n-        self.assertEqual(attr.value, 'value')\n+        self.assertEqual(attr.value, \"value\")\n         self.assertEqual(attr.priority, 0)\n \n     def test_set_settingsattribute(self):\n         myattr = SettingsAttribute(0, 30)  # Note priority 30\n-        self.settings.set('TEST_ATTR', myattr, 10)\n-        self.assertEqual(self.settings.get('TEST_ATTR'), 0)\n-        self.assertEqual(self.settings.getpriority('TEST_ATTR'), 30)\n+        self.settings.set(\"TEST_ATTR\", myattr, 10)\n+        self.assertEqual(self.settings.get(\"TEST_ATTR\"), 0)\n+        self.assertEqual(self.settings.getpriority(\"TEST_ATTR\"), 30)\n \n     def test_set_instance_identity_on_update(self):\n-        attr = SettingsAttribute('value', 0)\n-        self.settings.attributes = {'TEST_OPTION': attr}\n-        self.settings.set('TEST_OPTION', 'othervalue', 10)\n+        attr = SettingsAttribute(\"value\", 0)\n+        self.settings.attributes = {\"TEST_OPTION\": attr}\n+        self.settings.set(\"TEST_OPTION\", \"othervalue\", 10)\n \n-        self.assertIn('TEST_OPTION', self.settings.attributes)\n-        self.assertIs(attr, self.settings.attributes['TEST_OPTION'])\n+        self.assertIn(\"TEST_OPTION\", self.settings.attributes)\n+        self.assertIs(attr, self.settings.attributes[\"TEST_OPTION\"])\n \n     def test_set_calls_settings_attributes_methods_on_update(self):\n-        attr = SettingsAttribute('value', 10)\n-        with mock.patch.object(attr, '__setattr__') as mock_setattr, mock.patch.object(attr, 'set') as mock_set:\n+        attr = SettingsAttribute(\"value\", 10)\n+        with mock.patch.object(attr, \"__setattr__\") as mock_setattr, mock.patch.object(\n+            attr, \"set\"\n+        ) as mock_set:\n \n-            self.settings.attributes = {'TEST_OPTION': attr}\n+            self.settings.attributes = {\"TEST_OPTION\": attr}\n \n             for priority in (0, 10, 20):\n-                self.settings.set('TEST_OPTION', 'othervalue', priority)\n-                mock_set.assert_called_once_with('othervalue', priority)\n+                self.settings.set(\"TEST_OPTION\", \"othervalue\", priority)\n+                mock_set.assert_called_once_with(\"othervalue\", priority)\n                 self.assertFalse(mock_setattr.called)\n                 mock_set.reset_mock()\n                 mock_setattr.reset_mock()\n \n     def test_setitem(self):\n         settings = BaseSettings()\n-        settings.set('key', 'a', 'default')\n-        settings['key'] = 'b'\n-        self.assertEqual(settings['key'], 'b')\n-        self.assertEqual(settings.getpriority('key'), 20)\n-        settings['key'] = 'c'\n-        self.assertEqual(settings['key'], 'c')\n-        settings['key2'] = 'x'\n-        self.assertIn('key2', settings)\n-        self.assertEqual(settings['key2'], 'x')\n-        self.assertEqual(settings.getpriority('key2'), 20)\n+        settings.set(\"key\", \"a\", \"default\")\n+        settings[\"key\"] = \"b\"\n+        self.assertEqual(settings[\"key\"], \"b\")\n+        self.assertEqual(settings.getpriority(\"key\"), 20)\n+        settings[\"key\"] = \"c\"\n+        self.assertEqual(settings[\"key\"], \"c\")\n+        settings[\"key2\"] = \"x\"\n+        self.assertIn(\"key2\", settings)\n+        self.assertEqual(settings[\"key2\"], \"x\")\n+        self.assertEqual(settings.getpriority(\"key2\"), 20)\n \n     def test_setdict_alias(self):\n-        with mock.patch.object(self.settings, 'set') as mock_set:\n-            self.settings.setdict({'TEST_1': 'value1', 'TEST_2': 'value2'}, 10)\n+        with mock.patch.object(self.settings, \"set\") as mock_set:\n+            self.settings.setdict({\"TEST_1\": \"value1\", \"TEST_2\": \"value2\"}, 10)\n             self.assertEqual(mock_set.call_count, 2)\n-            calls = [mock.call('TEST_1', 'value1', 10),\n-                     mock.call('TEST_2', 'value2', 10)]\n+            calls = [\n+                mock.call(\"TEST_1\", \"value1\", 10),\n+                mock.call(\"TEST_2\", \"value2\", 10),\n+            ]\n             mock_set.assert_has_calls(calls, any_order=True)\n \n     def test_setmodule_only_load_uppercase_vars(self):\n-        class ModuleMock():\n-            UPPERCASE_VAR = 'value'\n-            MIXEDcase_VAR = 'othervalue'\n-            lowercase_var = 'anothervalue'\n+        class ModuleMock:\n+            UPPERCASE_VAR = \"value\"\n+            MIXEDcase_VAR = \"othervalue\"\n+            lowercase_var = \"anothervalue\"\n \n         self.settings.attributes = {}\n         self.settings.setmodule(ModuleMock(), 10)\n-        self.assertIn('UPPERCASE_VAR', self.settings.attributes)\n-        self.assertNotIn('MIXEDcase_VAR', self.settings.attributes)\n-        self.assertNotIn('lowercase_var', self.settings.attributes)\n+        self.assertIn(\"UPPERCASE_VAR\", self.settings.attributes)\n+        self.assertNotIn(\"MIXEDcase_VAR\", self.settings.attributes)\n+        self.assertNotIn(\"lowercase_var\", self.settings.attributes)\n         self.assertEqual(len(self.settings.attributes), 1)\n \n     def test_setmodule_alias(self):\n-        with mock.patch.object(self.settings, 'set') as mock_set:\n+        with mock.patch.object(self.settings, \"set\") as mock_set:\n             self.settings.setmodule(default_settings, 10)\n-            mock_set.assert_any_call('TEST_DEFAULT', 'defvalue', 10)\n-            mock_set.assert_any_call('TEST_DICT', {'key': 'val'}, 10)\n+            mock_set.assert_any_call(\"TEST_DEFAULT\", \"defvalue\", 10)\n+            mock_set.assert_any_call(\"TEST_DICT\", {\"key\": \"val\"}, 10)\n \n     def test_setmodule_by_path(self):\n         self.settings.attributes = {}\n@@ -143,11 +150,9 @@ class BaseSettingsTest(unittest.TestCase):\n         ctrl_attributes = self.settings.attributes.copy()\n \n         self.settings.attributes = {}\n-        self.settings.setmodule(\n-            'tests.test_settings.default_settings', 10)\n+        self.settings.setmodule(\"tests.test_settings.default_settings\", 10)\n \n-        self.assertCountEqual(self.settings.attributes.keys(),\n-                              ctrl_attributes.keys())\n+        self.assertCountEqual(self.settings.attributes.keys(), ctrl_attributes.keys())\n \n         for key in ctrl_attributes.keys():\n             attr = self.settings.attributes[key]\n@@ -156,181 +161,196 @@ class BaseSettingsTest(unittest.TestCase):\n             self.assertEqual(attr.priority, ctrl_attr.priority)\n \n     def test_update(self):\n-        settings = BaseSettings({'key_lowprio': 0}, priority=0)\n-        settings.set('key_highprio', 10, priority=50)\n-        custom_settings = BaseSettings({'key_lowprio': 1, 'key_highprio': 11},\n-                                       priority=30)\n-        custom_settings.set('newkey_one', None, priority=50)\n-        custom_dict = {'key_lowprio': 2, 'key_highprio': 12, 'newkey_two': None}\n+        settings = BaseSettings({\"key_lowprio\": 0}, priority=0)\n+        settings.set(\"key_highprio\", 10, priority=50)\n+        custom_settings = BaseSettings(\n+            {\"key_lowprio\": 1, \"key_highprio\": 11}, priority=30\n+        )\n+        custom_settings.set(\"newkey_one\", None, priority=50)\n+        custom_dict = {\"key_lowprio\": 2, \"key_highprio\": 12, \"newkey_two\": None}\n \n         settings.update(custom_dict, priority=20)\n-        self.assertEqual(settings['key_lowprio'], 2)\n-        self.assertEqual(settings.getpriority('key_lowprio'), 20)\n-        self.assertEqual(settings['key_highprio'], 10)\n-        self.assertIn('newkey_two', settings)\n-        self.assertEqual(settings.getpriority('newkey_two'), 20)\n+        self.assertEqual(settings[\"key_lowprio\"], 2)\n+        self.assertEqual(settings.getpriority(\"key_lowprio\"), 20)\n+        self.assertEqual(settings[\"key_highprio\"], 10)\n+        self.assertIn(\"newkey_two\", settings)\n+        self.assertEqual(settings.getpriority(\"newkey_two\"), 20)\n \n         settings.update(custom_settings)\n-        self.assertEqual(settings['key_lowprio'], 1)\n-        self.assertEqual(settings.getpriority('key_lowprio'), 30)\n-        self.assertEqual(settings['key_highprio'], 10)\n-        self.assertIn('newkey_one', settings)\n-        self.assertEqual(settings.getpriority('newkey_one'), 50)\n+        self.assertEqual(settings[\"key_lowprio\"], 1)\n+        self.assertEqual(settings.getpriority(\"key_lowprio\"), 30)\n+        self.assertEqual(settings[\"key_highprio\"], 10)\n+        self.assertIn(\"newkey_one\", settings)\n+        self.assertEqual(settings.getpriority(\"newkey_one\"), 50)\n \n-        settings.update({'key_lowprio': 3}, priority=20)\n-        self.assertEqual(settings['key_lowprio'], 1)\n+        settings.update({\"key_lowprio\": 3}, priority=20)\n+        self.assertEqual(settings[\"key_lowprio\"], 1)\n \n     def test_update_jsonstring(self):\n-        settings = BaseSettings({'number': 0, 'dict': BaseSettings({'key': 'val'})})\n+        settings = BaseSettings({\"number\": 0, \"dict\": BaseSettings({\"key\": \"val\"})})\n         settings.update('{\"number\": 1, \"newnumber\": 2}')\n-        self.assertEqual(settings['number'], 1)\n-        self.assertEqual(settings['newnumber'], 2)\n+        self.assertEqual(settings[\"number\"], 1)\n+        self.assertEqual(settings[\"newnumber\"], 2)\n         settings.set(\"dict\", '{\"key\": \"newval\", \"newkey\": \"newval2\"}')\n-        self.assertEqual(settings['dict']['key'], \"newval\")\n-        self.assertEqual(settings['dict']['newkey'], \"newval2\")\n+        self.assertEqual(settings[\"dict\"][\"key\"], \"newval\")\n+        self.assertEqual(settings[\"dict\"][\"newkey\"], \"newval2\")\n \n     def test_delete(self):\n-        settings = BaseSettings({'key': None})\n-        settings.set('key_highprio', None, priority=50)\n-        settings.delete('key')\n-        settings.delete('key_highprio')\n-        self.assertNotIn('key', settings)\n-        self.assertIn('key_highprio', settings)\n-        del settings['key_highprio']\n-        self.assertNotIn('key_highprio', settings)\n+        settings = BaseSettings({\"key\": None})\n+        settings.set(\"key_highprio\", None, priority=50)\n+        settings.delete(\"key\")\n+        settings.delete(\"key_highprio\")\n+        self.assertNotIn(\"key\", settings)\n+        self.assertIn(\"key_highprio\", settings)\n+        del settings[\"key_highprio\"]\n+        self.assertNotIn(\"key_highprio\", settings)\n \n     def test_get(self):\n         test_configuration = {\n-            'TEST_ENABLED1': '1',\n-            'TEST_ENABLED2': True,\n-            'TEST_ENABLED3': 1,\n-            'TEST_ENABLED4': 'True',\n-            'TEST_ENABLED5': 'true',\n-            'TEST_ENABLED_WRONG': 'on',\n-            'TEST_DISABLED1': '0',\n-            'TEST_DISABLED2': False,\n-            'TEST_DISABLED3': 0,\n-            'TEST_DISABLED4': 'False',\n-            'TEST_DISABLED5': 'false',\n-            'TEST_DISABLED_WRONG': 'off',\n-            'TEST_INT1': 123,\n-            'TEST_INT2': '123',\n-            'TEST_FLOAT1': 123.45,\n-            'TEST_FLOAT2': '123.45',\n-            'TEST_LIST1': ['one', 'two'],\n-            'TEST_LIST2': 'one,two',\n-            'TEST_STR': 'value',\n-            'TEST_DICT1': {'key1': 'val1', 'ke2': 3},\n-            'TEST_DICT2': '{\"key1\": \"val1\", \"ke2\": 3}',\n+            \"TEST_ENABLED1\": \"1\",\n+            \"TEST_ENABLED2\": True,\n+            \"TEST_ENABLED3\": 1,\n+            \"TEST_ENABLED4\": \"True\",\n+            \"TEST_ENABLED5\": \"true\",\n+            \"TEST_ENABLED_WRONG\": \"on\",\n+            \"TEST_DISABLED1\": \"0\",\n+            \"TEST_DISABLED2\": False,\n+            \"TEST_DISABLED3\": 0,\n+            \"TEST_DISABLED4\": \"False\",\n+            \"TEST_DISABLED5\": \"false\",\n+            \"TEST_DISABLED_WRONG\": \"off\",\n+            \"TEST_INT1\": 123,\n+            \"TEST_INT2\": \"123\",\n+            \"TEST_FLOAT1\": 123.45,\n+            \"TEST_FLOAT2\": \"123.45\",\n+            \"TEST_LIST1\": [\"one\", \"two\"],\n+            \"TEST_LIST2\": \"one,two\",\n+            \"TEST_STR\": \"value\",\n+            \"TEST_DICT1\": {\"key1\": \"val1\", \"ke2\": 3},\n+            \"TEST_DICT2\": '{\"key1\": \"val1\", \"ke2\": 3}',\n         }\n         settings = self.settings\n-        settings.attributes = {key: SettingsAttribute(value, 0) for key, value\n-                               in test_configuration.items()}\n+        settings.attributes = {\n+            key: SettingsAttribute(value, 0)\n+            for key, value in test_configuration.items()\n+        }\n \n-        self.assertTrue(settings.getbool('TEST_ENABLED1'))\n-        self.assertTrue(settings.getbool('TEST_ENABLED2'))\n-        self.assertTrue(settings.getbool('TEST_ENABLED3'))\n-        self.assertTrue(settings.getbool('TEST_ENABLED4'))\n-        self.assertTrue(settings.getbool('TEST_ENABLED5'))\n-        self.assertFalse(settings.getbool('TEST_ENABLEDx'))\n-        self.assertTrue(settings.getbool('TEST_ENABLEDx', True))\n-        self.assertFalse(settings.getbool('TEST_DISABLED1'))\n-        self.assertFalse(settings.getbool('TEST_DISABLED2'))\n-        self.assertFalse(settings.getbool('TEST_DISABLED3'))\n-        self.assertFalse(settings.getbool('TEST_DISABLED4'))\n-        self.assertFalse(settings.getbool('TEST_DISABLED5'))\n-        self.assertEqual(settings.getint('TEST_INT1'), 123)\n-        self.assertEqual(settings.getint('TEST_INT2'), 123)\n-        self.assertEqual(settings.getint('TEST_INTx'), 0)\n-        self.assertEqual(settings.getint('TEST_INTx', 45), 45)\n-        self.assertEqual(settings.getfloat('TEST_FLOAT1'), 123.45)\n-        self.assertEqual(settings.getfloat('TEST_FLOAT2'), 123.45)\n-        self.assertEqual(settings.getfloat('TEST_FLOATx'), 0.0)\n-        self.assertEqual(settings.getfloat('TEST_FLOATx', 55.0), 55.0)\n-        self.assertEqual(settings.getlist('TEST_LIST1'), ['one', 'two'])\n-        self.assertEqual(settings.getlist('TEST_LIST2'), ['one', 'two'])\n-        self.assertEqual(settings.getlist('TEST_LISTx'), [])\n-        self.assertEqual(settings.getlist('TEST_LISTx', ['default']), ['default'])\n-        self.assertEqual(settings['TEST_STR'], 'value')\n-        self.assertEqual(settings.get('TEST_STR'), 'value')\n-        self.assertEqual(settings['TEST_STRx'], None)\n-        self.assertEqual(settings.get('TEST_STRx'), None)\n-        self.assertEqual(settings.get('TEST_STRx', 'default'), 'default')\n-        self.assertEqual(settings.getdict('TEST_DICT1'), {'key1': 'val1', 'ke2': 3})\n-        self.assertEqual(settings.getdict('TEST_DICT2'), {'key1': 'val1', 'ke2': 3})\n-        self.assertEqual(settings.getdict('TEST_DICT3'), {})\n-        self.assertEqual(settings.getdict('TEST_DICT3', {'key1': 5}), {'key1': 5})\n-        self.assertRaises(ValueError, settings.getdict, 'TEST_LIST1')\n-        self.assertRaises(ValueError, settings.getbool, 'TEST_ENABLED_WRONG')\n-        self.assertRaises(ValueError, settings.getbool, 'TEST_DISABLED_WRONG')\n+        self.assertTrue(settings.getbool(\"TEST_ENABLED1\"))\n+        self.assertTrue(settings.getbool(\"TEST_ENABLED2\"))\n+        self.assertTrue(settings.getbool(\"TEST_ENABLED3\"))\n+        self.assertTrue(settings.getbool(\"TEST_ENABLED4\"))\n+        self.assertTrue(settings.getbool(\"TEST_ENABLED5\"))\n+        self.assertFalse(settings.getbool(\"TEST_ENABLEDx\"))\n+        self.assertTrue(settings.getbool(\"TEST_ENABLEDx\", True))\n+        self.assertFalse(settings.getbool(\"TEST_DISABLED1\"))\n+        self.assertFalse(settings.getbool(\"TEST_DISABLED2\"))\n+        self.assertFalse(settings.getbool(\"TEST_DISABLED3\"))\n+        self.assertFalse(settings.getbool(\"TEST_DISABLED4\"))\n+        self.assertFalse(settings.getbool(\"TEST_DISABLED5\"))\n+        self.assertEqual(settings.getint(\"TEST_INT1\"), 123)\n+        self.assertEqual(settings.getint(\"TEST_INT2\"), 123)\n+        self.assertEqual(settings.getint(\"TEST_INTx\"), 0)\n+        self.assertEqual(settings.getint(\"TEST_INTx\", 45), 45)\n+        self.assertEqual(settings.getfloat(\"TEST_FLOAT1\"), 123.45)\n+        self.assertEqual(settings.getfloat(\"TEST_FLOAT2\"), 123.45)\n+        self.assertEqual(settings.getfloat(\"TEST_FLOATx\"), 0.0)\n+        self.assertEqual(settings.getfloat(\"TEST_FLOATx\", 55.0), 55.0)\n+        self.assertEqual(settings.getlist(\"TEST_LIST1\"), [\"one\", \"two\"])\n+        self.assertEqual(settings.getlist(\"TEST_LIST2\"), [\"one\", \"two\"])\n+        self.assertEqual(settings.getlist(\"TEST_LISTx\"), [])\n+        self.assertEqual(settings.getlist(\"TEST_LISTx\", [\"default\"]), [\"default\"])\n+        self.assertEqual(settings[\"TEST_STR\"], \"value\")\n+        self.assertEqual(settings.get(\"TEST_STR\"), \"value\")\n+        self.assertEqual(settings[\"TEST_STRx\"], None)\n+        self.assertEqual(settings.get(\"TEST_STRx\"), None)\n+        self.assertEqual(settings.get(\"TEST_STRx\", \"default\"), \"default\")\n+        self.assertEqual(settings.getdict(\"TEST_DICT1\"), {\"key1\": \"val1\", \"ke2\": 3})\n+        self.assertEqual(settings.getdict(\"TEST_DICT2\"), {\"key1\": \"val1\", \"ke2\": 3})\n+        self.assertEqual(settings.getdict(\"TEST_DICT3\"), {})\n+        self.assertEqual(settings.getdict(\"TEST_DICT3\", {\"key1\": 5}), {\"key1\": 5})\n+        self.assertRaises(ValueError, settings.getdict, \"TEST_LIST1\")\n+        self.assertRaises(ValueError, settings.getbool, \"TEST_ENABLED_WRONG\")\n+        self.assertRaises(ValueError, settings.getbool, \"TEST_DISABLED_WRONG\")\n \n     def test_getpriority(self):\n-        settings = BaseSettings({'key': 'value'}, priority=99)\n-        self.assertEqual(settings.getpriority('key'), 99)\n-        self.assertEqual(settings.getpriority('nonexistentkey'), None)\n+        settings = BaseSettings({\"key\": \"value\"}, priority=99)\n+        self.assertEqual(settings.getpriority(\"key\"), 99)\n+        self.assertEqual(settings.getpriority(\"nonexistentkey\"), None)\n \n     def test_getwithbase(self):\n-        s = BaseSettings({'TEST_BASE': BaseSettings({1: 1, 2: 2}, 'project'),\n-                          'TEST': BaseSettings({1: 10, 3: 30}, 'default'),\n-                          'HASNOBASE': BaseSettings({3: 3000}, 'default')})\n-        s['TEST'].set(2, 200, 'cmdline')\n-        self.assertCountEqual(s.getwithbase('TEST'), {1: 1, 2: 200, 3: 30})\n-        self.assertCountEqual(s.getwithbase('HASNOBASE'), s['HASNOBASE'])\n-        self.assertEqual(s.getwithbase('NONEXISTENT'), {})\n+        s = BaseSettings(\n+            {\n+                \"TEST_BASE\": BaseSettings({1: 1, 2: 2}, \"project\"),\n+                \"TEST\": BaseSettings({1: 10, 3: 30}, \"default\"),\n+                \"HASNOBASE\": BaseSettings({3: 3000}, \"default\"),\n+            }\n+        )\n+        s[\"TEST\"].set(2, 200, \"cmdline\")\n+        self.assertCountEqual(s.getwithbase(\"TEST\"), {1: 1, 2: 200, 3: 30})\n+        self.assertCountEqual(s.getwithbase(\"HASNOBASE\"), s[\"HASNOBASE\"])\n+        self.assertEqual(s.getwithbase(\"NONEXISTENT\"), {})\n \n     def test_maxpriority(self):\n         # Empty settings should return 'default'\n         self.assertEqual(self.settings.maxpriority(), 0)\n-        self.settings.set('A', 0, 10)\n-        self.settings.set('B', 0, 30)\n+        self.settings.set(\"A\", 0, 10)\n+        self.settings.set(\"B\", 0, 30)\n         self.assertEqual(self.settings.maxpriority(), 30)\n \n     def test_copy(self):\n         values = {\n-            'TEST_BOOL': True,\n-            'TEST_LIST': ['one', 'two'],\n-            'TEST_LIST_OF_LISTS': [['first_one', 'first_two'],\n-                                   ['second_one', 'second_two']]\n+            \"TEST_BOOL\": True,\n+            \"TEST_LIST\": [\"one\", \"two\"],\n+            \"TEST_LIST_OF_LISTS\": [\n+                [\"first_one\", \"first_two\"],\n+                [\"second_one\", \"second_two\"],\n+            ],\n         }\n         self.settings.setdict(values)\n         copy = self.settings.copy()\n-        self.settings.set('TEST_BOOL', False)\n-        self.assertTrue(copy.get('TEST_BOOL'))\n+        self.settings.set(\"TEST_BOOL\", False)\n+        self.assertTrue(copy.get(\"TEST_BOOL\"))\n \n-        test_list = self.settings.get('TEST_LIST')\n-        test_list.append('three')\n-        self.assertListEqual(copy.get('TEST_LIST'), ['one', 'two'])\n+        test_list = self.settings.get(\"TEST_LIST\")\n+        test_list.append(\"three\")\n+        self.assertListEqual(copy.get(\"TEST_LIST\"), [\"one\", \"two\"])\n \n-        test_list_of_lists = self.settings.get('TEST_LIST_OF_LISTS')\n-        test_list_of_lists[0].append('first_three')\n-        self.assertListEqual(copy.get('TEST_LIST_OF_LISTS')[0],\n-                             ['first_one', 'first_two'])\n+        test_list_of_lists = self.settings.get(\"TEST_LIST_OF_LISTS\")\n+        test_list_of_lists[0].append(\"first_three\")\n+        self.assertListEqual(\n+            copy.get(\"TEST_LIST_OF_LISTS\")[0], [\"first_one\", \"first_two\"]\n+        )\n \n     def test_copy_to_dict(self):\n-        s = BaseSettings({'TEST_STRING': 'a string',\n-                          'TEST_LIST': [1, 2],\n-                          'TEST_BOOLEAN': False,\n-                          'TEST_BASE': BaseSettings({1: 1, 2: 2}, 'project'),\n-                          'TEST': BaseSettings({1: 10, 3: 30}, 'default'),\n-                          'HASNOBASE': BaseSettings({3: 3000}, 'default')})\n+        s = BaseSettings(\n+            {\n+                \"TEST_STRING\": \"a string\",\n+                \"TEST_LIST\": [1, 2],\n+                \"TEST_BOOLEAN\": False,\n+                \"TEST_BASE\": BaseSettings({1: 1, 2: 2}, \"project\"),\n+                \"TEST\": BaseSettings({1: 10, 3: 30}, \"default\"),\n+                \"HASNOBASE\": BaseSettings({3: 3000}, \"default\"),\n+            }\n+        )\n         self.assertDictEqual(\n             s.copy_to_dict(),\n             {\n-                'HASNOBASE': {3: 3000},\n-                'TEST': {1: 10, 3: 30},\n-                'TEST_BASE': {1: 1, 2: 2},\n-                'TEST_LIST': [1, 2],\n-                'TEST_BOOLEAN': False,\n-                'TEST_STRING': 'a string',\n-            }\n+                \"HASNOBASE\": {3: 3000},\n+                \"TEST\": {1: 10, 3: 30},\n+                \"TEST_BASE\": {1: 1, 2: 2},\n+                \"TEST_LIST\": [1, 2],\n+                \"TEST_BOOLEAN\": False,\n+                \"TEST_STRING\": \"a string\",\n+            },\n         )\n \n     def test_freeze(self):\n         self.settings.freeze()\n         with self.assertRaises(TypeError) as cm:\n-            self.settings.set('TEST_BOOL', False)\n-            self.assertEqual(str(cm.exception),\n-                             \"Trying to modify an immutable Settings object\")\n+            self.settings.set(\"TEST_BOOL\", False)\n+            self.assertEqual(\n+                str(cm.exception), \"Trying to modify an immutable Settings object\"\n+            )\n \n     def test_frozencopy(self):\n         frozencopy = self.settings.frozencopy()\n@@ -339,83 +359,84 @@ class BaseSettingsTest(unittest.TestCase):\n \n \n class SettingsTest(unittest.TestCase):\n-\n     def setUp(self):\n         self.settings = Settings()\n \n-    @mock.patch.dict('scrapy.settings.SETTINGS_PRIORITIES', {'default': 10})\n-    @mock.patch('scrapy.settings.default_settings', default_settings)\n+    @mock.patch.dict(\"scrapy.settings.SETTINGS_PRIORITIES\", {\"default\": 10})\n+    @mock.patch(\"scrapy.settings.default_settings\", default_settings)\n     def test_initial_defaults(self):\n         settings = Settings()\n         self.assertEqual(len(settings.attributes), 2)\n-        self.assertIn('TEST_DEFAULT', settings.attributes)\n+        self.assertIn(\"TEST_DEFAULT\", settings.attributes)\n \n-        attr = settings.attributes['TEST_DEFAULT']\n+        attr = settings.attributes[\"TEST_DEFAULT\"]\n         self.assertIsInstance(attr, SettingsAttribute)\n-        self.assertEqual(attr.value, 'defvalue')\n+        self.assertEqual(attr.value, \"defvalue\")\n         self.assertEqual(attr.priority, 10)\n \n-    @mock.patch.dict('scrapy.settings.SETTINGS_PRIORITIES', {})\n-    @mock.patch('scrapy.settings.default_settings', {})\n+    @mock.patch.dict(\"scrapy.settings.SETTINGS_PRIORITIES\", {})\n+    @mock.patch(\"scrapy.settings.default_settings\", {})\n     def test_initial_values(self):\n-        settings = Settings({'TEST_OPTION': 'value'}, 10)\n+        settings = Settings({\"TEST_OPTION\": \"value\"}, 10)\n         self.assertEqual(len(settings.attributes), 1)\n-        self.assertIn('TEST_OPTION', settings.attributes)\n+        self.assertIn(\"TEST_OPTION\", settings.attributes)\n \n-        attr = settings.attributes['TEST_OPTION']\n+        attr = settings.attributes[\"TEST_OPTION\"]\n         self.assertIsInstance(attr, SettingsAttribute)\n-        self.assertEqual(attr.value, 'value')\n+        self.assertEqual(attr.value, \"value\")\n         self.assertEqual(attr.priority, 10)\n \n-    @mock.patch('scrapy.settings.default_settings', default_settings)\n+    @mock.patch(\"scrapy.settings.default_settings\", default_settings)\n     def test_autopromote_dicts(self):\n         settings = Settings()\n-        mydict = settings.get('TEST_DICT')\n+        mydict = settings.get(\"TEST_DICT\")\n         self.assertIsInstance(mydict, BaseSettings)\n-        self.assertIn('key', mydict)\n-        self.assertEqual(mydict['key'], 'val')\n-        self.assertEqual(mydict.getpriority('key'), 0)\n+        self.assertIn(\"key\", mydict)\n+        self.assertEqual(mydict[\"key\"], \"val\")\n+        self.assertEqual(mydict.getpriority(\"key\"), 0)\n \n-    @mock.patch('scrapy.settings.default_settings', default_settings)\n+    @mock.patch(\"scrapy.settings.default_settings\", default_settings)\n     def test_getdict_autodegrade_basesettings(self):\n         settings = Settings()\n-        mydict = settings.getdict('TEST_DICT')\n+        mydict = settings.getdict(\"TEST_DICT\")\n         self.assertIsInstance(mydict, dict)\n         self.assertEqual(len(mydict), 1)\n-        self.assertIn('key', mydict)\n-        self.assertEqual(mydict['key'], 'val')\n+        self.assertIn(\"key\", mydict)\n+        self.assertEqual(mydict[\"key\"], \"val\")\n \n     def test_passing_objects_as_values(self):\n         from scrapy.core.downloader.handlers.file import FileDownloadHandler\n         from scrapy.utils.misc import create_instance\n         from scrapy.utils.test import get_crawler\n \n-        class TestPipeline():\n+        class TestPipeline:\n             def process_item(self, i, s):\n                 return i\n \n-        settings = Settings({\n-            'ITEM_PIPELINES': {\n+        settings = Settings(\n+            {\n+                \"ITEM_PIPELINES\": {\n                     TestPipeline: 800,\n                 },\n-            'DOWNLOAD_HANDLERS': {\n-                'ftp': FileDownloadHandler,\n+                \"DOWNLOAD_HANDLERS\": {\n+                    \"ftp\": FileDownloadHandler,\n                 },\n-        })\n+            }\n+        )\n \n-        self.assertIn('ITEM_PIPELINES', settings.attributes)\n+        self.assertIn(\"ITEM_PIPELINES\", settings.attributes)\n \n-        mypipeline, priority = settings.getdict('ITEM_PIPELINES').popitem()\n+        mypipeline, priority = settings.getdict(\"ITEM_PIPELINES\").popitem()\n         self.assertEqual(priority, 800)\n         self.assertEqual(mypipeline, TestPipeline)\n         self.assertIsInstance(mypipeline(), TestPipeline)\n-        self.assertEqual(mypipeline().process_item('item', None), 'item')\n+        self.assertEqual(mypipeline().process_item(\"item\", None), \"item\")\n \n-        myhandler = settings.getdict('DOWNLOAD_HANDLERS').pop('ftp')\n+        myhandler = settings.getdict(\"DOWNLOAD_HANDLERS\").pop(\"ftp\")\n         self.assertEqual(myhandler, FileDownloadHandler)\n         myhandler_instance = create_instance(myhandler, None, get_crawler())\n         self.assertIsInstance(myhandler_instance, FileDownloadHandler)\n-        self.assertTrue(hasattr(myhandler_instance, 'download_request'))\n+        self.assertTrue(hasattr(myhandler_instance, \"download_request\"))\n \n \n if __name__ == \"__main__\":\n\n@@ -1,4 +1,3 @@\n+TEST_DEFAULT = \"defvalue\"\n \n-TEST_DEFAULT = 'defvalue'\n-\n-TEST_DICT = {'key': 'val'}\n+TEST_DICT = {\"key\": \"val\"}\n\n@@ -9,15 +9,16 @@ from tests.mockserver import MockServer\n \n \n class ItemSpider(Spider):\n-    name = 'itemspider'\n+    name = \"itemspider\"\n \n     def start_requests(self):\n         for index in range(10):\n-            yield Request(self.mockserver.url(f'/status?n=200&id={index}'),\n-                          meta={'index': index})\n+            yield Request(\n+                self.mockserver.url(f\"/status?n=200&id={index}\"), meta={\"index\": index}\n+            )\n \n     def parse(self, response):\n-        return {'index': response.meta['index']}\n+        return {\"index\": response.meta[\"index\"]}\n \n \n class AsyncSignalTestCase(unittest.TestCase):\n@@ -41,4 +42,4 @@ class AsyncSignalTestCase(unittest.TestCase):\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertEqual(len(self.items), 10)\n         for index in range(10):\n-            self.assertIn({'index': index}, self.items)\n+            self.assertIn({\"index\": index}, self.items)\n\n@@ -37,39 +37,39 @@ class SpiderTest(unittest.TestCase):\n \n     def test_base_spider(self):\n         spider = self.spider_class(\"example.com\")\n-        self.assertEqual(spider.name, 'example.com')\n+        self.assertEqual(spider.name, \"example.com\")\n         self.assertEqual(spider.start_urls, [])\n \n     def test_start_requests(self):\n-        spider = self.spider_class('example.com')\n+        spider = self.spider_class(\"example.com\")\n         start_requests = spider.start_requests()\n         self.assertTrue(inspect.isgenerator(start_requests))\n         self.assertEqual(list(start_requests), [])\n \n     def test_spider_args(self):\n         \"\"\"``__init__`` method arguments are assigned to spider attributes\"\"\"\n-        spider = self.spider_class('example.com', foo='bar')\n-        self.assertEqual(spider.foo, 'bar')\n+        spider = self.spider_class(\"example.com\", foo=\"bar\")\n+        self.assertEqual(spider.foo, \"bar\")\n \n     def test_spider_without_name(self):\n         \"\"\"``__init__`` method arguments are assigned to spider attributes\"\"\"\n         self.assertRaises(ValueError, self.spider_class)\n-        self.assertRaises(ValueError, self.spider_class, somearg='foo')\n+        self.assertRaises(ValueError, self.spider_class, somearg=\"foo\")\n \n     def test_from_crawler_crawler_and_settings_population(self):\n         crawler = get_crawler()\n-        spider = self.spider_class.from_crawler(crawler, 'example.com')\n-        self.assertTrue(hasattr(spider, 'crawler'))\n+        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n+        self.assertTrue(hasattr(spider, \"crawler\"))\n         self.assertIs(spider.crawler, crawler)\n-        self.assertTrue(hasattr(spider, 'settings'))\n+        self.assertTrue(hasattr(spider, \"settings\"))\n         self.assertIs(spider.settings, crawler.settings)\n \n     def test_from_crawler_init_call(self):\n-        with mock.patch.object(self.spider_class, '__init__',\n-                               return_value=None) as mock_init:\n-            self.spider_class.from_crawler(get_crawler(), 'example.com',\n-                                           foo='bar')\n-            mock_init.assert_called_once_with('example.com', foo='bar')\n+        with mock.patch.object(\n+            self.spider_class, \"__init__\", return_value=None\n+        ) as mock_init:\n+            self.spider_class.from_crawler(get_crawler(), \"example.com\", foo=\"bar\")\n+            mock_init.assert_called_once_with(\"example.com\", foo=\"bar\")\n \n     def test_closed_signal_call(self):\n         class TestSpider(self.spider_class):\n@@ -79,39 +79,39 @@ class SpiderTest(unittest.TestCase):\n                 self.closed_called = True\n \n         crawler = get_crawler()\n-        spider = TestSpider.from_crawler(crawler, 'example.com')\n-        crawler.signals.send_catch_log(signal=signals.spider_opened,\n-                                       spider=spider)\n-        crawler.signals.send_catch_log(signal=signals.spider_closed,\n-                                       spider=spider, reason=None)\n+        spider = TestSpider.from_crawler(crawler, \"example.com\")\n+        crawler.signals.send_catch_log(signal=signals.spider_opened, spider=spider)\n+        crawler.signals.send_catch_log(\n+            signal=signals.spider_closed, spider=spider, reason=None\n+        )\n         self.assertTrue(spider.closed_called)\n \n     def test_update_settings(self):\n-        spider_settings = {'TEST1': 'spider', 'TEST2': 'spider'}\n-        project_settings = {'TEST1': 'project', 'TEST3': 'project'}\n+        spider_settings = {\"TEST1\": \"spider\", \"TEST2\": \"spider\"}\n+        project_settings = {\"TEST1\": \"project\", \"TEST3\": \"project\"}\n         self.spider_class.custom_settings = spider_settings\n-        settings = Settings(project_settings, priority='project')\n+        settings = Settings(project_settings, priority=\"project\")\n \n         self.spider_class.update_settings(settings)\n-        self.assertEqual(settings.get('TEST1'), 'spider')\n-        self.assertEqual(settings.get('TEST2'), 'spider')\n-        self.assertEqual(settings.get('TEST3'), 'project')\n+        self.assertEqual(settings.get(\"TEST1\"), \"spider\")\n+        self.assertEqual(settings.get(\"TEST2\"), \"spider\")\n+        self.assertEqual(settings.get(\"TEST3\"), \"project\")\n \n     def test_logger(self):\n-        spider = self.spider_class('example.com')\n+        spider = self.spider_class(\"example.com\")\n         with LogCapture() as lc:\n-            spider.logger.info('test log msg')\n-        lc.check(('example.com', 'INFO', 'test log msg'))\n+            spider.logger.info(\"test log msg\")\n+        lc.check((\"example.com\", \"INFO\", \"test log msg\"))\n \n         record = lc.records[0]\n-        self.assertIn('spider', record.__dict__)\n+        self.assertIn(\"spider\", record.__dict__)\n         self.assertIs(record.spider, spider)\n \n     def test_log(self):\n-        spider = self.spider_class('example.com')\n-        with mock.patch('scrapy.spiders.Spider.logger') as mock_logger:\n-            spider.log('test log msg', 'INFO')\n-        mock_logger.log.assert_called_once_with('INFO', 'test log msg')\n+        spider = self.spider_class(\"example.com\")\n+        with mock.patch(\"scrapy.spiders.Spider.logger\") as mock_logger:\n+            spider.log(\"test log msg\", \"INFO\")\n+        mock_logger.log.assert_called_once_with(\"INFO\", \"test log msg\")\n \n \n class InitSpiderTest(SpiderTest):\n@@ -132,37 +132,45 @@ class XMLFeedSpiderTest(SpiderTest):\n         </url>\n         <url><loc>http://www.example.com/</loc><y:updated>2009-08-16</updated><other value=\"foo\"/></url>\n         </urlset>\"\"\"\n-        response = XmlResponse(url='http://example.com/sitemap.xml', body=body)\n+        response = XmlResponse(url=\"http://example.com/sitemap.xml\", body=body)\n \n         class _XMLSpider(self.spider_class):\n-            itertag = 'url'\n+            itertag = \"url\"\n             namespaces = (\n-                ('a', 'http://www.google.com/schemas/sitemap/0.84'),\n-                ('b', 'http://www.example.com/schemas/extras/1.0'),\n+                (\"a\", \"http://www.google.com/schemas/sitemap/0.84\"),\n+                (\"b\", \"http://www.example.com/schemas/extras/1.0\"),\n             )\n \n             def parse_node(self, response, selector):\n                 yield {\n-                    'loc': selector.xpath('a:loc/text()').getall(),\n-                    'updated': selector.xpath('b:updated/text()').getall(),\n-                    'other': selector.xpath('other/@value').getall(),\n-                    'custom': selector.xpath('other/@b:custom').getall(),\n+                    \"loc\": selector.xpath(\"a:loc/text()\").getall(),\n+                    \"updated\": selector.xpath(\"b:updated/text()\").getall(),\n+                    \"other\": selector.xpath(\"other/@value\").getall(),\n+                    \"custom\": selector.xpath(\"other/@b:custom\").getall(),\n                 }\n \n-        for iterator in ('iternodes', 'xml'):\n-            spider = _XMLSpider('example', iterator=iterator)\n+        for iterator in (\"iternodes\", \"xml\"):\n+            spider = _XMLSpider(\"example\", iterator=iterator)\n             output = list(spider._parse(response))\n             self.assertEqual(len(output), 2, iterator)\n-            self.assertEqual(output, [\n-                {'loc': ['http://www.example.com/Special-Offers.html'],\n-                 'updated': ['2009-08-16'],\n-                 'custom': ['fuu'],\n-                 'other': ['bar']},\n-                {'loc': [],\n-                 'updated': ['2009-08-16'],\n-                 'other': ['foo'],\n-                 'custom': []},\n-            ], iterator)\n+            self.assertEqual(\n+                output,\n+                [\n+                    {\n+                        \"loc\": [\"http://www.example.com/Special-Offers.html\"],\n+                        \"updated\": [\"2009-08-16\"],\n+                        \"custom\": [\"fuu\"],\n+                        \"other\": [\"bar\"],\n+                    },\n+                    {\n+                        \"loc\": [],\n+                        \"updated\": [\"2009-08-16\"],\n+                        \"other\": [\"foo\"],\n+                        \"custom\": [],\n+                    },\n+                ],\n+                iterator,\n+            )\n \n \n class CSVFeedSpiderTest(SpiderTest):\n@@ -170,7 +178,7 @@ class CSVFeedSpiderTest(SpiderTest):\n     spider_class = CSVFeedSpider\n \n     def test_parse_rows(self):\n-        body = get_testdata('feeds', 'feed-sample6.csv')\n+        body = get_testdata(\"feeds\", \"feed-sample6.csv\")\n         response = Response(\"http://example.org/dummy.csv\", body=body)\n \n         class _CrawlSpider(self.spider_class):\n@@ -183,7 +191,7 @@ class CSVFeedSpiderTest(SpiderTest):\n \n         spider = _CrawlSpider()\n         rows = list(spider.parse_rows(response))\n-        assert rows[0] == {'id': '1', 'name': 'alpha', 'value': 'foobar'}\n+        assert rows[0] == {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"}\n         assert len(rows) == 4\n \n \n@@ -203,34 +211,38 @@ class CrawlSpiderTest(SpiderTest):\n \n     def test_rule_without_link_extractor(self):\n \n-        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n+        response = HtmlResponse(\n+            \"http://example.org/somepage/index.html\", body=self.test_body\n+        )\n \n         class _CrawlSpider(self.spider_class):\n             name = \"test\"\n-            allowed_domains = ['example.org']\n-            rules = (\n-                Rule(),\n-            )\n+            allowed_domains = [\"example.org\"]\n+            rules = (Rule(),)\n \n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n-        self.assertEqual([r.url for r in output],\n-                         ['http://example.org/somepage/item/12.html',\n-                          'http://example.org/about.html',\n-                          'http://example.org/nofollow.html'])\n+        self.assertEqual(\n+            [r.url for r in output],\n+            [\n+                \"http://example.org/somepage/item/12.html\",\n+                \"http://example.org/about.html\",\n+                \"http://example.org/nofollow.html\",\n+            ],\n+        )\n \n     def test_process_links(self):\n \n-        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n+        response = HtmlResponse(\n+            \"http://example.org/somepage/index.html\", body=self.test_body\n+        )\n \n         class _CrawlSpider(self.spider_class):\n             name = \"test\"\n-            allowed_domains = ['example.org']\n-            rules = (\n-                Rule(LinkExtractor(), process_links=\"dummy_process_links\"),\n-            )\n+            allowed_domains = [\"example.org\"]\n+            rules = (Rule(LinkExtractor(), process_links=\"dummy_process_links\"),)\n \n             def dummy_process_links(self, links):\n                 return links\n@@ -239,47 +251,54 @@ class CrawlSpiderTest(SpiderTest):\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n-        self.assertEqual([r.url for r in output],\n-                         ['http://example.org/somepage/item/12.html',\n-                          'http://example.org/about.html',\n-                          'http://example.org/nofollow.html'])\n+        self.assertEqual(\n+            [r.url for r in output],\n+            [\n+                \"http://example.org/somepage/item/12.html\",\n+                \"http://example.org/about.html\",\n+                \"http://example.org/nofollow.html\",\n+            ],\n+        )\n \n     def test_process_links_filter(self):\n \n-        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n+        response = HtmlResponse(\n+            \"http://example.org/somepage/index.html\", body=self.test_body\n+        )\n \n         class _CrawlSpider(self.spider_class):\n             import re\n \n             name = \"test\"\n-            allowed_domains = ['example.org']\n-            rules = (\n-                Rule(LinkExtractor(), process_links=\"filter_process_links\"),\n-            )\n-            _test_regex = re.compile('nofollow')\n+            allowed_domains = [\"example.org\"]\n+            rules = (Rule(LinkExtractor(), process_links=\"filter_process_links\"),)\n+            _test_regex = re.compile(\"nofollow\")\n \n             def filter_process_links(self, links):\n-                return [link for link in links\n-                        if not self._test_regex.search(link.url)]\n+                return [link for link in links if not self._test_regex.search(link.url)]\n \n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 2)\n         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n-        self.assertEqual([r.url for r in output],\n-                         ['http://example.org/somepage/item/12.html',\n-                          'http://example.org/about.html'])\n+        self.assertEqual(\n+            [r.url for r in output],\n+            [\n+                \"http://example.org/somepage/item/12.html\",\n+                \"http://example.org/about.html\",\n+            ],\n+        )\n \n     def test_process_links_generator(self):\n \n-        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n+        response = HtmlResponse(\n+            \"http://example.org/somepage/index.html\", body=self.test_body\n+        )\n \n         class _CrawlSpider(self.spider_class):\n             name = \"test\"\n-            allowed_domains = ['example.org']\n-            rules = (\n-                Rule(LinkExtractor(), process_links=\"dummy_process_links\"),\n-            )\n+            allowed_domains = [\"example.org\"]\n+            rules = (Rule(LinkExtractor(), process_links=\"dummy_process_links\"),)\n \n             def dummy_process_links(self, links):\n                 for link in links:\n@@ -289,21 +308,27 @@ class CrawlSpiderTest(SpiderTest):\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n-        self.assertEqual([r.url for r in output],\n-                         ['http://example.org/somepage/item/12.html',\n-                          'http://example.org/about.html',\n-                          'http://example.org/nofollow.html'])\n+        self.assertEqual(\n+            [r.url for r in output],\n+            [\n+                \"http://example.org/somepage/item/12.html\",\n+                \"http://example.org/about.html\",\n+                \"http://example.org/nofollow.html\",\n+            ],\n+        )\n \n     def test_process_request(self):\n \n-        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n+        response = HtmlResponse(\n+            \"http://example.org/somepage/index.html\", body=self.test_body\n+        )\n \n         def process_request_change_domain(request, response):\n-            return request.replace(url=request.url.replace('.org', '.com'))\n+            return request.replace(url=request.url.replace(\".org\", \".com\"))\n \n         class _CrawlSpider(self.spider_class):\n             name = \"test\"\n-            allowed_domains = ['example.org']\n+            allowed_domains = [\"example.org\"]\n             rules = (\n                 Rule(LinkExtractor(), process_request=process_request_change_domain),\n             )\n@@ -312,47 +337,61 @@ class CrawlSpiderTest(SpiderTest):\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n-        self.assertEqual([r.url for r in output],\n-                         ['http://example.com/somepage/item/12.html',\n-                          'http://example.com/about.html',\n-                          'http://example.com/nofollow.html'])\n+        self.assertEqual(\n+            [r.url for r in output],\n+            [\n+                \"http://example.com/somepage/item/12.html\",\n+                \"http://example.com/about.html\",\n+                \"http://example.com/nofollow.html\",\n+            ],\n+        )\n \n     def test_process_request_with_response(self):\n \n-        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n+        response = HtmlResponse(\n+            \"http://example.org/somepage/index.html\", body=self.test_body\n+        )\n \n         def process_request_meta_response_class(request, response):\n-            request.meta['response_class'] = response.__class__.__name__\n+            request.meta[\"response_class\"] = response.__class__.__name__\n             return request\n \n         class _CrawlSpider(self.spider_class):\n             name = \"test\"\n-            allowed_domains = ['example.org']\n+            allowed_domains = [\"example.org\"]\n             rules = (\n-                Rule(LinkExtractor(), process_request=process_request_meta_response_class),\n+                Rule(\n+                    LinkExtractor(), process_request=process_request_meta_response_class\n+                ),\n             )\n \n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n-        self.assertEqual([r.url for r in output],\n-                         ['http://example.org/somepage/item/12.html',\n-                          'http://example.org/about.html',\n-                          'http://example.org/nofollow.html'])\n-        self.assertEqual([r.meta['response_class'] for r in output],\n-                         ['HtmlResponse', 'HtmlResponse', 'HtmlResponse'])\n+        self.assertEqual(\n+            [r.url for r in output],\n+            [\n+                \"http://example.org/somepage/item/12.html\",\n+                \"http://example.org/about.html\",\n+                \"http://example.org/nofollow.html\",\n+            ],\n+        )\n+        self.assertEqual(\n+            [r.meta[\"response_class\"] for r in output],\n+            [\"HtmlResponse\", \"HtmlResponse\", \"HtmlResponse\"],\n+        )\n \n     def test_process_request_instance_method(self):\n \n-        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n+        response = HtmlResponse(\n+            \"http://example.org/somepage/index.html\", body=self.test_body\n+        )\n \n         class _CrawlSpider(self.spider_class):\n             name = \"test\"\n-            allowed_domains = ['example.org']\n-            rules = (\n-                Rule(LinkExtractor(), process_request='process_request_upper'),\n-            )\n+            allowed_domains = [\"example.org\"]\n+            rules = (Rule(LinkExtractor(), process_request=\"process_request_upper\"),)\n \n             def process_request_upper(self, request, response):\n                 return request.replace(url=request.url.upper())\n@@ -361,55 +400,69 @@ class CrawlSpiderTest(SpiderTest):\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n-        self.assertEqual([r.url for r in output],\n-                         [safe_url_string('http://EXAMPLE.ORG/SOMEPAGE/ITEM/12.HTML'),\n-                          safe_url_string('http://EXAMPLE.ORG/ABOUT.HTML'),\n-                          safe_url_string('http://EXAMPLE.ORG/NOFOLLOW.HTML')])\n+        self.assertEqual(\n+            [r.url for r in output],\n+            [\n+                safe_url_string(\"http://EXAMPLE.ORG/SOMEPAGE/ITEM/12.HTML\"),\n+                safe_url_string(\"http://EXAMPLE.ORG/ABOUT.HTML\"),\n+                safe_url_string(\"http://EXAMPLE.ORG/NOFOLLOW.HTML\"),\n+            ],\n+        )\n \n     def test_process_request_instance_method_with_response(self):\n \n-        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n+        response = HtmlResponse(\n+            \"http://example.org/somepage/index.html\", body=self.test_body\n+        )\n \n         class _CrawlSpider(self.spider_class):\n             name = \"test\"\n-            allowed_domains = ['example.org']\n+            allowed_domains = [\"example.org\"]\n             rules = (\n-                Rule(LinkExtractor(), process_request='process_request_meta_response_class'),\n+                Rule(\n+                    LinkExtractor(),\n+                    process_request=\"process_request_meta_response_class\",\n+                ),\n             )\n \n             def process_request_meta_response_class(self, request, response):\n-                request.meta['response_class'] = response.__class__.__name__\n+                request.meta[\"response_class\"] = response.__class__.__name__\n                 return request\n \n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n-        self.assertEqual([r.url for r in output],\n-                         ['http://example.org/somepage/item/12.html',\n-                          'http://example.org/about.html',\n-                          'http://example.org/nofollow.html'])\n-        self.assertEqual([r.meta['response_class'] for r in output],\n-                         ['HtmlResponse', 'HtmlResponse', 'HtmlResponse'])\n+        self.assertEqual(\n+            [r.url for r in output],\n+            [\n+                \"http://example.org/somepage/item/12.html\",\n+                \"http://example.org/about.html\",\n+                \"http://example.org/nofollow.html\",\n+            ],\n+        )\n+        self.assertEqual(\n+            [r.meta[\"response_class\"] for r in output],\n+            [\"HtmlResponse\", \"HtmlResponse\", \"HtmlResponse\"],\n+        )\n \n     def test_follow_links_attribute_population(self):\n         crawler = get_crawler()\n-        spider = self.spider_class.from_crawler(crawler, 'example.com')\n-        self.assertTrue(hasattr(spider, '_follow_links'))\n+        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n+        self.assertTrue(hasattr(spider, \"_follow_links\"))\n         self.assertTrue(spider._follow_links)\n \n-        settings_dict = {'CRAWLSPIDER_FOLLOW_LINKS': False}\n+        settings_dict = {\"CRAWLSPIDER_FOLLOW_LINKS\": False}\n         crawler = get_crawler(settings_dict=settings_dict)\n-        spider = self.spider_class.from_crawler(crawler, 'example.com')\n-        self.assertTrue(hasattr(spider, '_follow_links'))\n+        spider = self.spider_class.from_crawler(crawler, \"example.com\")\n+        self.assertTrue(hasattr(spider, \"_follow_links\"))\n         self.assertFalse(spider._follow_links)\n \n     def test_start_url(self):\n         spider = self.spider_class(\"example.com\")\n-        spider.start_url = 'https://www.example.com'\n+        spider.start_url = \"https://www.example.com\"\n \n-        with self.assertRaisesRegex(AttributeError,\n-                                    r'^Crawling could not start.*$'):\n+        with self.assertRaisesRegex(AttributeError, r\"^Crawling could not start.*$\"):\n             list(spider.start_requests())\n \n \n@@ -419,7 +472,7 @@ class SitemapSpiderTest(SpiderTest):\n \n     BODY = b\"SITEMAP\"\n     f = BytesIO()\n-    g = gzip.GzipFile(fileobj=f, mode='w+b')\n+    g = gzip.GzipFile(fileobj=f, mode=\"w+b\")\n     g.write(BODY)\n     g.close()\n     GZBODY = f.getvalue()\n@@ -439,8 +492,11 @@ class SitemapSpiderTest(SpiderTest):\n         self.assertSitemapBody(r, None)\n \n     def test_get_sitemap_body_gzip_headers(self):\n-        r = Response(url=\"http://www.example.com/sitemap\", body=self.GZBODY,\n-                     headers={\"content-type\": \"application/gzip\"})\n+        r = Response(\n+            url=\"http://www.example.com/sitemap\",\n+            body=self.GZBODY,\n+            headers={\"content-type\": \"application/gzip\"},\n+        )\n         self.assertSitemapBody(r, self.BODY)\n \n     def test_get_sitemap_body_xml_url(self):\n@@ -465,11 +521,15 @@ Sitemap: /sitemap-relative-url.xml\n \n         r = TextResponse(url=\"http://www.example.com/robots.txt\", body=robots)\n         spider = self.spider_class(\"example.com\")\n-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],\n-                         ['http://example.com/sitemap.xml',\n-                          'http://example.com/sitemap-product-index.xml',\n-                          'http://example.com/sitemap-uppercase.xml',\n-                          'http://www.example.com/sitemap-relative-url.xml'])\n+        self.assertEqual(\n+            [req.url for req in spider._parse_sitemap(r)],\n+            [\n+                \"http://example.com/sitemap.xml\",\n+                \"http://example.com/sitemap-product-index.xml\",\n+                \"http://example.com/sitemap-uppercase.xml\",\n+                \"http://www.example.com/sitemap-relative-url.xml\",\n+            ],\n+        )\n \n     def test_alternate_url_locs(self):\n         sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n@@ -488,15 +548,21 @@ Sitemap: /sitemap-relative-url.xml\n     </urlset>\"\"\"\n         r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n         spider = self.spider_class(\"example.com\")\n-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],\n-                         ['http://www.example.com/english/'])\n+        self.assertEqual(\n+            [req.url for req in spider._parse_sitemap(r)],\n+            [\"http://www.example.com/english/\"],\n+        )\n \n         spider.sitemap_alternate_links = True\n-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],\n-                         ['http://www.example.com/english/',\n-                          'http://www.example.com/deutsch/',\n-                          'http://www.example.com/schweiz-deutsch/',\n-                          'http://www.example.com/italiano/'])\n+        self.assertEqual(\n+            [req.url for req in spider._parse_sitemap(r)],\n+            [\n+                \"http://www.example.com/english/\",\n+                \"http://www.example.com/deutsch/\",\n+                \"http://www.example.com/schweiz-deutsch/\",\n+                \"http://www.example.com/italiano/\",\n+            ],\n+        )\n \n     def test_sitemap_filter(self):\n         sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n@@ -515,20 +581,24 @@ Sitemap: /sitemap-relative-url.xml\n         class FilteredSitemapSpider(self.spider_class):\n             def sitemap_filter(self, entries):\n                 from datetime import datetime\n+\n                 for entry in entries:\n-                    date_time = datetime.strptime(entry['lastmod'], '%Y-%m-%d')\n+                    date_time = datetime.strptime(entry[\"lastmod\"], \"%Y-%m-%d\")\n                     if date_time.year > 2008:\n                         yield entry\n \n         r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n         spider = self.spider_class(\"example.com\")\n-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],\n-                         ['http://www.example.com/english/',\n-                          'http://www.example.com/portuguese/'])\n+        self.assertEqual(\n+            [req.url for req in spider._parse_sitemap(r)],\n+            [\"http://www.example.com/english/\", \"http://www.example.com/portuguese/\"],\n+        )\n \n         spider = FilteredSitemapSpider(\"example.com\")\n-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],\n-                         ['http://www.example.com/english/'])\n+        self.assertEqual(\n+            [req.url for req in spider._parse_sitemap(r)],\n+            [\"http://www.example.com/english/\"],\n+        )\n \n     def test_sitemap_filter_with_alternate_links(self):\n         sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n@@ -549,21 +619,27 @@ Sitemap: /sitemap-relative-url.xml\n         class FilteredSitemapSpider(self.spider_class):\n             def sitemap_filter(self, entries):\n                 for entry in entries:\n-                    alternate_links = entry.get('alternate', tuple())\n+                    alternate_links = entry.get(\"alternate\", tuple())\n                     for link in alternate_links:\n-                        if '/deutsch/' in link:\n-                            entry['loc'] = link\n+                        if \"/deutsch/\" in link:\n+                            entry[\"loc\"] = link\n                             yield entry\n \n         r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n         spider = self.spider_class(\"example.com\")\n-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],\n-                         ['http://www.example.com/english/article_1/',\n-                          'http://www.example.com/english/article_2/'])\n+        self.assertEqual(\n+            [req.url for req in spider._parse_sitemap(r)],\n+            [\n+                \"http://www.example.com/english/article_1/\",\n+                \"http://www.example.com/english/article_2/\",\n+            ],\n+        )\n \n         spider = FilteredSitemapSpider(\"example.com\")\n-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],\n-                         ['http://www.example.com/deutsch/article_1/'])\n+        self.assertEqual(\n+            [req.url for req in spider._parse_sitemap(r)],\n+            [\"http://www.example.com/deutsch/article_1/\"],\n+        )\n \n     def test_sitemapindex_filter(self):\n         sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n@@ -581,27 +657,35 @@ Sitemap: /sitemap-relative-url.xml\n         class FilteredSitemapSpider(self.spider_class):\n             def sitemap_filter(self, entries):\n                 from datetime import datetime\n+\n                 for entry in entries:\n-                    date_time = datetime.strptime(entry['lastmod'].split('T')[0], '%Y-%m-%d')\n+                    date_time = datetime.strptime(\n+                        entry[\"lastmod\"].split(\"T\")[0], \"%Y-%m-%d\"\n+                    )\n                     if date_time.year > 2004:\n                         yield entry\n \n         r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n         spider = self.spider_class(\"example.com\")\n-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],\n-                         ['http://www.example.com/sitemap1.xml',\n-                          'http://www.example.com/sitemap2.xml'])\n+        self.assertEqual(\n+            [req.url for req in spider._parse_sitemap(r)],\n+            [\n+                \"http://www.example.com/sitemap1.xml\",\n+                \"http://www.example.com/sitemap2.xml\",\n+            ],\n+        )\n \n         spider = FilteredSitemapSpider(\"example.com\")\n-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],\n-                         ['http://www.example.com/sitemap2.xml'])\n+        self.assertEqual(\n+            [req.url for req in spider._parse_sitemap(r)],\n+            [\"http://www.example.com/sitemap2.xml\"],\n+        )\n \n \n class DeprecationTest(unittest.TestCase):\n-\n     def test_crawl_spider(self):\n         assert issubclass(CrawlSpider, Spider)\n-        assert isinstance(CrawlSpider(name='foo'), Spider)\n+        assert isinstance(CrawlSpider(name=\"foo\"), Spider)\n \n \n class NoParseMethodSpiderTest(unittest.TestCase):\n@@ -609,10 +693,10 @@ class NoParseMethodSpiderTest(unittest.TestCase):\n     spider_class = Spider\n \n     def test_undefined_parse_method(self):\n-        spider = self.spider_class('example.com')\n-        text = b'Random text'\n+        spider = self.spider_class(\"example.com\")\n+        text = b\"Random text\"\n         resp = TextResponse(url=\"http://www.example.com/random_url\", body=text)\n \n-        exc_msg = 'Spider.parse callback is not defined'\n+        exc_msg = \"Spider.parse callback is not defined\"\n         with self.assertRaisesRegex(NotImplementedError, exc_msg):\n             spider.parse(resp)\n\n@@ -28,19 +28,18 @@ def _copytree(source: Path, target: Path):\n \n \n class SpiderLoaderTest(unittest.TestCase):\n-\n     def setUp(self):\n-        orig_spiders_dir = module_dir / 'test_spiders'\n+        orig_spiders_dir = module_dir / \"test_spiders\"\n         self.tmpdir = Path(tempfile.mkdtemp())\n-        self.spiders_dir = self.tmpdir / 'test_spiders_xxx'\n+        self.spiders_dir = self.tmpdir / \"test_spiders_xxx\"\n         _copytree(orig_spiders_dir, self.spiders_dir)\n         sys.path.append(str(self.tmpdir))\n-        settings = Settings({'SPIDER_MODULES': ['test_spiders_xxx']})\n+        settings = Settings({\"SPIDER_MODULES\": [\"test_spiders_xxx\"]})\n         self.spider_loader = SpiderLoader.from_settings(settings)\n \n     def tearDown(self):\n         del self.spider_loader\n-        del sys.modules['test_spiders_xxx']\n+        del sys.modules[\"test_spiders_xxx\"]\n         sys.path.remove(str(self.tmpdir))\n \n     def test_interface(self):\n@@ -48,78 +47,86 @@ class SpiderLoaderTest(unittest.TestCase):\n \n     def test_list(self):\n         self.assertEqual(\n-            set(self.spider_loader.list()),\n-            {'spider1', 'spider2', 'spider3', 'spider4'})\n+            set(self.spider_loader.list()), {\"spider1\", \"spider2\", \"spider3\", \"spider4\"}\n+        )\n \n     def test_load(self):\n         spider1 = self.spider_loader.load(\"spider1\")\n-        self.assertEqual(spider1.__name__, 'Spider1')\n+        self.assertEqual(spider1.__name__, \"Spider1\")\n \n     def test_find_by_request(self):\n         self.assertEqual(\n-            self.spider_loader.find_by_request(Request('http://scrapy1.org/test')),\n-            ['spider1'])\n+            self.spider_loader.find_by_request(Request(\"http://scrapy1.org/test\")),\n+            [\"spider1\"],\n+        )\n         self.assertEqual(\n-            self.spider_loader.find_by_request(Request('http://scrapy2.org/test')),\n-            ['spider2'])\n+            self.spider_loader.find_by_request(Request(\"http://scrapy2.org/test\")),\n+            [\"spider2\"],\n+        )\n         self.assertEqual(\n-            set(self.spider_loader.find_by_request(Request('http://scrapy3.org/test'))),\n-            {'spider1', 'spider2'})\n+            set(self.spider_loader.find_by_request(Request(\"http://scrapy3.org/test\"))),\n+            {\"spider1\", \"spider2\"},\n+        )\n         self.assertEqual(\n-            self.spider_loader.find_by_request(Request('http://scrapy999.org/test')),\n-            [])\n+            self.spider_loader.find_by_request(Request(\"http://scrapy999.org/test\")), []\n+        )\n         self.assertEqual(\n-            self.spider_loader.find_by_request(Request('http://spider3.com')),\n-            [])\n+            self.spider_loader.find_by_request(Request(\"http://spider3.com\")), []\n+        )\n         self.assertEqual(\n-            self.spider_loader.find_by_request(Request('http://spider3.com/onlythis')),\n-            ['spider3'])\n+            self.spider_loader.find_by_request(Request(\"http://spider3.com/onlythis\")),\n+            [\"spider3\"],\n+        )\n \n     def test_load_spider_module(self):\n-        module = 'tests.test_spiderloader.test_spiders.spider1'\n-        settings = Settings({'SPIDER_MODULES': [module]})\n+        module = \"tests.test_spiderloader.test_spiders.spider1\"\n+        settings = Settings({\"SPIDER_MODULES\": [module]})\n         self.spider_loader = SpiderLoader.from_settings(settings)\n         assert len(self.spider_loader._spiders) == 1\n \n     def test_load_spider_module_multiple(self):\n-        prefix = 'tests.test_spiderloader.test_spiders.'\n-        module = ','.join(prefix + s for s in ('spider1', 'spider2'))\n-        settings = Settings({'SPIDER_MODULES': module})\n+        prefix = \"tests.test_spiderloader.test_spiders.\"\n+        module = \",\".join(prefix + s for s in (\"spider1\", \"spider2\"))\n+        settings = Settings({\"SPIDER_MODULES\": module})\n         self.spider_loader = SpiderLoader.from_settings(settings)\n         assert len(self.spider_loader._spiders) == 2\n \n     def test_load_base_spider(self):\n-        module = 'tests.test_spiderloader.test_spiders.spider0'\n-        settings = Settings({'SPIDER_MODULES': [module]})\n+        module = \"tests.test_spiderloader.test_spiders.spider0\"\n+        settings = Settings({\"SPIDER_MODULES\": [module]})\n         self.spider_loader = SpiderLoader.from_settings(settings)\n         assert len(self.spider_loader._spiders) == 0\n \n     def test_crawler_runner_loading(self):\n-        module = 'tests.test_spiderloader.test_spiders.spider1'\n-        runner = CrawlerRunner({\n-            'SPIDER_MODULES': [module],\n-            'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n-        })\n+        module = \"tests.test_spiderloader.test_spiders.spider1\"\n+        runner = CrawlerRunner(\n+            {\n+                \"SPIDER_MODULES\": [module],\n+                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n+            }\n+        )\n \n-        self.assertRaisesRegex(KeyError, 'Spider not found',\n-                               runner.create_crawler, 'spider2')\n+        self.assertRaisesRegex(\n+            KeyError, \"Spider not found\", runner.create_crawler, \"spider2\"\n+        )\n \n-        crawler = runner.create_crawler('spider1')\n+        crawler = runner.create_crawler(\"spider1\")\n         self.assertTrue(issubclass(crawler.spidercls, scrapy.Spider))\n-        self.assertEqual(crawler.spidercls.name, 'spider1')\n+        self.assertEqual(crawler.spidercls.name, \"spider1\")\n \n     def test_bad_spider_modules_exception(self):\n \n-        module = 'tests.test_spiderloader.test_spiders.doesnotexist'\n-        settings = Settings({'SPIDER_MODULES': [module]})\n+        module = \"tests.test_spiderloader.test_spiders.doesnotexist\"\n+        settings = Settings({\"SPIDER_MODULES\": [module]})\n         self.assertRaises(ImportError, SpiderLoader.from_settings, settings)\n \n     def test_bad_spider_modules_warning(self):\n \n         with warnings.catch_warnings(record=True) as w:\n-            module = 'tests.test_spiderloader.test_spiders.doesnotexist'\n-            settings = Settings({'SPIDER_MODULES': [module],\n-                                 'SPIDER_LOADER_WARN_ONLY': True})\n+            module = \"tests.test_spiderloader.test_spiders.doesnotexist\"\n+            settings = Settings(\n+                {\"SPIDER_MODULES\": [module], \"SPIDER_LOADER_WARN_ONLY\": True}\n+            )\n             spider_loader = SpiderLoader.from_settings(settings)\n             if str(w[0].message).startswith(\"_SixMetaPathImporter\"):\n                 # needed on 3.10 because of https://github.com/benjaminp/six/issues/349,\n@@ -133,24 +140,25 @@ class SpiderLoaderTest(unittest.TestCase):\n \n \n class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n-\n     def setUp(self):\n-        orig_spiders_dir = module_dir / 'test_spiders'\n+        orig_spiders_dir = module_dir / \"test_spiders\"\n         self.tmpdir = Path(self.mktemp())\n         self.tmpdir.mkdir()\n-        self.spiders_dir = self.tmpdir / 'test_spiders_xxx'\n+        self.spiders_dir = self.tmpdir / \"test_spiders_xxx\"\n         _copytree(orig_spiders_dir, self.spiders_dir)\n         sys.path.append(str(self.tmpdir))\n-        self.settings = Settings({'SPIDER_MODULES': ['test_spiders_xxx']})\n+        self.settings = Settings({\"SPIDER_MODULES\": [\"test_spiders_xxx\"]})\n \n     def tearDown(self):\n-        del sys.modules['test_spiders_xxx']\n+        del sys.modules[\"test_spiders_xxx\"]\n         sys.path.remove(str(self.tmpdir))\n \n     def test_dupename_warning(self):\n         # copy 1 spider module so as to have duplicate spider name\n-        shutil.copyfile(self.tmpdir / 'test_spiders_xxx' / 'spider3.py',\n-                        self.tmpdir / 'test_spiders_xxx' / 'spider3dupe.py')\n+        shutil.copyfile(\n+            self.tmpdir / \"test_spiders_xxx\" / \"spider3.py\",\n+            self.tmpdir / \"test_spiders_xxx\" / \"spider3dupe.py\",\n+        )\n \n         with warnings.catch_warnings(record=True) as w:\n             spider_loader = SpiderLoader.from_settings(self.settings)\n@@ -166,15 +174,19 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n             self.assertNotIn(\"'spider4'\", msg)\n \n             spiders = set(spider_loader.list())\n-            self.assertEqual(spiders, {'spider1', 'spider2', 'spider3', 'spider4'})\n+            self.assertEqual(spiders, {\"spider1\", \"spider2\", \"spider3\", \"spider4\"})\n \n     def test_multiple_dupename_warning(self):\n         # copy 2 spider modules so as to have duplicate spider name\n         # This should issue 2 warning, 1 for each duplicate spider name\n-        shutil.copyfile(self.tmpdir / 'test_spiders_xxx' / 'spider1.py',\n-                        self.tmpdir / 'test_spiders_xxx' / 'spider1dupe.py')\n-        shutil.copyfile(self.tmpdir / 'test_spiders_xxx' / 'spider2.py',\n-                        self.tmpdir / 'test_spiders_xxx' / 'spider2dupe.py')\n+        shutil.copyfile(\n+            self.tmpdir / \"test_spiders_xxx\" / \"spider1.py\",\n+            self.tmpdir / \"test_spiders_xxx\" / \"spider1dupe.py\",\n+        )\n+        shutil.copyfile(\n+            self.tmpdir / \"test_spiders_xxx\" / \"spider2.py\",\n+            self.tmpdir / \"test_spiders_xxx\" / \"spider2dupe.py\",\n+        )\n \n         with warnings.catch_warnings(record=True) as w:\n             spider_loader = SpiderLoader.from_settings(self.settings)\n@@ -192,4 +204,4 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n             self.assertNotIn(\"'spider4'\", msg)\n \n             spiders = set(spider_loader.list())\n-            self.assertEqual(spiders, {'spider1', 'spider2', 'spider3', 'spider4'})\n+            self.assertEqual(spiders, {\"spider1\", \"spider2\", \"spider3\", \"spider4\"})\n\n@@ -3,8 +3,8 @@ from scrapy.spiders import Spider\n \n class Spider4(Spider):\n     name = \"spider4\"\n-    allowed_domains = ['spider4.com']\n+    allowed_domains = [\"spider4.com\"]\n \n     @classmethod\n     def handles_request(cls, request):\n-        return request.url == 'http://spider4.com/onlythis'\n+        return request.url == \"http://spider4.com/onlythis\"\n\n@@ -3,8 +3,8 @@ from scrapy.spiders import Spider\n \n class Spider3(Spider):\n     name = \"spider3\"\n-    allowed_domains = ['spider3.com']\n+    allowed_domains = [\"spider3.com\"]\n \n     @classmethod\n     def handles_request(cls, request):\n-        return request.url == 'http://spider3.com/onlythis'\n+        return request.url == \"http://spider3.com/onlythis\"\n\n@@ -17,12 +17,11 @@ from scrapy.core.spidermw import SpiderMiddlewareManager\n \n \n class SpiderMiddlewareTestCase(TestCase):\n-\n     def setUp(self):\n-        self.request = Request('http://example.com/index.html')\n+        self.request = Request(\"http://example.com/index.html\")\n         self.response = Response(self.request.url, request=self.request)\n-        self.crawler = get_crawler(Spider, {'SPIDER_MIDDLEWARES_BASE': {}})\n-        self.spider = self.crawler._create_spider('foo')\n+        self.crawler = get_crawler(Spider, {\"SPIDER_MIDDLEWARES_BASE\": {}})\n+        self.spider = self.crawler._create_spider(\"foo\")\n         self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n \n     def _scrape_response(self):\n@@ -30,7 +29,9 @@ class SpiderMiddlewareTestCase(TestCase):\n         Raise exception in case of failure.\n         \"\"\"\n         scrape_func = mock.MagicMock()\n-        dfd = self.mwman.scrape_response(scrape_func, self.response, self.request, self.spider)\n+        dfd = self.mwman.scrape_response(\n+            scrape_func, self.response, self.request, self.spider\n+        )\n         # catch deferred result and return the value\n         results = []\n         dfd.addBoth(results.append)\n@@ -43,7 +44,6 @@ class ProcessSpiderInputInvalidOutput(SpiderMiddlewareTestCase):\n     \"\"\"Invalid return value for process_spider_input method\"\"\"\n \n     def test_invalid_process_spider_input(self):\n-\n         class InvalidProcessSpiderInputMiddleware:\n             def process_spider_input(self, response, spider):\n                 return 1\n@@ -58,7 +58,6 @@ class ProcessSpiderOutputInvalidOutput(SpiderMiddlewareTestCase):\n     \"\"\"Invalid return value for process_spider_output method\"\"\"\n \n     def test_invalid_process_spider_output(self):\n-\n         class InvalidProcessSpiderOutputMiddleware:\n             def process_spider_output(self, response, result, spider):\n                 return 1\n@@ -73,7 +72,6 @@ class ProcessSpiderExceptionInvalidOutput(SpiderMiddlewareTestCase):\n     \"\"\"Invalid return value for process_spider_exception method\"\"\"\n \n     def test_invalid_process_spider_exception(self):\n-\n         class InvalidProcessSpiderOutputExceptionMiddleware:\n             def process_spider_exception(self, response, exception, spider):\n                 return 1\n@@ -93,7 +91,6 @@ class ProcessSpiderExceptionReRaise(SpiderMiddlewareTestCase):\n     \"\"\"Re raise the exception by returning None\"\"\"\n \n     def test_process_spider_exception_return_none(self):\n-\n         class ProcessSpiderExceptionReturnNoneMiddleware:\n             def process_spider_exception(self, response, exception, spider):\n                 return None\n@@ -124,23 +121,31 @@ class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n         return {i: c for c, i in enumerate(mw_classes, start=start_index)}\n \n     def _scrape_func(self, *args, **kwargs):\n-        yield {'foo': 1}\n-        yield {'foo': 2}\n-        yield {'foo': 3}\n+        yield {\"foo\": 1}\n+        yield {\"foo\": 2}\n+        yield {\"foo\": 3}\n \n     @defer.inlineCallbacks\n     def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):\n         setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n-        self.crawler = get_crawler(Spider, {'SPIDER_MIDDLEWARES_BASE': {}, 'SPIDER_MIDDLEWARES': setting})\n-        self.spider = self.crawler._create_spider('foo')\n+        self.crawler = get_crawler(\n+            Spider, {\"SPIDER_MIDDLEWARES_BASE\": {}, \"SPIDER_MIDDLEWARES\": setting}\n+        )\n+        self.spider = self.crawler._create_spider(\"foo\")\n         self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n-        result = yield self.mwman.scrape_response(self._scrape_func, self.response, self.request, self.spider)\n+        result = yield self.mwman.scrape_response(\n+            self._scrape_func, self.response, self.request, self.spider\n+        )\n         return result\n \n     @defer.inlineCallbacks\n-    def _test_simple_base(self, *mw_classes, downgrade: bool = False, start_index: Optional[int] = None):\n+    def _test_simple_base(\n+        self, *mw_classes, downgrade: bool = False, start_index: Optional[int] = None\n+    ):\n         with LogCapture() as log:\n-            result = yield self._get_middleware_result(*mw_classes, start_index=start_index)\n+            result = yield self._get_middleware_result(\n+                *mw_classes, start_index=start_index\n+            )\n         self.assertIsInstance(result, collections.abc.Iterable)\n         result_list = list(result)\n         self.assertEqual(len(result_list), self.RESULT_COUNT)\n@@ -148,9 +153,13 @@ class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n         self.assertEqual(\"downgraded to a non-async\" in str(log), downgrade)\n \n     @defer.inlineCallbacks\n-    def _test_asyncgen_base(self, *mw_classes, downgrade: bool = False, start_index: Optional[int] = None):\n+    def _test_asyncgen_base(\n+        self, *mw_classes, downgrade: bool = False, start_index: Optional[int] = None\n+    ):\n         with LogCapture() as log:\n-            result = yield self._get_middleware_result(*mw_classes, start_index=start_index)\n+            result = yield self._get_middleware_result(\n+                *mw_classes, start_index=start_index\n+            )\n         self.assertIsInstance(result, collections.abc.AsyncIterator)\n         result_list = yield deferred_from_coro(collect_asyncgen(result))\n         self.assertEqual(len(result_list), self.RESULT_COUNT)\n@@ -182,20 +191,21 @@ class ProcessSpiderOutputUniversalMiddleware:\n \n class ProcessSpiderExceptionSimpleIterableMiddleware:\n     def process_spider_exception(self, response, exception, spider):\n-        yield {'foo': 1}\n-        yield {'foo': 2}\n-        yield {'foo': 3}\n+        yield {\"foo\": 1}\n+        yield {\"foo\": 2}\n+        yield {\"foo\": 3}\n \n \n class ProcessSpiderExceptionAsyncIterableMiddleware:\n     async def process_spider_exception(self, response, exception, spider):\n-        yield {'foo': 1}\n+        yield {\"foo\": 1}\n         d = defer.Deferred()\n         from twisted.internet import reactor\n+\n         reactor.callLater(0, d.callback, None)\n         await maybe_deferred_to_future(d)\n-        yield {'foo': 2}\n-        yield {'foo': 3}\n+        yield {\"foo\": 2}\n+        yield {\"foo\": 3}\n \n \n class ProcessSpiderOutputSimple(BaseAsyncSpiderMiddlewareTestCase):\n@@ -216,14 +226,11 @@ class ProcessSpiderOutputSimple(BaseAsyncSpiderMiddlewareTestCase):\n \n     def test_simple_asyncgen(self):\n         \"\"\"Simple mw -> asyncgen mw; upgrade\"\"\"\n-        return self._test_asyncgen_base(self.MW_ASYNCGEN,\n-                                        self.MW_SIMPLE)\n+        return self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_SIMPLE)\n \n     def test_asyncgen_simple(self):\n         \"\"\"Asyncgen mw -> simple mw; upgrade then downgrade\"\"\"\n-        return self._test_simple_base(self.MW_SIMPLE,\n-                                      self.MW_ASYNCGEN,\n-                                      downgrade=True)\n+        return self._test_simple_base(self.MW_SIMPLE, self.MW_ASYNCGEN, downgrade=True)\n \n     def test_universal(self):\n         \"\"\"Universal mw\"\"\"\n@@ -231,23 +238,19 @@ class ProcessSpiderOutputSimple(BaseAsyncSpiderMiddlewareTestCase):\n \n     def test_universal_simple(self):\n         \"\"\"Universal mw -> simple mw\"\"\"\n-        return self._test_simple_base(self.MW_SIMPLE,\n-                                      self.MW_UNIVERSAL)\n+        return self._test_simple_base(self.MW_SIMPLE, self.MW_UNIVERSAL)\n \n     def test_simple_universal(self):\n         \"\"\"Simple mw -> universal mw\"\"\"\n-        return self._test_simple_base(self.MW_UNIVERSAL,\n-                                      self.MW_SIMPLE)\n+        return self._test_simple_base(self.MW_UNIVERSAL, self.MW_SIMPLE)\n \n     def test_universal_asyncgen(self):\n         \"\"\"Universal mw -> asyncgen mw; upgrade\"\"\"\n-        return self._test_asyncgen_base(self.MW_ASYNCGEN,\n-                                        self.MW_UNIVERSAL)\n+        return self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_UNIVERSAL)\n \n     def test_asyncgen_universal(self):\n         \"\"\"Asyncgen mw -> universal mw; upgrade\"\"\"\n-        return self._test_asyncgen_base(self.MW_UNIVERSAL,\n-                                        self.MW_ASYNCGEN)\n+        return self._test_asyncgen_base(self.MW_UNIVERSAL, self.MW_ASYNCGEN)\n \n \n class ProcessSpiderOutputAsyncGen(ProcessSpiderOutputSimple):\n@@ -259,14 +262,13 @@ class ProcessSpiderOutputAsyncGen(ProcessSpiderOutputSimple):\n \n     def test_simple(self):\n         \"\"\"Simple mw; downgrade\"\"\"\n-        return self._test_simple_base(self.MW_SIMPLE,\n-                                      downgrade=True)\n+        return self._test_simple_base(self.MW_SIMPLE, downgrade=True)\n \n     def test_simple_asyncgen(self):\n         \"\"\"Simple mw -> asyncgen mw; downgrade then upgrade\"\"\"\n-        return self._test_asyncgen_base(self.MW_ASYNCGEN,\n-                                        self.MW_SIMPLE,\n-                                        downgrade=True)\n+        return self._test_asyncgen_base(\n+            self.MW_ASYNCGEN, self.MW_SIMPLE, downgrade=True\n+        )\n \n     def test_universal(self):\n         \"\"\"Universal mw\"\"\"\n@@ -274,15 +276,11 @@ class ProcessSpiderOutputAsyncGen(ProcessSpiderOutputSimple):\n \n     def test_universal_simple(self):\n         \"\"\"Universal mw -> simple mw; downgrade\"\"\"\n-        return self._test_simple_base(self.MW_SIMPLE,\n-                                      self.MW_UNIVERSAL,\n-                                      downgrade=True)\n+        return self._test_simple_base(self.MW_SIMPLE, self.MW_UNIVERSAL, downgrade=True)\n \n     def test_simple_universal(self):\n         \"\"\"Simple mw -> universal mw; downgrade\"\"\"\n-        return self._test_simple_base(self.MW_UNIVERSAL,\n-                                      self.MW_SIMPLE,\n-                                      downgrade=True)\n+        return self._test_simple_base(self.MW_UNIVERSAL, self.MW_SIMPLE, downgrade=True)\n \n \n class ProcessSpiderOutputNonIterableMiddleware:\n@@ -299,7 +297,6 @@ class ProcessSpiderOutputCoroutineMiddleware:\n \n \n class ProcessSpiderOutputInvalidResult(BaseAsyncSpiderMiddlewareTestCase):\n-\n     @defer.inlineCallbacks\n     def test_non_iterable(self):\n         with self.assertRaisesRegex(\n@@ -338,13 +335,15 @@ class ProcessStartRequestsSimple(BaseAsyncSpiderMiddlewareTestCase):\n \n     def _start_requests(self):\n         for i in range(3):\n-            yield Request(f'https://example.com/{i}', dont_filter=True)\n+            yield Request(f\"https://example.com/{i}\", dont_filter=True)\n \n     @defer.inlineCallbacks\n     def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):\n         setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n-        self.crawler = get_crawler(Spider, {'SPIDER_MIDDLEWARES_BASE': {}, 'SPIDER_MIDDLEWARES': setting})\n-        self.spider = self.crawler._create_spider('foo')\n+        self.crawler = get_crawler(\n+            Spider, {\"SPIDER_MIDDLEWARES_BASE\": {}, \"SPIDER_MIDDLEWARES\": setting}\n+        )\n+        self.spider = self.crawler._create_spider(\"foo\")\n         self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n         start_requests = iter(self._start_requests())\n         results = yield self.mwman.process_start_requests(start_requests, self.spider)\n@@ -383,41 +382,57 @@ class UniversalMiddlewareManagerTest(TestCase):\n     def test_simple_mw(self):\n         mw = ProcessSpiderOutputSimpleMiddleware\n         self.mwman._add_middleware(mw)\n-        self.assertEqual(self.mwman.methods['process_spider_output'][0], mw.process_spider_output)\n+        self.assertEqual(\n+            self.mwman.methods[\"process_spider_output\"][0], mw.process_spider_output\n+        )\n \n     def test_async_mw(self):\n         mw = ProcessSpiderOutputAsyncGenMiddleware\n         self.mwman._add_middleware(mw)\n-        self.assertEqual(self.mwman.methods['process_spider_output'][0], mw.process_spider_output)\n+        self.assertEqual(\n+            self.mwman.methods[\"process_spider_output\"][0], mw.process_spider_output\n+        )\n \n     def test_universal_mw(self):\n         mw = ProcessSpiderOutputUniversalMiddleware\n         self.mwman._add_middleware(mw)\n-        self.assertEqual(self.mwman.methods['process_spider_output'][0],\n-                         (mw.process_spider_output, mw.process_spider_output_async))\n+        self.assertEqual(\n+            self.mwman.methods[\"process_spider_output\"][0],\n+            (mw.process_spider_output, mw.process_spider_output_async),\n+        )\n \n     def test_universal_mw_no_sync(self):\n         with LogCapture() as log:\n             self.mwman._add_middleware(UniversalMiddlewareNoSync)\n-        self.assertIn(\"UniversalMiddlewareNoSync has process_spider_output_async\"\n-                      \" without process_spider_output\", str(log))\n-        self.assertEqual(self.mwman.methods['process_spider_output'][0], None)\n+        self.assertIn(\n+            \"UniversalMiddlewareNoSync has process_spider_output_async\"\n+            \" without process_spider_output\",\n+            str(log),\n+        )\n+        self.assertEqual(self.mwman.methods[\"process_spider_output\"][0], None)\n \n     def test_universal_mw_both_sync(self):\n         mw = UniversalMiddlewareBothSync\n         with LogCapture() as log:\n             self.mwman._add_middleware(mw)\n-        self.assertIn(\"UniversalMiddlewareBothSync.process_spider_output_async \"\n-                      \"is not an async generator function\", str(log))\n-        self.assertEqual(self.mwman.methods['process_spider_output'][0], mw.process_spider_output)\n+        self.assertIn(\n+            \"UniversalMiddlewareBothSync.process_spider_output_async \"\n+            \"is not an async generator function\",\n+            str(log),\n+        )\n+        self.assertEqual(\n+            self.mwman.methods[\"process_spider_output\"][0], mw.process_spider_output\n+        )\n \n     def test_universal_mw_both_async(self):\n         with LogCapture() as log:\n             self.mwman._add_middleware(UniversalMiddlewareBothAsync)\n-        self.assertIn(\"UniversalMiddlewareBothAsync.process_spider_output \"\n+        self.assertIn(\n+            \"UniversalMiddlewareBothAsync.process_spider_output \"\n             \"is an async generator function while process_spider_output_async exists\",\n-                      str(log))\n-        self.assertEqual(self.mwman.methods['process_spider_output'][0], None)\n+            str(log),\n+        )\n+        self.assertEqual(self.mwman.methods[\"process_spider_output\"][0], None)\n \n \n class BuiltinMiddlewareSimpleTest(BaseAsyncSpiderMiddlewareTestCase):\n@@ -429,10 +444,12 @@ class BuiltinMiddlewareSimpleTest(BaseAsyncSpiderMiddlewareTestCase):\n     @defer.inlineCallbacks\n     def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):\n         setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n-        self.crawler = get_crawler(Spider, {'SPIDER_MIDDLEWARES': setting})\n-        self.spider = self.crawler._create_spider('foo')\n+        self.crawler = get_crawler(Spider, {\"SPIDER_MIDDLEWARES\": setting})\n+        self.spider = self.crawler._create_spider(\"foo\")\n         self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n-        result = yield self.mwman.scrape_response(self._scrape_func, self.response, self.request, self.spider)\n+        result = yield self.mwman.scrape_response(\n+            self._scrape_func, self.response, self.request, self.spider\n+        )\n         return result\n \n     def test_just_builtin(self):\n@@ -501,7 +518,9 @@ class ProcessSpiderExceptionTest(BaseAsyncSpiderMiddlewareTestCase):\n \n     @defer.inlineCallbacks\n     def _test_asyncgen_nodowngrade(self, *mw_classes):\n-        with self.assertRaisesRegex(_InvalidOutput, \"Async iterable returned from .+ cannot be downgraded\"):\n+        with self.assertRaisesRegex(\n+            _InvalidOutput, \"Async iterable returned from .+ cannot be downgraded\"\n+        ):\n             yield self._get_middleware_result(*mw_classes)\n \n     def test_exc_simple(self):\n@@ -514,20 +533,16 @@ class ProcessSpiderExceptionTest(BaseAsyncSpiderMiddlewareTestCase):\n \n     def test_exc_simple_simple(self):\n         \"\"\"Simple exc mw -> simple output mw\"\"\"\n-        return self._test_simple_base(self.MW_SIMPLE,\n-                                      self.MW_EXC_SIMPLE)\n+        return self._test_simple_base(self.MW_SIMPLE, self.MW_EXC_SIMPLE)\n \n     def test_exc_async_async(self):\n         \"\"\"Async exc mw -> async output mw\"\"\"\n-        return self._test_asyncgen_base(self.MW_ASYNCGEN,\n-                                        self.MW_EXC_ASYNCGEN)\n+        return self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_EXC_ASYNCGEN)\n \n     def test_exc_simple_async(self):\n         \"\"\"Simple exc mw -> async output mw; upgrade\"\"\"\n-        return self._test_asyncgen_base(self.MW_ASYNCGEN,\n-                                        self.MW_EXC_SIMPLE)\n+        return self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_EXC_SIMPLE)\n \n     def test_exc_async_simple(self):\n         \"\"\"Async exc mw -> simple output mw; cannot work as downgrading is not supported\"\"\"\n-        return self._test_asyncgen_nodowngrade(self.MW_SIMPLE,\n-                                               self.MW_EXC_ASYNCGEN)\n+        return self._test_asyncgen_nodowngrade(self.MW_SIMPLE, self.MW_EXC_ASYNCGEN)\n\n@@ -8,10 +8,9 @@ from scrapy.utils.test import get_crawler\n \n \n class TestDepthMiddleware(TestCase):\n-\n     def setUp(self):\n         crawler = get_crawler(Spider)\n-        self.spider = crawler._create_spider('scrapytest.org')\n+        self.spider = crawler._create_spider(\"scrapytest.org\")\n \n         self.stats = StatsCollector(crawler)\n         self.stats.open_spider(self.spider)\n@@ -19,24 +18,24 @@ class TestDepthMiddleware(TestCase):\n         self.mw = DepthMiddleware(1, self.stats, True)\n \n     def test_process_spider_output(self):\n-        req = Request('http://scrapytest.org')\n-        resp = Response('http://scrapytest.org')\n+        req = Request(\"http://scrapytest.org\")\n+        resp = Response(\"http://scrapytest.org\")\n         resp.request = req\n-        result = [Request('http://scrapytest.org')]\n+        result = [Request(\"http://scrapytest.org\")]\n \n         out = list(self.mw.process_spider_output(resp, result, self.spider))\n         self.assertEqual(out, result)\n \n-        rdc = self.stats.get_value('request_depth_count/1', spider=self.spider)\n+        rdc = self.stats.get_value(\"request_depth_count/1\", spider=self.spider)\n         self.assertEqual(rdc, 1)\n \n-        req.meta['depth'] = 1\n+        req.meta[\"depth\"] = 1\n \n         out2 = list(self.mw.process_spider_output(resp, result, self.spider))\n         self.assertEqual(out2, [])\n \n-        rdm = self.stats.get_value('request_depth_max', spider=self.spider)\n+        rdm = self.stats.get_value(\"request_depth_max\", spider=self.spider)\n         self.assertEqual(rdm, 1)\n \n     def tearDown(self):\n-        self.stats.close_spider(self.spider, '')\n+        self.stats.close_spider(self.spider, \"\")\n\n@@ -15,7 +15,7 @@ from tests.spiders import MockServerSpider\n \n \n class _HttpErrorSpider(MockServerSpider):\n-    name = 'httperror'\n+    name = \"httperror\"\n     bypass_status_codes = set()\n \n     def __init__(self, *args, **kwargs):\n@@ -59,28 +59,35 @@ def _responses(request, status_codes):\n \n \n class TestHttpErrorMiddleware(TestCase):\n-\n     def setUp(self):\n         crawler = get_crawler(Spider)\n-        self.spider = Spider.from_crawler(crawler, name='foo')\n+        self.spider = Spider.from_crawler(crawler, name=\"foo\")\n         self.mw = HttpErrorMiddleware(Settings({}))\n-        self.req = Request('http://scrapytest.org')\n+        self.req = Request(\"http://scrapytest.org\")\n         self.res200, self.res404 = _responses(self.req, [200, 404])\n \n     def test_process_spider_input(self):\n         self.assertIsNone(self.mw.process_spider_input(self.res200, self.spider))\n-        self.assertRaises(HttpError, self.mw.process_spider_input, self.res404, self.spider)\n+        self.assertRaises(\n+            HttpError, self.mw.process_spider_input, self.res404, self.spider\n+        )\n \n     def test_process_spider_exception(self):\n         self.assertEqual(\n             [],\n-            self.mw.process_spider_exception(self.res404, HttpError(self.res404), self.spider))\n-        self.assertIsNone(self.mw.process_spider_exception(self.res404, Exception(), self.spider))\n+            self.mw.process_spider_exception(\n+                self.res404, HttpError(self.res404), self.spider\n+            ),\n+        )\n+        self.assertIsNone(\n+            self.mw.process_spider_exception(self.res404, Exception(), self.spider)\n+        )\n \n     def test_handle_httpstatus_list(self):\n         res = self.res404.copy()\n-        res.request = Request('http://scrapytest.org',\n-                              meta={'handle_httpstatus_list': [404]})\n+        res.request = Request(\n+            \"http://scrapytest.org\", meta={\"handle_httpstatus_list\": [404]}\n+        )\n         self.assertIsNone(self.mw.process_spider_input(res, self.spider))\n \n         self.spider.handle_httpstatus_list = [404]\n@@ -91,18 +98,22 @@ class TestHttpErrorMiddlewareSettings(TestCase):\n     \"\"\"Similar test, but with settings\"\"\"\n \n     def setUp(self):\n-        self.spider = Spider('foo')\n-        self.mw = HttpErrorMiddleware(Settings({'HTTPERROR_ALLOWED_CODES': (402,)}))\n-        self.req = Request('http://scrapytest.org')\n+        self.spider = Spider(\"foo\")\n+        self.mw = HttpErrorMiddleware(Settings({\"HTTPERROR_ALLOWED_CODES\": (402,)}))\n+        self.req = Request(\"http://scrapytest.org\")\n         self.res200, self.res404, self.res402 = _responses(self.req, [200, 404, 402])\n \n     def test_process_spider_input(self):\n         self.assertIsNone(self.mw.process_spider_input(self.res200, self.spider))\n-        self.assertRaises(HttpError, self.mw.process_spider_input, self.res404, self.spider)\n+        self.assertRaises(\n+            HttpError, self.mw.process_spider_input, self.res404, self.spider\n+        )\n         self.assertIsNone(self.mw.process_spider_input(self.res402, self.spider))\n \n     def test_meta_overrides_settings(self):\n-        request = Request('http://scrapytest.org', meta={'handle_httpstatus_list': [404]})\n+        request = Request(\n+            \"http://scrapytest.org\", meta={\"handle_httpstatus_list\": [404]}\n+        )\n         res404 = self.res404.copy()\n         res404.request = request\n         res402 = self.res402.copy()\n@@ -114,15 +125,16 @@ class TestHttpErrorMiddlewareSettings(TestCase):\n     def test_spider_override_settings(self):\n         self.spider.handle_httpstatus_list = [404]\n         self.assertIsNone(self.mw.process_spider_input(self.res404, self.spider))\n-        self.assertRaises(HttpError, self.mw.process_spider_input, self.res402, self.spider)\n+        self.assertRaises(\n+            HttpError, self.mw.process_spider_input, self.res402, self.spider\n+        )\n \n \n class TestHttpErrorMiddlewareHandleAll(TestCase):\n-\n     def setUp(self):\n-        self.spider = Spider('foo')\n-        self.mw = HttpErrorMiddleware(Settings({'HTTPERROR_ALLOW_ALL': True}))\n-        self.req = Request('http://scrapytest.org')\n+        self.spider = Spider(\"foo\")\n+        self.mw = HttpErrorMiddleware(Settings({\"HTTPERROR_ALLOW_ALL\": True}))\n+        self.req = Request(\"http://scrapytest.org\")\n         self.res200, self.res404, self.res402 = _responses(self.req, [200, 404, 402])\n \n     def test_process_spider_input(self):\n@@ -130,7 +142,9 @@ class TestHttpErrorMiddlewareHandleAll(TestCase):\n         self.assertIsNone(self.mw.process_spider_input(self.res404, self.spider))\n \n     def test_meta_overrides_settings(self):\n-        request = Request('http://scrapytest.org', meta={'handle_httpstatus_list': [404]})\n+        request = Request(\n+            \"http://scrapytest.org\", meta={\"handle_httpstatus_list\": [404]}\n+        )\n         res404 = self.res404.copy()\n         res404.request = request\n         res402 = self.res402.copy()\n@@ -142,8 +156,12 @@ class TestHttpErrorMiddlewareHandleAll(TestCase):\n     def test_httperror_allow_all_false(self):\n         crawler = get_crawler(_HttpErrorSpider)\n         mw = HttpErrorMiddleware.from_crawler(crawler)\n-        request_httpstatus_false = Request('http://scrapytest.org', meta={'handle_httpstatus_all': False})\n-        request_httpstatus_true = Request('http://scrapytest.org', meta={'handle_httpstatus_all': True})\n+        request_httpstatus_false = Request(\n+            \"http://scrapytest.org\", meta={\"handle_httpstatus_all\": False}\n+        )\n+        request_httpstatus_true = Request(\n+            \"http://scrapytest.org\", meta={\"handle_httpstatus_all\": True}\n+        )\n         res404 = self.res404.copy()\n         res404.request = request_httpstatus_false\n         res402 = self.res402.copy()\n@@ -166,28 +184,28 @@ class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n         crawler = get_crawler(_HttpErrorSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n         assert not crawler.spider.skipped, crawler.spider.skipped\n-        self.assertEqual(crawler.spider.parsed, {'200'})\n-        self.assertEqual(crawler.spider.failed, {'404', '402', '500'})\n+        self.assertEqual(crawler.spider.parsed, {\"200\"})\n+        self.assertEqual(crawler.spider.failed, {\"404\", \"402\", \"500\"})\n \n         get_value = crawler.stats.get_value\n-        self.assertEqual(get_value('httperror/response_ignored_count'), 3)\n-        self.assertEqual(get_value('httperror/response_ignored_status_count/404'), 1)\n-        self.assertEqual(get_value('httperror/response_ignored_status_count/402'), 1)\n-        self.assertEqual(get_value('httperror/response_ignored_status_count/500'), 1)\n+        self.assertEqual(get_value(\"httperror/response_ignored_count\"), 3)\n+        self.assertEqual(get_value(\"httperror/response_ignored_status_count/404\"), 1)\n+        self.assertEqual(get_value(\"httperror/response_ignored_status_count/402\"), 1)\n+        self.assertEqual(get_value(\"httperror/response_ignored_status_count/500\"), 1)\n \n     @defer.inlineCallbacks\n     def test_logging(self):\n         crawler = get_crawler(_HttpErrorSpider)\n         with LogCapture() as log:\n             yield crawler.crawl(mockserver=self.mockserver, bypass_status_codes={402})\n-        self.assertEqual(crawler.spider.parsed, {'200', '402'})\n-        self.assertEqual(crawler.spider.skipped, {'402'})\n-        self.assertEqual(crawler.spider.failed, {'404', '500'})\n+        self.assertEqual(crawler.spider.parsed, {\"200\", \"402\"})\n+        self.assertEqual(crawler.spider.skipped, {\"402\"})\n+        self.assertEqual(crawler.spider.failed, {\"404\", \"500\"})\n \n-        self.assertIn('Ignoring response <404', str(log))\n-        self.assertIn('Ignoring response <500', str(log))\n-        self.assertNotIn('Ignoring response <200', str(log))\n-        self.assertNotIn('Ignoring response <402', str(log))\n+        self.assertIn(\"Ignoring response <404\", str(log))\n+        self.assertIn(\"Ignoring response <500\", str(log))\n+        self.assertNotIn(\"Ignoring response <200\", str(log))\n+        self.assertNotIn(\"Ignoring response <402\", str(log))\n \n     @defer.inlineCallbacks\n     def test_logging_level(self):\n@@ -195,22 +213,22 @@ class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n         crawler = get_crawler(_HttpErrorSpider)\n         with LogCapture(level=logging.INFO) as log:\n             yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertEqual(crawler.spider.parsed, {'200'})\n-        self.assertEqual(crawler.spider.failed, {'404', '402', '500'})\n+        self.assertEqual(crawler.spider.parsed, {\"200\"})\n+        self.assertEqual(crawler.spider.failed, {\"404\", \"402\", \"500\"})\n \n-        self.assertIn('Ignoring response <402', str(log))\n-        self.assertIn('Ignoring response <404', str(log))\n-        self.assertIn('Ignoring response <500', str(log))\n-        self.assertNotIn('Ignoring response <200', str(log))\n+        self.assertIn(\"Ignoring response <402\", str(log))\n+        self.assertIn(\"Ignoring response <404\", str(log))\n+        self.assertIn(\"Ignoring response <500\", str(log))\n+        self.assertNotIn(\"Ignoring response <200\", str(log))\n \n         # with level WARNING, we shouldn't capture anything from HttpError\n         crawler = get_crawler(_HttpErrorSpider)\n         with LogCapture(level=logging.WARNING) as log:\n             yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertEqual(crawler.spider.parsed, {'200'})\n-        self.assertEqual(crawler.spider.failed, {'404', '402', '500'})\n+        self.assertEqual(crawler.spider.parsed, {\"200\"})\n+        self.assertEqual(crawler.spider.failed, {\"404\", \"402\", \"500\"})\n \n-        self.assertNotIn('Ignoring response <402', str(log))\n-        self.assertNotIn('Ignoring response <404', str(log))\n-        self.assertNotIn('Ignoring response <500', str(log))\n-        self.assertNotIn('Ignoring response <200', str(log))\n+        self.assertNotIn(\"Ignoring response <402\", str(log))\n+        self.assertNotIn(\"Ignoring response <404\", str(log))\n+        self.assertNotIn(\"Ignoring response <500\", str(log))\n+        self.assertNotIn(\"Ignoring response <200\", str(log))\n\n@@ -9,7 +9,6 @@ from scrapy.utils.test import get_crawler\n \n \n class TestOffsiteMiddleware(TestCase):\n-\n     def setUp(self):\n         crawler = get_crawler(Spider)\n         self.spider = crawler._create_spider(**self._get_spiderargs())\n@@ -17,28 +16,31 @@ class TestOffsiteMiddleware(TestCase):\n         self.mw.spider_opened(self.spider)\n \n     def _get_spiderargs(self):\n-        return dict(name='foo', allowed_domains=['scrapytest.org', 'scrapy.org', 'scrapy.test.org'])\n+        return dict(\n+            name=\"foo\",\n+            allowed_domains=[\"scrapytest.org\", \"scrapy.org\", \"scrapy.test.org\"],\n+        )\n \n     def test_process_spider_output(self):\n-        res = Response('http://scrapytest.org')\n+        res = Response(\"http://scrapytest.org\")\n \n         onsite_reqs = [\n-            Request('http://scrapytest.org/1'),\n-            Request('http://scrapy.org/1'),\n-            Request('http://sub.scrapy.org/1'),\n-            Request('http://offsite.tld/letmepass', dont_filter=True),\n-            Request('http://scrapy.test.org/'),\n-            Request('http://scrapy.test.org:8000/'),\n+            Request(\"http://scrapytest.org/1\"),\n+            Request(\"http://scrapy.org/1\"),\n+            Request(\"http://sub.scrapy.org/1\"),\n+            Request(\"http://offsite.tld/letmepass\", dont_filter=True),\n+            Request(\"http://scrapy.test.org/\"),\n+            Request(\"http://scrapy.test.org:8000/\"),\n         ]\n         offsite_reqs = [\n-            Request('http://scrapy2.org'),\n-            Request('http://offsite.tld/'),\n-            Request('http://offsite.tld/scrapytest.org'),\n-            Request('http://offsite.tld/rogue.scrapytest.org'),\n-            Request('http://rogue.scrapytest.org.haha.com'),\n-            Request('http://roguescrapytest.org'),\n-            Request('http://test.org/'),\n-            Request('http://notscrapy.test.org/'),\n+            Request(\"http://scrapy2.org\"),\n+            Request(\"http://offsite.tld/\"),\n+            Request(\"http://offsite.tld/scrapytest.org\"),\n+            Request(\"http://offsite.tld/rogue.scrapytest.org\"),\n+            Request(\"http://rogue.scrapytest.org.haha.com\"),\n+            Request(\"http://roguescrapytest.org\"),\n+            Request(\"http://test.org/\"),\n+            Request(\"http://notscrapy.test.org/\"),\n         ]\n         reqs = onsite_reqs + offsite_reqs\n \n@@ -47,40 +49,40 @@ class TestOffsiteMiddleware(TestCase):\n \n \n class TestOffsiteMiddleware2(TestOffsiteMiddleware):\n-\n     def _get_spiderargs(self):\n-        return dict(name='foo', allowed_domains=None)\n+        return dict(name=\"foo\", allowed_domains=None)\n \n     def test_process_spider_output(self):\n-        res = Response('http://scrapytest.org')\n-        reqs = [Request('http://a.com/b.html'), Request('http://b.com/1')]\n+        res = Response(\"http://scrapytest.org\")\n+        reqs = [Request(\"http://a.com/b.html\"), Request(\"http://b.com/1\")]\n         out = list(self.mw.process_spider_output(res, reqs, self.spider))\n         self.assertEqual(out, reqs)\n \n \n class TestOffsiteMiddleware3(TestOffsiteMiddleware2):\n-\n     def _get_spiderargs(self):\n-        return dict(name='foo')\n+        return dict(name=\"foo\")\n \n \n class TestOffsiteMiddleware4(TestOffsiteMiddleware3):\n-\n     def _get_spiderargs(self):\n-        bad_hostname = urlparse('http:////scrapytest.org').hostname\n-        return dict(name='foo', allowed_domains=['scrapytest.org', None, bad_hostname])\n+        bad_hostname = urlparse(\"http:////scrapytest.org\").hostname\n+        return dict(name=\"foo\", allowed_domains=[\"scrapytest.org\", None, bad_hostname])\n \n     def test_process_spider_output(self):\n-        res = Response('http://scrapytest.org')\n-        reqs = [Request('http://scrapytest.org/1')]\n+        res = Response(\"http://scrapytest.org\")\n+        reqs = [Request(\"http://scrapytest.org/1\")]\n         out = list(self.mw.process_spider_output(res, reqs, self.spider))\n         self.assertEqual(out, reqs)\n \n \n class TestOffsiteMiddleware5(TestOffsiteMiddleware4):\n-\n     def test_get_host_regex(self):\n-        self.spider.allowed_domains = ['http://scrapytest.org', 'scrapy.org', 'scrapy.test.org']\n+        self.spider.allowed_domains = [\n+            \"http://scrapytest.org\",\n+            \"scrapy.org\",\n+            \"scrapy.test.org\",\n+        ]\n         with warnings.catch_warnings(record=True) as w:\n             warnings.simplefilter(\"always\")\n             self.mw.get_host_regex(self.spider)\n@@ -88,9 +90,12 @@ class TestOffsiteMiddleware5(TestOffsiteMiddleware4):\n \n \n class TestOffsiteMiddleware6(TestOffsiteMiddleware4):\n-\n     def test_get_host_regex(self):\n-        self.spider.allowed_domains = ['scrapytest.org:8000', 'scrapy.org', 'scrapy.test.org']\n+        self.spider.allowed_domains = [\n+            \"scrapytest.org:8000\",\n+            \"scrapy.org\",\n+            \"scrapy.test.org\",\n+        ]\n         with warnings.catch_warnings(record=True) as w:\n             warnings.simplefilter(\"always\")\n             self.mw.get_host_regex(self.spider)\n\n@@ -9,7 +9,9 @@ from tests.mockserver import MockServer\n \n class LogExceptionMiddleware:\n     def process_spider_exception(self, response, exception, spider):\n-        spider.logger.info('Middleware: %s exception caught', exception.__class__.__name__)\n+        spider.logger.info(\n+            \"Middleware: %s exception caught\", exception.__class__.__name__\n+        )\n         return None\n \n \n@@ -17,34 +19,36 @@ class LogExceptionMiddleware:\n # (0) recover from an exception on a spider callback\n class RecoveryMiddleware:\n     def process_spider_exception(self, response, exception, spider):\n-        spider.logger.info('Middleware: %s exception caught', exception.__class__.__name__)\n+        spider.logger.info(\n+            \"Middleware: %s exception caught\", exception.__class__.__name__\n+        )\n         return [\n-            {'from': 'process_spider_exception'},\n-            Request(response.url, meta={'dont_fail': True}, dont_filter=True),\n+            {\"from\": \"process_spider_exception\"},\n+            Request(response.url, meta={\"dont_fail\": True}, dont_filter=True),\n         ]\n \n \n class RecoverySpider(Spider):\n-    name = 'RecoverySpider'\n+    name = \"RecoverySpider\"\n     custom_settings = {\n-        'SPIDER_MIDDLEWARES_BASE': {},\n-        'SPIDER_MIDDLEWARES': {\n+        \"SPIDER_MIDDLEWARES_BASE\": {},\n+        \"SPIDER_MIDDLEWARES\": {\n             RecoveryMiddleware: 10,\n         },\n     }\n \n     def start_requests(self):\n-        yield Request(self.mockserver.url('/status?n=200'))\n+        yield Request(self.mockserver.url(\"/status?n=200\"))\n \n     def parse(self, response):\n-        yield {'test': 1}\n-        self.logger.info('DONT_FAIL: %s', response.meta.get('dont_fail'))\n-        if not response.meta.get('dont_fail'):\n+        yield {\"test\": 1}\n+        self.logger.info(\"DONT_FAIL: %s\", response.meta.get(\"dont_fail\"))\n+        if not response.meta.get(\"dont_fail\"):\n             raise TabError()\n \n \n class RecoveryAsyncGenSpider(RecoverySpider):\n-    name = 'RecoveryAsyncGenSpider'\n+    name = \"RecoveryAsyncGenSpider\"\n \n     async def parse(self, response):\n         for r in super().parse(response):\n@@ -55,14 +59,14 @@ class RecoveryAsyncGenSpider(RecoverySpider):\n # (1) exceptions from a spider middleware's process_spider_input method\n class FailProcessSpiderInputMiddleware:\n     def process_spider_input(self, response, spider):\n-        spider.logger.info('Middleware: will raise IndexError')\n+        spider.logger.info(\"Middleware: will raise IndexError\")\n         raise IndexError()\n \n \n class ProcessSpiderInputSpiderWithoutErrback(Spider):\n-    name = 'ProcessSpiderInputSpiderWithoutErrback'\n+    name = \"ProcessSpiderInputSpiderWithoutErrback\"\n     custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n+        \"SPIDER_MIDDLEWARES\": {\n             # spider\n             FailProcessSpiderInputMiddleware: 8,\n             LogExceptionMiddleware: 6,\n@@ -71,55 +75,57 @@ class ProcessSpiderInputSpiderWithoutErrback(Spider):\n     }\n \n     def start_requests(self):\n-        yield Request(url=self.mockserver.url('/status?n=200'), callback=self.parse)\n+        yield Request(url=self.mockserver.url(\"/status?n=200\"), callback=self.parse)\n \n     def parse(self, response):\n-        return {'from': 'callback'}\n+        return {\"from\": \"callback\"}\n \n \n class ProcessSpiderInputSpiderWithErrback(ProcessSpiderInputSpiderWithoutErrback):\n-    name = 'ProcessSpiderInputSpiderWithErrback'\n+    name = \"ProcessSpiderInputSpiderWithErrback\"\n \n     def start_requests(self):\n-        yield Request(self.mockserver.url('/status?n=200'), self.parse, errback=self.errback)\n+        yield Request(\n+            self.mockserver.url(\"/status?n=200\"), self.parse, errback=self.errback\n+        )\n \n     def errback(self, failure):\n-        self.logger.info('Got a Failure on the Request errback')\n-        return {'from': 'errback'}\n+        self.logger.info(\"Got a Failure on the Request errback\")\n+        return {\"from\": \"errback\"}\n \n \n # ================================================================================\n # (2) exceptions from a spider callback (generator)\n class GeneratorCallbackSpider(Spider):\n-    name = 'GeneratorCallbackSpider'\n+    name = \"GeneratorCallbackSpider\"\n     custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n+        \"SPIDER_MIDDLEWARES\": {\n             LogExceptionMiddleware: 10,\n         },\n     }\n \n     def start_requests(self):\n-        yield Request(self.mockserver.url('/status?n=200'))\n+        yield Request(self.mockserver.url(\"/status?n=200\"))\n \n     def parse(self, response):\n-        yield {'test': 1}\n-        yield {'test': 2}\n+        yield {\"test\": 1}\n+        yield {\"test\": 2}\n         raise ImportError()\n \n \n class AsyncGeneratorCallbackSpider(GeneratorCallbackSpider):\n     async def parse(self, response):\n-        yield {'test': 1}\n-        yield {'test': 2}\n+        yield {\"test\": 1}\n+        yield {\"test\": 2}\n         raise ImportError()\n \n \n # ================================================================================\n # (2.1) exceptions from a spider callback (generator, middleware right after callback)\n class GeneratorCallbackSpiderMiddlewareRightAfterSpider(GeneratorCallbackSpider):\n-    name = 'GeneratorCallbackSpiderMiddlewareRightAfterSpider'\n+    name = \"GeneratorCallbackSpiderMiddlewareRightAfterSpider\"\n     custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n+        \"SPIDER_MIDDLEWARES\": {\n             LogExceptionMiddleware: 100000,\n         },\n     }\n@@ -128,26 +134,26 @@ class GeneratorCallbackSpiderMiddlewareRightAfterSpider(GeneratorCallbackSpider)\n # ================================================================================\n # (3) exceptions from a spider callback (not a generator)\n class NotGeneratorCallbackSpider(Spider):\n-    name = 'NotGeneratorCallbackSpider'\n+    name = \"NotGeneratorCallbackSpider\"\n     custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n+        \"SPIDER_MIDDLEWARES\": {\n             LogExceptionMiddleware: 10,\n         },\n     }\n \n     def start_requests(self):\n-        yield Request(self.mockserver.url('/status?n=200'))\n+        yield Request(self.mockserver.url(\"/status?n=200\"))\n \n     def parse(self, response):\n-        return [{'test': 1}, {'test': 1 / 0}]\n+        return [{\"test\": 1}, {\"test\": 1 / 0}]\n \n \n # ================================================================================\n # (3.1) exceptions from a spider callback (not a generator, middleware right after callback)\n class NotGeneratorCallbackSpiderMiddlewareRightAfterSpider(NotGeneratorCallbackSpider):\n-    name = 'NotGeneratorCallbackSpiderMiddlewareRightAfterSpider'\n+    name = \"NotGeneratorCallbackSpiderMiddlewareRightAfterSpider\"\n     custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n+        \"SPIDER_MIDDLEWARES\": {\n             LogExceptionMiddleware: 100000,\n         },\n     }\n@@ -158,26 +164,26 @@ class NotGeneratorCallbackSpiderMiddlewareRightAfterSpider(NotGeneratorCallbackS\n class _GeneratorDoNothingMiddleware:\n     def process_spider_output(self, response, result, spider):\n         for r in result:\n-            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')\n+            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n             yield r\n \n     def process_spider_exception(self, response, exception, spider):\n-        method = f'{self.__class__.__name__}.process_spider_exception'\n-        spider.logger.info('%s: %s caught', method, exception.__class__.__name__)\n+        method = f\"{self.__class__.__name__}.process_spider_exception\"\n+        spider.logger.info(\"%s: %s caught\", method, exception.__class__.__name__)\n         return None\n \n \n class GeneratorFailMiddleware:\n     def process_spider_output(self, response, result, spider):\n         for r in result:\n-            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')\n+            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n             yield r\n             raise LookupError()\n \n     def process_spider_exception(self, response, exception, spider):\n-        method = f'{self.__class__.__name__}.process_spider_exception'\n-        spider.logger.info('%s: %s caught', method, exception.__class__.__name__)\n-        yield {'processed': [method]}\n+        method = f\"{self.__class__.__name__}.process_spider_exception\"\n+        spider.logger.info(\"%s: %s caught\", method, exception.__class__.__name__)\n+        yield {\"processed\": [method]}\n \n \n class GeneratorDoNothingAfterFailureMiddleware(_GeneratorDoNothingMiddleware):\n@@ -187,13 +193,13 @@ class GeneratorDoNothingAfterFailureMiddleware(_GeneratorDoNothingMiddleware):\n class GeneratorRecoverMiddleware:\n     def process_spider_output(self, response, result, spider):\n         for r in result:\n-            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')\n+            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n             yield r\n \n     def process_spider_exception(self, response, exception, spider):\n-        method = f'{self.__class__.__name__}.process_spider_exception'\n-        spider.logger.info('%s: %s caught', method, exception.__class__.__name__)\n-        yield {'processed': [method]}\n+        method = f\"{self.__class__.__name__}.process_spider_exception\"\n+        spider.logger.info(\"%s: %s caught\", method, exception.__class__.__name__)\n+        yield {\"processed\": [method]}\n \n \n class GeneratorDoNothingAfterRecoveryMiddleware(_GeneratorDoNothingMiddleware):\n@@ -201,9 +207,9 @@ class GeneratorDoNothingAfterRecoveryMiddleware(_GeneratorDoNothingMiddleware):\n \n \n class GeneratorOutputChainSpider(Spider):\n-    name = 'GeneratorOutputChainSpider'\n+    name = \"GeneratorOutputChainSpider\"\n     custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n+        \"SPIDER_MIDDLEWARES\": {\n             GeneratorFailMiddleware: 10,\n             GeneratorDoNothingAfterFailureMiddleware: 8,\n             GeneratorRecoverMiddleware: 5,\n@@ -212,27 +218,28 @@ class GeneratorOutputChainSpider(Spider):\n     }\n \n     def start_requests(self):\n-        yield Request(self.mockserver.url('/status?n=200'))\n+        yield Request(self.mockserver.url(\"/status?n=200\"))\n \n     def parse(self, response):\n-        yield {'processed': ['parse-first-item']}\n-        yield {'processed': ['parse-second-item']}\n+        yield {\"processed\": [\"parse-first-item\"]}\n+        yield {\"processed\": [\"parse-second-item\"]}\n \n \n # ================================================================================\n # (5) exceptions from a middleware process_spider_output method (not generator)\n \n+\n class _NotGeneratorDoNothingMiddleware:\n     def process_spider_output(self, response, result, spider):\n         out = []\n         for r in result:\n-            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')\n+            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n             out.append(r)\n         return out\n \n     def process_spider_exception(self, response, exception, spider):\n-        method = f'{self.__class__.__name__}.process_spider_exception'\n-        spider.logger.info('%s: %s caught', method, exception.__class__.__name__)\n+        method = f\"{self.__class__.__name__}.process_spider_exception\"\n+        spider.logger.info(\"%s: %s caught\", method, exception.__class__.__name__)\n         return None\n \n \n@@ -240,15 +247,15 @@ class NotGeneratorFailMiddleware:\n     def process_spider_output(self, response, result, spider):\n         out = []\n         for r in result:\n-            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')\n+            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n             out.append(r)\n         raise ReferenceError()\n         return out\n \n     def process_spider_exception(self, response, exception, spider):\n-        method = f'{self.__class__.__name__}.process_spider_exception'\n-        spider.logger.info('%s: %s caught', method, exception.__class__.__name__)\n-        return [{'processed': [method]}]\n+        method = f\"{self.__class__.__name__}.process_spider_exception\"\n+        spider.logger.info(\"%s: %s caught\", method, exception.__class__.__name__)\n+        return [{\"processed\": [method]}]\n \n \n class NotGeneratorDoNothingAfterFailureMiddleware(_NotGeneratorDoNothingMiddleware):\n@@ -259,14 +266,14 @@ class NotGeneratorRecoverMiddleware:\n     def process_spider_output(self, response, result, spider):\n         out = []\n         for r in result:\n-            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')\n+            r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n             out.append(r)\n         return out\n \n     def process_spider_exception(self, response, exception, spider):\n-        method = f'{self.__class__.__name__}.process_spider_exception'\n-        spider.logger.info('%s: %s caught', method, exception.__class__.__name__)\n-        return [{'processed': [method]}]\n+        method = f\"{self.__class__.__name__}.process_spider_exception\"\n+        spider.logger.info(\"%s: %s caught\", method, exception.__class__.__name__)\n+        return [{\"processed\": [method]}]\n \n \n class NotGeneratorDoNothingAfterRecoveryMiddleware(_NotGeneratorDoNothingMiddleware):\n@@ -274,9 +281,9 @@ class NotGeneratorDoNothingAfterRecoveryMiddleware(_NotGeneratorDoNothingMiddlew\n \n \n class NotGeneratorOutputChainSpider(Spider):\n-    name = 'NotGeneratorOutputChainSpider'\n+    name = \"NotGeneratorOutputChainSpider\"\n     custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n+        \"SPIDER_MIDDLEWARES\": {\n             NotGeneratorFailMiddleware: 10,\n             NotGeneratorDoNothingAfterFailureMiddleware: 8,\n             NotGeneratorRecoverMiddleware: 5,\n@@ -285,10 +292,13 @@ class NotGeneratorOutputChainSpider(Spider):\n     }\n \n     def start_requests(self):\n-        return [Request(self.mockserver.url('/status?n=200'))]\n+        return [Request(self.mockserver.url(\"/status?n=200\"))]\n \n     def parse(self, response):\n-        return [{'processed': ['parse-first-item']}, {'processed': ['parse-second-item']}]\n+        return [\n+            {\"processed\": [\"parse-first-item\"]},\n+            {\"processed\": [\"parse-second-item\"]},\n+        ]\n \n \n # ================================================================================\n@@ -402,7 +412,9 @@ class TestSpiderMiddleware(TestCase):\n         (3.1) Special case of (3): Exceptions should be caught\n         even if the middleware is placed right after the spider\n         \"\"\"\n-        log31 = yield self.crawl_log(NotGeneratorCallbackSpiderMiddlewareRightAfterSpider)\n+        log31 = yield self.crawl_log(\n+            NotGeneratorCallbackSpiderMiddlewareRightAfterSpider\n+        )\n         self.assertIn(\"Middleware: ZeroDivisionError exception caught\", str(log31))\n         self.assertNotIn(\"item_scraped_count\", str(log31))\n \n@@ -418,28 +430,40 @@ class TestSpiderMiddleware(TestCase):\n         \"\"\"\n         log4 = yield self.crawl_log(GeneratorOutputChainSpider)\n         self.assertIn(\"'item_scraped_count': 2\", str(log4))\n-        self.assertIn(\"GeneratorRecoverMiddleware.process_spider_exception: LookupError caught\", str(log4))\n+        self.assertIn(\n+            \"GeneratorRecoverMiddleware.process_spider_exception: LookupError caught\",\n+            str(log4),\n+        )\n         self.assertIn(\n             \"GeneratorDoNothingAfterFailureMiddleware.process_spider_exception: LookupError caught\",\n-            str(log4))\n+            str(log4),\n+        )\n         self.assertNotIn(\n             \"GeneratorFailMiddleware.process_spider_exception: LookupError caught\",\n-            str(log4))\n+            str(log4),\n+        )\n         self.assertNotIn(\n             \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_exception: LookupError caught\",\n-            str(log4))\n-        item_from_callback = {'processed': [\n-            'parse-first-item',\n-            'GeneratorFailMiddleware.process_spider_output',\n-            'GeneratorDoNothingAfterFailureMiddleware.process_spider_output',\n-            'GeneratorRecoverMiddleware.process_spider_output',\n-            'GeneratorDoNothingAfterRecoveryMiddleware.process_spider_output']}\n-        item_recovered = {'processed': [\n-            'GeneratorRecoverMiddleware.process_spider_exception',\n-            'GeneratorDoNothingAfterRecoveryMiddleware.process_spider_output']}\n+            str(log4),\n+        )\n+        item_from_callback = {\n+            \"processed\": [\n+                \"parse-first-item\",\n+                \"GeneratorFailMiddleware.process_spider_output\",\n+                \"GeneratorDoNothingAfterFailureMiddleware.process_spider_output\",\n+                \"GeneratorRecoverMiddleware.process_spider_output\",\n+                \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_output\",\n+            ]\n+        }\n+        item_recovered = {\n+            \"processed\": [\n+                \"GeneratorRecoverMiddleware.process_spider_exception\",\n+                \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_output\",\n+            ]\n+        }\n         self.assertIn(str(item_from_callback), str(log4))\n         self.assertIn(str(item_recovered), str(log4))\n-        self.assertNotIn('parse-second-item', str(log4))\n+        self.assertNotIn(\"parse-second-item\", str(log4))\n \n     @defer.inlineCallbacks\n     def test_not_a_generator_output_chain(self):\n@@ -453,17 +477,28 @@ class TestSpiderMiddleware(TestCase):\n         \"\"\"\n         log5 = yield self.crawl_log(NotGeneratorOutputChainSpider)\n         self.assertIn(\"'item_scraped_count': 1\", str(log5))\n-        self.assertIn(\"GeneratorRecoverMiddleware.process_spider_exception: ReferenceError caught\", str(log5))\n+        self.assertIn(\n+            \"GeneratorRecoverMiddleware.process_spider_exception: ReferenceError caught\",\n+            str(log5),\n+        )\n         self.assertIn(\n             \"GeneratorDoNothingAfterFailureMiddleware.process_spider_exception: ReferenceError caught\",\n-            str(log5))\n-        self.assertNotIn(\"GeneratorFailMiddleware.process_spider_exception: ReferenceError caught\", str(log5))\n+            str(log5),\n+        )\n+        self.assertNotIn(\n+            \"GeneratorFailMiddleware.process_spider_exception: ReferenceError caught\",\n+            str(log5),\n+        )\n         self.assertNotIn(\n             \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_exception: ReferenceError caught\",\n-            str(log5))\n-        item_recovered = {'processed': [\n-            'NotGeneratorRecoverMiddleware.process_spider_exception',\n-            'NotGeneratorDoNothingAfterRecoveryMiddleware.process_spider_output']}\n+            str(log5),\n+        )\n+        item_recovered = {\n+            \"processed\": [\n+                \"NotGeneratorRecoverMiddleware.process_spider_exception\",\n+                \"NotGeneratorDoNothingAfterRecoveryMiddleware.process_spider_output\",\n+            ]\n+        }\n         self.assertIn(str(item_recovered), str(log5))\n-        self.assertNotIn('parse-first-item', str(log5))\n-        self.assertNotIn('parse-second-item', str(log5))\n+        self.assertNotIn(\"parse-first-item\", str(log5))\n+        self.assertNotIn(\"parse-second-item\", str(log5))\n\n@@ -36,11 +36,11 @@ class TestRefererMiddleware(TestCase):\n     resp_headers = {}\n     settings = {}\n     scenarii = [\n-        ('http://scrapytest.org', 'http://scrapytest.org/', b'http://scrapytest.org'),\n+        (\"http://scrapytest.org\", \"http://scrapytest.org/\", b\"http://scrapytest.org\"),\n     ]\n \n     def setUp(self):\n-        self.spider = Spider('foo')\n+        self.spider = Spider(\"foo\")\n         settings = Settings(self.settings)\n         self.mw = RefererMiddleware(settings)\n \n@@ -56,7 +56,7 @@ class TestRefererMiddleware(TestCase):\n             response = self.get_response(origin)\n             request = self.get_request(target)\n             out = list(self.mw.process_spider_output(response, [request], self.spider))\n-            self.assertEqual(out[0].headers.get('Referer'), referrer)\n+            self.assertEqual(out[0].headers.get(\"Referer\"), referrer)\n \n \n class MixinDefault:\n@@ -65,101 +65,184 @@ class MixinDefault:\n \n     with some additional filtering of s3://\n     \"\"\"\n+\n     scenarii = [\n-        ('https://example.com/', 'https://scrapy.org/', b'https://example.com/'),\n-        ('http://example.com/', 'http://scrapy.org/', b'http://example.com/'),\n-        ('http://example.com/', 'https://scrapy.org/', b'http://example.com/'),\n-        ('https://example.com/', 'http://scrapy.org/', None),\n-\n+        (\"https://example.com/\", \"https://scrapy.org/\", b\"https://example.com/\"),\n+        (\"http://example.com/\", \"http://scrapy.org/\", b\"http://example.com/\"),\n+        (\"http://example.com/\", \"https://scrapy.org/\", b\"http://example.com/\"),\n+        (\"https://example.com/\", \"http://scrapy.org/\", None),\n         # no credentials leak\n-        ('http://user:password@example.com/', 'https://scrapy.org/', b'http://example.com/'),\n-\n+        (\n+            \"http://user:password@example.com/\",\n+            \"https://scrapy.org/\",\n+            b\"http://example.com/\",\n+        ),\n         # no referrer leak for local schemes\n-        ('file:///home/path/to/somefile.html', 'https://scrapy.org/', None),\n-        ('file:///home/path/to/somefile.html', 'http://scrapy.org/', None),\n-\n+        (\"file:///home/path/to/somefile.html\", \"https://scrapy.org/\", None),\n+        (\"file:///home/path/to/somefile.html\", \"http://scrapy.org/\", None),\n         # no referrer leak for s3 origins\n-        ('s3://mybucket/path/to/data.csv', 'https://scrapy.org/', None),\n-        ('s3://mybucket/path/to/data.csv', 'http://scrapy.org/', None),\n+        (\"s3://mybucket/path/to/data.csv\", \"https://scrapy.org/\", None),\n+        (\"s3://mybucket/path/to/data.csv\", \"http://scrapy.org/\", None),\n     ]\n \n \n class MixinNoReferrer:\n     scenarii = [\n-        ('https://example.com/page.html', 'https://example.com/', None),\n-        ('http://www.example.com/', 'https://scrapy.org/', None),\n-        ('http://www.example.com/', 'http://scrapy.org/', None),\n-        ('https://www.example.com/', 'http://scrapy.org/', None),\n-        ('file:///home/path/to/somefile.html', 'http://scrapy.org/', None),\n+        (\"https://example.com/page.html\", \"https://example.com/\", None),\n+        (\"http://www.example.com/\", \"https://scrapy.org/\", None),\n+        (\"http://www.example.com/\", \"http://scrapy.org/\", None),\n+        (\"https://www.example.com/\", \"http://scrapy.org/\", None),\n+        (\"file:///home/path/to/somefile.html\", \"http://scrapy.org/\", None),\n     ]\n \n \n class MixinNoReferrerWhenDowngrade:\n     scenarii = [\n         # TLS to TLS: send non-empty referrer\n-        ('https://example.com/page.html', 'https://not.example.com/', b'https://example.com/page.html'),\n-        ('https://example.com/page.html', 'https://scrapy.org/', b'https://example.com/page.html'),\n-        ('https://example.com:443/page.html', 'https://scrapy.org/', b'https://example.com/page.html'),\n-        ('https://example.com:444/page.html', 'https://scrapy.org/', b'https://example.com:444/page.html'),\n-        ('ftps://example.com/urls.zip', 'https://scrapy.org/', b'ftps://example.com/urls.zip'),\n-\n+        (\n+            \"https://example.com/page.html\",\n+            \"https://not.example.com/\",\n+            b\"https://example.com/page.html\",\n+        ),\n+        (\n+            \"https://example.com/page.html\",\n+            \"https://scrapy.org/\",\n+            b\"https://example.com/page.html\",\n+        ),\n+        (\n+            \"https://example.com:443/page.html\",\n+            \"https://scrapy.org/\",\n+            b\"https://example.com/page.html\",\n+        ),\n+        (\n+            \"https://example.com:444/page.html\",\n+            \"https://scrapy.org/\",\n+            b\"https://example.com:444/page.html\",\n+        ),\n+        (\n+            \"ftps://example.com/urls.zip\",\n+            \"https://scrapy.org/\",\n+            b\"ftps://example.com/urls.zip\",\n+        ),\n         # TLS to non-TLS: do not send referrer\n-        ('https://example.com/page.html', 'http://not.example.com/', None),\n-        ('https://example.com/page.html', 'http://scrapy.org/', None),\n-        ('ftps://example.com/urls.zip', 'http://scrapy.org/', None),\n-\n+        (\"https://example.com/page.html\", \"http://not.example.com/\", None),\n+        (\"https://example.com/page.html\", \"http://scrapy.org/\", None),\n+        (\"ftps://example.com/urls.zip\", \"http://scrapy.org/\", None),\n         # non-TLS to TLS or non-TLS: send referrer\n-        ('http://example.com/page.html', 'https://not.example.com/', b'http://example.com/page.html'),\n-        ('http://example.com/page.html', 'https://scrapy.org/', b'http://example.com/page.html'),\n-        ('http://example.com:8080/page.html', 'https://scrapy.org/', b'http://example.com:8080/page.html'),\n-        ('http://example.com:80/page.html', 'http://not.example.com/', b'http://example.com/page.html'),\n-        ('http://example.com/page.html', 'http://scrapy.org/', b'http://example.com/page.html'),\n-        ('http://example.com:443/page.html', 'http://scrapy.org/', b'http://example.com:443/page.html'),\n-        ('ftp://example.com/urls.zip', 'http://scrapy.org/', b'ftp://example.com/urls.zip'),\n-        ('ftp://example.com/urls.zip', 'https://scrapy.org/', b'ftp://example.com/urls.zip'),\n-\n+        (\n+            \"http://example.com/page.html\",\n+            \"https://not.example.com/\",\n+            b\"http://example.com/page.html\",\n+        ),\n+        (\n+            \"http://example.com/page.html\",\n+            \"https://scrapy.org/\",\n+            b\"http://example.com/page.html\",\n+        ),\n+        (\n+            \"http://example.com:8080/page.html\",\n+            \"https://scrapy.org/\",\n+            b\"http://example.com:8080/page.html\",\n+        ),\n+        (\n+            \"http://example.com:80/page.html\",\n+            \"http://not.example.com/\",\n+            b\"http://example.com/page.html\",\n+        ),\n+        (\n+            \"http://example.com/page.html\",\n+            \"http://scrapy.org/\",\n+            b\"http://example.com/page.html\",\n+        ),\n+        (\n+            \"http://example.com:443/page.html\",\n+            \"http://scrapy.org/\",\n+            b\"http://example.com:443/page.html\",\n+        ),\n+        (\n+            \"ftp://example.com/urls.zip\",\n+            \"http://scrapy.org/\",\n+            b\"ftp://example.com/urls.zip\",\n+        ),\n+        (\n+            \"ftp://example.com/urls.zip\",\n+            \"https://scrapy.org/\",\n+            b\"ftp://example.com/urls.zip\",\n+        ),\n         # test for user/password stripping\n-        ('http://user:password@example.com/page.html', 'https://not.example.com/', b'http://example.com/page.html'),\n+        (\n+            \"http://user:password@example.com/page.html\",\n+            \"https://not.example.com/\",\n+            b\"http://example.com/page.html\",\n+        ),\n     ]\n \n \n class MixinSameOrigin:\n     scenarii = [\n         # Same origin (protocol, host, port): send referrer\n-        ('https://example.com/page.html', 'https://example.com/not-page.html', b'https://example.com/page.html'),\n-        ('http://example.com/page.html', 'http://example.com/not-page.html', b'http://example.com/page.html'),\n-        ('https://example.com:443/page.html', 'https://example.com/not-page.html', b'https://example.com/page.html'),\n-        ('http://example.com:80/page.html', 'http://example.com/not-page.html', b'http://example.com/page.html'),\n-        ('http://example.com/page.html', 'http://example.com:80/not-page.html', b'http://example.com/page.html'),\n         (\n-            'http://example.com:8888/page.html',\n-            'http://example.com:8888/not-page.html',\n-            b'http://example.com:8888/page.html',\n+            \"https://example.com/page.html\",\n+            \"https://example.com/not-page.html\",\n+            b\"https://example.com/page.html\",\n         ),\n-\n-        # Different host: do NOT send referrer\n-        ('https://example.com/page.html', 'https://not.example.com/otherpage.html', None),\n-        ('http://example.com/page.html', 'http://not.example.com/otherpage.html', None),\n-        ('http://example.com/page.html', 'http://www.example.com/otherpage.html', None),\n-\n-        # Different port: do NOT send referrer\n-        ('https://example.com:444/page.html', 'https://example.com/not-page.html', None),\n-        ('http://example.com:81/page.html', 'http://example.com/not-page.html', None),\n-        ('http://example.com/page.html', 'http://example.com:81/not-page.html', None),\n-\n-        # Different protocols: do NOT send refferer\n-        ('https://example.com/page.html', 'http://example.com/not-page.html', None),\n-        ('https://example.com/page.html', 'http://not.example.com/', None),\n-        ('ftps://example.com/urls.zip', 'https://example.com/not-page.html', None),\n-        ('ftp://example.com/urls.zip', 'http://example.com/not-page.html', None),\n-        ('ftps://example.com/urls.zip', 'https://example.com/not-page.html', None),\n-\n-        # test for user/password stripping\n-        ('https://user:password@example.com/page.html', 'http://example.com/not-page.html', None),\n         (\n-            'https://user:password@example.com/page.html',\n-            'https://example.com/not-page.html',\n-            b'https://example.com/page.html',\n+            \"http://example.com/page.html\",\n+            \"http://example.com/not-page.html\",\n+            b\"http://example.com/page.html\",\n+        ),\n+        (\n+            \"https://example.com:443/page.html\",\n+            \"https://example.com/not-page.html\",\n+            b\"https://example.com/page.html\",\n+        ),\n+        (\n+            \"http://example.com:80/page.html\",\n+            \"http://example.com/not-page.html\",\n+            b\"http://example.com/page.html\",\n+        ),\n+        (\n+            \"http://example.com/page.html\",\n+            \"http://example.com:80/not-page.html\",\n+            b\"http://example.com/page.html\",\n+        ),\n+        (\n+            \"http://example.com:8888/page.html\",\n+            \"http://example.com:8888/not-page.html\",\n+            b\"http://example.com:8888/page.html\",\n+        ),\n+        # Different host: do NOT send referrer\n+        (\n+            \"https://example.com/page.html\",\n+            \"https://not.example.com/otherpage.html\",\n+            None,\n+        ),\n+        (\"http://example.com/page.html\", \"http://not.example.com/otherpage.html\", None),\n+        (\"http://example.com/page.html\", \"http://www.example.com/otherpage.html\", None),\n+        # Different port: do NOT send referrer\n+        (\n+            \"https://example.com:444/page.html\",\n+            \"https://example.com/not-page.html\",\n+            None,\n+        ),\n+        (\"http://example.com:81/page.html\", \"http://example.com/not-page.html\", None),\n+        (\"http://example.com/page.html\", \"http://example.com:81/not-page.html\", None),\n+        # Different protocols: do NOT send refferer\n+        (\"https://example.com/page.html\", \"http://example.com/not-page.html\", None),\n+        (\"https://example.com/page.html\", \"http://not.example.com/\", None),\n+        (\"ftps://example.com/urls.zip\", \"https://example.com/not-page.html\", None),\n+        (\"ftp://example.com/urls.zip\", \"http://example.com/not-page.html\", None),\n+        (\"ftps://example.com/urls.zip\", \"https://example.com/not-page.html\", None),\n+        # test for user/password stripping\n+        (\n+            \"https://user:password@example.com/page.html\",\n+            \"http://example.com/not-page.html\",\n+            None,\n+        ),\n+        (\n+            \"https://user:password@example.com/page.html\",\n+            \"https://example.com/not-page.html\",\n+            b\"https://example.com/page.html\",\n         ),\n     ]\n \n@@ -167,78 +250,158 @@ class MixinSameOrigin:\n class MixinOrigin:\n     scenarii = [\n         # TLS or non-TLS to TLS or non-TLS: referrer origin is sent (yes, even for downgrades)\n-        ('https://example.com/page.html', 'https://example.com/not-page.html', b'https://example.com/'),\n-        ('https://example.com/page.html', 'https://scrapy.org', b'https://example.com/'),\n-        ('https://example.com/page.html', 'http://scrapy.org', b'https://example.com/'),\n-        ('http://example.com/page.html', 'http://scrapy.org', b'http://example.com/'),\n-\n+        (\n+            \"https://example.com/page.html\",\n+            \"https://example.com/not-page.html\",\n+            b\"https://example.com/\",\n+        ),\n+        (\n+            \"https://example.com/page.html\",\n+            \"https://scrapy.org\",\n+            b\"https://example.com/\",\n+        ),\n+        (\"https://example.com/page.html\", \"http://scrapy.org\", b\"https://example.com/\"),\n+        (\"http://example.com/page.html\", \"http://scrapy.org\", b\"http://example.com/\"),\n         # test for user/password stripping\n-        ('https://user:password@example.com/page.html', 'http://scrapy.org', b'https://example.com/'),\n+        (\n+            \"https://user:password@example.com/page.html\",\n+            \"http://scrapy.org\",\n+            b\"https://example.com/\",\n+        ),\n     ]\n \n \n class MixinStrictOrigin:\n     scenarii = [\n         # TLS or non-TLS to TLS or non-TLS: referrer origin is sent but not for downgrades\n-        ('https://example.com/page.html', 'https://example.com/not-page.html', b'https://example.com/'),\n-        ('https://example.com/page.html', 'https://scrapy.org', b'https://example.com/'),\n-        ('http://example.com/page.html', 'http://scrapy.org', b'http://example.com/'),\n-\n+        (\n+            \"https://example.com/page.html\",\n+            \"https://example.com/not-page.html\",\n+            b\"https://example.com/\",\n+        ),\n+        (\n+            \"https://example.com/page.html\",\n+            \"https://scrapy.org\",\n+            b\"https://example.com/\",\n+        ),\n+        (\"http://example.com/page.html\", \"http://scrapy.org\", b\"http://example.com/\"),\n         # downgrade: send nothing\n-        ('https://example.com/page.html', 'http://scrapy.org', None),\n-\n+        (\"https://example.com/page.html\", \"http://scrapy.org\", None),\n         # upgrade: send origin\n-        ('http://example.com/page.html', 'https://scrapy.org', b'http://example.com/'),\n-\n+        (\"http://example.com/page.html\", \"https://scrapy.org\", b\"http://example.com/\"),\n         # test for user/password stripping\n-        ('https://user:password@example.com/page.html', 'https://scrapy.org', b'https://example.com/'),\n-        ('https://user:password@example.com/page.html', 'http://scrapy.org', None),\n+        (\n+            \"https://user:password@example.com/page.html\",\n+            \"https://scrapy.org\",\n+            b\"https://example.com/\",\n+        ),\n+        (\"https://user:password@example.com/page.html\", \"http://scrapy.org\", None),\n     ]\n \n \n class MixinOriginWhenCrossOrigin:\n     scenarii = [\n         # Same origin (protocol, host, port): send referrer\n-        ('https://example.com/page.html', 'https://example.com/not-page.html', b'https://example.com/page.html'),\n-        ('http://example.com/page.html', 'http://example.com/not-page.html', b'http://example.com/page.html'),\n-        ('https://example.com:443/page.html', 'https://example.com/not-page.html', b'https://example.com/page.html'),\n-        ('http://example.com:80/page.html', 'http://example.com/not-page.html', b'http://example.com/page.html'),\n-        ('http://example.com/page.html', 'http://example.com:80/not-page.html', b'http://example.com/page.html'),\n         (\n-            'http://example.com:8888/page.html',\n-            'http://example.com:8888/not-page.html',\n-            b'http://example.com:8888/page.html',\n+            \"https://example.com/page.html\",\n+            \"https://example.com/not-page.html\",\n+            b\"https://example.com/page.html\",\n+        ),\n+        (\n+            \"http://example.com/page.html\",\n+            \"http://example.com/not-page.html\",\n+            b\"http://example.com/page.html\",\n+        ),\n+        (\n+            \"https://example.com:443/page.html\",\n+            \"https://example.com/not-page.html\",\n+            b\"https://example.com/page.html\",\n+        ),\n+        (\n+            \"http://example.com:80/page.html\",\n+            \"http://example.com/not-page.html\",\n+            b\"http://example.com/page.html\",\n+        ),\n+        (\n+            \"http://example.com/page.html\",\n+            \"http://example.com:80/not-page.html\",\n+            b\"http://example.com/page.html\",\n+        ),\n+        (\n+            \"http://example.com:8888/page.html\",\n+            \"http://example.com:8888/not-page.html\",\n+            b\"http://example.com:8888/page.html\",\n         ),\n-\n         # Different host: send origin as referrer\n-        ('https://example2.com/page.html', 'https://scrapy.org/otherpage.html', b'https://example2.com/'),\n-        ('https://example2.com/page.html', 'https://not.example2.com/otherpage.html', b'https://example2.com/'),\n-        ('http://example2.com/page.html', 'http://not.example2.com/otherpage.html', b'http://example2.com/'),\n+        (\n+            \"https://example2.com/page.html\",\n+            \"https://scrapy.org/otherpage.html\",\n+            b\"https://example2.com/\",\n+        ),\n+        (\n+            \"https://example2.com/page.html\",\n+            \"https://not.example2.com/otherpage.html\",\n+            b\"https://example2.com/\",\n+        ),\n+        (\n+            \"http://example2.com/page.html\",\n+            \"http://not.example2.com/otherpage.html\",\n+            b\"http://example2.com/\",\n+        ),\n         # exact match required\n-        ('http://example2.com/page.html', 'http://www.example2.com/otherpage.html', b'http://example2.com/'),\n-\n+        (\n+            \"http://example2.com/page.html\",\n+            \"http://www.example2.com/otherpage.html\",\n+            b\"http://example2.com/\",\n+        ),\n         # Different port: send origin as referrer\n-        ('https://example3.com:444/page.html', 'https://example3.com/not-page.html', b'https://example3.com:444/'),\n-        ('http://example3.com:81/page.html', 'http://example3.com/not-page.html', b'http://example3.com:81/'),\n-\n+        (\n+            \"https://example3.com:444/page.html\",\n+            \"https://example3.com/not-page.html\",\n+            b\"https://example3.com:444/\",\n+        ),\n+        (\n+            \"http://example3.com:81/page.html\",\n+            \"http://example3.com/not-page.html\",\n+            b\"http://example3.com:81/\",\n+        ),\n         # Different protocols: send origin as referrer\n-        ('https://example4.com/page.html', 'http://example4.com/not-page.html', b'https://example4.com/'),\n-        ('https://example4.com/page.html', 'http://not.example4.com/', b'https://example4.com/'),\n-        ('ftps://example4.com/urls.zip', 'https://example4.com/not-page.html', b'ftps://example4.com/'),\n-        ('ftp://example4.com/urls.zip', 'http://example4.com/not-page.html', b'ftp://example4.com/'),\n-        ('ftps://example4.com/urls.zip', 'https://example4.com/not-page.html', b'ftps://example4.com/'),\n-\n+        (\n+            \"https://example4.com/page.html\",\n+            \"http://example4.com/not-page.html\",\n+            b\"https://example4.com/\",\n+        ),\n+        (\n+            \"https://example4.com/page.html\",\n+            \"http://not.example4.com/\",\n+            b\"https://example4.com/\",\n+        ),\n+        (\n+            \"ftps://example4.com/urls.zip\",\n+            \"https://example4.com/not-page.html\",\n+            b\"ftps://example4.com/\",\n+        ),\n+        (\n+            \"ftp://example4.com/urls.zip\",\n+            \"http://example4.com/not-page.html\",\n+            b\"ftp://example4.com/\",\n+        ),\n+        (\n+            \"ftps://example4.com/urls.zip\",\n+            \"https://example4.com/not-page.html\",\n+            b\"ftps://example4.com/\",\n+        ),\n         # test for user/password stripping\n         (\n-            'https://user:password@example5.com/page.html',\n-            'https://example5.com/not-page.html',\n-            b'https://example5.com/page.html',\n+            \"https://user:password@example5.com/page.html\",\n+            \"https://example5.com/not-page.html\",\n+            b\"https://example5.com/page.html\",\n         ),\n         # TLS to non-TLS downgrade: send origin\n         (\n-            'https://user:password@example5.com/page.html',\n-            'http://example5.com/not-page.html',\n-            b'https://example5.com/',\n+            \"https://user:password@example5.com/page.html\",\n+            \"http://example5.com/not-page.html\",\n+            b\"https://example5.com/\",\n         ),\n     ]\n \n@@ -246,90 +409,215 @@ class MixinOriginWhenCrossOrigin:\n class MixinStrictOriginWhenCrossOrigin:\n     scenarii = [\n         # Same origin (protocol, host, port): send referrer\n-        ('https://example.com/page.html', 'https://example.com/not-page.html', b'https://example.com/page.html'),\n-        ('http://example.com/page.html', 'http://example.com/not-page.html', b'http://example.com/page.html'),\n-        ('https://example.com:443/page.html', 'https://example.com/not-page.html', b'https://example.com/page.html'),\n-        ('http://example.com:80/page.html', 'http://example.com/not-page.html', b'http://example.com/page.html'),\n-        ('http://example.com/page.html', 'http://example.com:80/not-page.html', b'http://example.com/page.html'),\n         (\n-            'http://example.com:8888/page.html',\n-            'http://example.com:8888/not-page.html',\n-            b'http://example.com:8888/page.html',\n+            \"https://example.com/page.html\",\n+            \"https://example.com/not-page.html\",\n+            b\"https://example.com/page.html\",\n+        ),\n+        (\n+            \"http://example.com/page.html\",\n+            \"http://example.com/not-page.html\",\n+            b\"http://example.com/page.html\",\n+        ),\n+        (\n+            \"https://example.com:443/page.html\",\n+            \"https://example.com/not-page.html\",\n+            b\"https://example.com/page.html\",\n+        ),\n+        (\n+            \"http://example.com:80/page.html\",\n+            \"http://example.com/not-page.html\",\n+            b\"http://example.com/page.html\",\n+        ),\n+        (\n+            \"http://example.com/page.html\",\n+            \"http://example.com:80/not-page.html\",\n+            b\"http://example.com/page.html\",\n+        ),\n+        (\n+            \"http://example.com:8888/page.html\",\n+            \"http://example.com:8888/not-page.html\",\n+            b\"http://example.com:8888/page.html\",\n         ),\n-\n         # Different host: send origin as referrer\n-        ('https://example2.com/page.html', 'https://scrapy.org/otherpage.html', b'https://example2.com/'),\n-        ('https://example2.com/page.html', 'https://not.example2.com/otherpage.html', b'https://example2.com/'),\n-        ('http://example2.com/page.html', 'http://not.example2.com/otherpage.html', b'http://example2.com/'),\n+        (\n+            \"https://example2.com/page.html\",\n+            \"https://scrapy.org/otherpage.html\",\n+            b\"https://example2.com/\",\n+        ),\n+        (\n+            \"https://example2.com/page.html\",\n+            \"https://not.example2.com/otherpage.html\",\n+            b\"https://example2.com/\",\n+        ),\n+        (\n+            \"http://example2.com/page.html\",\n+            \"http://not.example2.com/otherpage.html\",\n+            b\"http://example2.com/\",\n+        ),\n         # exact match required\n-        ('http://example2.com/page.html', 'http://www.example2.com/otherpage.html', b'http://example2.com/'),\n-\n+        (\n+            \"http://example2.com/page.html\",\n+            \"http://www.example2.com/otherpage.html\",\n+            b\"http://example2.com/\",\n+        ),\n         # Different port: send origin as referrer\n-        ('https://example3.com:444/page.html', 'https://example3.com/not-page.html', b'https://example3.com:444/'),\n-        ('http://example3.com:81/page.html', 'http://example3.com/not-page.html', b'http://example3.com:81/'),\n-\n+        (\n+            \"https://example3.com:444/page.html\",\n+            \"https://example3.com/not-page.html\",\n+            b\"https://example3.com:444/\",\n+        ),\n+        (\n+            \"http://example3.com:81/page.html\",\n+            \"http://example3.com/not-page.html\",\n+            b\"http://example3.com:81/\",\n+        ),\n         # downgrade\n-        ('https://example4.com/page.html', 'http://example4.com/not-page.html', None),\n-        ('https://example4.com/page.html', 'http://not.example4.com/', None),\n-\n+        (\"https://example4.com/page.html\", \"http://example4.com/not-page.html\", None),\n+        (\"https://example4.com/page.html\", \"http://not.example4.com/\", None),\n         # non-TLS to non-TLS\n-        ('ftp://example4.com/urls.zip', 'http://example4.com/not-page.html', b'ftp://example4.com/'),\n-\n+        (\n+            \"ftp://example4.com/urls.zip\",\n+            \"http://example4.com/not-page.html\",\n+            b\"ftp://example4.com/\",\n+        ),\n         # upgrade\n-        ('http://example4.com/page.html', 'https://example4.com/not-page.html', b'http://example4.com/'),\n-        ('http://example4.com/page.html', 'https://not.example4.com/', b'http://example4.com/'),\n-\n+        (\n+            \"http://example4.com/page.html\",\n+            \"https://example4.com/not-page.html\",\n+            b\"http://example4.com/\",\n+        ),\n+        (\n+            \"http://example4.com/page.html\",\n+            \"https://not.example4.com/\",\n+            b\"http://example4.com/\",\n+        ),\n         # Different protocols: send origin as referrer\n-        ('ftps://example4.com/urls.zip', 'https://example4.com/not-page.html', b'ftps://example4.com/'),\n-        ('ftps://example4.com/urls.zip', 'https://example4.com/not-page.html', b'ftps://example4.com/'),\n-\n+        (\n+            \"ftps://example4.com/urls.zip\",\n+            \"https://example4.com/not-page.html\",\n+            b\"ftps://example4.com/\",\n+        ),\n+        (\n+            \"ftps://example4.com/urls.zip\",\n+            \"https://example4.com/not-page.html\",\n+            b\"ftps://example4.com/\",\n+        ),\n         # test for user/password stripping\n         (\n-            'https://user:password@example5.com/page.html',\n-            'https://example5.com/not-page.html',\n-            b'https://example5.com/page.html',\n+            \"https://user:password@example5.com/page.html\",\n+            \"https://example5.com/not-page.html\",\n+            b\"https://example5.com/page.html\",\n         ),\n-\n         # TLS to non-TLS downgrade: send nothing\n-        ('https://user:password@example5.com/page.html', 'http://example5.com/not-page.html', None),\n+        (\n+            \"https://user:password@example5.com/page.html\",\n+            \"http://example5.com/not-page.html\",\n+            None,\n+        ),\n     ]\n \n \n class MixinUnsafeUrl:\n     scenarii = [\n         # TLS to TLS: send referrer\n-        ('https://example.com/sekrit.html', 'http://not.example.com/', b'https://example.com/sekrit.html'),\n-        ('https://example1.com/page.html', 'https://not.example1.com/', b'https://example1.com/page.html'),\n-        ('https://example1.com/page.html', 'https://scrapy.org/', b'https://example1.com/page.html'),\n-        ('https://example1.com:443/page.html', 'https://scrapy.org/', b'https://example1.com/page.html'),\n-        ('https://example1.com:444/page.html', 'https://scrapy.org/', b'https://example1.com:444/page.html'),\n-        ('ftps://example1.com/urls.zip', 'https://scrapy.org/', b'ftps://example1.com/urls.zip'),\n-\n-        # TLS to non-TLS: send referrer (yes, it's unsafe)\n-        ('https://example2.com/page.html', 'http://not.example2.com/', b'https://example2.com/page.html'),\n-        ('https://example2.com/page.html', 'http://scrapy.org/', b'https://example2.com/page.html'),\n-        ('ftps://example2.com/urls.zip', 'http://scrapy.org/', b'ftps://example2.com/urls.zip'),\n-\n-        # non-TLS to TLS or non-TLS: send referrer (yes, it's unsafe)\n-        ('http://example3.com/page.html', 'https://not.example3.com/', b'http://example3.com/page.html'),\n-        ('http://example3.com/page.html', 'https://scrapy.org/', b'http://example3.com/page.html'),\n-        ('http://example3.com:8080/page.html', 'https://scrapy.org/', b'http://example3.com:8080/page.html'),\n-        ('http://example3.com:80/page.html', 'http://not.example3.com/', b'http://example3.com/page.html'),\n-        ('http://example3.com/page.html', 'http://scrapy.org/', b'http://example3.com/page.html'),\n-        ('http://example3.com:443/page.html', 'http://scrapy.org/', b'http://example3.com:443/page.html'),\n-        ('ftp://example3.com/urls.zip', 'http://scrapy.org/', b'ftp://example3.com/urls.zip'),\n-        ('ftp://example3.com/urls.zip', 'https://scrapy.org/', b'ftp://example3.com/urls.zip'),\n-\n-        # test for user/password stripping\n         (\n-            'http://user:password@example4.com/page.html',\n-            'https://not.example4.com/',\n-            b'http://example4.com/page.html',\n+            \"https://example.com/sekrit.html\",\n+            \"http://not.example.com/\",\n+            b\"https://example.com/sekrit.html\",\n         ),\n         (\n-            'https://user:password@example4.com/page.html',\n-            'http://scrapy.org/',\n-            b'https://example4.com/page.html',\n+            \"https://example1.com/page.html\",\n+            \"https://not.example1.com/\",\n+            b\"https://example1.com/page.html\",\n+        ),\n+        (\n+            \"https://example1.com/page.html\",\n+            \"https://scrapy.org/\",\n+            b\"https://example1.com/page.html\",\n+        ),\n+        (\n+            \"https://example1.com:443/page.html\",\n+            \"https://scrapy.org/\",\n+            b\"https://example1.com/page.html\",\n+        ),\n+        (\n+            \"https://example1.com:444/page.html\",\n+            \"https://scrapy.org/\",\n+            b\"https://example1.com:444/page.html\",\n+        ),\n+        (\n+            \"ftps://example1.com/urls.zip\",\n+            \"https://scrapy.org/\",\n+            b\"ftps://example1.com/urls.zip\",\n+        ),\n+        # TLS to non-TLS: send referrer (yes, it's unsafe)\n+        (\n+            \"https://example2.com/page.html\",\n+            \"http://not.example2.com/\",\n+            b\"https://example2.com/page.html\",\n+        ),\n+        (\n+            \"https://example2.com/page.html\",\n+            \"http://scrapy.org/\",\n+            b\"https://example2.com/page.html\",\n+        ),\n+        (\n+            \"ftps://example2.com/urls.zip\",\n+            \"http://scrapy.org/\",\n+            b\"ftps://example2.com/urls.zip\",\n+        ),\n+        # non-TLS to TLS or non-TLS: send referrer (yes, it's unsafe)\n+        (\n+            \"http://example3.com/page.html\",\n+            \"https://not.example3.com/\",\n+            b\"http://example3.com/page.html\",\n+        ),\n+        (\n+            \"http://example3.com/page.html\",\n+            \"https://scrapy.org/\",\n+            b\"http://example3.com/page.html\",\n+        ),\n+        (\n+            \"http://example3.com:8080/page.html\",\n+            \"https://scrapy.org/\",\n+            b\"http://example3.com:8080/page.html\",\n+        ),\n+        (\n+            \"http://example3.com:80/page.html\",\n+            \"http://not.example3.com/\",\n+            b\"http://example3.com/page.html\",\n+        ),\n+        (\n+            \"http://example3.com/page.html\",\n+            \"http://scrapy.org/\",\n+            b\"http://example3.com/page.html\",\n+        ),\n+        (\n+            \"http://example3.com:443/page.html\",\n+            \"http://scrapy.org/\",\n+            b\"http://example3.com:443/page.html\",\n+        ),\n+        (\n+            \"ftp://example3.com/urls.zip\",\n+            \"http://scrapy.org/\",\n+            b\"ftp://example3.com/urls.zip\",\n+        ),\n+        (\n+            \"ftp://example3.com/urls.zip\",\n+            \"https://scrapy.org/\",\n+            b\"ftp://example3.com/urls.zip\",\n+        ),\n+        # test for user/password stripping\n+        (\n+            \"http://user:password@example4.com/page.html\",\n+            \"https://not.example4.com/\",\n+            b\"http://example4.com/page.html\",\n+        ),\n+        (\n+            \"https://user:password@example4.com/page.html\",\n+            \"http://scrapy.org/\",\n+            b\"https://example4.com/page.html\",\n         ),\n     ]\n \n@@ -340,35 +628,49 @@ class TestRefererMiddlewareDefault(MixinDefault, TestRefererMiddleware):\n \n # --- Tests using settings to set policy using class path\n class TestSettingsNoReferrer(MixinNoReferrer, TestRefererMiddleware):\n-    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerPolicy'}\n+    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.NoReferrerPolicy\"}\n \n \n-class TestSettingsNoReferrerWhenDowngrade(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):\n-    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy'}\n+class TestSettingsNoReferrerWhenDowngrade(\n+    MixinNoReferrerWhenDowngrade, TestRefererMiddleware\n+):\n+    settings = {\n+        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy\"\n+    }\n \n \n class TestSettingsSameOrigin(MixinSameOrigin, TestRefererMiddleware):\n-    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.SameOriginPolicy'}\n+    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.SameOriginPolicy\"}\n \n \n class TestSettingsOrigin(MixinOrigin, TestRefererMiddleware):\n-    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.OriginPolicy'}\n+    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginPolicy\"}\n \n \n class TestSettingsStrictOrigin(MixinStrictOrigin, TestRefererMiddleware):\n-    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.StrictOriginPolicy'}\n+    settings = {\n+        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.StrictOriginPolicy\"\n+    }\n \n \n-class TestSettingsOriginWhenCrossOrigin(MixinOriginWhenCrossOrigin, TestRefererMiddleware):\n-    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}\n+class TestSettingsOriginWhenCrossOrigin(\n+    MixinOriginWhenCrossOrigin, TestRefererMiddleware\n+):\n+    settings = {\n+        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy\"\n+    }\n \n \n-class TestSettingsStrictOriginWhenCrossOrigin(MixinStrictOriginWhenCrossOrigin, TestRefererMiddleware):\n-    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy'}\n+class TestSettingsStrictOriginWhenCrossOrigin(\n+    MixinStrictOriginWhenCrossOrigin, TestRefererMiddleware\n+):\n+    settings = {\n+        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy\"\n+    }\n \n \n class TestSettingsUnsafeUrl(MixinUnsafeUrl, TestRefererMiddleware):\n-    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.UnsafeUrlPolicy'}\n+    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.UnsafeUrlPolicy\"}\n \n \n class CustomPythonOrgPolicy(ReferrerPolicy):\n@@ -376,77 +678,95 @@ class CustomPythonOrgPolicy(ReferrerPolicy):\n     A dummy policy that returns referrer as http(s)://python.org\n     depending on the scheme of the target URL.\n     \"\"\"\n+\n     def referrer(self, response, request):\n         scheme = urlparse(request).scheme\n-        if scheme == 'https':\n-            return b'https://python.org/'\n-        if scheme == 'http':\n-            return b'http://python.org/'\n+        if scheme == \"https\":\n+            return b\"https://python.org/\"\n+        if scheme == \"http\":\n+            return b\"http://python.org/\"\n \n \n class TestSettingsCustomPolicy(TestRefererMiddleware):\n-    settings = {'REFERRER_POLICY': CustomPythonOrgPolicy}\n+    settings = {\"REFERRER_POLICY\": CustomPythonOrgPolicy}\n     scenarii = [\n-        ('https://example.com/', 'https://scrapy.org/', b'https://python.org/'),\n-        ('http://example.com/', 'http://scrapy.org/', b'http://python.org/'),\n-        ('http://example.com/', 'https://scrapy.org/', b'https://python.org/'),\n-        ('https://example.com/', 'http://scrapy.org/', b'http://python.org/'),\n-        ('file:///home/path/to/somefile.html', 'https://scrapy.org/', b'https://python.org/'),\n-        ('file:///home/path/to/somefile.html', 'http://scrapy.org/', b'http://python.org/'),\n-\n+        (\"https://example.com/\", \"https://scrapy.org/\", b\"https://python.org/\"),\n+        (\"http://example.com/\", \"http://scrapy.org/\", b\"http://python.org/\"),\n+        (\"http://example.com/\", \"https://scrapy.org/\", b\"https://python.org/\"),\n+        (\"https://example.com/\", \"http://scrapy.org/\", b\"http://python.org/\"),\n+        (\n+            \"file:///home/path/to/somefile.html\",\n+            \"https://scrapy.org/\",\n+            b\"https://python.org/\",\n+        ),\n+        (\n+            \"file:///home/path/to/somefile.html\",\n+            \"http://scrapy.org/\",\n+            b\"http://python.org/\",\n+        ),\n     ]\n \n \n # --- Tests using Request meta dict to set policy\n class TestRequestMetaDefault(MixinDefault, TestRefererMiddleware):\n-    req_meta = {'referrer_policy': POLICY_SCRAPY_DEFAULT}\n+    req_meta = {\"referrer_policy\": POLICY_SCRAPY_DEFAULT}\n \n \n class TestRequestMetaNoReferrer(MixinNoReferrer, TestRefererMiddleware):\n-    req_meta = {'referrer_policy': POLICY_NO_REFERRER}\n+    req_meta = {\"referrer_policy\": POLICY_NO_REFERRER}\n \n \n-class TestRequestMetaNoReferrerWhenDowngrade(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):\n-    req_meta = {'referrer_policy': POLICY_NO_REFERRER_WHEN_DOWNGRADE}\n+class TestRequestMetaNoReferrerWhenDowngrade(\n+    MixinNoReferrerWhenDowngrade, TestRefererMiddleware\n+):\n+    req_meta = {\"referrer_policy\": POLICY_NO_REFERRER_WHEN_DOWNGRADE}\n \n \n class TestRequestMetaSameOrigin(MixinSameOrigin, TestRefererMiddleware):\n-    req_meta = {'referrer_policy': POLICY_SAME_ORIGIN}\n+    req_meta = {\"referrer_policy\": POLICY_SAME_ORIGIN}\n \n \n class TestRequestMetaOrigin(MixinOrigin, TestRefererMiddleware):\n-    req_meta = {'referrer_policy': POLICY_ORIGIN}\n+    req_meta = {\"referrer_policy\": POLICY_ORIGIN}\n \n \n class TestRequestMetaSrictOrigin(MixinStrictOrigin, TestRefererMiddleware):\n-    req_meta = {'referrer_policy': POLICY_STRICT_ORIGIN}\n+    req_meta = {\"referrer_policy\": POLICY_STRICT_ORIGIN}\n \n \n-class TestRequestMetaOriginWhenCrossOrigin(MixinOriginWhenCrossOrigin, TestRefererMiddleware):\n-    req_meta = {'referrer_policy': POLICY_ORIGIN_WHEN_CROSS_ORIGIN}\n+class TestRequestMetaOriginWhenCrossOrigin(\n+    MixinOriginWhenCrossOrigin, TestRefererMiddleware\n+):\n+    req_meta = {\"referrer_policy\": POLICY_ORIGIN_WHEN_CROSS_ORIGIN}\n \n \n-class TestRequestMetaStrictOriginWhenCrossOrigin(MixinStrictOriginWhenCrossOrigin, TestRefererMiddleware):\n-    req_meta = {'referrer_policy': POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN}\n+class TestRequestMetaStrictOriginWhenCrossOrigin(\n+    MixinStrictOriginWhenCrossOrigin, TestRefererMiddleware\n+):\n+    req_meta = {\"referrer_policy\": POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN}\n \n \n class TestRequestMetaUnsafeUrl(MixinUnsafeUrl, TestRefererMiddleware):\n-    req_meta = {'referrer_policy': POLICY_UNSAFE_URL}\n+    req_meta = {\"referrer_policy\": POLICY_UNSAFE_URL}\n \n \n class TestRequestMetaPredecence001(MixinUnsafeUrl, TestRefererMiddleware):\n-    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.SameOriginPolicy'}\n-    req_meta = {'referrer_policy': POLICY_UNSAFE_URL}\n+    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.SameOriginPolicy\"}\n+    req_meta = {\"referrer_policy\": POLICY_UNSAFE_URL}\n \n \n class TestRequestMetaPredecence002(MixinNoReferrer, TestRefererMiddleware):\n-    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy'}\n-    req_meta = {'referrer_policy': POLICY_NO_REFERRER}\n+    settings = {\n+        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy\"\n+    }\n+    req_meta = {\"referrer_policy\": POLICY_NO_REFERRER}\n \n \n class TestRequestMetaPredecence003(MixinUnsafeUrl, TestRefererMiddleware):\n-    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}\n-    req_meta = {'referrer_policy': POLICY_UNSAFE_URL}\n+    settings = {\n+        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy\"\n+    }\n+    req_meta = {\"referrer_policy\": POLICY_UNSAFE_URL}\n \n \n class TestRequestMetaSettingFallback(TestCase):\n@@ -456,57 +776,65 @@ class TestRequestMetaSettingFallback(TestCase):\n             # When an unknown policy is referenced in Request.meta\n             # (here, a typo error),\n             # the policy defined in settings takes precedence\n-            {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'},\n+            {\n+                \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy\"\n+            },\n             {},\n-            {'referrer_policy': 'ssscrapy-default'},\n+            {\"referrer_policy\": \"ssscrapy-default\"},\n             OriginWhenCrossOriginPolicy,\n-            True\n+            True,\n         ),\n         (\n             # same as above but with string value for settings policy\n-            {'REFERRER_POLICY': 'origin-when-cross-origin'},\n+            {\"REFERRER_POLICY\": \"origin-when-cross-origin\"},\n             {},\n-            {'referrer_policy': 'ssscrapy-default'},\n+            {\"referrer_policy\": \"ssscrapy-default\"},\n             OriginWhenCrossOriginPolicy,\n-            True\n+            True,\n         ),\n         (\n             # request meta references a wrong policy but it is set,\n             # so the Referrer-Policy header in response is not used,\n             # and the settings' policy is applied\n-            {'REFERRER_POLICY': 'origin-when-cross-origin'},\n-            {'Referrer-Policy': 'unsafe-url'},\n-            {'referrer_policy': 'ssscrapy-default'},\n+            {\"REFERRER_POLICY\": \"origin-when-cross-origin\"},\n+            {\"Referrer-Policy\": \"unsafe-url\"},\n+            {\"referrer_policy\": \"ssscrapy-default\"},\n             OriginWhenCrossOriginPolicy,\n-            True\n+            True,\n         ),\n         (\n             # here, request meta does not set the policy\n             # so response headers take precedence\n-            {'REFERRER_POLICY': 'origin-when-cross-origin'},\n-            {'Referrer-Policy': 'unsafe-url'},\n+            {\"REFERRER_POLICY\": \"origin-when-cross-origin\"},\n+            {\"Referrer-Policy\": \"unsafe-url\"},\n             {},\n             UnsafeUrlPolicy,\n-            False\n+            False,\n         ),\n         (\n             # here, request meta does not set the policy,\n             # but response headers also use an unknown policy,\n             # so the settings' policy is used\n-            {'REFERRER_POLICY': 'origin-when-cross-origin'},\n-            {'Referrer-Policy': 'unknown'},\n+            {\"REFERRER_POLICY\": \"origin-when-cross-origin\"},\n+            {\"Referrer-Policy\": \"unknown\"},\n             {},\n             OriginWhenCrossOriginPolicy,\n-            True\n-        )\n+            True,\n+        ),\n     ]\n \n     def test(self):\n \n-        origin = 'http://www.scrapy.org'\n-        target = 'http://www.example.com'\n+        origin = \"http://www.scrapy.org\"\n+        target = \"http://www.example.com\"\n \n-        for settings, response_headers, request_meta, policy_class, check_warning in self.params[3:]:\n+        for (\n+            settings,\n+            response_headers,\n+            request_meta,\n+            policy_class,\n+            check_warning,\n+        ) in self.params[3:]:\n             mw = RefererMiddleware(Settings(settings))\n \n             response = Response(origin, headers=response_headers)\n@@ -522,7 +850,6 @@ class TestRequestMetaSettingFallback(TestCase):\n \n \n class TestSettingsPolicyByName(TestCase):\n-\n     def test_valid_name(self):\n         for s, p in [\n             (POLICY_SCRAPY_DEFAULT, DefaultReferrerPolicy),\n@@ -535,7 +862,7 @@ class TestSettingsPolicyByName(TestCase):\n             (POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN, StrictOriginWhenCrossOriginPolicy),\n             (POLICY_UNSAFE_URL, UnsafeUrlPolicy),\n         ]:\n-            settings = Settings({'REFERRER_POLICY': s})\n+            settings = Settings({\"REFERRER_POLICY\": s})\n             mw = RefererMiddleware(settings)\n             self.assertEqual(mw.default_policy, p)\n \n@@ -551,98 +878,121 @@ class TestSettingsPolicyByName(TestCase):\n             (POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN, StrictOriginWhenCrossOriginPolicy),\n             (POLICY_UNSAFE_URL, UnsafeUrlPolicy),\n         ]:\n-            settings = Settings({'REFERRER_POLICY': s.upper()})\n+            settings = Settings({\"REFERRER_POLICY\": s.upper()})\n             mw = RefererMiddleware(settings)\n             self.assertEqual(mw.default_policy, p)\n \n     def test_invalid_name(self):\n-        settings = Settings({'REFERRER_POLICY': 'some-custom-unknown-policy'})\n+        settings = Settings({\"REFERRER_POLICY\": \"some-custom-unknown-policy\"})\n         with self.assertRaises(RuntimeError):\n             RefererMiddleware(settings)\n \n \n class TestPolicyHeaderPredecence001(MixinUnsafeUrl, TestRefererMiddleware):\n-    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.SameOriginPolicy'}\n-    resp_headers = {'Referrer-Policy': POLICY_UNSAFE_URL.upper()}\n+    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.SameOriginPolicy\"}\n+    resp_headers = {\"Referrer-Policy\": POLICY_UNSAFE_URL.upper()}\n \n \n class TestPolicyHeaderPredecence002(MixinNoReferrer, TestRefererMiddleware):\n-    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy'}\n-    resp_headers = {'Referrer-Policy': POLICY_NO_REFERRER.swapcase()}\n+    settings = {\n+        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy\"\n+    }\n+    resp_headers = {\"Referrer-Policy\": POLICY_NO_REFERRER.swapcase()}\n \n \n-class TestPolicyHeaderPredecence003(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):\n-    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}\n-    resp_headers = {'Referrer-Policy': POLICY_NO_REFERRER_WHEN_DOWNGRADE.title()}\n+class TestPolicyHeaderPredecence003(\n+    MixinNoReferrerWhenDowngrade, TestRefererMiddleware\n+):\n+    settings = {\n+        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy\"\n+    }\n+    resp_headers = {\"Referrer-Policy\": POLICY_NO_REFERRER_WHEN_DOWNGRADE.title()}\n \n \n-class TestPolicyHeaderPredecence004(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):\n+class TestPolicyHeaderPredecence004(\n+    MixinNoReferrerWhenDowngrade, TestRefererMiddleware\n+):\n     \"\"\"\n     The empty string means \"no-referrer-when-downgrade\"\n     \"\"\"\n-    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}\n-    resp_headers = {'Referrer-Policy': ''}\n+\n+    settings = {\n+        \"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy\"\n+    }\n+    resp_headers = {\"Referrer-Policy\": \"\"}\n \n \n class TestReferrerOnRedirect(TestRefererMiddleware):\n \n-    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.UnsafeUrlPolicy'}\n+    settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.UnsafeUrlPolicy\"}\n     scenarii = [\n         (\n-            'http://scrapytest.org/1',      # parent\n-            'http://scrapytest.org/2',      # target\n+            \"http://scrapytest.org/1\",  # parent\n+            \"http://scrapytest.org/2\",  # target\n             (\n                 # redirections: code, URL\n-                (301, 'http://scrapytest.org/3'),\n-                (301, 'http://scrapytest.org/4'),\n+                (301, \"http://scrapytest.org/3\"),\n+                (301, \"http://scrapytest.org/4\"),\n             ),\n-            b'http://scrapytest.org/1',  # expected initial referer\n-            b'http://scrapytest.org/1',  # expected referer for the redirection request\n+            b\"http://scrapytest.org/1\",  # expected initial referer\n+            b\"http://scrapytest.org/1\",  # expected referer for the redirection request\n         ),\n         (\n-            'https://scrapytest.org/1',\n-            'https://scrapytest.org/2',\n+            \"https://scrapytest.org/1\",\n+            \"https://scrapytest.org/2\",\n             (\n                 # redirecting to non-secure URL\n-                (301, 'http://scrapytest.org/3'),\n+                (301, \"http://scrapytest.org/3\"),\n             ),\n-            b'https://scrapytest.org/1',\n-            b'https://scrapytest.org/1',\n+            b\"https://scrapytest.org/1\",\n+            b\"https://scrapytest.org/1\",\n         ),\n         (\n-            'https://scrapytest.org/1',\n-            'https://scrapytest.com/2',\n+            \"https://scrapytest.org/1\",\n+            \"https://scrapytest.com/2\",\n             (\n                 # redirecting to non-secure URL: different origin\n-                (301, 'http://scrapytest.com/3'),\n+                (301, \"http://scrapytest.com/3\"),\n             ),\n-            b'https://scrapytest.org/1',\n-            b'https://scrapytest.org/1',\n+            b\"https://scrapytest.org/1\",\n+            b\"https://scrapytest.org/1\",\n         ),\n     ]\n \n     def setUp(self):\n-        self.spider = Spider('foo')\n+        self.spider = Spider(\"foo\")\n         settings = Settings(self.settings)\n         self.referrermw = RefererMiddleware(settings)\n         self.redirectmw = RedirectMiddleware(settings)\n \n     def test(self):\n \n-        for parent, target, redirections, init_referrer, final_referrer in self.scenarii:\n+        for (\n+            parent,\n+            target,\n+            redirections,\n+            init_referrer,\n+            final_referrer,\n+        ) in self.scenarii:\n             response = self.get_response(parent)\n             request = self.get_request(target)\n \n-            out = list(self.referrermw.process_spider_output(response, [request], self.spider))\n-            self.assertEqual(out[0].headers.get('Referer'), init_referrer)\n+            out = list(\n+                self.referrermw.process_spider_output(response, [request], self.spider)\n+            )\n+            self.assertEqual(out[0].headers.get(\"Referer\"), init_referrer)\n \n             for status, url in redirections:\n-                response = Response(request.url, headers={'Location': url}, status=status)\n-                request = self.redirectmw.process_response(request, response, self.spider)\n+                response = Response(\n+                    request.url, headers={\"Location\": url}, status=status\n+                )\n+                request = self.redirectmw.process_response(\n+                    request, response, self.spider\n+                )\n                 self.referrermw.request_scheduled(request, self.spider)\n \n             assert isinstance(request, Request)\n-            self.assertEqual(request.headers.get('Referer'), final_referrer)\n+            self.assertEqual(request.headers.get(\"Referer\"), final_referrer)\n \n \n class TestReferrerOnRedirectNoReferrer(TestReferrerOnRedirect):\n@@ -650,34 +1000,31 @@ class TestReferrerOnRedirectNoReferrer(TestReferrerOnRedirect):\n     No Referrer policy never sets the \"Referer\" header.\n     HTTP redirections should not change that.\n     \"\"\"\n-    settings = {'REFERRER_POLICY': 'no-referrer'}\n+\n+    settings = {\"REFERRER_POLICY\": \"no-referrer\"}\n     scenarii = [\n         (\n-            'http://scrapytest.org/1',      # parent\n-            'http://scrapytest.org/2',      # target\n+            \"http://scrapytest.org/1\",  # parent\n+            \"http://scrapytest.org/2\",  # target\n             (\n                 # redirections: code, URL\n-                (301, 'http://scrapytest.org/3'),\n-                (301, 'http://scrapytest.org/4'),\n+                (301, \"http://scrapytest.org/3\"),\n+                (301, \"http://scrapytest.org/4\"),\n             ),\n             None,  # expected initial \"Referer\"\n             None,  # expected \"Referer\" for the redirection request\n         ),\n         (\n-            'https://scrapytest.org/1',\n-            'https://scrapytest.org/2',\n-            (\n-                (301, 'http://scrapytest.org/3'),\n-            ),\n+            \"https://scrapytest.org/1\",\n+            \"https://scrapytest.org/2\",\n+            ((301, \"http://scrapytest.org/3\"),),\n             None,\n             None,\n         ),\n         (\n-            'https://scrapytest.org/1',\n-            'https://example.com/2',    # different origin\n-            (\n-                (301, 'http://scrapytest.com/3'),\n-            ),\n+            \"https://scrapytest.org/1\",\n+            \"https://example.com/2\",  # different origin\n+            ((301, \"http://scrapytest.com/3\"),),\n             None,\n             None,\n         ),\n@@ -692,37 +1039,38 @@ class TestReferrerOnRedirectSameOrigin(TestReferrerOnRedirect):\n     HTTP redirections to a different domain or a lower secure level\n     should have the \"Referer\" removed.\n     \"\"\"\n-    settings = {'REFERRER_POLICY': 'same-origin'}\n+\n+    settings = {\"REFERRER_POLICY\": \"same-origin\"}\n     scenarii = [\n         (\n-            'http://scrapytest.org/101',      # origin\n-            'http://scrapytest.org/102',      # target\n+            \"http://scrapytest.org/101\",  # origin\n+            \"http://scrapytest.org/102\",  # target\n             (\n                 # redirections: code, URL\n-                (301, 'http://scrapytest.org/103'),\n-                (301, 'http://scrapytest.org/104'),\n+                (301, \"http://scrapytest.org/103\"),\n+                (301, \"http://scrapytest.org/104\"),\n             ),\n-            b'http://scrapytest.org/101',  # expected initial \"Referer\"\n-            b'http://scrapytest.org/101',  # expected referer for the redirection request\n+            b\"http://scrapytest.org/101\",  # expected initial \"Referer\"\n+            b\"http://scrapytest.org/101\",  # expected referer for the redirection request\n         ),\n         (\n-            'https://scrapytest.org/201',\n-            'https://scrapytest.org/202',\n+            \"https://scrapytest.org/201\",\n+            \"https://scrapytest.org/202\",\n             (\n                 # redirecting from secure to non-secure URL == different origin\n-                (301, 'http://scrapytest.org/203'),\n+                (301, \"http://scrapytest.org/203\"),\n             ),\n-            b'https://scrapytest.org/201',\n+            b\"https://scrapytest.org/201\",\n             None,\n         ),\n         (\n-            'https://scrapytest.org/301',\n-            'https://scrapytest.org/302',\n+            \"https://scrapytest.org/301\",\n+            \"https://scrapytest.org/302\",\n             (\n                 # different domain == different origin\n-                (301, 'http://example.com/303'),\n+                (301, \"http://example.com/303\"),\n             ),\n-            b'https://scrapytest.org/301',\n+            b\"https://scrapytest.org/301\",\n             None,\n         ),\n     ]\n@@ -737,63 +1085,65 @@ class TestReferrerOnRedirectStrictOrigin(TestReferrerOnRedirect):\n     Redirections from secure to non-secure URLs should have the\n     \"Referrer\" header removed if necessary.\n     \"\"\"\n-    settings = {'REFERRER_POLICY': POLICY_STRICT_ORIGIN}\n+\n+    settings = {\"REFERRER_POLICY\": POLICY_STRICT_ORIGIN}\n     scenarii = [\n         (\n-            'http://scrapytest.org/101',\n-            'http://scrapytest.org/102',\n+            \"http://scrapytest.org/101\",\n+            \"http://scrapytest.org/102\",\n             (\n-                (301, 'http://scrapytest.org/103'),\n-                (301, 'http://scrapytest.org/104'),\n+                (301, \"http://scrapytest.org/103\"),\n+                (301, \"http://scrapytest.org/104\"),\n             ),\n-            b'http://scrapytest.org/',  # send origin\n-            b'http://scrapytest.org/',  # redirects to same origin: send origin\n+            b\"http://scrapytest.org/\",  # send origin\n+            b\"http://scrapytest.org/\",  # redirects to same origin: send origin\n         ),\n         (\n-            'https://scrapytest.org/201',\n-            'https://scrapytest.org/202',\n+            \"https://scrapytest.org/201\",\n+            \"https://scrapytest.org/202\",\n             (\n                 # redirecting to non-secure URL: no referrer\n-                (301, 'http://scrapytest.org/203'),\n+                (301, \"http://scrapytest.org/203\"),\n             ),\n-            b'https://scrapytest.org/',\n+            b\"https://scrapytest.org/\",\n             None,\n         ),\n         (\n-            'https://scrapytest.org/301',\n-            'https://scrapytest.org/302',\n+            \"https://scrapytest.org/301\",\n+            \"https://scrapytest.org/302\",\n             (\n                 # redirecting to non-secure URL (different domain): no referrer\n-                (301, 'http://example.com/303'),\n+                (301, \"http://example.com/303\"),\n             ),\n-            b'https://scrapytest.org/',\n+            b\"https://scrapytest.org/\",\n             None,\n         ),\n         (\n-            'http://scrapy.org/401',\n-            'http://example.com/402',\n-            (\n-                (301, 'http://scrapytest.org/403'),\n-            ),\n-            b'http://scrapy.org/',\n-            b'http://scrapy.org/',\n+            \"http://scrapy.org/401\",\n+            \"http://example.com/402\",\n+            ((301, \"http://scrapytest.org/403\"),),\n+            b\"http://scrapy.org/\",\n+            b\"http://scrapy.org/\",\n         ),\n         (\n-            'https://scrapy.org/501',\n-            'https://example.com/502',\n+            \"https://scrapy.org/501\",\n+            \"https://example.com/502\",\n             (\n                 # HTTPS all along, so origin referrer is kept as-is\n-                (301, 'https://google.com/503'),\n-                (301, 'https://facebook.com/504'),\n+                (301, \"https://google.com/503\"),\n+                (301, \"https://facebook.com/504\"),\n             ),\n-            b'https://scrapy.org/',\n-            b'https://scrapy.org/',\n+            b\"https://scrapy.org/\",\n+            b\"https://scrapy.org/\",\n         ),\n         (\n-            'https://scrapytest.org/601',\n-            'http://scrapytest.org/602',                # TLS to non-TLS: no referrer\n+            \"https://scrapytest.org/601\",\n+            \"http://scrapytest.org/602\",  # TLS to non-TLS: no referrer\n             (\n-                (301, 'https://scrapytest.org/603'),    # TLS URL again: (still) no referrer\n+                (\n+                    301,\n+                    \"https://scrapytest.org/603\",\n+                ),  # TLS URL again: (still) no referrer\n             ),\n             None,\n             None,\n@@ -810,67 +1160,64 @@ class TestReferrerOnRedirectOriginWhenCrossOrigin(TestReferrerOnRedirect):\n     Redirections to a different origin should strip the \"Referer\"\n     to the parent origin.\n     \"\"\"\n-    settings = {'REFERRER_POLICY': POLICY_ORIGIN_WHEN_CROSS_ORIGIN}\n+\n+    settings = {\"REFERRER_POLICY\": POLICY_ORIGIN_WHEN_CROSS_ORIGIN}\n     scenarii = [\n         (\n-            'http://scrapytest.org/101',      # origin\n-            'http://scrapytest.org/102',      # target + redirection\n+            \"http://scrapytest.org/101\",  # origin\n+            \"http://scrapytest.org/102\",  # target + redirection\n             (\n                 # redirections: code, URL\n-                (301, 'http://scrapytest.org/103'),\n-                (301, 'http://scrapytest.org/104'),\n+                (301, \"http://scrapytest.org/103\"),\n+                (301, \"http://scrapytest.org/104\"),\n             ),\n-            b'http://scrapytest.org/101',  # expected initial referer\n-            b'http://scrapytest.org/101',  # expected referer for the redirection request\n+            b\"http://scrapytest.org/101\",  # expected initial referer\n+            b\"http://scrapytest.org/101\",  # expected referer for the redirection request\n         ),\n         (\n-            'https://scrapytest.org/201',\n-            'https://scrapytest.org/202',\n+            \"https://scrapytest.org/201\",\n+            \"https://scrapytest.org/202\",\n             (\n                 # redirecting to non-secure URL: send origin\n-                (301, 'http://scrapytest.org/203'),\n+                (301, \"http://scrapytest.org/203\"),\n             ),\n-            b'https://scrapytest.org/201',\n-            b'https://scrapytest.org/',\n+            b\"https://scrapytest.org/201\",\n+            b\"https://scrapytest.org/\",\n         ),\n         (\n-            'https://scrapytest.org/301',\n-            'https://scrapytest.org/302',\n+            \"https://scrapytest.org/301\",\n+            \"https://scrapytest.org/302\",\n             (\n                 # redirecting to non-secure URL (different domain): send origin\n-                (301, 'http://example.com/303'),\n+                (301, \"http://example.com/303\"),\n             ),\n-            b'https://scrapytest.org/301',\n-            b'https://scrapytest.org/',\n+            b\"https://scrapytest.org/301\",\n+            b\"https://scrapytest.org/\",\n         ),\n         (\n-            'http://scrapy.org/401',\n-            'http://example.com/402',\n-            (\n-                (301, 'http://scrapytest.org/403'),\n-            ),\n-            b'http://scrapy.org/',\n-            b'http://scrapy.org/',\n+            \"http://scrapy.org/401\",\n+            \"http://example.com/402\",\n+            ((301, \"http://scrapytest.org/403\"),),\n+            b\"http://scrapy.org/\",\n+            b\"http://scrapy.org/\",\n         ),\n         (\n-            'https://scrapy.org/501',\n-            'https://example.com/502',\n+            \"https://scrapy.org/501\",\n+            \"https://example.com/502\",\n             (\n                 # all different domains: send origin\n-                (301, 'https://google.com/503'),\n-                (301, 'https://facebook.com/504'),\n+                (301, \"https://google.com/503\"),\n+                (301, \"https://facebook.com/504\"),\n             ),\n-            b'https://scrapy.org/',\n-            b'https://scrapy.org/',\n+            b\"https://scrapy.org/\",\n+            b\"https://scrapy.org/\",\n         ),\n         (\n-            'https://scrapytest.org/301',\n-            'http://scrapytest.org/302',                # TLS to non-TLS: send origin\n-            (\n-                (301, 'https://scrapytest.org/303'),    # TLS URL again: send origin (also)\n-            ),\n-            b'https://scrapytest.org/',\n-            b'https://scrapytest.org/',\n+            \"https://scrapytest.org/301\",\n+            \"http://scrapytest.org/302\",  # TLS to non-TLS: send origin\n+            ((301, \"https://scrapytest.org/303\"),),  # TLS URL again: send origin (also)\n+            b\"https://scrapytest.org/\",\n+            b\"https://scrapytest.org/\",\n         ),\n     ]\n \n@@ -886,64 +1233,66 @@ class TestReferrerOnRedirectStrictOriginWhenCrossOrigin(TestReferrerOnRedirect):\n     Redirections to a different origin should strip the \"Referer\" to the parent origin,\n     and from https:// to http:// will remove the \"Referer\" header.\n     \"\"\"\n-    settings = {'REFERRER_POLICY': POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN}\n+\n+    settings = {\"REFERRER_POLICY\": POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN}\n     scenarii = [\n         (\n-            'http://scrapytest.org/101',      # origin\n-            'http://scrapytest.org/102',      # target + redirection\n+            \"http://scrapytest.org/101\",  # origin\n+            \"http://scrapytest.org/102\",  # target + redirection\n             (\n                 # redirections: code, URL\n-                (301, 'http://scrapytest.org/103'),\n-                (301, 'http://scrapytest.org/104'),\n+                (301, \"http://scrapytest.org/103\"),\n+                (301, \"http://scrapytest.org/104\"),\n             ),\n-            b'http://scrapytest.org/101',  # expected initial referer\n-            b'http://scrapytest.org/101',  # expected referer for the redirection request\n+            b\"http://scrapytest.org/101\",  # expected initial referer\n+            b\"http://scrapytest.org/101\",  # expected referer for the redirection request\n         ),\n         (\n-            'https://scrapytest.org/201',\n-            'https://scrapytest.org/202',\n+            \"https://scrapytest.org/201\",\n+            \"https://scrapytest.org/202\",\n             (\n                 # redirecting to non-secure URL: do not send the \"Referer\" header\n-                (301, 'http://scrapytest.org/203'),\n+                (301, \"http://scrapytest.org/203\"),\n             ),\n-            b'https://scrapytest.org/201',\n+            b\"https://scrapytest.org/201\",\n             None,\n         ),\n         (\n-            'https://scrapytest.org/301',\n-            'https://scrapytest.org/302',\n+            \"https://scrapytest.org/301\",\n+            \"https://scrapytest.org/302\",\n             (\n                 # redirecting to non-secure URL (different domain): send origin\n-                (301, 'http://example.com/303'),\n+                (301, \"http://example.com/303\"),\n             ),\n-            b'https://scrapytest.org/301',\n+            b\"https://scrapytest.org/301\",\n             None,\n         ),\n         (\n-            'http://scrapy.org/401',\n-            'http://example.com/402',\n-            (\n-                (301, 'http://scrapytest.org/403'),\n-            ),\n-            b'http://scrapy.org/',\n-            b'http://scrapy.org/',\n+            \"http://scrapy.org/401\",\n+            \"http://example.com/402\",\n+            ((301, \"http://scrapytest.org/403\"),),\n+            b\"http://scrapy.org/\",\n+            b\"http://scrapy.org/\",\n         ),\n         (\n-            'https://scrapy.org/501',\n-            'https://example.com/502',\n+            \"https://scrapy.org/501\",\n+            \"https://example.com/502\",\n             (\n                 # all different domains: send origin\n-                (301, 'https://google.com/503'),\n-                (301, 'https://facebook.com/504'),\n+                (301, \"https://google.com/503\"),\n+                (301, \"https://facebook.com/504\"),\n             ),\n-            b'https://scrapy.org/',\n-            b'https://scrapy.org/',\n+            b\"https://scrapy.org/\",\n+            b\"https://scrapy.org/\",\n         ),\n         (\n-            'https://scrapytest.org/601',\n-            'http://scrapytest.org/602',                # TLS to non-TLS: do not send \"Referer\"\n+            \"https://scrapytest.org/601\",\n+            \"http://scrapytest.org/602\",  # TLS to non-TLS: do not send \"Referer\"\n             (\n-                (301, 'https://scrapytest.org/603'),    # TLS URL again: (still) send nothing\n+                (\n+                    301,\n+                    \"https://scrapytest.org/603\",\n+                ),  # TLS URL again: (still) send nothing\n             ),\n             None,\n             None,\n\n@@ -10,23 +10,24 @@ from scrapy.settings import Settings\n \n \n class TestUrlLengthMiddleware(TestCase):\n-\n     def setUp(self):\n         self.maxlength = 25\n-        settings = Settings({'URLLENGTH_LIMIT': self.maxlength})\n+        settings = Settings({\"URLLENGTH_LIMIT\": self.maxlength})\n \n         crawler = get_crawler(Spider)\n-        self.spider = crawler._create_spider('foo')\n+        self.spider = crawler._create_spider(\"foo\")\n         self.stats = crawler.stats\n         self.mw = UrlLengthMiddleware.from_settings(settings)\n \n-        self.response = Response('http://scrapytest.org')\n-        self.short_url_req = Request('http://scrapytest.org/')\n-        self.long_url_req = Request('http://scrapytest.org/this_is_a_long_url')\n+        self.response = Response(\"http://scrapytest.org\")\n+        self.short_url_req = Request(\"http://scrapytest.org/\")\n+        self.long_url_req = Request(\"http://scrapytest.org/this_is_a_long_url\")\n         self.reqs = [self.short_url_req, self.long_url_req]\n \n     def process_spider_output(self):\n-        return list(self.mw.process_spider_output(self.response, self.reqs, self.spider))\n+        return list(\n+            self.mw.process_spider_output(self.response, self.reqs, self.spider)\n+        )\n \n     def test_middleware_works(self):\n         self.assertEqual(self.process_spider_output(), [self.short_url_req])\n@@ -35,7 +36,9 @@ class TestUrlLengthMiddleware(TestCase):\n         with LogCapture() as log:\n             self.process_spider_output()\n \n-        ric = self.stats.get_value('urllength/request_ignored_count', spider=self.spider)\n+        ric = self.stats.get_value(\n+            \"urllength/request_ignored_count\", spider=self.spider\n+        )\n         self.assertEqual(ric, 1)\n \n-        self.assertIn(f'Ignoring link (url length > {self.maxlength})', str(log))\n+        self.assertIn(f\"Ignoring link (url length > {self.maxlength})\", str(log))\n\n@@ -10,24 +10,23 @@ from scrapy.utils.test import get_crawler\n \n \n class SpiderStateTest(unittest.TestCase):\n-\n     def test_store_load(self):\n         jobdir = self.mktemp()\n         Path(jobdir).mkdir()\n         try:\n-            spider = Spider(name='default')\n+            spider = Spider(name=\"default\")\n             dt = datetime.now()\n \n             ss = SpiderState(jobdir)\n             ss.spider_opened(spider)\n-            spider.state['one'] = 1\n-            spider.state['dt'] = dt\n+            spider.state[\"one\"] = 1\n+            spider.state[\"dt\"] = dt\n             ss.spider_closed(spider)\n \n-            spider2 = Spider(name='default')\n+            spider2 = Spider(name=\"default\")\n             ss2 = SpiderState(jobdir)\n             ss2.spider_opened(spider2)\n-            self.assertEqual(spider.state, {'one': 1, 'dt': dt})\n+            self.assertEqual(spider.state, {\"one\": 1, \"dt\": dt})\n             ss2.spider_closed(spider2)\n         finally:\n             shutil.rmtree(jobdir)\n@@ -35,7 +34,7 @@ class SpiderStateTest(unittest.TestCase):\n     def test_state_attribute(self):\n         # state attribute must be present if jobdir is not set, to provide a\n         # consistent interface\n-        spider = Spider(name='default')\n+        spider = Spider(name=\"default\")\n         ss = SpiderState()\n         ss.spider_opened(spider)\n         self.assertEqual(spider.state, {})\n\n@@ -31,20 +31,19 @@ def nonserializable_object_test(self):\n     q = self.queue()\n     self.assertRaises(ValueError, q.push, lambda x: x)\n     # Selectors should fail (lxml.html.HtmlElement objects can't be pickled)\n-    sel = Selector(text='<html><body><p>some text</p></body></html>')\n+    sel = Selector(text=\"<html><body><p>some text</p></body></html>\")\n     self.assertRaises(ValueError, q.push, sel)\n \n \n class FifoDiskQueueTestMixin:\n-\n     def test_serialize(self):\n         q = self.queue()\n-        q.push('a')\n+        q.push(\"a\")\n         q.push(123)\n-        q.push({'a': 'dict'})\n-        self.assertEqual(q.pop(), 'a')\n+        q.push({\"a\": \"dict\"})\n+        self.assertEqual(q.pop(), \"a\")\n         self.assertEqual(q.pop(), 123)\n-        self.assertEqual(q.pop(), {'a': 'dict'})\n+        self.assertEqual(q.pop(), {\"a\": \"dict\"})\n \n     test_nonserializable_object = nonserializable_object_test\n \n@@ -81,7 +80,7 @@ class PickleFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n \n     def test_serialize_item(self):\n         q = self.queue()\n-        i = TestItem(name='foo')\n+        i = TestItem(name=\"foo\")\n         q.push(i)\n         i2 = q.pop()\n         assert isinstance(i2, TestItem)\n@@ -94,17 +93,17 @@ class PickleFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n         loader2 = q.pop()\n         assert isinstance(loader2, TestLoader)\n         assert loader2.default_item_class is TestItem\n-        self.assertEqual(loader2.name_out('x'), 'xx')\n+        self.assertEqual(loader2.name_out(\"x\"), \"xx\")\n \n     def test_serialize_request_recursive(self):\n         q = self.queue()\n-        r = Request('http://www.example.com')\n-        r.meta['request'] = r\n+        r = Request(\"http://www.example.com\")\n+        r.meta[\"request\"] = r\n         q.push(r)\n         r2 = q.pop()\n         assert isinstance(r2, Request)\n         self.assertEqual(r.url, r2.url)\n-        assert r2.meta['request'] is r2\n+        assert r2.meta[\"request\"] is r2\n \n     def test_non_pickable_object(self):\n         q = self.queue()\n@@ -115,7 +114,7 @@ class PickleFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n                 self.assertIsInstance(exc.__context__, pickle.PicklingError)\n             else:\n                 self.assertIsInstance(exc.__context__, AttributeError)\n-        sel = Selector(text='<html><body><p>some text</p></body></html>')\n+        sel = Selector(text=\"<html><body><p>some text</p></body></html>\")\n         try:\n             q.push(sel)\n         except ValueError as exc:\n@@ -139,33 +138,30 @@ class ChunkSize4PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n \n \n class LifoDiskQueueTestMixin:\n-\n     def test_serialize(self):\n         q = self.queue()\n-        q.push('a')\n+        q.push(\"a\")\n         q.push(123)\n-        q.push({'a': 'dict'})\n-        self.assertEqual(q.pop(), {'a': 'dict'})\n+        q.push({\"a\": \"dict\"})\n+        self.assertEqual(q.pop(), {\"a\": \"dict\"})\n         self.assertEqual(q.pop(), 123)\n-        self.assertEqual(q.pop(), 'a')\n+        self.assertEqual(q.pop(), \"a\")\n \n     test_nonserializable_object = nonserializable_object_test\n \n \n class MarshalLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):\n-\n     def queue(self):\n         return _MarshalLifoSerializationDiskQueue(self.qpath)\n \n \n class PickleLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):\n-\n     def queue(self):\n         return _PickleLifoSerializationDiskQueue(self.qpath)\n \n     def test_serialize_item(self):\n         q = self.queue()\n-        i = TestItem(name='foo')\n+        i = TestItem(name=\"foo\")\n         q.push(i)\n         i2 = q.pop()\n         assert isinstance(i2, TestItem)\n@@ -178,14 +174,14 @@ class PickleLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):\n         loader2 = q.pop()\n         assert isinstance(loader2, TestLoader)\n         assert loader2.default_item_class is TestItem\n-        self.assertEqual(loader2.name_out('x'), 'xx')\n+        self.assertEqual(loader2.name_out(\"x\"), \"xx\")\n \n     def test_serialize_request_recursive(self):\n         q = self.queue()\n-        r = Request('http://www.example.com')\n-        r.meta['request'] = r\n+        r = Request(\"http://www.example.com\")\n+        r.meta[\"request\"] = r\n         q.push(r)\n         r2 = q.pop()\n         assert isinstance(r2, Request)\n         self.assertEqual(r.url, r2.url)\n-        assert r2.meta['request'] is r2\n+        assert r2.meta[\"request\"] is r2\n\n@@ -69,7 +69,10 @@ class RequestQueueTestMixin:\n         req = Request(\"http://www.example.com\")\n         q.push(req)\n         self.assertEqual(len(q), 1)\n-        with self.assertRaises(NotImplementedError, msg=\"The underlying queue class does not implement 'peek'\"):\n+        with self.assertRaises(\n+            NotImplementedError,\n+            msg=\"The underlying queue class does not implement 'peek'\",\n+        ):\n             q.peek()\n         self.assertEqual(q.pop().url, req.url)\n         self.assertEqual(len(q), 0)\n@@ -117,7 +120,10 @@ class FifoQueueMixin(RequestQueueTestMixin):\n         q.push(req1)\n         q.push(req2)\n         q.push(req3)\n-        with self.assertRaises(NotImplementedError, msg=\"The underlying queue class does not implement 'peek'\"):\n+        with self.assertRaises(\n+            NotImplementedError,\n+            msg=\"The underlying queue class does not implement 'peek'\",\n+        ):\n             q.peek()\n         self.assertEqual(len(q), 3)\n         self.assertEqual(q.pop().url, req1.url)\n@@ -170,7 +176,10 @@ class LifoQueueMixin(RequestQueueTestMixin):\n         q.push(req1)\n         q.push(req2)\n         q.push(req3)\n-        with self.assertRaises(NotImplementedError, msg=\"The underlying queue class does not implement 'peek'\"):\n+        with self.assertRaises(\n+            NotImplementedError,\n+            msg=\"The underlying queue class does not implement 'peek'\",\n+        ):\n             q.peek()\n         self.assertEqual(len(q), 3)\n         self.assertEqual(q.pop().url, req3.url)\n@@ -195,12 +204,16 @@ class PickleLifoDiskQueueRequestTest(LifoQueueMixin, BaseQueueTestCase):\n \n class MarshalFifoDiskQueueRequestTest(FifoQueueMixin, BaseQueueTestCase):\n     def queue(self):\n-        return MarshalFifoDiskQueue.from_crawler(crawler=self.crawler, key=\"marshal/fifo\")\n+        return MarshalFifoDiskQueue.from_crawler(\n+            crawler=self.crawler, key=\"marshal/fifo\"\n+        )\n \n \n class MarshalLifoDiskQueueRequestTest(LifoQueueMixin, BaseQueueTestCase):\n     def queue(self):\n-        return MarshalLifoDiskQueue.from_crawler(crawler=self.crawler, key=\"marshal/lifo\")\n+        return MarshalLifoDiskQueue.from_crawler(\n+            crawler=self.crawler, key=\"marshal/lifo\"\n+        )\n \n \n class FifoMemoryQueueRequestTest(FifoQueueMixin, BaseQueueTestCase):\n\n@@ -9,12 +9,11 @@ from scrapy.utils.test import get_crawler\n \n \n class CoreStatsExtensionTest(unittest.TestCase):\n-\n     def setUp(self):\n         self.crawler = get_crawler(Spider)\n-        self.spider = self.crawler._create_spider('foo')\n+        self.spider = self.crawler._create_spider(\"foo\")\n \n-    @mock.patch('scrapy.extensions.corestats.datetime')\n+    @mock.patch(\"scrapy.extensions.corestats.datetime\")\n     def test_core_stats_default_stats_collector(self, mock_datetime):\n         fixed_datetime = datetime(2019, 12, 1, 11, 38)\n         mock_datetime.utcnow = mock.Mock(return_value=fixed_datetime)\n@@ -24,19 +23,19 @@ class CoreStatsExtensionTest(unittest.TestCase):\n         ext.item_scraped({}, self.spider)\n         ext.response_received(self.spider)\n         ext.item_dropped({}, self.spider, ZeroDivisionError())\n-        ext.spider_closed(self.spider, 'finished')\n+        ext.spider_closed(self.spider, \"finished\")\n         self.assertEqual(\n             ext.stats._stats,\n             {\n-                'start_time': fixed_datetime,\n-                'finish_time': fixed_datetime,\n-                'item_scraped_count': 1,\n-                'response_received_count': 1,\n-                'item_dropped_count': 1,\n-                'item_dropped_reasons_count/ZeroDivisionError': 1,\n-                'finish_reason': 'finished',\n-                'elapsed_time_seconds': 0.0,\n-            }\n+                \"start_time\": fixed_datetime,\n+                \"finish_time\": fixed_datetime,\n+                \"item_scraped_count\": 1,\n+                \"response_received_count\": 1,\n+                \"item_dropped_count\": 1,\n+                \"item_dropped_reasons_count/ZeroDivisionError\": 1,\n+                \"finish_reason\": \"finished\",\n+                \"elapsed_time_seconds\": 0.0,\n+            },\n         )\n \n     def test_core_stats_dummy_stats_collector(self):\n@@ -46,53 +45,52 @@ class CoreStatsExtensionTest(unittest.TestCase):\n         ext.item_scraped({}, self.spider)\n         ext.response_received(self.spider)\n         ext.item_dropped({}, self.spider, ZeroDivisionError())\n-        ext.spider_closed(self.spider, 'finished')\n+        ext.spider_closed(self.spider, \"finished\")\n         self.assertEqual(ext.stats._stats, {})\n \n \n class StatsCollectorTest(unittest.TestCase):\n-\n     def setUp(self):\n         self.crawler = get_crawler(Spider)\n-        self.spider = self.crawler._create_spider('foo')\n+        self.spider = self.crawler._create_spider(\"foo\")\n \n     def test_collector(self):\n         stats = StatsCollector(self.crawler)\n         self.assertEqual(stats.get_stats(), {})\n-        self.assertEqual(stats.get_value('anything'), None)\n-        self.assertEqual(stats.get_value('anything', 'default'), 'default')\n-        stats.set_value('test', 'value')\n-        self.assertEqual(stats.get_stats(), {'test': 'value'})\n-        stats.set_value('test2', 23)\n-        self.assertEqual(stats.get_stats(), {'test': 'value', 'test2': 23})\n-        self.assertEqual(stats.get_value('test2'), 23)\n-        stats.inc_value('test2')\n-        self.assertEqual(stats.get_value('test2'), 24)\n-        stats.inc_value('test2', 6)\n-        self.assertEqual(stats.get_value('test2'), 30)\n-        stats.max_value('test2', 6)\n-        self.assertEqual(stats.get_value('test2'), 30)\n-        stats.max_value('test2', 40)\n-        self.assertEqual(stats.get_value('test2'), 40)\n-        stats.max_value('test3', 1)\n-        self.assertEqual(stats.get_value('test3'), 1)\n-        stats.min_value('test2', 60)\n-        self.assertEqual(stats.get_value('test2'), 40)\n-        stats.min_value('test2', 35)\n-        self.assertEqual(stats.get_value('test2'), 35)\n-        stats.min_value('test4', 7)\n-        self.assertEqual(stats.get_value('test4'), 7)\n+        self.assertEqual(stats.get_value(\"anything\"), None)\n+        self.assertEqual(stats.get_value(\"anything\", \"default\"), \"default\")\n+        stats.set_value(\"test\", \"value\")\n+        self.assertEqual(stats.get_stats(), {\"test\": \"value\"})\n+        stats.set_value(\"test2\", 23)\n+        self.assertEqual(stats.get_stats(), {\"test\": \"value\", \"test2\": 23})\n+        self.assertEqual(stats.get_value(\"test2\"), 23)\n+        stats.inc_value(\"test2\")\n+        self.assertEqual(stats.get_value(\"test2\"), 24)\n+        stats.inc_value(\"test2\", 6)\n+        self.assertEqual(stats.get_value(\"test2\"), 30)\n+        stats.max_value(\"test2\", 6)\n+        self.assertEqual(stats.get_value(\"test2\"), 30)\n+        stats.max_value(\"test2\", 40)\n+        self.assertEqual(stats.get_value(\"test2\"), 40)\n+        stats.max_value(\"test3\", 1)\n+        self.assertEqual(stats.get_value(\"test3\"), 1)\n+        stats.min_value(\"test2\", 60)\n+        self.assertEqual(stats.get_value(\"test2\"), 40)\n+        stats.min_value(\"test2\", 35)\n+        self.assertEqual(stats.get_value(\"test2\"), 35)\n+        stats.min_value(\"test4\", 7)\n+        self.assertEqual(stats.get_value(\"test4\"), 7)\n \n     def test_dummy_collector(self):\n         stats = DummyStatsCollector(self.crawler)\n         self.assertEqual(stats.get_stats(), {})\n-        self.assertEqual(stats.get_value('anything'), None)\n-        self.assertEqual(stats.get_value('anything', 'default'), 'default')\n-        stats.set_value('test', 'value')\n-        stats.inc_value('v1')\n-        stats.max_value('v2', 100)\n-        stats.min_value('v3', 100)\n-        stats.open_spider('a')\n-        stats.set_value('test', 'value', spider=self.spider)\n+        self.assertEqual(stats.get_value(\"anything\"), None)\n+        self.assertEqual(stats.get_value(\"anything\", \"default\"), \"default\")\n+        stats.set_value(\"test\", \"value\")\n+        stats.inc_value(\"v1\")\n+        stats.max_value(\"v2\", 100)\n+        stats.min_value(\"v3\", 100)\n+        stats.open_spider(\"a\")\n+        stats.set_value(\"test\", \"value\", spider=self.spider)\n         self.assertEqual(stats.get_stats(), {})\n-        self.assertEqual(stats.get_stats('a'), {})\n+        self.assertEqual(stats.get_stats(\"a\"), {})\n\n@@ -4,7 +4,6 @@ import scrapy\n \n \n class ToplevelTestCase(TestCase):\n-\n     def test_version(self):\n         self.assertIs(type(scrapy.__version__), str)\n \n@@ -13,18 +12,22 @@ class ToplevelTestCase(TestCase):\n \n     def test_request_shortcut(self):\n         from scrapy.http import Request, FormRequest\n+\n         self.assertIs(scrapy.Request, Request)\n         self.assertIs(scrapy.FormRequest, FormRequest)\n \n     def test_spider_shortcut(self):\n         from scrapy.spiders import Spider\n+\n         self.assertIs(scrapy.Spider, Spider)\n \n     def test_selector_shortcut(self):\n         from scrapy.selector import Selector\n+\n         self.assertIs(scrapy.Selector, Selector)\n \n     def test_item_shortcut(self):\n         from scrapy.item import Item, Field\n+\n         self.assertIs(scrapy.Item, Item)\n         self.assertIs(scrapy.Field, Field)\n\n@@ -3,10 +3,9 @@ import unittest\n \n \n class UrlparseTestCase(unittest.TestCase):\n-\n     def test_s3_url(self):\n-        p = urlparse('s3://bucket/key/name?param=value')\n-        self.assertEqual(p.scheme, 's3')\n-        self.assertEqual(p.hostname, 'bucket')\n-        self.assertEqual(p.path, '/key/name')\n-        self.assertEqual(p.query, 'param=value')\n+        p = urlparse(\"s3://bucket/key/name?param=value\")\n+        self.assertEqual(p.scheme, \"s3\")\n+        self.assertEqual(p.hostname, \"bucket\")\n+        self.assertEqual(p.path, \"/key/name\")\n+        self.assertEqual(p.query, \"param=value\")\n\n@@ -6,12 +6,13 @@ from pytest import mark\n from scrapy.utils.reactor import is_asyncio_reactor_installed, install_reactor\n \n \n-@mark.usefixtures('reactor_pytest')\n+@mark.usefixtures(\"reactor_pytest\")\n class AsyncioTest(TestCase):\n-\n     def test_is_asyncio_reactor_installed(self):\n         # the result should depend only on the pytest --reactor argument\n-        self.assertEqual(is_asyncio_reactor_installed(), self.reactor_pytest == 'asyncio')\n+        self.assertEqual(\n+            is_asyncio_reactor_installed(), self.reactor_pytest == \"asyncio\"\n+        )\n \n     def test_install_asyncio_reactor(self):\n         with warnings.catch_warnings(record=True) as w:\n\n@@ -7,169 +7,205 @@ from scrapy.utils.conf import (\n     arglist_to_dict,\n     build_component_list,\n     feed_complete_default_values_from_settings,\n-    feed_process_params_from_cli\n+    feed_process_params_from_cli,\n )\n \n \n class BuildComponentListTest(unittest.TestCase):\n-\n     def test_build_dict(self):\n-        d = {'one': 1, 'two': None, 'three': 8, 'four': 4}\n-        self.assertEqual(build_component_list(d, convert=lambda x: x),\n-                         ['one', 'four', 'three'])\n+        d = {\"one\": 1, \"two\": None, \"three\": 8, \"four\": 4}\n+        self.assertEqual(\n+            build_component_list(d, convert=lambda x: x), [\"one\", \"four\", \"three\"]\n+        )\n \n     def test_backward_compatible_build_dict(self):\n-        base = {'one': 1, 'two': 2, 'three': 3, 'five': 5, 'six': None}\n-        custom = {'two': None, 'three': 8, 'four': 4}\n-        self.assertEqual(build_component_list(base, custom,\n-                                              convert=lambda x: x),\n-                         ['one', 'four', 'five', 'three'])\n+        base = {\"one\": 1, \"two\": 2, \"three\": 3, \"five\": 5, \"six\": None}\n+        custom = {\"two\": None, \"three\": 8, \"four\": 4}\n+        self.assertEqual(\n+            build_component_list(base, custom, convert=lambda x: x),\n+            [\"one\", \"four\", \"five\", \"three\"],\n+        )\n \n     def test_return_list(self):\n-        custom = ['a', 'b', 'c']\n-        self.assertEqual(build_component_list(None, custom,\n-                                              convert=lambda x: x),\n-                         custom)\n+        custom = [\"a\", \"b\", \"c\"]\n+        self.assertEqual(\n+            build_component_list(None, custom, convert=lambda x: x), custom\n+        )\n \n     def test_map_dict(self):\n-        custom = {'one': 1, 'two': 2, 'three': 3}\n-        self.assertEqual(build_component_list({}, custom,\n-                                              convert=lambda x: x.upper()),\n-                         ['ONE', 'TWO', 'THREE'])\n+        custom = {\"one\": 1, \"two\": 2, \"three\": 3}\n+        self.assertEqual(\n+            build_component_list({}, custom, convert=lambda x: x.upper()),\n+            [\"ONE\", \"TWO\", \"THREE\"],\n+        )\n \n     def test_map_list(self):\n-        custom = ['a', 'b', 'c']\n-        self.assertEqual(build_component_list(None, custom,\n-                                              lambda x: x.upper()),\n-                         ['A', 'B', 'C'])\n+        custom = [\"a\", \"b\", \"c\"]\n+        self.assertEqual(\n+            build_component_list(None, custom, lambda x: x.upper()), [\"A\", \"B\", \"C\"]\n+        )\n \n     def test_duplicate_components_in_dict(self):\n-        duplicate_dict = {'one': 1, 'two': 2, 'ONE': 4}\n-        self.assertRaises(ValueError, build_component_list, {}, duplicate_dict,\n-                          convert=lambda x: x.lower())\n+        duplicate_dict = {\"one\": 1, \"two\": 2, \"ONE\": 4}\n+        self.assertRaises(\n+            ValueError,\n+            build_component_list,\n+            {},\n+            duplicate_dict,\n+            convert=lambda x: x.lower(),\n+        )\n \n     def test_duplicate_components_in_list(self):\n-        duplicate_list = ['a', 'b', 'a']\n+        duplicate_list = [\"a\", \"b\", \"a\"]\n         with self.assertRaises(ValueError) as cm:\n             build_component_list(None, duplicate_list, convert=lambda x: x)\n         self.assertIn(str(duplicate_list), str(cm.exception))\n \n     def test_duplicate_components_in_basesettings(self):\n         # Higher priority takes precedence\n-        duplicate_bs = BaseSettings({'one': 1, 'two': 2}, priority=0)\n-        duplicate_bs.set('ONE', 4, priority=10)\n-        self.assertEqual(build_component_list(duplicate_bs,\n-                                              convert=lambda x: x.lower()),\n-                         ['two', 'one'])\n-        duplicate_bs.set('one', duplicate_bs['one'], priority=20)\n-        self.assertEqual(build_component_list(duplicate_bs,\n-                                              convert=lambda x: x.lower()),\n-                         ['one', 'two'])\n+        duplicate_bs = BaseSettings({\"one\": 1, \"two\": 2}, priority=0)\n+        duplicate_bs.set(\"ONE\", 4, priority=10)\n+        self.assertEqual(\n+            build_component_list(duplicate_bs, convert=lambda x: x.lower()),\n+            [\"two\", \"one\"],\n+        )\n+        duplicate_bs.set(\"one\", duplicate_bs[\"one\"], priority=20)\n+        self.assertEqual(\n+            build_component_list(duplicate_bs, convert=lambda x: x.lower()),\n+            [\"one\", \"two\"],\n+        )\n         # Same priority raises ValueError\n-        duplicate_bs.set('ONE', duplicate_bs['ONE'], priority=20)\n-        self.assertRaises(ValueError, build_component_list, duplicate_bs,\n-                          convert=lambda x: x.lower())\n+        duplicate_bs.set(\"ONE\", duplicate_bs[\"ONE\"], priority=20)\n+        self.assertRaises(\n+            ValueError, build_component_list, duplicate_bs, convert=lambda x: x.lower()\n+        )\n \n     def test_valid_numbers(self):\n         # work well with None and numeric values\n-        d = {'a': 10, 'b': None, 'c': 15, 'd': 5.0}\n-        self.assertEqual(build_component_list(d, convert=lambda x: x),\n-                         ['d', 'a', 'c'])\n-        d = {'a': 33333333333333333333, 'b': 11111111111111111111, 'c': 22222222222222222222}\n-        self.assertEqual(build_component_list(d, convert=lambda x: x),\n-                         ['b', 'c', 'a'])\n+        d = {\"a\": 10, \"b\": None, \"c\": 15, \"d\": 5.0}\n+        self.assertEqual(build_component_list(d, convert=lambda x: x), [\"d\", \"a\", \"c\"])\n+        d = {\n+            \"a\": 33333333333333333333,\n+            \"b\": 11111111111111111111,\n+            \"c\": 22222222222222222222,\n+        }\n+        self.assertEqual(build_component_list(d, convert=lambda x: x), [\"b\", \"c\", \"a\"])\n         # raise exception for invalid values\n-        d = {'one': '5'}\n+        d = {\"one\": \"5\"}\n         self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {'one': '1.0'}\n+        d = {\"one\": \"1.0\"}\n         self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {'one': [1, 2, 3]}\n+        d = {\"one\": [1, 2, 3]}\n         self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {'one': {'a': 'a', 'b': 2}}\n+        d = {\"one\": {\"a\": \"a\", \"b\": 2}}\n         self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {'one': 'lorem ipsum'}\n+        d = {\"one\": \"lorem ipsum\"}\n         self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n \n \n class UtilsConfTestCase(unittest.TestCase):\n-\n     def test_arglist_to_dict(self):\n         self.assertEqual(\n-            arglist_to_dict(['arg1=val1', 'arg2=val2']),\n-            {'arg1': 'val1', 'arg2': 'val2'})\n+            arglist_to_dict([\"arg1=val1\", \"arg2=val2\"]),\n+            {\"arg1\": \"val1\", \"arg2\": \"val2\"},\n+        )\n \n \n class FeedExportConfigTestCase(unittest.TestCase):\n-\n     def test_feed_export_config_invalid_format(self):\n         settings = Settings()\n-        self.assertRaises(UsageError, feed_process_params_from_cli, settings, ['items.dat'], 'noformat')\n+        self.assertRaises(\n+            UsageError,\n+            feed_process_params_from_cli,\n+            settings,\n+            [\"items.dat\"],\n+            \"noformat\",\n+        )\n \n     def test_feed_export_config_mismatch(self):\n         settings = Settings()\n         self.assertRaises(\n             UsageError,\n-            feed_process_params_from_cli, settings, ['items1.dat', 'items2.dat'], 'noformat'\n+            feed_process_params_from_cli,\n+            settings,\n+            [\"items1.dat\", \"items2.dat\"],\n+            \"noformat\",\n         )\n \n     def test_feed_export_config_backward_compatible(self):\n         with warnings.catch_warnings(record=True) as cw:\n             settings = Settings()\n             self.assertEqual(\n-                {'items.dat': {'format': 'csv'}},\n-                feed_process_params_from_cli(settings, ['items.dat'], 'csv')\n+                {\"items.dat\": {\"format\": \"csv\"}},\n+                feed_process_params_from_cli(settings, [\"items.dat\"], \"csv\"),\n             )\n             self.assertEqual(cw[0].category, ScrapyDeprecationWarning)\n \n     def test_feed_export_config_explicit_formats(self):\n         settings = Settings()\n         self.assertEqual(\n-            {'items_1.dat': {'format': 'json'}, 'items_2.dat': {'format': 'xml'}, 'items_3.dat': {'format': 'csv'}},\n-            feed_process_params_from_cli(settings, ['items_1.dat:json', 'items_2.dat:xml', 'items_3.dat:csv'])\n+            {\n+                \"items_1.dat\": {\"format\": \"json\"},\n+                \"items_2.dat\": {\"format\": \"xml\"},\n+                \"items_3.dat\": {\"format\": \"csv\"},\n+            },\n+            feed_process_params_from_cli(\n+                settings, [\"items_1.dat:json\", \"items_2.dat:xml\", \"items_3.dat:csv\"]\n+            ),\n         )\n \n     def test_feed_export_config_implicit_formats(self):\n         settings = Settings()\n         self.assertEqual(\n-            {'items_1.json': {'format': 'json'}, 'items_2.xml': {'format': 'xml'}, 'items_3.csv': {'format': 'csv'}},\n-            feed_process_params_from_cli(settings, ['items_1.json', 'items_2.xml', 'items_3.csv'])\n+            {\n+                \"items_1.json\": {\"format\": \"json\"},\n+                \"items_2.xml\": {\"format\": \"xml\"},\n+                \"items_3.csv\": {\"format\": \"csv\"},\n+            },\n+            feed_process_params_from_cli(\n+                settings, [\"items_1.json\", \"items_2.xml\", \"items_3.csv\"]\n+            ),\n         )\n \n     def test_feed_export_config_stdout(self):\n         settings = Settings()\n         self.assertEqual(\n-            {'stdout:': {'format': 'pickle'}},\n-            feed_process_params_from_cli(settings, ['-:pickle'])\n+            {\"stdout:\": {\"format\": \"pickle\"}},\n+            feed_process_params_from_cli(settings, [\"-:pickle\"]),\n         )\n \n     def test_feed_export_config_overwrite(self):\n         settings = Settings()\n         self.assertEqual(\n-            {'output.json': {'format': 'json', 'overwrite': True}},\n-            feed_process_params_from_cli(settings, [], None, ['output.json'])\n+            {\"output.json\": {\"format\": \"json\", \"overwrite\": True}},\n+            feed_process_params_from_cli(settings, [], None, [\"output.json\"]),\n         )\n \n     def test_output_and_overwrite_output(self):\n         with self.assertRaises(UsageError):\n             feed_process_params_from_cli(\n                 Settings(),\n-                ['output1.json'],\n+                [\"output1.json\"],\n                 None,\n-                ['output2.json'],\n+                [\"output2.json\"],\n             )\n \n     def test_feed_complete_default_values_from_settings_empty(self):\n         feed = {}\n-        settings = Settings({\n+        settings = Settings(\n+            {\n                 \"FEED_EXPORT_ENCODING\": \"custom encoding\",\n                 \"FEED_EXPORT_FIELDS\": [\"f1\", \"f2\", \"f3\"],\n                 \"FEED_EXPORT_INDENT\": 42,\n                 \"FEED_STORE_EMPTY\": True,\n                 \"FEED_URI_PARAMS\": (1, 2, 3, 4),\n                 \"FEED_EXPORT_BATCH_ITEM_COUNT\": 2,\n-        })\n+            }\n+        )\n         new_feed = feed_complete_default_values_from_settings(feed, settings)\n-        self.assertEqual(new_feed, {\n+        self.assertEqual(\n+            new_feed,\n+            {\n                 \"encoding\": \"custom encoding\",\n                 \"fields\": [\"f1\", \"f2\", \"f3\"],\n                 \"indent\": 42,\n@@ -177,22 +213,27 @@ class FeedExportConfigTestCase(unittest.TestCase):\n                 \"uri_params\": (1, 2, 3, 4),\n                 \"batch_item_count\": 2,\n                 \"item_export_kwargs\": {},\n-        })\n+            },\n+        )\n \n     def test_feed_complete_default_values_from_settings_non_empty(self):\n         feed = {\n             \"encoding\": \"other encoding\",\n             \"fields\": None,\n         }\n-        settings = Settings({\n+        settings = Settings(\n+            {\n                 \"FEED_EXPORT_ENCODING\": \"custom encoding\",\n                 \"FEED_EXPORT_FIELDS\": [\"f1\", \"f2\", \"f3\"],\n                 \"FEED_EXPORT_INDENT\": 42,\n                 \"FEED_STORE_EMPTY\": True,\n                 \"FEED_EXPORT_BATCH_ITEM_COUNT\": 2,\n-        })\n+            }\n+        )\n         new_feed = feed_complete_default_values_from_settings(feed, settings)\n-        self.assertEqual(new_feed, {\n+        self.assertEqual(\n+            new_feed,\n+            {\n                 \"encoding\": \"other encoding\",\n                 \"fields\": None,\n                 \"indent\": 42,\n@@ -200,7 +241,8 @@ class FeedExportConfigTestCase(unittest.TestCase):\n                 \"uri_params\": None,\n                 \"batch_item_count\": 2,\n                 \"item_export_kwargs\": {},\n-        })\n+            },\n+        )\n \n \n if __name__ == \"__main__\":\n\n@@ -1,14 +1,17 @@\n import unittest\n \n from scrapy.utils.console import get_shell_embed_func\n+\n try:\n     import bpython\n+\n     bpy = True\n     del bpython\n except ImportError:\n     bpy = False\n try:\n     import IPython\n+\n     ipy = True\n     del IPython\n except ImportError:\n@@ -16,29 +19,28 @@ except ImportError:\n \n \n class UtilsConsoleTestCase(unittest.TestCase):\n-\n     def test_get_shell_embed_func(self):\n \n-        shell = get_shell_embed_func(['invalid'])\n+        shell = get_shell_embed_func([\"invalid\"])\n         self.assertEqual(shell, None)\n \n-        shell = get_shell_embed_func(['invalid', 'python'])\n+        shell = get_shell_embed_func([\"invalid\", \"python\"])\n         self.assertTrue(callable(shell))\n-        self.assertEqual(shell.__name__, '_embed_standard_shell')\n+        self.assertEqual(shell.__name__, \"_embed_standard_shell\")\n \n-    @unittest.skipIf(not bpy, 'bpython not available in testenv')\n+    @unittest.skipIf(not bpy, \"bpython not available in testenv\")\n     def test_get_shell_embed_func2(self):\n \n-        shell = get_shell_embed_func(['bpython'])\n+        shell = get_shell_embed_func([\"bpython\"])\n         self.assertTrue(callable(shell))\n-        self.assertEqual(shell.__name__, '_embed_bpython_shell')\n+        self.assertEqual(shell.__name__, \"_embed_bpython_shell\")\n \n-    @unittest.skipIf(not ipy, 'IPython not available in testenv')\n+    @unittest.skipIf(not ipy, \"IPython not available in testenv\")\n     def test_get_shell_embed_func3(self):\n \n         # default shell should be 'ipython'\n         shell = get_shell_embed_func()\n-        self.assertEqual(shell.__name__, '_embed_ipython_shell')\n+        self.assertEqual(shell.__name__, \"_embed_ipython_shell\")\n \n \n if __name__ == \"__main__\":\n\n@@ -34,10 +34,7 @@ class CurlToRequestKwargsTest(unittest.TestCase):\n             \"method\": \"GET\",\n             \"url\": \"https://api.test.com/\",\n             \"headers\": [\n-                (\n-                    \"Authorization\",\n-                    basic_auth_header(\"some_username\", \"some_password\")\n-                )\n+                (\"Authorization\", basic_auth_header(\"some_username\", \"some_password\"))\n             ],\n         }\n         self._test_command(curl_command, expected_result)\n@@ -77,11 +74,11 @@ class CurlToRequestKwargsTest(unittest.TestCase):\n                 (\"Connection\", \"keep-alive\"),\n             ],\n             \"cookies\": {\n-                '_gauges_unique_year': '1',\n-                '_gauges_unique_hour': '1',\n-                '_gauges_unique_day': '1',\n-                '_gauges_unique': '1',\n-                '_gauges_unique_month': '1'\n+                \"_gauges_unique_year\": \"1\",\n+                \"_gauges_unique_hour\": \"1\",\n+                \"_gauges_unique_day\": \"1\",\n+                \"_gauges_unique\": \"1\",\n+                \"_gauges_unique_month\": \"1\",\n             },\n         }\n         self._test_command(curl_command, expected_result)\n@@ -110,11 +107,11 @@ class CurlToRequestKwargsTest(unittest.TestCase):\n             \"e.org&size=small&topping=cheese&topping=onion&delivery=12\"\n             \"%3A15&comments=\",\n             \"cookies\": {\n-                '_gauges_unique_year': '1',\n-                '_gauges_unique_hour': '1',\n-                '_gauges_unique_day': '1',\n-                '_gauges_unique': '1',\n-                '_gauges_unique_month': '1'\n+                \"_gauges_unique_year\": \"1\",\n+                \"_gauges_unique_hour\": \"1\",\n+                \"_gauges_unique_day\": \"1\",\n+                \"_gauges_unique\": \"1\",\n+                \"_gauges_unique_month\": \"1\",\n             },\n             \"headers\": [\n                 (\"Origin\", \"http://httpbin.org\"),\n@@ -152,16 +149,17 @@ class CurlToRequestKwargsTest(unittest.TestCase):\n             \"body\": (\n                 \"excerptLength=200&enableDidYouMean=true&sortCriteria=ffirstz3\"\n                 \"2xnamez32x201740686%20ascending&queryFunctions=%5B%5D&ranking\"\n-                \"Functions=%5B%5D\")\n+                \"Functions=%5B%5D\"\n+            ),\n         }\n         self._test_command(curl_command, expected_result)\n \n     def test_explicit_get_with_data(self):\n-        curl_command = 'curl httpbin.org/anything -X GET --data asdf'\n+        curl_command = \"curl httpbin.org/anything -X GET --data asdf\"\n         expected_result = {\n             \"method\": \"GET\",\n             \"url\": \"http://httpbin.org/anything\",\n-            \"body\": \"asdf\"\n+            \"body\": \"asdf\",\n         }\n         self._test_command(curl_command, expected_result)\n \n@@ -189,9 +187,7 @@ class CurlToRequestKwargsTest(unittest.TestCase):\n \n     def test_delete(self):\n         curl_command = 'curl -X \"DELETE\" https://www.url.com/page'\n-        expected_result = {\n-            \"method\": \"DELETE\", \"url\": \"https://www.url.com/page\"\n-        }\n+        expected_result = {\"method\": \"DELETE\", \"url\": \"https://www.url.com/page\"}\n         self._test_command(curl_command, expected_result)\n \n     def test_get_silent(self):\n@@ -209,8 +205,8 @@ class CurlToRequestKwargsTest(unittest.TestCase):\n     def test_ignore_unknown_options(self):\n         # case 1: ignore_unknown_options=True:\n         with warnings.catch_warnings():  # avoid warning when executing tests\n-            warnings.simplefilter('ignore')\n-            curl_command = 'curl --bar --baz http://www.example.com'\n+            warnings.simplefilter(\"ignore\")\n+            curl_command = \"curl --bar --baz http://www.example.com\"\n             expected_result = {\"method\": \"GET\", \"url\": \"http://www.example.com\"}\n             self.assertEqual(curl_to_request_kwargs(curl_command), expected_result)\n \n@@ -219,13 +215,12 @@ class CurlToRequestKwargsTest(unittest.TestCase):\n             ValueError,\n             \"Unrecognized options:.*--bar.*--baz\",\n             lambda: curl_to_request_kwargs(\n-                \"curl --bar --baz http://www.example.com\",\n-                ignore_unknown_options=False\n+                \"curl --bar --baz http://www.example.com\", ignore_unknown_options=False\n             ),\n         )\n \n     def test_must_start_with_curl_error(self):\n         self.assertRaises(\n             ValueError,\n-            lambda: curl_to_request_kwargs(\"carl -X POST http://example.org\")\n+            lambda: curl_to_request_kwargs(\"carl -X POST http://example.org\"),\n         )\n\n@@ -3,26 +3,30 @@ import unittest\n from collections.abc import Mapping, MutableMapping\n \n from scrapy.http import Request\n-from scrapy.utils.datatypes import CaselessDict, LocalCache, LocalWeakReferencedCache, SequenceExclude\n+from scrapy.utils.datatypes import (\n+    CaselessDict,\n+    LocalCache,\n+    LocalWeakReferencedCache,\n+    SequenceExclude,\n+)\n from scrapy.utils.python import garbage_collect\n \n \n-__doctests__ = ['scrapy.utils.datatypes']\n+__doctests__ = [\"scrapy.utils.datatypes\"]\n \n \n class CaselessDictTest(unittest.TestCase):\n-\n     def test_init_dict(self):\n-        seq = {'red': 1, 'black': 3}\n+        seq = {\"red\": 1, \"black\": 3}\n         d = CaselessDict(seq)\n-        self.assertEqual(d['red'], 1)\n-        self.assertEqual(d['black'], 3)\n+        self.assertEqual(d[\"red\"], 1)\n+        self.assertEqual(d[\"black\"], 3)\n \n     def test_init_pair_sequence(self):\n-        seq = (('red', 1), ('black', 3))\n+        seq = ((\"red\", 1), (\"black\", 3))\n         d = CaselessDict(seq)\n-        self.assertEqual(d['red'], 1)\n-        self.assertEqual(d['black'], 3)\n+        self.assertEqual(d[\"red\"], 1)\n+        self.assertEqual(d[\"black\"], 3)\n \n     def test_init_mapping(self):\n         class MyMapping(Mapping):\n@@ -40,8 +44,8 @@ class CaselessDictTest(unittest.TestCase):\n \n         seq = MyMapping(red=1, black=3)\n         d = CaselessDict(seq)\n-        self.assertEqual(d['red'], 1)\n-        self.assertEqual(d['black'], 3)\n+        self.assertEqual(d[\"red\"], 1)\n+        self.assertEqual(d[\"black\"], 3)\n \n     def test_init_mutable_mapping(self):\n         class MyMutableMapping(MutableMapping):\n@@ -65,72 +69,72 @@ class CaselessDictTest(unittest.TestCase):\n \n         seq = MyMutableMapping(red=1, black=3)\n         d = CaselessDict(seq)\n-        self.assertEqual(d['red'], 1)\n-        self.assertEqual(d['black'], 3)\n+        self.assertEqual(d[\"red\"], 1)\n+        self.assertEqual(d[\"black\"], 3)\n \n     def test_caseless(self):\n         d = CaselessDict()\n-        d['key_Lower'] = 1\n-        self.assertEqual(d['KEy_loWer'], 1)\n-        self.assertEqual(d.get('KEy_loWer'), 1)\n+        d[\"key_Lower\"] = 1\n+        self.assertEqual(d[\"KEy_loWer\"], 1)\n+        self.assertEqual(d.get(\"KEy_loWer\"), 1)\n \n-        d['KEY_LOWER'] = 3\n-        self.assertEqual(d['key_Lower'], 3)\n-        self.assertEqual(d.get('key_Lower'), 3)\n+        d[\"KEY_LOWER\"] = 3\n+        self.assertEqual(d[\"key_Lower\"], 3)\n+        self.assertEqual(d.get(\"key_Lower\"), 3)\n \n     def test_delete(self):\n-        d = CaselessDict({'key_lower': 1})\n-        del d['key_LOWER']\n-        self.assertRaises(KeyError, d.__getitem__, 'key_LOWER')\n-        self.assertRaises(KeyError, d.__getitem__, 'key_lower')\n+        d = CaselessDict({\"key_lower\": 1})\n+        del d[\"key_LOWER\"]\n+        self.assertRaises(KeyError, d.__getitem__, \"key_LOWER\")\n+        self.assertRaises(KeyError, d.__getitem__, \"key_lower\")\n \n     def test_getdefault(self):\n         d = CaselessDict()\n-        self.assertEqual(d.get('c', 5), 5)\n-        d['c'] = 10\n-        self.assertEqual(d.get('c', 5), 10)\n+        self.assertEqual(d.get(\"c\", 5), 5)\n+        d[\"c\"] = 10\n+        self.assertEqual(d.get(\"c\", 5), 10)\n \n     def test_setdefault(self):\n-        d = CaselessDict({'a': 1, 'b': 2})\n+        d = CaselessDict({\"a\": 1, \"b\": 2})\n \n-        r = d.setdefault('A', 5)\n+        r = d.setdefault(\"A\", 5)\n         self.assertEqual(r, 1)\n-        self.assertEqual(d['A'], 1)\n+        self.assertEqual(d[\"A\"], 1)\n \n-        r = d.setdefault('c', 5)\n+        r = d.setdefault(\"c\", 5)\n         self.assertEqual(r, 5)\n-        self.assertEqual(d['C'], 5)\n+        self.assertEqual(d[\"C\"], 5)\n \n     def test_fromkeys(self):\n-        keys = ('a', 'b')\n+        keys = (\"a\", \"b\")\n \n         d = CaselessDict.fromkeys(keys)\n-        self.assertEqual(d['A'], None)\n-        self.assertEqual(d['B'], None)\n+        self.assertEqual(d[\"A\"], None)\n+        self.assertEqual(d[\"B\"], None)\n \n         d = CaselessDict.fromkeys(keys, 1)\n-        self.assertEqual(d['A'], 1)\n-        self.assertEqual(d['B'], 1)\n+        self.assertEqual(d[\"A\"], 1)\n+        self.assertEqual(d[\"B\"], 1)\n \n         instance = CaselessDict()\n         d = instance.fromkeys(keys)\n-        self.assertEqual(d['A'], None)\n-        self.assertEqual(d['B'], None)\n+        self.assertEqual(d[\"A\"], None)\n+        self.assertEqual(d[\"B\"], None)\n \n         d = instance.fromkeys(keys, 1)\n-        self.assertEqual(d['A'], 1)\n-        self.assertEqual(d['B'], 1)\n+        self.assertEqual(d[\"A\"], 1)\n+        self.assertEqual(d[\"B\"], 1)\n \n     def test_contains(self):\n         d = CaselessDict()\n-        d['a'] = 1\n-        assert 'a' in d\n+        d[\"a\"] = 1\n+        assert \"a\" in d\n \n     def test_pop(self):\n         d = CaselessDict()\n-        d['a'] = 1\n-        self.assertEqual(d.pop('A'), 1)\n-        self.assertRaises(KeyError, d.pop, 'A')\n+        d[\"a\"] = 1\n+        self.assertEqual(d.pop(\"A\"), 1)\n+        self.assertRaises(KeyError, d.pop, \"A\")\n \n     def test_normkey(self):\n         class MyDict(CaselessDict):\n@@ -138,8 +142,8 @@ class CaselessDictTest(unittest.TestCase):\n                 return key.title()\n \n         d = MyDict()\n-        d['key-one'] = 2\n-        self.assertEqual(list(d.keys()), ['Key-One'])\n+        d[\"key-one\"] = 2\n+        self.assertEqual(list(d.keys()), [\"Key-One\"])\n \n     def test_normvalue(self):\n         class MyDict(CaselessDict):\n@@ -147,39 +151,38 @@ class CaselessDictTest(unittest.TestCase):\n                 if value is not None:\n                     return value + 1\n \n-        d = MyDict({'key': 1})\n-        self.assertEqual(d['key'], 2)\n-        self.assertEqual(d.get('key'), 2)\n+        d = MyDict({\"key\": 1})\n+        self.assertEqual(d[\"key\"], 2)\n+        self.assertEqual(d.get(\"key\"), 2)\n \n         d = MyDict()\n-        d['key'] = 1\n-        self.assertEqual(d['key'], 2)\n-        self.assertEqual(d.get('key'), 2)\n+        d[\"key\"] = 1\n+        self.assertEqual(d[\"key\"], 2)\n+        self.assertEqual(d.get(\"key\"), 2)\n \n         d = MyDict()\n-        d.setdefault('key', 1)\n-        self.assertEqual(d['key'], 2)\n-        self.assertEqual(d.get('key'), 2)\n+        d.setdefault(\"key\", 1)\n+        self.assertEqual(d[\"key\"], 2)\n+        self.assertEqual(d.get(\"key\"), 2)\n \n         d = MyDict()\n-        d.update({'key': 1})\n-        self.assertEqual(d['key'], 2)\n-        self.assertEqual(d.get('key'), 2)\n+        d.update({\"key\": 1})\n+        self.assertEqual(d[\"key\"], 2)\n+        self.assertEqual(d.get(\"key\"), 2)\n \n-        d = MyDict.fromkeys(('key',), 1)\n-        self.assertEqual(d['key'], 2)\n-        self.assertEqual(d.get('key'), 2)\n+        d = MyDict.fromkeys((\"key\",), 1)\n+        self.assertEqual(d[\"key\"], 2)\n+        self.assertEqual(d.get(\"key\"), 2)\n \n     def test_copy(self):\n-        h1 = CaselessDict({'header1': 'value'})\n+        h1 = CaselessDict({\"header1\": \"value\"})\n         h2 = copy.copy(h1)\n         self.assertEqual(h1, h2)\n-        self.assertEqual(h1.get('header1'), h2.get('header1'))\n+        self.assertEqual(h1.get(\"header1\"), h2.get(\"header1\"))\n         assert isinstance(h2, CaselessDict)\n \n \n class SequenceExcludeTest(unittest.TestCase):\n-\n     def test_list(self):\n         seq = [1, 2, 3]\n         d = SequenceExclude(seq)\n@@ -226,25 +229,24 @@ class SequenceExcludeTest(unittest.TestCase):\n \n         # supplied sequence is a set, so checking for list (non)inclusion fails\n         self.assertRaises(TypeError, (0, 1, 2) in d)\n-        self.assertRaises(TypeError, d.__contains__, ['a', 'b', 'c'])\n+        self.assertRaises(TypeError, d.__contains__, [\"a\", \"b\", \"c\"])\n \n         for v in [-3, \"test\", 1.1]:\n             self.assertNotIn(v, d)\n \n \n class LocalCacheTest(unittest.TestCase):\n-\n     def test_cache_with_limit(self):\n         cache = LocalCache(limit=2)\n-        cache['a'] = 1\n-        cache['b'] = 2\n-        cache['c'] = 3\n+        cache[\"a\"] = 1\n+        cache[\"b\"] = 2\n+        cache[\"c\"] = 3\n         self.assertEqual(len(cache), 2)\n-        self.assertNotIn('a', cache)\n-        self.assertIn('b', cache)\n-        self.assertIn('c', cache)\n-        self.assertEqual(cache['b'], 2)\n-        self.assertEqual(cache['c'], 3)\n+        self.assertNotIn(\"a\", cache)\n+        self.assertIn(\"b\", cache)\n+        self.assertIn(\"c\", cache)\n+        self.assertEqual(cache[\"b\"], 2)\n+        self.assertEqual(cache[\"c\"], 3)\n \n     def test_cache_without_limit(self):\n         maximum = 10**4\n@@ -258,12 +260,11 @@ class LocalCacheTest(unittest.TestCase):\n \n \n class LocalWeakReferencedCacheTest(unittest.TestCase):\n-\n     def test_cache_with_limit(self):\n         cache = LocalWeakReferencedCache(limit=2)\n-        r1 = Request('https://example.org')\n-        r2 = Request('https://example.com')\n-        r3 = Request('https://example.net')\n+        r1 = Request(\"https://example.org\")\n+        r2 = Request(\"https://example.com\")\n+        r3 = Request(\"https://example.net\")\n         cache[r1] = 1\n         cache[r2] = 2\n         cache[r3] = 3\n@@ -299,7 +300,7 @@ class LocalWeakReferencedCacheTest(unittest.TestCase):\n         cache = LocalWeakReferencedCache()\n         refs = []\n         for x in range(max):\n-            refs.append(Request(f'https://example.org/{x}'))\n+            refs.append(Request(f\"https://example.org/{x}\"))\n             cache[refs[-1]] = x\n         self.assertEqual(len(cache), max)\n         for i, r in enumerate(refs):\n\n@@ -68,41 +68,43 @@ def eb1(failure, arg1, arg2):\n \n \n class DeferUtilsTest(unittest.TestCase):\n-\n     @defer.inlineCallbacks\n     def test_process_chain(self):\n-        x = yield process_chain([cb1, cb2, cb3], 'res', 'v1', 'v2')\n+        x = yield process_chain([cb1, cb2, cb3], \"res\", \"v1\", \"v2\")\n         self.assertEqual(x, \"(cb3 (cb2 (cb1 res v1 v2) v1 v2) v1 v2)\")\n \n         gotexc = False\n         try:\n-            yield process_chain([cb1, cb_fail, cb3], 'res', 'v1', 'v2')\n+            yield process_chain([cb1, cb_fail, cb3], \"res\", \"v1\", \"v2\")\n         except TypeError:\n             gotexc = True\n         self.assertTrue(gotexc)\n \n     @defer.inlineCallbacks\n     def test_process_chain_both(self):\n-        x = yield process_chain_both([cb_fail, cb2, cb3], [None, eb1, None], 'res', 'v1', 'v2')\n+        x = yield process_chain_both(\n+            [cb_fail, cb2, cb3], [None, eb1, None], \"res\", \"v1\", \"v2\"\n+        )\n         self.assertEqual(x, \"(cb3 (eb1 TypeError v1 v2) v1 v2)\")\n \n         fail = Failure(ZeroDivisionError())\n-        x = yield process_chain_both([eb1, cb2, cb3], [eb1, None, None], fail, 'v1', 'v2')\n+        x = yield process_chain_both(\n+            [eb1, cb2, cb3], [eb1, None, None], fail, \"v1\", \"v2\"\n+        )\n         self.assertEqual(x, \"(cb3 (cb2 (eb1 ZeroDivisionError v1 v2) v1 v2) v1 v2)\")\n \n     @defer.inlineCallbacks\n     def test_process_parallel(self):\n-        x = yield process_parallel([cb1, cb2, cb3], 'res', 'v1', 'v2')\n-        self.assertEqual(x, ['(cb1 res v1 v2)', '(cb2 res v1 v2)', '(cb3 res v1 v2)'])\n+        x = yield process_parallel([cb1, cb2, cb3], \"res\", \"v1\", \"v2\")\n+        self.assertEqual(x, [\"(cb1 res v1 v2)\", \"(cb2 res v1 v2)\", \"(cb3 res v1 v2)\"])\n \n     def test_process_parallel_failure(self):\n-        d = process_parallel([cb1, cb_fail, cb3], 'res', 'v1', 'v2')\n+        d = process_parallel([cb1, cb_fail, cb3], \"res\", \"v1\", \"v2\")\n         self.failUnlessFailure(d, TypeError)\n         return d\n \n \n class IterErrbackTest(unittest.TestCase):\n-\n     def test_iter_errback_good(self):\n         def itergood():\n             for x in range(10):\n@@ -128,7 +130,6 @@ class IterErrbackTest(unittest.TestCase):\n \n \n class AiterErrbackTest(unittest.TestCase):\n-\n     @deferred_f_from_coro_f\n     async def test_aiter_errback_good(self):\n         async def itergood():\n@@ -182,6 +183,7 @@ class AsyncCooperatorTest(unittest.TestCase):\n     We also want to simulate the real usage, with arbitrary delays between getting the values\n     from the iterable. We also want to simulate sync and async results from the callable.\n     \"\"\"\n+\n     CONCURRENT_ITEMS = 50\n \n     @staticmethod\n\n@@ -20,22 +20,23 @@ class NewName(SomeBaseClass):\n \n \n class WarnWhenSubclassedTest(unittest.TestCase):\n-\n     def _mywarnings(self, w, category=MyWarning):\n         return [x for x in w if x.category is MyWarning]\n \n     def test_no_warning_on_definition(self):\n         with warnings.catch_warnings(record=True) as w:\n-            create_deprecated_class('Deprecated', NewName)\n+            create_deprecated_class(\"Deprecated\", NewName)\n \n         w = self._mywarnings(w)\n         self.assertEqual(w, [])\n \n     def test_subclassing_warning_message(self):\n-        Deprecated = create_deprecated_class('Deprecated', NewName,\n-                                             warn_category=MyWarning)\n+        Deprecated = create_deprecated_class(\n+            \"Deprecated\", NewName, warn_category=MyWarning\n+        )\n \n         with warnings.catch_warnings(record=True) as w:\n+\n             class UserClass(Deprecated):\n                 pass\n \n@@ -46,17 +47,21 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n             \"tests.test_utils_deprecate.UserClass inherits from \"\n             \"deprecated class tests.test_utils_deprecate.Deprecated, \"\n             \"please inherit from tests.test_utils_deprecate.NewName.\"\n-            \" (warning only on first subclass, there may be others)\"\n+            \" (warning only on first subclass, there may be others)\",\n         )\n         self.assertEqual(w[0].lineno, inspect.getsourcelines(UserClass)[1])\n \n     def test_custom_class_paths(self):\n-        Deprecated = create_deprecated_class('Deprecated', NewName,\n-                                             new_class_path='foo.NewClass',\n-                                             old_class_path='bar.OldClass',\n-                                             warn_category=MyWarning)\n+        Deprecated = create_deprecated_class(\n+            \"Deprecated\",\n+            NewName,\n+            new_class_path=\"foo.NewClass\",\n+            old_class_path=\"bar.OldClass\",\n+            warn_category=MyWarning,\n+        )\n \n         with warnings.catch_warnings(record=True) as w:\n+\n             class UserClass(Deprecated):\n                 pass\n \n@@ -64,17 +69,18 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n \n         w = self._mywarnings(w)\n         self.assertEqual(len(w), 2)\n-        self.assertIn('foo.NewClass', str(w[0].message))\n-        self.assertIn('bar.OldClass', str(w[0].message))\n-        self.assertIn('foo.NewClass', str(w[1].message))\n-        self.assertIn('bar.OldClass', str(w[1].message))\n+        self.assertIn(\"foo.NewClass\", str(w[0].message))\n+        self.assertIn(\"bar.OldClass\", str(w[0].message))\n+        self.assertIn(\"foo.NewClass\", str(w[1].message))\n+        self.assertIn(\"bar.OldClass\", str(w[1].message))\n \n     def test_subclassing_warns_only_on_direct_childs(self):\n-        Deprecated = create_deprecated_class('Deprecated', NewName,\n-                                             warn_once=False,\n-                                             warn_category=MyWarning)\n+        Deprecated = create_deprecated_class(\n+            \"Deprecated\", NewName, warn_once=False, warn_category=MyWarning\n+        )\n \n         with warnings.catch_warnings(record=True) as w:\n+\n             class UserClass(Deprecated):\n                 pass\n \n@@ -83,13 +89,15 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n \n         w = self._mywarnings(w)\n         self.assertEqual(len(w), 1)\n-        self.assertIn('UserClass', str(w[0].message))\n+        self.assertIn(\"UserClass\", str(w[0].message))\n \n     def test_subclassing_warns_once_by_default(self):\n-        Deprecated = create_deprecated_class('Deprecated', NewName,\n-                                             warn_category=MyWarning)\n+        Deprecated = create_deprecated_class(\n+            \"Deprecated\", NewName, warn_category=MyWarning\n+        )\n \n         with warnings.catch_warnings(record=True) as w:\n+\n             class UserClass(Deprecated):\n                 pass\n \n@@ -101,15 +109,16 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n \n         w = self._mywarnings(w)\n         self.assertEqual(len(w), 1)\n-        self.assertIn('UserClass', str(w[0].message))\n+        self.assertIn(\"UserClass\", str(w[0].message))\n \n     def test_warning_on_instance(self):\n-        Deprecated = create_deprecated_class('Deprecated', NewName,\n-                                             warn_category=MyWarning)\n+        Deprecated = create_deprecated_class(\n+            \"Deprecated\", NewName, warn_category=MyWarning\n+        )\n \n         # ignore subclassing warnings\n         with warnings.catch_warnings():\n-            warnings.simplefilter('ignore', MyWarning)\n+            warnings.simplefilter(\"ignore\", MyWarning)\n \n             class UserClass(Deprecated):\n                 pass\n@@ -123,13 +132,13 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         self.assertEqual(\n             str(w[0].message),\n             \"tests.test_utils_deprecate.Deprecated is deprecated, \"\n-            \"instantiate tests.test_utils_deprecate.NewName instead.\"\n+            \"instantiate tests.test_utils_deprecate.NewName instead.\",\n         )\n         self.assertEqual(w[0].lineno, lineno)\n \n     def test_warning_auto_message(self):\n         with warnings.catch_warnings(record=True) as w:\n-            Deprecated = create_deprecated_class('Deprecated', NewName)\n+            Deprecated = create_deprecated_class(\"Deprecated\", NewName)\n \n             class UserClass2(Deprecated):\n                 pass\n@@ -140,8 +149,8 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n \n     def test_issubclass(self):\n         with warnings.catch_warnings():\n-            warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n-            DeprecatedName = create_deprecated_class('DeprecatedName', NewName)\n+            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n+            DeprecatedName = create_deprecated_class(\"DeprecatedName\", NewName)\n \n             class UpdatedUserClass1(NewName):\n                 pass\n@@ -176,8 +185,8 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n \n     def test_isinstance(self):\n         with warnings.catch_warnings():\n-            warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n-            DeprecatedName = create_deprecated_class('DeprecatedName', NewName)\n+            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n+            DeprecatedName = create_deprecated_class(\"DeprecatedName\", NewName)\n \n             class UpdatedUserClass2(NewName):\n                 pass\n@@ -210,24 +219,28 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n \n     def test_clsdict(self):\n         with warnings.catch_warnings():\n-            warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n-            Deprecated = create_deprecated_class('Deprecated', NewName, {'foo': 'bar'})\n+            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n+            Deprecated = create_deprecated_class(\"Deprecated\", NewName, {\"foo\": \"bar\"})\n \n-        self.assertEqual(Deprecated.foo, 'bar')\n+        self.assertEqual(Deprecated.foo, \"bar\")\n \n     def test_deprecate_a_class_with_custom_metaclass(self):\n-        Meta1 = type('Meta1', (type,), {})\n-        New = Meta1('New', (), {})\n-        create_deprecated_class('Deprecated', New)\n+        Meta1 = type(\"Meta1\", (type,), {})\n+        New = Meta1(\"New\", (), {})\n+        create_deprecated_class(\"Deprecated\", New)\n \n     def test_deprecate_subclass_of_deprecated_class(self):\n         with warnings.catch_warnings(record=True) as w:\n-            warnings.simplefilter('always')\n-            Deprecated = create_deprecated_class('Deprecated', NewName,\n-                                                 warn_category=MyWarning)\n-            AlsoDeprecated = create_deprecated_class('AlsoDeprecated', Deprecated,\n-                                                     new_class_path='foo.Bar',\n-                                                     warn_category=MyWarning)\n+            warnings.simplefilter(\"always\")\n+            Deprecated = create_deprecated_class(\n+                \"Deprecated\", NewName, warn_category=MyWarning\n+            )\n+            AlsoDeprecated = create_deprecated_class(\n+                \"AlsoDeprecated\",\n+                Deprecated,\n+                new_class_path=\"foo.Bar\",\n+                warn_category=MyWarning,\n+            )\n \n         w = self._mywarnings(w)\n         self.assertEqual(len(w), 0, str(map(str, w)))\n@@ -240,15 +253,15 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n \n         w = self._mywarnings(w)\n         self.assertEqual(len(w), 2)\n-        self.assertIn('AlsoDeprecated', str(w[0].message))\n-        self.assertIn('foo.Bar', str(w[0].message))\n-        self.assertIn('AlsoDeprecated', str(w[1].message))\n-        self.assertIn('foo.Bar', str(w[1].message))\n+        self.assertIn(\"AlsoDeprecated\", str(w[0].message))\n+        self.assertIn(\"foo.Bar\", str(w[0].message))\n+        self.assertIn(\"AlsoDeprecated\", str(w[1].message))\n+        self.assertIn(\"foo.Bar\", str(w[1].message))\n \n     def test_inspect_stack(self):\n-        with mock.patch('inspect.stack', side_effect=IndexError):\n+        with mock.patch(\"inspect.stack\", side_effect=IndexError):\n             with warnings.catch_warnings(record=True) as w:\n-                DeprecatedName = create_deprecated_class('DeprecatedName', NewName)\n+                DeprecatedName = create_deprecated_class(\"DeprecatedName\", NewName)\n \n                 class SubClass(DeprecatedName):\n                     pass\n@@ -256,27 +269,30 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         self.assertIn(\"Error detecting parent module\", str(w[0].message))\n \n \n-@mock.patch('scrapy.utils.deprecate.DEPRECATION_RULES',\n-            [('scrapy.contrib.pipeline.', 'scrapy.pipelines.'),\n-             ('scrapy.contrib.', 'scrapy.extensions.')])\n+@mock.patch(\n+    \"scrapy.utils.deprecate.DEPRECATION_RULES\",\n+    [\n+        (\"scrapy.contrib.pipeline.\", \"scrapy.pipelines.\"),\n+        (\"scrapy.contrib.\", \"scrapy.extensions.\"),\n+    ],\n+)\n class UpdateClassPathTest(unittest.TestCase):\n-\n     def test_old_path_gets_fixed(self):\n         with warnings.catch_warnings(record=True) as w:\n-            output = update_classpath('scrapy.contrib.debug.Debug')\n-        self.assertEqual(output, 'scrapy.extensions.debug.Debug')\n+            output = update_classpath(\"scrapy.contrib.debug.Debug\")\n+        self.assertEqual(output, \"scrapy.extensions.debug.Debug\")\n         self.assertEqual(len(w), 1)\n         self.assertIn(\"scrapy.contrib.debug.Debug\", str(w[0].message))\n         self.assertIn(\"scrapy.extensions.debug.Debug\", str(w[0].message))\n \n     def test_sorted_replacement(self):\n         with warnings.catch_warnings():\n-            warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n-            output = update_classpath('scrapy.contrib.pipeline.Pipeline')\n-        self.assertEqual(output, 'scrapy.pipelines.Pipeline')\n+            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n+            output = update_classpath(\"scrapy.contrib.pipeline.Pipeline\")\n+        self.assertEqual(output, \"scrapy.pipelines.Pipeline\")\n \n     def test_unmatched_path_stays_the_same(self):\n         with warnings.catch_warnings(record=True) as w:\n-            output = update_classpath('scrapy.unmatched.Path')\n-        self.assertEqual(output, 'scrapy.unmatched.Path')\n+            output = update_classpath(\"scrapy.unmatched.Path\")\n+        self.assertEqual(output, \"scrapy.unmatched.Path\")\n         self.assertEqual(len(w), 0)\n\n@@ -6,14 +6,14 @@ from scrapy.utils.display import pformat, pprint\n \n \n class TestDisplay(TestCase):\n-    object = {'a': 1}\n+    object = {\"a\": 1}\n     colorized_string = (\n         \"{\\x1b[33m'\\x1b[39;49;00m\\x1b[33ma\\x1b[39;49;00m\\x1b[33m'\"\n         \"\\x1b[39;49;00m: \\x1b[34m1\\x1b[39;49;00m}\\n\"\n     )\n     plain_string = \"{'a': 1}\"\n \n-    @mock.patch('sys.platform', 'linux')\n+    @mock.patch(\"sys.platform\", \"linux\")\n     @mock.patch(\"sys.stdout.isatty\")\n     def test_pformat(self, isatty):\n         isatty.return_value = True\n@@ -27,44 +27,47 @@ class TestDisplay(TestCase):\n     def test_pformat_not_tty(self):\n         self.assertEqual(pformat(self.object), self.plain_string)\n \n-    @mock.patch('sys.platform', 'win32')\n-    @mock.patch('platform.version')\n+    @mock.patch(\"sys.platform\", \"win32\")\n+    @mock.patch(\"platform.version\")\n     @mock.patch(\"sys.stdout.isatty\")\n     def test_pformat_old_windows(self, isatty, version):\n         isatty.return_value = True\n-        version.return_value = '10.0.14392'\n+        version.return_value = \"10.0.14392\"\n         self.assertEqual(pformat(self.object), self.colorized_string)\n \n-    @mock.patch('sys.platform', 'win32')\n-    @mock.patch('scrapy.utils.display._enable_windows_terminal_processing')\n-    @mock.patch('platform.version')\n+    @mock.patch(\"sys.platform\", \"win32\")\n+    @mock.patch(\"scrapy.utils.display._enable_windows_terminal_processing\")\n+    @mock.patch(\"platform.version\")\n     @mock.patch(\"sys.stdout.isatty\")\n-    def test_pformat_windows_no_terminal_processing(self, isatty, version, terminal_processing):\n+    def test_pformat_windows_no_terminal_processing(\n+        self, isatty, version, terminal_processing\n+    ):\n         isatty.return_value = True\n-        version.return_value = '10.0.14393'\n+        version.return_value = \"10.0.14393\"\n         terminal_processing.return_value = False\n         self.assertEqual(pformat(self.object), self.plain_string)\n \n-    @mock.patch('sys.platform', 'win32')\n-    @mock.patch('scrapy.utils.display._enable_windows_terminal_processing')\n-    @mock.patch('platform.version')\n+    @mock.patch(\"sys.platform\", \"win32\")\n+    @mock.patch(\"scrapy.utils.display._enable_windows_terminal_processing\")\n+    @mock.patch(\"platform.version\")\n     @mock.patch(\"sys.stdout.isatty\")\n     def test_pformat_windows(self, isatty, version, terminal_processing):\n         isatty.return_value = True\n-        version.return_value = '10.0.14393'\n+        version.return_value = \"10.0.14393\"\n         terminal_processing.return_value = True\n         self.assertEqual(pformat(self.object), self.colorized_string)\n \n-    @mock.patch('sys.platform', 'linux')\n+    @mock.patch(\"sys.platform\", \"linux\")\n     @mock.patch(\"sys.stdout.isatty\")\n     def test_pformat_no_pygments(self, isatty):\n         isatty.return_value = True\n \n         import builtins\n+\n         real_import = builtins.__import__\n \n         def mock_import(name, globals, locals, fromlist, level):\n-            if 'pygments' in name:\n+            if \"pygments\" in name:\n                 raise ImportError\n             return real_import(name, globals, locals, fromlist, level)\n \n@@ -73,6 +76,6 @@ class TestDisplay(TestCase):\n         builtins.__import__ = real_import\n \n     def test_pprint(self):\n-        with mock.patch('sys.stdout', new=StringIO()) as mock_out:\n+        with mock.patch(\"sys.stdout\", new=StringIO()) as mock_out:\n             pprint(self.object)\n             self.assertEqual(mock_out.getvalue(), \"{'a': 1}\\n\")\n\n@@ -8,13 +8,15 @@ from scrapy.http import Response\n from tests import tests_datadir\n \n \n-SAMPLEDIR = Path(tests_datadir, 'compressed')\n+SAMPLEDIR = Path(tests_datadir, \"compressed\")\n \n \n class GunzipTest(unittest.TestCase):\n-\n     def test_gunzip_basic(self):\n-        r1 = Response(\"http://www.example.com\", body=(SAMPLEDIR / 'feed-sample1.xml.gz').read_bytes())\n+        r1 = Response(\n+            \"http://www.example.com\",\n+            body=(SAMPLEDIR / \"feed-sample1.xml.gz\").read_bytes(),\n+        )\n         self.assertTrue(gzip_magic_number(r1))\n \n         r2 = Response(\"http://www.example.com\", body=gunzip(r1.body))\n@@ -22,18 +24,23 @@ class GunzipTest(unittest.TestCase):\n         self.assertEqual(len(r2.body), 9950)\n \n     def test_gunzip_truncated(self):\n-        text = gunzip((SAMPLEDIR / 'truncated-crc-error.gz').read_bytes())\n-        assert text.endswith(b'</html')\n+        text = gunzip((SAMPLEDIR / \"truncated-crc-error.gz\").read_bytes())\n+        assert text.endswith(b\"</html\")\n \n     def test_gunzip_no_gzip_file_raises(self):\n-        self.assertRaises(IOError, gunzip, (SAMPLEDIR / 'feed-sample1.xml').read_bytes())\n+        self.assertRaises(\n+            IOError, gunzip, (SAMPLEDIR / \"feed-sample1.xml\").read_bytes()\n+        )\n \n     def test_gunzip_truncated_short(self):\n-        r1 = Response(\"http://www.example.com\", body=(SAMPLEDIR / 'truncated-crc-error-short.gz').read_bytes())\n+        r1 = Response(\n+            \"http://www.example.com\",\n+            body=(SAMPLEDIR / \"truncated-crc-error-short.gz\").read_bytes(),\n+        )\n         self.assertTrue(gzip_magic_number(r1))\n \n         r2 = Response(\"http://www.example.com\", body=gunzip(r1.body))\n-        assert r2.body.endswith(b'</html>')\n+        assert r2.body.endswith(b\"</html>\")\n         self.assertFalse(gzip_magic_number(r2))\n \n     def test_is_gzipped_empty(self):\n@@ -41,7 +48,11 @@ class GunzipTest(unittest.TestCase):\n         self.assertFalse(gzip_magic_number(r1))\n \n     def test_gunzip_illegal_eof(self):\n-        text = html_to_unicode('charset=cp1252', gunzip((SAMPLEDIR / 'unexpected-eof.gz').read_bytes()))[1]\n-        expected_text = (SAMPLEDIR / 'unexpected-eof-output.txt').read_text(encoding=\"utf-8\")\n+        text = html_to_unicode(\n+            \"charset=cp1252\", gunzip((SAMPLEDIR / \"unexpected-eof.gz\").read_bytes())\n+        )[1]\n+        expected_text = (SAMPLEDIR / \"unexpected-eof-output.txt\").read_text(\n+            encoding=\"utf-8\"\n+        )\n         self.assertEqual(len(text), len(expected_text))\n         self.assertEqual(text, expected_text)\n\n@@ -6,7 +6,6 @@ from scrapy.utils.httpobj import urlparse_cached\n \n \n class HttpobjUtilsTest(unittest.TestCase):\n-\n     def test_urlparse_cached(self):\n         url = \"http://www.example.com/index.html\"\n         request1 = Request(url)\n\n@@ -28,14 +28,18 @@ class XmliterTestCase(unittest.TestCase):\n \n         response = XmlResponse(url=\"http://example.com\", body=body)\n         attrs = []\n-        for x in self.xmliter(response, 'product'):\n-            attrs.append((\n-                x.attrib['id'],\n+        for x in self.xmliter(response, \"product\"):\n+            attrs.append(\n+                (\n+                    x.attrib[\"id\"],\n                     x.xpath(\"name/text()\").getall(),\n-                x.xpath(\"./type/text()\").getall()))\n+                    x.xpath(\"./type/text()\").getall(),\n+                )\n+            )\n \n-        self.assertEqual(attrs,\n-                         [('001', ['Name 1'], ['Type 1']), ('002', ['Name 2'], ['Type 2'])])\n+        self.assertEqual(\n+            attrs, [(\"001\", [\"Name 1\"], [\"Type 1\"]), (\"002\", [\"Name 2\"], [\"Type 2\"])]\n+        )\n \n     def test_xmliter_unusual_node(self):\n         body = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n@@ -45,8 +49,10 @@ class XmliterTestCase(unittest.TestCase):\n             </root>\n         \"\"\"\n         response = XmlResponse(url=\"http://example.com\", body=body)\n-        nodenames = [e.xpath('name()').getall() for e in self.xmliter(response, 'matchme...')]\n-        self.assertEqual(nodenames, [['matchme...']])\n+        nodenames = [\n+            e.xpath(\"name()\").getall() for e in self.xmliter(response, \"matchme...\")\n+        ]\n+        self.assertEqual(nodenames, [[\"matchme...\"]])\n \n     def test_xmliter_unicode(self):\n         # example taken from https://github.com/scrapy/scrapy/issues/1665\n@@ -88,29 +94,35 @@ class XmliterTestCase(unittest.TestCase):\n \n         for r in (\n             # with bytes\n-            XmlResponse(url=\"http://example.com\", body=body.encode('utf-8')),\n+            XmlResponse(url=\"http://example.com\", body=body.encode(\"utf-8\")),\n             # Unicode body needs encoding information\n-            XmlResponse(url=\"http://example.com\", body=body, encoding='utf-8'),\n+            XmlResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\"),\n         ):\n             attrs = []\n-            for x in self.xmliter(r, 'þingflokkur'):\n-                attrs.append((x.attrib['id'],\n-                              x.xpath('./skammstafanir/stuttskammstöfun/text()').getall(),\n-                              x.xpath('./tímabil/fyrstaþing/text()').getall()))\n+            for x in self.xmliter(r, \"þingflokkur\"):\n+                attrs.append(\n+                    (\n+                        x.attrib[\"id\"],\n+                        x.xpath(\"./skammstafanir/stuttskammstöfun/text()\").getall(),\n+                        x.xpath(\"./tímabil/fyrstaþing/text()\").getall(),\n+                    )\n+                )\n \n-            self.assertEqual(attrs,\n-                             [('26', ['-'], ['80']),\n-                              ('21', ['Ab'], ['76']),\n-                              ('27', ['A'], ['27'])])\n+            self.assertEqual(\n+                attrs,\n+                [(\"26\", [\"-\"], [\"80\"]), (\"21\", [\"Ab\"], [\"76\"]), (\"27\", [\"A\"], [\"27\"])],\n+            )\n \n     def test_xmliter_text(self):\n         body = (\n             '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n-            '<products><product>one</product><product>two</product></products>'\n+            \"<products><product>one</product><product>two</product></products>\"\n         )\n \n-        self.assertEqual([x.xpath(\"text()\").getall() for x in self.xmliter(body, 'product')],\n-                         [['one'], ['two']])\n+        self.assertEqual(\n+            [x.xpath(\"text()\").getall() for x in self.xmliter(body, \"product\")],\n+            [[\"one\"], [\"two\"]],\n+        )\n \n     def test_xmliter_namespaces(self):\n         body = b\"\"\"\n@@ -131,22 +143,25 @@ class XmliterTestCase(unittest.TestCase):\n                 </channel>\n             </rss>\n         \"\"\"\n-        response = XmlResponse(url='http://mydummycompany.com', body=body)\n-        my_iter = self.xmliter(response, 'item')\n+        response = XmlResponse(url=\"http://mydummycompany.com\", body=body)\n+        my_iter = self.xmliter(response, \"item\")\n         node = next(my_iter)\n-        node.register_namespace('g', 'http://base.google.com/ns/1.0')\n-        self.assertEqual(node.xpath('title/text()').getall(), ['Item 1'])\n-        self.assertEqual(node.xpath('description/text()').getall(), ['This is item 1'])\n-        self.assertEqual(node.xpath('link/text()').getall(), ['http://www.mydummycompany.com/items/1'])\n+        node.register_namespace(\"g\", \"http://base.google.com/ns/1.0\")\n+        self.assertEqual(node.xpath(\"title/text()\").getall(), [\"Item 1\"])\n+        self.assertEqual(node.xpath(\"description/text()\").getall(), [\"This is item 1\"])\n         self.assertEqual(\n-            node.xpath('g:image_link/text()').getall(),\n-            ['http://www.mydummycompany.com/images/item1.jpg']\n+            node.xpath(\"link/text()\").getall(),\n+            [\"http://www.mydummycompany.com/items/1\"],\n         )\n-        self.assertEqual(node.xpath('g:id/text()').getall(), ['ITEM_1'])\n-        self.assertEqual(node.xpath('g:price/text()').getall(), ['400'])\n-        self.assertEqual(node.xpath('image_link/text()').getall(), [])\n-        self.assertEqual(node.xpath('id/text()').getall(), [])\n-        self.assertEqual(node.xpath('price/text()').getall(), [])\n+        self.assertEqual(\n+            node.xpath(\"g:image_link/text()\").getall(),\n+            [\"http://www.mydummycompany.com/images/item1.jpg\"],\n+        )\n+        self.assertEqual(node.xpath(\"g:id/text()\").getall(), [\"ITEM_1\"])\n+        self.assertEqual(node.xpath(\"g:price/text()\").getall(), [\"400\"])\n+        self.assertEqual(node.xpath(\"image_link/text()\").getall(), [])\n+        self.assertEqual(node.xpath(\"id/text()\").getall(), [])\n+        self.assertEqual(node.xpath(\"price/text()\").getall(), [])\n \n     def test_xmliter_namespaced_nodename(self):\n         body = b\"\"\"\n@@ -167,11 +182,14 @@ class XmliterTestCase(unittest.TestCase):\n                 </channel>\n             </rss>\n         \"\"\"\n-        response = XmlResponse(url='http://mydummycompany.com', body=body)\n-        my_iter = self.xmliter(response, 'g:image_link')\n+        response = XmlResponse(url=\"http://mydummycompany.com\", body=body)\n+        my_iter = self.xmliter(response, \"g:image_link\")\n         node = next(my_iter)\n-        node.register_namespace('g', 'http://base.google.com/ns/1.0')\n-        self.assertEqual(node.xpath('text()').extract(), ['http://www.mydummycompany.com/images/item1.jpg'])\n+        node.register_namespace(\"g\", \"http://base.google.com/ns/1.0\")\n+        self.assertEqual(\n+            node.xpath(\"text()\").extract(),\n+            [\"http://www.mydummycompany.com/images/item1.jpg\"],\n+        )\n \n     def test_xmliter_namespaced_nodename_missing(self):\n         body = b\"\"\"\n@@ -192,45 +210,45 @@ class XmliterTestCase(unittest.TestCase):\n                 </channel>\n             </rss>\n         \"\"\"\n-        response = XmlResponse(url='http://mydummycompany.com', body=body)\n-        my_iter = self.xmliter(response, 'g:link_image')\n+        response = XmlResponse(url=\"http://mydummycompany.com\", body=body)\n+        my_iter = self.xmliter(response, \"g:link_image\")\n         with self.assertRaises(StopIteration):\n             next(my_iter)\n \n     def test_xmliter_exception(self):\n         body = (\n             '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n-            '<products><product>one</product><product>two</product></products>'\n+            \"<products><product>one</product><product>two</product></products>\"\n         )\n \n-        iter = self.xmliter(body, 'product')\n+        iter = self.xmliter(body, \"product\")\n         next(iter)\n         next(iter)\n \n         self.assertRaises(StopIteration, next, iter)\n \n     def test_xmliter_objtype_exception(self):\n-        i = self.xmliter(42, 'product')\n+        i = self.xmliter(42, \"product\")\n         self.assertRaises(TypeError, next, i)\n \n     def test_xmliter_encoding(self):\n         body = (\n             b'<?xml version=\"1.0\" encoding=\"ISO-8859-9\"?>\\n'\n-            b'<xml>\\n'\n-            b'    <item>Some Turkish Characters \\xd6\\xc7\\xde\\xdd\\xd0\\xdc \\xfc\\xf0\\xfd\\xfe\\xe7\\xf6</item>\\n'\n-            b'</xml>\\n\\n'\n+            b\"<xml>\\n\"\n+            b\"    <item>Some Turkish Characters \\xd6\\xc7\\xde\\xdd\\xd0\\xdc \\xfc\\xf0\\xfd\\xfe\\xe7\\xf6</item>\\n\"\n+            b\"</xml>\\n\\n\"\n         )\n-        response = XmlResponse('http://www.example.com', body=body)\n+        response = XmlResponse(\"http://www.example.com\", body=body)\n         self.assertEqual(\n-            next(self.xmliter(response, 'item')).get(),\n-            '<item>Some Turkish Characters \\xd6\\xc7\\u015e\\u0130\\u011e\\xdc \\xfc\\u011f\\u0131\\u015f\\xe7\\xf6</item>'\n+            next(self.xmliter(response, \"item\")).get(),\n+            \"<item>Some Turkish Characters \\xd6\\xc7\\u015e\\u0130\\u011e\\xdc \\xfc\\u011f\\u0131\\u015f\\xe7\\xf6</item>\",\n         )\n \n \n class LxmlXmliterTestCase(XmliterTestCase):\n     xmliter = staticmethod(xmliter_lxml)\n \n-    @mark.xfail(reason='known bug of the current implementation')\n+    @mark.xfail(reason=\"known bug of the current implementation\")\n     def test_xmliter_namespaced_nodename(self):\n         super().test_xmliter_namespaced_nodename()\n \n@@ -252,16 +270,24 @@ class LxmlXmliterTestCase(XmliterTestCase):\n                 </channel>\n             </rss>\n         \"\"\"\n-        response = XmlResponse(url='http://mydummycompany.com', body=body)\n+        response = XmlResponse(url=\"http://mydummycompany.com\", body=body)\n \n-        no_namespace_iter = self.xmliter(response, 'image_link')\n+        no_namespace_iter = self.xmliter(response, \"image_link\")\n         self.assertEqual(len(list(no_namespace_iter)), 0)\n \n-        namespace_iter = self.xmliter(response, 'image_link', 'http://base.google.com/ns/1.0')\n+        namespace_iter = self.xmliter(\n+            response, \"image_link\", \"http://base.google.com/ns/1.0\"\n+        )\n         node = next(namespace_iter)\n-        self.assertEqual(node.xpath('text()').getall(), ['http://www.mydummycompany.com/images/item1.jpg'])\n+        self.assertEqual(\n+            node.xpath(\"text()\").getall(),\n+            [\"http://www.mydummycompany.com/images/item1.jpg\"],\n+        )\n         node = next(namespace_iter)\n-        self.assertEqual(node.xpath('text()').getall(), ['http://www.mydummycompany.com/images/item2.jpg'])\n+        self.assertEqual(\n+            node.xpath(\"text()\").getall(),\n+            [\"http://www.mydummycompany.com/images/item2.jpg\"],\n+        )\n \n     def test_xmliter_namespaces_prefix(self):\n         body = b\"\"\"\n@@ -282,36 +308,42 @@ class LxmlXmliterTestCase(XmliterTestCase):\n \n         </root>\n         \"\"\"\n-        response = XmlResponse(url='http://mydummycompany.com', body=body)\n-        my_iter = self.xmliter(response, 'table', 'http://www.w3.org/TR/html4/', 'h')\n+        response = XmlResponse(url=\"http://mydummycompany.com\", body=body)\n+        my_iter = self.xmliter(response, \"table\", \"http://www.w3.org/TR/html4/\", \"h\")\n \n         node = next(my_iter)\n-        self.assertEqual(len(node.xpath('h:tr/h:td').getall()), 2)\n-        self.assertEqual(node.xpath('h:tr/h:td[1]/text()').getall(), ['Apples'])\n-        self.assertEqual(node.xpath('h:tr/h:td[2]/text()').getall(), ['Bananas'])\n+        self.assertEqual(len(node.xpath(\"h:tr/h:td\").getall()), 2)\n+        self.assertEqual(node.xpath(\"h:tr/h:td[1]/text()\").getall(), [\"Apples\"])\n+        self.assertEqual(node.xpath(\"h:tr/h:td[2]/text()\").getall(), [\"Bananas\"])\n \n-        my_iter = self.xmliter(response, 'table', 'http://www.w3schools.com/furniture', 'f')\n+        my_iter = self.xmliter(\n+            response, \"table\", \"http://www.w3schools.com/furniture\", \"f\"\n+        )\n \n         node = next(my_iter)\n-        self.assertEqual(node.xpath('f:name/text()').getall(), ['African Coffee Table'])\n+        self.assertEqual(node.xpath(\"f:name/text()\").getall(), [\"African Coffee Table\"])\n \n     def test_xmliter_objtype_exception(self):\n-        i = self.xmliter(42, 'product')\n+        i = self.xmliter(42, \"product\")\n         self.assertRaises(TypeError, next, i)\n \n \n class UtilsCsvTestCase(unittest.TestCase):\n     def test_csviter_defaults(self):\n-        body = get_testdata('feeds', 'feed-sample3.csv')\n+        body = get_testdata(\"feeds\", \"feed-sample3.csv\")\n         response = TextResponse(url=\"http://example.com/\", body=body)\n         csv = csviter(response)\n \n         result = [row for row in csv]\n-        self.assertEqual(result,\n-                         [{'id': '1', 'name': 'alpha', 'value': 'foobar'},\n-                          {'id': '2', 'name': 'unicode', 'value': '\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {'id': '3', 'name': 'multi', 'value': \"foo\\nbar\"},\n-                          {'id': '4', 'name': 'empty', 'value': ''}])\n+        self.assertEqual(\n+            result,\n+            [\n+                {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n+                {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n+                {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n+                {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n+            ],\n+        )\n \n         # explicit type check cuz' we no like stinkin' autocasting! yarrr\n         for result_row in result:\n@@ -319,88 +351,120 @@ class UtilsCsvTestCase(unittest.TestCase):\n             self.assertTrue(all((isinstance(v, str) for v in result_row.values())))\n \n     def test_csviter_delimiter(self):\n-        body = get_testdata('feeds', 'feed-sample3.csv').replace(b',', b'\\t')\n+        body = get_testdata(\"feeds\", \"feed-sample3.csv\").replace(b\",\", b\"\\t\")\n         response = TextResponse(url=\"http://example.com/\", body=body)\n-        csv = csviter(response, delimiter='\\t')\n+        csv = csviter(response, delimiter=\"\\t\")\n \n-        self.assertEqual([row for row in csv],\n-                         [{'id': '1', 'name': 'alpha', 'value': 'foobar'},\n-                          {'id': '2', 'name': 'unicode', 'value': '\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {'id': '3', 'name': 'multi', 'value': \"foo\\nbar\"},\n-                          {'id': '4', 'name': 'empty', 'value': ''}])\n+        self.assertEqual(\n+            [row for row in csv],\n+            [\n+                {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n+                {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n+                {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n+                {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n+            ],\n+        )\n \n     def test_csviter_quotechar(self):\n-        body1 = get_testdata('feeds', 'feed-sample6.csv')\n-        body2 = get_testdata('feeds', 'feed-sample6.csv').replace(b',', b'|')\n+        body1 = get_testdata(\"feeds\", \"feed-sample6.csv\")\n+        body2 = get_testdata(\"feeds\", \"feed-sample6.csv\").replace(b\",\", b\"|\")\n \n         response1 = TextResponse(url=\"http://example.com/\", body=body1)\n         csv1 = csviter(response1, quotechar=\"'\")\n \n-        self.assertEqual([row for row in csv1],\n-                         [{'id': '1', 'name': 'alpha', 'value': 'foobar'},\n-                          {'id': '2', 'name': 'unicode', 'value': '\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {'id': '3', 'name': 'multi', 'value': \"foo\\nbar\"},\n-                          {'id': '4', 'name': 'empty', 'value': ''}])\n+        self.assertEqual(\n+            [row for row in csv1],\n+            [\n+                {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n+                {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n+                {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n+                {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n+            ],\n+        )\n \n         response2 = TextResponse(url=\"http://example.com/\", body=body2)\n         csv2 = csviter(response2, delimiter=\"|\", quotechar=\"'\")\n \n-        self.assertEqual([row for row in csv2],\n-                         [{'id': '1', 'name': 'alpha', 'value': 'foobar'},\n-                          {'id': '2', 'name': 'unicode', 'value': '\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {'id': '3', 'name': 'multi', 'value': \"foo\\nbar\"},\n-                          {'id': '4', 'name': 'empty', 'value': ''}])\n+        self.assertEqual(\n+            [row for row in csv2],\n+            [\n+                {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n+                {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n+                {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n+                {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n+            ],\n+        )\n \n     def test_csviter_wrong_quotechar(self):\n-        body = get_testdata('feeds', 'feed-sample6.csv')\n+        body = get_testdata(\"feeds\", \"feed-sample6.csv\")\n         response = TextResponse(url=\"http://example.com/\", body=body)\n         csv = csviter(response)\n \n-        self.assertEqual([row for row in csv],\n-                         [{\"'id'\": \"1\", \"'name'\": \"'alpha'\", \"'value'\": \"'foobar'\"},\n-                          {\"'id'\": \"2\", \"'name'\": \"'unicode'\", \"'value'\": \"'\\xfan\\xedc\\xf3d\\xe9\\u203d'\"},\n+        self.assertEqual(\n+            [row for row in csv],\n+            [\n+                {\"'id'\": \"1\", \"'name'\": \"'alpha'\", \"'value'\": \"'foobar'\"},\n+                {\n+                    \"'id'\": \"2\",\n+                    \"'name'\": \"'unicode'\",\n+                    \"'value'\": \"'\\xfan\\xedc\\xf3d\\xe9\\u203d'\",\n+                },\n                 {\"'id'\": \"'3'\", \"'name'\": \"'multi'\", \"'value'\": \"'foo\"},\n-                          {\"'id'\": \"4\", \"'name'\": \"'empty'\", \"'value'\": \"\"}])\n+                {\"'id'\": \"4\", \"'name'\": \"'empty'\", \"'value'\": \"\"},\n+            ],\n+        )\n \n     def test_csviter_delimiter_binary_response_assume_utf8_encoding(self):\n-        body = get_testdata('feeds', 'feed-sample3.csv').replace(b',', b'\\t')\n+        body = get_testdata(\"feeds\", \"feed-sample3.csv\").replace(b\",\", b\"\\t\")\n         response = Response(url=\"http://example.com/\", body=body)\n-        csv = csviter(response, delimiter='\\t')\n+        csv = csviter(response, delimiter=\"\\t\")\n \n-        self.assertEqual([row for row in csv],\n-                         [{'id': '1', 'name': 'alpha', 'value': 'foobar'},\n-                          {'id': '2', 'name': 'unicode', 'value': '\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {'id': '3', 'name': 'multi', 'value': \"foo\\nbar\"},\n-                          {'id': '4', 'name': 'empty', 'value': ''}])\n+        self.assertEqual(\n+            [row for row in csv],\n+            [\n+                {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n+                {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n+                {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n+                {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n+            ],\n+        )\n \n     def test_csviter_headers(self):\n-        sample = get_testdata('feeds', 'feed-sample3.csv').splitlines()\n-        headers, body = sample[0].split(b','), b'\\n'.join(sample[1:])\n+        sample = get_testdata(\"feeds\", \"feed-sample3.csv\").splitlines()\n+        headers, body = sample[0].split(b\",\"), b\"\\n\".join(sample[1:])\n \n         response = TextResponse(url=\"http://example.com/\", body=body)\n-        csv = csviter(response, headers=[h.decode('utf-8') for h in headers])\n+        csv = csviter(response, headers=[h.decode(\"utf-8\") for h in headers])\n \n-        self.assertEqual([row for row in csv],\n-                         [{'id': '1', 'name': 'alpha', 'value': 'foobar'},\n-                          {'id': '2', 'name': 'unicode', 'value': '\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {'id': '3', 'name': 'multi', 'value': 'foo\\nbar'},\n-                          {'id': '4', 'name': 'empty', 'value': ''}])\n+        self.assertEqual(\n+            [row for row in csv],\n+            [\n+                {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n+                {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n+                {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n+                {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n+            ],\n+        )\n \n     def test_csviter_falserow(self):\n-        body = get_testdata('feeds', 'feed-sample3.csv')\n-        body = b'\\n'.join((body, b'a,b', b'a,b,c,d'))\n+        body = get_testdata(\"feeds\", \"feed-sample3.csv\")\n+        body = b\"\\n\".join((body, b\"a,b\", b\"a,b,c,d\"))\n \n         response = TextResponse(url=\"http://example.com/\", body=body)\n         csv = csviter(response)\n \n-        self.assertEqual([row for row in csv],\n-                         [{'id': '1', 'name': 'alpha', 'value': 'foobar'},\n-                          {'id': '2', 'name': 'unicode', 'value': '\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {'id': '3', 'name': 'multi', 'value': \"foo\\nbar\"},\n-                          {'id': '4', 'name': 'empty', 'value': ''}])\n+        self.assertEqual(\n+            [row for row in csv],\n+            [\n+                {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n+                {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n+                {\"id\": \"3\", \"name\": \"multi\", \"value\": \"foo\\nbar\"},\n+                {\"id\": \"4\", \"name\": \"empty\", \"value\": \"\"},\n+            ],\n+        )\n \n     def test_csviter_exception(self):\n-        body = get_testdata('feeds', 'feed-sample3.csv')\n+        body = get_testdata(\"feeds\", \"feed-sample3.csv\")\n \n         response = TextResponse(url=\"http://example.com/\", body=body)\n         iter = csviter(response)\n@@ -412,35 +476,41 @@ class UtilsCsvTestCase(unittest.TestCase):\n         self.assertRaises(StopIteration, next, iter)\n \n     def test_csviter_encoding(self):\n-        body1 = get_testdata('feeds', 'feed-sample4.csv')\n-        body2 = get_testdata('feeds', 'feed-sample5.csv')\n+        body1 = get_testdata(\"feeds\", \"feed-sample4.csv\")\n+        body2 = get_testdata(\"feeds\", \"feed-sample5.csv\")\n \n-        response = TextResponse(url=\"http://example.com/\", body=body1, encoding='latin1')\n+        response = TextResponse(\n+            url=\"http://example.com/\", body=body1, encoding=\"latin1\"\n+        )\n         csv = csviter(response)\n         self.assertEqual(\n             list(csv),\n             [\n-                {'id': '1', 'name': 'latin1', 'value': 'test'},\n-                {'id': '2', 'name': 'something', 'value': '\\xf1\\xe1\\xe9\\xf3'},\n-            ]\n+                {\"id\": \"1\", \"name\": \"latin1\", \"value\": \"test\"},\n+                {\"id\": \"2\", \"name\": \"something\", \"value\": \"\\xf1\\xe1\\xe9\\xf3\"},\n+            ],\n         )\n \n-        response = TextResponse(url=\"http://example.com/\", body=body2, encoding='cp852')\n+        response = TextResponse(url=\"http://example.com/\", body=body2, encoding=\"cp852\")\n         csv = csviter(response)\n         self.assertEqual(\n             list(csv),\n             [\n-                {'id': '1', 'name': 'cp852', 'value': 'test'},\n-                {'id': '2', 'name': 'something', 'value': '\\u255a\\u2569\\u2569\\u2569\\u2550\\u2550\\u2557'},\n-            ]\n+                {\"id\": \"1\", \"name\": \"cp852\", \"value\": \"test\"},\n+                {\n+                    \"id\": \"2\",\n+                    \"name\": \"something\",\n+                    \"value\": \"\\u255a\\u2569\\u2569\\u2569\\u2550\\u2550\\u2557\",\n+                },\n+            ],\n         )\n \n \n class TestHelper(unittest.TestCase):\n-    bbody = b'utf8-body'\n-    ubody = bbody.decode('utf8')\n-    txtresponse = TextResponse(url='http://example.org/', body=bbody, encoding='utf-8')\n-    response = Response(url='http://example.org/', body=bbody)\n+    bbody = b\"utf8-body\"\n+    ubody = bbody.decode(\"utf8\")\n+    txtresponse = TextResponse(url=\"http://example.org/\", body=bbody, encoding=\"utf-8\")\n+    response = Response(url=\"http://example.org/\", body=bbody)\n \n     def test_body_or_str(self):\n         for obj in (self.bbody, self.ubody, self.txtresponse, self.response):\n@@ -454,8 +524,9 @@ class TestHelper(unittest.TestCase):\n             self.assertTrue(type(r1) is not type(r3))\n \n     def _assert_type_and_value(self, a, b, obj):\n-        self.assertTrue(type(a) is type(b),\n-                        f'Got {type(a)}, expected {type(b)} for { obj!r}')\n+        self.assertTrue(\n+            type(a) is type(b), f\"Got {type(a)}, expected {type(b)} for { obj!r}\"\n+        )\n         self.assertEqual(a, b)\n \n \n\n@@ -5,14 +5,17 @@ import unittest\n from testfixtures import LogCapture\n from twisted.python.failure import Failure\n \n-from scrapy.utils.log import (failure_to_exc_info, TopLevelFormatter,\n-                              LogCounterHandler, StreamLogger)\n+from scrapy.utils.log import (\n+    failure_to_exc_info,\n+    TopLevelFormatter,\n+    LogCounterHandler,\n+    StreamLogger,\n+)\n from scrapy.utils.test import get_crawler\n from scrapy.extensions import telnet\n \n \n class FailureToExcInfoTest(unittest.TestCase):\n-\n     def test_failure(self):\n         try:\n             0 / 0\n@@ -23,48 +26,46 @@ class FailureToExcInfoTest(unittest.TestCase):\n         self.assertTupleEqual(exc_info, failure_to_exc_info(failure))\n \n     def test_non_failure(self):\n-        self.assertIsNone(failure_to_exc_info('test'))\n+        self.assertIsNone(failure_to_exc_info(\"test\"))\n \n \n class TopLevelFormatterTest(unittest.TestCase):\n-\n     def setUp(self):\n         self.handler = LogCapture()\n-        self.handler.addFilter(TopLevelFormatter(['test']))\n+        self.handler.addFilter(TopLevelFormatter([\"test\"]))\n \n     def test_top_level_logger(self):\n-        logger = logging.getLogger('test')\n+        logger = logging.getLogger(\"test\")\n         with self.handler as log:\n-            logger.warning('test log msg')\n-        log.check(('test', 'WARNING', 'test log msg'))\n+            logger.warning(\"test log msg\")\n+        log.check((\"test\", \"WARNING\", \"test log msg\"))\n \n     def test_children_logger(self):\n-        logger = logging.getLogger('test.test1')\n+        logger = logging.getLogger(\"test.test1\")\n         with self.handler as log:\n-            logger.warning('test log msg')\n-        log.check(('test', 'WARNING', 'test log msg'))\n+            logger.warning(\"test log msg\")\n+        log.check((\"test\", \"WARNING\", \"test log msg\"))\n \n     def test_overlapping_name_logger(self):\n-        logger = logging.getLogger('test2')\n+        logger = logging.getLogger(\"test2\")\n         with self.handler as log:\n-            logger.warning('test log msg')\n-        log.check(('test2', 'WARNING', 'test log msg'))\n+            logger.warning(\"test log msg\")\n+        log.check((\"test2\", \"WARNING\", \"test log msg\"))\n \n     def test_different_name_logger(self):\n-        logger = logging.getLogger('different')\n+        logger = logging.getLogger(\"different\")\n         with self.handler as log:\n-            logger.warning('test log msg')\n-        log.check(('different', 'WARNING', 'test log msg'))\n+            logger.warning(\"test log msg\")\n+        log.check((\"different\", \"WARNING\", \"test log msg\"))\n \n \n class LogCounterHandlerTest(unittest.TestCase):\n-\n     def setUp(self):\n-        settings = {'LOG_LEVEL': 'WARNING'}\n+        settings = {\"LOG_LEVEL\": \"WARNING\"}\n         if not telnet.TWISTED_CONCH_AVAILABLE:\n             # disable it to avoid the extra warning\n-            settings['TELNETCONSOLE_ENABLED'] = False\n-        self.logger = logging.getLogger('test')\n+            settings[\"TELNETCONSOLE_ENABLED\"] = False\n+        self.logger = logging.getLogger(\"test\")\n         self.logger.setLevel(logging.NOTSET)\n         self.logger.propagate = False\n         self.crawler = get_crawler(settings_dict=settings)\n@@ -76,26 +77,25 @@ class LogCounterHandlerTest(unittest.TestCase):\n         self.logger.removeHandler(self.handler)\n \n     def test_init(self):\n-        self.assertIsNone(self.crawler.stats.get_value('log_count/DEBUG'))\n-        self.assertIsNone(self.crawler.stats.get_value('log_count/INFO'))\n-        self.assertIsNone(self.crawler.stats.get_value('log_count/WARNING'))\n-        self.assertIsNone(self.crawler.stats.get_value('log_count/ERROR'))\n-        self.assertIsNone(self.crawler.stats.get_value('log_count/CRITICAL'))\n+        self.assertIsNone(self.crawler.stats.get_value(\"log_count/DEBUG\"))\n+        self.assertIsNone(self.crawler.stats.get_value(\"log_count/INFO\"))\n+        self.assertIsNone(self.crawler.stats.get_value(\"log_count/WARNING\"))\n+        self.assertIsNone(self.crawler.stats.get_value(\"log_count/ERROR\"))\n+        self.assertIsNone(self.crawler.stats.get_value(\"log_count/CRITICAL\"))\n \n     def test_accepted_level(self):\n-        self.logger.error('test log msg')\n-        self.assertEqual(self.crawler.stats.get_value('log_count/ERROR'), 1)\n+        self.logger.error(\"test log msg\")\n+        self.assertEqual(self.crawler.stats.get_value(\"log_count/ERROR\"), 1)\n \n     def test_filtered_out_level(self):\n-        self.logger.debug('test log msg')\n-        self.assertIsNone(self.crawler.stats.get_value('log_count/INFO'))\n+        self.logger.debug(\"test log msg\")\n+        self.assertIsNone(self.crawler.stats.get_value(\"log_count/INFO\"))\n \n \n class StreamLoggerTest(unittest.TestCase):\n-\n     def setUp(self):\n         self.stdout = sys.stdout\n-        logger = logging.getLogger('test')\n+        logger = logging.getLogger(\"test\")\n         logger.setLevel(logging.WARNING)\n         sys.stdout = StreamLogger(logger, logging.ERROR)\n \n@@ -104,5 +104,5 @@ class StreamLoggerTest(unittest.TestCase):\n \n     def test_redirect(self):\n         with LogCapture() as log:\n-            print('test log msg')\n-        log.check(('test', 'ERROR', 'test log msg'))\n+            print(\"test log msg\")\n+        log.check((\"test\", \"ERROR\", \"test log msg\"))\n\n@@ -5,118 +5,122 @@ from pathlib import Path\n from unittest import mock\n \n from scrapy.item import Item, Field\n-from scrapy.utils.misc import arg_to_iter, create_instance, load_object, rel_has_nofollow, set_environ, walk_modules\n+from scrapy.utils.misc import (\n+    arg_to_iter,\n+    create_instance,\n+    load_object,\n+    rel_has_nofollow,\n+    set_environ,\n+    walk_modules,\n+)\n \n \n-__doctests__ = ['scrapy.utils.misc']\n+__doctests__ = [\"scrapy.utils.misc\"]\n \n \n class UtilsMiscTestCase(unittest.TestCase):\n-\n     def test_load_object_class(self):\n         obj = load_object(Field)\n         self.assertIs(obj, Field)\n-        obj = load_object('scrapy.item.Field')\n+        obj = load_object(\"scrapy.item.Field\")\n         self.assertIs(obj, Field)\n \n     def test_load_object_function(self):\n         obj = load_object(load_object)\n         self.assertIs(obj, load_object)\n-        obj = load_object('scrapy.utils.misc.load_object')\n+        obj = load_object(\"scrapy.utils.misc.load_object\")\n         self.assertIs(obj, load_object)\n \n     def test_load_object_exceptions(self):\n-        self.assertRaises(ImportError, load_object, 'nomodule999.mod.function')\n-        self.assertRaises(NameError, load_object, 'scrapy.utils.misc.load_object999')\n+        self.assertRaises(ImportError, load_object, \"nomodule999.mod.function\")\n+        self.assertRaises(NameError, load_object, \"scrapy.utils.misc.load_object999\")\n         self.assertRaises(TypeError, load_object, {})\n \n     def test_walk_modules(self):\n-        mods = walk_modules('tests.test_utils_misc.test_walk_modules')\n+        mods = walk_modules(\"tests.test_utils_misc.test_walk_modules\")\n         expected = [\n-            'tests.test_utils_misc.test_walk_modules',\n-            'tests.test_utils_misc.test_walk_modules.mod',\n-            'tests.test_utils_misc.test_walk_modules.mod.mod0',\n-            'tests.test_utils_misc.test_walk_modules.mod1',\n+            \"tests.test_utils_misc.test_walk_modules\",\n+            \"tests.test_utils_misc.test_walk_modules.mod\",\n+            \"tests.test_utils_misc.test_walk_modules.mod.mod0\",\n+            \"tests.test_utils_misc.test_walk_modules.mod1\",\n         ]\n         self.assertEqual({m.__name__ for m in mods}, set(expected))\n \n-        mods = walk_modules('tests.test_utils_misc.test_walk_modules.mod')\n+        mods = walk_modules(\"tests.test_utils_misc.test_walk_modules.mod\")\n         expected = [\n-            'tests.test_utils_misc.test_walk_modules.mod',\n-            'tests.test_utils_misc.test_walk_modules.mod.mod0',\n+            \"tests.test_utils_misc.test_walk_modules.mod\",\n+            \"tests.test_utils_misc.test_walk_modules.mod.mod0\",\n         ]\n         self.assertEqual({m.__name__ for m in mods}, set(expected))\n \n-        mods = walk_modules('tests.test_utils_misc.test_walk_modules.mod1')\n+        mods = walk_modules(\"tests.test_utils_misc.test_walk_modules.mod1\")\n         expected = [\n-            'tests.test_utils_misc.test_walk_modules.mod1',\n+            \"tests.test_utils_misc.test_walk_modules.mod1\",\n         ]\n         self.assertEqual({m.__name__ for m in mods}, set(expected))\n \n-        self.assertRaises(ImportError, walk_modules, 'nomodule999')\n+        self.assertRaises(ImportError, walk_modules, \"nomodule999\")\n \n     def test_walk_modules_egg(self):\n-        egg = str(Path(__file__).parent / 'test.egg')\n+        egg = str(Path(__file__).parent / \"test.egg\")\n         sys.path.append(egg)\n         try:\n-            mods = walk_modules('testegg')\n+            mods = walk_modules(\"testegg\")\n             expected = [\n-                'testegg.spiders',\n-                'testegg.spiders.a',\n-                'testegg.spiders.b',\n-                'testegg'\n+                \"testegg.spiders\",\n+                \"testegg.spiders.a\",\n+                \"testegg.spiders.b\",\n+                \"testegg\",\n             ]\n             self.assertEqual({m.__name__ for m in mods}, set(expected))\n         finally:\n             sys.path.remove(egg)\n \n     def test_arg_to_iter(self):\n-\n         class TestItem(Item):\n             name = Field()\n \n-        assert hasattr(arg_to_iter(None), '__iter__')\n-        assert hasattr(arg_to_iter(100), '__iter__')\n-        assert hasattr(arg_to_iter('lala'), '__iter__')\n-        assert hasattr(arg_to_iter([1, 2, 3]), '__iter__')\n-        assert hasattr(arg_to_iter(c for c in 'abcd'), '__iter__')\n+        assert hasattr(arg_to_iter(None), \"__iter__\")\n+        assert hasattr(arg_to_iter(100), \"__iter__\")\n+        assert hasattr(arg_to_iter(\"lala\"), \"__iter__\")\n+        assert hasattr(arg_to_iter([1, 2, 3]), \"__iter__\")\n+        assert hasattr(arg_to_iter(c for c in \"abcd\"), \"__iter__\")\n \n         self.assertEqual(list(arg_to_iter(None)), [])\n-        self.assertEqual(list(arg_to_iter('lala')), ['lala'])\n+        self.assertEqual(list(arg_to_iter(\"lala\")), [\"lala\"])\n         self.assertEqual(list(arg_to_iter(100)), [100])\n-        self.assertEqual(list(arg_to_iter(c for c in 'abc')), ['a', 'b', 'c'])\n+        self.assertEqual(list(arg_to_iter(c for c in \"abc\")), [\"a\", \"b\", \"c\"])\n         self.assertEqual(list(arg_to_iter([1, 2, 3])), [1, 2, 3])\n-        self.assertEqual(list(arg_to_iter({'a': 1})), [{'a': 1}])\n-        self.assertEqual(list(arg_to_iter(TestItem(name=\"john\"))), [TestItem(name=\"john\")])\n+        self.assertEqual(list(arg_to_iter({\"a\": 1})), [{\"a\": 1}])\n+        self.assertEqual(\n+            list(arg_to_iter(TestItem(name=\"john\"))), [TestItem(name=\"john\")]\n+        )\n \n     def test_create_instance(self):\n         settings = mock.MagicMock()\n-        crawler = mock.MagicMock(spec_set=['settings'])\n-        args = (True, 100.)\n-        kwargs = {'key': 'val'}\n+        crawler = mock.MagicMock(spec_set=[\"settings\"])\n+        args = (True, 100.0)\n+        kwargs = {\"key\": \"val\"}\n \n         def _test_with_settings(mock, settings):\n             create_instance(mock, settings, None, *args, **kwargs)\n-            if hasattr(mock, 'from_crawler'):\n+            if hasattr(mock, \"from_crawler\"):\n                 self.assertEqual(mock.from_crawler.call_count, 0)\n-            if hasattr(mock, 'from_settings'):\n-                mock.from_settings.assert_called_once_with(settings, *args,\n-                                                           **kwargs)\n+            if hasattr(mock, \"from_settings\"):\n+                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)\n                 self.assertEqual(mock.call_count, 0)\n             else:\n                 mock.assert_called_once_with(*args, **kwargs)\n \n         def _test_with_crawler(mock, settings, crawler):\n             create_instance(mock, settings, crawler, *args, **kwargs)\n-            if hasattr(mock, 'from_crawler'):\n-                mock.from_crawler.assert_called_once_with(crawler, *args,\n-                                                          **kwargs)\n-                if hasattr(mock, 'from_settings'):\n+            if hasattr(mock, \"from_crawler\"):\n+                mock.from_crawler.assert_called_once_with(crawler, *args, **kwargs)\n+                if hasattr(mock, \"from_settings\"):\n                     self.assertEqual(mock.from_settings.call_count, 0)\n                 self.assertEqual(mock.call_count, 0)\n-            elif hasattr(mock, 'from_settings'):\n-                mock.from_settings.assert_called_once_with(settings, *args,\n-                                                           **kwargs)\n+            elif hasattr(mock, \"from_settings\"):\n+                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)\n                 self.assertEqual(mock.call_count, 0)\n             else:\n                 mock.assert_called_once_with(*args, **kwargs)\n@@ -127,10 +131,10 @@ class UtilsMiscTestCase(unittest.TestCase):\n         #   3. with from_crawler() constructor\n         #   4. with from_settings() and from_crawler() constructor\n         spec_sets = (\n-            ['__qualname__'],\n-            ['__qualname__', 'from_settings'],\n-            ['__qualname__', 'from_crawler'],\n-            ['__qualname__', 'from_settings', 'from_crawler'],\n+            [\"__qualname__\"],\n+            [\"__qualname__\", \"from_settings\"],\n+            [\"__qualname__\", \"from_crawler\"],\n+            [\"__qualname__\", \"from_settings\", \"from_crawler\"],\n         )\n         for specs in spec_sets:\n             m = mock.MagicMock(spec_set=specs)\n@@ -139,10 +143,9 @@ class UtilsMiscTestCase(unittest.TestCase):\n             _test_with_crawler(m, settings, crawler)\n \n         # Check adoption of crawler settings\n-        m = mock.MagicMock(spec_set=['__qualname__', 'from_settings'])\n+        m = mock.MagicMock(spec_set=[\"__qualname__\", \"from_settings\"])\n         create_instance(m, None, crawler, *args, **kwargs)\n-        m.from_settings.assert_called_once_with(crawler.settings, *args,\n-                                                **kwargs)\n+        m.from_settings.assert_called_once_with(crawler.settings, *args, **kwargs)\n \n         with self.assertRaises(ValueError):\n             create_instance(m, None, None)\n@@ -152,25 +155,25 @@ class UtilsMiscTestCase(unittest.TestCase):\n             create_instance(m, settings, None)\n \n     def test_set_environ(self):\n-        assert os.environ.get('some_test_environ') is None\n-        with set_environ(some_test_environ='test_value'):\n-            assert os.environ.get('some_test_environ') == 'test_value'\n-        assert os.environ.get('some_test_environ') is None\n+        assert os.environ.get(\"some_test_environ\") is None\n+        with set_environ(some_test_environ=\"test_value\"):\n+            assert os.environ.get(\"some_test_environ\") == \"test_value\"\n+        assert os.environ.get(\"some_test_environ\") is None\n \n-        os.environ['some_test_environ'] = 'test'\n-        assert os.environ.get('some_test_environ') == 'test'\n-        with set_environ(some_test_environ='test_value'):\n-            assert os.environ.get('some_test_environ') == 'test_value'\n-        assert os.environ.get('some_test_environ') == 'test'\n+        os.environ[\"some_test_environ\"] = \"test\"\n+        assert os.environ.get(\"some_test_environ\") == \"test\"\n+        with set_environ(some_test_environ=\"test_value\"):\n+            assert os.environ.get(\"some_test_environ\") == \"test_value\"\n+        assert os.environ.get(\"some_test_environ\") == \"test\"\n \n     def test_rel_has_nofollow(self):\n-        assert rel_has_nofollow('ugc nofollow') is True\n-        assert rel_has_nofollow('ugc,nofollow') is True\n-        assert rel_has_nofollow('ugc') is False\n-        assert rel_has_nofollow('nofollow') is True\n-        assert rel_has_nofollow('nofollowfoo') is False\n-        assert rel_has_nofollow('foonofollow') is False\n-        assert rel_has_nofollow('ugc,  ,  nofollow') is True\n+        assert rel_has_nofollow(\"ugc nofollow\") is True\n+        assert rel_has_nofollow(\"ugc,nofollow\") is True\n+        assert rel_has_nofollow(\"ugc\") is False\n+        assert rel_has_nofollow(\"nofollow\") is True\n+        assert rel_has_nofollow(\"nofollowfoo\") is False\n+        assert rel_has_nofollow(\"foonofollow\") is False\n+        assert rel_has_nofollow(\"ugc,  ,  nofollow\") is True\n \n \n if __name__ == \"__main__\":\n\n@@ -3,7 +3,10 @@ import warnings\n from functools import partial\n from unittest import mock\n \n-from scrapy.utils.misc import is_generator_with_return_value, warn_on_generator_with_return_value\n+from scrapy.utils.misc import (\n+    is_generator_with_return_value,\n+    warn_on_generator_with_return_value,\n+)\n \n \n def _indentation_error(*args, **kwargs):\n@@ -39,7 +42,6 @@ def generator_that_returns_stuff():\n \n \n class UtilsMiscPy3TestCase(unittest.TestCase):\n-\n     def test_generators_return_something(self):\n         def f1():\n             yield 1\n@@ -77,7 +79,10 @@ https://example.org\n         with warnings.catch_warnings(record=True) as w:\n             warn_on_generator_with_return_value(None, top_level_return_something)\n             self.assertEqual(len(w), 1)\n-            self.assertIn('The \"NoneType.top_level_return_something\" method is a generator', str(w[0].message))\n+            self.assertIn(\n+                'The \"NoneType.top_level_return_something\" method is a generator',\n+                str(w[0].message),\n+            )\n         with warnings.catch_warnings(record=True) as w:\n             warn_on_generator_with_return_value(None, f1)\n             self.assertEqual(len(w), 1)\n@@ -170,6 +175,7 @@ https://example.org\n         def decorator(func):\n             def inner_func():\n                 func()\n+\n             return inner_func\n \n         @decorator\n@@ -249,12 +255,14 @@ https://example.org\n             warn_on_generator_with_return_value(None, l3)\n             self.assertEqual(len(w), 0)\n \n-    @mock.patch(\"scrapy.utils.misc.is_generator_with_return_value\", new=_indentation_error)\n+    @mock.patch(\n+        \"scrapy.utils.misc.is_generator_with_return_value\", new=_indentation_error\n+    )\n     def test_indentation_error(self):\n         with warnings.catch_warnings(record=True) as w:\n             warn_on_generator_with_return_value(None, top_level_return_none)\n             self.assertEqual(len(w), 1)\n-            self.assertIn('Unable to determine', str(w[0].message))\n+            self.assertIn(\"Unable to determine\", str(w[0].message))\n \n     def test_partial(self):\n         def cb(arg1, arg2):\n\n@@ -16,7 +16,7 @@ def inside_a_project():\n \n     try:\n         os.chdir(project_dir)\n-        Path('scrapy.cfg').touch()\n+        Path(\"scrapy.cfg\").touch()\n \n         yield project_dir\n     finally:\n@@ -26,21 +26,15 @@ def inside_a_project():\n \n class ProjectUtilsTest(unittest.TestCase):\n     def test_data_path_outside_project(self):\n-        self.assertEqual(\n-            str(Path('.scrapy', 'somepath')),\n-            data_path('somepath')\n-        )\n-        abspath = str(Path(os.path.sep, 'absolute', 'path'))\n+        self.assertEqual(str(Path(\".scrapy\", \"somepath\")), data_path(\"somepath\"))\n+        abspath = str(Path(os.path.sep, \"absolute\", \"path\"))\n         self.assertEqual(abspath, data_path(abspath))\n \n     def test_data_path_inside_project(self):\n         with inside_a_project() as proj_path:\n-            expected = Path(proj_path, '.scrapy', 'somepath')\n-            self.assertEqual(\n-                expected.resolve(),\n-                Path(data_path('somepath')).resolve()\n-            )\n-            abspath = str(Path(os.path.sep, 'absolute', 'path').resolve())\n+            expected = Path(proj_path, \".scrapy\", \"somepath\")\n+            self.assertEqual(expected.resolve(), Path(data_path(\"somepath\")).resolve())\n+            abspath = str(Path(os.path.sep, \"absolute\", \"path\").resolve())\n             self.assertEqual(abspath, data_path(abspath))\n \n \n@@ -59,22 +53,21 @@ def set_env(**update):\n \n \n class GetProjectSettingsTestCase(unittest.TestCase):\n-\n     def test_valid_envvar(self):\n-        value = 'tests.test_cmdline.settings'\n+        value = \"tests.test_cmdline.settings\"\n         envvars = {\n-            'SCRAPY_SETTINGS_MODULE': value,\n+            \"SCRAPY_SETTINGS_MODULE\": value,\n         }\n         with warnings.catch_warnings():\n             warnings.simplefilter(\"error\")\n             with set_env(**envvars):\n                 settings = get_project_settings()\n \n-        assert settings.get('SETTINGS_MODULE') == value\n+        assert settings.get(\"SETTINGS_MODULE\") == value\n \n     def test_invalid_envvar(self):\n         envvars = {\n-            'SCRAPY_FOO': 'bar',\n+            \"SCRAPY_FOO\": \"bar\",\n         }\n         with set_env(**envvars):\n             settings = get_project_settings()\n@@ -82,12 +75,12 @@ class GetProjectSettingsTestCase(unittest.TestCase):\n         assert settings.get(\"SCRAPY_FOO\") is None\n \n     def test_valid_and_invalid_envvars(self):\n-        value = 'tests.test_cmdline.settings'\n+        value = \"tests.test_cmdline.settings\"\n         envvars = {\n-            'SCRAPY_FOO': 'bar',\n-            'SCRAPY_SETTINGS_MODULE': value,\n+            \"SCRAPY_FOO\": \"bar\",\n+            \"SCRAPY_SETTINGS_MODULE\": value,\n         }\n         with set_env(**envvars):\n             settings = get_project_settings()\n-        assert settings.get('SETTINGS_MODULE') == value\n-        assert settings.get('SCRAPY_FOO') is None\n+        assert settings.get(\"SETTINGS_MODULE\") == value\n+        assert settings.get(\"SCRAPY_FOO\") is None\n\n@@ -7,12 +7,19 @@ from twisted.trial import unittest\n from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n from scrapy.utils.defer import deferred_f_from_coro_f, aiter_errback\n from scrapy.utils.python import (\n-    memoizemethod_noargs, binary_is_text, equal_attributes,\n-    get_func_args, to_bytes, to_unicode,\n-    without_none_values, MutableChain, MutableAsyncChain)\n+    memoizemethod_noargs,\n+    binary_is_text,\n+    equal_attributes,\n+    get_func_args,\n+    to_bytes,\n+    to_unicode,\n+    without_none_values,\n+    MutableChain,\n+    MutableAsyncChain,\n+)\n \n \n-__doctests__ = ['scrapy.utils.python']\n+__doctests__ = [\"scrapy.utils.python\"]\n \n \n class MutableChainTest(unittest.TestCase):\n@@ -79,48 +86,41 @@ class MutableAsyncChainTest(unittest.TestCase):\n \n class ToUnicodeTest(unittest.TestCase):\n     def test_converting_an_utf8_encoded_string_to_unicode(self):\n-        self.assertEqual(to_unicode(b'lel\\xc3\\xb1e'), 'lel\\xf1e')\n+        self.assertEqual(to_unicode(b\"lel\\xc3\\xb1e\"), \"lel\\xf1e\")\n \n     def test_converting_a_latin_1_encoded_string_to_unicode(self):\n-        self.assertEqual(to_unicode(b'lel\\xf1e', 'latin-1'), 'lel\\xf1e')\n+        self.assertEqual(to_unicode(b\"lel\\xf1e\", \"latin-1\"), \"lel\\xf1e\")\n \n     def test_converting_a_unicode_to_unicode_should_return_the_same_object(self):\n-        self.assertEqual(to_unicode('\\xf1e\\xf1e\\xf1e'), '\\xf1e\\xf1e\\xf1e')\n+        self.assertEqual(to_unicode(\"\\xf1e\\xf1e\\xf1e\"), \"\\xf1e\\xf1e\\xf1e\")\n \n     def test_converting_a_strange_object_should_raise_TypeError(self):\n         self.assertRaises(TypeError, to_unicode, 423)\n \n     def test_errors_argument(self):\n-        self.assertEqual(\n-            to_unicode(b'a\\xedb', 'utf-8', errors='replace'),\n-            'a\\ufffdb'\n-        )\n+        self.assertEqual(to_unicode(b\"a\\xedb\", \"utf-8\", errors=\"replace\"), \"a\\ufffdb\")\n \n \n class ToBytesTest(unittest.TestCase):\n     def test_converting_a_unicode_object_to_an_utf_8_encoded_string(self):\n-        self.assertEqual(to_bytes('\\xa3 49'), b'\\xc2\\xa3 49')\n+        self.assertEqual(to_bytes(\"\\xa3 49\"), b\"\\xc2\\xa3 49\")\n \n     def test_converting_a_unicode_object_to_a_latin_1_encoded_string(self):\n-        self.assertEqual(to_bytes('\\xa3 49', 'latin-1'), b'\\xa3 49')\n+        self.assertEqual(to_bytes(\"\\xa3 49\", \"latin-1\"), b\"\\xa3 49\")\n \n     def test_converting_a_regular_bytes_to_bytes_should_return_the_same_object(self):\n-        self.assertEqual(to_bytes(b'lel\\xf1e'), b'lel\\xf1e')\n+        self.assertEqual(to_bytes(b\"lel\\xf1e\"), b\"lel\\xf1e\")\n \n     def test_converting_a_strange_object_should_raise_TypeError(self):\n         self.assertRaises(TypeError, to_bytes, unittest)\n \n     def test_errors_argument(self):\n-        self.assertEqual(\n-            to_bytes('a\\ufffdb', 'latin-1', errors='replace'),\n-            b'a?b'\n-        )\n+        self.assertEqual(to_bytes(\"a\\ufffdb\", \"latin-1\", errors=\"replace\"), b\"a?b\")\n \n \n class MemoizedMethodTest(unittest.TestCase):\n     def test_memoizemethod_noargs(self):\n         class A:\n-\n             @memoizemethod_noargs\n             def cached(self):\n                 return object()\n@@ -141,7 +141,7 @@ class BinaryIsTextTest(unittest.TestCase):\n         assert binary_is_text(b\"hello\")\n \n     def test_utf_16_strings_contain_null_bytes(self):\n-        assert binary_is_text(\"hello\".encode('utf-16'))\n+        assert binary_is_text(\"hello\".encode(\"utf-16\"))\n \n     def test_one_with_encoding(self):\n         assert binary_is_text(b\"<div>Price \\xa3</div>\")\n@@ -151,7 +151,6 @@ class BinaryIsTextTest(unittest.TestCase):\n \n \n class UtilsPythonTestCase(unittest.TestCase):\n-\n     def test_equal_attributes(self):\n         class Obj:\n             pass\n@@ -161,44 +160,44 @@ class UtilsPythonTestCase(unittest.TestCase):\n         # no attributes given return False\n         self.assertFalse(equal_attributes(a, b, []))\n         # not existent attributes\n-        self.assertFalse(equal_attributes(a, b, ['x', 'y']))\n+        self.assertFalse(equal_attributes(a, b, [\"x\", \"y\"]))\n \n         a.x = 1\n         b.x = 1\n         # equal attribute\n-        self.assertTrue(equal_attributes(a, b, ['x']))\n+        self.assertTrue(equal_attributes(a, b, [\"x\"]))\n \n         b.y = 2\n         # obj1 has no attribute y\n-        self.assertFalse(equal_attributes(a, b, ['x', 'y']))\n+        self.assertFalse(equal_attributes(a, b, [\"x\", \"y\"]))\n \n         a.y = 2\n         # equal attributes\n-        self.assertTrue(equal_attributes(a, b, ['x', 'y']))\n+        self.assertTrue(equal_attributes(a, b, [\"x\", \"y\"]))\n \n         a.y = 1\n         # differente attributes\n-        self.assertFalse(equal_attributes(a, b, ['x', 'y']))\n+        self.assertFalse(equal_attributes(a, b, [\"x\", \"y\"]))\n \n         # test callable\n         a.meta = {}\n         b.meta = {}\n-        self.assertTrue(equal_attributes(a, b, ['meta']))\n+        self.assertTrue(equal_attributes(a, b, [\"meta\"]))\n \n         # compare ['meta']['a']\n-        a.meta['z'] = 1\n-        b.meta['z'] = 1\n+        a.meta[\"z\"] = 1\n+        b.meta[\"z\"] = 1\n \n-        get_z = operator.itemgetter('z')\n-        get_meta = operator.attrgetter('meta')\n+        get_z = operator.itemgetter(\"z\")\n+        get_meta = operator.attrgetter(\"meta\")\n \n         def compare_z(obj):\n             return get_z(get_meta(obj))\n \n-        self.assertTrue(equal_attributes(a, b, [compare_z, 'x']))\n+        self.assertTrue(equal_attributes(a, b, [compare_z, \"x\"]))\n         # fail z equality\n-        a.meta['z'] = 2\n-        self.assertFalse(equal_attributes(a, b, [compare_z, 'x']))\n+        a.meta[\"z\"] = 2\n+        self.assertFalse(equal_attributes(a, b, [compare_z, \"x\"]))\n \n     def test_get_func_args(self):\n         def f1(a, b, c):\n@@ -218,7 +217,6 @@ class UtilsPythonTestCase(unittest.TestCase):\n                 pass\n \n         class Callable:\n-\n             def __call__(self, a, b, c):\n                 pass\n \n@@ -228,33 +226,38 @@ class UtilsPythonTestCase(unittest.TestCase):\n         partial_f2 = functools.partial(f1, b=None)\n         partial_f3 = functools.partial(partial_f2, None)\n \n-        self.assertEqual(get_func_args(f1), ['a', 'b', 'c'])\n-        self.assertEqual(get_func_args(f2), ['a', 'b', 'c'])\n-        self.assertEqual(get_func_args(f3), ['a', 'b', 'c'])\n-        self.assertEqual(get_func_args(A), ['a', 'b', 'c'])\n-        self.assertEqual(get_func_args(a.method), ['a', 'b', 'c'])\n-        self.assertEqual(get_func_args(partial_f1), ['b', 'c'])\n-        self.assertEqual(get_func_args(partial_f2), ['a', 'c'])\n-        self.assertEqual(get_func_args(partial_f3), ['c'])\n-        self.assertEqual(get_func_args(cal), ['a', 'b', 'c'])\n+        self.assertEqual(get_func_args(f1), [\"a\", \"b\", \"c\"])\n+        self.assertEqual(get_func_args(f2), [\"a\", \"b\", \"c\"])\n+        self.assertEqual(get_func_args(f3), [\"a\", \"b\", \"c\"])\n+        self.assertEqual(get_func_args(A), [\"a\", \"b\", \"c\"])\n+        self.assertEqual(get_func_args(a.method), [\"a\", \"b\", \"c\"])\n+        self.assertEqual(get_func_args(partial_f1), [\"b\", \"c\"])\n+        self.assertEqual(get_func_args(partial_f2), [\"a\", \"c\"])\n+        self.assertEqual(get_func_args(partial_f3), [\"c\"])\n+        self.assertEqual(get_func_args(cal), [\"a\", \"b\", \"c\"])\n         self.assertEqual(get_func_args(object), [])\n \n-        if platform.python_implementation() == 'CPython':\n+        if platform.python_implementation() == \"CPython\":\n             # TODO: how do we fix this to return the actual argument names?\n             self.assertEqual(get_func_args(str.split), [])\n             self.assertEqual(get_func_args(\" \".join), [])\n             self.assertEqual(get_func_args(operator.itemgetter(2)), [])\n-        elif platform.python_implementation() == 'PyPy':\n-            self.assertEqual(get_func_args(str.split, stripself=True), ['sep', 'maxsplit'])\n-            self.assertEqual(get_func_args(operator.itemgetter(2), stripself=True), ['obj'])\n-            self.assertEqual(get_func_args(\" \".join, stripself=True), ['iterable'])\n+        elif platform.python_implementation() == \"PyPy\":\n+            self.assertEqual(\n+                get_func_args(str.split, stripself=True), [\"sep\", \"maxsplit\"]\n+            )\n+            self.assertEqual(\n+                get_func_args(operator.itemgetter(2), stripself=True), [\"obj\"]\n+            )\n+            self.assertEqual(get_func_args(\" \".join, stripself=True), [\"iterable\"])\n \n     def test_without_none_values(self):\n         self.assertEqual(without_none_values([1, None, 3, 4]), [1, 3, 4])\n         self.assertEqual(without_none_values((1, None, 3, 4)), (1, 3, 4))\n         self.assertEqual(\n-            without_none_values({'one': 1, 'none': None, 'three': 3, 'four': 4}),\n-            {'one': 1, 'three': 3, 'four': 4})\n+            without_none_values({\"one\": 1, \"none\": None, \"three\": 3, \"four\": 4}),\n+            {\"one\": 1, \"three\": 3, \"four\": 4},\n+        )\n \n \n if __name__ == \"__main__\":\n\n@@ -23,24 +23,32 @@ from scrapy.utils.test import get_crawler\n \n \n class UtilsRequestTest(unittest.TestCase):\n-\n     def test_request_authenticate(self):\n         r = Request(\"http://www.example.com\")\n-        request_authenticate(r, 'someuser', 'somepass')\n-        self.assertEqual(r.headers['Authorization'], b'Basic c29tZXVzZXI6c29tZXBhc3M=')\n+        request_authenticate(r, \"someuser\", \"somepass\")\n+        self.assertEqual(r.headers[\"Authorization\"], b\"Basic c29tZXVzZXI6c29tZXBhc3M=\")\n \n     def test_request_httprepr(self):\n         r1 = Request(\"http://www.example.com\")\n-        self.assertEqual(request_httprepr(r1), b'GET / HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n')\n+        self.assertEqual(\n+            request_httprepr(r1), b\"GET / HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n\"\n+        )\n \n         r1 = Request(\"http://www.example.com/some/page.html?arg=1\")\n-        self.assertEqual(request_httprepr(r1), b'GET /some/page.html?arg=1 HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n')\n-\n-        r1 = Request(\"http://www.example.com\", method='POST',\n-                     headers={\"Content-type\": b\"text/html\"}, body=b\"Some body\")\n         self.assertEqual(\n             request_httprepr(r1),\n-            b'POST / HTTP/1.1\\r\\nHost: www.example.com\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body'\n+            b\"GET /some/page.html?arg=1 HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n\",\n+        )\n+\n+        r1 = Request(\n+            \"http://www.example.com\",\n+            method=\"POST\",\n+            headers={\"Content-type\": b\"text/html\"},\n+            body=b\"Some body\",\n+        )\n+        self.assertEqual(\n+            request_httprepr(r1),\n+            b\"POST / HTTP/1.1\\r\\nHost: www.example.com\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body\",\n         )\n \n     def test_request_httprepr_for_non_http_request(self):\n@@ -61,77 +69,77 @@ class FingerprintTest(unittest.TestCase):\n     known_hashes: Tuple[Tuple[Request, Union[bytes, str], Dict], ...] = (\n         (\n             Request(\"http://example.org\"),\n-            b'xs\\xd7\\x0c3uj\\x15\\xfe\\xd7d\\x9b\\xa9\\t\\xe0d\\xbf\\x9cXD',\n+            b\"xs\\xd7\\x0c3uj\\x15\\xfe\\xd7d\\x9b\\xa9\\t\\xe0d\\xbf\\x9cXD\",\n             {},\n         ),\n         (\n             Request(\"https://example.org\"),\n-            b'\\xc04\\x85P,\\xaa\\x91\\x06\\xf8t\\xb4\\xbd*\\xd9\\xe9\\x8a:m\\xc3l',\n+            b\"\\xc04\\x85P,\\xaa\\x91\\x06\\xf8t\\xb4\\xbd*\\xd9\\xe9\\x8a:m\\xc3l\",\n             {},\n         ),\n         (\n             Request(\"https://example.org?a\"),\n-            b'G\\xad\\xb8Ck\\x19\\x1c\\xed\\x838,\\x01\\xc4\\xde;\\xee\\xa5\\x94a\\x0c',\n+            b\"G\\xad\\xb8Ck\\x19\\x1c\\xed\\x838,\\x01\\xc4\\xde;\\xee\\xa5\\x94a\\x0c\",\n             {},\n         ),\n         (\n             Request(\"https://example.org?a=b\"),\n-            b'\\x024MYb\\x8a\\xc2\\x1e\\xbc>\\xd6\\xac*\\xda\\x9cF\\xc1r\\x7f\\x17',\n+            b\"\\x024MYb\\x8a\\xc2\\x1e\\xbc>\\xd6\\xac*\\xda\\x9cF\\xc1r\\x7f\\x17\",\n             {},\n         ),\n         (\n             Request(\"https://example.org?a=b&a\"),\n-            b't+\\xe8*\\xfb\\x84\\xe3v\\x1a}\\x88p\\xc0\\xccB\\xd7\\x9d\\xfez\\x96',\n+            b\"t+\\xe8*\\xfb\\x84\\xe3v\\x1a}\\x88p\\xc0\\xccB\\xd7\\x9d\\xfez\\x96\",\n             {},\n         ),\n         (\n             Request(\"https://example.org?a=b&a=c\"),\n-            b'\\xda\\x1ec\\xd0\\x9c\\x08s`\\xb4\\x9b\\xe2\\xb6R\\xf8k\\xef\\xeaQG\\xef',\n+            b\"\\xda\\x1ec\\xd0\\x9c\\x08s`\\xb4\\x9b\\xe2\\xb6R\\xf8k\\xef\\xeaQG\\xef\",\n             {},\n         ),\n         (\n-            Request(\"https://example.org\", method='POST'),\n-            b'\\x9d\\xcdA\\x0fT\\x02:\\xca\\xa0}\\x90\\xda\\x05B\\xded\\x8aN7\\x1d',\n+            Request(\"https://example.org\", method=\"POST\"),\n+            b\"\\x9d\\xcdA\\x0fT\\x02:\\xca\\xa0}\\x90\\xda\\x05B\\xded\\x8aN7\\x1d\",\n             {},\n         ),\n         (\n-            Request(\"https://example.org\", body=b'a'),\n-            b'\\xc34z>\\xd8\\x99\\x8b\\xda7\\x05r\\x99I\\xa8\\xa0x;\\xa41_',\n+            Request(\"https://example.org\", body=b\"a\"),\n+            b\"\\xc34z>\\xd8\\x99\\x8b\\xda7\\x05r\\x99I\\xa8\\xa0x;\\xa41_\",\n             {},\n         ),\n         (\n-            Request(\"https://example.org\", method='POST', body=b'a'),\n-            b'5`\\xe2y4\\xd0\\x9d\\xee\\xe0\\xbatw\\x87Q\\xe8O\\xd78\\xfc\\xe7',\n+            Request(\"https://example.org\", method=\"POST\", body=b\"a\"),\n+            b\"5`\\xe2y4\\xd0\\x9d\\xee\\xe0\\xbatw\\x87Q\\xe8O\\xd78\\xfc\\xe7\",\n             {},\n         ),\n         (\n-            Request(\"https://example.org#a\", headers={'A': b'B'}),\n-            b'\\xc04\\x85P,\\xaa\\x91\\x06\\xf8t\\xb4\\xbd*\\xd9\\xe9\\x8a:m\\xc3l',\n+            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n+            b\"\\xc04\\x85P,\\xaa\\x91\\x06\\xf8t\\xb4\\xbd*\\xd9\\xe9\\x8a:m\\xc3l\",\n             {},\n         ),\n         (\n-            Request(\"https://example.org#a\", headers={'A': b'B'}),\n-            b']\\xc7\\x1f\\xf2\\xafG2\\xbc\\xa4\\xfa\\x99\\n33\\xda\\x18\\x94\\x81U.',\n-            {'include_headers': ['A']},\n+            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n+            b\"]\\xc7\\x1f\\xf2\\xafG2\\xbc\\xa4\\xfa\\x99\\n33\\xda\\x18\\x94\\x81U.\",\n+            {\"include_headers\": [\"A\"]},\n         ),\n         (\n-            Request(\"https://example.org#a\", headers={'A': b'B'}),\n-            b'<\\x1a\\xeb\\x85y\\xdeW\\xfb\\xdcq\\x88\\xee\\xaf\\x17\\xdd\\x0c\\xbfH\\x18\\x1f',\n-            {'keep_fragments': True},\n+            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n+            b\"<\\x1a\\xeb\\x85y\\xdeW\\xfb\\xdcq\\x88\\xee\\xaf\\x17\\xdd\\x0c\\xbfH\\x18\\x1f\",\n+            {\"keep_fragments\": True},\n         ),\n         (\n-            Request(\"https://example.org#a\", headers={'A': b'B'}),\n-            b'\\xc1\\xef~\\x94\\x9bS\\xc1\\x83\\t\\xdcz8\\x9f\\xdc{\\x11\\x16I.\\x11',\n-            {'include_headers': ['A'], 'keep_fragments': True},\n+            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n+            b\"\\xc1\\xef~\\x94\\x9bS\\xc1\\x83\\t\\xdcz8\\x9f\\xdc{\\x11\\x16I.\\x11\",\n+            {\"include_headers\": [\"A\"], \"keep_fragments\": True},\n         ),\n         (\n             Request(\"https://example.org/ab\"),\n-            b'N\\xe5l\\xb8\\x12@iw\\xe2\\xf3\\x1bp\\xea\\xffp!u\\xe2\\x8a\\xc6',\n+            b\"N\\xe5l\\xb8\\x12@iw\\xe2\\xf3\\x1bp\\xea\\xffp!u\\xe2\\x8a\\xc6\",\n             {},\n         ),\n         (\n-            Request(\"https://example.org/a\", body=b'b'),\n-            b'_NOv\\xbco$6\\xfcW\\x9f\\xb24g\\x9f\\xbb\\xdd\\xa82\\xc5',\n+            Request(\"https://example.org/a\", body=b\"b\"),\n+            b\"_NOv\\xbco$6\\xfcW\\x9f\\xb24g\\x9f\\xbb\\xdd\\xa82\\xc5\",\n             {},\n         ),\n     )\n@@ -143,42 +151,42 @@ class FingerprintTest(unittest.TestCase):\n         self.assertEqual(self.function(r1), self.function(r2))\n \n     def test_query_string_key_without_value(self):\n-        r1 = Request('http://www.example.com/hnnoticiaj1.aspx?78132,199')\n-        r2 = Request('http://www.example.com/hnnoticiaj1.aspx?78160,199')\n+        r1 = Request(\"http://www.example.com/hnnoticiaj1.aspx?78132,199\")\n+        r2 = Request(\"http://www.example.com/hnnoticiaj1.aspx?78160,199\")\n         self.assertNotEqual(self.function(r1), self.function(r2))\n \n     def test_caching(self):\n-        r1 = Request('http://www.example.com/hnnoticiaj1.aspx?78160,199')\n-        self.assertEqual(\n-            self.function(r1),\n-            self.cache[r1][self.default_cache_key]\n-        )\n+        r1 = Request(\"http://www.example.com/hnnoticiaj1.aspx?78160,199\")\n+        self.assertEqual(self.function(r1), self.cache[r1][self.default_cache_key])\n \n     def test_header(self):\n         r1 = Request(\"http://www.example.com/members/offers.html\")\n         r2 = Request(\"http://www.example.com/members/offers.html\")\n-        r2.headers['SESSIONID'] = b\"somehash\"\n+        r2.headers[\"SESSIONID\"] = b\"somehash\"\n         self.assertEqual(self.function(r1), self.function(r2))\n \n     def test_headers(self):\n         r1 = Request(\"http://www.example.com/\")\n         r2 = Request(\"http://www.example.com/\")\n-        r2.headers['Accept-Language'] = b'en'\n+        r2.headers[\"Accept-Language\"] = b\"en\"\n         r3 = Request(\"http://www.example.com/\")\n-        r3.headers['Accept-Language'] = b'en'\n-        r3.headers['SESSIONID'] = b\"somehash\"\n+        r3.headers[\"Accept-Language\"] = b\"en\"\n+        r3.headers[\"SESSIONID\"] = b\"somehash\"\n \n         self.assertEqual(self.function(r1), self.function(r2), self.function(r3))\n \n-        self.assertEqual(self.function(r1),\n-                         self.function(r1, include_headers=['Accept-Language']))\n+        self.assertEqual(\n+            self.function(r1), self.function(r1, include_headers=[\"Accept-Language\"])\n+        )\n \n         self.assertNotEqual(\n-            self.function(r1),\n-            self.function(r2, include_headers=['Accept-Language']))\n+            self.function(r1), self.function(r2, include_headers=[\"Accept-Language\"])\n+        )\n \n-        self.assertEqual(self.function(r3, include_headers=['accept-language', 'sessionid']),\n-                         self.function(r3, include_headers=['SESSIONID', 'Accept-Language']))\n+        self.assertEqual(\n+            self.function(r3, include_headers=[\"accept-language\", \"sessionid\"]),\n+            self.function(r3, include_headers=[\"SESSIONID\", \"Accept-Language\"]),\n+        )\n \n     def test_fragment(self):\n         r1 = Request(\"http://www.example.com/test.html\")\n@@ -190,8 +198,8 @@ class FingerprintTest(unittest.TestCase):\n \n     def test_method_and_body(self):\n         r1 = Request(\"http://www.example.com\")\n-        r2 = Request(\"http://www.example.com\", method='POST')\n-        r3 = Request(\"http://www.example.com\", method='POST', body=b'request body')\n+        r2 = Request(\"http://www.example.com\", method=\"POST\")\n+        r3 = Request(\"http://www.example.com\", method=\"POST\", body=b\"request body\")\n \n         self.assertNotEqual(self.function(r1), self.function(r2))\n         self.assertNotEqual(self.function(r2), self.function(r3))\n@@ -209,7 +217,7 @@ class FingerprintTest(unittest.TestCase):\n         # would put the body right after the URL.\n         r1 = Request(\"http://www.example.com/foo\")\n         fp1 = self.function(r1)\n-        r2 = Request(\"http://www.example.com/f\", body=b'oo')\n+        r2 = Request(\"http://www.example.com/f\", body=b\"oo\")\n         fp2 = self.function(r2)\n         self.assertNotEqual(fp1, fp2)\n \n@@ -217,13 +225,9 @@ class FingerprintTest(unittest.TestCase):\n         \"\"\"Test hardcoded hashes, to make sure future changes to not introduce\n         backward incompatibilities.\"\"\"\n         actual = [\n-            self.function(request, **kwargs)\n-            for request, _, kwargs in self.known_hashes\n-        ]\n-        expected = [\n-            _fingerprint\n-            for _, _fingerprint, _ in self.known_hashes\n+            self.function(request, **kwargs) for request, _, kwargs in self.known_hashes\n         ]\n+        expected = [_fingerprint for _, _fingerprint, _ in self.known_hashes]\n         self.assertEqual(actual, expected)\n \n \n@@ -233,77 +237,77 @@ class RequestFingerprintTest(FingerprintTest):\n     known_hashes: Tuple[Tuple[Request, Union[bytes, str], Dict], ...] = (\n         (\n             Request(\"http://example.org\"),\n-            'b2e5245ef826fd9576c93bd6e392fce3133fab62',\n+            \"b2e5245ef826fd9576c93bd6e392fce3133fab62\",\n             {},\n         ),\n         (\n             Request(\"https://example.org\"),\n-            'bd10a0a89ea32cdee77917320f1309b0da87e892',\n+            \"bd10a0a89ea32cdee77917320f1309b0da87e892\",\n             {},\n         ),\n         (\n             Request(\"https://example.org?a\"),\n-            '2fb7d48ae02f04b749f40caa969c0bc3c43204ce',\n+            \"2fb7d48ae02f04b749f40caa969c0bc3c43204ce\",\n             {},\n         ),\n         (\n             Request(\"https://example.org?a=b\"),\n-            '42e5fe149b147476e3f67ad0670c57b4cc57856a',\n+            \"42e5fe149b147476e3f67ad0670c57b4cc57856a\",\n             {},\n         ),\n         (\n             Request(\"https://example.org?a=b&a\"),\n-            'd23a9787cb56c6375c2cae4453c5a8c634526942',\n+            \"d23a9787cb56c6375c2cae4453c5a8c634526942\",\n             {},\n         ),\n         (\n             Request(\"https://example.org?a=b&a=c\"),\n-            '9a18a7a8552a9182b7f1e05d33876409e421e5c5',\n+            \"9a18a7a8552a9182b7f1e05d33876409e421e5c5\",\n             {},\n         ),\n         (\n-            Request(\"https://example.org\", method='POST'),\n-            'ba20a80cb5c5ca460021ceefb3c2467b2bfd1bc6',\n+            Request(\"https://example.org\", method=\"POST\"),\n+            \"ba20a80cb5c5ca460021ceefb3c2467b2bfd1bc6\",\n             {},\n         ),\n         (\n-            Request(\"https://example.org\", body=b'a'),\n-            '4bb136e54e715a4ea7a9dd1101831765d33f2d60',\n+            Request(\"https://example.org\", body=b\"a\"),\n+            \"4bb136e54e715a4ea7a9dd1101831765d33f2d60\",\n             {},\n         ),\n         (\n-            Request(\"https://example.org\", method='POST', body=b'a'),\n-            '6c6595374a304b293be762f7b7be3f54e9947c65',\n+            Request(\"https://example.org\", method=\"POST\", body=b\"a\"),\n+            \"6c6595374a304b293be762f7b7be3f54e9947c65\",\n             {},\n         ),\n         (\n-            Request(\"https://example.org#a\", headers={'A': b'B'}),\n-            'bd10a0a89ea32cdee77917320f1309b0da87e892',\n+            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n+            \"bd10a0a89ea32cdee77917320f1309b0da87e892\",\n             {},\n         ),\n         (\n-            Request(\"https://example.org#a\", headers={'A': b'B'}),\n-            '515b633cb3ca502a33a9d8c890e889ec1e425e65',\n-            {'include_headers': ['A']},\n+            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n+            \"515b633cb3ca502a33a9d8c890e889ec1e425e65\",\n+            {\"include_headers\": [\"A\"]},\n         ),\n         (\n-            Request(\"https://example.org#a\", headers={'A': b'B'}),\n-            '505c96e7da675920dfef58725e8c957dfdb38f47',\n-            {'keep_fragments': True},\n+            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n+            \"505c96e7da675920dfef58725e8c957dfdb38f47\",\n+            {\"keep_fragments\": True},\n         ),\n         (\n-            Request(\"https://example.org#a\", headers={'A': b'B'}),\n-            'd6f673cdcb661b7970c2b9a00ee63e87d1e2e5da',\n-            {'include_headers': ['A'], 'keep_fragments': True},\n+            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n+            \"d6f673cdcb661b7970c2b9a00ee63e87d1e2e5da\",\n+            {\"include_headers\": [\"A\"], \"keep_fragments\": True},\n         ),\n         (\n             Request(\"https://example.org/ab\"),\n-            '4e2870fee58582d6f81755e9b8fdefe3cba0c951',\n+            \"4e2870fee58582d6f81755e9b8fdefe3cba0c951\",\n             {},\n         ),\n         (\n-            Request(\"https://example.org/a\", body=b'b'),\n-            '4e2870fee58582d6f81755e9b8fdefe3cba0c951',\n+            Request(\"https://example.org/a\", body=b\"b\"),\n+            \"4e2870fee58582d6f81755e9b8fdefe3cba0c951\",\n             {},\n         ),\n     )\n@@ -314,36 +318,29 @@ class RequestFingerprintTest(FingerprintTest):\n     def tearDown(self) -> None:\n         warnings.simplefilter(\"default\", ScrapyDeprecationWarning)\n \n-    @pytest.mark.xfail(reason='known bug kept for backward compatibility', strict=True)\n+    @pytest.mark.xfail(reason=\"known bug kept for backward compatibility\", strict=True)\n     def test_part_separation(self):\n         super().test_part_separation()\n \n \n class RequestFingerprintDeprecationTest(unittest.TestCase):\n-\n     def test_deprecation_default_parameters(self):\n         with pytest.warns(ScrapyDeprecationWarning) as warnings:\n             request_fingerprint(Request(\"http://www.example.com\"))\n         messages = [str(warning.message) for warning in warnings]\n         self.assertTrue(\n-            any(\n-                'Call to deprecated function' in message\n-                for message in messages\n+            any(\"Call to deprecated function\" in message for message in messages)\n         )\n-        )\n-        self.assertFalse(any('non-default' in message for message in messages))\n+        self.assertFalse(any(\"non-default\" in message for message in messages))\n \n     def test_deprecation_non_default_parameters(self):\n         with pytest.warns(ScrapyDeprecationWarning) as warnings:\n             request_fingerprint(Request(\"http://www.example.com\"), keep_fragments=True)\n         messages = [str(warning.message) for warning in warnings]\n         self.assertTrue(\n-            any(\n-                'Call to deprecated function' in message\n-                for message in messages\n+            any(\"Call to deprecated function\" in message for message in messages)\n         )\n-        )\n-        self.assertTrue(any('non-default' in message for message in messages))\n+        self.assertTrue(any(\"non-default\" in message for message in messages))\n \n \n class RequestFingerprintAsBytesTest(FingerprintTest):\n@@ -352,24 +349,21 @@ class RequestFingerprintAsBytesTest(FingerprintTest):\n     known_hashes = RequestFingerprintTest.known_hashes\n \n     def test_caching(self):\n-        r1 = Request('http://www.example.com/hnnoticiaj1.aspx?78160,199')\n+        r1 = Request(\"http://www.example.com/hnnoticiaj1.aspx?78160,199\")\n         self.assertEqual(\n-            self.function(r1),\n-            bytes.fromhex(self.cache[r1][self.default_cache_key])\n+            self.function(r1), bytes.fromhex(self.cache[r1][self.default_cache_key])\n         )\n \n-    @pytest.mark.xfail(reason='known bug kept for backward compatibility', strict=True)\n+    @pytest.mark.xfail(reason=\"known bug kept for backward compatibility\", strict=True)\n     def test_part_separation(self):\n         super().test_part_separation()\n \n     def test_hashes(self):\n         actual = [\n-            self.function(request, **kwargs)\n-            for request, _, kwargs in self.known_hashes\n+            self.function(request, **kwargs) for request, _, kwargs in self.known_hashes\n         ]\n         expected = [\n-            bytes.fromhex(_fingerprint)\n-            for _, _fingerprint, _ in self.known_hashes\n+            bytes.fromhex(_fingerprint) for _, _fingerprint, _ in self.known_hashes\n         ]\n         self.assertEqual(actual, expected)\n \n@@ -385,8 +379,10 @@ def request_fingerprint_2_6(request, include_headers=None, keep_fragments=False)\n     if cache_key not in cache:\n         fp = sha1()\n         fp.update(to_bytes(request.method))\n-        fp.update(to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments)))\n-        fp.update(request.body or b'')\n+        fp.update(\n+            to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments))\n+        )\n+        fp.update(request.body or b\"\")\n         if include_headers:\n             for hdr in include_headers:\n                 if hdr in request.headers:\n@@ -401,39 +397,38 @@ REQUEST_OBJECTS_TO_TEST = (\n     Request(\"http://www.example.com/\"),\n     Request(\"http://www.example.com/query?id=111&cat=222\"),\n     Request(\"http://www.example.com/query?cat=222&id=111\"),\n-    Request('http://www.example.com/hnnoticiaj1.aspx?78132,199'),\n-    Request('http://www.example.com/hnnoticiaj1.aspx?78160,199'),\n+    Request(\"http://www.example.com/hnnoticiaj1.aspx?78132,199\"),\n+    Request(\"http://www.example.com/hnnoticiaj1.aspx?78160,199\"),\n     Request(\"http://www.example.com/members/offers.html\"),\n     Request(\n         \"http://www.example.com/members/offers.html\",\n-        headers={'SESSIONID': b\"somehash\"},\n+        headers={\"SESSIONID\": b\"somehash\"},\n     ),\n     Request(\n         \"http://www.example.com/\",\n-        headers={'Accept-Language': b\"en\"},\n+        headers={\"Accept-Language\": b\"en\"},\n     ),\n     Request(\n         \"http://www.example.com/\",\n         headers={\n-            'Accept-Language': b\"en\",\n-            'SESSIONID': b\"somehash\",\n+            \"Accept-Language\": b\"en\",\n+            \"SESSIONID\": b\"somehash\",\n         },\n     ),\n     Request(\"http://www.example.com/test.html\"),\n     Request(\"http://www.example.com/test.html#fragment\"),\n-    Request(\"http://www.example.com\", method='POST'),\n-    Request(\"http://www.example.com\", method='POST', body=b'request body'),\n+    Request(\"http://www.example.com\", method=\"POST\"),\n+    Request(\"http://www.example.com\", method=\"POST\", body=b\"request body\"),\n )\n \n \n class BackwardCompatibilityTestCase(unittest.TestCase):\n-\n     def test_function_backward_compatibility(self):\n         include_headers_to_test = (\n             None,\n-            ['Accept-Language'],\n-            ['accept-language', 'sessionid'],\n-            ['SESSIONID', 'Accept-Language'],\n+            [\"Accept-Language\"],\n+            [\"accept-language\", \"sessionid\"],\n+            [\"SESSIONID\", \"Accept-Language\"],\n         )\n         for request_object in REQUEST_OBJECTS_TO_TEST:\n             for include_headers in include_headers_to_test:\n@@ -475,14 +470,14 @@ class BackwardCompatibilityTestCase(unittest.TestCase):\n                     fp = sha1()\n                     fp.update(to_bytes(request.method))\n                     fp.update(to_bytes(canonicalize_url(request.url)))\n-                    fp.update(request.body or b'')\n+                    fp.update(request.body or b\"\")\n                     self.cache[request] = fp.digest()\n                 return self.cache[request]\n \n         for request_object in REQUEST_OBJECTS_TO_TEST:\n             with warnings.catch_warnings() as logged_warnings:\n                 settings = {\n-                    'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,\n+                    \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n                 }\n                 crawler = get_crawler(settings_dict=settings)\n                 fp = crawler.request_fingerprinter.fingerprint(request_object)\n@@ -492,11 +487,10 @@ class BackwardCompatibilityTestCase(unittest.TestCase):\n \n \n class RequestFingerprinterTestCase(unittest.TestCase):\n-\n     def test_default_implementation(self):\n         with warnings.catch_warnings(record=True) as logged_warnings:\n             crawler = get_crawler(prevent_warnings=False)\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         self.assertEqual(\n             crawler.request_fingerprinter.fingerprint(request),\n             _request_fingerprint_as_bytes(request),\n@@ -505,11 +499,11 @@ class RequestFingerprinterTestCase(unittest.TestCase):\n \n     def test_deprecated_implementation(self):\n         settings = {\n-            'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.6',\n+            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.6\",\n         }\n         with warnings.catch_warnings(record=True) as logged_warnings:\n             crawler = get_crawler(settings_dict=settings)\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         self.assertEqual(\n             crawler.request_fingerprinter.fingerprint(request),\n             _request_fingerprint_as_bytes(request),\n@@ -518,11 +512,11 @@ class RequestFingerprinterTestCase(unittest.TestCase):\n \n     def test_recommended_implementation(self):\n         settings = {\n-            'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n+            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n         }\n         with warnings.catch_warnings(record=True) as logged_warnings:\n             crawler = get_crawler(settings_dict=settings)\n-        request = Request('https://example.com')\n+        request = Request(\"https://example.com\")\n         self.assertEqual(\n             crawler.request_fingerprinter.fingerprint(request),\n             fingerprint(request),\n@@ -531,34 +525,30 @@ class RequestFingerprinterTestCase(unittest.TestCase):\n \n     def test_unknown_implementation(self):\n         settings = {\n-            'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.5',\n+            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.5\",\n         }\n         with self.assertRaises(ValueError):\n             get_crawler(settings_dict=settings)\n \n \n class CustomRequestFingerprinterTestCase(unittest.TestCase):\n-\n     def test_include_headers(self):\n-\n         class RequestFingerprinter:\n-\n             def fingerprint(self, request):\n-                return fingerprint(request, include_headers=['X-ID'])\n+                return fingerprint(request, include_headers=[\"X-ID\"])\n \n         settings = {\n-            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,\n+            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n         }\n         crawler = get_crawler(settings_dict=settings)\n \n-        r1 = Request(\"http://www.example.com\", headers={'X-ID': '1'})\n+        r1 = Request(\"http://www.example.com\", headers={\"X-ID\": \"1\"})\n         fp1 = crawler.request_fingerprinter.fingerprint(r1)\n-        r2 = Request(\"http://www.example.com\", headers={'X-ID': '2'})\n+        r2 = Request(\"http://www.example.com\", headers={\"X-ID\": \"2\"})\n         fp2 = crawler.request_fingerprinter.fingerprint(r2)\n         self.assertNotEqual(fp1, fp2)\n \n     def test_dont_canonicalize(self):\n-\n         class RequestFingerprinter:\n             cache = WeakKeyDictionary()\n \n@@ -570,7 +560,7 @@ class CustomRequestFingerprinterTestCase(unittest.TestCase):\n                 return self.cache[request]\n \n         settings = {\n-            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,\n+            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n         }\n         crawler = get_crawler(settings_dict=settings)\n \n@@ -581,26 +571,24 @@ class CustomRequestFingerprinterTestCase(unittest.TestCase):\n         self.assertNotEqual(fp1, fp2)\n \n     def test_meta(self):\n-\n         class RequestFingerprinter:\n-\n             def fingerprint(self, request):\n-                if 'fingerprint' in request.meta:\n-                    return request.meta['fingerprint']\n+                if \"fingerprint\" in request.meta:\n+                    return request.meta[\"fingerprint\"]\n                 return fingerprint(request)\n \n         settings = {\n-            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,\n+            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n         }\n         crawler = get_crawler(settings_dict=settings)\n \n         r1 = Request(\"http://www.example.com\")\n         fp1 = crawler.request_fingerprinter.fingerprint(r1)\n-        r2 = Request(\"http://www.example.com\", meta={'fingerprint': 'a'})\n+        r2 = Request(\"http://www.example.com\", meta={\"fingerprint\": \"a\"})\n         fp2 = crawler.request_fingerprinter.fingerprint(r2)\n-        r3 = Request(\"http://www.example.com\", meta={'fingerprint': 'a'})\n+        r3 = Request(\"http://www.example.com\", meta={\"fingerprint\": \"a\"})\n         fp3 = crawler.request_fingerprinter.fingerprint(r3)\n-        r4 = Request(\"http://www.example.com\", meta={'fingerprint': 'b'})\n+        r4 = Request(\"http://www.example.com\", meta={\"fingerprint\": \"b\"})\n         fp4 = crawler.request_fingerprinter.fingerprint(r4)\n         self.assertNotEqual(fp1, fp2)\n         self.assertNotEqual(fp1, fp4)\n@@ -608,55 +596,50 @@ class CustomRequestFingerprinterTestCase(unittest.TestCase):\n         self.assertEqual(fp2, fp3)\n \n     def test_from_crawler(self):\n-\n         class RequestFingerprinter:\n-\n             @classmethod\n             def from_crawler(cls, crawler):\n                 return cls(crawler)\n \n             def __init__(self, crawler):\n-                self._fingerprint = crawler.settings['FINGERPRINT']\n+                self._fingerprint = crawler.settings[\"FINGERPRINT\"]\n \n             def fingerprint(self, request):\n                 return self._fingerprint\n \n         settings = {\n-            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,\n-            'FINGERPRINT': b'fingerprint',\n+            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n+            \"FINGERPRINT\": b\"fingerprint\",\n         }\n         crawler = get_crawler(settings_dict=settings)\n \n         request = Request(\"http://www.example.com\")\n         fingerprint = crawler.request_fingerprinter.fingerprint(request)\n-        self.assertEqual(fingerprint, settings['FINGERPRINT'])\n+        self.assertEqual(fingerprint, settings[\"FINGERPRINT\"])\n \n     def test_from_settings(self):\n-\n         class RequestFingerprinter:\n-\n             @classmethod\n             def from_settings(cls, settings):\n                 return cls(settings)\n \n             def __init__(self, settings):\n-                self._fingerprint = settings['FINGERPRINT']\n+                self._fingerprint = settings[\"FINGERPRINT\"]\n \n             def fingerprint(self, request):\n                 return self._fingerprint\n \n         settings = {\n-            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,\n-            'FINGERPRINT': b'fingerprint',\n+            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n+            \"FINGERPRINT\": b\"fingerprint\",\n         }\n         crawler = get_crawler(settings_dict=settings)\n \n         request = Request(\"http://www.example.com\")\n         fingerprint = crawler.request_fingerprinter.fingerprint(request)\n-        self.assertEqual(fingerprint, settings['FINGERPRINT'])\n+        self.assertEqual(fingerprint, settings[\"FINGERPRINT\"])\n \n     def test_from_crawler_and_settings(self):\n-\n         class RequestFingerprinter:\n \n             # This method is ignored due to the presence of from_crawler\n@@ -669,20 +652,20 @@ class CustomRequestFingerprinterTestCase(unittest.TestCase):\n                 return cls(crawler)\n \n             def __init__(self, crawler):\n-                self._fingerprint = crawler.settings['FINGERPRINT']\n+                self._fingerprint = crawler.settings[\"FINGERPRINT\"]\n \n             def fingerprint(self, request):\n                 return self._fingerprint\n \n         settings = {\n-            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,\n-            'FINGERPRINT': b'fingerprint',\n+            \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n+            \"FINGERPRINT\": b\"fingerprint\",\n         }\n         crawler = get_crawler(settings_dict=settings)\n \n         request = Request(\"http://www.example.com\")\n         fingerprint = crawler.request_fingerprinter.fingerprint(request)\n-        self.assertEqual(fingerprint, settings['FINGERPRINT'])\n+        self.assertEqual(fingerprint, settings[\"FINGERPRINT\"])\n \n \n if __name__ == \"__main__\":\n\n@@ -6,32 +6,49 @@ from urllib.parse import urlparse\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Response, TextResponse, HtmlResponse\n from scrapy.utils.python import to_bytes\n-from scrapy.utils.response import (response_httprepr, open_in_browser,\n-                                   get_meta_refresh, get_base_url, response_status_message)\n+from scrapy.utils.response import (\n+    response_httprepr,\n+    open_in_browser,\n+    get_meta_refresh,\n+    get_base_url,\n+    response_status_message,\n+)\n \n \n-__doctests__ = ['scrapy.utils.response']\n+__doctests__ = [\"scrapy.utils.response\"]\n \n \n class ResponseUtilsTest(unittest.TestCase):\n-    dummy_response = TextResponse(url='http://example.org/', body=b'dummy_response')\n+    dummy_response = TextResponse(url=\"http://example.org/\", body=b\"dummy_response\")\n \n     def test_response_httprepr(self):\n         with warnings.catch_warnings():\n             warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n \n             r1 = Response(\"http://www.example.com\")\n-            self.assertEqual(response_httprepr(r1), b'HTTP/1.1 200 OK\\r\\n\\r\\n')\n+            self.assertEqual(response_httprepr(r1), b\"HTTP/1.1 200 OK\\r\\n\\r\\n\")\n \n-            r1 = Response(\"http://www.example.com\", status=404,\n-                          headers={\"Content-type\": \"text/html\"}, body=b\"Some body\")\n-            self.assertEqual(response_httprepr(r1),\n-                             b'HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n+            r1 = Response(\n+                \"http://www.example.com\",\n+                status=404,\n+                headers={\"Content-type\": \"text/html\"},\n+                body=b\"Some body\",\n+            )\n+            self.assertEqual(\n+                response_httprepr(r1),\n+                b\"HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body\",\n+            )\n \n-            r1 = Response(\"http://www.example.com\", status=6666,\n-                          headers={\"Content-type\": \"text/html\"}, body=b\"Some body\")\n-            self.assertEqual(response_httprepr(r1),\n-                             b'HTTP/1.1 6666 \\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n+            r1 = Response(\n+                \"http://www.example.com\",\n+                status=6666,\n+                headers={\"Content-type\": \"text/html\"},\n+                body=b\"Some body\",\n+            )\n+            self.assertEqual(\n+                response_httprepr(r1),\n+                b\"HTTP/1.1 6666 \\r\\nContent-Type: text/html\\r\\n\\r\\nSome body\",\n+            )\n \n     def test_open_in_browser(self):\n         url = \"http:///www.example.com/some/page.html\"\n@@ -40,10 +57,11 @@ class ResponseUtilsTest(unittest.TestCase):\n         def browser_open(burl):\n             path = urlparse(burl).path\n             if not path or not Path(path).exists():\n-                path = burl.replace('file://', '')\n+                path = burl.replace(\"file://\", \"\")\n             bbody = Path(path).read_bytes()\n             self.assertIn(b'<base href=\"' + to_bytes(url) + b'\">', bbody)\n             return True\n+\n         response = HtmlResponse(url, body=body)\n         assert open_in_browser(response, _openfunc=browser_open), \"Browser not called\"\n \n@@ -51,45 +69,60 @@ class ResponseUtilsTest(unittest.TestCase):\n         self.assertRaises(TypeError, open_in_browser, resp, debug=True)\n \n     def test_get_meta_refresh(self):\n-        r1 = HtmlResponse(\"http://www.example.com\", body=b\"\"\"\n+        r1 = HtmlResponse(\n+            \"http://www.example.com\",\n+            body=b\"\"\"\n         <html>\n         <head><title>Dummy</title><meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n         <body>blahablsdfsal&amp;</body>\n-        </html>\"\"\")\n-        r2 = HtmlResponse(\"http://www.example.com\", body=b\"\"\"\n+        </html>\"\"\",\n+        )\n+        r2 = HtmlResponse(\n+            \"http://www.example.com\",\n+            body=b\"\"\"\n         <html>\n         <head><title>Dummy</title><noScript>\n         <meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n         </noSCRIPT>\n         <body>blahablsdfsal&amp;</body>\n-        </html>\"\"\")\n-        r3 = HtmlResponse(\"http://www.example.com\", body=b\"\"\"\n+        </html>\"\"\",\n+        )\n+        r3 = HtmlResponse(\n+            \"http://www.example.com\",\n+            body=b\"\"\"\n     <noscript><meta http-equiv=\"REFRESH\" content=\"0;url=http://www.example.com/newpage</noscript>\n     <script type=\"text/javascript\">\n     if(!checkCookies()){\n         document.write('<meta http-equiv=\"REFRESH\" content=\"0;url=http://www.example.com/newpage\">');\n     }\n     </script>\n-        \"\"\")\n-        self.assertEqual(get_meta_refresh(r1), (5.0, 'http://example.org/newpage'))\n+        \"\"\",\n+        )\n+        self.assertEqual(get_meta_refresh(r1), (5.0, \"http://example.org/newpage\"))\n         self.assertEqual(get_meta_refresh(r2), (None, None))\n         self.assertEqual(get_meta_refresh(r3), (None, None))\n \n     def test_get_base_url(self):\n-        resp = HtmlResponse(\"http://www.example.com\", body=b\"\"\"\n+        resp = HtmlResponse(\n+            \"http://www.example.com\",\n+            body=b\"\"\"\n         <html>\n         <head><base href=\"http://www.example.com/img/\" target=\"_blank\"></head>\n         <body>blahablsdfsal&amp;</body>\n-        </html>\"\"\")\n+        </html>\"\"\",\n+        )\n         self.assertEqual(get_base_url(resp), \"http://www.example.com/img/\")\n \n-        resp2 = HtmlResponse(\"http://www.example.com\", body=b\"\"\"\n-        <html><body>blahablsdfsal&amp;</body></html>\"\"\")\n+        resp2 = HtmlResponse(\n+            \"http://www.example.com\",\n+            body=b\"\"\"\n+        <html><body>blahablsdfsal&amp;</body></html>\"\"\",\n+        )\n         self.assertEqual(get_base_url(resp2), \"http://www.example.com\")\n \n     def test_response_status_message(self):\n-        self.assertEqual(response_status_message(200), '200 OK')\n-        self.assertEqual(response_status_message(404), '404 Not Found')\n+        self.assertEqual(response_status_message(200), \"200 OK\")\n+        self.assertEqual(response_status_message(404), \"404 Not Found\")\n         self.assertEqual(response_status_message(573), \"573 Unknown Status\")\n \n     def test_inject_base_url(self):\n@@ -98,36 +131,50 @@ class ResponseUtilsTest(unittest.TestCase):\n         def check_base_url(burl):\n             path = urlparse(burl).path\n             if not path or not Path(path).exists():\n-                path = burl.replace('file://', '')\n+                path = burl.replace(\"file://\", \"\")\n             bbody = Path(path).read_bytes()\n             self.assertEqual(bbody.count(b'<base href=\"' + to_bytes(url) + b'\">'), 1)\n             return True\n \n-        r1 = HtmlResponse(url, body=b\"\"\"\n+        r1 = HtmlResponse(\n+            url,\n+            body=b\"\"\"\n         <html>\n             <head><title>Dummy</title></head>\n             <body><p>Hello world.</p></body>\n-        </html>\"\"\")\n-        r2 = HtmlResponse(url, body=b\"\"\"\n+        </html>\"\"\",\n+        )\n+        r2 = HtmlResponse(\n+            url,\n+            body=b\"\"\"\n         <html>\n             <head id=\"foo\"><title>Dummy</title></head>\n             <body>Hello world.</body>\n-        </html>\"\"\")\n-        r3 = HtmlResponse(url, body=b\"\"\"\n+        </html>\"\"\",\n+        )\n+        r3 = HtmlResponse(\n+            url,\n+            body=b\"\"\"\n         <html>\n             <head><title>Dummy</title></head>\n             <body>\n                 <header>Hello header</header>\n                 <p>Hello world.</p>\n             </body>\n-        </html>\"\"\")\n-        r4 = HtmlResponse(url, body=b\"\"\"\n+        </html>\"\"\",\n+        )\n+        r4 = HtmlResponse(\n+            url,\n+            body=b\"\"\"\n         <html>\n             <!-- <head>Dummy comment</head> -->\n             <head><title>Dummy</title></head>\n             <body><p>Hello world.</p></body>\n-        </html>\"\"\")\n-        r5 = HtmlResponse(url, body=b\"\"\"\n+        </html>\"\"\",\n+        )\n+        r5 = HtmlResponse(\n+            url,\n+            body=b\"\"\"\n         <html>\n             <!--[if IE]>\n             <head><title>IE head</title></head>\n@@ -136,10 +183,19 @@ class ResponseUtilsTest(unittest.TestCase):\n             <head><title>Standard head</title></head>\n             <!--<![endif]-->\n             <body><p>Hello world.</p></body>\n-        </html>\"\"\")\n+        </html>\"\"\",\n+        )\n \n         assert open_in_browser(r1, _openfunc=check_base_url), \"Inject base url\"\n-        assert open_in_browser(r2, _openfunc=check_base_url), \"Inject base url with argumented head\"\n-        assert open_in_browser(r3, _openfunc=check_base_url), \"Inject unique base url with misleading tag\"\n-        assert open_in_browser(r4, _openfunc=check_base_url), \"Inject unique base url with misleading comment\"\n-        assert open_in_browser(r5, _openfunc=check_base_url), \"Inject unique base url with conditional comment\"\n+        assert open_in_browser(\n+            r2, _openfunc=check_base_url\n+        ), \"Inject base url with argumented head\"\n+        assert open_in_browser(\n+            r3, _openfunc=check_base_url\n+        ), \"Inject unique base url with misleading tag\"\n+        assert open_in_browser(\n+            r4, _openfunc=check_base_url\n+        ), \"Inject unique base url with misleading comment\"\n+        assert open_in_browser(\n+            r5, _openfunc=check_base_url\n+        ), \"Inject unique base url with conditional comment\"\n\n@@ -12,7 +12,6 @@ from scrapy.utils.serialize import ScrapyJSONEncoder\n \n \n class JsonEncoderTestCase(unittest.TestCase):\n-\n     def setUp(self):\n         self.encoder = ScrapyJSONEncoder(sort_keys=True)\n \n@@ -25,19 +24,27 @@ class JsonEncoderTestCase(unittest.TestCase):\n         ts = \"10:11:12\"\n         dec = Decimal(\"1000.12\")\n         decs = \"1000.12\"\n-        s = {'foo'}\n-        ss = ['foo']\n+        s = {\"foo\"}\n+        ss = [\"foo\"]\n         dt_set = {dt}\n         dt_sets = [dts]\n \n-        for input, output in [('foo', 'foo'), (d, ds), (t, ts), (dt, dts),\n-                              (dec, decs), (['foo', d], ['foo', ds]), (s, ss),\n-                              (dt_set, dt_sets)]:\n-            self.assertEqual(self.encoder.encode(input),\n-                             json.dumps(output, sort_keys=True))\n+        for input, output in [\n+            (\"foo\", \"foo\"),\n+            (d, ds),\n+            (t, ts),\n+            (dt, dts),\n+            (dec, decs),\n+            ([\"foo\", d], [\"foo\", ds]),\n+            (s, ss),\n+            (dt_set, dt_sets),\n+        ]:\n+            self.assertEqual(\n+                self.encoder.encode(input), json.dumps(output, sort_keys=True)\n+            )\n \n     def test_encode_deferred(self):\n-        self.assertIn('Deferred', self.encoder.encode(defer.Deferred()))\n+        self.assertIn(\"Deferred\", self.encoder.encode(defer.Deferred()))\n \n     def test_encode_request(self):\n         r = Request(\"http://www.example.com/lala\")\n@@ -61,8 +68,7 @@ class JsonEncoderTestCase(unittest.TestCase):\n         item = TestDataClass(name=\"Product\", url=\"http://product.org\", price=1)\n         encoded = self.encoder.encode(item)\n         self.assertEqual(\n-            encoded,\n-            '{\"name\": \"Product\", \"price\": 1, \"url\": \"http://product.org\"}'\n+            encoded, '{\"name\": \"Product\", \"price\": 1, \"url\": \"http://product.org\"}'\n         )\n \n     def test_encode_attrs_item(self):\n@@ -75,6 +81,5 @@ class JsonEncoderTestCase(unittest.TestCase):\n         item = AttrsItem(name=\"Product\", url=\"http://product.org\", price=1)\n         encoded = self.encoder.encode(item)\n         self.assertEqual(\n-            encoded,\n-            '{\"name\": \"Product\", \"price\": 1, \"url\": \"http://product.org\"}'\n+            encoded, '{\"name\": \"Product\", \"price\": 1, \"url\": \"http://product.org\"}'\n         )\n\n@@ -12,7 +12,6 @@ from scrapy.utils.test import get_from_asyncio_queue\n \n \n class SendCatchLogTest(unittest.TestCase):\n-\n     @defer.inlineCallbacks\n     def test_send_catch_log(self):\n         test_signal = object()\n@@ -22,16 +21,18 @@ class SendCatchLogTest(unittest.TestCase):\n         dispatcher.connect(self.ok_handler, signal=test_signal)\n         with LogCapture() as log:\n             result = yield defer.maybeDeferred(\n-                self._get_result, test_signal, arg='test',\n-                handlers_called=handlers_called\n+                self._get_result,\n+                test_signal,\n+                arg=\"test\",\n+                handlers_called=handlers_called,\n             )\n \n         assert self.error_handler in handlers_called\n         assert self.ok_handler in handlers_called\n         self.assertEqual(len(log.records), 1)\n         record = log.records[0]\n-        self.assertIn('error_handler', record.getMessage())\n-        self.assertEqual(record.levelname, 'ERROR')\n+        self.assertIn(\"error_handler\", record.getMessage())\n+        self.assertEqual(record.levelname, \"ERROR\")\n         self.assertEqual(result[0][0], self.error_handler)\n         self.assertIsInstance(result[0][1], Failure)\n         self.assertEqual(result[1], (self.ok_handler, \"OK\"))\n@@ -48,32 +49,29 @@ class SendCatchLogTest(unittest.TestCase):\n \n     def ok_handler(self, arg, handlers_called):\n         handlers_called.add(self.ok_handler)\n-        assert arg == 'test'\n+        assert arg == \"test\"\n         return \"OK\"\n \n \n class SendCatchLogDeferredTest(SendCatchLogTest):\n-\n     def _get_result(self, signal, *a, **kw):\n         return send_catch_log_deferred(signal, *a, **kw)\n \n \n class SendCatchLogDeferredTest2(SendCatchLogDeferredTest):\n-\n     def ok_handler(self, arg, handlers_called):\n         handlers_called.add(self.ok_handler)\n-        assert arg == 'test'\n+        assert arg == \"test\"\n         d = defer.Deferred()\n         reactor.callLater(0, d.callback, \"OK\")\n         return d\n \n \n-@mark.usefixtures('reactor_pytest')\n+@mark.usefixtures(\"reactor_pytest\")\n class SendCatchLogDeferredAsyncDefTest(SendCatchLogDeferredTest):\n-\n     async def ok_handler(self, arg, handlers_called):\n         handlers_called.add(self.ok_handler)\n-        assert arg == 'test'\n+        assert arg == \"test\"\n         await defer.succeed(42)\n         return \"OK\"\n \n@@ -83,10 +81,9 @@ class SendCatchLogDeferredAsyncDefTest(SendCatchLogDeferredTest):\n \n @mark.only_asyncio()\n class SendCatchLogDeferredAsyncioTest(SendCatchLogDeferredTest):\n-\n     async def ok_handler(self, arg, handlers_called):\n         handlers_called.add(self.ok_handler)\n-        assert arg == 'test'\n+        assert arg == \"test\"\n         await asyncio.sleep(0.2)\n         return await get_from_asyncio_queue(\"OK\")\n \n@@ -95,7 +92,6 @@ class SendCatchLogDeferredAsyncioTest(SendCatchLogDeferredTest):\n \n \n class SendCatchLogTest2(unittest.TestCase):\n-\n     def test_error_logged_if_deferred_not_supported(self):\n         def test_handler():\n             return defer.Deferred()\n\n@@ -4,9 +4,9 @@ from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n \n \n class SitemapTest(unittest.TestCase):\n-\n     def test_sitemap(self):\n-        s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        s = Sitemap(\n+            b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n   <url>\n     <loc>http://www.example.com/</loc>\n@@ -20,19 +20,30 @@ class SitemapTest(unittest.TestCase):\n     <changefreq>weekly</changefreq>\n     <priority>0.8</priority>\n   </url>\n-</urlset>\"\"\")\n-        assert s.type == 'urlset'\n+</urlset>\"\"\"\n+        )\n+        assert s.type == \"urlset\"\n         self.assertEqual(\n             list(s),\n             [\n-                {'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'},\n-                {'priority': '0.8', 'loc': 'http://www.example.com/Special-Offers.html',\n-                 'lastmod': '2009-08-16', 'changefreq': 'weekly'},\n-            ]\n+                {\n+                    \"priority\": \"1\",\n+                    \"loc\": \"http://www.example.com/\",\n+                    \"lastmod\": \"2009-08-16\",\n+                    \"changefreq\": \"daily\",\n+                },\n+                {\n+                    \"priority\": \"0.8\",\n+                    \"loc\": \"http://www.example.com/Special-Offers.html\",\n+                    \"lastmod\": \"2009-08-16\",\n+                    \"changefreq\": \"weekly\",\n+                },\n+            ],\n         )\n \n     def test_sitemap_index(self):\n-        s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        s = Sitemap(\n+            b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n    <sitemap>\n       <loc>http://www.example.com/sitemap1.xml.gz</loc>\n@@ -42,21 +53,29 @@ class SitemapTest(unittest.TestCase):\n       <loc>http://www.example.com/sitemap2.xml.gz</loc>\n       <lastmod>2005-01-01</lastmod>\n    </sitemap>\n-</sitemapindex>\"\"\")\n-        assert s.type == 'sitemapindex'\n+</sitemapindex>\"\"\"\n+        )\n+        assert s.type == \"sitemapindex\"\n         self.assertEqual(\n             list(s),\n             [\n-                {'loc': 'http://www.example.com/sitemap1.xml.gz', 'lastmod': '2004-10-01T18:23:17+00:00'},\n-                {'loc': 'http://www.example.com/sitemap2.xml.gz', 'lastmod': '2005-01-01'},\n-            ]\n+                {\n+                    \"loc\": \"http://www.example.com/sitemap1.xml.gz\",\n+                    \"lastmod\": \"2004-10-01T18:23:17+00:00\",\n+                },\n+                {\n+                    \"loc\": \"http://www.example.com/sitemap2.xml.gz\",\n+                    \"lastmod\": \"2005-01-01\",\n+                },\n+            ],\n         )\n \n     def test_sitemap_strip(self):\n         \"\"\"Assert we can deal with trailing spaces inside <loc> tags - we've\n         seen those\n         \"\"\"\n-        s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        s = Sitemap(\n+            b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n   <url>\n     <loc> http://www.example.com/</loc>\n@@ -69,19 +88,26 @@ class SitemapTest(unittest.TestCase):\n     <lastmod />\n   </url>\n </urlset>\n-\"\"\")\n+\"\"\"\n+        )\n         self.assertEqual(\n             list(s),\n             [\n-                {'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'},\n-                {'loc': 'http://www.example.com/2', 'lastmod': ''},\n-            ]\n+                {\n+                    \"priority\": \"1\",\n+                    \"loc\": \"http://www.example.com/\",\n+                    \"lastmod\": \"2009-08-16\",\n+                    \"changefreq\": \"daily\",\n+                },\n+                {\"loc\": \"http://www.example.com/2\", \"lastmod\": \"\"},\n+            ],\n         )\n \n     def test_sitemap_wrong_ns(self):\n         \"\"\"We have seen sitemaps with wrongs ns. Presumably, Google still works\n         with these, though is not 100% confirmed\"\"\"\n-        s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        s = Sitemap(\n+            b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n   <url xmlns=\"\">\n     <loc> http://www.example.com/</loc>\n@@ -94,19 +120,26 @@ class SitemapTest(unittest.TestCase):\n     <lastmod />\n   </url>\n </urlset>\n-\"\"\")\n+\"\"\"\n+        )\n         self.assertEqual(\n             list(s),\n             [\n-                {'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'},\n-                {'loc': 'http://www.example.com/2', 'lastmod': ''},\n-            ]\n+                {\n+                    \"priority\": \"1\",\n+                    \"loc\": \"http://www.example.com/\",\n+                    \"lastmod\": \"2009-08-16\",\n+                    \"changefreq\": \"daily\",\n+                },\n+                {\"loc\": \"http://www.example.com/2\", \"lastmod\": \"\"},\n+            ],\n         )\n \n     def test_sitemap_wrong_ns2(self):\n         \"\"\"We have seen sitemaps with wrongs ns. Presumably, Google still works\n         with these, though is not 100% confirmed\"\"\"\n-        s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        s = Sitemap(\n+            b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <urlset>\n   <url xmlns=\"\">\n     <loc> http://www.example.com/</loc>\n@@ -119,14 +152,20 @@ class SitemapTest(unittest.TestCase):\n     <lastmod />\n   </url>\n </urlset>\n-\"\"\")\n-        assert s.type == 'urlset'\n+\"\"\"\n+        )\n+        assert s.type == \"urlset\"\n         self.assertEqual(\n             list(s),\n             [\n-                {'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'},\n-                {'loc': 'http://www.example.com/2', 'lastmod': ''},\n-            ]\n+                {\n+                    \"priority\": \"1\",\n+                    \"loc\": \"http://www.example.com/\",\n+                    \"lastmod\": \"2009-08-16\",\n+                    \"changefreq\": \"daily\",\n+                },\n+                {\"loc\": \"http://www.example.com/2\", \"lastmod\": \"\"},\n+            ],\n         )\n \n     def test_sitemap_urls_from_robots(self):\n@@ -148,15 +187,20 @@ Sitemap: /sitemap-relative-url.xml\n Disallow: /forum/search/\n Disallow: /forum/active/\n \"\"\"\n-        self.assertEqual(list(sitemap_urls_from_robots(robots, base_url='http://example.com')),\n-                         ['http://example.com/sitemap.xml',\n-                          'http://example.com/sitemap-product-index.xml',\n-                          'http://example.com/sitemap-uppercase.xml',\n-                          'http://example.com/sitemap-relative-url.xml'])\n+        self.assertEqual(\n+            list(sitemap_urls_from_robots(robots, base_url=\"http://example.com\")),\n+            [\n+                \"http://example.com/sitemap.xml\",\n+                \"http://example.com/sitemap-product-index.xml\",\n+                \"http://example.com/sitemap-uppercase.xml\",\n+                \"http://example.com/sitemap-relative-url.xml\",\n+            ],\n+        )\n \n     def test_sitemap_blanklines(self):\n         \"\"\"Assert we can deal with starting blank lines before <xml> tag\"\"\"\n-        s = Sitemap(b\"\"\"\n+        s = Sitemap(\n+            b\"\"\"\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n \n@@ -178,29 +222,34 @@ Disallow: /forum/active/\n \n <!-- end cache -->\n </sitemapindex>\n-\"\"\")\n-        self.assertEqual(list(s), [\n-            {'lastmod': '2013-07-15', 'loc': 'http://www.example.com/sitemap1.xml'},\n-            {'lastmod': '2013-07-15', 'loc': 'http://www.example.com/sitemap2.xml'},\n-            {'lastmod': '2013-07-15', 'loc': 'http://www.example.com/sitemap3.xml'},\n-        ])\n+\"\"\"\n+        )\n+        self.assertEqual(\n+            list(s),\n+            [\n+                {\"lastmod\": \"2013-07-15\", \"loc\": \"http://www.example.com/sitemap1.xml\"},\n+                {\"lastmod\": \"2013-07-15\", \"loc\": \"http://www.example.com/sitemap2.xml\"},\n+                {\"lastmod\": \"2013-07-15\", \"loc\": \"http://www.example.com/sitemap3.xml\"},\n+            ],\n+        )\n \n     def test_comment(self):\n-        s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        s = Sitemap(\n+            b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n     <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n         xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n         <url>\n             <loc>http://www.example.com/</loc>\n             <!-- this is a comment on which the parser might raise an exception if implemented incorrectly -->\n         </url>\n-    </urlset>\"\"\")\n+    </urlset>\"\"\"\n+        )\n \n-        self.assertEqual(list(s), [\n-            {'loc': 'http://www.example.com/'}\n-        ])\n+        self.assertEqual(list(s), [{\"loc\": \"http://www.example.com/\"}])\n \n     def test_alternate(self):\n-        s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        s = Sitemap(\n+            b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n     <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n         xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n         <url>\n@@ -213,24 +262,26 @@ Disallow: /forum/active/\n                 href=\"http://www.example.com/english/\"/>\n             <xhtml:link rel=\"alternate\" hreflang=\"en\"/><!-- wrong tag without href -->\n         </url>\n-    </urlset>\"\"\")\n+    </urlset>\"\"\"\n+        )\n \n         self.assertEqual(\n             list(s),\n             [\n                 {\n-                    'loc': 'http://www.example.com/english/',\n-                    'alternate': [\n-                        'http://www.example.com/deutsch/',\n-                        'http://www.example.com/schweiz-deutsch/',\n-                        'http://www.example.com/english/',\n+                    \"loc\": \"http://www.example.com/english/\",\n+                    \"alternate\": [\n+                        \"http://www.example.com/deutsch/\",\n+                        \"http://www.example.com/schweiz-deutsch/\",\n+                        \"http://www.example.com/english/\",\n                     ],\n                 }\n-            ]\n+            ],\n         )\n \n     def test_xml_entity_expansion(self):\n-        s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+        s = Sitemap(\n+            b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n           <!DOCTYPE foo [\n           <!ELEMENT foo ANY >\n           <!ENTITY xxe SYSTEM \"file:///etc/passwd\" >\n@@ -240,10 +291,11 @@ Disallow: /forum/active/\n               <loc>http://127.0.0.1:8000/&xxe;</loc>\n             </url>\n           </urlset>\n-        \"\"\")\n+        \"\"\"\n+        )\n \n-        self.assertEqual(list(s), [{'loc': 'http://127.0.0.1:8000/'}])\n+        self.assertEqual(list(s), [{\"loc\": \"http://127.0.0.1:8000/\"}])\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -7,18 +7,17 @@ from scrapy.utils.spider import iterate_spider_output, iter_spider_classes\n \n \n class MySpider1(Spider):\n-    name = 'myspider1'\n+    name = \"myspider1\"\n \n \n class MySpider2(Spider):\n-    name = 'myspider2'\n+    name = \"myspider2\"\n \n \n class UtilsSpidersTestCase(unittest.TestCase):\n-\n     def test_iterate_spider_output(self):\n         i = Item()\n-        r = Request('http://scrapytest.org')\n+        r = Request(\"http://scrapytest.org\")\n         o = object()\n \n         self.assertEqual(list(iterate_spider_output(i)), [i])\n@@ -28,6 +27,7 @@ class UtilsSpidersTestCase(unittest.TestCase):\n \n     def test_iter_spider_classes(self):\n         import tests.test_utils_spider\n+\n         it = iter_spider_classes(tests.test_utils_spider)\n         self.assertEqual(set(it), {MySpider1, MySpider2})\n \n\n@@ -5,11 +5,10 @@ import unittest\n from scrapy.utils.template import render_templatefile\n \n \n-__doctests__ = ['scrapy.utils.template']\n+__doctests__ = [\"scrapy.utils.template\"]\n \n \n class UtilsRenderTemplateFileTestCase(unittest.TestCase):\n-\n     def setUp(self):\n         self.tmp_path = mkdtemp()\n \n@@ -18,24 +17,24 @@ class UtilsRenderTemplateFileTestCase(unittest.TestCase):\n \n     def test_simple_render(self):\n \n-        context = dict(project_name='proj', name='spi', classname='TheSpider')\n-        template = 'from ${project_name}.spiders.${name} import ${classname}'\n-        rendered = 'from proj.spiders.spi import TheSpider'\n+        context = dict(project_name=\"proj\", name=\"spi\", classname=\"TheSpider\")\n+        template = \"from ${project_name}.spiders.${name} import ${classname}\"\n+        rendered = \"from proj.spiders.spi import TheSpider\"\n \n-        template_path = Path(self.tmp_path, 'templ.py.tmpl')\n-        render_path = Path(self.tmp_path, 'templ.py')\n+        template_path = Path(self.tmp_path, \"templ.py.tmpl\")\n+        render_path = Path(self.tmp_path, \"templ.py\")\n \n-        template_path.write_text(template, encoding='utf8')\n+        template_path.write_text(template, encoding=\"utf8\")\n         assert template_path.is_file()  # Failure of test itself\n \n         render_templatefile(template_path, **context)\n \n         self.assertFalse(template_path.exists())\n-        self.assertEqual(render_path.read_text(encoding='utf8'), rendered)\n+        self.assertEqual(render_path.read_text(encoding=\"utf8\"), rendered)\n \n         render_path.unlink()\n         assert not render_path.exists()  # Failure of test itself\n \n \n-if '__main__' == __name__:\n+if \"__main__\" == __name__:\n     unittest.main()\n\n@@ -17,7 +17,6 @@ class Bar(trackref.object_ref):\n \n \n class TrackrefTestCase(unittest.TestCase):\n-\n     def setUp(self):\n         trackref.live_refs.clear()\n \n@@ -27,34 +26,39 @@ class TrackrefTestCase(unittest.TestCase):\n         o3 = Foo()  # NOQA\n         self.assertEqual(\n             trackref.format_live_refs(),\n-            '''\\\n+            \"\"\"\\\n Live References\n \n Bar                                 1   oldest: 0s ago\n Foo                                 2   oldest: 0s ago\n-''')\n+\"\"\",\n+        )\n \n         self.assertEqual(\n             trackref.format_live_refs(ignore=Foo),\n-            '''\\\n+            \"\"\"\\\n Live References\n \n Bar                                 1   oldest: 0s ago\n-''')\n+\"\"\",\n+        )\n \n-    @mock.patch('sys.stdout', new_callable=StringIO)\n+    @mock.patch(\"sys.stdout\", new_callable=StringIO)\n     def test_print_live_refs_empty(self, stdout):\n         trackref.print_live_refs()\n-        self.assertEqual(stdout.getvalue(), 'Live References\\n\\n\\n')\n+        self.assertEqual(stdout.getvalue(), \"Live References\\n\\n\\n\")\n \n-    @mock.patch('sys.stdout', new_callable=StringIO)\n+    @mock.patch(\"sys.stdout\", new_callable=StringIO)\n     def test_print_live_refs_with_objects(self, stdout):\n         o1 = Foo()  # NOQA\n         trackref.print_live_refs()\n-        self.assertEqual(stdout.getvalue(), '''\\\n+        self.assertEqual(\n+            stdout.getvalue(),\n+            \"\"\"\\\n Live References\n \n-Foo                                 1   oldest: 0s ago\\n\\n''')\n+Foo                                 1   oldest: 0s ago\\n\\n\"\"\",\n+        )\n \n     def test_get_oldest(self):\n         o1 = Foo()  # NOQA\n@@ -68,18 +72,18 @@ Foo                                 1   oldest: 0s ago\\n\\n''')\n             sleep(0.01)\n             o3_time = time()\n         if o3_time <= o1_time:\n-            raise SkipTest('time.time is not precise enough')\n+            raise SkipTest(\"time.time is not precise enough\")\n \n         o3 = Foo()  # NOQA\n-        self.assertIs(trackref.get_oldest('Foo'), o1)\n-        self.assertIs(trackref.get_oldest('Bar'), o2)\n-        self.assertIsNone(trackref.get_oldest('XXX'))\n+        self.assertIs(trackref.get_oldest(\"Foo\"), o1)\n+        self.assertIs(trackref.get_oldest(\"Bar\"), o2)\n+        self.assertIsNone(trackref.get_oldest(\"XXX\"))\n \n     def test_iter_all(self):\n         o1 = Foo()  # NOQA\n         o2 = Bar()  # NOQA\n         o3 = Foo()  # NOQA\n         self.assertEqual(\n-            set(trackref.iter_all('Foo')),\n+            set(trackref.iter_all(\"Foo\")),\n             {o1, o3},\n         )\n\n@@ -13,207 +13,301 @@ from scrapy.utils.url import (\n     url_has_any_extension,\n )\n \n-__doctests__ = ['scrapy.utils.url']\n+__doctests__ = [\"scrapy.utils.url\"]\n \n \n class UrlUtilsTest(unittest.TestCase):\n-\n     def test_url_is_from_any_domain(self):\n-        url = 'http://www.wheele-bin-art.co.uk/get/product/123'\n-        self.assertTrue(url_is_from_any_domain(url, ['wheele-bin-art.co.uk']))\n-        self.assertFalse(url_is_from_any_domain(url, ['art.co.uk']))\n+        url = \"http://www.wheele-bin-art.co.uk/get/product/123\"\n+        self.assertTrue(url_is_from_any_domain(url, [\"wheele-bin-art.co.uk\"]))\n+        self.assertFalse(url_is_from_any_domain(url, [\"art.co.uk\"]))\n \n-        url = 'http://wheele-bin-art.co.uk/get/product/123'\n-        self.assertTrue(url_is_from_any_domain(url, ['wheele-bin-art.co.uk']))\n-        self.assertFalse(url_is_from_any_domain(url, ['art.co.uk']))\n+        url = \"http://wheele-bin-art.co.uk/get/product/123\"\n+        self.assertTrue(url_is_from_any_domain(url, [\"wheele-bin-art.co.uk\"]))\n+        self.assertFalse(url_is_from_any_domain(url, [\"art.co.uk\"]))\n \n-        url = 'http://www.Wheele-Bin-Art.co.uk/get/product/123'\n-        self.assertTrue(url_is_from_any_domain(url, ['wheele-bin-art.CO.UK']))\n-        self.assertTrue(url_is_from_any_domain(url, ['WHEELE-BIN-ART.CO.UK']))\n+        url = \"http://www.Wheele-Bin-Art.co.uk/get/product/123\"\n+        self.assertTrue(url_is_from_any_domain(url, [\"wheele-bin-art.CO.UK\"]))\n+        self.assertTrue(url_is_from_any_domain(url, [\"WHEELE-BIN-ART.CO.UK\"]))\n \n-        url = 'http://192.169.0.15:8080/mypage.html'\n-        self.assertTrue(url_is_from_any_domain(url, ['192.169.0.15:8080']))\n-        self.assertFalse(url_is_from_any_domain(url, ['192.169.0.15']))\n+        url = \"http://192.169.0.15:8080/mypage.html\"\n+        self.assertTrue(url_is_from_any_domain(url, [\"192.169.0.15:8080\"]))\n+        self.assertFalse(url_is_from_any_domain(url, [\"192.169.0.15\"]))\n \n         url = (\n-            'javascript:%20document.orderform_2581_1190810811.mode.value=%27add%27;%20'\n-            'javascript:%20document.orderform_2581_1190810811.submit%28%29'\n+            \"javascript:%20document.orderform_2581_1190810811.mode.value=%27add%27;%20\"\n+            \"javascript:%20document.orderform_2581_1190810811.submit%28%29\"\n+        )\n+        self.assertFalse(url_is_from_any_domain(url, [\"testdomain.com\"]))\n+        self.assertFalse(\n+            url_is_from_any_domain(url + \".testdomain.com\", [\"testdomain.com\"])\n         )\n-        self.assertFalse(url_is_from_any_domain(url, ['testdomain.com']))\n-        self.assertFalse(url_is_from_any_domain(url + '.testdomain.com', ['testdomain.com']))\n \n     def test_url_is_from_spider(self):\n-        spider = Spider(name='example.com')\n-        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))\n-        self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', spider))\n-        self.assertFalse(url_is_from_spider('http://www.example.org/some/page.html', spider))\n-        self.assertFalse(url_is_from_spider('http://www.example.net/some/page.html', spider))\n+        spider = Spider(name=\"example.com\")\n+        self.assertTrue(\n+            url_is_from_spider(\"http://www.example.com/some/page.html\", spider)\n+        )\n+        self.assertTrue(\n+            url_is_from_spider(\"http://sub.example.com/some/page.html\", spider)\n+        )\n+        self.assertFalse(\n+            url_is_from_spider(\"http://www.example.org/some/page.html\", spider)\n+        )\n+        self.assertFalse(\n+            url_is_from_spider(\"http://www.example.net/some/page.html\", spider)\n+        )\n \n     def test_url_is_from_spider_class_attributes(self):\n         class MySpider(Spider):\n-            name = 'example.com'\n-        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', MySpider))\n-        self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', MySpider))\n-        self.assertFalse(url_is_from_spider('http://www.example.org/some/page.html', MySpider))\n-        self.assertFalse(url_is_from_spider('http://www.example.net/some/page.html', MySpider))\n+            name = \"example.com\"\n+\n+        self.assertTrue(\n+            url_is_from_spider(\"http://www.example.com/some/page.html\", MySpider)\n+        )\n+        self.assertTrue(\n+            url_is_from_spider(\"http://sub.example.com/some/page.html\", MySpider)\n+        )\n+        self.assertFalse(\n+            url_is_from_spider(\"http://www.example.org/some/page.html\", MySpider)\n+        )\n+        self.assertFalse(\n+            url_is_from_spider(\"http://www.example.net/some/page.html\", MySpider)\n+        )\n \n     def test_url_is_from_spider_with_allowed_domains(self):\n-        spider = Spider(name='example.com', allowed_domains=['example.org', 'example.net'])\n-        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))\n-        self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', spider))\n-        self.assertTrue(url_is_from_spider('http://example.com/some/page.html', spider))\n-        self.assertTrue(url_is_from_spider('http://www.example.org/some/page.html', spider))\n-        self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', spider))\n-        self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', spider))\n+        spider = Spider(\n+            name=\"example.com\", allowed_domains=[\"example.org\", \"example.net\"]\n+        )\n+        self.assertTrue(\n+            url_is_from_spider(\"http://www.example.com/some/page.html\", spider)\n+        )\n+        self.assertTrue(\n+            url_is_from_spider(\"http://sub.example.com/some/page.html\", spider)\n+        )\n+        self.assertTrue(url_is_from_spider(\"http://example.com/some/page.html\", spider))\n+        self.assertTrue(\n+            url_is_from_spider(\"http://www.example.org/some/page.html\", spider)\n+        )\n+        self.assertTrue(\n+            url_is_from_spider(\"http://www.example.net/some/page.html\", spider)\n+        )\n+        self.assertFalse(\n+            url_is_from_spider(\"http://www.example.us/some/page.html\", spider)\n+        )\n \n-        spider = Spider(name='example.com', allowed_domains={'example.com', 'example.net'})\n-        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))\n+        spider = Spider(\n+            name=\"example.com\", allowed_domains={\"example.com\", \"example.net\"}\n+        )\n+        self.assertTrue(\n+            url_is_from_spider(\"http://www.example.com/some/page.html\", spider)\n+        )\n \n-        spider = Spider(name='example.com', allowed_domains=('example.com', 'example.net'))\n-        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))\n+        spider = Spider(\n+            name=\"example.com\", allowed_domains=(\"example.com\", \"example.net\")\n+        )\n+        self.assertTrue(\n+            url_is_from_spider(\"http://www.example.com/some/page.html\", spider)\n+        )\n \n     def test_url_is_from_spider_with_allowed_domains_class_attributes(self):\n         class MySpider(Spider):\n-            name = 'example.com'\n-            allowed_domains = ('example.org', 'example.net')\n-        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', MySpider))\n-        self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', MySpider))\n-        self.assertTrue(url_is_from_spider('http://example.com/some/page.html', MySpider))\n-        self.assertTrue(url_is_from_spider('http://www.example.org/some/page.html', MySpider))\n-        self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', MySpider))\n-        self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', MySpider))\n+            name = \"example.com\"\n+            allowed_domains = (\"example.org\", \"example.net\")\n+\n+        self.assertTrue(\n+            url_is_from_spider(\"http://www.example.com/some/page.html\", MySpider)\n+        )\n+        self.assertTrue(\n+            url_is_from_spider(\"http://sub.example.com/some/page.html\", MySpider)\n+        )\n+        self.assertTrue(\n+            url_is_from_spider(\"http://example.com/some/page.html\", MySpider)\n+        )\n+        self.assertTrue(\n+            url_is_from_spider(\"http://www.example.org/some/page.html\", MySpider)\n+        )\n+        self.assertTrue(\n+            url_is_from_spider(\"http://www.example.net/some/page.html\", MySpider)\n+        )\n+        self.assertFalse(\n+            url_is_from_spider(\"http://www.example.us/some/page.html\", MySpider)\n+        )\n \n     def test_url_has_any_extension(self):\n-        deny_extensions = {'.' + e for e in arg_to_iter(IGNORED_EXTENSIONS)}\n-        self.assertTrue(url_has_any_extension(\"http://www.example.com/archive.tar.gz\", deny_extensions))\n-        self.assertTrue(url_has_any_extension(\"http://www.example.com/page.doc\", deny_extensions))\n-        self.assertTrue(url_has_any_extension(\"http://www.example.com/page.pdf\", deny_extensions))\n-        self.assertFalse(url_has_any_extension(\"http://www.example.com/page.htm\", deny_extensions))\n-        self.assertFalse(url_has_any_extension(\"http://www.example.com/\", deny_extensions))\n-        self.assertFalse(url_has_any_extension(\"http://www.example.com/page.doc.html\", deny_extensions))\n+        deny_extensions = {\".\" + e for e in arg_to_iter(IGNORED_EXTENSIONS)}\n+        self.assertTrue(\n+            url_has_any_extension(\n+                \"http://www.example.com/archive.tar.gz\", deny_extensions\n+            )\n+        )\n+        self.assertTrue(\n+            url_has_any_extension(\"http://www.example.com/page.doc\", deny_extensions)\n+        )\n+        self.assertTrue(\n+            url_has_any_extension(\"http://www.example.com/page.pdf\", deny_extensions)\n+        )\n+        self.assertFalse(\n+            url_has_any_extension(\"http://www.example.com/page.htm\", deny_extensions)\n+        )\n+        self.assertFalse(\n+            url_has_any_extension(\"http://www.example.com/\", deny_extensions)\n+        )\n+        self.assertFalse(\n+            url_has_any_extension(\n+                \"http://www.example.com/page.doc.html\", deny_extensions\n+            )\n+        )\n \n \n class AddHttpIfNoScheme(unittest.TestCase):\n-\n     def test_add_scheme(self):\n-        self.assertEqual(add_http_if_no_scheme('www.example.com'), 'http://www.example.com')\n+        self.assertEqual(\n+            add_http_if_no_scheme(\"www.example.com\"), \"http://www.example.com\"\n+        )\n \n     def test_without_subdomain(self):\n-        self.assertEqual(add_http_if_no_scheme('example.com'), 'http://example.com')\n+        self.assertEqual(add_http_if_no_scheme(\"example.com\"), \"http://example.com\")\n \n     def test_path(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('www.example.com/some/page.html'),\n-            'http://www.example.com/some/page.html')\n+            add_http_if_no_scheme(\"www.example.com/some/page.html\"),\n+            \"http://www.example.com/some/page.html\",\n+        )\n \n     def test_port(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('www.example.com:80'),\n-            'http://www.example.com:80')\n+            add_http_if_no_scheme(\"www.example.com:80\"), \"http://www.example.com:80\"\n+        )\n \n     def test_fragment(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('www.example.com/some/page#frag'),\n-            'http://www.example.com/some/page#frag')\n+            add_http_if_no_scheme(\"www.example.com/some/page#frag\"),\n+            \"http://www.example.com/some/page#frag\",\n+        )\n \n     def test_query(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('www.example.com/do?a=1&b=2&c=3'),\n-            'http://www.example.com/do?a=1&b=2&c=3')\n+            add_http_if_no_scheme(\"www.example.com/do?a=1&b=2&c=3\"),\n+            \"http://www.example.com/do?a=1&b=2&c=3\",\n+        )\n \n     def test_username_password(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('username:password@www.example.com'),\n-            'http://username:password@www.example.com')\n+            add_http_if_no_scheme(\"username:password@www.example.com\"),\n+            \"http://username:password@www.example.com\",\n+        )\n \n     def test_complete_url(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),\n-            'http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag')\n+            add_http_if_no_scheme(\n+                \"username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\"\n+            ),\n+            \"http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\",\n+        )\n \n     def test_preserve_http(self):\n-        self.assertEqual(add_http_if_no_scheme('http://www.example.com'), 'http://www.example.com')\n+        self.assertEqual(\n+            add_http_if_no_scheme(\"http://www.example.com\"), \"http://www.example.com\"\n+        )\n \n     def test_preserve_http_without_subdomain(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('http://example.com'),\n-            'http://example.com')\n+            add_http_if_no_scheme(\"http://example.com\"), \"http://example.com\"\n+        )\n \n     def test_preserve_http_path(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('http://www.example.com/some/page.html'),\n-            'http://www.example.com/some/page.html')\n+            add_http_if_no_scheme(\"http://www.example.com/some/page.html\"),\n+            \"http://www.example.com/some/page.html\",\n+        )\n \n     def test_preserve_http_port(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('http://www.example.com:80'),\n-            'http://www.example.com:80')\n+            add_http_if_no_scheme(\"http://www.example.com:80\"),\n+            \"http://www.example.com:80\",\n+        )\n \n     def test_preserve_http_fragment(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('http://www.example.com/some/page#frag'),\n-            'http://www.example.com/some/page#frag')\n+            add_http_if_no_scheme(\"http://www.example.com/some/page#frag\"),\n+            \"http://www.example.com/some/page#frag\",\n+        )\n \n     def test_preserve_http_query(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('http://www.example.com/do?a=1&b=2&c=3'),\n-            'http://www.example.com/do?a=1&b=2&c=3')\n+            add_http_if_no_scheme(\"http://www.example.com/do?a=1&b=2&c=3\"),\n+            \"http://www.example.com/do?a=1&b=2&c=3\",\n+        )\n \n     def test_preserve_http_username_password(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('http://username:password@www.example.com'),\n-            'http://username:password@www.example.com')\n+            add_http_if_no_scheme(\"http://username:password@www.example.com\"),\n+            \"http://username:password@www.example.com\",\n+        )\n \n     def test_preserve_http_complete_url(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),\n-            'http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag')\n+            add_http_if_no_scheme(\n+                \"http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\"\n+            ),\n+            \"http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\",\n+        )\n \n     def test_protocol_relative(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('//www.example.com'), 'http://www.example.com')\n+            add_http_if_no_scheme(\"//www.example.com\"), \"http://www.example.com\"\n+        )\n \n     def test_protocol_relative_without_subdomain(self):\n-        self.assertEqual(\n-            add_http_if_no_scheme('//example.com'), 'http://example.com')\n+        self.assertEqual(add_http_if_no_scheme(\"//example.com\"), \"http://example.com\")\n \n     def test_protocol_relative_path(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('//www.example.com/some/page.html'),\n-            'http://www.example.com/some/page.html')\n+            add_http_if_no_scheme(\"//www.example.com/some/page.html\"),\n+            \"http://www.example.com/some/page.html\",\n+        )\n \n     def test_protocol_relative_port(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('//www.example.com:80'),\n-            'http://www.example.com:80')\n+            add_http_if_no_scheme(\"//www.example.com:80\"), \"http://www.example.com:80\"\n+        )\n \n     def test_protocol_relative_fragment(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('//www.example.com/some/page#frag'),\n-            'http://www.example.com/some/page#frag')\n+            add_http_if_no_scheme(\"//www.example.com/some/page#frag\"),\n+            \"http://www.example.com/some/page#frag\",\n+        )\n \n     def test_protocol_relative_query(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('//www.example.com/do?a=1&b=2&c=3'),\n-            'http://www.example.com/do?a=1&b=2&c=3')\n+            add_http_if_no_scheme(\"//www.example.com/do?a=1&b=2&c=3\"),\n+            \"http://www.example.com/do?a=1&b=2&c=3\",\n+        )\n \n     def test_protocol_relative_username_password(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('//username:password@www.example.com'),\n-            'http://username:password@www.example.com')\n+            add_http_if_no_scheme(\"//username:password@www.example.com\"),\n+            \"http://username:password@www.example.com\",\n+        )\n \n     def test_protocol_relative_complete_url(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('//username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),\n-            'http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag')\n+            add_http_if_no_scheme(\n+                \"//username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\"\n+            ),\n+            \"http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag\",\n+        )\n \n     def test_preserve_https(self):\n         self.assertEqual(\n-            add_http_if_no_scheme('https://www.example.com'),\n-            'https://www.example.com')\n+            add_http_if_no_scheme(\"https://www.example.com\"), \"https://www.example.com\"\n+        )\n \n     def test_preserve_ftp(self):\n-        self.assertEqual(add_http_if_no_scheme('ftp://www.example.com'), 'ftp://www.example.com')\n+        self.assertEqual(\n+            add_http_if_no_scheme(\"ftp://www.example.com\"), \"ftp://www.example.com\"\n+        )\n \n \n class GuessSchemeTest(unittest.TestCase):\n@@ -223,8 +317,10 @@ class GuessSchemeTest(unittest.TestCase):\n def create_guess_scheme_t(args):\n     def do_expected(self):\n         url = guess_scheme(args[0])\n-        assert url.startswith(args[1]), \\\n-            f'Wrong scheme guessed: for `{args[0]}` got `{url}`, expected `{args[1]}...`'\n+        assert url.startswith(\n+            args[1]\n+        ), f\"Wrong scheme guessed: for `{args[0]}` got `{url}`, expected `{args[1]}...`\"\n+\n     return do_expected\n \n \n@@ -233,100 +329,100 @@ def create_skipped_scheme_t(args):\n         raise unittest.SkipTest(args[2])\n         url = guess_scheme(args[0])\n         assert url.startswith(args[1])\n+\n     return do_expected\n \n \n for k, args in enumerate(\n     [\n-        ('/index', 'file://'),\n-        ('/index.html', 'file://'),\n-        ('./index.html', 'file://'),\n-        ('../index.html', 'file://'),\n-        ('../../index.html', 'file://'),\n-        ('./data/index.html', 'file://'),\n-        ('.hidden/data/index.html', 'file://'),\n-        ('/home/user/www/index.html', 'file://'),\n-        ('//home/user/www/index.html', 'file://'),\n-        ('file:///home/user/www/index.html', 'file://'),\n-\n-        ('index.html', 'http://'),\n-        ('example.com', 'http://'),\n-        ('www.example.com', 'http://'),\n-        ('www.example.com/index.html', 'http://'),\n-        ('http://example.com', 'http://'),\n-        ('http://example.com/index.html', 'http://'),\n-        ('localhost', 'http://'),\n-        ('localhost/index.html', 'http://'),\n-\n+        (\"/index\", \"file://\"),\n+        (\"/index.html\", \"file://\"),\n+        (\"./index.html\", \"file://\"),\n+        (\"../index.html\", \"file://\"),\n+        (\"../../index.html\", \"file://\"),\n+        (\"./data/index.html\", \"file://\"),\n+        (\".hidden/data/index.html\", \"file://\"),\n+        (\"/home/user/www/index.html\", \"file://\"),\n+        (\"//home/user/www/index.html\", \"file://\"),\n+        (\"file:///home/user/www/index.html\", \"file://\"),\n+        (\"index.html\", \"http://\"),\n+        (\"example.com\", \"http://\"),\n+        (\"www.example.com\", \"http://\"),\n+        (\"www.example.com/index.html\", \"http://\"),\n+        (\"http://example.com\", \"http://\"),\n+        (\"http://example.com/index.html\", \"http://\"),\n+        (\"localhost\", \"http://\"),\n+        (\"localhost/index.html\", \"http://\"),\n         # some corner cases (default to http://)\n-        ('/', 'http://'),\n-        ('.../test', 'http://'),\n+        (\"/\", \"http://\"),\n+        (\".../test\", \"http://\"),\n     ],\n     start=1,\n ):\n     t_method = create_guess_scheme_t(args)\n-    t_method.__name__ = f'test_uri_{k:03}'\n+    t_method.__name__ = f\"test_uri_{k:03}\"\n     setattr(GuessSchemeTest, t_method.__name__, t_method)\n \n # TODO: the following tests do not pass with current implementation\n for k, args in enumerate(\n     [\n         (\n-            r'C:\\absolute\\path\\to\\a\\file.html',\n-            'file://',\n-            'Windows filepath are not supported for scrapy shell',\n+            r\"C:\\absolute\\path\\to\\a\\file.html\",\n+            \"file://\",\n+            \"Windows filepath are not supported for scrapy shell\",\n         ),\n     ],\n     start=1,\n ):\n     t_method = create_skipped_scheme_t(args)\n-    t_method.__name__ = f'test_uri_skipped_{k:03}'\n+    t_method.__name__ = f\"test_uri_skipped_{k:03}\"\n     setattr(GuessSchemeTest, t_method.__name__, t_method)\n \n \n class StripUrl(unittest.TestCase):\n-\n     def test_noop(self):\n-        self.assertEqual(strip_url(\n-            'http://www.example.com/index.html'),\n-            'http://www.example.com/index.html')\n+        self.assertEqual(\n+            strip_url(\"http://www.example.com/index.html\"),\n+            \"http://www.example.com/index.html\",\n+        )\n \n     def test_noop_query_string(self):\n-        self.assertEqual(strip_url(\n-            'http://www.example.com/index.html?somekey=somevalue'),\n-            'http://www.example.com/index.html?somekey=somevalue')\n+        self.assertEqual(\n+            strip_url(\"http://www.example.com/index.html?somekey=somevalue\"),\n+            \"http://www.example.com/index.html?somekey=somevalue\",\n+        )\n \n     def test_fragments(self):\n-        self.assertEqual(strip_url(\n-            'http://www.example.com/index.html?somekey=somevalue#section', strip_fragment=False),\n-            'http://www.example.com/index.html?somekey=somevalue#section')\n+        self.assertEqual(\n+            strip_url(\n+                \"http://www.example.com/index.html?somekey=somevalue#section\",\n+                strip_fragment=False,\n+            ),\n+            \"http://www.example.com/index.html?somekey=somevalue#section\",\n+        )\n \n     def test_path(self):\n         for input_url, origin, output_url in [\n-            ('http://www.example.com/',\n-             False,\n-             'http://www.example.com/'),\n-\n-            ('http://www.example.com',\n-             False,\n-             'http://www.example.com'),\n-\n-            ('http://www.example.com',\n-             True,\n-             'http://www.example.com/'),\n+            (\"http://www.example.com/\", False, \"http://www.example.com/\"),\n+            (\"http://www.example.com\", False, \"http://www.example.com\"),\n+            (\"http://www.example.com\", True, \"http://www.example.com/\"),\n         ]:\n             self.assertEqual(strip_url(input_url, origin_only=origin), output_url)\n \n     def test_credentials(self):\n         for i, o in [\n-            ('http://username@www.example.com/index.html?somekey=somevalue#section',\n-             'http://www.example.com/index.html?somekey=somevalue'),\n-\n-            ('https://username:@www.example.com/index.html?somekey=somevalue#section',\n-             'https://www.example.com/index.html?somekey=somevalue'),\n-\n-            ('ftp://username:password@www.example.com/index.html?somekey=somevalue#section',\n-             'ftp://www.example.com/index.html?somekey=somevalue'),\n+            (\n+                \"http://username@www.example.com/index.html?somekey=somevalue#section\",\n+                \"http://www.example.com/index.html?somekey=somevalue\",\n+            ),\n+            (\n+                \"https://username:@www.example.com/index.html?somekey=somevalue#section\",\n+                \"https://www.example.com/index.html?somekey=somevalue\",\n+            ),\n+            (\n+                \"ftp://username:password@www.example.com/index.html?somekey=somevalue#section\",\n+                \"ftp://www.example.com/index.html?somekey=somevalue\",\n+            ),\n         ]:\n             self.assertEqual(strip_url(i, strip_credentials=True), o)\n \n@@ -334,124 +430,163 @@ class StripUrl(unittest.TestCase):\n         for i, o in [\n             # user: \"username@\"\n             # password: none\n-            ('http://username%40@www.example.com/index.html?somekey=somevalue#section',\n-             'http://www.example.com/index.html?somekey=somevalue'),\n-\n+            (\n+                \"http://username%40@www.example.com/index.html?somekey=somevalue#section\",\n+                \"http://www.example.com/index.html?somekey=somevalue\",\n+            ),\n             # user: \"username:pass\"\n             # password: \"\"\n-            ('https://username%3Apass:@www.example.com/index.html?somekey=somevalue#section',\n-             'https://www.example.com/index.html?somekey=somevalue'),\n-\n+            (\n+                \"https://username%3Apass:@www.example.com/index.html?somekey=somevalue#section\",\n+                \"https://www.example.com/index.html?somekey=somevalue\",\n+            ),\n             # user: \"me\"\n             # password: \"user@domain.com\"\n-            ('ftp://me:user%40domain.com@www.example.com/index.html?somekey=somevalue#section',\n-             'ftp://www.example.com/index.html?somekey=somevalue'),\n+            (\n+                \"ftp://me:user%40domain.com@www.example.com/index.html?somekey=somevalue#section\",\n+                \"ftp://www.example.com/index.html?somekey=somevalue\",\n+            ),\n         ]:\n             self.assertEqual(strip_url(i, strip_credentials=True), o)\n \n     def test_default_ports_creds_off(self):\n         for i, o in [\n-            ('http://username:password@www.example.com:80/index.html?somekey=somevalue#section',\n-             'http://www.example.com/index.html?somekey=somevalue'),\n-\n-            ('http://username:password@www.example.com:8080/index.html#section',\n-             'http://www.example.com:8080/index.html'),\n-\n-            ('http://username:password@www.example.com:443/index.html?somekey=somevalue&someotherkey=sov#section',\n-             'http://www.example.com:443/index.html?somekey=somevalue&someotherkey=sov'),\n-\n-            ('https://username:password@www.example.com:443/index.html',\n-             'https://www.example.com/index.html'),\n-\n-            ('https://username:password@www.example.com:442/index.html',\n-             'https://www.example.com:442/index.html'),\n-\n-            ('https://username:password@www.example.com:80/index.html',\n-             'https://www.example.com:80/index.html'),\n-\n-            ('ftp://username:password@www.example.com:21/file.txt',\n-             'ftp://www.example.com/file.txt'),\n-\n-            ('ftp://username:password@www.example.com:221/file.txt',\n-             'ftp://www.example.com:221/file.txt'),\n+            (\n+                \"http://username:password@www.example.com:80/index.html?somekey=somevalue#section\",\n+                \"http://www.example.com/index.html?somekey=somevalue\",\n+            ),\n+            (\n+                \"http://username:password@www.example.com:8080/index.html#section\",\n+                \"http://www.example.com:8080/index.html\",\n+            ),\n+            (\n+                \"http://username:password@www.example.com:443/index.html?somekey=somevalue&someotherkey=sov#section\",\n+                \"http://www.example.com:443/index.html?somekey=somevalue&someotherkey=sov\",\n+            ),\n+            (\n+                \"https://username:password@www.example.com:443/index.html\",\n+                \"https://www.example.com/index.html\",\n+            ),\n+            (\n+                \"https://username:password@www.example.com:442/index.html\",\n+                \"https://www.example.com:442/index.html\",\n+            ),\n+            (\n+                \"https://username:password@www.example.com:80/index.html\",\n+                \"https://www.example.com:80/index.html\",\n+            ),\n+            (\n+                \"ftp://username:password@www.example.com:21/file.txt\",\n+                \"ftp://www.example.com/file.txt\",\n+            ),\n+            (\n+                \"ftp://username:password@www.example.com:221/file.txt\",\n+                \"ftp://www.example.com:221/file.txt\",\n+            ),\n         ]:\n             self.assertEqual(strip_url(i), o)\n \n     def test_default_ports(self):\n         for i, o in [\n-            ('http://username:password@www.example.com:80/index.html',\n-             'http://username:password@www.example.com/index.html'),\n-\n-            ('http://username:password@www.example.com:8080/index.html',\n-             'http://username:password@www.example.com:8080/index.html'),\n-\n-            ('http://username:password@www.example.com:443/index.html',\n-             'http://username:password@www.example.com:443/index.html'),\n-\n-            ('https://username:password@www.example.com:443/index.html',\n-             'https://username:password@www.example.com/index.html'),\n-\n-            ('https://username:password@www.example.com:442/index.html',\n-             'https://username:password@www.example.com:442/index.html'),\n-\n-            ('https://username:password@www.example.com:80/index.html',\n-             'https://username:password@www.example.com:80/index.html'),\n-\n-            ('ftp://username:password@www.example.com:21/file.txt',\n-             'ftp://username:password@www.example.com/file.txt'),\n-\n-            ('ftp://username:password@www.example.com:221/file.txt',\n-             'ftp://username:password@www.example.com:221/file.txt'),\n+            (\n+                \"http://username:password@www.example.com:80/index.html\",\n+                \"http://username:password@www.example.com/index.html\",\n+            ),\n+            (\n+                \"http://username:password@www.example.com:8080/index.html\",\n+                \"http://username:password@www.example.com:8080/index.html\",\n+            ),\n+            (\n+                \"http://username:password@www.example.com:443/index.html\",\n+                \"http://username:password@www.example.com:443/index.html\",\n+            ),\n+            (\n+                \"https://username:password@www.example.com:443/index.html\",\n+                \"https://username:password@www.example.com/index.html\",\n+            ),\n+            (\n+                \"https://username:password@www.example.com:442/index.html\",\n+                \"https://username:password@www.example.com:442/index.html\",\n+            ),\n+            (\n+                \"https://username:password@www.example.com:80/index.html\",\n+                \"https://username:password@www.example.com:80/index.html\",\n+            ),\n+            (\n+                \"ftp://username:password@www.example.com:21/file.txt\",\n+                \"ftp://username:password@www.example.com/file.txt\",\n+            ),\n+            (\n+                \"ftp://username:password@www.example.com:221/file.txt\",\n+                \"ftp://username:password@www.example.com:221/file.txt\",\n+            ),\n         ]:\n-            self.assertEqual(strip_url(i, strip_default_port=True, strip_credentials=False), o)\n+            self.assertEqual(\n+                strip_url(i, strip_default_port=True, strip_credentials=False), o\n+            )\n \n     def test_default_ports_keep(self):\n         for i, o in [\n-            ('http://username:password@www.example.com:80/index.html?somekey=somevalue&someotherkey=sov#section',\n-             'http://username:password@www.example.com:80/index.html?somekey=somevalue&someotherkey=sov'),\n-\n-            ('http://username:password@www.example.com:8080/index.html?somekey=somevalue&someotherkey=sov#section',\n-             'http://username:password@www.example.com:8080/index.html?somekey=somevalue&someotherkey=sov'),\n-\n-            ('http://username:password@www.example.com:443/index.html',\n-             'http://username:password@www.example.com:443/index.html'),\n-\n-            ('https://username:password@www.example.com:443/index.html',\n-             'https://username:password@www.example.com:443/index.html'),\n-\n-            ('https://username:password@www.example.com:442/index.html',\n-             'https://username:password@www.example.com:442/index.html'),\n-\n-            ('https://username:password@www.example.com:80/index.html',\n-             'https://username:password@www.example.com:80/index.html'),\n-\n-            ('ftp://username:password@www.example.com:21/file.txt',\n-             'ftp://username:password@www.example.com:21/file.txt'),\n-\n-            ('ftp://username:password@www.example.com:221/file.txt',\n-             'ftp://username:password@www.example.com:221/file.txt'),\n+            (\n+                \"http://username:password@www.example.com:80/index.html?somekey=somevalue&someotherkey=sov#section\",\n+                \"http://username:password@www.example.com:80/index.html?somekey=somevalue&someotherkey=sov\",\n+            ),\n+            (\n+                \"http://username:password@www.example.com:8080/index.html?somekey=somevalue&someotherkey=sov#section\",\n+                \"http://username:password@www.example.com:8080/index.html?somekey=somevalue&someotherkey=sov\",\n+            ),\n+            (\n+                \"http://username:password@www.example.com:443/index.html\",\n+                \"http://username:password@www.example.com:443/index.html\",\n+            ),\n+            (\n+                \"https://username:password@www.example.com:443/index.html\",\n+                \"https://username:password@www.example.com:443/index.html\",\n+            ),\n+            (\n+                \"https://username:password@www.example.com:442/index.html\",\n+                \"https://username:password@www.example.com:442/index.html\",\n+            ),\n+            (\n+                \"https://username:password@www.example.com:80/index.html\",\n+                \"https://username:password@www.example.com:80/index.html\",\n+            ),\n+            (\n+                \"ftp://username:password@www.example.com:21/file.txt\",\n+                \"ftp://username:password@www.example.com:21/file.txt\",\n+            ),\n+            (\n+                \"ftp://username:password@www.example.com:221/file.txt\",\n+                \"ftp://username:password@www.example.com:221/file.txt\",\n+            ),\n         ]:\n-            self.assertEqual(strip_url(i, strip_default_port=False, strip_credentials=False), o)\n+            self.assertEqual(\n+                strip_url(i, strip_default_port=False, strip_credentials=False), o\n+            )\n \n     def test_origin_only(self):\n         for i, o in [\n-            ('http://username:password@www.example.com/index.html',\n-             'http://www.example.com/'),\n-\n-            ('http://username:password@www.example.com:80/foo/bar?query=value#somefrag',\n-             'http://www.example.com/'),\n-\n-            ('http://username:password@www.example.com:8008/foo/bar?query=value#somefrag',\n-             'http://www.example.com:8008/'),\n-\n-            ('https://username:password@www.example.com:443/index.html',\n-             'https://www.example.com/'),\n+            (\n+                \"http://username:password@www.example.com/index.html\",\n+                \"http://www.example.com/\",\n+            ),\n+            (\n+                \"http://username:password@www.example.com:80/foo/bar?query=value#somefrag\",\n+                \"http://www.example.com/\",\n+            ),\n+            (\n+                \"http://username:password@www.example.com:8008/foo/bar?query=value#somefrag\",\n+                \"http://www.example.com:8008/\",\n+            ),\n+            (\n+                \"https://username:password@www.example.com:443/index.html\",\n+                \"https://www.example.com/\",\n+            ),\n         ]:\n             self.assertEqual(strip_url(i, origin_only=True), o)\n \n \n class IsPathTestCase(unittest.TestCase):\n-\n     def test_path(self):\n         for input_value, output_value in (\n             # https://en.wikipedia.org/wiki/Path_(computing)#Representations_of_paths_by_operating_system_and_shell\n@@ -467,10 +602,11 @@ class IsPathTestCase(unittest.TestCase):\n             (r\"\\\\?\\UNC\\Server01\\user\\docs\\Letter.txt\", True),\n             (r\"\\\\?\\C:\\user\\docs\\Letter.txt\", True),\n             (r\"C:\\user\\docs\\somefile.ext:alternate_stream_name\", True),\n-\n             (r\"https://example.com\", False),\n         ):\n-            self.assertEqual(_is_filesystem_path(input_value), output_value, input_value)\n+            self.assertEqual(\n+                _is_filesystem_path(input_value), output_value, input_value\n+            )\n \n \n if __name__ == \"__main__\":\n\n@@ -9,6 +9,7 @@ import OpenSSL.SSL\n from twisted.trial import unittest\n from twisted.web import server, static, util, resource\n from twisted.internet import reactor, defer\n+\n try:\n     from twisted.internet.testing import StringTransport\n except ImportError:\n@@ -37,17 +38,24 @@ from tests.mockserver import (\n \n def getPage(url, contextFactory=None, response_transform=None, *args, **kwargs):\n     \"\"\"Adapted version of twisted.web.client.getPage\"\"\"\n+\n     def _clientfactory(url, *args, **kwargs):\n         url = to_unicode(url)\n-        timeout = kwargs.pop('timeout', 0)\n+        timeout = kwargs.pop(\"timeout\", 0)\n         f = client.ScrapyHTTPClientFactory(\n-            Request(url, *args, **kwargs), timeout=timeout)\n+            Request(url, *args, **kwargs), timeout=timeout\n+        )\n         f.deferred.addCallback(response_transform or (lambda r: r.body))\n         return f\n \n     from twisted.web.client import _makeGetterFactory\n+\n     return _makeGetterFactory(\n-        to_bytes(url), _clientfactory, contextFactory=contextFactory, *args, **kwargs,\n+        to_bytes(url),\n+        _clientfactory,\n+        contextFactory=contextFactory,\n+        *args,\n+        **kwargs,\n     ).deferred\n \n \n@@ -59,51 +67,80 @@ class ParseUrlTestCase(unittest.TestCase):\n         return (f.scheme, f.netloc, f.host, f.port, f.path)\n \n     def testParse(self):\n-        lip = '127.0.0.1'\n+        lip = \"127.0.0.1\"\n         tests = (\n-            (\"http://127.0.0.1?c=v&c2=v2#fragment\", ('http', lip, lip, 80, '/?c=v&c2=v2')),\n-            (\"http://127.0.0.1/?c=v&c2=v2#fragment\", ('http', lip, lip, 80, '/?c=v&c2=v2')),\n-            (\"http://127.0.0.1/foo?c=v&c2=v2#frag\", ('http', lip, lip, 80, '/foo?c=v&c2=v2')),\n-            (\"http://127.0.0.1:100?c=v&c2=v2#fragment\", ('http', lip + ':100', lip, 100, '/?c=v&c2=v2')),\n-            (\"http://127.0.0.1:100/?c=v&c2=v2#frag\", ('http', lip + ':100', lip, 100, '/?c=v&c2=v2')),\n-            (\"http://127.0.0.1:100/foo?c=v&c2=v2#frag\", ('http', lip + ':100', lip, 100, '/foo?c=v&c2=v2')),\n-\n-            (\"http://127.0.0.1\", ('http', lip, lip, 80, '/')),\n-            (\"http://127.0.0.1/\", ('http', lip, lip, 80, '/')),\n-            (\"http://127.0.0.1/foo\", ('http', lip, lip, 80, '/foo')),\n-            (\"http://127.0.0.1?param=value\", ('http', lip, lip, 80, '/?param=value')),\n-            (\"http://127.0.0.1/?param=value\", ('http', lip, lip, 80, '/?param=value')),\n-            (\"http://127.0.0.1:12345/foo\", ('http', lip + ':12345', lip, 12345, '/foo')),\n-            (\"http://spam:12345/foo\", ('http', 'spam:12345', 'spam', 12345, '/foo')),\n-            (\"http://spam.test.org/foo\", ('http', 'spam.test.org', 'spam.test.org', 80, '/foo')),\n-\n-            (\"https://127.0.0.1/foo\", ('https', lip, lip, 443, '/foo')),\n-            (\"https://127.0.0.1/?param=value\", ('https', lip, lip, 443, '/?param=value')),\n-            (\"https://127.0.0.1:12345/\", ('https', lip + ':12345', lip, 12345, '/')),\n-\n-            (\"http://scrapytest.org/foo \", ('http', 'scrapytest.org', 'scrapytest.org', 80, '/foo')),\n-            (\"http://egg:7890 \", ('http', 'egg:7890', 'egg', 7890, '/')),\n+            (\n+                \"http://127.0.0.1?c=v&c2=v2#fragment\",\n+                (\"http\", lip, lip, 80, \"/?c=v&c2=v2\"),\n+            ),\n+            (\n+                \"http://127.0.0.1/?c=v&c2=v2#fragment\",\n+                (\"http\", lip, lip, 80, \"/?c=v&c2=v2\"),\n+            ),\n+            (\n+                \"http://127.0.0.1/foo?c=v&c2=v2#frag\",\n+                (\"http\", lip, lip, 80, \"/foo?c=v&c2=v2\"),\n+            ),\n+            (\n+                \"http://127.0.0.1:100?c=v&c2=v2#fragment\",\n+                (\"http\", lip + \":100\", lip, 100, \"/?c=v&c2=v2\"),\n+            ),\n+            (\n+                \"http://127.0.0.1:100/?c=v&c2=v2#frag\",\n+                (\"http\", lip + \":100\", lip, 100, \"/?c=v&c2=v2\"),\n+            ),\n+            (\n+                \"http://127.0.0.1:100/foo?c=v&c2=v2#frag\",\n+                (\"http\", lip + \":100\", lip, 100, \"/foo?c=v&c2=v2\"),\n+            ),\n+            (\"http://127.0.0.1\", (\"http\", lip, lip, 80, \"/\")),\n+            (\"http://127.0.0.1/\", (\"http\", lip, lip, 80, \"/\")),\n+            (\"http://127.0.0.1/foo\", (\"http\", lip, lip, 80, \"/foo\")),\n+            (\"http://127.0.0.1?param=value\", (\"http\", lip, lip, 80, \"/?param=value\")),\n+            (\"http://127.0.0.1/?param=value\", (\"http\", lip, lip, 80, \"/?param=value\")),\n+            (\n+                \"http://127.0.0.1:12345/foo\",\n+                (\"http\", lip + \":12345\", lip, 12345, \"/foo\"),\n+            ),\n+            (\"http://spam:12345/foo\", (\"http\", \"spam:12345\", \"spam\", 12345, \"/foo\")),\n+            (\n+                \"http://spam.test.org/foo\",\n+                (\"http\", \"spam.test.org\", \"spam.test.org\", 80, \"/foo\"),\n+            ),\n+            (\"https://127.0.0.1/foo\", (\"https\", lip, lip, 443, \"/foo\")),\n+            (\n+                \"https://127.0.0.1/?param=value\",\n+                (\"https\", lip, lip, 443, \"/?param=value\"),\n+            ),\n+            (\"https://127.0.0.1:12345/\", (\"https\", lip + \":12345\", lip, 12345, \"/\")),\n+            (\n+                \"http://scrapytest.org/foo \",\n+                (\"http\", \"scrapytest.org\", \"scrapytest.org\", 80, \"/foo\"),\n+            ),\n+            (\"http://egg:7890 \", (\"http\", \"egg:7890\", \"egg\", 7890, \"/\")),\n         )\n \n         for url, test in tests:\n-            test = tuple(\n-                to_bytes(x) if not isinstance(x, int) else x for x in test)\n+            test = tuple(to_bytes(x) if not isinstance(x, int) else x for x in test)\n             self.assertEqual(client._parse(url), test, url)\n \n \n class ScrapyHTTPPageGetterTests(unittest.TestCase):\n-\n     def test_earlyHeaders(self):\n         # basic test stolen from twisted HTTPageGetter\n-        factory = client.ScrapyHTTPClientFactory(Request(\n-            url='http://foo/bar',\n+        factory = client.ScrapyHTTPClientFactory(\n+            Request(\n+                url=\"http://foo/bar\",\n                 body=\"some data\",\n                 headers={\n-                'Host': 'example.net',\n-                'User-Agent': 'fooble',\n-                'Cookie': 'blah blah',\n-                'Content-Length': '12981',\n-                'Useful': 'value'}))\n+                    \"Host\": \"example.net\",\n+                    \"User-Agent\": \"fooble\",\n+                    \"Cookie\": \"blah blah\",\n+                    \"Content-Length\": \"12981\",\n+                    \"Useful\": \"value\",\n+                },\n+            )\n+        )\n \n         self._test(\n             factory,\n@@ -115,22 +152,22 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n             b\"Host: example.net\\r\\n\"\n             b\"Cookie: blah blah\\r\\n\"\n             b\"\\r\\n\"\n-            b\"some data\")\n+            b\"some data\",\n+        )\n \n         # test minimal sent headers\n-        factory = client.ScrapyHTTPClientFactory(Request('http://foo/bar'))\n-        self._test(\n-            factory,\n-            b\"GET /bar HTTP/1.0\\r\\n\"\n-            b\"Host: foo\\r\\n\"\n-            b\"\\r\\n\")\n+        factory = client.ScrapyHTTPClientFactory(Request(\"http://foo/bar\"))\n+        self._test(factory, b\"GET /bar HTTP/1.0\\r\\n\" b\"Host: foo\\r\\n\" b\"\\r\\n\")\n \n         # test a simple POST with body and content-type\n-        factory = client.ScrapyHTTPClientFactory(Request(\n-            method='POST',\n-            url='http://foo/bar',\n-            body='name=value',\n-            headers={'Content-Type': 'application/x-www-form-urlencoded'}))\n+        factory = client.ScrapyHTTPClientFactory(\n+            Request(\n+                method=\"POST\",\n+                url=\"http://foo/bar\",\n+                body=\"name=value\",\n+                headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n+            )\n+        )\n \n         self._test(\n             factory,\n@@ -140,29 +177,29 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n             b\"Content-Type: application/x-www-form-urlencoded\\r\\n\"\n             b\"Content-Length: 10\\r\\n\"\n             b\"\\r\\n\"\n-            b\"name=value\")\n+            b\"name=value\",\n+        )\n \n         # test a POST method with no body provided\n-        factory = client.ScrapyHTTPClientFactory(Request(\n-            method='POST',\n-            url='http://foo/bar'\n-        ))\n+        factory = client.ScrapyHTTPClientFactory(\n+            Request(method=\"POST\", url=\"http://foo/bar\")\n+        )\n \n         self._test(\n             factory,\n-            b\"POST /bar HTTP/1.0\\r\\n\"\n-            b\"Host: foo\\r\\n\"\n-            b\"Content-Length: 0\\r\\n\"\n-            b\"\\r\\n\")\n+            b\"POST /bar HTTP/1.0\\r\\n\" b\"Host: foo\\r\\n\" b\"Content-Length: 0\\r\\n\" b\"\\r\\n\",\n+        )\n \n         # test with single and multivalued headers\n-        factory = client.ScrapyHTTPClientFactory(Request(\n-            url='http://foo/bar',\n+        factory = client.ScrapyHTTPClientFactory(\n+            Request(\n+                url=\"http://foo/bar\",\n                 headers={\n-                'X-Meta-Single': 'single',\n-                'X-Meta-Multivalued': ['value1', 'value2'],\n+                    \"X-Meta-Single\": \"single\",\n+                    \"X-Meta-Multivalued\": [\"value1\", \"value2\"],\n                 },\n-        ))\n+            )\n+        )\n \n         self._test(\n             factory,\n@@ -171,16 +208,21 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n             b\"X-Meta-Multivalued: value1\\r\\n\"\n             b\"X-Meta-Multivalued: value2\\r\\n\"\n             b\"X-Meta-Single: single\\r\\n\"\n-            b\"\\r\\n\")\n+            b\"\\r\\n\",\n+        )\n \n         # same test with single and multivalued headers but using Headers class\n-        factory = client.ScrapyHTTPClientFactory(Request(\n-            url='http://foo/bar',\n-            headers=Headers({\n-                'X-Meta-Single': 'single',\n-                'X-Meta-Multivalued': ['value1', 'value2'],\n-            }),\n-        ))\n+        factory = client.ScrapyHTTPClientFactory(\n+            Request(\n+                url=\"http://foo/bar\",\n+                headers=Headers(\n+                    {\n+                        \"X-Meta-Single\": \"single\",\n+                        \"X-Meta-Multivalued\": [\"value1\", \"value2\"],\n+                    }\n+                ),\n+            )\n+        )\n \n         self._test(\n             factory,\n@@ -189,7 +231,8 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n             b\"X-Meta-Multivalued: value1\\r\\n\"\n             b\"X-Meta-Multivalued: value2\\r\\n\"\n             b\"X-Meta-Single: single\\r\\n\"\n-            b\"\\r\\n\")\n+            b\"\\r\\n\",\n+        )\n \n     def _test(self, factory, testvalue):\n         transport = StringTransport()\n@@ -197,14 +240,13 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n         protocol.factory = factory\n         protocol.makeConnection(transport)\n         self.assertEqual(\n-            set(transport.value().splitlines()),\n-            set(testvalue.splitlines()))\n+            set(transport.value().splitlines()), set(testvalue.splitlines())\n+        )\n         return testvalue\n \n     def test_non_standard_line_endings(self):\n         # regression test for: http://dev.scrapy.org/ticket/258\n-        factory = client.ScrapyHTTPClientFactory(Request(\n-            url='http://foo/bar'))\n+        factory = client.ScrapyHTTPClientFactory(Request(url=\"http://foo/bar\"))\n         protocol = client.ScrapyHTTPPageGetter()\n         protocol.factory = factory\n         protocol.headers = Headers()\n@@ -212,15 +254,17 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n         protocol.dataReceived(b\"Hello: World\\n\")\n         protocol.dataReceived(b\"Foo: Bar\\n\")\n         protocol.dataReceived(b\"\\n\")\n-        self.assertEqual(protocol.headers, Headers({'Hello': ['World'], 'Foo': ['Bar']}))\n+        self.assertEqual(\n+            protocol.headers, Headers({\"Hello\": [\"World\"], \"Foo\": [\"Bar\"]})\n+        )\n \n \n class EncodingResource(resource.Resource):\n-    out_encoding = 'cp1251'\n+    out_encoding = \"cp1251\"\n \n     def render(self, request):\n         body = to_unicode(request.content.read())\n-        request.setHeader(b'content-encoding', self.out_encoding)\n+        request.setHeader(b\"content-encoding\", self.out_encoding)\n         return body.encode(self.out_encoding)\n \n \n@@ -257,16 +301,22 @@ class WebClientTestCase(unittest.TestCase):\n     def testPayload(self):\n         s = \"0123456789\" * 10\n         return getPage(self.getURL(\"payload\"), body=s).addCallback(\n-            self.assertEqual, to_bytes(s))\n+            self.assertEqual, to_bytes(s)\n+        )\n \n     def testHostHeader(self):\n         # if we pass Host header explicitly, it should be used, otherwise\n         # it should extract from url\n-        return defer.gatherResults([\n+        return defer.gatherResults(\n+            [\n                 getPage(self.getURL(\"host\")).addCallback(\n-                self.assertEqual, to_bytes(f\"127.0.0.1:{self.portno}\")),\n-            getPage(self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"}).addCallback(\n-                self.assertEqual, to_bytes(\"www.example.com\"))])\n+                    self.assertEqual, to_bytes(f\"127.0.0.1:{self.portno}\")\n+                ),\n+                getPage(\n+                    self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"}\n+                ).addCallback(self.assertEqual, to_bytes(\"www.example.com\")),\n+            ]\n+        )\n \n     def test_getPage(self):\n         \"\"\"\n@@ -283,11 +333,16 @@ class WebClientTestCase(unittest.TestCase):\n         the empty string if the method is C{HEAD} and there is a successful\n         response code.\n         \"\"\"\n+\n         def _getPage(method):\n             return getPage(self.getURL(\"file\"), method=method)\n-        return defer.gatherResults([\n+\n+        return defer.gatherResults(\n+            [\n                 _getPage(\"head\").addCallback(self.assertEqual, b\"\"),\n-            _getPage(\"HEAD\").addCallback(self.assertEqual, b\"\")])\n+                _getPage(\"HEAD\").addCallback(self.assertEqual, b\"\"),\n+            ]\n+        )\n \n     def test_timeoutNotTriggering(self):\n         \"\"\"\n@@ -296,8 +351,7 @@ class WebClientTestCase(unittest.TestCase):\n         called back with the contents of the page.\n         \"\"\"\n         d = getPage(self.getURL(\"host\"), timeout=100)\n-        d.addCallback(\n-            self.assertEqual, to_bytes(f\"127.0.0.1:{self.portno}\"))\n+        d.addCallback(self.assertEqual, to_bytes(f\"127.0.0.1:{self.portno}\"))\n         return d\n \n     def test_timeoutTriggering(self):\n@@ -307,8 +361,8 @@ class WebClientTestCase(unittest.TestCase):\n         L{Deferred} is errbacked with a L{error.TimeoutError}.\n         \"\"\"\n         finished = self.assertFailure(\n-            getPage(self.getURL(\"wait\"), timeout=0.000001),\n-            defer.TimeoutError)\n+            getPage(self.getURL(\"wait\"), timeout=0.000001), defer.TimeoutError\n+        )\n \n         def cleanup(passthrough):\n             # Clean up the server which is hanging around not doing\n@@ -319,27 +373,28 @@ class WebClientTestCase(unittest.TestCase):\n             if connected:\n                 connected[0].transport.loseConnection()\n             return passthrough\n+\n         finished.addBoth(cleanup)\n         return finished\n \n     def testNotFound(self):\n-        return getPage(self.getURL('notsuchfile')).addCallback(self._cbNoSuchFile)\n+        return getPage(self.getURL(\"notsuchfile\")).addCallback(self._cbNoSuchFile)\n \n     def _cbNoSuchFile(self, pageData):\n-        self.assertIn(b'404 - No Such Resource', pageData)\n+        self.assertIn(b\"404 - No Such Resource\", pageData)\n \n     def testFactoryInfo(self):\n-        url = self.getURL('file')\n+        url = self.getURL(\"file\")\n         _, _, host, port, _ = client._parse(url)\n         factory = client.ScrapyHTTPClientFactory(Request(url))\n         reactor.connectTCP(to_unicode(host), port, factory)\n         return factory.deferred.addCallback(self._cbFactoryInfo, factory)\n \n     def _cbFactoryInfo(self, ignoredResult, factory):\n-        self.assertEqual(factory.status, b'200')\n-        self.assertTrue(factory.version.startswith(b'HTTP/'))\n-        self.assertEqual(factory.message, b'OK')\n-        self.assertEqual(factory.response_headers[b'content-length'], b'10')\n+        self.assertEqual(factory.status, b\"200\")\n+        self.assertTrue(factory.version.startswith(b\"HTTP/\"))\n+        self.assertEqual(factory.message, b\"OK\")\n+        self.assertEqual(factory.response_headers[b\"content-length\"], b\"10\")\n \n     def testRedirect(self):\n         return getPage(self.getURL(\"redirect\")).addCallback(self._cbRedirect)\n@@ -349,20 +404,24 @@ class WebClientTestCase(unittest.TestCase):\n             pageData,\n             b'\\n<html>\\n    <head>\\n        <meta http-equiv=\"refresh\" content=\"0;URL=/file\">\\n'\n             b'    </head>\\n    <body bgcolor=\"#FFFFFF\" text=\"#000000\">\\n    '\n-            b'<a href=\"/file\">click here</a>\\n    </body>\\n</html>\\n')\n+            b'<a href=\"/file\">click here</a>\\n    </body>\\n</html>\\n',\n+        )\n \n     def test_encoding(self):\n         \"\"\"Test that non-standart body encoding matches\n         Content-Encoding header\"\"\"\n-        body = b'\\xd0\\x81\\xd1\\x8e\\xd0\\xaf'\n-        dfd = getPage(self.getURL('encoding'), body=body, response_transform=lambda r: r)\n+        body = b\"\\xd0\\x81\\xd1\\x8e\\xd0\\xaf\"\n+        dfd = getPage(\n+            self.getURL(\"encoding\"), body=body, response_transform=lambda r: r\n+        )\n         return dfd.addCallback(self._check_Encoding, body)\n \n     def _check_Encoding(self, response, original_body):\n-        content_encoding = to_unicode(response.headers[b'Content-Encoding'])\n+        content_encoding = to_unicode(response.headers[b\"Content-Encoding\"])\n         self.assertEqual(content_encoding, EncodingResource.out_encoding)\n         self.assertEqual(\n-            response.body.decode(content_encoding), to_unicode(original_body))\n+            response.body.decode(content_encoding), to_unicode(original_body)\n+        )\n \n \n class WebClientSSLTestCase(unittest.TestCase):\n@@ -370,9 +429,11 @@ class WebClientSSLTestCase(unittest.TestCase):\n \n     def _listen(self, site):\n         return reactor.listenSSL(\n-            0, site,\n+            0,\n+            site,\n             contextFactory=self.context_factory or ssl_context_factory(),\n-            interface=\"127.0.0.1\")\n+            interface=\"127.0.0.1\",\n+        )\n \n     def getURL(self, path):\n         return f\"https://127.0.0.1:{self.portno}/{path}\"\n@@ -396,25 +457,34 @@ class WebClientSSLTestCase(unittest.TestCase):\n     def testPayload(self):\n         s = \"0123456789\" * 10\n         return getPage(self.getURL(\"payload\"), body=s).addCallback(\n-            self.assertEqual, to_bytes(s))\n+            self.assertEqual, to_bytes(s)\n+        )\n \n \n class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n     # we try to use a cipher that is not enabled by default in OpenSSL\n-    custom_ciphers = 'CAMELLIA256-SHA'\n+    custom_ciphers = \"CAMELLIA256-SHA\"\n     context_factory = ssl_context_factory(cipher_string=custom_ciphers)\n \n     def testPayload(self):\n         s = \"0123456789\" * 10\n-        settings = Settings({'DOWNLOADER_CLIENT_TLS_CIPHERS': self.custom_ciphers})\n-        client_context_factory = create_instance(ScrapyClientContextFactory, settings=settings, crawler=None)\n+        settings = Settings({\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.custom_ciphers})\n+        client_context_factory = create_instance(\n+            ScrapyClientContextFactory, settings=settings, crawler=None\n+        )\n         return getPage(\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n         ).addCallback(self.assertEqual, to_bytes(s))\n \n     def testPayloadDisabledCipher(self):\n         s = \"0123456789\" * 10\n-        settings = Settings({'DOWNLOADER_CLIENT_TLS_CIPHERS': 'ECDHE-RSA-AES256-GCM-SHA384'})\n-        client_context_factory = create_instance(ScrapyClientContextFactory, settings=settings, crawler=None)\n-        d = getPage(self.getURL(\"payload\"), body=s, contextFactory=client_context_factory)\n+        settings = Settings(\n+            {\"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"ECDHE-RSA-AES256-GCM-SHA384\"}\n+        )\n+        client_context_factory = create_instance(\n+            ScrapyClientContextFactory, settings=settings, crawler=None\n+        )\n+        d = getPage(\n+            self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n+        )\n         return self.assertFailure(d, OpenSSL.SSL.Error)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
