{"custom_id": "scrapy#2cfbd13c17085f282a74cc7ff2c898590e7fe4c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 144 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -93,9 +93,9 @@ class DjangoItemTest(unittest.TestCase):\n             },\n             i.errors)\n \n-        i = BasePersonItem(name='John', age='22')\n+        i = BasePersonItem(name='John')\n+        self.assertTrue(i.is_valid(exclude=['age']))\n         self.assertDictEqual({}, i.errors)\n-        self.assertTrue(i.is_valid())\n \n         # once the item is validated, it does not validate again\n         i['name'] = long_name\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0a5586fafda730b2e69ff0c6704e5259c05ae8ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 93 | Lines Deleted: 82 | Files Changed: 3 | Hunks: 14 | Methods Changed: 16 | Complexity Δ (Sum/Max): -64/0 | Churn Δ: 175 | Churn Cumulative: 1623 | Contributors (this commit): 7 | Commits (past 90d): 16 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,18 +1,7 @@\n-import os\n-from os.path import join, exists\n-from time import time\n-import cPickle as pickle\n-\n-from w3lib.http import headers_dict_to_raw, headers_raw_to_dict\n-\n from scrapy import signals\n-from scrapy.http import Headers\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n-from scrapy.responsetypes import responsetypes\n-from scrapy.utils.request import request_fingerprint\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n-from scrapy.utils.project import data_path\n \n \n class HttpCacheMiddleware(object):\n@@ -67,71 +56,16 @@ class HttpCacheMiddleware(object):\n         return urlparse_cached(request).scheme not in self.ignore_schemes\n \n \n-class FilesystemCacheStorage(object):\n+from scrapy.contrib.httpcache import FilesystemCacheStorage as _FilesystemCacheStorage\n \n-    def __init__(self, settings):\n-        self.cachedir = data_path(settings['HTTPCACHE_DIR'])\n-        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n \n-    def open_spider(self, spider):\n-        pass\n+class FilesystemCacheStorage(_FilesystemCacheStorage):\n \n-    def close_spider(self, spider):\n-        pass\n-\n-    def retrieve_response(self, spider, request):\n-        \"\"\"Return response if present in cache, or None otherwise.\"\"\"\n-        metadata = self._read_meta(spider, request)\n-        if metadata is None:\n-            return # not cached\n-        rpath = self._get_request_path(spider, request)\n-        with open(join(rpath, 'response_body'), 'rb') as f:\n-            body = f.read()\n-        with open(join(rpath, 'response_headers'), 'rb') as f:\n-            rawheaders = f.read()\n-        url = metadata.get('response_url')\n-        status = metadata['status']\n-        headers = Headers(headers_raw_to_dict(rawheaders))\n-        respcls = responsetypes.from_args(headers=headers, url=url)\n-        response = respcls(url=url, headers=headers, status=status, body=body)\n-        return response\n-\n-    def store_response(self, spider, request, response):\n-        \"\"\"Store the given response in the cache.\"\"\"\n-        rpath = self._get_request_path(spider, request)\n-        if not exists(rpath):\n-            os.makedirs(rpath)\n-        metadata = {\n-            'url': request.url,\n-            'method': request.method,\n-            'status': response.status,\n-            'response_url': response.url,\n-            'timestamp': time(),\n-        }\n-        with open(join(rpath, 'meta'), 'wb') as f:\n-            f.write(repr(metadata))\n-        with open(join(rpath, 'pickled_meta'), 'wb') as f:\n-            pickle.dump(metadata, f, protocol=2)\n-        with open(join(rpath, 'response_headers'), 'wb') as f:\n-            f.write(headers_dict_to_raw(response.headers))\n-        with open(join(rpath, 'response_body'), 'wb') as f:\n-            f.write(response.body)\n-        with open(join(rpath, 'request_headers'), 'wb') as f:\n-            f.write(headers_dict_to_raw(request.headers))\n-        with open(join(rpath, 'request_body'), 'wb') as f:\n-            f.write(request.body)\n-\n-    def _get_request_path(self, spider, request):\n-        key = request_fingerprint(request)\n-        return join(self.cachedir, spider.name, key[0:2], key)\n-\n-    def _read_meta(self, spider, request):\n-        rpath = self._get_request_path(spider, request)\n-        metapath = join(rpath, 'pickled_meta')\n-        if not exists(metapath):\n-            return # not found\n-        mtime = os.stat(rpath).st_mtime\n-        if 0 < self.expiration_secs < time() - mtime:\n-            return # expired\n-        with open(metapath, 'rb') as f:\n-            return pickle.load(f)\n+    def __init__(self, *args, **kwargs):\n+        import warnings\n+        from scrapy.exceptions import ScrapyDeprecationWarning\n+        warnings.warn('Importing FilesystemCacheStorage from '\n+                      'scrapy.contrib.downloadermiddlware.httpcache is '\n+                      'deprecated, use scrapy.contrib.httpcache instead.',\n+                      category=ScrapyDeprecationWarning, stacklevel=1)\n+        super(_FilesystemCacheStorage, self).__init__(*args, **kwargs)\n\n@@ -1,7 +1,7 @@\n import os\n from time import time\n import cPickle as pickle\n-\n+from w3lib.http import headers_raw_to_dict, headers_dict_to_raw\n from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n@@ -59,3 +59,73 @@ class DbmCacheStorage(object):\n \n     def _request_key(self, request):\n         return request_fingerprint(request)\n+\n+\n+class FilesystemCacheStorage(object):\n+\n+    def __init__(self, settings):\n+        self.cachedir = data_path(settings['HTTPCACHE_DIR'])\n+        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n+\n+    def open_spider(self, spider):\n+        pass\n+\n+    def close_spider(self, spider):\n+        pass\n+\n+    def retrieve_response(self, spider, request):\n+        \"\"\"Return response if present in cache, or None otherwise.\"\"\"\n+        metadata = self._read_meta(spider, request)\n+        if metadata is None:\n+            return  # not cached\n+        rpath = self._get_request_path(spider, request)\n+        with open(os.path.join(rpath, 'response_body'), 'rb') as f:\n+            body = f.read()\n+        with open(os.path.join(rpath, 'response_headers'), 'rb') as f:\n+            rawheaders = f.read()\n+        url = metadata.get('response_url')\n+        status = metadata['status']\n+        headers = Headers(headers_raw_to_dict(rawheaders))\n+        respcls = responsetypes.from_args(headers=headers, url=url)\n+        response = respcls(url=url, headers=headers, status=status, body=body)\n+        return response\n+\n+    def store_response(self, spider, request, response):\n+        \"\"\"Store the given response in the cache.\"\"\"\n+        rpath = self._get_request_path(spider, request)\n+        if not os.path.exists(rpath):\n+            os.makedirs(rpath)\n+        metadata = {\n+            'url': request.url,\n+            'method': request.method,\n+            'status': response.status,\n+            'response_url': response.url,\n+            'timestamp': time(),\n+        }\n+        with open(os.path.join(rpath, 'meta'), 'wb') as f:\n+            f.write(repr(metadata))\n+        with open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:\n+            pickle.dump(metadata, f, protocol=2)\n+        with open(os.path.join(rpath, 'response_headers'), 'wb') as f:\n+            f.write(headers_dict_to_raw(response.headers))\n+        with open(os.path.join(rpath, 'response_body'), 'wb') as f:\n+            f.write(response.body)\n+        with open(os.path.join(rpath, 'request_headers'), 'wb') as f:\n+            f.write(headers_dict_to_raw(request.headers))\n+        with open(os.path.join(rpath, 'request_body'), 'wb') as f:\n+            f.write(request.body)\n+\n+    def _get_request_path(self, spider, request):\n+        key = request_fingerprint(request)\n+        return os.path.join(self.cachedir, spider.name, key[0:2], key)\n+\n+    def _read_meta(self, spider, request):\n+        rpath = self._get_request_path(spider, request)\n+        metapath = os.path.join(rpath, 'pickled_meta')\n+        if not os.path.exists(metapath):\n+            return  # not found\n+        mtime = os.stat(rpath).st_mtime\n+        if 0 < self.expiration_secs < time() - mtime:\n+            return  # expired\n+        with open(metapath, 'rb') as f:\n+            return pickle.load(f)\n\n@@ -9,13 +9,13 @@ from scrapy.spider import BaseSpider\n from scrapy.settings import Settings\n from scrapy.exceptions import IgnoreRequest\n from scrapy.utils.test import get_crawler\n-from scrapy.contrib.downloadermiddleware.httpcache import \\\n-    FilesystemCacheStorage, HttpCacheMiddleware\n+from scrapy.contrib.httpcache import FilesystemCacheStorage, DbmCacheStorage\n+from scrapy.contrib.downloadermiddleware.httpcache import HttpCacheMiddleware\n \n \n class HttpCacheMiddlewareTest(unittest.TestCase):\n \n-    storage_class = FilesystemCacheStorage\n+    storage_class = DbmCacheStorage\n \n     def setUp(self):\n         self.crawler = get_crawler()\n@@ -23,8 +23,10 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         self.tmpdir = tempfile.mkdtemp()\n         self.request = Request('http://www.example.com',\n                                headers={'User-Agent': 'test'})\n-        self.response = Response('http://www.example.com', headers=\n-            {'Content-Type': 'text/html'}, body='test body', status=202)\n+        self.response = Response('http://www.example.com',\n+                                 headers={'Content-Type': 'text/html'},\n+                                 body='test body',\n+                                 status=202)\n         self.crawler.stats.open_spider(self.spider)\n \n     def tearDown(self):\n@@ -177,5 +179,10 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         self.assertEqual(response1.headers, response2.headers)\n         self.assertEqual(response1.body, response2.body)\n \n+\n+class FilesystemCacheStorageTest(HttpCacheMiddlewareTest):\n+\n+    storage = FilesystemCacheStorage\n+\n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cdecc760eeef2c7c60b2c9883c5c0eee0cd38e6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 484 | Lines Deleted: 463 | Files Changed: 3 | Hunks: 58 | Methods Changed: 60 | Complexity Δ (Sum/Max): 66/42 | Churn Δ: 947 | Churn Cumulative: 2570 | Contributors (this commit): 7 | Commits (past 90d): 19 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,61 +1,167 @@\n-import os\n-from os.path import join, exists\n from time import time\n-import cPickle as pickle\n-\n-from w3lib.http import headers_dict_to_raw, headers_raw_to_dict\n-\n+from email.utils import formatdate\n+from weakref import WeakKeyDictionary\n from scrapy import signals\n-from scrapy.http import Headers\n-from scrapy.http.request import Request\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n-from scrapy.responsetypes import responsetypes\n-from scrapy.utils.request import request_fingerprint\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n-from scrapy.utils.project import data_path\n+from scrapy.contrib.httpcache import rfc1123_to_epoch, parse_cachecontrol\n \n \n class DummyPolicy(object):\n+\n     def __init__(self, settings):\n         self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n         self.ignore_http_codes = map(int, settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES'))\n \n-    def should_cache_response(self, response):\n-        return response.status not in self.ignore_http_codes\n-\n     def should_cache_request(self, request):\n         return urlparse_cached(request).scheme not in self.ignore_schemes\n \n+    def should_cache_response(self, response, request):\n+        return response.status not in self.ignore_http_codes\n+\n+    def is_cached_response_fresh(self, response, request):\n+        return True\n+\n+    def is_cached_response_valid(self, cachedresponse, response, request):\n+        return True\n+\n+\n+class RFC2616Policy(object):\n+\n+    MAXAGE = 3600 * 24 * 365  # one year\n \n-class RFC2616Policy(DummyPolicy):\n     def __init__(self, settings):\n-        super(RFC2616Policy, self).__init__(settings)\n+        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n+        self._cc_parsed = WeakKeyDictionary()\n \n-    def should_cache_response(self, response):\n-        retval = super(RFC2616Policy, self).should_cache_response(response)\n-\n-        if response.headers.has_key('cache-control'):\n-            retval = retval and (response.headers['cache-control'].lower().find('no-store') == -1)\n-        #retval = retval and self.policy_response(response)\n-        return retval\n+    def _parse_cachecontrol(self, r):\n+        if r not in self._cc_parsed:\n+            cch = r.headers.get('Cache-Control', '')\n+            self._cc_parsed[r] = parse_cachecontrol(cch)\n+        return self._cc_parsed[r]\n \n     def should_cache_request(self, request):\n-        retval = super(RFC2616Policy, self).should_cache_request(request)\n+        if urlparse_cached(request).scheme in self.ignore_schemes:\n+            return False\n+        cc = self._parse_cachecontrol(request)\n+        # obey user-agent directive \"Cache-Control: no-store\"\n+        if 'no-store' in cc:\n+            return False\n+        # Any other is eligible for caching\n+        return True\n+\n+    def should_cache_response(self, response, request):\n+        # What is cacheable - http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec14.9.1\n+        # Response cacheability - http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4\n+        # Status code 206 is not included because cache can not deal with partial contents\n+        cc = self._parse_cachecontrol(response)\n+        # obey directive \"Cache-Control: no-store\"\n+        if 'no-store' in cc:\n+            return False\n+        # Never cache 304 (Not Modified) responses\n+        elif response.status == 304:\n+            return False\n+        # Any hint on response expiration is good\n+        elif 'max-age' in cc or 'Expires' in response.headers:\n+            return True\n+        # Firefox fallbacks this statuses to one year expiration if none is set\n+        elif response.status in (300, 301, 308):\n+            return True\n+        # Other statuses without expiration requires at least one validator\n+        elif response.status in (200, 203, 401):\n+            return 'Last-Modified' in response.headers or 'ETag' in response.headers\n+        # Any other is probably not eligible for caching\n+        # Makes no sense to cache responses that does not contain expiration\n+        # info and can not be revalidated\n+        else:\n+            return False\n+\n+    def is_cached_response_fresh(self, cachedresponse, request):\n+        cc = self._parse_cachecontrol(cachedresponse)\n+        if 'no-cache' in cc:\n+            return False\n+\n+        now = time()\n+        freshnesslifetime = self._compute_freshness_lifetime(cachedresponse, request, now)\n+        currentage = self._compute_current_age(cachedresponse, request, now)\n+        if currentage < freshnesslifetime:\n+            return True\n+        # Cached response is stale, try to set validators if any\n+        self._set_conditional_validators(request, cachedresponse)\n+        return False\n+\n+    def is_cached_response_valid(self, cachedresponse, response, request):\n+        return response.status == 304\n+\n+    def _set_conditional_validators(self, request, cachedresponse):\n+        if 'Last-Modified' in cachedresponse.headers:\n+            request.headers['If-Modified-Since'] = cachedresponse.headers['Last-Modified']\n+\n+        if 'ETag' in cachedresponse.headers:\n+            request.headers['If-None-Match'] = cachedresponse.headers['ETag']\n+\n+    def _compute_freshness_lifetime(self, response, request, now):\n+        # Reference nsHttpResponseHead::ComputeFresshnessLifetime\n+        # http://dxr.mozilla.org/mozilla-central/netwerk/protocol/http/nsHttpResponseHead.cpp.html#l259\n+        cc = self._parse_cachecontrol(response)\n+        if 'max-age' in cc:\n+            try:\n+                return max(0, int(cc['max-age']))\n+            except ValueError:\n+                pass\n+\n+        # Parse date header or synthesize it if none exists\n+        date = rfc1123_to_epoch(response.headers.get('Date')) or now\n+\n+        # Try HTTP/1.0 Expires header\n+        if 'Expires' in response.headers:\n+            expires = rfc1123_to_epoch(response.headers['Expires'])\n+            # When parsing Expires header fails RFC 2616 section 14.21 says we\n+            # should treat this as an expiration time in the past.\n+            return max(0, expires - date) if expires else 0\n+\n+        # Fallback to heuristic using last-modified header\n+        # This is not in RFC but on Firefox caching implementation\n+        lastmodified = rfc1123_to_epoch(response.headers.get('Last-Modified'))\n+        if lastmodified and lastmodified <= date:\n+            return (date - lastmodified) / 10\n+\n+        # This request can be cached indefinitely\n+        if response.status in (300, 301, 308):\n+            return self.MAXAGE\n+\n+        # Insufficient information to compute fresshness lifetime\n+        return 0\n+\n+    def _compute_current_age(self, response, request, now):\n+        # Reference nsHttpResponseHead::ComputeCurrentAge\n+        # http://dxr.mozilla.org/mozilla-central/netwerk/protocol/http/nsHttpResponseHead.cpp.html\n+        currentage = 0\n+        # If Date header is not set we assume it is a fast connection, and\n+        # clock is in sync with the server\n+        date = rfc1123_to_epoch(response.headers.get('Date')) or now\n+        if now > date:\n+            currentage = now - date\n+\n+        if 'Age' in response.headers:\n+            try:\n+                age = int(response.headers['Age'])\n+                currentage = max(currentage, age)\n+            except ValueError:\n+                pass\n+\n+        return currentage\n \n-        if request.headers.has_key('cache-control'):\n-            retval = retval and (request.headers['cache-control'].lower().find('no-store') == -1)\n-        #retval = retval and self.policy_request(request)\n-        return retval\n \n class HttpCacheMiddleware(object):\n \n     def __init__(self, settings, stats):\n         if not settings.getbool('HTTPCACHE_ENABLED'):\n             raise NotConfigured\n+        self.policy = load_object(settings['HTTPCACHE_POLICY'])(settings)\n         self.storage = load_object(settings['HTTPCACHE_STORAGE'])(settings)\n         self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')\n-        self.policy = load_object(settings['HTTPCACHE_POLICY'])(settings)\n         self.stats = stats\n \n     @classmethod\n@@ -72,112 +178,72 @@ class HttpCacheMiddleware(object):\n         self.storage.close_spider(spider)\n \n     def process_request(self, request, spider):\n+        # Skip uncacheable requests\n         if not self.policy.should_cache_request(request):\n-            return\n-        response = self.storage.retrieve_response(spider, request)\n-\n-        # Response cached, but stale\n-        if response and type(response) is Request:\n-            # Return None so that Scrapy continues processing\n-            self.stats.inc_value('httpcache/revalidation', spider=spider)\n+            request.meta['_dont_cache'] = True  # flag as uncacheable\n             return\n \n-        if response and self.policy.should_cache_response(response):\n-            self.stats.inc_value('httpcache/hit', spider=spider)\n-            if isinstance(self.policy, RFC2616Policy):\n-                # Response cached and fresh\n-                raise IgnoreRequest(\"Ignored request already in cache: %s\" % request)\n-            else:\n-                response.flags.append('cached')\n-                return response\n-\n-        # Response not cached\n+        # Look for cached response and check if expired\n+        cachedresponse = self.storage.retrieve_response(spider, request)\n+        if cachedresponse is None:\n             self.stats.inc_value('httpcache/miss', spider=spider)\n             if self.ignore_missing:\n+                self.stats.inc_value('httpcache/ignore', spider=spider)\n                 raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n+            return  # first time request\n+\n+        # Return cached response only if not expired\n+        cachedresponse.flags.append('cached')\n+        if self.policy.is_cached_response_fresh(cachedresponse, request):\n+            self.stats.inc_value('httpcache/hit', spider=spider)\n+            return cachedresponse\n+\n+        # Keep a reference to cached response to avoid a second cache lookup on\n+        # process_response hook\n+        request.meta['cached_response'] = cachedresponse\n \n     def process_response(self, request, response, spider):\n-        if (self.policy.should_cache_request(request)\n-            and self.policy.should_cache_response(response)):\n-            if isinstance(self.policy, RFC2616Policy):\n-                if response.status != 304:\n-                    self.storage.store_response(spider, request, response)\n-                    self.stats.inc_value('httpcache/store', spider=spider)\n-                else:\n-                    response.flags.append('cached')\n-                    self.stats.inc_value('httpcache/hit', spider=spider)\n-            else:\n-                if 'cached' not in response.flags:\n-                    self.storage.store_response(spider, request, response)\n-                    self.stats.inc_value('httpcache/store', spider=spider)\n+        # Skip cached responses and uncacheable requests\n+        if 'cached' in response.flags or '_dont_cache' in request.meta:\n+            request.meta.pop('_dont_cache', None)\n             return response\n \n+        # RFC2616 requires origin server to set Date header,\n+        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18\n+        if 'Date' not in response.headers:\n+            response.headers['Date'] = formatdate(usegmt=1)\n \n-class FilesystemCacheStorage(object):\n-\n-    def __init__(self, settings):\n-        self.cachedir = data_path(settings['HTTPCACHE_DIR'])\n-        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n-\n-    def open_spider(self, spider):\n-        pass\n-\n-    def close_spider(self, spider):\n-        pass\n-\n-    def retrieve_response(self, spider, request):\n-        \"\"\"Return response if present in cache, or None otherwise.\"\"\"\n-        metadata = self._read_meta(spider, request)\n-        if metadata is None:\n-            return # not cached\n-        rpath = self._get_request_path(spider, request)\n-        with open(join(rpath, 'response_body'), 'rb') as f:\n-            body = f.read()\n-        with open(join(rpath, 'response_headers'), 'rb') as f:\n-            rawheaders = f.read()\n-        url = metadata.get('response_url')\n-        status = metadata['status']\n-        headers = Headers(headers_raw_to_dict(rawheaders))\n-        respcls = responsetypes.from_args(headers=headers, url=url)\n-        response = respcls(url=url, headers=headers, status=status, body=body)\n+        # Do not validate first-hand responses\n+        cachedresponse = request.meta.pop('cached_response', None)\n+        if cachedresponse is None:\n+            self.stats.inc_value('httpcache/firsthand', spider=spider)\n+            self._cache_response(spider, response, request, cachedresponse)\n             return response\n \n-    def store_response(self, spider, request, response):\n-        \"\"\"Store the given response in the cache.\"\"\"\n-        rpath = self._get_request_path(spider, request)\n-        if not exists(rpath):\n-            os.makedirs(rpath)\n-        metadata = {\n-            'url': request.url,\n-            'method': request.method,\n-            'status': response.status,\n-            'response_url': response.url,\n-            'timestamp': time(),\n-        }\n-        with open(join(rpath, 'meta'), 'wb') as f:\n-            f.write(repr(metadata))\n-        with open(join(rpath, 'pickled_meta'), 'wb') as f:\n-            pickle.dump(metadata, f, protocol=2)\n-        with open(join(rpath, 'response_headers'), 'wb') as f:\n-            f.write(headers_dict_to_raw(response.headers))\n-        with open(join(rpath, 'response_body'), 'wb') as f:\n-            f.write(response.body)\n-        with open(join(rpath, 'request_headers'), 'wb') as f:\n-            f.write(headers_dict_to_raw(request.headers))\n-        with open(join(rpath, 'request_body'), 'wb') as f:\n-            f.write(request.body)\n+        if self.policy.is_cached_response_valid(cachedresponse, response, request):\n+            self.stats.inc_value('httpcache/revalidate', spider=spider)\n+            return cachedresponse\n \n-    def _get_request_path(self, spider, request):\n-        key = request_fingerprint(request)\n-        return join(self.cachedir, spider.name, key[0:2], key)\n+        self.stats.inc_value('httpcache/invalidate', spider=spider)\n+        self._cache_response(spider, response, request, cachedresponse)\n+        return response\n \n-    def _read_meta(self, spider, request):\n-        rpath = self._get_request_path(spider, request)\n-        metapath = join(rpath, 'pickled_meta')\n-        if not exists(metapath):\n-            return # not found\n-        mtime = os.stat(rpath).st_mtime\n-        if 0 < self.expiration_secs < time() - mtime:\n-            return # expired\n-        with open(metapath, 'rb') as f:\n-            return pickle.load(f)\n+    def _cache_response(self, spider, response, request, cachedresponse):\n+        if self.policy.should_cache_response(response, request):\n+            self.stats.inc_value('httpcache/store', spider=spider)\n+            self.storage.store_response(spider, request, response)\n+        else:\n+            self.stats.inc_value('httpcache/uncacheable', spider=spider)\n+\n+\n+from scrapy.contrib.httpcache import FilesystemCacheStorage as _FilesystemCacheStorage\n+class FilesystemCacheStorage(_FilesystemCacheStorage):\n+\n+    def __init__(self, *args, **kwargs):\n+        import warnings\n+        from scrapy.exceptions import ScrapyDeprecationWarning\n+        warnings.warn('Importing FilesystemCacheStorage from '\n+                      'scrapy.contrib.downloadermiddlware.httpcache is '\n+                      'deprecated, use scrapy.contrib.httpcache instead.',\n+                      category=ScrapyDeprecationWarning, stacklevel=1)\n+        super(_FilesystemCacheStorage, self).__init__(*args, **kwargs)\n\n@@ -1,12 +1,9 @@\n import os\n-import calendar\n-import email.utils\n from time import time\n import cPickle as pickle\n-\n-from scrapy import log\n+from email.utils import mktime_tz, parsedate_tz\n+from w3lib.http import headers_raw_to_dict, headers_dict_to_raw\n from scrapy.http import Headers\n-from scrapy.exceptions import IgnoreRequest\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.project import data_path\n@@ -65,139 +62,97 @@ class DbmCacheStorage(object):\n         return request_fingerprint(request)\n \n \n-class BaseRealCacheStorage(object):\n-    \"\"\"\n-    Most of the code was taken from the httplib2 (MIT License)\n-    https://code.google.com/p/httplib2/source/browse/python2/httplib2/__init__.py\n-    \"\"\"\n+class FilesystemCacheStorage(object):\n \n-    def parse_cache_control(self, headers):\n-        retval = {}\n-        if headers.has_key('cache-control'):\n-            parts =  headers['cache-control'].split(',')\n-            parts_with_args = [tuple([x.strip().lower() for x in part.split(\"=\", 1)]) for part in parts if -1 != part.find(\"=\")]\n-            parts_wo_args = [(name.strip().lower(), 1) for name in parts if -1 == name.find(\"=\")]\n-            retval = dict(parts_with_args + parts_wo_args)\n-        return retval\n-    \n-    \n-    def get(self, response_headers, request_headers):\n-        \"\"\"Determine freshness from the Date, Expires and Cache-Control headers.\n-\n-        We don't handle the following:\n-\n-        1. Cache-Control: max-stale\n-        2. Age: headers are not used in the calculations.\n-\n-        Not that this algorithm is simpler than you might think\n-        because we are operating as a private (non-shared) cache.\n-        This lets us ignore 's-maxage'. We can also ignore\n-        'proxy-invalidate' since we aren't a proxy.\n-        We will never return a stale document as\n-        fresh as a design decision, and thus the non-implementation\n-        of 'max-stale'. This also lets us safely ignore 'must-revalidate'\n-        since we operate as if every server has sent 'must-revalidate'.\n-        Since we are private we get to ignore both 'public' and\n-        'private' parameters. We also ignore 'no-transform' since\n-        we don't do any transformations.\n-        The 'no-store' parameter is handled at a higher level.\n-        So the only Cache-Control parameters we look at are:\n-\n-        no-cache\n-        only-if-cached\n-        max-age\n-        min-fresh\n-        \"\"\"\n-\n-        retval = \"STALE\"\n-        cc = self.parse_cache_control(request_headers)\n-        cc_response = self.parse_cache_control(response_headers)\n-        \n-        if request_headers.has_key('pragma') and request_headers['pragma'].lower().find('no-cache') != -1:\n-            retval = \"TRANSPARENT\"\n-            if 'cache-control' not in request_headers:\n-                request_headers['cache-control'] = 'no-cache'\n-        elif cc.has_key('no-cache'):\n-            retval = \"TRANSPARENT\"\n-        elif cc_response.has_key('no-cache'):\n-            retval = \"STALE\"\n-        elif cc.has_key('only-if-cached'):\n-            retval = \"FRESH\"\n-        elif response_headers.has_key('date'):\n-            date = calendar.timegm(email.utils.parsedate_tz(response_headers['date']))\n-            now = time()\n-            current_age = max(0, now - date)\n-            if cc_response.has_key('max-age'):\n-                try:\n-                    freshness_lifetime = int(cc_response['max-age'])\n-                except ValueError:\n-                    freshness_lifetime = 0\n-            elif response_headers.has_key('expires'):\n-                expires = email.utils.parsedate_tz(response_headers['expires'])\n-                if None == expires:\n-                    freshness_lifetime = 0\n-                else:\n-                    freshness_lifetime = max(0, calendar.timegm(expires) - date)\n-            else:\n-                freshness_lifetime = 0\n-            if cc.has_key('max-age'):\n-                try:\n-                    freshness_lifetime = int(cc['max-age'])\n-                except ValueError:\n-                    freshness_lifetime = 0\n-            if cc.has_key('min-fresh'):\n-                try:\n-                    min_fresh = int(cc['min-fresh'])\n-                except ValueError:\n-                    min_fresh = 0\n-                current_age += min_fresh\n-            if freshness_lifetime > current_age:\n-                retval = \"FRESH\"\n-        return retval\n-\n-    def retrieve_cache(self, spider, request, response_headers, response_status, response_url='', response_body=''):\n-        # Determine our course of action:\n-        #   Is the cached entry fresh or stale?\n-        #\n-        # There seems to be three possible answers:\n-        # 1. [FRESH] Return the Response object\n-        # 2. [STALE] Update the Request object with cache validators if available\n-        # 3. [TRANSPARENT] Don't update the Request with cache validators (Cache-Control: no-cache)\n-        entry_disposition = self.get(Headers(response_headers), Headers(request.headers))\n-        \n-        # Per the RFC, requests should not be repeated in these situations\n-        if response_status in [400, 401, 403, 410]:\n-            raise IgnoreRequest(\"Ignored request because cached response status is %d.\" % response_status)\n-        \n-        if entry_disposition == \"FRESH\":\n-            log.msg(\"Cache is FRESH\", level=log.DEBUG, spider=spider)\n-            \n-            headers = Headers(response_headers)\n-            respcls = responsetypes.from_args(headers=headers, url=response_url)\n-            response = respcls(url=response_url, headers=headers, status=response_status, body=response_body)\n-            \n-            return response\n-\n-        new_request = request.copy()\n-        if entry_disposition == \"STALE\":\n-            log.msg(\"Cache is STALE, updating Request object with cache validators\", level=log.DEBUG, spider=spider)\n-            if response_headers.has_key('ETag') and not 'If-None-Match' in request.headers:\n-                new_request.headers['If-None-Match'] = response_headers['ETag']\n-            if response_headers.has_key('Last-Modified') and not 'Last-Modified' in request.headers:\n-                new_request.headers['If-Modified-Since'] = response_headers['Last-Modified']\n-        elif entry_disposition == \"TRANSPARENT\":\n-            log.msg(\"Cache is TRANSPARENT, not adding cache validators to Request object\", level=log.DEBUG, spider=spider)\n-        \n-        return new_request\n-\n-\n-class DbmRealCacheStorage(DbmCacheStorage, BaseRealCacheStorage):\n     def __init__(self, settings):\n-        super(DbmRealCacheStorage, self).__init__(settings)\n+        self.cachedir = data_path(settings['HTTPCACHE_DIR'])\n+        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n+\n+    def open_spider(self, spider):\n+        pass\n+\n+    def close_spider(self, spider):\n+        pass\n \n     def retrieve_response(self, spider, request):\n-        data = self._read_data(spider, request)\n-        if data is None:\n+        \"\"\"Return response if present in cache, or None otherwise.\"\"\"\n+        metadata = self._read_meta(spider, request)\n+        if metadata is None:\n             return  # not cached\n-        else:\n-            return self.retrieve_cache(spider, request, data['headers'], data['status'], data['url'], data['body'])\n+        rpath = self._get_request_path(spider, request)\n+        with open(os.path.join(rpath, 'response_body'), 'rb') as f:\n+            body = f.read()\n+        with open(os.path.join(rpath, 'response_headers'), 'rb') as f:\n+            rawheaders = f.read()\n+        url = metadata.get('response_url')\n+        status = metadata['status']\n+        headers = Headers(headers_raw_to_dict(rawheaders))\n+        respcls = responsetypes.from_args(headers=headers, url=url)\n+        response = respcls(url=url, headers=headers, status=status, body=body)\n+        return response\n+\n+    def store_response(self, spider, request, response):\n+        \"\"\"Store the given response in the cache.\"\"\"\n+        rpath = self._get_request_path(spider, request)\n+        if not os.path.exists(rpath):\n+            os.makedirs(rpath)\n+        metadata = {\n+            'url': request.url,\n+            'method': request.method,\n+            'status': response.status,\n+            'response_url': response.url,\n+            'timestamp': time(),\n+        }\n+        with open(os.path.join(rpath, 'meta'), 'wb') as f:\n+            f.write(repr(metadata))\n+        with open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:\n+            pickle.dump(metadata, f, protocol=2)\n+        with open(os.path.join(rpath, 'response_headers'), 'wb') as f:\n+            f.write(headers_dict_to_raw(response.headers))\n+        with open(os.path.join(rpath, 'response_body'), 'wb') as f:\n+            f.write(response.body)\n+        with open(os.path.join(rpath, 'request_headers'), 'wb') as f:\n+            f.write(headers_dict_to_raw(request.headers))\n+        with open(os.path.join(rpath, 'request_body'), 'wb') as f:\n+            f.write(request.body)\n+\n+    def _get_request_path(self, spider, request):\n+        key = request_fingerprint(request)\n+        return os.path.join(self.cachedir, spider.name, key[0:2], key)\n+\n+    def _read_meta(self, spider, request):\n+        rpath = self._get_request_path(spider, request)\n+        metapath = os.path.join(rpath, 'pickled_meta')\n+        if not os.path.exists(metapath):\n+            return  # not found\n+        mtime = os.stat(rpath).st_mtime\n+        if 0 < self.expiration_secs < time() - mtime:\n+            return  # expired\n+        with open(metapath, 'rb') as f:\n+            return pickle.load(f)\n+\n+\n+def parse_cachecontrol(header):\n+    \"\"\"Parse Cache-Control header\n+\n+    http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n+\n+    >>> cachecontrol_directives('public, max-age=3600')\n+    {'public': None, 'max-age': '3600'}\n+    >>> cachecontrol_directives('')\n+    {}\n+\n+    \"\"\"\n+    directives = {}\n+    for directive in header.split(','):\n+        key, sep, val = directive.strip().partition('=')\n+        if key:\n+            directives[key.lower()] = val if sep else None\n+    return directives\n+\n+\n+def rfc1123_to_epoch(date_str):\n+    try:\n+        return mktime_tz(parsedate_tz(date_str))\n+    except Exception:\n+        return None\n\n@@ -10,31 +10,27 @@ from scrapy.spider import BaseSpider\n from scrapy.settings import Settings\n from scrapy.exceptions import IgnoreRequest\n from scrapy.utils.test import get_crawler\n-from scrapy.contrib.downloadermiddleware.httpcache import \\\n-    FilesystemCacheStorage, HttpCacheMiddleware\n-from scrapy.contrib.httpcache import DbmRealCacheStorage\n+from scrapy.contrib.downloadermiddleware.httpcache import HttpCacheMiddleware\n \n \n-class HttpCacheMiddlewareTest(unittest.TestCase):\n+class _BaseTest(unittest.TestCase):\n \n-    storage_class = FilesystemCacheStorage\n-    realcache_storage_class = DbmRealCacheStorage\n-\n-    dummy_policy = 'scrapy.contrib.downloadermiddleware.httpcache.DummyPolicy'\n-    rfc2616_policy = 'scrapy.contrib.downloadermiddleware.httpcache.RFC2616Policy'\n-\n-    yesterday = email.utils.formatdate(time.time() - 1 * 24 * 60 * 60)\n-    now = email.utils.formatdate()\n-    tomorrow = email.utils.formatdate(time.time() + 1 * 24 * 60 * 60)\n+    storage_class = 'scrapy.contrib.httpcache.DbmCacheStorage'\n+    policy_class = 'scrapy.contrib.downloadermiddleware.httpcache.DummyPolicy'\n \n     def setUp(self):\n+        self.yesterday = email.utils.formatdate(time.time() - 86400)\n+        self.today = email.utils.formatdate()\n+        self.tomorrow = email.utils.formatdate(time.time() + 86400)\n         self.crawler = get_crawler()\n         self.spider = BaseSpider('example.com')\n         self.tmpdir = tempfile.mkdtemp()\n         self.request = Request('http://www.example.com',\n                                headers={'User-Agent': 'test'})\n-        self.response = Response('http://www.example.com', headers=\n-            {'Content-Type': 'text/html'}, body='test body', status=202)\n+        self.response = Response('http://www.example.com',\n+                                 headers={'Content-Type': 'text/html'},\n+                                 body='test body',\n+                                 status=202)\n         self.crawler.stats.open_spider(self.spider)\n \n     def tearDown(self):\n@@ -47,23 +43,21 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n             'HTTPCACHE_DIR': self.tmpdir,\n             'HTTPCACHE_EXPIRATION_SECS': 1,\n             'HTTPCACHE_IGNORE_HTTP_CODES': [],\n-            'HTTPCACHE_POLICY': self.dummy_policy\n+            'HTTPCACHE_POLICY': self.policy_class,\n+            'HTTPCACHE_STORAGE': self.storage_class,\n         }\n         settings.update(new_settings)\n         return Settings(settings)\n \n     @contextmanager\n     def _storage(self, **new_settings):\n-        settings = self._get_settings(**new_settings)\n-        if settings.get('HTTPCACHE_POLICY') == self.dummy_policy:\n-            storage = self.storage_class(settings)\n-        else:\n-            storage = self.realcache_storage_class(settings)\n-        storage.open_spider(self.spider)\n-        try:\n-            yield storage\n-        finally:\n-            storage.close_spider(self.spider)\n+        with self._middleware(**new_settings) as mw:\n+            yield mw.storage\n+\n+    @contextmanager\n+    def _policy(self, **new_settings):\n+        with self._middleware(**new_settings) as mw:\n+            yield mw.policy\n \n     @contextmanager\n     def _middleware(self, **new_settings):\n@@ -75,6 +69,27 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         finally:\n             mw.spider_closed(self.spider)\n \n+    def assertEqualResponse(self, response1, response2):\n+        self.assertEqual(response1.url, response2.url)\n+        self.assertEqual(response1.status, response2.status)\n+        self.assertEqual(response1.headers, response2.headers)\n+        self.assertEqual(response1.body, response2.body)\n+\n+    def assertEqualRequest(self, request1, request2):\n+        self.assertEqual(request1.url, request2.url)\n+        self.assertEqual(request1.headers, request2.headers)\n+        self.assertEqual(request1.body, request2.body)\n+\n+    def assertEqualRequestButWithCacheValidators(self, request1, request2):\n+        self.assertEqual(request1.url, request2.url)\n+        assert not 'If-None-Match' in request1.headers\n+        assert not 'If-Modified-Since' in request1.headers\n+        assert any(h in request2.headers for h in ('If-None-Match', 'If-Modified-Since'))\n+        self.assertEqual(request1.body, request2.body)\n+\n+\n+class DefaultStorageTest(_BaseTest):\n+\n     def test_storage(self):\n         with self._storage() as storage:\n             request2 = self.request.copy()\n@@ -95,11 +110,23 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n             time.sleep(0.5)  # give the chance to expire\n             assert storage.retrieve_response(self.spider, self.request)\n \n+\n+class DbmStorageTest(DefaultStorageTest):\n+\n+    storage_class = 'scrapy.contrib.httpcache.DbmCacheStorage'\n+\n+\n+class FilesystemStorageTest(DefaultStorageTest):\n+\n+    storage_class = 'scrapy.contrib.httpcache.FilesystemCacheStorage'\n+\n+\n+class DefaultMiddlewaretest(_BaseTest):\n+\n     def test_middleware(self):\n         with self._middleware() as mw:\n             assert mw.process_request(self.request, self.spider) is None\n             mw.process_response(self.request, self.response, self.spider)\n-\n             response = mw.process_request(self.request, self.spider)\n             assert isinstance(response, HtmlResponse)\n             self.assertEqualResponse(self.response, response)\n@@ -109,10 +136,8 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         with self._middleware() as mw:\n             req = Request('http://host.com/path')\n             res = Response('http://host2.net/test.html')\n-\n             assert mw.process_request(req, self.spider) is None\n             mw.process_response(req, res, self.spider)\n-\n             cached = mw.process_request(req, self.spider)\n             assert isinstance(cached, Response)\n             self.assertEqualResponse(res, cached)\n@@ -185,192 +210,167 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n             self.assertEqualResponse(self.response, response)\n             assert 'cached' in response.flags\n \n-    def test_middleware_rfc2616policy_response304_not_cached(self):\n-        # test response is not cached because the status is 304 Not Modified\n-        # (so it should be cached already)\n-        with self._middleware(HTTPCACHE_POLICY=self.rfc2616_policy) as mw:\n-            assert mw.process_request(self.request, self.spider) is None\n-            response = Response('http://www.example.com', status=304)\n-            mw.process_response(self.request, response, self.spider)\n \n-            assert 'cached' in response.flags\n-            assert mw.storage.retrieve_response(self.spider, self.request) is None\n-            assert mw.process_request(self.request, self.spider) is None\n+class DummyMiddlewareTest(DefaultStorageTest):\n \n-    def test_middleware_rfc2616policy_response_nostore_not_cached(self):\n-        # test response is not cached because of the Cache-Control 'no-store' directive\n-        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.2\n-        with self._middleware(HTTPCACHE_POLICY=self.rfc2616_policy) as mw:\n-            assert mw.process_request(self.request, self.spider) is None\n-            response = Response('http://www.example.com', headers=\n-                {'Content-Type': 'text/html', 'Cache-Control': 'no-store'},\n-                body='test body', status=200)\n-            mw.process_response(self.request, response, self.spider)\n+    policy_class = 'scrapy.contrib.downloadermiddleware.httpcache.DummyPolicy'\n \n-            assert mw.storage.retrieve_response(self.spider, self.request) is None\n-            assert mw.process_request(self.request, self.spider) is None\n \n-    def test_middleware_rfc2616policy_request_nostore_not_cached(self):\n-        # test response is not cached because of the request's Cache-Control 'no-store' directive\n-        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.2\n-        with self._middleware(HTTPCACHE_POLICY=self.rfc2616_policy) as mw:\n-            request = Request('http://www.example.com',\n-                headers={'User-Agent': 'test', 'Cache-Control': 'no-store'})\n-            assert mw.process_request(request, self.spider) is None\n-            mw.process_response(request, self.response, self.spider)\n+class RFC2616MiddlewareTest(DefaultStorageTest):\n \n-            assert mw.storage.retrieve_response(self.spider, request) is None\n-            assert mw.process_request(request, self.spider) is None\n+    policy_class = 'scrapy.contrib.downloadermiddleware.httpcache.RFC2616Policy'\n \n-    def test_middleware_rfc2616policy_response_cached_and_fresh(self):\n-        # test response cached and fresh\n-        with self._middleware(HTTPCACHE_POLICY=self.rfc2616_policy) as mw:\n-            response = mw.process_response(self.request, self.response, self.spider)\n-            self.assertRaises(IgnoreRequest, mw.process_request, self.request, self.spider)\n-            assert 'cached' not in response.flags\n+    def _process_requestresponse(self, mw, request, response):\n+        try:\n+            result = mw.process_request(request, self.spider)\n+            if result:\n+                assert isinstance(result, (Request, Response))\n+                return result\n+            else:\n+                result = mw.process_response(request, response, self.spider)\n+                assert isinstance(result, Response)\n+                return result\n+        except Exception:\n+            print 'Request', request\n+            print 'Response', response\n+            print 'Result', result\n+            raise\n \n-    def test_middleware_rfc2616policy_response_cached_and_stale(self):\n-        # test response cached but stale\n-        with self._middleware(HTTPCACHE_POLICY=self.rfc2616_policy,\n-            HTTPCACHE_STORAGE = 'scrapy.contrib.httpcache.DbmRealCacheStorage') as mw:\n-            response = Response('http://www.example.com', headers=\n-                {'Content-Type': 'text/html', 'Cache-Control': 'no-cache'},\n-                body='test body', status=200)\n-            mw.process_response(self.request, response, self.spider)\n-            assert mw.process_request(self.request, self.spider) is None\n+    def test_request_cacheability(self):\n+        res0 = Response(self.request.url, status=200,\n+                        headers={'Expires': self.tomorrow})\n+        req0 = Request('http://example.com')\n+        req1 = req0.replace(headers={'Cache-Control': 'no-store'})\n+        with self._middleware() as mw:\n+            res1 = self._process_requestresponse(mw, req1, res0)\n+            self.assertEqualResponse(res1, res0)\n+            assert mw.storage.retrieve_response(self.spider, req1) is None\n+            res2 = self._process_requestresponse(mw, req0, res0)\n+            assert 'cached' not in res2.flags\n+            res3 = mw.process_request(req0, self.spider)\n+            assert 'cached' in res3.flags\n+            self.assertEqualResponse(res2, res3)\n \n-            response = mw.storage.retrieve_response(self.spider, self.request)\n-            assert isinstance(response, Request)\n+    def test_response_cacheability(self):\n+        responses = [\n+            # 304 is not cacheable no matter what servers sends\n+            (False, 304, {}),\n+            (False, 304, {'Last-Modified': self.yesterday}),\n+            (False, 304, {'Expires': self.tomorrow}),\n+            (False, 304, {'Etag': 'bar'}),\n+            (False, 304, {'Cache-Control': 'max-age=3600'}),\n+            # Always obey no-store cache control\n+            (False, 200, {'Cache-Control': 'no-store'}),\n+            (False, 200, {'Cache-Control': 'no-store, max-age=300'}),  # invalid\n+            (False, 200, {'Cache-Control': 'no-store', 'Expires': self.tomorrow}),  # invalid\n+            # Ignore responses missing expiration and/or validation headers\n+            (False, 200, {}),\n+            (False, 302, {}),\n+            (False, 307, {}),\n+            (False, 404, {}),\n+            # Cache responses with expiration and/or validation headers\n+            (True, 200, {'Last-Modified': self.yesterday}),\n+            (True, 203, {'Last-Modified': self.yesterday}),\n+            (True, 300, {'Last-Modified': self.yesterday}),\n+            (True, 301, {'Last-Modified': self.yesterday}),\n+            (True, 401, {'Last-Modified': self.yesterday}),\n+            (True, 404, {'Cache-Control': 'public, max-age=600'}),\n+            (True, 302, {'Expires': self.tomorrow}),\n+            (True, 200, {'Etag': 'foo'}),\n+        ]\n+        with self._middleware() as mw:\n+            for idx, (shouldcache, status, headers) in enumerate(responses):\n+                req0 = Request('http://example-%d.com' % idx)\n+                res0 = Response(req0.url, status=status, headers=headers)\n+                res1 = self._process_requestresponse(mw, req0, res0)\n+                res304 = res0.replace(status=304)\n+                res2 = self._process_requestresponse(mw, req0, res304 if shouldcache else res0)\n+                self.assertEqualResponse(res1, res0)\n+                self.assertEqualResponse(res2, res0)\n+                resc = mw.storage.retrieve_response(self.spider, req0)\n+                if shouldcache:\n+                    self.assertEqualResponse(resc, res1)\n+                    assert 'cached' in res2.flags and res2.status != 304\n+                else:\n+                    self.assertFalse(resc)\n+                    assert 'cached' not in res2.flags\n \n-    def test_storage_rfc2616policy_response_cached_and_fresh(self):\n-        # test response is cached and is fresh\n-        # (response requested should be same as response received)\n-        with self._storage(HTTPCACHE_POLICY=self.rfc2616_policy) as storage:\n-            assert storage.retrieve_response(self.spider, self.request) is None\n+    def test_cached_and_fresh(self):\n+        sampledata = [\n+            (200, {'Date': self.yesterday, 'Expires': self.tomorrow}),\n+            (200, {'Date': self.yesterday, 'Cache-Control': 'max-age=86405'}),\n+            (200, {'Age': '299', 'Cache-Control': 'max-age=300'}),\n+            # Obey max-age if present over any others\n+            (200, {'Date': self.today,\n+                   'Age': '86405',\n+                   'Cache-Control': 'max-age=' + str(86400 * 3),\n+                   'Expires': self.yesterday,\n+                   'Last-Modified': self.yesterday,\n+                   }),\n+            # obey Expires if max-age is not present\n+            (200, {'Date': self.yesterday,\n+                   'Age': '86400',\n+                   'Cache-Control': 'public',\n+                   'Expires': self.tomorrow,\n+                   'Last-Modified': self.yesterday,\n+                   }),\n+            # Default missing Date header to right now\n+            (200, {'Expires': self.tomorrow}),\n+            # Firefox - Expires if age is greater than 10% of (Date - Last-Modified)\n+            (200, {'Date': self.today, 'Last-Modified': self.yesterday, 'Age': str(86400 / 10 - 1)}),\n+            # Firefox - Set one year maxage to permanent redirects missing expiration info\n+            (300, {}), (301, {}), (308, {}),\n+        ]\n+        with self._middleware() as mw:\n+            for idx, (status, headers) in enumerate(sampledata):\n+                req0 = Request('http://example-%d.com' % idx)\n+                res0 = Response(req0.url, status=status, headers=headers)\n+                # cache fresh response\n+                res1 = self._process_requestresponse(mw, req0, res0)\n+                self.assertEqualResponse(res1, res0)\n+                assert 'cached' not in res1.flags\n+                # return fresh cached response without network interaction\n+                res2 = self._process_requestresponse(mw, req0, None)\n+                self.assertEqualResponse(res1, res2)\n+                assert 'cached' in res2.flags\n \n-            response = Response('http://www.example.com', headers=\n-                {'Content-Type': 'text/html', 'Date': self.yesterday, 'Expires': self.tomorrow},\n-                body='test body', status=200)\n-            storage.store_response(self.spider, self.request, response)\n-            response2 = storage.retrieve_response(self.spider, self.request)\n-            self.assertEqualResponse(response, response2)\n+    def test_cached_and_stale(self):\n+        sampledata = [\n+            (200, {'Date': self.today, 'Expires': self.yesterday}),\n+            (200, {'Date': self.today, 'Expires': self.yesterday, 'Last-Modified': self.yesterday}),\n+            (200, {'Expires': self.yesterday}),\n+            (200, {'Expires': self.yesterday, 'ETag': 'foo'}),\n+            (200, {'Expires': self.yesterday, 'Last-Modified': self.yesterday}),\n+            (200, {'Expires': self.tomorrow, 'Age': '86405'}),\n+            (200, {'Cache-Control': 'max-age=86400', 'Age': '86405'}),\n+            # no-cache forces expiration, also revalidation if validators exists\n+            (200, {'Cache-Control': 'no-cache'}),\n+            (200, {'Cache-Control': 'no-cache', 'ETag': 'foo'}),\n+            (200, {'Cache-Control': 'no-cache', 'Last-Modified': self.yesterday}),\n+        ]\n+        with self._middleware() as mw:\n+            for idx, (status, headers) in enumerate(sampledata):\n+                req0 = Request('http://example-%d.com' % idx)\n+                res0a = Response(req0.url, status=status, headers=headers)\n+                # cache expired response\n+                res1 = self._process_requestresponse(mw, req0, res0a)\n+                self.assertEqualResponse(res1, res0a)\n+                assert 'cached' not in res1.flags\n+                # Same request but as cached response is stale a new response must\n+                # be returned\n+                res0b = res0a.replace(body='bar')\n+                res2 = self._process_requestresponse(mw, req0, res0b)\n+                self.assertEqualResponse(res2, res0b)\n+                assert 'cached' not in res2.flags\n+                # Previous response expired too, subsequent request to same\n+                # resource must revalidate and succeed on 304 if validators\n+                # are present\n+                if 'ETag' in headers or 'Last-Modified' in headers:\n+                    res0c = res0b.replace(status=304)\n+                    res3 = self._process_requestresponse(mw, req0, res0c)\n+                    self.assertEqualResponse(res3, res0b)\n+                    assert 'cached' in res3.flags\n \n-    def test_storage_rfc2616policy_response403_cached_and_further_requests_ignored(self):\n-        # test response is cached but further requests are ignored\n-        # because response status is 403 (as per the RFC)\n-        with self._storage(HTTPCACHE_POLICY=self.rfc2616_policy) as storage:\n-            assert storage.retrieve_response(self.spider, self.request) is None\n-\n-            response = Response('http://www.example.com', headers=\n-                {'Content-Type': 'text/html', 'Date': self.yesterday, 'Expires': self.tomorrow},\n-                body='test body', status=403)\n-            storage.store_response(self.spider, self.request, response)\n-            self.assertRaises(IgnoreRequest, storage.retrieve_response,\n-                self.spider, self.request)\n-\n-    def test_storage_rfc2616policy_response_cached_and_stale(self):\n-        # test response is cached and is stale (no cache validators inserted)\n-        # (request should be same as response received)\n-        with self._storage(HTTPCACHE_POLICY=self.rfc2616_policy) as storage:\n-            assert storage.retrieve_response(self.spider, self.request) is None\n-\n-            response = Response('http://www.example.com', headers=\n-                {'Content-Type': 'text/html', 'Date': self.now, 'Expires': self.yesterday},\n-                body='test body', status=200)\n-            storage.store_response(self.spider, self.request, response)\n-            response2 = storage.retrieve_response(self.spider, self.request)\n-            assert isinstance(response2, Request)\n-            self.assertEqualRequest(self.request, response2)\n-\n-    def test_storage_rfc2616policy_response_cached_and_stale_with_cache_validators(self):\n-        # test response is cached and is stale and cache validators are inserted\n-        with self._storage(HTTPCACHE_POLICY=self.rfc2616_policy) as storage:\n-            assert storage.retrieve_response(self.spider, self.request) is None\n-\n-            response = Response('http://www.example.com', headers=\n-                {'Content-Type': 'text/html', 'Date': self.now, 'Expires': self.yesterday,\n-                'Last-Modified': self.yesterday}, body='test body', status=200)\n-            storage.store_response(self.spider, self.request, response)\n-            response2 = storage.retrieve_response(self.spider, self.request)\n-            assert isinstance(response2, Request)\n-            self.assertEqualRequestButWithCacheValidators(self.request, response2)\n-\n-    def test_storage_rfc2616policy_response_cached_and_transparent(self):\n-        # test response is not cached because of the request's Cache-Control 'no-cache' directive\n-        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.2\n-        with self._storage(HTTPCACHE_POLICY=self.rfc2616_policy) as storage:\n-            request = Request('http://www.example.com',\n-                headers={'User-Agent': 'test', 'Cache-Control': 'no-cache'})\n-            assert storage.retrieve_response(self.spider, request) is None\n-            storage.store_response(self.spider, request, self.response)\n-            response = storage.retrieve_response(self.spider, request)\n-            assert isinstance(response, Request)\n-            self.assertEqualRequest(request, response)\n-    \n-    def test_http_cache_request_policy(self):\n-        callback = lambda r: (r.url == 'http://www.example.com/')\n-        \n-        # Should be cached\n-        req, res = Request('http://www.example.com/'), Response('http://www.example.com/')\n-        with self._middleware(HTTPCACHE_POLICY_REQUEST=callback) as mw:\n-            assert mw.process_request(req, self.spider) is None\n-            mw.process_response(req, res, self.spider)\n-\n-            cached = mw.process_request(req, self.spider)\n-            assert isinstance(cached, Response), type(cached)\n-            self.assertEqualResponse(res, cached)\n-            assert 'cached' in cached.flags\n-        \n-        # Should not be cached\n-        req, res = Request('http://www.test.com/'), Response('http://www.test.com/')\n-        with self._middleware(HTTPCACHE_POLICY_REQUEST=callback) as mw:\n-            assert mw.process_request(req, self.spider) is None\n-            mw.process_response(req, res, self.spider)\n-\n-            assert mw.storage.retrieve_response(self.spider, req) is None\n-            assert mw.process_request(req, self.spider) is None\n-            \n-    def test_http_cache_response_policy(self):\n-        callback = lambda r: (r.url == 'http://www.example.com/')\n-        \n-        # Should be cached\n-        req, res = Request('http://www.example.com/'), Response('http://www.example.com/')\n-        with self._middleware(HTTPCACHE_POLICY_RESPONSE=callback) as mw:\n-            assert mw.process_request(req, self.spider) is None\n-            mw.process_response(req, res, self.spider)\n-\n-            cached = mw.process_request(req, self.spider)\n-            assert isinstance(cached, Response), type(cached)\n-            self.assertEqualResponse(res, cached)\n-            assert 'cached' in cached.flags\n-        \n-        # Should not be cached\n-        req, res = Request('http://www.test.com/'), Response('http://www.test.com/')\n-        with self._middleware(HTTPCACHE_POLICY_RESPONSE=callback) as mw:\n-            assert mw.process_request(req, self.spider) is None\n-            mw.process_response(req, res, self.spider)\n-\n-            assert mw.storage.retrieve_response(self.spider, req) is None\n-            assert mw.process_request(req, self.spider) is None\n-\n-    def assertEqualResponse(self, response1, response2):\n-        self.assertEqual(response1.url, response2.url)\n-        self.assertEqual(response1.status, response2.status)\n-        self.assertEqual(response1.headers, response2.headers)\n-        self.assertEqual(response1.body, response2.body)\n-\n-    def assertEqualRequest(self, request1, request2):\n-        self.assertEqual(request1.url, request2.url)\n-        self.assertEqual(request1.headers, request2.headers)\n-        self.assertEqual(request1.body, request2.body)\n-\n-    def assertEqualRequestButWithCacheValidators(self, request1, request2):\n-        self.assertEqual(request1.url, request2.url)\n-        assert not request1.headers.has_key('If-None-Match')\n-        assert not request1.headers.has_key('If-Modified-Since')\n-        assert (request2.headers.has_key('If-None-Match') or \\\n-            request2.headers.has_key('If-Modified-Since'))\n-        self.assertEqual(request1.body, request2.body)\n \n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
