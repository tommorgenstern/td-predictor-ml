{"custom_id": "keras#4b24dc05d2bc929c3091a5560ad5d2d0b65db831", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2631 | Contributors (this commit): 1 | Commits (past 90d): 14 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -48,7 +48,7 @@ WEIGHTS_HASHES = {\n       \"40a20c5548a5e9202f69735ecc06c990e6b7c9d2de39f0361e27baeb24cb7c45\"),\n   \"large\":\n     (\"070c5ed9ed289581e477741d3b34beffa920db8cf590899d6d2c67fba2a198a6\",\n-      \"40a20c5548a5e9202f69735ecc06c990e6b7c9d2de39f0361e27baeb24cb7c45\"),\n+      \"96f02b6f0753d4f543261bc9d09bed650f24dd6bc02ddde3066135b63d23a1cd\"),\n   \"xlarge\":\n     (\"c1f5ccab661354fc3a79a10fa99af82f0fbf10ec65cb894a3ae0815f17a889ee\",\n       \"de3f8a54174130e0cecdc71583354753d557fcf1f4487331558e2a16ba0cfe05\"),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#de36b4544e405a5794b80fd821aa97d8f9a91a90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 71 | Churn Cumulative: 7428 | Contributors (this commit): 9 | Commits (past 90d): 53 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1177,6 +1177,47 @@ class RandomRotation(BaseImageAugmentationLayer):\n     output.set_shape(original_shape)\n     return output\n \n+  def augment_bounding_boxes(self, image, bounding_boxes, transformation):\n+    image = tf.expand_dims(image, 0)\n+    image_shape = tf.shape(image)\n+    h = image_shape[H_AXIS]\n+    w = image_shape[W_AXIS]\n+    bbox_dtype = bounding_boxes.dtype\n+    # origin coordinates, all the points on the image are rotated around this\n+    # point\n+    origin_x, origin_y = int(h / 2), int(w / 2)\n+    angle = transformation['angle']\n+    angle = -angle\n+    # calculate coordinates of all four corners of the bounding box\n+    point = tf.stack([\n+        tf.stack([bounding_boxes[:, 0], bounding_boxes[:, 1]], axis=1),\n+        tf.stack([bounding_boxes[:, 2], bounding_boxes[:, 1]], axis=1),\n+        tf.stack([bounding_boxes[:, 2], bounding_boxes[:, 3]], axis=1),\n+        tf.stack([bounding_boxes[:, 0], bounding_boxes[:, 3]], axis=1)], axis=1)\n+    # point_x : x coordinates of all corners of the bounding box\n+    point_x = tf.gather(point, [0], axis=2)\n+    # point_y : y cordinates of all corners of the bounding box\n+    point_y = tf.gather(point, [1], axis=2)\n+    # rotated bbox coordinates\n+    # new_x : new position of x coordinates of corners of bounding box\n+    new_x = origin_x + tf.multiply(tf.cos(angle), tf.cast(\n+        (point_x - origin_x), dtype=tf.float32)) - tf.multiply(\n+            tf.sin(angle), tf.cast((point_y - origin_y), dtype=tf.float32))\n+    # new_y : new position of y coordinates of corners of bounding box\n+    new_y = origin_y + tf.multiply(tf.sin(angle), tf.cast(\n+        (point_x - origin_x), dtype=tf.float32)) + tf.multiply(\n+            tf.cos(angle), tf.cast((point_y - origin_y), dtype=tf.float32))\n+    # rotated bbox coordinates\n+    out = tf.concat([new_x, new_y], axis=2)\n+    # find readjusted coordinates of bounding box to represent it in corners\n+    # format\n+    min_cordinates = tf.math.reduce_min(out, axis=1)\n+    max_cordinates = tf.math.reduce_max(out, axis=1)\n+    bboxes_out = tf.concat([min_cordinates, max_cordinates], axis=1)\n+    # cordinates cannot be float values, it is casted to int32\n+    bboxes_out = tf.cast(bboxes_out, bbox_dtype)\n+    return bboxes_out\n+\n   def augment_label(self, label, transformation):\n     return label\n \n\n@@ -1612,6 +1612,36 @@ class RandomRotationTest(test_combinations.TestCase):\n       expected_output = np.reshape(expected_output, (5, 5, 1))\n       self.assertAllClose(expected_output, output_image)\n \n+  def test_augment_bbox(self):\n+    with test_utils.use_gpu():\n+      input_image = np.random.random((512, 512, 3)).astype(np.float32)\n+      bboxes = tf.convert_to_tensor([[200,200,400,400],[100,100,300,300]])\n+      # 180 rotation.\n+      layer = image_preprocessing.RandomRotation(factor=(0.5, 0.5))\n+      output_bbox = layer.augment_bounding_boxes(\n+          input_image, bboxes, transformation=layer.get_random_transformation())\n+      expected_output = np.asarray([\n+          [111, 112, 312, 312],\n+          [212, 211, 412, 412]\n+      ]).astype(np.int32)\n+      expected_output = np.reshape(expected_output, ( 2, 4))\n+      self.assertAllClose(expected_output, output_bbox)\n+\n+  def test_augment_bbox_dict_input(self):\n+    with test_utils.use_gpu():\n+      input_image = np.random.random((512, 512, 3)).astype(np.float32)\n+      bboxes = tf.convert_to_tensor([[200,200,400,400],[100,100,300,300]])\n+      input = {'images':input_image, 'bounding_boxes':bboxes}\n+      # 180 rotation.\n+      layer = image_preprocessing.RandomRotation(factor=(0.0833, 0.0833))\n+      output_bbox = layer(input)\n+      expected_output = np.asarray([\n+          [179, 135, 452, 408],\n+          [42, 98, 316, 372]\n+      ]).astype(np.int32)\n+      expected_output = np.reshape(expected_output, ( 2, 4))\n+      self.assertAllClose(expected_output, output_bbox['bounding_boxes'])\n+\n   @test_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1fa09eb3aa2063b415491e014be332a0abec41d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 45 | Files Changed: 2 | Hunks: 11 | Methods Changed: 4 | Complexity Δ (Sum/Max): -10/0 | Churn Δ: 77 | Churn Cumulative: 3831 | Contributors (this commit): 57 | Commits (past 90d): 14 | Contributors (cumulative): 65 | DMM Complexity: 1.0\n\nDIFF:\n@@ -117,30 +117,44 @@ def count_params(weights):\n   return int(sum(np.prod(p) for p in standardized_weight_shapes))\n \n \n-def get_layer_index_bound_by_layer_name(model, layer_names):\n-  \"\"\"Return specific range of layers to plot, mainly for sub-graph plot models.\n+def get_layer_index_bound_by_layer_name(model, layer_range=None):\n+  \"\"\"Return first and last indices of layers in layer_range,\n+  mainly for sub-graph plot models.\n \n   Args:\n     model: tf.keras.Model\n-    layer_names: unique name of layer of the model, type(str)\n+    layer_names: unique name of layer of the model, type(str). If None\n+        all layers will be included\n \n   Returns:\n     return the index value of layer based on its unique name (layer_names)\n   \"\"\"\n-  lower_index = []\n-  upper_index = []\n-  for idx, layer in enumerate(model.layers):\n-    if re.match(layer_names[0], layer.name):\n-      lower_index.append(idx)\n-    if re.match(layer_names[1], layer.name):\n-      upper_index.append(idx)\n+  if layer_range is not None:\n+    if len(layer_range) != 2:\n+      raise ValueError(\n+          'layer_range must be of shape (2,). Received: '\n+          f'layer_range = {layer_range} of length {len(layer_range)}')\n+    if (not isinstance(layer_range[0], str) or\n+        not isinstance(layer_range[1], str)):\n+      raise ValueError(\n+          'layer_range should contain string type only. '\n+          f'Received: {layer_range}')\n+  else:\n+    return [0, len(model.layers)]\n+\n+  lower_index = [idx for idx, layer in enumerate(model.layers)\n+                 if re.match(layer_range[0], layer.name)]\n+  upper_index = [idx for idx, layer in enumerate(model.layers)\n+                 if re.match(layer_range[1], layer.name)]\n+\n   if not lower_index or not upper_index:\n     raise ValueError(\n         'Passed layer_names does not match to layers in the model. '\n-        f'Recieved: {layer_names}')\n+        f'Recieved: {layer_range}')\n+\n   if min(lower_index) > max(upper_index):\n-    return [min(upper_index), max(lower_index)]\n-  return [min(lower_index), max(upper_index)]\n+    return [min(upper_index), max(lower_index)+1]\n+  return [min(lower_index), max(upper_index)+1]\n \n \n def print_summary(model,\n@@ -236,21 +250,8 @@ def print_summary(model,\n     positions.append(line_length)\n     to_display.append('Trainable')\n \n-  if layer_range is not None:\n-    if len(layer_range) != 2:\n-      raise ValueError(\n-          'layer_range must be of shape (2,). Received: '\n-          f'layer_range = {layer_range} of length {len(layer_range)}')\n-    if (not isinstance(layer_range[0], str) or\n-        not isinstance(layer_range[1], str)):\n-      raise ValueError(\n-          'layer_range should contain string type only. '\n-          f'Received: {layer_range}')\n   layer_range = get_layer_index_bound_by_layer_name(\n       model, layer_range)\n-    if layer_range[0] < 0 or layer_range[1] > len(model.layers):\n-      raise ValueError('Both values in layer_range should be in range (0, '\n-                       f'{len(model.layers)}. Received: {layer_range}')\n \n   def print_row(fields, positions, nested_level=0):\n     left_to_print = [str(x) for x in fields]\n@@ -386,9 +387,7 @@ def print_summary(model,\n                '|' * nested_level)\n \n   layers = model.layers\n-  for i, layer in enumerate(layers):\n-    if layer_range and (i < layer_range[0] or i > layer_range[1]):\n-      continue\n+  for layer in model.layers[layer_range[0]:layer_range[1]]:\n     print_layer(layer)\n   print_fn('=' * line_length)\n \n\n@@ -147,20 +147,8 @@ def model_to_dot(model,\n     dot.set('dpi', dpi)\n     dot.set_node_defaults(shape='record')\n \n-  if layer_range is not None:\n-    if len(layer_range) != 2:\n-      raise ValueError(\n-          'layer_range must be of shape (2,). Received: '\n-          f'layer_range = {layer_range} of length {len(layer_range)}')\n-    if (not isinstance(layer_range[0], str) or\n-        not isinstance(layer_range[1], str)):\n-      raise ValueError(\n-          'layer_range should contain string type only. '\n-          f'Received: {layer_range}')\n-    layer_range = layer_utils.get_layer_index_bound_by_layer_name(model, layer_range)\n-    if layer_range[0] < 0 or layer_range[1] > len(model.layers):\n-      raise ValueError('Both values in layer_range should be in range (0, '\n-                       f'{len(model.layers)}. Received: {layer_range}')\n+  layer_range = layer_utils.get_layer_index_bound_by_layer_name(\n+      model, layer_range)\n \n   sub_n_first_node = {}\n   sub_n_last_node = {}\n@@ -179,7 +167,7 @@ def model_to_dot(model,\n \n   # Create graph nodes.\n   for i, layer in enumerate(layers):\n-    if (layer_range) and (i < layer_range[0] or i > layer_range[1]):\n+    if (layer_range) and (i < layer_range[0] or i >= layer_range[1]):\n       continue\n \n     layer_id = str(id(layer))\n@@ -279,7 +267,7 @@ def model_to_dot(model,\n \n   # Connect nodes with edges.\n   for i, layer in enumerate(layers):\n-    if (layer_range) and (i <= layer_range[0] or i > layer_range[1]):\n+    if (layer_range) and (i <= layer_range[0] or i >= layer_range[1]):\n       continue\n     layer_id = str(id(layer))\n     for i, node in enumerate(layer._inbound_nodes):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#391a68f0c0b0e96fee3cb248dec0fa6d9be7b67e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 2642 | Contributors (this commit): 36 | Commits (past 90d): 7 | Contributors (cumulative): 36 | DMM Complexity: None\n\nDIFF:\n@@ -118,8 +118,8 @@ def count_params(weights):\n \n \n def get_layer_index_bound_by_layer_name(model, layer_range=None):\n-  \"\"\"Return first and last indices of layers in layer_range,\n-  mainly for sub-graph plot models.\n+  \"\"\"Return indices for slicing layers list to get just layers\n+  in layer_range, mainly for sub-graph plot models.\n \n   Args:\n     model: tf.keras.Model\n@@ -127,7 +127,8 @@ def get_layer_index_bound_by_layer_name(model, layer_range=None):\n         all layers will be included\n \n   Returns:\n-    return the index value of layer based on its unique name (layer_names)\n+    return the index value of layer based on its unique name (layer_names).\n+        Output will be [first_layer_index, last_layer_index + 1]\n   \"\"\"\n   if layer_range is not None:\n     if len(layer_range) != 2:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aa7b4c3622d4bb2e6e2045f7044294f26fdd0159", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2635 | Contributors (this commit): 1 | Commits (past 90d): 15 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -420,7 +420,9 @@ def ConvNeXt(depths,\n   # Stochastic depth schedule.\n   # This is referred from the original ConvNeXt codebase:\n   # https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py#L86\n-  depth_drop_rates = [x for x in tf.linspace(0.0, drop_path_rate, sum(depths))]\n+  depth_drop_rates = [\n+    float(x) for x in tf.linspace(0.0, drop_path_rate, sum(depths))\n+  ]\n \n   # First apply downsampling blocks and then apply ConvNeXt stages.\n   cur = 0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e59560ff06fd0d7bd75e33106c283ae523b7a3c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 73 | Files Changed: 2 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 112 | Churn Cumulative: 1263 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -261,33 +261,22 @@ class LayerUtilsTest(tf.test.TestCase):\n       lines = reader.readlines()\n       reader.close()\n       check_str = (\n-          'Model: '\n-          '\"trainable\"\\n'\n-          '___________________________________________________________________'\n-          '_________\\n'\n-          ' Layer (type)                Output Shape              Param #   '\n-          'Trainable  \\n'\n-          '==================================================================='\n-          '=========\\n'\n-          ' conv (Conv2D)               (None, 2, 3, 2)           62        N '\n+          'Model: \"trainable\"\\n'\n+          '____________________________________________________________________________\\n'\n+          ' Layer (type)                Output Shape              Param #   Trainable  \\n'\n+          '============================================================================\\n'\n+          ' conv (Conv2D)               (None, 2, 3, 2)           62        N          \\n'\n           '                                                                            \\n'\n-          '                                                                   '\n-          '         '\n-          '\\n flat (Flatten)              (None, 12)                0         '\n-          'Y          \\n'\n-          '                                                                   '\n+          ' flat (Flatten)              (None, 12)                0         Y          \\n'\n           '                                                                            \\n'\n-          ' dense (Dense)               (None, 5)                 65        Y '\n+          ' dense (Dense)               (None, 5)                 65        Y          \\n'\n           '                                                                            \\n'\n-          '                                                                   '\n-          '         \\n'\n-          '================================================================='\n-          '===========\\n'\n-          'Total params: 127\\nTrainable params: 65\\nNon-trainable params: 62\\n'\n-          '___________________________________________________________________'\n-          '_________\\n'\n-          '___________________________________________________________________'\n-          '_________\\n'\n+          '============================================================================\\n'\n+          'Total params: 127\\n'\n+          'Trainable params: 65\\n'\n+          'Non-trainable params: 62\\n'\n+          '____________________________________________________________________________\\n'\n+          '____________________________________________________________________________\\n'\n       )\n \n       fin_str = ''\n@@ -338,53 +327,32 @@ class LayerUtilsTest(tf.test.TestCase):\n       lines = reader.readlines()\n       reader.close()\n       check_str = (\n-          'Model: '\n-          '\"model_2\"\\n'\n-          '___________________________________________________________________'\n-          '_________\\n'\n-          ' Layer (type)                Output Shape              Param #   '\n-          'Trainable  \\n'\n-          '==================================================================='\n-          '=========\\n'\n-          ' input3 (InputLayer)         [(None, None, None, 3)]   0         Y '\n+          'Model: \"model_2\"\\n'\n+          '____________________________________________________________________________\\n'\n+          ' Layer (type)                Output Shape              Param #   Trainable  \\n'\n+          '============================================================================\\n'\n+          ' input3 (InputLayer)         [(None, None, None, 3)]   0         Y          \\n'\n           '                                                                            \\n'\n-          '                                                                   '\n-          '         \\n'\n-          ' model_1 (Functional)        (None, None, None, 3)     24        Y '\n-          '         \\n'\n-          '|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯'\n-          '¯¯¯¯¯¯¯¯|\\n'\n-          '| input1 (InputLayer)       [(None, None, None, 3)]   0         Y  '\n-          '        |\\n'\n-          '|                                                                  '\n-          '        |\\n'\n-          '| model (Functional)        (None, None, None, 3)     24        Y  '\n-          '        |\\n'\n-          '||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯'\n-          '¯¯¯¯¯¯¯||\\n'\n-          '|| input2 (InputLayer)     [(None, None, None, 3)]   0         Y   '\n+          ' model_1 (Functional)        (None, None, None, 3)     24        Y          \\n'\n+          '|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n'\n+          '| input1 (InputLayer)       [(None, None, None, 3)]   0         Y          |\\n'\n           '|                                                                          |\\n'\n-          '||                                                                 '\n-          '       ||\\n'\n-          '|| conv2d (Conv2D)         (None, None, None, 3)     12        N   '\n-          '       ||\\n'\n-          '||                                                                 '\n-          '       ||\\n'\n-          '|| batch_normalization (BatchN  (None, None, None, 3)  12      Y   '\n-          '       ||\\n'\n-          '|| ormalization)                                                   '\n-          '       ||\\n'\n-          '|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯'\n-          '¯¯¯¯¯¯¯¯|\\n'\n-          '¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯'\n-          '¯¯¯¯¯¯¯¯¯\\n'\n-          '==================================================================='\n-          '=========\\n'\n-          'Total params: 24\\nTrainable params: 6\\nNon-trainable params: 18\\n'\n-          '___________________________________________________________________'\n-          '_________\\n'\n-          '___________________________________________________________________'\n-          '_________\\n'\n+          '| model (Functional)        (None, None, None, 3)     24        Y          |\\n'\n+          '||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\\n'\n+          '|| input2 (InputLayer)     [(None, None, None, 3)]   0         Y          ||\\n'\n+          '||                                                                        ||\\n'\n+          '|| conv2d (Conv2D)         (None, None, None, 3)     12        N          ||\\n'\n+          '||                                                                        ||\\n'\n+          '|| batch_normalization (BatchN  (None, None, None, 3)  12      Y          ||\\n'\n+          '|| ormalization)                                                          ||\\n'\n+          '|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n'\n+          '¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\\n'\n+          '============================================================================\\n'\n+          'Total params: 24\\n'\n+          'Trainable params: 6\\n'\n+          'Non-trainable params: 18\\n'\n+          '____________________________________________________________________________\\n'\n+          '____________________________________________________________________________\\n'\n       )\n \n       fin_str = ''\n\n@@ -223,10 +223,8 @@ class ModelToDotFormatTest(tf.test.TestCase, parameterized.TestCase):\n def get_layer_ids_from_model(model, layer_range):\n   layer_range = layer_utils.get_layer_index_bound_by_layer_name(\n       model, layer_range)\n-  layer_ids_from_model = []\n-  for i, layer in enumerate(model.layers):\n-    if i >= layer_range[0] and i <= layer_range[1]:\n-      layer_ids_from_model.append(str(id(layer)))\n+  layer_ids_from_model = [\n+      str(id(layer)) for layer in model.layers[layer_range[0]:layer_range[1]]]\n   return layer_ids_from_model\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c9e5b0d67b211a88ad2d849327df4b748ffbe2d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2637 | Contributors (this commit): 1 | Commits (past 90d): 16 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -34,7 +34,7 @@ from keras.engine import training\n import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n-BASE_WEIGHTS_PATH = \"https://storage.googleapis.com/convnext-tf/keras-applications-temp/convnext/\"\n+BASE_WEIGHTS_PATH = \"https://storage.googleapis.com/tensorflow/keras-applications/convnext/\"\n \n WEIGHTS_HASHES = {\n   \"tiny\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b22e2cf260baeaafbb3e11daf24a6ae72c7d61f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 191 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -143,6 +143,7 @@ def _train_with_recompute(n_steps):\n class GradientCheckpointTest(tf.test.TestCase):\n \n   def test_raises_oom_exception(self):\n+    self.skipTest('b/232015009: flaky test')\n     if not _limit_gpu_memory():\n       self.skipTest('No virtual GPUs found')\n     with self.assertRaises(Exception) as context:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06f5ef7989db314ee210455b04fe6f71e8dc57a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 57 | Files Changed: 2 | Hunks: 34 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 112 | Churn Cumulative: 2606 | Contributors (this commit): 4 | Commits (past 90d): 38 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -24,8 +24,10 @@ import warnings\n import random\n \n import numpy as np\n+from tensorflow.python.util.tf_export import keras_export\n \n \n+@keras_export('keras.utils.split_dataset', v1=[])\n def split_dataset(dataset,\n                   left_size=None,\n                   right_size=None,\n@@ -53,8 +55,9 @@ def split_dataset(dataset,\n   dataset_type_spec = _get_type_spec(dataset)\n \n   if dataset_type_spec not in [tf.data.Dataset, list, tuple, np.ndarray]:\n-    raise TypeError('`dataset` must be either a tf.data.Dataset object '\n-                    f'or a list/tuple of arrays. Received : {type(dataset)}')\n+    raise TypeError('The `dataset` argument must be either a `tf.data.Dataset` '\n+                    'object or a list/tuple of arrays. '\n+                    f'Received: dataset={dataset} of type {type(dataset)}')\n \n   if right_size is None and left_size is None:\n     raise ValueError('At least one of the `left_size` or `right_size` '\n@@ -114,7 +117,7 @@ def _convert_dataset_to_list(dataset,\n         samples. Defaults to True.\n \n   Returns:\n-      List: A list of tuples/numpy arrays.\n+      List: A list of tuples/NumPy arrays.\n   \"\"\"\n   dataset_iterator = _get_data_iterator_from_dataset(dataset, dataset_type_spec)\n   dataset_as_list = []\n@@ -157,15 +160,15 @@ def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n       expected_shape = dataset[0].shape\n       for i, element in enumerate(dataset):\n         if np.array(element).shape[0] != expected_shape[0]:\n-          raise ValueError('Received a list of numpy arrays with different '\n+          raise ValueError('Received a list of NumPy arrays with different '\n                            f'lengths. Mismatch found at index {i}, '\n                            f'Expected shape={expected_shape} '\n                            f'Received shape={np.array(element).shape}.'\n-                           f'Please provide a list of numpy arrays with '\n-                           f'same length.')\n+                           f'Please provide a list of NumPy arrays with '\n+                           f'the same length.')\n     else:\n-      raise ValueError('Expected a list of numpy.ndarrays,'\n-                       'Received: {}'.format(type(dataset[0])))\n+      raise ValueError('Expected a list of `numpy.ndarray` objects,'\n+                       f'Received: {type(dataset[0])}')\n \n     return iter(zip(*dataset))\n   elif dataset_type_spec == tuple:\n@@ -177,15 +180,15 @@ def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n       expected_shape = dataset[0].shape\n       for i, element in enumerate(dataset):\n         if np.array(element).shape[0] != expected_shape[0]:\n-          raise ValueError('Received a tuple of numpy arrays with different '\n+          raise ValueError('Received a tuple of NumPy arrays with different '\n                            f'lengths. Mismatch found at index {i}, '\n                            f'Expected shape={expected_shape} '\n                            f'Received shape={np.array(element).shape}.'\n-                           f'Please provide a tuple of numpy arrays with '\n-                           'same length.')\n+                           f'Please provide a tuple of NumPy arrays with '\n+                           'the same length.')\n     else:\n-      raise ValueError('Expected a tuple of numpy.ndarrays, '\n-                       'Received: {}'.format(type(dataset[0])))\n+      raise ValueError('Expected a tuple of `numpy.ndarray` objects, '\n+                       f'Received: {type(dataset[0])}')\n \n     return iter(zip(*dataset))\n   elif dataset_type_spec == tf.data.Dataset:\n@@ -231,8 +234,8 @@ def _get_next_sample(dataset_iterator, ensure_shape_similarity,\n     yield first_sample\n   except StopIteration:\n     raise ValueError('Received an empty Dataset. `dataset` must '\n-                     'be a non-empty list/tuple of numpy.ndarrays '\n-                     'or tf.data.Dataset objects.')\n+                     'be a non-empty list/tuple of `numpy.ndarray` objects '\n+                     'or `tf.data.Dataset` objects.')\n \n   for i, sample in enumerate(dataset_iterator):\n     if ensure_shape_similarity:\n@@ -248,9 +251,9 @@ def _get_next_sample(dataset_iterator, ensure_shape_similarity,\n         if int(cur_time - start_time) > 10 and data_size_warning_flag:\n           warnings.warn(\n               'The dataset is taking longer than 10 seconds to '\n-              'iterate. This may be due to the size of the dataset. '\n-              'Please consider using a smaller dataset'\n-              '(e.g. < 10,000 samples).',\n+              'iterate over. This may be due to the size of the dataset. '\n+              'Keep in mind that the `split_dataset` utility is only for '\n+              'small in-memory dataset (e.g. < 10,000 samples).',\n               category=ResourceWarning,\n               source='split_dataset')\n           data_size_warning_flag = False\n@@ -313,7 +316,8 @@ def _rescale_dataset_split_sizes(left_size, right_size, total_length):\n \n   # check right_size is a integer or float\n   if right_size is not None and right_size_type not in [int, float]:\n-    raise TypeError(f'Invalid `right_size` Type.Expected: int or float or None,'\n+    raise TypeError(f'Invalid `right_size` Type. '\n+                    'Expected: int or float or None,'\n                     f'Received: type(right_size)={right_size_type}.')\n \n   # check left_size and right_size are non-zero\n@@ -325,7 +329,7 @@ def _rescale_dataset_split_sizes(left_size, right_size, total_length):\n   if (left_size_type == int and (left_size <= 0 or left_size >= total_length) or\n       left_size_type == float and (left_size <= 0 or left_size >= 1)):\n     raise ValueError('`left_size` should be either a positive integer '\n-                     f'and smaller than {total_length} or a float '\n+                     f'smaller than {total_length}, or a float '\n                      'within the range `[0, 1]`. Received: left_size='\n                      f'{left_size}')\n \n@@ -368,7 +372,7 @@ def _rescale_dataset_split_sizes(left_size, right_size, total_length):\n   for split, side in [(left_size, 'left'), (right_size, 'right')]:\n     if split == 0:\n       raise ValueError(f'With `dataset` of length={total_length}, `left_size`='\n-                       '{left_size} and `right_size`={right_size}.'\n+                       f'{left_size} and `right_size`={right_size}.'\n                        f'Resulting {side} side dataset split will be empty. '\n                        'Adjust any of the aforementioned parameters')\n \n@@ -460,8 +464,7 @@ def index_directory(directory,\n         raise ValueError(\n             'The `class_names` passed did not match the '\n             'names of the subdirectories of the target directory. '\n-            'Expected: %s, but received: %s' %\n-            (subdirs, class_names))\n+            f'Expected: {subdirs}, but received: {class_names}')\n   class_indices = dict(zip(class_names, range(len(class_names))))\n \n   # Build an index of the files\n@@ -482,9 +485,9 @@ def index_directory(directory,\n   if labels not in ('inferred', None):\n     if len(labels) != len(filenames):\n       raise ValueError('Expected the lengths of `labels` to match the number '\n-                       'of files in the target directory. len(labels) is %s '\n-                       'while we found %s files in %s.' % (\n-                           len(labels), len(filenames), directory))\n+                       'of files in the target directory. len(labels) is '\n+                       f'{len(labels)} while we found {len(filenames)} files '\n+                       f'in directory {directory}.')\n   else:\n     i = 0\n     labels = np.zeros((len(filenames),), dtype='int32')\n@@ -493,10 +496,10 @@ def index_directory(directory,\n       i += len(partial_labels)\n \n   if labels is None:\n-    print('Found %d files.' % (len(filenames),))\n+    print(f'Found {len(filenames)} files.')\n   else:\n-    print('Found %d files belonging to %d classes.' %\n-          (len(filenames), len(class_names)))\n+    print(f'Found {len(filenames)} files belonging '\n+          f'to {len(class_names)} classes.')\n   pool.close()\n   pool.join()\n   file_paths = [tf.io.gfile.join(directory, fname) for fname in filenames]\n@@ -570,16 +573,16 @@ def get_training_or_validation_split(samples, labels, validation_split, subset):\n \n   num_val_samples = int(validation_split * len(samples))\n   if subset == 'training':\n-    print('Using %d files for training.' % (len(samples) - num_val_samples,))\n+    print(f'Using {len(samples) - num_val_samples} files for training.')\n     samples = samples[:-num_val_samples]\n     labels = labels[:-num_val_samples]\n   elif subset == 'validation':\n-    print('Using %d files for validation.' % (num_val_samples,))\n+    print(f'Using {num_val_samples} files for validation.')\n     samples = samples[-num_val_samples:]\n     labels = labels[-num_val_samples:]\n   else:\n     raise ValueError('`subset` must be either \"training\" '\n-                     'or \"validation\", received: %s' % (subset,))\n+                     f'or \"validation\", received: {subset}')\n   return samples, labels\n \n \n@@ -622,14 +625,14 @@ def check_validation_split_arg(validation_split, subset, shuffle, seed):\n   \"\"\"\n   if validation_split and not 0 < validation_split < 1:\n     raise ValueError(\n-        '`validation_split` must be between 0 and 1, received: %s' %\n-        (validation_split,))\n+        '`validation_split` must be between 0 and 1, '\n+        f'received: {validation_split}')\n   if (validation_split or subset) and not (validation_split and subset):\n     raise ValueError(\n         'If `subset` is set, `validation_split` must be set, and inversely.')\n   if subset not in ('training', 'validation', 'both', None):\n     raise ValueError('`subset` must be either \"training\", '\n-                     '\"validation\" or \"both\", received: %s' % (subset,))\n+                     f'\"validation\" or \"both\", received: {subset}')\n   if validation_split and shuffle and seed is None:\n     raise ValueError(\n         'If using `validation_split` and shuffling the data, you must provide '\n\n@@ -60,14 +60,14 @@ class SplitDatasetTest(tf.test.TestCase):\n \n   def test_dataset_with_invalid_shape(self):\n     with self.assertRaisesRegex(\n-        ValueError, 'Received a list of numpy arrays '\n-        'with different length'):\n+        ValueError, 'Received a list of NumPy arrays '\n+        'with different lengths'):\n       dataset = [np.ones(shape=(200, 32)), np.zeros(shape=(100, 32))]\n       dataset_utils.split_dataset(dataset, left_size=4)\n \n     with self.assertRaisesRegex(\n-        ValueError, 'Received a tuple of numpy arrays '\n-        'with different length'):\n+        ValueError, 'Received a tuple of NumPy arrays '\n+        'with different lengths'):\n       dataset = (np.ones(shape=(200, 32)), np.zeros(shape=(201, 32)))\n       dataset_utils.split_dataset(dataset, left_size=4)\n \n@@ -293,35 +293,30 @@ class SplitDatasetTest(tf.test.TestCase):\n \n   def test_invalid_dataset(self):\n     with self.assertRaisesRegex(\n-        TypeError, '`dataset` must be either a tf.data.Dataset '\n-        'object or a list/tuple of arrays. Received '\n-        ': <class \\'NoneType\\'>'):\n+        TypeError, 'The `dataset` argument must be either a `tf.data.Dataset` '\n+        'object or a list/tuple of arrays.'):\n       dataset_utils.split_dataset(dataset=None, left_size=5)\n     with self.assertRaisesRegex(\n-        TypeError, '`dataset` must be either a tf.data.Dataset '\n-        'object or a list/tuple of arrays. Received '\n-        ': <class \\'int\\'>'):\n+        TypeError, 'The `dataset` argument must be either a `tf.data.Dataset` '\n+        'object or a list/tuple of arrays.'):\n       dataset_utils.split_dataset(dataset=1, left_size=5)\n     with self.assertRaisesRegex(\n-        TypeError, '`dataset` must be either a tf.data.Dataset '\n-        'object or a list/tuple of arrays. Received '\n-        ': <class \\'float\\'>'):\n+        TypeError, 'The `dataset` argument must be either a `tf.data.Dataset` '\n+        'object or a list/tuple of arrays.'):\n       dataset_utils.split_dataset(dataset=float(1.2), left_size=5)\n     with self.assertRaisesRegex(\n-        TypeError, '`dataset` must be either a tf.data.Dataset '\n-        'object or a list/tuple of arrays. Received '\n-        ': <class \\'dict\\'>'):\n+        TypeError, 'The `dataset` argument must be either a `tf.data.Dataset` '\n+        'object or a list/tuple of arrays.'):\n       dataset_utils.split_dataset(dataset=dict({}), left_size=5)\n     with self.assertRaisesRegex(\n-        TypeError, '`dataset` must be either a tf.data.Dataset '\n-        'object or a list/tuple of arrays. Received '\n-        ': <class \\'float\\'>'):\n+        TypeError, 'The `dataset` argument must be either a `tf.data.Dataset` '\n+        'object or a list/tuple of arrays.'):\n       dataset_utils.split_dataset(dataset=float('INF'), left_size=5)\n \n   def test_valid_left_and_right_sizes(self):\n     dataset = np.array([1, 2, 3])\n     splitted_dataset = dataset_utils.split_dataset(dataset, 1, 2)\n-    assert (len(splitted_dataset) == 2)\n+    self.assertLen(splitted_dataset, 2)\n     left_split, right_split = splitted_dataset\n     self.assertEqual(len(left_split), 1)\n     self.assertEqual(len(right_split), 2)\n@@ -358,7 +353,7 @@ class SplitDatasetTest(tf.test.TestCase):\n     dataset = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n     splitted_dataset = dataset_utils.split_dataset(\n         dataset, left_size=0.1, right_size=0.9)\n-    assert (len(splitted_dataset) == 2)\n+    self.assertLen(splitted_dataset, 2)\n     left_split, right_split = splitted_dataset\n     self.assertEqual(len(left_split), 1)\n     self.assertEqual(len(right_split), 9)\n@@ -368,7 +363,7 @@ class SplitDatasetTest(tf.test.TestCase):\n     dataset = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n     splitted_dataset = dataset_utils.split_dataset(\n         dataset, left_size=2, right_size=5)\n-    assert (len(splitted_dataset) == 2)\n+    self.assertLen(splitted_dataset, 2)\n     left_split, right_split = splitted_dataset\n     self.assertEqual(len(left_split), 2)\n     self.assertEqual(len(right_split), 5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c38a617ec9f9321b097ddb1e6ba801eacc1b6e77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1536 | Contributors (this commit): 16 | Commits (past 90d): 3 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -241,7 +241,7 @@ class L1L2(Regularizer):\n     if self.l1:\n       regularization += self.l1 * tf.reduce_sum(tf.abs(x))\n     if self.l2:\n-      regularization += self.l2 * tf.reduce_sum(tf.square(x))\n+      regularization += 2.0 * self.l2 * tf.nn.l2_loss(x)\n     return regularization\n \n   def get_config(self):\n@@ -310,7 +310,7 @@ class L2(Regularizer):\n     self.l2 = backend.cast_to_floatx(l2)\n \n   def __call__(self, x):\n-    return self.l2 * tf.reduce_sum(tf.square(x))\n+    return 2.0 * self.l2 * tf.nn.l2_loss(x)\n \n   def get_config(self):\n     return {'l2': float(self.l2)}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2944bc68a9372606c1fd5de2f76e00076b5d26e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1538 | Contributors (this commit): 17 | Commits (past 90d): 4 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -368,7 +368,7 @@ class OrthogonalRegularizer(Regularizer):\n       size = inputs.shape[1]\n     product_no_diagonal = product * (1. - tf.eye(size, dtype=inputs.dtype))\n     num_pairs = size * (size - 1.) / 2.\n-    return self.factor * 0.5 * tf.reduce_sum(product_no_diagonal) / num_pairs\n+    return self.factor * 0.5 * tf.reduce_sum(tf.abs(product_no_diagonal)) / num_pairs\n \n   def get_config(self):\n     return {'factor': float(self.factor), 'mode': self.mode}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5541781cf29e6315c57df63172fee6fcdc1cc984", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 22 | Files Changed: 2 | Hunks: 16 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 69 | Churn Cumulative: 7497 | Contributors (this commit): 9 | Commits (past 90d): 53 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -35,6 +35,7 @@ W_AXIS = -2\n \n IMAGES = 'images'\n LABELS = 'labels'\n+TARGETS = 'targets'\n BOUNDING_BOXES = 'bounding_boxes'\n \n \n@@ -349,6 +350,21 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     \"\"\"\n     raise NotImplementedError()\n \n+  @doc_controls.for_subclass_implementers\n+  def augment_target(self, target, transformation):\n+    \"\"\"Augment a single target during training.\n+\n+    Args:\n+      target: 1D label to the layer. Forwarded from `layer.call()`.\n+      transformation: The transformation object produced by\n+        `get_random_transformation`. Used to coordinate the randomness between\n+        image, label and bounding box.\n+\n+    Returns:\n+      output 1D tensor, which will be forward to `layer.call()`.\n+    \"\"\"\n+    return self.augment_label(target, transformation)\n+\n   @doc_controls.for_subclass_implementers\n   def augment_bounding_boxes(self, image, bounding_boxes, transformation=None):\n     \"\"\"Augment bounding boxes for one image during training.\n@@ -387,12 +403,12 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n   def call(self, inputs, training=True):\n     inputs = self._ensure_inputs_are_compute_dtype(inputs)\n     if training:\n-      inputs, is_dict = self._format_inputs(inputs)\n+      inputs, is_dict, use_targets = self._format_inputs(inputs)\n       images = inputs[IMAGES]\n       if images.shape.rank == 3:\n-        return self._format_output(self._augment(inputs), is_dict)\n+        return self._format_output(self._augment(inputs), is_dict, use_targets)\n       elif images.shape.rank == 4:\n-        return self._format_output(self._batch_augment(inputs), is_dict)\n+        return self._format_output(self._batch_augment(inputs), is_dict, use_targets)\n       else:\n         raise ValueError('Image augmentation layers are expecting inputs to be '\n                          'rank 3 (HWC) or 4D (NHWC) tensors. Got shape: '\n@@ -409,7 +425,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     image = self.augment_image(image, transformation=transformation)\n     result = {IMAGES: image}\n     if label is not None:\n-      label = self.augment_label(label, transformation=transformation)\n+      label = self.augment_target(label, transformation=transformation)\n       result[LABELS] = label\n     if bounding_box is not None:\n       bounding_box = self.augment_bounding_boxes(\n@@ -423,17 +439,25 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n   def _format_inputs(self, inputs):\n     if tf.is_tensor(inputs):\n       # single image input tensor\n-      return {IMAGES: inputs}, False\n-    elif isinstance(inputs, dict):\n+      return {IMAGES: inputs}, False, False\n+    elif isinstance(inputs, dict) and TARGETS in inputs:\n       # TODO(scottzhu): Check if it only contains the valid keys\n-      return inputs, True\n+      inputs[LABELS] = inputs[TARGETS]\n+      del inputs[TARGETS]\n+      return inputs, True, True\n+    elif isinstance(inputs, dict):\n+      return inputs, True, False\n     else:\n       raise ValueError(\n           f'Expect the inputs to be image tensor or dict. Got {inputs}')\n \n-  def _format_output(self, output, is_dict):\n+  def _format_output(self, output, is_dict, use_targets):\n     if not is_dict:\n       return output[IMAGES]\n+    elif use_targets:\n+      output[TARGETS] = output[LABELS]\n+      del output[LABELS]\n+      return output\n     else:\n       return output\n \n@@ -491,24 +515,16 @@ class RandomCrop(BaseImageAugmentationLayer):\n     self.seed = seed\n \n   def call(self, inputs, training=True):\n-    inputs = self._ensure_inputs_are_compute_dtype(inputs)\n-    inputs, is_dict = self._format_inputs(inputs)\n+\n     if training:\n-      images = inputs['images']\n-      if images.shape.rank == 3:\n-        return self._format_output(self._augment(inputs), is_dict)\n-      elif images.shape.rank == 4:\n-        return self._format_output(self._batch_augment(inputs), is_dict)\n-      else:\n-        raise ValueError('Image augmentation layers are expecting inputs to be '\n-                         'rank 3 (HWC) or 4D (NHWC) tensors. Got shape: '\n-                         f'{images.shape}')\n+      return super().call(inputs, training)\n     else:\n+      inputs = self._ensure_inputs_are_compute_dtype(inputs)\n+      inputs, is_dict, targets = self._format_inputs(inputs)\n       output = inputs\n-      output['images'] = self._resize(inputs['images'])\n       # self._resize() returns valid results for both batched and unbatched\n-      # input\n-      return self._format_output(output, is_dict)\n+      output['images'] = self._resize(inputs['images'])\n+      return self._format_output(output, is_dict, targets)\n \n   def get_random_transformation(self,\n                                 image=None,\n\n@@ -2224,6 +2224,15 @@ class BaseImageAugmentationLayerTest(test_combinations.TestCase):\n     expected_output = {'images': image + 2.0, 'labels': label + 2.0}\n     self.assertAllClose(output, expected_output)\n \n+  def test_augment_image_and_target(self):\n+    add_layer = RandomAddLayer(fixed_value=2.0)\n+    image = np.random.random(size=(8, 8, 3)).astype('float32')\n+    label = np.random.random(size=(1,)).astype('float32')\n+\n+    output = add_layer({'images': image, 'targets': label})\n+    expected_output = {'images': image + 2.0, 'targets': label + 2.0}\n+    self.assertAllClose(output, expected_output)\n+\n   def test_augment_batch_images_and_labels(self):\n     add_layer = RandomAddLayer()\n     images = np.random.random(size=(2, 8, 8, 3)).astype('float32')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#23a9149a8da92767b18f1e0e02de99d60e4e602f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1540 | Contributors (this commit): 17 | Commits (past 90d): 5 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -241,6 +241,7 @@ class L1L2(Regularizer):\n     if self.l1:\n       regularization += self.l1 * tf.reduce_sum(tf.abs(x))\n     if self.l2:\n+      # equivalent to \"self.l2 * tf.reduce_sum(tf.square(x))\"\n       regularization += 2.0 * self.l2 * tf.nn.l2_loss(x)\n     return regularization\n \n@@ -310,6 +311,7 @@ class L2(Regularizer):\n     self.l2 = backend.cast_to_floatx(l2)\n \n   def __call__(self, x):\n+    # equivalent to \"self.l2 * tf.reduce_sum(tf.square(x))\"\n     return 2.0 * self.l2 * tf.nn.l2_loss(x)\n \n   def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
