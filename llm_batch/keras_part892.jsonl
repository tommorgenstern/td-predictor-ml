{"custom_id": "keras#1ea81a16497644b5130862b0702c7315b6dfc887", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 162 | Lines Deleted: 27 | Files Changed: 6 | Hunks: 31 | Methods Changed: 18 | Complexity Δ (Sum/Max): 21/13 | Churn Δ: 189 | Churn Cumulative: 4845 | Contributors (this commit): 21 | Commits (past 90d): 25 | Contributors (cumulative): 58 | DMM Complexity: 0.8068181818181818\n\nDIFF:\n@@ -2185,6 +2185,15 @@ def swapaxes(x, axis1, axis2):\n \n \n def take(x, indices, axis=None):\n+    x = convert_to_tensor(x)\n+    if axis is None:\n+        x = tf.reshape(x, (-1,))\n+        axis = 0\n+\n+    def fix_negative_indices(i):\n+        # Correct the indices using \"fill\" mode which is the same as in jax\n+        return tf.where(i < 0, i + tf.cast(tf.shape(x)[axis], i.dtype), i)\n+\n     if isinstance(indices, tf.SparseTensor):\n         if x.dtype not in (tf.float16, tf.float32, tf.float64, tf.bfloat16):\n             warnings.warn(\n@@ -2192,16 +2201,19 @@ def take(x, indices, axis=None):\n                 f\"`x.dtype={x.dtype}` when `indices` is a sparse tensor; \"\n                 \"densifying `indices`.\"\n             )\n-            return take(x, convert_to_tensor(indices, sparse=False), axis=axis)\n-        if axis is None:\n-            x = tf.reshape(x, (-1,))\n+            indices = convert_to_tensor(indices, sparse=False)\n         elif axis != 0:\n             warnings.warn(\n                 \"`take` with the TensorFlow backend does not support \"\n                 f\"`axis={axis}` when `indices` is a sparse tensor; \"\n                 \"densifying `indices`.\"\n             )\n-            return take(x, convert_to_tensor(indices, sparse=False), axis=axis)\n+            indices = convert_to_tensor(indices, sparse=False)\n+        else:\n+            indices = sparse.sparse_with_values(\n+                indices, fix_negative_indices(indices.values)\n+            )\n+            # `expand_dims` on `indices` prevents combiner from being applied.\n             output = tf.nn.safe_embedding_lookup_sparse(\n                 embedding_weights=tf.convert_to_tensor(x),\n                 sparse_ids=tf.sparse.expand_dims(indices, axis=-1),\n@@ -2209,18 +2221,14 @@ def take(x, indices, axis=None):\n             )\n             output.set_shape(indices.shape + output.shape[len(indices.shape) :])\n             return output\n+    elif isinstance(indices, tf.RaggedTensor):\n+        indices = indices.with_values(fix_negative_indices(indices.values))\n+        if axis == 0:\n+            return tf.nn.embedding_lookup(x, indices)\n+        else:\n+            return tf.gather(x, indices, axis=axis)\n \n-    x = convert_to_tensor(x)\n-    indices = convert_to_tensor(indices)\n-    if axis is None:\n-        x = tf.reshape(x, [-1])\n-        axis = 0\n-    # Correct the indices using \"fill\" mode which is the same as in jax\n-    indices = tf.where(\n-        indices < 0,\n-        indices + tf.cast(tf.shape(x)[axis], indices.dtype),\n-        indices,\n-    )\n+    indices = fix_negative_indices(convert_to_tensor(indices))\n     return tf.gather(x, indices, axis=axis)\n \n \n\n@@ -8,6 +8,7 @@ from keras.src import ops\n from keras.src import quantizers\n from keras.src import regularizers\n from keras.src.api_export import keras_export\n+from keras.src.backend import KerasTensor\n from keras.src.layers.layer import Layer\n \n \n@@ -156,6 +157,12 @@ class Embedding(Layer):\n     def compute_output_shape(self, input_shape):\n         return (*input_shape, self.output_dim)\n \n+    def compute_output_spec(self, inputs):\n+        output_shape = (*inputs.shape, self.output_dim)\n+        return KerasTensor(\n+            output_shape, dtype=self.compute_dtype, ragged=inputs.ragged\n+        )\n+\n     def enable_lora(\n         self,\n         rank,\n\n@@ -61,6 +61,27 @@ class EmbeddingTest(test_case.TestCase):\n             supports_masking=False,\n         )\n \n+    @pytest.mark.skipif(\n+        not backend.SUPPORTS_RAGGED_TENSORS,\n+        reason=\"Backend does not support ragged tensors.\",\n+    )\n+    def test_ragged(self):\n+        self.run_layer_test(\n+            layers.Embedding,\n+            {\"input_dim\": 5, \"output_dim\": 4},\n+            input_shape=(2, 3),\n+            input_dtype=\"int32\",\n+            input_ragged=True,\n+            expected_output_shape=(2, None, 4),\n+            expected_output_ragged=True,\n+            expected_num_trainable_weights=1,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+            # run_training_check=False,\n+        )\n+\n     def test_correctness(self):\n         layer = layers.Embedding(input_dim=3, output_dim=2)\n         layer.build()\n\n@@ -5425,15 +5425,17 @@ class Take(Operation):\n         x_shape = list(x.shape)\n         if isinstance(indices, KerasTensor):\n             indices_shape = list(indices.shape)\n+            ragged = indices.ragged\n         else:\n             indices_shape = list(getattr(np.array(indices), \"shape\", []))\n+            ragged = False\n         if self.axis is None:\n             return KerasTensor(indices_shape, dtype=x.dtype)\n \n         # make sure axis is non-negative\n         axis = len(x_shape) + self.axis if self.axis < 0 else self.axis\n         output_shape = x_shape[:axis] + indices_shape + x_shape[axis + 1 :]\n-        return KerasTensor(output_shape, dtype=x.dtype)\n+        return KerasTensor(output_shape, dtype=x.dtype, ragged=ragged)\n \n \n @keras_export([\"keras.ops.take\", \"keras.ops.numpy.take\"])\n\n@@ -3078,17 +3078,61 @@ class NumpyTwoInputOpsCorrectnessTest(testing.TestCase):\n         if backend.backend() == \"tensorflow\":\n             import tensorflow as tf\n \n-            indices = tf.SparseTensor([[0, 0], [1, 2]], [1, 2], (2, 3))\n+            indices = tf.SparseTensor([[0, 0], [1, 2]], [-1, 2], (2, 3))\n         elif backend.backend() == \"jax\":\n             import jax.experimental.sparse as jax_sparse\n \n-            indices = jax_sparse.BCOO(([1, 2], [[0, 0], [1, 2]]), shape=(2, 3))\n+            indices = jax_sparse.BCOO(([-1, 2], [[0, 0], [1, 2]]), shape=(2, 3))\n \n         self.assertAllClose(\n             knp.take(x, indices, axis=axis),\n             np.take(x, backend.convert_to_numpy(indices), axis=axis),\n         )\n \n+    @parameterized.named_parameters(\n+        named_product(\n+            [\n+                {\"testcase_name\": \"axis_none\", \"axis\": None},\n+                {\"testcase_name\": \"axis_0\", \"axis\": 0},\n+                {\"testcase_name\": \"axis_1\", \"axis\": 1},\n+                {\"testcase_name\": \"axis_minus1\", \"axis\": -1},\n+            ],\n+            dtype=[\n+                \"float16\",\n+                \"float32\",\n+                \"float64\",\n+                \"uint8\",\n+                \"int8\",\n+                \"int16\",\n+                \"int32\",\n+            ],\n+        )\n+    )\n+    @pytest.mark.skipif(\n+        not backend.SUPPORTS_RAGGED_TENSORS,\n+        reason=\"Backend does not support ragged tensors.\",\n+    )\n+    def test_take_ragged(self, dtype, axis):\n+        rng = np.random.default_rng(0)\n+        x = (4 * rng.standard_normal((3, 4, 5))).astype(dtype)\n+\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            indices = tf.ragged.constant([[2], [0, -1, 1]])\n+            mask = backend.convert_to_numpy(tf.ones_like(indices))\n+\n+        if axis == 0:\n+            mask = np.expand_dims(mask, (2, 3))\n+        elif axis == 1:\n+            mask = np.expand_dims(mask, (2,))\n+\n+        self.assertAllClose(\n+            knp.take(x, indices, axis=axis),\n+            np.take(x, backend.convert_to_numpy(indices), axis=axis)\n+            * mask.astype(dtype),\n+        )\n+\n     def test_take_along_axis(self):\n         x = np.arange(24).reshape([1, 2, 3, 4])\n         indices = np.ones([1, 4, 1, 1], dtype=np.int32)\n\n@@ -16,6 +16,7 @@ from keras.src.backend.common import is_float_dtype\n from keras.src.backend.common import standardize_dtype\n from keras.src.backend.common.global_state import clear_session\n from keras.src.backend.common.keras_tensor import KerasTensor\n+from keras.src.losses.losses import MeanSquaredError\n from keras.src.models import Model\n from keras.src.utils import traceback_utils\n \n@@ -100,6 +101,22 @@ class TestCase(parameterized.TestCase, unittest.TestCase):\n                 f\"Backend {backend.backend()} does not support sparse tensors\",\n             )\n \n+    def assertRagged(self, x, ragged=True):\n+        if isinstance(x, KerasTensor):\n+            self.assertEqual(x.ragged, ragged)\n+        elif backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            if ragged:\n+                self.assertIsInstance(x, tf.RaggedTensor)\n+            else:\n+                self.assertNotIsInstance(x, tf.RaggedTensor)\n+        else:\n+            self.assertFalse(\n+                ragged,\n+                f\"Backend {backend.backend()} does not support ragged tensors\",\n+            )\n+\n     def assertDType(self, x, dtype, msg=None):\n         if hasattr(x, \"dtype\"):\n             x_dtype = backend.standardize_dtype(x.dtype)\n@@ -159,11 +176,13 @@ class TestCase(parameterized.TestCase, unittest.TestCase):\n         input_shape=None,\n         input_dtype=None,\n         input_sparse=False,\n+        input_ragged=False,\n         input_data=None,\n         call_kwargs=None,\n         expected_output_shape=None,\n         expected_output_dtype=None,\n         expected_output_sparse=False,\n+        expected_output_ragged=False,\n         expected_output=None,\n         expected_num_trainable_weights=None,\n         expected_num_non_trainable_weights=None,\n@@ -188,6 +207,8 @@ class TestCase(parameterized.TestCase, unittest.TestCase):\n             input_dtype: Corresponding input dtype.\n             input_sparse: Whether the input is a sparse tensor (this requires\n                 the backend to support sparse tensors).\n+            input_ragged: Whether the input is a ragged tensor (this requires\n+                the backend to support ragged tensors).\n             input_data: Tensor (or list/dict of tensors)\n                 to call the layer on.\n             call_kwargs: Dict of arguments to use when calling the\n@@ -198,6 +219,8 @@ class TestCase(parameterized.TestCase, unittest.TestCase):\n             expected_output_dtype: dtype expected as output.\n             expected_output_sparse: Whether the output is expected to be sparse\n                 (this requires the backend to support sparse tensors).\n+            expected_output_ragged: Whether the output is expected to be ragged\n+                (this requires the backend to support ragged tensors).\n             expected_output: Expected output tensor -- only\n                 to be specified if input_data is provided.\n             expected_num_trainable_weights: Expected number\n@@ -280,7 +303,7 @@ class TestCase(parameterized.TestCase, unittest.TestCase):\n         if input_data is not None or input_shape is not None:\n             if input_data is None:\n                 input_data = create_eager_tensors(\n-                    input_shape, input_dtype, input_sparse\n+                    input_shape, input_dtype, input_sparse, input_ragged\n                 )\n             layer = layer_cls(**init_kwargs)\n             if isinstance(input_data, dict):\n@@ -357,7 +380,13 @@ class TestCase(parameterized.TestCase, unittest.TestCase):\n             if expected_output_shape is not None:\n \n                 def verify_shape(expected_shape, x):\n-                    return expected_shape == x.shape\n+                    shape = x.shape\n+                    if len(shape) != len(expected_shape):\n+                        return False\n+                    for expected_dim, dim in zip(expected_shape, shape):\n+                        if expected_dim is not None and expected_dim != dim:\n+                            return False\n+                    return True\n \n                 shapes_match = tree.map_structure_up_to(\n                     output, verify_shape, expected_output_shape, output\n@@ -383,6 +412,9 @@ class TestCase(parameterized.TestCase, unittest.TestCase):\n             if expected_output_sparse:\n                 for x in tree.flatten(output):\n                     self.assertSparse(x)\n+            if expected_output_ragged:\n+                for x in tree.flatten(output):\n+                    self.assertRagged(x)\n             if eager:\n                 if expected_output is not None:\n                     self.assertEqual(type(expected_output), type(output))\n@@ -430,7 +462,11 @@ class TestCase(parameterized.TestCase, unittest.TestCase):\n             jit_compile = \"auto\"\n             if backend.backend() == \"tensorflow\" and input_sparse:\n                 jit_compile = False\n-            model.compile(optimizer=\"sgd\", loss=\"mse\", jit_compile=jit_compile)\n+            model.compile(\n+                optimizer=\"sgd\",\n+                loss=MeanSquaredError(reduction=\"sum\"),\n+                jit_compile=jit_compile,\n+            )\n             model.fit(data_generator(), steps_per_epoch=1, verbose=0)\n \n         # Build test.\n@@ -452,13 +488,13 @@ class TestCase(parameterized.TestCase, unittest.TestCase):\n             if input_shape is None:\n                 keras_tensor_inputs = tree.map_structure(\n                     lambda x: create_keras_tensors(\n-                        ops.shape(x), x.dtype, input_sparse\n+                        ops.shape(x), x.dtype, input_sparse, input_ragged\n                     ),\n                     input_data,\n                 )\n             else:\n                 keras_tensor_inputs = create_keras_tensors(\n-                    input_shape, input_dtype, input_sparse\n+                    input_shape, input_dtype, input_sparse, input_ragged\n                 )\n             layer = layer_cls(**init_kwargs)\n             if isinstance(keras_tensor_inputs, dict):\n@@ -589,22 +625,24 @@ def uses_cpu():\n     return False\n \n \n-def create_keras_tensors(input_shape, dtype, sparse):\n+def create_keras_tensors(input_shape, dtype, sparse, ragged):\n     if isinstance(input_shape, dict):\n         return {\n             utils.removesuffix(k, \"_shape\"): KerasTensor(\n-                v, dtype=dtype[k], sparse=sparse\n+                v, dtype=dtype[k], sparse=sparse, ragged=ragged\n             )\n             for k, v in input_shape.items()\n         }\n     return map_shape_dtype_structure(\n-        lambda shape, dt: KerasTensor(shape, dtype=dt, sparse=sparse),\n+        lambda shape, dt: KerasTensor(\n+            shape, dtype=dt, sparse=sparse, ragged=ragged\n+        ),\n         input_shape,\n         dtype,\n     )\n \n \n-def create_eager_tensors(input_shape, dtype, sparse):\n+def create_eager_tensors(input_shape, dtype, sparse, ragged):\n     from keras.src.backend import random\n \n     if set(tree.flatten(dtype)).difference(\n@@ -651,6 +689,21 @@ def create_eager_tensors(input_shape, dtype, sparse):\n                 f\"Sparse is unsupported with backend {backend.backend()}\"\n             )\n \n+    elif ragged:\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            def create_fn(shape, dt):\n+                rng = np.random.default_rng(0)\n+                x = (4 * rng.standard_normal(shape)).astype(dt)\n+                x = np.multiply(x, rng.random(shape) < 0.7)\n+                return tf.RaggedTensor.from_tensor(x, padding=0)\n+\n+        else:\n+            raise ValueError(\n+                f\"Ragged is unsupported with backend {backend.backend()}\"\n+            )\n+\n     else:\n \n         def create_fn(shape, dt):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8e4b4abcca0002c4435d54aa191d3e76a92e44d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1609 | Contributors (this commit): 16 | Commits (past 90d): 22 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1036,7 +1036,10 @@ def nan_to_num(x, nan=0.0, posinf=None, neginf=None):\n \n \n def ndim(x):\n-    raise NotImplementedError(\"`ndim` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_shape = ov_opset.shape_of(x).output(0)\n+    x_dim = ov_opset.shape_of(x_shape, \"i64\")\n+    return x_dim\n \n \n def nonzero(x):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b840164a1b1804fa0007ce32e31126f0bcdb7066", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 399 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,7 +16,7 @@ from keras.src.backend.common import is_float_dtype\n from keras.src.backend.common import standardize_dtype\n from keras.src.backend.common.global_state import clear_session\n from keras.src.backend.common.keras_tensor import KerasTensor\n-from keras.src.losses.losses import MeanSquaredError\n+from keras.src.losses.loss import Loss\n from keras.src.models import Model\n from keras.src.utils import traceback_utils\n \n@@ -446,6 +446,11 @@ class TestCase(parameterized.TestCase, unittest.TestCase):\n                 while True:\n                     yield data\n \n+            # Single op loss to avoid compilation issues with ragged / sparse.\n+            class TestLoss(Loss):\n+                def __call__(self, y_true, y_pred, sample_weight=None):\n+                    return ops.sum(y_pred)\n+\n             # test the \"default\" path for each backend by setting\n             # jit_compile=\"auto\".\n             # for tensorflow and jax backends auto is jitted\n@@ -463,9 +468,7 @@ class TestCase(parameterized.TestCase, unittest.TestCase):\n             if backend.backend() == \"tensorflow\" and input_sparse:\n                 jit_compile = False\n             model.compile(\n-                optimizer=\"sgd\",\n-                loss=MeanSquaredError(reduction=\"sum\"),\n-                jit_compile=jit_compile,\n+                optimizer=\"sgd\", loss=TestLoss(), jit_compile=jit_compile\n             )\n             model.fit(data_generator(), steps_per_epoch=1, verbose=0)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7b6b424feef65b366bfa776d2155ee1b2967b5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 176 | Lines Deleted: 0 | Files Changed: 15 | Hunks: 20 | Methods Changed: 12 | Complexity Δ (Sum/Max): 15/4 | Churn Δ: 176 | Churn Cumulative: 6413 | Contributors (this commit): 28 | Commits (past 90d): 41 | Contributors (cumulative): 148 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,6 +33,7 @@ from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n from keras.src.activations.activations import sparse_plus\n+from keras.src.activations.activations import sparse_sigmoid\n from keras.src.activations.activations import sparsemax\n from keras.src.activations.activations import squareplus\n from keras.src.activations.activations import tanh\n\n@@ -103,6 +103,7 @@ from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n from keras.src.ops.nn import sparse_plus\n+from keras.src.ops.nn import sparse_sigmoid\n from keras.src.ops.nn import sparsemax\n from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n\n@@ -47,6 +47,7 @@ from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n from keras.src.ops.nn import sparse_plus\n+from keras.src.ops.nn import sparse_sigmoid\n from keras.src.ops.nn import sparsemax\n from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n\n@@ -33,6 +33,7 @@ from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n from keras.src.activations.activations import sparse_plus\n+from keras.src.activations.activations import sparse_sigmoid\n from keras.src.activations.activations import sparsemax\n from keras.src.activations.activations import squareplus\n from keras.src.activations.activations import tanh\n\n@@ -103,6 +103,7 @@ from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n from keras.src.ops.nn import sparse_plus\n+from keras.src.ops.nn import sparse_sigmoid\n from keras.src.ops.nn import sparsemax\n from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n\n@@ -47,6 +47,7 @@ from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n from keras.src.ops.nn import sparse_plus\n+from keras.src.ops.nn import sparse_sigmoid\n from keras.src.ops.nn import sparsemax\n from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n\n@@ -24,6 +24,7 @@ from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n from keras.src.activations.activations import sparse_plus\n+from keras.src.activations.activations import sparse_sigmoid\n from keras.src.activations.activations import sparsemax\n from keras.src.activations.activations import squareplus\n from keras.src.activations.activations import tanh\n@@ -53,6 +54,7 @@ ALL_OBJECTS = {\n     tanh_shrink,\n     threshold,\n     sigmoid,\n+    sparse_sigmoid,\n     exponential,\n     hard_sigmoid,\n     hard_silu,\n\n@@ -552,6 +552,27 @@ def log_sigmoid(x):\n     return ops.log_sigmoid(x)\n \n \n+@keras_export(\"keras.activations.sparse_sigmoid\")\n+def sparse_sigmoid(x):\n+    \"\"\"Sparse sigmoid activation function.\n+\n+    It is defined as\n+\n+    `f(x) = 0` for `x <= -1`,\n+    `f(x) = 0.5 * (x + 1)` for `-1 < x < 1`,\n+    `f(x) = 1` for `x >= 1`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Reference:\n+\n+    - [M. Blondel, A. F. T. Martins, V. Niculae, 2019](https://arxiv.org/pdf/1901.02324)\n+\n+    \"\"\"\n+    return ops.sparse_sigmoid(x)\n+\n+\n @keras_export([\"keras.activations.hard_silu\", \"keras.activations.hard_swish\"])\n def hard_silu(x):\n     \"\"\"Hard SiLU activation function, also known as Hard Swish.\n\n@@ -40,6 +40,10 @@ def _ref_hard_sigmoid(x):\n     return z\n \n \n+def _ref_sparse_sigmoid(x):\n+    return np.where(x <= -1, 0, np.where(x >= 1, 1, 0.5 * (x + 1)))\n+\n+\n def _ref_log_sigmoid(x):\n     return -1 * _ref_softplus(-x)\n \n@@ -341,6 +345,45 @@ class ActivationsTest(testing.TestCase):\n             result_positive_above_1, expected_positive_above_1, rtol=1e-05\n         )\n \n+    def test_sparse_sigmoid(self):\n+        # Basic test for random values between 0 and 1\n+        x = np.random.uniform(0, 1, (2, 5))\n+        result = activations.sparse_sigmoid(x[np.newaxis, :])[0]\n+        expected = np.vectorize(_ref_sparse_sigmoid)(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+        # Test with 1D array\n+        x_1d = np.random.uniform(-10, 10, 5)\n+        result_1d = activations.sparse_sigmoid(x_1d)\n+        expected_1d = np.vectorize(_ref_sparse_sigmoid)(x_1d)\n+        self.assertAllClose(result_1d, expected_1d, rtol=1e-05)\n+\n+        # Test with 3D array\n+        x_3d = np.random.uniform(-10, 10, (3, 3, 3))\n+        result_3d = activations.sparse_sigmoid(x_3d)\n+        expected_3d = np.vectorize(_ref_sparse_sigmoid)(x_3d)\n+        self.assertAllClose(result_3d, expected_3d, rtol=1e-05)\n+\n+        # Test large positive values\n+        x_large_positive = np.random.uniform(10, 100, (2, 5))\n+        result_large_positive = activations.sparse_sigmoid(x_large_positive)\n+        expected_large_positive = np.vectorize(_ref_sparse_sigmoid)(\n+            x_large_positive\n+        )\n+        self.assertAllClose(\n+            result_large_positive, expected_large_positive, rtol=1e-05\n+        )\n+\n+        # Test large negative values\n+        x_large_negative = np.random.uniform(-100, -10, (2, 5))\n+        result_large_negative = activations.sparse_sigmoid(x_large_negative)\n+        expected_large_negative = np.vectorize(_ref_sparse_sigmoid)(\n+            x_large_negative\n+        )\n+        self.assertAllClose(\n+            result_large_negative, expected_large_negative, rtol=1e-05\n+        )\n+\n     def test_log_sigmoid(self):\n         # Basic test for random values between 0 and 1\n         x = np.random.uniform(0, 1, (2, 5))\n\n@@ -36,6 +36,11 @@ def sigmoid(x):\n     return jnn.sigmoid(x)\n \n \n+def sparse_sigmoid(x):\n+    x = convert_to_tensor(x)\n+    return jnn.sparse_sigmoid(x)\n+\n+\n def tanh(x):\n     x = convert_to_tensor(x)\n     return jnn.tanh(x)\n\n@@ -31,6 +31,17 @@ def sigmoid(x):\n     return np.array(1.0, x.dtype) / (np.array(1.0, x.dtype) + np.exp(-x))\n \n \n+def sparse_sigmoid(x):\n+    x = convert_to_tensor(x)\n+    return np.where(\n+        x <= -1,\n+        np.array(0.0, x.dtype),\n+        np.where(\n+            x >= 1, np.array(1.0, x.dtype), np.array(0.5 * (x + 1), x.dtype)\n+        ),\n+    )\n+\n+\n def tanh(x):\n     return np.tanh(x)\n \n\n@@ -26,6 +26,15 @@ def sigmoid(x):\n     return output\n \n \n+def sparse_sigmoid(x):\n+    x = convert_to_tensor(x)\n+    return tf.where(\n+        x <= -1,\n+        tf.constant(0.0, dtype=x.dtype),\n+        tf.where(x >= 1, tf.constant(1.0, dtype=x.dtype), 0.5 * (x + 1)),\n+    )\n+\n+\n def tanh(x):\n     return tf.nn.tanh(x)\n \n\n@@ -29,6 +29,19 @@ def sigmoid(x):\n     return tnn.sigmoid(x)\n \n \n+def sparse_sigmoid(x):\n+    x = convert_to_tensor(x)\n+    return torch.where(\n+        x <= -1,\n+        torch.tensor(0.0, device=x.device, dtype=x.dtype),\n+        torch.where(\n+            x >= 1,\n+            torch.tensor(1.0, device=x.device, dtype=x.dtype),\n+            0.5 * (x + 1),\n+        ),\n+    )\n+\n+\n def tanh(x):\n     x = convert_to_tensor(x)\n     return tnn.tanh(x)\n\n@@ -110,6 +110,42 @@ def sigmoid(x):\n     return backend.nn.sigmoid(x)\n \n \n+class SparseSigmoid(Operation):\n+    def call(self, x):\n+        return backend.nn.sparse_sigmoid(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_export([\"keras.ops.sparse_sigmoid\", \"keras.ops.nn.sparse_sigmoid\"])\n+def sparse_sigmoid(x):\n+    \"\"\"Sparse sigmoid activation function.\n+\n+    It is defined as\n+\n+    `f(x) = 0` for `x <= -1`,\n+    `f(x) = 0.5 * (x + 1)` for `-1 < x < 1`,\n+    `f(x) = 1` for `x >= 1`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = keras.ops.convert_to_tensor([-6.0, 1.0, 0.0, 1.0, 6.0])\n+    >>> keras.ops.sparse_sigmoid(x)\n+    array([0. , 1. , 0.5, 1. , 1. ], dtype=float32)\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return SparseSigmoid().symbolic_call(x)\n+    return backend.nn.sparse_sigmoid(x)\n+\n+\n class Softplus(Operation):\n     def call(self, x):\n         return backend.nn.softplus(x)\n\n@@ -100,6 +100,10 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.sigmoid(x).shape, (None, 2, 3))\n \n+    def test_sparse_sigmoid(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.sparse_sigmoid(x).shape, (None, 2, 3))\n+\n     def test_softplus(self):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.softplus(x).shape, (None, 2, 3))\n@@ -785,6 +789,10 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.sigmoid(x).shape, (1, 2, 3))\n \n+    def test_sparse_sigmoid(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.sparse_sigmoid(x).shape, (1, 2, 3))\n+\n     def test_softplus(self):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.softplus(x).shape, (1, 2, 3))\n@@ -1306,6 +1314,10 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             knn.sigmoid(x), [0.26894143, 0.5, 0.7310586, 0.880797, 0.95257413]\n         )\n \n+    def test_sparse_sigmoid(self):\n+        x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(knn.sparse_sigmoid(x), [0.0, 0.5, 1.0, 1.0, 1.0])\n+\n     def test_softplus(self):\n         x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n         self.assertAllClose(\n@@ -2883,6 +2895,24 @@ class NNOpsDtypeTest(testing.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n+    def test_sparse_sigmoid(self, dtype):\n+        import jax.nn as jnn\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((), dtype=dtype)\n+        x_jax = jnp.ones((), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnn.sparse_sigmoid(x_jax).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knn.sparse_sigmoid(x).dtype),\n+            expected_dtype,\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knn.SparseSigmoid().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n     def test_silu(self, dtype):\n         import jax.nn as jnn\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#65f7e6dc0871236ffae4a20e1e398048e9e942e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1615 | Contributors (this commit): 16 | Commits (past 90d): 23 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -1043,9 +1043,9 @@ def ndim(x):\n \n \n def nonzero(x):\n-    raise NotImplementedError(\n-        \"`nonzero` is not supported with openvino backend\"\n-    )\n+    x = get_ov_output(x)\n+    res = ov_opset.non_zero(data=x, output_type=\"i32\").output(0)\n+    return OpenVINOKerasTensor(res)\n \n \n def not_equal(x1, x2):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#50dae307fe9023742e9c91e567032eeb78c5b3c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 14 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 4413 | Contributors (this commit): 20 | Commits (past 90d): 25 | Contributors (cumulative): 51 | DMM Complexity: 1.0\n\nDIFF:\n@@ -895,10 +895,12 @@ def not_equal(x1, x2):\n     return jnp.not_equal(x1, x2)\n \n \n+@sparse.elementwise_unary(linear=False)\n def ones_like(x, dtype=None):\n     return jnp.ones_like(x, dtype=dtype)\n \n \n+@sparse.elementwise_unary(linear=True)\n def zeros_like(x, dtype=None):\n     return jnp.zeros_like(x, dtype=dtype)\n \n\n@@ -1798,10 +1798,12 @@ def not_equal(x1, x2):\n     return tf.not_equal(x1, x2)\n \n \n+@sparse.elementwise_unary\n def ones_like(x, dtype=None):\n     return tf.ones_like(x, dtype=dtype)\n \n \n+@sparse.elementwise_unary\n def zeros_like(x, dtype=None):\n     return tf.zeros_like(x, dtype=dtype)\n \n\n@@ -4391,7 +4391,8 @@ class OnesLike(Operation):\n     def compute_output_spec(self, x, dtype=None):\n         if dtype is None:\n             dtype = x.dtype\n-        return KerasTensor(x.shape, dtype=dtype)\n+        sparse = getattr(x, \"sparse\", False)\n+        return KerasTensor(x.shape, dtype=dtype, sparse=sparse)\n \n \n @keras_export([\"keras.ops.ones_like\", \"keras.ops.numpy.ones_like\"])\n@@ -4417,7 +4418,8 @@ class ZerosLike(Operation):\n     def compute_output_spec(self, x, dtype=None):\n         if dtype is None:\n             dtype = x.dtype\n-        return KerasTensor(x.shape, dtype=dtype)\n+        sparse = getattr(x, \"sparse\", False)\n+        return KerasTensor(x.shape, dtype=dtype, sparse=sparse)\n \n \n @keras_export(\n\n@@ -5097,6 +5097,7 @@ class SparseTest(testing.TestCase):\n         \"imag\",\n         \"log1p\",\n         \"negative\",\n+        \"ones_like\",\n         \"real\",\n         \"round\",\n         \"sign\",\n@@ -5106,6 +5107,7 @@ class SparseTest(testing.TestCase):\n         \"square\",\n         \"tan\",\n         \"tanh\",\n+        \"zeros_like\",\n     ]\n     ELEMENTWISE_UNARY_OPS_TESTS = [\n         {\n@@ -5287,10 +5289,11 @@ class SparseTest(testing.TestCase):\n             x = np.array([[1, 0.5, -0.7], [0.9, 0.2, -1]])\n         x = create_sparse_tensor(x)\n         x_np = backend.convert_to_numpy(x)\n+        expected = np_op(x_np) * backend.convert_to_numpy(knp.ones_like(x))\n \n-        self.assertAllClose(op_function(x), np_op(x_np))\n+        self.assertAllClose(op_function(x), expected)\n         self.assertSameSparseness(op_function(x), x)\n-        self.assertAllClose(op_class()(x), np_op(x_np))\n+        self.assertAllClose(op_class()(x), expected)\n         self.assertSameSparseness(op_class()(x), x)\n \n     @parameterized.named_parameters(ELEMENTWISE_UNARY_OPS_TESTS)\n@@ -5303,10 +5306,11 @@ class SparseTest(testing.TestCase):\n             x = np.array([[1, 0.5, -0.7], [0.9, 0.2, -1]])\n         x = create_indexed_slices(x)\n         x_np = backend.convert_to_numpy(x)\n+        expected = np_op(x_np) * backend.convert_to_numpy(knp.ones_like(x))\n \n-        self.assertAllClose(op_function(x), np_op(x_np))\n+        self.assertAllClose(op_function(x), expected)\n         self.assertSameSparseness(op_function(x), x)\n-        self.assertAllClose(op_class()(x), np_op(x_np))\n+        self.assertAllClose(op_class()(x), expected)\n         self.assertSameSparseness(op_class()(x), x)\n \n     @parameterized.named_parameters(OTHER_UNARY_OPS_TESTS)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dd9132ff68a4626ee6a1477611ce414f43e4d3de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 296 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -564,12 +564,12 @@ def standardize_dtype(dtype):\n     dtype = dtypes.PYTHON_DTYPES_MAP.get(dtype, dtype)\n     if hasattr(dtype, \"name\"):\n         dtype = dtype.name\n+    elif hasattr(dtype, \"__name__\"):\n+        dtype = dtype.__name__\n     elif hasattr(dtype, \"__str__\") and (\n         \"torch\" in str(dtype) or \"jax.numpy\" in str(dtype)\n     ):\n         dtype = str(dtype).split(\".\")[-1]\n-    elif hasattr(dtype, \"__name__\"):\n-        dtype = dtype.__name__\n \n     if dtype not in dtypes.ALLOWED_DTYPES:\n         raise ValueError(f\"Invalid dtype: {dtype}\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8a6e83bd237e72a8306d95e3e82802c97b9f8ee8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 344 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -159,8 +159,9 @@ class Embedding(Layer):\n \n     def compute_output_spec(self, inputs):\n         output_shape = (*inputs.shape, self.output_dim)\n+        ragged = getattr(inputs, \"ragged\", False)\n         return KerasTensor(\n-            output_shape, dtype=self.compute_dtype, ragged=inputs.ragged\n+            output_shape, dtype=self.compute_dtype, ragged=ragged\n         )\n \n     def enable_lora(\n\n@@ -100,8 +100,8 @@ class GeneratorDataAdapterTest(testing.TestCase):\n                 self.assertEqual(by.shape, (2, 2))\n             if use_sample_weight:\n                 self.assertIsInstance(bsw, expected_class)\n-            for i in range(by.shape[0]):\n-                sample_order.append(by[i, 0])\n+            for j in range(by.shape[0]):\n+                sample_order.append(by[j, 0])\n         self.assertAllClose(sample_order, list(range(34)))\n \n     def test_with_different_shapes(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1e9466a3367da6c42bd1736d3ac1e5b959b7a354", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 237 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -158,7 +158,7 @@ class Embedding(Layer):\n         return (*input_shape, self.output_dim)\n \n     def compute_output_spec(self, inputs):\n-        output_shape = (*inputs.shape, self.output_dim)\n+        output_shape = self.compute_output_shape(inputs.shape)\n         ragged = getattr(inputs, \"ragged\", False)\n         return KerasTensor(\n             output_shape, dtype=self.compute_dtype, ragged=ragged\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#84c3bfc9bcdeda591d199ef6d378646e383413ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 39 | Churn Cumulative: 280 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -457,6 +457,10 @@ class DataParallel(Distribution):\n         return TensorLayout(data_shard_spec, self.device_mesh)\n \n     def get_variable_layout(self, variable):\n+        # First check if the variable already has a layout assigned.\n+        if getattr(variable, \"_layout\", None) is not None:\n+            return variable._layout\n+        # Otherwise, replicate variable.\n         variable_shard_spec = [None] * len(variable.shape)\n         return TensorLayout(variable_shard_spec, self.device_mesh)\n \n@@ -610,6 +614,10 @@ class ModelParallel(Distribution):\n         return TensorLayout(data_shard_spec, self.device_mesh)\n \n     def get_variable_layout(self, variable):\n+        # First check if the variable already has a layout assigned.\n+        if getattr(variable, \"_layout\", None) is not None:\n+            return variable._layout\n+        # Check the layout map.\n         variable_layout = self._layout_map[variable.path]\n         if variable_layout is not None:\n             return variable_layout\n\n@@ -234,6 +234,21 @@ class DataParallelDistributionTest(testing.TestCase):\n         self.assertIs(variable_layout.device_mesh, self.device_mesh)\n         self.assertEqual(variable_layout.axes, (None,))\n \n+    @pytest.mark.skipif(testing.jax_uses_gpu(), reason=\"CI segfault\")\n+    def test_get_variable_layout_with_explicit_layout(self):\n+        distribution = distribution_lib.DataParallel(\n+            device_mesh=self.device_mesh\n+        )\n+\n+        explicit_mesh = distribution_lib.DeviceMesh((8,), [\"x\"], self.devices)\n+        explicit_layout = distribution_lib.TensorLayout([\"x\"], explicit_mesh)\n+\n+        variable = backend.Variable(initializer=[1, 2, 3])\n+        variable._layout = explicit_layout\n+        variable_layout = distribution.get_variable_layout(variable)\n+        self.assertIs(variable_layout.device_mesh, explicit_mesh)\n+        self.assertEqual(variable_layout.axes, explicit_layout.axes)\n+\n     def test_get_tensor_layout(self):\n         distribution = distribution_lib.DataParallel(\n             device_mesh=self.device_mesh\n@@ -320,6 +335,22 @@ class ModelParallelDistributionTest(testing.TestCase):\n         layout = distribution.get_tensor_layout(\"/model/layer/other_tensor\")\n         self.assertIsNone(layout)\n \n+    @pytest.mark.skipif(testing.jax_uses_gpu(), reason=\"CI segfault\")\n+    def test_get_variable_layout_with_explicit_layout(self):\n+        layout_map = distribution_lib.LayoutMap(self.device_mesh)\n+        layout_map[\".*kernel\"] = distribution_lib.TensorLayout([None, \"model\"])\n+        distribution = distribution_lib.ModelParallel(\n+            layout_map=layout_map, batch_dim_name=\"data\"\n+        )\n+\n+        explicit_mesh = distribution_lib.DeviceMesh((8,), [\"x\"], self.devices)\n+        explicit_layout = distribution_lib.TensorLayout([\"x\"], explicit_mesh)\n+        variable = backend.Variable(initializer=[1, 2, 3], name=\"kernel\")\n+        variable._layout = explicit_layout\n+        variable_layout = distribution.get_variable_layout(variable)\n+        self.assertIs(variable_layout.device_mesh, explicit_mesh)\n+        self.assertEqual(variable_layout.axes, explicit_layout.axes)\n+\n     def test_distribute_dataset(self):\n         # We can only verify the single worker/process case in OSS for now.\n         dataset = tf.data.Dataset.range(8)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#41d77373a3f7b90858f5a8a56651d59074f9e3e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 45 | Churn Cumulative: 336 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -424,6 +424,11 @@ class BaseOptimizer(KerasSaveable):\n             self._check_variables_are_known(trainable_variables)\n \n         with backend.name_scope(self.name, caller=self):\n+            # Filter empty gradients.\n+            grads, trainable_variables = self._filter_empty_gradients(\n+                grads, trainable_variables\n+            )\n+\n             # Overwrite targeted variables directly with their gradients if\n             # their `overwrite_with_gradient` is set.\n             grads, trainable_variables = (\n@@ -432,10 +437,6 @@ class BaseOptimizer(KerasSaveable):\n                 )\n             )\n \n-            # Filter empty gradients.\n-            grads, trainable_variables = self._filter_empty_gradients(\n-                grads, trainable_variables\n-            )\n             if len(list(grads)) == 0:\n                 return\n \n\n@@ -102,6 +102,12 @@ class LossScaleOptimizer(optimizer.Optimizer):\n             ),\n         )\n \n+    def _overwrite_variable_with_gradient(self, variable):\n+        return (\n+            hasattr(variable, \"overwrite_with_gradient\")\n+            and variable.overwrite_with_gradient\n+        )\n+\n     def _stateless_handle_finite_grads(\n         self, optimizer_variables, grads, trainable_variables\n     ):\n@@ -130,7 +136,10 @@ class LossScaleOptimizer(optimizer.Optimizer):\n             # Unscale gradients.\n             scale = self.dynamic_scale\n             unscaled_grads = [\n-                g if g is None else ops.divide(g, scale) for g in grads\n+                g\n+                if g is None or self._overwrite_variable_with_gradient(v)\n+                else ops.divide(g, scale)\n+                for g, v in zip(grads, trainable_variables)\n             ]\n             (\n                 new_trainable_variables,\n@@ -171,8 +180,12 @@ class LossScaleOptimizer(optimizer.Optimizer):\n     def _stateful_handle_finite_grads(self, grads, trainable_variables):\n         scale = self.dynamic_scale\n         # Unscale gradients.\n+        tvs = trainable_variables or self._trainable_variables\n         unscaled_grads = [\n-            g if g is None else ops.divide(g, scale) for g in grads\n+            g\n+            if g is None or self._overwrite_variable_with_gradient(v)\n+            else ops.divide(g, scale)\n+            for g, v in zip(grads, tvs)\n         ]\n         self.inner_optimizer.apply(\n             unscaled_grads, trainable_variables=trainable_variables\n\n@@ -65,6 +65,25 @@ class LossScaleOptimizerTest(testing.TestCase):\n             optimizer.apply(grads, vars)\n         self.assertAllClose(vars, [[1.0, 2.0, 3.0, 4.0]], rtol=1e-4, atol=1e-4)\n \n+    @parameterized.named_parameters((\"stateless\", True), (\"stateful\", False))\n+    def test_finite_step_with_overwrite(self, stateless):\n+        self._skip_test_for_stateless(stateless)\n+\n+        inner_optimizer = SGD(learning_rate=0.5)\n+        optimizer = LossScaleOptimizer(inner_optimizer)\n+        grads = [ops.array([1.0, 6.0, 7.0, 2.0])]\n+        vars = [backend.Variable([1.0, 2.0, 3.0, 4.0])]\n+        vars[0].overwrite_with_gradient = True\n+\n+        if stateless:\n+            optimizer.build(vars)\n+            vars, _ = optimizer.stateless_apply(\n+                optimizer.variables, grads, vars\n+            )\n+        else:\n+            optimizer.apply(grads, vars)\n+        self.assertAllClose(vars, grads)\n+\n     @parameterized.named_parameters((\"stateless\", True), (\"stateful\", False))\n     def test_downscaling(self, stateless):\n         self._skip_test_for_stateless(stateless)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f430bb5c5621784a4a041ad158132bd330c353dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 1627 | Contributors (this commit): 17 | Commits (past 90d): 24 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -938,7 +938,17 @@ def log10(x):\n \n \n def log1p(x):\n-    raise NotImplementedError(\"`log1p` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_type = x.get_element_type()\n+\n+    if x_type.is_integral():\n+        x_type = OPENVINO_DTYPES[config.floatx()]\n+        x = ov_opset.convert(x, x_type)\n+\n+    one_const = ov_opset.constant(1, x_type).output(0)\n+    added = ov_opset.add(x, one_const).output(0)\n+    result = ov_opset.log(added).output(0)\n+    return OpenVINOKerasTensor(result)\n \n \n def log2(x):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#87d99f2ed84416ce78bbd0261c39e97ac59f3525", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 26 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n from keras.src.api_export import keras_export\n \n # Unique source of truth for the version number.\n-__version__ = \"3.9.0\"\n+__version__ = \"3.10.0\"\n \n \n @keras_export(\"keras.version\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#80493013bcfe46875faf8be486be70c93d6ba774", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 41 | Churn Cumulative: 1668 | Contributors (this commit): 18 | Commits (past 90d): 25 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1016,7 +1016,46 @@ def meshgrid(*x, indexing=\"xy\"):\n \n \n def min(x, axis=None, keepdims=False, initial=None):\n-    raise NotImplementedError(\"`min` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    original_type = x.get_element_type()\n+    x_type = original_type\n+    x_shape = x.get_partial_shape().to_shape()\n+\n+    is_bool = x_type == Type.boolean\n+    if is_bool:\n+        x = ov_opset.convert(x, Type.i32).output(0)\n+        x_type = Type.i32\n+\n+    if isinstance(axis, tuple) and len(axis) == 0:\n+        return OpenVINOKerasTensor(x)\n+\n+    if axis is None:\n+        flatten_shape = ov_opset.constant([-1], Type.i32).output(0)\n+        x = ov_opset.reshape(x, flatten_shape, False).output(0)\n+        axis = 0\n+\n+    if isinstance(axis, tuple):\n+        axis = list(axis)\n+\n+    axis_const = ov_opset.constant(axis, Type.i32).output(0)\n+    min_result = ov_opset.reduce_min(x, axis_const, keepdims).output(0)\n+\n+    if initial is not None:\n+        initial_tensor = ov_opset.constant(initial, x_type).output(0)\n+        min_result = ov_opset.minimum(min_result, initial_tensor).output(0)\n+\n+    if keepdims:\n+        result_shape = [1] * len(x_shape)\n+        min_result = ov_opset.reshape(\n+            min_result,\n+            ov_opset.constant(result_shape, Type.i32).output(0),\n+            False,\n+        ).output(0)\n+\n+    if is_bool:\n+        min_result = ov_opset.convert(min_result, Type.boolean).output(0)\n+\n+    return OpenVINOKerasTensor(min_result)\n \n \n def minimum(x1, x2):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
