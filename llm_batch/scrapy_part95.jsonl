{"custom_id": "scrapy#56abafec61fdd34dd0e158583566fdf606d82310", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 12 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 350 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -36,7 +36,7 @@ def _get_commands_from_module(module):\n     return d\n \n def _get_commands_dict():\n-    cmds = _get_commands_from_module('scrapy.command.commands')\n+    cmds = _get_commands_from_module('scrapy.commands')\n     cmds_module = settings['COMMANDS_MODULE']\n     if cmds_module:\n         cmds.update(_get_commands_from_module(cmds_module))\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2d3135603ecf58eaa46cd31b59782836804bf945", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 20 | Churn Cumulative: 20 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,20 @@\n+from scrapy.command import ScrapyCommand\n+from scrapy.commands import fetch\n+from scrapy.utils.response import open_in_browser\n+\n+class Command(fetch.Command):\n+\n+    def short_desc(self):\n+        return \"Open a URL in browser, as seen by Scrapy\"\n+\n+    def long_desc(self):\n+        return \"Fetch a URL using the Scrapy downloader and show its \" \\\n+            \"contents in a browser\"\n+\n+    def add_options(self, parser):\n+        ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"--spider\", dest=\"spider\",\n+            help=\"use this spider\")\n+\n+    def _print_response(self, response, opts):\n+        open_in_browser(response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dff763c6830a09181a4c3859a20c849ec9213700", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 37 | Files Changed: 16 | Hunks: 33 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 69 | Churn Cumulative: 3335 | Contributors (this commit): 6 | Commits (past 90d): 32 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,7 +10,7 @@ from twisted.internet import reactor\n from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n-from scrapy.core.engine import scrapyengine\n+from scrapy.core.manager import scrapymanager\n from scrapy.conf import settings\n \n class CloseSpider(object):\n@@ -29,13 +29,14 @@ class CloseSpider(object):\n         dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n     def spider_opened(self, spider):\n-        self.tasks[spider] = reactor.callLater(self.timeout, scrapyengine.close_spider, \\\n-            spider=spider, reason='closespider_timeout')\n+        self.tasks[spider] = reactor.callLater(self.timeout, \\\n+            scrapymanager.engine.close_spider, spider=spider, \\\n+            reason='closespider_timeout')\n         \n     def item_passed(self, item, spider):\n         self.counts[spider] += 1\n         if self.counts[spider] == self.itempassed:\n-            scrapyengine.close_spider(spider, 'closespider_itempassed')\n+            scrapymanager.engine.close_spider(spider, 'closespider_itempassed')\n \n     def spider_closed(self, spider):\n         self.counts.pop(spider, None)\n\n@@ -9,7 +9,7 @@ import robotparser\n from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n-from scrapy.core.engine import scrapyengine\n+from scrapy.core.manager import scrapymanager\n from scrapy.core.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n@@ -42,7 +42,7 @@ class RobotsTxtMiddleware(object):\n             self._parsers[netloc] = None\n             robotsurl = \"%s://%s/robots.txt\" % (url.scheme, url.netloc)\n             robotsreq = Request(robotsurl, priority=self.DOWNLOAD_PRIORITY)\n-            dfd = scrapyengine.download(robotsreq, spider)\n+            dfd = scrapymanager.engine.download(robotsreq, spider)\n             dfd.addCallback(self._parse_robots)\n             self._spider_netlocs[spider].add(netloc)\n         return self._parsers[netloc]\n\n@@ -27,7 +27,7 @@ import cPickle as pickle\n \n from scrapy.xlib.pydispatch import dispatcher\n \n-from scrapy.core.engine import scrapyengine\n+from scrapy.core.manager import scrapymanager\n from scrapy.core.exceptions import NotConfigured\n from scrapy.core import signals\n from scrapy.utils.response import response_httprepr\n@@ -60,7 +60,7 @@ class ItemSamplerPipeline(object):\n             stats.set_value(\"items_sampled\", sampled, spider=spider)\n             log.msg(\"Sampled %s\" % item, spider=spider, level=log.INFO)\n             if close_spider and sampled == items_per_spider:\n-                scrapyengine.close_spider(spider)\n+                scrapymanager.engine.close_spider(spider)\n         return item\n \n     def engine_stopped(self):\n\n@@ -13,7 +13,6 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core import signals\n from scrapy import log\n from scrapy.core.manager import scrapymanager\n-from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n from scrapy.mail import MailSender\n from scrapy.conf import settings\n\n@@ -4,7 +4,7 @@ from twisted.internet.defer import Deferred, DeferredList\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy import log\n from scrapy.core import signals\n-from scrapy.core.engine import scrapyengine\n+from scrapy.core.manager import scrapymanager\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\n \n@@ -103,7 +103,7 @@ class MediaPipeline(object):\n \n         \"\"\"\n         request.priority = self.DOWNLOAD_PRIORITY\n-        return scrapyengine.download(request, info.spider)\n+        return scrapymanager.engine.download(request, info.spider)\n \n     def media_to_download(self, request, info):\n         \"\"\" Ongoing request hook pre-cache\n\n@@ -9,7 +9,7 @@ from scrapy.xlib.pydispatch import dispatcher\n from collections import defaultdict\n \n from scrapy.core import signals\n-from scrapy.core.engine import scrapyengine\n+from scrapy.core.manager import scrapymanager\n from scrapy.core.exceptions import NotConfigured, DontCloseSpider\n from scrapy.conf import settings\n \n@@ -26,7 +26,7 @@ class SpiderCloseDelay(object):\n \n     def spider_idle(self, spider):\n         try:\n-            lastseen = scrapyengine.downloader.sites[spider].lastseen\n+            lastseen = scrapymanager.engine.downloader.sites[spider].lastseen\n         except KeyError:\n             lastseen = None\n         if not lastseen:\n\n@@ -7,7 +7,7 @@ from itertools import imap\n from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n-from scrapy.core.engine import scrapyengine\n+from scrapy.core.manager import scrapymanager\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n from scrapy.http import Request\n@@ -61,5 +61,5 @@ class RequestLimitMiddleware(object):\n             return request_or_other\n \n     def _pending_count(self, spider):\n-        pending = scrapyengine.scheduler.pending_requests.get(spider, [])\n+        pending = scrapymanager.engine.scheduler.pending_requests.get(spider, [])\n         return len(pending)\n\n@@ -4,7 +4,7 @@ Live statistics extension\n from datetime import datetime\n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core import signals\n-from scrapy.core.engine import scrapyengine\n+from scrapy.core.manager import scrapymanager\n from scrapy.management.web import banner, webconsole_discover_module\n \n class SpiderStats(object):\n@@ -44,8 +44,8 @@ class LiveStats(object):\n             self.domains[spider].crawled += 1\n             \n     def webconsole_render(self, wc_request):\n-        sch = scrapyengine.scheduler\n-        dwl = scrapyengine.downloader\n+        sch = scrapymanager.engine.scheduler\n+        dwl = scrapymanager.engine.downloader\n \n         totdomains = totscraped = totcrawled = totscheduled = totactive = totdqueued = tottransf = 0\n         s = banner(self)\n\n@@ -5,7 +5,7 @@ See documentation in docs/topics/extensions.rst\n \"\"\"\n \n from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.core.engine import scrapyengine\n+from scrapy.core.manager import scrapymanager\n from scrapy.management.web import banner, webconsole_discover_module\n \n class SchedulerQueue(object):\n@@ -21,7 +21,7 @@ class SchedulerQueue(object):\n     def webconsole_render(self, wc_request):\n         s = banner(self)\n         s += \"<ul>\\n\"\n-        for domain, request_queue in scrapyengine.scheduler.pending_requests.iteritems():\n+        for domain, request_queue in scrapymanager.engine.scheduler.pending_requests.iteritems():\n             s += \"<li>\\n\"\n             s += \"%s (<b>%s</b> requests)\\n\" % (domain, len(request_queue))\n             s += \"<ul>\\n\"\n\n@@ -6,7 +6,6 @@ from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy.core.manager import scrapymanager\n-from scrapy.core.engine import scrapyengine\n from scrapy.spider import spiders\n from scrapy.management.web import banner, webconsole_discover_module\n from scrapy.conf import settings\n@@ -117,7 +116,7 @@ class Spiderctl(object):\n             stopped_spiders = []\n             for name in args[\"stop_running_spiders\"]:\n                 if name in self.running:\n-                    scrapyengine.close_spider(self.running[name])\n+                    scrapymanager.engine.close_spider(self.running[name])\n                     stopped_spiders.append(name)\n             s += \"Stopped spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(stopped_spiders)\n             s += \"</p>\"\n@@ -132,14 +131,14 @@ class Spiderctl(object):\n                 s += \"</p>\"\n         if \"add_pending_spiders\" in args:\n             for name in args[\"add_pending_spiders\"]:\n-                if name not in scrapyengine.scheduler.pending_requests:\n+                if name not in scrapymanager.engine.scheduler.pending_requests:\n                     scrapymanager.queue.append_spider_name(name)\n             s += \"<p>\"\n             s += \"Scheduled spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"add_pending_spiders\"])\n             s += \"</p>\"\n         if \"rerun_finished_spiders\" in args:\n             for name in args[\"rerun_finished_spiders\"]:\n-                if name not in scrapyengine.scheduler.pending_requests:\n+                if name not in scrapymanager.engine.scheduler.pending_requests:\n                     scrapymanager.queue.append_spider_name(name)\n                 self.finished.remove(name)\n             s += \"<p>\"\n\n@@ -286,5 +286,3 @@ class ExecutionEngine(object):\n \n     def _finish_stopping_engine(self):\n         send_catch_log(signal=signals.engine_stopped, sender=self.__class__)\n-\n-scrapyengine = ExecutionEngine()\n\n@@ -2,7 +2,7 @@ import signal\n \n from twisted.internet import reactor, defer\n \n-from scrapy.core.engine import scrapyengine\n+from scrapy.core.engine import ExecutionEngine\n from scrapy.core.queue import ExecutionQueue\n from scrapy.extension import extensions\n from scrapy import log\n@@ -15,7 +15,7 @@ class ExecutionManager(object):\n     def __init__(self):\n         self.configured = False\n         self.control_reactor = True\n-        self.engine = scrapyengine\n+        self.engine = ExecutionEngine()\n \n     def configure(self, control_reactor=True, queue=None):\n         self.control_reactor = control_reactor\n\n@@ -13,7 +13,6 @@ from twisted.internet import reactor, protocol\n from scrapy.extension import extensions\n from scrapy.core.exceptions import NotConfigured\n from scrapy.core.manager import scrapymanager\n-from scrapy.core.engine import scrapyengine\n from scrapy.spider import spiders\n from scrapy.stats import stats\n from scrapy.utils.trackref import print_live_refs\n@@ -28,7 +27,7 @@ except ImportError:\n \n # if you add entries here also update topics/telnetconsole.rst\n telnet_namespace = {\n-    'engine': scrapyengine,\n+    'engine': scrapymanager.engine,\n     'manager': scrapymanager,\n     'extensions': extensions,\n     'stats': stats,\n\n@@ -13,7 +13,7 @@ from twisted.web import server, resource\n \n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.core.engine import scrapyengine\n+from scrapy.core.manager import scrapymanager\n from scrapy.conf import settings\n \n # web management signals\n@@ -38,7 +38,7 @@ def banner(module=None):\n     s += \"<head><title>Scrapy</title></head>\\n\"\n     s += \"<body>\\n\"\n     s += \"<h1><a href='/'>Scrapy web console</a></h1>\\n\"\n-    uptime = time() - scrapyengine.start_time\n+    uptime = time() - scrapymanager.engine.start_time\n     s += \"<p>Bot: <b>%s</b> | Host: <b>%s</b> | Uptime: <b>%ds</b></p>\\n\" % \\\n         (settings['BOT_NAME'], socket.gethostname(), uptime)\n     if module:\n\n@@ -17,7 +17,6 @@ from scrapy.utils.misc import load_object\n from scrapy.utils.response import open_in_browser\n from scrapy.conf import settings\n from scrapy.core.manager import scrapymanager\n-from scrapy.core.engine import scrapyengine\n from scrapy.core.queue import KeepAliveExecutionQueue\n from scrapy.http import Request, TextResponse\n \n@@ -59,8 +58,8 @@ class Shell(object):\n             log_multiple=True)\n \n         print \"Fetching %s...\" % request\n-        scrapyengine.open_spider(spider)\n-        response = threads.blockingCallFromThread(reactor, scrapyengine.schedule, \\\n+        scrapymanager.engine.open_spider(spider)\n+        response = threads.blockingCallFromThread(reactor, scrapymanager.engine.schedule, \\\n             request, spider)\n         if response:\n             self.populate_vars(url, response, request, spider)\n\n@@ -2,12 +2,12 @@\n \n from time import time\n \n-from scrapy.core.engine import scrapyengine\n+from scrapy.core.manager import scrapymanager\n \n def get_engine_status(engine=None):\n     \"\"\"Return a report of the current engine status\"\"\"\n     if engine is None:\n-        engine = scrapyengine\n+        engine = scrapymanager.engine\n \n     global_tests = [\n         \"time()-engine.start_time\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dfa7b23959c844255e7959904a58c78acffc27d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 25 | Churn Cumulative: 87 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,20 +1,36 @@\n+import sys\n+import os\n import weakref\n-import unittest\n+import shutil\n \n-# just a hack to avoid cyclic imports of scrapy.spider when running this test\n+from twisted.trial import unittest\n+\n+# ugly hack to avoid cyclic imports of scrapy.spider when running this test\n # alone\n import scrapy.spider \n from scrapy.contrib.spidermanager import TwistedPluginSpiderManager\n from scrapy.http import Request\n \n+module_dir = os.path.dirname(os.path.abspath(__file__))\n+\n class TwistedPluginSpiderManagerTest(unittest.TestCase):\n \n     def setUp(self):\n+        orig_spiders_dir = os.path.join(module_dir, 'test_spiders')\n+        self.tmpdir = self.mktemp()\n+        os.mkdir(self.tmpdir)\n+        self.spiders_dir = os.path.join(self.tmpdir, 'test_spiders_xxx')\n+        shutil.copytree(orig_spiders_dir, self.spiders_dir)\n+        sys.path.append(self.tmpdir)\n         self.spiderman = TwistedPluginSpiderManager()\n         assert not self.spiderman.loaded\n-        self.spiderman.load(['scrapy.tests.test_contrib_spidermanager'])\n+        self.spiderman.load(['test_spiders_xxx'])\n         assert self.spiderman.loaded\n \n+    def tearDown(self):\n+        del self.spiderman\n+        sys.path.remove(self.tmpdir)\n+\n     def test_list(self):\n         self.assertEqual(set(self.spiderman.list()), \n             set(['spider1', 'spider2']))\n@@ -57,6 +73,3 @@ class TwistedPluginSpiderManagerTest(unittest.TestCase):\n         self.spiderman.close_spider(spider1)\n         spider2 = self.spiderman.create(\"spider1\")\n         assert spider1 is not spider2\n-\n-if __name__ == '__main__':\n-    unittest.main()\n\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a71dc295af55f29dc579d83eaca096d9a12aa128", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 87 | Churn Cumulative: 431 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 7 | DMM Complexity: 0.2708333333333333\n\nDIFF:\n@@ -141,6 +141,7 @@ LOG_STDOUT = False\n LOG_LEVEL = 'DEBUG'\n LOG_FILE = None\n \n+MAIL_DEBUG = False\n MAIL_HOST = 'localhost'\n MAIL_FROM = 'scrapy@localhost'\n \n\n@@ -17,6 +17,13 @@ from twisted.mail.smtp import SMTPSenderFactory\n from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n+from scrapy.utils.signal import send_catch_log\n+\n+\n+# signal sent when message is sent\n+# args: to, subject, body, cc, attach, msg\n+mail_sent = object()\n+\n \n class MailSender(object):\n \n@@ -53,6 +60,14 @@ class MailSender(object):\n         else:\n             msg.set_payload(body)\n \n+        send_catch_log(signal=mail_sent, to=to, subject=subject, body=body,\n+                       cc=cc, attach=attachs, msg=msg)\n+\n+        if settings.getbool('MAIL_DEBUG'):\n+            log.msg('Debug mail sent OK: To=%s Cc=%s Subject=\"%s\" Attachs=%d' % \\\n+                (to, cc, subject, len(attachs)), level=log.DEBUG)\n+            return\n+\n         dfd = self._sendmail(self.smtphost, self.mailfrom, rcpts, msg.as_string())\n         dfd.addCallbacks(self._sent_ok, self._sent_failed,\n             callbackArgs=[to, cc, subject, len(attachs)],\n\n@@ -0,0 +1,71 @@\n+from cStringIO import StringIO\n+import unittest\n+\n+from scrapy.xlib.pydispatch import dispatcher\n+\n+from scrapy.conf import settings\n+from scrapy.mail import MailSender, mail_sent\n+\n+\n+class MailSenderTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        settings.disabled = False\n+        settings.overrides['MAIL_DEBUG'] = True\n+\n+        self.catched_msg = None\n+\n+        dispatcher.connect(self._catch_mail_sent, signal=mail_sent)\n+\n+    def test_send(self):\n+        mailsender = MailSender()\n+        mailsender.send(to=['test@scrapy.org'], subject='subject', body='body')\n+\n+        assert self.catched_msg\n+\n+        self.assertEqual(self.catched_msg['to'], ['test@scrapy.org'])\n+        self.assertEqual(self.catched_msg['subject'], 'subject')\n+        self.assertEqual(self.catched_msg['body'], 'body')\n+\n+        msg = self.catched_msg['msg']\n+        self.assertEqual(msg['to'], 'test@scrapy.org')\n+        self.assertEqual(msg['subject'], 'subject')\n+        self.assertEqual(msg.get_payload(), 'body')\n+\n+    def test_send_attach(self):\n+        attach = StringIO()\n+        attach.write('content')\n+        attach.seek(0)\n+        attachs = [('attachment', 'text/plain', attach)]\n+\n+        mailsender = MailSender()\n+        mailsender.send(to=['test@scrapy.org'], subject='subject', body='body',\n+                       attachs=attachs)\n+\n+        assert self.catched_msg\n+        self.assertEqual(self.catched_msg['to'], ['test@scrapy.org'])\n+        self.assertEqual(self.catched_msg['subject'], 'subject')\n+        self.assertEqual(self.catched_msg['body'], 'body')\n+\n+        msg = self.catched_msg['msg']\n+        self.assertEqual(msg['to'], 'test@scrapy.org')\n+        self.assertEqual(msg['subject'], 'subject')\n+\n+        payload = msg.get_payload()\n+        assert isinstance(payload, list)\n+        self.assertEqual(len(payload), 2)\n+\n+        text, attach = payload\n+        self.assertEqual(text.get_payload(decode=True), 'body')\n+        self.assertEqual(attach.get_payload(decode=True), 'content')\n+\n+    def tearDown(self):\n+        del settings.overrides['MAIL_DEBUG']\n+        settings.disabled = True\n+\n+    def _catch_mail_sent(self, **kwargs):\n+        self.catched_msg = dict(**kwargs)\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e995c5c7ff26ecbc52a57e6209aed7149c59c688", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 53 | Churn Cumulative: 1155 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,14 +4,26 @@ tests for page parsing\n Page parsing effectiveness is measured through the evaluation system. These\n tests should focus on specific bits of functionality work correctly.\n \"\"\"\n-from unittest import TestCase\n+from twisted.trial.unittest import TestCase, SkipTest\n from scrapy.contrib.ibl.htmlpage import HtmlPage\n-from scrapy.contrib.ibl.extraction import InstanceBasedLearningExtractor\n from scrapy.contrib.ibl.descriptor import (FieldDescriptor as A, \n         ItemDescriptor)\n from scrapy.contrib.ibl.extractors import (contains_any_numbers,\n         image_url)\n \n+try:\n+    import nltk\n+except ImportError:\n+    nltk = None\n+\n+try:\n+    import numpy\n+except ImportError:\n+    numpy = None\n+\n+if nltk and numpy:\n+    from scrapy.contrib.ibl.extraction import InstanceBasedLearningExtractor\n+\n # simple page with all features\n \n ANNOTATED_PAGE1 = u\"\"\"\n@@ -778,7 +790,13 @@ TEST_DATA = [\n \n class TestExtraction(TestCase):\n \n-    def _run(self, name, templates, page, extractors, expected_output):\n+    def setUp(self):\n+        if not nltk:\n+            raise SkipTest(\"nltk not available\")\n+        if not numpy:\n+            raise SkipTest(\"numpy not available\")\n+\n+    def _run_extraction(self, name, templates, page, extractors, expected_output):\n         self.trace = None\n         template_pages = [HtmlPage(None, {}, t) for t in templates]\n         extractor = InstanceBasedLearningExtractor(template_pages, extractors, True)\n@@ -810,7 +828,7 @@ class TestExtraction(TestCase):\n     def test_expected_outputs(self):\n         try:\n             for data in TEST_DATA:\n-                self._run(*data)\n+                self._run_extraction(*data)\n         except AssertionError:\n             if self.trace:\n                 print \"Trace:\"\n\n@@ -5,16 +5,29 @@ import os\n from cStringIO import StringIO                                                                                                    \n from gzip import GzipFile\n \n-from unittest import TestCase\n+from twisted.trial.unittest import TestCase, SkipTest\n from scrapy.utils.python import str_to_unicode\n from scrapy.utils.py26 import json\n \n from scrapy.contrib.ibl.htmlpage import HtmlPage\n-from scrapy.contrib.ibl.extraction.pageparsing import (InstanceLearningParser, \n-        TemplatePageParser, ExtractionPageParser)\n-from scrapy.contrib.ibl.extraction.pageobjects import TokenDict, TokenType\n from scrapy.tests.test_contrib_ibl import path\n \n+try:\n+    import nltk\n+except ImportError:\n+    nltk = None\n+\n+try:\n+    import numpy\n+except ImportError:\n+    numpy = None\n+\n+if nltk and numpy:\n+    from scrapy.contrib.ibl.extraction.pageparsing import (\n+        InstanceLearningParser, TemplatePageParser, ExtractionPageParser)\n+    from scrapy.contrib.ibl.extraction.pageobjects import TokenDict, TokenType\n+\n+\n SIMPLE_PAGE = u\"\"\"\n <html> <p some-attr=\"foo\">this is a test</p> </html>\n \"\"\"\n@@ -160,6 +173,12 @@ def _tags(pp, predicate):\n \n class TestPageParsing(TestCase):\n \n+    def setUp(self):\n+        if not nltk:\n+            raise SkipTest(\"nltk not available\")\n+        if not numpy:\n+            raise SkipTest(\"numpy not available\")\n+\n     def test_instance_parsing(self):\n         pp = _parse_page(InstanceLearningParser, SIMPLE_PAGE)\n         # all tags\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#031eb1e5edefe1c5841e110f6e50d24e8663ec62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 39 | Files Changed: 2 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 39 | Churn Cumulative: 401 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -208,8 +208,6 @@ SPIDER_MIDDLEWARES_BASE = {\n \n SPIDER_MODULES = []\n \n-SPIDER_SCHEDULER = 'scrapy.contrib.spiderscheduler.FifoSpiderScheduler'\n-\n SPIDERPROFILER_ENABLED = False\n \n STATS_CLASS = 'scrapy.stats.collector.MemoryStatsCollector'\n\n@@ -1,37 +0,0 @@\n-\"\"\"\n-The Spider Scheduler keeps track of next spiders to scrape. They must implement\n-the following methods:\n-\n-* next_spider()\n-  return next spider to scrape and remove it from pending queue\n-\n-* add_spider(spider)\n-  add spider to pending queue\n-\n-* remove_pending_spider(spider)\n-  remove (all occurrences) of spider from pending queue, do nothing if not\n-  pending\n-\n-* has_pending_spider(spider)\n-  Return ``True`` if the spider is pending to scrape, ``False`` otherwise\n-\n-\"\"\"\n-\n-class FifoSpiderScheduler(object):\n-    \"\"\"Basic spider scheduler based on a FIFO queue\"\"\"\n-\n-    def __init__(self):\n-        self._pending_spiders = []\n-\n-    def next_spider(self) :\n-        if self._pending_spiders:\n-            return self._pending_spiders.pop(0)\n-\n-    def add_spider(self, spider):\n-        self._pending_spiders.append(spider)\n-\n-    def remove_pending_spider(self, spider):\n-        self._pending_spiders = [d for d in self._pending_spiders if d != spider]\n-\n-    def has_pending_spider(self, spider):\n-        return spider in self._pending_spiders\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fcdc4ee7d9983615df7d4600ee94eedbb1a5f0ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 31 | Churn Cumulative: 199 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: 0.7083333333333334\n\nDIFF:\n@@ -15,6 +15,14 @@ class RedirectMiddleware(object):\n         self.priority_adjust = settings.getint('REDIRECT_PRIORITY_ADJUST')\n \n     def process_response(self, request, response, spider):\n+        if request.method.upper() == 'HEAD':\n+            if response.status in [301, 302, 303, 307] and 'Location' in response.headers:\n+                redirected_url = urljoin_rfc(request.url, response.headers['location'])\n+                redirected = request.replace(url=redirected_url)\n+                return self._redirect(redirected, request, spider, response.status)\n+            else:\n+                return response\n+\n         if response.status in [302, 303] and 'Location' in response.headers:\n             redirected_url = urljoin_rfc(request.url, response.headers['location'])\n             redirected = self._redirect_request_using_get(request, redirected_url)\n\n@@ -18,19 +18,25 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         assert req2.priority > req.priority\n \n     def test_redirect_301(self):\n+        def _test(method):\n             url = 'http://www.example.com/301'\n             url2 = 'http://www.example.com/redirected'\n-        req = Request(url)\n+            req = Request(url, method=method)\n             rsp = Response(url, headers={'Location': url2}, status=301)\n \n             req2 = self.mw.process_response(req, rsp, self.spider)\n             assert isinstance(req2, Request)\n             self.assertEqual(req2.url, url2)\n+            self.assertEqual(req2.method, method)\n \n             # response without Location header but with status code is 3XX should be ignored\n             del rsp.headers['Location']\n             assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n+        _test('GET')\n+        _test('POST')\n+        _test('HEAD')\n+\n     def test_redirect_302(self):\n         url = 'http://www.example.com/302'\n         url2 = 'http://www.example.com/redirected2'\n@@ -53,6 +59,21 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         del rsp.headers['Location']\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n+    def test_redirect_302_head(self):\n+        url = 'http://www.example.com/302'\n+        url2 = 'http://www.example.com/redirected2'\n+        req = Request(url, method='HEAD')\n+        rsp = Response(url, headers={'Location': url2}, status=302)\n+\n+        req2 = self.mw.process_response(req, rsp, self.spider)\n+        assert isinstance(req2, Request)\n+        self.assertEqual(req2.url, url2)\n+        self.assertEqual(req2.method, 'HEAD')\n+\n+        # response without Location header but with status code is 3XX should be ignored\n+        del rsp.headers['Location']\n+        assert self.mw.process_response(req, rsp, self.spider) is rsp\n+\n     def test_meta_refresh(self):\n         body = \"\"\"<html>\n             <head><meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9523cab25cbffdae0f94e4fd31847bc54e34640c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1928 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -281,7 +281,7 @@ class ExecutionEngine(object):\n         dfd.addErrback(log.err, \"Unhandled error on SpiderManager.close_spider()\",\n             spider=spider)\n         dfd.addBoth(lambda _: log.msg(\"Spider closed (%s)\" % reason, spider=spider))\n-        reactor.callLater(0, self._spider_closed_callback)\n+        dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n         return dfd\n \n     def _finish_stopping_engine(self):\n\n@@ -52,7 +52,7 @@ class ExecutionManager(object):\n             self.engine.crawl(request, spider)\n \n     @defer.inlineCallbacks\n-    def _spider_closed(self):\n+    def _spider_closed(self, spider=None):\n         if not self.engine.open_spiders:\n             is_finished = yield defer.maybeDeferred(self.queue.is_finished)\n             if is_finished:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4595c92cc2e9bba3fd631eb4b3ff7d19b1afdc62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 32 | Files Changed: 3 | Hunks: 17 | Methods Changed: 12 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 65 | Churn Cumulative: 2361 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -139,6 +139,7 @@ class Downloader(object):\n         site = self.sites.get(spider)\n         if site and site.closing and not site.active:\n             del self.sites[spider]\n+            site.closing.callback(None)\n \n     def _download(self, site, request, spider):\n         # The order is very important for the following deferreds. Do not change!\n@@ -165,9 +166,7 @@ class Downloader(object):\n \n     def open_spider(self, spider):\n         \"\"\"Allocate resources to begin processing a spider\"\"\"\n-        if spider in self.sites:\n-            raise RuntimeError('Downloader spider already opened: %s' % spider)\n-\n+        assert spider not in self.sites, \"Spider already opened: %s\" % spider\n         self.sites[spider] = SpiderInfo(\n             download_delay=getattr(spider, 'download_delay', None),\n             max_concurrent_requests=getattr(spider, 'max_concurrent_requests', None)\n@@ -175,13 +174,12 @@ class Downloader(object):\n \n     def close_spider(self, spider):\n         \"\"\"Free any resources associated with the given spider\"\"\"\n+        assert spider in self.sites, \"Spider not opened: %s\" % spider\n         site = self.sites.get(spider)\n-        if not site or site.closing:\n-            raise RuntimeError('Downloader spider already closed: %s' % spider)\n-\n-        site.closing = True\n+        site.closing = defer.Deferred()\n         site.cancel_request_calls()\n         self._process_queue(spider)\n+        return site.closing\n \n     def is_idle(self):\n         return not self.sites\n\n@@ -153,6 +153,8 @@ class ExecutionEngine(object):\n             log.msg(\"Unable to crawl Request with no callback: %s\" % request,\n                 level=log.ERROR, spider=spider)\n             return\n+        if spider in self.closing: # ignore requests for spiders being closed\n+            return\n         schd = mustbe_deferred(self.schedule, request, spider)\n         # FIXME: we can't log errors because we would be preventing them from\n         # propagating to the request errback. This should be fixed after the\n@@ -242,43 +244,41 @@ class ExecutionEngine(object):\n \n     def close_spider(self, spider, reason='cancelled'):\n         \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n-        if spider not in self.closing:\n+        if spider in self.closing:\n+            return defer.succeed(None)\n         log.msg(\"Closing spider (%s)\" % reason, spider=spider)\n         self.closing[spider] = reason\n-            self.downloader.close_spider(spider)\n         self.scheduler.clear_pending_requests(spider)\n-            return self._finish_closing_spider_if_idle(spider)\n-        return defer.succeed(None)\n+        dfd = self.downloader.close_spider(spider)\n+        dfd.addBoth(lambda _: self.scheduler.close_spider(spider))\n+        dfd.addErrback(log.err, \"Unhandled error in scheduler.close_spider()\", \\\n+            spider=spider)\n+        dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n+        dfd.addErrback(log.err, \"Unhandled error in scraper.close_spider()\", \\\n+            spider=spider)\n+        dfd.addBoth(lambda _: self._finish_closing_spider(spider))\n+        if self.killed:\n+            return self._finish_closing_spider(spider)\n+        return dfd\n \n     def _close_all_spiders(self):\n         dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n         dlist = defer.DeferredList(dfds)\n         return dlist\n \n-    def _finish_closing_spider_if_idle(self, spider):\n-        \"\"\"Call _finish_closing_spider if spider is idle\"\"\"\n-        if self.spider_is_idle(spider) or self.killed:\n-            return self._finish_closing_spider(spider)\n-        else:\n-            dfd = defer.Deferred()\n-            dfd.addCallback(self._finish_closing_spider_if_idle)\n-            delay = 5 if self.running else 1\n-            reactor.callLater(delay, dfd.callback, spider)\n-            return dfd\n-\n     def _finish_closing_spider(self, spider):\n         \"\"\"This function is called after the spider has been closed\"\"\"\n-        self.scheduler.close_spider(spider)\n-        self.scraper.close_spider(spider)\n         reason = self.closing.pop(spider, 'finished')\n         send_catch_log(signal=signals.spider_closed, sender=self.__class__, \\\n             spider=spider, reason=reason)\n-        stats.close_spider(spider, reason=reason)\n         call = self._next_request_calls.pop(spider, None)\n         if call and call.active():\n             call.cancel()\n-        dfd = defer.maybeDeferred(spiders.close_spider, spider)\n-        dfd.addErrback(log.err, \"Unhandled error on SpiderManager.close_spider()\",\n+        dfd = defer.maybeDeferred(stats.close_spider, spider, reason=reason)\n+        dfd.addErrback(log.err, \"Unhandled error in stats.close_spider()\",\n+            spider=spider)\n+        dfd.addBoth(lambda _: spiders.close_spider(spider))\n+        dfd.addErrback(log.err, \"Unhandled error in spiders.close_spider()\",\n             spider=spider)\n         dfd.addBoth(lambda _: log.msg(\"Spider closed (%s)\" % reason, spider=spider))\n         dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n\n@@ -29,6 +29,7 @@ class SpiderInfo(object):\n         self.active = set()\n         self.active_size = 0\n         self.itemproc_size = 0\n+        self.closing = None\n \n     def add_response_request(self, response, request):\n         deferred = defer.Deferred()\n@@ -68,16 +69,15 @@ class Scraper(object):\n \n     def open_spider(self, spider):\n         \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n-        if spider in self.sites:\n-            raise RuntimeError('Scraper spider already opened: %s' % spider)\n+        assert spider not in self.sites, \"Spider already opened: %s\" % spider\n         self.sites[spider] = SpiderInfo()\n         self.itemproc.open_spider(spider)\n \n     def close_spider(self, spider):\n         \"\"\"Close a spider being scraped and release its resources\"\"\"\n-        if spider not in self.sites:\n-            raise RuntimeError('Scraper spider already closed: %s' % spider)\n-        self.sites.pop(spider)\n+        assert spider in self.sites, \"Spider not opened: %s\" % spider\n+        site = self.sites[spider]\n+        site.closing = defer.Deferred()\n         self.itemproc.close_spider(spider)\n \n     def is_idle(self):\n@@ -93,6 +93,9 @@ class Scraper(object):\n         #    spider=spider)\n         def finish_scraping(_):\n             site.finish_response(response)\n+            if site.closing and site.is_idle():\n+                del self.sites[spider]\n+                site.closing.callback(None)\n             self._scrape_next(spider, site)\n             return _\n         dfd.addBoth(finish_scraping)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#38b57931528b473877d2326cbc1e13dae4ae0bf1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 43 | Churn Cumulative: 358 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -110,7 +110,7 @@ EXTENSIONS = {}\n EXTENSIONS_BASE = {\n     'scrapy.contrib.corestats.CoreStats': 0,\n     'scrapy.management.web.WebConsole': 0,\n-    'scrapy.management.telnet.TelnetConsole': 0,\n+    'scrapy.telnet.TelnetConsole': 0,\n     'scrapy.contrib.webconsole.scheduler.SchedulerQueue': 0,\n     'scrapy.contrib.webconsole.livestats.LiveStats': 0,\n     'scrapy.contrib.webconsole.spiderctl.Spiderctl': 0,\n\n@@ -15,6 +15,7 @@ from scrapy.core.exceptions import NotConfigured\n from scrapy.core.manager import scrapymanager\n from scrapy.spider import spiders\n from scrapy.stats import stats\n+from scrapy.utils.signal import send_catch_log\n from scrapy.utils.trackref import print_live_refs\n from scrapy.utils.engine import print_engine_status\n from scrapy.conf import settings\n@@ -25,8 +26,28 @@ try:\n except ImportError:\n     hpy = None\n \n-# if you add entries here also update topics/telnetconsole.rst\n-telnet_namespace = {\n+# signal to update telnet variables\n+# args: telnet_vars\n+update_telnet_vars = object()\n+\n+\n+class TelnetConsole(protocol.ServerFactory):\n+\n+    def __init__(self):\n+        if not settings.getbool('TELNETCONSOLE_ENABLED'):\n+            raise NotConfigured\n+        self.noisy = False\n+        port = settings.getint('TELNETCONSOLE_PORT')\n+        reactor.callWhenRunning(reactor.listenTCP, port, self)\n+\n+    def protocol(self):\n+        telnet_vars = self._get_telnet_vars()\n+        return telnet.TelnetTransport(telnet.TelnetBootstrapProtocol,\n+            insults.ServerProtocol, manhole.Manhole, telnet_vars)\n+\n+    def _get_telnet_vars(self):\n+        # Note: if you add entries here also update topics/telnetconsole.rst\n+        telnet_vars = {\n             'engine': scrapymanager.engine,\n             'manager': scrapymanager,\n             'extensions': extensions,\n@@ -38,17 +59,5 @@ telnet_namespace = {\n             'prefs': print_live_refs,\n             'hpy': hpy,\n         }\n-\n-def makeProtocol():\n-    return telnet.TelnetTransport(telnet.TelnetBootstrapProtocol,\n-        insults.ServerProtocol, manhole.Manhole, telnet_namespace)\n-\n-class TelnetConsole(protocol.ServerFactory):\n-\n-    def __init__(self):\n-        if not settings.getbool('TELNETCONSOLE_ENABLED'):\n-            raise NotConfigured\n-        self.protocol = makeProtocol\n-        self.noisy = False\n-        port = settings.getint('TELNETCONSOLE_PORT')\n-        reactor.callWhenRunning(reactor.listenTCP, port, self)\n+        send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n+        return telnet_vars\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#72df5cb7ef9d466bbf248c06c3e1f2aee00b2d2e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 227 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -38,8 +38,6 @@ def parse_url(url):\n \n class Shell(object):\n \n-    requires_project = False\n-\n     def __init__(self, update_vars=None, nofetch=False):\n         self.vars = {}\n         self.update_vars = update_vars\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#76ed9d442b10f6a002625a32c6fcaeb01e8c84c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 5 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 16/16 | Churn Δ: 6 | Churn Cumulative: 498 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -6,9 +6,9 @@ from twisted.internet import defer\n from twisted.python.failure import Failure\n \n from scrapy.utils.datatypes import PriorityQueue, PriorityStack\n+from scrapy.core.schedulermw import SchedulerMiddlewareManager\n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.conf import settings\n-from .middleware import SchedulerMiddlewareManager\n \n class Scheduler(object):\n     \"\"\"The scheduler decides what to scrape next. In other words, it defines the\n\n@@ -1,2 +0,0 @@\n-from scrapy.core.scheduler.schedulers import Scheduler\n-from scrapy.core.scheduler.middleware import SchedulerMiddlewareManager\n\n\n@@ -12,7 +12,7 @@ from scrapy.core.exceptions import IgnoreRequest, DropItem\n from scrapy.core import signals\n from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n-from scrapy.spider.middleware import SpiderMiddlewareManager\n+from scrapy.core.spidermw import SpiderMiddlewareManager\n from scrapy import log\n from scrapy.stats import stats\n from scrapy.conf import settings\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#62f5c61a9d70d98bb9a0ed6243524c0ee3e2764e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 262 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -124,11 +124,15 @@ class RequestTest(unittest.TestCase):\n         def somecallback():\n             pass\n \n-        r1 = self.request_class(\"http://www.example.com\", callback=somecallback)\n+        r1 = self.request_class(\"http://www.example.com\", callback=somecallback, errback=somecallback)\n         r1.meta['foo'] = 'bar'\n         r2 = r1.copy()\n \n-        assert r1.deferred is not r2.deferred\n+        # make sure copy does not propagate callbacks\n+        assert r1.callback is somecallback\n+        assert r1.errback is somecallback\n+        assert r2.callback is None\n+        assert r2.errback is None\n \n         # make sure meta dict is shallow copied\n         assert r1.meta is not r2.meta, \"meta must be a shallow copy, not identical\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2499dfee5ed64a4dfdf67fd2e794ba5d8e9403b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 132 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -63,10 +63,6 @@ class RequestGeneratorTest(unittest.TestCase):\n         requests = reqgen.generate_requests(self.response)\n         self.failUnless(self._equal_requests_list(requests, self.requests))\n \n-        for req in requests:\n-            # check callback\n-            self.failUnlessEqual(req.deferred, self.deferred)\n-\n     def test_request_processor(self):\n         extractors = [\n             SgmlRequestExtractor()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6a33d6c4d08f73007129657a5b16ce69c3490446", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 749 | Lines Deleted: 21 | Files Changed: 17 | Hunks: 24 | Methods Changed: 60 | Complexity Δ (Sum/Max): 123/34 | Churn Δ: 770 | Churn Cumulative: 1391 | Contributors (this commit): 5 | Commits (past 90d): 32 | Contributors (cumulative): 24 | DMM Complexity: 0.8552631578947368\n\nDIFF:\n@@ -0,0 +1,114 @@\n+#!/usr/bin/env python\n+\"\"\"\n+Example script to control and monitor Scrapy using its web service. It only\n+provides a reduced functionality as its main purpose is to illustrate how to\n+write a web service client. Feel free to improve or write you own.\n+\"\"\"\n+\n+import sys, optparse, urllib\n+from urlparse import urljoin\n+\n+from scrapy.utils.jsonrpc import jsonrpc_client_call, JsonRpcError\n+from scrapy.utils.py26 import json\n+\n+def get_commands():\n+    return {\n+        'help': cmd_help,\n+        'run': cmd_run,\n+        'list-available': cmd_list_available,\n+        'list-running': cmd_list_running,\n+        'list-resources': cmd_list_resources,\n+        'list-extensions': cmd_list_extensions,\n+        'get-global-stats': cmd_get_global_stats,\n+        'get-spider-stats': cmd_get_spider_stats,\n+    }\n+\n+def cmd_help(args, opts):\n+    \"\"\"help - list available commands\"\"\"\n+    print \"Available commands:\"\n+    for _, func in sorted(get_commands().items()):\n+        print \"  \", func.__doc__\n+\n+def cmd_run(args, opts):\n+    \"\"\"run <spider_name> - schedule spider for running\"\"\"\n+    jsonrpc_call(opts, 'manager/queue', 'append_spider_name', args[0])\n+\n+def cmd_list_running(args, opts):\n+    \"\"\"list-running - list running spiders\"\"\"\n+    for x in json_get(opts, 'manager/engine/open_spiders'):\n+        print x\n+\n+def cmd_list_available(args, opts):\n+    \"\"\"list-available - list name of available spiders\"\"\"\n+    for x in jsonrpc_call(opts, 'spiders', 'list'):\n+        print x\n+\n+def cmd_list_resources(args, opts):\n+    \"\"\"list-resources - list available web service resources\"\"\"\n+    for x in json_get(opts, '')['resources']:\n+        print x\n+\n+def cmd_list_extensions(args, opts):\n+    \"\"\"list-extensions - list enabled extensions\"\"\"\n+    for x in jsonrpc_call(opts, 'extensions/enabled', 'keys'):\n+        print x\n+\n+def cmd_get_spider_stats(args, opts):\n+    \"\"\"get-spider-stats <spider> - get stats of a running spider\"\"\"\n+    stats = jsonrpc_call(opts, 'stats', 'get_stats', args[0])\n+    for name, value in stats.items():\n+        print \"%-40s %s\" % (name, value)\n+\n+def cmd_get_global_stats(args, opts):\n+    \"\"\"get-global-stats - get global stats\"\"\"\n+    stats = jsonrpc_call(opts, 'stats', 'get_stats')\n+    for name, value in stats.items():\n+        print \"%-40s %s\" % (name, value)\n+\n+def get_wsurl(opts, path):\n+    return urljoin(\"http://%s:%s/\"% (opts.host, opts.port), path)\n+\n+def jsonrpc_call(opts, path, method, *args, **kwargs):\n+    url = get_wsurl(opts, path)\n+    return jsonrpc_client_call(url, method, *args, **kwargs)\n+\n+def json_get(opts, path):\n+    url = get_wsurl(opts, path)\n+    return json.loads(urllib.urlopen(url).read())\n+\n+def parse_opts():\n+    usage = \"%prog [options] <command> [arg] ...\"\n+    description = \"Scrapy web service control script. Use '%prog help' \" \\\n+        \"to see the list of available commands.\"\n+    op = optparse.OptionParser(usage=usage, description=description)\n+    op.add_option(\"-H\", dest=\"host\", default=\"localhost\", \\\n+        help=\"Scrapy host to connect to\")\n+    op.add_option(\"-P\", dest=\"port\", type=\"int\", default=6080, \\\n+        help=\"Scrapy port to connect to\")\n+    opts, args = op.parse_args()\n+    if not args:\n+        op.print_help()\n+        sys.exit(2)\n+    cmdname, cmdargs, opts = args[0], args[1:], opts\n+    commands = get_commands()\n+    if cmdname not in commands:\n+        sys.stderr.write(\"Unknown command: %s\\n\\n\" % cmdname)\n+        cmd_help(None, None)\n+        sys.exit(1)\n+    return commands[cmdname], cmdargs, opts\n+\n+def main():\n+    cmd, args, opts = parse_opts()\n+    try:\n+        cmd(args, opts)\n+    except IndexError:\n+        print cmd.__doc__\n+    except JsonRpcError, e:\n+        print str(e)\n+        if e.data:\n+            print \"Server Traceback below:\"\n+            print e.data\n+\n+\n+if __name__ == '__main__':\n+    main()\n\n@@ -109,13 +109,8 @@ EXTENSIONS = {}\n \n EXTENSIONS_BASE = {\n     'scrapy.contrib.corestats.CoreStats': 0,\n-    'scrapy.management.web.WebConsole': 0,\n+    'scrapy.webservice.WebService': 0,\n     'scrapy.telnet.TelnetConsole': 0,\n-    'scrapy.contrib.webconsole.scheduler.SchedulerQueue': 0,\n-    'scrapy.contrib.webconsole.livestats.LiveStats': 0,\n-    'scrapy.contrib.webconsole.spiderctl.Spiderctl': 0,\n-    'scrapy.contrib.webconsole.enginestatus.EngineStatus': 0,\n-    'scrapy.contrib.webconsole.stats.StatsDump': 0,\n     'scrapy.contrib.memusage.MemoryUsage': 0,\n     'scrapy.contrib.memdebug.MemoryDebugger': 0,\n     'scrapy.contrib.closespider.CloseSpider': 0,\n@@ -232,3 +227,14 @@ WEBCONSOLE_ENABLED = True\n WEBCONSOLE_PORT = 6080\n WEBCONSOLE_LOGFILE = None\n \n+WEBSERVICE_ENABLED = True\n+WEBSERVICE_LOGFILE = None\n+WEBSERVICE_PORT = 6080\n+WEBSERVICE_RESOURCES = {}\n+WEBSERVICE_RESOURCES_BASE = {\n+    'scrapy.contrib.webservice.manager.ManagerResource': 1,\n+    'scrapy.contrib.webservice.enginestatus.EngineStatusResource': 1,\n+    'scrapy.contrib.webservice.extensions.ExtensionsResource': 1,\n+    'scrapy.contrib.webservice.spiders.SpidersResource': 1,\n+    'scrapy.contrib.webservice.stats.StatsResource': 1,\n+}\n\n@@ -0,0 +1,3 @@\n+import warnings\n+warnings.warn(\"Web console is deprecated. Consider using web service instead.\", \\\n+    DeprecationWarning, stacklevel=2)\n\n\n@@ -0,0 +1,24 @@\n+from scrapy.webservice import JsonResource\n+from scrapy.core.manager import scrapymanager\n+from scrapy.utils.engine import get_engine_status\n+\n+class EngineStatusResource(JsonResource):\n+\n+    ws_name = 'enginestatus'\n+\n+    def __init__(self, spider_name=None, _manager=scrapymanager):\n+        JsonResource.__init__(self)\n+        self._spider_name = spider_name\n+        self.isLeaf = spider_name is not None\n+        self._manager = _manager\n+\n+    def render_GET(self, txrequest):\n+        status = get_engine_status(self._manager.engine)\n+        if self._spider_name is None:\n+            return status\n+        for sp, st in status['spiders'].items():\n+            if sp.name == self._spider_name:\n+                return st\n+\n+    def getChild(self, name, txrequest):\n+        return EngineStatusResource(name, self._manager)\n\n@@ -0,0 +1,10 @@\n+from scrapy.webservice import JsonRpcResource\n+from scrapy.extension import extensions\n+\n+class ExtensionsResource(JsonRpcResource):\n+\n+    ws_name = 'extensions'\n+\n+    def __init__(self, _extensions=extensions):\n+        JsonRpcResource.__init__(self)\n+        self._target = _extensions\n\n@@ -0,0 +1,10 @@\n+from scrapy.webservice import JsonRpcResource\n+from scrapy.core.manager import scrapymanager\n+\n+class ManagerResource(JsonRpcResource):\n+\n+    ws_name = 'manager'\n+\n+    def __init__(self, _manager=scrapymanager):\n+        JsonRpcResource.__init__(self)\n+        self._target = _manager\n\n@@ -0,0 +1,10 @@\n+from scrapy.webservice import JsonRpcResource\n+from scrapy.spider import spiders\n+\n+class SpidersResource(JsonRpcResource):\n+\n+    ws_name = 'spiders'\n+\n+    def __init__(self, _spiders=spiders):\n+        JsonRpcResource.__init__(self)\n+        self._target = _spiders\n\n@@ -0,0 +1,10 @@\n+from scrapy.webservice import JsonRpcResource\n+from scrapy.stats import stats\n+\n+class StatsResource(JsonRpcResource):\n+\n+    ws_name = 'stats'\n+\n+    def __init__(self, _stats=stats):\n+        JsonRpcResource.__init__(self)\n+        self._target = _stats\n\n@@ -1,8 +1,6 @@\n-\"\"\"\n-Scrapy Web Console extension\n-\n-See docs/topics/webconsole.rst\n-\"\"\"\n+import warnings\n+warnings.warn(\"Scrapy web console is deprecated. Consider using web service instead.\", \\\n+    DeprecationWarning, stacklevel=2)\n \n import re\n import socket\n\n@@ -77,6 +77,6 @@ class BaseSpider(object_ref):\n         raise NotImplementedError\n \n     def __str__(self):\n-        return \"<%s %r>\" % (type(self).__name__, self.name)\n+        return \"<%s %r at 0x%0x>\" % (type(self).__name__, self.name, id(self))\n \n     __repr__ = __str__\n\n@@ -0,0 +1,112 @@\n+import unittest\n+from cStringIO import StringIO\n+\n+from scrapy.utils.jsonrpc import jsonrpc_client_call, jsonrpc_server_call, \\\n+    JsonRpcError, jsonrpc_errors\n+from scrapy.utils.py26 import json\n+\n+class urllib_stub(object):\n+    def __init__(self, result=None, error=None):\n+        response = {}\n+        if result:\n+            response.update(result=result)\n+        if error:\n+            response.update(error=error)\n+        self.response = json.dumps(response)\n+        self.request = None\n+\n+    def urlopen(self, url, request):\n+        self.url = url\n+        self.request = request\n+        return StringIO(self.response)\n+\n+class TestTarget(object):\n+\n+    def call(self, *args, **kwargs):\n+        return list(args), kwargs\n+\n+    def exception(self):\n+        raise Exception(\"testing-errors\")\n+\n+class JsonRpcUtilsTestCase(unittest.TestCase):\n+\n+    def test_jsonrpc_client_call_request(self):\n+        ul = urllib_stub(1)\n+        jsonrpc_client_call('url', 'test', 'one', 2, _urllib=ul)\n+        req = json.loads(ul.request)\n+        assert 'id' in req\n+        self.assertEqual(ul.url, 'url')\n+        self.assertEqual(req['jsonrpc'], '2.0')\n+        self.assertEqual(req['method'], 'test')\n+        self.assertEqual(req['params'], ['one', 2])\n+\n+    def test_jsonrpc_client_call_response(self):\n+        ul = urllib_stub()\n+        # must return result or error\n+        self.assertRaises(ValueError, jsonrpc_client_call, 'url', 'test', _urllib=ul)\n+        ul = urllib_stub(result={'one': 1})\n+        self.assertEquals(jsonrpc_client_call('url', 'test', _urllib=ul), {'one': 1})\n+        ul = urllib_stub(error={'code': 123, 'message': 'hello', 'data': 'some data'})\n+\n+        raised = False\n+        try:\n+            jsonrpc_client_call('url', 'test', _urllib=ul)\n+        except JsonRpcError, e:\n+            raised = True\n+            self.assertEqual(e.code, 123)\n+            self.assertEqual(e.message, 'hello')\n+            self.assertEqual(e.data, 'some data')\n+            assert '123' in str(e)\n+            assert 'hello' in str(e)\n+        assert raised, \"JsonRpcError not raised\"\n+\n+    def test_jsonrpc_server_call(self):\n+        t = TestTarget()\n+        r = jsonrpc_server_call(t, 'invalid json data')\n+        assert 'error' in r\n+        assert r['jsonrpc'] == '2.0'\n+        assert r['id'] is None\n+        self.assertEqual(r['error']['code'], jsonrpc_errors.PARSE_ERROR)\n+        assert 'Traceback' in r['error']['data']\n+\n+        r = jsonrpc_server_call(t, '{\"test\": \"test\"}')\n+        assert 'error' in r\n+        assert r['jsonrpc'] == '2.0'\n+        assert r['id'] is None\n+        self.assertEqual(r['error']['code'], jsonrpc_errors.INVALID_REQUEST)\n+\n+        r = jsonrpc_server_call(t, '{\"method\": \"notfound\", \"id\": 1}')\n+        assert 'error' in r\n+        assert r['jsonrpc'] == '2.0'\n+        assert r['id'] == 1\n+        self.assertEqual(r['error']['code'], jsonrpc_errors.METHOD_NOT_FOUND)\n+\n+        r = jsonrpc_server_call(t, '{\"method\": \"exception\", \"id\": 1}')\n+        assert 'error' in r\n+        assert r['jsonrpc'] == '2.0'\n+        assert r['id'] == 1\n+        self.assertEqual(r['error']['code'], jsonrpc_errors.INTERNAL_ERROR)\n+        assert 'testing-errors' in r['error']['message']\n+        assert 'Traceback' in r['error']['data']\n+\n+        r = jsonrpc_server_call(t, '{\"method\": \"call\", \"id\": 2}')\n+        assert 'result' in r\n+        assert r['jsonrpc'] == '2.0'\n+        assert r['id'] == 2\n+        self.assertEqual(r['result'], ([], {}))\n+\n+        r = jsonrpc_server_call(t, '{\"method\": \"call\", \"params\": [456, 123], \"id\": 3}')\n+        assert 'result' in r\n+        assert r['jsonrpc'] == '2.0'\n+        assert r['id'] == 3\n+        self.assertEqual(r['result'], ([456, 123], {}))\n+\n+        r = jsonrpc_server_call(t, '{\"method\": \"call\", \"params\": {\"data\": 789}, \"id\": 3}')\n+        assert 'result' in r\n+        assert r['jsonrpc'] == '2.0'\n+        assert r['id'] == 3\n+        self.assertEqual(r['result'], ([], {'data': 789}))\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n+\n\n@@ -0,0 +1,125 @@\n+import unittest\n+import datetime\n+from decimal import Decimal\n+\n+from scrapy.utils.serialize import SpiderReferencer, ScrapyJSONEncoder, ScrapyJSONDecoder\n+from scrapy.utils.py26 import json\n+from scrapy.spider import BaseSpider\n+from scrapy.http import Request, Response\n+\n+\n+class ExecutionEngineStub(object):\n+    def __init__(self, open_spiders):\n+        self.open_spiders = open_spiders\n+\n+class ExecutionMangerStub(object):\n+    def __init__(self, open_spiders):\n+        self.engine = ExecutionEngineStub(open_spiders)\n+\n+class BaseTestCase(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.spider1 = BaseSpider('name1')\n+        self.spider2 = BaseSpider('name2')\n+        open_spiders = set([self.spider1, self.spider2])\n+        manager = ExecutionMangerStub(open_spiders)\n+        self.spref = SpiderReferencer(manager)\n+        self.encoder = ScrapyJSONEncoder(spref=self.spref)\n+        self.decoder = ScrapyJSONDecoder(spref=self.spref)\n+\n+class SpiderReferencerTestCase(BaseTestCase):\n+\n+    def test_spiders_and_references(self):\n+        ref1 = self.spref.get_reference_from_spider(self.spider1)\n+        assert isinstance(ref1, str)\n+        assert self.spider1.name in ref1\n+        ref2 = self.spref.get_reference_from_spider(self.spider2)\n+        ref1_ = self.spref.get_reference_from_spider(self.spider1)\n+        assert ref1 == ref1_\n+        assert ref1 != ref2\n+\n+        sp1 = self.spref.get_spider_from_reference(ref1)\n+        sp2 = self.spref.get_spider_from_reference(ref2)\n+        sp1_ = self.spref.get_spider_from_reference(ref1)\n+        assert isinstance(sp1, BaseSpider)\n+        assert sp1 is not sp2\n+        assert sp1 is sp1_\n+\n+        # must return string as-is if spider id not found\n+        assert 'lala' == self.spref.get_spider_from_reference('lala')\n+        # must raise RuntimeError if spider id is not found and spider is not running\n+        self.assertRaises(RuntimeError, self.spref.get_spider_from_reference, 'spider:fffffff')\n+\n+    def test_encode_decode(self):\n+        sr = self.spref\n+        sp1 = self.spider1\n+        sp2 = self.spider2\n+        ref1 = sr.get_reference_from_spider(sp1)\n+        ref2 = sr.get_reference_from_spider(sp2)\n+\n+        examples = [\n+            ('lala', 'lala'),\n+            (sp1, ref1),\n+            (['lala', sp1], ['lala', ref1]),\n+            ({'lala': sp1}, {'lala': ref1}),\n+            ({sp1: sp2}, {ref1: ref2}),\n+            ({sp1: {sp2: ['lala', sp1]}}, {ref1: {ref2: ['lala', ref1]}})\n+        ]\n+        for spiders, refs in examples:\n+            self.assertEqual(sr.encode_references(spiders), refs)\n+            self.assertEqual(sr.decode_references(refs), spiders)\n+\n+class JsonEncoderTestCase(BaseTestCase):\n+    \n+    def test_encode_decode(self):\n+        sr = self.spref\n+        sp1 = self.spider1\n+        sp2 = self.spider2\n+        ref1 = sr.get_reference_from_spider(sp1)\n+        ref2 = sr.get_reference_from_spider(sp2)\n+        dt = datetime.datetime(2010, 1, 2, 10, 11, 12)\n+        dts = \"2010-01-02 10:11:12\"\n+        d = datetime.date(2010, 1, 2)\n+        ds = \"2010-01-02\"\n+        t = datetime.time(10, 11, 12)\n+        ts = \"10:11:12\"\n+        dec = Decimal(\"1000.12\")\n+        decs = \"1000.12\"\n+\n+        examples_encode_decode = [\n+            ('lala', 'lala'),\n+            (sp1, ref1),\n+            (['lala', sp1], ['lala', ref1]),\n+            ({'lala': sp1}, {'lala': ref1}),\n+            ({sp1: sp2}, {ref1: ref2}),\n+            ({sp1: {sp2: ['lala', sp1]}}, {ref1: {ref2: ['lala', ref1]}})\n+        ]\n+        for spiders, refs in examples_encode_decode:\n+            self.assertEqual(self.encoder.encode(spiders), json.dumps(refs))\n+            self.assertEqual(self.decoder.decode(json.dumps(refs)), spiders)\n+\n+        examples_encode_only = [\n+            ({sp1: dt}, {ref1: dts}),\n+            ({sp1: d}, {ref1: ds}),\n+            ({sp1: t}, {ref1: ts}),\n+            ({sp1: dec}, {ref1: decs}),\n+        ]\n+        for spiders, refs in examples_encode_only:\n+            self.assertEqual(self.encoder.encode(spiders), json.dumps(refs))\n+\n+    def test_encode_request(self):\n+        r = Request(\"http://www.example.com/lala\")\n+        rs = self.encoder.encode(r)\n+        assert r.method in rs\n+        assert r.url in rs\n+\n+    def test_encode_response(self):\n+        r = Response(\"http://www.example.com/lala\")\n+        rs = self.encoder.encode(r)\n+        assert r.url in rs\n+        assert str(r.status) in rs\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n+\n\n@@ -37,23 +37,34 @@ def get_engine_status(engine=None):\n         \"engine.scraper.sites[spider].needs_backout()\",\n     ]\n \n-    s = \"Execution engine status\\n\\n\"\n-\n+    status = {'global': {}, 'spiders': {}}\n     for test in global_tests:\n         try:\n-            s += \"%-47s : %s\\n\" % (test, eval(test))\n+            status['global'][test] = eval(test)\n         except Exception, e:\n-            s += \"%-47s : %s (exception)\\n\" % (test, type(e).__name__)\n-    s += \"\\n\"\n+            status['global'][test] = \"%s (exception)\" % type(e).__name__\n     for spider in engine.downloader.sites:\n-        s += \"Spider: %s\\n\" % spider\n+        x = {}\n         for test in spider_tests:\n             try:\n-                s += \"  %-50s : %s\\n\" % (test, eval(test))\n+                x[test] = eval(test)\n             except Exception, e:\n-                s += \"  %-50s : %s (exception)\\n\" % (test, type(e).__name__)\n+                x[test] = \"%s (exception)\" % type(e).__name__\n+            status['spiders'][spider] = x\n+    return status\n+\n+def format_engine_status(engine=None):\n+    status = get_engine_status(engine)\n+    s = \"Execution engine status\\n\\n\"\n+    for test, result in status['global'].items():\n+        s += \"%-47s : %s\\n\" % (test, result)\n+    s += \"\\n\"\n+    for spider, tests in status['spiders'].items():\n+        s += \"Spider: %s\\n\" % spider\n+        for test, result in tests.items():\n+            s += \"  %-50s : %s\\n\" % (test, result)\n     return s\n \n def print_engine_status(engine=None):\n-    print get_engine_status(engine)\n+    print format_engine_status(engine)\n \n\n@@ -0,0 +1,94 @@\n+\"\"\"\n+This module implements the JSON-RPC 2.0 protocol, as defined in:\n+http://groups.google.com/group/json-rpc/web/json-rpc-2-0\n+\"\"\"\n+\n+import urllib\n+import traceback\n+\n+from scrapy.utils.py26 import json\n+from scrapy.utils.serialize import ScrapyJSONDecoder\n+\n+# JSON-RPC 2.0 errors, as defined in:\n+class jsonrpc_errors:\n+    PARSE_ERROR = -32700\n+    INVALID_REQUEST = -32600\n+    METHOD_NOT_FOUND = -32601\n+    INVALID_PARAMS = -32602\n+    INTERNAL_ERROR = -32603\n+\n+class JsonRpcError(Exception):\n+\n+    def __init__(self, code, message, data=None):\n+        super(JsonRpcError, self).__init__()\n+        self.code = code\n+        self.message = message\n+        self.data = data\n+\n+    def __str__(self):\n+        return \"JSON-RPC error (code %d): %s\" % (self.code, self.message)\n+\n+def jsonrpc_client_call(url, method, *args, **kwargs):\n+    \"\"\"Execute a JSON-RPC call on the given url\"\"\"\n+    _urllib = kwargs.pop('_urllib', urllib)\n+    req = {'jsonrpc': '2.0', 'method': method, 'params': args or kwargs, 'id': 1}\n+    res = json.loads(_urllib.urlopen(url, json.dumps(req)).read())\n+    if 'result' in res:\n+        return res['result']\n+    elif 'error' in res:\n+        er = res['error']\n+        raise JsonRpcError(er['code'], er['message'], er['data'])\n+    else:\n+        msg = \"JSON-RPC response must contain 'result' or 'error': %s\" % res\n+        raise ValueError(msg)\n+\n+def jsonrpc_server_call(target, jsonrpc_request, json_decoder=None):\n+    \"\"\"Execute the given JSON-RPC request (as JSON-encoded string) on the given\n+    target object and return the JSON-RPC response, as a dict\n+    \"\"\"\n+    if json_decoder is None:\n+        json_decoder = ScrapyJSONDecoder()\n+\n+    try:\n+        req = json_decoder.decode(jsonrpc_request)\n+    except Exception, e:\n+        return jsonrpc_error(None, jsonrpc_errors.PARSE_ERROR, 'Parse error', \\\n+            traceback.format_exc())\n+\n+    try:\n+        id, methname = req['id'], req['method']\n+    except KeyError:\n+        return jsonrpc_error(None, jsonrpc_errors.INVALID_REQUEST, 'Invalid Request')\n+\n+    try:\n+        method = getattr(target, methname)\n+    except AttributeError:\n+        return jsonrpc_error(id, jsonrpc_errors.METHOD_NOT_FOUND, 'Method not found')\n+\n+    params = req.get('params', [])\n+    a, kw = ([], params) if isinstance(params, dict) else (params, {})\n+    try:\n+        return jsonrpc_result(id, method(*a, **kw))\n+    except Exception, e:\n+        return jsonrpc_error(id, jsonrpc_errors.INTERNAL_ERROR, str(e), \\\n+            traceback.format_exc())\n+\n+def jsonrpc_error(id, code, message, data=None):\n+    \"\"\"Create JSON-RPC error response\"\"\"\n+    return {\n+        'jsonrpc': '2.0',\n+        'error': {\n+            'code': code,\n+            'message': message,\n+            'data': data,\n+        },\n+        'id': id,\n+    }\n+\n+def jsonrpc_result(id, result):\n+    \"\"\"Create JSON-RPC result response\"\"\"\n+    return {\n+        'jsonrpc': '2.0',\n+        'result': result,\n+        'id': id,\n+    }\n\n@@ -0,0 +1,115 @@\n+import re\n+import datetime\n+import decimal\n+\n+from scrapy.core.manager import scrapymanager\n+from scrapy.spider import BaseSpider\n+from scrapy.http import Request, Response\n+from scrapy.utils.py26 import json\n+\n+\n+class SpiderReferencer(object):\n+    \"\"\"Class to serialize (and deserialize) objects (typically dicts)\n+    containing references to running spiders (ie. Spider objects). This is\n+    required because simplejson fails to serialize dicts containing\n+    non-primitive types as keys, even when you override\n+    ScrapyJSONEncoder.default() with a custom encoding mechanism.\n+    \"\"\"\n+\n+    spider_ref_re = re.compile('^spider:([0-9a-f]+)(:.*)?$')\n+\n+    def __init__(self, manager=None):\n+        self.manager = manager or scrapymanager\n+\n+    def get_reference_from_spider(self, spider):\n+        return 'spider:%x:%s' % (id(spider), spider.name)\n+\n+    def get_spider_from_reference(self, ref):\n+        \"\"\"Returns the Spider referenced by text, if text is a spider\n+        reference. Otherwise it returns the text itself. If the text references\n+        a non-running spider it raises a RuntimeError.\n+        \"\"\"\n+        m = self.spider_ref_re.search(ref)\n+        if m:\n+            spid = int(m.group(1), 16)\n+            for spider in self.manager.engine.open_spiders:\n+                if id(spider) == spid:\n+                    return spider\n+            raise RuntimeError(\"Spider not running: %s\" % ref)\n+        return ref\n+\n+    def encode_references(self, obj):\n+        \"\"\"Look for Spider objects and replace them with spider references\"\"\"\n+        if isinstance(obj, BaseSpider):\n+            return self.get_reference_from_spider(obj)\n+        elif isinstance(obj, dict):\n+            d = {}\n+            for k, v in obj.items():\n+                k = self.encode_references(k)\n+                v = self.encode_references(v)\n+                d[k] = v\n+            return d\n+        elif isinstance(obj, (list, tuple)):\n+            return [self.encode_references(x) for x in obj]\n+        else:\n+            return obj\n+\n+    def decode_references(self, obj):\n+        \"\"\"Look for spider references and replace them with Spider objects\"\"\"\n+        if isinstance(obj, basestring):\n+            return self.get_spider_from_reference(obj)\n+        elif isinstance(obj, dict):\n+            d = {}\n+            for k, v in obj.items():\n+                k = self.decode_references(k)\n+                v = self.decode_references(v)\n+                d[k] = v\n+            return d\n+        elif isinstance(obj, (list, tuple)):\n+            return [self.decode_references(x) for x in obj]\n+        else:\n+            return obj\n+\n+\n+class ScrapyJSONEncoder(json.JSONEncoder):\n+\n+    DATE_FORMAT = \"%Y-%m-%d\"\n+    TIME_FORMAT = \"%H:%M:%S\"\n+\n+    def __init__(self, *a, **kw):\n+        self.spref = kw.pop('spref', None) or SpiderReferencer()\n+        super(ScrapyJSONEncoder, self).__init__(*a, **kw)\n+\n+    def encode(self, o):\n+        if self.spref:\n+            o = self.spref.encode_references(o)\n+        return super(ScrapyJSONEncoder, self).encode(o)\n+\n+    def default(self, o):\n+        if isinstance(o, datetime.datetime):\n+            return o.strftime(\"%s %s\" % (self.DATE_FORMAT, self.TIME_FORMAT))\n+        elif isinstance(o, datetime.date):\n+            return o.strftime(self.DATE_FORMAT)\n+        elif isinstance(o, datetime.time):\n+            return o.strftime(self.TIME_FORMAT)\n+        elif isinstance(o, decimal.Decimal):\n+            return str(o)\n+        elif isinstance(o, Request):\n+            return \"<%s %s %s>\" % (type(o).__name__, o.method, o.url)\n+        elif isinstance(o, Response):\n+            return \"<%s %s %s>\" % (type(o).__name__, o.status, o.url)\n+        else:\n+            return super(ScrapyJSONEncoder, self).default(o)\n+\n+\n+class ScrapyJSONDecoder(json.JSONDecoder):\n+\n+    def __init__(self, *a, **kw):\n+        self.spref = kw.pop('spref', None) or SpiderReferencer()\n+        super(ScrapyJSONDecoder, self).__init__(*a, **kw)\n+\n+    def decode(self, s):\n+        o = super(ScrapyJSONDecoder, self).decode(s)\n+        if self.spref:\n+            o = self.spref.decode_references(o)\n+        return o\n\n@@ -0,0 +1,86 @@\n+\"\"\"\n+Scrapy web services extension\n+\n+See docs/topics/ws.rst\n+\"\"\"\n+\n+from twisted.internet import reactor\n+from twisted.web import server, resource, error\n+\n+from scrapy.core.exceptions import NotConfigured\n+from scrapy.utils.jsonrpc import jsonrpc_server_call\n+from scrapy.utils.serialize import ScrapyJSONEncoder, ScrapyJSONDecoder\n+from scrapy.utils.misc import load_object\n+from scrapy.utils.conf import build_component_list\n+from scrapy.conf import settings\n+\n+\n+class JsonResource(resource.Resource):\n+\n+    ws_name = None\n+    json_encoder = ScrapyJSONEncoder()\n+\n+    def render(self, txrequest):\n+        r = resource.Resource.render(self, txrequest)\n+        r = self.json_encoder.encode(r)\n+        txrequest.setHeader('Content-Type', 'application/json')\n+        txrequest.setHeader('Content-Length', len(r))\n+        return r\n+\n+\n+class JsonRpcResource(JsonResource):\n+\n+    json_decoder = ScrapyJSONDecoder()\n+\n+    def __init__(self, target=None):\n+        JsonResource.__init__(self)\n+        self._target = target\n+\n+    def render_GET(self, txrequest):\n+        return self.get_target()\n+\n+    def render_POST(self, txrequest):\n+        reqstr = txrequest.content.read()\n+        target = self.get_target()\n+        return jsonrpc_server_call(target, reqstr, self.json_decoder)\n+\n+    def getChild(self, name, txrequest):\n+        target = self.get_target()\n+        try:\n+            newtarget = getattr(target, name)\n+            return JsonRpcResource(newtarget)\n+        except AttributeError:\n+            return error.NoResource(\"No such child resource.\")\n+\n+    def get_target(self):\n+        return self._target\n+\n+\n+class RootResource(JsonResource):\n+\n+    def render_GET(self, txrequest):\n+        return {'resources': self.children.keys()}\n+\n+    def getChild(self, name, txrequest):\n+        if name == '':\n+            return self\n+        return JsonResource.getChild(self, name, txrequest)\n+\n+\n+class WebService(server.Site):\n+\n+    def __init__(self):\n+        if not settings.getbool('WEBSERVICE_ENABLED'):\n+            raise NotConfigured\n+        logfile = settings['WEBSERVICE_LOGFILE']\n+        port = settings.getint('WEBSERVICE_PORT')\n+        root = RootResource()\n+        reslist = build_component_list(settings['WEBSERVICE_RESOURCES_BASE'], \\\n+            settings['WEBSERVICE_RESOURCES'])\n+        for res_cls in map(load_object, reslist):\n+            res = res_cls()\n+            root.putChild(res.ws_name, res)\n+        server.Site.__init__(self, root, logPath=logfile)\n+        self.noisy = False\n+        reactor.callWhenRunning(reactor.listenTCP, port, self)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
