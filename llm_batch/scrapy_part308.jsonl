{"custom_id": "scrapy#f9f008e935c5c892c2839d354ded3ee213057d9e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 581 | Contributors (this commit): 26 | Commits (past 90d): 12 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -33,6 +33,7 @@ install_requires = [\n     'protego>=0.1.15',\n     'itemadapter>=0.1.0',\n     'h2>=3.2.0',\n+    'typing-extensions>=3.7',\n ]\n extras_require = {}\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#38d361792c02ae2b25323258d070c04d8906495a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1526 | Contributors (this commit): 2 | Commits (past 90d): 35 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -71,8 +71,8 @@ class ScrapyH2Agent:\n \n     def __init__(\n         self, context_factory,\n-        connect_timeout=10,\n-        bind_address: Optional[bytes] = None, pool: H2ConnectionPool = None,\n+        pool: H2ConnectionPool,\n+        connect_timeout=10, bind_address: Optional[bytes] = None,\n         crawler=None\n     ) -> None:\n         self._context_factory = context_factory\n\n@@ -323,7 +323,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n             elif isinstance(event, SettingsAcknowledged):\n                 self.settings_acknowledged(event)\n             elif isinstance(event, UnknownFrameReceived):\n-                logger.debug(f'UnknownFrameReceived: frame={event.frame}')\n+                logger.debug('UnknownFrameReceived: frame={}'.format(event.frame))\n \n     # Event handler functions starts here\n     def connection_terminated(self, event: ConnectionTerminated) -> None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#75fe3d13657e11bdabb9b26b452c6cdacecffec7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1931 | Contributors (this commit): 22 | Commits (past 90d): 11 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -287,7 +287,7 @@ class HttpTestCase(unittest.TestCase):\n     def test_timeout_download_from_spider_nodata_rcvd(self):\n         # client connects but no data is received\n         spider = Spider('foo')\n-        meta = {'download_timeout': 0.2}\n+        meta = {'download_timeout': 0.5}\n         request = Request(self.getURL('wait'), meta=meta)\n         d = self.download_request(request, spider)\n         yield self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n@@ -296,7 +296,7 @@ class HttpTestCase(unittest.TestCase):\n     def test_timeout_download_from_spider_server_hangs(self):\n         # client connects, server send headers and some body bytes but hangs\n         spider = Spider('foo')\n-        meta = {'download_timeout': 0.2}\n+        meta = {'download_timeout': 0.5}\n         request = Request(self.getURL('hang-after-headers'), meta=meta)\n         d = self.download_request(request, spider)\n         yield self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2aa4f3cbf96ad55aa4a1fa064758338daf16b110", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 239 | Lines Deleted: 32 | Files Changed: 3 | Hunks: 10 | Methods Changed: 21 | Complexity Δ (Sum/Max): 20/17 | Churn Δ: 271 | Churn Cumulative: 3652 | Contributors (this commit): 31 | Commits (past 90d): 6 | Contributors (cumulative): 46 | DMM Complexity: 1.0\n\nDIFF:\n@@ -243,12 +243,17 @@ class ExecutionEngine:\n                     % (type(response), response)\n                 )\n             if isinstance(response, Response):\n-                response.request = request  # tie request to response received\n-                logkws = self.logformatter.crawled(request, response, spider)\n+                if response.request is None:\n+                    response.request = request\n+                logkws = self.logformatter.crawled(response.request, response, spider)\n                 if logkws is not None:\n                     logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n-                self.signals.send_catch_log(signals.response_received,\n-                                            response=response, request=request, spider=spider)\n+                self.signals.send_catch_log(\n+                    signal=signals.response_received,\n+                    response=response,\n+                    request=response.request,\n+                    spider=spider,\n+                )\n             return response\n \n         def _on_complete(_):\n\n@@ -12,7 +12,7 @@ from scrapy import signals\n from scrapy.core.spidermw import SpiderMiddlewareManager\n from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy.http import Request, Response\n-from scrapy.utils.defer import defer_result, defer_succeed, iter_errback, parallel\n+from scrapy.utils.defer import defer_fail, defer_succeed, iter_errback, parallel\n from scrapy.utils.log import failure_to_exc_info, logformatter_adapter\n from scrapy.utils.misc import load_object, warn_on_generator_with_return_value\n from scrapy.utils.spider import iterate_spider_output\n@@ -120,40 +120,40 @@ class Scraper:\n             response, request, deferred = slot.next_response_request_deferred()\n             self._scrape(response, request, spider).chainDeferred(deferred)\n \n-    def _scrape(self, response, request, spider):\n-        \"\"\"Handle the downloaded response or failure through the spider\n-        callback/errback\"\"\"\n-        if not isinstance(response, (Response, Failure)):\n-            raise TypeError(\n-                \"Incorrect type: expected Response or Failure, got %s: %r\"\n-                % (type(response), response)\n-            )\n-\n-        dfd = self._scrape2(response, request, spider)  # returns spider's processed output\n-        dfd.addErrback(self.handle_spider_error, request, response, spider)\n-        dfd.addCallback(self.handle_spider_output, request, response, spider)\n+    def _scrape(self, result, request, spider):\n+        \"\"\"\n+        Handle the downloaded response or failure through the spider callback/errback\n+        \"\"\"\n+        if not isinstance(result, (Response, Failure)):\n+            raise TypeError(\"Incorrect type: expected Response or Failure, got %s: %r\" % (type(result), result))\n+        dfd = self._scrape2(result, request, spider)  # returns spider's processed output\n+        dfd.addErrback(self.handle_spider_error, request, result, spider)\n+        dfd.addCallback(self.handle_spider_output, request, result, spider)\n         return dfd\n \n-    def _scrape2(self, request_result, request, spider):\n-        \"\"\"Handle the different cases of request's result been a Response or a\n-        Failure\"\"\"\n-        if not isinstance(request_result, Failure):\n-            return self.spidermw.scrape_response(\n-                self.call_spider, request_result, request, spider)\n-        else:\n-            dfd = self.call_spider(request_result, request, spider)\n-            return dfd.addErrback(\n-                self._log_download_errors, request_result, request, spider)\n+    def _scrape2(self, result, request, spider):\n+        \"\"\"\n+        Handle the different cases of request's result been a Response or a Failure\n+        \"\"\"\n+        if isinstance(result, Response):\n+            return self.spidermw.scrape_response(self.call_spider, result, request, spider)\n+        else:  # result is a Failure\n+            dfd = self.call_spider(result, request, spider)\n+            return dfd.addErrback(self._log_download_errors, result, request, spider)\n \n     def call_spider(self, result, request, spider):\n+        if isinstance(result, Response):\n+            if getattr(result, \"request\", None) is None:\n                 result.request = request\n-        dfd = defer_result(result)\n-        callback = request.callback or spider._parse\n+            callback = result.request.callback or spider._parse\n             warn_on_generator_with_return_value(spider, callback)\n+            dfd = defer_succeed(result)\n+            dfd.addCallback(callback, **result.request.cb_kwargs)\n+        else:  # result is a Failure\n+            result.request = request\n             warn_on_generator_with_return_value(spider, request.errback)\n-        dfd.addCallbacks(callback=callback,\n-                         errback=request.errback,\n-                         callbackKeywords=request.cb_kwargs)\n+            dfd = defer_fail(result)\n+            dfd.addErrback(request.errback)\n         return dfd.addCallback(iterate_spider_output)\n \n     def handle_spider_error(self, _failure, request, response, spider):\n\n@@ -0,0 +1,202 @@\n+from twisted.internet import defer\n+from twisted.trial.unittest import TestCase\n+\n+from scrapy import Request, signals\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.http.response import Response\n+\n+from testfixtures import LogCapture\n+\n+from tests.mockserver import MockServer\n+from tests.spiders import SingleRequestSpider\n+\n+\n+OVERRIDEN_URL = \"https://example.org\"\n+\n+\n+class ProcessResponseMiddleware:\n+    def process_response(self, request, response, spider):\n+        return response.replace(request=Request(OVERRIDEN_URL))\n+\n+\n+class RaiseExceptionRequestMiddleware:\n+    def process_request(self, request, spider):\n+        1 / 0\n+        return request\n+\n+\n+class CatchExceptionOverrideRequestMiddleware:\n+    def process_exception(self, request, exception, spider):\n+        return Response(\n+            url=\"http://localhost/\",\n+            body=b\"Caught \" + exception.__class__.__name__.encode(\"utf-8\"),\n+            request=Request(OVERRIDEN_URL),\n+        )\n+\n+\n+class CatchExceptionDoNotOverrideRequestMiddleware:\n+    def process_exception(self, request, exception, spider):\n+        return Response(\n+            url=\"http://localhost/\",\n+            body=b\"Caught \" + exception.__class__.__name__.encode(\"utf-8\"),\n+        )\n+\n+\n+class AlternativeCallbacksSpider(SingleRequestSpider):\n+    name = \"alternative_callbacks_spider\"\n+\n+    def alt_callback(self, response, foo=None):\n+        self.logger.info(\"alt_callback was invoked with foo=%s\", foo)\n+\n+\n+class AlternativeCallbacksMiddleware:\n+    def process_response(self, request, response, spider):\n+        new_request = request.replace(\n+            url=OVERRIDEN_URL,\n+            callback=spider.alt_callback,\n+            cb_kwargs={\"foo\": \"bar\"},\n+        )\n+        return response.replace(request=new_request)\n+\n+\n+class CrawlTestCase(TestCase):\n+\n+    def setUp(self):\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n+\n+    def tearDown(self):\n+        self.mockserver.__exit__(None, None, None)\n+\n+    @defer.inlineCallbacks\n+    def test_response_200(self):\n+        url = self.mockserver.url(\"/status?n=200\")\n+        crawler = CrawlerRunner().create_crawler(SingleRequestSpider)\n+        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n+        response = crawler.spider.meta[\"responses\"][0]\n+        self.assertEqual(response.request.url, url)\n+\n+    @defer.inlineCallbacks\n+    def test_response_error(self):\n+        for status in (\"404\", \"500\"):\n+            url = self.mockserver.url(\"/status?n={}\".format(status))\n+            crawler = CrawlerRunner().create_crawler(SingleRequestSpider)\n+            yield crawler.crawl(seed=url, mockserver=self.mockserver)\n+            failure = crawler.spider.meta[\"failure\"]\n+            response = failure.value.response\n+            self.assertEqual(failure.request.url, url)\n+            self.assertEqual(response.request.url, url)\n+\n+    @defer.inlineCallbacks\n+    def test_downloader_middleware_raise_exception(self):\n+        url = self.mockserver.url(\"/status?n=200\")\n+        runner = CrawlerRunner(settings={\n+            \"DOWNLOADER_MIDDLEWARES\": {\n+                __name__ + \".RaiseExceptionRequestMiddleware\": 590,\n+            },\n+        })\n+        crawler = runner.create_crawler(SingleRequestSpider)\n+        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n+        failure = crawler.spider.meta[\"failure\"]\n+        self.assertEqual(failure.request.url, url)\n+        self.assertIsInstance(failure.value, ZeroDivisionError)\n+\n+    @defer.inlineCallbacks\n+    def test_downloader_middleware_override_request_in_process_response(self):\n+        \"\"\"\n+        Downloader middleware which returns a response with an specific 'request' attribute.\n+\n+        * The spider callback should receive the overriden response.request\n+        * Handlers listening to the response_received signal should receive the overriden response.request\n+        * The \"crawled\" log message should show the overriden response.request\n+        \"\"\"\n+        signal_params = {}\n+\n+        def signal_handler(response, request, spider):\n+            signal_params[\"response\"] = response\n+            signal_params[\"request\"] = request\n+\n+        url = self.mockserver.url(\"/status?n=200\")\n+        runner = CrawlerRunner(settings={\n+            \"DOWNLOADER_MIDDLEWARES\": {\n+                __name__ + \".ProcessResponseMiddleware\": 595,\n+            }\n+        })\n+        crawler = runner.create_crawler(SingleRequestSpider)\n+        crawler.signals.connect(signal_handler, signal=signals.response_received)\n+\n+        with LogCapture() as log:\n+            yield crawler.crawl(seed=url, mockserver=self.mockserver)\n+\n+        response = crawler.spider.meta[\"responses\"][0]\n+        self.assertEqual(response.request.url, OVERRIDEN_URL)\n+\n+        self.assertEqual(signal_params[\"response\"].url, url)\n+        self.assertEqual(signal_params[\"request\"].url, OVERRIDEN_URL)\n+\n+        log.check_present(\n+            (\"scrapy.core.engine\", \"DEBUG\", \"Crawled (200) <GET {}> (referer: None)\".format(OVERRIDEN_URL)),\n+        )\n+\n+    @defer.inlineCallbacks\n+    def test_downloader_middleware_override_in_process_exception(self):\n+        \"\"\"\n+        An exception is raised but caught by the next middleware, which\n+        returns a Response with a specific 'request' attribute.\n+\n+        The spider callback should receive the overriden response.request\n+        \"\"\"\n+        url = self.mockserver.url(\"/status?n=200\")\n+        runner = CrawlerRunner(settings={\n+            \"DOWNLOADER_MIDDLEWARES\": {\n+                __name__ + \".RaiseExceptionRequestMiddleware\": 590,\n+                __name__ + \".CatchExceptionOverrideRequestMiddleware\": 595,\n+            },\n+        })\n+        crawler = runner.create_crawler(SingleRequestSpider)\n+        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n+        response = crawler.spider.meta[\"responses\"][0]\n+        self.assertEqual(response.body, b\"Caught ZeroDivisionError\")\n+        self.assertEqual(response.request.url, OVERRIDEN_URL)\n+\n+    @defer.inlineCallbacks\n+    def test_downloader_middleware_do_not_override_in_process_exception(self):\n+        \"\"\"\n+        An exception is raised but caught by the next middleware, which\n+        returns a Response without a specific 'request' attribute.\n+\n+        The spider callback should receive the original response.request\n+        \"\"\"\n+        url = self.mockserver.url(\"/status?n=200\")\n+        runner = CrawlerRunner(settings={\n+            \"DOWNLOADER_MIDDLEWARES\": {\n+                __name__ + \".RaiseExceptionRequestMiddleware\": 590,\n+                __name__ + \".CatchExceptionDoNotOverrideRequestMiddleware\": 595,\n+            },\n+        })\n+        crawler = runner.create_crawler(SingleRequestSpider)\n+        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n+        response = crawler.spider.meta[\"responses\"][0]\n+        self.assertEqual(response.body, b\"Caught ZeroDivisionError\")\n+        self.assertEqual(response.request.url, url)\n+\n+    @defer.inlineCallbacks\n+    def test_downloader_middleware_alternative_callback(self):\n+        \"\"\"\n+        Downloader middleware which returns a response with a\n+        specific 'request' attribute, with an alternative callback\n+        \"\"\"\n+        runner = CrawlerRunner(settings={\n+            \"DOWNLOADER_MIDDLEWARES\": {\n+                __name__ + \".AlternativeCallbacksMiddleware\": 595,\n+            }\n+        })\n+        crawler = runner.create_crawler(AlternativeCallbacksSpider)\n+\n+        with LogCapture() as log:\n+            url = self.mockserver.url(\"/status?n=200\")\n+            yield crawler.crawl(seed=url, mockserver=self.mockserver)\n+\n+        log.check_present(\n+            (\"alternative_callbacks_spider\", \"INFO\", \"alt_callback was invoked with foo=bar\"),\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a8e08d51cd50e8f0b58a60992d310e56d4f71641", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 8/5 | Churn Δ: 106 | Churn Cumulative: 1356 | Contributors (this commit): 23 | Commits (past 90d): 14 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -66,16 +66,9 @@ class Command(ScrapyCommand):\n             print(\"Cannot create a spider with the same name as your project\")\n             return\n \n-        try:\n-            spidercls = self.crawler_process.spider_loader.load(name)\n-        except KeyError:\n-            pass\n-        else:\n-            # if spider already exists and not --force then halt\n-            if not opts.force:\n-                print(\"Spider %r already exists in module:\" % name)\n-                print(\"  %s\" % spidercls.__module__)\n+        if not opts.force and self._spider_exists(name):\n             return\n+\n         template_file = self._find_template(opts.template)\n         if template_file:\n             self._genspider(module, name, domain, opts.template, template_file)\n@@ -119,6 +112,34 @@ class Command(ScrapyCommand):\n             if filename.endswith('.tmpl'):\n                 print(\"  %s\" % splitext(filename)[0])\n \n+    def _spider_exists(self, name):\n+        if not self.settings.get('NEWSPIDER_MODULE'):\n+            # if run as a standalone command and file with same filename already exists\n+            if exists(name + \".py\"):\n+                print(\"%s already exists\" % (abspath(name + \".py\")))\n+                return True\n+            return False\n+\n+        try:\n+            spidercls = self.crawler_process.spider_loader.load(name)\n+        except KeyError:\n+            pass\n+        else:\n+            # if spider with same name exists\n+            print(\"Spider %r already exists in module:\" % name)\n+            print(\"  %s\" % spidercls.__module__)\n+            return True\n+\n+        # a file with the same name exists in the target directory\n+        spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])\n+        spiders_dir = dirname(spiders_module.__file__)\n+        spiders_dir_abs = abspath(spiders_dir)\n+        if exists(join(spiders_dir_abs, name + \".py\")):\n+            print(\"%s already exists\" % (join(spiders_dir_abs, (name + \".py\"))))\n+            return True\n+\n+        return False\n+\n     @property\n     def templates_dir(self):\n         return join(\n\n@@ -8,7 +8,7 @@ import sys\n import tempfile\n from contextlib import contextmanager\n from itertools import chain\n-from os.path import exists, join, abspath\n+from os.path import exists, join, abspath, getmtime\n from pathlib import Path\n from shutil import rmtree, copytree\n from stat import S_IWRITE as ANYONE_WRITE_PERMISSION\n@@ -337,8 +337,11 @@ class GenspiderCommandTest(CommandTest):\n         p, out, err = self.proc('genspider', spname, 'test.com', *args)\n         self.assertIn(\"Created spider %r using template %r in module\" % (spname, tplname), out)\n         self.assertTrue(exists(join(self.proj_mod_path, 'spiders', 'test_spider.py')))\n+        modify_time_before = getmtime(join(self.proj_mod_path, 'spiders', 'test_spider.py'))\n         p, out, err = self.proc('genspider', spname, 'test.com', *args)\n         self.assertIn(\"Spider %r already exists in module\" % spname, out)\n+        modify_time_after = getmtime(join(self.proj_mod_path, 'spiders', 'test_spider.py'))\n+        self.assertEqual(modify_time_after, modify_time_before)\n \n     def test_template_basic(self):\n         self.test_template('basic')\n@@ -360,6 +363,40 @@ class GenspiderCommandTest(CommandTest):\n         self.assertEqual(2, self.call('genspider', self.project_name))\n         assert not exists(join(self.proj_mod_path, 'spiders', '%s.py' % self.project_name))\n \n+    def test_same_filename_as_existing_spider(self, force=False):\n+        file_name = 'example'\n+        file_path = join(self.proj_mod_path, 'spiders', '%s.py' % file_name)\n+        self.assertEqual(0, self.call('genspider', file_name, 'example.com'))\n+        assert exists(file_path)\n+\n+        # change name of spider but not its file name\n+        with open(file_path, 'r+') as spider_file:\n+            file_data = spider_file.read()\n+            file_data = file_data.replace(\"name = \\'example\\'\", \"name = \\'renamed\\'\")\n+            spider_file.seek(0)\n+            spider_file.write(file_data)\n+            spider_file.truncate()\n+        modify_time_before = getmtime(file_path)\n+        file_contents_before = file_data\n+\n+        if force:\n+            p, out, err = self.proc('genspider', '--force', file_name, 'example.com')\n+            self.assertIn(\"Created spider %r using template \\'basic\\' in module\" % file_name, out)\n+            modify_time_after = getmtime(file_path)\n+            self.assertNotEqual(modify_time_after, modify_time_before)\n+            file_contents_after = open(file_path, 'r').read()\n+            self.assertNotEqual(file_contents_after, file_contents_before)\n+        else:\n+            p, out, err = self.proc('genspider', file_name, 'example.com')\n+            self.assertIn(\"%s already exists\" % (file_path), out)\n+            modify_time_after = getmtime(file_path)\n+            self.assertEqual(modify_time_after, modify_time_before)\n+            file_contents_after = open(file_path, 'r').read()\n+            self.assertEqual(file_contents_after, file_contents_before)\n+\n+    def test_same_filename_as_existing_spider_force(self):\n+        self.test_same_filename_as_existing_spider(force=True)\n+\n \n class GenspiderStandaloneCommandTest(ProjectTest):\n \n@@ -367,6 +404,34 @@ class GenspiderStandaloneCommandTest(ProjectTest):\n         self.call('genspider', 'example', 'example.com')\n         assert exists(join(self.temp_path, 'example.py'))\n \n+    def test_same_name_as_existing_file(self, force=False):\n+        file_name = 'example'\n+        file_path = join(self.temp_path, file_name + '.py')\n+        p, out, err = self.proc('genspider', file_name, 'example.com')\n+        self.assertIn(\"Created spider %r using template \\'basic\\' \" % file_name, out)\n+        assert exists(file_path)\n+        modify_time_before = getmtime(file_path)\n+        file_contents_before = open(file_path, 'r').read()\n+\n+        if force:\n+            # use different template to ensure contents were changed\n+            p, out, err = self.proc('genspider', '--force', '-t', 'crawl', file_name, 'example.com')\n+            self.assertIn(\"Created spider %r using template \\'crawl\\' \" % file_name, out)\n+            modify_time_after = getmtime(file_path)\n+            self.assertNotEqual(modify_time_after, modify_time_before)\n+            file_contents_after = open(file_path, 'r').read()\n+            self.assertNotEqual(file_contents_after, file_contents_before)\n+        else:\n+            p, out, err = self.proc('genspider', file_name, 'example.com')\n+            self.assertIn(\"%s already exists\" % join(self.temp_path, file_name + \".py\"), out)\n+            modify_time_after = getmtime(file_path)\n+            self.assertEqual(modify_time_after, modify_time_before)\n+            file_contents_after = open(file_path, 'r').read()\n+            self.assertEqual(file_contents_after, file_contents_before)\n+\n+    def test_same_name_as_existing_file_force(self):\n+        self.test_same_name_as_existing_file(force=True)\n+\n \n class MiscCommandsTest(CommandTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#55edf8d3b8885541cdbf9d1c62d9a6bbf634e2a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 17 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 41 | Churn Cumulative: 106 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -1,4 +1,5 @@\n from email.utils import formatdate\n+from typing import Optional, Type, TypeVar\n \n from twisted.internet import defer\n from twisted.internet.error import (\n@@ -13,10 +14,19 @@ from twisted.internet.error import (\n from twisted.web.client import ResponseFailed\n \n from scrapy import signals\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n+from scrapy.http.request import Request\n+from scrapy.http.response import Response\n+from scrapy.settings import Settings\n+from scrapy.spiders import Spider\n+from scrapy.statscollectors import StatsCollector\n from scrapy.utils.misc import load_object\n \n \n+HttpCacheMiddlewareTV = TypeVar(\"HttpCacheMiddlewareTV\", bound=\"HttpCacheMiddleware\")\n+\n+\n class HttpCacheMiddleware:\n \n     DOWNLOAD_EXCEPTIONS = (defer.TimeoutError, TimeoutError, DNSLookupError,\n@@ -24,7 +34,7 @@ class HttpCacheMiddleware:\n                            ConnectionLost, TCPTimedOutError, ResponseFailed,\n                            IOError)\n \n-    def __init__(self, settings, stats):\n+    def __init__(self, settings: Settings, stats: StatsCollector) -> None:\n         if not settings.getbool('HTTPCACHE_ENABLED'):\n             raise NotConfigured\n         self.policy = load_object(settings['HTTPCACHE_POLICY'])(settings)\n@@ -33,26 +43,26 @@ class HttpCacheMiddleware:\n         self.stats = stats\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls: Type[HttpCacheMiddlewareTV], crawler: Crawler) -> HttpCacheMiddlewareTV:\n         o = cls(crawler.settings, crawler.stats)\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n         return o\n \n-    def spider_opened(self, spider):\n+    def spider_opened(self, spider: Spider) -> None:\n         self.storage.open_spider(spider)\n \n-    def spider_closed(self, spider):\n+    def spider_closed(self, spider: Spider) -> None:\n         self.storage.close_spider(spider)\n \n-    def process_request(self, request, spider):\n+    def process_request(self, request: Request, spider: Spider) -> Optional[Response]:\n         if request.meta.get('dont_cache', False):\n-            return\n+            return None\n \n         # Skip uncacheable requests\n         if not self.policy.should_cache_request(request):\n             request.meta['_dont_cache'] = True  # flag as uncacheable\n-            return\n+            return None\n \n         # Look for cached response and check if expired\n         cachedresponse = self.storage.retrieve_response(spider, request)\n@@ -61,7 +71,7 @@ class HttpCacheMiddleware:\n             if self.ignore_missing:\n                 self.stats.inc_value('httpcache/ignore', spider=spider)\n                 raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n-            return  # first time request\n+            return None  # first time request\n \n         # Return cached response only if not expired\n         cachedresponse.flags.append('cached')\n@@ -73,7 +83,9 @@ class HttpCacheMiddleware:\n         # process_response hook\n         request.meta['cached_response'] = cachedresponse\n \n-    def process_response(self, request, response, spider):\n+        return None\n+\n+    def process_response(self, request: Request, response: Response, spider: Spider) -> Response:\n         if request.meta.get('dont_cache', False):\n             return response\n \n@@ -85,7 +97,7 @@ class HttpCacheMiddleware:\n         # RFC2616 requires origin server to set Date header,\n         # https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18\n         if 'Date' not in response.headers:\n-            response.headers['Date'] = formatdate(usegmt=1)\n+            response.headers['Date'] = formatdate(usegmt=True)\n \n         # Do not validate first-hand responses\n         cachedresponse = request.meta.pop('cached_response', None)\n@@ -102,13 +114,18 @@ class HttpCacheMiddleware:\n         self._cache_response(spider, response, request, cachedresponse)\n         return response\n \n-    def process_exception(self, request, exception, spider):\n+    def process_exception(\n+        self, request: Request, exception: Exception, spider: Spider\n+    ) -> Optional[Response]:\n         cachedresponse = request.meta.pop('cached_response', None)\n         if cachedresponse is not None and isinstance(exception, self.DOWNLOAD_EXCEPTIONS):\n             self.stats.inc_value('httpcache/errorrecovery', spider=spider)\n             return cachedresponse\n+        return None\n \n-    def _cache_response(self, spider, response, request, cachedresponse):\n+    def _cache_response(\n+        self, spider: Spider, response: Response, request: Request, cachedresponse: Optional[Response]\n+    ) -> None:\n         if self.policy.should_cache_response(response, request):\n             self.stats.inc_value('httpcache/store', spider=spider)\n             self.storage.store_response(spider, request, response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e70975f0bb4e8378bf0e00ae7a7014247cd59441", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 737 | Lines Deleted: 111 | Files Changed: 11 | Hunks: 77 | Methods Changed: 78 | Complexity Δ (Sum/Max): 82/34 | Churn Δ: 848 | Churn Cumulative: 7848 | Contributors (this commit): 56 | Commits (past 90d): 65 | Contributors (cumulative): 144 | DMM Complexity: 0.9235412474849095\n\nDIFF:\n@@ -115,9 +115,11 @@ class BaseRunSpiderCommand(ScrapyCommand):\n         parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n                           help=\"set spider argument (may be repeated)\")\n         parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n-                          help=\"dump scraped items into FILE (use - for stdout)\")\n+                          help=\"append scraped items to the end of FILE (use - for stdout)\")\n+        parser.add_option(\"-O\", \"--overwrite-output\", metavar=\"FILE\", action=\"append\",\n+                          help=\"dump scraped items into FILE, overwriting any existing file\")\n         parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n-                          help=\"format to use for dumping items with -o\")\n+                          help=\"format to use for dumping items\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -125,6 +127,11 @@ class BaseRunSpiderCommand(ScrapyCommand):\n             opts.spargs = arglist_to_dict(opts.spargs)\n         except ValueError:\n             raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n-        if opts.output:\n-            feeds = feed_process_params_from_cli(self.settings, opts.output, opts.output_format)\n+        if opts.output or opts.overwrite_output:\n+            feeds = feed_process_params_from_cli(\n+                self.settings,\n+                opts.output,\n+                opts.output_format,\n+                opts.overwrite_output,\n+            )\n             self.settings.set('FEEDS', feeds, priority='cmdline')\n\n@@ -24,17 +24,34 @@ from scrapy.utils.conf import feed_complete_default_values_from_settings\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import create_instance, load_object\n-from scrapy.utils.python import without_none_values\n+from scrapy.utils.python import get_func_args, without_none_values\n \n \n logger = logging.getLogger(__name__)\n \n \n+def build_storage(builder, uri, *args, feed_options=None, preargs=(), **kwargs):\n+    argument_names = get_func_args(builder)\n+    if 'feed_options' in argument_names:\n+        kwargs['feed_options'] = feed_options\n+    else:\n+        warnings.warn(\n+            \"{} does not support the 'feed_options' keyword argument. Add a \"\n+            \"'feed_options' parameter to its signature to remove this \"\n+            \"warning. This parameter will become mandatory in a future \"\n+            \"version of Scrapy.\"\n+            .format(builder.__qualname__),\n+            category=ScrapyDeprecationWarning\n+        )\n+    return builder(*preargs, uri, *args, **kwargs)\n+\n+\n class IFeedStorage(Interface):\n     \"\"\"Interface that all Feed Storages must implement\"\"\"\n \n-    def __init__(uri):\n-        \"\"\"Initialize the storage with the parameters given in the URI\"\"\"\n+    def __init__(uri, *, feed_options=None):\n+        \"\"\"Initialize the storage with the parameters given in the URI and the\n+        feed-specific options (see :setting:`FEEDS`)\"\"\"\n \n     def open(spider):\n         \"\"\"Open the storage for the given spider. It must return a file-like\n@@ -64,10 +81,15 @@ class BlockingFeedStorage:\n @implementer(IFeedStorage)\n class StdoutFeedStorage:\n \n-    def __init__(self, uri, _stdout=None):\n+    def __init__(self, uri, _stdout=None, *, feed_options=None):\n         if not _stdout:\n             _stdout = sys.stdout.buffer\n         self._stdout = _stdout\n+        if feed_options and feed_options.get('overwrite', False) is True:\n+            logger.warning('Standard output (stdout) storage does not support '\n+                           'overwriting. To suppress this warning, remove the '\n+                           'overwrite option from your FEEDS setting, or set '\n+                           'it to False.')\n \n     def open(self, spider):\n         return self._stdout\n@@ -79,14 +101,16 @@ class StdoutFeedStorage:\n @implementer(IFeedStorage)\n class FileFeedStorage:\n \n-    def __init__(self, uri):\n+    def __init__(self, uri, *, feed_options=None):\n         self.path = file_uri_to_path(uri)\n+        feed_options = feed_options or {}\n+        self.write_mode = 'wb' if feed_options.get('overwrite', False) else 'ab'\n \n     def open(self, spider):\n         dirname = os.path.dirname(self.path)\n         if dirname and not os.path.exists(dirname):\n             os.makedirs(dirname)\n-        return open(self.path, 'ab')\n+        return open(self.path, self.write_mode)\n \n     def store(self, file):\n         file.close()\n@@ -94,7 +118,8 @@ class FileFeedStorage:\n \n class S3FeedStorage(BlockingFeedStorage):\n \n-    def __init__(self, uri, access_key=None, secret_key=None, acl=None):\n+    def __init__(self, uri, access_key=None, secret_key=None, acl=None, *,\n+                 feed_options=None):\n         u = urlparse(uri)\n         self.bucketname = u.hostname\n         self.access_key = u.username or access_key\n@@ -111,14 +136,20 @@ class S3FeedStorage(BlockingFeedStorage):\n         else:\n             import boto\n             self.connect_s3 = boto.connect_s3\n+        if feed_options and feed_options.get('overwrite', True) is False:\n+            logger.warning('S3 does not support appending to files. To '\n+                           'suppress this warning, remove the overwrite '\n+                           'option from your FEEDS setting or set it to True.')\n \n     @classmethod\n-    def from_crawler(cls, crawler, uri):\n-        return cls(\n-            uri=uri,\n+    def from_crawler(cls, crawler, uri, *, feed_options=None):\n+        return build_storage(\n+            cls,\n+            uri,\n             access_key=crawler.settings['AWS_ACCESS_KEY_ID'],\n             secret_key=crawler.settings['AWS_SECRET_ACCESS_KEY'],\n-            acl=crawler.settings['FEED_STORAGE_S3_ACL'] or None\n+            acl=crawler.settings['FEED_STORAGE_S3_ACL'] or None,\n+            feed_options=feed_options,\n         )\n \n     def _store_in_thread(self, file):\n@@ -135,6 +166,7 @@ class S3FeedStorage(BlockingFeedStorage):\n             kwargs = {'policy': self.acl} if self.acl else {}\n             key.set_contents_from_file(file, **kwargs)\n             key.close()\n+        file.close()\n \n \n class GCSFeedStorage(BlockingFeedStorage):\n@@ -165,27 +197,31 @@ class GCSFeedStorage(BlockingFeedStorage):\n \n class FTPFeedStorage(BlockingFeedStorage):\n \n-    def __init__(self, uri, use_active_mode=False):\n+    def __init__(self, uri, use_active_mode=False, *, feed_options=None):\n         u = urlparse(uri)\n         self.host = u.hostname\n         self.port = int(u.port or '21')\n         self.username = u.username\n-        self.password = unquote(u.password)\n+        self.password = unquote(u.password or '')\n         self.path = u.path\n         self.use_active_mode = use_active_mode\n+        self.overwrite = not feed_options or feed_options.get('overwrite', True)\n \n     @classmethod\n-    def from_crawler(cls, crawler, uri):\n-        return cls(\n-            uri=uri,\n-            use_active_mode=crawler.settings.getbool('FEED_STORAGE_FTP_ACTIVE')\n+    def from_crawler(cls, crawler, uri, *, feed_options=None):\n+        return build_storage(\n+            cls,\n+            uri,\n+            crawler.settings.getbool('FEED_STORAGE_FTP_ACTIVE'),\n+            feed_options=feed_options,\n         )\n \n     def _store_in_thread(self, file):\n         ftp_store_file(\n             path=self.path, file=file, host=self.host,\n             port=self.port, username=self.username,\n-            password=self.password, use_active_mode=self.use_active_mode\n+            password=self.password, use_active_mode=self.use_active_mode,\n+            overwrite=self.overwrite,\n         )\n \n \n@@ -242,32 +278,32 @@ class FeedExporter:\n                 category=ScrapyDeprecationWarning, stacklevel=2,\n             )\n             uri = str(self.settings['FEED_URI'])  # handle pathlib.Path objects\n-            feed = {'format': self.settings.get('FEED_FORMAT', 'jsonlines')}\n-            self.feeds[uri] = feed_complete_default_values_from_settings(feed, self.settings)\n+            feed_options = {'format': self.settings.get('FEED_FORMAT', 'jsonlines')}\n+            self.feeds[uri] = feed_complete_default_values_from_settings(feed_options, self.settings)\n         # End: Backward compatibility for FEED_URI and FEED_FORMAT settings\n \n         # 'FEEDS' setting takes precedence over 'FEED_URI'\n-        for uri, feed in self.settings.getdict('FEEDS').items():\n+        for uri, feed_options in self.settings.getdict('FEEDS').items():\n             uri = str(uri)  # handle pathlib.Path objects\n-            self.feeds[uri] = feed_complete_default_values_from_settings(feed, self.settings)\n+            self.feeds[uri] = feed_complete_default_values_from_settings(feed_options, self.settings)\n \n         self.storages = self._load_components('FEED_STORAGES')\n         self.exporters = self._load_components('FEED_EXPORTERS')\n-        for uri, feed in self.feeds.items():\n-            if not self._storage_supported(uri):\n+        for uri, feed_options in self.feeds.items():\n+            if not self._storage_supported(uri, feed_options):\n                 raise NotConfigured\n             if not self._settings_are_valid():\n                 raise NotConfigured\n-            if not self._exporter_supported(feed['format']):\n+            if not self._exporter_supported(feed_options['format']):\n                 raise NotConfigured\n \n     def open_spider(self, spider):\n-        for uri, feed in self.feeds.items():\n-            uri_params = self._get_uri_params(spider, feed['uri_params'])\n+        for uri, feed_options in self.feeds.items():\n+            uri_params = self._get_uri_params(spider, feed_options['uri_params'])\n             self.slots.append(self._start_new_batch(\n                 batch_id=1,\n                 uri=uri % uri_params,\n-                feed=feed,\n+                feed_options=feed_options,\n                 spider=spider,\n                 uri_template=uri,\n             ))\n@@ -306,32 +342,32 @@ class FeedExporter:\n         )\n         return d\n \n-    def _start_new_batch(self, batch_id, uri, feed, spider, uri_template):\n+    def _start_new_batch(self, batch_id, uri, feed_options, spider, uri_template):\n         \"\"\"\n         Redirect the output data stream to a new file.\n         Execute multiple times if FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified\n         :param batch_id: sequence number of current batch\n         :param uri: uri of the new batch to start\n-        :param feed: dict with parameters of feed\n+        :param feed_options: dict with parameters of feed\n         :param spider: user spider\n         :param uri_template: template of uri which contains %(batch_time)s or %(batch_id)d to create new uri\n         \"\"\"\n-        storage = self._get_storage(uri)\n+        storage = self._get_storage(uri, feed_options)\n         file = storage.open(spider)\n         exporter = self._get_exporter(\n             file=file,\n-            format=feed['format'],\n-            fields_to_export=feed['fields'],\n-            encoding=feed['encoding'],\n-            indent=feed['indent'],\n+            format=feed_options['format'],\n+            fields_to_export=feed_options['fields'],\n+            encoding=feed_options['encoding'],\n+            indent=feed_options['indent'],\n         )\n         slot = _FeedSlot(\n             file=file,\n             exporter=exporter,\n             storage=storage,\n             uri=uri,\n-            format=feed['format'],\n-            store_empty=feed['store_empty'],\n+            format=feed_options['format'],\n+            store_empty=feed_options['store_empty'],\n             batch_id=batch_id,\n             uri_template=uri_template,\n         )\n@@ -355,7 +391,7 @@ class FeedExporter:\n                 slots.append(self._start_new_batch(\n                     batch_id=slot.batch_id + 1,\n                     uri=slot.uri_template % uri_params,\n-                    feed=self.feeds[slot.uri_template],\n+                    feed_options=self.feeds[slot.uri_template],\n                     spider=spider,\n                     uri_template=slot.uri_template,\n                 ))\n@@ -394,11 +430,11 @@ class FeedExporter:\n                 return False\n         return True\n \n-    def _storage_supported(self, uri):\n+    def _storage_supported(self, uri, feed_options):\n         scheme = urlparse(uri).scheme\n         if scheme in self.storages:\n             try:\n-                self._get_storage(uri)\n+                self._get_storage(uri, feed_options)\n                 return True\n             except NotConfigured as e:\n                 logger.error(\"Disabled feed storage scheme: %(scheme)s. \"\n@@ -416,8 +452,30 @@ class FeedExporter:\n     def _get_exporter(self, file, format, *args, **kwargs):\n         return self._get_instance(self.exporters[format], file, *args, **kwargs)\n \n-    def _get_storage(self, uri):\n-        return self._get_instance(self.storages[urlparse(uri).scheme], uri)\n+    def _get_storage(self, uri, feed_options):\n+        \"\"\"Fork of create_instance specific to feed storage classes\n+\n+        It supports not passing the *feed_options* parameters to classes that\n+        do not support it, and issuing a deprecation warning instead.\n+        \"\"\"\n+        feedcls = self.storages[urlparse(uri).scheme]\n+        crawler = getattr(self, 'crawler', None)\n+\n+        def build_instance(builder, *preargs):\n+            return build_storage(builder, uri, preargs=preargs)\n+\n+        if crawler and hasattr(feedcls, 'from_crawler'):\n+            instance = build_instance(feedcls.from_crawler, crawler)\n+            method_name = 'from_crawler'\n+        elif hasattr(feedcls, 'from_settings'):\n+            instance = build_instance(feedcls.from_settings, self.settings)\n+            method_name = 'from_settings'\n+        else:\n+            instance = build_instance(feedcls)\n+            method_name = '__new__'\n+        if instance is None:\n+            raise TypeError(\"%s.%s returned None\" % (feedcls.__qualname__, method_name))\n+        return instance\n \n     def _get_uri_params(self, spider, uri_params, slot=None):\n         params = {}\n\n@@ -127,7 +127,8 @@ def feed_complete_default_values_from_settings(feed, settings):\n     return out\n \n \n-def feed_process_params_from_cli(settings, output, output_format=None):\n+def feed_process_params_from_cli(settings, output, output_format=None,\n+                                 overwrite_output=None):\n     \"\"\"\n     Receives feed export params (from the 'crawl' or 'runspider' commands),\n     checks for inconsistencies in their quantities and returns a dictionary\n@@ -139,22 +140,39 @@ def feed_process_params_from_cli(settings, output, output_format=None):\n \n     def check_valid_format(output_format):\n         if output_format not in valid_output_formats:\n-            raise UsageError(\"Unrecognized output format '%s', set one after a\"\n-                             \" colon using the -o option (i.e. -o <URI>:<FORMAT>)\"\n-                             \" or as a file extension, from the supported list %s\" %\n-                             (output_format, tuple(valid_output_formats)))\n+            raise UsageError(\n+                \"Unrecognized output format '%s'. Set a supported one (%s) \"\n+                \"after a colon at the end of the output URI (i.e. -o/-O \"\n+                \"<URI>:<FORMAT>) or as a file extension.\" % (\n+                    output_format,\n+                    tuple(valid_output_formats),\n+                )\n+            )\n+\n+    overwrite = False\n+    if overwrite_output:\n+        if output:\n+            raise UsageError(\n+                \"Please use only one of -o/--output and -O/--overwrite-output\"\n+            )\n+        output = overwrite_output\n+        overwrite = True\n \n     if output_format:\n         if len(output) == 1:\n             check_valid_format(output_format)\n-            warnings.warn('The -t command line option is deprecated in favor'\n-                          ' of specifying the output format within the -o'\n-                          ' option, please check the -o option docs for more details',\n-                          category=ScrapyDeprecationWarning, stacklevel=2)\n+            message = (\n+                'The -t command line option is deprecated in favor of '\n+                'specifying the output format within the output URI. See the '\n+                'documentation of the -o and -O options for more information.',\n+            )\n+            warnings.warn(message, ScrapyDeprecationWarning, stacklevel=2)\n             return {output[0]: {'format': output_format}}\n         else:\n-            raise UsageError('The -t command line option cannot be used if multiple'\n-                             ' output files are specified with the -o option')\n+            raise UsageError(\n+                'The -t command-line option cannot be used if multiple output '\n+                'URIs are specified'\n+            )\n \n     result = {}\n     for element in output:\n@@ -168,8 +186,10 @@ def feed_process_params_from_cli(settings, output, output_format=None):\n                 feed_uri = 'stdout:'\n         check_valid_format(feed_format)\n         result[feed_uri] = {'format': feed_format}\n+        if overwrite:\n+            result[feed_uri]['overwrite'] = True\n \n-    # FEEDS setting should take precedence over the -o and -t CLI options\n+    # FEEDS setting should take precedence over the matching CLI options\n     result.update(settings.getdict('FEEDS'))\n \n     return result\n\n@@ -20,7 +20,7 @@ def ftp_makedirs_cwd(ftp, path, first_call=True):\n \n def ftp_store_file(\n         *, path, file, host, port,\n-        username, password, use_active_mode=False):\n+        username, password, use_active_mode=False, overwrite=True):\n     \"\"\"Opens a FTP connection with passed credentials,sets current directory\n     to the directory extracted from given path, then uploads the file to server\n     \"\"\"\n@@ -32,4 +32,6 @@ def ftp_store_file(\n         file.seek(0)\n         dirname, filename = posixpath.split(path)\n         ftp_makedirs_cwd(ftp, dirname)\n-        ftp.storbinary('STOR %s' % filename, file)\n+        command = 'STOR' if overwrite else 'APPE'\n+        ftp.storbinary('%s %s' % (command, filename), file)\n+        file.close()\n\n@@ -198,7 +198,8 @@ def _getargspec_py23(func):\n def get_func_args(func, stripself=False):\n     \"\"\"Return the argument name list of a callable\"\"\"\n     if inspect.isfunction(func):\n-        func_args, _, _, _ = _getargspec_py23(func)\n+        spec = inspect.getfullargspec(func)\n+        func_args = spec.args + spec.kwonlyargs\n     elif inspect.isclass(func):\n         return get_func_args(func.__init__, True)\n     elif inspect.ismethod(func):\n\n@@ -0,0 +1,24 @@\n+from argparse import ArgumentParser\n+\n+from pyftpdlib.authorizers import DummyAuthorizer\n+from pyftpdlib.handlers import FTPHandler\n+from pyftpdlib.servers import FTPServer\n+\n+\n+def main():\n+    parser = ArgumentParser()\n+    parser.add_argument('-d', '--directory')\n+    args = parser.parse_args()\n+\n+    authorizer = DummyAuthorizer()\n+    full_permissions = 'elradfmwMT'\n+    authorizer.add_anonymous(args.directory, perm=full_permissions)\n+    handler = FTPHandler\n+    handler.authorizer = authorizer\n+    address = ('127.0.0.1', 2121)\n+    server = FTPServer(address, handler)\n+    server.serve_forever()\n+\n+\n+if __name__ == '__main__':\n+    main()\n\n@@ -3,7 +3,10 @@ import json\n import os\n import random\n import sys\n+from pathlib import Path\n+from shutil import rmtree\n from subprocess import Popen, PIPE\n+from tempfile import mkdtemp\n from urllib.parse import urlencode\n \n from OpenSSL import SSL\n@@ -256,6 +259,29 @@ class MockDNSServer:\n         self.proc.communicate()\n \n \n+class MockFTPServer:\n+    \"\"\"Creates an FTP server on port 2121 with a default passwordless user\n+    (anonymous) and a temporary root path that you can read from the\n+    :attr:`path` attribute.\"\"\"\n+\n+    def __enter__(self):\n+        self.path = Path(mkdtemp())\n+        self.proc = Popen([sys.executable, '-u', '-m', 'tests.ftpserver', '-d', str(self.path)],\n+                          stderr=PIPE, env=get_testenv())\n+        for line in self.proc.stderr:\n+            if b'starting FTP server' in line:\n+                break\n+        return self\n+\n+    def __exit__(self, exc_type, exc_value, traceback):\n+        rmtree(str(self.path))\n+        self.proc.kill()\n+        self.proc.communicate()\n+\n+    def url(self, path):\n+        return 'ftp://127.0.0.1:2121/' + path\n+\n+\n def ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.crt', cipher_string=None):\n     factory = ssl.DefaultOpenSSLContextFactory(\n         os.path.join(os.path.dirname(__file__), keyfile),\n\n@@ -74,6 +74,7 @@ class ProjectTest(unittest.TestCase):\n \n         def kill_proc():\n             p.kill()\n+            p.communicate()\n             assert False, 'Command took too much time to complete'\n \n         timer = Timer(15, kill_proc)\n@@ -569,6 +570,55 @@ class BadSpider(scrapy.Spider):\n         log = self.get_log(self.debug_log_spider, args=[])\n         self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n+    def test_output(self):\n+        spider_code = \"\"\"\n+import scrapy\n+\n+class MySpider(scrapy.Spider):\n+    name = 'myspider'\n+\n+    def start_requests(self):\n+        self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))\n+        return []\n+\"\"\"\n+        args = ['-o', 'example.json']\n+        log = self.get_log(spider_code, args=args)\n+        self.assertIn(\"[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}\", log)\n+\n+    def test_overwrite_output(self):\n+        spider_code = \"\"\"\n+import json\n+import scrapy\n+\n+class MySpider(scrapy.Spider):\n+    name = 'myspider'\n+\n+    def start_requests(self):\n+        self.logger.debug(\n+            'FEEDS: {}'.format(\n+                json.dumps(self.settings.getdict('FEEDS'), sort_keys=True)\n+            )\n+        )\n+        return []\n+\"\"\"\n+        args = ['-O', 'example.json']\n+        log = self.get_log(spider_code, args=args)\n+        self.assertIn('[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}', log)\n+\n+    def test_output_and_overwrite_output(self):\n+        spider_code = \"\"\"\n+import scrapy\n+\n+class MySpider(scrapy.Spider):\n+    name = 'myspider'\n+\n+    def start_requests(self):\n+        return []\n+\"\"\"\n+        args = ['-o', 'example1.json', '-O', 'example2.json']\n+        log = self.get_log(spider_code, args=args)\n+        self.assertIn(\"error: Please use only one of -o/--output and -O/--overwrite-output\", log)\n+\n \n class BenchCommandTest(CommandTest):\n \n@@ -577,3 +627,79 @@ class BenchCommandTest(CommandTest):\n                               '-s', 'CLOSESPIDER_TIMEOUT=0.01')\n         self.assertIn('INFO: Crawled', log)\n         self.assertNotIn('Unhandled Error', log)\n+\n+\n+class CrawlCommandTest(CommandTest):\n+\n+    def crawl(self, code, args=()):\n+        fname = abspath(join(self.proj_mod_path, 'spiders', 'myspider.py'))\n+        with open(fname, 'w') as f:\n+            f.write(code)\n+        return self.proc('crawl', 'myspider', *args)\n+\n+    def get_log(self, code, args=()):\n+        _, _, stderr = self.crawl(code, args=args)\n+        return stderr\n+\n+    def test_no_output(self):\n+        spider_code = \"\"\"\n+import scrapy\n+\n+class MySpider(scrapy.Spider):\n+    name = 'myspider'\n+\n+    def start_requests(self):\n+        self.logger.debug('It works!')\n+        return []\n+\"\"\"\n+        log = self.get_log(spider_code)\n+        self.assertIn(\"[myspider] DEBUG: It works!\", log)\n+\n+    def test_output(self):\n+        spider_code = \"\"\"\n+import scrapy\n+\n+class MySpider(scrapy.Spider):\n+    name = 'myspider'\n+\n+    def start_requests(self):\n+        self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))\n+        return []\n+\"\"\"\n+        args = ['-o', 'example.json']\n+        log = self.get_log(spider_code, args=args)\n+        self.assertIn(\"[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}\", log)\n+\n+    def test_overwrite_output(self):\n+        spider_code = \"\"\"\n+import json\n+import scrapy\n+\n+class MySpider(scrapy.Spider):\n+    name = 'myspider'\n+\n+    def start_requests(self):\n+        self.logger.debug(\n+            'FEEDS: {}'.format(\n+                json.dumps(self.settings.getdict('FEEDS'), sort_keys=True)\n+            )\n+        )\n+        return []\n+\"\"\"\n+        args = ['-O', 'example.json']\n+        log = self.get_log(spider_code, args=args)\n+        self.assertIn('[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}', log)\n+\n+    def test_output_and_overwrite_output(self):\n+        spider_code = \"\"\"\n+import scrapy\n+\n+class MySpider(scrapy.Spider):\n+    name = 'myspider'\n+\n+    def start_requests(self):\n+        return []\n+\"\"\"\n+        args = ['-o', 'example1.json', '-O', 'example2.json']\n+        log = self.get_log(spider_code, args=args)\n+        self.assertIn(\"error: Please use only one of -o/--output and -O/--overwrite-output\", log)\n\n@@ -5,6 +5,7 @@ import random\n import shutil\n import string\n import tempfile\n+import warnings\n from abc import ABC, abstractmethod\n from collections import defaultdict\n from io import BytesIO\n@@ -25,7 +26,7 @@ from zope.interface.verify import verifyObject\n \n import scrapy\n from scrapy.crawler import CrawlerRunner\n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.exporters import CsvItemExporter\n from scrapy.extensions.feedexport import (\n     BlockingFeedStorage,\n@@ -46,7 +47,7 @@ from scrapy.utils.test import (\n     mock_google_cloud_storage,\n )\n \n-from tests.mockserver import MockServer\n+from tests.mockserver import MockFTPServer, MockServer\n \n \n class FileFeedStorageTest(unittest.TestCase):\n@@ -75,8 +76,28 @@ class FileFeedStorageTest(unittest.TestCase):\n         st = FileFeedStorage(path)\n         verifyObject(IFeedStorage, st)\n \n+    def _store(self, feed_options=None):\n+        path = os.path.abspath(self.mktemp())\n+        storage = FileFeedStorage(path, feed_options=feed_options)\n+        spider = scrapy.Spider(\"default\")\n+        file = storage.open(spider)\n+        file.write(b\"content\")\n+        storage.store(file)\n+        return path\n+\n+    def test_append(self):\n+        path = self._store()\n+        return self._assert_stores(FileFeedStorage(path), path, b\"contentcontent\")\n+\n+    def test_overwrite(self):\n+        path = self._store({\"overwrite\": True})\n+        return self._assert_stores(\n+            FileFeedStorage(path, feed_options={\"overwrite\": True}),\n+            path\n+        )\n+\n     @defer.inlineCallbacks\n-    def _assert_stores(self, storage, path):\n+    def _assert_stores(self, storage, path, expected_content=b\"content\"):\n         spider = scrapy.Spider(\"default\")\n         file = storage.open(spider)\n         file.write(b\"content\")\n@@ -84,7 +105,7 @@ class FileFeedStorageTest(unittest.TestCase):\n         self.assertTrue(os.path.exists(path))\n         try:\n             with open(path, 'rb') as fp:\n-                self.assertEqual(fp.read(), b\"content\")\n+                self.assertEqual(fp.read(), expected_content)\n         finally:\n             os.unlink(path)\n \n@@ -99,49 +120,74 @@ class FTPFeedStorageTest(unittest.TestCase):\n         spider = TestSpider.from_crawler(crawler)\n         return spider\n \n-    def test_store(self):\n-        uri = os.environ.get('FEEDTEST_FTP_URI')\n-        path = os.environ.get('FEEDTEST_FTP_PATH')\n-        if not (uri and path):\n-            raise unittest.SkipTest(\"No FTP server available for testing\")\n-        st = FTPFeedStorage(uri)\n-        verifyObject(IFeedStorage, st)\n-        return self._assert_stores(st, path)\n+    def _store(self, uri, content, feed_options=None, settings=None):\n+        crawler = get_crawler(settings_dict=settings or {})\n+        storage = FTPFeedStorage.from_crawler(\n+            crawler,\n+            uri,\n+            feed_options=feed_options,\n+        )\n+        verifyObject(IFeedStorage, storage)\n+        spider = self.get_test_spider()\n+        file = storage.open(spider)\n+        file.write(content)\n+        return storage.store(file)\n \n-    def test_store_active_mode(self):\n-        uri = os.environ.get('FEEDTEST_FTP_URI')\n-        path = os.environ.get('FEEDTEST_FTP_PATH')\n-        if not (uri and path):\n-            raise unittest.SkipTest(\"No FTP server available for testing\")\n-        use_active_mode = {'FEED_STORAGE_FTP_ACTIVE': True}\n-        crawler = get_crawler(settings_dict=use_active_mode)\n-        st = FTPFeedStorage.from_crawler(crawler, uri)\n-        verifyObject(IFeedStorage, st)\n-        return self._assert_stores(st, path)\n+    def _assert_stored(self, path, content):\n+        self.assertTrue(path.exists())\n+        try:\n+            with path.open('rb') as fp:\n+                self.assertEqual(fp.read(), content)\n+        finally:\n+            os.unlink(str(path))\n+\n+    @defer.inlineCallbacks\n+    def test_append(self):\n+        with MockFTPServer() as ftp_server:\n+            filename = 'file'\n+            url = ftp_server.url(filename)\n+            feed_options = {'overwrite': False}\n+            yield self._store(url, b\"foo\", feed_options=feed_options)\n+            yield self._store(url, b\"bar\", feed_options=feed_options)\n+            self._assert_stored(ftp_server.path / filename, b\"foobar\")\n+\n+    @defer.inlineCallbacks\n+    def test_overwrite(self):\n+        with MockFTPServer() as ftp_server:\n+            filename = 'file'\n+            url = ftp_server.url(filename)\n+            yield self._store(url, b\"foo\")\n+            yield self._store(url, b\"bar\")\n+            self._assert_stored(ftp_server.path / filename, b\"bar\")\n+\n+    @defer.inlineCallbacks\n+    def test_append_active_mode(self):\n+        with MockFTPServer() as ftp_server:\n+            settings = {'FEED_STORAGE_FTP_ACTIVE': True}\n+            filename = 'file'\n+            url = ftp_server.url(filename)\n+            feed_options = {'overwrite': False}\n+            yield self._store(url, b\"foo\", feed_options=feed_options, settings=settings)\n+            yield self._store(url, b\"bar\", feed_options=feed_options, settings=settings)\n+            self._assert_stored(ftp_server.path / filename, b\"foobar\")\n+\n+    @defer.inlineCallbacks\n+    def test_overwrite_active_mode(self):\n+        with MockFTPServer() as ftp_server:\n+            settings = {'FEED_STORAGE_FTP_ACTIVE': True}\n+            filename = 'file'\n+            url = ftp_server.url(filename)\n+            yield self._store(url, b\"foo\", settings=settings)\n+            yield self._store(url, b\"bar\", settings=settings)\n+            self._assert_stored(ftp_server.path / filename, b\"bar\")\n \n     def test_uri_auth_quote(self):\n         # RFC3986: 3.2.1. User Information\n         pw_quoted = quote(string.punctuation, safe='')\n-        st = FTPFeedStorage('ftp://foo:%s@example.com/some_path' % pw_quoted)\n+        st = FTPFeedStorage('ftp://foo:%s@example.com/some_path' % pw_quoted,\n+                            {})\n         self.assertEqual(st.password, string.punctuation)\n \n-    @defer.inlineCallbacks\n-    def _assert_stores(self, storage, path):\n-        spider = self.get_test_spider()\n-        file = storage.open(spider)\n-        file.write(b\"content\")\n-        yield storage.store(file)\n-        self.assertTrue(os.path.exists(path))\n-        try:\n-            with open(path, 'rb') as fp:\n-                self.assertEqual(fp.read(), b\"content\")\n-            # again, to check s3 objects are overwritten\n-            yield storage.store(BytesIO(b\"new content\"))\n-            with open(path, 'rb') as fp:\n-                self.assertEqual(fp.read(), b\"new content\")\n-        finally:\n-            os.unlink(path)\n-\n \n class BlockingFeedStorageTest(unittest.TestCase):\n \n@@ -190,8 +236,10 @@ class S3FeedStorageTest(unittest.TestCase):\n                            'AWS_SECRET_ACCESS_KEY': 'settings_secret'}\n         crawler = get_crawler(settings_dict=aws_credentials)\n         # Instantiate with crawler\n-        storage = S3FeedStorage.from_crawler(crawler,\n-                                             's3://mybucket/export.csv')\n+        storage = S3FeedStorage.from_crawler(\n+            crawler,\n+            's3://mybucket/export.csv',\n+        )\n         self.assertEqual(storage.access_key, 'settings_key')\n         self.assertEqual(storage.secret_key, 'settings_secret')\n         # Instantiate directly\n@@ -254,7 +302,7 @@ class S3FeedStorageTest(unittest.TestCase):\n         crawler = get_crawler(settings_dict=settings)\n         storage = S3FeedStorage.from_crawler(\n             crawler,\n-            's3://mybucket/export.csv'\n+            's3://mybucket/export.csv',\n         )\n         self.assertEqual(storage.access_key, 'access_key')\n         self.assertEqual(storage.secret_key, 'secret_key')\n@@ -269,7 +317,7 @@ class S3FeedStorageTest(unittest.TestCase):\n         crawler = get_crawler(settings_dict=settings)\n         storage = S3FeedStorage.from_crawler(\n             crawler,\n-            's3://mybucket/export.csv'\n+            's3://mybucket/export.csv',\n         )\n         self.assertEqual(storage.access_key, 'access_key')\n         self.assertEqual(storage.secret_key, 'secret_key')\n@@ -370,6 +418,27 @@ class S3FeedStorageTest(unittest.TestCase):\n             key.set_contents_from_file.call_args\n         )\n \n+    def test_overwrite_default(self):\n+        with LogCapture() as log:\n+            S3FeedStorage(\n+                's3://mybucket/export.csv',\n+                'access_key',\n+                'secret_key',\n+                'custom-acl'\n+            )\n+        self.assertNotIn('S3 does not support appending to files', str(log))\n+\n+    def test_overwrite_false(self):\n+        with LogCapture() as log:\n+            S3FeedStorage(\n+                's3://mybucket/export.csv',\n+                'access_key',\n+                'secret_key',\n+                'custom-acl',\n+                feed_options={'overwrite': False},\n+            )\n+        self.assertIn('S3 does not support appending to files', str(log))\n+\n \n class GCSFeedStorageTest(unittest.TestCase):\n \n@@ -439,12 +508,22 @@ class StdoutFeedStorageTest(unittest.TestCase):\n         yield storage.store(file)\n         self.assertEqual(out.getvalue(), b\"content\")\n \n+    def test_overwrite_default(self):\n+        with LogCapture() as log:\n+            StdoutFeedStorage('stdout:')\n+        self.assertNotIn('Standard output (stdout) storage does not support overwriting', str(log))\n+\n+    def test_overwrite_true(self):\n+        with LogCapture() as log:\n+            StdoutFeedStorage('stdout:', feed_options={'overwrite': True})\n+        self.assertIn('Standard output (stdout) storage does not support overwriting', str(log))\n+\n \n class FromCrawlerMixin:\n     init_with_crawler = False\n \n     @classmethod\n-    def from_crawler(cls, crawler, *args, **kwargs):\n+    def from_crawler(cls, crawler, *args, feed_options=None, **kwargs):\n         cls.init_with_crawler = True\n         return cls(*args, **kwargs)\n \n@@ -454,7 +533,11 @@ class FromCrawlerCsvItemExporter(CsvItemExporter, FromCrawlerMixin):\n \n \n class FromCrawlerFileFeedStorage(FileFeedStorage, FromCrawlerMixin):\n-    pass\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, *args, feed_options=None, **kwargs):\n+        cls.init_with_crawler = True\n+        return cls(*args, feed_options=feed_options, **kwargs)\n \n \n class DummyBlockingFeedStorage(BlockingFeedStorage):\n@@ -588,8 +671,8 @@ class FeedExportTest(FeedExportTestBase):\n \n         FEEDS = settings.get('FEEDS') or {}\n         settings['FEEDS'] = {\n-            printf_escape(path_to_url(file_path)): feed\n-            for file_path, feed in FEEDS.items()\n+            printf_escape(path_to_url(file_path)): feed_options\n+            for file_path, feed_options in FEEDS.items()\n         }\n \n         content = {}\n@@ -599,12 +682,12 @@ class FeedExportTest(FeedExportTestBase):\n                 spider_cls.start_urls = [s.url('/')]\n                 yield runner.crawl(spider_cls)\n \n-            for file_path, feed in FEEDS.items():\n+            for file_path, feed_options in FEEDS.items():\n                 if not os.path.exists(str(file_path)):\n                     continue\n \n                 with open(str(file_path), 'rb') as f:\n-                    content[feed['format']] = f.read()\n+                    content[feed_options['format']] = f.read()\n \n         finally:\n             for file_path in FEEDS.keys():\n@@ -1542,3 +1625,262 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             content = json.loads(content.decode('utf-8'))\n             expected_batch, items = items[:batch_size], items[batch_size:]\n             self.assertEqual(expected_batch, content)\n+\n+\n+class FeedExportInitTest(unittest.TestCase):\n+\n+    def test_unsupported_storage(self):\n+        settings = {\n+            'FEEDS': {\n+                'unsupported://uri': {},\n+            },\n+        }\n+        crawler = get_crawler(settings_dict=settings)\n+        with self.assertRaises(NotConfigured):\n+            FeedExporter.from_crawler(crawler)\n+\n+    def test_unsupported_format(self):\n+        settings = {\n+            'FEEDS': {\n+                'file://path': {\n+                    'format': 'unsupported_format',\n+                },\n+            },\n+        }\n+        crawler = get_crawler(settings_dict=settings)\n+        with self.assertRaises(NotConfigured):\n+            FeedExporter.from_crawler(crawler)\n+\n+\n+class StdoutFeedStorageWithoutFeedOptions(StdoutFeedStorage):\n+\n+    def __init__(self, uri):\n+        super().__init__(uri)\n+\n+\n+class StdoutFeedStoragePreFeedOptionsTest(unittest.TestCase):\n+    \"\"\"Make sure that any feed exporter created by users before the\n+    introduction of the ``feed_options`` parameter continues to work as\n+    expected, and simply issues a warning.\"\"\"\n+\n+    def test_init(self):\n+        settings_dict = {\n+            'FEED_URI': 'file:///tmp/foobar',\n+            'FEED_STORAGES': {\n+                'file': 'tests.test_feedexport.StdoutFeedStorageWithoutFeedOptions'\n+            },\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        feed_exporter = FeedExporter.from_crawler(crawler)\n+        spider = scrapy.Spider(\"default\")\n+        with warnings.catch_warnings(record=True) as w:\n+            feed_exporter.open_spider(spider)\n+            messages = tuple(str(item.message) for item in w\n+                             if item.category is ScrapyDeprecationWarning)\n+            self.assertEqual(\n+                messages,\n+                (\n+                    (\n+                        \"StdoutFeedStorageWithoutFeedOptions does not support \"\n+                        \"the 'feed_options' keyword argument. Add a \"\n+                        \"'feed_options' parameter to its signature to remove \"\n+                        \"this warning. This parameter will become mandatory \"\n+                        \"in a future version of Scrapy.\"\n+                    ),\n+                )\n+            )\n+\n+\n+class FileFeedStorageWithoutFeedOptions(FileFeedStorage):\n+\n+    def __init__(self, uri):\n+        super().__init__(uri)\n+\n+\n+class FileFeedStoragePreFeedOptionsTest(unittest.TestCase):\n+    \"\"\"Make sure that any feed exporter created by users before the\n+    introduction of the ``feed_options`` parameter continues to work as\n+    expected, and simply issues a warning.\"\"\"\n+\n+    maxDiff = None\n+\n+    def test_init(self):\n+        settings_dict = {\n+            'FEED_URI': 'file:///tmp/foobar',\n+            'FEED_STORAGES': {\n+                'file': 'tests.test_feedexport.FileFeedStorageWithoutFeedOptions'\n+            },\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        feed_exporter = FeedExporter.from_crawler(crawler)\n+        spider = scrapy.Spider(\"default\")\n+        with warnings.catch_warnings(record=True) as w:\n+            feed_exporter.open_spider(spider)\n+            messages = tuple(str(item.message) for item in w\n+                             if item.category is ScrapyDeprecationWarning)\n+            self.assertEqual(\n+                messages,\n+                (\n+                    (\n+                        \"FileFeedStorageWithoutFeedOptions does not support \"\n+                        \"the 'feed_options' keyword argument. Add a \"\n+                        \"'feed_options' parameter to its signature to remove \"\n+                        \"this warning. This parameter will become mandatory \"\n+                        \"in a future version of Scrapy.\"\n+                    ),\n+                )\n+            )\n+\n+\n+class S3FeedStorageWithoutFeedOptions(S3FeedStorage):\n+\n+    def __init__(self, uri, access_key, secret_key, acl):\n+        super().__init__(uri, access_key, secret_key, acl)\n+\n+\n+class S3FeedStorageWithoutFeedOptionsWithFromCrawler(S3FeedStorage):\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, uri):\n+        return super().from_crawler(crawler, uri)\n+\n+\n+class S3FeedStoragePreFeedOptionsTest(unittest.TestCase):\n+    \"\"\"Make sure that any feed exporter created by users before the\n+    introduction of the ``feed_options`` parameter continues to work as\n+    expected, and simply issues a warning.\"\"\"\n+\n+    maxDiff = None\n+\n+    def test_init(self):\n+        settings_dict = {\n+            'FEED_URI': 'file:///tmp/foobar',\n+            'FEED_STORAGES': {\n+                'file': 'tests.test_feedexport.S3FeedStorageWithoutFeedOptions'\n+            },\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        feed_exporter = FeedExporter.from_crawler(crawler)\n+        spider = scrapy.Spider(\"default\")\n+        spider.crawler = crawler\n+        with warnings.catch_warnings(record=True) as w:\n+            feed_exporter.open_spider(spider)\n+            messages = tuple(str(item.message) for item in w\n+                             if item.category is ScrapyDeprecationWarning)\n+            self.assertEqual(\n+                messages,\n+                (\n+                    (\n+                        \"S3FeedStorageWithoutFeedOptions does not support \"\n+                        \"the 'feed_options' keyword argument. Add a \"\n+                        \"'feed_options' parameter to its signature to remove \"\n+                        \"this warning. This parameter will become mandatory \"\n+                        \"in a future version of Scrapy.\"\n+                    ),\n+                )\n+            )\n+\n+    def test_from_crawler(self):\n+        settings_dict = {\n+            'FEED_URI': 'file:///tmp/foobar',\n+            'FEED_STORAGES': {\n+                'file': 'tests.test_feedexport.S3FeedStorageWithoutFeedOptionsWithFromCrawler'\n+            },\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        feed_exporter = FeedExporter.from_crawler(crawler)\n+        spider = scrapy.Spider(\"default\")\n+        spider.crawler = crawler\n+        with warnings.catch_warnings(record=True) as w:\n+            feed_exporter.open_spider(spider)\n+            messages = tuple(str(item.message) for item in w\n+                             if item.category is ScrapyDeprecationWarning)\n+            self.assertEqual(\n+                messages,\n+                (\n+                    (\n+                        \"S3FeedStorageWithoutFeedOptionsWithFromCrawler.from_crawler \"\n+                        \"does not support the 'feed_options' keyword argument. Add a \"\n+                        \"'feed_options' parameter to its signature to remove \"\n+                        \"this warning. This parameter will become mandatory \"\n+                        \"in a future version of Scrapy.\"\n+                    ),\n+                )\n+            )\n+\n+\n+class FTPFeedStorageWithoutFeedOptions(FTPFeedStorage):\n+\n+    def __init__(self, uri, use_active_mode=False):\n+        super().__init__(uri)\n+\n+\n+class FTPFeedStorageWithoutFeedOptionsWithFromCrawler(FTPFeedStorage):\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, uri):\n+        return super().from_crawler(crawler, uri)\n+\n+\n+class FTPFeedStoragePreFeedOptionsTest(unittest.TestCase):\n+    \"\"\"Make sure that any feed exporter created by users before the\n+    introduction of the ``feed_options`` parameter continues to work as\n+    expected, and simply issues a warning.\"\"\"\n+\n+    maxDiff = None\n+\n+    def test_init(self):\n+        settings_dict = {\n+            'FEED_URI': 'file:///tmp/foobar',\n+            'FEED_STORAGES': {\n+                'file': 'tests.test_feedexport.FTPFeedStorageWithoutFeedOptions'\n+            },\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        feed_exporter = FeedExporter.from_crawler(crawler)\n+        spider = scrapy.Spider(\"default\")\n+        spider.crawler = crawler\n+        with warnings.catch_warnings(record=True) as w:\n+            feed_exporter.open_spider(spider)\n+            messages = tuple(str(item.message) for item in w\n+                             if item.category is ScrapyDeprecationWarning)\n+            self.assertEqual(\n+                messages,\n+                (\n+                    (\n+                        \"FTPFeedStorageWithoutFeedOptions does not support \"\n+                        \"the 'feed_options' keyword argument. Add a \"\n+                        \"'feed_options' parameter to its signature to remove \"\n+                        \"this warning. This parameter will become mandatory \"\n+                        \"in a future version of Scrapy.\"\n+                    ),\n+                )\n+            )\n+\n+    def test_from_crawler(self):\n+        settings_dict = {\n+            'FEED_URI': 'file:///tmp/foobar',\n+            'FEED_STORAGES': {\n+                'file': 'tests.test_feedexport.FTPFeedStorageWithoutFeedOptionsWithFromCrawler'\n+            },\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        feed_exporter = FeedExporter.from_crawler(crawler)\n+        spider = scrapy.Spider(\"default\")\n+        spider.crawler = crawler\n+        with warnings.catch_warnings(record=True) as w:\n+            feed_exporter.open_spider(spider)\n+            messages = tuple(str(item.message) for item in w\n+                             if item.category is ScrapyDeprecationWarning)\n+            self.assertEqual(\n+                messages,\n+                (\n+                    (\n+                        \"FTPFeedStorageWithoutFeedOptionsWithFromCrawler.from_crawler \"\n+                        \"does not support the 'feed_options' keyword argument. Add a \"\n+                        \"'feed_options' parameter to its signature to remove \"\n+                        \"this warning. This parameter will become mandatory \"\n+                        \"in a future version of Scrapy.\"\n+                    ),\n+                )\n+            )\n\n@@ -141,6 +141,22 @@ class FeedExportConfigTestCase(unittest.TestCase):\n             feed_process_params_from_cli(settings, ['-:pickle'])\n         )\n \n+    def test_feed_export_config_overwrite(self):\n+        settings = Settings()\n+        self.assertEqual(\n+            {'output.json': {'format': 'json', 'overwrite': True}},\n+            feed_process_params_from_cli(settings, [], None, ['output.json'])\n+        )\n+\n+    def test_output_and_overwrite_output(self):\n+        with self.assertRaises(UsageError):\n+            feed_process_params_from_cli(\n+                Settings(),\n+                ['output1.json'],\n+                None,\n+                ['output2.json'],\n+            )\n+\n     def test_feed_complete_default_values_from_settings_empty(self):\n         feed = {}\n         settings = Settings({\n\n@@ -179,6 +179,9 @@ class UtilsPythonTestCase(unittest.TestCase):\n         def f2(a, b=None, c=None):\n             pass\n \n+        def f3(a, b=None, *, c=None):\n+            pass\n+\n         class A:\n             def __init__(self, a, b, c):\n                 pass\n@@ -199,6 +202,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n \n         self.assertEqual(get_func_args(f1), ['a', 'b', 'c'])\n         self.assertEqual(get_func_args(f2), ['a', 'b', 'c'])\n+        self.assertEqual(get_func_args(f3), ['a', 'b', 'c'])\n         self.assertEqual(get_func_args(A), ['a', 'b', 'c'])\n         self.assertEqual(get_func_args(a.method), ['a', 'b', 'c'])\n         self.assertEqual(get_func_args(partial_f1), ['b', 'c'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
