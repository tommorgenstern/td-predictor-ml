{"custom_id": "keras#773a9add982b97aa26a377b5cb13e01661c778f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 212 | Contributors (this commit): 5 | Commits (past 90d): 14 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,7 @@\n import os\n \n import numpy as np\n+import pytest\n from tensorflow import data as tf_data\n \n from keras_core import backend\n@@ -35,6 +36,7 @@ class DicretizationTest(testing.TestCase):\n         output = layer(np.array([[0.0, 0.1, 0.3]]))\n         self.assertTrue(output.dtype, \"int32\")\n \n+    @pytest.mark.skipif(backend.backend() == \"torch\", reason=\"TODO: fix me\")\n     def test_correctness(self):\n         # int mode\n         layer = layers.Discretization(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e369dd27532b77b7dbceda3d1784c3be3fad779f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 216 | Contributors (this commit): 5 | Commits (past 90d): 15 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -36,7 +36,9 @@ class DicretizationTest(testing.TestCase):\n         output = layer(np.array([[0.0, 0.1, 0.3]]))\n         self.assertTrue(output.dtype, \"int32\")\n \n-    @pytest.mark.skipif(backend.backend() == \"torch\", reason=\"TODO: fix me\")\n+    @pytest.mark.skipif(\n+        backend.backend() in (\"torch\", \"numpy\"), reason=\"TODO: fix me\"\n+    )\n     def test_correctness(self):\n         # int mode\n         layer = layers.Discretization(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#768ab0a0f9a78dc06955a3442e2315ed62261ef6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 412 | Lines Deleted: 10 | Files Changed: 9 | Hunks: 34 | Methods Changed: 22 | Complexity Δ (Sum/Max): 40/19 | Churn Δ: 422 | Churn Cumulative: 11168 | Contributors (this commit): 24 | Commits (past 90d): 109 | Contributors (cumulative): 61 | DMM Complexity: 0.8815331010452961\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import builtins\n+import functools\n import warnings\n \n import tensorflow as tf\n@@ -7,6 +9,8 @@ from keras_core.backend.tensorflow.core import convert_to_tensor\n \n \n def add(x1, x2):\n+    if isinstance(x1, tf.SparseTensor) or isinstance(x2, tf.SparseTensor):\n+        return tf.sparse.add(x1, x2)\n     return tfnp.add(x1, x2)\n \n \n@@ -38,6 +42,11 @@ def einsum(subscripts, *operands, **kwargs):\n \n \n def subtract(x1, x2):\n+    if isinstance(x1, tf.SparseTensor) or isinstance(x2, tf.SparseTensor):\n+        if isinstance(x2, tf.SparseTensor):\n+            return tf.sparse.add(x1, tf.sparse.map_values(tf.negative, x2))\n+        else:\n+            return tf.sparse.add(x1, tf.negative(x2))\n     return tfnp.subtract(x1, x2)\n \n \n@@ -62,6 +71,40 @@ def matmul(x1, x2):\n \n \n def multiply(x1, x2):\n+    if isinstance(x1, tf.SparseTensor):\n+        if isinstance(x2, tf.SparseTensor):\n+            ones_like_int8 = functools.partial(tf.ones_like, dtype=tf.int8)\n+            zeros_like_int8 = functools.partial(tf.zeros_like, dtype=tf.int8)\n+\n+            # compute the intersection of indices in the form of a sparse tensor\n+            # containing ones as values\n+            ones1 = tf.sparse.map_values(ones_like_int8, x1)\n+            ones2 = tf.sparse.map_values(ones_like_int8, x2)\n+            # tf.sets.intersection ignores the last dimension when comparing,\n+            # so we need to add a dummy extra dimension and then remove it\n+            intersection = tf.sparse.reshape(\n+                tf.sets.intersection(\n+                    tf.sparse.expand_dims(ones1, axis=-1),\n+                    tf.sparse.expand_dims(ones2, axis=-1),\n+                ),\n+                x1.dense_shape,\n+            )\n+\n+            # compute the masks to remove indices in x1 and x2 that are not part\n+            # of the intersection, then trim x1 and x2\n+            zeros1 = tf.sparse.map_values(zeros_like_int8, x1)\n+            zeros2 = tf.sparse.map_values(zeros_like_int8, x2)\n+            mask1 = tf.sparse.add(zeros1, intersection)\n+            mask2 = tf.sparse.add(zeros2, intersection)\n+            x1_trimmed = tf.sparse.retain(x1, tf.cast(mask1.values, tf.bool))\n+            x2_trimmed = tf.sparse.retain(x2, tf.cast(mask2.values, tf.bool))\n+\n+            # now it is an element-wise multiplication on the values\n+            return tf.sparse.map_values(tf.multiply, x1_trimmed, x2_trimmed)\n+        else:\n+            return x1 * x2\n+    elif isinstance(x2, tf.SparseTensor):\n+        return x2 * x1\n     return tfnp.multiply(x1, x2)\n \n \n@@ -202,6 +245,15 @@ def clip(x, x_min, x_max):\n \n \n def concatenate(xs, axis=0):\n+    sparse_count = builtins.sum(isinstance(x, tf.SparseTensor) for x in xs)\n+    if sparse_count:\n+        if sparse_count == len(xs):\n+            return tf.sparse.concat(axis=axis, sp_inputs=xs)\n+        else:\n+            xs = [\n+                tf.sparse.to_dense(x) if isinstance(x, tf.SparseTensor) else x\n+                for x in xs\n+            ]\n     return tfnp.concatenate(xs, axis=axis)\n \n \n@@ -420,6 +472,13 @@ def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n \n \n def maximum(x1, x2):\n+    if isinstance(x1, tf.SparseTensor):\n+        if isinstance(x2, tf.SparseTensor):\n+            return tf.sparse.maximum(x1, x2)\n+        else:\n+            x1 = tf.sparse.to_dense(x1)\n+    elif isinstance(x2, tf.SparseTensor):\n+        x2 = tf.sparse.to_dense(x2)\n     return tfnp.maximum(x1, x2)\n \n \n@@ -449,6 +508,13 @@ def min(x, axis=None, keepdims=False, initial=None):\n \n \n def minimum(x1, x2):\n+    if isinstance(x1, tf.SparseTensor):\n+        if isinstance(x2, tf.SparseTensor):\n+            return tf.sparse.minimum(x1, x2)\n+        else:\n+            x1 = tf.sparse.to_dense(x1)\n+    elif isinstance(x2, tf.SparseTensor):\n+        x2 = tf.sparse.to_dense(x2)\n     return tfnp.minimum(x1, x2)\n \n \n\n@@ -1,3 +1,4 @@\n+from keras_core import ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.merging.base_merge import Merge\n \n@@ -32,7 +33,7 @@ class Add(Merge):\n     def _merge_function(self, inputs):\n         output = inputs[0]\n         for i in range(1, len(inputs)):\n-            output = output + inputs[i]\n+            output = ops.add(output, inputs[i])\n         return output\n \n \n\n@@ -1,3 +1,4 @@\n+from keras_core import ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.merging.base_merge import Merge\n \n@@ -32,7 +33,7 @@ class Average(Merge):\n     def _merge_function(self, inputs):\n         output = inputs[0]\n         for i in range(1, len(inputs)):\n-            output = output + inputs[i]\n+            output = ops.add(output, inputs[i])\n         return output / len(inputs)\n \n \n\n@@ -1,5 +1,6 @@\n from keras_core import backend\n from keras_core import ops\n+from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.layers.layer import Layer\n \n \n@@ -208,6 +209,13 @@ class Merge(Layer):\n             output_shape = (None,) + output_shape\n         return output_shape\n \n+    def compute_output_spec(self, inputs):\n+        output_shape = self.compute_output_shape([x.shape for x in inputs])\n+        output_sparse = all(x.sparse for x in inputs)\n+        return KerasTensor(\n+            output_shape, dtype=self.compute_dtype, sparse=output_sparse\n+        )\n+\n     def compute_mask(self, inputs, mask=None):\n         if mask is None:\n             return None\n\n@@ -220,3 +220,59 @@ class MergingLayersTest(testing.TestCase, parameterized.TestCase):\n             ValueError, \"layer should be called on exactly 2 inputs\"\n         ):\n             layers.Subtract()([input_1])\n+\n+    @parameterized.named_parameters(TEST_PARAMETERS)\n+    @pytest.mark.skipif(\n+        not backend.SUPPORTS_SPARSE_TENSORS,\n+        reason=\"Backend does not support sparse tensors.\",\n+    )\n+    def test_sparse(\n+        self,\n+        layer_class,\n+        np_op,\n+        init_kwargs={},\n+        input_shape=(2, 4, 5),\n+        expected_output_shape=(2, 4, 5),\n+        **kwargs\n+    ):\n+        import tensorflow as tf\n+\n+        if layer_class == layers.Dot:\n+            pytest.skip(\"Dot layer does not support sparse tensors.\")\n+\n+        self.run_layer_test(\n+            layer_class,\n+            init_kwargs=init_kwargs,\n+            input_shape=[input_shape, input_shape],\n+            input_sparse=True,\n+            expected_output_shape=expected_output_shape,\n+            expected_output_sparse=True,\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+            run_training_check=False,\n+            run_mixed_precision_check=False,\n+        )\n+\n+        layer = layer_class(**init_kwargs)\n+\n+        # Merging a sparse tensor with a dense tensor, or a dense tensor with a\n+        # sparse tensor produces a dense tensor\n+        x1 = tf.SparseTensor(\n+            indices=[[0, 0], [1, 2]], values=[1.0, 2.0], dense_shape=(2, 3)\n+        )\n+        x1_np = tf.sparse.to_dense(x1).numpy()\n+        x2 = np.random.rand(2, 3)\n+        self.assertAllClose(layer([x1, x2]), np_op(x1_np, x2, **init_kwargs))\n+        self.assertAllClose(layer([x2, x1]), np_op(x2, x1_np, **init_kwargs))\n+\n+        # Merging a sparse tensor with a sparse tensor produces a sparse tensor\n+        x3 = tf.SparseTensor(\n+            indices=[[0, 0], [1, 1]], values=[4.0, 5.0], dense_shape=(2, 3)\n+        )\n+        x3_np = tf.sparse.to_dense(x3).numpy()\n+\n+        self.assertIsInstance(layer([x1, x3]), tf.SparseTensor)\n+        self.assertAllClose(layer([x1, x3]), np_op(x1_np, x3_np, **init_kwargs))\n\n@@ -1,3 +1,4 @@\n+from keras_core import ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.merging.base_merge import Merge\n \n@@ -32,7 +33,7 @@ class Multiply(Merge):\n     def _merge_function(self, inputs):\n         output = inputs[0]\n         for i in range(1, len(inputs)):\n-            output = output * inputs[i]\n+            output = ops.multiply(output, inputs[i])\n         return output\n \n \n\n@@ -1,3 +1,4 @@\n+from keras_core import ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.merging.base_merge import Merge\n \n@@ -44,7 +45,7 @@ class Subtract(Merge):\n                 \"A `Subtract` layer should be called on exactly 2 inputs. \"\n                 f\"Received: inputs={inputs}\"\n             )\n-        return inputs[0] - inputs[1]\n+        return ops.subtract(inputs[0], inputs[1])\n \n \n @keras_core_export(\"keras_core.layers.subtract\")\n\n@@ -303,7 +303,10 @@ class Add(Operation):\n         x1_shape = getattr(x1, \"shape\", [])\n         x2_shape = getattr(x2, \"shape\", [])\n         output_shape = broadcast_shapes(x1_shape, x2_shape)\n-        return KerasTensor(output_shape, dtype=x1.dtype)\n+        x1_sparse = getattr(x1, \"sparse\", True)\n+        x2_sparse = getattr(x2, \"sparse\", True)\n+        output_sparse = x1_sparse and x2_sparse\n+        return KerasTensor(output_shape, dtype=x1.dtype, sparse=output_sparse)\n \n \n @keras_core_export([\"keras_core.ops.add\", \"keras_core.ops.numpy.add\"])\n@@ -1386,6 +1389,7 @@ class Concatenate(Operation):\n     def compute_output_spec(self, xs):\n         first_shape = xs[0].shape\n         total_size_on_axis = 0\n+        all_sparse = True\n         for x in xs:\n             if not shape_equal(\n                 x.shape, first_shape, axis=[self.axis], allow_none=True\n@@ -1400,9 +1404,11 @@ class Concatenate(Operation):\n                 total_size_on_axis = None\n             else:\n                 total_size_on_axis += x.shape[self.axis]\n+            if not x.sparse:\n+                all_sparse = False\n         output_shape = list(first_shape)\n         output_shape[self.axis] = total_size_on_axis\n-        return KerasTensor(output_shape, dtype=x.dtype)\n+        return KerasTensor(output_shape, dtype=x.dtype, sparse=all_sparse)\n \n \n @keras_core_export(\n@@ -3443,7 +3449,10 @@ class Maximum(Operation):\n         x1_shape = getattr(x1, \"shape\", [])\n         x2_shape = getattr(x2, \"shape\", [])\n         output_shape = broadcast_shapes(x1_shape, x2_shape)\n-        return KerasTensor(output_shape, dtype=x1.dtype)\n+        x1_sparse = getattr(x1, \"sparse\", True)\n+        x2_sparse = getattr(x2, \"sparse\", True)\n+        output_sparse = x1_sparse and x2_sparse\n+        return KerasTensor(output_shape, dtype=x1.dtype, sparse=output_sparse)\n \n \n @keras_core_export([\"keras_core.ops.maximum\", \"keras_core.ops.numpy.maximum\"])\n@@ -3582,7 +3591,10 @@ class Minimum(Operation):\n         x1_shape = getattr(x1, \"shape\", [])\n         x2_shape = getattr(x2, \"shape\", [])\n         output_shape = broadcast_shapes(x1_shape, x2_shape)\n-        return KerasTensor(output_shape, dtype=x1.dtype)\n+        x1_sparse = getattr(x1, \"sparse\", True)\n+        x2_sparse = getattr(x2, \"sparse\", True)\n+        output_sparse = x1_sparse and x2_sparse\n+        return KerasTensor(output_shape, dtype=x1.dtype, sparse=output_sparse)\n \n \n @keras_core_export([\"keras_core.ops.minimum\", \"keras_core.ops.numpy.minimum\"])\n@@ -5080,7 +5092,10 @@ class Subtract(Operation):\n         x1_shape = getattr(x1, \"shape\", [])\n         x2_shape = getattr(x2, \"shape\", [])\n         output_shape = broadcast_shapes(x1_shape, x2_shape)\n-        return KerasTensor(output_shape, dtype=x1.dtype)\n+        x1_sparse = getattr(x1, \"sparse\", True)\n+        x2_sparse = getattr(x2, \"sparse\", True)\n+        output_sparse = x1_sparse and x2_sparse\n+        return KerasTensor(output_shape, dtype=x1.dtype, sparse=output_sparse)\n \n \n @keras_core_export([\"keras_core.ops.subtract\", \"keras_core.ops.numpy.subtract\"])\n@@ -5107,7 +5122,10 @@ class Multiply(Operation):\n         x1_shape = getattr(x1, \"shape\", [])\n         x2_shape = getattr(x2, \"shape\", [])\n         output_shape = broadcast_shapes(x1_shape, x2_shape)\n-        return KerasTensor(output_shape, dtype=x1.dtype)\n+        x1_sparse = getattr(x1, \"sparse\", True)\n+        x2_sparse = getattr(x2, \"sparse\", True)\n+        output_sparse = x1_sparse or x2_sparse\n+        return KerasTensor(output_shape, dtype=x1.dtype, sparse=output_sparse)\n \n \n @keras_core_export([\"keras_core.ops.multiply\", \"keras_core.ops.numpy.multiply\"])\n\n@@ -371,6 +371,25 @@ class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n             y = KerasTensor([2, 3, 4])\n             knp.add(x, y)\n \n+    def test_add_sparse(self):\n+        x = KerasTensor((2, 3), sparse=True)\n+        y = KerasTensor((2, 3))\n+        result = knp.add(x, y)\n+        self.assertEqual(result.shape, (2, 3))\n+        self.assertFalse(result.sparse)\n+\n+        x = KerasTensor((2, 3))\n+        y = KerasTensor((2, 3), sparse=True)\n+        result = knp.add(x, y)\n+        self.assertEqual(result.shape, (2, 3))\n+        self.assertFalse(result.sparse)\n+\n+        x = KerasTensor((2, 3), sparse=True)\n+        y = KerasTensor((2, 3), sparse=True)\n+        result = knp.add(x, y)\n+        self.assertEqual(result.shape, (2, 3))\n+        self.assertTrue(result.sparse)\n+\n     def test_subtract(self):\n         x = KerasTensor([2, 3])\n         y = KerasTensor([2, 3])\n@@ -384,6 +403,25 @@ class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n             y = KerasTensor([2, 3, 4])\n             knp.subtract(x, y)\n \n+    def test_subtract_sparse(self):\n+        x = KerasTensor((2, 3), sparse=True)\n+        y = KerasTensor((2, 3))\n+        result = knp.subtract(x, y)\n+        self.assertEqual(result.shape, (2, 3))\n+        self.assertFalse(result.sparse)\n+\n+        x = KerasTensor((2, 3))\n+        y = KerasTensor((2, 3), sparse=True)\n+        result = knp.subtract(x, y)\n+        self.assertEqual(result.shape, (2, 3))\n+        self.assertFalse(result.sparse)\n+\n+        x = KerasTensor((2, 3), sparse=True)\n+        y = KerasTensor((2, 3), sparse=True)\n+        result = knp.subtract(x, y)\n+        self.assertEqual(result.shape, (2, 3))\n+        self.assertTrue(result.sparse)\n+\n     def test_multiply(self):\n         x = KerasTensor([2, 3])\n         y = KerasTensor([2, 3])\n@@ -397,6 +435,25 @@ class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n             y = KerasTensor([2, 3, 4])\n             knp.multiply(x, y)\n \n+    def test_multiply_sparse(self):\n+        x = KerasTensor((2, 3), sparse=True)\n+        y = KerasTensor((2, 3))\n+        result = knp.multiply(x, y)\n+        self.assertEqual(result.shape, (2, 3))\n+        self.assertTrue(result.sparse)\n+\n+        x = KerasTensor((2, 3))\n+        y = KerasTensor((2, 3), sparse=True)\n+        result = knp.multiply(x, y)\n+        self.assertEqual(result.shape, (2, 3))\n+        self.assertTrue(result.sparse)\n+\n+        x = KerasTensor((2, 3), sparse=True)\n+        y = KerasTensor((2, 3), sparse=True)\n+        result = knp.multiply(x, y)\n+        self.assertEqual(result.shape, (2, 3))\n+        self.assertTrue(result.sparse)\n+\n     def test_matmul(self):\n         x = KerasTensor([2, 3])\n         y = KerasTensor([3, 2])\n@@ -650,6 +707,25 @@ class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n             y = KerasTensor([2, 3, 4])\n             knp.maximum(x, y)\n \n+    def test_maximum_sparse(self):\n+        x = KerasTensor((2, 3), sparse=True)\n+        y = KerasTensor((2, 3))\n+        result = knp.maximum(x, y)\n+        self.assertEqual(result.shape, (2, 3))\n+        self.assertFalse(result.sparse)\n+\n+        x = KerasTensor((2, 3))\n+        y = KerasTensor((2, 3), sparse=True)\n+        result = knp.maximum(x, y)\n+        self.assertEqual(result.shape, (2, 3))\n+        self.assertFalse(result.sparse)\n+\n+        x = KerasTensor((2, 3), sparse=True)\n+        y = KerasTensor((2, 3), sparse=True)\n+        result = knp.maximum(x, y)\n+        self.assertEqual(result.shape, (2, 3))\n+        self.assertTrue(result.sparse)\n+\n     def test_minimum(self):\n         x = KerasTensor([2, 3])\n         y = KerasTensor([2, 3])\n@@ -663,6 +739,25 @@ class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n             y = KerasTensor([2, 3, 4])\n             knp.minimum(x, y)\n \n+    def test_minimum_sparse(self):\n+        x = KerasTensor((2, 3), sparse=True)\n+        y = KerasTensor((2, 3))\n+        result = knp.minimum(x, y)\n+        self.assertEqual(result.shape, (2, 3))\n+        self.assertFalse(result.sparse)\n+\n+        x = KerasTensor((2, 3))\n+        y = KerasTensor((2, 3), sparse=True)\n+        result = knp.minimum(x, y)\n+        self.assertEqual(result.shape, (2, 3))\n+        self.assertFalse(result.sparse)\n+\n+        x = KerasTensor((2, 3), sparse=True)\n+        y = KerasTensor((2, 3), sparse=True)\n+        result = knp.minimum(x, y)\n+        self.assertEqual(result.shape, (2, 3))\n+        self.assertTrue(result.sparse)\n+\n     def test_mod(self):\n         x = KerasTensor([2, 3])\n         y = KerasTensor([2, 3])\n@@ -966,6 +1061,25 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n             y = KerasTensor([None, 4, 6])\n             knp.concatenate([x, y], axis=1)\n \n+    def test_concatenate_sparse(self):\n+        x = KerasTensor((2, 3), sparse=True)\n+        y = KerasTensor((2, 3))\n+        result = knp.concatenate([x, y], axis=1)\n+        self.assertEqual(result.shape, (2, 6))\n+        self.assertFalse(result.sparse)\n+\n+        x = KerasTensor((2, 3))\n+        y = KerasTensor((2, 3), sparse=True)\n+        result = knp.concatenate([x, y], axis=1)\n+        self.assertEqual(result.shape, (2, 6))\n+        self.assertFalse(result.sparse)\n+\n+        x = KerasTensor((2, 3), sparse=True)\n+        y = KerasTensor((2, 3), sparse=True)\n+        result = knp.concatenate([x, y], axis=1)\n+        self.assertEqual(result.shape, (2, 6))\n+        self.assertTrue(result.sparse)\n+\n     def test_conjugate(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.conjugate(x).shape, (None, 3))\n@@ -2376,6 +2490,85 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n             standardize_dtype(knp.Digitize()(x, bins).dtype) == \"int32\"\n         )\n \n+    @parameterized.named_parameters(\n+        [\n+            {\n+                \"testcase_name\": \"add\",\n+                \"op_function\": knp.add,\n+                \"op_class\": knp.Add,\n+                \"np_op\": np.add,\n+            },\n+            {\n+                \"testcase_name\": \"subtract\",\n+                \"op_function\": knp.subtract,\n+                \"op_class\": knp.Subtract,\n+                \"np_op\": np.subtract,\n+            },\n+            {\n+                \"testcase_name\": \"multiply\",\n+                \"op_function\": knp.multiply,\n+                \"op_class\": knp.Multiply,\n+                \"np_op\": np.multiply,\n+                \"mixed_inputs_produce_sparse_output\": True,\n+            },\n+            {\n+                \"testcase_name\": \"minimum\",\n+                \"op_function\": knp.minimum,\n+                \"op_class\": knp.Minimum,\n+                \"np_op\": np.minimum,\n+            },\n+            {\n+                \"testcase_name\": \"maximum\",\n+                \"op_function\": knp.maximum,\n+                \"op_class\": knp.Maximum,\n+                \"np_op\": np.maximum,\n+            },\n+        ]\n+    )\n+    @pytest.mark.skipif(\n+        not backend.SUPPORTS_SPARSE_TENSORS,\n+        reason=\"Backend does not support sparse tensors.\",\n+    )\n+    def test_sparse(\n+        self,\n+        op_function,\n+        op_class,\n+        np_op,\n+        mixed_inputs_produce_sparse_output=False,\n+    ):\n+        import tensorflow as tf\n+\n+        x = tf.SparseTensor(\n+            indices=[[0, 0], [1, 2]], values=[1.0, 2.0], dense_shape=(2, 3)\n+        )\n+        x_np = tf.sparse.to_dense(x).numpy()\n+\n+        y = tf.SparseTensor(\n+            indices=[[0, 0], [1, 1]], values=[4.0, 5.0], dense_shape=(2, 3)\n+        )\n+        y_np = tf.sparse.to_dense(y).numpy()\n+        z = np.random.rand(2, 3).astype(\"float32\")\n+\n+        # sparse tensor and dense tensor as inputs\n+        if mixed_inputs_produce_sparse_output:\n+            self.assertIsInstance(op_function(x, z), tf.SparseTensor)\n+            self.assertIsInstance(op_class()(x, z), tf.SparseTensor)\n+        self.assertAllClose(op_function(x, z), np_op(x_np, z))\n+        self.assertAllClose(op_class()(x, z), np_op(x_np, z))\n+\n+        # dense tensor and sparse tensor as inputs\n+        if mixed_inputs_produce_sparse_output:\n+            self.assertIsInstance(op_function(z, x), tf.SparseTensor)\n+            self.assertIsInstance(op_class()(z, x), tf.SparseTensor)\n+        self.assertAllClose(op_function(z, x), np_op(z, x_np))\n+        self.assertAllClose(op_class()(z, x), np_op(z, x_np))\n+\n+        # sparse tensor and sparse tensor as inputs\n+        self.assertIsInstance(op_function(x, y), tf.SparseTensor)\n+        self.assertIsInstance(op_class()(x, y), tf.SparseTensor)\n+        self.assertAllClose(op_function(x, y), np_op(x_np, y_np))\n+        self.assertAllClose(op_class()(x, y), np_op(x_np, y_np))\n+\n \n class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     def test_mean(self):\n@@ -2755,6 +2948,63 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             np.concatenate([x, y], axis=1),\n         )\n \n+    @parameterized.named_parameters(\n+        [\n+            {\"testcase_name\": \"axis_0\", \"axis\": 0},\n+            {\"testcase_name\": \"axis_1\", \"axis\": 1},\n+        ]\n+    )\n+    @pytest.mark.skipif(\n+        not backend.SUPPORTS_SPARSE_TENSORS,\n+        reason=\"Backend does not support sparse tensors.\",\n+    )\n+    def test_concatenate_sparse(self, axis):\n+        import tensorflow as tf\n+\n+        x = tf.SparseTensor(\n+            indices=[[0, 0], [1, 2]], values=[1.0, 2.0], dense_shape=(2, 3)\n+        )\n+        x_np = tf.sparse.to_dense(x).numpy()\n+\n+        y = tf.SparseTensor(\n+            indices=[[0, 0], [1, 1]], values=[4.0, 5.0], dense_shape=(2, 3)\n+        )\n+        y_np = tf.sparse.to_dense(y).numpy()\n+        z = np.random.rand(2, 3).astype(\"float32\")\n+\n+        self.assertAllClose(\n+            knp.concatenate([x, z], axis=axis),\n+            np.concatenate([x_np, z], axis=axis),\n+        )\n+        self.assertAllClose(\n+            knp.concatenate([z, x], axis=axis),\n+            np.concatenate([z, x_np], axis=axis),\n+        )\n+        self.assertAllClose(\n+            knp.concatenate([x, y], axis=axis),\n+            np.concatenate([x_np, y_np], axis=axis),\n+        )\n+\n+        self.assertAllClose(\n+            knp.Concatenate(axis=axis)([x, z]),\n+            np.concatenate([x_np, z], axis=axis),\n+        )\n+        self.assertAllClose(\n+            knp.Concatenate(axis=axis)([z, x]),\n+            np.concatenate([z, x_np], axis=axis),\n+        )\n+        self.assertAllClose(\n+            knp.Concatenate(axis=axis)([x, y]),\n+            np.concatenate([x_np, y_np], axis=axis),\n+        )\n+\n+        self.assertIsInstance(\n+            knp.concatenate([x, y], axis=axis), tf.SparseTensor\n+        )\n+        self.assertIsInstance(\n+            knp.Concatenate(axis=axis)([x, y]), tf.SparseTensor\n+        )\n+\n     def test_conjugate(self):\n         x = np.array([[1 + 2j, 2 + 3j], [3 + 4j, 4 + 5j]])\n         self.assertAllClose(knp.conjugate(x), np.conjugate(x))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#283159e61eaef301e02743e61a128f7e38f8efc6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 17 | Files Changed: 7 | Hunks: 12 | Methods Changed: 12 | Complexity Δ (Sum/Max): 11/5 | Churn Δ: 93 | Churn Cumulative: 7274 | Contributors (this commit): 24 | Commits (past 90d): 126 | Contributors (cumulative): 67 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,10 +13,21 @@ def add(x1, x2):\n \n def bincount(x, weights=None, minlength=0):\n     if len(x.shape) == 2:\n-        bincounts = [\n-            jnp.bincount(arr, weights=weights, minlength=minlength)\n-            for arr in list(x)\n-        ]\n+        if weights is None:\n+\n+            def bincount_fn(arr):\n+                return jnp.bincount(arr, minlength=minlength)\n+\n+            bincounts = list(map(bincount_fn, x))\n+        else:\n+\n+            def bincount_fn(arr_w):\n+                return jnp.bincount(\n+                    arr_w[0], weights=arr_w[1], minlength=minlength\n+                )\n+\n+            bincounts = list(map(bincount_fn, zip(x, weights)))\n+\n         return jnp.stack(bincounts)\n     return jnp.bincount(x, weights=weights, minlength=minlength)\n \n\n@@ -133,6 +133,23 @@ def average(x, axis=None, weights=None):\n \n \n def bincount(x, weights=None, minlength=0):\n+    if len(x.shape) == 2:\n+        if weights is None:\n+\n+            def bincount_fn(arr):\n+                return np.bincount(arr, minlength=minlength)\n+\n+            bincounts = list(map(bincount_fn, x))\n+        else:\n+\n+            def bincount_fn(arr_w):\n+                return np.bincount(\n+                    arr_w[0], weights=arr_w[1], minlength=minlength\n+                )\n+\n+            bincounts = list(map(bincount_fn, zip(x, weights)))\n+\n+        return np.stack(bincounts)\n     return np.bincount(x, weights, minlength)\n \n \n\n@@ -245,7 +245,25 @@ def average(x, axis=None, weights=None):\n \n def bincount(x, weights=None, minlength=0):\n     x = convert_to_tensor(x, dtype=int)\n+    if weights is not None:\n         weights = convert_to_tensor(weights)\n+    if len(x.shape) == 2:\n+        if weights is None:\n+\n+            def bincount_fn(arr):\n+                return torch.bincount(arr, minlength=minlength)\n+\n+            bincounts = list(map(bincount_fn, x))\n+        else:\n+\n+            def bincount_fn(arr_w):\n+                return torch.bincount(\n+                    arr_w[0], weights=arr_w[1], minlength=minlength\n+                )\n+\n+            bincounts = list(map(bincount_fn, zip(x, weights)))\n+\n+        return torch.stack(bincounts)\n     return torch.bincount(x, weights, minlength)\n \n \n\n@@ -233,6 +233,7 @@ class Discretization(TFDataLayer):\n             output_mode=self.output_mode,\n             depth=len(self.bin_boundaries) + 1,\n             dtype=self.compute_dtype,\n+            count_weights=None,\n             backend_module=self.backend,\n         )\n         return outputs\n\n@@ -1,7 +1,6 @@\n import os\n \n import numpy as np\n-import pytest\n from tensorflow import data as tf_data\n \n from keras_core import backend\n@@ -36,9 +35,6 @@ class DicretizationTest(testing.TestCase):\n         output = layer(np.array([[0.0, 0.1, 0.3]]))\n         self.assertTrue(output.dtype, \"int32\")\n \n-    @pytest.mark.skipif(\n-        backend.backend() in (\"torch\", \"numpy\"), reason=\"TODO: fix me\"\n-    )\n     def test_correctness(self):\n         # int mode\n         layer = layers.Discretization(\n\n@@ -2892,6 +2892,23 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             knp.Bincount(weights=weights, minlength=minlength)(x),\n             np.bincount(x, weights=weights, minlength=minlength),\n         )\n+        x = np.array([[1, 1, 2, 3, 2, 4, 4, 5]])\n+        weights = np.array([[0, 0, 3, 2, 1, 1, 4, 2]])\n+        expected_output = np.array([[0, 0, 4, 2, 5, 2]])\n+        self.assertAllClose(\n+            knp.bincount(x, weights=weights, minlength=minlength),\n+            expected_output,\n+        )\n+        self.assertAllClose(\n+            knp.Bincount(weights=weights, minlength=minlength)(x),\n+            expected_output,\n+        )\n+        # test with weights=None\n+        expected_output = np.array([[0, 2, 2, 1, 2, 1]])\n+        self.assertAllClose(\n+            knp.Bincount(weights=None, minlength=minlength)(x),\n+            expected_output,\n+        )\n \n     def test_broadcast_to(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n\n@@ -125,10 +125,6 @@ def encode_categorical_inputs(\n     # In all cases, we should uprank scalar input to a single sample.\n     if len(backend_module.shape(inputs)) == 0:\n         inputs = backend_module.numpy.expand_dims(inputs, -1)\n-    # One hot will unprank only if the final output dimension is not already 1.\n-    if output_mode == \"one_hot\":\n-        if backend_module.shape(inputs)[-1] != 1:\n-            inputs = backend_module.numpy.expand_dims(inputs, -1)\n \n     if len(backend_module.shape(inputs)) > 2:\n         raise ValueError(\n@@ -139,16 +135,19 @@ def encode_categorical_inputs(\n         )\n \n     binary_output = output_mode in (\"multi_hot\", \"one_hot\")\n-    bincounts = backend_module.numpy.bincount(\n-        inputs,\n-        weights=count_weights,\n-        minlength=depth,\n-    )\n     if binary_output:\n+        if output_mode == \"one_hot\":\n+            bincounts = backend_module.nn.one_hot(inputs, depth)\n+        elif output_mode == \"multi_hot\":\n             one_hot_input = backend_module.nn.one_hot(inputs, depth)\n             bincounts = backend_module.numpy.where(\n                 backend_module.numpy.any(one_hot_input, axis=-2), 1, 0\n             )\n+    else:\n+        bincounts = backend_module.numpy.bincount(\n+            inputs,\n+            minlength=depth,\n+        )\n     bincounts = backend_module.cast(bincounts, dtype)\n \n     return bincounts\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6887971949b0ad073d0d765d0fa3d80d47ea2de8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 183 | Files Changed: 4 | Hunks: 30 | Methods Changed: 11 | Complexity Δ (Sum/Max): -14/0 | Churn Δ: 238 | Churn Cumulative: 2619 | Contributors (this commit): 8 | Commits (past 90d): 39 | Contributors (cumulative): 19 | DMM Complexity: 0.5196078431372549\n\nDIFF:\n@@ -3,7 +3,6 @@ import itertools\n import operator\n \n import torch\n-import torch.nn.functional as tnn\n \n from keras_core.backend.torch.core import convert_to_tensor\n \n@@ -82,76 +81,17 @@ def resize(\n     return resized\n \n \n-AFFINE_TRANSFORM_INTERPOLATIONS = (\n-    \"nearest\",\n-    \"bilinear\",\n-)\n-AFFINE_TRANSFORM_FILL_MODES = {\n-    \"constant\": \"zeros\",\n-    \"nearest\": \"border\",\n-    # \"wrap\",  not supported by torch\n-    \"mirror\": \"reflection\",  # torch's reflection is mirror in other backends\n-    \"reflect\": \"reflection\",  # if fill_mode==reflect, redirect to mirror\n+AFFINE_TRANSFORM_INTERPOLATIONS = {\n+    \"nearest\": 0,\n+    \"bilinear\": 1,\n+}\n+AFFINE_TRANSFORM_FILL_MODES = {\n+    \"constant\",\n+    \"nearest\",\n+    \"wrap\",\n+    \"mirror\",\n+    \"reflect\",\n }\n-\n-\n-def _apply_grid_transform(\n-    img,\n-    grid,\n-    interpolation=\"bilinear\",\n-    fill_mode=\"zeros\",\n-    fill_value=None,\n-):\n-    \"\"\"\n-    Modified from https://github.com/pytorch/vision/blob/main/torchvision/transforms/v2/functional/_geometry.py\n-    \"\"\"  # noqa: E501\n-\n-    # We are using context knowledge that grid should have float dtype\n-    fp = img.dtype == grid.dtype\n-    float_img = img if fp else img.to(grid.dtype)\n-\n-    shape = float_img.shape\n-    # Append a dummy mask for customized fill colors, should be faster than\n-    # grid_sample() twice\n-    if fill_value is not None:\n-        mask = torch.ones(\n-            (shape[0], 1, shape[2], shape[3]),\n-            dtype=float_img.dtype,\n-            device=float_img.device,\n-        )\n-        float_img = torch.cat((float_img, mask), dim=1)\n-\n-    float_img = tnn.grid_sample(\n-        float_img,\n-        grid,\n-        mode=interpolation,\n-        padding_mode=fill_mode,\n-        align_corners=True,\n-    )\n-    # Fill with required color\n-    if fill_value is not None:\n-        float_img, mask = torch.tensor_split(float_img, indices=(-1,), dim=-3)\n-        mask = mask.expand_as(float_img)\n-        fill_list = (\n-            fill_value\n-            if isinstance(fill_value, (tuple, list))\n-            else [float(fill_value)]\n-        )\n-        fill_img = torch.tensor(\n-            fill_list, dtype=float_img.dtype, device=float_img.device\n-        ).view(1, -1, 1, 1)\n-        if interpolation == \"nearest\":\n-            bool_mask = mask < 0.5\n-            float_img[bool_mask] = fill_img.expand_as(float_img)[bool_mask]\n-        else:  # 'bilinear'\n-            # The following is mathematically equivalent to:\n-            # img * mask + (1.0 - mask) * fill =\n-            # img * mask - fill * mask + fill =\n-            # mask * (img - fill) + fill\n-            float_img = float_img.sub_(fill_img).mul_(mask).add_(fill_img)\n-\n-    img = float_img.round_().to(img.dtype) if not fp else float_img\n-    return img\n \n \n def affine_transform(\n@@ -162,17 +102,16 @@ def affine_transform(\n     fill_value=0,\n     data_format=\"channels_last\",\n ):\n-    if interpolation not in AFFINE_TRANSFORM_INTERPOLATIONS:\n+    if interpolation not in AFFINE_TRANSFORM_INTERPOLATIONS.keys():\n         raise ValueError(\n             \"Invalid value for argument `interpolation`. Expected of one \"\n-            f\"{AFFINE_TRANSFORM_INTERPOLATIONS}. Received: \"\n+            f\"{set(AFFINE_TRANSFORM_INTERPOLATIONS.keys())}. Received: \"\n             f\"interpolation={interpolation}\"\n         )\n-    if fill_mode not in AFFINE_TRANSFORM_FILL_MODES.keys():\n+    if fill_mode not in AFFINE_TRANSFORM_FILL_MODES:\n         raise ValueError(\n             \"Invalid value for argument `fill_mode`. Expected of one \"\n-            f\"{set(AFFINE_TRANSFORM_FILL_MODES.keys())}. \"\n-            f\"Received: fill_mode={fill_mode}\"\n+            f\"{AFFINE_TRANSFORM_FILL_MODES}. Received: fill_mode={fill_mode}\"\n         )\n \n     image = convert_to_tensor(image)\n@@ -191,10 +130,6 @@ def affine_transform(\n             f\"transform.shape={transform.shape}\"\n         )\n \n-    # the default fill_value of tnn.grid_sample is \"zeros\"\n-    if fill_mode != \"constant\" or (fill_mode == \"constant\" and fill_value == 0):\n-        fill_value = None\n-\n     # unbatched case\n     need_squeeze = False\n     if image.ndim == 3:\n@@ -203,22 +138,23 @@ def affine_transform(\n     if transform.ndim == 1:\n         transform = transform.unsqueeze(dim=0)\n \n-    if data_format == \"channels_last\":\n-        image = image.permute((0, 3, 1, 2))\n+    if data_format == \"channels_first\":\n+        image = image.permute((0, 2, 3, 1))\n \n     batch_size = image.shape[0]\n-    h, w, c = image.shape[-2], image.shape[-1], image.shape[-3]\n \n     # get indices\n-    shape = [h, w, c]  # (H, W, C)\n     meshgrid = torch.meshgrid(\n-        *[torch.arange(size) for size in shape], indexing=\"ij\"\n+        *[\n+            torch.arange(size, dtype=transform.dtype, device=transform.device)\n+            for size in image.shape[1:]\n+        ],\n+        indexing=\"ij\",\n     )\n     indices = torch.concatenate(\n         [torch.unsqueeze(x, dim=-1) for x in meshgrid], dim=-1\n     )\n     indices = torch.tile(indices, (batch_size, 1, 1, 1, 1))\n-    indices = indices.to(transform)\n \n     # swap the values\n     a0 = transform[:, 0].clone()\n@@ -243,27 +179,23 @@ def affine_transform(\n     coordinates = torch.einsum(\"Bhwij, Bjk -> Bhwik\", indices, transform)\n     coordinates = torch.moveaxis(coordinates, source=-1, destination=1)\n     coordinates += torch.reshape(a=offset, shape=(*offset.shape, 1, 1, 1))\n-    coordinates = coordinates[:, 0:2, ..., 0]\n-    coordinates = coordinates.permute((0, 2, 3, 1))\n \n-    # normalize coordinates\n-    coordinates[:, :, :, 1] = coordinates[:, :, :, 1] / (w - 1) * 2.0 - 1.0\n-    coordinates[:, :, :, 0] = coordinates[:, :, :, 0] / (h - 1) * 2.0 - 1.0\n-    grid = torch.stack(\n-        [coordinates[:, :, :, 1], coordinates[:, :, :, 0]], dim=-1\n-    )\n-\n-    affined = _apply_grid_transform(\n-        image,\n-        grid,\n-        interpolation=interpolation,\n-        # if fill_mode==reflect, redirect to mirror\n-        fill_mode=AFFINE_TRANSFORM_FILL_MODES[fill_mode],\n+    # Note: torch.stack is faster than torch.vmap when the batch size is small.\n+    affined = torch.stack(\n+        [\n+            map_coordinates(\n+                image[i],\n+                coordinates[i],\n+                order=AFFINE_TRANSFORM_INTERPOLATIONS[interpolation],\n+                fill_mode=fill_mode,\n                 fill_value=fill_value,\n             )\n+            for i in range(len(image))\n+        ],\n+    )\n \n-    if data_format == \"channels_last\":\n-        affined = affined.permute((0, 2, 3, 1))\n+    if data_format == \"channels_first\":\n+        affined = affined.permute((0, 3, 1, 2))\n     if need_squeeze:\n         affined = affined.squeeze(dim=0)\n     return affined\n@@ -282,7 +214,8 @@ def _reflect_index_fixer(index, size):\n \n \n _INDEX_FIXERS = {\n-    \"constant\": lambda index, size: index,\n+    # we need to take care of out-of-bound indices in torch\n+    \"constant\": lambda index, size: torch.clip(index, 0, size - 1),\n     \"nearest\": lambda index, size: torch.clip(index, 0, size - 1),\n     \"wrap\": lambda index, size: index % size,\n     \"mirror\": _mirror_index_fixer,\n@@ -301,8 +234,7 @@ def _nearest_indices_and_weights(coordinate):\n         coordinate if _is_integer(coordinate) else torch.round(coordinate)\n     )\n     index = coordinate.to(torch.int32)\n-    weight = torch.tensor(1).to(torch.int32)\n-    return [(index, weight)]\n+    return [(index, 1)]\n \n \n def _linear_indices_and_weights(coordinate):\n@@ -318,7 +250,9 @@ def map_coordinates(\n ):\n     input_arr = convert_to_tensor(input)\n     coordinate_arrs = [convert_to_tensor(c) for c in coordinates]\n-    fill_value = convert_to_tensor(fill_value, input_arr.dtype)\n+    # skip tensor creation as possible\n+    if isinstance(fill_value, (int, float)) and _is_integer(input_arr):\n+        fill_value = int(fill_value)\n \n     if len(coordinates) != len(input_arr.shape):\n         raise ValueError(\n@@ -330,16 +264,9 @@ def map_coordinates(\n     if index_fixer is None:\n         raise ValueError(\n             \"Invalid value for argument `fill_mode`. Expected one of \"\n-            f\"{set(_INDEX_FIXERS.keys())}. Received: \"\n-            f\"fill_mode={fill_mode}\"\n+            f\"{set(_INDEX_FIXERS.keys())}. Received: fill_mode={fill_mode}\"\n         )\n \n-    def is_valid(index, size):\n-        if fill_mode == \"constant\":\n-            return (0 <= index) & (index < size)\n-        else:\n-            return True\n-\n     if order == 0:\n         interp_fun = _nearest_indices_and_weights\n     elif order == 1:\n@@ -347,6 +274,16 @@ def map_coordinates(\n     else:\n         raise NotImplementedError(\"map_coordinates currently requires order<=1\")\n \n+    if fill_mode == \"constant\":\n+\n+        def is_valid(index, size):\n+            return (0 <= index) & (index < size)\n+\n+    else:\n+\n+        def is_valid(index, size):\n+            return True\n+\n     valid_1d_interpolations = []\n     for coordinate, size in zip(coordinate_arrs, input_arr.shape):\n         interp_nodes = interp_fun(coordinate)\n\n@@ -58,18 +58,6 @@ class RandomTranslationTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.parameters([\"channels_first\", \"channels_last\"])\n     def test_random_translation_up_numeric_reflect(self, data_format):\n         input_image = np.arange(0, 25)\n-        if backend.backend() == \"torch\":\n-            # redirect fill_mode=reflect to fill_mode=mirror\n-            expected_output = np.asarray(\n-                [\n-                    [5, 6, 7, 8, 9],\n-                    [10, 11, 12, 13, 14],\n-                    [15, 16, 17, 18, 19],\n-                    [20, 21, 22, 23, 24],\n-                    [15, 16, 17, 18, 19],\n-                ]\n-            )\n-        else:\n         expected_output = np.asarray(\n             [\n                 [5, 6, 7, 8, 9],\n@@ -145,18 +133,6 @@ class RandomTranslationTest(testing.TestCase, parameterized.TestCase):\n     def test_random_translation_down_numeric_reflect(self, data_format):\n         input_image = np.arange(0, 25)\n         # Shifting by .2 * 5 = 1 pixel.\n-        if backend.backend() == \"torch\":\n-            # redirect fill_mode=reflect to fill_mode=mirror\n-            expected_output = np.asarray(\n-                [\n-                    [5, 6, 7, 8, 9],\n-                    [0, 1, 2, 3, 4],\n-                    [5, 6, 7, 8, 9],\n-                    [10, 11, 12, 13, 14],\n-                    [15, 16, 17, 18, 19],\n-                ]\n-            )\n-        else:\n         expected_output = np.asarray(\n             [\n                 [0, 1, 2, 3, 4],\n@@ -196,21 +172,6 @@ class RandomTranslationTest(testing.TestCase, parameterized.TestCase):\n     ):\n         input_image = np.arange(0, 16)\n         # Shifting by .2 * 5 = 1 pixel.\n-        if backend.backend() == \"torch\":\n-            # redirect fill_mode=reflect to fill_mode=mirror\n-            expected_output = np.asarray(\n-                [\n-                    [8, 9],\n-                    [6, 7],\n-                    [4, 5],\n-                    [2, 3],\n-                    [0, 1],\n-                    [2, 3],\n-                    [4, 5],\n-                    [6, 7],\n-                ]\n-            )\n-        else:\n         expected_output = np.asarray(\n             [\n                 [6, 7],\n@@ -290,18 +251,6 @@ class RandomTranslationTest(testing.TestCase, parameterized.TestCase):\n     def test_random_translation_left_numeric_reflect(self, data_format):\n         input_image = np.arange(0, 25)\n         # Shifting by .2 * 5 = 1 pixel.\n-        if backend.backend() == \"torch\":\n-            # redirect fill_mode=reflect to fill_mode=mirror\n-            expected_output = np.asarray(\n-                [\n-                    [1, 2, 3, 4, 3],\n-                    [6, 7, 8, 9, 8],\n-                    [11, 12, 13, 14, 13],\n-                    [16, 17, 18, 19, 18],\n-                    [21, 22, 23, 24, 23],\n-                ]\n-            )\n-        else:\n         expected_output = np.asarray(\n             [\n                 [1, 2, 3, 4, 4],\n\n@@ -195,10 +195,6 @@ def affine_transform(\n                 The input is extended by wrapping around to the opposite edge.\n             - `\"nearest\"`: `(a a a a | a b c d | d d d d)`\n                 The input is extended by the nearest pixel.\n-            Note that when using torch backend, `\"reflect\"` is redirected to\n-            `\"mirror\"` `(c d c b | a b c d | c b a b)` because torch does not\n-            support `\"reflect\"`.\n-            Note that torch backend does not support `\"wrap\"`.\n         fill_value: Value used for points outside the boundaries of the input if\n             `fill_mode=\"constant\"`. Defaults to `0`.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n\n@@ -238,16 +238,6 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         ]\n     )\n     def test_affine_transform(self, interpolation, fill_mode, data_format):\n-        if backend.backend() == \"torch\" and fill_mode == \"wrap\":\n-            self.skipTest(\n-                \"In torch backend, applying affine_transform with \"\n-                \"fill_mode=wrap is not supported\"\n-            )\n-        if backend.backend() == \"torch\" and fill_mode == \"reflect\":\n-            self.skipTest(\n-                \"In torch backend, applying affine_transform with \"\n-                \"fill_mode=reflect is redirected to fill_mode=mirror\"\n-            )\n         if backend.backend() == \"tensorflow\" and fill_mode == \"mirror\":\n             self.skipTest(\n                 \"In tensorflow backend, applying affine_transform with \"\n@@ -259,10 +249,10 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n                 \"affine_transform with fill_mode=wrap is inconsistent with\"\n                 \"scipy\"\n             )\n-        # TODO: `nearest` interpolation and `nearest` fill_mode in torch and jax\n-        # causes random index shifting, resulting in significant differences in\n-        # output which leads to failure\n-        if backend.backend() in (\"torch\", \"jax\") and interpolation == \"nearest\":\n+        # TODO: `nearest` interpolation in jax and torch causes random index\n+        # shifting, resulting in significant differences in output which leads\n+        # to failure\n+        if backend.backend() in (\"jax\", \"torch\") and interpolation == \"nearest\":\n             self.skipTest(\n                 f\"In {backend.backend()} backend, \"\n                 f\"interpolation={interpolation} causes index shifting and \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#75f5d6a76a92e687280c8b56b062e326510f8bb4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 958 | Lines Deleted: 115 | Files Changed: 6 | Hunks: 25 | Methods Changed: 95 | Complexity Δ (Sum/Max): 104/70 | Churn Δ: 1073 | Churn Cumulative: 3117 | Contributors (this commit): 7 | Commits (past 90d): 22 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,10 +12,11 @@ from keras_core.models.sequential import Sequential\n \n class LambdaCallbackTest(testing.TestCase):\n     @pytest.mark.requires_trainable_backend\n-    def test_LambdaCallback(self):\n-        BATCH_SIZE = 4\n+    def test_lambda_callback(self):\n+        \"\"\"Test standard LambdaCallback functionalities with training.\"\"\"\n+        batch_size = 4\n         model = Sequential(\n-            [layers.Input(shape=(2,), batch_size=BATCH_SIZE), layers.Dense(1)]\n+            [layers.Input(shape=(2,), batch_size=batch_size), layers.Dense(1)]\n         )\n         model.compile(\n             optimizer=optimizers.SGD(), loss=losses.MeanSquaredError()\n@@ -34,7 +35,7 @@ class LambdaCallbackTest(testing.TestCase):\n             model.fit(\n                 x,\n                 y,\n-                batch_size=BATCH_SIZE,\n+                batch_size=batch_size,\n                 validation_split=0.2,\n                 callbacks=[lambda_log_callback],\n                 epochs=5,\n@@ -44,3 +45,121 @@ class LambdaCallbackTest(testing.TestCase):\n             self.assertTrue(any(\"on_epoch_begin\" in log for log in logs.output))\n             self.assertTrue(any(\"on_epoch_end\" in log for log in logs.output))\n             self.assertTrue(any(\"on_train_end\" in log for log in logs.output))\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_lambda_callback_with_batches(self):\n+        \"\"\"Test LambdaCallback's behavior with batch-level callbacks.\"\"\"\n+        batch_size = 4\n+        model = Sequential(\n+            [layers.Input(shape=(2,), batch_size=batch_size), layers.Dense(1)]\n+        )\n+        model.compile(\n+            optimizer=optimizers.SGD(), loss=losses.MeanSquaredError()\n+        )\n+        x = np.random.randn(16, 2)\n+        y = np.random.randn(16, 1)\n+        lambda_log_callback = callbacks.LambdaCallback(\n+            on_train_batch_begin=lambda batch, logs: logging.warning(\n+                \"on_train_batch_begin\"\n+            ),\n+            on_train_batch_end=lambda batch, logs: logging.warning(\n+                \"on_train_batch_end\"\n+            ),\n+        )\n+        with self.assertLogs(level=\"WARNING\") as logs:\n+            model.fit(\n+                x,\n+                y,\n+                batch_size=batch_size,\n+                validation_split=0.2,\n+                callbacks=[lambda_log_callback],\n+                epochs=5,\n+                verbose=0,\n+            )\n+            self.assertTrue(\n+                any(\"on_train_batch_begin\" in log for log in logs.output)\n+            )\n+            self.assertTrue(\n+                any(\"on_train_batch_end\" in log for log in logs.output)\n+            )\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_lambda_callback_with_kwargs(self):\n+        \"\"\"Test LambdaCallback's behavior with custom defined callback.\"\"\"\n+        batch_size = 4\n+        model = Sequential(\n+            [layers.Input(shape=(2,), batch_size=batch_size), layers.Dense(1)]\n+        )\n+        model.compile(\n+            optimizer=optimizers.SGD(), loss=losses.MeanSquaredError()\n+        )\n+        x = np.random.randn(16, 2)\n+        y = np.random.randn(16, 1)\n+        model.fit(\n+            x, y, batch_size=batch_size, epochs=1, verbose=0\n+        )  # Train briefly for evaluation to work.\n+\n+        def custom_on_test_begin(logs):\n+            logging.warning(\"custom_on_test_begin_executed\")\n+\n+        lambda_log_callback = callbacks.LambdaCallback(\n+            on_test_begin=custom_on_test_begin\n+        )\n+        with self.assertLogs(level=\"WARNING\") as logs:\n+            model.evaluate(\n+                x,\n+                y,\n+                batch_size=batch_size,\n+                callbacks=[lambda_log_callback],\n+                verbose=0,\n+            )\n+            self.assertTrue(\n+                any(\n+                    \"custom_on_test_begin_executed\" in log\n+                    for log in logs.output\n+                )\n+            )\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_lambda_callback_no_args(self):\n+        \"\"\"Test initializing LambdaCallback without any arguments.\"\"\"\n+        lambda_callback = callbacks.LambdaCallback()\n+        self.assertIsInstance(lambda_callback, callbacks.LambdaCallback)\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_lambda_callback_with_additional_kwargs(self):\n+        \"\"\"Test initializing LambdaCallback with non-predefined kwargs.\"\"\"\n+\n+        def custom_callback(logs):\n+            pass\n+\n+        lambda_callback = callbacks.LambdaCallback(\n+            custom_method=custom_callback\n+        )\n+        self.assertTrue(hasattr(lambda_callback, \"custom_method\"))\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_lambda_callback_during_prediction(self):\n+        \"\"\"Test LambdaCallback's functionality during model prediction.\"\"\"\n+        batch_size = 4\n+        model = Sequential(\n+            [layers.Input(shape=(2,), batch_size=batch_size), layers.Dense(1)]\n+        )\n+        model.compile(\n+            optimizer=optimizers.SGD(), loss=losses.MeanSquaredError()\n+        )\n+        x = np.random.randn(16, 2)\n+\n+        def custom_on_predict_begin(logs):\n+            logging.warning(\"on_predict_begin_executed\")\n+\n+        lambda_callback = callbacks.LambdaCallback(\n+            on_predict_begin=custom_on_predict_begin\n+        )\n+        with self.assertLogs(level=\"WARNING\") as logs:\n+            model.predict(\n+                x, batch_size=batch_size, callbacks=[lambda_callback], verbose=0\n+            )\n+            self.assertTrue(\n+                any(\"on_predict_begin_executed\" in log for log in logs.output)\n+            )\n\n@@ -37,10 +37,9 @@ def save_model_to_hdf5(model, filepath, overwrite=True, include_optimizer=True):\n             if not proceed:\n                 return\n \n-        # Try creating dir if not exist\n         dirpath = os.path.dirname(filepath)\n-        if not os.path.exists(dirpath):\n-            os.path.makedirs(dirpath)\n+        if dirpath and not os.path.exists(dirpath):\n+            os.makedirs(dirpath, exist_ok=True)\n \n         f = h5py.File(filepath, mode=\"w\")\n         opened_new_file = True\n\n@@ -481,3 +481,19 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n \n         # Compare output\n         self.assertAllClose(ref_output, output, atol=1e-5)\n+\n+\n+@pytest.mark.requires_trainable_backend\n+class DirectoryCreationTest(testing.TestCase):\n+    def test_directory_creation_on_save(self):\n+        \"\"\"Test if directory is created on model save.\"\"\"\n+        model = get_sequential_model(keras_core)\n+        nested_dirpath = os.path.join(\n+            self.get_temp_dir(), \"dir1\", \"dir2\", \"dir3\"\n+        )\n+        filepath = os.path.join(nested_dirpath, \"model.h5\")\n+        self.assertFalse(os.path.exists(nested_dirpath))\n+        legacy_h5_format.save_model_to_hdf5(model, filepath)\n+        self.assertTrue(os.path.exists(nested_dirpath))\n+        loaded_model = legacy_h5_format.load_model_from_hdf5(filepath)\n+        self.assertEqual(model.to_json(), loaded_model.to_json())\n\n@@ -0,0 +1,178 @@\n+import os\n+import unittest.mock as mock\n+\n+import numpy as np\n+from absl import logging\n+\n+from keras_core import layers\n+from keras_core.models import Sequential\n+from keras_core.saving import saving_api\n+from keras_core.testing import test_case\n+\n+\n+class SaveModelTests(test_case.TestCase):\n+    def get_model(self):\n+        return Sequential(\n+            [\n+                layers.Dense(5, input_shape=(3,)),\n+                layers.Softmax(),\n+            ]\n+        )\n+\n+    def test_basic_saving(self):\n+        \"\"\"Test basic model saving and loading.\"\"\"\n+        model = self.get_model()\n+        filepath = os.path.join(self.get_temp_dir(), \"test_model.keras\")\n+        saving_api.save_model(model, filepath)\n+\n+        loaded_model = saving_api.load_model(filepath)\n+        x = np.random.uniform(size=(10, 3))\n+        self.assertTrue(np.allclose(model.predict(x), loaded_model.predict(x)))\n+\n+    def test_invalid_save_format(self):\n+        \"\"\"Test deprecated save_format argument.\"\"\"\n+        model = self.get_model()\n+        with self.assertRaisesRegex(\n+            ValueError, \"The `save_format` argument is deprecated\"\n+        ):\n+            saving_api.save_model(model, \"model.txt\", save_format=True)\n+\n+    def test_unsupported_arguments(self):\n+        \"\"\"Test unsupported argument during model save.\"\"\"\n+        model = self.get_model()\n+        filepath = os.path.join(self.get_temp_dir(), \"test_model.keras\")\n+        with self.assertRaisesRegex(\n+            ValueError, r\"The following argument\\(s\\) are not supported\"\n+        ):\n+            saving_api.save_model(model, filepath, random_arg=True)\n+\n+    def test_save_h5_format(self):\n+        \"\"\"Test saving model in h5 format.\"\"\"\n+        model = self.get_model()\n+        filepath_h5 = os.path.join(self.get_temp_dir(), \"test_model.h5\")\n+        saving_api.save_model(model, filepath_h5)\n+        self.assertTrue(os.path.exists(filepath_h5))\n+        os.remove(filepath_h5)\n+\n+    def test_save_unsupported_extension(self):\n+        \"\"\"Test saving model with unsupported extension.\"\"\"\n+        model = self.get_model()\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid filepath extension for saving\"\n+        ):\n+            saving_api.save_model(model, \"model.png\")\n+\n+\n+class LoadModelTests(test_case.TestCase):\n+    def get_model(self):\n+        return Sequential(\n+            [\n+                layers.Dense(5, input_shape=(3,)),\n+                layers.Softmax(),\n+            ]\n+        )\n+\n+    def test_basic_load(self):\n+        \"\"\"Test basic model loading.\"\"\"\n+        model = self.get_model()\n+        filepath = os.path.join(self.get_temp_dir(), \"test_model.keras\")\n+        saving_api.save_model(model, filepath)\n+\n+        loaded_model = saving_api.load_model(filepath)\n+        x = np.random.uniform(size=(10, 3))\n+        self.assertTrue(np.allclose(model.predict(x), loaded_model.predict(x)))\n+\n+    def test_load_unsupported_format(self):\n+        \"\"\"Test loading model with unsupported format.\"\"\"\n+        with self.assertRaisesRegex(ValueError, \"File format not supported\"):\n+            saving_api.load_model(\"model.pkl\")\n+\n+    def test_load_keras_not_zip(self):\n+        \"\"\"Test loading keras file that's not a zip.\"\"\"\n+        with self.assertRaisesRegex(ValueError, \"File not found\"):\n+            saving_api.load_model(\"not_a_zip.keras\")\n+\n+    def test_load_h5_format(self):\n+        \"\"\"Test loading model in h5 format.\"\"\"\n+        model = self.get_model()\n+        filepath_h5 = os.path.join(self.get_temp_dir(), \"test_model.h5\")\n+        saving_api.save_model(model, filepath_h5)\n+        loaded_model = saving_api.load_model(filepath_h5)\n+        x = np.random.uniform(size=(10, 3))\n+        self.assertTrue(np.allclose(model.predict(x), loaded_model.predict(x)))\n+        os.remove(filepath_h5)\n+\n+    def test_load_model_with_custom_objects(self):\n+        \"\"\"Test loading model with custom objects.\"\"\"\n+\n+        class CustomLayer(layers.Layer):\n+            def call(self, inputs):\n+                return inputs\n+\n+        model = Sequential([CustomLayer(input_shape=(3,))])\n+        filepath = os.path.join(self.get_temp_dir(), \"custom_model.keras\")\n+        model.save(filepath)\n+        loaded_model = saving_api.load_model(\n+            filepath, custom_objects={\"CustomLayer\": CustomLayer}\n+        )\n+        self.assertIsInstance(loaded_model.layers[0], CustomLayer)\n+        os.remove(filepath)\n+\n+\n+class LoadWeightsTests(test_case.TestCase):\n+    def get_model(self):\n+        return Sequential(\n+            [\n+                layers.Dense(5, input_shape=(3,)),\n+                layers.Softmax(),\n+            ]\n+        )\n+\n+    def test_load_keras_weights(self):\n+        \"\"\"Test loading keras weights.\"\"\"\n+        model = self.get_model()\n+        filepath = os.path.join(self.get_temp_dir(), \"test_weights.weights.h5\")\n+        model.save_weights(filepath)\n+        original_weights = model.get_weights()\n+        model.load_weights(filepath)\n+        loaded_weights = model.get_weights()\n+        for orig, loaded in zip(original_weights, loaded_weights):\n+            self.assertTrue(np.array_equal(orig, loaded))\n+\n+    def test_load_h5_weights_by_name(self):\n+        \"\"\"Test loading h5 weights by name.\"\"\"\n+        model = self.get_model()\n+        filepath = os.path.join(self.get_temp_dir(), \"test_weights.weights.h5\")\n+        model.save_weights(filepath)\n+        with self.assertRaisesRegex(ValueError, \"Invalid keyword arguments\"):\n+            model.load_weights(filepath, by_name=True)\n+\n+    def test_load_weights_invalid_extension(self):\n+        \"\"\"Test loading weights with unsupported extension.\"\"\"\n+        model = self.get_model()\n+        with self.assertRaisesRegex(ValueError, \"File format not supported\"):\n+            model.load_weights(\"invalid_extension.pkl\")\n+\n+\n+class SaveModelTestsWarning(test_case.TestCase):\n+    def get_model(self):\n+        return Sequential(\n+            [\n+                layers.Dense(5, input_shape=(3,)),\n+                layers.Softmax(),\n+            ]\n+        )\n+\n+    def test_h5_deprecation_warning(self):\n+        \"\"\"Test deprecation warning for h5 format.\"\"\"\n+        model = self.get_model()\n+        filepath = os.path.join(self.get_temp_dir(), \"test_model.h5\")\n+\n+        with mock.patch.object(logging, \"warning\") as mock_warn:\n+            saving_api.save_model(model, filepath)\n+            mock_warn.assert_called_once_with(\n+                \"You are saving your model as an HDF5 file via `model.save()`. \"\n+                \"This file format is considered legacy. \"\n+                \"We recommend using instead the native Keras format, \"\n+                \"e.g. `model.save('my_model.keras')`.\"\n+            )\n\n@@ -386,9 +386,19 @@ def validate_file(fpath, file_hash, algorithm=\"auto\", chunk_size=65535):\n \n \n def is_remote_path(filepath):\n-    \"\"\"Returns `True` for paths that represent a remote GCS location.\"\"\"\n-    # TODO: improve generality.\n-    if re.match(r\"^(/cns|/cfs|/gcs|.*://).*$\", str(filepath)):\n+    \"\"\"\n+    Determines if a given filepath indicates a remote location.\n+\n+    This function checks if the filepath represents a known remote pattern\n+    such as GCS (`/gcs`), CNS (`/cns`), CFS (`/cfs`), HDFS (`/hdfs`)\n+\n+    Args:\n+        filepath (str): The path to be checked.\n+\n+    Returns:\n+        bool: True if the filepath is a recognized remote path, otherwise False\n+    \"\"\"\n+    if re.match(r\"^(/cns|/cfs|/gcs|/hdfs|.*://).*$\", str(filepath)):\n         return True\n     return False\n \n@@ -445,7 +455,7 @@ def rmtree(path):\n             return gfile.rmtree(path)\n         else:\n             _raise_if_no_gfile(path)\n-    return shutil.rmtree\n+    return shutil.rmtree(path)\n \n \n def listdir(path):\n\n@@ -1,125 +1,354 @@\n+import hashlib\n import os\n+import pathlib\n+import shutil\n import tarfile\n+import tempfile\n import urllib\n import zipfile\n+from unittest.mock import patch\n \n from keras_core.testing import test_case\n from keras_core.utils import file_utils\n \n \n-class TestGetFile(test_case.TestCase):\n-    def test_get_file_and_validate_it(self):\n-        \"\"\"Tests get_file from a url, plus extraction and validation.\"\"\"\n+class PathToStringTest(test_case.TestCase):\n+    def test_path_to_string_with_string_path(self):\n+        path = \"/path/to/file.txt\"\n+        string_path = file_utils.path_to_string(path)\n+        self.assertEqual(string_path, path)\n+\n+    def test_path_to_string_with_PathLike_object(self):\n+        path = pathlib.Path(\"/path/to/file.txt\")\n+        string_path = file_utils.path_to_string(path)\n+        self.assertEqual(string_path, str(path))\n+\n+    def test_path_to_string_with_non_string_typed_path_object(self):\n+        class NonStringTypedPathObject:\n+            def __fspath__(self):\n+                return \"/path/to/file.txt\"\n+\n+        path = NonStringTypedPathObject()\n+        string_path = file_utils.path_to_string(path)\n+        self.assertEqual(string_path, \"/path/to/file.txt\")\n+\n+    def test_path_to_string_with_none_path(self):\n+        string_path = file_utils.path_to_string(None)\n+        self.assertEqual(string_path, None)\n+\n+\n+class ResolvePathTest(test_case.TestCase):\n+    def test_resolve_path_with_absolute_path(self):\n+        path = \"/path/to/file.txt\"\n+        resolved_path = file_utils.resolve_path(path)\n+        self.assertEqual(resolved_path, os.path.realpath(os.path.abspath(path)))\n+\n+    def test_resolve_path_with_relative_path(self):\n+        path = \"./file.txt\"\n+        resolved_path = file_utils.resolve_path(path)\n+        self.assertEqual(resolved_path, os.path.realpath(os.path.abspath(path)))\n+\n+\n+class IsPathInDirTest(test_case.TestCase):\n+    def test_is_path_in_dir_with_absolute_paths(self):\n+        base_dir = \"/path/to/base_dir\"\n+        path = \"/path/to/base_dir/file.txt\"\n+        self.assertTrue(file_utils.is_path_in_dir(path, base_dir))\n+\n+\n+class IsLinkInDirTest(test_case.TestCase):\n+    def setUp(self):\n+        self._cleanup(\"test_path/to/base_dir\")\n+        self._cleanup(\"./base_dir\")\n+\n+    def _cleanup(self, base_dir):\n+        if os.path.exists(base_dir):\n+            shutil.rmtree(base_dir)\n+\n+    def test_is_link_in_dir_with_absolute_paths(self):\n+        base_dir = \"test_path/to/base_dir\"\n+        link_path = os.path.join(base_dir, \"symlink\")\n+        target_path = os.path.join(base_dir, \"file.txt\")\n+\n+        # Create the base_dir directory if it does not exist.\n+        os.makedirs(base_dir, exist_ok=True)\n+\n+        # Create the file.txt file.\n+        with open(target_path, \"w\") as f:\n+            f.write(\"Hello, world!\")\n+\n+        os.symlink(target_path, link_path)\n+\n+        # Creating a stat_result-like object with a name attribute\n+        info = os.lstat(link_path)\n+        info = type(\n+            \"stat_with_name\",\n+            (object,),\n+            {\n+                \"name\": os.path.basename(link_path),\n+                \"linkname\": os.readlink(link_path),\n+            },\n+        )\n+\n+        self.assertTrue(file_utils.is_link_in_dir(info, base_dir))\n+\n+    def test_is_link_in_dir_with_relative_paths(self):\n+        base_dir = \"./base_dir\"\n+        link_path = os.path.join(base_dir, \"symlink\")\n+        target_path = os.path.join(base_dir, \"file.txt\")\n+\n+        # Create the base_dir directory if it does not exist.\n+        os.makedirs(base_dir, exist_ok=True)\n+\n+        # Create the file.txt file.\n+        with open(target_path, \"w\") as f:\n+            f.write(\"Hello, world!\")\n+\n+        os.symlink(target_path, link_path)\n+\n+        # Creating a stat_result-like object with a name attribute\n+        info = os.lstat(link_path)\n+        info = type(\n+            \"stat_with_name\",\n+            (object,),\n+            {\n+                \"name\": os.path.basename(link_path),\n+                \"linkname\": os.readlink(link_path),\n+            },\n+        )\n+\n+        self.assertTrue(file_utils.is_link_in_dir(info, base_dir))\n+\n+    def tearDown(self):\n+        self._cleanup(\"test_path/to/base_dir\")\n+        self._cleanup(\"./base_dir\")\n+\n+\n+class FilterSafePathsTest(test_case.TestCase):\n+    def setUp(self):\n+        self.base_dir = os.path.join(os.getcwd(), \"temp_dir\")\n+        os.makedirs(self.base_dir, exist_ok=True)\n+        self.tar_path = os.path.join(self.base_dir, \"test.tar\")\n+\n+    def tearDown(self):\n+        os.remove(self.tar_path)\n+        shutil.rmtree(self.base_dir)\n+\n+    def test_member_within_base_dir(self):\n+        \"\"\"Test a member within the base directory.\"\"\"\n+        with tarfile.open(self.tar_path, \"w\") as tar:\n+            tar.add(__file__, arcname=\"safe_path.txt\")\n+        with tarfile.open(self.tar_path, \"r\") as tar:\n+            members = list(file_utils.filter_safe_paths(tar.getmembers()))\n+            self.assertEqual(len(members), 1)\n+            self.assertEqual(members[0].name, \"safe_path.txt\")\n+\n+    def test_symlink_within_base_dir(self):\n+        \"\"\"Test a symlink pointing within the base directory.\"\"\"\n+        symlink_path = os.path.join(self.base_dir, \"symlink.txt\")\n+        target_path = os.path.join(self.base_dir, \"target.txt\")\n+        with open(target_path, \"w\") as f:\n+            f.write(\"target\")\n+        os.symlink(target_path, symlink_path)\n+        with tarfile.open(self.tar_path, \"w\") as tar:\n+            tar.add(symlink_path, arcname=\"symlink.txt\")\n+        with tarfile.open(self.tar_path, \"r\") as tar:\n+            members = list(file_utils.filter_safe_paths(tar.getmembers()))\n+            self.assertEqual(len(members), 1)\n+            self.assertEqual(members[0].name, \"symlink.txt\")\n+        os.remove(symlink_path)\n+        os.remove(target_path)\n+\n+    def test_invalid_path_warning(self):\n+        \"\"\"Test warning for an invalid path during archive extraction.\"\"\"\n+        invalid_path = os.path.join(os.getcwd(), \"invalid.txt\")\n+        with open(invalid_path, \"w\") as f:\n+            f.write(\"invalid\")\n+        with tarfile.open(self.tar_path, \"w\") as tar:\n+            tar.add(\n+                invalid_path, arcname=\"../../invalid.txt\"\n+            )  # Path intended to be outside of base dir\n+        with tarfile.open(self.tar_path, \"r\") as tar:\n+            with patch(\"warnings.warn\") as mock_warn:\n+                _ = list(file_utils.filter_safe_paths(tar.getmembers()))\n+                warning_msg = (\n+                    \"Skipping invalid path during archive extraction: \"\n+                    \"'../../invalid.txt'.\"\n+                )\n+                mock_warn.assert_called_with(warning_msg, stacklevel=2)\n+        os.remove(invalid_path)\n+\n+    def test_symbolic_link_in_base_dir(self):\n+        \"\"\"symbolic link within the base directory is correctly processed.\"\"\"\n+        symlink_path = os.path.join(self.base_dir, \"symlink.txt\")\n+        target_path = os.path.join(self.base_dir, \"target.txt\")\n+\n+        # Create a target file and then a symbolic link pointing to it.\n+        with open(target_path, \"w\") as f:\n+            f.write(\"target\")\n+        os.symlink(target_path, symlink_path)\n+\n+        # Add the symbolic link to the tar archive.\n+        with tarfile.open(self.tar_path, \"w\") as tar:\n+            tar.add(symlink_path, arcname=\"symlink.txt\")\n+\n+        with tarfile.open(self.tar_path, \"r\") as tar:\n+            members = list(file_utils.filter_safe_paths(tar.getmembers()))\n+            self.assertEqual(len(members), 1)\n+            self.assertEqual(members[0].name, \"symlink.txt\")\n+            self.assertTrue(\n+                members[0].issym()\n+            )  # Explicitly assert it's a symbolic link.\n+\n+        os.remove(symlink_path)\n+        os.remove(target_path)\n+\n+\n+class ExtractArchiveTest(test_case.TestCase):\n+    def setUp(self):\n+        \"\"\"Create temporary directories and files for testing.\"\"\"\n+        self.temp_dir = tempfile.mkdtemp()\n+        self.file_content = \"Hello, world!\"\n+\n+        # Create sample files to be archived\n+        with open(os.path.join(self.temp_dir, \"sample.txt\"), \"w\") as f:\n+            f.write(self.file_content)\n+\n+    def tearDown(self):\n+        \"\"\"Clean up temporary directories.\"\"\"\n+        shutil.rmtree(self.temp_dir)\n+\n+    def create_tar(self):\n+        archive_path = os.path.join(self.temp_dir, \"sample.tar\")\n+        with tarfile.open(archive_path, \"w\") as archive:\n+            archive.add(\n+                os.path.join(self.temp_dir, \"sample.txt\"), arcname=\"sample.txt\"\n+            )\n+        return archive_path\n+\n+    def create_zip(self):\n+        archive_path = os.path.join(self.temp_dir, \"sample.zip\")\n+        with zipfile.ZipFile(archive_path, \"w\") as archive:\n+            archive.write(\n+                os.path.join(self.temp_dir, \"sample.txt\"), arcname=\"sample.txt\"\n+            )\n+        return archive_path\n+\n+    def test_extract_tar(self):\n+        archive_path = self.create_tar()\n+        extract_path = os.path.join(self.temp_dir, \"extract_tar\")\n+        result = file_utils.extract_archive(archive_path, extract_path, \"tar\")\n+        self.assertTrue(result)\n+        with open(os.path.join(extract_path, \"sample.txt\"), \"r\") as f:\n+            self.assertEqual(f.read(), self.file_content)\n+\n+    def test_extract_zip(self):\n+        archive_path = self.create_zip()\n+        extract_path = os.path.join(self.temp_dir, \"extract_zip\")\n+        result = file_utils.extract_archive(archive_path, extract_path, \"zip\")\n+        self.assertTrue(result)\n+        with open(os.path.join(extract_path, \"sample.txt\"), \"r\") as f:\n+            self.assertEqual(f.read(), self.file_content)\n+\n+    def test_extract_auto(self):\n+        # This will test the 'auto' functionality\n+        tar_archive_path = self.create_tar()\n+        zip_archive_path = self.create_zip()\n+\n+        extract_tar_path = os.path.join(self.temp_dir, \"extract_auto_tar\")\n+        extract_zip_path = os.path.join(self.temp_dir, \"extract_auto_zip\")\n+\n+        self.assertTrue(\n+            file_utils.extract_archive(tar_archive_path, extract_tar_path)\n+        )\n+        self.assertTrue(\n+            file_utils.extract_archive(zip_archive_path, extract_zip_path)\n+        )\n+\n+        with open(os.path.join(extract_tar_path, \"sample.txt\"), \"r\") as f:\n+            self.assertEqual(f.read(), self.file_content)\n+\n+        with open(os.path.join(extract_zip_path, \"sample.txt\"), \"r\") as f:\n+            self.assertEqual(f.read(), self.file_content)\n+\n+    def test_non_existent_file(self):\n+        extract_path = os.path.join(self.temp_dir, \"non_existent\")\n+        with self.assertRaises(FileNotFoundError):\n+            file_utils.extract_archive(\"non_existent.tar\", extract_path)\n+\n+    def test_archive_format_none(self):\n+        archive_path = self.create_tar()\n+        extract_path = os.path.join(self.temp_dir, \"none_format\")\n+        result = file_utils.extract_archive(archive_path, extract_path, None)\n+        self.assertFalse(result)\n+\n+    def test_runtime_error_during_extraction(self):\n+        tar_path = self.create_tar()\n+        extract_path = os.path.join(self.temp_dir, \"runtime_error_extraction\")\n+\n+        with patch.object(\n+            tarfile.TarFile, \"extractall\", side_effect=RuntimeError\n+        ):\n+            with self.assertRaises(RuntimeError):\n+                file_utils.extract_archive(tar_path, extract_path, \"tar\")\n+        self.assertFalse(os.path.exists(extract_path))\n+\n+    def test_keyboard_interrupt_during_extraction(self):\n+        tar_path = self.create_tar()\n+        extract_path = os.path.join(\n+            self.temp_dir, \"keyboard_interrupt_extraction\"\n+        )\n+\n+        with patch.object(\n+            tarfile.TarFile, \"extractall\", side_effect=KeyboardInterrupt\n+        ):\n+            with self.assertRaises(KeyboardInterrupt):\n+                file_utils.extract_archive(tar_path, extract_path, \"tar\")\n+        self.assertFalse(os.path.exists(extract_path))\n+\n+\n+class GetFileTest(test_case.TestCase):\n+    def setUp(self):\n+        \"\"\"Set up temporary directories and sample files.\"\"\"\n+        self.temp_dir = self.get_temp_dir()\n+        self.file_path = os.path.join(self.temp_dir, \"sample_file.txt\")\n+        with open(self.file_path, \"w\") as f:\n+            f.write(\"Sample content\")\n+\n+    def test_valid_tar_extraction(self):\n+        \"\"\"Test valid tar.gz extraction and hash validation.\"\"\"\n         dest_dir = self.get_temp_dir()\n         orig_dir = self.get_temp_dir()\n+        text_file_path, tar_file_path = self._create_tar_file(orig_dir)\n+        self._test_file_extraction_and_validation(\n+            dest_dir, tar_file_path, \"tar.gz\"\n+        )\n \n+    def test_valid_zip_extraction(self):\n+        \"\"\"Test valid zip extraction and hash validation.\"\"\"\n+        dest_dir = self.get_temp_dir()\n+        orig_dir = self.get_temp_dir()\n+        text_file_path, zip_file_path = self._create_zip_file(orig_dir)\n+        self._test_file_extraction_and_validation(\n+            dest_dir, zip_file_path, \"zip\"\n+        )\n+\n+    def test_valid_text_file_download(self):\n+        \"\"\"Test valid text file download and hash validation.\"\"\"\n+        dest_dir = self.get_temp_dir()\n+        orig_dir = self.get_temp_dir()\n         text_file_path = os.path.join(orig_dir, \"test.txt\")\n-        zip_file_path = os.path.join(orig_dir, \"test.zip\")\n-        tar_file_path = os.path.join(orig_dir, \"test.tar.gz\")\n-\n         with open(text_file_path, \"w\") as text_file:\n             text_file.write(\"Float like a butterfly, sting like a bee.\")\n-\n-        with tarfile.open(tar_file_path, \"w:gz\") as tar_file:\n-            tar_file.add(text_file_path)\n-\n-        with zipfile.ZipFile(zip_file_path, \"w\") as zip_file:\n-            zip_file.write(text_file_path)\n-\n-        origin = urllib.parse.urljoin(\n-            \"file://\",\n-            urllib.request.pathname2url(os.path.abspath(tar_file_path)),\n+        self._test_file_extraction_and_validation(\n+            dest_dir, text_file_path, None\n         )\n \n-        path = file_utils.get_file(\n-            \"test.txt\", origin, untar=True, cache_subdir=dest_dir\n-        )\n-        filepath = path + \".tar.gz\"\n-        hashval_sha256 = file_utils.hash_file(filepath)\n-        hashval_md5 = file_utils.hash_file(filepath, algorithm=\"md5\")\n-        path = file_utils.get_file(\n-            \"test.txt\",\n-            origin,\n-            md5_hash=hashval_md5,\n-            untar=True,\n-            cache_subdir=dest_dir,\n-        )\n-        path = file_utils.get_file(\n-            filepath,\n-            origin,\n-            file_hash=hashval_sha256,\n-            extract=True,\n-            cache_subdir=dest_dir,\n-        )\n-        self.assertTrue(os.path.exists(filepath))\n-        self.assertTrue(file_utils.validate_file(filepath, hashval_sha256))\n-        self.assertTrue(file_utils.validate_file(filepath, hashval_md5))\n-        os.remove(filepath)\n-\n-        origin = urllib.parse.urljoin(\n-            \"file://\",\n-            urllib.request.pathname2url(os.path.abspath(zip_file_path)),\n-        )\n-\n-        hashval_sha256 = file_utils.hash_file(zip_file_path)\n-        hashval_md5 = file_utils.hash_file(zip_file_path, algorithm=\"md5\")\n-        path = file_utils.get_file(\n-            \"test\",\n-            origin,\n-            md5_hash=hashval_md5,\n-            extract=True,\n-            cache_subdir=dest_dir,\n-        )\n-        path = file_utils.get_file(\n-            \"test\",\n-            origin,\n-            file_hash=hashval_sha256,\n-            extract=True,\n-            cache_subdir=dest_dir,\n-        )\n-        self.assertTrue(os.path.exists(path))\n-        self.assertTrue(file_utils.validate_file(path, hashval_sha256))\n-        self.assertTrue(file_utils.validate_file(path, hashval_md5))\n-        os.remove(path)\n-\n-        for file_path, extract in [\n-            (text_file_path, False),\n-            (tar_file_path, True),\n-            (zip_file_path, True),\n-        ]:\n-            origin = urllib.parse.urljoin(\n-                \"file://\",\n-                urllib.request.pathname2url(os.path.abspath(file_path)),\n-            )\n-            hashval_sha256 = file_utils.hash_file(file_path)\n-            path = file_utils.get_file(\n-                origin=origin,\n-                file_hash=hashval_sha256,\n-                extract=extract,\n-                cache_subdir=dest_dir,\n-            )\n-            self.assertTrue(os.path.exists(path))\n-            self.assertTrue(file_utils.validate_file(path, hashval_sha256))\n-            os.remove(path)\n-\n-        with self.assertRaisesRegexp(\n-            ValueError, 'Please specify the \"origin\".*'\n-        ):\n-            _ = file_utils.get_file()\n-\n     def test_get_file_with_tgz_extension(self):\n-        \"\"\"Tests get_file from a url, plus extraction and validation.\"\"\"\n+        \"\"\"Test extraction of file with .tar.gz extension.\"\"\"\n         dest_dir = self.get_temp_dir()\n         orig_dir = dest_dir\n-\n-        text_file_path = os.path.join(orig_dir, \"test.txt\")\n-        tar_file_path = os.path.join(orig_dir, \"test.tar.gz\")\n-\n-        with open(text_file_path, \"w\") as text_file:\n-            text_file.write(\"Float like a butterfly, sting like a bee.\")\n-\n-        with tarfile.open(tar_file_path, \"w:gz\") as tar_file:\n-            tar_file.add(text_file_path)\n+        text_file_path, tar_file_path = self._create_tar_file(orig_dir)\n \n         origin = urllib.parse.urljoin(\n             \"file://\",\n@@ -133,7 +362,7 @@ class TestGetFile(test_case.TestCase):\n         self.assertTrue(os.path.exists(path))\n \n     def test_get_file_with_integrity_check(self):\n-        \"\"\"Tests get_file with validation before download.\"\"\"\n+        \"\"\"Test file download with integrity check.\"\"\"\n         orig_dir = self.get_temp_dir()\n         file_path = os.path.join(orig_dir, \"test.txt\")\n \n@@ -150,7 +379,7 @@ class TestGetFile(test_case.TestCase):\n         self.assertTrue(os.path.exists(path))\n \n     def test_get_file_with_failed_integrity_check(self):\n-        \"\"\"Tests get_file with validation before download.\"\"\"\n+        \"\"\"Test file download with failed integrity check.\"\"\"\n         orig_dir = self.get_temp_dir()\n         file_path = os.path.join(orig_dir, \"test.txt\")\n \n@@ -167,3 +396,295 @@ class TestGetFile(test_case.TestCase):\n             ValueError, \"Incomplete or corrupted file.*\"\n         ):\n             _ = file_utils.get_file(\"test.txt\", origin, file_hash=hashval)\n+\n+    def _create_tar_file(self, directory):\n+        \"\"\"Helper function to create a tar file.\"\"\"\n+        text_file_path = os.path.join(directory, \"test.txt\")\n+        tar_file_path = os.path.join(directory, \"test.tar.gz\")\n+        with open(text_file_path, \"w\") as text_file:\n+            text_file.write(\"Float like a butterfly, sting like a bee.\")\n+\n+        with tarfile.open(tar_file_path, \"w:gz\") as tar_file:\n+            tar_file.add(text_file_path)\n+\n+        return text_file_path, tar_file_path\n+\n+    def _create_zip_file(self, directory):\n+        \"\"\"Helper function to create a zip file.\"\"\"\n+        text_file_path = os.path.join(directory, \"test.txt\")\n+        zip_file_path = os.path.join(directory, \"test.zip\")\n+        with open(text_file_path, \"w\") as text_file:\n+            text_file.write(\"Float like a butterfly, sting like a bee.\")\n+\n+        with zipfile.ZipFile(zip_file_path, \"w\") as zip_file:\n+            zip_file.write(text_file_path)\n+\n+        return text_file_path, zip_file_path\n+\n+    def _test_file_extraction_and_validation(\n+        self, dest_dir, file_path, archive_type\n+    ):\n+        \"\"\"Helper function for file extraction and validation.\"\"\"\n+        origin = urllib.parse.urljoin(\n+            \"file://\",\n+            urllib.request.pathname2url(os.path.abspath(file_path)),\n+        )\n+\n+        hashval_sha256 = file_utils.hash_file(file_path)\n+        hashval_md5 = file_utils.hash_file(file_path, algorithm=\"md5\")\n+\n+        if archive_type:\n+            extract = True\n+        else:\n+            extract = False\n+\n+        path = file_utils.get_file(\n+            \"test\",\n+            origin,\n+            md5_hash=hashval_md5,\n+            extract=extract,\n+            cache_subdir=dest_dir,\n+        )\n+        path = file_utils.get_file(\n+            \"test\",\n+            origin,\n+            file_hash=hashval_sha256,\n+            extract=extract,\n+            cache_subdir=dest_dir,\n+        )\n+        self.assertTrue(os.path.exists(path))\n+        self.assertTrue(file_utils.validate_file(path, hashval_sha256))\n+        self.assertTrue(file_utils.validate_file(path, hashval_md5))\n+        os.remove(path)\n+\n+    def test_exists(self):\n+        temp_dir = self.get_temp_dir()\n+        file_path = os.path.join(temp_dir, \"test_exists.txt\")\n+\n+        with open(file_path, \"w\") as f:\n+            f.write(\"test\")\n+\n+        self.assertTrue(file_utils.exists(file_path))\n+        self.assertFalse(\n+            file_utils.exists(os.path.join(temp_dir, \"non_existent.txt\"))\n+        )\n+\n+    def test_file_open_read(self):\n+        temp_dir = self.get_temp_dir()\n+        file_path = os.path.join(temp_dir, \"test_file.txt\")\n+        content = \"test content\"\n+\n+        with open(file_path, \"w\") as f:\n+            f.write(content)\n+\n+        with file_utils.File(file_path, \"r\") as f:\n+            self.assertEqual(f.read(), content)\n+\n+    def test_file_open_write(self):\n+        temp_dir = self.get_temp_dir()\n+        file_path = os.path.join(temp_dir, \"test_file_write.txt\")\n+        content = \"test write content\"\n+\n+        with file_utils.File(file_path, \"w\") as f:\n+            f.write(content)\n+\n+        with open(file_path, \"r\") as f:\n+            self.assertEqual(f.read(), content)\n+\n+    def test_isdir(self):\n+        temp_dir = self.get_temp_dir()\n+        self.assertTrue(file_utils.isdir(temp_dir))\n+\n+        file_path = os.path.join(temp_dir, \"test_isdir.txt\")\n+        with open(file_path, \"w\") as f:\n+            f.write(\"test\")\n+        self.assertFalse(file_utils.isdir(file_path))\n+\n+    def test_join_simple(self):\n+        self.assertEqual(file_utils.join(\"/path\", \"to\", \"dir\"), \"/path/to/dir\")\n+\n+    def test_join_single_directory(self):\n+        self.assertEqual(file_utils.join(\"/path\"), \"/path\")\n+\n+    def test_listdir(self):\n+        content = file_utils.listdir(self.temp_dir)\n+        self.assertIn(\"sample_file.txt\", content)\n+\n+    def test_makedirs_and_rmtree(self):\n+        new_dir = os.path.join(self.temp_dir, \"new_directory\")\n+        file_utils.makedirs(new_dir)\n+        self.assertTrue(os.path.isdir(new_dir))\n+        file_utils.rmtree(new_dir)\n+        self.assertFalse(os.path.exists(new_dir))\n+\n+    def test_copy(self):\n+        dest_path = os.path.join(self.temp_dir, \"copy_sample_file.txt\")\n+        file_utils.copy(self.file_path, dest_path)\n+        self.assertTrue(os.path.exists(dest_path))\n+        with open(dest_path, \"r\") as f:\n+            content = f.read()\n+        self.assertEqual(content, \"Sample content\")\n+\n+    def test_remove_sub_directory(self):\n+        parent_dir = os.path.join(self.get_temp_dir(), \"parent_directory\")\n+        child_dir = os.path.join(parent_dir, \"child_directory\")\n+        file_utils.makedirs(child_dir)\n+        file_utils.rmtree(parent_dir)\n+        self.assertFalse(os.path.exists(parent_dir))\n+        self.assertFalse(os.path.exists(child_dir))\n+\n+    def test_remove_files_inside_directory(self):\n+        dir_path = os.path.join(self.get_temp_dir(), \"test_directory\")\n+        file_path = os.path.join(dir_path, \"test.txt\")\n+        file_utils.makedirs(dir_path)\n+        with open(file_path, \"w\") as f:\n+            f.write(\"Test content\")\n+        file_utils.rmtree(dir_path)\n+        self.assertFalse(os.path.exists(dir_path))\n+        self.assertFalse(os.path.exists(file_path))\n+\n+    def test_handle_complex_paths(self):\n+        complex_dir = os.path.join(self.get_temp_dir(), \"complex dir@#%&!\")\n+        file_utils.makedirs(complex_dir)\n+        file_utils.rmtree(complex_dir)\n+        self.assertFalse(os.path.exists(complex_dir))\n+\n+\n+class HashFileTest(test_case.TestCase):\n+    def setUp(self):\n+        self.test_content = b\"Hello, World!\"\n+        self.temp_file = tempfile.NamedTemporaryFile(delete=False)\n+        self.temp_file.write(self.test_content)\n+        self.temp_file.close()\n+\n+    def tearDown(self):\n+        os.remove(self.temp_file.name)\n+\n+    def test_hash_file_sha256(self):\n+        \"\"\"Test SHA256 hashing of a file.\"\"\"\n+        expected_sha256 = (\n+            \"dffd6021bb2bd5b0af676290809ec3a53191dd81c7f70a4b28688a362182986f\"\n+        )\n+        calculated_sha256 = file_utils.hash_file(\n+            self.temp_file.name, algorithm=\"sha256\"\n+        )\n+        self.assertEqual(expected_sha256, calculated_sha256)\n+\n+    def test_hash_file_md5(self):\n+        \"\"\"Test MD5 hashing of a file.\"\"\"\n+        expected_md5 = \"65a8e27d8879283831b664bd8b7f0ad4\"\n+        calculated_md5 = file_utils.hash_file(\n+            self.temp_file.name, algorithm=\"md5\"\n+        )\n+        self.assertEqual(expected_md5, calculated_md5)\n+\n+\n+class TestValidateFile(test_case.TestCase):\n+    def setUp(self):\n+        self.tmp_file = tempfile.NamedTemporaryFile(delete=False)\n+        self.tmp_file.write(b\"Hello, World!\")\n+        self.tmp_file.close()\n+\n+        self.sha256_hash = (\n+            \"dffd6021bb2bd5b0af676290809ec3a53191dd81c7f70a4b28688a362182986f\"\n+        )\n+        self.md5_hash = \"65a8e27d8879283831b664bd8b7f0ad4\"\n+\n+    def test_validate_file_sha256(self):\n+        \"\"\"Validate SHA256 hash of a file.\"\"\"\n+        self.assertTrue(\n+            file_utils.validate_file(\n+                self.tmp_file.name, self.sha256_hash, \"sha256\"\n+            )\n+        )\n+\n+    def test_validate_file_md5(self):\n+        \"\"\"Validate MD5 hash of a file.\"\"\"\n+        self.assertTrue(\n+            file_utils.validate_file(self.tmp_file.name, self.md5_hash, \"md5\")\n+        )\n+\n+    def test_validate_file_auto_sha256(self):\n+        \"\"\"Auto-detect and validate SHA256 hash.\"\"\"\n+        self.assertTrue(\n+            file_utils.validate_file(\n+                self.tmp_file.name, self.sha256_hash, \"auto\"\n+            )\n+        )\n+\n+    def test_validate_file_auto_md5(self):\n+        \"\"\"Auto-detect and validate MD5 hash.\"\"\"\n+        self.assertTrue(\n+            file_utils.validate_file(self.tmp_file.name, self.md5_hash, \"auto\")\n+        )\n+\n+    def test_validate_file_wrong_hash(self):\n+        \"\"\"Test validation with incorrect hash.\"\"\"\n+        wrong_hash = \"deadbeef\" * 8\n+        self.assertFalse(\n+            file_utils.validate_file(self.tmp_file.name, wrong_hash, \"sha256\")\n+        )\n+\n+    def tearDown(self):\n+        os.remove(self.tmp_file.name)\n+\n+\n+class ResolveHasherTest(test_case.TestCase):\n+    def test_resolve_hasher_sha256(self):\n+        \"\"\"Test resolving hasher for sha256 algorithm.\"\"\"\n+        hasher = file_utils.resolve_hasher(\"sha256\")\n+        self.assertIsInstance(hasher, type(hashlib.sha256()))\n+\n+    def test_resolve_hasher_auto_sha256(self):\n+        \"\"\"Auto-detect and resolve hasher for sha256.\"\"\"\n+        hasher = file_utils.resolve_hasher(\"auto\", file_hash=\"a\" * 64)\n+        self.assertIsInstance(hasher, type(hashlib.sha256()))\n+\n+    def test_resolve_hasher_auto_md5(self):\n+        \"\"\"Auto-detect and resolve hasher for md5.\"\"\"\n+        hasher = file_utils.resolve_hasher(\"auto\", file_hash=\"a\" * 32)\n+        self.assertIsInstance(hasher, type(hashlib.md5()))\n+\n+    def test_resolve_hasher_default(self):\n+        \"\"\"Resolve hasher with a random algorithm value.\"\"\"\n+        hasher = file_utils.resolve_hasher(\"random_value\")\n+        self.assertIsInstance(hasher, type(hashlib.md5()))\n+\n+\n+class IsRemotePathTest(test_case.TestCase):\n+    def test_gcs_remote_path(self):\n+        self.assertTrue(file_utils.is_remote_path(\"/gcs/some/path/to/file.txt\"))\n+        self.assertTrue(file_utils.is_remote_path(\"/gcs/another/directory/\"))\n+        self.assertTrue(file_utils.is_remote_path(\"gcs://bucket/some/file.txt\"))\n+\n+    def test_hdfs_remote_path(self):\n+        self.assertTrue(file_utils.is_remote_path(\"hdfs://some/path/on/hdfs\"))\n+        self.assertTrue(file_utils.is_remote_path(\"/hdfs/some/local/path\"))\n+\n+    def test_cns_remote_path(self):\n+        self.assertTrue(file_utils.is_remote_path(\"/cns/some/path\"))\n+\n+    def test_cfs_remote_path(self):\n+        self.assertTrue(file_utils.is_remote_path(\"/cfs/some/path\"))\n+\n+    def test_non_remote_paths(self):\n+        self.assertFalse(file_utils.is_remote_path(\"/local/path/to/file.txt\"))\n+        self.assertFalse(\n+            file_utils.is_remote_path(\"C:\\\\local\\\\path\\\\on\\\\windows\\\\file.txt\")\n+        )\n+        self.assertFalse(file_utils.is_remote_path(\"~/relative/path/\"))\n+        self.assertFalse(file_utils.is_remote_path(\"./another/relative/path\"))\n+        self.assertFalse(file_utils.is_remote_path(\"/local/path\"))\n+        self.assertFalse(file_utils.is_remote_path(\"./relative/path\"))\n+        self.assertFalse(file_utils.is_remote_path(\"~/relative/path\"))\n+\n+\n+class TestRaiseIfNoGFile(test_case.TestCase):\n+    def test_raise_if_no_gfile_raises_correct_message(self):\n+        path = \"gs://bucket/some/file.txt\"\n+        expected_error_msg = (\n+            \"Handling remote paths requires installing TensorFlow \"\n+            f\".*Received path: {path}\"\n+        )\n+        with self.assertRaisesRegex(ValueError, expected_error_msg):\n+            file_utils._raise_if_no_gfile(path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
