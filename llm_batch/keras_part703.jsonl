{"custom_id": "keras#0f837a83118c8f8177df25e80570761be9ccdbbe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c8c57d9a098a7e44e1a47e712e9fafef9344fb4b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 47 | Files Changed: 2 | Hunks: 14 | Methods Changed: 5 | Complexity Δ (Sum/Max): -5/1 | Churn Δ: 113 | Churn Cumulative: 1215 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 6 | DMM Complexity: 0.9090909090909091\n\nDIFF:\n@@ -18,7 +18,7 @@ except ImportError:\n @keras_core_export(\n     [\"keras_core.saving.save_model\", \"keras_core.models.save_model\"]\n )\n-def save_model(model, filepath, overwrite=True, save_format=None, **kwargs):\n+def save_model(model, filepath, overwrite=True, **kwargs):\n     \"\"\"Saves a model as a `.keras` file.\n \n     Args:\n@@ -52,8 +52,25 @@ def save_model(model, filepath, overwrite=True, save_format=None, **kwargs):\n \n     Thus models can be reinstantiated in the exact same state.\n     \"\"\"\n-    save_format = get_save_format(filepath, save_format)\n     include_optimizer = kwargs.pop(\"include_optimizer\", True)\n+    save_format = kwargs.pop(\"save_format\", False)\n+    if save_format:\n+        if str(filepath).endswith((\".h5\", \".hdf5\")) or str(filepath).endswith(\n+            \".keras\"\n+        ):\n+            logging.warning(\n+                \"The `save_format` argument is deprecated in Keras Core. \"\n+                \"We recommend removing this argument as it can be inferred \"\n+                \"from the file path. \"\n+                f\"Received: save_format={save_format}\"\n+            )\n+        else:\n+            raise ValueError(\n+                \"The `save_format` argument is deprecated in Keras Core. \"\n+                \"Please remove this argument and pass a file path with \"\n+                \"either `.keras` or `.h5` extension.\"\n+                f\"Received: save_format={save_format}\"\n+            )\n     if kwargs:\n         raise ValueError(\n             \"The following argument(s) are not supported: \"\n@@ -61,7 +78,7 @@ def save_model(model, filepath, overwrite=True, save_format=None, **kwargs):\n         )\n \n     # Deprecation warnings\n-    if save_format == \"h5\":\n+    if str(filepath).endswith((\".h5\", \".hdf5\")):\n         logging.warning(\n             \"You are saving your model as an HDF5 file via `model.save()`. \"\n             \"This file format is considered legacy. \"\n@@ -69,7 +86,7 @@ def save_model(model, filepath, overwrite=True, save_format=None, **kwargs):\n             \"e.g. `model.save('my_model.keras')`.\"\n         )\n \n-    if save_format == \"keras\":\n+    if str(filepath).endswith(\".keras\"):\n         # If file exists and should not be overwritten.\n         try:\n             exists = os.path.exists(filepath)\n@@ -80,16 +97,17 @@ def save_model(model, filepath, overwrite=True, save_format=None, **kwargs):\n             if not proceed:\n                 return\n         saving_lib.save_model(model, filepath)\n-    elif save_format == \"h5\":\n+    elif str(filepath).endswith((\".h5\", \".hdf5\")):\n         legacy_h5_format.save_model_to_hdf5(\n             model, filepath, overwrite, include_optimizer\n         )\n     else:\n-        # TODO(nkovela): Replace code route when SavedModel format is supported\n         raise ValueError(\n             \"Invalid filepath extension for saving. \"\n             \"Please add either a `.keras` extension for the native Keras \"\n             f\"format (recommended) or a `.h5` extension. \"\n+            \"Use `tf.saved_model.save()` if you want to export a SavedModel \"\n+            \"for use with TFLite/TFServing/etc. \"\n             f\"Received: filepath = {filepath}.\"\n         )\n \n@@ -133,8 +151,6 @@ def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n     It is recommended that you use layer attributes to\n     access specific variables, e.g. `model.get_layer(\"dense_1\").kernel`.\n     \"\"\"\n-    save_format = get_save_format(filepath, save_format=None)\n-\n     is_keras_zip = str(filepath).endswith(\".keras\") and zipfile.is_zipfile(\n         filepath\n     )\n@@ -164,12 +180,21 @@ def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n             compile=compile,\n             safe_mode=safe_mode,\n         )\n-    if save_format == \"h5\":\n+    if str(filepath).endswith((\".h5\", \".hdf5\")):\n         return legacy_h5_format.load_model_from_hdf5(filepath)\n+    elif str(filepath).endswith(\".keras\"):\n+        raise ValueError(\n+            f\"File not found: filepath={filepath}. \"\n+            \"Please ensure the file is an accessible `.keras` \"\n+            \"zip file.\"\n+        )\n     else:\n-        # TODO(nkovela): Replace code route when SavedModel format is supported\n-        raise NotImplementedError(\n-            \"The SavedModel format is not currently supported.\"\n+        raise ValueError(\n+            f\"File format not supported: filepath={filepath}. \"\n+            \"Keras Core only supports V3 `.keras` files and \"\n+            \"legacy H5 format files (`.h5` extension). \"\n+            \"Note that the legacy SavedModel format is not \"\n+            \"supported in Keras Core.\"\n         )\n \n \n@@ -209,38 +234,3 @@ def load_weights(model, filepath, skip_mismatch=False, **kwargs):\n             \"Keras Core only supports V3 `.keras` and `.weights.h5` \"\n             \"files.\"\n         )\n-\n-\n-def get_save_format(filepath, save_format):\n-    if save_format:\n-        if save_format == \"keras_v3\":\n-            return \"keras\"\n-        if save_format == \"keras\":\n-            return \"keras\"\n-        else:\n-            return \"h5\"\n-        if save_format in (\"h5\", \"hdf5\"):\n-            return \"h5\"\n-        if save_format in (\"tf\", \"tensorflow\"):\n-            return \"tf\"\n-\n-        raise ValueError(\n-            \"Unknown `save_format` argument. Expected one of \"\n-            \"'keras', 'tf', or 'h5'. \"\n-            f\"Received: save_format{save_format}\"\n-        )\n-\n-    # No save format specified: infer from filepath.\n-\n-    if str(filepath).endswith(\".keras\"):\n-        return \"keras\"\n-    else:\n-        return \"h5\"\n-\n-    if str(filepath).endswith((\".h5\", \".hdf5\")):\n-        return \"h5\"\n-\n-    if h5py is not None and isinstance(filepath, h5py.File):\n-        return \"h5\"\n-\n-    return \"tf\"\n\n@@ -527,6 +527,35 @@ class SavingTest(testing.TestCase):\n             np.array(new_model.layers[2].kernel), new_layer_kernel_value\n         )\n \n+    def test_api_errors(self):\n+        from keras_core.saving import saving_api\n+\n+        model = _get_basic_functional_model()\n+\n+        # Saving API errors\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"mymodel\")\n+        with self.assertRaisesRegex(ValueError, \"argument is deprecated\"):\n+            saving_api.save_model(model, temp_filepath, save_format=\"keras\")\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"mymodel.notkeras\")\n+        with self.assertRaisesRegex(ValueError, \"Invalid filepath extension\"):\n+            saving_api.save_model(model, temp_filepath)\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"mymodel.keras\")\n+        with self.assertRaisesRegex(ValueError, \"are not supported\"):\n+            saving_api.save_model(model, temp_filepath, invalid_arg=\"hello\")\n+\n+        # Loading API errors\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"non_existent.keras\")\n+        with self.assertRaisesRegex(\n+            ValueError, \"Please ensure the file is an accessible\"\n+        ):\n+            _ = saving_api.load_model(temp_filepath)\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"my_saved_model\")\n+        with self.assertRaisesRegex(ValueError, \"File format not supported\"):\n+            _ = saving_api.load_model(temp_filepath)\n+\n \n # def test_safe_mode(self):\n #     temp_filepath = os.path.join(self.get_temp_dir(), \"unsafe_model.keras\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7a67db3dcb3f076774a70a1a00fc3c0e56b6e2cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 490 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 13 | Complexity Δ (Sum/Max): 20/20 | Churn Δ: 490 | Churn Cumulative: 490 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,490 @@\n+\"\"\"\n+Title: English-to-Spanish translation with a sequence-to-sequence Transformer\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2021/05/26\n+Last modified: 2023/02/25\n+Description: Implementing a sequence-to-sequence Transformer and training it on a machine translation task.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+In this example, we'll build a sequence-to-sequence Transformer model, which\n+we'll train on an English-to-Spanish machine translation task.\n+\n+You'll learn how to:\n+\n+- Vectorize text using the Keras `TextVectorization` layer.\n+- Implement a `TransformerEncoder` layer, a `TransformerDecoder` layer,\n+and a `PositionalEmbedding` layer.\n+- Prepare data for training a sequence-to-sequence model.\n+- Use the trained model to generate translations of never-seen-before\n+input sentences (sequence-to-sequence inference).\n+\n+The code featured here is adapted from the book\n+[Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)\n+(chapter 11: Deep learning for text).\n+The present example is fairly barebones, so for detailed explanations of\n+how each building block works, as well as the theory behind Transformers,\n+I recommend reading the book.\n+\"\"\"\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+# We set the backend to TensorFlow. The code works with\n+# both `tensorflow` and `torch`. It does not work with JAX\n+# due to the behavior of `jax.numpy.tile` in a jit scope\n+# (used in `TransformerDecoder.get_causal_attention_mask()`:\n+# `tile` in JAX does not support a dynamic `reps` argument.\n+# You can make the code work in JAX by wrapping the\n+# inside of the `get_causal_attention_mask` method in\n+# a decorator to prevent jit compilation:\n+# `with jax.ensure_compile_time_eval():`.\n+import os\n+os[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+import pathlib\n+import random\n+import string\n+import re\n+import numpy as np\n+\n+import tensorflow.data as tf_data\n+import tensorflow.strings as tf_strings\n+\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core import ops\n+from keras_core.layers import TextVectorization\n+\n+\"\"\"\n+## Downloading the data\n+\n+We'll be working with an English-to-Spanish translation dataset\n+provided by [Anki](https://www.manythings.org/anki/). Let's download it:\n+\"\"\"\n+\n+text_file = keras.utils.get_file(\n+    fname=\"spa-eng.zip\",\n+    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n+    extract=True,\n+)\n+text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\n+\n+\"\"\"\n+## Parsing the data\n+\n+Each line contains an English sentence and its corresponding Spanish sentence.\n+The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n+We prepend the token `\"[start]\"` and we append the token `\"[end]\"` to the Spanish sentence.\n+\"\"\"\n+\n+with open(text_file) as f:\n+    lines = f.read().split(\"\\n\")[:-1]\n+text_pairs = []\n+for line in lines:\n+    eng, spa = line.split(\"\\t\")\n+    spa = \"[start] \" + spa + \" [end]\"\n+    text_pairs.append((eng, spa))\n+\n+\"\"\"\n+Here's what our sentence pairs look like:\n+\"\"\"\n+\n+for _ in range(5):\n+    print(random.choice(text_pairs))\n+\n+\"\"\"\n+Now, let's split the sentence pairs into a training set, a validation set,\n+and a test set.\n+\"\"\"\n+\n+random.shuffle(text_pairs)\n+num_val_samples = int(0.15 * len(text_pairs))\n+num_train_samples = len(text_pairs) - 2 * num_val_samples\n+train_pairs = text_pairs[:num_train_samples]\n+val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n+test_pairs = text_pairs[num_train_samples + num_val_samples :]\n+\n+print(f\"{len(text_pairs)} total pairs\")\n+print(f\"{len(train_pairs)} training pairs\")\n+print(f\"{len(val_pairs)} validation pairs\")\n+print(f\"{len(test_pairs)} test pairs\")\n+\n+\"\"\"\n+## Vectorizing the text data\n+\n+We'll use two instances of the `TextVectorization` layer to vectorize the text\n+data (one for English and one for Spanish),\n+that is to say, to turn the original strings into integer sequences\n+where each integer represents the index of a word in a vocabulary.\n+\n+The English layer will use the default string standardization (strip punctuation characters)\n+and splitting scheme (split on whitespace), while\n+the Spanish layer will use a custom standardization, where we add the character\n+`\"¿\"` to the set of punctuation characters to be stripped.\n+\n+Note: in a production-grade machine translation model, I would not recommend\n+stripping the punctuation characters in either language. Instead, I would recommend turning\n+each punctuation character into its own token,\n+which you could achieve by providing a custom `split` function to the `TextVectorization` layer.\n+\"\"\"\n+\n+strip_chars = string.punctuation + \"¿\"\n+strip_chars = strip_chars.replace(\"[\", \"\")\n+strip_chars = strip_chars.replace(\"]\", \"\")\n+\n+vocab_size = 15000\n+sequence_length = 20\n+batch_size = 64\n+\n+\n+def custom_standardization(input_string):\n+    lowercase = tf_strings.lower(input_string)\n+    return tf_strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n+\n+\n+eng_vectorization = TextVectorization(\n+    max_tokens=vocab_size,\n+    output_mode=\"int\",\n+    output_sequence_length=sequence_length,\n+)\n+spa_vectorization = TextVectorization(\n+    max_tokens=vocab_size,\n+    output_mode=\"int\",\n+    output_sequence_length=sequence_length + 1,\n+    standardize=custom_standardization,\n+)\n+train_eng_texts = [pair[0] for pair in train_pairs]\n+train_spa_texts = [pair[1] for pair in train_pairs]\n+eng_vectorization.adapt(train_eng_texts)\n+spa_vectorization.adapt(train_spa_texts)\n+\n+\"\"\"\n+Next, we'll format our datasets.\n+\n+At each training step, the model will seek to predict target words N+1 (and beyond)\n+using the source sentence and the target words 0 to N.\n+\n+As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n+\n+- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n+`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n+that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n+- `target` is the target sentence offset by one step:\n+it provides the next words in the target sentence -- what the model will try to predict.\n+\"\"\"\n+\n+def format_dataset(eng, spa):\n+    eng = eng_vectorization(eng)\n+    spa = spa_vectorization(spa)\n+    return (\n+        {\n+            \"encoder_inputs\": eng,\n+            \"decoder_inputs\": spa[:, :-1],\n+        },\n+        spa[:, 1:],\n+    )\n+\n+\n+def make_dataset(pairs):\n+    eng_texts, spa_texts = zip(*pairs)\n+    eng_texts = list(eng_texts)\n+    spa_texts = list(spa_texts)\n+    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n+    dataset = dataset.batch(batch_size)\n+    dataset = dataset.map(format_dataset)\n+    return dataset.shuffle(2048).prefetch(16).cache()\n+\n+\n+train_ds = make_dataset(train_pairs)\n+val_ds = make_dataset(val_pairs)\n+\n+\"\"\"\n+Let's take a quick look at the sequence shapes\n+(we have batches of 64 pairs, and all sequences are 20 steps long):\n+\"\"\"\n+\n+for inputs, targets in train_ds.take(1):\n+    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n+    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n+    print(f\"targets.shape: {targets.shape}\")\n+\n+\"\"\"\n+## Building the model\n+\n+Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n+and a `TransformerDecoder` chained together. To make the model aware of word order,\n+we also use a `PositionalEmbedding` layer.\n+\n+The source sequence will be pass to the `TransformerEncoder`,\n+which will produce a new representation of it.\n+This new representation will then be passed\n+to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n+The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n+\n+A key detail that makes this possible is causal masking\n+(see method `get_causal_attention_mask()` on the `TransformerDecoder`).\n+The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n+sure that it only uses information from target tokens 0 to N when predicting token N+1\n+(otherwise, it could use information from the future, which would\n+result in a model that cannot be used at inference time).\n+\"\"\"\n+import keras_core.ops as ops\n+\n+class TransformerEncoder(layers.Layer):\n+    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n+        super().__init__(**kwargs)\n+        self.embed_dim = embed_dim\n+        self.dense_dim = dense_dim\n+        self.num_heads = num_heads\n+        self.attention = layers.MultiHeadAttention(\n+            num_heads=num_heads, key_dim=embed_dim\n+        )\n+        self.dense_proj = keras.Sequential(\n+            [\n+                layers.Dense(dense_dim, activation=\"relu\"),\n+                layers.Dense(embed_dim),\n+            ]\n+        )\n+        self.layernorm_1 = layers.LayerNormalization()\n+        self.layernorm_2 = layers.LayerNormalization()\n+        self.supports_masking = True\n+\n+    def call(self, inputs, mask=None):\n+        if mask is not None:\n+            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n+        else:\n+            padding_mask = None\n+\n+        attention_output = self.attention(\n+            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n+        )\n+        proj_input = self.layernorm_1(inputs + attention_output)\n+        proj_output = self.dense_proj(proj_input)\n+        return self.layernorm_2(proj_input + proj_output)\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"embed_dim\": self.embed_dim,\n+                \"dense_dim\": self.dense_dim,\n+                \"num_heads\": self.num_heads,\n+            }\n+        )\n+        return config\n+\n+\n+class PositionalEmbedding(layers.Layer):\n+    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n+        super().__init__(**kwargs)\n+        self.token_embeddings = layers.Embedding(\n+            input_dim=vocab_size, output_dim=embed_dim\n+        )\n+        self.position_embeddings = layers.Embedding(\n+            input_dim=sequence_length, output_dim=embed_dim\n+        )\n+        self.sequence_length = sequence_length\n+        self.vocab_size = vocab_size\n+        self.embed_dim = embed_dim\n+\n+    def call(self, inputs):\n+        length = ops.shape(inputs)[-1]\n+        positions = ops.arange(0, length, 1)\n+        embedded_tokens = self.token_embeddings(inputs)\n+        embedded_positions = self.position_embeddings(positions)\n+        return embedded_tokens + embedded_positions\n+\n+    def compute_mask(self, inputs, mask=None):\n+        if mask is None:\n+            return None\n+        else:\n+            return ops.not_equal(inputs, 0)\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"sequence_length\": self.sequence_length,\n+                \"vocab_size\": self.vocab_size,\n+                \"embed_dim\": self.embed_dim,\n+            }\n+        )\n+        return config\n+\n+\n+class TransformerDecoder(layers.Layer):\n+    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n+        super().__init__(**kwargs)\n+        self.embed_dim = embed_dim\n+        self.latent_dim = latent_dim\n+        self.num_heads = num_heads\n+        self.attention_1 = layers.MultiHeadAttention(\n+            num_heads=num_heads, key_dim=embed_dim\n+        )\n+        self.attention_2 = layers.MultiHeadAttention(\n+            num_heads=num_heads, key_dim=embed_dim\n+        )\n+        self.dense_proj = keras.Sequential(\n+            [\n+                layers.Dense(latent_dim, activation=\"relu\"),\n+                layers.Dense(embed_dim),\n+            ]\n+        )\n+        self.layernorm_1 = layers.LayerNormalization()\n+        self.layernorm_2 = layers.LayerNormalization()\n+        self.layernorm_3 = layers.LayerNormalization()\n+        self.supports_masking = True\n+\n+    def call(self, inputs, encoder_outputs, mask=None):\n+        causal_mask = self.get_causal_attention_mask(inputs)\n+        if mask is not None:\n+            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n+            padding_mask = ops.minimum(padding_mask, causal_mask)\n+        else:\n+            padding_mask = None\n+\n+        attention_output_1 = self.attention_1(\n+            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n+        )\n+        out_1 = self.layernorm_1(inputs + attention_output_1)\n+\n+        attention_output_2 = self.attention_2(\n+            query=out_1,\n+            value=encoder_outputs,\n+            key=encoder_outputs,\n+            attention_mask=padding_mask,\n+        )\n+        out_2 = self.layernorm_2(out_1 + attention_output_2)\n+\n+        proj_output = self.dense_proj(out_2)\n+        return self.layernorm_3(out_2 + proj_output)\n+\n+    def get_causal_attention_mask(self, inputs):\n+        input_shape = ops.shape(inputs)\n+        batch_size, sequence_length = input_shape[0], input_shape[1]\n+        i = ops.arange(sequence_length)[:, None]\n+        j = ops.arange(sequence_length)\n+        mask = ops.cast(i >= j, dtype=\"int32\")\n+        mask = ops.reshape(mask, (1, input_shape[1], input_shape[1]))\n+        mult = ops.concatenate(\n+            [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])],\n+            axis=0,\n+        )\n+        return ops.tile(mask, mult)\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"embed_dim\": self.embed_dim,\n+                \"latent_dim\": self.latent_dim,\n+                \"num_heads\": self.num_heads,\n+            }\n+        )\n+        return config\n+\n+\n+\"\"\"\n+Next, we assemble the end-to-end model.\n+\"\"\"\n+\n+embed_dim = 256\n+latent_dim = 2048\n+num_heads = 8\n+\n+encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n+x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n+encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n+encoder = keras.Model(encoder_inputs, encoder_outputs)\n+\n+decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n+encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n+x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n+x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n+x = layers.Dropout(0.5)(x)\n+decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n+decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n+\n+decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n+transformer = keras.Model(\n+    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n+)\n+\n+\"\"\"\n+## Training our model\n+\n+We'll use accuracy as a quick way to monitor training progress on the validation data.\n+Note that machine translation typically uses BLEU scores as well as other metrics, rather than accuracy.\n+\n+Here we only train for 1 epoch, but to get the model to actually converge\n+you should train for at least 30 epochs.\n+\"\"\"\n+\n+epochs = 1  # This should be at least 30 for convergence\n+\n+transformer.summary()\n+transformer.compile(\n+    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n+)\n+transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)\n+\n+\"\"\"\n+## Decoding test sentences\n+\n+Finally, let's demonstrate how to translate brand new English sentences.\n+We simply feed into the model the vectorized English sentence\n+as well as the target token `\"[start]\"`, then we repeatedly generated the next token, until\n+we hit the token `\"[end]\"`.\n+\"\"\"\n+\n+spa_vocab = spa_vectorization.get_vocabulary()\n+spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n+max_decoded_sentence_length = 20\n+\n+\n+def decode_sequence(input_sentence):\n+    tokenized_input_sentence = eng_vectorization([input_sentence])\n+    decoded_sentence = \"[start]\"\n+    for i in range(max_decoded_sentence_length):\n+        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n+        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n+\n+        # ops.argmax(predictions[0, i, :]) is not a concrete value for jax here\n+        sampled_token_index = ops.convert_to_numpy(ops.argmax(predictions[0, i, :])).item(0)\n+        sampled_token = spa_index_lookup[sampled_token_index]\n+        decoded_sentence += \" \" + sampled_token\n+\n+        if sampled_token == \"[end]\":\n+            break\n+    return decoded_sentence\n+\n+\n+test_eng_texts = [pair[0] for pair in test_pairs]\n+for _ in range(30):\n+    input_sentence = random.choice(test_eng_texts)\n+    translated = decode_sequence(input_sentence)\n+\n+\"\"\"\n+After 30 epochs, we get results such as:\n+\n+> She handed him the money.\n+> [start] ella le pasó el dinero [end]\n+\n+> Tom has never heard Mary sing.\n+> [start] tom nunca ha oído cantar a mary [end]\n+\n+> Perhaps she will come tomorrow.\n+> [start] tal vez ella vendrá mañana [end]\n+\n+> I love to write.\n+> [start] me encanta escribir [end]\n+\n+> His French is improving little by little.\n+> [start] su francés va a [UNK] sólo un poco [end]\n+\n+> My hotel told me to call you.\n+> [start] mi hotel me dijo que te [UNK] [end]\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6319257806ad557b64b0aa484758fa0f46b1daa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 14 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,2 +1,2 @@\n # Unique source of truth for the version number.\n-__version__ = \"0.1.3\"\n+__version__ = \"0.1.4\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#756a80719c695db47c554838cddac9b40df0374b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 1048 | Contributors (this commit): 5 | Commits (past 90d): 11 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -271,7 +271,7 @@ class CustomNonPaddingTokenLoss(keras.losses.Loss):\n \n     def call(self, y_true, y_pred):\n         loss_fn = keras.losses.SparseCategoricalCrossentropy(\n-            from_logits=True, reduction=keras.losses.Reduction.NONE\n+            from_logits=True, reduction=None\n         )\n         loss = loss_fn(y_true, y_pred)\n         mask = keras.backend.cast((y_true > 0), dtype=\"float32\")\n\n@@ -4,7 +4,6 @@ import string\n \n import numpy as np\n \n-from keras_core import backend\n from keras_core import constraints\n from keras_core import initializers\n from keras_core import ops\n@@ -314,9 +313,7 @@ class MultiHeadAttention(Layer):\n         )\n         self._softmax = Softmax(axis=norm_axes)\n         self._dropout_layer = Dropout(rate=self._dropout)\n-        self._inverse_sqrt_key_dim = backend.convert_to_tensor(\n-            1.0 / math.sqrt(float(self._key_dim))\n-        )\n+        self._inverse_sqrt_key_dim = 1.0 / math.sqrt(float(self._key_dim))\n \n     def _masked_softmax(self, attention_scores, attention_mask=None):\n         # Normalize the attention scores to probabilities.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#918502e9fd70bd7baff70975ac0fc2ed25e8f3d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1261 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -35,7 +35,7 @@ def ignore_files(_, filenames):\n     return [f for f in filenames if f.endswith(\"_test.py\")]\n \n \n-def build():\n+def build(root_path):\n     if os.path.exists(build_directory):\n         raise ValueError(f\"Directory already exists: {build_directory}\")\n \n@@ -43,7 +43,6 @@ def build():\n     try:\n         # Copy sources (`keras_core/` directory and setup files) to build\n         # directory\n-        root_path = pathlib.Path(__file__).parent.resolve()\n         os.chdir(root_path)\n         os.mkdir(build_directory)\n         shutil.copytree(\n@@ -181,6 +180,7 @@ if __name__ == \"__main__\":\n         help=\"Whether to install the generated wheel file.\",\n     )\n     args = parser.parse_args()\n-    whl_path = build()\n+    root_path = pathlib.Path(__file__).parent.resolve()\n+    whl_path = build(root_path)\n     if whl_path and args.install:\n         install_whl(whl_path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#720ba6268871f75cc86ec46d6d971bf1fdb81c2c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 314 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 12 | Complexity Δ (Sum/Max): 18/18 | Churn Δ: 314 | Churn Cumulative: 314 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.7636363636363637\n\nDIFF:\n@@ -0,0 +1,314 @@\n+\"\"\"\n+Title: Text generation with a miniature GPT\n+Author: [Apoorv Nandan](https://twitter.com/NandanApoorv)\n+Date created: 2020/05/29\n+Last modified: 2020/05/29\n+Description: Implement a miniature version of GPT and train it to generate text.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+This example demonstrates how to implement an autoregressive language model\n+using a miniature version of the GPT model.\n+The model consists of a single Transformer block with causal masking\n+in its attention layer.\n+We use the text from the IMDB sentiment classification dataset for training\n+and generate new movie reviews for a given prompt.\n+When using this script with your own dataset, make sure it has at least\n+1 million words.\n+\n+This example should be run with `tf-nightly>=2.3.0-dev20200531` or\n+with TensorFlow 2.3 or higher.\n+\n+**References:**\n+\n+- [GPT](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n+- [GPT-2](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)\n+- [GPT-3](https://arxiv.org/abs/2005.14165)\n+\"\"\"\n+\"\"\"\n+## Setup\n+\"\"\"\n+# We set the backend to TensorFlow. The code works with\n+# both `tensorflow` and `torch`. It does not work with JAX\n+# due to the behavior of `jax.numpy.tile` in a jit scope\n+# (used in `causal_attention_mask()`: `tile` in JAX does\n+# not support a dynamic `reps` argument.\n+# You can make the code work in JAX by wrapping the\n+# inside of the `causal_attention_mask` function in\n+# a decorator to prevent jit compilation:\n+# `with jax.ensure_compile_time_eval():`.\n+import os\n+os.environ['KERAS_BACKEND'] = 'tensorflow'\n+\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core import ops\n+from keras_core.layers import TextVectorization\n+import numpy as np\n+import os\n+import string\n+import random\n+import tensorflow\n+import tensorflow.data as tf_data\n+import tensorflow.strings as tf_strings\n+\n+\n+\"\"\"\n+## Implement a Transformer block as a layer\n+\"\"\"\n+\n+\n+def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n+    \"\"\"\n+    Mask the upper half of the dot product matrix in self attention.\n+    This prevents flow of information from future tokens to current token.\n+    1's in the lower triangle, counting from the lower right corner.\n+    \"\"\"\n+    i = ops.arange(n_dest)[:, None]\n+    j = ops.arange(n_src)\n+    m = i >= j - n_src + n_dest\n+    mask = ops.cast(m, dtype)\n+    mask = ops.reshape(mask, [1, n_dest, n_src])\n+    mult = ops.concatenate(\n+        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n+    )\n+    return ops.tile(mask, mult)\n+\n+\n+class TransformerBlock(layers.Layer):\n+    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n+        super().__init__()\n+        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n+        self.ffn = keras.Sequential(\n+            [\n+                layers.Dense(ff_dim, activation=\"relu\"),\n+                layers.Dense(embed_dim),\n+            ]\n+        )\n+        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n+        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n+        self.dropout1 = layers.Dropout(rate)\n+        self.dropout2 = layers.Dropout(rate)\n+\n+    def call(self, inputs):\n+        input_shape = ops.shape(inputs)\n+        batch_size = input_shape[0]\n+        seq_len = input_shape[1]\n+        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n+        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n+        attention_output = self.dropout1(attention_output)\n+        out1 = self.layernorm1(inputs + attention_output)\n+        ffn_output = self.ffn(out1)\n+        ffn_output = self.dropout2(ffn_output)\n+        return self.layernorm2(out1 + ffn_output)\n+\n+\n+\"\"\"\n+## Implement an embedding layer\n+\n+Create two separate embedding layers: one for tokens and one for token index\n+(positions).\n+\"\"\"\n+\n+\n+class TokenAndPositionEmbedding(layers.Layer):\n+    def __init__(self, maxlen, vocab_size, embed_dim):\n+        super().__init__()\n+        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n+        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n+\n+    def call(self, x):\n+        maxlen = ops.shape(x)[-1]\n+        positions = ops.arange(0, maxlen, 1)\n+        positions = self.pos_emb(positions)\n+        x = self.token_emb(x)\n+        return x + positions\n+\n+\n+\"\"\"\n+## Implement the miniature GPT model\n+\"\"\"\n+vocab_size = 20000  # Only consider the top 20k words\n+maxlen = 80  # Max sequence size\n+embed_dim = 256  # Embedding size for each token\n+num_heads = 2  # Number of attention heads\n+feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n+\n+\n+def create_model():\n+    inputs = layers.Input(shape=(maxlen,), dtype=\"int32\")\n+    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n+    x = embedding_layer(inputs)\n+    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n+    x = transformer_block(x)\n+    outputs = layers.Dense(vocab_size)(x)\n+    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n+    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n+    model.compile(\n+        \"adam\",\n+        loss=[loss_fn, None],\n+    )  # No loss and optimization based on word embeddings from transformer block\n+    return model\n+\n+\n+\"\"\"\n+## Prepare the data for word-level language modelling\n+\n+Download the IMDB dataset and combine training and validation sets for a text\n+generation task.\n+\"\"\"\n+\n+\"\"\"shell\n+curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n+tar -xf aclImdb_v1.tar.gz\n+\"\"\"\n+\n+\n+batch_size = 128\n+\n+# The dataset contains each review in a separate text file\n+# The text files are present in four different folders\n+# Create a list all files\n+filenames = []\n+directories = [\n+    \"aclImdb/train/pos\",\n+    \"aclImdb/train/neg\",\n+    \"aclImdb/test/pos\",\n+    \"aclImdb/test/neg\",\n+]\n+for dir in directories:\n+    for f in os.listdir(dir):\n+        filenames.append(os.path.join(dir, f))\n+\n+print(f\"{len(filenames)} files\")\n+\n+# Create a dataset from text files\n+random.shuffle(filenames)\n+text_ds = tf_data.TextLineDataset(filenames)\n+text_ds = text_ds.shuffle(buffer_size=256)\n+text_ds = text_ds.batch(batch_size)\n+\n+\n+def custom_standardization(input_string):\n+    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n+    lowercased = tf_strings.lower(input_string)\n+    stripped_html = tf_strings.regex_replace(lowercased, \"<br />\", \" \")\n+    return tf_strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n+\n+\n+# Create a vectorization layer and adapt it to the text\n+vectorize_layer = TextVectorization(\n+    standardize=custom_standardization,\n+    max_tokens=vocab_size - 1,\n+    output_mode=\"int\",\n+    output_sequence_length=maxlen + 1,\n+)\n+vectorize_layer.adapt(text_ds)\n+vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n+\n+\n+def prepare_lm_inputs_labels(text):\n+    \"\"\"\n+    Shift word sequences by 1 position so that the target for position (i) is\n+    word at position (i+1). The model will use all words up till position (i)\n+    to predict the next word.\n+    \"\"\"\n+    text = tensorflow.expand_dims(text, -1)\n+    tokenized_sentences = vectorize_layer(text)\n+    x = tokenized_sentences[:, :-1]\n+    y = tokenized_sentences[:, 1:]\n+    return x, y\n+\n+\n+text_ds = text_ds.map(prepare_lm_inputs_labels, num_parallel_calls=tf_data.AUTOTUNE)\n+text_ds = text_ds.prefetch(tf_data.AUTOTUNE)\n+\n+\n+\"\"\"\n+## Implement a Keras callback for generating text\n+\"\"\"\n+\n+\n+class TextGenerator(keras.callbacks.Callback):\n+    \"\"\"A callback to generate text from a trained model.\n+    1. Feed some starting prompt to the model\n+    2. Predict probabilities for the next token\n+    3. Sample the next token and add it to the next input\n+\n+    Arguments:\n+        max_tokens: Integer, the number of tokens to be generated after prompt.\n+        start_tokens: List of integers, the token indices for the starting prompt.\n+        index_to_word: List of strings, obtained from the TextVectorization layer.\n+        top_k: Integer, sample from the `top_k` token predictions.\n+        print_every: Integer, print after this many epochs.\n+    \"\"\"\n+\n+    def __init__(\n+        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n+    ):\n+        self.max_tokens = max_tokens\n+        self.start_tokens = start_tokens\n+        self.index_to_word = index_to_word\n+        self.print_every = print_every\n+        self.k = top_k\n+\n+    def sample_from(self, logits):\n+        logits, indices = ops.top_k(logits, k=self.k, sorted=True)\n+        indices = np.asarray(indices).astype(\"int32\")\n+        preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n+        preds = np.asarray(preds).astype(\"float32\")\n+        return np.random.choice(indices, p=preds)\n+\n+    def detokenize(self, number):\n+        return self.index_to_word[number]\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        start_tokens = [_ for _ in self.start_tokens]\n+        if (epoch + 1) % self.print_every != 0:\n+            return\n+        num_tokens_generated = 0\n+        tokens_generated = []\n+        while num_tokens_generated <= self.max_tokens:\n+            pad_len = maxlen - len(start_tokens)\n+            sample_index = len(start_tokens) - 1\n+            if pad_len < 0:\n+                x = start_tokens[:maxlen]\n+                sample_index = maxlen - 1\n+            elif pad_len > 0:\n+                x = start_tokens + [0] * pad_len\n+            else:\n+                x = start_tokens\n+            x = np.array([x])\n+            y, _ = self.model.predict(x)\n+            sample_token = self.sample_from(y[0][sample_index])\n+            tokens_generated.append(sample_token)\n+            start_tokens.append(sample_token)\n+            num_tokens_generated = len(tokens_generated)\n+        txt = \" \".join(\n+            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n+        )\n+        print(f\"generated text:\\n{txt}\\n\")\n+\n+\n+# Tokenize starting prompt\n+word_to_index = {}\n+for index, word in enumerate(vocab):\n+    word_to_index[word] = index\n+\n+start_prompt = \"this movie is\"\n+start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n+num_tokens_generated = 40\n+text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)\n+\n+\n+\"\"\"\n+## Train the model\n+\n+Note: This code should preferably be run on GPU.\n+\"\"\"\n+\n+model = create_model()\n+\n+model.fit(text_ds, verbose=2, epochs=25, callbacks=[text_gen_callback])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#31735d71879fcba260d69d93bc675ecf5004f946", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 16 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 31 | Churn Cumulative: 430 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.65\n\nDIFF:\n@@ -1,6 +1,7 @@\n import numpy as np\n \n from keras_core.api_export import keras_core_export\n+from keras_core.backend.config import standardize_data_format\n from keras_core.utils import dataset_utils\n from keras_core.utils import image_utils\n from keras_core.utils.module_utils import tensorflow as tf\n@@ -29,6 +30,7 @@ def image_dataset_from_directory(\n     interpolation=\"bilinear\",\n     follow_links=False,\n     crop_to_aspect_ratio=False,\n+    data_format=None,\n ):\n     \"\"\"Generates a `tf.data.Dataset` from image files in a directory.\n \n@@ -111,6 +113,8 @@ def image_dataset_from_directory(\n             (of size `image_size`) that matches the target aspect ratio. By\n             default (`crop_to_aspect_ratio=False`), aspect ratio may not be\n             preserved.\n+        data_format: If None uses keras_core.config.image_data_format()\n+            otherwise either 'channel_last' or 'channel_first'.\n \n     Returns:\n \n@@ -142,6 +146,7 @@ def image_dataset_from_directory(\n     - if `color_mode` is `\"rgba\"`,\n         there are 4 channels in the image tensors.\n     \"\"\"\n+\n     if labels not in (\"inferred\", None):\n         if not isinstance(labels, (list, tuple)):\n             raise ValueError(\n@@ -220,6 +225,8 @@ def image_dataset_from_directory(\n             f\"class_names. Received: class_names={class_names}\"\n         )\n \n+    data_format = standardize_data_format(data_format=data_format)\n+\n     if subset == \"both\":\n         (\n             image_paths_train,\n@@ -252,7 +259,9 @@ def image_dataset_from_directory(\n             num_classes=len(class_names),\n             interpolation=interpolation,\n             crop_to_aspect_ratio=crop_to_aspect_ratio,\n+            data_format=data_format,\n         )\n+\n         val_dataset = paths_and_labels_to_dataset(\n             image_paths=image_paths_val,\n             image_size=image_size,\n@@ -262,6 +271,7 @@ def image_dataset_from_directory(\n             num_classes=len(class_names),\n             interpolation=interpolation,\n             crop_to_aspect_ratio=crop_to_aspect_ratio,\n+            data_format=data_format,\n         )\n \n         if batch_size is not None:\n@@ -288,6 +298,7 @@ def image_dataset_from_directory(\n         # Include file paths for images as attribute.\n         train_dataset.file_paths = image_paths_train\n         val_dataset.file_paths = image_paths_val\n+\n         dataset = [train_dataset, val_dataset]\n     else:\n         image_paths, labels = dataset_utils.get_training_or_validation_split(\n@@ -308,6 +319,7 @@ def image_dataset_from_directory(\n             num_classes=len(class_names),\n             interpolation=interpolation,\n             crop_to_aspect_ratio=crop_to_aspect_ratio,\n+            data_format=data_format,\n         )\n \n         if batch_size is not None:\n@@ -320,12 +332,12 @@ def image_dataset_from_directory(\n                 dataset = dataset.shuffle(buffer_size=1024, seed=seed)\n \n         dataset = dataset.prefetch(tf.data.AUTOTUNE)\n-\n         # Users may need to reference `class_names`.\n         dataset.class_names = class_names\n \n         # Include file paths for images as attribute.\n         dataset.file_paths = image_paths\n+\n     return dataset\n \n \n@@ -337,12 +349,19 @@ def paths_and_labels_to_dataset(\n     label_mode,\n     num_classes,\n     interpolation,\n+    data_format,\n     crop_to_aspect_ratio=False,\n ):\n     \"\"\"Constructs a dataset of images and labels.\"\"\"\n     # TODO(fchollet): consider making num_parallel_calls settable\n     path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n-    args = (image_size, num_channels, interpolation, crop_to_aspect_ratio)\n+    args = (\n+        image_size,\n+        num_channels,\n+        interpolation,\n+        data_format,\n+        crop_to_aspect_ratio,\n+    )\n     img_ds = path_ds.map(\n         lambda x: load_image(x, *args), num_parallel_calls=tf.data.AUTOTUNE\n     )\n@@ -355,7 +374,12 @@ def paths_and_labels_to_dataset(\n \n \n def load_image(\n-    path, image_size, num_channels, interpolation, crop_to_aspect_ratio=False\n+    path,\n+    image_size,\n+    num_channels,\n+    interpolation,\n+    data_format,\n+    crop_to_aspect_ratio=False,\n ):\n     \"\"\"Load an image from a path and resize it.\"\"\"\n     img = tf.io.read_file(path)\n@@ -369,6 +393,7 @@ def load_image(\n             img,\n             image_size,\n             interpolation=interpolation,\n+            data_format=data_format,\n             backend_module=tf_backend,\n         )\n     else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c2265f33582017c2188146ffbfadadc57b265b99", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 211 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 11 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 211 | Churn Cumulative: 1053 | Contributors (this commit): 5 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -22,6 +22,24 @@ class Relu(Operation):\n \n @keras_core_export([\"keras_core.ops.relu\", \"keras_core.ops.nn.relu\"])\n def relu(x):\n+    \"\"\"Rectified linear unit activation function.\n+\n+    It is defined as `f(x) = max(0, x)`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-1., 0., 1.])\n+    >>> x_relu = keras_core.ops.relu(x)\n+    >>> print(x_relu)\n+    array([0., 0., 1.], shape=(3,), dtype=float64)\n+\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Relu().symbolic_call(x)\n     return backend.nn.relu(x)\n@@ -37,6 +55,24 @@ class Relu6(Operation):\n \n @keras_core_export([\"keras_core.ops.relu6\", \"keras_core.ops.nn.relu6\"])\n def relu6(x):\n+    \"\"\"Rectified linear unit activation function with upper bound of 6.\n+\n+    It is defined as `f(x) = np.clip(x, 0, 6)`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-1., 0., 1., 6., 7.])\n+    >>> x_relu6 = keras_core.ops.relu6(x)\n+    >>> print(x_relu6)\n+    array([0., 0., 1., 6., 6.], shape=(5,), dtype=float64)\n+\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Relu6().symbolic_call(x)\n     return backend.nn.relu6(x)\n@@ -52,6 +88,24 @@ class Sigmoid(Operation):\n \n @keras_core_export([\"keras_core.ops.sigmoid\", \"keras_core.ops.nn.sigmoid\"])\n def sigmoid(x):\n+    \"\"\"Sigmoid activation function.\n+\n+    It is defined as `f(x) = 1 / (1 + exp(-x))`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-1., 0., 1.])\n+    >>> x_sigmoid = keras_core.ops.sigmoid(x)\n+    >>> print(x_sigmoid)\n+    array([0.26894143, 0.5, 0.7310586 ], dtype=float64)\n+\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Sigmoid().symbolic_call(x)\n     return backend.nn.sigmoid(x)\n@@ -67,6 +121,25 @@ class Softplus(Operation):\n \n @keras_core_export([\"keras_core.ops.softplus\", \"keras_core.ops.nn.softplus\"])\n def softplus(x):\n+    \"\"\"Softplus activation function.\n+\n+    It is defined as `f(x) = log(exp(x) + 1)`, where `log` is the natural\n+    logarithm and `exp` is the exponential function.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-1., 0., 1.])\n+    >>> x_softplus = keras_core.ops.softplus(x)\n+    >>> print(x_softplus)\n+    array([0.31326166, 0.6931472 , 1.3132616 ], shape=(3,), dtype=float64)\n+\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Softplus().symbolic_call(x)\n     return backend.nn.softplus(x)\n@@ -82,6 +155,24 @@ class Softsign(Operation):\n \n @keras_core_export([\"keras_core.ops.softsign\", \"keras_core.ops.nn.softsign\"])\n def softsign(x):\n+    \"\"\"Softsign activation function.\n+\n+    It is defined as `f(x) = x / (abs(x) + 1)`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-1., 0., 1.])\n+    >>> x_softsign = keras_core.ops.softsign(x)\n+    >>> print(x_softsign)\n+    array([-0.5, 0., 0.5], shape=(3,), dtype=float64)\n+\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Softsign().symbolic_call(x)\n     return backend.nn.softsign(x)\n@@ -97,6 +188,24 @@ class Silu(Operation):\n \n @keras_core_export([\"keras_core.ops.silu\", \"keras_core.ops.nn.silu\"])\n def silu(x):\n+    \"\"\"Sigmoid-weighted linear unit activation function.\n+\n+    It is defined as `f(x) = x * sigmoid(x)`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-1., 0., 1.])\n+    >>> x_silu = keras_core.ops.silu(x)\n+    >>> print(x_silu)\n+    array([-0.26894143, 0., 0.7310586], shape=(3,), dtype=float64)\n+\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Silu().symbolic_call(x)\n     return backend.nn.silu(x)\n@@ -132,6 +241,24 @@ class LogSigmoid(Operation):\n     ]\n )\n def log_sigmoid(x):\n+    \"\"\"Logarithm of the sigmoid activation function.\n+\n+    It is defined as `f(x) = log(1 / (1 + exp(-x)))`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-1., 0., 1.])\n+    >>> x_log_sigmoid = keras_core.ops.log_sigmoid(x)\n+    >>> print(x_log_sigmoid)\n+    array([-1.3132616, -0.6931472, -0.3132616], shape=(3,), dtype=float64)\n+\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return LogSigmoid().symbolic_call(x)\n     return backend.nn.log_sigmoid(x)\n@@ -153,6 +280,28 @@ class LeakyRelu(Operation):\n     [\"keras_core.ops.leaky_relu\", \"keras_core.ops.nn.leaky_relu\"]\n )\n def leaky_relu(x, negative_slope=0.2):\n+    \"\"\"Leaky version of a Rectified Linear Unit.\n+\n+    It allows a small gradient when the unit is not active, it is defined as:\n+\n+    `f(x) = alpha * x for x < 0` or `f(x) = x for x >= 0`.\n+\n+    Args:\n+        x: Input tensor.\n+        negative_slope: Slope of the activation function at x < 0.\n+            Defaults to `0.2`.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-1., 0., 1.])\n+    >>> x_leaky_relu = keras_core.ops.leaky_relu(x)\n+    >>> print(x_leaky_relu)\n+    array([-0.2,  0. ,  1. ], shape=(3,), dtype=float64)\n+\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return LeakyRelu(negative_slope).symbolic_call(x)\n     return backend.nn.leaky_relu(x, negative_slope=negative_slope)\n@@ -173,6 +322,26 @@ class HardSigmoid(Operation):\n     ]\n )\n def hard_sigmoid(x):\n+    \"\"\"Hard sigmoid activation function.\n+\n+    It is defined as:\n+\n+    `0 if x < -2.5`, `1 if x > 2.5`, `(0.2 * x) + 0.5 if -2.5 <= x <= 2.5`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-1., 0., 1.])\n+    >>> x_hard_sigmoid = keras_core.ops.hard_sigmoid(x)\n+    >>> print(x_hard_sigmoid)\n+    array([0.3, 0.5, 0.7], shape=(3,), dtype=float64)\n+\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return HardSigmoid().symbolic_call(x)\n     return backend.nn.hard_sigmoid(x)\n@@ -192,6 +361,27 @@ class Elu(Operation):\n \n @keras_core_export([\"keras_core.ops.elu\", \"keras_core.ops.nn.elu\"])\n def elu(x, alpha=1.0):\n+    \"\"\"Exponential Linear Unit.\n+\n+    It is defined as:\n+\n+    `f(x) =  alpha * (exp(x) - 1.) for x < 0`, `f(x) = x for x >= 0`.\n+\n+    Args:\n+        x: Input tensor.\n+        alpha: A scalar, slope of positive section. Defaults to `1.0`.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-1., 0., 1.])\n+    >>> x_elu = keras_core.ops.elu(x)\n+    >>> print(x_elu)\n+    array([-0.63212055, 0., 1.], shape=(3,), dtype=float64)\n+\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Elu(alpha).symbolic_call(x)\n     return backend.nn.elu(x, alpha=alpha)\n@@ -207,6 +397,27 @@ class Selu(Operation):\n \n @keras_core_export([\"keras_core.ops.selu\", \"keras_core.ops.nn.selu\"])\n def selu(x):\n+    \"\"\"Scaled Exponential Linear Unit (SELU).\n+\n+    It is defined as:\n+\n+    `f(x) =  scale * alpha * (exp(x) - 1.) for x < 0`,\n+    `f(x) = scale * x for x >= 0`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-1., 0., 1.])\n+    >>> x_selu = keras_core.ops.selu(x)\n+    >>> print(x_selu)\n+    array([-1.11133055, 0., 1.05070098], shape=(3,), dtype=float64)\n+\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Selu().symbolic_call(x)\n     return backend.nn.selu(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
