{"custom_id": "keras#2eba9003ea33054e42191a965dd45b426c0b39c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1319 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -629,8 +629,6 @@ def ResNetRS(\n                 variance=[0.229**2, 0.224**2, 0.225**2],\n                 axis=bn_axis,\n             )(x)\n-        else:\n-            x = layers.Rescaling(scale=1.0 / 255)(x)\n \n     # Build stem\n     x = STEM(bn_momentum=bn_momentum, bn_epsilon=bn_epsilon, activation=activation)(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#46121eed08d0feef743eacef3b66206df45cf656", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 95 | Lines Deleted: 7 | Files Changed: 5 | Hunks: 13 | Methods Changed: 10 | Complexity Δ (Sum/Max): 12/4 | Churn Δ: 102 | Churn Cumulative: 4524 | Contributors (this commit): 13 | Commits (past 90d): 17 | Contributors (cumulative): 32 | DMM Complexity: 0.5689655172413793\n\nDIFF:\n@@ -30,6 +30,7 @@ from keras.engine import input_spec\n from keras.engine import node as node_module\n from keras.engine import training as training_lib\n from keras.engine import training_utils\n+from keras.saving.saved_model import json_utils\n from keras.saving.saved_model import network_serialization\n from keras.utils import generic_utils\n from keras.utils import tf_inspect\n@@ -1206,9 +1207,6 @@ def reconstruct_from_config(config, custom_objects=None, created_layers=None):\n     input_tensors = []\n     for input_data in tf.nest.flatten(node_data):\n       input_data = input_data.as_list()\n-      inbound_layer_name = input_data[0]\n-      inbound_node_index = input_data[1]\n-      inbound_tensor_index = input_data[2]\n       if len(input_data) == 3:\n         kwargs = {}\n       elif len(input_data) == 4:\n@@ -1221,7 +1219,10 @@ def reconstruct_from_config(config, custom_objects=None, created_layers=None):\n       else:\n         raise ValueError('Improperly formatted model config.')\n \n-      if inbound_layer_name != node_module._CONSTANT_VALUE:\n+      if input_data[0] != node_module._CONSTANT_VALUE:\n+        inbound_layer_name = input_data[0]\n+        inbound_node_index = input_data[1]\n+        inbound_tensor_index = input_data[2]\n         inbound_layer = created_layers[inbound_layer_name]\n         inbound_node_index = get_node_index(inbound_layer, inbound_node_index)\n \n@@ -1231,8 +1232,20 @@ def reconstruct_from_config(config, custom_objects=None, created_layers=None):\n         input_tensors.append(\n             tf.nest.flatten(inbound_node.outputs)[inbound_tensor_index])\n       else:\n-        # We received a constant w/ no Keras history attached\n-        input_tensors.append(inbound_tensor_index)\n+        # We received a constant w/ no Keras history attached,\n+        # which means it is a constant tensor input.\n+        # Input is a constant value.\n+        # Format = [_CONSTANT_VALUE, -1, const_val, kwargs]\n+        assert input_data[1] == -1\n+        assert len(input_data) >= 3\n+        const_val = input_data[2]\n+        if (isinstance(const_val, tuple) and\n+            len(const_val) == 2 and\n+            const_val[0] == node_module._COMPOSITE_TYPE):\n+          # It is a composite tensor.\n+          input_tensors.append(json_utils.decode(const_val[1]))\n+        else:\n+          input_tensors.append(const_val)\n     input_tensors = tf.nest.pack_sequence_as(node_data, input_tensors)\n     # Call layer on its inputs, thus creating the node\n     # and building the layer if needed.\n\n@@ -28,6 +28,8 @@ from keras.saving.saved_model import json_utils\n from keras.utils import tf_utils\n \n _CONSTANT_VALUE = '_CONSTANT_VALUE'\n+# Using dict to avoid conflict with constant string tensor.\n+_COMPOSITE_TYPE = {'_TYPE': 'COMPOSITE'}\n \n \n class Node:\n@@ -194,6 +196,11 @@ class Node:\n       if isinstance(t, tf.Tensor):\n         return backend.get_value(t).tolist()\n \n+      # Not using json_utils to serialize both constant Tensor and constant\n+      # CompositeTensor for saving format backward compatibility.\n+      if isinstance(t, tf.__internal__.CompositeTensor):\n+        return (_COMPOSITE_TYPE, json_utils.Encoder().encode(t))\n+\n       return t\n \n     kwargs = tf.nest.map_structure(_serialize_keras_tensor, kwargs)\n\n@@ -1205,6 +1205,32 @@ class TestWholeModelSaving(test_combinations.TestCase):\n     self.assertNotIn(generic_utils.CustomMaskWarning,\n                      {warning.category for warning in w})\n \n+  # Test only in eager mode because ragged tensor inputs\n+  # cannot be used in graph mode.\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['eager']))\n+  @test_utils.run_v2_only\n+  def test_save_functional_with_ragged_constant_input(self):\n+    input1 = keras.Input(shape=[])\n+    input2 = tf.ragged.constant([[1., 2.], [3.]])\n+    outputs = keras.layers.Add()([input1, input2])\n+    model = keras.Model(input1, outputs)\n+    saved_model_dir = self._save_model_dir()\n+    model.save(saved_model_dir)\n+    keras.models.load_model(saved_model_dir)\n+\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['eager']))\n+  @test_utils.run_v2_only\n+  def test_save_functional_with_constant_input(self):\n+    input1 = keras.Input(shape=[2])\n+    input2 = tf.constant([[1., 2.]])\n+    outputs = keras.layers.Add()([input1, input2])\n+    model = keras.Model(input1, outputs)\n+    saved_model_dir = self._save_model_dir()\n+    model.save(saved_model_dir)\n+    keras.models.load_model(saved_model_dir)\n+\n \n # Factory functions to create models that will be serialized inside a Network.\n def _make_graph_network(input_size, output_size):\n\n@@ -36,6 +36,9 @@ from keras.utils import generic_utils\n from tensorflow.python.framework import type_spec\n \n \n+_EXTENSION_TYPE_SPEC = '_EXTENSION_TYPE_SPEC'\n+\n+\n class Encoder(json.JSONEncoder):\n   \"\"\"JSON encoder and decoder that handles TensorShapes and tuples.\"\"\"\n \n@@ -99,6 +102,15 @@ def _decode_helper(obj, deserialize=False, module_objects=None,\n     elif obj['class_name'] == 'TypeSpec':\n       return type_spec.lookup(obj['type_spec'])._deserialize(  # pylint: disable=protected-access\n           _decode_helper(obj['serialized']))\n+    elif obj['class_name'] == 'CompositeTensor':\n+      spec = obj['spec']\n+      tensors = []\n+      for dtype, tensor in obj['tensors']:\n+        tensors.append(tf.constant(tensor, dtype=tf.dtypes.as_dtype(dtype)))\n+      return tf.nest.pack_sequence_as(\n+          _decode_helper(spec),\n+          tensors,\n+          expand_composites=True)\n     elif obj['class_name'] == '__tuple__':\n       return tuple(_decode_helper(i) for i in obj['items'])\n     elif obj['class_name'] == '__ellipsis__':\n@@ -177,6 +189,14 @@ def get_json_type(obj):\n       raise ValueError(\n           f'Unable to serialize {obj} to JSON, because the TypeSpec '\n           f'class {type(obj)} has not been registered.')\n+  if isinstance(obj, tf.__internal__.CompositeTensor):\n+    spec = tf.type_spec_from_value(obj)\n+    tensors = []\n+    for tensor in tf.nest.flatten(obj, expand_composites=True):\n+      tensors.append((tensor.dtype.name, tensor.numpy().tolist()))\n+    return {'class_name': 'CompositeTensor',\n+            'spec': get_json_type(spec),\n+            'tensors': tensors}\n \n   if isinstance(obj, enum.Enum):\n     return obj.value\n\n@@ -19,9 +19,11 @@ import tensorflow.compat.v2 as tf\n \n import enum\n from keras.saving.saved_model import json_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n \n \n-class JsonUtilsTest(tf.test.TestCase):\n+class JsonUtilsTest(test_combinations.TestCase):\n \n   def test_encode_decode_tensor_shape(self):\n     metadata = {\n@@ -68,5 +70,25 @@ class JsonUtilsTest(tf.test.TestCase):\n     loaded = json_utils.decode(string)\n     self.assertAllEqual({'key': 'a', 'key2': 'b'}, loaded)\n \n+  @test_utils.run_v2_only\n+  def test_encode_decode_ragged_tensor(self):\n+    x = tf.ragged.constant([[1., 2.], [3.]])\n+    string = json_utils.Encoder().encode(x)\n+    loaded = json_utils.decode(string)\n+    self.assertAllEqual(loaded, x)\n+\n+  @test_utils.run_v2_only\n+  def test_encode_decode_extension_type_tensor(self):\n+    class MaskedTensor(tf.experimental.ExtensionType):\n+      __name__ = 'MaskedTensor'\n+      values: tf.Tensor\n+      mask: tf.Tensor\n+    x = MaskedTensor(values=[[1, 2, 3], [4, 5, 6]],\n+                     mask=[[True, True, False], [True, False, True]])\n+    string = json_utils.Encoder().encode(x)\n+    loaded = json_utils.decode(string)\n+    self.assertAllEqual(loaded, x)\n+\n+\n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3fe7a484d2f945b304afe6d96ff9adb80d885097", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 17 | Methods Changed: 6 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 42 | Churn Cumulative: 8180 | Contributors (this commit): 13 | Commits (past 90d): 6 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4387,7 +4387,8 @@ def rnn(step_function,\n         unroll=False,\n         input_length=None,\n         time_major=False,\n-        zero_output_for_mask=False):\n+        zero_output_for_mask=False,\n+        return_all_outputs=True):\n   \"\"\"Iterates over the time dimension of a tensor.\n \n   Args:\n@@ -4430,13 +4431,19 @@ def rnn(step_function,\n       zero_output_for_mask: Boolean. If True, the output for masked timestep\n           will be zeros, whereas in the False case, output from previous\n           timestep is returned.\n+      return_all_outputs: Boolean. If True, all outputs of the process will be\n+          returned, whereas in the False case, only last_output will be kept\n+          during the process, saving the corresponding memory, and\n+          outputs=[last_output] is returned.\n \n   Returns:\n       A tuple, `(last_output, outputs, new_states)`.\n           last_output: the latest output of the rnn, of shape `(samples, ...)`\n-          outputs: tensor with shape `(samples, time, ...)` where each\n-              entry `outputs[s, t]` is the output of the step function\n-              at time `t` for sample `s`.\n+          outputs:\n+              - If `return_all_outputs`: a tensor with shape\n+              `(samples, time, ...)` where each entry `outputs[s, t]` is the\n+              output of the step function at time `t` for sample `s`\n+              - Else, a tensor equal to last_output\n           new_states: list of tensors, latest states returned by\n               the step function, of shape `(samples, ...)`.\n \n@@ -4547,8 +4554,12 @@ def rnn(step_function,\n             for m, s, ps in zip(tiled_mask_t, flat_new_states, flat_states))\n         states = tf.nest.pack_sequence_as(states, flat_final_states)\n \n+        if return_all_outputs:\n           successive_outputs.append(output)\n           successive_states.append(states)\n+        else:\n+          successive_outputs = [output]\n+          successive_states = [states]\n       last_output = successive_outputs[-1]\n       new_states = successive_states[-1]\n       outputs = tf.stack(successive_outputs)\n@@ -4565,8 +4576,12 @@ def rnn(step_function,\n       for i in range(time_steps):\n         inp = _get_input_tensor(i)\n         output, states = step_function(inp, tuple(states) + tuple(constants))\n+        if return_all_outputs:\n           successive_outputs.append(output)\n           successive_states.append(states)\n+        else:\n+          successive_outputs = [output]\n+          successive_states = [states]\n       last_output = successive_outputs[-1]\n       new_states = successive_states[-1]\n       outputs = tf.stack(successive_outputs)\n@@ -4597,10 +4612,12 @@ def rnn(step_function,\n     # the value is discarded.\n     output_time_zero, _ = step_function(\n         input_time_zero, tuple(initial_states) + tuple(constants))\n+\n+    output_ta_size = time_steps_t if return_all_outputs else 1\n     output_ta = tuple(\n         tf.TensorArray(\n             dtype=out.dtype,\n-            size=time_steps_t,\n+            size=output_ta_size,\n             element_shape=out.shape,\n             tensor_array_name='output_ta_%s' % i)\n         for i, out in enumerate(tf.nest.flatten(output_time_zero)))\n@@ -4700,9 +4717,11 @@ def rnn(step_function,\n                                                  flat_state)\n         new_states = tf.nest.pack_sequence_as(new_states, flat_final_state)\n \n+        ta_index_to_write = time if return_all_outputs else 0\n         output_ta_t = tuple(\n-            ta.write(time, out)\n+            ta.write(ta_index_to_write, out)\n             for ta, out in zip(output_ta_t, flat_new_output))\n+\n         return (time + 1, output_ta_t,\n                 tuple(flat_new_output)) + tuple(new_states)\n \n@@ -4735,8 +4754,11 @@ def rnn(step_function,\n             new_state.set_shape(state.shape)\n \n         flat_output = tf.nest.flatten(output)\n+        ta_index_to_write = time if return_all_outputs else 0\n         output_ta_t = tuple(\n-            ta.write(time, out) for ta, out in zip(output_ta_t, flat_output))\n+            ta.write(ta_index_to_write, out)\n+            for ta, out in zip(output_ta_t, flat_output))\n+\n         new_states = tf.nest.pack_sequence_as(initial_states, flat_new_state)\n         return (time + 1, output_ta_t) + tuple(new_states)\n \n@@ -4758,7 +4780,10 @@ def rnn(step_function,\n   def set_shape(output_):\n     if isinstance(output_, tf.Tensor):\n       shape = output_.shape.as_list()\n+      if return_all_outputs:\n         shape[0] = time_steps\n+      else:\n+        shape[0] = 1\n       shape[1] = batch\n       output_.set_shape(shape)\n     return output_\n\n@@ -295,7 +295,8 @@ class ConvRNN(RNN):\n                                                constants=constants,\n                                                go_backwards=self.go_backwards,\n                                                mask=mask,\n-                                               input_length=timesteps)\n+                                               input_length=timesteps,\n+                                               return_all_outputs=self.return_sequences)\n     if self.stateful:\n       updates = [\n           backend.update(self_state, state)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e0a970d08ddba7f700876439c44b379e19d3cc33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 30 | Churn Cumulative: 19140 | Contributors (this commit): 127 | Commits (past 90d): 47 | Contributors (cumulative): 136 | DMM Complexity: 1.0\n\nDIFF:\n@@ -274,6 +274,26 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n     result = model(tf.zeros((10, 10), layout=self.layout_2d), training=True)\n     self.assertAllClose(result, tf.zeros((10, 30), layout=self.layout_2d))\n \n+  def test_init_model_with_empty_layout_map(self):\n+    # Create empty layout map, which means all the weights just default to\n+    # all replicated.\n+    layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)\n+    with layout_map_lib.layout_map_scope(layout_map):\n+      model = tf.keras.Sequential([\n+          layers.Dense(20, name='d1', input_shape=(10,)),\n+          layers.Dropout(0.1),\n+          layers.Dense(30, name='d2')\n+      ])\n+\n+    self.assertLen(model.layers, 3)\n+    d1 = model.layers[0]\n+    d2 = model.layers[2]\n+\n+    self.assertEqual(d1.kernel.layout, self.layout_2d)\n+    self.assertEqual(d1.bias.layout, self.layout_1d)\n+    self.assertEqual(d2.kernel.layout, self.layout_2d)\n+    self.assertEqual(d2.bias.layout, self.layout_1d)\n+\n \n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -260,10 +260,10 @@ class Functional(training_lib.Model):\n     self._set_save_spec(self._nested_inputs)\n     tf_utils.assert_no_legacy_layers(self.layers)\n \n-    # Note that this method is used by both functional and sequential model,\n-    # so we can't just this method in functional.__init__, which will miss the\n-    # coverage of sequential model.\n-    if self._layout_map:\n+    # Note that this method is used by both functional and sequential models,\n+    # so we can't just have this method in functional.__init__, which will miss\n+    #  the coverage of sequential model.\n+    if self._layout_map is not None:\n       layout_map_lib._map_functional_model_variable(self, self._layout_map)\n \n   @property\n\n@@ -459,7 +459,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n   @traceback_utils.filter_traceback\n   def __call__(self, *args, **kwargs):\n-    if self._layout_map and not self.built:\n+    if self._layout_map is not None and not self.built:\n       # Note that this method is only overridden for DTensor and layout\n       # injection purpose.\n       # Capture the inputs and create graph input as replacement for model\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#62aab556c6252e54b9f3ee3fa65243aecd6aea52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2796 | Lines Deleted: 2197 | Files Changed: 12 | Hunks: 248 | Methods Changed: 287 | Complexity Δ (Sum/Max): 94/65 | Churn Δ: 4993 | Churn Cumulative: 8699 | Contributors (this commit): 2 | Commits (past 90d): 26 | Contributors (cumulative): 14 | DMM Complexity: 0.7454545454545455\n\nDIFF:\n@@ -1,517 +0,0 @@\n-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Module contains the base implementation of RNN cell wrappers.\"\"\"\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import hashlib\n-import numbers\n-import sys\n-import types as python_types\n-import warnings\n-\n-from keras.utils import generic_utils\n-import tensorflow.compat.v2 as tf\n-\n-\n-class DropoutWrapperBase:\n-  \"\"\"Operator adding dropout to inputs and outputs of the given cell.\"\"\"\n-\n-  def __init__(self,\n-               cell,\n-               input_keep_prob=1.0,\n-               output_keep_prob=1.0,\n-               state_keep_prob=1.0,\n-               variational_recurrent=False,\n-               input_size=None,\n-               dtype=None,\n-               seed=None,\n-               dropout_state_filter_visitor=None,\n-               **kwargs):\n-    \"\"\"Create a cell with added input, state, and/or output dropout.\n-\n-    If `variational_recurrent` is set to `True` (**NOT** the default behavior),\n-    then the same dropout mask is applied at every step, as described in:\n-    [A Theoretically Grounded Application of Dropout in Recurrent\n-    Neural Networks. Y. Gal, Z. Ghahramani](https://arxiv.org/abs/1512.05287).\n-\n-    Otherwise a different dropout mask is applied at every time step.\n-\n-    Note, by default (unless a custom `dropout_state_filter` is provided),\n-    the memory state (`c` component of any `LSTMStateTuple`) passing through\n-    a `DropoutWrapper` is never modified.  This behavior is described in the\n-    above article.\n-\n-    Args:\n-      cell: an RNNCell, a projection to output_size is added to it.\n-      input_keep_prob: unit Tensor or float between 0 and 1, input keep\n-        probability; if it is constant and 1, no input dropout will be added.\n-      output_keep_prob: unit Tensor or float between 0 and 1, output keep\n-        probability; if it is constant and 1, no output dropout will be added.\n-      state_keep_prob: unit Tensor or float between 0 and 1, output keep\n-        probability; if it is constant and 1, no output dropout will be added.\n-        State dropout is performed on the outgoing states of the cell. **Note**\n-        the state components to which dropout is applied when `state_keep_prob`\n-        is in `(0, 1)` are also determined by the argument\n-        `dropout_state_filter_visitor` (e.g. by default dropout is never applied\n-        to the `c` component of an `LSTMStateTuple`).\n-      variational_recurrent: Python bool.  If `True`, then the same dropout\n-        pattern is applied across all time steps per run call. If this parameter\n-        is set, `input_size` **must** be provided.\n-      input_size: (optional) (possibly nested tuple of) `TensorShape` objects\n-        containing the depth(s) of the input tensors expected to be passed in to\n-        the `DropoutWrapper`.  Required and used **iff** `variational_recurrent\n-        = True` and `input_keep_prob < 1`.\n-      dtype: (optional) The `dtype` of the input, state, and output tensors.\n-        Required and used **iff** `variational_recurrent = True`.\n-      seed: (optional) integer, the randomness seed.\n-      dropout_state_filter_visitor: (optional), default: (see below).  Function\n-        that takes any hierarchical level of the state and returns a scalar or\n-        depth=1 structure of Python booleans describing which terms in the state\n-        should be dropped out.  In addition, if the function returns `True`,\n-        dropout is applied across this sublevel.  If the function returns\n-        `False`, dropout is not applied across this entire sublevel.\n-        Default behavior: perform dropout on all terms except the memory (`c`)\n-          state of `LSTMCellState` objects, and don't try to apply dropout to\n-        `TensorArray` objects: ```\n-        def dropout_state_filter_visitor(s):\n-          if isinstance(s, LSTMCellState): # Never perform dropout on the c\n-            state. return LSTMCellState(c=False, h=True)\n-          elif isinstance(s, TensorArray): return False return True ```\n-      **kwargs: dict of keyword arguments for base layer.\n-\n-    Raises:\n-      TypeError: if `cell` is not an `RNNCell`, or `keep_state_fn` is provided\n-        but not `callable`.\n-      ValueError: if any of the keep_probs are not between 0 and 1.\n-    \"\"\"\n-    super(DropoutWrapperBase, self).__init__(cell, dtype=dtype, **kwargs)\n-\n-    if (dropout_state_filter_visitor is not None and\n-        not callable(dropout_state_filter_visitor)):\n-      raise TypeError(\"dropout_state_filter_visitor must be callable. \"\n-                      f\"Received: {dropout_state_filter_visitor}\")\n-    self._dropout_state_filter = (\n-        dropout_state_filter_visitor or _default_dropout_state_filter_visitor)\n-    with tf.name_scope(\"DropoutWrapperInit\"):\n-\n-      def tensor_and_const_value(v):\n-        tensor_value = tf.convert_to_tensor(v)\n-        const_value = tf.get_static_value(tensor_value)\n-        return (tensor_value, const_value)\n-\n-      for prob, attr in [(input_keep_prob, \"input_keep_prob\"),\n-                         (state_keep_prob, \"state_keep_prob\"),\n-                         (output_keep_prob, \"output_keep_prob\")]:\n-        tensor_prob, const_prob = tensor_and_const_value(prob)\n-        if const_prob is not None:\n-          if const_prob < 0 or const_prob > 1:\n-            raise ValueError(\n-                f\"Parameter {attr} must be between 0 and 1. \"\n-                \"Received {const_prob}\")\n-          setattr(self, \"_%s\" % attr, float(const_prob))\n-        else:\n-          setattr(self, \"_%s\" % attr, tensor_prob)\n-\n-    # Set variational_recurrent, seed before running the code below\n-    self._variational_recurrent = variational_recurrent\n-    self._input_size = input_size\n-    self._seed = seed\n-\n-    self._recurrent_input_noise = None\n-    self._recurrent_state_noise = None\n-    self._recurrent_output_noise = None\n-\n-    if variational_recurrent:\n-      if dtype is None:\n-        raise ValueError(\n-            \"When variational_recurrent=True, dtype must be provided\")\n-\n-      def convert_to_batch_shape(s):\n-        # Prepend a 1 for the batch dimension; for recurrent\n-        # variational dropout we use the same dropout mask for all\n-        # batch elements.\n-        return tf.concat(([1], tf.TensorShape(s).as_list()), 0)\n-\n-      def batch_noise(s, inner_seed):\n-        shape = convert_to_batch_shape(s)\n-        return tf.random.uniform(shape, seed=inner_seed, dtype=dtype)\n-\n-      if (not isinstance(self._input_keep_prob, numbers.Real) or\n-          self._input_keep_prob < 1.0):\n-        if input_size is None:\n-          raise ValueError(\n-              \"When variational_recurrent=True and input_keep_prob < 1.0 or \"\n-              \"is unknown, input_size must be provided\")\n-        self._recurrent_input_noise = _enumerated_map_structure_up_to(\n-            input_size,\n-            lambda i, s: batch_noise(s, inner_seed=self._gen_seed(\"input\", i)),\n-            input_size)\n-      self._recurrent_state_noise = _enumerated_map_structure_up_to(\n-          cell.state_size,\n-          lambda i, s: batch_noise(s, inner_seed=self._gen_seed(\"state\", i)),\n-          cell.state_size)\n-      self._recurrent_output_noise = _enumerated_map_structure_up_to(\n-          cell.output_size,\n-          lambda i, s: batch_noise(s, inner_seed=self._gen_seed(\"output\", i)),\n-          cell.output_size)\n-\n-  def _gen_seed(self, salt_prefix, index):\n-    if self._seed is None:\n-      return None\n-    salt = \"%s_%d\" % (salt_prefix, index)\n-    string = (str(self._seed) + salt).encode(\"utf-8\")\n-    return int(hashlib.md5(string).hexdigest()[:8], 16) & 0x7FFFFFFF\n-\n-  @property\n-  def wrapped_cell(self):\n-    return self.cell\n-\n-  @property\n-  def state_size(self):\n-    return self.cell.state_size\n-\n-  @property\n-  def output_size(self):\n-    return self.cell.output_size\n-\n-  def build(self, inputs_shape):\n-    self.cell.build(inputs_shape)\n-    self.built = True\n-\n-  def zero_state(self, batch_size, dtype):\n-    with tf.name_scope(type(self).__name__ + \"ZeroState\"):\n-      return self.cell.zero_state(batch_size, dtype)\n-\n-  def _variational_recurrent_dropout_value(\n-      self, unused_index, value, noise, keep_prob):\n-    \"\"\"Performs dropout given the pre-calculated noise tensor.\"\"\"\n-    # uniform [keep_prob, 1.0 + keep_prob)\n-    random_tensor = keep_prob + noise\n-\n-    # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)\n-    binary_tensor = tf.floor(random_tensor)\n-    ret = tf.divide(value, keep_prob) * binary_tensor\n-    ret.set_shape(value.get_shape())\n-    return ret\n-\n-  def _dropout(self,\n-               values,\n-               salt_prefix,\n-               recurrent_noise,\n-               keep_prob,\n-               shallow_filtered_substructure=None):\n-    \"\"\"Decides whether to perform standard dropout or recurrent dropout.\"\"\"\n-\n-    if shallow_filtered_substructure is None:\n-      # Put something so we traverse the entire structure; inside the\n-      # dropout function we check to see if leafs of this are bool or not.\n-      shallow_filtered_substructure = values\n-\n-    if not self._variational_recurrent:\n-\n-      def dropout(i, do_dropout, v):\n-        if not isinstance(do_dropout, bool) or do_dropout:\n-          return tf.nn.dropout(\n-              v, rate=1. - keep_prob, seed=self._gen_seed(salt_prefix, i))\n-        else:\n-          return v\n-\n-      return _enumerated_map_structure_up_to(\n-          shallow_filtered_substructure, dropout,\n-          *[shallow_filtered_substructure, values])\n-    else:\n-\n-      def dropout(i, do_dropout, v, n):\n-        if not isinstance(do_dropout, bool) or do_dropout:\n-          return self._variational_recurrent_dropout_value(i, v, n, keep_prob)\n-        else:\n-          return v\n-\n-      return _enumerated_map_structure_up_to(\n-          shallow_filtered_substructure, dropout,\n-          *[shallow_filtered_substructure, values, recurrent_noise])\n-\n-  def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n-    \"\"\"Runs the wrapped cell and applies dropout.\n-\n-    Args:\n-      inputs: A tensor with wrapped cell's input.\n-      state: A tensor or tuple of tensors with wrapped cell's state.\n-      cell_call_fn: Wrapped cell's method to use for step computation (cell's\n-        `__call__` or 'call' method).\n-      **kwargs: Additional arguments.\n-\n-    Returns:\n-      A pair containing:\n-\n-      - Output: A tensor with cell's output.\n-      - New state: A tensor or tuple of tensors with new wrapped cell's state.\n-    \"\"\"\n-\n-    def _should_dropout(p):\n-      return (not isinstance(p, float)) or p < 1\n-\n-    if _should_dropout(self._input_keep_prob):\n-      inputs = self._dropout(inputs, \"input\", self._recurrent_input_noise,\n-                             self._input_keep_prob)\n-    output, new_state = cell_call_fn(inputs, state, **kwargs)\n-    if _should_dropout(self._state_keep_prob):\n-      # Identify which subsets of the state to perform dropout on and\n-      # which ones to keep.\n-      shallow_filtered_substructure = tf.__internal__.nest.get_traverse_shallow_structure(\n-          self._dropout_state_filter, new_state)\n-      new_state = self._dropout(new_state, \"state\", self._recurrent_state_noise,\n-                                self._state_keep_prob,\n-                                shallow_filtered_substructure)\n-    if _should_dropout(self._output_keep_prob):\n-      output = self._dropout(output, \"output\", self._recurrent_output_noise,\n-                             self._output_keep_prob)\n-    return output, new_state\n-\n-  def get_config(self):\n-    \"\"\"Returns the config of the dropout wrapper.\"\"\"\n-    config = {\n-        \"input_keep_prob\": self._input_keep_prob,\n-        \"output_keep_prob\": self._output_keep_prob,\n-        \"state_keep_prob\": self._state_keep_prob,\n-        \"variational_recurrent\": self._variational_recurrent,\n-        \"input_size\": self._input_size,\n-        \"seed\": self._seed,\n-    }\n-    if self._dropout_state_filter != _default_dropout_state_filter_visitor:\n-      function, function_type, function_module = _serialize_function_to_config(\n-          self._dropout_state_filter)\n-      config.update({\"dropout_fn\": function,\n-                     \"dropout_fn_type\": function_type,\n-                     \"dropout_fn_module\": function_module})\n-    base_config = super(DropoutWrapperBase, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @classmethod\n-  def from_config(cls, config, custom_objects=None):\n-    if \"dropout_fn\" in config:\n-      config = config.copy()\n-      dropout_state_filter = _parse_config_to_function(\n-          config, custom_objects, \"dropout_fn\", \"dropout_fn_type\",\n-          \"dropout_fn_module\")\n-      config.pop(\"dropout_fn\")\n-      config[\"dropout_state_filter_visitor\"] = dropout_state_filter\n-    return super(DropoutWrapperBase, cls).from_config(\n-        config, custom_objects=custom_objects)\n-\n-\n-class ResidualWrapperBase:\n-  \"\"\"RNNCell wrapper that ensures cell inputs are added to the outputs.\"\"\"\n-\n-  def __init__(self, cell, residual_fn=None, **kwargs):\n-    \"\"\"Constructs a `ResidualWrapper` for `cell`.\n-\n-    Args:\n-      cell: An instance of `RNNCell`.\n-      residual_fn: (Optional) The function to map raw cell inputs and raw cell\n-        outputs to the actual cell outputs of the residual network.\n-        Defaults to calling nest.map_structure on (lambda i, o: i + o), inputs\n-          and outputs.\n-      **kwargs: dict of keyword arguments for base layer.\n-    \"\"\"\n-    super(ResidualWrapperBase, self).__init__(cell, **kwargs)\n-    self._residual_fn = residual_fn\n-\n-  @property\n-  def state_size(self):\n-    return self.cell.state_size\n-\n-  @property\n-  def output_size(self):\n-    return self.cell.output_size\n-\n-  def zero_state(self, batch_size, dtype):\n-    with tf.name_scope(type(self).__name__ + \"ZeroState\"):\n-      return self.cell.zero_state(batch_size, dtype)\n-\n-  def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n-    \"\"\"Run the cell and then apply the residual_fn on its inputs to its outputs.\n-\n-    Args:\n-      inputs: cell inputs.\n-      state: cell state.\n-      cell_call_fn: Wrapped cell's method to use for step computation (cell's\n-        `__call__` or 'call' method).\n-      **kwargs: Additional arguments passed to the wrapped cell's `call`.\n-\n-    Returns:\n-      Tuple of cell outputs and new state.\n-\n-    Raises:\n-      TypeError: If cell inputs and outputs have different structure (type).\n-      ValueError: If cell inputs and outputs have different structure (value).\n-    \"\"\"\n-    outputs, new_state = cell_call_fn(inputs, state, **kwargs)\n-\n-    # Ensure shapes match\n-    def assert_shape_match(inp, out):\n-      inp.get_shape().assert_is_compatible_with(out.get_shape())\n-\n-    def default_residual_fn(inputs, outputs):\n-      tf.nest.assert_same_structure(inputs, outputs)\n-      tf.nest.map_structure(assert_shape_match, inputs, outputs)\n-      return tf.nest.map_structure(lambda inp, out: inp + out, inputs, outputs)\n-\n-    res_outputs = (self._residual_fn or default_residual_fn)(inputs, outputs)\n-    return (res_outputs, new_state)\n-\n-  def get_config(self):\n-    \"\"\"Returns the config of the residual wrapper.\"\"\"\n-    if self._residual_fn is not None:\n-      function, function_type, function_module = _serialize_function_to_config(\n-          self._residual_fn)\n-      config = {\n-          \"residual_fn\": function,\n-          \"residual_fn_type\": function_type,\n-          \"residual_fn_module\": function_module\n-      }\n-    else:\n-      config = {}\n-    base_config = super(ResidualWrapperBase, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @classmethod\n-  def from_config(cls, config, custom_objects=None):\n-    if \"residual_fn\" in config:\n-      config = config.copy()\n-      residual_function = _parse_config_to_function(config, custom_objects,\n-                                                    \"residual_fn\",\n-                                                    \"residual_fn_type\",\n-                                                    \"residual_fn_module\")\n-      config[\"residual_fn\"] = residual_function\n-    return super(ResidualWrapperBase, cls).from_config(\n-        config, custom_objects=custom_objects)\n-\n-\n-class DeviceWrapperBase:\n-  \"\"\"Operator that ensures an RNNCell runs on a particular device.\"\"\"\n-\n-  def __init__(self, cell, device, **kwargs):\n-    \"\"\"Construct a `DeviceWrapper` for `cell` with device `device`.\n-\n-    Ensures the wrapped `cell` is called with `tf.device(device)`.\n-\n-    Args:\n-      cell: An instance of `RNNCell`.\n-      device: A device string or function, for passing to `tf.device`.\n-      **kwargs: dict of keyword arguments for base layer.\n-    \"\"\"\n-    super(DeviceWrapperBase, self).__init__(cell, **kwargs)\n-    self._device = device\n-\n-  @property\n-  def state_size(self):\n-    return self.cell.state_size\n-\n-  @property\n-  def output_size(self):\n-    return self.cell.output_size\n-\n-  def zero_state(self, batch_size, dtype):\n-    with tf.name_scope(type(self).__name__ + \"ZeroState\"):\n-      with tf.compat.v1.device(self._device):\n-        return self.cell.zero_state(batch_size, dtype)\n-\n-  def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n-    \"\"\"Run the cell on specified device.\"\"\"\n-    with tf.compat.v1.device(self._device):\n-      return cell_call_fn(inputs, state, **kwargs)\n-\n-  def get_config(self):\n-    config = {\"device\": self._device}\n-    base_config = super(DeviceWrapperBase, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-def _serialize_function_to_config(function):\n-  \"\"\"Serialize the function for get_config().\"\"\"\n-  if isinstance(function, python_types.LambdaType):\n-    output = generic_utils.func_dump(function)\n-    output_type = \"lambda\"\n-    module = function.__module__\n-  elif callable(function):\n-    output = function.__name__\n-    output_type = \"function\"\n-    module = function.__module__\n-  else:\n-    raise ValueError(\n-        f\"Unrecognized function type for input: {type(function)}\")\n-\n-  return output, output_type, module\n-\n-\n-def _parse_config_to_function(config, custom_objects, func_attr_name,\n-                              func_type_attr_name, module_attr_name):\n-  \"\"\"Reconstruct the function from the config.\"\"\"\n-  globs = globals()\n-  module = config.pop(module_attr_name, None)\n-  if module in sys.modules:\n-    globs.update(sys.modules[module].__dict__)\n-  elif module is not None:\n-    # Note: we don't know the name of the function if it's a lambda.\n-    warnings.warn(\n-        \"{} is not loaded, but a layer uses it. \"\n-        \"It may cause errors.\".format(module),\n-        UserWarning,\n-        stacklevel=2)\n-  if custom_objects:\n-    globs.update(custom_objects)\n-  function_type = config.pop(func_type_attr_name)\n-  if function_type == \"function\":\n-    # Simple lookup in custom objects\n-    function = generic_utils.deserialize_keras_object(\n-        config[func_attr_name],\n-        custom_objects=custom_objects,\n-        printable_module_name=\"function in wrapper\")\n-  elif function_type == \"lambda\":\n-    # Unsafe deserialization from bytecode\n-    function = generic_utils.func_load(\n-        config[func_attr_name], globs=globs)\n-  else:\n-    raise TypeError(\n-        f\"Unknown function type received: {function_type}. \"\n-        \"Expected types are ['function', 'lambda']\")\n-  return function\n-\n-\n-def _default_dropout_state_filter_visitor(substate):\n-  from keras.layers.rnn.legacy_cells import LSTMStateTuple  # pylint: disable=g-import-not-at-top\n-  if isinstance(substate, LSTMStateTuple):\n-    # Do not perform dropout on the memory state.\n-    return LSTMStateTuple(c=False, h=True)\n-  elif isinstance(substate, tf.TensorArray):\n-    return False\n-  return True\n-\n-\n-def _enumerated_map_structure_up_to(shallow_structure, map_fn, *args, **kwargs):\n-  ix = [0]\n-\n-  def enumerated_fn(*inner_args, **inner_kwargs):\n-    r = map_fn(ix[0], *inner_args, **inner_kwargs)\n-    ix[0] += 1\n-    return r\n-\n-  return tf.__internal__.nest.map_structure_up_to(shallow_structure,\n-                                                  enumerated_fn, *args,\n-                                                  **kwargs)\n\n@@ -407,7 +407,7 @@ class RNNTest(test_combinations.TestCase):\n     self.assertAllClose(y_np, y_np_3, atol=1e-4)\n \n     # Test stacking.\n-    cells = [gru_v1.GRUCell(8),\n+    cells = [gru.GRUCell(8),\n              RNNCellWithConstants(12, constant_size=3),\n              RNNCellWithConstants(32, constant_size=3)]\n     layer = keras.layers.RNN(cells)\n@@ -425,7 +425,7 @@ class RNNTest(test_combinations.TestCase):\n     # Test GRUCell reset_after property.\n     x = keras.Input((None, 5))\n     c = keras.Input((3,))\n-    cells = [gru_v1.GRUCell(32, reset_after=True)]\n+    cells = [gru.GRUCell(32, reset_after=True)]\n     layer = keras.layers.RNN(cells)\n     y = layer(x, constants=c)\n     model = keras.models.Model([x, c], y)\n@@ -468,7 +468,7 @@ class RNNTest(test_combinations.TestCase):\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n     # Test stacking.\n-    cells = [gru_v1.GRUCell(8),\n+    cells = [gru.GRUCell(8),\n              RNNCellWithConstants(12, constant_size=3),\n              RNNCellWithConstants(32, constant_size=3)]\n     layer = keras.layers.RNN(cells)\n@@ -545,7 +545,7 @@ class RNNTest(test_combinations.TestCase):\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n     # Test stacking.\n-    cells = [gru_v1.GRUCell(8),\n+    cells = [gru.GRUCell(8),\n              RNNCellWithConstants(12, constant_size=3),\n              RNNCellWithConstants(32, constant_size=3)]\n     layer = keras.layers.RNN(cells)\n\n@@ -21,11 +21,17 @@\n # probably be deprecated and removed in future since similar API is available in\n # existing Keras RNN API.\n \n+import hashlib\n+import numbers\n+import sys\n+import types as python_types\n+import warnings\n \n-from keras.layers.rnn import base_cell_wrappers\n-from keras.layers.rnn import lstm_v1\n+from keras.layers.rnn import lstm\n from keras.layers.rnn.abstract_rnn_cell import AbstractRNNCell\n+from keras.utils import generic_utils\n from keras.utils import tf_inspect\n+import tensorflow.compat.v2 as tf\n \n from tensorflow.python.util.tf_export import tf_export\n \n@@ -46,6 +52,25 @@ class _RNNCellWrapper(AbstractRNNCell):\n         cell_call_spec.varkw is not None\n     )\n \n+  def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n+    \"\"\"Calls the wrapped cell and performs the wrapping logic.\n+\n+    This method is called from the wrapper's `call` or `__call__` methods.\n+\n+    Args:\n+      inputs: A tensor with wrapped cell's input.\n+      state: A tensor or tuple of tensors with wrapped cell's state.\n+      cell_call_fn: Wrapped cell's method to use for step computation (cell's\n+        `__call__` or 'call' method).\n+      **kwargs: Additional arguments.\n+\n+    Returns:\n+      A pair containing:\n+      - Output: A tensor with cell's output.\n+      - New state: A tensor or tuple of tensors with new wrapped cell's state.\n+    \"\"\"\n+    raise NotImplementedError\n+\n   def call(self, inputs, state, **kwargs):\n     \"\"\"Runs the RNN cell step computation.\n \n@@ -75,6 +100,22 @@ class _RNNCellWrapper(AbstractRNNCell):\n     self.cell.build(inputs_shape)\n     self.built = True\n \n+  @property\n+  def wrapped_cell(self):\n+    return self.cell\n+\n+  @property\n+  def state_size(self):\n+    return self.cell.state_size\n+\n+  @property\n+  def output_size(self):\n+    return self.cell.output_size\n+\n+  def zero_state(self, batch_size, dtype):\n+    with tf.name_scope(type(self).__name__ + \"ZeroState\"):\n+      return self.cell.zero_state(batch_size, dtype)\n+\n   def get_config(self):\n     config = {\n         \"cell\": {\n@@ -94,35 +135,450 @@ class _RNNCellWrapper(AbstractRNNCell):\n \n \n @tf_export(\"nn.RNNCellDropoutWrapper\", v1=[])\n-class DropoutWrapper(base_cell_wrappers.DropoutWrapperBase, _RNNCellWrapper):\n+class DropoutWrapper(_RNNCellWrapper):\n   \"\"\"Operator adding dropout to inputs and outputs of the given cell.\"\"\"\n \n-  def __init__(self, *args, **kwargs):  # pylint: disable=useless-super-delegation\n-    super(DropoutWrapper, self).__init__(*args, **kwargs)\n-    if isinstance(self.cell, lstm_v1.LSTMCell):\n+  def __init__(self,\n+               cell,\n+               input_keep_prob=1.0,\n+               output_keep_prob=1.0,\n+               state_keep_prob=1.0,\n+               variational_recurrent=False,\n+               input_size=None,\n+               dtype=None,\n+               seed=None,\n+               dropout_state_filter_visitor=None,\n+               **kwargs):\n+    \"\"\"Create a cell with added input, state, and/or output dropout.\n+\n+    If `variational_recurrent` is set to `True` (**NOT** the default behavior),\n+    then the same dropout mask is applied at every step, as described in:\n+    [A Theoretically Grounded Application of Dropout in Recurrent\n+    Neural Networks. Y. Gal, Z. Ghahramani](https://arxiv.org/abs/1512.05287).\n+\n+    Otherwise a different dropout mask is applied at every time step.\n+\n+    Note, by default (unless a custom `dropout_state_filter` is provided),\n+    the memory state (`c` component of any `LSTMStateTuple`) passing through\n+    a `DropoutWrapper` is never modified.  This behavior is described in the\n+    above article.\n+\n+    Args:\n+      cell: an RNNCell, a projection to output_size is added to it.\n+      input_keep_prob: unit Tensor or float between 0 and 1, input keep\n+        probability; if it is constant and 1, no input dropout will be added.\n+      output_keep_prob: unit Tensor or float between 0 and 1, output keep\n+        probability; if it is constant and 1, no output dropout will be added.\n+      state_keep_prob: unit Tensor or float between 0 and 1, output keep\n+        probability; if it is constant and 1, no output dropout will be added.\n+        State dropout is performed on the outgoing states of the cell. **Note**\n+        the state components to which dropout is applied when `state_keep_prob`\n+        is in `(0, 1)` are also determined by the argument\n+        `dropout_state_filter_visitor` (e.g. by default dropout is never applied\n+        to the `c` component of an `LSTMStateTuple`).\n+      variational_recurrent: Python bool.  If `True`, then the same dropout\n+        pattern is applied across all time steps per run call. If this parameter\n+        is set, `input_size` **must** be provided.\n+      input_size: (optional) (possibly nested tuple of) `TensorShape` objects\n+        containing the depth(s) of the input tensors expected to be passed in to\n+        the `DropoutWrapper`.  Required and used **iff** `variational_recurrent\n+        = True` and `input_keep_prob < 1`.\n+      dtype: (optional) The `dtype` of the input, state, and output tensors.\n+        Required and used **iff** `variational_recurrent = True`.\n+      seed: (optional) integer, the randomness seed.\n+      dropout_state_filter_visitor: (optional), default: (see below).  Function\n+        that takes any hierarchical level of the state and returns a scalar or\n+        depth=1 structure of Python booleans describing which terms in the state\n+        should be dropped out.  In addition, if the function returns `True`,\n+        dropout is applied across this sublevel.  If the function returns\n+        `False`, dropout is not applied across this entire sublevel.\n+        Default behavior: perform dropout on all terms except the memory (`c`)\n+          state of `LSTMCellState` objects, and don't try to apply dropout to\n+        `TensorArray` objects: ```\n+        def dropout_state_filter_visitor(s):\n+          if isinstance(s, LSTMCellState): # Never perform dropout on the c\n+            state. return LSTMCellState(c=False, h=True)\n+          elif isinstance(s, TensorArray): return False return True ```\n+      **kwargs: dict of keyword arguments for base layer.\n+\n+    Raises:\n+      TypeError: if `cell` is not an `RNNCell`, or `keep_state_fn` is provided\n+        but not `callable`.\n+      ValueError: if any of the keep_probs are not between 0 and 1.\n+    \"\"\"\n+    if isinstance(cell, lstm.LSTMCell):\n       raise ValueError(\"keras LSTM cell does not work with DropoutWrapper. \"\n                        \"Please use LSTMCell(dropout=x, recurrent_dropout=y) \"\n                        \"instead.\")\n+    super(DropoutWrapper, self).__init__(cell, dtype=dtype, **kwargs)\n \n-  __init__.__doc__ = base_cell_wrappers.DropoutWrapperBase.__init__.__doc__\n+    if (dropout_state_filter_visitor is not None and\n+        not callable(dropout_state_filter_visitor)):\n+      raise TypeError(\"dropout_state_filter_visitor must be callable. \"\n+                      f\"Received: {dropout_state_filter_visitor}\")\n+    self._dropout_state_filter = (\n+        dropout_state_filter_visitor or _default_dropout_state_filter_visitor)\n+    with tf.name_scope(\"DropoutWrapperInit\"):\n+\n+      def tensor_and_const_value(v):\n+        tensor_value = tf.convert_to_tensor(v)\n+        const_value = tf.get_static_value(tensor_value)\n+        return (tensor_value, const_value)\n+\n+      for prob, attr in [(input_keep_prob, \"input_keep_prob\"),\n+                         (state_keep_prob, \"state_keep_prob\"),\n+                         (output_keep_prob, \"output_keep_prob\")]:\n+        tensor_prob, const_prob = tensor_and_const_value(prob)\n+        if const_prob is not None:\n+          if const_prob < 0 or const_prob > 1:\n+            raise ValueError(\n+                f\"Parameter {attr} must be between 0 and 1. \"\n+                \"Received {const_prob}\")\n+          setattr(self, \"_%s\" % attr, float(const_prob))\n+        else:\n+          setattr(self, \"_%s\" % attr, tensor_prob)\n+\n+    # Set variational_recurrent, seed before running the code below\n+    self._variational_recurrent = variational_recurrent\n+    self._input_size = input_size\n+    self._seed = seed\n+\n+    self._recurrent_input_noise = None\n+    self._recurrent_state_noise = None\n+    self._recurrent_output_noise = None\n+\n+    if variational_recurrent:\n+      if dtype is None:\n+        raise ValueError(\n+            \"When variational_recurrent=True, dtype must be provided\")\n+\n+      def convert_to_batch_shape(s):\n+        # Prepend a 1 for the batch dimension; for recurrent\n+        # variational dropout we use the same dropout mask for all\n+        # batch elements.\n+        return tf.concat(([1], tf.TensorShape(s).as_list()), 0)\n+\n+      def batch_noise(s, inner_seed):\n+        shape = convert_to_batch_shape(s)\n+        return tf.random.uniform(shape, seed=inner_seed, dtype=dtype)\n+\n+      if (not isinstance(self._input_keep_prob, numbers.Real) or\n+          self._input_keep_prob < 1.0):\n+        if input_size is None:\n+          raise ValueError(\n+              \"When variational_recurrent=True and input_keep_prob < 1.0 or \"\n+              \"is unknown, input_size must be provided\")\n+        self._recurrent_input_noise = _enumerated_map_structure_up_to(\n+            input_size,\n+            lambda i, s: batch_noise(s, inner_seed=self._gen_seed(\"input\", i)),\n+            input_size)\n+      self._recurrent_state_noise = _enumerated_map_structure_up_to(\n+          cell.state_size,\n+          lambda i, s: batch_noise(s, inner_seed=self._gen_seed(\"state\", i)),\n+          cell.state_size)\n+      self._recurrent_output_noise = _enumerated_map_structure_up_to(\n+          cell.output_size,\n+          lambda i, s: batch_noise(s, inner_seed=self._gen_seed(\"output\", i)),\n+          cell.output_size)\n+\n+  def _gen_seed(self, salt_prefix, index):\n+    if self._seed is None:\n+      return None\n+    salt = \"%s_%d\" % (salt_prefix, index)\n+    string = (str(self._seed) + salt).encode(\"utf-8\")\n+    return int(hashlib.md5(string).hexdigest()[:8], 16) & 0x7FFFFFFF\n+\n+  def _variational_recurrent_dropout_value(\n+      self, unused_index, value, noise, keep_prob):\n+    \"\"\"Performs dropout given the pre-calculated noise tensor.\"\"\"\n+    # uniform [keep_prob, 1.0 + keep_prob)\n+    random_tensor = keep_prob + noise\n+\n+    # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)\n+    binary_tensor = tf.floor(random_tensor)\n+    ret = tf.divide(value, keep_prob) * binary_tensor\n+    ret.set_shape(value.get_shape())\n+    return ret\n+\n+  def _dropout(self,\n+               values,\n+               salt_prefix,\n+               recurrent_noise,\n+               keep_prob,\n+               shallow_filtered_substructure=None):\n+    \"\"\"Decides whether to perform standard dropout or recurrent dropout.\"\"\"\n+\n+    if shallow_filtered_substructure is None:\n+      # Put something so we traverse the entire structure; inside the\n+      # dropout function we check to see if leafs of this are bool or not.\n+      shallow_filtered_substructure = values\n+\n+    if not self._variational_recurrent:\n+\n+      def dropout(i, do_dropout, v):\n+        if not isinstance(do_dropout, bool) or do_dropout:\n+          return tf.nn.dropout(\n+              v, rate=1. - keep_prob, seed=self._gen_seed(salt_prefix, i))\n+        else:\n+          return v\n+\n+      return _enumerated_map_structure_up_to(\n+          shallow_filtered_substructure, dropout,\n+          *[shallow_filtered_substructure, values])\n+    else:\n+\n+      def dropout(i, do_dropout, v, n):\n+        if not isinstance(do_dropout, bool) or do_dropout:\n+          return self._variational_recurrent_dropout_value(i, v, n, keep_prob)\n+        else:\n+          return v\n+\n+      return _enumerated_map_structure_up_to(\n+          shallow_filtered_substructure, dropout,\n+          *[shallow_filtered_substructure, values, recurrent_noise])\n+\n+  def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n+    \"\"\"Runs the wrapped cell and applies dropout.\n+\n+    Args:\n+      inputs: A tensor with wrapped cell's input.\n+      state: A tensor or tuple of tensors with wrapped cell's state.\n+      cell_call_fn: Wrapped cell's method to use for step computation (cell's\n+        `__call__` or 'call' method).\n+      **kwargs: Additional arguments.\n+\n+    Returns:\n+      A pair containing:\n+\n+      - Output: A tensor with cell's output.\n+      - New state: A tensor or tuple of tensors with new wrapped cell's state.\n+    \"\"\"\n+\n+    def _should_dropout(p):\n+      return (not isinstance(p, float)) or p < 1\n+\n+    if _should_dropout(self._input_keep_prob):\n+      inputs = self._dropout(inputs, \"input\", self._recurrent_input_noise,\n+                             self._input_keep_prob)\n+    output, new_state = cell_call_fn(inputs, state, **kwargs)\n+    if _should_dropout(self._state_keep_prob):\n+      # Identify which subsets of the state to perform dropout on and\n+      # which ones to keep.\n+      shallow_filtered_substructure = tf.__internal__.nest.get_traverse_shallow_structure(\n+          self._dropout_state_filter, new_state)\n+      new_state = self._dropout(new_state, \"state\", self._recurrent_state_noise,\n+                                self._state_keep_prob,\n+                                shallow_filtered_substructure)\n+    if _should_dropout(self._output_keep_prob):\n+      output = self._dropout(output, \"output\", self._recurrent_output_noise,\n+                             self._output_keep_prob)\n+    return output, new_state\n+\n+  def get_config(self):\n+    \"\"\"Returns the config of the dropout wrapper.\"\"\"\n+    config = {\n+        \"input_keep_prob\": self._input_keep_prob,\n+        \"output_keep_prob\": self._output_keep_prob,\n+        \"state_keep_prob\": self._state_keep_prob,\n+        \"variational_recurrent\": self._variational_recurrent,\n+        \"input_size\": self._input_size,\n+        \"seed\": self._seed,\n+    }\n+    if self._dropout_state_filter != _default_dropout_state_filter_visitor:  # pylint: disable=comparison-with-callable\n+      function, function_type, function_module = _serialize_function_to_config(\n+          self._dropout_state_filter)\n+      config.update({\"dropout_fn\": function,\n+                     \"dropout_fn_type\": function_type,\n+                     \"dropout_fn_module\": function_module})\n+    base_config = super(DropoutWrapper, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config, custom_objects=None):\n+    if \"dropout_fn\" in config:\n+      config = config.copy()\n+      dropout_state_filter = _parse_config_to_function(\n+          config, custom_objects, \"dropout_fn\", \"dropout_fn_type\",\n+          \"dropout_fn_module\")\n+      config.pop(\"dropout_fn\")\n+      config[\"dropout_state_filter_visitor\"] = dropout_state_filter\n+    return super(DropoutWrapper, cls).from_config(\n+        config, custom_objects=custom_objects)\n \n \n @tf_export(\"nn.RNNCellResidualWrapper\", v1=[])\n-class ResidualWrapper(base_cell_wrappers.ResidualWrapperBase,\n-                      _RNNCellWrapper):\n+class ResidualWrapper(_RNNCellWrapper):\n   \"\"\"RNNCell wrapper that ensures cell inputs are added to the outputs.\"\"\"\n \n-  def __init__(self, *args, **kwargs):  # pylint: disable=useless-super-delegation\n-    super(ResidualWrapper, self).__init__(*args, **kwargs)\n+  def __init__(self, cell, residual_fn=None, **kwargs):\n+    \"\"\"Constructs a `ResidualWrapper` for `cell`.\n \n-  __init__.__doc__ = base_cell_wrappers.ResidualWrapperBase.__init__.__doc__\n+    Args:\n+      cell: An instance of `RNNCell`.\n+      residual_fn: (Optional) The function to map raw cell inputs and raw cell\n+        outputs to the actual cell outputs of the residual network.\n+        Defaults to calling nest.map_structure on (lambda i, o: i + o), inputs\n+          and outputs.\n+      **kwargs: dict of keyword arguments for base layer.\n+    \"\"\"\n+    super(ResidualWrapper, self).__init__(cell, **kwargs)\n+    self._residual_fn = residual_fn\n+\n+  def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n+    \"\"\"Run the cell and then apply the residual_fn on its inputs to its outputs.\n+\n+    Args:\n+      inputs: cell inputs.\n+      state: cell state.\n+      cell_call_fn: Wrapped cell's method to use for step computation (cell's\n+        `__call__` or 'call' method).\n+      **kwargs: Additional arguments passed to the wrapped cell's `call`.\n+\n+    Returns:\n+      Tuple of cell outputs and new state.\n+\n+    Raises:\n+      TypeError: If cell inputs and outputs have different structure (type).\n+      ValueError: If cell inputs and outputs have different structure (value).\n+    \"\"\"\n+    outputs, new_state = cell_call_fn(inputs, state, **kwargs)\n+\n+    # Ensure shapes match\n+    def assert_shape_match(inp, out):\n+      inp.get_shape().assert_is_compatible_with(out.get_shape())\n+\n+    def default_residual_fn(inputs, outputs):\n+      tf.nest.assert_same_structure(inputs, outputs)\n+      tf.nest.map_structure(assert_shape_match, inputs, outputs)\n+      return tf.nest.map_structure(lambda inp, out: inp + out, inputs, outputs)\n+\n+    res_outputs = (self._residual_fn or default_residual_fn)(inputs, outputs)\n+    return (res_outputs, new_state)\n+\n+  def get_config(self):\n+    \"\"\"Returns the config of the residual wrapper.\"\"\"\n+    if self._residual_fn is not None:\n+      function, function_type, function_module = _serialize_function_to_config(\n+          self._residual_fn)\n+      config = {\n+          \"residual_fn\": function,\n+          \"residual_fn_type\": function_type,\n+          \"residual_fn_module\": function_module\n+      }\n+    else:\n+      config = {}\n+    base_config = super(ResidualWrapper, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config, custom_objects=None):\n+    if \"residual_fn\" in config:\n+      config = config.copy()\n+      residual_function = _parse_config_to_function(config, custom_objects,\n+                                                    \"residual_fn\",\n+                                                    \"residual_fn_type\",\n+                                                    \"residual_fn_module\")\n+      config[\"residual_fn\"] = residual_function\n+    return super(ResidualWrapper, cls).from_config(\n+        config, custom_objects=custom_objects)\n \n \n @tf_export(\"nn.RNNCellDeviceWrapper\", v1=[])\n-class DeviceWrapper(base_cell_wrappers.DeviceWrapperBase, _RNNCellWrapper):\n+class DeviceWrapper(_RNNCellWrapper):\n   \"\"\"Operator that ensures an RNNCell runs on a particular device.\"\"\"\n \n-  def __init__(self, *args, **kwargs):  # pylint: disable=useless-super-delegation\n-    super(DeviceWrapper, self).__init__(*args, **kwargs)\n+  def __init__(self, cell, device, **kwargs):\n+    \"\"\"Construct a `DeviceWrapper` for `cell` with device `device`.\n \n-  __init__.__doc__ = base_cell_wrappers.DeviceWrapperBase.__init__.__doc__\n+    Ensures the wrapped `cell` is called with `tf.device(device)`.\n+\n+    Args:\n+      cell: An instance of `RNNCell`.\n+      device: A device string or function, for passing to `tf.device`.\n+      **kwargs: dict of keyword arguments for base layer.\n+    \"\"\"\n+    super(DeviceWrapper, self).__init__(cell, **kwargs)\n+    self._device = device\n+\n+  def zero_state(self, batch_size, dtype):\n+    with tf.name_scope(type(self).__name__ + \"ZeroState\"):\n+      with tf.compat.v1.device(self._device):\n+        return self.cell.zero_state(batch_size, dtype)\n+\n+  def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n+    \"\"\"Run the cell on specified device.\"\"\"\n+    with tf.compat.v1.device(self._device):\n+      return cell_call_fn(inputs, state, **kwargs)\n+\n+  def get_config(self):\n+    config = {\"device\": self._device}\n+    base_config = super(DeviceWrapper, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+def _serialize_function_to_config(function):\n+  \"\"\"Serialize the function for get_config().\"\"\"\n+  if isinstance(function, python_types.LambdaType):\n+    output = generic_utils.func_dump(function)\n+    output_type = \"lambda\"\n+    module = function.__module__\n+  elif callable(function):\n+    output = function.__name__\n+    output_type = \"function\"\n+    module = function.__module__\n+  else:\n+    raise ValueError(\n+        f\"Unrecognized function type for input: {type(function)}\")\n+\n+  return output, output_type, module\n+\n+\n+def _parse_config_to_function(config, custom_objects, func_attr_name,\n+                              func_type_attr_name, module_attr_name):\n+  \"\"\"Reconstruct the function from the config.\"\"\"\n+  globs = globals()\n+  module = config.pop(module_attr_name, None)\n+  if module in sys.modules:\n+    globs.update(sys.modules[module].__dict__)\n+  elif module is not None:\n+    # Note: we don't know the name of the function if it's a lambda.\n+    warnings.warn(\n+        \"{} is not loaded, but a layer uses it. \"\n+        \"It may cause errors.\".format(module),\n+        UserWarning,\n+        stacklevel=2)\n+  if custom_objects:\n+    globs.update(custom_objects)\n+  function_type = config.pop(func_type_attr_name)\n+  if function_type == \"function\":\n+    # Simple lookup in custom objects\n+    function = generic_utils.deserialize_keras_object(\n+        config[func_attr_name],\n+        custom_objects=custom_objects,\n+        printable_module_name=\"function in wrapper\")\n+  elif function_type == \"lambda\":\n+    # Unsafe deserialization from bytecode\n+    function = generic_utils.func_load(\n+        config[func_attr_name], globs=globs)\n+  else:\n+    raise TypeError(\n+        f\"Unknown function type received: {function_type}. \"\n+        \"Expected types are ['function', 'lambda']\")\n+  return function\n+\n+\n+def _default_dropout_state_filter_visitor(substate):\n+  return not isinstance(substate, tf.TensorArray)\n+\n+\n+def _enumerated_map_structure_up_to(shallow_structure, map_fn, *args, **kwargs):\n+  ix = [0]\n+\n+  def enumerated_fn(*inner_args, **inner_kwargs):\n+    r = map_fn(ix[0], *inner_args, **inner_kwargs)\n+    ix[0] += 1\n+    return r\n+\n+  return tf.__internal__.nest.map_structure_up_to(shallow_structure,\n+                                                  enumerated_fn, *args,\n+                                                  **kwargs)\n\n@@ -19,18 +19,29 @@ import uuid\n \n from keras import activations\n from keras import backend\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n from keras.engine import base_layer\n+from keras.engine.input_spec import InputSpec\n from keras.layers.rnn import gru_lstm_utils\n-from keras.layers.rnn import gru_v1\n+from keras.layers.rnn import rnn_utils\n+from keras.layers.rnn.base_rnn import RNN\n from keras.layers.rnn.dropout_rnn_cell_mixin import DropoutRNNCellMixin\n+from keras.utils import tf_utils\n import tensorflow.compat.v2 as tf\n \n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n \n+RECURRENT_DROPOUT_WARNING_MSG = (\n+    'RNN `implementation=2` is not supported when `recurrent_dropout` is set. '\n+    'Using `implementation=1`.')\n+\n+\n @keras_export('keras.layers.GRUCell', v1=[])\n-class GRUCell(gru_v1.GRUCell):\n+class GRUCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n   \"\"\"Cell class for the GRU layer.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n@@ -120,29 +131,224 @@ class GRUCell(gru_v1.GRUCell):\n                recurrent_dropout=0.,\n                reset_after=True,\n                **kwargs):\n-    super(GRUCell, self).__init__(\n-        units,\n-        activation=activation,\n-        recurrent_activation=recurrent_activation,\n-        use_bias=use_bias,\n-        kernel_initializer=kernel_initializer,\n-        recurrent_initializer=recurrent_initializer,\n-        bias_initializer=bias_initializer,\n-        kernel_regularizer=kernel_regularizer,\n-        recurrent_regularizer=recurrent_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        kernel_constraint=kernel_constraint,\n-        recurrent_constraint=recurrent_constraint,\n-        bias_constraint=bias_constraint,\n-        dropout=dropout,\n-        recurrent_dropout=recurrent_dropout,\n-        implementation=kwargs.pop('implementation', 2),\n-        reset_after=reset_after,\n-        **kwargs)\n+    if units < 0:\n+      raise ValueError(f'Received an invalid value for argument `units`, '\n+                       f'expected a positive integer, got {units}.')\n+    # By default use cached variable under v2 mode, see b/143699808.\n+    if tf.compat.v1.executing_eagerly_outside_functions():\n+      self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n+    else:\n+      self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n+    super(GRUCell, self).__init__(**kwargs)\n+    self.units = units\n+    self.activation = activations.get(activation)\n+    self.recurrent_activation = activations.get(recurrent_activation)\n+    self.use_bias = use_bias\n+\n+    self.kernel_initializer = initializers.get(kernel_initializer)\n+    self.recurrent_initializer = initializers.get(recurrent_initializer)\n+    self.bias_initializer = initializers.get(bias_initializer)\n+\n+    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n+    self.bias_regularizer = regularizers.get(bias_regularizer)\n+\n+    self.kernel_constraint = constraints.get(kernel_constraint)\n+    self.recurrent_constraint = constraints.get(recurrent_constraint)\n+    self.bias_constraint = constraints.get(bias_constraint)\n+\n+    self.dropout = min(1., max(0., dropout))\n+    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n+\n+    implementation = kwargs.pop('implementation', 2)\n+    if self.recurrent_dropout != 0 and implementation != 1:\n+      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)\n+      self.implementation = 1\n+    else:\n+      self.implementation = implementation\n+    self.reset_after = reset_after\n+    self.state_size = self.units\n+    self.output_size = self.units\n+\n+  @tf_utils.shape_type_conversion\n+  def build(self, input_shape):\n+    input_dim = input_shape[-1]\n+    default_caching_device = rnn_utils.caching_device(self)\n+    self.kernel = self.add_weight(\n+        shape=(input_dim, self.units * 3),\n+        name='kernel',\n+        initializer=self.kernel_initializer,\n+        regularizer=self.kernel_regularizer,\n+        constraint=self.kernel_constraint,\n+        caching_device=default_caching_device)\n+    self.recurrent_kernel = self.add_weight(\n+        shape=(self.units, self.units * 3),\n+        name='recurrent_kernel',\n+        initializer=self.recurrent_initializer,\n+        regularizer=self.recurrent_regularizer,\n+        constraint=self.recurrent_constraint,\n+        caching_device=default_caching_device)\n+\n+    if self.use_bias:\n+      if not self.reset_after:\n+        bias_shape = (3 * self.units,)\n+      else:\n+        # separate biases for input and recurrent kernels\n+        # Note: the shape is intentionally different from CuDNNGRU biases\n+        # `(2 * 3 * self.units,)`, so that we can distinguish the classes\n+        # when loading and converting saved weights.\n+        bias_shape = (2, 3 * self.units)\n+      self.bias = self.add_weight(shape=bias_shape,\n+                                  name='bias',\n+                                  initializer=self.bias_initializer,\n+                                  regularizer=self.bias_regularizer,\n+                                  constraint=self.bias_constraint,\n+                                  caching_device=default_caching_device)\n+    else:\n+      self.bias = None\n+    self.built = True\n+\n+  def call(self, inputs, states, training=None):\n+    h_tm1 = states[0] if tf.nest.is_nested(\n+        states) else states  # previous memory\n+\n+    dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=3)\n+    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n+        h_tm1, training, count=3)\n+\n+    if self.use_bias:\n+      if not self.reset_after:\n+        input_bias, recurrent_bias = self.bias, None\n+      else:\n+        input_bias, recurrent_bias = tf.unstack(self.bias)\n+\n+    if self.implementation == 1:\n+      if 0. < self.dropout < 1.:\n+        inputs_z = inputs * dp_mask[0]\n+        inputs_r = inputs * dp_mask[1]\n+        inputs_h = inputs * dp_mask[2]\n+      else:\n+        inputs_z = inputs\n+        inputs_r = inputs\n+        inputs_h = inputs\n+\n+      x_z = backend.dot(inputs_z, self.kernel[:, :self.units])\n+      x_r = backend.dot(inputs_r, self.kernel[:, self.units:self.units * 2])\n+      x_h = backend.dot(inputs_h, self.kernel[:, self.units * 2:])\n+\n+      if self.use_bias:\n+        x_z = backend.bias_add(x_z, input_bias[:self.units])\n+        x_r = backend.bias_add(x_r, input_bias[self.units: self.units * 2])\n+        x_h = backend.bias_add(x_h, input_bias[self.units * 2:])\n+\n+      if 0. < self.recurrent_dropout < 1.:\n+        h_tm1_z = h_tm1 * rec_dp_mask[0]\n+        h_tm1_r = h_tm1 * rec_dp_mask[1]\n+        h_tm1_h = h_tm1 * rec_dp_mask[2]\n+      else:\n+        h_tm1_z = h_tm1\n+        h_tm1_r = h_tm1\n+        h_tm1_h = h_tm1\n+\n+      recurrent_z = backend.dot(h_tm1_z, self.recurrent_kernel[:, :self.units])\n+      recurrent_r = backend.dot(\n+          h_tm1_r, self.recurrent_kernel[:, self.units:self.units * 2])\n+      if self.reset_after and self.use_bias:\n+        recurrent_z = backend.bias_add(recurrent_z, recurrent_bias[:self.units])\n+        recurrent_r = backend.bias_add(\n+            recurrent_r, recurrent_bias[self.units:self.units * 2])\n+\n+      z = self.recurrent_activation(x_z + recurrent_z)\n+      r = self.recurrent_activation(x_r + recurrent_r)\n+\n+      # reset gate applied after/before matrix multiplication\n+      if self.reset_after:\n+        recurrent_h = backend.dot(\n+            h_tm1_h, self.recurrent_kernel[:, self.units * 2:])\n+        if self.use_bias:\n+          recurrent_h = backend.bias_add(\n+              recurrent_h, recurrent_bias[self.units * 2:])\n+        recurrent_h = r * recurrent_h\n+      else:\n+        recurrent_h = backend.dot(\n+            r * h_tm1_h, self.recurrent_kernel[:, self.units * 2:])\n+\n+      hh = self.activation(x_h + recurrent_h)\n+    else:\n+      if 0. < self.dropout < 1.:\n+        inputs = inputs * dp_mask[0]\n+\n+      # inputs projected by all gate matrices at once\n+      matrix_x = backend.dot(inputs, self.kernel)\n+      if self.use_bias:\n+        # biases: bias_z_i, bias_r_i, bias_h_i\n+        matrix_x = backend.bias_add(matrix_x, input_bias)\n+\n+      x_z, x_r, x_h = tf.split(matrix_x, 3, axis=-1)\n+\n+      if self.reset_after:\n+        # hidden state projected by all gate matrices at once\n+        matrix_inner = backend.dot(h_tm1, self.recurrent_kernel)\n+        if self.use_bias:\n+          matrix_inner = backend.bias_add(matrix_inner, recurrent_bias)\n+      else:\n+        # hidden state projected separately for update/reset and new\n+        matrix_inner = backend.dot(\n+            h_tm1, self.recurrent_kernel[:, :2 * self.units])\n+\n+      recurrent_z, recurrent_r, recurrent_h = tf.split(\n+          matrix_inner, [self.units, self.units, -1], axis=-1)\n+\n+      z = self.recurrent_activation(x_z + recurrent_z)\n+      r = self.recurrent_activation(x_r + recurrent_r)\n+\n+      if self.reset_after:\n+        recurrent_h = r * recurrent_h\n+      else:\n+        recurrent_h = backend.dot(\n+            r * h_tm1, self.recurrent_kernel[:, 2 * self.units:])\n+\n+      hh = self.activation(x_h + recurrent_h)\n+    # previous and candidate state mixed by update gate\n+    h = z * h_tm1 + (1 - z) * hh\n+    new_state = [h] if tf.nest.is_nested(states) else h\n+    return h, new_state\n+\n+  def get_config(self):\n+    config = {\n+        'units': self.units,\n+        'activation': activations.serialize(self.activation),\n+        'recurrent_activation':\n+            activations.serialize(self.recurrent_activation),\n+        'use_bias': self.use_bias,\n+        'kernel_initializer': initializers.serialize(self.kernel_initializer),\n+        'recurrent_initializer':\n+            initializers.serialize(self.recurrent_initializer),\n+        'bias_initializer': initializers.serialize(self.bias_initializer),\n+        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n+        'recurrent_regularizer':\n+            regularizers.serialize(self.recurrent_regularizer),\n+        'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n+        'kernel_constraint': constraints.serialize(self.kernel_constraint),\n+        'recurrent_constraint':\n+            constraints.serialize(self.recurrent_constraint),\n+        'bias_constraint': constraints.serialize(self.bias_constraint),\n+        'dropout': self.dropout,\n+        'recurrent_dropout': self.recurrent_dropout,\n+        'implementation': self.implementation,\n+        'reset_after': self.reset_after\n+    }\n+    config.update(rnn_utils.config_for_enable_caching_device(self))\n+    base_config = super(GRUCell, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n+    return rnn_utils.generate_zero_filled_state_for_cell(\n+        self, inputs, batch_size, dtype)\n \n \n @keras_export('keras.layers.GRU', v1=[])\n-class GRU(DropoutRNNCellMixin, gru_v1.GRU, base_layer.BaseRandomLayer):\n+class GRU(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n   \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n@@ -298,8 +504,17 @@ class GRU(DropoutRNNCellMixin, gru_v1.GRU, base_layer.BaseRandomLayer):\n     # return_runtime is a flag for testing, which shows the real backend\n     # implementation chosen by grappler in graph mode.\n     self._return_runtime = kwargs.pop('return_runtime', False)\n-\n-    super(GRU, self).__init__(\n+    implementation = kwargs.pop('implementation', 2)\n+    if implementation == 0:\n+      logging.warning('`implementation=0` has been deprecated, '\n+                      'and now defaults to `implementation=2`.'\n+                      'Please update your layer call.')\n+    if 'enable_caching_device' in kwargs:\n+      cell_kwargs = {'enable_caching_device':\n+                     kwargs.pop('enable_caching_device')}\n+    else:\n+      cell_kwargs = {}\n+    cell = GRUCell(\n         units,\n         activation=activation,\n         recurrent_activation=recurrent_activation,\n@@ -310,21 +525,28 @@ class GRU(DropoutRNNCellMixin, gru_v1.GRU, base_layer.BaseRandomLayer):\n         kernel_regularizer=kernel_regularizer,\n         recurrent_regularizer=recurrent_regularizer,\n         bias_regularizer=bias_regularizer,\n-        activity_regularizer=activity_regularizer,\n         kernel_constraint=kernel_constraint,\n         recurrent_constraint=recurrent_constraint,\n         bias_constraint=bias_constraint,\n         dropout=dropout,\n         recurrent_dropout=recurrent_dropout,\n-        implementation=kwargs.pop('implementation', 2),\n+        implementation=implementation,\n+        reset_after=reset_after,\n+        dtype=kwargs.get('dtype'),\n+        trainable=kwargs.get('trainable', True),\n+        **cell_kwargs)\n+    super(GRU, self).__init__(\n+        cell,\n         return_sequences=return_sequences,\n         return_state=return_state,\n         go_backwards=go_backwards,\n         stateful=stateful,\n         unroll=unroll,\n         time_major=time_major,\n-        reset_after=reset_after,\n         **kwargs)\n+    self.activity_regularizer = regularizers.get(activity_regularizer)\n+    self.input_spec = [InputSpec(ndim=3)]\n+\n     # GPU kernel uses following setting by default and not configurable.\n     self._could_use_gpu_kernel = (\n         self.activation in (activations.tanh, tf.tanh) and\n@@ -401,6 +623,124 @@ class GRU(DropoutRNNCellMixin, gru_v1.GRU, base_layer.BaseRandomLayer):\n     else:\n       return output\n \n+  @property\n+  def units(self):\n+    return self.cell.units\n+\n+  @property\n+  def activation(self):\n+    return self.cell.activation\n+\n+  @property\n+  def recurrent_activation(self):\n+    return self.cell.recurrent_activation\n+\n+  @property\n+  def use_bias(self):\n+    return self.cell.use_bias\n+\n+  @property\n+  def kernel_initializer(self):\n+    return self.cell.kernel_initializer\n+\n+  @property\n+  def recurrent_initializer(self):\n+    return self.cell.recurrent_initializer\n+\n+  @property\n+  def bias_initializer(self):\n+    return self.cell.bias_initializer\n+\n+  @property\n+  def kernel_regularizer(self):\n+    return self.cell.kernel_regularizer\n+\n+  @property\n+  def recurrent_regularizer(self):\n+    return self.cell.recurrent_regularizer\n+\n+  @property\n+  def bias_regularizer(self):\n+    return self.cell.bias_regularizer\n+\n+  @property\n+  def kernel_constraint(self):\n+    return self.cell.kernel_constraint\n+\n+  @property\n+  def recurrent_constraint(self):\n+    return self.cell.recurrent_constraint\n+\n+  @property\n+  def bias_constraint(self):\n+    return self.cell.bias_constraint\n+\n+  @property\n+  def dropout(self):\n+    return self.cell.dropout\n+\n+  @property\n+  def recurrent_dropout(self):\n+    return self.cell.recurrent_dropout\n+\n+  @property\n+  def implementation(self):\n+    return self.cell.implementation\n+\n+  @property\n+  def reset_after(self):\n+    return self.cell.reset_after\n+\n+  def get_config(self):\n+    config = {\n+        'units':\n+            self.units,\n+        'activation':\n+            activations.serialize(self.activation),\n+        'recurrent_activation':\n+            activations.serialize(self.recurrent_activation),\n+        'use_bias':\n+            self.use_bias,\n+        'kernel_initializer':\n+            initializers.serialize(self.kernel_initializer),\n+        'recurrent_initializer':\n+            initializers.serialize(self.recurrent_initializer),\n+        'bias_initializer':\n+            initializers.serialize(self.bias_initializer),\n+        'kernel_regularizer':\n+            regularizers.serialize(self.kernel_regularizer),\n+        'recurrent_regularizer':\n+            regularizers.serialize(self.recurrent_regularizer),\n+        'bias_regularizer':\n+            regularizers.serialize(self.bias_regularizer),\n+        'activity_regularizer':\n+            regularizers.serialize(self.activity_regularizer),\n+        'kernel_constraint':\n+            constraints.serialize(self.kernel_constraint),\n+        'recurrent_constraint':\n+            constraints.serialize(self.recurrent_constraint),\n+        'bias_constraint':\n+            constraints.serialize(self.bias_constraint),\n+        'dropout':\n+            self.dropout,\n+        'recurrent_dropout':\n+            self.recurrent_dropout,\n+        'implementation':\n+            self.implementation,\n+        'reset_after':\n+            self.reset_after\n+    }\n+    config.update(rnn_utils.config_for_enable_caching_device(self.cell))\n+    base_config = super(GRU, self).get_config()\n+    del base_config['cell']\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config):\n+    if 'implementation' in config and config['implementation'] == 0:\n+      config['implementation'] = 1\n+    return cls(**config)\n+\n   def _defun_gru_call(self, inputs, initial_state, training, mask,\n                       sequence_lengths):\n     # Use the new defun approach for backend implementation swap.\n\n@@ -21,9 +21,7 @@ import shutil\n \n from absl.testing import parameterized\n import keras\n-from keras.layers.rnn import gru\n from keras.layers.rnn import gru_lstm_utils\n-from keras.layers.rnn import gru_v1\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import np_utils\n@@ -44,7 +42,14 @@ _config = tf.compat.v1.ConfigProto(graph_options=_graph_options)\n \n @test_utils.run_all_without_tensor_float_32('RNN GRU can use TF32 on GPU')\n @test_combinations.run_all_keras_modes(config=_config)\n-class GRUV2Test(test_combinations.TestCase):\n+class GRUGraphRewriteTest(test_combinations.TestCase):\n+\n+  input_shape = 10\n+  output_shape = 8\n+  rnn_state_size = 8\n+  timestep = 4\n+  batch = 100\n+  epoch = 1\n \n   @parameterized.named_parameters(\n       ('non_tan_activation', 'relu', 'sigmoid', 0, False, True, True),\n@@ -54,10 +59,12 @@ class GRUV2Test(test_combinations.TestCase):\n       ('not_use_bias', 'tanh', 'sigmoid', 0, False, False, True),\n       ('not_reset_after', 'tanh', 'sigmoid', 0, False, True, False)\n   )\n+  @test_utils.run_v2_only\n   def test_could_use_defun_backend(self, activation, recurrent_activation,\n                                    recurrent_dropout, unroll, use_bias,\n                                    reset_after):\n-    layer = gru.GRU(1,\n+    layer = keras.layers.GRU(\n+        1,\n         activation=activation,\n         recurrent_activation=recurrent_activation,\n         recurrent_dropout=recurrent_dropout,\n@@ -68,31 +75,26 @@ class GRUV2Test(test_combinations.TestCase):\n \n   @test_utils.run_v2_only\n   def test_use_on_default_activation_with_gpu_kernel(self):\n-    layer = gru.GRU(1, activation=tf.tanh)\n+    layer = keras.layers.GRU(1, activation=tf.tanh)\n     self.assertTrue(layer._could_use_gpu_kernel)\n \n-    layer = gru.GRU(1, recurrent_activation=tf.sigmoid)\n+    layer = keras.layers.GRU(1, recurrent_activation=tf.sigmoid)\n     self.assertTrue(layer._could_use_gpu_kernel)\n \n   def test_keras_model_with_gru(self):\n-    input_shape = 10\n-    rnn_state_size = 8\n-    output_shape = 8\n-    timestep = 4\n-    batch = 100\n     epoch = 10\n \n     (x_train, y_train), _ = test_utils.get_test_data(\n-        train_samples=batch,\n+        train_samples=self.batch,\n         test_samples=0,\n-        input_shape=(timestep, input_shape),\n-        num_classes=output_shape)\n-    y_train = np_utils.to_categorical(y_train, output_shape)\n+        input_shape=(self.timestep, self.input_shape),\n+        num_classes=self.output_shape)\n+    y_train = np_utils.to_categorical(y_train, self.output_shape)\n \n-    layer = gru.GRU(rnn_state_size)\n+    layer = keras.layers.GRU(self.rnn_state_size)\n \n     inputs = keras.layers.Input(\n-        shape=[timestep, input_shape], dtype=tf.float32)\n+        shape=[self.timestep, self.input_shape], dtype=tf.float32)\n \n     outputs = layer(inputs)\n     model = keras.models.Model(inputs, outputs)\n@@ -106,7 +108,7 @@ class GRUV2Test(test_combinations.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    layer = gru.GRU(units, input_shape=(None, embedding_dim))\n+    layer = keras.layers.GRU(units, input_shape=(None, embedding_dim))\n     model = keras.models.Sequential()\n     model.add(layer)\n     model.compile(tf.compat.v1.train.GradientDescentOptimizer(0.001), 'mse')\n@@ -119,70 +121,20 @@ class GRUV2Test(test_combinations.TestCase):\n     targets = np.abs(np.random.random((2, 3, 5)))\n     targets /= targets.sum(axis=-1, keepdims=True)\n     model = keras.models.Sequential()\n-    model.add(gru.GRU(10, return_sequences=True, unroll=False))\n-    model.add(gru.GRU(5, return_sequences=True, unroll=False))\n+    model.add(keras.layers.GRU(10, return_sequences=True, unroll=False))\n+    model.add(keras.layers.GRU(5, return_sequences=True, unroll=False))\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))\n     model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n \n   def test_from_config_GRU(self):\n-    layer_class = gru.GRU\n+    layer_class = keras.layers.GRU\n     for stateful in (False, True):\n       l1 = layer_class(units=1, stateful=stateful)\n       l2 = layer_class.from_config(l1.get_config())\n       assert l1.get_config() == l2.get_config()\n \n-  @tf.test.disable_with_predicate(\n-      pred=tf.test.is_built_with_rocm,\n-      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n-  @test_utils.run_v2_only\n-  def test_gru_v2_feature_parity_with_canonical_gru(self):\n-    input_shape = 10\n-    rnn_state_size = 8\n-    timestep = 4\n-    batch = 20\n-\n-    (x_train, y_train), _ = test_utils.get_test_data(\n-        train_samples=batch,\n-        test_samples=0,\n-        input_shape=(timestep, input_shape),\n-        num_classes=rnn_state_size,\n-        random_seed=87654321)\n-    y_train = np_utils.to_categorical(y_train, rnn_state_size)\n-    # For the last batch item of the test data, we filter out the last\n-    # timestep to simulate the variable length sequence and masking test.\n-    x_train[-2:, -1, :] = 0.0\n-    y_train[-2:] = 0\n-\n-    inputs = keras.layers.Input(\n-        shape=[timestep, input_shape], dtype=tf.float32)\n-    masked_input = keras.layers.Masking()(inputs)\n-    gru_layer = gru_v1.GRU(rnn_state_size,\n-                           recurrent_activation='sigmoid',\n-                           reset_after=True)\n-    output = gru_layer(masked_input)\n-    gru_model = keras.models.Model(inputs, output)\n-    weights = gru_model.get_weights()\n-    y_1 = gru_model.predict(x_train)\n-    gru_model.compile('rmsprop', 'mse')\n-    gru_model.fit(x_train, y_train)\n-    y_2 = gru_model.predict(x_train)\n-\n-    with test_utils.device(should_use_gpu=True):\n-      cudnn_layer = gru.GRU(rnn_state_size,\n-                            recurrent_activation='sigmoid',\n-                            reset_after=True)\n-      cudnn_model = keras.models.Model(inputs, cudnn_layer(masked_input))\n-    cudnn_model.set_weights(weights)\n-    y_3 = cudnn_model.predict(x_train)\n-    cudnn_model.compile('rmsprop', 'mse')\n-    cudnn_model.fit(x_train, y_train)\n-    y_4 = cudnn_model.predict(x_train)\n-\n-    self.assertAllClose(y_1, y_3, rtol=2e-5, atol=2e-5)\n-    self.assertAllClose(y_2, y_4, rtol=2e-5, atol=2e-5)\n-\n   @parameterized.named_parameters(\n       # test_name, use_bias, bias_initializer, activation\n       ('normal', True, 'zeros'),\n@@ -204,7 +156,7 @@ class GRUV2Test(test_combinations.TestCase):\n     def build_model():\n       inputs = keras.layers.Input(\n           shape=[timestep, input_dim], dtype=tf.float32)\n-      layer = gru.GRU(\n+      layer = keras.layers.GRU(\n           units,\n           use_bias=use_bias,\n           bias_initializer=bias_initializer)\n@@ -223,93 +175,31 @@ class GRUV2Test(test_combinations.TestCase):\n     self.assertAllClose(layer.get_weights(), new_layer.get_weights())\n \n   def test_gru_v2_output_on_multiple_kernel(self):\n-    input_shape = 10\n-    rnn_state_size = 8\n-    timestep = 4\n-    batch = 100\n-\n-    x_train = np.random.random((batch, timestep, input_shape))\n+    x_train = np.random.random((self.batch, self.timestep, self.input_shape))\n \n     inputs = keras.layers.Input(\n-        shape=[timestep, input_shape], dtype=tf.float32)\n+        shape=[self.timestep, self.input_shape], dtype=tf.float32)\n     with test_utils.device(should_use_gpu=False):\n-      layer = gru.GRU(rnn_state_size)\n+      layer = keras.layers.GRU(self.rnn_state_size)\n       output = layer(inputs)\n       cpu_model = keras.models.Model(inputs, output)\n       weights = cpu_model.get_weights()\n       y_1 = cpu_model.predict(x_train)\n \n     with test_utils.device(should_use_gpu=True):\n-      layer = gru.GRU(rnn_state_size)\n+      layer = keras.layers.GRU(self.rnn_state_size)\n       output = layer(inputs)\n       gpu_model = keras.models.Model(inputs, output)\n       gpu_model.set_weights(weights)\n       y_2 = gpu_model.predict(x_train)\n \n-    # Note that cuDNN uses 'sigmoid' as activation, so the GRU V2 uses\n-    # 'sigmoid' as default. Construct the canonical GRU with sigmoid to achieve\n-    # the same output.\n-    with test_utils.device(should_use_gpu=True):\n-      layer = gru_v1.GRU(rnn_state_size,\n-                         recurrent_activation='sigmoid',\n-                         reset_after=True)\n-      output = layer(inputs)\n-      canonical_model = keras.models.Model(inputs, output)\n-      canonical_model.set_weights(weights)\n-      y_3 = canonical_model.predict(x_train)\n-\n     self.assertAllClose(y_1, y_2, rtol=1e-5, atol=1e-5)\n-    self.assertAllClose(y_2, y_3, rtol=1e-5, atol=1e-5)\n-\n-  @parameterized.named_parameters(\n-      # test_name, time_major, go_backwards\n-      ('normal', False, False),\n-      ('time_major', True, False),\n-      ('go_backwards', False, True),\n-      ('both', True, True),\n-  )\n-  def test_time_major_and_go_backward(self, time_major, go_backwards):\n-    input_shape = 10\n-    rnn_state_size = 8\n-    timestep = 4\n-    batch = 100\n-\n-    x_train = np.random.random((batch, timestep, input_shape))\n-\n-    def build_model(layer_cls):\n-      inputs = keras.layers.Input(\n-          shape=[timestep, input_shape], dtype=tf.float32)\n-      layer = layer_cls(rnn_state_size,\n-                        recurrent_activation='sigmoid',\n-                        time_major=time_major,\n-                        return_sequences=True,\n-                        go_backwards=go_backwards,\n-                        reset_after=True)\n-      if time_major:\n-        converted_input = keras.layers.Lambda(\n-            lambda t: tf.transpose(t, [1, 0, 2]))(inputs)\n-        outputs = layer(converted_input)\n-        outputs = keras.layers.Lambda(\n-            lambda t: tf.transpose(t, [1, 0, 2]))(outputs)\n-      else:\n-        outputs = layer(inputs)\n-      return keras.models.Model(inputs, outputs)\n-\n-    gru_model = build_model(gru_v1.GRU)\n-    y_ref = gru_model.predict(x_train)\n-    weights = gru_model.get_weights()\n-\n-    gru_v2_model = build_model(gru.GRU)\n-    gru_v2_model.set_weights(weights)\n-    y = gru_v2_model.predict(x_train)\n-\n-    self.assertAllClose(y, y_ref)\n \n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n       skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   def test_with_masking_layer_GRU(self):\n-    layer_class = gru.GRU\n+    layer_class = keras.layers.GRU\n     inputs = np.random.random((2, 3, 4))\n     targets = np.abs(np.random.random((2, 3, 5)))\n     targets /= targets.sum(axis=-1, keepdims=True)\n@@ -329,8 +219,8 @@ class GRUV2Test(test_combinations.TestCase):\n     targets /= targets.sum(axis=-1, keepdims=True)\n     model = keras.models.Sequential()\n     model.add(keras.layers.Masking(input_shape=(3, 4)))\n-    model.add(gru.GRU(10, return_sequences=True, unroll=False))\n-    model.add(gru.GRU(5, return_sequences=True, unroll=False))\n+    model.add(keras.layers.GRU(10, return_sequences=True, unroll=False))\n+    model.add(keras.layers.GRU(5, return_sequences=True, unroll=False))\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))\n@@ -342,7 +232,7 @@ class GRUV2Test(test_combinations.TestCase):\n     embedding_dim = 4\n     units = 2\n     test_utils.layer_test(\n-        gru.GRU,\n+        keras.layers.GRU,\n         kwargs={'units': units,\n                 'return_sequences': True},\n         input_shape=(num_samples, timesteps, embedding_dim))\n@@ -357,7 +247,7 @@ class GRUV2Test(test_combinations.TestCase):\n     embedding_dim = 4\n     units = 2\n     test_utils.layer_test(\n-        gru.GRU,\n+        keras.layers.GRU,\n         kwargs={'units': units,\n                 'return_sequences': True,\n                 'dtype': 'float64'},\n@@ -368,7 +258,7 @@ class GRUV2Test(test_combinations.TestCase):\n       pred=tf.test.is_built_with_rocm,\n       skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   def test_return_states_GRU(self):\n-    layer_class = gru.GRU\n+    layer_class = keras.layers.GRU\n     x = np.random.random((2, 3, 4))\n     y = np.abs(np.random.random((2, 5)))\n     s = np.abs(np.random.random((2, 5)))\n@@ -388,7 +278,7 @@ class GRUV2Test(test_combinations.TestCase):\n     embedding_dim = 4\n     units = 2\n     test_utils.layer_test(\n-        gru.GRU,\n+        keras.layers.GRU,\n         kwargs={'units': units,\n                 'dropout': 0.1,\n                 'recurrent_dropout': 0.1},\n@@ -396,7 +286,7 @@ class GRUV2Test(test_combinations.TestCase):\n \n   def test_constraints_GRU(self):\n     embedding_dim = 4\n-    layer_class = gru.GRU\n+    layer_class = keras.layers.GRU\n     k_constraint = keras.constraints.max_norm(0.01)\n     r_constraint = keras.constraints.max_norm(0.01)\n     b_constraint = keras.constraints.max_norm(0.01)\n@@ -420,14 +310,14 @@ class GRUV2Test(test_combinations.TestCase):\n     embedding_dim = 4\n     units = 2\n     test_utils.layer_test(\n-        gru.GRU,\n+        keras.layers.GRU,\n         kwargs={'units': units,\n                 'implementation': implementation_mode},\n         input_shape=(num_samples, timesteps, embedding_dim))\n \n   def test_regularizers_GRU(self):\n     embedding_dim = 4\n-    layer_class = gru.GRU\n+    layer_class = keras.layers.GRU\n     layer = layer_class(\n         5,\n         return_sequences=False,\n@@ -455,7 +345,7 @@ class GRUV2Test(test_combinations.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    layer_class = gru.GRU\n+    layer_class = keras.layers.GRU\n     model = keras.models.Sequential()\n     model.add(\n         keras.layers.Embedding(\n@@ -536,7 +426,7 @@ class GRUV2Test(test_combinations.TestCase):\n     model = keras.Sequential([\n         keras.layers.Embedding(vocab_size, embedding_dim,\n                                batch_input_shape=[batch_size, timestep]),\n-        gru.GRU(units, return_sequences=True, stateful=True),\n+        keras.layers.GRU(units, return_sequences=True, stateful=True),\n         keras.layers.Dense(vocab_size)\n     ])\n     model.compile(\n@@ -559,18 +449,11 @@ class GRUV2Test(test_combinations.TestCase):\n     mask = np.ones((batch_size, timestep)).astype(np.bool)\n     mask[:, masksteps:] = 0\n \n-    # Test for V1 behavior.\n-    lstm_v1 = gru_v1.GRU(units, return_sequences=True, go_backwards=True)\n+    gru_layer = keras.layers.GRU(\n+        units, return_sequences=True, go_backwards=True)\n     with test_utils.device(should_use_gpu=True):\n-      outputs_masked_v1 = lstm_v1(inputs, mask=tf.constant(mask))\n-      outputs_trimmed_v1 = lstm_v1(inputs[:, :masksteps])\n-    self.assertAllClose(outputs_masked_v1[:, -masksteps:], outputs_trimmed_v1)\n-\n-    # Test for V2 behavior.\n-    lstm = gru.GRU(units, return_sequences=True, go_backwards=True)\n-    with test_utils.device(should_use_gpu=True):\n-      outputs_masked = lstm(inputs, mask=tf.constant(mask))\n-      outputs_trimmed = lstm(inputs[:, :masksteps])\n+      outputs_masked = gru_layer(inputs, mask=tf.constant(mask))\n+      outputs_trimmed = gru_layer(inputs[:, :masksteps])\n     self.assertAllClose(outputs_masked[:, -masksteps:], outputs_trimmed)\n \n   @tf_test_util.enable_output_all_intermediates\n@@ -583,7 +466,7 @@ class GRUV2Test(test_combinations.TestCase):\n           (x, y)).shuffle(100).batch(32)\n \n       inp = keras.layers.Input(shape=(4, 8))\n-      layer = gru.GRU(1)(inp)\n+      layer = keras.layers.GRU(1)(inp)\n       layer = keras.layers.Dense(1)(layer)\n \n       model = keras.models.Model(inp, layer)\n@@ -610,7 +493,7 @@ class GRUV2Test(test_combinations.TestCase):\n             mask_zero=True,\n             input_length=timestep,\n             batch_input_shape=(num_samples, timestep)))\n-    layer = gru.GRU(units)\n+    layer = keras.layers.GRU(units)\n     model.add(layer)\n     model.compile(\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n@@ -623,14 +506,14 @@ class GRUV2Test(test_combinations.TestCase):\n   def test_deepcopy(self):\n     if not tf.executing_eagerly():\n       self.skipTest('v2-only test')\n-    original_layer = gru.GRU(5)\n+    original_layer = keras.layers.GRU(5)\n     copied_layer = copy.deepcopy(original_layer)\n     self.assertEqual(copied_layer.units, 5)\n     self.assertEqual(original_layer.get_config(), original_layer.get_config())\n \n     # Copy layer before layer call on inputs without weight initialization.\n     inputs = np.random.normal(size=[32, 10, 8]).astype(np.float32)\n-    original_layer = gru.GRU(4)\n+    original_layer = keras.layers.GRU(4)\n     copied_layer = copy.deepcopy(original_layer)\n     outputs = original_layer(inputs)\n     copied_outputs = copied_layer(inputs)\n@@ -638,53 +521,12 @@ class GRUV2Test(test_combinations.TestCase):\n         self.evaluate(outputs), self.evaluate(copied_outputs))\n \n     # Copy layer after layer call on inputs with weight initialization.\n-    original_layer = gru.GRU(4)\n+    original_layer = keras.layers.GRU(4)\n     outputs = original_layer(inputs)\n     copied_layer = copy.deepcopy(original_layer)\n     copied_outputs = copied_layer(inputs)\n     self.assertAllClose(self.evaluate(outputs), self.evaluate(copied_outputs))\n \n-\n-@test_utils.run_all_without_tensor_float_32('RNN GRU can use TF32 on GPU')\n-class GRULayerGradientTapeTest(test_combinations.TestCase):\n-\n-  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n-  def test_in_tape(self):\n-    with self.test_session(config=_config):\n-      time_steps = 10\n-      embedding_size = 11\n-      gru_unit_size = 12\n-\n-      gru_layer = gru.GRU(\n-          gru_unit_size,\n-          return_sequences=True,\n-          return_state=True,\n-          recurrent_activation='sigmoid',\n-          recurrent_initializer='glorot_uniform')\n-\n-      x = tf.random.uniform([1, time_steps, embedding_size])\n-      y = tf.random.uniform([1, gru_unit_size])\n-\n-      with tf.GradientTape() as tape:\n-        hidden_state = tf.zeros([1, gru_unit_size], dtype=tf.float32)\n-        _, state = gru_layer(x, initial_state=hidden_state)\n-\n-        loss = tf.reduce_mean(tf.square(state - y))\n-\n-      tape.gradient(loss, gru_layer.variables)\n-\n-\n-@test_utils.run_all_without_tensor_float_32('RNN GRU can use TF32 on GPU')\n-@test_combinations.run_all_keras_modes(config=_config)\n-class GRUGraphRewriteTest(test_combinations.TestCase):\n-\n-  input_shape = 10\n-  output_shape = 8\n-  rnn_state_size = 8\n-  timestep = 4\n-  batch = 100\n-  epoch = 1\n-\n   def _test_runtime_with_model(self, model):\n     (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=self.batch,\n@@ -713,7 +555,7 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n \n   @test_utils.run_v2_only\n   def test_GRU_runtime(self):\n-    layer = gru.GRU(self.rnn_state_size, return_runtime=True)\n+    layer = keras.layers.GRU(self.rnn_state_size, return_runtime=True)\n \n     inputs = keras.layers.Input(\n         shape=[self.timestep, self.input_shape], dtype=tf.float32)\n@@ -734,7 +576,7 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n   def test_GRU_runtime_with_mask(self):\n     # Masking will affect which backend is selected based on whether the mask\n     # is strictly right padded.\n-    layer = gru.GRU(self.rnn_state_size, return_runtime=True)\n+    layer = keras.layers.GRU(self.rnn_state_size, return_runtime=True)\n \n     inputs = keras.layers.Input(\n         shape=[self.timestep, self.input_shape], dtype=tf.float32)\n@@ -790,7 +632,7 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n     # This test is to demonstrate the graph rewrite of grappler plugin under\n     # the condition that the function returns different number of internal\n     # states.\n-    layer = gru.GRU(self.rnn_state_size, return_runtime=True)\n+    layer = keras.layers.GRU(self.rnn_state_size, return_runtime=True)\n \n     inputs = keras.layers.Input(\n         shape=[self.timestep, self.input_shape], dtype=tf.float32)\n@@ -814,5 +656,287 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n     self._test_runtime_with_model(model)\n \n \n+@test_utils.run_all_without_tensor_float_32('RNN GRU can use TF32 on GPU')\n+class GRULayerGradientTapeTest(test_combinations.TestCase):\n+\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n+  def test_in_tape(self):\n+    with self.test_session(config=_config):\n+      time_steps = 10\n+      embedding_size = 11\n+      gru_unit_size = 12\n+\n+      gru_layer = keras.layers.GRU(\n+          gru_unit_size,\n+          return_sequences=True,\n+          return_state=True,\n+          recurrent_activation='sigmoid',\n+          recurrent_initializer='glorot_uniform')\n+\n+      x = tf.random.uniform([1, time_steps, embedding_size])\n+      y = tf.random.uniform([1, gru_unit_size])\n+\n+      with tf.GradientTape() as tape:\n+        hidden_state = tf.zeros([1, gru_unit_size], dtype=tf.float32)\n+        _, state = gru_layer(x, initial_state=hidden_state)\n+\n+        loss = tf.reduce_mean(tf.square(state - y))\n+\n+      tape.gradient(loss, gru_layer.variables)\n+\n+\n+@test_combinations.run_all_keras_modes\n+class GRULayerTest(test_combinations.TestCase):\n+\n+  def test_return_sequences_gru(self):\n+    num_samples = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 2\n+    test_utils.layer_test(\n+        keras.layers.GRU,\n+        kwargs={'units': units,\n+                'return_sequences': True},\n+        input_shape=(num_samples, timesteps, embedding_dim))\n+\n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Double type is not yet supported in ROCm')\n+  @test_utils.run_v2_only\n+  def test_float64_gru(self):\n+    num_samples = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 2\n+    test_utils.layer_test(\n+        keras.layers.GRU,\n+        kwargs={'units': units,\n+                'return_sequences': True,\n+                'dtype': 'float64'},\n+        input_shape=(num_samples, timesteps, embedding_dim),\n+        input_dtype='float64')\n+\n+  def test_dynamic_behavior_gru(self):\n+    num_samples = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 2\n+    layer = keras.layers.GRU(units, input_shape=(None, embedding_dim))\n+    model = keras.models.Sequential()\n+    model.add(layer)\n+    model.compile(\n+        'rmsprop',\n+        'mse',\n+        run_eagerly=test_utils.should_run_eagerly())\n+    x = np.random.random((num_samples, timesteps, embedding_dim))\n+    y = np.random.random((num_samples, units))\n+    model.train_on_batch(x, y)\n+\n+  def test_dropout_gru(self):\n+    num_samples = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 2\n+    test_utils.layer_test(\n+        keras.layers.GRU,\n+        kwargs={'units': units,\n+                'dropout': 0.1,\n+                'recurrent_dropout': 0.1},\n+        input_shape=(num_samples, timesteps, embedding_dim))\n+\n+  def test_recurrent_dropout_with_implementation_restriction(self):\n+    layer = keras.layers.GRU(2, recurrent_dropout=0.1, implementation=2)\n+    # The implementation is force to 1 due to the limit of recurrent_dropout.\n+    self.assertEqual(layer.implementation, 1)\n+\n+  @parameterized.parameters([0, 1, 2])\n+  def test_implementation_mode_gru(self, implementation_mode):\n+    num_samples = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 2\n+    test_utils.layer_test(\n+        keras.layers.GRU,\n+        kwargs={'units': units,\n+                'implementation': implementation_mode},\n+        input_shape=(num_samples, timesteps, embedding_dim))\n+\n+  def test_reset_after_gru(self):\n+    num_samples = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 2\n+\n+    (x_train, y_train), _ = test_utils.get_test_data(\n+        train_samples=num_samples,\n+        test_samples=0,\n+        input_shape=(timesteps, embedding_dim),\n+        num_classes=units)\n+    y_train = np_utils.to_categorical(y_train, units)\n+\n+    inputs = keras.layers.Input(shape=[timesteps, embedding_dim])\n+    gru_layer = keras.layers.GRU(units,\n+                                 reset_after=True)\n+    output = gru_layer(inputs)\n+    gru_model = keras.models.Model(inputs, output)\n+    gru_model.compile(\n+        'rmsprop',\n+        'mse',\n+        run_eagerly=test_utils.should_run_eagerly())\n+    gru_model.fit(x_train, y_train)\n+    gru_model.predict(x_train)\n+\n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='MIOpen only supports packed input output')\n+  def test_with_masking_layer_gru(self):\n+    layer_class = keras.layers.GRU\n+    inputs = np.random.random((2, 3, 4))\n+    targets = np.abs(np.random.random((2, 3, 5)))\n+    targets /= targets.sum(axis=-1, keepdims=True)\n+    model = keras.models.Sequential()\n+    model.add(keras.layers.Masking(input_shape=(3, 4)))\n+    model.add(layer_class(units=5, return_sequences=True, unroll=False))\n+    model.compile(\n+        loss='categorical_crossentropy',\n+        optimizer='rmsprop',\n+        run_eagerly=test_utils.should_run_eagerly())\n+    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n+\n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='MIOpen only supports packed input output')\n+  def test_statefulness_gru(self):\n+    num_samples = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 2\n+    layer_class = keras.layers.GRU\n+\n+    model = keras.models.Sequential()\n+    model.add(\n+        keras.layers.Embedding(\n+            4,\n+            embedding_dim,\n+            mask_zero=True,\n+            input_length=timesteps,\n+            batch_input_shape=(num_samples, timesteps)))\n+    layer = layer_class(\n+        units, return_sequences=False, stateful=True, weights=None)\n+    model.add(layer)\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        run_eagerly=test_utils.should_run_eagerly())\n+    out1 = model.predict(np.ones((num_samples, timesteps)))\n+    self.assertEqual(out1.shape, (num_samples, units))\n+\n+    # train once so that the states change\n+    model.train_on_batch(\n+        np.ones((num_samples, timesteps)), np.ones((num_samples, units)))\n+    out2 = model.predict(np.ones((num_samples, timesteps)))\n+\n+    # if the state is not reset, output should be different\n+    self.assertNotEqual(out1.max(), out2.max())\n+\n+    # check that output changes after states are reset\n+    # (even though the model itself didn't change)\n+    layer.reset_states()\n+    out3 = model.predict(np.ones((num_samples, timesteps)))\n+    self.assertNotEqual(out2.max(), out3.max())\n+\n+    # check that container-level reset_states() works\n+    model.reset_states()\n+    out4 = model.predict(np.ones((num_samples, timesteps)))\n+    np.testing.assert_allclose(out3, out4, atol=1e-5)\n+\n+    # check that the call to `predict` updated the states\n+    out5 = model.predict(np.ones((num_samples, timesteps)))\n+    self.assertNotEqual(out4.max(), out5.max())\n+\n+    # Check masking\n+    layer.reset_states()\n+\n+    left_padded_input = np.ones((num_samples, timesteps))\n+    left_padded_input[0, :1] = 0\n+    left_padded_input[1, :2] = 0\n+    out6 = model.predict(left_padded_input)\n+\n+    layer.reset_states()\n+\n+    right_padded_input = np.ones((num_samples, timesteps))\n+    right_padded_input[0, -1:] = 0\n+    right_padded_input[1, -2:] = 0\n+    out7 = model.predict(right_padded_input)\n+\n+    np.testing.assert_allclose(out7, out6, atol=1e-5)\n+\n+  def test_get_initial_states(self):\n+    batch_size = 4\n+    cell = keras.layers.GRUCell(20)\n+    initial_state = cell.get_initial_state(\n+        batch_size=batch_size, dtype=tf.float32)\n+    _, state = cell(np.ones((batch_size, 20), dtype=np.float32), initial_state)\n+    self.assertEqual(state.shape, initial_state.shape)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n+class GRULayerGenericTest(tf.test.TestCase):\n+\n+  def test_constraints_gru(self):\n+    embedding_dim = 4\n+    layer_class = keras.layers.GRU\n+    k_constraint = keras.constraints.max_norm(0.01)\n+    r_constraint = keras.constraints.max_norm(0.01)\n+    b_constraint = keras.constraints.max_norm(0.01)\n+    layer = layer_class(\n+        5,\n+        return_sequences=False,\n+        weights=None,\n+        input_shape=(None, embedding_dim),\n+        kernel_constraint=k_constraint,\n+        recurrent_constraint=r_constraint,\n+        bias_constraint=b_constraint)\n+    layer.build((None, None, embedding_dim))\n+    self.assertEqual(layer.cell.kernel.constraint, k_constraint)\n+    self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)\n+    self.assertEqual(layer.cell.bias.constraint, b_constraint)\n+\n+  def test_from_config_gru(self):\n+    layer_class = keras.layers.GRU\n+    for stateful in (False, True):\n+      l1 = layer_class(units=1, stateful=stateful)\n+      l2 = layer_class.from_config(l1.get_config())\n+      assert l1.get_config() == l2.get_config()\n+\n+  def test_deep_copy_gru(self):\n+    cell = keras.layers.GRUCell(5)\n+    copied_cell = copy.deepcopy(cell)\n+    self.assertEqual(copied_cell.units, 5)\n+    self.assertEqual(cell.get_config(), copied_cell.get_config())\n+\n+  def test_regularizers_gru(self):\n+    embedding_dim = 4\n+    layer_class = keras.layers.GRU\n+    layer = layer_class(\n+        5,\n+        return_sequences=False,\n+        weights=None,\n+        input_shape=(None, embedding_dim),\n+        kernel_regularizer=keras.regularizers.l1(0.01),\n+        recurrent_regularizer=keras.regularizers.l1(0.01),\n+        bias_regularizer='l2',\n+        activity_regularizer='l1')\n+    layer.build((None, None, 2))\n+    self.assertLen(layer.losses, 3)\n+\n+    x = keras.backend.variable(np.ones((2, 3, 2)))\n+    layer(x)\n+    if tf.executing_eagerly():\n+      self.assertLen(layer.losses, 4)\n+    else:\n+      self.assertLen(layer.get_losses_for(x), 1)\n+\n+\n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -16,29 +16,20 @@\n # pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n from keras import activations\n-from keras import backend\n from keras import constraints\n from keras import initializers\n from keras import regularizers\n-from keras.engine import base_layer\n from keras.engine.input_spec import InputSpec\n+from keras.layers.rnn import gru\n from keras.layers.rnn import rnn_utils\n from keras.layers.rnn.base_rnn import RNN\n-from keras.layers.rnn.dropout_rnn_cell_mixin import DropoutRNNCellMixin\n-from keras.utils import tf_utils\n-import tensorflow.compat.v2 as tf\n \n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n \n-RECURRENT_DROPOUT_WARNING_MSG = (\n-    'RNN `implementation=2` is not supported when `recurrent_dropout` is set. '\n-    'Using `implementation=1`.')\n-\n-\n @keras_export(v1=['keras.layers.GRUCell'])\n-class GRUCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n+class GRUCell(gru.GRUCell):\n   \"\"\"Cell class for the GRU layer.\n \n   Args:\n@@ -104,220 +95,25 @@ class GRUCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n                recurrent_dropout=0.,\n                reset_after=False,\n                **kwargs):\n-    if units < 0:\n-      raise ValueError(f'Received an invalid value for argument `units`, '\n-                       f'expected a positive integer, got {units}.')\n-    # By default use cached variable under v2 mode, see b/143699808.\n-    if tf.compat.v1.executing_eagerly_outside_functions():\n-      self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n-    else:\n-      self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n-    super(GRUCell, self).__init__(**kwargs)\n-    self.units = units\n-    self.activation = activations.get(activation)\n-    self.recurrent_activation = activations.get(recurrent_activation)\n-    self.use_bias = use_bias\n-\n-    self.kernel_initializer = initializers.get(kernel_initializer)\n-    self.recurrent_initializer = initializers.get(recurrent_initializer)\n-    self.bias_initializer = initializers.get(bias_initializer)\n-\n-    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n-    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n-    self.bias_regularizer = regularizers.get(bias_regularizer)\n-\n-    self.kernel_constraint = constraints.get(kernel_constraint)\n-    self.recurrent_constraint = constraints.get(recurrent_constraint)\n-    self.bias_constraint = constraints.get(bias_constraint)\n-\n-    self.dropout = min(1., max(0., dropout))\n-    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n-\n-    implementation = kwargs.pop('implementation', 1)\n-    if self.recurrent_dropout != 0 and implementation != 1:\n-      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)\n-      self.implementation = 1\n-    else:\n-      self.implementation = implementation\n-    self.reset_after = reset_after\n-    self.state_size = self.units\n-    self.output_size = self.units\n-\n-  @tf_utils.shape_type_conversion\n-  def build(self, input_shape):\n-    input_dim = input_shape[-1]\n-    default_caching_device = rnn_utils.caching_device(self)\n-    self.kernel = self.add_weight(\n-        shape=(input_dim, self.units * 3),\n-        name='kernel',\n-        initializer=self.kernel_initializer,\n-        regularizer=self.kernel_regularizer,\n-        constraint=self.kernel_constraint,\n-        caching_device=default_caching_device)\n-    self.recurrent_kernel = self.add_weight(\n-        shape=(self.units, self.units * 3),\n-        name='recurrent_kernel',\n-        initializer=self.recurrent_initializer,\n-        regularizer=self.recurrent_regularizer,\n-        constraint=self.recurrent_constraint,\n-        caching_device=default_caching_device)\n-\n-    if self.use_bias:\n-      if not self.reset_after:\n-        bias_shape = (3 * self.units,)\n-      else:\n-        # separate biases for input and recurrent kernels\n-        # Note: the shape is intentionally different from CuDNNGRU biases\n-        # `(2 * 3 * self.units,)`, so that we can distinguish the classes\n-        # when loading and converting saved weights.\n-        bias_shape = (2, 3 * self.units)\n-      self.bias = self.add_weight(shape=bias_shape,\n-                                  name='bias',\n-                                  initializer=self.bias_initializer,\n-                                  regularizer=self.bias_regularizer,\n-                                  constraint=self.bias_constraint,\n-                                  caching_device=default_caching_device)\n-    else:\n-      self.bias = None\n-    self.built = True\n-\n-  def call(self, inputs, states, training=None):\n-    h_tm1 = states[0] if tf.nest.is_nested(\n-        states) else states  # previous memory\n-\n-    dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=3)\n-    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n-        h_tm1, training, count=3)\n-\n-    if self.use_bias:\n-      if not self.reset_after:\n-        input_bias, recurrent_bias = self.bias, None\n-      else:\n-        input_bias, recurrent_bias = tf.unstack(self.bias)\n-\n-    if self.implementation == 1:\n-      if 0. < self.dropout < 1.:\n-        inputs_z = inputs * dp_mask[0]\n-        inputs_r = inputs * dp_mask[1]\n-        inputs_h = inputs * dp_mask[2]\n-      else:\n-        inputs_z = inputs\n-        inputs_r = inputs\n-        inputs_h = inputs\n-\n-      x_z = backend.dot(inputs_z, self.kernel[:, :self.units])\n-      x_r = backend.dot(inputs_r, self.kernel[:, self.units:self.units * 2])\n-      x_h = backend.dot(inputs_h, self.kernel[:, self.units * 2:])\n-\n-      if self.use_bias:\n-        x_z = backend.bias_add(x_z, input_bias[:self.units])\n-        x_r = backend.bias_add(x_r, input_bias[self.units: self.units * 2])\n-        x_h = backend.bias_add(x_h, input_bias[self.units * 2:])\n-\n-      if 0. < self.recurrent_dropout < 1.:\n-        h_tm1_z = h_tm1 * rec_dp_mask[0]\n-        h_tm1_r = h_tm1 * rec_dp_mask[1]\n-        h_tm1_h = h_tm1 * rec_dp_mask[2]\n-      else:\n-        h_tm1_z = h_tm1\n-        h_tm1_r = h_tm1\n-        h_tm1_h = h_tm1\n-\n-      recurrent_z = backend.dot(h_tm1_z, self.recurrent_kernel[:, :self.units])\n-      recurrent_r = backend.dot(\n-          h_tm1_r, self.recurrent_kernel[:, self.units:self.units * 2])\n-      if self.reset_after and self.use_bias:\n-        recurrent_z = backend.bias_add(recurrent_z, recurrent_bias[:self.units])\n-        recurrent_r = backend.bias_add(\n-            recurrent_r, recurrent_bias[self.units:self.units * 2])\n-\n-      z = self.recurrent_activation(x_z + recurrent_z)\n-      r = self.recurrent_activation(x_r + recurrent_r)\n-\n-      # reset gate applied after/before matrix multiplication\n-      if self.reset_after:\n-        recurrent_h = backend.dot(\n-            h_tm1_h, self.recurrent_kernel[:, self.units * 2:])\n-        if self.use_bias:\n-          recurrent_h = backend.bias_add(\n-              recurrent_h, recurrent_bias[self.units * 2:])\n-        recurrent_h = r * recurrent_h\n-      else:\n-        recurrent_h = backend.dot(\n-            r * h_tm1_h, self.recurrent_kernel[:, self.units * 2:])\n-\n-      hh = self.activation(x_h + recurrent_h)\n-    else:\n-      if 0. < self.dropout < 1.:\n-        inputs = inputs * dp_mask[0]\n-\n-      # inputs projected by all gate matrices at once\n-      matrix_x = backend.dot(inputs, self.kernel)\n-      if self.use_bias:\n-        # biases: bias_z_i, bias_r_i, bias_h_i\n-        matrix_x = backend.bias_add(matrix_x, input_bias)\n-\n-      x_z, x_r, x_h = tf.split(matrix_x, 3, axis=-1)\n-\n-      if self.reset_after:\n-        # hidden state projected by all gate matrices at once\n-        matrix_inner = backend.dot(h_tm1, self.recurrent_kernel)\n-        if self.use_bias:\n-          matrix_inner = backend.bias_add(matrix_inner, recurrent_bias)\n-      else:\n-        # hidden state projected separately for update/reset and new\n-        matrix_inner = backend.dot(\n-            h_tm1, self.recurrent_kernel[:, :2 * self.units])\n-\n-      recurrent_z, recurrent_r, recurrent_h = tf.split(\n-          matrix_inner, [self.units, self.units, -1], axis=-1)\n-\n-      z = self.recurrent_activation(x_z + recurrent_z)\n-      r = self.recurrent_activation(x_r + recurrent_r)\n-\n-      if self.reset_after:\n-        recurrent_h = r * recurrent_h\n-      else:\n-        recurrent_h = backend.dot(\n-            r * h_tm1, self.recurrent_kernel[:, 2 * self.units:])\n-\n-      hh = self.activation(x_h + recurrent_h)\n-    # previous and candidate state mixed by update gate\n-    h = z * h_tm1 + (1 - z) * hh\n-    new_state = [h] if tf.nest.is_nested(states) else h\n-    return h, new_state\n-\n-  def get_config(self):\n-    config = {\n-        'units': self.units,\n-        'activation': activations.serialize(self.activation),\n-        'recurrent_activation':\n-            activations.serialize(self.recurrent_activation),\n-        'use_bias': self.use_bias,\n-        'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-        'recurrent_initializer':\n-            initializers.serialize(self.recurrent_initializer),\n-        'bias_initializer': initializers.serialize(self.bias_initializer),\n-        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n-        'recurrent_regularizer':\n-            regularizers.serialize(self.recurrent_regularizer),\n-        'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-        'kernel_constraint': constraints.serialize(self.kernel_constraint),\n-        'recurrent_constraint':\n-            constraints.serialize(self.recurrent_constraint),\n-        'bias_constraint': constraints.serialize(self.bias_constraint),\n-        'dropout': self.dropout,\n-        'recurrent_dropout': self.recurrent_dropout,\n-        'implementation': self.implementation,\n-        'reset_after': self.reset_after\n-    }\n-    config.update(rnn_utils.config_for_enable_caching_device(self))\n-    base_config = super(GRUCell, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n-    return rnn_utils.generate_zero_filled_state_for_cell(\n-        self, inputs, batch_size, dtype)\n+    super(GRUCell, self).__init__(\n+        units,\n+        activation=activation,\n+        recurrent_activation=recurrent_activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        bias_initializer=bias_initializer,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        implementation=kwargs.pop('implementation', 1),\n+        reset_after=reset_after,\n+        **kwargs)\n \n \n @keras_export(v1=['keras.layers.GRU'])\n\n@@ -13,269 +13,147 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Tests for GRU V1 layer.\"\"\"\n-\n-import copy\n+# pylint: disable=g-direct-tensorflow-import\n \n from absl.testing import parameterized\n import keras\n+from keras.layers.rnn import gru\n+from keras.layers.rnn import gru_v1\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import np_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n+from tensorflow.core.protobuf import rewriter_config_pb2\n \n-@test_combinations.run_all_keras_modes\n-class GRULayerTest(test_combinations.TestCase):\n \n-  def test_return_sequences_gru(self):\n-    num_samples = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 2\n-    test_utils.layer_test(\n-        keras.layers.GRU,\n-        kwargs={'units': units,\n-                'return_sequences': True},\n-        input_shape=(num_samples, timesteps, embedding_dim))\n+# Global config for grappler setting that is used for graph mode test.\n+_rewrites = rewriter_config_pb2.RewriterConfig()\n+_rewrites.implementation_selector = rewriter_config_pb2.RewriterConfig.ON\n+_rewrites.min_graph_nodes = -1\n+_graph_options = tf.compat.v1.GraphOptions(rewrite_options=_rewrites)\n+_config = tf.compat.v1.ConfigProto(graph_options=_graph_options)\n+\n+\n+@test_utils.run_all_without_tensor_float_32('RNN GRU can use TF32 on GPU')\n+@test_combinations.run_all_keras_modes(config=_config)\n+class GRUGraphRewriteTest(test_combinations.TestCase):\n \n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n-      skip_message='Double type is not yet supported in ROCm')\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   @test_utils.run_v2_only\n-  def test_float64_gru(self):\n-    num_samples = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 2\n-    test_utils.layer_test(\n-        keras.layers.GRU,\n-        kwargs={'units': units,\n-                'return_sequences': True,\n-                'dtype': 'float64'},\n-        input_shape=(num_samples, timesteps, embedding_dim),\n-        input_dtype='float64')\n-\n-  def test_dynamic_behavior_gru(self):\n-    num_samples = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 2\n-    layer = keras.layers.GRU(units, input_shape=(None, embedding_dim))\n-    model = keras.models.Sequential()\n-    model.add(layer)\n-    model.compile(\n-        'rmsprop',\n-        'mse',\n-        run_eagerly=test_utils.should_run_eagerly())\n-    x = np.random.random((num_samples, timesteps, embedding_dim))\n-    y = np.random.random((num_samples, units))\n-    model.train_on_batch(x, y)\n-\n-  def test_dropout_gru(self):\n-    num_samples = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 2\n-    test_utils.layer_test(\n-        keras.layers.GRU,\n-        kwargs={'units': units,\n-                'dropout': 0.1,\n-                'recurrent_dropout': 0.1},\n-        input_shape=(num_samples, timesteps, embedding_dim))\n-\n-  def test_recurrent_dropout_with_implementation_restriction(self):\n-    layer = keras.layers.GRU(2, recurrent_dropout=0.1, implementation=2)\n-    # The implementation is force to 1 due to the limit of recurrent_dropout.\n-    self.assertEqual(layer.implementation, 1)\n-\n-  @parameterized.parameters([0, 1, 2])\n-  def test_implementation_mode_gru(self, implementation_mode):\n-    num_samples = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 2\n-    test_utils.layer_test(\n-        keras.layers.GRU,\n-        kwargs={'units': units,\n-                'implementation': implementation_mode},\n-        input_shape=(num_samples, timesteps, embedding_dim))\n-\n-  def test_reset_after_gru(self):\n-    num_samples = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 2\n+  def test_gru_feature_parity_v1_v2(self):\n+    input_shape = 10\n+    rnn_state_size = 8\n+    timestep = 4\n+    batch = 20\n \n     (x_train, y_train), _ = test_utils.get_test_data(\n-        train_samples=num_samples,\n+        train_samples=batch,\n         test_samples=0,\n-        input_shape=(timesteps, embedding_dim),\n-        num_classes=units)\n-    y_train = np_utils.to_categorical(y_train, units)\n+        input_shape=(timestep, input_shape),\n+        num_classes=rnn_state_size,\n+        random_seed=87654321)\n+    y_train = np_utils.to_categorical(y_train, rnn_state_size)\n+    # For the last batch item of the test data, we filter out the last\n+    # timestep to simulate the variable length sequence and masking test.\n+    x_train[-2:, -1, :] = 0.0\n+    y_train[-2:] = 0\n \n-    inputs = keras.layers.Input(shape=[timesteps, embedding_dim])\n-    gru_layer = keras.layers.GRU(units,\n+    inputs = keras.layers.Input(\n+        shape=[timestep, input_shape], dtype=tf.float32)\n+    masked_input = keras.layers.Masking()(inputs)\n+    gru_layer = gru_v1.GRU(rnn_state_size,\n+                           recurrent_activation='sigmoid',\n                            reset_after=True)\n-    output = gru_layer(inputs)\n+    output = gru_layer(masked_input)\n     gru_model = keras.models.Model(inputs, output)\n-    gru_model.compile(\n-        'rmsprop',\n-        'mse',\n-        run_eagerly=test_utils.should_run_eagerly())\n+    weights = gru_model.get_weights()\n+    y_1 = gru_model.predict(x_train)\n+    gru_model.compile('rmsprop', 'mse')\n     gru_model.fit(x_train, y_train)\n-    gru_model.predict(x_train)\n+    y_2 = gru_model.predict(x_train)\n \n-  @tf.test.disable_with_predicate(\n-      pred=tf.test.is_built_with_rocm,\n-      skip_message='MIOpen only supports packed input output')\n-  def test_with_masking_layer_gru(self):\n-    layer_class = keras.layers.GRU\n-    inputs = np.random.random((2, 3, 4))\n-    targets = np.abs(np.random.random((2, 3, 5)))\n-    targets /= targets.sum(axis=-1, keepdims=True)\n-    model = keras.models.Sequential()\n-    model.add(keras.layers.Masking(input_shape=(3, 4)))\n-    model.add(layer_class(units=5, return_sequences=True, unroll=False))\n-    model.compile(\n-        loss='categorical_crossentropy',\n-        optimizer='rmsprop',\n-        run_eagerly=test_utils.should_run_eagerly())\n-    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n+    with test_utils.device(should_use_gpu=True):\n+      cudnn_layer = gru.GRU(rnn_state_size,\n+                            recurrent_activation='sigmoid',\n+                            reset_after=True)\n+      cudnn_model = keras.models.Model(inputs, cudnn_layer(masked_input))\n+    cudnn_model.set_weights(weights)\n+    y_3 = cudnn_model.predict(x_train)\n+    cudnn_model.compile('rmsprop', 'mse')\n+    cudnn_model.fit(x_train, y_train)\n+    y_4 = cudnn_model.predict(x_train)\n \n-  @tf.test.disable_with_predicate(\n-      pred=tf.test.is_built_with_rocm,\n-      skip_message='MIOpen only supports packed input output')\n-  def test_statefulness_gru(self):\n-    num_samples = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 2\n-    layer_class = keras.layers.GRU\n+    self.assertAllClose(y_1, y_3, rtol=2e-5, atol=2e-5)\n+    self.assertAllClose(y_2, y_4, rtol=2e-5, atol=2e-5)\n \n-    model = keras.models.Sequential()\n-    model.add(\n-        keras.layers.Embedding(\n-            4,\n-            embedding_dim,\n-            mask_zero=True,\n-            input_length=timesteps,\n-            batch_input_shape=(num_samples, timesteps)))\n-    layer = layer_class(\n-        units, return_sequences=False, stateful=True, weights=None)\n-    model.add(layer)\n-    model.compile(\n-        optimizer='sgd',\n-        loss='mse',\n-        run_eagerly=test_utils.should_run_eagerly())\n-    out1 = model.predict(np.ones((num_samples, timesteps)))\n-    self.assertEqual(out1.shape, (num_samples, units))\n+  @parameterized.named_parameters(\n+      # test_name, time_major, go_backwards\n+      ('normal', False, False),\n+      ('time_major', True, False),\n+      ('go_backwards', False, True),\n+      ('both', True, True),\n+  )\n+  def test_time_major_and_go_backward_v1_v2(self, time_major, go_backwards):\n+    input_shape = 10\n+    rnn_state_size = 8\n+    timestep = 4\n+    batch = 100\n \n-    # train once so that the states change\n-    model.train_on_batch(\n-        np.ones((num_samples, timesteps)), np.ones((num_samples, units)))\n-    out2 = model.predict(np.ones((num_samples, timesteps)))\n+    x_train = np.random.random((batch, timestep, input_shape))\n \n-    # if the state is not reset, output should be different\n-    self.assertNotEqual(out1.max(), out2.max())\n-\n-    # check that output changes after states are reset\n-    # (even though the model itself didn't change)\n-    layer.reset_states()\n-    out3 = model.predict(np.ones((num_samples, timesteps)))\n-    self.assertNotEqual(out2.max(), out3.max())\n-\n-    # check that container-level reset_states() works\n-    model.reset_states()\n-    out4 = model.predict(np.ones((num_samples, timesteps)))\n-    np.testing.assert_allclose(out3, out4, atol=1e-5)\n-\n-    # check that the call to `predict` updated the states\n-    out5 = model.predict(np.ones((num_samples, timesteps)))\n-    self.assertNotEqual(out4.max(), out5.max())\n-\n-    # Check masking\n-    layer.reset_states()\n-\n-    left_padded_input = np.ones((num_samples, timesteps))\n-    left_padded_input[0, :1] = 0\n-    left_padded_input[1, :2] = 0\n-    out6 = model.predict(left_padded_input)\n-\n-    layer.reset_states()\n-\n-    right_padded_input = np.ones((num_samples, timesteps))\n-    right_padded_input[0, -1:] = 0\n-    right_padded_input[1, -2:] = 0\n-    out7 = model.predict(right_padded_input)\n-\n-    np.testing.assert_allclose(out7, out6, atol=1e-5)\n-\n-  def test_get_initial_states(self):\n-    batch_size = 4\n-    cell = keras.layers.GRUCell(20)\n-    initial_state = cell.get_initial_state(\n-        batch_size=batch_size, dtype=tf.float32)\n-    _, state = cell(np.ones((batch_size, 20), dtype=np.float32), initial_state)\n-    self.assertEqual(state.shape, initial_state.shape)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n-class GRULayerGenericTest(tf.test.TestCase):\n-\n-  def test_constraints_gru(self):\n-    embedding_dim = 4\n-    layer_class = keras.layers.GRU\n-    k_constraint = keras.constraints.max_norm(0.01)\n-    r_constraint = keras.constraints.max_norm(0.01)\n-    b_constraint = keras.constraints.max_norm(0.01)\n-    layer = layer_class(\n-        5,\n-        return_sequences=False,\n-        weights=None,\n-        input_shape=(None, embedding_dim),\n-        kernel_constraint=k_constraint,\n-        recurrent_constraint=r_constraint,\n-        bias_constraint=b_constraint)\n-    layer.build((None, None, embedding_dim))\n-    self.assertEqual(layer.cell.kernel.constraint, k_constraint)\n-    self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)\n-    self.assertEqual(layer.cell.bias.constraint, b_constraint)\n-\n-  def test_from_config_gru(self):\n-    layer_class = keras.layers.GRU\n-    for stateful in (False, True):\n-      l1 = layer_class(units=1, stateful=stateful)\n-      l2 = layer_class.from_config(l1.get_config())\n-      assert l1.get_config() == l2.get_config()\n-\n-  def test_deep_copy_gru(self):\n-    cell = keras.layers.GRUCell(5)\n-    copied_cell = copy.deepcopy(cell)\n-    self.assertEqual(copied_cell.units, 5)\n-    self.assertEqual(cell.get_config(), copied_cell.get_config())\n-\n-  def test_regularizers_gru(self):\n-    embedding_dim = 4\n-    layer_class = keras.layers.GRU\n-    layer = layer_class(\n-        5,\n-        return_sequences=False,\n-        weights=None,\n-        input_shape=(None, embedding_dim),\n-        kernel_regularizer=keras.regularizers.l1(0.01),\n-        recurrent_regularizer=keras.regularizers.l1(0.01),\n-        bias_regularizer='l2',\n-        activity_regularizer='l1')\n-    layer.build((None, None, 2))\n-    self.assertLen(layer.losses, 3)\n-\n-    x = keras.backend.variable(np.ones((2, 3, 2)))\n-    layer(x)\n-    if tf.executing_eagerly():\n-      self.assertLen(layer.losses, 4)\n+    def build_model(layer_cls):\n+      inputs = keras.layers.Input(\n+          shape=[timestep, input_shape], dtype=tf.float32)\n+      layer = layer_cls(rnn_state_size,\n+                        recurrent_activation='sigmoid',\n+                        time_major=time_major,\n+                        return_sequences=True,\n+                        go_backwards=go_backwards,\n+                        reset_after=True)\n+      if time_major:\n+        converted_input = keras.layers.Lambda(\n+            lambda t: tf.transpose(t, [1, 0, 2]))(inputs)\n+        outputs = layer(converted_input)\n+        outputs = keras.layers.Lambda(\n+            lambda t: tf.transpose(t, [1, 0, 2]))(outputs)\n       else:\n-      self.assertLen(layer.get_losses_for(x), 1)\n+        outputs = layer(inputs)\n+      return keras.models.Model(inputs, outputs)\n+\n+    gru_model = build_model(gru_v1.GRU)\n+    y_ref = gru_model.predict(x_train)\n+    weights = gru_model.get_weights()\n+\n+    gru_v2_model = build_model(gru.GRU)\n+    gru_v2_model.set_weights(weights)\n+    y = gru_v2_model.predict(x_train)\n+\n+    self.assertAllClose(y, y_ref)\n+\n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n+  @test_utils.run_v2_only\n+  def test_explicit_device_with_go_backward_and_mask_v1(self):\n+    batch_size = 8\n+    timestep = 7\n+    masksteps = 5\n+    units = 4\n+\n+    inputs = np.random.randn(batch_size, timestep, units).astype(np.float32)\n+    mask = np.ones((batch_size, timestep)).astype(np.bool)\n+    mask[:, masksteps:] = 0\n+\n+    gru_layer = gru_v1.GRU(\n+        units, return_sequences=True, go_backwards=True)\n+    with test_utils.device(should_use_gpu=True):\n+      outputs_masked = gru_layer(inputs, mask=tf.constant(mask))\n+      outputs_trimmed = gru_layer(inputs[:, :masksteps])\n+    self.assertAllClose(outputs_masked[:, -masksteps:], outputs_trimmed)\n \n \n if __name__ == '__main__':\n\n@@ -19,7 +19,12 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-from keras.layers.rnn import base_cell_wrappers\n+import hashlib\n+import numbers\n+\n+from keras.layers.rnn.cell_wrappers import _enumerated_map_structure_up_to\n+from keras.layers.rnn.cell_wrappers import _parse_config_to_function\n+from keras.layers.rnn.cell_wrappers import _serialize_function_to_config\n from keras.layers.rnn.legacy_cells import RNNCell\n import tensorflow.compat.v2 as tf\n \n@@ -132,6 +137,18 @@ class _RNNCellWrapperV1(RNNCell):\n     return self._call_wrapped_cell(\n         inputs, state, cell_call_fn=self.cell.__call__, scope=scope)\n \n+  @property\n+  def state_size(self):\n+    return self.cell.state_size\n+\n+  @property\n+  def output_size(self):\n+    return self.cell.output_size\n+\n+  def zero_state(self, batch_size, dtype):\n+    with tf.name_scope(type(self).__name__ + \"ZeroState\"):\n+      return self.cell.zero_state(batch_size, dtype)\n+\n   def get_config(self):\n     config = {\n         \"cell\": {\n@@ -157,34 +174,398 @@ class _RNNCellWrapperV1(RNNCell):\n \n @keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.DropoutWrapper\"])\n @tf_export(v1=[\"nn.rnn_cell.DropoutWrapper\"])\n-class DropoutWrapper(base_cell_wrappers.DropoutWrapperBase,\n-                     _RNNCellWrapperV1):\n+class DropoutWrapper(_RNNCellWrapperV1):\n   \"\"\"Operator adding dropout to inputs and outputs of the given cell.\"\"\"\n \n-  def __init__(self, *args, **kwargs):  # pylint: disable=useless-super-delegation\n-    super(DropoutWrapper, self).__init__(*args, **kwargs)\n+  def __init__(self,\n+               cell,\n+               input_keep_prob=1.0,\n+               output_keep_prob=1.0,\n+               state_keep_prob=1.0,\n+               variational_recurrent=False,\n+               input_size=None,\n+               dtype=None,\n+               seed=None,\n+               dropout_state_filter_visitor=None,\n+               **kwargs):\n+    \"\"\"Create a cell with added input, state, and/or output dropout.\n \n-  __init__.__doc__ = base_cell_wrappers.DropoutWrapperBase.__init__.__doc__\n+    If `variational_recurrent` is set to `True` (**NOT** the default behavior),\n+    then the same dropout mask is applied at every step, as described in:\n+    [A Theoretically Grounded Application of Dropout in Recurrent\n+    Neural Networks. Y. Gal, Z. Ghahramani](https://arxiv.org/abs/1512.05287).\n+\n+    Otherwise a different dropout mask is applied at every time step.\n+\n+    Note, by default (unless a custom `dropout_state_filter` is provided),\n+    the memory state (`c` component of any `LSTMStateTuple`) passing through\n+    a `DropoutWrapper` is never modified.  This behavior is described in the\n+    above article.\n+\n+    Args:\n+      cell: an RNNCell, a projection to output_size is added to it.\n+      input_keep_prob: unit Tensor or float between 0 and 1, input keep\n+        probability; if it is constant and 1, no input dropout will be added.\n+      output_keep_prob: unit Tensor or float between 0 and 1, output keep\n+        probability; if it is constant and 1, no output dropout will be added.\n+      state_keep_prob: unit Tensor or float between 0 and 1, output keep\n+        probability; if it is constant and 1, no output dropout will be added.\n+        State dropout is performed on the outgoing states of the cell. **Note**\n+        the state components to which dropout is applied when `state_keep_prob`\n+        is in `(0, 1)` are also determined by the argument\n+        `dropout_state_filter_visitor` (e.g. by default dropout is never applied\n+        to the `c` component of an `LSTMStateTuple`).\n+      variational_recurrent: Python bool.  If `True`, then the same dropout\n+        pattern is applied across all time steps per run call. If this parameter\n+        is set, `input_size` **must** be provided.\n+      input_size: (optional) (possibly nested tuple of) `TensorShape` objects\n+        containing the depth(s) of the input tensors expected to be passed in to\n+        the `DropoutWrapper`.  Required and used **iff** `variational_recurrent\n+        = True` and `input_keep_prob < 1`.\n+      dtype: (optional) The `dtype` of the input, state, and output tensors.\n+        Required and used **iff** `variational_recurrent = True`.\n+      seed: (optional) integer, the randomness seed.\n+      dropout_state_filter_visitor: (optional), default: (see below).  Function\n+        that takes any hierarchical level of the state and returns a scalar or\n+        depth=1 structure of Python booleans describing which terms in the state\n+        should be dropped out.  In addition, if the function returns `True`,\n+        dropout is applied across this sublevel.  If the function returns\n+        `False`, dropout is not applied across this entire sublevel.\n+        Default behavior: perform dropout on all terms except the memory (`c`)\n+          state of `LSTMCellState` objects, and don't try to apply dropout to\n+        `TensorArray` objects: ```\n+        def dropout_state_filter_visitor(s):\n+          if isinstance(s, LSTMCellState): # Never perform dropout on the c\n+            state. return LSTMCellState(c=False, h=True)\n+          elif isinstance(s, TensorArray): return False return True ```\n+      **kwargs: dict of keyword arguments for base layer.\n+\n+    Raises:\n+      TypeError: if `cell` is not an `RNNCell`, or `keep_state_fn` is provided\n+        but not `callable`.\n+      ValueError: if any of the keep_probs are not between 0 and 1.\n+    \"\"\"\n+    super(DropoutWrapper, self).__init__(cell, dtype=dtype, **kwargs)\n+\n+    if (dropout_state_filter_visitor is not None and\n+        not callable(dropout_state_filter_visitor)):\n+      raise TypeError(\"dropout_state_filter_visitor must be callable. \"\n+                      f\"Received: {dropout_state_filter_visitor}\")\n+    self._dropout_state_filter = (\n+        dropout_state_filter_visitor or _default_dropout_state_filter_visitor)\n+    with tf.name_scope(\"DropoutWrapperInit\"):\n+\n+      def tensor_and_const_value(v):\n+        tensor_value = tf.convert_to_tensor(v)\n+        const_value = tf.get_static_value(tensor_value)\n+        return (tensor_value, const_value)\n+\n+      for prob, attr in [(input_keep_prob, \"input_keep_prob\"),\n+                         (state_keep_prob, \"state_keep_prob\"),\n+                         (output_keep_prob, \"output_keep_prob\")]:\n+        tensor_prob, const_prob = tensor_and_const_value(prob)\n+        if const_prob is not None:\n+          if const_prob < 0 or const_prob > 1:\n+            raise ValueError(\n+                f\"Parameter {attr} must be between 0 and 1. \"\n+                \"Received {const_prob}\")\n+          setattr(self, \"_%s\" % attr, float(const_prob))\n+        else:\n+          setattr(self, \"_%s\" % attr, tensor_prob)\n+\n+    # Set variational_recurrent, seed before running the code below\n+    self._variational_recurrent = variational_recurrent\n+    self._input_size = input_size\n+    self._seed = seed\n+\n+    self._recurrent_input_noise = None\n+    self._recurrent_state_noise = None\n+    self._recurrent_output_noise = None\n+\n+    if variational_recurrent:\n+      if dtype is None:\n+        raise ValueError(\n+            \"When variational_recurrent=True, dtype must be provided\")\n+\n+      def convert_to_batch_shape(s):\n+        # Prepend a 1 for the batch dimension; for recurrent\n+        # variational dropout we use the same dropout mask for all\n+        # batch elements.\n+        return tf.concat(([1], tf.TensorShape(s).as_list()), 0)\n+\n+      def batch_noise(s, inner_seed):\n+        shape = convert_to_batch_shape(s)\n+        return tf.random.uniform(shape, seed=inner_seed, dtype=dtype)\n+\n+      if (not isinstance(self._input_keep_prob, numbers.Real) or\n+          self._input_keep_prob < 1.0):\n+        if input_size is None:\n+          raise ValueError(\n+              \"When variational_recurrent=True and input_keep_prob < 1.0 or \"\n+              \"is unknown, input_size must be provided\")\n+        self._recurrent_input_noise = _enumerated_map_structure_up_to(\n+            input_size,\n+            lambda i, s: batch_noise(s, inner_seed=self._gen_seed(\"input\", i)),\n+            input_size)\n+      self._recurrent_state_noise = _enumerated_map_structure_up_to(\n+          cell.state_size,\n+          lambda i, s: batch_noise(s, inner_seed=self._gen_seed(\"state\", i)),\n+          cell.state_size)\n+      self._recurrent_output_noise = _enumerated_map_structure_up_to(\n+          cell.output_size,\n+          lambda i, s: batch_noise(s, inner_seed=self._gen_seed(\"output\", i)),\n+          cell.output_size)\n+\n+  def _gen_seed(self, salt_prefix, index):\n+    if self._seed is None:\n+      return None\n+    salt = \"%s_%d\" % (salt_prefix, index)\n+    string = (str(self._seed) + salt).encode(\"utf-8\")\n+    return int(hashlib.md5(string).hexdigest()[:8], 16) & 0x7FFFFFFF\n+\n+  @property\n+  def wrapped_cell(self):\n+    return self.cell\n+\n+  def build(self, inputs_shape):\n+    self.cell.build(inputs_shape)\n+    self.built = True\n+\n+  def _variational_recurrent_dropout_value(\n+      self, unused_index, value, noise, keep_prob):\n+    \"\"\"Performs dropout given the pre-calculated noise tensor.\"\"\"\n+    # uniform [keep_prob, 1.0 + keep_prob)\n+    random_tensor = keep_prob + noise\n+\n+    # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)\n+    binary_tensor = tf.floor(random_tensor)\n+    ret = tf.divide(value, keep_prob) * binary_tensor\n+    ret.set_shape(value.get_shape())\n+    return ret\n+\n+  def _dropout(self,\n+               values,\n+               salt_prefix,\n+               recurrent_noise,\n+               keep_prob,\n+               shallow_filtered_substructure=None):\n+    \"\"\"Decides whether to perform standard dropout or recurrent dropout.\"\"\"\n+\n+    if shallow_filtered_substructure is None:\n+      # Put something so we traverse the entire structure; inside the\n+      # dropout function we check to see if leafs of this are bool or not.\n+      shallow_filtered_substructure = values\n+\n+    if not self._variational_recurrent:\n+\n+      def dropout(i, do_dropout, v):\n+        if not isinstance(do_dropout, bool) or do_dropout:\n+          return tf.nn.dropout(\n+              v, rate=1. - keep_prob, seed=self._gen_seed(salt_prefix, i))\n+        else:\n+          return v\n+\n+      return _enumerated_map_structure_up_to(\n+          shallow_filtered_substructure, dropout,\n+          *[shallow_filtered_substructure, values])\n+    else:\n+\n+      def dropout(i, do_dropout, v, n):\n+        if not isinstance(do_dropout, bool) or do_dropout:\n+          return self._variational_recurrent_dropout_value(i, v, n, keep_prob)\n+        else:\n+          return v\n+\n+      return _enumerated_map_structure_up_to(\n+          shallow_filtered_substructure, dropout,\n+          *[shallow_filtered_substructure, values, recurrent_noise])\n+\n+  def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n+    \"\"\"Runs the wrapped cell and applies dropout.\n+\n+    Args:\n+      inputs: A tensor with wrapped cell's input.\n+      state: A tensor or tuple of tensors with wrapped cell's state.\n+      cell_call_fn: Wrapped cell's method to use for step computation (cell's\n+        `__call__` or 'call' method).\n+      **kwargs: Additional arguments.\n+\n+    Returns:\n+      A pair containing:\n+\n+      - Output: A tensor with cell's output.\n+      - New state: A tensor or tuple of tensors with new wrapped cell's state.\n+    \"\"\"\n+\n+    def _should_dropout(p):\n+      return (not isinstance(p, float)) or p < 1\n+\n+    if _should_dropout(self._input_keep_prob):\n+      inputs = self._dropout(inputs, \"input\", self._recurrent_input_noise,\n+                             self._input_keep_prob)\n+    output, new_state = cell_call_fn(inputs, state, **kwargs)\n+    if _should_dropout(self._state_keep_prob):\n+      # Identify which subsets of the state to perform dropout on and\n+      # which ones to keep.\n+      shallow_filtered_substructure = tf.__internal__.nest.get_traverse_shallow_structure(\n+          self._dropout_state_filter, new_state)\n+      new_state = self._dropout(new_state, \"state\", self._recurrent_state_noise,\n+                                self._state_keep_prob,\n+                                shallow_filtered_substructure)\n+    if _should_dropout(self._output_keep_prob):\n+      output = self._dropout(output, \"output\", self._recurrent_output_noise,\n+                             self._output_keep_prob)\n+    return output, new_state\n+\n+  def get_config(self):\n+    \"\"\"Returns the config of the dropout wrapper.\"\"\"\n+    config = {\n+        \"input_keep_prob\": self._input_keep_prob,\n+        \"output_keep_prob\": self._output_keep_prob,\n+        \"state_keep_prob\": self._state_keep_prob,\n+        \"variational_recurrent\": self._variational_recurrent,\n+        \"input_size\": self._input_size,\n+        \"seed\": self._seed,\n+    }\n+    if self._dropout_state_filter != _default_dropout_state_filter_visitor:  # pylint: disable=comparison-with-callable\n+      function, function_type, function_module = _serialize_function_to_config(\n+          self._dropout_state_filter)\n+      config.update({\"dropout_fn\": function,\n+                     \"dropout_fn_type\": function_type,\n+                     \"dropout_fn_module\": function_module})\n+    base_config = super(DropoutWrapper, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config, custom_objects=None):\n+    if \"dropout_fn\" in config:\n+      config = config.copy()\n+      dropout_state_filter = _parse_config_to_function(\n+          config, custom_objects, \"dropout_fn\", \"dropout_fn_type\",\n+          \"dropout_fn_module\")\n+      config.pop(\"dropout_fn\")\n+      config[\"dropout_state_filter_visitor\"] = dropout_state_filter\n+    return super(DropoutWrapper, cls).from_config(\n+        config, custom_objects=custom_objects)\n \n \n @keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.ResidualWrapper\"])\n @tf_export(v1=[\"nn.rnn_cell.ResidualWrapper\"])\n-class ResidualWrapper(base_cell_wrappers.ResidualWrapperBase,\n-                      _RNNCellWrapperV1):\n+class ResidualWrapper(_RNNCellWrapperV1):\n   \"\"\"RNNCell wrapper that ensures cell inputs are added to the outputs.\"\"\"\n \n-  def __init__(self, *args, **kwargs):  # pylint: disable=useless-super-delegation\n-    super(ResidualWrapper, self).__init__(*args, **kwargs)\n+  def __init__(self, cell, residual_fn=None, **kwargs):\n+    \"\"\"Constructs a `ResidualWrapper` for `cell`.\n \n-  __init__.__doc__ = base_cell_wrappers.ResidualWrapperBase.__init__.__doc__\n+    Args:\n+      cell: An instance of `RNNCell`.\n+      residual_fn: (Optional) The function to map raw cell inputs and raw cell\n+        outputs to the actual cell outputs of the residual network.\n+        Defaults to calling nest.map_structure on (lambda i, o: i + o), inputs\n+          and outputs.\n+      **kwargs: dict of keyword arguments for base layer.\n+    \"\"\"\n+    super(ResidualWrapper, self).__init__(cell, **kwargs)\n+    self._residual_fn = residual_fn\n+\n+  def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n+    \"\"\"Run the cell and then apply the residual_fn on its inputs to its outputs.\n+\n+    Args:\n+      inputs: cell inputs.\n+      state: cell state.\n+      cell_call_fn: Wrapped cell's method to use for step computation (cell's\n+        `__call__` or 'call' method).\n+      **kwargs: Additional arguments passed to the wrapped cell's `call`.\n+\n+    Returns:\n+      Tuple of cell outputs and new state.\n+\n+    Raises:\n+      TypeError: If cell inputs and outputs have different structure (type).\n+      ValueError: If cell inputs and outputs have different structure (value).\n+    \"\"\"\n+    outputs, new_state = cell_call_fn(inputs, state, **kwargs)\n+\n+    # Ensure shapes match\n+    def assert_shape_match(inp, out):\n+      inp.get_shape().assert_is_compatible_with(out.get_shape())\n+\n+    def default_residual_fn(inputs, outputs):\n+      tf.nest.assert_same_structure(inputs, outputs)\n+      tf.nest.map_structure(assert_shape_match, inputs, outputs)\n+      return tf.nest.map_structure(lambda inp, out: inp + out, inputs, outputs)\n+\n+    res_outputs = (self._residual_fn or default_residual_fn)(inputs, outputs)\n+    return (res_outputs, new_state)\n+\n+  def get_config(self):\n+    \"\"\"Returns the config of the residual wrapper.\"\"\"\n+    if self._residual_fn is not None:\n+      function, function_type, function_module = _serialize_function_to_config(\n+          self._residual_fn)\n+      config = {\n+          \"residual_fn\": function,\n+          \"residual_fn_type\": function_type,\n+          \"residual_fn_module\": function_module\n+      }\n+    else:\n+      config = {}\n+    base_config = super(ResidualWrapper, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config, custom_objects=None):\n+    if \"residual_fn\" in config:\n+      config = config.copy()\n+      residual_function = _parse_config_to_function(config, custom_objects,\n+                                                    \"residual_fn\",\n+                                                    \"residual_fn_type\",\n+                                                    \"residual_fn_module\")\n+      config[\"residual_fn\"] = residual_function\n+    return super(ResidualWrapper, cls).from_config(\n+        config, custom_objects=custom_objects)\n \n \n @keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.DeviceWrapper\"])\n @tf_export(v1=[\"nn.rnn_cell.DeviceWrapper\"])\n-class DeviceWrapper(base_cell_wrappers.DeviceWrapperBase,\n-                    _RNNCellWrapperV1):\n+class DeviceWrapper(_RNNCellWrapperV1):\n+  \"\"\"Operator that ensures an RNNCell runs on a particular device.\"\"\"\n \n-  def __init__(self, *args, **kwargs):  # pylint: disable=useless-super-delegation\n-    super(DeviceWrapper, self).__init__(*args, **kwargs)\n+  def __init__(self, cell, device, **kwargs):\n+    \"\"\"Construct a `DeviceWrapper` for `cell` with device `device`.\n \n-  __init__.__doc__ = base_cell_wrappers.DeviceWrapperBase.__init__.__doc__\n+    Ensures the wrapped `cell` is called with `tf.device(device)`.\n+\n+    Args:\n+      cell: An instance of `RNNCell`.\n+      device: A device string or function, for passing to `tf.device`.\n+      **kwargs: dict of keyword arguments for base layer.\n+    \"\"\"\n+    super(DeviceWrapper, self).__init__(cell, **kwargs)\n+    self._device = device\n+\n+  def zero_state(self, batch_size, dtype):\n+    with tf.name_scope(type(self).__name__ + \"ZeroState\"):\n+      with tf.compat.v1.device(self._device):\n+        return self.cell.zero_state(batch_size, dtype)\n+\n+  def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n+    \"\"\"Run the cell on specified device.\"\"\"\n+    with tf.compat.v1.device(self._device):\n+      return cell_call_fn(inputs, state, **kwargs)\n+\n+  def get_config(self):\n+    config = {\"device\": self._device}\n+    base_config = super(DeviceWrapper, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+def _default_dropout_state_filter_visitor(substate):\n+  from keras.layers.rnn.legacy_cells import LSTMStateTuple  # pylint: disable=g-import-not-at-top\n+  if isinstance(substate, LSTMStateTuple):\n+    # Do not perform dropout on the memory state.\n+    return LSTMStateTuple(c=False, h=True)\n+  elif isinstance(substate, tf.TensorArray):\n+    return False\n+  return True\n\n@@ -19,19 +19,29 @@ import uuid\n \n from keras import activations\n from keras import backend\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n from keras.engine import base_layer\n from keras.engine.input_spec import InputSpec\n from keras.layers.rnn import gru_lstm_utils\n-from keras.layers.rnn import lstm_v1\n+from keras.layers.rnn import rnn_utils\n+from keras.layers.rnn.base_rnn import RNN\n from keras.layers.rnn.dropout_rnn_cell_mixin import DropoutRNNCellMixin\n+from keras.utils import tf_utils\n import tensorflow.compat.v2 as tf\n \n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n \n+RECURRENT_DROPOUT_WARNING_MSG = (\n+    'RNN `implementation=2` is not supported when `recurrent_dropout` is set. '\n+    'Using `implementation=1`.')\n+\n+\n @keras_export('keras.layers.LSTMCell', v1=[])\n-class LSTMCell(lstm_v1.LSTMCell):\n+class LSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n   \"\"\"Cell class for the LSTM layer.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n@@ -125,29 +135,216 @@ class LSTMCell(lstm_v1.LSTMCell):\n                dropout=0.,\n                recurrent_dropout=0.,\n                **kwargs):\n-    super(LSTMCell, self).__init__(\n-        units,\n-        activation=activation,\n-        recurrent_activation=recurrent_activation,\n-        use_bias=use_bias,\n-        kernel_initializer=kernel_initializer,\n-        recurrent_initializer=recurrent_initializer,\n-        bias_initializer=bias_initializer,\n-        unit_forget_bias=unit_forget_bias,\n-        kernel_regularizer=kernel_regularizer,\n-        recurrent_regularizer=recurrent_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        kernel_constraint=kernel_constraint,\n-        recurrent_constraint=recurrent_constraint,\n-        bias_constraint=bias_constraint,\n-        dropout=dropout,\n-        recurrent_dropout=recurrent_dropout,\n-        implementation=kwargs.pop('implementation', 2),\n-        **kwargs)\n+    if units < 0:\n+      raise ValueError(f'Received an invalid value for argument `units`, '\n+                       f'expected a positive integer, got {units}.')\n+    # By default use cached variable under v2 mode, see b/143699808.\n+    if tf.compat.v1.executing_eagerly_outside_functions():\n+      self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n+    else:\n+      self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n+    super(LSTMCell, self).__init__(**kwargs)\n+    self.units = units\n+    self.activation = activations.get(activation)\n+    self.recurrent_activation = activations.get(recurrent_activation)\n+    self.use_bias = use_bias\n+\n+    self.kernel_initializer = initializers.get(kernel_initializer)\n+    self.recurrent_initializer = initializers.get(recurrent_initializer)\n+    self.bias_initializer = initializers.get(bias_initializer)\n+    self.unit_forget_bias = unit_forget_bias\n+\n+    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n+    self.bias_regularizer = regularizers.get(bias_regularizer)\n+\n+    self.kernel_constraint = constraints.get(kernel_constraint)\n+    self.recurrent_constraint = constraints.get(recurrent_constraint)\n+    self.bias_constraint = constraints.get(bias_constraint)\n+\n+    self.dropout = min(1., max(0., dropout))\n+    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n+    implementation = kwargs.pop('implementation', 2)\n+    if self.recurrent_dropout != 0 and implementation != 1:\n+      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)\n+      self.implementation = 1\n+    else:\n+      self.implementation = implementation\n+    self.state_size = [self.units, self.units]\n+    self.output_size = self.units\n+\n+  @tf_utils.shape_type_conversion\n+  def build(self, input_shape):\n+    default_caching_device = rnn_utils.caching_device(self)\n+    input_dim = input_shape[-1]\n+    self.kernel = self.add_weight(\n+        shape=(input_dim, self.units * 4),\n+        name='kernel',\n+        initializer=self.kernel_initializer,\n+        regularizer=self.kernel_regularizer,\n+        constraint=self.kernel_constraint,\n+        caching_device=default_caching_device)\n+    self.recurrent_kernel = self.add_weight(\n+        shape=(self.units, self.units * 4),\n+        name='recurrent_kernel',\n+        initializer=self.recurrent_initializer,\n+        regularizer=self.recurrent_regularizer,\n+        constraint=self.recurrent_constraint,\n+        caching_device=default_caching_device)\n+\n+    if self.use_bias:\n+      if self.unit_forget_bias:\n+\n+        def bias_initializer(_, *args, **kwargs):\n+          return backend.concatenate([\n+              self.bias_initializer((self.units,), *args, **kwargs),\n+              initializers.get('ones')((self.units,), *args, **kwargs),\n+              self.bias_initializer((self.units * 2,), *args, **kwargs),\n+          ])\n+      else:\n+        bias_initializer = self.bias_initializer\n+      self.bias = self.add_weight(\n+          shape=(self.units * 4,),\n+          name='bias',\n+          initializer=bias_initializer,\n+          regularizer=self.bias_regularizer,\n+          constraint=self.bias_constraint,\n+          caching_device=default_caching_device)\n+    else:\n+      self.bias = None\n+    self.built = True\n+\n+  def _compute_carry_and_output(self, x, h_tm1, c_tm1):\n+    \"\"\"Computes carry and output using split kernels.\"\"\"\n+    x_i, x_f, x_c, x_o = x\n+    h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1\n+    i = self.recurrent_activation(\n+        x_i + backend.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]))\n+    f = self.recurrent_activation(x_f + backend.dot(\n+        h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))\n+    c = f * c_tm1 + i * self.activation(x_c + backend.dot(\n+        h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))\n+    o = self.recurrent_activation(\n+        x_o + backend.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:]))\n+    return c, o\n+\n+  def _compute_carry_and_output_fused(self, z, c_tm1):\n+    \"\"\"Computes carry and output using fused kernels.\"\"\"\n+    z0, z1, z2, z3 = z\n+    i = self.recurrent_activation(z0)\n+    f = self.recurrent_activation(z1)\n+    c = f * c_tm1 + i * self.activation(z2)\n+    o = self.recurrent_activation(z3)\n+    return c, o\n+\n+  def call(self, inputs, states, training=None):\n+    h_tm1 = states[0]  # previous memory state\n+    c_tm1 = states[1]  # previous carry state\n+\n+    dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n+    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n+        h_tm1, training, count=4)\n+\n+    if self.implementation == 1:\n+      if 0 < self.dropout < 1.:\n+        inputs_i = inputs * dp_mask[0]\n+        inputs_f = inputs * dp_mask[1]\n+        inputs_c = inputs * dp_mask[2]\n+        inputs_o = inputs * dp_mask[3]\n+      else:\n+        inputs_i = inputs\n+        inputs_f = inputs\n+        inputs_c = inputs\n+        inputs_o = inputs\n+      k_i, k_f, k_c, k_o = tf.split(\n+          self.kernel, num_or_size_splits=4, axis=1)\n+      x_i = backend.dot(inputs_i, k_i)\n+      x_f = backend.dot(inputs_f, k_f)\n+      x_c = backend.dot(inputs_c, k_c)\n+      x_o = backend.dot(inputs_o, k_o)\n+      if self.use_bias:\n+        b_i, b_f, b_c, b_o = tf.split(\n+            self.bias, num_or_size_splits=4, axis=0)\n+        x_i = backend.bias_add(x_i, b_i)\n+        x_f = backend.bias_add(x_f, b_f)\n+        x_c = backend.bias_add(x_c, b_c)\n+        x_o = backend.bias_add(x_o, b_o)\n+\n+      if 0 < self.recurrent_dropout < 1.:\n+        h_tm1_i = h_tm1 * rec_dp_mask[0]\n+        h_tm1_f = h_tm1 * rec_dp_mask[1]\n+        h_tm1_c = h_tm1 * rec_dp_mask[2]\n+        h_tm1_o = h_tm1 * rec_dp_mask[3]\n+      else:\n+        h_tm1_i = h_tm1\n+        h_tm1_f = h_tm1\n+        h_tm1_c = h_tm1\n+        h_tm1_o = h_tm1\n+      x = (x_i, x_f, x_c, x_o)\n+      h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)\n+      c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)\n+    else:\n+      if 0. < self.dropout < 1.:\n+        inputs = inputs * dp_mask[0]\n+      z = backend.dot(inputs, self.kernel)\n+      z += backend.dot(h_tm1, self.recurrent_kernel)\n+      if self.use_bias:\n+        z = backend.bias_add(z, self.bias)\n+\n+      z = tf.split(z, num_or_size_splits=4, axis=1)\n+      c, o = self._compute_carry_and_output_fused(z, c_tm1)\n+\n+    h = o * self.activation(c)\n+    return h, [h, c]\n+\n+  def get_config(self):\n+    config = {\n+        'units':\n+            self.units,\n+        'activation':\n+            activations.serialize(self.activation),\n+        'recurrent_activation':\n+            activations.serialize(self.recurrent_activation),\n+        'use_bias':\n+            self.use_bias,\n+        'kernel_initializer':\n+            initializers.serialize(self.kernel_initializer),\n+        'recurrent_initializer':\n+            initializers.serialize(self.recurrent_initializer),\n+        'bias_initializer':\n+            initializers.serialize(self.bias_initializer),\n+        'unit_forget_bias':\n+            self.unit_forget_bias,\n+        'kernel_regularizer':\n+            regularizers.serialize(self.kernel_regularizer),\n+        'recurrent_regularizer':\n+            regularizers.serialize(self.recurrent_regularizer),\n+        'bias_regularizer':\n+            regularizers.serialize(self.bias_regularizer),\n+        'kernel_constraint':\n+            constraints.serialize(self.kernel_constraint),\n+        'recurrent_constraint':\n+            constraints.serialize(self.recurrent_constraint),\n+        'bias_constraint':\n+            constraints.serialize(self.bias_constraint),\n+        'dropout':\n+            self.dropout,\n+        'recurrent_dropout':\n+            self.recurrent_dropout,\n+        'implementation':\n+            self.implementation\n+    }\n+    config.update(rnn_utils.config_for_enable_caching_device(self))\n+    base_config = super(LSTMCell, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n+    return list(rnn_utils.generate_zero_filled_state_for_cell(\n+        self, inputs, batch_size, dtype))\n \n \n @keras_export('keras.layers.LSTM', v1=[])\n-class LSTM(DropoutRNNCellMixin, lstm_v1.LSTM, base_layer.BaseRandomLayer):\n+class LSTM(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n   \"\"\"Long Short-Term Memory layer - Hochreiter 1997.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n@@ -288,26 +485,39 @@ class LSTM(DropoutRNNCellMixin, lstm_v1.LSTM, base_layer.BaseRandomLayer):\n     # return_runtime is a flag for testing, which shows the real backend\n     # implementation chosen by grappler in graph mode.\n     self.return_runtime = kwargs.pop('return_runtime', False)\n-\n-    super(LSTM, self).__init__(\n+    implementation = kwargs.pop('implementation', 2)\n+    if implementation == 0:\n+      logging.warning('`implementation=0` has been deprecated, '\n+                      'and now defaults to `implementation=1`.'\n+                      'Please update your layer call.')\n+    if 'enable_caching_device' in kwargs:\n+      cell_kwargs = {'enable_caching_device':\n+                     kwargs.pop('enable_caching_device')}\n+    else:\n+      cell_kwargs = {}\n+    cell = LSTMCell(\n         units,\n         activation=activation,\n         recurrent_activation=recurrent_activation,\n         use_bias=use_bias,\n         kernel_initializer=kernel_initializer,\n         recurrent_initializer=recurrent_initializer,\n-        bias_initializer=bias_initializer,\n         unit_forget_bias=unit_forget_bias,\n+        bias_initializer=bias_initializer,\n         kernel_regularizer=kernel_regularizer,\n         recurrent_regularizer=recurrent_regularizer,\n         bias_regularizer=bias_regularizer,\n-        activity_regularizer=activity_regularizer,\n         kernel_constraint=kernel_constraint,\n         recurrent_constraint=recurrent_constraint,\n         bias_constraint=bias_constraint,\n         dropout=dropout,\n         recurrent_dropout=recurrent_dropout,\n-        implementation=kwargs.pop('implementation', 2),\n+        implementation=implementation,\n+        dtype=kwargs.get('dtype'),\n+        trainable=kwargs.get('trainable', True),\n+        **cell_kwargs)\n+    super(LSTM, self).__init__(\n+        cell,\n         return_sequences=return_sequences,\n         return_state=return_state,\n         go_backwards=go_backwards,\n@@ -315,7 +525,8 @@ class LSTM(DropoutRNNCellMixin, lstm_v1.LSTM, base_layer.BaseRandomLayer):\n         time_major=time_major,\n         unroll=unroll,\n         **kwargs)\n-\n+    self.activity_regularizer = regularizers.get(activity_regularizer)\n+    self.input_spec = [InputSpec(ndim=3)]\n     self.state_spec = [\n         InputSpec(shape=(None, dim)) for dim in (self.units, self.units)\n     ]\n@@ -480,6 +691,124 @@ class LSTM(DropoutRNNCellMixin, lstm_v1.LSTM, base_layer.BaseRandomLayer):\n     else:\n       return output\n \n+  @property\n+  def units(self):\n+    return self.cell.units\n+\n+  @property\n+  def activation(self):\n+    return self.cell.activation\n+\n+  @property\n+  def recurrent_activation(self):\n+    return self.cell.recurrent_activation\n+\n+  @property\n+  def use_bias(self):\n+    return self.cell.use_bias\n+\n+  @property\n+  def kernel_initializer(self):\n+    return self.cell.kernel_initializer\n+\n+  @property\n+  def recurrent_initializer(self):\n+    return self.cell.recurrent_initializer\n+\n+  @property\n+  def bias_initializer(self):\n+    return self.cell.bias_initializer\n+\n+  @property\n+  def unit_forget_bias(self):\n+    return self.cell.unit_forget_bias\n+\n+  @property\n+  def kernel_regularizer(self):\n+    return self.cell.kernel_regularizer\n+\n+  @property\n+  def recurrent_regularizer(self):\n+    return self.cell.recurrent_regularizer\n+\n+  @property\n+  def bias_regularizer(self):\n+    return self.cell.bias_regularizer\n+\n+  @property\n+  def kernel_constraint(self):\n+    return self.cell.kernel_constraint\n+\n+  @property\n+  def recurrent_constraint(self):\n+    return self.cell.recurrent_constraint\n+\n+  @property\n+  def bias_constraint(self):\n+    return self.cell.bias_constraint\n+\n+  @property\n+  def dropout(self):\n+    return self.cell.dropout\n+\n+  @property\n+  def recurrent_dropout(self):\n+    return self.cell.recurrent_dropout\n+\n+  @property\n+  def implementation(self):\n+    return self.cell.implementation\n+\n+  def get_config(self):\n+    config = {\n+        'units':\n+            self.units,\n+        'activation':\n+            activations.serialize(self.activation),\n+        'recurrent_activation':\n+            activations.serialize(self.recurrent_activation),\n+        'use_bias':\n+            self.use_bias,\n+        'kernel_initializer':\n+            initializers.serialize(self.kernel_initializer),\n+        'recurrent_initializer':\n+            initializers.serialize(self.recurrent_initializer),\n+        'bias_initializer':\n+            initializers.serialize(self.bias_initializer),\n+        'unit_forget_bias':\n+            self.unit_forget_bias,\n+        'kernel_regularizer':\n+            regularizers.serialize(self.kernel_regularizer),\n+        'recurrent_regularizer':\n+            regularizers.serialize(self.recurrent_regularizer),\n+        'bias_regularizer':\n+            regularizers.serialize(self.bias_regularizer),\n+        'activity_regularizer':\n+            regularizers.serialize(self.activity_regularizer),\n+        'kernel_constraint':\n+            constraints.serialize(self.kernel_constraint),\n+        'recurrent_constraint':\n+            constraints.serialize(self.recurrent_constraint),\n+        'bias_constraint':\n+            constraints.serialize(self.bias_constraint),\n+        'dropout':\n+            self.dropout,\n+        'recurrent_dropout':\n+            self.recurrent_dropout,\n+        'implementation':\n+            self.implementation\n+    }\n+    config.update(rnn_utils.config_for_enable_caching_device(self.cell))\n+    base_config = super(LSTM, self).get_config()\n+    del base_config['cell']\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config):\n+    if 'implementation' in config and config['implementation'] == 0:\n+      config['implementation'] = 1\n+    return cls(**config)\n+\n \n def standard_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias,\n                   mask, time_major, go_backwards, sequence_lengths,\n\n@@ -18,13 +18,10 @@\n import copy\n import os\n import shutil\n-import time\n \n from absl.testing import parameterized\n import keras\n from keras.layers.rnn import gru_lstm_utils\n-from keras.layers.rnn import lstm\n-from keras.layers.rnn import lstm_v1\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import np_utils\n@@ -33,7 +30,6 @@ import tensorflow.compat.v2 as tf\n \n from tensorflow.core.protobuf import rewriter_config_pb2\n from tensorflow.python.framework import test_util as tf_test_util\n-from tensorflow.python.platform import tf_logging as logging\n \n \n # Global config for grappler setting that is used for graph mode test.\n@@ -45,7 +41,14 @@ _config = tf.compat.v1.ConfigProto(graph_options=_graph_options)\n \n \n @test_combinations.run_all_keras_modes(config=_config)\n-class LSTMV2Test(test_combinations.TestCase):\n+class LSTMGraphRewriteTest(test_combinations.TestCase):\n+\n+  input_shape = 10\n+  output_shape = 8\n+  rnn_state_size = 8\n+  timestep = 4\n+  batch = 100\n+  epoch = 1\n \n   @parameterized.named_parameters(\n       ('non_tan_activation', 'relu', 'sigmoid', 0, False, True),\n@@ -54,9 +57,10 @@ class LSTMV2Test(test_combinations.TestCase):\n       ('unroll', 'tanh', 'sigmoid', 0, True, True),\n       ('not_use_bias', 'tanh', 'sigmoid', 0, False, False),\n   )\n+  @test_utils.run_v2_only\n   def test_could_use_defun_backend(self, activation, recurrent_activation,\n                                    recurrent_dropout, unroll, use_bias):\n-    layer = lstm.LSTM(\n+    layer = keras.layers.LSTM(\n         1,\n         activation=activation,\n         recurrent_activation=recurrent_activation,\n@@ -67,10 +71,10 @@ class LSTMV2Test(test_combinations.TestCase):\n \n   @test_utils.run_v2_only\n   def test_use_on_default_activation_with_gpu_kernel(self):\n-    layer = lstm.LSTM(1, activation=tf.tanh)\n+    layer = keras.layers.LSTM(1, activation=tf.tanh)\n     self.assertTrue(layer._could_use_gpu_kernel)\n \n-    layer = lstm.LSTM(1, recurrent_activation=tf.sigmoid)\n+    layer = keras.layers.LSTM(1, recurrent_activation=tf.sigmoid)\n     self.assertTrue(layer._could_use_gpu_kernel)\n \n   def test_static_shape_inference_LSTM(self):\n@@ -83,7 +87,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     inputs = keras.layers.Dense(\n         embedding_dim, input_shape=(timesteps, embedding_dim))\n     model.add(inputs)\n-    layer = lstm.LSTM(units, return_sequences=True)\n+    layer = keras.layers.LSTM(units, return_sequences=True)\n     model.add(layer)\n     outputs = model.layers[-1].output\n     self.assertEqual(outputs.shape.as_list(), [None, timesteps, units])\n@@ -93,7 +97,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    layer = lstm.LSTM(units, input_shape=(None, embedding_dim))\n+    layer = keras.layers.LSTM(units, input_shape=(None, embedding_dim))\n     model = keras.models.Sequential()\n     model.add(layer)\n     model.compile(tf.compat.v1.train.GradientDescentOptimizer(0.001), 'mse')\n@@ -106,15 +110,15 @@ class LSTMV2Test(test_combinations.TestCase):\n     targets = np.abs(np.random.random((2, 3, 5)))\n     targets /= targets.sum(axis=-1, keepdims=True)\n     model = keras.models.Sequential()\n-    model.add(lstm.LSTM(10, return_sequences=True, unroll=False))\n-    model.add(lstm.LSTM(5, return_sequences=True, unroll=False))\n+    model.add(keras.layers.LSTM(10, return_sequences=True, unroll=False))\n+    model.add(keras.layers.LSTM(5, return_sequences=True, unroll=False))\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))\n     model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n \n   def test_from_config_LSTM(self):\n-    layer_class = lstm.LSTM\n+    layer_class = keras.layers.LSTM\n     for stateful in (False, True):\n       l1 = layer_class(units=1, stateful=stateful)\n       l2 = layer_class.from_config(l1.get_config())\n@@ -130,7 +134,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     # Test with Keras tensor\n     inputs = keras.Input((timesteps, embedding_dim))\n     initial_state = [keras.Input((units,)) for _ in range(num_states)]\n-    layer = lstm.LSTM(units)\n+    layer = keras.layers.LSTM(units)\n     if len(initial_state) == 1:\n       output = layer(inputs, initial_state=initial_state[0])\n     else:\n@@ -164,7 +168,7 @@ class LSTMV2Test(test_combinations.TestCase):\n         keras.backend.random_normal_variable((num_samples, units), 0, 1)\n         for _ in range(num_states)\n     ]\n-    layer = lstm.LSTM(units)\n+    layer = keras.layers.LSTM(units)\n     output = layer(inputs, initial_state=initial_state)\n \n     model = keras.models.Model(inputs, output)\n@@ -183,7 +187,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     units = 3\n     num_samples = 2\n \n-    layer = lstm.LSTM(units, stateful=True)\n+    layer = keras.layers.LSTM(units, stateful=True)\n     layer.build((num_samples, timesteps, embedding_dim))\n     initial_weight_count = len(layer.weights)\n     layer.reset_states()\n@@ -223,7 +227,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     inputs = keras.Input((timesteps, embedding_dim))\n     _ = keras.layers.Masking()(inputs)\n     initial_state = [keras.Input((units,)) for _ in range(num_states)]\n-    output = lstm.LSTM(units)(\n+    output = keras.layers.LSTM(units)(\n         inputs, initial_state=initial_state)\n \n     model = keras.models.Model([inputs] + initial_state, output)\n@@ -250,7 +254,7 @@ class LSTMV2Test(test_combinations.TestCase):\n \n     inputs = keras.Input(batch_shape=(num_samples, timesteps, embedding_dim))\n     masked = keras.layers.Masking()(inputs)\n-    layer = lstm.LSTM(units, return_state=True, stateful=True)\n+    layer = keras.layers.LSTM(units, return_state=True, stateful=True)\n     outputs = layer(masked)\n     state = outputs[1:]\n     assert len(state) == num_states\n@@ -267,11 +271,11 @@ class LSTMV2Test(test_combinations.TestCase):\n     num_samples = 2\n \n     inputs = keras.Input(batch_shape=(num_samples, timesteps, embedding_dim))\n-    layer = lstm.LSTM(\n+    layer = keras.layers.LSTM(\n         units, return_state=True, return_sequences=True)\n     outputs = layer(inputs)\n     output, state = outputs[0], outputs[1:]\n-    output = lstm.LSTM(units)(output, initial_state=state)\n+    output = keras.layers.LSTM(units)(output, initial_state=state)\n     model = keras.models.Model(inputs, output)\n \n     inputs = np.random.random((num_samples, timesteps, embedding_dim))\n@@ -283,7 +287,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     units = 3\n     num_samples = 2\n     num_states = 2\n-    layer_class = lstm.LSTM\n+    layer_class = keras.layers.LSTM\n \n     # Test with Keras tensor\n     main_inputs = keras.Input((timesteps, embedding_dim))\n@@ -308,52 +312,6 @@ class LSTMV2Test(test_combinations.TestCase):\n     targets = np.random.random((num_samples, units))\n     model.train_on_batch([main_inputs] + initial_state, targets)\n \n-  @tf.test.disable_with_predicate(\n-      pred=tf.test.is_built_with_rocm,\n-      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n-  @test_utils.run_v2_only\n-  def test_lstm_v2_feature_parity_with_canonical_lstm(self):\n-    input_shape = 10\n-    rnn_state_size = 8\n-    timestep = 4\n-    batch = 20\n-\n-    (x_train, y_train), _ = test_utils.get_test_data(\n-        train_samples=batch,\n-        test_samples=0,\n-        input_shape=(timestep, input_shape),\n-        num_classes=rnn_state_size,\n-        random_seed=87654321)\n-    y_train = np_utils.to_categorical(y_train, rnn_state_size)\n-    # For the last batch item of the test data, we filter out the last\n-    # timestep to simulate the variable length sequence and masking test.\n-    x_train[-2:, -1, :] = 0.0\n-    y_train[-2:] = 0\n-\n-    inputs = keras.layers.Input(\n-        shape=[timestep, input_shape], dtype=tf.float32)\n-    masked_input = keras.layers.Masking()(inputs)\n-    lstm_layer = lstm_v1.LSTM(rnn_state_size, recurrent_activation='sigmoid')\n-    output = lstm_layer(masked_input)\n-    lstm_model = keras.models.Model(inputs, output)\n-    weights = lstm_model.get_weights()\n-    y_1 = lstm_model.predict(x_train)\n-    lstm_model.compile('rmsprop', 'mse')\n-    lstm_model.fit(x_train, y_train)\n-    y_2 = lstm_model.predict(x_train)\n-\n-    with test_utils.device(should_use_gpu=True):\n-      cudnn_layer = lstm.LSTM(rnn_state_size)\n-      cudnn_model = keras.models.Model(inputs, cudnn_layer(masked_input))\n-    cudnn_model.set_weights(weights)\n-    y_3 = cudnn_model.predict(x_train)\n-    cudnn_model.compile('rmsprop', 'mse')\n-    cudnn_model.fit(x_train, y_train)\n-    y_4 = cudnn_model.predict(x_train)\n-\n-    self.assertAllClose(y_1, y_3, rtol=1e-5, atol=2e-5)\n-    self.assertAllClose(y_2, y_4, rtol=1e-5, atol=2e-5)\n-\n   @parameterized.named_parameters(('v0', 0), ('v1', 1), ('v2', 2))\n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n@@ -364,14 +322,14 @@ class LSTMV2Test(test_combinations.TestCase):\n     embedding_dim = 4\n     units = 2\n     test_utils.layer_test(\n-        lstm.LSTM,\n+        keras.layers.LSTM,\n         kwargs={\n             'units': units,\n             'implementation': implementation_mode\n         },\n         input_shape=(num_samples, timesteps, embedding_dim))\n \n-    layer_class = lstm.LSTM\n+    layer_class = keras.layers.LSTM\n     k_constraint = keras.constraints.max_norm(0.01)\n     r_constraint = keras.constraints.max_norm(0.01)\n     b_constraint = keras.constraints.max_norm(0.01)\n@@ -388,7 +346,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)\n     self.assertEqual(layer.cell.bias.constraint, b_constraint)\n \n-    layer_class = lstm.LSTM\n+    layer_class = keras.layers.LSTM\n     inputs = np.random.random((2, 3, 4))\n     targets = np.abs(np.random.random((2, 3, 5)))\n     targets /= targets.sum(axis=-1, keepdims=True)\n@@ -409,82 +367,13 @@ class LSTMV2Test(test_combinations.TestCase):\n     targets /= targets.sum(axis=-1, keepdims=True)\n     model = keras.models.Sequential()\n     model.add(keras.layers.Masking(input_shape=(3, 4)))\n-    model.add(lstm.LSTM(10, return_sequences=True, unroll=False))\n-    model.add(lstm.LSTM(5, return_sequences=True, unroll=False))\n+    model.add(keras.layers.LSTM(10, return_sequences=True, unroll=False))\n+    model.add(keras.layers.LSTM(5, return_sequences=True, unroll=False))\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))\n     model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n \n-  @parameterized.named_parameters(\n-      # test_name, time_major, go_backwards\n-      ('normal', False, False),\n-      ('time_major', True, False),\n-      ('go_backwards', False, True),\n-      ('both', True, True),\n-  )\n-  def test_time_major_and_go_backward(self, time_major, go_backwards):\n-    input_shape = 10\n-    rnn_state_size = 8\n-    timestep = 4\n-    batch = 100\n-\n-    x_train = np.random.random((batch, timestep, input_shape))\n-\n-    def build_model(layer_cls):\n-      inputs = keras.layers.Input(\n-          shape=[timestep, input_shape], dtype=tf.float32)\n-      layer = layer_cls(rnn_state_size,\n-                        recurrent_activation='sigmoid',\n-                        time_major=time_major,\n-                        return_sequences=True,\n-                        go_backwards=go_backwards)\n-      if time_major:\n-        converted_input = keras.layers.Lambda(\n-            lambda t: tf.transpose(t, [1, 0, 2]))(inputs)\n-        outputs = layer(converted_input)\n-        outputs = keras.layers.Lambda(\n-            lambda t: tf.transpose(t, [1, 0, 2]))(outputs)\n-      else:\n-        outputs = layer(inputs)\n-      return keras.models.Model(inputs, outputs)\n-\n-    lstm_model = build_model(lstm_v1.LSTM)\n-    y_ref = lstm_model.predict(x_train)\n-    weights = lstm_model.get_weights()\n-\n-    lstm_v2_model = build_model(lstm.LSTM)\n-    lstm_v2_model.set_weights(weights)\n-    y = lstm_v2_model.predict(x_train)\n-\n-    self.assertAllClose(y, y_ref)\n-\n-    input_shape = 10\n-    rnn_state_size = 8\n-    output_shape = 8\n-    timestep = 4\n-    batch = 100\n-    epoch = 10\n-\n-    (x_train, y_train), _ = test_utils.get_test_data(\n-        train_samples=batch,\n-        test_samples=0,\n-        input_shape=(timestep, input_shape),\n-        num_classes=output_shape)\n-    y_train = np_utils.to_categorical(y_train, output_shape)\n-\n-    layer = lstm.LSTM(rnn_state_size)\n-\n-    inputs = keras.layers.Input(\n-        shape=[timestep, input_shape], dtype=tf.float32)\n-\n-    outputs = layer(inputs)\n-    model = keras.models.Model(inputs, outputs)\n-    model.compile('rmsprop', loss='mse')\n-    model.fit(x_train, y_train, epochs=epoch)\n-    model.evaluate(x_train, y_train)\n-    model.predict(x_train)\n-\n   @parameterized.named_parameters(\n       # test_name, use_bias, bias_initializer, activation\n       ('normal', True, 'zeros'),\n@@ -506,7 +395,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     def build_model():\n       inputs = keras.layers.Input(\n           shape=[timestep, input_dim], dtype=tf.float32)\n-      layer = lstm.LSTM(\n+      layer = keras.layers.LSTM(\n           units,\n           use_bias=use_bias,\n           bias_initializer=bias_initializer)\n@@ -525,42 +414,25 @@ class LSTMV2Test(test_combinations.TestCase):\n     self.assertAllClose(layer.get_weights(), new_layer.get_weights())\n \n   def test_lstm_output_on_multiple_kernel(self):\n-    input_shape = 10\n-    rnn_state_size = 8\n-    timestep = 4\n-    batch = 100\n-\n-    x_train = np.random.random((batch, timestep, input_shape))\n+    x_train = np.random.random((self.batch, self.timestep, self.input_shape))\n \n     inputs = keras.layers.Input(\n-        shape=[timestep, input_shape], dtype=tf.float32)\n+        shape=[self.timestep, self.input_shape], dtype=tf.float32)\n     with test_utils.device(should_use_gpu=False):\n-      layer = lstm.LSTM(rnn_state_size)\n+      layer = keras.layers.LSTM(self.rnn_state_size)\n       output = layer(inputs)\n       cpu_model = keras.models.Model(inputs, output)\n       weights = cpu_model.get_weights()\n     y_1 = cpu_model.predict(x_train)\n \n     with test_utils.device(should_use_gpu=True):\n-      layer = lstm.LSTM(rnn_state_size)\n+      layer = keras.layers.LSTM(self.rnn_state_size)\n       output = layer(inputs)\n       gpu_model = keras.models.Model(inputs, output)\n       gpu_model.set_weights(weights)\n     y_2 = gpu_model.predict(x_train)\n \n-    # Note that cuDNN uses 'sigmoid' as activation, so the LSTM V2 uses\n-    # 'sigmoid' as default. Construct the canonical LSTM with sigmoid to achieve\n-    # the same output.\n-    with test_utils.device(should_use_gpu=True):\n-      layer = lstm_v1.LSTM(rnn_state_size, recurrent_activation='sigmoid')\n-      output = layer(inputs)\n-      canonical_model = keras.models.Model(inputs, output)\n-      # Remove the extra cudnn bias since canonical lstm will not use it.\n-      canonical_model.set_weights(weights[:3])\n-    y_3 = canonical_model.predict(x_train)\n-\n     self.assertAllClose(y_1, y_2)\n-    self.assertAllClose(y_2, y_3)\n \n   def test_return_sequences_LSTM(self):\n     num_samples = 2\n@@ -568,7 +440,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     embedding_dim = 4\n     units = 2\n     test_utils.layer_test(\n-        lstm.LSTM,\n+        keras.layers.LSTM,\n         kwargs={\n             'units': units,\n             'return_sequences': True\n@@ -585,7 +457,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     embedding_dim = 4\n     units = 2\n     test_utils.layer_test(\n-        lstm.LSTM,\n+        keras.layers.LSTM,\n         kwargs={\n             'units': units,\n             'return_sequences': True,\n@@ -596,7 +468,7 @@ class LSTMV2Test(test_combinations.TestCase):\n \n   def test_regularizers_LSTM(self):\n     embedding_dim = 4\n-    layer_class = lstm.LSTM\n+    layer_class = keras.layers.LSTM\n     layer = layer_class(\n         5,\n         return_sequences=False,\n@@ -623,7 +495,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    layer_class = lstm.LSTM\n+    layer_class = keras.layers.LSTM\n     model = keras.models.Sequential()\n     model.add(\n         keras.layers.Embedding(\n@@ -704,7 +576,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     model = keras.Sequential([\n         keras.layers.Embedding(vocab_size, embedding_dim,\n                                batch_input_shape=[batch_size, timestep]),\n-        lstm.LSTM(units, return_sequences=True, stateful=True),\n+        keras.layers.LSTM(units, return_sequences=True, stateful=True),\n         keras.layers.Dense(vocab_size)\n     ])\n     model.compile(\n@@ -719,7 +591,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     embedding_dim = 4\n     units = 2\n     test_utils.layer_test(\n-        lstm.LSTM,\n+        keras.layers.LSTM,\n         kwargs={\n             'units': units,\n             'dropout': 0.1,\n@@ -733,9 +605,9 @@ class LSTMV2Test(test_combinations.TestCase):\n     vocab_size = 1000\n     model = keras.Sequential([\n         keras.layers.Embedding(vocab_size, 64),\n-        keras.layers.Bidirectional(lstm.LSTM(\n+        keras.layers.Bidirectional(keras.layers.LSTM(\n             64, return_sequences=True)),\n-        keras.layers.Bidirectional(lstm.LSTM(32)),\n+        keras.layers.Bidirectional(keras.layers.LSTM(32)),\n         keras.layers.Dense(64, activation='relu'),\n         keras.layers.Dense(1, activation='sigmoid')\n     ])\n@@ -764,16 +636,8 @@ class LSTMV2Test(test_combinations.TestCase):\n     mask = np.ones((batch_size, timestep)).astype(np.bool)\n     mask[:, masksteps:] = 0\n \n-    # Test for V1 behavior.\n-    lstm_v1_layer = lstm_v1.LSTM(\n+    lstm_layer = keras.layers.LSTM(\n         units, return_sequences=True, go_backwards=True)\n-    with test_utils.device(should_use_gpu=True):\n-      outputs_masked_v1 = lstm_v1_layer(inputs, mask=tf.constant(mask))\n-      outputs_trimmed_v1 = lstm_v1_layer(inputs[:, :masksteps])\n-    self.assertAllClose(outputs_masked_v1[:, -masksteps:], outputs_trimmed_v1)\n-\n-    # Test for V2 behavior.\n-    lstm_layer = lstm.LSTM(units, return_sequences=True, go_backwards=True)\n     with test_utils.device(should_use_gpu=True):\n       outputs_masked = lstm_layer(inputs, mask=tf.constant(mask))\n       outputs_trimmed = lstm_layer(inputs[:, :masksteps])\n@@ -789,7 +653,7 @@ class LSTMV2Test(test_combinations.TestCase):\n           (x, y)).shuffle(100).batch(32)\n \n       inp = keras.layers.Input(shape=(4, 8))\n-      layer = lstm.LSTM(1)(inp)\n+      layer = keras.layers.LSTM(1)(inp)\n       layer = keras.layers.Dense(1)(layer)\n \n       model = keras.models.Model(inp, layer)\n@@ -816,7 +680,7 @@ class LSTMV2Test(test_combinations.TestCase):\n             mask_zero=True,\n             input_length=timestep,\n             batch_input_shape=(num_samples, timestep)))\n-    layer = lstm.LSTM(units)\n+    layer = keras.layers.LSTM(units)\n     model.add(layer)\n     model.compile(\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n@@ -829,14 +693,14 @@ class LSTMV2Test(test_combinations.TestCase):\n   def test_deepcopy(self):\n     if not tf.executing_eagerly():\n       self.skipTest('v2-only test')\n-    original_layer = lstm.LSTM(5)\n+    original_layer = keras.layers.LSTM(5)\n     copied_layer = copy.deepcopy(original_layer)\n     self.assertEqual(copied_layer.units, 5)\n     self.assertEqual(original_layer.get_config(), original_layer.get_config())\n \n     # Copy layer before layer call on inputs without weight initialization.\n     inputs = np.random.normal(size=[32, 10, 8]).astype(np.float32)\n-    original_layer = lstm.LSTM(4)\n+    original_layer = keras.layers.LSTM(4)\n     copied_layer = copy.deepcopy(original_layer)\n     outputs = original_layer(inputs)\n     copied_outputs = copied_layer(inputs)\n@@ -844,23 +708,12 @@ class LSTMV2Test(test_combinations.TestCase):\n         self.evaluate(outputs), self.evaluate(copied_outputs))\n \n     # Copy layer after layer call on inputs with weight initialization.\n-    original_layer = lstm.LSTM(4)\n+    original_layer = keras.layers.LSTM(4)\n     outputs = original_layer(inputs)\n     copied_layer = copy.deepcopy(original_layer)\n     copied_outputs = copied_layer(inputs)\n     self.assertAllClose(self.evaluate(outputs), self.evaluate(copied_outputs))\n \n-\n-@test_combinations.run_all_keras_modes(config=_config)\n-class LSTMGraphRewriteTest(test_combinations.TestCase):\n-\n-  input_shape = 10\n-  output_shape = 8\n-  rnn_state_size = 8\n-  timestep = 4\n-  batch = 100\n-  epoch = 1\n-\n   def _test_runtime_with_model(self, model):\n \n     (x_train, y_train), _ = test_utils.get_test_data(\n@@ -891,7 +744,7 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n \n   @test_utils.run_v2_only\n   def test_LSTM_runtime(self):\n-    layer = lstm.LSTM(self.rnn_state_size, return_runtime=True)\n+    layer = keras.layers.LSTM(self.rnn_state_size, return_runtime=True)\n \n     inputs = keras.layers.Input(\n         shape=[self.timestep, self.input_shape], dtype=tf.float32)\n@@ -912,7 +765,7 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n   def test_LSTM_runtime_with_mask(self):\n     # Masking will affect which backend is selected based on whether the mask\n     # is strictly right padded.\n-    layer = lstm.LSTM(self.rnn_state_size, return_runtime=True)\n+    layer = keras.layers.LSTM(self.rnn_state_size, return_runtime=True)\n \n     inputs = keras.layers.Input(\n         shape=[self.timestep, self.input_shape], dtype=tf.float32)\n@@ -968,7 +821,7 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n     # This test is to demonstrate the graph rewrite of grappler plugin under\n     # the condition that the function returns different number of internal\n     # states.\n-    layer = lstm.LSTM(self.rnn_state_size, return_runtime=True)\n+    layer = keras.layers.LSTM(self.rnn_state_size, return_runtime=True)\n \n     inputs = keras.layers.Input(\n         shape=[self.timestep, self.input_shape], dtype=tf.float32)\n@@ -992,142 +845,426 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n     self._test_runtime_with_model(model)\n \n \n-class LSTMPerformanceTest(tf.test.Benchmark):\n+@test_combinations.run_all_keras_modes\n+class LSTMLayerTest(test_combinations.TestCase):\n \n-  def _measure_performance(self, test_config, model, x_train, y_train):\n-    batch = test_config['batch']\n-    epoch = test_config['epoch']\n-    warmup_epoch = test_config['warmup_epoch']\n+  def test_return_sequences_LSTM(self):\n+    num_samples = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 2\n+    test_utils.layer_test(\n+        keras.layers.LSTM,\n+        kwargs={'units': units,\n+                'return_sequences': True},\n+        input_shape=(num_samples, timesteps, embedding_dim))\n \n-    # warm up the model\n-    model.fit(x_train, y_train, batch_size=batch, epochs=warmup_epoch)\n-    start_time = time.time()\n-    model.fit(x_train, y_train, batch_size=batch, epochs=epoch - warmup_epoch)\n-    end_time = time.time()\n-    return (end_time - start_time) / (epoch - warmup_epoch)\n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Double type is yet not supported in ROCm')\n+  @test_utils.run_v2_only\n+  def test_float64_LSTM(self):\n+    num_samples = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 2\n+    test_utils.layer_test(\n+        keras.layers.LSTM,\n+        kwargs={'units': units,\n+                'return_sequences': True,\n+                'dtype': 'float64'},\n+        input_shape=(num_samples, timesteps, embedding_dim),\n+        input_dtype='float64')\n \n-  def _time_performance_run_cudnn_lstm(self, test_config, x_train, y_train):\n-    # Get the performance number for standard Cudnn LSTM\n-    input_shape = test_config['input_shape']\n-    rnn_state_size = test_config['rnn_state_size']\n-    timestep = test_config['timestep']\n+  def test_static_shape_inference_LSTM(self):\n+    # Github issue: 15165\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 2\n \n-    cudnn_lstm_layer = keras.layers.CuDNNLSTM(rnn_state_size)\n-    inputs = keras.layers.Input(\n-        shape=[timestep, input_shape], dtype=tf.float32)\n+    model = keras.models.Sequential()\n+    inputs = keras.layers.Dense(embedding_dim,\n+                                input_shape=(timesteps, embedding_dim))\n+    model.add(inputs)\n+    layer = keras.layers.LSTM(units, return_sequences=True)\n+    model.add(layer)\n+    outputs = model.layers[-1].output\n+    self.assertEqual(outputs.shape.as_list(), [None, timesteps, units])\n \n-    outputs = cudnn_lstm_layer(inputs)\n-    model = keras.models.Model(inputs, outputs)\n-    model.compile('sgd', 'mse')\n+  def test_dynamic_behavior_LSTM(self):\n+    num_samples = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 2\n+    layer = keras.layers.LSTM(units, input_shape=(None, embedding_dim))\n+    model = keras.models.Sequential()\n+    model.add(layer)\n+    model.compile(\n+        'rmsprop',\n+        'mse',\n+        run_eagerly=test_utils.should_run_eagerly())\n \n-    sec_per_epoch = self._measure_performance(\n-        test_config, model, x_train, y_train)\n-    logging.info('Average performance for %s per epoch is: %s',\n-                 'CuDNN LSTM', sec_per_epoch)\n-    return sec_per_epoch\n+    x = np.random.random((num_samples, timesteps, embedding_dim))\n+    y = np.random.random((num_samples, units))\n+    model.train_on_batch(x, y)\n \n-  def _time_performance_run_unifed_lstm_gpu(\n-      self, test_config, x_train, y_train):\n-    # Get performance number for lstm_v2 with grappler swap the impl\n-    input_shape = test_config['input_shape']\n-    rnn_state_size = test_config['rnn_state_size']\n-    timestep = test_config['timestep']\n+  def test_dropout_LSTM(self):\n+    num_samples = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 2\n+    test_utils.layer_test(\n+        keras.layers.LSTM,\n+        kwargs={'units': units,\n+                'dropout': 0.1,\n+                'recurrent_dropout': 0.1},\n+        input_shape=(num_samples, timesteps, embedding_dim))\n \n-    layer = lstm.LSTM(rnn_state_size)\n-    inputs = keras.layers.Input(\n-        shape=[timestep, input_shape], dtype=tf.float32)\n+  def test_recurrent_dropout_with_implementation_restriction(self):\n+    layer = keras.layers.LSTM(2, recurrent_dropout=0.1, implementation=2)\n+    # The implementation is force to 1 due to the limit of recurrent_dropout.\n+    self.assertEqual(layer.implementation, 1)\n \n+  @parameterized.parameters([0, 1, 2])\n+  def test_implementation_mode_LSTM(self, implementation_mode):\n+    num_samples = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 2\n+    test_utils.layer_test(\n+        keras.layers.LSTM,\n+        kwargs={'units': units,\n+                'implementation': implementation_mode},\n+        input_shape=(num_samples, timesteps, embedding_dim))\n+\n+  def test_constraints_LSTM(self):\n+    embedding_dim = 4\n+    layer_class = keras.layers.LSTM\n+    k_constraint = keras.constraints.max_norm(0.01)\n+    r_constraint = keras.constraints.max_norm(0.01)\n+    b_constraint = keras.constraints.max_norm(0.01)\n+    layer = layer_class(\n+        5,\n+        return_sequences=False,\n+        weights=None,\n+        input_shape=(None, embedding_dim),\n+        kernel_constraint=k_constraint,\n+        recurrent_constraint=r_constraint,\n+        bias_constraint=b_constraint)\n+    layer.build((None, None, embedding_dim))\n+    self.assertEqual(layer.cell.kernel.constraint, k_constraint)\n+    self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)\n+    self.assertEqual(layer.cell.bias.constraint, b_constraint)\n+\n+  @parameterized.parameters([True, False])\n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input.')\n+  def test_with_masking_layer_LSTM(self, unroll):\n+    layer_class = keras.layers.LSTM\n+    inputs = np.random.random((2, 3, 4))\n+    targets = np.abs(np.random.random((2, 3, 5)))\n+    targets /= targets.sum(axis=-1, keepdims=True)\n+    model = keras.models.Sequential()\n+    model.add(keras.layers.Masking(input_shape=(3, 4)))\n+    model.add(layer_class(units=5, return_sequences=True, unroll=unroll))\n+    model.compile(\n+        loss='categorical_crossentropy',\n+        optimizer='rmsprop',\n+        run_eagerly=test_utils.should_run_eagerly())\n+    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n+\n+  @parameterized.parameters([True, False])\n+  def test_masking_with_stacking_LSTM(self, unroll):\n+    inputs = np.random.random((2, 3, 4))\n+    targets = np.abs(np.random.random((2, 3, 5)))\n+    targets /= targets.sum(axis=-1, keepdims=True)\n+    model = keras.models.Sequential()\n+    model.add(keras.layers.Masking(input_shape=(3, 4)))\n+    lstm_cells = [keras.layers.LSTMCell(10), keras.layers.LSTMCell(5)]\n+    model.add(keras.layers.RNN(\n+        lstm_cells, return_sequences=True, unroll=unroll))\n+    model.compile(\n+        loss='categorical_crossentropy',\n+        optimizer='rmsprop',\n+        run_eagerly=test_utils.should_run_eagerly())\n+    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n+\n+  def test_from_config_LSTM(self):\n+    layer_class = keras.layers.LSTM\n+    for stateful in (False, True):\n+      l1 = layer_class(units=1, stateful=stateful)\n+      l2 = layer_class.from_config(l1.get_config())\n+      assert l1.get_config() == l2.get_config()\n+\n+  def test_deep_copy_LSTM(self):\n+    cell = keras.layers.LSTMCell(5)\n+    copied_cell = copy.deepcopy(cell)\n+    self.assertEqual(copied_cell.units, 5)\n+    self.assertEqual(cell.get_config(), copied_cell.get_config())\n+\n+  def test_specify_initial_state_keras_tensor(self):\n+    num_states = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 3\n+    num_samples = 2\n+\n+    # Test with Keras tensor\n+    inputs = keras.Input((timesteps, embedding_dim))\n+    initial_state = [keras.Input((units,)) for _ in range(num_states)]\n+    layer = keras.layers.LSTM(units)\n+    if len(initial_state) == 1:\n+      output = layer(inputs, initial_state=initial_state[0])\n+    else:\n+      output = layer(inputs, initial_state=initial_state)\n+    self.assertTrue(\n+        any(initial_state[0] is t\n+            for t in layer._inbound_nodes[0].input_tensors))\n+\n+    model = keras.models.Model([inputs] + initial_state, output)\n+    model.compile(\n+        loss='categorical_crossentropy',\n+        optimizer=tf.compat.v1.train.AdamOptimizer(),\n+        run_eagerly=test_utils.should_run_eagerly())\n+\n+    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n+    initial_state = [np.random.random((num_samples, units))\n+                     for _ in range(num_states)]\n+    targets = np.random.random((num_samples, units))\n+    model.train_on_batch([inputs] + initial_state, targets)\n+\n+  def test_specify_initial_state_non_keras_tensor(self):\n+    num_states = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 3\n+    num_samples = 2\n+\n+    # Test with non-Keras tensor\n+    inputs = keras.Input((timesteps, embedding_dim))\n+    initial_state = [keras.backend.random_normal_variable(\n+        (num_samples, units), 0, 1)\n+                     for _ in range(num_states)]\n+    layer = keras.layers.LSTM(units)\n+    output = layer(inputs, initial_state=initial_state)\n+\n+    model = keras.models.Model(inputs, output)\n+    model.compile(\n+        loss='categorical_crossentropy',\n+        optimizer=tf.compat.v1.train.AdamOptimizer(),\n+        run_eagerly=test_utils.should_run_eagerly())\n+\n+    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n+    targets = np.random.random((num_samples, units))\n+    model.train_on_batch(inputs, targets)\n+\n+  def test_reset_states_with_values(self):\n+    num_states = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 3\n+    num_samples = 2\n+\n+    layer = keras.layers.LSTM(units, stateful=True)\n+    layer.build((num_samples, timesteps, embedding_dim))\n+    layer.reset_states()\n+    assert len(layer.states) == num_states\n+    assert layer.states[0] is not None\n+    self.assertAllClose(\n+        keras.backend.eval(layer.states[0]),\n+        np.zeros(keras.backend.int_shape(layer.states[0])),\n+        atol=1e-4)\n+    state_shapes = [keras.backend.int_shape(state) for state in layer.states]\n+    values = [np.ones(shape) for shape in state_shapes]\n+    if len(values) == 1:\n+      values = values[0]\n+    layer.reset_states(values)\n+    self.assertAllClose(\n+        keras.backend.eval(layer.states[0]),\n+        np.ones(keras.backend.int_shape(layer.states[0])),\n+        atol=1e-4)\n+\n+    # Test with invalid data\n+    with self.assertRaises(ValueError):\n+      layer.reset_states([1] * (len(layer.states) + 1))\n+\n+  def test_specify_state_with_masking(self):\n+    num_states = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 3\n+    num_samples = 2\n+\n+    inputs = keras.Input((timesteps, embedding_dim))\n+    _ = keras.layers.Masking()(inputs)\n+    initial_state = [keras.Input((units,)) for _ in range(num_states)]\n+    output = keras.layers.LSTM(units)(inputs, initial_state=initial_state)\n+\n+    model = keras.models.Model([inputs] + initial_state, output)\n+    model.compile(\n+        loss='categorical_crossentropy',\n+        optimizer='rmsprop',\n+        run_eagerly=test_utils.should_run_eagerly())\n+\n+    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n+    initial_state = [np.random.random((num_samples, units))\n+                     for _ in range(num_states)]\n+    targets = np.random.random((num_samples, units))\n+    model.train_on_batch([inputs] + initial_state, targets)\n+\n+  def test_return_state(self):\n+    num_states = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 3\n+    num_samples = 2\n+\n+    inputs = keras.Input(batch_shape=(num_samples, timesteps, embedding_dim))\n+    layer = keras.layers.LSTM(units, return_state=True, stateful=True)\n     outputs = layer(inputs)\n-    model = keras.models.Model(inputs, outputs)\n-    model.compile('sgd', 'mse')\n+    state = outputs[1:]\n+    assert len(state) == num_states\n+    model = keras.models.Model(inputs, state[0])\n \n-    sec_per_epoch = self._measure_performance(\n-        test_config, model, x_train, y_train)\n-    logging.info('Average performance for %s per epoch is: %s',\n-                 'LSTM V2', sec_per_epoch)\n-    return sec_per_epoch\n+    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n+    state = model.predict(inputs)\n+    self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)\n \n-  def _time_performance_run_normal_lstm(\n-      self, test_config, x_train, y_train):\n-    # Get performance number for standard LSTM on GPU.\n-    input_shape = test_config['input_shape']\n-    rnn_state_size = test_config['rnn_state_size']\n-    timestep = test_config['timestep']\n-\n-    layer = lstm_v1.LSTM(rnn_state_size)\n-    inputs = keras.layers.Input(\n-        shape=[timestep, input_shape], dtype=tf.float32)\n+  def test_state_reuse(self):\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 3\n+    num_samples = 2\n \n+    inputs = keras.Input(batch_shape=(num_samples, timesteps, embedding_dim))\n+    layer = keras.layers.LSTM(units, return_state=True, return_sequences=True)\n     outputs = layer(inputs)\n-    model = keras.models.Model(inputs, outputs)\n-    model.compile('sgd', 'mse')\n+    output, state = outputs[0], outputs[1:]\n+    output = keras.layers.LSTM(units)(output, initial_state=state)\n+    model = keras.models.Model(inputs, output)\n \n-    sec_per_epoch = self._measure_performance(\n-        test_config, model, x_train, y_train)\n-    logging.info('Average performance for %s per epoch is: %s',\n-                 'Normal LSTM', sec_per_epoch)\n-    return sec_per_epoch\n+    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n+    outputs = model.predict(inputs)\n \n-  def _benchmark_performance_with_standard_cudnn_impl(self):\n-    if not tf.test.is_gpu_available():\n-      self.skipTest('performance test will only run on GPU')\n+  def test_initial_states_as_other_inputs(self):\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 3\n+    num_samples = 2\n+    num_states = 2\n+    layer_class = keras.layers.LSTM\n \n-    mode = 'eager' if tf.executing_eagerly() else 'graph'\n-    batch = 64\n-    num_batch = 10\n-    test_config = {\n-        'input_shape': 128,\n-        'rnn_state_size': 64,\n-        'output_shape': 64,\n-        'timestep': 50,\n-        'batch': batch,\n-        'epoch': 20,\n-        # The performance for warmup epoch is ignored.\n-        'warmup_epoch': 1,\n-    }\n-    (x_train, y_train), _ = test_utils.get_test_data(\n-        train_samples=(batch * num_batch),\n-        test_samples=0,\n-        input_shape=(test_config['timestep'], test_config['input_shape']),\n-        num_classes=test_config['output_shape'])\n-    y_train = np_utils.to_categorical(y_train, test_config['output_shape'])\n+    # Test with Keras tensor\n+    main_inputs = keras.Input((timesteps, embedding_dim))\n+    initial_state = [keras.Input((units,)) for _ in range(num_states)]\n+    inputs = [main_inputs] + initial_state\n \n-    cudnn_sec_per_epoch = self._time_performance_run_cudnn_lstm(\n-        test_config, x_train, y_train)\n-    lstm_v2_sec_per_epoch = self._time_performance_run_unifed_lstm_gpu(\n-        test_config, x_train, y_train)\n-    normal_lstm_sec_per_epoch = self._time_performance_run_normal_lstm(\n-        test_config, x_train, y_train)\n+    layer = layer_class(units)\n+    output = layer(inputs)\n+    self.assertTrue(\n+        any(initial_state[0] is t\n+            for t in layer._inbound_nodes[0].input_tensors))\n \n-    cudnn_vs_v2 = cudnn_sec_per_epoch / lstm_v2_sec_per_epoch\n-    v2_vs_normal = normal_lstm_sec_per_epoch / lstm_v2_sec_per_epoch\n+    model = keras.models.Model(inputs, output)\n+    model.compile(\n+        loss='categorical_crossentropy',\n+        optimizer=tf.compat.v1.train.AdamOptimizer(),\n+        run_eagerly=test_utils.should_run_eagerly())\n \n-    self.report_benchmark(name='keras_cudnn_lstm_' + mode,\n-                          wall_time=cudnn_sec_per_epoch,\n-                          iters=test_config['epoch'],\n-                          extras=test_config)\n-    self.report_benchmark(name='keras_lstm_v2_' + mode,\n-                          wall_time=lstm_v2_sec_per_epoch,\n-                          iters=test_config['epoch'],\n-                          extras=test_config)\n-    self.report_benchmark(name='keras_canonical_lstm_' + mode,\n-                          wall_time=normal_lstm_sec_per_epoch,\n-                          iters=test_config['epoch'],\n-                          extras=test_config)\n+    main_inputs = np.random.random((num_samples, timesteps, embedding_dim))\n+    initial_state = [np.random.random((num_samples, units))\n+                     for _ in range(num_states)]\n+    targets = np.random.random((num_samples, units))\n+    model.train_on_batch([main_inputs] + initial_state, targets)\n \n-    logging.info('Expect the performance of LSTM V2 is within 80% of '\n-                 'cuDNN LSTM, got {0:.2f}%'.format(cudnn_vs_v2 * 100))\n-    logging.info('Expect the performance of LSTM V2 is more than 5 times'\n-                 ' of normal LSTM, got {0:.2f}'.format(v2_vs_normal))\n+  def test_regularizers_LSTM(self):\n+    embedding_dim = 4\n+    layer_class = keras.layers.LSTM\n+    layer = layer_class(\n+        5,\n+        return_sequences=False,\n+        weights=None,\n+        input_shape=(None, embedding_dim),\n+        kernel_regularizer=keras.regularizers.l1(0.01),\n+        recurrent_regularizer=keras.regularizers.l1(0.01),\n+        bias_regularizer='l2',\n+        activity_regularizer='l1')\n+    layer.build((None, None, 2))\n+    self.assertEqual(len(layer.losses), 3)\n+    x = keras.backend.variable(np.ones((2, 3, 2)))\n+    layer(x)\n+    if tf.executing_eagerly():\n+      self.assertEqual(len(layer.losses), 4)\n+    else:\n+      self.assertEqual(len(layer.get_losses_for(x)), 1)\n \n-  def benchmark_performance_graph(self):\n-    with tf.compat.v1.get_default_graph().as_default():\n-      with tf.compat.v1.Session(config=_config):\n-        self._benchmark_performance_with_standard_cudnn_impl()\n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input.')\n+  def test_statefulness_LSTM(self):\n+    num_samples = 2\n+    timesteps = 3\n+    embedding_dim = 4\n+    units = 2\n+    layer_class = keras.layers.LSTM\n+    model = keras.models.Sequential()\n+    model.add(\n+        keras.layers.Embedding(\n+            4,\n+            embedding_dim,\n+            mask_zero=True,\n+            input_length=timesteps,\n+            batch_input_shape=(num_samples, timesteps)))\n+    layer = layer_class(\n+        units, return_sequences=False, stateful=True, weights=None)\n+    model.add(layer)\n+    model.compile(\n+        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n+        loss='mse',\n+        run_eagerly=test_utils.should_run_eagerly())\n+    out1 = model.predict(np.ones((num_samples, timesteps)))\n+    self.assertEqual(out1.shape, (num_samples, units))\n \n-  def benchmark_performance_eager(self):\n-    with tf.__internal__.eager_context.eager_mode():\n-      self._benchmark_performance_with_standard_cudnn_impl()\n+    # train once so that the states change\n+    model.train_on_batch(\n+        np.ones((num_samples, timesteps)), np.ones((num_samples, units)))\n+    out2 = model.predict(np.ones((num_samples, timesteps)))\n+\n+    # if the state is not reset, output should be different\n+    self.assertNotEqual(out1.max(), out2.max())\n+\n+    # check that output changes after states are reset\n+    # (even though the model itself didn't change)\n+    layer.reset_states()\n+    out3 = model.predict(np.ones((num_samples, timesteps)))\n+    self.assertNotEqual(out2.max(), out3.max())\n+\n+    # check that container-level reset_states() works\n+    model.reset_states()\n+    out4 = model.predict(np.ones((num_samples, timesteps)))\n+    self.assertAllClose(out3, out4, atol=1e-5)\n+\n+    # check that the call to `predict` updated the states\n+    out5 = model.predict(np.ones((num_samples, timesteps)))\n+    self.assertNotEqual(out4.max(), out5.max())\n+\n+    # Check masking\n+    layer.reset_states()\n+\n+    left_padded_input = np.ones((num_samples, timesteps))\n+    left_padded_input[0, :1] = 0\n+    left_padded_input[1, :2] = 0\n+    out6 = model.predict(left_padded_input)\n+\n+    layer.reset_states()\n+\n+    right_padded_input = np.ones((num_samples, timesteps))\n+    right_padded_input[0, -1:] = 0\n+    right_padded_input[1, -2:] = 0\n+    out7 = model.predict(right_padded_input)\n+\n+    self.assertAllClose(out7, out6, atol=1e-5)\n \n \n if __name__ == '__main__':\n\n@@ -16,29 +16,20 @@\n # pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n from keras import activations\n-from keras import backend\n from keras import constraints\n from keras import initializers\n from keras import regularizers\n-from keras.engine import base_layer\n from keras.engine.input_spec import InputSpec\n+from keras.layers.rnn import lstm\n from keras.layers.rnn import rnn_utils\n from keras.layers.rnn.base_rnn import RNN\n-from keras.layers.rnn.dropout_rnn_cell_mixin import DropoutRNNCellMixin\n-from keras.utils import tf_utils\n-import tensorflow.compat.v2 as tf\n \n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n \n-RECURRENT_DROPOUT_WARNING_MSG = (\n-    'RNN `implementation=2` is not supported when `recurrent_dropout` is set. '\n-    'Using `implementation=1`.')\n-\n-\n @keras_export(v1=['keras.layers.LSTMCell'])\n-class LSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n+class LSTMCell(lstm.LSTMCell):\n   \"\"\"Cell class for the LSTM layer.\n \n   Args:\n@@ -107,212 +98,25 @@ class LSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n                dropout=0.,\n                recurrent_dropout=0.,\n                **kwargs):\n-    if units < 0:\n-      raise ValueError(f'Received an invalid value for argument `units`, '\n-                       f'expected a positive integer, got {units}.')\n-    # By default use cached variable under v2 mode, see b/143699808.\n-    if tf.compat.v1.executing_eagerly_outside_functions():\n-      self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n-    else:\n-      self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n-    super(LSTMCell, self).__init__(**kwargs)\n-    self.units = units\n-    self.activation = activations.get(activation)\n-    self.recurrent_activation = activations.get(recurrent_activation)\n-    self.use_bias = use_bias\n-\n-    self.kernel_initializer = initializers.get(kernel_initializer)\n-    self.recurrent_initializer = initializers.get(recurrent_initializer)\n-    self.bias_initializer = initializers.get(bias_initializer)\n-    self.unit_forget_bias = unit_forget_bias\n-\n-    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n-    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n-    self.bias_regularizer = regularizers.get(bias_regularizer)\n-\n-    self.kernel_constraint = constraints.get(kernel_constraint)\n-    self.recurrent_constraint = constraints.get(recurrent_constraint)\n-    self.bias_constraint = constraints.get(bias_constraint)\n-\n-    self.dropout = min(1., max(0., dropout))\n-    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n-    implementation = kwargs.pop('implementation', 1)\n-    if self.recurrent_dropout != 0 and implementation != 1:\n-      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)\n-      self.implementation = 1\n-    else:\n-      self.implementation = implementation\n-    self.state_size = [self.units, self.units]\n-    self.output_size = self.units\n-\n-  @tf_utils.shape_type_conversion\n-  def build(self, input_shape):\n-    default_caching_device = rnn_utils.caching_device(self)\n-    input_dim = input_shape[-1]\n-    self.kernel = self.add_weight(\n-        shape=(input_dim, self.units * 4),\n-        name='kernel',\n-        initializer=self.kernel_initializer,\n-        regularizer=self.kernel_regularizer,\n-        constraint=self.kernel_constraint,\n-        caching_device=default_caching_device)\n-    self.recurrent_kernel = self.add_weight(\n-        shape=(self.units, self.units * 4),\n-        name='recurrent_kernel',\n-        initializer=self.recurrent_initializer,\n-        regularizer=self.recurrent_regularizer,\n-        constraint=self.recurrent_constraint,\n-        caching_device=default_caching_device)\n-\n-    if self.use_bias:\n-      if self.unit_forget_bias:\n-\n-        def bias_initializer(_, *args, **kwargs):\n-          return backend.concatenate([\n-              self.bias_initializer((self.units,), *args, **kwargs),\n-              initializers.get('ones')((self.units,), *args, **kwargs),\n-              self.bias_initializer((self.units * 2,), *args, **kwargs),\n-          ])\n-      else:\n-        bias_initializer = self.bias_initializer\n-      self.bias = self.add_weight(\n-          shape=(self.units * 4,),\n-          name='bias',\n-          initializer=bias_initializer,\n-          regularizer=self.bias_regularizer,\n-          constraint=self.bias_constraint,\n-          caching_device=default_caching_device)\n-    else:\n-      self.bias = None\n-    self.built = True\n-\n-  def _compute_carry_and_output(self, x, h_tm1, c_tm1):\n-    \"\"\"Computes carry and output using split kernels.\"\"\"\n-    x_i, x_f, x_c, x_o = x\n-    h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1\n-    i = self.recurrent_activation(\n-        x_i + backend.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]))\n-    f = self.recurrent_activation(x_f + backend.dot(\n-        h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))\n-    c = f * c_tm1 + i * self.activation(x_c + backend.dot(\n-        h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))\n-    o = self.recurrent_activation(\n-        x_o + backend.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:]))\n-    return c, o\n-\n-  def _compute_carry_and_output_fused(self, z, c_tm1):\n-    \"\"\"Computes carry and output using fused kernels.\"\"\"\n-    z0, z1, z2, z3 = z\n-    i = self.recurrent_activation(z0)\n-    f = self.recurrent_activation(z1)\n-    c = f * c_tm1 + i * self.activation(z2)\n-    o = self.recurrent_activation(z3)\n-    return c, o\n-\n-  def call(self, inputs, states, training=None):\n-    h_tm1 = states[0]  # previous memory state\n-    c_tm1 = states[1]  # previous carry state\n-\n-    dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n-    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n-        h_tm1, training, count=4)\n-\n-    if self.implementation == 1:\n-      if 0 < self.dropout < 1.:\n-        inputs_i = inputs * dp_mask[0]\n-        inputs_f = inputs * dp_mask[1]\n-        inputs_c = inputs * dp_mask[2]\n-        inputs_o = inputs * dp_mask[3]\n-      else:\n-        inputs_i = inputs\n-        inputs_f = inputs\n-        inputs_c = inputs\n-        inputs_o = inputs\n-      k_i, k_f, k_c, k_o = tf.split(\n-          self.kernel, num_or_size_splits=4, axis=1)\n-      x_i = backend.dot(inputs_i, k_i)\n-      x_f = backend.dot(inputs_f, k_f)\n-      x_c = backend.dot(inputs_c, k_c)\n-      x_o = backend.dot(inputs_o, k_o)\n-      if self.use_bias:\n-        b_i, b_f, b_c, b_o = tf.split(\n-            self.bias, num_or_size_splits=4, axis=0)\n-        x_i = backend.bias_add(x_i, b_i)\n-        x_f = backend.bias_add(x_f, b_f)\n-        x_c = backend.bias_add(x_c, b_c)\n-        x_o = backend.bias_add(x_o, b_o)\n-\n-      if 0 < self.recurrent_dropout < 1.:\n-        h_tm1_i = h_tm1 * rec_dp_mask[0]\n-        h_tm1_f = h_tm1 * rec_dp_mask[1]\n-        h_tm1_c = h_tm1 * rec_dp_mask[2]\n-        h_tm1_o = h_tm1 * rec_dp_mask[3]\n-      else:\n-        h_tm1_i = h_tm1\n-        h_tm1_f = h_tm1\n-        h_tm1_c = h_tm1\n-        h_tm1_o = h_tm1\n-      x = (x_i, x_f, x_c, x_o)\n-      h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)\n-      c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)\n-    else:\n-      if 0. < self.dropout < 1.:\n-        inputs = inputs * dp_mask[0]\n-      z = backend.dot(inputs, self.kernel)\n-      z += backend.dot(h_tm1, self.recurrent_kernel)\n-      if self.use_bias:\n-        z = backend.bias_add(z, self.bias)\n-\n-      z = tf.split(z, num_or_size_splits=4, axis=1)\n-      c, o = self._compute_carry_and_output_fused(z, c_tm1)\n-\n-    h = o * self.activation(c)\n-    return h, [h, c]\n-\n-  def get_config(self):\n-    config = {\n-        'units':\n-            self.units,\n-        'activation':\n-            activations.serialize(self.activation),\n-        'recurrent_activation':\n-            activations.serialize(self.recurrent_activation),\n-        'use_bias':\n-            self.use_bias,\n-        'kernel_initializer':\n-            initializers.serialize(self.kernel_initializer),\n-        'recurrent_initializer':\n-            initializers.serialize(self.recurrent_initializer),\n-        'bias_initializer':\n-            initializers.serialize(self.bias_initializer),\n-        'unit_forget_bias':\n-            self.unit_forget_bias,\n-        'kernel_regularizer':\n-            regularizers.serialize(self.kernel_regularizer),\n-        'recurrent_regularizer':\n-            regularizers.serialize(self.recurrent_regularizer),\n-        'bias_regularizer':\n-            regularizers.serialize(self.bias_regularizer),\n-        'kernel_constraint':\n-            constraints.serialize(self.kernel_constraint),\n-        'recurrent_constraint':\n-            constraints.serialize(self.recurrent_constraint),\n-        'bias_constraint':\n-            constraints.serialize(self.bias_constraint),\n-        'dropout':\n-            self.dropout,\n-        'recurrent_dropout':\n-            self.recurrent_dropout,\n-        'implementation':\n-            self.implementation\n-    }\n-    config.update(rnn_utils.config_for_enable_caching_device(self))\n-    base_config = super(LSTMCell, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n-    return list(rnn_utils.generate_zero_filled_state_for_cell(\n-        self, inputs, batch_size, dtype))\n+    super(LSTMCell, self).__init__(\n+        units,\n+        activation=activation,\n+        recurrent_activation=recurrent_activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        bias_initializer=bias_initializer,\n+        unit_forget_bias=unit_forget_bias,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        implementation=kwargs.pop('implementation', 1),\n+        **kwargs)\n \n \n @keras_export(v1=['keras.layers.LSTM'])\n\n@@ -13,437 +13,308 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Tests for LSTM V1 layer.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n \n-import copy\n+import time\n \n from absl.testing import parameterized\n import keras\n+from keras.layers.rnn import lstm\n+from keras.layers.rnn import lstm_v1\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n+from keras.utils import np_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n+from tensorflow.core.protobuf import rewriter_config_pb2\n+from tensorflow.python.platform import tf_logging as logging\n \n-@test_combinations.run_all_keras_modes\n-class LSTMLayerTest(test_combinations.TestCase):\n \n-  def test_return_sequences_LSTM(self):\n-    num_samples = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 2\n-    test_utils.layer_test(\n-        keras.layers.LSTM,\n-        kwargs={'units': units,\n-                'return_sequences': True},\n-        input_shape=(num_samples, timesteps, embedding_dim))\n+# Global config for grappler setting that is used for graph mode test.\n+_rewrites = rewriter_config_pb2.RewriterConfig()\n+_rewrites.implementation_selector = rewriter_config_pb2.RewriterConfig.ON\n+_rewrites.min_graph_nodes = -1\n+_graph_options = tf.compat.v1.GraphOptions(rewrite_options=_rewrites)\n+_config = tf.compat.v1.ConfigProto(graph_options=_graph_options)\n+\n+\n+@test_combinations.run_all_keras_modes(config=_config)\n+class LSTMGraphRewriteTest(test_combinations.TestCase):\n \n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n-      skip_message='Double type is yet not supported in ROCm')\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   @test_utils.run_v2_only\n-  def test_float64_LSTM(self):\n-    num_samples = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 2\n-    test_utils.layer_test(\n-        keras.layers.LSTM,\n-        kwargs={'units': units,\n-                'return_sequences': True,\n-                'dtype': 'float64'},\n-        input_shape=(num_samples, timesteps, embedding_dim),\n-        input_dtype='float64')\n+  def test_lstm_feature_parity_v1_v2(self):\n+    input_shape = 10\n+    rnn_state_size = 8\n+    timestep = 4\n+    batch = 20\n \n-  def test_static_shape_inference_LSTM(self):\n-    # Github issue: 15165\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 2\n+    (x_train, y_train), _ = test_utils.get_test_data(\n+        train_samples=batch,\n+        test_samples=0,\n+        input_shape=(timestep, input_shape),\n+        num_classes=rnn_state_size,\n+        random_seed=87654321)\n+    y_train = np_utils.to_categorical(y_train, rnn_state_size)\n+    # For the last batch item of the test data, we filter out the last\n+    # timestep to simulate the variable length sequence and masking test.\n+    x_train[-2:, -1, :] = 0.0\n+    y_train[-2:] = 0\n \n-    model = keras.models.Sequential()\n-    inputs = keras.layers.Dense(embedding_dim,\n-                                input_shape=(timesteps, embedding_dim))\n-    model.add(inputs)\n-    layer = keras.layers.LSTM(units, return_sequences=True)\n-    model.add(layer)\n-    outputs = model.layers[-1].output\n-    self.assertEqual(outputs.shape.as_list(), [None, timesteps, units])\n+    inputs = keras.layers.Input(\n+        shape=[timestep, input_shape], dtype=tf.float32)\n+    masked_input = keras.layers.Masking()(inputs)\n+    lstm_layer = lstm_v1.LSTM(rnn_state_size, recurrent_activation='sigmoid')\n+    output = lstm_layer(masked_input)\n+    lstm_model = keras.models.Model(inputs, output)\n+    weights = lstm_model.get_weights()\n+    y_1 = lstm_model.predict(x_train)\n+    lstm_model.compile('rmsprop', 'mse')\n+    lstm_model.fit(x_train, y_train)\n+    y_2 = lstm_model.predict(x_train)\n \n-  def test_dynamic_behavior_LSTM(self):\n-    num_samples = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 2\n-    layer = keras.layers.LSTM(units, input_shape=(None, embedding_dim))\n-    model = keras.models.Sequential()\n-    model.add(layer)\n-    model.compile(\n-        'rmsprop',\n-        'mse',\n-        run_eagerly=test_utils.should_run_eagerly())\n+    with test_utils.device(should_use_gpu=True):\n+      cudnn_layer = lstm.LSTM(rnn_state_size)\n+      cudnn_model = keras.models.Model(inputs, cudnn_layer(masked_input))\n+    cudnn_model.set_weights(weights)\n+    y_3 = cudnn_model.predict(x_train)\n+    cudnn_model.compile('rmsprop', 'mse')\n+    cudnn_model.fit(x_train, y_train)\n+    y_4 = cudnn_model.predict(x_train)\n \n-    x = np.random.random((num_samples, timesteps, embedding_dim))\n-    y = np.random.random((num_samples, units))\n-    model.train_on_batch(x, y)\n+    self.assertAllClose(y_1, y_3, rtol=1e-5, atol=2e-5)\n+    self.assertAllClose(y_2, y_4, rtol=1e-5, atol=2e-5)\n \n-  def test_dropout_LSTM(self):\n-    num_samples = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 2\n-    test_utils.layer_test(\n-        keras.layers.LSTM,\n-        kwargs={'units': units,\n-                'dropout': 0.1,\n-                'recurrent_dropout': 0.1},\n-        input_shape=(num_samples, timesteps, embedding_dim))\n+  @parameterized.named_parameters(\n+      # test_name, time_major, go_backwards\n+      ('normal', False, False),\n+      ('time_major', True, False),\n+      ('go_backwards', False, True),\n+      ('both', True, True),\n+  )\n+  def test_time_major_and_go_backward_v1_v2(self, time_major, go_backwards):\n+    input_shape = 10\n+    rnn_state_size = 8\n+    timestep = 4\n+    batch = 100\n \n-  def test_recurrent_dropout_with_implementation_restriction(self):\n-    layer = keras.layers.LSTM(2, recurrent_dropout=0.1, implementation=2)\n-    # The implementation is force to 1 due to the limit of recurrent_dropout.\n-    self.assertEqual(layer.implementation, 1)\n+    x_train = np.random.random((batch, timestep, input_shape))\n \n-  @parameterized.parameters([0, 1, 2])\n-  def test_implementation_mode_LSTM(self, implementation_mode):\n-    num_samples = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 2\n-    test_utils.layer_test(\n-        keras.layers.LSTM,\n-        kwargs={'units': units,\n-                'implementation': implementation_mode},\n-        input_shape=(num_samples, timesteps, embedding_dim))\n-\n-  def test_constraints_LSTM(self):\n-    embedding_dim = 4\n-    layer_class = keras.layers.LSTM\n-    k_constraint = keras.constraints.max_norm(0.01)\n-    r_constraint = keras.constraints.max_norm(0.01)\n-    b_constraint = keras.constraints.max_norm(0.01)\n-    layer = layer_class(\n-        5,\n-        return_sequences=False,\n-        weights=None,\n-        input_shape=(None, embedding_dim),\n-        kernel_constraint=k_constraint,\n-        recurrent_constraint=r_constraint,\n-        bias_constraint=b_constraint)\n-    layer.build((None, None, embedding_dim))\n-    self.assertEqual(layer.cell.kernel.constraint, k_constraint)\n-    self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)\n-    self.assertEqual(layer.cell.bias.constraint, b_constraint)\n-\n-  @parameterized.parameters([True, False])\n-  @tf.test.disable_with_predicate(\n-      pred=tf.test.is_built_with_rocm,\n-      skip_message='Skipping as ROCm MIOpen does not support padded input.')\n-  def test_with_masking_layer_LSTM(self, unroll):\n-    layer_class = keras.layers.LSTM\n-    inputs = np.random.random((2, 3, 4))\n-    targets = np.abs(np.random.random((2, 3, 5)))\n-    targets /= targets.sum(axis=-1, keepdims=True)\n-    model = keras.models.Sequential()\n-    model.add(keras.layers.Masking(input_shape=(3, 4)))\n-    model.add(layer_class(units=5, return_sequences=True, unroll=unroll))\n-    model.compile(\n-        loss='categorical_crossentropy',\n-        optimizer='rmsprop',\n-        run_eagerly=test_utils.should_run_eagerly())\n-    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n-\n-  @parameterized.parameters([True, False])\n-  def test_masking_with_stacking_LSTM(self, unroll):\n-    inputs = np.random.random((2, 3, 4))\n-    targets = np.abs(np.random.random((2, 3, 5)))\n-    targets /= targets.sum(axis=-1, keepdims=True)\n-    model = keras.models.Sequential()\n-    model.add(keras.layers.Masking(input_shape=(3, 4)))\n-    lstm_cells = [keras.layers.LSTMCell(10), keras.layers.LSTMCell(5)]\n-    model.add(keras.layers.RNN(\n-        lstm_cells, return_sequences=True, unroll=unroll))\n-    model.compile(\n-        loss='categorical_crossentropy',\n-        optimizer='rmsprop',\n-        run_eagerly=test_utils.should_run_eagerly())\n-    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n-\n-  def test_from_config_LSTM(self):\n-    layer_class = keras.layers.LSTM\n-    for stateful in (False, True):\n-      l1 = layer_class(units=1, stateful=stateful)\n-      l2 = layer_class.from_config(l1.get_config())\n-      assert l1.get_config() == l2.get_config()\n-\n-  def test_deep_copy_LSTM(self):\n-    cell = keras.layers.LSTMCell(5)\n-    copied_cell = copy.deepcopy(cell)\n-    self.assertEqual(copied_cell.units, 5)\n-    self.assertEqual(cell.get_config(), copied_cell.get_config())\n-\n-  def test_specify_initial_state_keras_tensor(self):\n-    num_states = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 3\n-    num_samples = 2\n-\n-    # Test with Keras tensor\n-    inputs = keras.Input((timesteps, embedding_dim))\n-    initial_state = [keras.Input((units,)) for _ in range(num_states)]\n-    layer = keras.layers.LSTM(units)\n-    if len(initial_state) == 1:\n-      output = layer(inputs, initial_state=initial_state[0])\n+    def build_model(layer_cls):\n+      inputs = keras.layers.Input(\n+          shape=[timestep, input_shape], dtype=tf.float32)\n+      layer = layer_cls(rnn_state_size,\n+                        recurrent_activation='sigmoid',\n+                        time_major=time_major,\n+                        return_sequences=True,\n+                        go_backwards=go_backwards)\n+      if time_major:\n+        converted_input = keras.layers.Lambda(\n+            lambda t: tf.transpose(t, [1, 0, 2]))(inputs)\n+        outputs = layer(converted_input)\n+        outputs = keras.layers.Lambda(\n+            lambda t: tf.transpose(t, [1, 0, 2]))(outputs)\n       else:\n-      output = layer(inputs, initial_state=initial_state)\n-    self.assertTrue(\n-        any(initial_state[0] is t\n-            for t in layer._inbound_nodes[0].input_tensors))\n-\n-    model = keras.models.Model([inputs] + initial_state, output)\n-    model.compile(\n-        loss='categorical_crossentropy',\n-        optimizer=tf.compat.v1.train.AdamOptimizer(),\n-        run_eagerly=test_utils.should_run_eagerly())\n-\n-    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n-    initial_state = [np.random.random((num_samples, units))\n-                     for _ in range(num_states)]\n-    targets = np.random.random((num_samples, units))\n-    model.train_on_batch([inputs] + initial_state, targets)\n-\n-  def test_specify_initial_state_non_keras_tensor(self):\n-    num_states = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 3\n-    num_samples = 2\n-\n-    # Test with non-Keras tensor\n-    inputs = keras.Input((timesteps, embedding_dim))\n-    initial_state = [keras.backend.random_normal_variable(\n-        (num_samples, units), 0, 1)\n-                     for _ in range(num_states)]\n-    layer = keras.layers.LSTM(units)\n-    output = layer(inputs, initial_state=initial_state)\n-\n-    model = keras.models.Model(inputs, output)\n-    model.compile(\n-        loss='categorical_crossentropy',\n-        optimizer=tf.compat.v1.train.AdamOptimizer(),\n-        run_eagerly=test_utils.should_run_eagerly())\n-\n-    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n-    targets = np.random.random((num_samples, units))\n-    model.train_on_batch(inputs, targets)\n-\n-  def test_reset_states_with_values(self):\n-    num_states = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 3\n-    num_samples = 2\n-\n-    layer = keras.layers.LSTM(units, stateful=True)\n-    layer.build((num_samples, timesteps, embedding_dim))\n-    layer.reset_states()\n-    assert len(layer.states) == num_states\n-    assert layer.states[0] is not None\n-    self.assertAllClose(\n-        keras.backend.eval(layer.states[0]),\n-        np.zeros(keras.backend.int_shape(layer.states[0])),\n-        atol=1e-4)\n-    state_shapes = [keras.backend.int_shape(state) for state in layer.states]\n-    values = [np.ones(shape) for shape in state_shapes]\n-    if len(values) == 1:\n-      values = values[0]\n-    layer.reset_states(values)\n-    self.assertAllClose(\n-        keras.backend.eval(layer.states[0]),\n-        np.ones(keras.backend.int_shape(layer.states[0])),\n-        atol=1e-4)\n-\n-    # Test with invalid data\n-    with self.assertRaises(ValueError):\n-      layer.reset_states([1] * (len(layer.states) + 1))\n-\n-  def test_specify_state_with_masking(self):\n-    num_states = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 3\n-    num_samples = 2\n-\n-    inputs = keras.Input((timesteps, embedding_dim))\n-    _ = keras.layers.Masking()(inputs)\n-    initial_state = [keras.Input((units,)) for _ in range(num_states)]\n-    output = keras.layers.LSTM(units)(inputs, initial_state=initial_state)\n-\n-    model = keras.models.Model([inputs] + initial_state, output)\n-    model.compile(\n-        loss='categorical_crossentropy',\n-        optimizer='rmsprop',\n-        run_eagerly=test_utils.should_run_eagerly())\n-\n-    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n-    initial_state = [np.random.random((num_samples, units))\n-                     for _ in range(num_states)]\n-    targets = np.random.random((num_samples, units))\n-    model.train_on_batch([inputs] + initial_state, targets)\n-\n-  def test_return_state(self):\n-    num_states = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 3\n-    num_samples = 2\n-\n-    inputs = keras.Input(batch_shape=(num_samples, timesteps, embedding_dim))\n-    layer = keras.layers.LSTM(units, return_state=True, stateful=True)\n         outputs = layer(inputs)\n-    state = outputs[1:]\n-    assert len(state) == num_states\n-    model = keras.models.Model(inputs, state[0])\n+      return keras.models.Model(inputs, outputs)\n \n-    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n-    state = model.predict(inputs)\n-    self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)\n+    lstm_model = build_model(lstm_v1.LSTM)\n+    y_ref = lstm_model.predict(x_train)\n+    weights = lstm_model.get_weights()\n \n-  def test_state_reuse(self):\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 3\n-    num_samples = 2\n+    lstm_v2_model = build_model(lstm.LSTM)\n+    lstm_v2_model.set_weights(weights)\n+    y = lstm_v2_model.predict(x_train)\n+\n+    self.assertAllClose(y, y_ref)\n+\n+    input_shape = 10\n+    rnn_state_size = 8\n+    output_shape = 8\n+    timestep = 4\n+    batch = 100\n+    epoch = 10\n+\n+    (x_train, y_train), _ = test_utils.get_test_data(\n+        train_samples=batch,\n+        test_samples=0,\n+        input_shape=(timestep, input_shape),\n+        num_classes=output_shape)\n+    y_train = np_utils.to_categorical(y_train, output_shape)\n+\n+    layer = lstm.LSTM(rnn_state_size)\n+\n+    inputs = keras.layers.Input(\n+        shape=[timestep, input_shape], dtype=tf.float32)\n \n-    inputs = keras.Input(batch_shape=(num_samples, timesteps, embedding_dim))\n-    layer = keras.layers.LSTM(units, return_state=True, return_sequences=True)\n     outputs = layer(inputs)\n-    output, state = outputs[0], outputs[1:]\n-    output = keras.layers.LSTM(units)(output, initial_state=state)\n-    model = keras.models.Model(inputs, output)\n-\n-    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n-    outputs = model.predict(inputs)\n-\n-  def test_initial_states_as_other_inputs(self):\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 3\n-    num_samples = 2\n-    num_states = 2\n-    layer_class = keras.layers.LSTM\n-\n-    # Test with Keras tensor\n-    main_inputs = keras.Input((timesteps, embedding_dim))\n-    initial_state = [keras.Input((units,)) for _ in range(num_states)]\n-    inputs = [main_inputs] + initial_state\n-\n-    layer = layer_class(units)\n-    output = layer(inputs)\n-    self.assertTrue(\n-        any(initial_state[0] is t\n-            for t in layer._inbound_nodes[0].input_tensors))\n-\n-    model = keras.models.Model(inputs, output)\n-    model.compile(\n-        loss='categorical_crossentropy',\n-        optimizer=tf.compat.v1.train.AdamOptimizer(),\n-        run_eagerly=test_utils.should_run_eagerly())\n-\n-    main_inputs = np.random.random((num_samples, timesteps, embedding_dim))\n-    initial_state = [np.random.random((num_samples, units))\n-                     for _ in range(num_states)]\n-    targets = np.random.random((num_samples, units))\n-    model.train_on_batch([main_inputs] + initial_state, targets)\n-\n-  def test_regularizers_LSTM(self):\n-    embedding_dim = 4\n-    layer_class = keras.layers.LSTM\n-    layer = layer_class(\n-        5,\n-        return_sequences=False,\n-        weights=None,\n-        input_shape=(None, embedding_dim),\n-        kernel_regularizer=keras.regularizers.l1(0.01),\n-        recurrent_regularizer=keras.regularizers.l1(0.01),\n-        bias_regularizer='l2',\n-        activity_regularizer='l1')\n-    layer.build((None, None, 2))\n-    self.assertEqual(len(layer.losses), 3)\n-    x = keras.backend.variable(np.ones((2, 3, 2)))\n-    layer(x)\n-    if tf.executing_eagerly():\n-      self.assertEqual(len(layer.losses), 4)\n-    else:\n-      self.assertEqual(len(layer.get_losses_for(x)), 1)\n+    model = keras.models.Model(inputs, outputs)\n+    model.compile('rmsprop', loss='mse')\n+    model.fit(x_train, y_train, epochs=epoch)\n+    model.evaluate(x_train, y_train)\n+    model.predict(x_train)\n \n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n-      skip_message='Skipping as ROCm MIOpen does not support padded input.')\n-  def test_statefulness_LSTM(self):\n-    num_samples = 2\n-    timesteps = 3\n-    embedding_dim = 4\n-    units = 2\n-    layer_class = keras.layers.LSTM\n-    model = keras.models.Sequential()\n-    model.add(\n-        keras.layers.Embedding(\n-            4,\n-            embedding_dim,\n-            mask_zero=True,\n-            input_length=timesteps,\n-            batch_input_shape=(num_samples, timesteps)))\n-    layer = layer_class(\n-        units, return_sequences=False, stateful=True, weights=None)\n-    model.add(layer)\n-    model.compile(\n-        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n-        loss='mse',\n-        run_eagerly=test_utils.should_run_eagerly())\n-    out1 = model.predict(np.ones((num_samples, timesteps)))\n-    self.assertEqual(out1.shape, (num_samples, units))\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n+  @test_utils.run_v2_only\n+  def test_explicit_device_with_go_backward_and_mask_v1(self):\n+    batch_size = 8\n+    timestep = 7\n+    masksteps = 5\n+    units = 4\n \n-    # train once so that the states change\n-    model.train_on_batch(\n-        np.ones((num_samples, timesteps)), np.ones((num_samples, units)))\n-    out2 = model.predict(np.ones((num_samples, timesteps)))\n+    inputs = np.random.randn(batch_size, timestep, units).astype(np.float32)\n+    mask = np.ones((batch_size, timestep)).astype(np.bool)\n+    mask[:, masksteps:] = 0\n \n-    # if the state is not reset, output should be different\n-    self.assertNotEqual(out1.max(), out2.max())\n+    lstm_v1_layer = lstm_v1.LSTM(\n+        units, return_sequences=True, go_backwards=True)\n+    with test_utils.device(should_use_gpu=True):\n+      outputs_masked_v1 = lstm_v1_layer(inputs, mask=tf.constant(mask))\n+      outputs_trimmed_v1 = lstm_v1_layer(inputs[:, :masksteps])\n+    self.assertAllClose(outputs_masked_v1[:, -masksteps:], outputs_trimmed_v1)\n \n-    # check that output changes after states are reset\n-    # (even though the model itself didn't change)\n-    layer.reset_states()\n-    out3 = model.predict(np.ones((num_samples, timesteps)))\n-    self.assertNotEqual(out2.max(), out3.max())\n \n-    # check that container-level reset_states() works\n-    model.reset_states()\n-    out4 = model.predict(np.ones((num_samples, timesteps)))\n-    self.assertAllClose(out3, out4, atol=1e-5)\n+class LSTMPerformanceTest(tf.test.Benchmark):\n \n-    # check that the call to `predict` updated the states\n-    out5 = model.predict(np.ones((num_samples, timesteps)))\n-    self.assertNotEqual(out4.max(), out5.max())\n+  def _measure_performance(self, test_config, model, x_train, y_train):\n+    batch = test_config['batch']\n+    epoch = test_config['epoch']\n+    warmup_epoch = test_config['warmup_epoch']\n \n-    # Check masking\n-    layer.reset_states()\n+    # warm up the model\n+    model.fit(x_train, y_train, batch_size=batch, epochs=warmup_epoch)\n+    start_time = time.time()\n+    model.fit(x_train, y_train, batch_size=batch, epochs=epoch - warmup_epoch)\n+    end_time = time.time()\n+    return (end_time - start_time) / (epoch - warmup_epoch)\n \n-    left_padded_input = np.ones((num_samples, timesteps))\n-    left_padded_input[0, :1] = 0\n-    left_padded_input[1, :2] = 0\n-    out6 = model.predict(left_padded_input)\n+  def _time_performance_run_cudnn_lstm(self, test_config, x_train, y_train):\n+    # Get the performance number for standard Cudnn LSTM\n+    input_shape = test_config['input_shape']\n+    rnn_state_size = test_config['rnn_state_size']\n+    timestep = test_config['timestep']\n \n-    layer.reset_states()\n+    cudnn_lstm_layer = keras.layers.CuDNNLSTM(rnn_state_size)\n+    inputs = keras.layers.Input(\n+        shape=[timestep, input_shape], dtype=tf.float32)\n \n-    right_padded_input = np.ones((num_samples, timesteps))\n-    right_padded_input[0, -1:] = 0\n-    right_padded_input[1, -2:] = 0\n-    out7 = model.predict(right_padded_input)\n+    outputs = cudnn_lstm_layer(inputs)\n+    model = keras.models.Model(inputs, outputs)\n+    model.compile('sgd', 'mse')\n \n-    self.assertAllClose(out7, out6, atol=1e-5)\n+    sec_per_epoch = self._measure_performance(\n+        test_config, model, x_train, y_train)\n+    logging.info('Average performance for %s per epoch is: %s',\n+                 'CuDNN LSTM', sec_per_epoch)\n+    return sec_per_epoch\n+\n+  def _time_performance_run_unifed_lstm_gpu(\n+      self, test_config, x_train, y_train):\n+    # Get performance number for lstm_v2 with grappler swap the impl\n+    input_shape = test_config['input_shape']\n+    rnn_state_size = test_config['rnn_state_size']\n+    timestep = test_config['timestep']\n+\n+    layer = keras.layers.LSTM(rnn_state_size)\n+    inputs = keras.layers.Input(\n+        shape=[timestep, input_shape], dtype=tf.float32)\n+\n+    outputs = layer(inputs)\n+    model = keras.models.Model(inputs, outputs)\n+    model.compile('sgd', 'mse')\n+\n+    sec_per_epoch = self._measure_performance(\n+        test_config, model, x_train, y_train)\n+    logging.info('Average performance for %s per epoch is: %s',\n+                 'LSTM V2', sec_per_epoch)\n+    return sec_per_epoch\n+\n+  def _time_performance_run_normal_lstm(\n+      self, test_config, x_train, y_train):\n+    # Get performance number for standard LSTM on GPU.\n+    input_shape = test_config['input_shape']\n+    rnn_state_size = test_config['rnn_state_size']\n+    timestep = test_config['timestep']\n+\n+    layer = lstm_v1.LSTM(rnn_state_size)\n+    inputs = keras.layers.Input(\n+        shape=[timestep, input_shape], dtype=tf.float32)\n+\n+    outputs = layer(inputs)\n+    model = keras.models.Model(inputs, outputs)\n+    model.compile('sgd', 'mse')\n+\n+    sec_per_epoch = self._measure_performance(\n+        test_config, model, x_train, y_train)\n+    logging.info('Average performance for %s per epoch is: %s',\n+                 'Normal LSTM', sec_per_epoch)\n+    return sec_per_epoch\n+\n+  def _benchmark_performance_with_standard_cudnn_impl(self):\n+    if not tf.test.is_gpu_available():\n+      self.skipTest('performance test will only run on GPU')\n+\n+    mode = 'eager' if tf.executing_eagerly() else 'graph'\n+    batch = 64\n+    num_batch = 10\n+    test_config = {\n+        'input_shape': 128,\n+        'rnn_state_size': 64,\n+        'output_shape': 64,\n+        'timestep': 50,\n+        'batch': batch,\n+        'epoch': 20,\n+        # The performance for warmup epoch is ignored.\n+        'warmup_epoch': 1,\n+    }\n+    (x_train, y_train), _ = test_utils.get_test_data(\n+        train_samples=(batch * num_batch),\n+        test_samples=0,\n+        input_shape=(test_config['timestep'], test_config['input_shape']),\n+        num_classes=test_config['output_shape'])\n+    y_train = np_utils.to_categorical(y_train, test_config['output_shape'])\n+\n+    cudnn_sec_per_epoch = self._time_performance_run_cudnn_lstm(\n+        test_config, x_train, y_train)\n+    lstm_v2_sec_per_epoch = self._time_performance_run_unifed_lstm_gpu(\n+        test_config, x_train, y_train)\n+    normal_lstm_sec_per_epoch = self._time_performance_run_normal_lstm(\n+        test_config, x_train, y_train)\n+\n+    cudnn_vs_v2 = cudnn_sec_per_epoch / lstm_v2_sec_per_epoch\n+    v2_vs_normal = normal_lstm_sec_per_epoch / lstm_v2_sec_per_epoch\n+\n+    self.report_benchmark(name='keras_cudnn_lstm_' + mode,\n+                          wall_time=cudnn_sec_per_epoch,\n+                          iters=test_config['epoch'],\n+                          extras=test_config)\n+    self.report_benchmark(name='keras_lstm_v2_' + mode,\n+                          wall_time=lstm_v2_sec_per_epoch,\n+                          iters=test_config['epoch'],\n+                          extras=test_config)\n+    self.report_benchmark(name='keras_canonical_lstm_' + mode,\n+                          wall_time=normal_lstm_sec_per_epoch,\n+                          iters=test_config['epoch'],\n+                          extras=test_config)\n+\n+    logging.info('Expect the performance of LSTM V2 is within 80% of '\n+                 'cuDNN LSTM, got {0:.2f}%'.format(cudnn_vs_v2 * 100))\n+    logging.info('Expect the performance of LSTM V2 is more than 5 times'\n+                 ' of normal LSTM, got {0:.2f}'.format(v2_vs_normal))\n+\n+  def benchmark_performance_graph(self):\n+    with tf.compat.v1.get_default_graph().as_default():\n+      with tf.compat.v1.Session(config=_config):\n+        self._benchmark_performance_with_standard_cudnn_impl()\n+\n+  def benchmark_performance_eager(self):\n+    with tf.__internal__.eager_context.eager_mode():\n+      self._benchmark_performance_with_standard_cudnn_impl()\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
