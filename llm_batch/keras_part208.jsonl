{"custom_id": "keras#ff9118f68a4a9083d3a16081f5643ac098e1be44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 12301 | Contributors (this commit): 73 | Commits (past 90d): 8 | Contributors (cumulative): 73 | DMM Complexity: None\n\nDIFF:\n@@ -61,7 +61,7 @@ class Masking(Layer):\n     def call(self, inputs):\n         boolean_mask = K.any(K.not_equal(inputs, self.mask_value),\n                              axis=-1, keepdims=True)\n-        return inputs * K.cast(boolean_mask, inputs.dtype)\n+        return inputs * K.cast(boolean_mask, K.dtype(inputs))\n \n     def get_config(self):\n         config = {'mask_value': self.mask_value}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c03482506bf7a82906edcd9f91e3abc8ef88bbed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 95 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 100 | Churn Cumulative: 2913 | Contributors (this commit): 41 | Commits (past 90d): 17 | Contributors (cumulative): 47 | DMM Complexity: 0.8297872340425532\n\nDIFF:\n@@ -25,6 +25,22 @@ except ImportError:\n     pil_image = None\n \n \n+if pil_image is not None:\n+    _PIL_INTERPOLATION_METHODS = {\n+        'nearest': pil_image.NEAREST,\n+        'bilinear': pil_image.BILINEAR,\n+        'bicubic': pil_image.BICUBIC,\n+    }\n+    # These methods were only introduced in version 3.4.0 (2016).\n+    if hasattr(pil_image, 'HAMMING'):\n+        _PIL_INTERPOLATION_METHODS['hamming'] = pil_image.HAMMING\n+    if hasattr(pil_image, 'BOX'):\n+        _PIL_INTERPOLATION_METHODS['box'] = pil_image.BOX\n+    # This method is new in version 1.1.3 (2013).\n+    if hasattr(pil_image, 'LANCZOS'):\n+        _PIL_INTERPOLATION_METHODS['lanczos'] = pil_image.LANCZOS\n+\n+\n def random_rotation(x, rg, row_axis=1, col_axis=2, channel_axis=0,\n                     fill_mode='nearest', cval=0.):\n     \"\"\"Performs a random rotation of a Numpy image tensor.\n@@ -302,7 +318,8 @@ def img_to_array(img, data_format=None):\n     return x\n \n \n-def load_img(path, grayscale=False, target_size=None):\n+def load_img(path, grayscale=False, target_size=None,\n+             interpolation='bilinear'):\n     \"\"\"Loads an image into PIL format.\n \n     # Arguments\n@@ -310,12 +327,19 @@ def load_img(path, grayscale=False, target_size=None):\n         grayscale: Boolean, whether to load the image as grayscale.\n         target_size: Either `None` (default to original size)\n             or tuple of ints `(img_height, img_width)`.\n+        interpolation: Interpolation method used to resample the image if the\n+            target size is different from that of the loaded image.\n+            Supported methods are \"nearest\", \"bilinear\", and \"bicubic\".\n+            If PIL version 1.1.3 or newer is installed, \"lanczos\" is also\n+            supported. If PIL version 3.4.0 or newer is installed, \"box\" and\n+            \"hamming\" are also supported. By default, \"bilinear\" is used.\n \n     # Returns\n         A PIL Image instance.\n \n     # Raises\n         ImportError: if PIL is not available.\n+        ValueError: if interpolation method is not supported.\n     \"\"\"\n     if pil_image is None:\n         raise ImportError('Could not import PIL.Image. '\n@@ -327,10 +351,17 @@ def load_img(path, grayscale=False, target_size=None):\n     else:\n         if img.mode != 'RGB':\n             img = img.convert('RGB')\n-    if target_size:\n-        hw_tuple = (target_size[1], target_size[0])\n-        if img.size != hw_tuple:\n-            img = img.resize(hw_tuple)\n+    if target_size is not None:\n+        width_height_tuple = (target_size[1], target_size[0])\n+        if img.size != width_height_tuple:\n+            if interpolation not in _PIL_INTERPOLATION_METHODS:\n+                raise ValueError(\n+                    'Invalid interpolation method {} specified. Supported '\n+                    'methods are {}'.format(\n+                        interpolation,\n+                        \", \".join(_PIL_INTERPOLATION_METHODS.keys())))\n+            resample = _PIL_INTERPOLATION_METHODS[interpolation]\n+            img = img.resize(width_height_tuple, resample)\n     return img\n \n \n\n@@ -307,6 +307,65 @@ class TestImage(object):\n                 transformed[i] = generator.random_transform(im)\n             transformed = generator.standardize(transformed)\n \n+    def test_load_img(self, tmpdir):\n+        filename = str(tmpdir / 'image.png')\n+\n+        original_im_array = np.array(255 * np.random.rand(100, 100, 3),\n+                                     dtype=np.uint8)\n+        original_im = image.array_to_img(original_im_array, scale=False)\n+        original_im.save(filename)\n+\n+        # Test that loaded image is exactly equal to original.\n+\n+        loaded_im = image.load_img(filename)\n+        loaded_im_array = image.img_to_array(loaded_im)\n+        assert loaded_im_array.shape == original_im_array.shape\n+        assert np.all(loaded_im_array == original_im_array)\n+\n+        loaded_im = image.load_img(filename, grayscale=True)\n+        loaded_im_array = image.img_to_array(loaded_im)\n+        assert loaded_im_array.shape == (original_im_array.shape[0],\n+                                         original_im_array.shape[1], 1)\n+\n+        # Test that nothing is changed when target size is equal to original.\n+\n+        loaded_im = image.load_img(filename, target_size=(100, 100))\n+        loaded_im_array = image.img_to_array(loaded_im)\n+        assert loaded_im_array.shape == original_im_array.shape\n+        assert np.all(loaded_im_array == original_im_array)\n+\n+        loaded_im = image.load_img(filename, grayscale=True,\n+                                   target_size=(100, 100))\n+        loaded_im_array = image.img_to_array(loaded_im)\n+        assert loaded_im_array.shape == (original_im_array.shape[0],\n+                                         original_im_array.shape[1], 1)\n+\n+        # Test down-sampling with bilinear interpolation.\n+\n+        loaded_im = image.load_img(filename, target_size=(25, 25))\n+        loaded_im_array = image.img_to_array(loaded_im)\n+        assert loaded_im_array.shape == (25, 25, 3)\n+\n+        loaded_im = image.load_img(filename, grayscale=True,\n+                                   target_size=(25, 25))\n+        loaded_im_array = image.img_to_array(loaded_im)\n+        assert loaded_im_array.shape == (25, 25, 1)\n+\n+        # Test down-sampling with nearest neighbor interpolation.\n+\n+        loaded_im_nearest = image.load_img(filename, target_size=(25, 25),\n+                                           interpolation=\"nearest\")\n+        loaded_im_array_nearest = image.img_to_array(loaded_im_nearest)\n+        assert loaded_im_array_nearest.shape == (25, 25, 3)\n+        assert np.any(loaded_im_array_nearest != loaded_im_array)\n+\n+        # Check that exception is raised if interpolation not supported.\n+\n+        loaded_im = image.load_img(filename, interpolation=\"unsupported\")\n+        with pytest.raises(ValueError):\n+            loaded_im = image.load_img(filename, target_size=(25, 25),\n+                                       interpolation=\"unsupported\")\n+\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f266c3345ec65c721d4599a685fde6611fb8609", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 789 | Contributors (this commit): 21 | Commits (past 90d): 2 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -292,7 +292,8 @@ class Progbar(object):\n \n         now = time.time()\n         if self.verbose == 1:\n-            if not force and (now - self.last_update) < self.interval:\n+            if (not force and (now - self.last_update) < self.interval and\n+                    current < self.target):\n                 return\n \n             prev_total_width = self.total_width\n@@ -322,7 +323,7 @@ class Progbar(object):\n                 time_per_unit = 0\n             eta = time_per_unit * (self.target - current)\n             info = ''\n-            if current < self.target and self.target is not -1:\n+            if current <= self.target and self.target is not -1:\n                 info += ' - ETA: %ds' % eta\n             else:\n                 info += ' - %ds' % (now - self.start)\n@@ -347,7 +348,7 @@ class Progbar(object):\n             if current >= self.target:\n                 sys.stdout.write('\\n')\n \n-        if self.verbose == 2:\n+        elif self.verbose == 2:\n             if current >= self.target:\n                 info = '%ds' % (now - self.start)\n                 for k in self.unique_values:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#797ea236df90136f7f4060d66bfdeff9dfdb8f51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 791 | Contributors (this commit): 21 | Commits (past 90d): 3 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -345,7 +345,7 @@ class Progbar(object):\n             sys.stdout.write(info)\n             sys.stdout.flush()\n \n-            if current >= self.target:\n+            if current >= self.target and self.target is not -1:\n                 sys.stdout.write('\\n')\n \n         elif self.verbose == 2:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2ba3b00fc7f5842ea0b125efc6422a3bed1b90f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a32f648ed1371cf5e82b5cbdd4d252355cc729e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 798 | Contributors (this commit): 21 | Commits (past 90d): 4 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -302,8 +302,8 @@ class Progbar(object):\n \n             if self.target is not -1:\n                 numdigits = int(np.floor(np.log10(self.target))) + 1\n-                barstr = '%%%dd/%%%dd [' % (numdigits, numdigits)\n-                bar = barstr % (current, self.target)\n+                barstr = '%%%dd/%d [' % (numdigits, self.target)\n+                bar = barstr % current\n                 prog = float(current) / self.target\n                 prog_width = int(self.width * prog)\n                 if prog_width > 0:\n@@ -314,6 +314,9 @@ class Progbar(object):\n                         bar += '='\n                 bar += ('.' * (self.width - prog_width))\n                 bar += ']'\n+            else:\n+                bar = '%7d/Unknown' % current\n+\n             sys.stdout.write(bar)\n             self.total_width = len(bar)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#79dab7a6acd0b65a5af1bc1404158e9d31c903bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 157 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 157 | Churn Cumulative: 157 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,157 @@\n+''' Trains a ResNet on the CIFAR10 dataset.\n+    Greater than 91% test accuracy (0.52 val_loss) after 50 epochs\n+    48sec per epoch on GTX 1080Ti\n+\n+    Deep Residual Learning for Image Recognition\n+    https://arxiv.org/pdf/1512.03385.pdf\n+'''\n+\n+from __future__ import print_function\n+import keras\n+from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n+from keras.layers import MaxPooling2D, AveragePooling2D, Input, Flatten\n+from keras.optimizers import Adam\n+from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n+from keras.preprocessing.image import ImageDataGenerator\n+from keras.regularizers import l2\n+from keras import backend as K\n+from keras.models import Model\n+from keras.datasets import cifar10\n+import numpy as np\n+import os\n+\n+\n+batch_size = 32\n+num_classes = 10\n+epochs = 100\n+\n+(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n+\n+# input image dimensions\n+img_rows, img_cols, channels = x_train.shape[1], x_train.shape[2], x_train.shape[3]\n+\n+if K.image_data_format() == 'channels_first':\n+    x_train = x_train.reshape(x_train.shape[0], channels, img_rows, img_cols)\n+    x_test = x_test.reshape(x_test.shape[0], channels, img_rows, img_cols)\n+    input_shape = (channels, img_rows, img_cols)\n+else:\n+    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, channels)\n+    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, channels)\n+    input_shape = (img_rows, img_cols, channels)\n+\n+x_train = x_train.astype('float32') / 255\n+x_test = x_test.astype('float32') / 255\n+print('x_train shape:', x_train.shape)\n+print(x_train.shape[0], 'train samples')\n+print(x_test.shape[0], 'test samples')\n+print('y_train shape:', y_train.shape)\n+\n+# convert class vectors to binary class matrices\n+y_train = keras.utils.to_categorical(y_train, num_classes)\n+y_test = keras.utils.to_categorical(y_test, num_classes)\n+\n+xin = Input(shape=input_shape)\n+\n+filters = 64\n+blocks = 4\n+sub_blocks = 2\n+x = Conv2D(filters=filters, kernel_size=7, padding='same', strides=2,\n+           kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(xin)\n+x = BatchNormalization()(x)\n+x = Activation('relu')(x)\n+\n+# Orig paper uses max pool after 1st conv. Reaches up 87% acc if use_max_pool = True.\n+# Cifar10 images are already too small at 32x32 to be maxpooled. So, we skip.\n+use_max_pool = False\n+if use_max_pool:\n+    x = MaxPooling2D(pool_size=3, strides=2, padding=\"same\")(x)\n+    blocks = 3\n+\n+for i in range(blocks):\n+    for j in range(sub_blocks):\n+        strides = 1\n+        is_first_layer_but_not_first_block = j == 0 and i > 0\n+        if is_first_layer_but_not_first_block:\n+            strides = 2\n+        y = Conv2D(filters=filters, kernel_size=3, padding='same', strides=strides,\n+                   kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(x)\n+        y = BatchNormalization()(y)\n+        y = Activation('relu')(y)\n+        y = Conv2D(filters=filters, kernel_size=3, padding='same',\n+                   kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(y)\n+        y = BatchNormalization()(y)\n+        if is_first_layer_but_not_first_block:\n+            x = Conv2D(filters=filters, kernel_size=1, padding='same', strides=2,\n+                       kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(x)\n+        x = keras.layers.add([x, y])\n+        x = Activation('relu')(x)\n+\n+    filters = 2 * filters\n+\n+x = AveragePooling2D()(x)\n+y = Flatten()(x)\n+yout = Dense(num_classes, activation='softmax', kernel_initializer=\"he_normal\")(y)\n+model = Model(inputs=[xin], outputs=[yout])\n+model.compile(loss='categorical_crossentropy', optimizer=Adam(),\n+              metrics=['accuracy'])\n+model.summary()\n+\n+# Save model and weights\n+save_dir = os.path.join(os.getcwd(), 'saved_models')\n+model_name = \"cifar10_resnet_model.hdf5\"\n+if not os.path.isdir(save_dir):\n+    os.makedirs(save_dir)\n+filepath = os.path.join(save_dir, model_name)\n+checkpoint = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\n+lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n+callbacks = [checkpoint, lr_reducer]\n+\n+data_augmentation = True\n+\n+if not data_augmentation:\n+    print('Not using data augmentation.')\n+    model.fit(x_train, y_train,\n+              batch_size=batch_size,\n+              epochs=epochs,\n+              validation_data=(x_test, y_test),\n+              shuffle=True,\n+              callbacks=callbacks)\n+else:\n+    print('Using real-time data augmentation.')\n+    # This will do preprocessing and realtime data augmentation:\n+    datagen = ImageDataGenerator(\n+        featurewise_center=False,  # set input mean to 0 over the dataset\n+        samplewise_center=False,  # set each sample mean to 0\n+        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n+        samplewise_std_normalization=False,  # divide each input by its std\n+        zca_whitening=False,  # apply ZCA whitening\n+        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n+        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n+        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n+        horizontal_flip=True,  # randomly flip images\n+        vertical_flip=False)  # randomly flip images\n+\n+    # Compute quantities required for featurewise normalization\n+    # (std, mean, and principal components if ZCA whitening is applied).\n+    datagen.fit(x_train)\n+\n+    # Fit the model on the batches generated by datagen.flow().\n+    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n+                        steps_per_epoch=x_train.shape[0] // batch_size,\n+                        validation_data=(x_test, y_test),\n+                        epochs=epochs, verbose=1, workers=4,\n+                        callbacks=callbacks)\n+\n+\n+score = model.evaluate(x_test, y_test, verbose=1)\n+print(\"\")\n+print('Test loss:', score[0])\n+print('Test accuracy:', score[1])\n+\n+score = model.evaluate_generator(datagen.flow(x_test, y_test,\n+                                 batch_size=batch_size,\n+                                 shuffle=False),\n+                                 steps=x_test.shape[0] // batch_size,\n+                                 workers=4)\n+print('Data gen test loss:', score[0])\n+print('Data gen test accuracy:', score[1])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eac0d42ea088d04247ab464edecfebd1c8ca9b7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 238 | Lines Deleted: 90 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/4 | Churn Δ: 328 | Churn Cumulative: 452 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,238 @@\n+'''Example script showing how to use a stateful LSTM model\n+and how its stateless counterpart performs.\n+\n+More documentation about the Keras LSTM model can be found at\n+https://keras.io/layers/recurrent/#lstm\n+\n+The models are trained on an input/output pair, where\n+the input is a generated uniformly distributed random sequence of length = \"input_len\",\n+and the output is a moving average of the input with window length = \"tsteps\".\n+Both \"input_len\" and \"tsteps\" are defined in the \"editable parameters\" section.\n+\n+A larger \"tsteps\" value means that the LSTM will need more memory\n+to figure out the input-output relationship.\n+This memory length is controlled by the \"lahead\" variable (more details below).\n+\n+The rest of the parameters are:\n+- input_len: the length of the generated input sequence\n+- lahead: the input sequence length that the LSTM is trained on for each output point\n+- batch_size, epochs: same parameters as in the model.fit(...) function\n+\n+When lahead > 1, the model input is preprocessed to a \"rolling window view\"\n+of the data, with the window length = \"lahead\".\n+This is similar to sklearn's \"view_as_windows\" with \"window_shape\" being a single number\n+Ref: http://scikit-image.org/docs/0.10.x/api/skimage.util.html#view-as-windows\n+\n+When lahead < tsteps, only the stateful LSTM converges because its\n+statefulness allows it to see beyond the capability that lahead\n+gave it to fit the n-point average. The stateless LSTM does not have\n+this capability, and hence is limited by its \"lahead\" parameter,\n+which is not sufficient to see the n-point average.\n+\n+When lahead >= tsteps, both the stateful and stateless LSTM converge.\n+'''\n+from __future__ import print_function\n+import numpy as np\n+import matplotlib.pyplot as plt\n+import pandas as pd\n+from keras.models import Sequential\n+from keras.layers import Dense, LSTM\n+\n+# ----------------------------------------------------------\n+# EDITABLE PARAMETERS\n+# Read the documentation in the script head for more details\n+# ----------------------------------------------------------\n+\n+# length of input\n+input_len = 1000\n+\n+# The window length of the moving average used to generate\n+# the output from the input in the input/output pair used\n+# to train the LSTM\n+# e.g. if tsteps=2 and input=[1, 2, 3, 4, 5],\n+#      then output=[1.5, 2.5, 3.5, 4.5]\n+tsteps = 2\n+\n+# The input sequence length that the LSTM is trained on for each output point\n+lahead = 1\n+\n+# training parameters passed to \"model.fit(...)\"\n+batch_size = 1\n+epochs = 10\n+\n+# ------------\n+# MAIN PROGRAM\n+# ------------\n+\n+print(\"*\" * 33)\n+if lahead >= tsteps:\n+    print(\"STATELESS LSTM WILL ALSO CONVERGE\")\n+else:\n+    print(\"STATELESS LSTM WILL NOT CONVERGE\")\n+print(\"*\" * 33)\n+\n+np.random.seed(1986)\n+\n+print('Generating Data...')\n+\n+\n+def gen_uniform_amp(amp=1, xn=10000):\n+    \"\"\"Generates uniform random data between\n+    -amp and +amp\n+    and of length xn\n+\n+    Arguments:\n+        amp: maximum/minimum range of uniform data\n+        xn: length of series\n+    \"\"\"\n+    data_input = np.random.uniform(-1 * amp, +1 * amp, xn)\n+    data_input = pd.DataFrame(data_input)\n+    return data_input\n+\n+# Since the output is a moving average of the input,\n+# the first few points of output will be NaN\n+# and will be dropped from the generated data\n+# before training the LSTM.\n+# Also, when lahead > 1,\n+# the preprocessing step later of \"rolling window view\"\n+# will also cause some points to be lost.\n+# For aesthetic reasons,\n+# in order to maintain generated data length = input_len after pre-processing,\n+# add a few points to account for the values that will be lost.\n+to_drop = max(tsteps - 1, lahead - 1)\n+data_input = gen_uniform_amp(amp=0.1, xn=input_len + to_drop)\n+\n+# set the target to be a N-point average of the input\n+expected_output = data_input.rolling(window=tsteps, center=False).mean()\n+\n+# when lahead > 1, need to convert the input to \"rolling window view\"\n+# https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html\n+if lahead > 1:\n+    data_input = np.repeat(data_input.values, repeats=lahead, axis=1)\n+    data_input = pd.DataFrame(data_input)\n+    for i, c in enumerate(data_input.columns):\n+        data_input[c] = data_input[c].shift(i)\n+\n+# drop the nan\n+expected_output = expected_output[to_drop:]\n+data_input = data_input[to_drop:]\n+\n+print('Input shape:', data_input.shape)\n+print('Output shape:', expected_output.shape)\n+print('Input head: ')\n+print(data_input.head())\n+print('Output head: ')\n+print(expected_output.head())\n+print('Input tail: ')\n+print(data_input.tail())\n+print('Output tail: ')\n+print(expected_output.tail())\n+\n+print('Plotting input and expected output')\n+plt.plot(data_input[0][:10], '.')\n+plt.plot(expected_output[0][:10], '-')\n+plt.legend(['Input', 'Expected output'])\n+plt.title('Input')\n+plt.show()\n+\n+\n+def create_model(stateful: bool):\n+    model = Sequential()\n+    model.add(LSTM(20,\n+              input_shape=(lahead, 1),\n+              batch_size=batch_size,\n+              stateful=stateful))\n+    model.add(Dense(1))\n+    model.compile(loss='mse', optimizer='adam')\n+    return model\n+\n+print('Creating Stateful Model...')\n+model_stateful = create_model(stateful=True)\n+\n+\n+# split train/test data\n+def split_data(x, y, ratio: int = 0.8):\n+    to_train = int(input_len * ratio)\n+    # tweak to match with batch_size\n+    to_train -= to_train % batch_size\n+\n+    x_train = x[:to_train]\n+    y_train = y[:to_train]\n+    x_test = x[to_train:]\n+    y_test = y[to_train:]\n+\n+    # tweak to match with batch_size\n+    to_drop = x.shape[0] % batch_size\n+    if to_drop > 0:\n+        x_test = x_test[:-1 * to_drop]\n+        y_test = y_test[:-1 * to_drop]\n+\n+    # some reshaping\n+    reshape_3 = lambda x: x.values.reshape((x.shape[0], x.shape[1], 1))\n+    x_train = reshape_3(x_train)\n+    x_test = reshape_3(x_test)\n+\n+    reshape_2 = lambda x: x.values.reshape((x.shape[0], 1))\n+    y_train = reshape_2(y_train)\n+    y_test = reshape_2(y_test)\n+\n+    return (x_train, y_train), (x_test, y_test)\n+\n+\n+(x_train, y_train), (x_test, y_test) = split_data(data_input, expected_output)\n+print('x_train.shape: ', x_train.shape)\n+print('y_train.shape: ', y_train.shape)\n+print('x_test.shape: ', x_test.shape)\n+print('y_test.shape: ', y_test.shape)\n+\n+print('Training')\n+for i in range(epochs):\n+    print('Epoch', i + 1, '/', epochs)\n+    # Note that the last state for sample i in a batch will\n+    # be used as initial state for sample i in the next batch.\n+    # Thus we are simultaneously training on batch_size series with\n+    # lower resolution than the original series contained in data_input.\n+    # Each of these series are offset by one step and can be\n+    # extracted with data_input[i::batch_size].\n+    model_stateful.fit(x_train,\n+                       y_train,\n+                       batch_size=batch_size,\n+                       epochs=1,\n+                       verbose=1,\n+                       validation_data=(x_test, y_test),\n+                       shuffle=False)\n+    model_stateful.reset_states()\n+\n+print('Predicting')\n+predicted_stateful = model_stateful.predict(x_test, batch_size=batch_size)\n+\n+print('Creating Stateless Model...')\n+model_stateless = create_model(stateful=False)\n+\n+print('Training')\n+model_stateless.fit(x_train,\n+                    y_train,\n+                    batch_size=batch_size,\n+                    epochs=epochs,\n+                    verbose=1,\n+                    validation_data=(x_test, y_test),\n+                    shuffle=False)\n+\n+print('Predicting')\n+predicted_stateless = model_stateless.predict(x_test, batch_size=batch_size)\n+\n+# ----------------------------\n+\n+print('Plotting Results')\n+plt.subplot(3, 1, 1)\n+plt.plot(y_test)\n+plt.title('Expected')\n+plt.subplot(3, 1, 2)\n+# drop the first \"tsteps-1\" because it is not possible to predict them\n+# since the \"previous\" timesteps to use do not exist\n+plt.plot((y_test - predicted_stateful).flatten()[tsteps - 1:])\n+plt.title('Stateful: Expected - Predicted')\n+plt.subplot(3, 1, 3)\n+plt.plot((y_test - predicted_stateless).flatten())\n+plt.title('Stateless: Expected - Predicted')\n+plt.show()\n\n@@ -1,90 +0,0 @@\n-'''Example script showing how to use stateful RNNs\n-to model long sequences efficiently.\n-'''\n-from __future__ import print_function\n-import numpy as np\n-import matplotlib.pyplot as plt\n-from keras.models import Sequential\n-from keras.layers import Dense, LSTM\n-\n-\n-# since we are using stateful rnn tsteps can be set to 1\n-tsteps = 1\n-batch_size = 25\n-epochs = 25\n-# number of elements ahead that are used to make the prediction\n-lahead = 1\n-\n-\n-def gen_cosine_amp(amp=100, period=1000, x0=0, xn=50000, step=1, k=0.0001):\n-    \"\"\"Generates an absolute cosine time series with the amplitude\n-    exponentially decreasing\n-\n-    Arguments:\n-        amp: amplitude of the cosine function\n-        period: period of the cosine function\n-        x0: initial x of the time series\n-        xn: final x of the time series\n-        step: step of the time series discretization\n-        k: exponential rate\n-    \"\"\"\n-    cos = np.zeros(((xn - x0) * step, 1, 1))\n-    for i in range(len(cos)):\n-        idx = x0 + i * step\n-        cos[i, 0, 0] = amp * np.cos(2 * np.pi * idx / period)\n-        cos[i, 0, 0] = cos[i, 0, 0] * np.exp(-k * idx)\n-    return cos\n-\n-\n-print('Generating Data...')\n-cos = gen_cosine_amp()\n-print('Input shape:', cos.shape)\n-\n-expected_output = np.zeros((len(cos), 1))\n-for i in range(len(cos) - lahead):\n-    expected_output[i, 0] = np.mean(cos[i + 1:i + lahead + 1])\n-\n-print('Output shape:', expected_output.shape)\n-\n-print('Creating Model...')\n-model = Sequential()\n-model.add(LSTM(50,\n-               input_shape=(tsteps, 1),\n-               batch_size=batch_size,\n-               return_sequences=True,\n-               stateful=True))\n-model.add(LSTM(50,\n-               return_sequences=False,\n-               stateful=True))\n-model.add(Dense(1))\n-model.compile(loss='mse', optimizer='rmsprop')\n-\n-print('Training')\n-for i in range(epochs):\n-    print('Epoch', i, '/', epochs)\n-\n-    # Note that the last state for sample i in a batch will\n-    # be used as initial state for sample i in the next batch.\n-    # Thus we are simultaneously training on batch_size series with\n-    # lower resolution than the original series contained in cos.\n-    # Each of these series are offset by one step and can be\n-    # extracted with cos[i::batch_size].\n-\n-    model.fit(cos, expected_output,\n-              batch_size=batch_size,\n-              epochs=1,\n-              verbose=1,\n-              shuffle=False)\n-    model.reset_states()\n-\n-print('Predicting')\n-predicted_output = model.predict(cos, batch_size=batch_size)\n-\n-print('Plotting Results')\n-plt.subplot(2, 1, 1)\n-plt.plot(expected_output)\n-plt.title('Expected')\n-plt.subplot(2, 1, 2)\n-plt.plot(predicted_output)\n-plt.title('Predicted')\n-plt.show()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b3370c0da4430367761e6762e85de7ab4962ff62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 877 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 24 | Complexity Δ (Sum/Max): 73/47 | Churn Δ: 877 | Churn Cumulative: 1089 | Contributors (this commit): 11 | Commits (past 90d): 7 | Contributors (cumulative): 15 | DMM Complexity: 0.8642384105960265\n\nDIFF:\n@@ -11,6 +11,7 @@ from .convolutional import *\n from .pooling import *\n from .local import *\n from .recurrent import *\n+from .cudnn_recurrent import *\n from .normalization import *\n from .embeddings import *\n from .noise import *\n\n@@ -0,0 +1,521 @@\n+from .. import backend as K\n+from .. import initializers\n+from .. import regularizers\n+from .. import constraints\n+from .recurrent import RNN\n+from ..layers import InputSpec\n+\n+from collections import namedtuple\n+\n+\n+class _CuDNNRNN(RNN):\n+    \"\"\"Private base class for CuDNNGRU and CuDNNLSTM.\n+\n+    # Arguments\n+        return_sequences: Boolean. Whether to return the last output.\n+            in the output sequence, or the full sequence.\n+        return_state: Boolean. Whether to return the last state\n+            in addition to the output.\n+        stateful: Boolean (default False). If True, the last state\n+            for each sample at index i in a batch will be used as initial\n+            state for the sample of index i in the following batch.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 return_sequences=False,\n+                 return_state=False,\n+                 stateful=False,\n+                 **kwargs):\n+        if K.backend() != 'tensorflow':\n+            raise RuntimeError('CuDNN RNNs are only available '\n+                               'with the TensorFlow backend.')\n+        super(RNN, self).__init__(**kwargs)\n+        self.return_sequences = return_sequences\n+        self.return_state = return_state\n+        self.stateful = stateful\n+        self.supports_masking = False\n+        self.input_spec = [InputSpec(ndim=3)]\n+        if hasattr(self.cell.state_size, '__len__'):\n+            self.state_spec = [InputSpec(shape=(None, dim))\n+                               for dim in self.cell.state_size]\n+        else:\n+            self.state_spec = InputSpec(shape=(None, self.cell.state_size))\n+        self._states = None\n+\n+    def _canonical_to_params(self, weights, biases):\n+        import tensorflow as tf\n+        weights = [tf.reshape(x, (-1,)) for x in weights]\n+        biases = [tf.reshape(x, (-1,)) for x in biases]\n+        return tf.concat(weights + biases, 0)\n+\n+    def call(self, inputs, mask=None, training=None, initial_state=None):\n+        if isinstance(mask, list):\n+            mask = mask[0]\n+        if mask is not None:\n+            raise ValueError('Masking is not supported for CuDNN RNNs.')\n+\n+        # input shape: `(samples, time (padded with zeros), input_dim)`\n+        # note that the .build() method of subclasses MUST define\n+        # self.input_spec and self.state_spec with complete input shapes.\n+        if isinstance(inputs, list):\n+            initial_state = inputs[1:]\n+            inputs = inputs[0]\n+        elif initial_state is not None:\n+            pass\n+        elif self.stateful:\n+            initial_state = self.states\n+        else:\n+            initial_state = self.get_initial_state(inputs)\n+\n+        if len(initial_state) != len(self.states):\n+            raise ValueError('Layer has ' + str(len(self.states)) +\n+                             ' states but was passed ' +\n+                             str(len(initial_state)) +\n+                             ' initial states.')\n+\n+        output, states = self._process_batch(inputs, initial_state)\n+\n+        if self.stateful:\n+            updates = []\n+            for i in range(len(states)):\n+                updates.append((self.states[i], states[i]))\n+            self.add_update(updates, inputs)\n+\n+        if self.return_state:\n+            return [output] + states\n+        else:\n+            return output\n+\n+    def get_config(self):\n+        config = {'return_sequences': self.return_sequences,\n+                  'return_state': self.return_state,\n+                  'stateful': self.stateful}\n+        base_config = super(RNN, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        return cls(**config)\n+\n+    @property\n+    def trainable_weights(self):\n+        if self.trainable and self.built:\n+            return [self.kernel, self.recurrent_kernel, self.bias]\n+        return []\n+\n+    @property\n+    def non_trainable_weights(self):\n+        if not self.trainable and self.built:\n+            return [self.kernel, self.recurrent_kernel, self.bias]\n+        return []\n+\n+    @property\n+    def losses(self):\n+        return super(RNN, self).losses\n+\n+    def get_losses_for(self, inputs=None):\n+        return super(RNN, self).get_losses_for(inputs=inputs)\n+\n+\n+class CuDNNGRU(_CuDNNRNN):\n+    \"\"\"Fast GRU implementation backed by CuDNN.\n+\n+    Can only be run on GPU.\n+\n+    # Arguments\n+        units: Positive integer, dimensionality of the output space.\n+        kernel_initializer: Initializer for the `kernel` weights matrix,\n+            used for the linear transformation of the inputs.\n+            (see [initializers](../initializers.md)).\n+        recurrent_initializer: Initializer for the `recurrent_kernel`\n+            weights matrix,\n+            used for the linear transformation of the recurrent state.\n+            (see [initializers](../initializers.md)).\n+        bias_initializer: Initializer for the bias vector\n+            (see [initializers](../initializers.md)).\n+        kernel_regularizer: Regularizer function applied to\n+            the `kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        recurrent_regularizer: Regularizer function applied to\n+            the `recurrent_kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        bias_regularizer: Regularizer function applied to the bias vector\n+            (see [regularizer](../regularizers.md)).\n+        activity_regularizer: Regularizer function applied to\n+            the output of the layer (its \"activation\").\n+            (see [regularizer](../regularizers.md)).\n+        kernel_constraint: Constraint function applied to\n+            the `kernel` weights matrix\n+            (see [constraints](../constraints.md)).\n+        recurrent_constraint: Constraint function applied to\n+            the `recurrent_kernel` weights matrix\n+            (see [constraints](../constraints.md)).\n+        bias_constraint: Constraint function applied to the bias vector\n+            (see [constraints](../constraints.md)).\n+        return_sequences: Boolean. Whether to return the last output.\n+            in the output sequence, or the full sequence.\n+        return_state: Boolean. Whether to return the last state\n+            in addition to the output.\n+        stateful: Boolean (default False). If True, the last state\n+            for each sample at index i in a batch will be used as initial\n+            state for the sample of index i in the following batch.\n+    \"\"\"\n+\n+    def __init__(self, units,\n+                 kernel_initializer='glorot_uniform',\n+                 recurrent_initializer='orthogonal',\n+                 bias_initializer='zeros',\n+                 kernel_regularizer=None,\n+                 recurrent_regularizer=None,\n+                 bias_regularizer=None,\n+                 activity_regularizer=None,\n+                 kernel_constraint=None,\n+                 recurrent_constraint=None,\n+                 bias_constraint=None,\n+                 return_sequences=False,\n+                 return_state=False,\n+                 stateful=False,\n+                 **kwargs):\n+        self.units = units\n+        super(CuDNNGRU, self).__init__(\n+            return_sequences=return_sequences,\n+            return_state=return_state,\n+            stateful=stateful,\n+            **kwargs)\n+\n+        self.kernel_initializer = initializers.get(kernel_initializer)\n+        self.recurrent_initializer = initializers.get(recurrent_initializer)\n+        self.bias_initializer = initializers.get(bias_initializer)\n+\n+        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n+        self.bias_regularizer = regularizers.get(bias_regularizer)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+\n+        self.kernel_constraint = constraints.get(kernel_constraint)\n+        self.recurrent_constraint = constraints.get(recurrent_constraint)\n+        self.bias_constraint = constraints.get(bias_constraint)\n+\n+    @property\n+    def cell(self):\n+        Cell = namedtuple('cell', 'state_size')\n+        cell = Cell(state_size=self.units)\n+        return cell\n+\n+    def build(self, input_shape):\n+        super(CuDNNGRU, self).build(input_shape)\n+        if isinstance(input_shape, list):\n+            input_shape = input_shape[0]\n+        input_dim = input_shape[-1]\n+\n+        from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\n+        self._cudnn_gru = cudnn_rnn_ops.CudnnGRU(\n+            num_layers=1,\n+            num_units=self.units,\n+            input_size=input_dim,\n+            input_mode='linear_input')\n+\n+        self.kernel = self.add_weight(shape=(input_dim, self.units * 3),\n+                                      name='kernel',\n+                                      initializer=self.kernel_initializer,\n+                                      regularizer=self.kernel_regularizer,\n+                                      constraint=self.kernel_constraint)\n+        self.recurrent_kernel = self.add_weight(\n+            shape=(self.units, self.units * 3),\n+            name='recurrent_kernel',\n+            initializer=self.recurrent_initializer,\n+            regularizer=self.recurrent_regularizer,\n+            constraint=self.recurrent_constraint)\n+\n+        self.bias = self.add_weight(shape=(self.units * 6,),\n+                                    name='bias',\n+                                    initializer=self.bias_initializer,\n+                                    regularizer=self.bias_regularizer,\n+                                    constraint=self.bias_constraint)\n+\n+        self.kernel_z = self.kernel[:, :self.units]\n+        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n+        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n+        self.recurrent_kernel_r = self.recurrent_kernel[:,\n+                                                        self.units:\n+                                                        self.units * 2]\n+        self.kernel_h = self.kernel[:, self.units * 2:]\n+        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n+\n+        self.bias_z_i = self.bias[:self.units]\n+        self.bias_r_i = self.bias[self.units: self.units * 2]\n+        self.bias_h_i = self.bias[self.units * 2: self.units * 3]\n+        self.bias_z = self.bias[self.units * 3: self.units * 4]\n+        self.bias_r = self.bias[self.units * 4: self.units * 5]\n+        self.bias_h = self.bias[self.units * 5:]\n+\n+        self.built = True\n+\n+    def _process_batch(self, inputs, initial_state):\n+        import tensorflow as tf\n+        inputs = tf.transpose(inputs, (1, 0, 2))\n+        input_h = initial_state[0]\n+        input_h = tf.expand_dims(input_h, axis=0)\n+\n+        params = self._canonical_to_params(\n+            weights=[\n+                self.kernel_r,\n+                self.kernel_z,\n+                self.kernel_h,\n+                self.recurrent_kernel_r,\n+                self.recurrent_kernel_z,\n+                self.recurrent_kernel_h,\n+            ],\n+            biases=[\n+                self.bias_r_i,\n+                self.bias_z_i,\n+                self.bias_h_i,\n+                self.bias_r,\n+                self.bias_z,\n+                self.bias_h,\n+            ],\n+        )\n+        outputs, h = self._cudnn_gru(\n+            inputs,\n+            input_h=input_h,\n+            params=params,\n+            is_training=True)\n+\n+        if self.stateful or self.return_state:\n+            h = h[0]\n+        if self.return_sequences:\n+            output = tf.transpose(outputs, (1, 0, 2))\n+        else:\n+            output = outputs[-1]\n+        return output, [h]\n+\n+    def get_config(self):\n+        config = {\n+            'units': self.units,\n+            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n+            'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+            'bias_initializer': initializers.serialize(self.bias_initializer),\n+            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n+            'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n+            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n+            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n+            'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+            'bias_constraint': constraints.serialize(self.bias_constraint)}\n+        base_config = super(CuDNNGRU, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class CuDNNLSTM(_CuDNNRNN):\n+    \"\"\"Fast LSTM implementation backed by CuDNN.\n+\n+    Can only be run on GPU.\n+\n+    # Arguments\n+        units: Positive integer, dimensionality of the output space.\n+        kernel_initializer: Initializer for the `kernel` weights matrix,\n+            used for the linear transformation of the inputs.\n+            (see [initializers](../initializers.md)).\n+        unit_forget_bias: Boolean.\n+            If True, add 1 to the bias of the forget gate at initialization.\n+            Setting it to true will also force `bias_initializer=\"zeros\"`.\n+            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+        recurrent_initializer: Initializer for the `recurrent_kernel`\n+            weights matrix,\n+            used for the linear transformation of the recurrent state.\n+            (see [initializers](../initializers.md)).\n+        bias_initializer: Initializer for the bias vector\n+            (see [initializers](../initializers.md)).\n+        kernel_regularizer: Regularizer function applied to\n+            the `kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        recurrent_regularizer: Regularizer function applied to\n+            the `recurrent_kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        bias_regularizer: Regularizer function applied to the bias vector\n+            (see [regularizer](../regularizers.md)).\n+        activity_regularizer: Regularizer function applied to\n+            the output of the layer (its \"activation\").\n+            (see [regularizer](../regularizers.md)).\n+        kernel_constraint: Constraint function applied to\n+            the `kernel` weights matrix\n+            (see [constraints](../constraints.md)).\n+        recurrent_constraint: Constraint function applied to\n+            the `recurrent_kernel` weights matrix\n+            (see [constraints](../constraints.md)).\n+        bias_constraint: Constraint function applied to the bias vector\n+            (see [constraints](../constraints.md)).\n+        return_sequences: Boolean. Whether to return the last output.\n+            in the output sequence, or the full sequence.\n+        return_state: Boolean. Whether to return the last state\n+            in addition to the output.\n+        stateful: Boolean (default False). If True, the last state\n+            for each sample at index i in a batch will be used as initial\n+            state for the sample of index i in the following batch.\n+    \"\"\"\n+    def __init__(self, units,\n+                 kernel_initializer='glorot_uniform',\n+                 recurrent_initializer='orthogonal',\n+                 bias_initializer='zeros',\n+                 unit_forget_bias=True,\n+                 kernel_regularizer=None,\n+                 recurrent_regularizer=None,\n+                 bias_regularizer=None,\n+                 activity_regularizer=None,\n+                 kernel_constraint=None,\n+                 recurrent_constraint=None,\n+                 bias_constraint=None,\n+                 return_sequences=False,\n+                 return_state=False,\n+                 stateful=False,\n+                 **kwargs):\n+        self.units = units\n+        super(CuDNNLSTM, self).__init__(\n+            return_sequences=return_sequences,\n+            return_state=return_state,\n+            stateful=stateful,\n+            **kwargs)\n+\n+        self.kernel_initializer = initializers.get(kernel_initializer)\n+        self.recurrent_initializer = initializers.get(recurrent_initializer)\n+        self.bias_initializer = initializers.get(bias_initializer)\n+        self.unit_forget_bias = unit_forget_bias\n+\n+        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n+        self.bias_regularizer = regularizers.get(bias_regularizer)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+\n+        self.kernel_constraint = constraints.get(kernel_constraint)\n+        self.recurrent_constraint = constraints.get(recurrent_constraint)\n+        self.bias_constraint = constraints.get(bias_constraint)\n+\n+    @property\n+    def cell(self):\n+        Cell = namedtuple('cell', 'state_size')\n+        cell = Cell(state_size=(self.units, self.units))\n+        return cell\n+\n+    def build(self, input_shape):\n+        super(CuDNNLSTM, self).build(input_shape)\n+        if isinstance(input_shape, list):\n+            input_shape = input_shape[0]\n+        input_dim = input_shape[-1]\n+\n+        from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\n+        self._cudnn_lstm = cudnn_rnn_ops.CudnnLSTM(\n+            num_layers=1,\n+            num_units=self.units,\n+            input_size=input_dim,\n+            input_mode='linear_input')\n+\n+        self.kernel = self.add_weight(shape=(input_dim, self.units * 4),\n+                                      name='kernel',\n+                                      initializer=self.kernel_initializer,\n+                                      regularizer=self.kernel_regularizer,\n+                                      constraint=self.kernel_constraint)\n+        self.recurrent_kernel = self.add_weight(\n+            shape=(self.units, self.units * 4),\n+            name='recurrent_kernel',\n+            initializer=self.recurrent_initializer,\n+            regularizer=self.recurrent_regularizer,\n+            constraint=self.recurrent_constraint)\n+\n+        if self.unit_forget_bias:\n+            def bias_initializer(shape, *args, **kwargs):\n+                return K.concatenate([\n+                    self.bias_initializer((self.units * 5,), *args, **kwargs),\n+                    initializers.Ones()((self.units,), *args, **kwargs),\n+                    self.bias_initializer((self.units * 2,), *args, **kwargs),\n+                ])\n+        else:\n+            bias_initializer = self.bias_initializer\n+        self.bias = self.add_weight(shape=(self.units * 8,),\n+                                    name='bias',\n+                                    initializer=bias_initializer,\n+                                    regularizer=self.bias_regularizer,\n+                                    constraint=self.bias_constraint)\n+\n+        self.kernel_i = self.kernel[:, :self.units]\n+        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n+        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n+        self.kernel_o = self.kernel[:, self.units * 3:]\n+\n+        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n+        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n+        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n+        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n+\n+        self.bias_i_i = self.bias[:self.units]\n+        self.bias_f_i = self.bias[self.units: self.units * 2]\n+        self.bias_c_i = self.bias[self.units * 2: self.units * 3]\n+        self.bias_o_i = self.bias[self.units * 3: self.units * 4]\n+        self.bias_i = self.bias[self.units * 4: self.units * 5]\n+        self.bias_f = self.bias[self.units * 5: self.units * 6]\n+        self.bias_c = self.bias[self.units * 6: self.units * 7]\n+        self.bias_o = self.bias[self.units * 7:]\n+\n+        self.built = True\n+\n+    def _process_batch(self, inputs, initial_state):\n+        import tensorflow as tf\n+        inputs = tf.transpose(inputs, (1, 0, 2))\n+        input_h = initial_state[0]\n+        input_c = initial_state[1]\n+        input_h = tf.expand_dims(input_h, axis=0)\n+        input_c = tf.expand_dims(input_c, axis=0)\n+\n+        params = self._canonical_to_params(\n+            weights=[\n+                self.kernel_i,\n+                self.kernel_f,\n+                self.kernel_c,\n+                self.kernel_o,\n+                self.recurrent_kernel_i,\n+                self.recurrent_kernel_f,\n+                self.recurrent_kernel_c,\n+                self.recurrent_kernel_o,\n+            ],\n+            biases=[\n+                self.bias_i_i,\n+                self.bias_f_i,\n+                self.bias_c_i,\n+                self.bias_o_i,\n+                self.bias_i,\n+                self.bias_f,\n+                self.bias_c,\n+                self.bias_o,\n+            ],\n+        )\n+        outputs, h, c = self._cudnn_lstm(\n+            inputs,\n+            input_h=input_h,\n+            input_c=input_c,\n+            params=params,\n+            is_training=True)\n+\n+        if self.stateful or self.return_state:\n+            h = h[0]\n+            c = c[0]\n+        if self.return_sequences:\n+            output = tf.transpose(outputs, (1, 0, 2))\n+        else:\n+            output = outputs[-1]\n+        return output, [h, c]\n+\n+    def get_config(self):\n+        config = {\n+            'units': self.units,\n+            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n+            'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+            'bias_initializer': initializers.serialize(self.bias_initializer),\n+            'unit_forget_bias': self.unit_forget_bias,\n+            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n+            'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n+            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n+            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n+            'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+            'bias_constraint': constraints.serialize(self.bias_constraint)}\n+        base_config = super(CuDNNLSTM, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,353 @@\n+import pytest\n+import numpy as np\n+from numpy.testing import assert_allclose\n+import keras\n+from keras.utils.test_utils import layer_test\n+from keras.utils.test_utils import keras_test\n+import time\n+\n+\n+@keras_test\n+@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n+                    reason='Requires TensorFlow backend')\n+@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n+                    reason='Requires GPU')\n+def test_cudnn_rnn_canonical_to_params_lstm():\n+    units = 1\n+    input_size = 1\n+    layer = keras.layers.CuDNNLSTM(units)\n+    layer.build((None, None, input_size))\n+\n+    params = layer._canonical_to_params(\n+        weights=[\n+            layer.kernel_i,\n+            layer.kernel_f,\n+            layer.kernel_c,\n+            layer.kernel_o,\n+            layer.recurrent_kernel_i,\n+            layer.recurrent_kernel_f,\n+            layer.recurrent_kernel_c,\n+            layer.recurrent_kernel_o,\n+        ],\n+        biases=[\n+            layer.bias_i_i,\n+            layer.bias_f_i,\n+            layer.bias_c_i,\n+            layer.bias_o_i,\n+            layer.bias_i,\n+            layer.bias_f,\n+            layer.bias_c,\n+            layer.bias_o,\n+        ],\n+    )\n+    ref_params = layer._cudnn_lstm.canonical_to_params(\n+        weights=[\n+            layer.kernel_i,\n+            layer.kernel_f,\n+            layer.kernel_c,\n+            layer.kernel_o,\n+            layer.recurrent_kernel_i,\n+            layer.recurrent_kernel_f,\n+            layer.recurrent_kernel_c,\n+            layer.recurrent_kernel_o,\n+        ],\n+        biases=[\n+            layer.bias_i_i,\n+            layer.bias_f_i,\n+            layer.bias_c_i,\n+            layer.bias_o_i,\n+            layer.bias_i,\n+            layer.bias_f,\n+            layer.bias_c,\n+            layer.bias_o,\n+        ],\n+    )\n+    ref_params_value = keras.backend.get_value(ref_params)\n+    params_value = keras.backend.get_value(params)\n+    diff = np.mean(ref_params_value - params_value)\n+    assert diff < 1e-8\n+\n+\n+@keras_test\n+@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n+                    reason='Requires TensorFlow backend')\n+@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n+                    reason='Requires GPU')\n+def test_cudnn_rnn_canonical_to_params_gru():\n+    units = 7\n+    input_size = 9\n+    layer = keras.layers.CuDNNGRU(units)\n+    layer.build((None, None, input_size))\n+\n+    ref_params = layer._cudnn_gru.canonical_to_params(\n+        weights=[\n+            layer.kernel_r,\n+            layer.kernel_z,\n+            layer.kernel_h,\n+            layer.recurrent_kernel_r,\n+            layer.recurrent_kernel_z,\n+            layer.recurrent_kernel_h,\n+        ],\n+        biases=[\n+            layer.bias_r_i,\n+            layer.bias_z_i,\n+            layer.bias_h_i,\n+            layer.bias_r,\n+            layer.bias_z,\n+            layer.bias_h,\n+        ],\n+    )\n+    params = layer._canonical_to_params(\n+        weights=[\n+            layer.kernel_r,\n+            layer.kernel_z,\n+            layer.kernel_h,\n+            layer.recurrent_kernel_r,\n+            layer.recurrent_kernel_z,\n+            layer.recurrent_kernel_h,\n+        ],\n+        biases=[\n+            layer.bias_r_i,\n+            layer.bias_z_i,\n+            layer.bias_h_i,\n+            layer.bias_r,\n+            layer.bias_z,\n+            layer.bias_h,\n+        ],\n+    )\n+    ref_params_value = keras.backend.get_value(ref_params)\n+    params_value = keras.backend.get_value(params)\n+    diff = np.mean(ref_params_value - params_value)\n+    assert diff < 1e-8\n+\n+\n+@keras_test\n+@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n+                    reason='Requires TensorFlow backend')\n+@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n+                    reason='Requires GPU')\n+def test_cudnn_rnn_timing():\n+    input_size = 1000\n+    timesteps = 60\n+    units = 256\n+    num_samples = 10000\n+\n+    times = []\n+    for rnn_type in ['lstm', 'gru']:\n+        for use_cudnn in [True, False]:\n+            start_time = time.time()\n+            inputs = keras.layers.Input(shape=(None, input_size))\n+            if use_cudnn:\n+                if rnn_type == 'lstm':\n+                    layer = keras.layers.CuDNNLSTM(units)\n+                else:\n+                    layer = keras.layers.CuDNNGRU(units)\n+            else:\n+                if rnn_type == 'lstm':\n+                    layer = keras.layers.LSTM(units)\n+                else:\n+                    layer = keras.layers.GRU(units)\n+            outputs = layer(inputs)\n+\n+            model = keras.models.Model(inputs, outputs)\n+            model.compile('sgd', 'mse')\n+\n+            x = np.random.random((num_samples, timesteps, input_size))\n+            y = np.random.random((num_samples, units))\n+            model.fit(x, y, epochs=4, batch_size=32)\n+\n+            times.append(time.time() - start_time)\n+\n+        speedup = times[1] / times[0]\n+        print(rnn_type, 'speedup', speedup)\n+        assert speedup > 3\n+        keras.backend.clear_session()\n+\n+\n+@keras_test\n+@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n+                    reason='Requires TensorFlow backend')\n+@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n+                    reason='Requires GPU')\n+def test_cudnn_rnn_basics():\n+    input_size = 10\n+    timesteps = 6\n+    units = 2\n+    num_samples = 32\n+    for layer_class in [keras.layers.CuDNNGRU, keras.layers.CuDNNLSTM]:\n+        for return_sequences in [True, False]:\n+            with keras.utils.CustomObjectScope(\n+                {'keras.layers.CuDNNGRU': keras.layers.CuDNNGRU,\n+                 'keras.layers.CuDNNLSTM': keras.layers.CuDNNLSTM}):\n+                layer_test(\n+                    layer_class,\n+                    kwargs={'units': units,\n+                            'return_sequences': return_sequences},\n+                    input_shape=(num_samples, timesteps, input_size))\n+\n+\n+@keras_test\n+@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n+                    reason='Requires TensorFlow backend')\n+@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n+                    reason='Requires GPU')\n+def test_trainability():\n+    input_size = 10\n+    units = 2\n+    for layer_class in [keras.layers.CuDNNGRU, keras.layers.CuDNNLSTM]:\n+        layer = layer_class(units)\n+        layer.build((None, None, input_size))\n+        assert len(layer.weights) == 3\n+        assert len(layer.trainable_weights) == 3\n+        assert len(layer.non_trainable_weights) == 0\n+        layer.trainable = False\n+        assert len(layer.weights) == 3\n+        assert len(layer.non_trainable_weights) == 3\n+        assert len(layer.trainable_weights) == 0\n+        layer.trainable = True\n+        assert len(layer.weights) == 3\n+        assert len(layer.trainable_weights) == 3\n+        assert len(layer.non_trainable_weights) == 0\n+\n+\n+@keras_test\n+@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n+                    reason='Requires TensorFlow backend')\n+@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n+                    reason='Requires GPU')\n+def test_regularizer():\n+    input_size = 10\n+    timesteps = 6\n+    units = 2\n+    num_samples = 32\n+    for layer_class in [keras.layers.CuDNNGRU, keras.layers.CuDNNLSTM]:\n+        layer = layer_class(units, return_sequences=False,\n+                            input_shape=(timesteps, input_size),\n+                            kernel_regularizer=keras.regularizers.l1(0.01),\n+                            recurrent_regularizer=keras.regularizers.l1(0.01),\n+                            bias_regularizer='l2')\n+        layer.build((None, None, input_size))\n+        assert len(layer.losses) == 3\n+\n+        layer = layer_class(units, return_sequences=False,\n+                            input_shape=(timesteps, input_size),\n+                            activity_regularizer='l2')\n+        assert layer.activity_regularizer\n+        x = keras.backend.variable(np.ones((num_samples,\n+                                            timesteps,\n+                                            input_size)))\n+        layer(x)\n+        assert len(layer.get_losses_for(x)) == 1\n+\n+\n+@keras_test\n+@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n+                    reason='Requires TensorFlow backend')\n+@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n+                    reason='Requires GPU')\n+def test_return_state():\n+    input_size = 10\n+    timesteps = 6\n+    units = 2\n+    num_samples = 32\n+\n+    for layer_class in [keras.layers.CuDNNGRU, keras.layers.CuDNNLSTM]:\n+        num_states = 2 if layer_class is keras.layers.CuDNNLSTM else 1\n+\n+        inputs = keras.Input(batch_shape=(num_samples, timesteps, input_size))\n+        layer = layer_class(units, return_state=True, stateful=True)\n+        outputs = layer(inputs)\n+        output, state = outputs[0], outputs[1:]\n+        assert len(state) == num_states\n+        model = keras.models.Model(inputs, state[0])\n+\n+        inputs = np.random.random((num_samples, timesteps, input_size))\n+        state = model.predict(inputs)\n+        np.testing.assert_allclose(\n+            keras.backend.eval(layer.states[0]), state, atol=1e-4)\n+\n+\n+@keras_test\n+@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n+                    reason='Requires TensorFlow backend')\n+@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n+                    reason='Requires GPU')\n+def test_specify_initial_state_keras_tensor():\n+    input_size = 10\n+    timesteps = 6\n+    units = 2\n+    num_samples = 32\n+    for layer_class in [keras.layers.CuDNNGRU, keras.layers.CuDNNLSTM]:\n+        num_states = 2 if layer_class is keras.layers.CuDNNLSTM else 1\n+\n+        inputs = keras.Input((timesteps, input_size))\n+        initial_state = [keras.Input((units,)) for _ in range(num_states)]\n+        layer = layer_class(units)\n+        if len(initial_state) == 1:\n+            output = layer(inputs, initial_state=initial_state[0])\n+        else:\n+            output = layer(inputs, initial_state=initial_state)\n+        assert initial_state[0] in layer.inbound_nodes[0].input_tensors\n+\n+        model = keras.models.Model([inputs] + initial_state, output)\n+        model.compile(loss='categorical_crossentropy', optimizer='adam')\n+\n+        inputs = np.random.random((num_samples, timesteps, input_size))\n+        initial_state = [np.random.random((num_samples, units))\n+                         for _ in range(num_states)]\n+        targets = np.random.random((num_samples, units))\n+        model.fit([inputs] + initial_state, targets)\n+\n+\n+@keras_test\n+@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n+                    reason='Requires TensorFlow backend')\n+@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n+                    reason='Requires GPU')\n+def test_statefulness():\n+    input_size = 10\n+    timesteps = 6\n+    units = 2\n+    num_samples = 32\n+\n+    for layer_class in [keras.layers.CuDNNGRU, keras.layers.CuDNNLSTM]:\n+        model = keras.models.Sequential()\n+        model.add(keras.layers.Embedding(10, input_size,\n+                                         input_length=timesteps,\n+                                         batch_input_shape=(num_samples,\n+                                                            timesteps)))\n+        layer = layer_class(units,\n+                            return_sequences=False,\n+                            stateful=True,\n+                            weights=None)\n+        model.add(layer)\n+        model.compile(optimizer='sgd', loss='mse')\n+        out1 = model.predict(np.ones((num_samples, timesteps)))\n+        assert(out1.shape == (num_samples, units))\n+\n+        # train once so that the states change\n+        model.train_on_batch(np.ones((num_samples, timesteps)),\n+                             np.ones((num_samples, units)))\n+        out2 = model.predict(np.ones((num_samples, timesteps)))\n+\n+        # if the state is not reset, output should be different\n+        assert(out1.max() != out2.max())\n+\n+        # check that output changes after states are reset\n+        # (even though the model itself didn't change)\n+        layer.reset_states()\n+        out3 = model.predict(np.ones((num_samples, timesteps)))\n+        assert(out2.max() != out3.max())\n+\n+        # check that container-level reset_states() works\n+        model.reset_states()\n+        out4 = model.predict(np.ones((num_samples, timesteps)))\n+        assert_allclose(out3, out4, atol=1e-5)\n+\n+        # check that the call to `predict` updated the states\n+        out5 = model.predict(np.ones((num_samples, timesteps)))\n+        assert(out4.max() != out5.max())\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n@@ -111,6 +111,8 @@ def test_TimeDistributed():\n \n \n @keras_test\n+@pytest.mark.skipif((K.backend() == 'cntk'),\n+                    reason='Flaky with CNTK backend')\n def test_TimeDistributed_learning_phase():\n     # test layers that need learning_phase to be set\n     np.random.seed(1234)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
