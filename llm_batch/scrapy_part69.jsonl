{"custom_id": "scrapy#5aeab5b2916e3e2f0cb56a667b89ea7cd68bf7a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 7 | Files Changed: 8 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 566 | Contributors (this commit): 3 | Commits (past 90d): 31 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n from scrapy.command import ScrapyCommand\n from scrapy.fetcher import fetch\n from scrapy.http import Request\n-from scrapy.item.models import BaseItem\n+from scrapy.item import BaseItem\n from scrapy.spider import spiders\n from scrapy.utils import display\n from scrapy import log\n\n@@ -6,7 +6,7 @@ See documentation in docs/item-pipeline.rst\n \n from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.item.models import BaseItem\n+from scrapy.item import BaseItem\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import defer_succeed, mustbe_deferred\n from scrapy.conf import settings\n\n@@ -6,7 +6,7 @@ See documentation in docs/ref/spiders.rst\n \"\"\"\n \n from scrapy.contrib.spiders.init import InitSpider\n-from scrapy.item.models import BaseItem\n+from scrapy.item import BaseItem\n from scrapy.http import Request\n from scrapy.utils.iterators import xmliter, csviter\n from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector\n\n@@ -9,7 +9,7 @@ from scrapy.utils.misc import arg_to_iter, load_object\n from scrapy.core.exceptions import IgnoreRequest, DropItem\n from scrapy.core import signals\n from scrapy.http import Request, Response\n-from scrapy.item.models import BaseItem\n+from scrapy.item import BaseItem\n from scrapy.spider.middleware import SpiderMiddlewareManager\n from scrapy import log\n from scrapy.stats import stats\n\n\n@@ -1 +0,0 @@\n-from scrapy.item.models import ScrapedItem\n\n@@ -1,6 +1,6 @@\n from UserDict import DictMixin\n \n-from scrapy.item.models import BaseItem\n+from scrapy.item import BaseItem\n \n class Field(dict):\n     \"\"\"Container of field metadata\"\"\"\n\n@@ -4,7 +4,7 @@ Helper functions for formatting and pretty printing some objects\n import sys\n import pprint as pypprint\n \n-from scrapy.item.models import BaseItem\n+from scrapy.item import BaseItem\n from scrapy.http import Request, Response\n \n nocolour = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c99e572fdbc99c60e10f2d9b40405c89ab178e64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 97 | Lines Deleted: 97 | Files Changed: 1 | Hunks: 26 | Methods Changed: 21 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 194 | Churn Cumulative: 424 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -38,36 +38,36 @@ class ItemLoaderTest(unittest.TestCase):\n     def test_load_item_using_default_loader(self):\n         i = TestItem()\n         i['summary'] = u'lala'\n-        ip = ItemLoader(item=i)\n-        ip.add_value('name', u'marta')\n-        item = ip.load_item()\n+        il = ItemLoader(item=i)\n+        il.add_value('name', u'marta')\n+        item = il.load_item()\n         assert item is i\n         self.assertEqual(item['summary'], u'lala')\n         self.assertEqual(item['name'], [u'marta'])\n \n     def test_load_item_using_custom_loader(self):\n-        ip = TestItemLoader()\n-        ip.add_value('name', u'marta')\n-        item = ip.load_item()\n+        il = TestItemLoader()\n+        il.add_value('name', u'marta')\n+        item = il.load_item()\n         self.assertEqual(item['name'], [u'Marta'])\n \n     def test_add_value(self):\n-        ip = TestItemLoader()\n-        ip.add_value('name', u'marta')\n-        self.assertEqual(ip.get_collected_values('name'), [u'Marta'])\n-        self.assertEqual(ip.get_output_value('name'), [u'Marta'])\n-        ip.add_value('name', u'pepe')\n-        self.assertEqual(ip.get_collected_values('name'), [u'Marta', u'Pepe'])\n-        self.assertEqual(ip.get_output_value('name'), [u'Marta', u'Pepe'])\n+        il = TestItemLoader()\n+        il.add_value('name', u'marta')\n+        self.assertEqual(il.get_collected_values('name'), [u'Marta'])\n+        self.assertEqual(il.get_output_value('name'), [u'Marta'])\n+        il.add_value('name', u'pepe')\n+        self.assertEqual(il.get_collected_values('name'), [u'Marta', u'Pepe'])\n+        self.assertEqual(il.get_output_value('name'), [u'Marta', u'Pepe'])\n \n     def test_replace_value(self):\n-        ip = TestItemLoader()\n-        ip.replace_value('name', u'marta')\n-        self.assertEqual(ip.get_collected_values('name'), [u'Marta'])\n-        self.assertEqual(ip.get_output_value('name'), [u'Marta'])\n-        ip.replace_value('name', u'pepe')\n-        self.assertEqual(ip.get_collected_values('name'), [u'Pepe'])\n-        self.assertEqual(ip.get_output_value('name'), [u'Pepe'])\n+        il = TestItemLoader()\n+        il.replace_value('name', u'marta')\n+        self.assertEqual(il.get_collected_values('name'), [u'Marta'])\n+        self.assertEqual(il.get_output_value('name'), [u'Marta'])\n+        il.replace_value('name', u'pepe')\n+        self.assertEqual(il.get_collected_values('name'), [u'Pepe'])\n+        self.assertEqual(il.get_output_value('name'), [u'Pepe'])\n \n     def test_apply_concat_filter(self):\n         def filter_world(x):\n@@ -77,154 +77,154 @@ class ItemLoaderTest(unittest.TestCase):\n         self.assertEqual(proc(['hello', 'world', 'this', 'is', 'scrapy']),\n                          ['HELLO', 'THIS', 'IS', 'SCRAPY'])\n \n-    def test_map_concat_filter_multiple_functions(self):\n+    def test_map_concat_filter_multil(self):\n         class TestItemLoader(NameItemLoader):\n             name_in = MapCompose(lambda v: v.title(), lambda v: v[:-1])\n \n-        ip = TestItemLoader()\n-        ip.add_value('name', u'marta')\n-        self.assertEqual(ip.get_output_value('name'), [u'Mart'])\n-        item = ip.load_item()\n+        il = TestItemLoader()\n+        il.add_value('name', u'marta')\n+        self.assertEqual(il.get_output_value('name'), [u'Mart'])\n+        item = il.load_item()\n         self.assertEqual(item['name'], [u'Mart'])\n \n     def test_default_input_processor(self):\n-        ip = DefaultedItemLoader()\n-        ip.add_value('name', u'marta')\n-        self.assertEqual(ip.get_output_value('name'), [u'mart'])\n+        il = DefaultedItemLoader()\n+        il.add_value('name', u'marta')\n+        self.assertEqual(il.get_output_value('name'), [u'mart'])\n \n     def test_inherited_default_input_processor(self):\n         class InheritDefaultedItemLoader(DefaultedItemLoader):\n             pass\n \n-        ip = InheritDefaultedItemLoader()\n-        ip.add_value('name', u'marta')\n-        self.assertEqual(ip.get_output_value('name'), [u'mart'])\n+        il = InheritDefaultedItemLoader()\n+        il.add_value('name', u'marta')\n+        self.assertEqual(il.get_output_value('name'), [u'mart'])\n \n     def test_input_processor_inheritance(self):\n         class ChildItemLoader(TestItemLoader):\n             url_in = MapCompose(lambda v: v.lower())\n \n-        ip = ChildItemLoader()\n-        ip.add_value('url', u'HTTP://scrapy.ORG')\n-        self.assertEqual(ip.get_output_value('url'), [u'http://scrapy.org'])\n-        ip.add_value('name', u'marta')\n-        self.assertEqual(ip.get_output_value('name'), [u'Marta'])\n+        il = ChildItemLoader()\n+        il.add_value('url', u'HTTP://scrapy.ORG')\n+        self.assertEqual(il.get_output_value('url'), [u'http://scrapy.org'])\n+        il.add_value('name', u'marta')\n+        self.assertEqual(il.get_output_value('name'), [u'Marta'])\n \n         class ChildChildItemLoader(ChildItemLoader):\n             url_in = MapCompose(lambda v: v.upper())\n             summary_in = MapCompose(lambda v: v)\n \n-        ip = ChildChildItemLoader()\n-        ip.add_value('url', u'http://scrapy.org')\n-        self.assertEqual(ip.get_output_value('url'), [u'HTTP://SCRAPY.ORG'])\n-        ip.add_value('name', u'marta')\n-        self.assertEqual(ip.get_output_value('name'), [u'Marta'])\n+        il = ChildChildItemLoader()\n+        il.add_value('url', u'http://scrapy.org')\n+        self.assertEqual(il.get_output_value('url'), [u'HTTP://SCRAPY.ORG'])\n+        il.add_value('name', u'marta')\n+        self.assertEqual(il.get_output_value('name'), [u'Marta'])\n \n     def test_empty_map_concat(self):\n         class IdentityDefaultedItemLoader(DefaultedItemLoader):\n             name_in = MapCompose()\n \n-        ip = IdentityDefaultedItemLoader()\n-        ip.add_value('name', u'marta')\n-        self.assertEqual(ip.get_output_value('name'), [u'marta'])\n+        il = IdentityDefaultedItemLoader()\n+        il.add_value('name', u'marta')\n+        self.assertEqual(il.get_output_value('name'), [u'marta'])\n \n     def test_identity_input_processor(self):\n         class IdentityDefaultedItemLoader(DefaultedItemLoader):\n             name_in = Identity()\n \n-        ip = IdentityDefaultedItemLoader()\n-        ip.add_value('name', u'marta')\n-        self.assertEqual(ip.get_output_value('name'), [u'marta'])\n+        il = IdentityDefaultedItemLoader()\n+        il.add_value('name', u'marta')\n+        self.assertEqual(il.get_output_value('name'), [u'marta'])\n \n     def test_extend_custom_input_processors(self):\n         class ChildItemLoader(TestItemLoader):\n             name_in = MapCompose(TestItemLoader.name_in, unicode.swapcase)\n \n-        ip = ChildItemLoader()\n-        ip.add_value('name', u'marta')\n-        self.assertEqual(ip.get_output_value('name'), [u'mARTA'])\n+        il = ChildItemLoader()\n+        il.add_value('name', u'marta')\n+        self.assertEqual(il.get_output_value('name'), [u'mARTA'])\n \n     def test_extend_default_input_processors(self):\n         class ChildDefaultedItemLoader(DefaultedItemLoader):\n             name_in = MapCompose(DefaultedItemLoader.default_input_processor, unicode.swapcase)\n \n-        ip = ChildDefaultedItemLoader()\n-        ip.add_value('name', u'marta')\n-        self.assertEqual(ip.get_output_value('name'), [u'MART'])\n+        il = ChildDefaultedItemLoader()\n+        il.add_value('name', u'marta')\n+        self.assertEqual(il.get_output_value('name'), [u'MART'])\n \n     def test_output_processor_using_function(self):\n-        ip = TestItemLoader()\n-        ip.add_value('name', [u'mar', u'ta'])\n-        self.assertEqual(ip.get_output_value('name'), [u'Mar', u'Ta'])\n+        il = TestItemLoader()\n+        il.add_value('name', [u'mar', u'ta'])\n+        self.assertEqual(il.get_output_value('name'), [u'Mar', u'Ta'])\n \n         class TakeFirstItemLoader(TestItemLoader):\n             name_out = u\" \".join\n \n-        ip = TakeFirstItemLoader()\n-        ip.add_value('name', [u'mar', u'ta'])\n-        self.assertEqual(ip.get_output_value('name'), u'Mar Ta')\n+        il = TakeFirstItemLoader()\n+        il.add_value('name', [u'mar', u'ta'])\n+        self.assertEqual(il.get_output_value('name'), u'Mar Ta')\n \n     def test_output_processor_using_classes(self):\n-        ip = TestItemLoader()\n-        ip.add_value('name', [u'mar', u'ta'])\n-        self.assertEqual(ip.get_output_value('name'), [u'Mar', u'Ta'])\n+        il = TestItemLoader()\n+        il.add_value('name', [u'mar', u'ta'])\n+        self.assertEqual(il.get_output_value('name'), [u'Mar', u'Ta'])\n \n         class TakeFirstItemLoader(TestItemLoader):\n             name_out = Join()\n \n-        ip = TakeFirstItemLoader()\n-        ip.add_value('name', [u'mar', u'ta'])\n-        self.assertEqual(ip.get_output_value('name'), u'Mar Ta')\n+        il = TakeFirstItemLoader()\n+        il.add_value('name', [u'mar', u'ta'])\n+        self.assertEqual(il.get_output_value('name'), u'Mar Ta')\n \n         class TakeFirstItemLoader(TestItemLoader):\n             name_out = Join(\"<br>\")\n \n-        ip = TakeFirstItemLoader()\n-        ip.add_value('name', [u'mar', u'ta'])\n-        self.assertEqual(ip.get_output_value('name'), u'Mar<br>Ta')\n+        il = TakeFirstItemLoader()\n+        il.add_value('name', [u'mar', u'ta'])\n+        self.assertEqual(il.get_output_value('name'), u'Mar<br>Ta')\n \n     def test_default_output_processor(self):\n-        ip = TestItemLoader()\n-        ip.add_value('name', [u'mar', u'ta'])\n-        self.assertEqual(ip.get_output_value('name'), [u'Mar', u'Ta'])\n+        il = TestItemLoader()\n+        il.add_value('name', [u'mar', u'ta'])\n+        self.assertEqual(il.get_output_value('name'), [u'Mar', u'Ta'])\n \n         class LalaItemLoader(TestItemLoader):\n             default_output_processor = Identity()\n \n-        ip = LalaItemLoader()\n-        ip.add_value('name', [u'mar', u'ta'])\n-        self.assertEqual(ip.get_output_value('name'), [u'Mar', u'Ta'])\n+        il = LalaItemLoader()\n+        il.add_value('name', [u'mar', u'ta'])\n+        self.assertEqual(il.get_output_value('name'), [u'Mar', u'Ta'])\n \n     def test_loader_context_on_declaration(self):\n         class ChildItemLoader(TestItemLoader):\n             url_in = MapCompose(processor_with_args, key=u'val')\n \n-        ip = ChildItemLoader()\n-        ip.add_value('url', u'text')\n-        self.assertEqual(ip.get_output_value('url'), ['val'])\n-        ip.replace_value('url', u'text2')\n-        self.assertEqual(ip.get_output_value('url'), ['val'])\n+        il = ChildItemLoader()\n+        il.add_value('url', u'text')\n+        self.assertEqual(il.get_output_value('url'), ['val'])\n+        il.replace_value('url', u'text2')\n+        self.assertEqual(il.get_output_value('url'), ['val'])\n \n     def test_loader_context_on_instantiation(self):\n         class ChildItemLoader(TestItemLoader):\n             url_in = MapCompose(processor_with_args)\n \n-        ip = ChildItemLoader(key=u'val')\n-        ip.add_value('url', u'text')\n-        self.assertEqual(ip.get_output_value('url'), ['val'])\n-        ip.replace_value('url', u'text2')\n-        self.assertEqual(ip.get_output_value('url'), ['val'])\n+        il = ChildItemLoader(key=u'val')\n+        il.add_value('url', u'text')\n+        self.assertEqual(il.get_output_value('url'), ['val'])\n+        il.replace_value('url', u'text2')\n+        self.assertEqual(il.get_output_value('url'), ['val'])\n \n     def test_loader_context_on_assign(self):\n         class ChildItemLoader(TestItemLoader):\n             url_in = MapCompose(processor_with_args)\n \n-        ip = ChildItemLoader()\n-        ip.context['key'] = u'val'\n-        ip.add_value('url', u'text')\n-        self.assertEqual(ip.get_output_value('url'), ['val'])\n-        ip.replace_value('url', u'text2')\n-        self.assertEqual(ip.get_output_value('url'), ['val'])\n+        il = ChildItemLoader()\n+        il.context['key'] = u'val'\n+        il.add_value('url', u'text')\n+        self.assertEqual(il.get_output_value('url'), ['val'])\n+        il.replace_value('url', u'text2')\n+        self.assertEqual(il.get_output_value('url'), ['val'])\n \n     def test_item_passed_to_input_processor_functions(self):\n         def processor(value, loader_context):\n@@ -234,15 +234,15 @@ class ItemLoaderTest(unittest.TestCase):\n             url_in = MapCompose(processor)\n \n         it = TestItem(name='marta')\n-        ip = ChildItemLoader(item=it)\n-        ip.add_value('url', u'text')\n-        self.assertEqual(ip.get_output_value('url'), ['marta'])\n-        ip.replace_value('url', u'text2')\n-        self.assertEqual(ip.get_output_value('url'), ['marta'])\n+        il = ChildItemLoader(item=it)\n+        il.add_value('url', u'text')\n+        self.assertEqual(il.get_output_value('url'), ['marta'])\n+        il.replace_value('url', u'text2')\n+        self.assertEqual(il.get_output_value('url'), ['marta'])\n \n     def test_add_value_on_unknown_field(self):\n-        ip = TestItemLoader()\n-        self.assertRaises(KeyError, ip.add_value, 'wrong_field', [u'lala', u'lolo'])\n+        il = TestItemLoader()\n+        self.assertRaises(KeyError, il.add_value, 'wrong_field', [u'lala', u'lolo'])\n \n     def test_compose_processor(self):\n         class TestItemLoader(NameItemLoader):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f0aea4aa4aa5c9045b5bc51159b47420404c589b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 13 | Files Changed: 3 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 16/14 | Churn Δ: 30 | Churn Cumulative: 30 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n\n@@ -1,11 +1,11 @@\n-from scrapy.newitem.exporters import BaseItemExporter\n+from scrapy.contrib.exporter import BaseItemExporter\n from scrapy.utils.serialization import serialize\n \n \n class JSONItemExporter(BaseItemExporter):\n \n     def __init__(self, file):\n-        super(BaseItemExporter, self).__init__()\n+        super(JSONItemExporter, self).__init__()\n         self.file = file\n \n     def export(self, item):\n\n@@ -1,10 +1,11 @@\n from cPickle import Pickler\n from cStringIO import StringIO\n-import pprint\n+\n from twisted.trial import unittest\n \n from scrapy.newitem import Item, Field\n-from scrapy.newitem.exporters import *\n+from scrapy.contrib.exporter import BaseItemExporter, PprintItemExporter, \\\n+    PickleItemExporter, CsvItemExporter, XmlItemExporter\n \n class TestItem(Item):\n     name = Field()\n@@ -115,17 +116,20 @@ class JSONItemExporterTest(unittest.TestCase):\n \n     def setUp(self):\n         try:\n-            from scrapy.newitem.exporters.jsonexporter import JSONItemExporter\n-            self.output = StringIO()\n-            self.ie = JSONItemExporter(self.output)\n-\n-        except ImportError, e:\n-            raise unittest.SkipTest(\"Json library not available\") \n+            import json\n+        except ImportError:\n+            try:\n+                import simplejson\n+            except ImportError:\n+                raise unittest.SkipTest(\"simplejson module not available\") \n \n     def test_export(self):\n+        from scrapy.contrib.exporter.jsonexporter import JSONItemExporter\n+\n+        output = StringIO()\n+        ie = JSONItemExporter(output)\n         i = TestItem(name=u'John', age=22)\n+        ie.export(i)\n \n-        self.ie.export(i)\n-\n-        self.assertEqual(self.output.getvalue(), '{\"age\": 22, \"name\": \"John\"}\\n')\n+        self.assertEqual(output.getvalue(), '{\"age\": 22, \"name\": \"John\"}\\n')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5d29c3427cec4cd38336b16dd699ec554d318606", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#066fd9fa8416391135191d944d59e65bc60f00c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 66 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -26,12 +26,12 @@ class ItemLoader(object):\n         self._values = defaultdict(list)\n \n     def add_value(self, field_name, value):\n-        parsed_value = self._parse_input_value(field_name, value)\n-        self._values[field_name] += arg_to_iter(parsed_value)\n+        processed_value = self._process_input_value(field_name, value)\n+        self._values[field_name] += arg_to_iter(processed_value)\n \n     def replace_value(self, field_name, value):\n-        parsed_value = self._parse_input_value(field_name, value)\n-        self._values[field_name] = arg_to_iter(parsed_value)\n+        processed_value = self._process_input_value(field_name, value)\n+        self._values[field_name] = arg_to_iter(processed_value)\n \n     def load_item(self):\n         item = self.item\n@@ -61,7 +61,7 @@ class ItemLoader(object):\n                 self.default_output_processor)\n         return proc\n \n-    def _parse_input_value(self, field_name, value):\n+    def _process_input_value(self, field_name, value):\n         proc = self.get_input_processor(field_name)\n         proc = wrap_loader_context(proc, self.context)\n         return proc(value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#16249814436572c18dc03f25a93df83ecc4ec014", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 30 | Churn Cumulative: 454 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,8 @@\n import unittest\n \n from scrapy.contrib.loader import ItemLoader, XPathItemLoader\n-from scrapy.contrib.loader.processor import Join, Identity, Compose, MapCompose\n+from scrapy.contrib.loader.processor import Join, Identity, TakeFirst, \\\n+    Compose, MapCompose\n from scrapy.newitem import Item, Field\n from scrapy.xpath import HtmlXPathSelector\n from scrapy.http import HtmlResponse\n@@ -254,6 +255,33 @@ class ItemLoaderTest(unittest.TestCase):\n         item = il.load_item()\n         self.assertEqual(item['name'], u'Mart')\n \n+class ProcessorsTest(unittest.TestCase):\n+\n+    def test_take_first(self):\n+        proc = TakeFirst()\n+        self.assertEqual(proc([None, '', 'hello', 'world']), 'hello')\n+\n+    def test_identity(self):\n+        proc = Identity()\n+        self.assertEqual(proc([None, '', 'hello', 'world']),\n+                         [None, '', 'hello', 'world'])\n+\n+    def test_join(self):\n+        proc = Join()\n+        self.assertRaises(TypeError, proc, [None, '', 'hello', 'world'])\n+        self.assertEqual(proc(['', 'hello', 'world']), u' hello world')\n+        self.assertEqual(proc(['hello', 'world']), u'hello world')\n+        self.assert_(isinstance(proc(['hello', 'world']), unicode))\n+\n+    def test_compose(self):\n+        proc = Compose(lambda v: v[0], str.upper)\n+        self.assertEqual(proc(['hello', 'world']), 'HELLO')\n+\n+    def test_mapcompose(self):\n+        filter_world = lambda x: None if x == 'world' else x\n+        proc = MapCompose(filter_world, unicode.upper)\n+        self.assertEqual(proc([u'hello', u'world', u'this', u'is', u'scrapy']),\n+                         [u'HELLO', u'THIS', u'IS', u'SCRAPY'])\n \n class TestXPathItemLoader(XPathItemLoader):\n     default_item_class = TestItem\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bebc8b20276e6b3bd6bc08a49edb0454ad1689a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 482 | Files Changed: 4 | Hunks: 4 | Methods Changed: 37 | Complexity Δ (Sum/Max): -81/0 | Churn Δ: 482 | Churn Cumulative: 545 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 9 | DMM Complexity: 0.3857677902621723\n\nDIFF:\n@@ -1,11 +0,0 @@\n-\"\"\"\n-WARNING: This module is obsolete and will be removed before the Scrapy 0.7\n-release\n-\"\"\"\n-\n-import warnings\n-\n-from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline\n-\n-warnings.warn(\"scrapy.contrib.item is obsolete and will be removed soon\", \\\n-                DeprecationWarning, stacklevel=2)\n\n@@ -1,129 +0,0 @@\n-import inspect\n-\n-from traceback import format_exc\n-from scrapy.conf import settings\n-\n-class AdaptorFunc(object):\n-    \"\"\"\n-    This is a private class that the AdaptorPipe uses to store\n-    its adaptation functions and their attributes.\n-    \"\"\"\n-    def __init__(self, adaptor):\n-        self.adaptor = adaptor\n-\n-        if inspect.isfunction(adaptor):\n-            self.func_args, _, _, _ = inspect.getargspec(adaptor)\n-        elif hasattr(adaptor, '__call__'):\n-            try:\n-                self.func_args, _, _, _ = inspect.getargspec(adaptor.__call__)\n-            except Exception:\n-                self.func_args = []\n-\n-        self.name = getattr(adaptor, 'func_name', None)\n-        if not self.name:\n-            if hasattr(adaptor, '__class__') and adaptor.__class__ is not type:\n-                self.name = adaptor.__class__.__name__\n-            else:\n-                self.name = adaptor.__name__\n-\n-    def __call__(self, value, kwargs):\n-        if 'adaptor_args' in self.func_args:\n-            return self.adaptor(value, kwargs)\n-        else:\n-            return self.adaptor(value)\n-\n-    def __repr__(self):\n-        return '<Adaptor %s >' % self.name\n-\n-class AdaptorPipe(list):\n-    \"\"\"\n-    Class that represents an item's attribute pipeline.\n-\n-    This class is itself a list, filled by adaptors to be run\n-    in order to filter the input.\n-    \"\"\"\n-    def __init__(self, adaptors):\n-        def _filter_adaptor(adaptor):\n-            return AdaptorFunc(adaptor) if not isinstance(adaptor, AdaptorFunc) else adaptor\n-\n-        if not hasattr(adaptors, '__iter__'):\n-            raise TypeError('You must provide AdaptorPipe a list of adaptors')\n-\n-        for adaptor in adaptors:\n-            if not callable(adaptor):\n-                raise TypeError(\"%s is not a callable\" % adaptor)\n-        super(AdaptorPipe, self).__init__(map(_filter_adaptor, adaptors))\n-        self.debug = settings.getbool('ADAPTORS_DEBUG')\n-\n-    def __call__(self, value, kwargs=None):\n-        \"\"\"\n-        Execute the adaptor pipeline for this attribute.\n-        \"\"\"\n-\n-        debug_padding = 0\n-        values = [value]\n-\n-        for adaptor in self:\n-            next_round = []\n-            for val in values:\n-                try:\n-                    if self.debug:\n-                        print \"%sinput | %s <\" % (' ' * debug_padding, adaptor.name), repr(val)\n-\n-                    val = adaptor(val, kwargs or {})\n-                    if isinstance(val, tuple):\n-                        next_round.extend(val)\n-                    elif val is not None:\n-                        next_round.append(val)\n-\n-                    if self.debug:\n-                        print \"%soutput | %s >\" % (' ' * debug_padding, adaptor.name), repr(val)\n-                except Exception:\n-                    print \"Error in '%s' adaptor. Traceback text:\" % adaptor.name\n-                    print format_exc()\n-                    return\n-\n-            debug_padding += 2\n-            values = next_round\n-\n-        return tuple(values)\n-\n-    def __add__(self, other):\n-        if callable(other):\n-            other = [other]\n-        elif hasattr(other, '__iter__'):\n-            other = list(other)\n-        return AdaptorPipe(super(AdaptorPipe, self).__add__(other))\n-\n-    def __repr__(self):\n-        return '<AdaptorPipe %s >' % super(AdaptorPipe, self).__repr__()\n-\n-    def add_adaptor(self, adaptor, position=None):\n-        if callable(adaptor):\n-            if not isinstance(adaptor, AdaptorFunc):\n-                adaptor = AdaptorFunc(adaptor)\n-            if position is None:\n-                self.append(adaptor)\n-            else:\n-                self.insert(position, adaptor)\n-\n-def adaptize(adaptor, my_args=None):\n-    \"\"\"\n-    This decorator helps you add to your pipelines adaptors that are able\n-    to receive extra keyword arguments from the spiders.\n-\n-    If my_args is None, it'll try to guess the arguments your adaptor receives\n-    by instropection, and if it can't guess them, it will act as if it were a normal\n-    adaptor.\n-    To do the last, you may also call adaptize with my_args = [], or @adaptize([])\n-    \"\"\"\n-    if my_args is None:\n-        if inspect.isfunction(adaptor):\n-            my_args, _, _, _ = inspect.getargspec(adaptor)\n-        else:\n-            my_args = []\n-\n-    def _adaptor(value, adaptor_args):\n-        kwargs = dict((key, val) for key, val in adaptor_args.items() if key in my_args)\n-        return adaptor(value, **kwargs)\n-    return _adaptor\n\n@@ -1,287 +0,0 @@\n-\"\"\"\n-This module contains some extra base models for scraped items which could be\n-useful in some Scrapy implementations\n-\"\"\"\n-\n-import copy\n-import hashlib\n-import warnings\n-\n-from pprint import PrettyPrinter\n-\n-from scrapy.item import ScrapedItem\n-from scrapy.contrib.item.adaptors import AdaptorPipe\n-from scrapy.core.exceptions import DropItem\n-from scrapy.utils.python import unique\n-\n-class ValidationError(DropItem):\n-    \"\"\"Indicates a data validation error\"\"\"\n-    def __init__(self, problem, value=None):\n-        self.problem = problem\n-        self.value = value\n-\n-    def __str__(self):\n-        if self.value is not None:\n-            return '%s \"%s\"' % (self.problem, self.value)\n-        else:\n-            return '%s' % (self.problem)\n-\n-class ValidationPipeline(object):\n-    def process_item(self, domain, item):\n-        item.validate()\n-        return item\n-\n-class RobustScrapedItem(ScrapedItem):\n-    \"\"\"\n-    A more robust scraped item class with a built-in validation mechanism and \n-    minimal versioning support\n-    \"\"\"\n-\n-    ATTRIBUTES = {\n-        'guid': basestring, # a global unique identifier\n-        'url': basestring,  # the main URL where this item was scraped from\n-    }\n-\n-    def __init__(self, data=None, adaptors_dict=None):\n-        super(RobustScrapedItem, self).__init__(data)\n-        self.__dict__['_adaptors_dict'] = adaptors_dict or {}\n-        self.__dict__['_version'] = None\n-\n-    def __getattr__(self, attr):\n-        # Return None for valid attributes not set, raise AttributeError for invalid attributes\n-        # Note that this method is called only when the attribute is not found in \n-        # self.__dict__ or the class/instance methods.\n-        if attr in self.ATTRIBUTES:\n-            return None\n-        else:\n-            raise AttributeError(attr)\n-\n-    def __setattr__(self, attr, value):\n-        \"\"\"\n-        Set an attribute checking it matches the attribute type declared in self.ATTRIBUTES\n-        \"\"\"\n-        if not attr.startswith('_') and attr not in self.ATTRIBUTES:\n-            raise AttributeError('Attribute \"%s\" is not a valid attribute name. You must add it to %s.ATTRIBUTES' % (attr, self.__class__.__name__))\n-\n-        if value is None:\n-            self.__dict__.pop(attr, None)\n-            return\n-\n-        if attr == '_adaptors_dict':\n-            return object.__setattr__(self, '_adaptors_dict', value)\n-\n-        type1 = self.ATTRIBUTES[attr]\n-        if hasattr(type1, '__iter__'):\n-            if not hasattr(value, '__iter__'):\n-                raise TypeError('Attribute \"%s\" must be a sequence' % attr)\n-            type2 = type1[0]\n-            for i in value:\n-                if not isinstance(i, type2):\n-                    raise TypeError('Attribute \"%s\" cannot contain %s, only %s' % (attr, i.__class__.__name__, type2.__name__))\n-        else:\n-            if not isinstance(value, type1):\n-                raise TypeError('Attribute \"%s\" must be %s, not %s' % (attr, type1.__name__, value.__class__.__name__))\n-\n-        self.__dict__[attr] = value\n-        self.__dict__['_version'] = None\n-\n-    def __delattr__(self, attr):\n-        \"\"\"\n-        Delete an attribute from the ScrapedItem instance if it exists.\n-        If not, raise an AttributeError.\n-        \"\"\"\n-        if attr in self.__dict__:\n-            del self.__dict__[attr]\n-            self.__dict__['_version'] = None\n-        else:\n-            raise AttributeError(\"Attribute '%s' doesn't exist\" % attr)\n-\n-    def __eq__(self, other):\n-        if isinstance(other, type(self)):\n-            return self.version == other.version\n-\n-    def __ne__(self, other):\n-        return self.version != other.version\n-\n-    def __sub__(self, other):\n-        return RobustItemDelta(other, self)\n-\n-    def __str__(self) :\n-        return \"%s: GUID=%s, url=%s\" % ( self.__class__.__name__ , self.guid, self.url )\n-\n-    def _add_single_attributes(self, attrname, attrtype, attributes):\n-        raise NotImplementedError('You must override _add_single_attributes method in order to join %s values into a single value.' % attrtype.__name__)\n-\n-    def attribute(self, attrname, *values, **kwargs):\n-        \"\"\"\n-        Set the provided values to the provided attribute (`attrname`) by filtering them\n-        through its adaptor pipeline first (if any).\n-\n-        If the attribute had been already set, it won't be overwritten unless the parameters\n-        `override` or `add` are True.\n-\n-        If `override` is True, the old value simply will be replaced with the new one.\n-\n-        If add is True (and there was an old value, of course), for multivalued attributes the values\n-        will be appended to the list of the already existing ones.\n-        For this same situation in single-valued attributes, the method _add_single_attributes will be\n-        called with the attribute's name and type, and the list of values to join as parameters.\n-\n-        The kwargs parameter is passed to the adaptors pipeline, which manages to transmit\n-        it to the adaptors themselves.\n-        \"\"\"\n-        def _clean_values(values):\n-            ret = []\n-            for val in values:\n-                if isinstance(val, tuple):\n-                    ret.extend(val)\n-                elif val:\n-                    ret.append(val)\n-            return ret\n-\n-        if not values:\n-            raise ValueError(\"You must specify at least one value when setting an attribute\")\n-        if attrname not in self.ATTRIBUTES:\n-            raise AttributeError('Attribute \"%s\" is not a valid attribute name. You must add it to %s.ATTRIBUTES' % (attrname, self.__class__.__name__))\n-\n-        add = kwargs.pop('add', False)\n-        override = kwargs.pop('override', False)\n-        unique_vals = kwargs.pop('unique', False)\n-        old_value = getattr(self, attrname, None)\n-        if old_value and not any([override, add]):\n-            return\n-\n-        attrtype = self.ATTRIBUTES.get(attrname)\n-        multivalued = isinstance(attrtype, list)\n-        adaptors_pipe = self._adaptors_dict.get(attrname)\n-        new_values = [adaptors_pipe(value, kwargs) for value in values] if adaptors_pipe else [values]\n-        new_values = _clean_values(new_values)\n-\n-        if old_value and not override:\n-            if multivalued:\n-                new_values = old_value + new_values\n-            else:\n-                new_values.insert(0, old_value)\n-\n-        if not multivalued:\n-            if add and len(new_values) > 1:\n-                new_values = self._add_single_attributes(attrname, attrtype, new_values)\n-            else:\n-                new_values = new_values[0] if new_values else None\n-        elif multivalued and unique_vals:\n-            new_values = unique(new_values)\n-\n-        setattr(self, attrname, new_values)\n-\n-    def set_attrib_adaptors(self, attrib, pipe):\n-        \"\"\" Set the adaptors (from a list or tuple) to be used for a specific attribute. \"\"\"\n-        self._adaptors_dict[attrib] = pipe if isinstance(pipe, AdaptorPipe) else AdaptorPipe(pipe)\n-\n-    def set_adaptors(self, adaptors_dict):\n-        \"\"\"\n-        Set the adaptors to use for this item. Receives a dict of the adaptors\n-        desired for each attribute and returns the item itself.\n-        \"\"\"\n-        self._adaptors_dict = {}\n-        for attrib, pipe in adaptors_dict.items():\n-            self.set_attrib_adaptors(attrib, pipe)\n-        return self\n-\n-    def add_adaptor(self, attrib, adaptor, position=None):\n-        \"\"\"\n-        Add an adaptor for the specified attribute at the given position.\n-        If position = None, then the adaptor is appended at the end of the pipeline.\n-        \"\"\"\n-        pipe = self._adaptors_dict.get(attrib, AdaptorPipe([]))\n-        pipe.add_adaptor(adaptor, position)\n-        self.set_attrib_adaptors(attrib, pipe)\n-\n-    def copy(self):\n-        \"\"\"Create a new RobustScrapedItem based on the current one\"\"\"\n-        return copy.deepcopy(self)\n-\n-    def validate(self):\n-        \"\"\"Method used to validate item attributes data\"\"\"\n-        raise NotImplemented\n-\n-    @property\n-    def version(self):\n-        \"\"\"\n-        Return a (cached) 40 char hash of all the item attributes.\n-\n-        WARNING: This cached version won't work if mutable products are\n-        modified directly like:\n-\n-        item.features.append('feature')\n-        \"\"\"\n-        if getattr(self, '_version', None):\n-            return self._version\n-        hash_ = hashlib.sha1()\n-        hash_.update(\"\".join([\"\".join([n, str(v)]) for n, v in sorted(self.__dict__.iteritems())]))\n-        return hash_.hexdigest()\n-\n-\n-class RobustItemDelta(object):\n-    \"\"\"\n-    This class represents the difference between\n-    a pair of RobustScrapedItems.\n-    \"\"\"\n-\n-    def __init__(self, old_item, new_item):\n-        if not isinstance(old_item, RobustScrapedItem) or \\\n-           not isinstance(new_item, RobustScrapedItem):\n-            raise TypeError(\"Both arguments must be RobustScrapedItem instances\")\n-\n-        if old_item.guid != new_item.guid:\n-            raise AttributeError(\"Item GUIDs must be equal in order to create a RobustItemDelta object\")\n-\n-        self.old_item = old_item\n-        self.new_item = new_item\n-        self.diff = self.do_diff()\n-\n-    def do_diff(self):\n-        \"\"\"\n-        This method should retreive a dictionary\n-        containing the changes between both items\n-        as in this example:\n-\n-        >>> delta.do_diff()\n-        >>> {'attrib': {'new': 'New value', 'old': 'Old value'}, # Common attributes\n-             'attrib2': {'new': 'New value 2', 'old': 'Old value 2'},\n-             'attrib3': [{'new': 'New list value', 'old': 'Old list value'}, # List attributes\n-                         {'new': 'New list value 2', 'old': 'Old list value 2'}]}\n-        \"\"\"\n-\n-        if self.old_item == self.new_item:\n-            return {}\n-\n-        diff = {}\n-        for key, value in self.old_item.__dict__.items():\n-            if key in self.old_item.ATTRIBUTES.keys():\n-                new_value = getattr(self.new_item, key)\n-                if value != new_value:\n-                    diff[key] = {'new': new_value, 'old': value}\n-        for key, value in self.new_item.__dict__.items():\n-            if value and key in self.new_item.ATTRIBUTES.keys():\n-                if not getattr(self.old_item, key):\n-                    diff[key] = {'new': value, 'old': None}\n-        return diff\n-\n-    def __eq__(self, other):\n-        if isinstance(other, RobustItemDelta):\n-            if other.old_item == self.old_item and \\\n-               other.new_item == self.new_item and \\\n-               other.diff == self.diff:\n-                return True\n-        return False\n-\n-    def __nonzero__(self):\n-        return bool(self.diff)\n-\n-    def __repr__(self):\n-        if self.diff:\n-            pp = PrettyPrinter(indent=3)\n-            return pp.pformat(self.diff)\n-        else:\n-            return 'No differences found between the provided items.'\n-\n\n@@ -1,55 +0,0 @@\n-# -*- coding: utf8 -*-\n-import unittest\n-\n-from scrapy.contrib.item import RobustScrapedItem\n-\n-class MyItem(RobustScrapedItem):\n-    ATTRIBUTES = {\n-        'guid': basestring,\n-        'name': basestring,\n-        'children': [basestring],\n-        'age': int,\n-        'alive': bool,\n-    }\n-\n-class RobustScrapedItemTestCase(unittest.TestCase):\n-    def setUp(self):\n-        self.item = MyItem()\n-\n-    def test_attribute_basic(self):\n-        self.assertRaises(ValueError, self.item.attribute, 'foo')\n-        self.assertRaises(AttributeError, self.item.attribute, 'foo', 'something')\n-\n-        self.item.attribute('name', 'John')\n-        self.assertEqual(self.item.name, 'John')\n-\n-        self.assertRaises(TypeError, self.item.attribute, 'children', ['Peter'])\n-        self.item.attribute('children', *['Peter'])\n-        self.assertEqual(self.item.children, ['Peter'])\n-\n-        self.item.attribute('age', 40)\n-        self.assertEqual(self.item.age, 40)\n-\n-        self.item.attribute('alive', False)\n-        self.assertEqual(self.item.alive, False)\n-\n-    def test_attribute_override(self):\n-        self.item.attribute('name', 'John')\n-        self.item.attribute('name', 'Charlie')\n-        self.assertEqual(self.item.name, 'John')\n-\n-        self.item.attribute('name', 'Charlie', override=True)\n-        self.assertEqual(self.item.name, 'Charlie')\n-\n-    def test_attribute_add(self):\n-        self.item.attribute('name', 'John')\n-        self.assertRaises(NotImplementedError, self.item.attribute, 'name', 'Doe', add=True)\n-\n-        self.item.attribute('children', *['Ken', 'Tom'])\n-        self.item.attribute('children', 'Bobby')\n-        self.item.attribute('children', 'Jimmy', add=True)\n-        self.assertEqual(self.item.children, ['Ken', 'Tom', 'Jimmy'])\n-\n-        self.item.attribute('children', 'Johnny', 'Rodrigo', add=True)\n-        self.assertEqual(self.item.children, ['Ken', 'Tom', 'Jimmy', 'Johnny', 'Rodrigo'])\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d8722c8c3459500a59144dcc1b5bf4661f5f6d55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 32 | Files Changed: 2 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 55 | Churn Cumulative: 225 | Contributors (this commit): 3 | Commits (past 90d): 32 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -119,6 +119,7 @@ MEMUSAGE_REPORT = False\n MEMUSAGE_WARNING_MB = 0\n \n MYSQL_CONNECTION_SETTINGS = {}\n+MYSQL_CONNECTION_PING_PERIOD = 0\n \n NEWSPIDER_MODULE = ''\n \n\n@@ -1,21 +1,18 @@\n-\"\"\"\n-Function for dealing with databases\n-\"\"\"\n+\"\"\"Functions for dealing with databases\"\"\"\n+\n import re\n+\n+import MySQLdb\n+\n+from scrapy.core.engine import scrapyengine\n from scrapy.conf import settings\n from scrapy import log\n \n-class URIValidationError(Exception):\n-    pass\n+mysql_uri_re = r\"mysql:\\/\\/(?P<user>[^:]+)(:(?P<passwd>[^@]+))?@(?P<host>[^/:]+)(:(?P<port>\\d+))?/(?P<db>.*)$\"\n \n-def parse_uri(db_uri):\n-    \"\"\"\n-    Parse mysql URI and return settings dict\n-    \"\"\"\n-    if not (isinstance(db_uri, basestring) and db_uri.startswith('mysql://')):\n-        raise URIValidationError(\"Incorrect MySQL URI: %s\" % db_uri)\n-\n-    m = re.search(r\"mysql:\\/\\/(?P<user>[^:]+)(:(?P<passwd>[^@]+))?@(?P<host>[^/:]+)(:(?P<port>\\d+))?/(?P<db>.*)$\", db_uri)\n+def parse_uri(mysql_uri):\n+    \"\"\"Parse mysql URI and return a dict with its parameters\"\"\"\n+    m = re.search(mysql_uri_re, mysql_uri)\n     if m:\n         d = m.groupdict()\n         if d['passwd'] is None:\n@@ -27,33 +24,26 @@ def parse_uri(db_uri):\n         return d\n \n def mysql_connect(db_uri_or_dict, **kwargs):\n-    \"\"\"\n-    Connects to a MySQL DB given a mysql URI\n-    \"\"\"\n-    import MySQLdb\n-    from scrapy.core.engine import scrapyengine\n-\n+    \"\"\"Connects to a MySQL DB given a mysql URI\"\"\"\n     if isinstance(db_uri_or_dict, dict):\n         d = db_uri_or_dict\n     else:\n         d = parse_uri(db_uri_or_dict)\n-\n-    if d:\n-        d['charset'] = 'utf8'\n+    if not d:\n+        return\n     d.update(settings.get(\"MYSQL_CONNECTION_SETTINGS\", {}))\n     d.update(kwargs)\n-\n-        dcopy = d.copy()\n-        if dcopy.get(\"passwd\"):\n-            dcopy[\"passwd\"] = \"********\"\n-        log.msg(\"Connecting db with settings %s\" % dcopy )\n-\n+    log.msg(\"Connecting to MySQL: db=%r, host=%r, user=%r\" % (d['db'], \\\n+        d['host'], d['user']), level=log.DEBUG)\n     conn = MySQLdb.connect(**d)\n \n-        #this is to maintain active the connection\n-        def _ping():\n-            log.msg(\"Pinging connection to %s/%s\" % (d.get('host'), d.get('db')) )\n+    # connection keep-alive\n+    def conn_ping():\n+        log.msg(\"Pinging connection to %s/%s\" % (d['host'], d['db']), \\\n+            level=log.DEBUG)\n         conn.ping()\n-        scrapyengine.addtask(_ping, settings.getint(\"MYSQL_CONNECTION_PING_PERIOD\", 600))\n+    ping_period = settings.getint(\"MYSQL_CONNECTION_PING_PERIOD\")\n+    if ping_period:\n+        scrapyengine.addtask(conn_ping, ping_period)\n \n     return conn\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d6f4e382aea78eb63fc1815bc22a6c7030971c2d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#23d0c08c65bb1f31d41a50ce7dac117ae945c700", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 6 | Churn Cumulative: 66 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,6 @@\n+import warnings\n+\n+from scrapy.utils.mysql import *\n+\n+warnings.warn(\"scrapy.utils.db module is depreacted, use scrapy.utils.mysql instead\",\n+    DeprecationWarning, stacklevel=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5494237cbbda77107ada1314057b97ebcea466bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 27 | Files Changed: 1 | Hunks: 28 | Methods Changed: 13 | Complexity Δ (Sum/Max): 437/437 | Churn Δ: 88 | Churn Cumulative: 88 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.4666666666666667\n\nDIFF:\n@@ -42,7 +42,7 @@ http://www.crummy.com/software/BeautifulSoup/documentation.html\n \n Here, have some legalese:\n \n-Copyright (c) 2004-2007, Leonard Richardson\n+Copyright (c) 2004-2008, Leonard Richardson\n \n All rights reserved.\n \n@@ -79,12 +79,13 @@ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE, DAMMIT.\n from __future__ import generators\n \n __author__ = \"Leonard Richardson (leonardr@segfault.org)\"\n-__version__ = \"3.0.6\"\n+__version__ = \"3.0.7a\"\n __copyright__ = \"Copyright (c) 2004-2008 Leonard Richardson\"\n __license__ = \"New-style BSD\"\n \n from sgmllib import SGMLParser, SGMLParseError\n import codecs\n+import markupbase\n import types\n import re\n import sgmllib\n@@ -92,9 +93,14 @@ try:\n   from htmlentitydefs import name2codepoint\n except ImportError:\n   name2codepoint = {}\n+try:\n+    set\n+except NameError:\n+    from sets import Set as set\n \n-#This hack makes Beautiful Soup able to parse XML with namespaces\n+#These hacks make Beautiful Soup able to parse XML with namespaces\n sgmllib.tagfind = re.compile('[a-zA-Z][-_.:a-zA-Z0-9]*')\n+markupbase._declname_match = re.compile(r'[a-zA-Z][-_.:a-zA-Z0-9]*\\s*').match\n \n DEFAULT_OUTPUT_ENCODING = \"utf-8\"\n \n@@ -391,6 +397,18 @@ class PageElement:\n \n class NavigableString(unicode, PageElement):\n \n+    def __new__(cls, value):\n+        \"\"\"Create a new NavigableString.\n+\n+        When unpickling a NavigableString, this method is called with\n+        the string in DEFAULT_OUTPUT_ENCODING. That encoding needs to be\n+        passed in to the superclass's __new__ or the superclass won't know\n+        how to handle non-ASCII characters.\n+        \"\"\"\n+        if isinstance(value, unicode):\n+            return unicode.__new__(cls, value)\n+        return unicode.__new__(cls, value, DEFAULT_OUTPUT_ENCODING)\n+\n     def __getnewargs__(self):\n         return (NavigableString.__str__(self),)\n \n@@ -982,6 +1000,7 @@ class BeautifulStoneSoup(Tag, SGMLParser):\n     NESTABLE_TAGS = {}\n     RESET_NESTING_TAGS = {}\n     QUOTE_TAGS = {}\n+    PRESERVE_WHITESPACE_TAGS = []\n \n     MARKUP_MASSAGE = [(re.compile('(<[^<>]*)/>'),\n                        lambda x: x.group(1) + ' />'),\n@@ -1005,7 +1024,7 @@ class BeautifulStoneSoup(Tag, SGMLParser):\n \n     def __init__(self, markup=\"\", parseOnlyThese=None, fromEncoding=None,\n                  markupMassage=True, smartQuotesTo=XML_ENTITIES,\n-                 convertEntities=None, selfClosingTags=None):\n+                 convertEntities=None, selfClosingTags=None, isHTML=False):\n         \"\"\"The Soup object is initialized as the 'root tag', and the\n         provided markup (which can be a string or a file-like object)\n         is fed into the underlying parser.\n@@ -1067,7 +1086,7 @@ class BeautifulStoneSoup(Tag, SGMLParser):\n         self.markup = markup\n         self.markupMassage = markupMassage\n         try:\n-            self._feed()\n+            self._feed(isHTML=isHTML)\n         except StopParsing:\n             pass\n         self.markup = None                 # The markup can now be GCed\n@@ -1082,7 +1101,7 @@ class BeautifulStoneSoup(Tag, SGMLParser):\n             return\n         return self.convert_codepoint(n)\n \n-    def _feed(self, inDocumentEncoding=None):\n+    def _feed(self, inDocumentEncoding=None, isHTML=False):\n         # Convert the document to Unicode.\n         markup = self.markup\n         if isinstance(markup, unicode):\n@@ -1091,9 +1110,10 @@ class BeautifulStoneSoup(Tag, SGMLParser):\n         else:\n             dammit = UnicodeDammit\\\n                      (markup, [self.fromEncoding, inDocumentEncoding],\n-                      smartQuotesTo=self.smartQuotesTo)\n+                      smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)\n             markup = dammit.unicode\n             self.originalEncoding = dammit.originalEncoding\n+            self.declaredHTMLEncoding = dammit.declaredHTMLEncoding\n         if markup:\n             if self.markupMassage:\n                 if not isList(self.markupMassage):\n@@ -1166,8 +1186,10 @@ class BeautifulStoneSoup(Tag, SGMLParser):\n \n     def endData(self, containerClass=NavigableString):\n         if self.currentData:\n-            currentData = ''.join(self.currentData)\n-            if not currentData.translate(self.STRIP_ASCII_SPACES):\n+            currentData = u''.join(self.currentData)\n+            if (currentData.translate(self.STRIP_ASCII_SPACES) == '' and\n+                not set([tag.name for tag in self.tagStack]).intersection(\n+                    self.PRESERVE_WHITESPACE_TAGS)):\n                 if '\\n' in currentData:\n                     currentData = '\\n'\n                 else:\n@@ -1444,12 +1466,15 @@ class BeautifulSoup(BeautifulStoneSoup):\n     def __init__(self, *args, **kwargs):\n         if not kwargs.has_key('smartQuotesTo'):\n             kwargs['smartQuotesTo'] = self.HTML_ENTITIES\n+        kwargs['isHTML'] = True\n         BeautifulStoneSoup.__init__(self, *args, **kwargs)\n \n     SELF_CLOSING_TAGS = buildTagMap(None,\n                                     ['br' , 'hr', 'input', 'img', 'meta',\n                                     'spacer', 'link', 'frame', 'base'])\n \n+    PRESERVE_WHITESPACE_TAGS = set(['pre', 'textarea'])\n+\n     QUOTE_TAGS = {'script' : None, 'textarea' : None}\n \n     #According to the HTML standard, each of these inline tags can\n@@ -1494,7 +1519,7 @@ class BeautifulSoup(BeautifulStoneSoup):\n                                 NESTABLE_LIST_TAGS, NESTABLE_TABLE_TAGS)\n \n     # Used to detect the charset in a META tag; see start_meta\n-    CHARSET_RE = re.compile(\"((^|;)\\s*charset=)([^;]*)\")\n+    CHARSET_RE = re.compile(\"((^|;)\\s*charset=)([^;]*)\", re.M)\n \n     def start_meta(self, attrs):\n         \"\"\"Beautiful Soup can detect a charset included in a META tag,\n@@ -1517,25 +1542,28 @@ class BeautifulSoup(BeautifulStoneSoup):\n         if httpEquiv and contentType: # It's an interesting meta tag.\n             match = self.CHARSET_RE.search(contentType)\n             if match:\n-                if getattr(self, 'declaredHTMLEncoding') or \\\n-                       (self.originalEncoding == self.fromEncoding):\n-                    # This is our second pass through the document, or\n-                    # else an encoding was specified explicitly and it\n-                    # worked. Rewrite the meta tag.\n-                    newAttr = self.CHARSET_RE.sub\\\n-                              (lambda(match):match.group(1) +\n-                               \"%SOUP-ENCODING%\", contentType)\n+                if (self.declaredHTMLEncoding is not None or\n+                    self.originalEncoding == self.fromEncoding):\n+                    # An HTML encoding was sniffed while converting\n+                    # the document to Unicode, or an HTML encoding was\n+                    # sniffed during a previous pass through the\n+                    # document, or an encoding was specified\n+                    # explicitly and it worked. Rewrite the meta tag.\n+                    def rewrite(match):\n+                        return match.group(1) + \"%SOUP-ENCODING%\"\n+                    newAttr = self.CHARSET_RE.sub(rewrite, contentType)\n                     attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],\n                                                newAttr)\n                     tagNeedsEncodingSubstitution = True\n                 else:\n                     # This is our first pass through the document.\n-                    # Go through it again with the new information.\n+                    # Go through it again with the encoding information.\n                     newCharset = match.group(3)\n                     if newCharset and newCharset != self.originalEncoding:\n                         self.declaredHTMLEncoding = newCharset\n                         self._feed(self.declaredHTMLEncoding)\n                         raise StopParsing\n+                    pass\n         tag = self.unknown_starttag(\"meta\", attrs)\n         if tag and tagNeedsEncodingSubstitution:\n             tag.containsSubstitutions = True\n@@ -1687,9 +1715,10 @@ class UnicodeDammit:\n                         \"x-sjis\" : \"shift-jis\" }\n \n     def __init__(self, markup, overrideEncodings=[],\n-                 smartQuotesTo='xml'):\n+                 smartQuotesTo='xml', isHTML=False):\n+        self.declaredHTMLEncoding = None\n         self.markup, documentEncoding, sniffedEncoding = \\\n-                     self._detectEncoding(markup)\n+                     self._detectEncoding(markup, isHTML)\n         self.smartQuotesTo = smartQuotesTo\n         self.triedEncodings = []\n         if markup == '' or isinstance(markup, unicode):\n@@ -1715,6 +1744,7 @@ class UnicodeDammit:\n             for proposed_encoding in (\"utf-8\", \"windows-1252\"):\n                 u = self._convertFrom(proposed_encoding)\n                 if u: break\n+\n         self.unicode = u\n         if not u: self.originalEncoding = None\n \n@@ -1782,7 +1812,7 @@ class UnicodeDammit:\n         newdata = unicode(data, encoding)\n         return newdata\n \n-    def _detectEncoding(self, xml_data):\n+    def _detectEncoding(self, xml_data, isHTML=False):\n         \"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n         xml_encoding = sniffed_xml_encoding = None\n         try:\n@@ -1830,13 +1860,17 @@ class UnicodeDammit:\n             else:\n                 sniffed_xml_encoding = 'ascii'\n                 pass\n-            xml_encoding_match = re.compile \\\n-                                 ('^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>')\\\n-                                 .match(xml_data)\n         except:\n             xml_encoding_match = None\n-        if xml_encoding_match:\n+        xml_encoding_match = re.compile(\n+            '^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>').match(xml_data)\n+        if not xml_encoding_match and isHTML:\n+            regexp = re.compile('<\\s*meta[^>]+charset=([^>]*?)[;\\'\">]', re.I)\n+            xml_encoding_match = regexp.search(xml_data)\n+        if xml_encoding_match is not None:\n             xml_encoding = xml_encoding_match.groups()[0].lower()\n+            if isHTML:\n+                self.declaredHTMLEncoding = xml_encoding\n             if sniffed_xml_encoding and \\\n                (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n                                  'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n@@ -1927,5 +1961,5 @@ class UnicodeDammit:\n #By default, act as an HTML pretty-printer.\n if __name__ == '__main__':\n     import sys\n-    soup = BeautifulSoup(sys.stdin.read())\n+    soup = BeautifulSoup(sys.stdin)\n     print soup.prettify()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
