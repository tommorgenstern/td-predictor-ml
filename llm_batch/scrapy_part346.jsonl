{"custom_id": "scrapy#0a21a9457b7aeafef3b9ee1c0206546d6c8fb294", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 1150 | Contributors (this commit): 31 | Commits (past 90d): 9 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -37,7 +37,7 @@ from scrapy.utils.request import referer_str\n logger = logging.getLogger(__name__)\n \n \n-def _to_string(path: Union[str, PathLike]):\n+def _to_string(path: Union[str, PathLike]) -> str:\n     return str(path)  # convert a Path object to string\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3054235dc09b1667c5c897f976eafa18845283e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 2 | Churn Cumulative: 223 | Contributors (this commit): 16 | Commits (past 90d): 5 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -38,6 +38,8 @@ class RobotsTxtMiddleware:\n     def process_request(self, request, spider):\n         if request.meta.get(\"dont_obey_robotstxt\"):\n             return\n+        if request.url.startswith(\"data:\") or request.url.startswith(\"file:\"):\n+            return\n         d = maybeDeferred(self.robot_parser, request, spider)\n         d.addCallback(self.process_request_2, request, spider)\n         return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#33b85a9e2a379b355398e2daf416130bb840167d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 589 | Contributors (this commit): 18 | Commits (past 90d): 4 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -214,6 +214,19 @@ Disallow: /some/randome/page.html\n         middleware.process_request_2(rp, Request(\"http://site.local/allowed\"), None)\n         rp.allowed.assert_called_once_with(\"http://site.local/allowed\", \"Examplebot\")\n \n+    def test_robotstxt_local_file(self):\n+        middleware = RobotsTxtMiddleware(self._get_emptybody_crawler())\n+        assert not middleware.process_request(\n+            Request(\"data:text/plain,Hello World data\"), None\n+        )\n+        assert not middleware.process_request(\n+            Request(\"file:///tests/sample_data/test_site/nothinghere.html\"), None\n+        )\n+        assert isinstance(\n+            middleware.process_request(Request(\"http://site.local/allowed\"), None),\n+            Deferred,\n+        )\n+\n     def assertNotIgnored(self, request, middleware):\n         spider = None  # not actually used\n         dfd = maybeDeferred(middleware.process_request, request, spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#94161d101cd47d2dac4be7e8325a24f0ddea86cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1160 | Contributors (this commit): 31 | Commits (past 90d): 10 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -36,7 +36,7 @@ from scrapy.utils.request import referer_str\n logger = logging.getLogger(__name__)\n \n \n-def _to_string(path: Union[str, os.PathLike[str]]):\n+def _to_string(path: Union[str, os.PathLike]) -> str:\n     return str(path)  # convert a Path object to string\n \n \n@@ -45,10 +45,10 @@ class FileException(Exception):\n \n \n class FSFilesStore:\n-    def __init__(self, basedir: Union[str, os.PathLike[str]]):\n+    def __init__(self, basedir: Union[str, os.PathLike]):\n         basedir = _to_string(basedir)\n-        if '://' in basedir:\n-            basedir = basedir.split('://', 1)[1]\n+        if \"://\" in basedir:\n+            basedir = basedir.split(\"://\", 1)[1]\n         self.basedir = basedir\n         self._mkdir(Path(self.basedir))\n         self.created_directories: DefaultDict[str, Set[str]] = defaultdict(set)\n@@ -70,7 +70,7 @@ class FSFilesStore:\n \n         return {'last_modified': last_modified, 'checksum': checksum}\n \n-    def _get_filesystem_path(self, path: Union[str, os.PathLike[str]]) -> Path:\n+    def _get_filesystem_path(self, path: Union[str, os.PathLike]) -> Path:\n         path_comps = _to_string(path).split('/')\n         return Path(self.basedir, *path_comps)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a1e2fbafdcaae9b70cf7c4216c5cb80d1df5268e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 9 | Churn Cumulative: 1189 | Contributors (this commit): 24 | Commits (past 90d): 5 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -484,19 +484,18 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n             self.assertEqual(getattr(pipeline_cls, pipe_inst_attr), expected_value)\n \n     def test_file_pipeline_using_pathlike_objects(self):\n-\n         class CustomFilesPipelineWithPathLikeDir(FilesPipeline):\n             def file_path(self, request, response=None, info=None, *, item=None):\n-                return Path('subdir') / Path(request.url).name\n+                return Path(\"subdir\") / Path(request.url).name\n \n         pipeline = CustomFilesPipelineWithPathLikeDir.from_settings(\n-            Settings({'FILES_STORE': Path('./Temp')})\n+            Settings({\"FILES_STORE\": Path(\"./Temp\")})\n         )\n         request = Request(\"http://example.com/image01.jpg\")\n-        self.assertEqual(pipeline.file_path(request), Path('subdir/image01.jpg'))\n+        self.assertEqual(pipeline.file_path(request), Path(\"subdir/image01.jpg\"))\n \n     def test_files_store_constructor_with_pathlike_object(self):\n-        path = Path('./FileDir')\n+        path = Path(\"./FileDir\")\n         fs_store = FSFilesStore(path)\n         self.assertEqual(fs_store.basedir, str(path))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f03b47db05e623189cf7719e647d18e8457494f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 32 | Files Changed: 3 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 61 | Churn Cumulative: 4050 | Contributors (this commit): 57 | Commits (past 90d): 29 | Contributors (cumulative): 83 | DMM Complexity: 0.14285714285714285\n\nDIFF:\n@@ -5,10 +5,8 @@ requests in Scrapy.\n See documentation in docs/topics/request-response.rst\n \"\"\"\n import inspect\n-from enum import Enum\n from typing import Callable, List, Optional, Tuple, Type, TypeVar, Union\n \n-from typing_extensions import Final\n from w3lib.url import safe_url_string\n \n import scrapy\n@@ -23,21 +21,22 @@ from scrapy.utils.url import escape_ajax\n RequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n \n \n-# https://github.com/python/typing/issues/689#issuecomment-561425237\n-class NoCallbackType(Enum):\n-    NO_CALLBACK = 0\n+def NO_CALLBACK(*args, **kwargs):\n+    \"\"\"When assigned to the ``callback`` parameter of\n+    :class:`~scrapy.http.Request`, it indicates that the request is not meant\n+    to have a spider callback at all.\n \n-\n-#: When assigned to the ``callback`` parameter of\n-#: :class:`~scrapy.http.Request`, it indicates that the request is not meant to\n-#: have a spider callback at all.\n-#:\n-#: This value should be used by :ref:`components <topics-components>`\n-#: that create and handle their own requests, e.g. through\n-#: :meth:`scrapy.core.engine.ExecutionEngine.download`, so that download\n-#: middlewares handling such requests can treat them differently from requests\n-#: intended for the :meth:`~scrapy.Spider.parse` callback.\n-NO_CALLBACK: Final = NoCallbackType.NO_CALLBACK\n+    This value should be used by :ref:`components <topics-components>` that\n+    create and handle their own requests, e.g. through\n+    :meth:`scrapy.core.engine.ExecutionEngine.download`, so that download\n+    middlewares handling such requests can treat them differently from requests\n+    intended for the :meth:`~scrapy.Spider.parse` callback.\n+    \"\"\"\n+    raise RuntimeError(\n+        \"The NO_CALLBACK callback has been called. This is a special callback \"\n+        \"value intended for requests whose callback is never meant to be \"\n+        \"called.\"\n+    )\n \n \n class Request(object_ref):\n@@ -67,8 +66,6 @@ class Request(object_ref):\n     Currently used by :meth:`Request.replace`, :meth:`Request.to_dict` and\n     :func:`~scrapy.utils.request.request_from_dict`.\n     \"\"\"\n-    callback: Union[None, NoCallbackType, Callable]\n-    errback: Optional[Callable]\n \n     def __init__(\n         self,\n@@ -94,8 +91,14 @@ class Request(object_ref):\n             raise TypeError(f\"Request priority not an integer: {priority!r}\")\n         self.priority = priority\n \n-        self._set_xback(\"callback\", callback)\n-        self._set_xback(\"errback\", errback)\n+        if not (callable(callback) or callback is None):\n+            raise TypeError(\n+                f\"callback must be a callable, got {type(callback).__name__}\"\n+            )\n+        if not (callable(errback) or errback is None):\n+            raise TypeError(f\"errback must be a callable, got {type(errback).__name__}\")\n+        self.callback = callback\n+        self.errback = errback\n \n         self.cookies = cookies or {}\n         self.headers = Headers(headers or {}, encoding=encoding)\n@@ -105,15 +108,6 @@ class Request(object_ref):\n         self._cb_kwargs = dict(cb_kwargs) if cb_kwargs else None\n         self.flags = [] if flags is None else list(flags)\n \n-    def _set_xback(self, name: str, value: Optional[Callable]) -> None:\n-        if not (\n-            callable(value)\n-            or value is None\n-            or (name == \"callback\" and value is NO_CALLBACK)\n-        ):\n-            raise TypeError(f\"{name} must be a callable, got {type(value).__name__}\")\n-        setattr(self, name, value)\n-\n     @property\n     def cb_kwargs(self) -> dict:\n         if self._cb_kwargs is None:\n\n@@ -34,7 +34,6 @@ install_requires = [\n     \"packaging\",\n     \"tldextract\",\n     \"lxml>=4.3.0\",\n-    \"typing-extensions>=3.10.0.0\",\n ]\n extras_require = {}\n cpython_dependencies = [\n\n@@ -314,8 +314,10 @@ class RequestTest(unittest.TestCase):\n         r5 = self.request_class(\n             url=\"http://example.com\",\n             callback=NO_CALLBACK,\n+            errback=NO_CALLBACK,\n         )\n         self.assertIs(r5.callback, NO_CALLBACK)\n+        self.assertIs(r5.errback, NO_CALLBACK)\n \n     def test_callback_and_errback_type(self):\n         with self.assertRaises(TypeError):\n@@ -328,8 +330,10 @@ class RequestTest(unittest.TestCase):\n                 callback=\"a_function\",\n                 errback=\"a_function\",\n             )\n-        with self.assertRaises(TypeError):\n-            self.request_class(\"http://example.com\", errback=NO_CALLBACK)\n+\n+    def test_no_callback(self):\n+        with self.assertRaises(RuntimeError):\n+            NO_CALLBACK()\n \n     def test_from_curl(self):\n         # Note: more curated tests regarding curl conversion are in\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c0efb271a23218656b5f629fde6d23c115a0c3de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 20 | Files Changed: 7 | Hunks: 17 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 40 | Churn Cumulative: 3315 | Contributors (this commit): 41 | Commits (past 90d): 38 | Contributors (cumulative): 97 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ class H2DownloadHandler:\n \n         from twisted.internet import reactor\n \n-        self._pool = H2ConnectionPool(reactor, settings)  # type: ignore[arg-type]\n+        self._pool = H2ConnectionPool(reactor, settings)\n         self._context_factory = load_context_factory_from_settings(settings, crawler)\n \n     @classmethod\n@@ -82,7 +82,7 @@ class ScrapyH2Agent:\n                     \"Tunneling via CONNECT method using HTTP/2.0 is not yet supported\"\n                 )\n             return self._ProxyAgent(\n-                reactor=reactor,  # type: ignore[arg-type]\n+                reactor=reactor,\n                 context_factory=self._context_factory,\n                 proxy_uri=URI.fromBytes(to_bytes(proxy, encoding=\"ascii\")),\n                 connect_timeout=timeout,\n@@ -91,7 +91,7 @@ class ScrapyH2Agent:\n             )\n \n         return self._Agent(\n-            reactor=reactor,  # type: ignore[arg-type]\n+            reactor=reactor,\n             context_factory=self._context_factory,\n             connect_timeout=timeout,\n             bind_address=bind_address,\n@@ -108,7 +108,7 @@ class ScrapyH2Agent:\n         d = agent.request(request, spider)\n         d.addCallback(self._cb_latency, request, start_time)\n \n-        timeout_cl = reactor.callLater(timeout, d.cancel)  # type: ignore[attr-defined]\n+        timeout_cl = reactor.callLater(timeout, d.cancel)\n         d.addBoth(self._cb_timeout, request, timeout, timeout_cl)\n         return d\n \n\n@@ -37,11 +37,11 @@ if __name__ == \"__main__\":\n \n     root = Root()\n     factory = Site(root)\n-    httpPort = reactor.listenTCP(8998, Site(root))  # type: ignore[attr-defined]\n+    httpPort = reactor.listenTCP(8998, Site(root))\n \n     def _print_listening():\n         httpHost = httpPort.getHost()\n         print(f\"Bench server at http://{httpHost.host}:{httpHost.port}\")\n \n-    reactor.callWhenRunning(_print_listening)  # type: ignore[attr-defined]\n-    reactor.run()  # type: ignore[attr-defined]\n+    reactor.callWhenRunning(_print_listening)\n+    reactor.run()\n\n@@ -40,7 +40,7 @@ def defer_fail(_failure: Failure) -> Deferred:\n     from twisted.internet import reactor\n \n     d: Deferred = Deferred()\n-    reactor.callLater(0.1, d.errback, _failure)  # type: ignore[attr-defined]\n+    reactor.callLater(0.1, d.errback, _failure)\n     return d\n \n \n@@ -54,7 +54,7 @@ def defer_succeed(result) -> Deferred:\n     from twisted.internet import reactor\n \n     d: Deferred = Deferred()\n-    reactor.callLater(0.1, d.callback, result)  # type: ignore[attr-defined]\n+    reactor.callLater(0.1, d.callback, result)\n     return d\n \n \n\n@@ -50,6 +50,6 @@ def test_site():\n if __name__ == \"__main__\":\n     from twisted.internet import reactor\n \n-    port = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")  # type: ignore[attr-defined]\n+    port = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n     print(f\"http://localhost:{port.getHost().port}/\")\n-    reactor.run()  # type: ignore[attr-defined]\n+    reactor.run()\n\n@@ -41,10 +41,10 @@ if __name__ == \"__main__\":\n         url = f\"http://not.a.real.domain:{port}/echo\"\n \n         servers = [(mock_dns_server.host, mock_dns_server.port)]\n-        reactor.installResolver(createResolver(servers=servers))  # type: ignore[attr-defined]\n+        reactor.installResolver(createResolver(servers=servers))\n \n         configure_logging()\n         runner = CrawlerRunner()\n         d = runner.crawl(LocalhostSpider, url=url)\n-        d.addBoth(lambda _: reactor.stop())  # type: ignore[attr-defined]\n-        reactor.run()  # type: ignore[attr-defined]\n+        d.addBoth(lambda _: reactor.stop())\n+        reactor.run()\n\n@@ -375,9 +375,9 @@ if __name__ == \"__main__\":\n     if args.type == \"http\":\n         root = Root()\n         factory = Site(root)\n-        httpPort = reactor.listenTCP(0, factory)  # type: ignore[attr-defined]\n+        httpPort = reactor.listenTCP(0, factory)\n         contextFactory = ssl_context_factory()\n-        httpsPort = reactor.listenSSL(0, factory, contextFactory)  # type: ignore[attr-defined]\n+        httpsPort = reactor.listenSSL(0, factory, contextFactory)\n \n         def print_listening():\n             httpHost = httpPort.getHost()\n@@ -391,11 +391,11 @@ if __name__ == \"__main__\":\n         clients = [MockDNSResolver()]\n         factory = DNSServerFactory(clients=clients)\n         protocol = dns.DNSDatagramProtocol(controller=factory)\n-        listener = reactor.listenUDP(0, protocol)  # type: ignore[attr-defined]\n+        listener = reactor.listenUDP(0, protocol)\n \n         def print_listening():\n             host = listener.getHost()\n             print(f\"{host.host}:{host.port}\")\n \n-    reactor.callWhenRunning(print_listening)  # type: ignore[attr-defined]\n-    reactor.run()  # type: ignore[attr-defined]\n+    reactor.callWhenRunning(print_listening)\n+    reactor.run()\n\n@@ -557,4 +557,4 @@ class EngineTest(unittest.TestCase):\n if __name__ == \"__main__\":\n     if len(sys.argv) > 1 and sys.argv[1] == \"runserver\":\n         start_test_site(debug=True)\n-        reactor.run()  # type: ignore[attr-defined]\n+        reactor.run()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ef794251f6bd238986c4e90763521a0b3ac02dc6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 429 | Contributors (this commit): 16 | Commits (past 90d): 3 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -8,11 +8,10 @@ import warnings\n \n from twisted import version as _txv\n \n+# Declare top-level shortcuts\n from scrapy.http import FormRequest, Request\n from scrapy.item import Field, Item\n from scrapy.selector import Selector\n-\n-# Declare top-level shortcuts\n from scrapy.spiders import Spider\n \n __all__ = [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#80453d53b19bbe288dc1e5b721f2f29acb89706a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 842 | Contributors (this commit): 22 | Commits (past 90d): 8 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -85,7 +85,7 @@ def mustbe_deferred(f: Callable, *args, **kw) -> Deferred:\n \n def parallel(\n     iterable: Iterable, count: int, callable: Callable, *args, **named\n-) -> DeferredList:\n+) -> Deferred:\n     \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n     using no more than ``count`` concurrent calls.\n \n@@ -202,11 +202,11 @@ class _AsyncCooperatorAdapter(Iterator):\n \n def parallel_async(\n     async_iterable: AsyncIterable, count: int, callable: Callable, *args, **named\n-) -> DeferredList:\n+) -> Deferred:\n     \"\"\"Like parallel but for async iterators\"\"\"\n     coop = Cooperator()\n     work = _AsyncCooperatorAdapter(async_iterable, callable, *args, **named)\n-    dl = DeferredList([coop.coiterate(work) for _ in range(count)])\n+    dl: Deferred = DeferredList([coop.coiterate(work) for _ in range(count)])\n     return dl\n \n \n@@ -245,7 +245,7 @@ def process_parallel(callbacks: Iterable[Callable], input, *a, **kw) -> Deferred\n     callbacks\n     \"\"\"\n     dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n-    d = DeferredList(dfds, fireOnOneErrback=True, consumeErrors=True)\n+    d: Deferred = DeferredList(dfds, fireOnOneErrback=True, consumeErrors=True)\n     d.addCallbacks(lambda r: [x[1] for x in r], lambda f: f.value.subFailure)\n     return d\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e1699479f6e48ce87dea1e6ed5661fea9ca7b1aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 652 | Contributors (this commit): 18 | Commits (past 90d): 12 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -27,7 +27,7 @@ def NO_CALLBACK(*args, **kwargs):\n \n     This value should be used by :ref:`components <topics-components>` that\n     create and handle their own requests, e.g. through\n-    :meth:`scrapy.core.engine.ExecutionEngine.download`, so that download\n+    :meth:`scrapy.core.engine.ExecutionEngine.download`, so that downloader\n     middlewares handling such requests can treat them differently from requests\n     intended for the :meth:`~scrapy.Spider.parse` callback.\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#389fd99e79374bad73faf98424c97ac804eb1a68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 15 | Churn Cumulative: 2328 | Contributors (this commit): 44 | Commits (past 90d): 22 | Contributors (cumulative): 71 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,6 +22,7 @@ from twisted.internet import defer, threads\n \n from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Request\n+from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.media import MediaPipeline\n from scrapy.settings import Settings\n from scrapy.utils.boto import is_botocore_available\n@@ -517,7 +518,7 @@ class FilesPipeline(MediaPipeline):\n     # Overridable Interface\n     def get_media_requests(self, item, info):\n         urls = ItemAdapter(item).get(self.files_urls_field, [])\n-        return [Request(u) for u in urls]\n+        return [Request(u, callback=NO_CALLBACK) for u in urls]\n \n     def file_downloaded(self, response, request, info, *, item=None):\n         path = self.file_path(request, response=response, info=info, item=item)\n\n@@ -13,6 +13,7 @@ from itemadapter import ItemAdapter\n \n from scrapy.exceptions import DropItem, NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request\n+from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.files import FileException, FilesPipeline\n \n # TODO: from scrapy.pipelines.media import MediaPipeline\n@@ -214,7 +215,7 @@ class ImagesPipeline(FilesPipeline):\n \n     def get_media_requests(self, item, info):\n         urls = ItemAdapter(item).get(self.images_urls_field, [])\n-        return [Request(u) for u in urls]\n+        return [Request(u, callback=NO_CALLBACK) for u in urls]\n \n     def item_completed(self, results, item, info):\n         with suppress(KeyError):\n\n@@ -18,6 +18,10 @@ from scrapy.utils.log import failure_to_exc_info\n logger = logging.getLogger(__name__)\n \n \n+def _DUMMY_CALLBACK(response):\n+    return response\n+\n+\n class MediaPipeline:\n \n     LOG_FAILED_RESULTS = True\n@@ -91,7 +95,10 @@ class MediaPipeline:\n \n     def _process_request(self, request, info, item):\n         fp = self._fingerprinter.fingerprint(request)\n-        cb = request.callback or (lambda _: _)\n+        if not request.callback or request.callback is NO_CALLBACK:\n+            cb = _DUMMY_CALLBACK\n+        else:\n+            cb = request.callback\n         eb = request.errback\n         request.callback = NO_CALLBACK\n         request.errback = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8c8894f4bec66c2af7d8dbbbf448109ff5dea22d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2395 | Contributors (this commit): 29 | Commits (past 90d): 6 | Contributors (cumulative): 29 | DMM Complexity: None\n\nDIFF:\n@@ -505,7 +505,7 @@ class GenspiderCommandTest(CommandTest):\n         # change name of spider but not its file name\n         with file_path.open(\"r+\", encoding=\"utf-8\") as spider_file:\n             file_data = spider_file.read()\n-            file_data = file_data.replace(\"name = 'example'\", \"name = 'renamed'\")\n+            file_data = file_data.replace('name = \"example\"', 'name = \"renamed\"')\n             spider_file.seek(0)\n             spider_file.write(file_data)\n             spider_file.truncate()\n@@ -538,14 +538,14 @@ class GenspiderCommandTest(CommandTest):\n             domain,\n             self.find_in_file(\n                 Path(self.proj_mod_path, \"spiders\", \"test_name.py\"),\n-                r\"allowed_domains\\s*=\\s*\\[\\'(.+)\\'\\]\",\n+                r\"allowed_domains\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n             ).group(1),\n         )\n         self.assertEqual(\n             f\"http://{domain}/\",\n             self.find_in_file(\n                 Path(self.proj_mod_path, \"spiders\", \"test_name.py\"),\n-                r\"start_urls\\s*=\\s*\\[\\'(.+)\\'\\]\",\n+                r\"start_urls\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n             ).group(1),\n         )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#068af85722a41a1361e170f104e64fccfff662b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 658 | Contributors (this commit): 18 | Commits (past 90d): 13 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -25,6 +25,12 @@ def NO_CALLBACK(*args, **kwargs):\n     :class:`~scrapy.http.Request`, it indicates that the request is not meant\n     to have a spider callback at all.\n \n+    For example:\n+\n+    .. code-block:: python\n+\n+       Request(\"https://example.com\", callback=NO_CALLBACK)\n+\n     This value should be used by :ref:`components <topics-components>` that\n     create and handle their own requests, e.g. through\n     :meth:`scrapy.core.engine.ExecutionEngine.download`, so that downloader\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2f2bcb006d349eeeed10018362c780496be96550", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 14 | Methods Changed: 10 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 1732 | Contributors (this commit): 23 | Commits (past 90d): 9 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -248,10 +248,7 @@ class CsvItemExporter(BaseItemExporter):\n         self.csv_writer.writerow(values)\n \n     def finish_exporting(self):\n-        # Detaching stream in order to avoid file closing.\n-        # The file will be closed with slot.storage.store\n-        # https://github.com/scrapy/scrapy/issues/5043\n-        self.stream.detach()\n+        self.stream.detach()  # Avoid closing the wrapped file.\n \n     def _build_row(self, values):\n         for s in values:\n\n@@ -85,6 +85,10 @@ class BaseItemExporterTest(unittest.TestCase):\n             if self.ie.__class__ is not BaseItemExporter:\n                 raise\n         self.ie.finish_exporting()\n+        # Delete the item exporter object, so that if it causes the output\n+        # file handle be closed, which should not be the case, follow-up\n+        # interactions with the output file handle will surface the issue.\n+        del self.ie\n         self._check_output()\n \n     def test_export_item(self):\n@@ -230,6 +234,7 @@ class PickleItemExporterTest(BaseItemExporterTest):\n         ie.export_item(i1)\n         ie.export_item(i2)\n         ie.finish_exporting()\n+        del ie  # See the first “del self.ie” in this file for context.\n         f.seek(0)\n         self.assertEqual(self.item_class(**pickle.load(f)), i1)\n         self.assertEqual(self.item_class(**pickle.load(f)), i2)\n@@ -241,6 +246,7 @@ class PickleItemExporterTest(BaseItemExporterTest):\n         ie.start_exporting()\n         ie.export_item(item)\n         ie.finish_exporting()\n+        del ie  # See the first “del self.ie” in this file for context.\n         self.assertEqual(pickle.loads(fp.getvalue()), item)\n \n \n@@ -267,6 +273,7 @@ class MarshalItemExporterTest(BaseItemExporterTest):\n         ie.start_exporting()\n         ie.export_item(item)\n         ie.finish_exporting()\n+        del ie  # See the first “del self.ie” in this file for context.\n         fp.seek(0)\n         self.assertEqual(marshal.load(fp), item)\n \n@@ -299,6 +306,7 @@ class CsvItemExporterTest(BaseItemExporterTest):\n         ie.start_exporting()\n         ie.export_item(item)\n         ie.finish_exporting()\n+        del ie  # See the first “del self.ie” in this file for context.\n         self.assertCsvEqual(fp.getvalue(), expected)\n \n     def test_header_export_all(self):\n@@ -330,6 +338,7 @@ class CsvItemExporterTest(BaseItemExporterTest):\n             ie.export_item(item)\n             ie.export_item(item)\n             ie.finish_exporting()\n+            del ie  # See the first “del self.ie” in this file for context.\n             self.assertCsvEqual(output.getvalue(),\n                                 b'age,name\\r\\n22,John\\xc2\\xa3\\r\\n22,John\\xc2\\xa3\\r\\n')\n \n@@ -414,6 +423,7 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         ie.start_exporting()\n         ie.export_item(item)\n         ie.finish_exporting()\n+        del ie  # See the first “del self.ie” in this file for context.\n         self.assertXmlEquivalent(fp.getvalue(), expected_value)\n \n     def _check_output(self):\n@@ -520,6 +530,7 @@ class JsonLinesItemExporterTest(BaseItemExporterTest):\n         self.ie.start_exporting()\n         self.ie.export_item(i3)\n         self.ie.finish_exporting()\n+        del self.ie  # See the first “del self.ie” in this file for context.\n         exported = json.loads(to_unicode(self.output.getvalue()))\n         self.assertEqual(exported, self._expected_nested)\n \n@@ -534,6 +545,7 @@ class JsonLinesItemExporterTest(BaseItemExporterTest):\n         self.ie.start_exporting()\n         self.ie.export_item(item)\n         self.ie.finish_exporting()\n+        del self.ie  # See the first “del self.ie” in this file for context.\n         exported = json.loads(to_unicode(self.output.getvalue()))\n         item['time'] = str(item['time'])\n         self.assertEqual(exported, item)\n@@ -561,6 +573,7 @@ class JsonItemExporterTest(JsonLinesItemExporterTest):\n         self.ie.export_item(item)\n         self.ie.export_item(item)\n         self.ie.finish_exporting()\n+        del self.ie  # See the first “del self.ie” in this file for context.\n         exported = json.loads(to_unicode(self.output.getvalue()))\n         self.assertEqual(exported, [ItemAdapter(item).asdict(), ItemAdapter(item).asdict()])\n \n@@ -577,6 +590,7 @@ class JsonItemExporterTest(JsonLinesItemExporterTest):\n         self.ie.start_exporting()\n         self.ie.export_item(i3)\n         self.ie.finish_exporting()\n+        del self.ie  # See the first “del self.ie” in this file for context.\n         exported = json.loads(to_unicode(self.output.getvalue()))\n         expected = {'name': 'Jesus', 'age': {'name': 'Maria', 'age': ItemAdapter(i1).asdict()}}\n         self.assertEqual(exported, [expected])\n@@ -588,6 +602,7 @@ class JsonItemExporterTest(JsonLinesItemExporterTest):\n         self.ie.start_exporting()\n         self.ie.export_item(i3)\n         self.ie.finish_exporting()\n+        del self.ie  # See the first “del self.ie” in this file for context.\n         exported = json.loads(to_unicode(self.output.getvalue()))\n         expected = {'name': 'Jesus', 'age': {'name': 'Maria', 'age': i1}}\n         self.assertEqual(exported, [expected])\n@@ -597,6 +612,7 @@ class JsonItemExporterTest(JsonLinesItemExporterTest):\n         self.ie.start_exporting()\n         self.ie.export_item(item)\n         self.ie.finish_exporting()\n+        del self.ie  # See the first “del self.ie” in this file for context.\n         exported = json.loads(to_unicode(self.output.getvalue()))\n         item['time'] = str(item['time'])\n         self.assertEqual(exported, [item])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#426f3ebb7b368084f6e77ccf8a121c85c7913049", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1166 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -92,7 +92,7 @@ class BaseItemExporterTest(unittest.TestCase):\n                 raise\n         self.ie.finish_exporting()\n         # Delete the item exporter object, so that if it causes the output\n-        # file handle be closed, which should not be the case, follow-up\n+        # file handle to be closed, which should not be the case, follow-up\n         # interactions with the output file handle will surface the issue.\n         del self.ie\n         self._check_output()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b07d3f85a3896f0c2d923d759e46e057c817e97d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 159 | Files Changed: 79 | Hunks: 159 | Methods Changed: 42 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 160 | Churn Cumulative: 56383 | Contributors (this commit): 196 | Commits (past 90d): 339 | Contributors (cumulative): 1170 | DMM Complexity: None\n\nDIFF:\n@@ -18,7 +18,6 @@ from pathlib import Path\n \n \n def main():\n-\n     # Used for remembering the file (and its contents)\n     # so we don't have to open the same file again.\n     _filename = None\n@@ -50,7 +49,6 @@ def main():\n             else:\n                 # If this is a new file\n                 if newfilename != _filename:\n-\n                     # Update the previous file\n                     if _filename:\n                         Path(_filename).write_text(_contents, encoding=\"utf-8\")\n\n@@ -13,7 +13,6 @@ from scrapy.spiders import Spider\n \n \n class QPSSpider(Spider):\n-\n     name = \"qps\"\n     benchurl = \"http://localhost:8880/\"\n \n\n@@ -14,7 +14,6 @@ from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n \n \n class ScrapyCommand:\n-\n     requires_project = False\n     crawler_process: Optional[CrawlerProcess] = None\n \n\n@@ -9,7 +9,6 @@ from scrapy.linkextractors import LinkExtractor\n \n \n class Command(ScrapyCommand):\n-\n     default_settings = {\n         \"LOG_LEVEL\": \"INFO\",\n         \"LOGSTATS_INTERVAL\": 1,\n\n@@ -3,7 +3,6 @@ from scrapy.exceptions import UsageError\n \n \n class Command(BaseRunSpiderCommand):\n-\n     requires_project = True\n \n     def syntax(self):\n\n@@ -6,7 +6,6 @@ from scrapy.exceptions import UsageError\n \n \n class Command(ScrapyCommand):\n-\n     requires_project = True\n     default_settings = {\"LOG_ENABLED\": False}\n \n\n@@ -10,7 +10,6 @@ from scrapy.utils.spider import DefaultSpider, spidercls_for_request\n \n \n class Command(ScrapyCommand):\n-\n     requires_project = False\n \n     def syntax(self):\n\n@@ -32,7 +32,6 @@ def extract_domain(url):\n \n \n class Command(ScrapyCommand):\n-\n     requires_project = False\n     default_settings = {\"LOG_ENABLED\": False}\n \n\n@@ -2,7 +2,6 @@ from scrapy.commands import ScrapyCommand\n \n \n class Command(ScrapyCommand):\n-\n     requires_project = True\n     default_settings = {\"LOG_ENABLED\": False}\n \n\n@@ -24,7 +24,6 @@ def _import_file(filepath: Union[str, PathLike]) -> ModuleType:\n \n \n class Command(BaseRunSpiderCommand):\n-\n     requires_project = False\n     default_settings = {\"SPIDER_LOADER_WARN_ONLY\": True}\n \n\n@@ -5,7 +5,6 @@ from scrapy.settings import BaseSettings\n \n \n class Command(ScrapyCommand):\n-\n     requires_project = False\n     default_settings = {\"LOG_ENABLED\": False, \"SPIDER_LOADER_WARN_ONLY\": True}\n \n\n@@ -13,7 +13,6 @@ from scrapy.utils.url import guess_scheme\n \n \n class Command(ScrapyCommand):\n-\n     requires_project = False\n     default_settings = {\n         \"KEEP_ALIVE\": True,\n\n@@ -28,7 +28,6 @@ def _make_writable(path):\n \n \n class Command(ScrapyCommand):\n-\n     requires_project = False\n     default_settings = {\"LOG_ENABLED\": False, \"SPIDER_LOADER_WARN_ONLY\": True}\n \n\n@@ -4,7 +4,6 @@ from scrapy.utils.versions import scrapy_components_versions\n \n \n class Command(ScrapyCommand):\n-\n     default_settings = {\"LOG_ENABLED\": False, \"SPIDER_LOADER_WARN_ONLY\": True}\n \n     def syntax(self):\n\n@@ -69,7 +69,6 @@ def _get_concurrency_delay(concurrency, spider, settings):\n \n \n class Downloader:\n-\n     DOWNLOAD_SLOT = \"download_slot\"\n \n     def __init__(self, crawler):\n\n@@ -292,7 +292,6 @@ class ScrapyProxyAgent(Agent):\n \n \n class ScrapyAgent:\n-\n     _Agent = Agent\n     _ProxyAgent = ScrapyProxyAgent\n     _TunnelingAgent = TunnelingAgent\n\n@@ -17,7 +17,6 @@ from scrapy.utils.defer import deferred_from_coro, mustbe_deferred\n \n \n class DownloaderMiddlewareManager(MiddlewareManager):\n-\n     component_name = \"downloader middleware\"\n \n     @classmethod\n\n@@ -40,7 +40,6 @@ def _parse(url):\n \n \n class ScrapyHTTPPageGetter(HTTPClient):\n-\n     delimiter = b\"\\n\"\n \n     def connectionMade(self):\n@@ -103,7 +102,6 @@ class ScrapyHTTPPageGetter(HTTPClient):\n # Twisted (https://github.com/twisted/twisted/pull/643), we merged its\n # non-overridden code into this class.\n class ScrapyHTTPClientFactory(ClientFactory):\n-\n     protocol = ScrapyHTTPPageGetter\n \n     waiting = 1\n\n@@ -46,7 +46,6 @@ def _isiterable(o) -> bool:\n \n \n class SpiderMiddlewareManager(MiddlewareManager):\n-\n     component_name = \"spider middleware\"\n \n     def __init__(self, *middlewares):\n\n@@ -30,7 +30,6 @@ class AjaxCrawlMiddleware:\n         return cls(crawler.settings)\n \n     def process_response(self, request, response, spider):\n-\n         if not isinstance(response, HtmlResponse) or response.status != 200:\n             return response\n \n\n@@ -27,7 +27,6 @@ HttpCacheMiddlewareTV = TypeVar(\"HttpCacheMiddlewareTV\", bound=\"HttpCacheMiddlew\n \n \n class HttpCacheMiddleware:\n-\n     DOWNLOAD_EXCEPTIONS = (\n         defer.TimeoutError,\n         TimeoutError,\n\n@@ -53,7 +53,6 @@ class HttpCompressionMiddleware:\n         request.headers.setdefault(\"Accept-Encoding\", b\", \".join(ACCEPTED_ENCODINGS))\n \n     def process_response(self, request, response, spider):\n-\n         if request.method == \"HEAD\":\n             return response\n         if isinstance(response, Response):\n\n@@ -26,7 +26,6 @@ def _build_redirect_request(source_request, *, url, **kwargs):\n \n \n class BaseRedirectMiddleware:\n-\n     enabled_setting = \"REDIRECT_ENABLED\"\n \n     def __init__(self, settings):\n@@ -115,7 +114,6 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n \n \n class MetaRefreshMiddleware(BaseRedirectMiddleware):\n-\n     enabled_setting = \"METAREFRESH_ENABLED\"\n \n     def __init__(self, settings):\n\n@@ -122,7 +122,6 @@ def get_retry_request(\n \n \n class RetryMiddleware:\n-\n     # IOError is raised by the HttpCompression middleware when trying to\n     # decompress an empty response\n     EXCEPTIONS_TO_RETRY = (\n\n@@ -8,7 +8,6 @@ from scrapy.utils.conf import build_component_list\n \n \n class ExtensionManager(MiddlewareManager):\n-\n     component_name = \"extension\"\n \n     @classmethod\n\n@@ -41,7 +41,6 @@ class DummyPolicy:\n \n \n class RFC2616Policy:\n-\n     MAXAGE = 3600 * 24 * 365  # one year\n \n     def __init__(self, settings):\n\n@@ -15,7 +15,6 @@ from scrapy.utils.deprecate import create_deprecated_class\n \n \n class JsonRequest(Request):\n-\n     attributes: Tuple[str, ...] = Request.attributes + (\"dumps_kwargs\",)\n \n     def __init__(self, *args, dumps_kwargs: Optional[dict] = None, **kwargs) -> None:\n\n@@ -29,7 +29,6 @@ _NONE = object()\n \n \n class TextResponse(Response):\n-\n     _DEFAULT_ENCODING = \"ascii\"\n     _cached_decoded_json = _NONE\n \n\n@@ -194,7 +194,6 @@ class LxmlLinkExtractor:\n         return True\n \n     def matches(self, url):\n-\n         if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains):\n             return False\n         if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):\n\n@@ -10,7 +10,6 @@ from scrapy.utils.defer import deferred_f_from_coro_f\n \n \n class ItemPipelineManager(MiddlewareManager):\n-\n     component_name = \"item pipeline\"\n \n     @classmethod\n\n@@ -187,7 +187,6 @@ class S3FilesStore:\n \n \n class GCSFilesStore:\n-\n     GCS_PROJECT_ID = None\n \n     CACHE_CONTROL = \"max-age=172800\"\n@@ -253,7 +252,6 @@ class GCSFilesStore:\n \n \n class FTPFilesStore:\n-\n     FTP_USERNAME = None\n     FTP_PASSWORD = None\n     USE_ACTIVE_MODE = None\n\n@@ -23,7 +23,6 @@ def _DUMMY_CALLBACK(response):\n \n \n class MediaPipeline:\n-\n     LOG_FAILED_RESULTS = True\n \n     class SpiderInfo:\n\n@@ -12,7 +12,6 @@ from scrapy.utils.python import binary_is_text, to_bytes, to_unicode\n \n \n class ResponseTypes:\n-\n     CLASSES = {\n         \"text/html\": \"scrapy.http.HtmlResponse\",\n         \"application/atom+xml\": \"scrapy.http.XmlResponse\",\n\n@@ -25,7 +25,6 @@ from scrapy.utils.response import open_in_browser\n \n \n class Shell:\n-\n     relevant_classes = (Crawler, Spider, Request, Response, Settings)\n \n     def __init__(self, crawler, update_vars=None, code=None):\n\n@@ -34,7 +34,6 @@ POLICY_SCRAPY_DEFAULT = \"scrapy-default\"\n \n \n class ReferrerPolicy:\n-\n     NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES\n     name: str\n \n\n@@ -60,7 +60,6 @@ class Rule:\n \n \n class CrawlSpider(Spider):\n-\n     rules: Sequence[Rule] = ()\n \n     def __init__(self, *a, **kw):\n\n@@ -89,7 +89,7 @@ class XMLFeedSpider(Spider):\n             yield node\n \n     def _register_namespaces(self, selector):\n-        for (prefix, uri) in self.namespaces:\n+        for prefix, uri in self.namespaces:\n             selector.register_namespace(prefix, uri)\n \n \n\n@@ -10,7 +10,6 @@ logger = logging.getLogger(__name__)\n \n \n class SitemapSpider(Spider):\n-\n     sitemap_urls = ()\n     sitemap_rules = [(\"\", \"parse\")]\n     sitemap_follow = [\"\"]\n\n@@ -6,7 +6,6 @@ from twisted.web.server import Site\n \n \n class Root(Resource):\n-\n     isLeaf = True\n \n     def getChild(self, name, request):\n\n@@ -11,7 +11,6 @@ from collections.abc import Mapping\n \n \n class CaselessDict(dict):\n-\n     __slots__ = ()\n \n     def __init__(self, seq=None):\n\n@@ -54,7 +54,6 @@ def create_deprecated_class(\n     \"\"\"\n \n     class DeprecatedClass(new_class.__class__):\n-\n         deprecated_class = None\n         warned_on_subclass = False\n \n\n@@ -9,7 +9,6 @@ from scrapy.http import Request, Response\n \n \n class ScrapyJSONEncoder(json.JSONEncoder):\n-\n     DATE_FORMAT = \"%Y-%m-%d\"\n     TIME_FORMAT = \"%H:%M:%S\"\n \n\n@@ -5,7 +5,6 @@ from twisted.internet import defer, protocol\n \n \n class ProcessTest:\n-\n     command = None\n     prefix = [sys.executable, \"-m\", \"scrapy.cmdline\"]\n     cwd = os.getcwd()  # trial chdirs to temp dir\n\n@@ -144,7 +144,6 @@ def strip_url(\n     origin_only=False,\n     strip_fragment=True,\n ):\n-\n     \"\"\"Strip URL string from some of its components:\n \n     - ``strip_credentials`` removes \"user:password@\"\n\n@@ -95,7 +95,6 @@ class BrokenDownloadResource(resource.Resource):\n \n \n class LeafResource(resource.Resource):\n-\n     isLeaf = True\n \n     def deferRequest(self, request, delay, f, *a, **kw):\n\n@@ -25,7 +25,6 @@ class MockServerSpider(Spider):\n \n \n class MetaSpider(MockServerSpider):\n-\n     name = \"meta\"\n \n     def __init__(self, *args, **kwargs):\n@@ -37,7 +36,6 @@ class MetaSpider(MockServerSpider):\n \n \n class FollowAllSpider(MetaSpider):\n-\n     name = \"follow\"\n     link_extractor = LinkExtractor()\n \n@@ -59,7 +57,6 @@ class FollowAllSpider(MetaSpider):\n \n \n class DelaySpider(MetaSpider):\n-\n     name = \"delay\"\n \n     def __init__(self, n=1, b=0, *args, **kwargs):\n@@ -81,7 +78,6 @@ class DelaySpider(MetaSpider):\n \n \n class SimpleSpider(MetaSpider):\n-\n     name = \"simple\"\n \n     def __init__(self, url=\"http://localhost:8998\", *args, **kwargs):\n@@ -93,7 +89,6 @@ class SimpleSpider(MetaSpider):\n \n \n class AsyncDefSpider(SimpleSpider):\n-\n     name = \"asyncdef\"\n \n     async def parse(self, response):\n@@ -102,7 +97,6 @@ class AsyncDefSpider(SimpleSpider):\n \n \n class AsyncDefAsyncioSpider(SimpleSpider):\n-\n     name = \"asyncdef_asyncio\"\n \n     async def parse(self, response):\n@@ -112,7 +106,6 @@ class AsyncDefAsyncioSpider(SimpleSpider):\n \n \n class AsyncDefAsyncioReturnSpider(SimpleSpider):\n-\n     name = \"asyncdef_asyncio_return\"\n \n     async def parse(self, response):\n@@ -123,7 +116,6 @@ class AsyncDefAsyncioReturnSpider(SimpleSpider):\n \n \n class AsyncDefAsyncioReturnSingleElementSpider(SimpleSpider):\n-\n     name = \"asyncdef_asyncio_return_single_element\"\n \n     async def parse(self, response):\n@@ -134,7 +126,6 @@ class AsyncDefAsyncioReturnSingleElementSpider(SimpleSpider):\n \n \n class AsyncDefAsyncioReqsReturnSpider(SimpleSpider):\n-\n     name = \"asyncdef_asyncio_reqs_return\"\n \n     async def parse(self, response):\n@@ -191,7 +182,6 @@ class AsyncDefDeferredMaybeWrappedSpider(SimpleSpider):\n \n \n class AsyncDefAsyncioGenSpider(SimpleSpider):\n-\n     name = \"asyncdef_asyncio_gen\"\n \n     async def parse(self, response):\n@@ -201,7 +191,6 @@ class AsyncDefAsyncioGenSpider(SimpleSpider):\n \n \n class AsyncDefAsyncioGenLoopSpider(SimpleSpider):\n-\n     name = \"asyncdef_asyncio_gen_loop\"\n \n     async def parse(self, response):\n@@ -212,7 +201,6 @@ class AsyncDefAsyncioGenLoopSpider(SimpleSpider):\n \n \n class AsyncDefAsyncioGenComplexSpider(SimpleSpider):\n-\n     name = \"asyncdef_asyncio_gen_complex\"\n     initial_reqs = 4\n     following_reqs = 3\n@@ -246,7 +234,6 @@ class AsyncDefAsyncioGenComplexSpider(SimpleSpider):\n \n \n class ItemSpider(FollowAllSpider):\n-\n     name = \"item\"\n \n     def parse(self, response):\n@@ -261,7 +248,6 @@ class DefaultError(Exception):\n \n \n class ErrorSpider(FollowAllSpider):\n-\n     name = \"error\"\n     exception_cls = DefaultError\n \n@@ -275,7 +261,6 @@ class ErrorSpider(FollowAllSpider):\n \n \n class BrokenStartRequestsSpider(FollowAllSpider):\n-\n     fail_before_yield = False\n     fail_yielding = False\n \n@@ -305,7 +290,6 @@ class BrokenStartRequestsSpider(FollowAllSpider):\n \n \n class SingleRequestSpider(MetaSpider):\n-\n     seed = None\n     callback_func = None\n     errback_func = None\n@@ -451,7 +435,6 @@ class CrawlSpiderWithProcessRequestCallbackKeywordArguments(CrawlSpiderWithParse\n \n \n class BytesReceivedCallbackSpider(MetaSpider):\n-\n     full_response_length = 2**18\n \n     @classmethod\n\n@@ -2,7 +2,6 @@ from tests.test_commands import CommandTest\n \n \n class CheckCommandTest(CommandTest):\n-\n     command = \"check\"\n \n     def setUp(self):\n\n@@ -6,7 +6,6 @@ from scrapy.utils.testsite import SiteTest\n \n \n class FetchTest(ProcessTest, SiteTest, unittest.TestCase):\n-\n     command = \"fetch\"\n \n     @defer.inlineCallbacks\n\n@@ -9,7 +9,6 @@ from tests import NON_EXISTING_RESOLVABLE, tests_datadir\n \n \n class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n-\n     command = \"shell\"\n \n     @defer.inlineCallbacks\n\n@@ -8,7 +8,6 @@ from scrapy.utils.testproc import ProcessTest\n \n \n class VersionTest(ProcessTest, unittest.TestCase):\n-\n     command = \"version\"\n \n     @defer.inlineCallbacks\n\n@@ -223,7 +223,6 @@ def get_permissions_dict(\n \n \n class StartprojectTemplatesTest(ProjectTest):\n-\n     maxDiff = None\n \n     def setUp(self):\n@@ -604,7 +603,6 @@ class MiscCommandsTest(CommandTest):\n \n \n class RunSpiderCommandTest(CommandTest):\n-\n     spider_filename = \"myspider.py\"\n \n     debug_log_spider = \"\"\"\n@@ -873,7 +871,6 @@ class MySpider(scrapy.Spider):\n \n @skipIf(platform.system() != \"Windows\", \"Windows required for .pyw files\")\n class WindowsRunSpiderCommandTest(RunSpiderCommandTest):\n-\n     spider_filename = \"myspider.pyw\"\n \n     def setUp(self):\n\n@@ -25,7 +25,6 @@ from tests.test_downloader_handlers import (\n \n @skipIf(not H2_ENABLED, \"HTTP/2 support in Twisted is not enabled\")\n class Https2TestCase(Https11TestCase):\n-\n     scheme = \"https\"\n     HTTP2_DATALOSS_SKIP_REASON = \"Content-Length mismatch raises InvalidBodyLengthError\"\n \n\n@@ -16,7 +16,6 @@ from scrapy.utils.test import get_crawler, get_from_asyncio_queue\n \n \n class ManagerTestCase(TestCase):\n-\n     settings_dict = None\n \n     def setUp(self):\n\n@@ -17,7 +17,6 @@ def _test_data(formats):\n \n \n class DecompressionMiddlewareTest(TestCase):\n-\n     test_formats = [\"tar\", \"xml.bz2\", \"xml.gz\", \"zip\"]\n     uncompressed_body, test_responses = _test_data(test_formats)\n \n\n@@ -14,7 +14,6 @@ from scrapy.utils.test import get_crawler\n \n \n class _BaseTest(unittest.TestCase):\n-\n     storage_class = \"scrapy.extensions.httpcache.DbmCacheStorage\"\n     policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n \n@@ -146,12 +145,10 @@ class DefaultStorageTest(_BaseTest):\n \n \n class DbmStorageTest(DefaultStorageTest):\n-\n     storage_class = \"scrapy.extensions.httpcache.DbmCacheStorage\"\n \n \n class DbmStorageWithCustomDbmModuleTest(DbmStorageTest):\n-\n     dbm_module = \"tests.mocks.dummydbm\"\n \n     def _get_settings(self, **new_settings):\n@@ -165,7 +162,6 @@ class DbmStorageWithCustomDbmModuleTest(DbmStorageTest):\n \n \n class FilesystemStorageTest(DefaultStorageTest):\n-\n     storage_class = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n \n \n@@ -176,7 +172,6 @@ class FilesystemStorageGzipTest(FilesystemStorageTest):\n \n \n class DummyPolicyTest(_BaseTest):\n-\n     policy_class = \"scrapy.extensions.httpcache.DummyPolicy\"\n \n     def test_middleware(self):\n@@ -270,7 +265,6 @@ class DummyPolicyTest(_BaseTest):\n \n \n class RFC2616PolicyTest(DefaultStorageTest):\n-\n     policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n \n     def _process_requestresponse(self, mw, request, response):\n\n@@ -13,7 +13,6 @@ spider = Spider(\"foo\")\n \n \n class TestHttpProxyMiddleware(TestCase):\n-\n     failureException = AssertionError\n \n     def setUp(self):\n\n@@ -127,7 +127,6 @@ class RetryTest(unittest.TestCase):\n \n \n class MaxRetryTimesTest(unittest.TestCase):\n-\n     invalid_url = \"http://www.scrapytest.org/invalid_url\"\n \n     def get_spider_and_middleware(self, settings=None):\n\n@@ -243,7 +243,6 @@ class CrawlerRun:\n class EngineTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_crawler(self):\n-\n         for spider in (\n             TestSpider,\n             DictItemsSpider,\n\n@@ -55,7 +55,6 @@ class CustomFieldDataclass:\n \n \n class BaseItemExporterTest(unittest.TestCase):\n-\n     item_class = TestItem\n     custom_field_item_class = CustomFieldItem\n \n@@ -513,13 +512,11 @@ class XmlItemExporterTest(BaseItemExporterTest):\n \n \n class XmlItemExporterDataclassTest(XmlItemExporterTest):\n-\n     item_class = TestDataClass\n     custom_field_item_class = CustomFieldDataclass\n \n \n class JsonLinesItemExporterTest(BaseItemExporterTest):\n-\n     _expected_nested = {\n         \"name\": \"Jesus\",\n         \"age\": {\"name\": \"Maria\", \"age\": {\"name\": \"Joseph\", \"age\": \"22\"}},\n@@ -559,13 +556,11 @@ class JsonLinesItemExporterTest(BaseItemExporterTest):\n \n \n class JsonLinesItemExporterDataclassTest(JsonLinesItemExporterTest):\n-\n     item_class = TestDataClass\n     custom_field_item_class = CustomFieldDataclass\n \n \n class JsonItemExporterTest(JsonLinesItemExporterTest):\n-\n     _expected_nested = [JsonLinesItemExporterTest._expected_nested]\n \n     def _get_exporter(self, **kwargs):\n@@ -627,13 +622,11 @@ class JsonItemExporterTest(JsonLinesItemExporterTest):\n \n \n class JsonItemExporterDataclassTest(JsonItemExporterTest):\n-\n     item_class = TestDataClass\n     custom_field_item_class = CustomFieldDataclass\n \n \n class CustomExporterItemTest(unittest.TestCase):\n-\n     item_class = TestItem\n \n     def setUp(self):\n@@ -664,7 +657,6 @@ class CustomExporterItemTest(unittest.TestCase):\n \n \n class CustomExporterDataclassTest(CustomExporterItemTest):\n-\n     item_class = TestDataClass\n \n \n\n@@ -1068,7 +1068,6 @@ class FeedExportTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_export_multiple_item_classes(self):\n-\n         items = [\n             self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n             self.MyItem2({\"hello\": \"world2\", \"foo\": \"bar2\"}),\n@@ -1711,7 +1710,6 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_gzip_plugin(self):\n-\n         filename = self._named_tempfile(\"gzip_file\")\n \n         settings = {\n@@ -1731,7 +1729,6 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_gzip_plugin_compresslevel(self):\n-\n         filename_to_compressed = {\n             self._named_tempfile(\"compresslevel_0\"): self.get_gzip_compressed(\n                 self.expected, compresslevel=0\n@@ -1839,7 +1836,6 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_lzma_plugin(self):\n-\n         filename = self._named_tempfile(\"lzma_file\")\n \n         settings = {\n@@ -1859,7 +1855,6 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_lzma_plugin_format(self):\n-\n         filename_to_compressed = {\n             self._named_tempfile(\"format_FORMAT_XZ\"): lzma.compress(\n                 self.expected, format=lzma.FORMAT_XZ\n@@ -1893,7 +1888,6 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_lzma_plugin_check(self):\n-\n         filename_to_compressed = {\n             self._named_tempfile(\"check_CHECK_NONE\"): lzma.compress(\n                 self.expected, check=lzma.CHECK_NONE\n@@ -1927,7 +1921,6 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_lzma_plugin_preset(self):\n-\n         filename_to_compressed = {\n             self._named_tempfile(\"preset_PRESET_0\"): lzma.compress(\n                 self.expected, preset=0\n@@ -1986,7 +1979,6 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_bz2_plugin(self):\n-\n         filename = self._named_tempfile(\"bz2_file\")\n \n         settings = {\n@@ -2006,7 +1998,6 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_bz2_plugin_compresslevel(self):\n-\n         filename_to_compressed = {\n             self._named_tempfile(\"compresslevel_1\"): bz2.compress(\n                 self.expected, compresslevel=1\n@@ -2056,7 +2047,6 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_custom_plugin_with_parameter(self):\n-\n         expected = b\"foo\\r\\n\\nbar\\r\\n\\n\"\n         filename = self._named_tempfile(\"newline\")\n \n@@ -2075,7 +2065,6 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_custom_plugin_with_compression(self):\n-\n         expected = b\"foo\\r\\n\\nbar\\r\\n\\n\"\n \n         filename_to_decompressor = {\n@@ -2555,7 +2544,6 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         ]\n \n         class CustomS3FeedStorage(S3FeedStorage):\n-\n             stubs = []\n \n             def open(self, *args, **kwargs):\n@@ -2828,7 +2816,6 @@ class FTPFeedStoragePreFeedOptionsTest(unittest.TestCase):\n \n \n class URIParamsTest:\n-\n     spider_name = \"uri_params_spider\"\n     deprecated_options = False\n \n\n@@ -19,7 +19,6 @@ from scrapy.utils.python import to_bytes, to_unicode\n \n \n class RequestTest(unittest.TestCase):\n-\n     request_class = Request\n     default_method = \"GET\"\n     default_headers = {}\n@@ -424,7 +423,6 @@ class RequestTest(unittest.TestCase):\n \n \n class FormRequestTest(RequestTest):\n-\n     request_class = FormRequest\n \n     def assertQueryEqual(self, first, second, msg=None):\n@@ -1447,7 +1445,6 @@ def _qs(req, encoding=\"utf-8\", to_unicode=False):\n \n \n class XmlRpcRequestTest(RequestTest):\n-\n     request_class = XmlRpcRequest\n     default_method = \"POST\"\n     default_headers = {b\"Content-Type\": [b\"text/xml\"]}\n\n@@ -23,7 +23,6 @@ from tests import get_testdata\n \n \n class BaseResponseTest(unittest.TestCase):\n-\n     response_class = Response\n \n     def test_init(self):\n@@ -349,7 +348,6 @@ class BaseResponseTest(unittest.TestCase):\n \n \n class TextResponseTest(BaseResponseTest):\n-\n     response_class = TextResponse\n \n     def test_replace(self):\n@@ -835,11 +833,9 @@ class TextResponseTest(BaseResponseTest):\n \n \n class HtmlResponseTest(TextResponseTest):\n-\n     response_class = HtmlResponse\n \n     def test_html_encoding(self):\n-\n         body = b\"\"\"<html><head><title>Some page</title>\n         <meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">\n         </head><body>Price: \\xa3100</body></html>'\n@@ -878,7 +874,6 @@ class HtmlResponseTest(TextResponseTest):\n \n \n class XmlResponseTest(TextResponseTest):\n-\n     response_class = XmlResponse\n \n     def test_xml_encoding(self):\n\n@@ -87,7 +87,6 @@ class BasicItemLoaderTest(unittest.TestCase):\n \n \n class InitializationTestMixin:\n-\n     item_class = None\n \n     def test_keep_single_value(self):\n\n@@ -469,7 +469,6 @@ class BasicItemLoaderTest(unittest.TestCase):\n \n \n class InitializationFromDictTest(unittest.TestCase):\n-\n     item_class = dict\n \n     def test_keep_single_value(self):\n\n@@ -16,7 +16,6 @@ from tests.spiders import ItemSpider\n \n \n class CustomItem(Item):\n-\n     name = Field()\n \n     def __str__(self):\n\n@@ -116,7 +116,6 @@ class FileDownloadCrawlTestCase(TestCase):\n                 self.assertTrue((self.tmpmediastore / i[\"path\"]).exists())\n \n     def _assert_files_download_failure(self, crawler, items, code, logs):\n-\n         # check that the item does NOT have the \"images/files\" field populated\n         self.assertEqual(len(items), 1)\n         self.assertIn(self.media_key, items[0])\n@@ -205,7 +204,6 @@ else:\n \n \n class ImageDownloadCrawlTestCase(FileDownloadCrawlTestCase):\n-\n     skip = skip_pillow\n \n     pipeline_class = \"scrapy.pipelines.images.ImagesPipeline\"\n\n@@ -33,7 +33,6 @@ else:\n \n \n class ImagesPipelineTestCase(unittest.TestCase):\n-\n     skip = skip_pillow\n \n     def setUp(self):\n@@ -325,7 +324,6 @@ class DeprecatedImagesPipeline(ImagesPipeline):\n \n \n class ImagesPipelineTestCaseFieldsMixin:\n-\n     skip = skip_pillow\n \n     def test_item_fields_default(self):\n@@ -420,7 +418,6 @@ class ImagesPipelineTestCaseFieldsAttrsItem(\n \n \n class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n-\n     skip = skip_pillow\n \n     img_cls_attribute_names = [\n\n@@ -37,7 +37,6 @@ def _mocked_download_func(request, info):\n \n \n class BaseMediaPipelineTestCase(unittest.TestCase):\n-\n     pipeline_class = MediaPipeline\n     settings = None\n \n@@ -213,7 +212,6 @@ class MockedMediaPipeline(MediaPipeline):\n \n \n class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n-\n     pipeline_class = MockedMediaPipeline\n \n     def _callback(self, result):\n\n@@ -154,7 +154,6 @@ class KeywordArgumentsSpider(MockServerSpider):\n \n \n class CallbackKeywordArgumentsTestCase(TestCase):\n-\n     maxDiff = None\n \n     def setUp(self):\n\n@@ -43,7 +43,6 @@ class MockDownloader:\n \n class MockCrawler(Crawler):\n     def __init__(self, priority_queue_cls, jobdir):\n-\n         settings = dict(\n             SCHEDULER_DEBUG=False,\n             SCHEDULER_DISK_QUEUE=\"scrapy.squeues.PickleLifoDiskQueue\",\n@@ -325,7 +324,6 @@ class TestIntegrationWithDownloaderAwareInMemory(TestCase):\n     @defer.inlineCallbacks\n     def test_integration_downloader_aware_priority_queue(self):\n         with MockServer() as mockserver:\n-\n             url = mockserver.url(\"/status?n=200\", is_secure=False)\n             start_urls = [url] * 6\n             yield self.crawler.crawl(start_urls)\n\n@@ -93,7 +93,6 @@ class BaseSettingsTest(unittest.TestCase):\n         with mock.patch.object(attr, \"__setattr__\") as mock_setattr, mock.patch.object(\n             attr, \"set\"\n         ) as mock_set:\n-\n             self.settings.attributes = {\"TEST_OPTION\": attr}\n \n             for priority in (0, 10, 20):\n\n@@ -26,7 +26,6 @@ from tests import get_testdata\n \n \n class SpiderTest(unittest.TestCase):\n-\n     spider_class = Spider\n \n     def setUp(self):\n@@ -115,12 +114,10 @@ class SpiderTest(unittest.TestCase):\n \n \n class InitSpiderTest(SpiderTest):\n-\n     spider_class = InitSpider\n \n \n class XMLFeedSpiderTest(SpiderTest):\n-\n     spider_class = XMLFeedSpider\n \n     def test_register_namespace(self):\n@@ -174,7 +171,6 @@ class XMLFeedSpiderTest(SpiderTest):\n \n \n class CSVFeedSpiderTest(SpiderTest):\n-\n     spider_class = CSVFeedSpider\n \n     def test_parse_rows(self):\n@@ -196,7 +192,6 @@ class CSVFeedSpiderTest(SpiderTest):\n \n \n class CrawlSpiderTest(SpiderTest):\n-\n     test_body = b\"\"\"<html><head><title>Page title<title>\n     <body>\n     <p><a href=\"item/12.html\">Item 12</a></p>\n@@ -210,7 +205,6 @@ class CrawlSpiderTest(SpiderTest):\n     spider_class = CrawlSpider\n \n     def test_rule_without_link_extractor(self):\n-\n         response = HtmlResponse(\n             \"http://example.org/somepage/index.html\", body=self.test_body\n         )\n@@ -234,7 +228,6 @@ class CrawlSpiderTest(SpiderTest):\n         )\n \n     def test_process_links(self):\n-\n         response = HtmlResponse(\n             \"http://example.org/somepage/index.html\", body=self.test_body\n         )\n@@ -261,7 +254,6 @@ class CrawlSpiderTest(SpiderTest):\n         )\n \n     def test_process_links_filter(self):\n-\n         response = HtmlResponse(\n             \"http://example.org/somepage/index.html\", body=self.test_body\n         )\n@@ -290,7 +282,6 @@ class CrawlSpiderTest(SpiderTest):\n         )\n \n     def test_process_links_generator(self):\n-\n         response = HtmlResponse(\n             \"http://example.org/somepage/index.html\", body=self.test_body\n         )\n@@ -318,7 +309,6 @@ class CrawlSpiderTest(SpiderTest):\n         )\n \n     def test_process_request(self):\n-\n         response = HtmlResponse(\n             \"http://example.org/somepage/index.html\", body=self.test_body\n         )\n@@ -347,7 +337,6 @@ class CrawlSpiderTest(SpiderTest):\n         )\n \n     def test_process_request_with_response(self):\n-\n         response = HtmlResponse(\n             \"http://example.org/somepage/index.html\", body=self.test_body\n         )\n@@ -383,7 +372,6 @@ class CrawlSpiderTest(SpiderTest):\n         )\n \n     def test_process_request_instance_method(self):\n-\n         response = HtmlResponse(\n             \"http://example.org/somepage/index.html\", body=self.test_body\n         )\n@@ -410,7 +398,6 @@ class CrawlSpiderTest(SpiderTest):\n         )\n \n     def test_process_request_instance_method_with_response(self):\n-\n         response = HtmlResponse(\n             \"http://example.org/somepage/index.html\", body=self.test_body\n         )\n@@ -467,7 +454,6 @@ class CrawlSpiderTest(SpiderTest):\n \n \n class SitemapSpiderTest(SpiderTest):\n-\n     spider_class = SitemapSpider\n \n     BODY = b\"SITEMAP\"\n@@ -689,7 +675,6 @@ class DeprecationTest(unittest.TestCase):\n \n \n class NoParseMethodSpiderTest(unittest.TestCase):\n-\n     spider_class = Spider\n \n     def test_undefined_parse_method(self):\n\n@@ -114,13 +114,11 @@ class SpiderLoaderTest(unittest.TestCase):\n         self.assertEqual(crawler.spidercls.name, \"spider1\")\n \n     def test_bad_spider_modules_exception(self):\n-\n         module = \"tests.test_spiderloader.test_spiders.doesnotexist\"\n         settings = Settings({\"SPIDER_MODULES\": [module]})\n         self.assertRaises(ImportError, SpiderLoader.from_settings, settings)\n \n     def test_bad_spider_modules_warning(self):\n-\n         with warnings.catch_warnings(record=True) as w:\n             module = \"tests.test_spiderloader.test_spiders.doesnotexist\"\n             settings = Settings(\n\n@@ -31,7 +31,6 @@ from scrapy.spiders import Spider\n \n \n class TestRefererMiddleware(TestCase):\n-\n     req_meta = {}\n     resp_headers = {}\n     settings = {}\n@@ -51,7 +50,6 @@ class TestRefererMiddleware(TestCase):\n         return Response(origin, headers=self.resp_headers)\n \n     def test(self):\n-\n         for origin, target, referrer in self.scenarii:\n             response = self.get_response(origin)\n             request = self.get_request(target)\n@@ -770,7 +768,6 @@ class TestRequestMetaPrecedence003(MixinUnsafeUrl, TestRefererMiddleware):\n \n \n class TestRequestMetaSettingFallback(TestCase):\n-\n     params = [\n         (\n             # When an unknown policy is referenced in Request.meta\n@@ -824,7 +821,6 @@ class TestRequestMetaSettingFallback(TestCase):\n     ]\n \n     def test(self):\n-\n         origin = \"http://www.scrapy.org\"\n         target = \"http://www.example.com\"\n \n@@ -923,7 +919,6 @@ class TestPolicyHeaderPrecedence004(\n \n \n class TestReferrerOnRedirect(TestRefererMiddleware):\n-\n     settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.UnsafeUrlPolicy\"}\n     scenarii = [\n         (\n@@ -966,7 +961,6 @@ class TestReferrerOnRedirect(TestRefererMiddleware):\n         self.redirectmw = RedirectMiddleware(settings)\n \n     def test(self):\n-\n         for (\n             parent,\n             target,\n\n@@ -73,7 +73,6 @@ class ChunkSize4MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n \n \n class PickleFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n-\n     chunksize = 100000\n \n     def queue(self):\n\n@@ -20,7 +20,6 @@ except ImportError:\n \n class UtilsConsoleTestCase(unittest.TestCase):\n     def test_get_shell_embed_func(self):\n-\n         shell = get_shell_embed_func([\"invalid\"])\n         self.assertEqual(shell, None)\n \n@@ -30,14 +29,12 @@ class UtilsConsoleTestCase(unittest.TestCase):\n \n     @unittest.skipIf(not bpy, \"bpython not available in testenv\")\n     def test_get_shell_embed_func2(self):\n-\n         shell = get_shell_embed_func([\"bpython\"])\n         self.assertTrue(callable(shell))\n         self.assertEqual(shell.__name__, \"_embed_bpython_shell\")\n \n     @unittest.skipIf(not ipy, \"IPython not available in testenv\")\n     def test_get_shell_embed_func3(self):\n-\n         # default shell should be 'ipython'\n         shell = get_shell_embed_func()\n         self.assertEqual(shell.__name__, \"_embed_ipython_shell\")\n\n@@ -7,7 +7,6 @@ from tests import get_testdata\n \n \n class XmliterTestCase(unittest.TestCase):\n-\n     xmliter = staticmethod(xmliter)\n \n     def test_xmliter(self):\n\n@@ -462,7 +462,6 @@ class BackwardCompatibilityTestCase(unittest.TestCase):\n         warning to be logged.\"\"\"\n \n         class RequestFingerprinter:\n-\n             cache = WeakKeyDictionary()\n \n             def fingerprint(self, request):\n@@ -641,7 +640,6 @@ class CustomRequestFingerprinterTestCase(unittest.TestCase):\n \n     def test_from_crawler_and_settings(self):\n         class RequestFingerprinter:\n-\n             # This method is ignored due to the presence of from_crawler\n             @classmethod\n             def from_settings(cls, settings):\n\n@@ -16,7 +16,6 @@ class UtilsRenderTemplateFileTestCase(unittest.TestCase):\n         rmtree(self.tmp_path)\n \n     def test_simple_render(self):\n-\n         context = dict(project_name=\"proj\", name=\"spi\", classname=\"TheSpider\")\n         template = \"from ${project_name}.spiders.${name} import ${classname}\"\n         rendered = \"from proj.spiders.spi import TheSpider\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
