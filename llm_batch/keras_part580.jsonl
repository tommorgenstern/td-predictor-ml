{"custom_id": "keras#85fbd5152f05a29b052f10ccae2fd1a3c2e128e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 100 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -32,7 +32,7 @@ def get_functional_model(keras):\n         moving_mean_initializer=\"uniform\", gamma_initializer=\"uniform\"\n     )(x)\n     x = keras.layers.Dense(4, activation=\"relu\")(x)\n-    x += residual\n+    x = keras.layers.add([x, residual])\n     outputs = keras.layers.Dense(5, activation=\"softmax\")(x)\n     return keras.Model(inputs, outputs)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#906dd65109b9f99fa101cdd78968508bc34275df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 88 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -41,17 +41,17 @@ class ReLU(Layer):\n         if max_value is not None and max_value < 0.0:\n             raise ValueError(\n                 \"max_value of a ReLU layer cannot be a negative \"\n-                f\"value. Received max_value: {max_value}\"\n+                f\"value. Received: max_value={max_value}\"\n             )\n         if negative_slope is None or negative_slope < 0.0:\n             raise ValueError(\n                 \"negative_slope of a ReLU layer cannot be a negative \"\n-                f\"value. Received negative_slope: {negative_slope}\"\n+                f\"value. Received: negative_slope={negative_slope}\"\n             )\n         if threshold is None or threshold < 0.0:\n             raise ValueError(\n                 \"threshold of a ReLU layer cannot be a negative \"\n-                f\"value. Received threshold: {threshold}\"\n+                f\"value. Received: threshold={threshold}\"\n             )\n \n         self.supports_masking = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3a2303a45b05b544b5ca9c06709ab54ca32f11c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 319 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 11 | Complexity Δ (Sum/Max): 26/21 | Churn Δ: 319 | Churn Cumulative: 1291 | Contributors (this commit): 6 | Commits (past 90d): 36 | Contributors (cumulative): 10 | DMM Complexity: 0.8275862068965517\n\nDIFF:\n@@ -1,3 +1,6 @@\n+import warnings\n+\n+from keras_core import initializers\n from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.losses.loss import squeeze_to_same_rank\n@@ -340,6 +343,228 @@ class LogCoshError(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n+# Adapted from TF-Addons implementation (RSquare class).\n+@keras_core_export(\"keras_core.metrics.R2Score\")\n+class R2Score(reduction_metrics.Metric):\n+    \"\"\"Computes R2 score.\n+\n+    Formula:\n+    ```python\n+    sum_squares_residuals = sum((y_true - y_pred) ** 2)\n+    sum_squares = sum((y_true - mean(y_true)) ** 2)\n+    R2 = 1 - sum_squares_residuals / sum_squares\n+    ```\n+    This is also called the\n+    [coefficient of\n+    determination](https://en.wikipedia.org/wiki/Coefficient_of_determination).\n+\n+    It indicates how close the fitted regression line\n+    is to ground-truth data.\n+\n+    - The highest score possible is 1.0. It indicates that the predictors\n+        perfectly accounts for variation in the target.\n+    - A score of 0.0 indicates that the predictors do not\n+        account for variation in the target.\n+    - It can also be negative if the model is worse than random.\n+\n+    This metric can also compute the \"Adjusted R2\" score.\n+\n+    Args:\n+        class_aggregation: Specifies how to aggregate scores corresponding to\n+            different output classes (or target dimensions),\n+            i.e. different dimensions on the last axis of the predictions.\n+            Equivalent to `multioutput` argument in Scikit-Learn.\n+            Should be one of\n+            `None` (no aggregation), `\"uniform_average\"`,\n+            `\"variance_weighted_average\"`.\n+        num_regressors: Number of independent regressors used\n+            (\"Adjusted R2\" score). 0 is the standard R2 score.\n+            Defaults to `0`.\n+        name: Optional. string name of the metric instance.\n+        dtype: Optional. data type of the metric result.\n+\n+    Example:\n+\n+    >>> y_true = np.array([[1], [4], [3]], dtype=np.float32)\n+    >>> y_pred = np.array([[2], [4], [4]], dtype=np.float32)\n+    >>> metric = keras_core.metrics.R2Score()\n+    >>> metric.update_state(y_true, y_pred)\n+    >>> result = metric.result()\n+    >>> result\n+    0.57142854\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        class_aggregation=\"uniform_average\",\n+        num_regressors=0,\n+        name=\"r2_score\",\n+        dtype=None,\n+    ):\n+        super().__init__(name=name, dtype=dtype)\n+\n+        valid_class_aggregation_values = (\n+            None,\n+            \"uniform_average\",\n+            \"variance_weighted_average\",\n+        )\n+        if class_aggregation not in valid_class_aggregation_values:\n+            raise ValueError(\n+                \"Invalid value for argument `class_aggregation`. Expected \"\n+                f\"one of {valid_class_aggregation_values}. \"\n+                f\"Received: class_aggregation={class_aggregation}\"\n+            )\n+        if num_regressors < 0:\n+            raise ValueError(\n+                \"Invalid value for argument `num_regressors`. \"\n+                \"Expected a value >= 0. \"\n+                f\"Received: num_regressors={num_regressors}\"\n+            )\n+        self.class_aggregation = class_aggregation\n+        self.num_regressors = num_regressors\n+        self.num_samples = self.add_variable(\n+            shape=(),\n+            initializer=initializers.Zeros(),\n+            name=\"num_samples\",\n+            dtype=\"int32\",\n+        )\n+        self._built = False\n+\n+    def _build(self, y_true_shape, y_pred_shape):\n+        if len(y_pred_shape) != 2 or len(y_true_shape) != 2:\n+            raise ValueError(\n+                \"R2Score expects 2D inputs with shape \"\n+                \"(batch_size, output_dim). Received input \"\n+                f\"shapes: y_pred.shape={y_pred_shape} and \"\n+                f\"y_true.shape={y_true_shape}.\"\n+            )\n+        if y_pred_shape[-1] is None or y_true_shape[-1] is None:\n+            raise ValueError(\n+                \"R2Score expects 2D inputs with shape \"\n+                \"(batch_size, output_dim), with output_dim fully \"\n+                \"defined (not None). Received input \"\n+                f\"shapes: y_pred.shape={y_pred_shape} and \"\n+                f\"y_true.shape={y_true_shape}.\"\n+            )\n+        num_classes = y_pred_shape[-1]\n+        self.squared_sum = self.add_variable(\n+            name=\"squared_sum\",\n+            shape=[num_classes],\n+            initializer=initializers.Zeros(),\n+        )\n+        self.sum = self.add_variable(\n+            name=\"sum\",\n+            shape=[num_classes],\n+            initializer=initializers.Zeros(),\n+        )\n+        self.total_mse = self.add_variable(\n+            name=\"residual\",\n+            shape=[num_classes],\n+            initializer=initializers.Zeros(),\n+        )\n+        self.count = self.add_variable(\n+            name=\"count\",\n+            shape=[num_classes],\n+            initializer=initializers.Zeros(),\n+        )\n+        self._built = True\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates root mean squared error statistics.\n+\n+        Args:\n+            y_true: The ground truth values.\n+            y_pred: The predicted values.\n+            sample_weight: Optional weighting of each example. Can\n+                be a `Tensor` whose rank is either 0, or the same rank as\n+                `y_true`, and must be broadcastable to `y_true`.\n+                Defaults to `1`.\n+\n+        Returns:\n+            Update op.\n+        \"\"\"\n+        y_true = ops.convert_to_tensor(y_true, dtype=self._dtype)\n+        y_pred = ops.convert_to_tensor(y_pred, dtype=self._dtype)\n+        y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n+        if not self._built:\n+            self._build(y_true.shape, y_pred.shape)\n+\n+        if sample_weight is None:\n+            sample_weight = 1\n+\n+        sample_weight = ops.convert_to_tensor(sample_weight, dtype=self.dtype)\n+\n+        if sample_weight.shape.rank == 1:\n+            # Make sure there's a features dimension\n+            sample_weight = ops.expand_dims(sample_weight, axis=1)\n+\n+        sample_weight = ops.broadcast_to(sample_weight, y_true.shape)\n+\n+        weighted_y_true = y_true * sample_weight\n+        self.sum.assign(self.sum + ops.sum(weighted_y_true, axis=0))\n+        self.squared_sum.assign(\n+            self.squared_sum + ops.sum(y_true * weighted_y_true, axis=0)\n+        )\n+        self.total_mse.assign(\n+            self.total_mse\n+            + ops.sum((y_true - y_pred) ** 2 * sample_weight, axis=0)\n+        )\n+        self.count.assign(self.count + ops.sum(sample_weight, axis=0))\n+        self.num_samples.assign(self.num_samples + ops.size(y_true))\n+\n+    def result(self):\n+        mean = self.sum / self.count\n+        total = self.squared_sum - self.sum * mean\n+        raw_scores = 1 - (self.total_mse / total)\n+        raw_scores = ops.where(ops.isinf(raw_scores), 0.0, raw_scores)\n+\n+        if self.class_aggregation == \"uniform_average\":\n+            r2_score = ops.mean(raw_scores)\n+        elif self.class_aggregation == \"variance_weighted_average\":\n+            weighted_sum = ops.sum(total * raw_scores)\n+            sum_of_weights = ops.sum(total)\n+            r2_score = weighted_sum / sum_of_weights\n+        else:\n+            r2_score = raw_scores\n+\n+        if self.num_regressors != 0:\n+            if self.num_regressors > self.num_samples - 1:\n+                warnings.warn(\n+                    \"More independent predictors than datapoints \"\n+                    \"in adjusted R2 score. Falling back to standard R2 score.\",\n+                    stacklevel=2,\n+                )\n+            elif self.num_regressors == self.num_samples - 1:\n+                warnings.warn(\n+                    \"Division by zero in Adjusted R2 score. \"\n+                    \"Falling back to standard R2 score.\",\n+                    stacklevel=2,\n+                )\n+            else:\n+                n = ops.convert_to_tensor(self.num_samples, dtype=\"float32\")\n+                p = ops.convert_to_tensor(self.num_regressors, dtype=\"float32\")\n+                num = ops.multiply(\n+                    ops.subtract(1.0, r2_score), ops.subtract(n, 1.0)\n+                )\n+                den = ops.subtract(ops.subtract(n, p), 1.0)\n+                r2_score = ops.subtract(1.0, ops.divide(num, den))\n+        return r2_score\n+\n+    def reset_state(self):\n+        for v in self.variables:\n+            v.assign(ops.zeros(v.shape))\n+\n+    def get_config(self):\n+        config = {\n+            \"name\": self.name,\n+            \"dtype\": self.dtype,\n+            \"class_aggregation\": self.class_aggregation,\n+            \"num_regressors\": self.num_regressors,\n+        }\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n+\n+\n def cosine_similarity(y_true, y_pred, axis=-1):\n     \"\"\"Computes the cosine similarity between labels and predictions.\n \n\n@@ -1,4 +1,5 @@\n import numpy as np\n+from absl.testing import parameterized\n \n from keras_core import testing\n from keras_core.metrics import regression_metrics as metrics\n@@ -296,3 +297,96 @@ class LogCoshErrorTest(testing.TestCase):\n         expected_result = np.multiply(self.expected_results, sample_weight)\n         expected_result = np.sum(expected_result) / np.sum(sample_weight)\n         self.assertAllClose(result, expected_result, atol=1e-3)\n+\n+\n+class R2ScoreTest(parameterized.TestCase, testing.TestCase):\n+    def _run_test(\n+        self,\n+        y_true,\n+        y_pred,\n+        sample_weights,\n+        class_aggregation,\n+        num_regressors,\n+        reference_result,\n+    ):\n+        r2 = metrics.R2Score(class_aggregation, num_regressors, dtype=\"float32\")\n+        r2.update_state(y_true, y_pred, sample_weights)\n+        result = r2.result().numpy()\n+        self.assertAllClose(result, reference_result, atol=1e-6)\n+\n+    def test_config(self):\n+        r2_obj = metrics.R2Score(\n+            class_aggregation=None, num_regressors=2, dtype=\"float32\"\n+        )\n+        self.assertEqual(r2_obj.class_aggregation, None)\n+        self.assertEqual(r2_obj.num_regressors, 2)\n+        self.assertEqual(r2_obj.dtype, \"float32\")\n+\n+        # Check save and restore config\n+        r2_obj2 = metrics.R2Score.from_config(r2_obj.get_config())\n+        self.assertEqual(r2_obj2.class_aggregation, None)\n+        self.assertEqual(r2_obj2.num_regressors, 2)\n+        self.assertEqual(r2_obj2.dtype, \"float32\")\n+\n+    @parameterized.parameters(\n+        # class_aggregation, num_regressors, result\n+        (None, 0, [0.37, -1.295, 0.565]),\n+        (\"uniform_average\", 0, -0.12),\n+        (\"variance_weighted_average\", 0, -0.12),\n+    )\n+    def test_r2_sklearn_comparison(\n+        self, class_aggregation, num_regressors, result\n+    ):\n+        y_true = [[0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0]]\n+        y_pred = [[0.4, 0.5, 0.6], [0.1, 0.2, 0.3], [0.5, 0.8, 0.2]]\n+        self._run_test(\n+            y_true,\n+            y_pred,\n+            None,\n+            class_aggregation=class_aggregation,\n+            num_regressors=num_regressors,\n+            reference_result=result,\n+        )\n+\n+    @parameterized.parameters(\n+        # class_aggregation, num_regressors, result\n+        (None, 0, [0.17305559, -8.836666, -0.521]),\n+        (None, 1, [0.054920673, -10.241904, -0.7382858]),\n+        (None, 2, [-0.10259259, -12.115555, -1.0280001]),\n+        (\"uniform_average\", 0, -3.0615367889404297),\n+        (\"uniform_average\", 1, -3.641756534576416),\n+        (\"uniform_average\", 2, -4.415382385253906),\n+        (\"variance_weighted_average\", 0, -1.3710224628448486),\n+        (\"variance_weighted_average\", 1, -1.7097399234771729),\n+        (\"variance_weighted_average\", 2, -2.161363363265991),\n+    )\n+    def test_r2_tfa_comparison(self, class_aggregation, num_regressors, result):\n+        y_true = [[0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0]]\n+        y_pred = [[0.4, 0.9, 1.6], [0.1, 1.2, 0.6], [1.5, 0.8, 0.6]]\n+        sample_weights = [0.8, 0.1, 0.4]\n+        self._run_test(\n+            y_true,\n+            y_pred,\n+            sample_weights,\n+            class_aggregation=class_aggregation,\n+            num_regressors=num_regressors,\n+            reference_result=result,\n+        )\n+\n+    def test_errors(self):\n+        # Bad class_aggregation value\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid value for argument `class_aggregation`\"\n+        ):\n+            metrics.R2Score(class_aggregation=\"wrong\")\n+\n+        # Bad num_regressors value\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid value for argument `num_regressors`\"\n+        ):\n+            metrics.R2Score(num_regressors=-1)\n+\n+        # Bad input shape\n+        with self.assertRaisesRegex(ValueError, \"expects 2D inputs with shape\"):\n+            r2 = metrics.R2Score()\n+            r2.update_state([0.0, 1.0], [0.0, 1.0])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6a4ef411a44c04da712a87d4befb4552c76f536", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 815 | Contributors (this commit): 6 | Commits (past 90d): 24 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -349,14 +349,16 @@ class R2Score(reduction_metrics.Metric):\n     \"\"\"Computes R2 score.\n \n     Formula:\n+\n     ```python\n     sum_squares_residuals = sum((y_true - y_pred) ** 2)\n     sum_squares = sum((y_true - mean(y_true)) ** 2)\n     R2 = 1 - sum_squares_residuals / sum_squares\n     ```\n+\n     This is also called the\n-    [coefficient of\n-    determination](https://en.wikipedia.org/wiki/Coefficient_of_determination).\n+    [coefficient of determination](\n+    https://en.wikipedia.org/wiki/Coefficient_of_determination).\n \n     It indicates how close the fitted regression line\n     is to ground-truth data.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b617279282cc49cdc4a18a26e1a8c178cb583586", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 98 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -41,8 +41,7 @@ class ReLUTest(testing.TestCase):\n     def test_invalid_usage(self):\n         with self.assertRaisesRegex(\n             ValueError,\n-            \"max_value of a ReLU layer cannot be a negative \"\n-            \"value. Received max_value: -10\",\n+            \"max_value of a ReLU layer cannot be a negative value\",\n         ):\n             self.run_layer_test(\n                 relu.ReLU,\n@@ -57,8 +56,7 @@ class ReLUTest(testing.TestCase):\n \n         with self.assertRaisesRegex(\n             ValueError,\n-            \"negative_slope of a ReLU layer cannot be a negative \"\n-            \"value. Received negative_slope: -10\",\n+            \"negative_slope of a ReLU layer cannot be a negative value\",\n         ):\n             self.run_layer_test(\n                 relu.ReLU,\n@@ -72,9 +70,7 @@ class ReLUTest(testing.TestCase):\n             )\n \n         with self.assertRaisesRegex(\n-            ValueError,\n-            \"threshold of a ReLU layer cannot be a negative \"\n-            \"value. Received threshold: -10\",\n+            ValueError, \"threshold of a ReLU layer cannot be a negative value\"\n         ):\n             self.run_layer_test(\n                 relu.ReLU,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3111ae0aacd4085de6fa57850abca972d235cbf9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1301 | Contributors (this commit): 6 | Commits (past 90d): 39 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -496,7 +496,7 @@ class R2Score(reduction_metrics.Metric):\n \n         sample_weight = ops.convert_to_tensor(sample_weight, dtype=self.dtype)\n \n-        if sample_weight.shape.rank == 1:\n+        if len(sample_weight.shape) == 1:\n             # Make sure there's a features dimension\n             sample_weight = ops.expand_dims(sample_weight, axis=1)\n \n\n@@ -311,7 +311,7 @@ class R2ScoreTest(parameterized.TestCase, testing.TestCase):\n     ):\n         r2 = metrics.R2Score(class_aggregation, num_regressors, dtype=\"float32\")\n         r2.update_state(y_true, y_pred, sample_weights)\n-        result = r2.result().numpy()\n+        result = r2.result()\n         self.assertAllClose(result, reference_result, atol=1e-6)\n \n     def test_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d04747f234eab990ab8c7b37a2e3dc5aa8663b77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 181 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 16/8 | Churn Δ: 181 | Churn Cumulative: 191 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: 0.7522935779816514\n\nDIFF:\n@@ -5,4 +5,5 @@ from keras_core.callbacks.early_stopping import EarlyStopping\n from keras_core.callbacks.history import History\n from keras_core.callbacks.lambda_callback import LambdaCallback\n from keras_core.callbacks.progbar_logger import ProgbarLogger\n+from keras_core.callbacks.remote_monitor import RemoteMonitor\n from keras_core.callbacks.terminate_on_nan import TerminateOnNaN\n\n@@ -0,0 +1,83 @@\n+import json\n+import warnings\n+\n+import numpy as np\n+\n+from keras_core.api_export import keras_core_export\n+from keras_core.callbacks.callback import Callback\n+\n+try:\n+    import requests\n+except ImportError:\n+    requests = None\n+\n+\n+@keras_core_export(\"keras_core.callbacks.RemoteMonitor\")\n+class RemoteMonitor(Callback):\n+    \"\"\"Callback used to stream events to a server.\n+\n+    Requires the `requests` library.\n+    Events are sent to `root + '/publish/epoch/end/'` by default. Calls are\n+    HTTP POST, with a `data` argument which is a\n+    JSON-encoded dictionary of event data.\n+    If `send_as_json=True`, the content type of the request will be\n+    `\"application/json\"`.\n+    Otherwise the serialized JSON will be sent within a form.\n+\n+    Args:\n+        root: String; root url of the target server.\n+        path: String; path relative to `root` to which the events will be sent.\n+        field: String; JSON field under which the data will be stored.\n+            The field is used only if the payload is sent within a form\n+            (i.e. send_as_json is set to False).\n+        headers: Dictionary; optional custom HTTP headers.\n+        send_as_json: Boolean; whether the request should be\n+            sent as `\"application/json\"`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        root=\"http://localhost:9000\",\n+        path=\"/publish/epoch/end/\",\n+        field=\"data\",\n+        headers=None,\n+        send_as_json=False,\n+    ):\n+        super().__init__()\n+\n+        self.root = root\n+        self.path = path\n+        self.field = field\n+        self.headers = headers\n+        self.send_as_json = send_as_json\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        if requests is None:\n+            raise ImportError(\"RemoteMonitor requires the `requests` library.\")\n+        logs = logs or {}\n+        send = {}\n+        send[\"epoch\"] = epoch\n+        for k, v in logs.items():\n+            # np.ndarray and np.generic are not scalar types\n+            # therefore we must unwrap their scalar values and\n+            # pass to the json-serializable dict 'send'\n+            if isinstance(v, (np.ndarray, np.generic)):\n+                send[k] = v.item()\n+            else:\n+                send[k] = v\n+        try:\n+            if self.send_as_json:\n+                requests.post(\n+                    self.root + self.path, json=send, headers=self.headers\n+                )\n+            else:\n+                requests.post(\n+                    self.root + self.path,\n+                    {self.field: json.dumps(send)},\n+                    headers=self.headers,\n+                )\n+        except requests.exceptions.RequestException:\n+            warnings.warn(\n+                f\"Could not reach RemoteMonitor root server at {self.root}\",\n+                stacklevel=2,\n+            )\n\n@@ -0,0 +1,97 @@\n+import warnings\n+from unittest import mock\n+\n+import numpy as np\n+\n+from keras_core import callbacks\n+from keras_core import layers\n+from keras_core import testing\n+from keras_core.models import Sequential\n+from keras_core.utils import numerical_utils\n+\n+try:\n+    import requests\n+except ImportError:\n+    requests = None\n+\n+\n+class TerminateOnNaNTest(testing.TestCase):\n+    def test_RemoteMonitor(self):\n+        if requests is None:\n+            self.skipTest(\"`requests` required to run this test\")\n+\n+        monitor = callbacks.RemoteMonitor()\n+        # This will raise a warning since the default address in unreachable:\n+        warning_msg = \"Could not reach RemoteMonitor root server\"\n+        with warnings.catch_warnings(record=True) as warning_logs:\n+            warnings.simplefilter(\"always\")\n+            monitor.on_epoch_end(0, logs={\"loss\": 0.0})\n+            self.assertIn(warning_msg, str(warning_logs[-1].message))\n+\n+    def test_RemoteMonitor_np_array(self):\n+        if requests is None:\n+            self.skipTest(\"`requests` required to run this test\")\n+\n+        with mock.patch(\"requests.post\") as requests_post:\n+            monitor = callbacks.RemoteMonitor(send_as_json=True)\n+            a = np.arange(1)  # a 1 by 1 array\n+            logs = {\"loss\": 0.0, \"val\": a}\n+            monitor.on_epoch_end(0, logs=logs)\n+            send = {\"loss\": 0.0, \"epoch\": 0, \"val\": 0}\n+            requests_post.assert_called_once_with(\n+                monitor.root + monitor.path, json=send, headers=monitor.headers\n+            )\n+\n+    def test_RemoteMonitor_np_float32(self):\n+        if requests is None:\n+            self.skipTest(\"`requests` required to run this test\")\n+\n+        with mock.patch(\"requests.post\") as requests_post:\n+            monitor = callbacks.RemoteMonitor(send_as_json=True)\n+            a = np.float32(1.0)  # a float32 generic type\n+            logs = {\"loss\": 0.0, \"val\": a}\n+            monitor.on_epoch_end(0, logs=logs)\n+            send = {\"loss\": 0.0, \"epoch\": 0, \"val\": 1.0}\n+            requests_post.assert_called_once_with(\n+                monitor.root + monitor.path, json=send, headers=monitor.headers\n+            )\n+\n+    def test_RemoteMonitorWithJsonPayload(self):\n+        if requests is None:\n+            self.skipTest(\"`requests` required to run this test\")\n+\n+        TRAIN_SAMPLES = 10\n+        TEST_SAMPLES = 10\n+        INPUT_DIM = 3\n+        NUM_CLASSES = 2\n+        BATCH_SIZE = 4\n+\n+        np.random.seed(1337)\n+        x_train = np.random.random((TRAIN_SAMPLES, INPUT_DIM))\n+        y_train = np.random.choice(np.arange(NUM_CLASSES), size=TRAIN_SAMPLES)\n+        x_test = np.random.random((TEST_SAMPLES, INPUT_DIM))\n+        y_test = np.random.choice(np.arange(NUM_CLASSES), size=TEST_SAMPLES)\n+        y_test = numerical_utils.to_categorical(y_test)\n+        y_train = numerical_utils.to_categorical(y_train)\n+\n+        model = Sequential([layers.Dense(NUM_CLASSES)])\n+        model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n+\n+        with mock.patch(\"requests.post\") as requests_post:\n+            monitor = callbacks.RemoteMonitor(send_as_json=True)\n+            hist = model.fit(\n+                x_train,\n+                y_train,\n+                batch_size=BATCH_SIZE,\n+                validation_data=(x_test, y_test),\n+                callbacks=[monitor],\n+                epochs=1,\n+            )\n+            send = {\n+                \"epoch\": 0,\n+                \"loss\": hist.history[\"loss\"][0],\n+                \"val_loss\": hist.history[\"val_loss\"][0],\n+            }\n+            requests_post.assert_called_once_with(\n+                monitor.root + monitor.path, json=send, headers=monitor.headers\n+            )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#71ce2a71272c57dd0b5990148328d285ec48e24e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1384 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 36 | Complexity Δ (Sum/Max): 88/51 | Churn Δ: 1386 | Churn Cumulative: 1389 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.9624478442280946\n\nDIFF:\n@@ -1,3 +1,933 @@\n+\"\"\"Various learning rate schedule functions.\"\"\"\n+\n+import math\n+\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.saving import serialization_lib\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.LearningRateSchedule\")\n class LearningRateSchedule:\n-    # TODO\n-    pass\n+    \"\"\"The learning rate schedule base class.\n+\n+    You can use a learning rate schedule to modulate how the learning rate\n+    of your optimizer changes over time.\n+\n+    Several built-in learning rate schedules are available, such as\n+    `keras_core.optimizers.schedules.ExponentialDecay` or\n+    `keras_core.optimizers.schedules.PiecewiseConstantDecay`:\n+\n+    ```python\n+    lr_schedule = keras_core.optimizers.schedules.ExponentialDecay(\n+        initial_learning_rate=1e-2,\n+        decay_steps=10000,\n+        decay_rate=0.9)\n+    optimizer = keras_core.optimizers.SGD(learning_rate=lr_schedule)\n+    ```\n+\n+    A `LearningRateSchedule` instance can be passed in as the `learning_rate`\n+    argument of any optimizer.\n+\n+    To implement your own schedule object, you should implement the `__call__`\n+    method, which takes a `step` argument (scalar integer tensor, the\n+    current training step count).\n+    Like for any other Keras object, you can also optionally\n+    make your object serializable by implementing the `get_config`\n+    and `from_config` methods.\n+\n+    Example:\n+\n+    ```python\n+    class MyLRSchedule(keras_core.optimizers.schedules.LearningRateSchedule):\n+\n+        def __init__(self, initial_learning_rate):\n+            self.initial_learning_rate = initial_learning_rate\n+\n+        def __call__(self, step):\n+            return self.initial_learning_rate / (step + 1)\n+\n+    optimizer = keras_core.optimizers.SGD(learning_rate=MyLRSchedule(0.1))\n+    ```\n+    \"\"\"\n+\n+    def __call__(self, step):\n+        raise NotImplementedError(\n+            f\"Learning rate schedule '{self.__class__.__name__}' \"\n+            \"must override `__call__(self, step)`.\"\n+        )\n+\n+    def get_config(self):\n+        raise NotImplementedError(\n+            f\"Learning rate schedule '{self.__class__.__name__}' \"\n+            \"must override `get_config()` in order to be serializable.\"\n+        )\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        \"\"\"Instantiates a `LearningRateSchedule` from its config.\n+\n+        Args:\n+            config: Output of `get_config()`.\n+\n+        Returns:\n+            A `LearningRateSchedule` instance.\n+        \"\"\"\n+        return cls(**config)\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.ExponentialDecay\")\n+class ExponentialDecay(LearningRateSchedule):\n+    \"\"\"A `LearningRateSchedule` that uses an exponential decay schedule.\n+\n+    When training a model, it is often useful to lower the learning rate as\n+    the training progresses. This schedule applies an exponential decay function\n+    to an optimizer step, given a provided initial learning rate.\n+\n+    The schedule is a 1-arg callable that produces a decayed learning\n+    rate when passed the current optimizer step. This can be useful for changing\n+    the learning rate value across different invocations of optimizer functions.\n+    It is computed as:\n+\n+    ```python\n+    def decayed_learning_rate(step):\n+        return initial_learning_rate * decay_rate ^ (step / decay_steps)\n+    ```\n+\n+    If the argument `staircase` is `True`, then `step / decay_steps` is\n+    an integer division and the decayed learning rate follows a\n+    staircase function.\n+\n+    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    as the learning rate.\n+    Example: When fitting a Keras model, decay every 100000 steps with a base\n+    of 0.96:\n+\n+    ```python\n+    initial_learning_rate = 0.1\n+    lr_schedule = keras_core.optimizers.schedules.ExponentialDecay(\n+        initial_learning_rate,\n+        decay_steps=100000,\n+        decay_rate=0.96,\n+        staircase=True)\n+\n+    model.compile(optimizer=keras_core.optimizers.SGD(learning_rate=lr_schedule),\n+                  loss='sparse_categorical_crossentropy',\n+                  metrics=['accuracy'])\n+\n+    model.fit(data, labels, epochs=5)\n+    ```\n+\n+    The learning rate schedule is also serializable and deserializable using\n+    `keras_core.optimizers.schedules.serialize` and\n+    `keras_core.optimizers.schedules.deserialize`.\n+\n+    Args:\n+        initial_learning_rate: A Python float. The initial learning rate.\n+        decay_steps: A Python integer. Must be positive. See the decay\n+            computation above.\n+        decay_rate: A Python float. The decay rate.\n+        staircase: Boolean.  If `True` decay the learning rate at discrete\n+            intervals.\n+        name: String.  Optional name of the operation.  Defaults to\n+            `\"ExponentialDecay`\".\n+\n+    Returns:\n+        A 1-arg callable learning rate schedule that takes the current optimizer\n+        step and outputs the decayed learning rate, a scalar tensor of the\n+        same type as `initial_learning_rate`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        initial_learning_rate,\n+        decay_steps,\n+        decay_rate,\n+        staircase=False,\n+        name=\"ExponentialDecay\",\n+    ):\n+        super().__init__()\n+        self.initial_learning_rate = initial_learning_rate\n+        self.decay_steps = decay_steps\n+        self.decay_rate = decay_rate\n+        self.staircase = staircase\n+        self.name = name\n+\n+    def __call__(self, step):\n+        with ops.name_scope(self.name):\n+            initial_learning_rate = ops.convert_to_tensor(\n+                self.initial_learning_rate\n+            )\n+            dtype = initial_learning_rate.dtype\n+            decay_steps = ops.cast(self.decay_steps, dtype)\n+            decay_rate = ops.cast(self.decay_rate, dtype)\n+\n+            global_step_recomp = ops.cast(step, dtype)\n+            p = global_step_recomp / decay_steps\n+            if self.staircase:\n+                p = ops.floor(p)\n+            return ops.multiply(initial_learning_rate, ops.power(decay_rate, p))\n+\n+    def get_config(self):\n+        return {\n+            \"initial_learning_rate\": self.initial_learning_rate,\n+            \"decay_steps\": self.decay_steps,\n+            \"decay_rate\": self.decay_rate,\n+            \"staircase\": self.staircase,\n+            \"name\": self.name,\n+        }\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.PiecewiseConstantDecay\")\n+class PiecewiseConstantDecay(LearningRateSchedule):\n+    \"\"\"A `LearningRateSchedule` that uses a piecewise constant decay schedule.\n+\n+    The function returns a 1-arg callable to compute the piecewise constant\n+    when passed the current optimizer step. This can be useful for changing the\n+    learning rate value across different invocations of optimizer functions.\n+\n+    Example: use a learning rate that's 1.0 for the first 100001 steps, 0.5\n+        for the next 10000 steps, and 0.1 for any additional steps.\n+\n+    ```python\n+    step = ops.array(0)\n+    boundaries = [100000, 110000]\n+    values = [1.0, 0.5, 0.1]\n+    learning_rate_fn = keras_core.optimizers.schedules.PiecewiseConstantDecay(\n+        boundaries, values)\n+\n+    # Later, whenever we perform an optimization step, we pass in the step.\n+    learning_rate = learning_rate_fn(step)\n+    ```\n+\n+    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    as the learning rate. The learning rate schedule is also serializable and\n+    deserializable using `keras_core.optimizers.schedules.serialize` and\n+    `keras_core.optimizers.schedules.deserialize`.\n+\n+    Args:\n+        boundaries: A list of Python numbers with strictly increasing\n+            entries, and with all elements having the same type as the\n+            optimizer step.\n+        values: A list of Python numbers that specifies the values for the\n+            intervals defined by `boundaries`. It should have one more\n+            element than `boundaries`, and all elements should have the same\n+            type.\n+        name: A string. Optional name of the operation. Defaults to\n+            `\"PiecewiseConstant\"`.\n+\n+    Returns:\n+        A 1-arg callable learning rate schedule that takes the current optimizer\n+        step and outputs the decayed learning rate, a scalar tensor of the\n+        same type as the boundary tensors.\n+\n+        The output of the 1-arg function that takes the `step`\n+        is `values[0]` when `step <= boundaries[0]`,\n+        `values[1]` when `step > boundaries[0]` and `step <= boundaries[1]`,\n+        ..., and `values[-1]` when `step > boundaries[-1]`.\n+\n+\n+    Raises:\n+        ValueError: if the number of elements in the `boundaries` and `values`\n+        lists do not match.\n+    \"\"\"\n+\n+    def __init__(self, boundaries, values, name=\"PiecewiseConstant\"):\n+        super().__init__()\n+\n+        if len(boundaries) != len(values) - 1:\n+            raise ValueError(\n+                \"The length of boundaries should be 1 less than the length of \"\n+                f\"values. Received: boundaries={boundaries} of length \"\n+                f\"{len(boundaries)}, and values={values} \"\n+                f\"of length {len(values)}.\"\n+            )\n+\n+        self.boundaries = boundaries\n+        self.values = values\n+        self.name = name\n+\n+    def __call__(self, step):\n+        with ops.name_scope(self.name):\n+            boundaries = [ops.convert_to_tensor(x) for x in self.boundaries]\n+            values = [ops.convert_to_tensor(x) for x in self.values]\n+            step = ops.convert_to_tensor(step)\n+\n+            for i, b in enumerate(boundaries):\n+                if b.dtype != step.dtype:\n+                    # We cast the boundaries to have the same type as the step\n+                    b = ops.cast(b, step.dtype)\n+                    boundaries[i] = b\n+\n+            result_dtype = values[0].dtype\n+            result_value = ops.array(0, dtype=result_dtype)\n+\n+            # For each range between boundaries, we check whether the step is\n+            # within that range, cast the resulting boolean to a number,\n+            # and multiply the result by the corresponding value for the range.\n+            # Taking the sum of these yields a piecewise constant function.\n+            step_less_than_first_boundary = ops.cast(\n+                step <= boundaries[0], result_dtype\n+            )\n+            result_value += step_less_than_first_boundary * values[0]\n+\n+            step_greater_than_last_boundary = ops.cast(\n+                step > boundaries[-1], result_dtype\n+            )\n+            result_value += step_greater_than_last_boundary * values[-1]\n+\n+            for low, high, value in zip(\n+                boundaries[:-1], boundaries[1:], values[1:-1]\n+            ):\n+                # Need to bind v here; can do this with lambda v=v: ...\n+                step_in_range = ops.cast(\n+                    (step > low) & (step <= high), result_dtype\n+                )\n+                result_value += step_in_range * value\n+\n+            return result_value\n+\n+    def get_config(self):\n+        return {\n+            \"boundaries\": self.boundaries,\n+            \"values\": self.values,\n+            \"name\": self.name,\n+        }\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.PolynomialDecay\")\n+class PolynomialDecay(LearningRateSchedule):\n+    \"\"\"A `LearningRateSchedule` that uses a polynomial decay schedule.\n+\n+    It is commonly observed that a monotonically decreasing learning rate, whose\n+    degree of change is carefully chosen, results in a better performing model.\n+    This schedule applies a polynomial decay function to an optimizer step,\n+    given a provided `initial_learning_rate`, to reach an `end_learning_rate`\n+    in the given `decay_steps`.\n+\n+    It requires a `step` value to compute the decayed learning rate. You\n+    can just pass a backend variable that you increment at each training\n+    step.\n+\n+    The schedule is a 1-arg callable that produces a decayed learning rate\n+    when passed the current optimizer step. This can be useful for changing the\n+    learning rate value across different invocations of optimizer functions.\n+    It is computed as:\n+\n+    ```python\n+    def decayed_learning_rate(step):\n+        step = min(step, decay_steps)\n+        return ((initial_learning_rate - end_learning_rate) *\n+                (1 - step / decay_steps) ^ (power)\n+               ) + end_learning_rate\n+    ```\n+\n+    If `cycle` is True then a multiple of `decay_steps` is used, the first one\n+    that is bigger than `step`.\n+\n+    ```python\n+    def decayed_learning_rate(step):\n+        decay_steps = decay_steps * ceil(step / decay_steps)\n+        return ((initial_learning_rate - end_learning_rate) *\n+                (1 - step / decay_steps) ^ (power)\n+               ) + end_learning_rate\n+    ```\n+\n+    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    as the learning rate.\n+    Example: Fit a model while decaying from 0.1 to 0.01 in 10000 steps using\n+    sqrt (i.e. power=0.5):\n+\n+    ```python\n+    ...\n+    starter_learning_rate = 0.1\n+    end_learning_rate = 0.01\n+    decay_steps = 10000\n+    learning_rate_fn = keras_core.optimizers.schedules.PolynomialDecay(\n+        starter_learning_rate,\n+        decay_steps,\n+        end_learning_rate,\n+        power=0.5)\n+\n+    model.compile(optimizer=keras_core.optimizers.SGD(\n+                      learning_rate=learning_rate_fn),\n+                  loss='sparse_categorical_crossentropy',\n+                  metrics=['accuracy'])\n+\n+    model.fit(data, labels, epochs=5)\n+    ```\n+\n+    The learning rate schedule is also serializable and deserializable using\n+    `keras_core.optimizers.schedules.serialize` and\n+    `keras_core.optimizers.schedules.deserialize`.\n+\n+    Args:\n+        initial_learning_rate: A Python float. The initial learning rate.\n+        decay_steps: A Python integer. Must be positive. See the decay\n+            computation above.\n+        end_learning_rate: A Python float. The minimal end learning rate.\n+        power: A Python float. The power of the polynomial. Defaults to\n+            `1.0`.\n+        cycle: A boolean, whether it should cycle beyond decay_steps.\n+        name: String.  Optional name of the operation. Defaults to\n+            `\"PolynomialDecay\"`.\n+\n+    Returns:\n+        A 1-arg callable learning rate schedule that takes the current optimizer\n+        step and outputs the decayed learning rate, a scalar tensor of the\n+        same type as `initial_learning_rate`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        initial_learning_rate,\n+        decay_steps,\n+        end_learning_rate=0.0001,\n+        power=1.0,\n+        cycle=False,\n+        name=\"PolynomialDecay\",\n+    ):\n+        super().__init__()\n+\n+        self.initial_learning_rate = initial_learning_rate\n+        self.decay_steps = decay_steps\n+        self.end_learning_rate = end_learning_rate\n+        self.power = power\n+        self.cycle = cycle\n+        self.name = name\n+\n+    def __call__(self, step):\n+        with ops.name_scope(self.name):\n+            initial_learning_rate = ops.convert_to_tensor(\n+                self.initial_learning_rate\n+            )\n+            dtype = initial_learning_rate.dtype\n+            end_learning_rate = ops.cast(self.end_learning_rate, dtype)\n+            power = ops.cast(self.power, dtype)\n+\n+            global_step_recomp = ops.cast(step, dtype)\n+            decay_steps_recomp = ops.cast(self.decay_steps, dtype)\n+            if self.cycle:\n+                # Find the first multiple of decay_steps that is bigger than\n+                # global_step. If global_step is zero set the multiplier to 1\n+                multiplier = ops.where(\n+                    ops.equal(global_step_recomp, 0),\n+                    1.0,\n+                    ops.ceil(global_step_recomp / self.decay_steps),\n+                )\n+                decay_steps_recomp = ops.multiply(\n+                    decay_steps_recomp, multiplier\n+                )\n+            else:\n+                # Make sure that the global_step used is not bigger than\n+                # decay_steps.\n+                global_step_recomp = ops.minimum(\n+                    global_step_recomp, decay_steps_recomp\n+                )\n+\n+            p = ops.divide(global_step_recomp, decay_steps_recomp)\n+            return ops.add(\n+                ops.multiply(\n+                    initial_learning_rate - end_learning_rate,\n+                    ops.power(1 - p, power),\n+                ),\n+                end_learning_rate,\n+            )\n+\n+    def get_config(self):\n+        return {\n+            \"initial_learning_rate\": self.initial_learning_rate,\n+            \"decay_steps\": self.decay_steps,\n+            \"end_learning_rate\": self.end_learning_rate,\n+            \"power\": self.power,\n+            \"cycle\": self.cycle,\n+            \"name\": self.name,\n+        }\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.InverseTimeDecay\")\n+class InverseTimeDecay(LearningRateSchedule):\n+    \"\"\"A `LearningRateSchedule` that uses an inverse time decay schedule.\n+\n+    When training a model, it is often useful to lower the learning rate as\n+    the training progresses. This schedule applies the inverse decay function\n+    to an optimizer step, given a provided initial learning rate.\n+    It requires a `step` value to compute the decayed learning rate. You can\n+    just pass a backend variable that you increment at each training step.\n+\n+    The schedule is a 1-arg callable that produces a decayed learning\n+    rate when passed the current optimizer step. This can be useful for changing\n+    the learning rate value across different invocations of optimizer functions.\n+    It is computed as:\n+\n+    ```python\n+    def decayed_learning_rate(step):\n+        return initial_learning_rate / (1 + decay_rate * step / decay_step)\n+    ```\n+\n+    or, if `staircase` is `True`, as:\n+\n+    ```python\n+    def decayed_learning_rate(step):\n+        return initial_learning_rate /\n+               (1 + decay_rate * floor(step / decay_step))\n+    ```\n+\n+    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    as the learning rate.\n+    Example: Fit a Keras model when decaying 1/t with a rate of 0.5:\n+\n+    ```python\n+    ...\n+    initial_learning_rate = 0.1\n+    decay_steps = 1.0\n+    decay_rate = 0.5\n+    learning_rate_fn = keras_core.optimizers.schedules.InverseTimeDecay(\n+        initial_learning_rate, decay_steps, decay_rate)\n+\n+    model.compile(optimizer=keras_core.optimizers.SGD(\n+                      learning_rate=learning_rate_fn),\n+                  loss='sparse_categorical_crossentropy',\n+                  metrics=['accuracy'])\n+\n+    model.fit(data, labels, epochs=5)\n+    ```\n+\n+    Args:\n+        initial_learning_rate: A Python float. The initial learning rate.\n+        decay_steps: How often to apply decay.\n+        decay_rate: A Python number.  The decay rate.\n+        staircase: Whether to apply decay in a discrete staircase, as o\n+        pposed to continuous, fashion.\n+        name: String.  Optional name of the operation.  Defaults to\n+            `\"InverseTimeDecay\"`.\n+\n+    Returns:\n+        A 1-arg callable learning rate schedule that takes the current optimizer\n+        step and outputs the decayed learning rate, a scalar tensor of the\n+        same type as `initial_learning_rate`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        initial_learning_rate,\n+        decay_steps,\n+        decay_rate,\n+        staircase=False,\n+        name=\"InverseTimeDecay\",\n+    ):\n+        super().__init__()\n+\n+        self.initial_learning_rate = initial_learning_rate\n+        self.decay_steps = decay_steps\n+        self.decay_rate = decay_rate\n+        self.staircase = staircase\n+        self.name = name\n+\n+    def __call__(self, step):\n+        with ops.name_scope(self.name):\n+            initial_learning_rate = ops.convert_to_tensor(\n+                self.initial_learning_rate\n+            )\n+            dtype = initial_learning_rate.dtype\n+            decay_steps = ops.cast(self.decay_steps, dtype)\n+            decay_rate = ops.cast(self.decay_rate, dtype)\n+\n+            global_step_recomp = ops.cast(step, dtype)\n+            p = global_step_recomp / decay_steps\n+            if self.staircase:\n+                p = ops.floor(p)\n+            const = ops.cast(ops.array(1), dtype)\n+            denom = ops.add(const, ops.multiply(decay_rate, p))\n+            return ops.divide(initial_learning_rate, denom)\n+\n+    def get_config(self):\n+        return {\n+            \"initial_learning_rate\": self.initial_learning_rate,\n+            \"decay_steps\": self.decay_steps,\n+            \"decay_rate\": self.decay_rate,\n+            \"staircase\": self.staircase,\n+            \"name\": self.name,\n+        }\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.CosineDecay\")\n+class CosineDecay(LearningRateSchedule):\n+    \"\"\"A `LearningRateSchedule` that uses a cosine decay with optional warmup.\n+\n+    See [Loshchilov & Hutter, ICLR2016](https://arxiv.org/abs/1608.03983),\n+    SGDR: Stochastic Gradient Descent with Warm Restarts.\n+\n+    For the idea of a linear warmup of our learning rate,\n+    see [Goyal et al.](https://arxiv.org/pdf/1706.02677.pdf).\n+\n+    When we begin training a model, we often want an initial increase in our\n+    learning rate followed by a decay. If `warmup_target` is an int, this\n+    schedule applies a linear increase per optimizer step to our learning rate\n+    from `initial_learning_rate` to `warmup_target` for a duration of\n+    `warmup_steps`. Afterwards, it applies a cosine decay function taking our\n+    learning rate from `warmup_target` to `alpha` for a duration of\n+    `decay_steps`. If `warmup_target` is None we skip warmup and our decay\n+    will take our learning rate from `initial_learning_rate` to `alpha`.\n+    It requires a `step` value to  compute the learning rate. You can\n+    just pass a backend variable that you increment at each training step.\n+\n+    The schedule is a 1-arg callable that produces a warmup followed by a\n+    decayed learning rate when passed the current optimizer step. This can be\n+    useful for changing the learning rate value across different invocations of\n+    optimizer functions.\n+\n+    Our warmup is computed as:\n+\n+    ```python\n+    def warmup_learning_rate(step):\n+        completed_fraction = step / warmup_steps\n+        total_delta = target_warmup - initial_learning_rate\n+        return completed_fraction * total_delta\n+    ```\n+\n+    And our decay is computed as:\n+\n+    ```python\n+    if warmup_target is None:\n+        initial_decay_lr = initial_learning_rate\n+    else:\n+        initial_decay_lr = warmup_target\n+\n+    def decayed_learning_rate(step):\n+        step = min(step, decay_steps)\n+        cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))\n+        decayed = (1 - alpha) * cosine_decay + alpha\n+        return initial_decay_lr * decayed\n+    ```\n+\n+    Example usage without warmup:\n+\n+    ```python\n+    decay_steps = 1000\n+    initial_learning_rate = 0.1\n+    lr_decayed_fn = keras_core.optimizers.schedules.CosineDecay(\n+        initial_learning_rate, decay_steps)\n+    ```\n+\n+    Example usage with warmup:\n+\n+    ```python\n+    decay_steps = 1000\n+    initial_learning_rate = 0\n+    warmup_steps = 1000\n+    target_learning_rate = 0.1\n+    lr_warmup_decayed_fn = keras_core.optimizers.schedules.CosineDecay(\n+        initial_learning_rate, decay_steps, warmup_target=target_learning_rate,\n+        warmup_steps=warmup_steps\n+    )\n+    ```\n+\n+    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    as the learning rate. The learning rate schedule is also serializable and\n+    deserializable using `keras_core.optimizers.schedules.serialize` and\n+    `keras_core.optimizers.schedules.deserialize`.\n+\n+    Args:\n+        initial_learning_rate: A Python float. The initial learning rate.\n+        decay_steps: A Python int. Number of steps to decay over.\n+        alpha: A Python float. Minimum learning rate value for decay as a\n+            fraction of `initial_learning_rate`.\n+        name: String. Optional name of the operation.  Defaults to\n+            `\"CosineDecay\"`.\n+        warmup_target: A Python float. The target learning rate for our\n+            warmup phase. Will cast to the `initial_learning_rate` datatype.\n+            Setting to `None` will skip warmup and begins decay phase from\n+            `initial_learning_rate`. Otherwise scheduler will warmup from\n+            `initial_learning_rate` to `warmup_target`.\n+        warmup_steps: A Python int. Number of steps to warmup over.\n+\n+    Returns:\n+        A 1-arg callable learning rate schedule that takes the current optimizer\n+        step and outputs the decayed learning rate, a scalar tensor of the\n+        same type as `initial_learning_rate`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        initial_learning_rate,\n+        decay_steps,\n+        alpha=0.0,\n+        name=\"CosineDecay\",\n+        warmup_target=None,\n+        warmup_steps=0,\n+    ):\n+        super().__init__()\n+\n+        self.initial_learning_rate = initial_learning_rate\n+        self.decay_steps = decay_steps\n+        self.alpha = alpha\n+        self.name = name\n+        self.warmup_steps = warmup_steps\n+        self.warmup_target = warmup_target\n+\n+    def _decay_function(self, step, decay_steps, decay_from_lr, dtype):\n+        with ops.name_scope(self.name):\n+            completed_fraction = step / decay_steps\n+            pi = ops.array(math.pi, dtype=dtype)\n+            cosine_decayed = 0.5 * (1.0 + ops.cos(pi * completed_fraction))\n+            decayed = (1 - self.alpha) * cosine_decayed + self.alpha\n+            return ops.multiply(decay_from_lr, decayed)\n+\n+    def _warmup_function(\n+        self, step, warmup_steps, warmup_target, initial_learning_rate\n+    ):\n+        with ops.name_scope(self.name):\n+            completed_fraction = step / warmup_steps\n+            total_step_delta = warmup_target - initial_learning_rate\n+            return total_step_delta * completed_fraction + initial_learning_rate\n+\n+    def __call__(self, step):\n+        with ops.name_scope(self.name):\n+            initial_learning_rate = ops.convert_to_tensor(\n+                self.initial_learning_rate\n+            )\n+            dtype = initial_learning_rate.dtype\n+            decay_steps = ops.cast(self.decay_steps, dtype)\n+            global_step_recomp = ops.cast(step, dtype)\n+\n+            if self.warmup_target is None:\n+                global_step_recomp = ops.minimum(\n+                    global_step_recomp, decay_steps\n+                )\n+                return self._decay_function(\n+                    global_step_recomp,\n+                    decay_steps,\n+                    initial_learning_rate,\n+                    dtype,\n+                )\n+\n+            warmup_target = ops.cast(self.warmup_target, dtype)\n+            warmup_steps = ops.cast(self.warmup_steps, dtype)\n+\n+            global_step_recomp = ops.minimum(\n+                global_step_recomp, decay_steps + warmup_steps\n+            )\n+\n+            return ops.cond(\n+                global_step_recomp < warmup_steps,\n+                lambda: self._warmup_function(\n+                    global_step_recomp,\n+                    warmup_steps,\n+                    warmup_target,\n+                    initial_learning_rate,\n+                ),\n+                lambda: self._decay_function(\n+                    global_step_recomp - warmup_steps,\n+                    decay_steps,\n+                    warmup_target,\n+                    dtype,\n+                ),\n+            )\n+\n+    def get_config(self):\n+        return {\n+            \"initial_learning_rate\": self.initial_learning_rate,\n+            \"decay_steps\": self.decay_steps,\n+            \"alpha\": self.alpha,\n+            \"name\": self.name,\n+            \"warmup_target\": self.warmup_target,\n+            \"warmup_steps\": self.warmup_steps,\n+        }\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.CosineDecayRestarts\")\n+class CosineDecayRestarts(LearningRateSchedule):\n+    \"\"\"A `LearningRateSchedule` that uses a cosine decay schedule with restarts.\n+\n+    See [Loshchilov & Hutter, ICLR2016](https://arxiv.org/abs/1608.03983),\n+    SGDR: Stochastic Gradient Descent with Warm Restarts.\n+\n+    When training a model, it is often useful to lower the learning rate as\n+    the training progresses. This schedule applies a cosine decay function with\n+    restarts to an optimizer step, given a provided initial learning rate.\n+    It requires a `step` value to compute the decayed learning rate. You can\n+    just pass a backend variable that you increment at each training step.\n+\n+    The schedule is a 1-arg callable that produces a decayed learning\n+    rate when passed the current optimizer step. This can be useful for changing\n+    the learning rate value across different invocations of optimizer functions.\n+\n+    The learning rate multiplier first decays\n+    from 1 to `alpha` for `first_decay_steps` steps. Then, a warm\n+    restart is performed. Each new warm restart runs for `t_mul` times more\n+    steps and with `m_mul` times initial learning rate as the new learning rate.\n+\n+    Example usage:\n+    ```python\n+    first_decay_steps = 1000\n+    lr_decayed_fn = (\n+        keras_core.optimizers.schedules.CosineDecayRestarts(\n+            initial_learning_rate,\n+            first_decay_steps))\n+    ```\n+\n+    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    as the learning rate. The learning rate schedule is also serializable and\n+    deserializable using `keras_core.optimizers.schedules.serialize` and\n+    `keras_core.optimizers.schedules.deserialize`.\n+\n+    Args:\n+        initial_learning_rate: A Python float. The initial learning rate.\n+        first_decay_steps: A Python integer. Number of steps to decay over.\n+        t_mul: A Python float. Used to derive the number of iterations in\n+            the i-th period.\n+        m_mul: A Python float. Used to derive the initial learning rate of\n+            the i-th period.\n+        alpha: A Python float. Minimum learning rate value as a fraction of\n+            the `initial_learning_rate`.\n+        name: String. Optional name of the operation. Defaults to\n+            `\"SGDRDecay\"`.\n+\n+    Returns:\n+        A 1-arg callable learning rate schedule that takes the current optimizer\n+        step and outputs the decayed learning rate, a scalar tensor of the\n+        same type as `initial_learning_rate`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        initial_learning_rate,\n+        first_decay_steps,\n+        t_mul=2.0,\n+        m_mul=1.0,\n+        alpha=0.0,\n+        name=\"SGDRDecay\",\n+    ):\n+        super().__init__()\n+\n+        self.initial_learning_rate = initial_learning_rate\n+        self.first_decay_steps = first_decay_steps\n+        self._t_mul = t_mul\n+        self._m_mul = m_mul\n+        self.alpha = alpha\n+        self.name = name\n+\n+    def __call__(self, step):\n+        with ops.name_scope(self.name):\n+            initial_learning_rate = ops.convert_to_tensor(\n+                self.initial_learning_rate\n+            )\n+            dtype = initial_learning_rate.dtype\n+            first_decay_steps = ops.cast(self.first_decay_steps, dtype)\n+            alpha = ops.cast(self.alpha, dtype)\n+            t_mul = ops.cast(self._t_mul, dtype)\n+            m_mul = ops.cast(self._m_mul, dtype)\n+\n+            global_step_recomp = ops.cast(step, dtype)\n+            completed_fraction = global_step_recomp / first_decay_steps\n+\n+            def compute_step(completed_fraction, geometric=False):\n+                \"\"\"Helper for `cond` operation.\"\"\"\n+                if geometric:\n+                    i_restart = ops.floor(\n+                        ops.log(1.0 - completed_fraction * (1.0 - t_mul))\n+                        / ops.log(t_mul)\n+                    )\n+\n+                    sum_r = (1.0 - t_mul**i_restart) / (1.0 - t_mul)\n+                    completed_fraction = (\n+                        completed_fraction - sum_r\n+                    ) / t_mul**i_restart\n+\n+                else:\n+                    i_restart = ops.floor(completed_fraction)\n+                    completed_fraction -= i_restart\n+\n+                return i_restart, completed_fraction\n+\n+            i_restart, completed_fraction = ops.cond(\n+                ops.equal(t_mul, 1.0),\n+                lambda: compute_step(completed_fraction, geometric=False),\n+                lambda: compute_step(completed_fraction, geometric=True),\n+            )\n+\n+            m_fac = m_mul**i_restart\n+            cosine_decayed = (\n+                0.5\n+                * m_fac\n+                * (\n+                    1.0\n+                    + ops.cos(\n+                        ops.array(math.pi, dtype=dtype) * completed_fraction\n+                    )\n+                )\n+            )\n+            decayed = (1 - alpha) * cosine_decayed + alpha\n+\n+            return ops.multiply(initial_learning_rate, decayed)\n+\n+    def get_config(self):\n+        return {\n+            \"initial_learning_rate\": self.initial_learning_rate,\n+            \"first_decay_steps\": self.first_decay_steps,\n+            \"t_mul\": self._t_mul,\n+            \"m_mul\": self._m_mul,\n+            \"alpha\": self.alpha,\n+            \"name\": self.name,\n+        }\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.serialize\")\n+def serialize(learning_rate_schedule):\n+    \"\"\"Serializes a `LearningRateSchedule` into a JSON-compatible dict.\n+\n+    Args:\n+        learning_rate_schedule: The `LearningRateSchedule` object to serialize.\n+\n+    Returns:\n+        A JSON-serializable dict representing the object's config.\n+\n+    Example:\n+\n+    >>> lr_schedule = keras_core.optimizers.schedules.ExponentialDecay(\n+    ...     0.1, decay_steps=100000, decay_rate=0.96, staircase=True)\n+    >>> keras_core.optimizers.schedules.serialize(lr_schedule)\n+    {'module': 'keras_core.optimizers.schedules',\n+    'class_name': 'ExponentialDecay', 'config': {...},\n+    'registered_name': None}\n+    \"\"\"\n+    return serialization_lib.serialize_keras_object(learning_rate_schedule)\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.deserialize\")\n+def deserialize(config, custom_objects=None):\n+    \"\"\"Instantiates a `LearningRateSchedule` object from a serialized form.\n+\n+    Args:\n+        config: The serialized form of the `LearningRateSchedule`. Dictionary of\n+            the form {'class_name': str, 'config': dict}.\n+        custom_objects: A dictionary mapping class names (or function names) of\n+            custom (non-Keras) objects to class/functions.\n+\n+    Returns:\n+        A `LearningRateSchedule` object.\n+\n+    Example:\n+\n+    ```python\n+    # Configuration for PolynomialDecay\n+    config = {\n+        'class_name': 'PolynomialDecay',\n+        'config': {'cycle': False,\n+            'decay_steps': 10000,\n+            'end_learning_rate': 0.01,\n+            'initial_learning_rate': 0.1,\n+            'name': None,\n+            'power': 0.5\n+        }\n+    }\n+    lr_schedule = keras_core.optimizers.schedules.deserialize(config)\n+    ```\n+    \"\"\"\n+    return serialization_lib.deserialize_keras_object(\n+        config,\n+        module_objects=globals(),\n+        custom_objects=custom_objects,\n+        printable_module_name=\"decay\",\n+    )\n\n@@ -0,0 +1,452 @@\n+\"\"\"Tests for learning rate schedule API.\"\"\"\n+\n+import math\n+\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import testing\n+from keras_core.optimizers.schedules import learning_rate_schedule\n+\n+\n+class ExponentialDecayTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            learning_rate_schedule.ExponentialDecay(\n+                initial_learning_rate=0.05,\n+                decay_steps=10,\n+                decay_rate=0.96,\n+                staircase=True,\n+                name=\"my_ed\",\n+            )\n+        )\n+\n+    def test_continuous(self):\n+        step = 5\n+        decayed_lr = learning_rate_schedule.ExponentialDecay(0.05, 10, 0.96)\n+        expected = 0.05 * 0.96 ** (5.0 / 10.0)\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_staircase(self):\n+        step = backend.Variable(1)\n+        decayed_lr = learning_rate_schedule.ExponentialDecay(\n+            0.1, 3, 0.96, staircase=True\n+        )\n+\n+        # No change to learning rate due to staircase\n+        expected = 0.1\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+        expected = 0.1\n+        step.assign(2)\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+        # Decayed learning rate\n+        expected = 0.1 * 0.96 ** (100 // 3)\n+        step.assign(100)\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_variables(self):\n+        step = backend.Variable(1)\n+        decayed_lr = learning_rate_schedule.ExponentialDecay(\n+            0.1, 3, 0.96, staircase=True\n+        )\n+\n+        # No change to learning rate\n+        step.assign(1)\n+        self.assertAllClose(decayed_lr(step), 0.1, 1e-6)\n+        step.assign(2)\n+        self.assertAllClose(decayed_lr(step), 0.1, 1e-6)\n+        # Decayed learning rate\n+        step.assign(100)\n+        expected = 0.1 * 0.96 ** (100 // 3)\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+\n+class PiecewiseConstantDecayTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            learning_rate_schedule.PiecewiseConstantDecay(\n+                boundaries=[10, 20], values=[1, 2, 3], name=\"my_pcd\"\n+            )\n+        )\n+\n+    def test_piecewise_values(self):\n+        x = backend.Variable(-999)\n+        decayed_lr = learning_rate_schedule.PiecewiseConstantDecay(\n+            [100, 110, 120], [1.0, 0.1, 0.01, 0.001]\n+        )\n+\n+        self.assertAllClose(decayed_lr(x), 1.0, 1e-6)\n+        x.assign(100)\n+        self.assertAllClose(decayed_lr(x), 1.0, 1e-6)\n+        x.assign(105)\n+        self.assertAllClose(decayed_lr(x), 0.1, 1e-6)\n+        x.assign(110)\n+        self.assertAllClose(decayed_lr(x), 0.1, 1e-6)\n+        x.assign(120)\n+        self.assertAllClose(decayed_lr(x), 0.01, 1e-6)\n+        x.assign(999)\n+        self.assertAllClose(decayed_lr(x), 0.001, 1e-6)\n+\n+    def test_boundary_values(self):\n+        # Test casting boundaries from int32 to int64.\n+        x_int64 = backend.Variable(0, dtype=\"int64\")\n+        boundaries, values = [1, 2, 3], [0.4, 0.5, 0.6, 0.7]\n+        decayed_lr = learning_rate_schedule.PiecewiseConstantDecay(\n+            boundaries, values\n+        )\n+\n+        self.assertAllClose(decayed_lr(x_int64), 0.4, 1e-6)\n+        x_int64.assign(1)\n+        self.assertAllClose(decayed_lr(x_int64), 0.4, 1e-6)\n+        x_int64.assign(2)\n+        self.assertAllClose(decayed_lr(x_int64), 0.5, 1e-6)\n+        x_int64.assign(3)\n+        self.assertAllClose(decayed_lr(x_int64), 0.6, 1e-6)\n+        x_int64.assign(4)\n+        self.assertAllClose(decayed_lr(x_int64), 0.7, 1e-6)\n+\n+\n+class LinearDecayTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            learning_rate_schedule.PolynomialDecay(\n+                initial_learning_rate=0.1,\n+                decay_steps=100,\n+                end_learning_rate=0.005,\n+                power=1.0,\n+                cycle=False,\n+                name=\"my_ld\",\n+            )\n+        )\n+\n+    def test_halfway(self):\n+        step = 5\n+        lr = 0.05\n+        end_lr = 0.0\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n+        expected = lr * 0.5\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_end(self):\n+        step = 10\n+        lr = 0.05\n+        end_lr = 0.001\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n+        expected = end_lr\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_halfway_with_end(self):\n+        step = 5\n+        lr = 0.05\n+        end_lr = 0.001\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n+        expected = (lr + end_lr) * 0.5\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_beyond_end(self):\n+        step = 15\n+        lr = 0.05\n+        end_lr = 0.001\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n+        expected = end_lr\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_beyond_end_with_cycle(self):\n+        step = 15\n+        lr = 0.05\n+        end_lr = 0.001\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, cycle=True\n+        )\n+        expected = (lr - end_lr) * 0.25 + end_lr\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+\n+class SqrtDecayTest(testing.TestCase):\n+    def test_halfway(self):\n+        step = 5\n+        lr = 0.05\n+        end_lr = 0.0\n+        power = 0.5\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, power=power\n+        )\n+        expected = lr * 0.5**power\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_end(self):\n+        step = 10\n+        lr = 0.05\n+        end_lr = 0.001\n+        power = 0.5\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, power=power\n+        )\n+        expected = end_lr\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_halfway_with_end(self):\n+        step = 5\n+        lr = 0.05\n+        end_lr = 0.001\n+        power = 0.5\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, power=power\n+        )\n+        expected = (lr - end_lr) * 0.5**power + end_lr\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_beyond_end(self):\n+        step = 15\n+        lr = 0.05\n+        end_lr = 0.001\n+        power = 0.5\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, power=power\n+        )\n+        expected = end_lr\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_beyond_end_with_cycle(self):\n+        step = 15\n+        lr = 0.05\n+        end_lr = 0.001\n+        power = 0.5\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, power=power, cycle=True\n+        )\n+        expected = (lr - end_lr) * 0.25**power + end_lr\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_begin_with_cycle(self):\n+        lr = 0.001\n+        decay_steps = 10\n+        step = 0\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, decay_steps, cycle=True\n+        )\n+        expected = lr\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+\n+class InverseTimeDecayTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            learning_rate_schedule.InverseTimeDecay(\n+                initial_learning_rate=0.05,\n+                decay_steps=10,\n+                decay_rate=0.96,\n+                staircase=True,\n+                name=\"my_itd\",\n+            )\n+        )\n+\n+    def test_decay(self):\n+        initial_lr = 0.1\n+        k = 10\n+        decay_rate = 0.96\n+        step = backend.Variable(0)\n+        decayed_lr = learning_rate_schedule.InverseTimeDecay(\n+            initial_lr, k, decay_rate\n+        )\n+\n+        for i in range(k + 1):\n+            expected = initial_lr / (1 + i / k * decay_rate)\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+            step.assign(step + 1)\n+\n+    def test_staircase(self):\n+        initial_lr = 0.1\n+        k = 10\n+        decay_rate = 0.96\n+        step = backend.Variable(0)\n+        decayed_lr = learning_rate_schedule.InverseTimeDecay(\n+            initial_lr, k, decay_rate, staircase=True\n+        )\n+\n+        for i in range(k + 1):\n+            expected = initial_lr / (1 + decay_rate * (i // k))\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+            step.assign(step + 1)\n+\n+\n+class CosineDecayTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            learning_rate_schedule.CosineDecay(\n+                initial_learning_rate=0.05,\n+                decay_steps=10,\n+                alpha=0.1,\n+                warmup_target=0.2,\n+                warmup_steps=2,\n+                name=\"my_cd\",\n+            )\n+        )\n+\n+    def np_cosine_decay(self, step, decay_steps, alpha=0.0):\n+        step = min(step, decay_steps)\n+        completed_fraction = step / decay_steps\n+        decay = 0.5 * (1.0 + math.cos(math.pi * completed_fraction))\n+        return (1.0 - alpha) * decay + alpha\n+\n+    def test_decay(self):\n+        num_training_steps = 1000\n+        initial_lr = 1.0\n+        for step in range(0, 1500, 250):\n+            decayed_lr = learning_rate_schedule.CosineDecay(\n+                initial_lr, num_training_steps\n+            )\n+            expected = self.np_cosine_decay(step, num_training_steps)\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def linear_warmup(self, step, warmup_steps, initial_lr, target_lr):\n+        completed_fraction = step / warmup_steps\n+        total_delta = target_lr - initial_lr\n+        return completed_fraction * total_delta\n+\n+    def test_warmup(self):\n+        warmup_steps = 1500\n+        initial_lr = 0.0\n+        target_lr = 10.0\n+        for step in range(0, 1500, 250):\n+            lr = learning_rate_schedule.CosineDecay(\n+                initial_lr,\n+                0,\n+                warmup_target=target_lr,\n+                warmup_steps=warmup_steps,\n+            )\n+            expected = self.linear_warmup(\n+                step, warmup_steps, initial_lr, target_lr\n+            )\n+            self.assertAllClose(lr(step), expected)\n+\n+    def test_alpha(self):\n+        num_training_steps = 1000\n+        initial_lr = 1.0\n+        alpha = 0.1\n+        for step in range(0, 1500, 250):\n+            decayed_lr = learning_rate_schedule.CosineDecay(\n+                initial_lr, num_training_steps, alpha\n+            )\n+            expected = self.np_cosine_decay(step, num_training_steps, alpha)\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_float64(self):\n+        num_training_steps = 1000\n+        initial_lr = np.float64(1.0)\n+        for step in range(0, 1500, 250):\n+            decayed_lr = learning_rate_schedule.CosineDecay(\n+                initial_lr, num_training_steps\n+            )\n+            expected = self.np_cosine_decay(step, num_training_steps)\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_warmup_decay(self):\n+        warmup_steps = 2000\n+        decay_steps = 1000\n+        initial_lr = 0.0\n+        target_lr = 10.0\n+        for step in range(0, 3000, 250):\n+            lr = learning_rate_schedule.CosineDecay(\n+                initial_lr,\n+                decay_steps,\n+                warmup_target=target_lr,\n+                warmup_steps=warmup_steps,\n+            )\n+            if step < warmup_steps + 1:\n+                expected = self.linear_warmup(\n+                    step, warmup_steps, initial_lr, target_lr\n+                )\n+            else:\n+                expected = target_lr * self.np_cosine_decay(\n+                    step - warmup_steps, decay_steps\n+                )\n+            self.assertAllClose(lr(step), expected)\n+\n+\n+class CosineDecayRestartsTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            learning_rate_schedule.CosineDecayRestarts(\n+                initial_learning_rate=0.05,\n+                first_decay_steps=10,\n+                alpha=0.1,\n+                t_mul=3.0,\n+                m_mul=4.0,\n+                name=\"my_cdr\",\n+            )\n+        )\n+\n+    def np_cosine_decay_restarts(\n+        self, step, decay_steps, t_mul=2.0, m_mul=1.0, alpha=0.0\n+    ):\n+        fac = 1.0\n+        while step >= decay_steps:\n+            step -= decay_steps\n+            decay_steps *= t_mul\n+            fac *= m_mul\n+\n+        completed_fraction = step / decay_steps\n+        decay = fac * 0.5 * (1.0 + math.cos(math.pi * completed_fraction))\n+        return (1.0 - alpha) * decay + alpha\n+\n+    def test_decay(self):\n+        num_training_steps = 1000\n+        initial_lr = 1.0\n+        for step in range(0, 1500, 250):\n+            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+                initial_lr, num_training_steps\n+            )\n+            expected = self.np_cosine_decay_restarts(step, num_training_steps)\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_float64(self):\n+        num_training_steps = 1000\n+        initial_lr = np.float64(1.0)\n+        for step in range(0, 1500, 250):\n+            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+                initial_lr, num_training_steps\n+            )\n+            expected = self.np_cosine_decay_restarts(step, num_training_steps)\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_alpha(self):\n+        num_training_steps = 1000\n+        initial_lr = 1.0\n+        alpha = 0.1\n+        for step in range(0, 1500, 250):\n+            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+                initial_lr, num_training_steps, alpha=alpha\n+            )\n+            expected = self.np_cosine_decay_restarts(\n+                step, num_training_steps, alpha=alpha\n+            )\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_mmul(self):\n+        num_training_steps = 1000\n+        initial_lr = 1.0\n+        m_mul = 0.9\n+        for step in range(0, 1500, 250):\n+            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+                initial_lr, num_training_steps, m_mul=m_mul\n+            )\n+            expected = self.np_cosine_decay_restarts(\n+                step, num_training_steps, m_mul=m_mul\n+            )\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_tmul(self):\n+        num_training_steps = 1000\n+        initial_lr = 1.0\n+        t_mul = 1.0\n+        for step in range(0, 1500, 250):\n+            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+                initial_lr, num_training_steps, t_mul=t_mul\n+            )\n+            expected = self.np_cosine_decay_restarts(\n+                step, num_training_steps, t_mul=t_mul\n+            )\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
