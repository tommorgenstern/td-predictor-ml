{"custom_id": "keras#9fd057bfd4d70bee6061d957e6993eda6cccc0d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 15 | Files Changed: 3 | Hunks: 8 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 39 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -194,9 +194,7 @@ class FloatDTypePolicy(DTypePolicy):\n         return f'<FloatDTypePolicy \"{self._name}\">'\n \n \n-@keras_export(\n-    [\"keras.QuantizedDTypePolicy\", \"keras.dtype_policies.QuantizedDTypePolicy\"]\n-)\n+@keras_export(\"keras.dtype_policies.QuantizedDTypePolicy\")\n class QuantizedDTypePolicy(DTypePolicy):\n     def __init__(self, name):\n         super().__init__(name)\n@@ -258,12 +256,7 @@ class QuantizedDTypePolicy(DTypePolicy):\n         return valid_policies\n \n \n-@keras_export(\n-    [\n-        \"keras.QuantizedFloat8DTypePolicy\",\n-        \"keras.dtype_policies.QuantizedFloat8DTypePolicy\",\n-    ]\n-)\n+@keras_export(\"keras.dtype_policies.QuantizedFloat8DTypePolicy\")\n class QuantizedFloat8DTypePolicy(QuantizedDTypePolicy):\n     def __init__(self, name, amax_history_length=1024):\n         super().__init__(name)\n\n@@ -57,7 +57,7 @@ class Quantizer:\n         raise NotImplementedError(f\"{self} does not implement get_config()\")\n \n \n-@keras_export([\"keras.quantizers.abs_max_quantize\"])\n+@keras_export(\"keras.quantizers.abs_max_quantize\")\n def abs_max_quantize(\n     inputs,\n     axis,\n@@ -75,7 +75,7 @@ def abs_max_quantize(\n     return outputs, scale\n \n \n-@keras_export([\"keras.AbsMaxQuantizer\", \"keras.quantizers.AbsMaxQuantizer\"])\n+@keras_export(\"keras.quantizers.AbsMaxQuantizer\")\n class AbsMaxQuantizer(Quantizer):\n     def __init__(\n         self,\n@@ -109,7 +109,7 @@ class AbsMaxQuantizer(Quantizer):\n \"\"\"Float8-related methods\"\"\"\n \n \n-@keras_export([\"keras.quantizers.compute_float8_scale\"])\n+@keras_export(\"keras.quantizers.compute_float8_scale\")\n def compute_float8_scale(amax, scale, dtype_max, margin=0):\n     # The algorithm for computing the new scale is sourced from\n     # https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/jax.html#transformer_engine.jax.update_fp8_metas\n@@ -122,7 +122,7 @@ def compute_float8_scale(amax, scale, dtype_max, margin=0):\n     return ops.reciprocal(sf)\n \n \n-@keras_export([\"keras.quantizers.compute_float8_amax_history\"])\n+@keras_export(\"keras.quantizers.compute_float8_amax_history\")\n def compute_float8_amax_history(x, amax_history):\n     amax_update = ops.cast(ops.max(ops.abs(x)), amax_history.dtype)\n     new_amax_history = ops.scatter_update(\n@@ -133,7 +133,7 @@ def compute_float8_amax_history(x, amax_history):\n     return new_amax_history\n \n \n-@keras_export([\"keras.quantizers.quantize_and_dequantize\"])\n+@keras_export(\"keras.quantizers.quantize_and_dequantize\")\n def quantize_and_dequantize(inputs, scale, quantized_dtype, compute_dtype):\n     # Quantize\n     quantized_dtype_max = ops.cast(\n\n@@ -1,7 +1,7 @@\n from keras.src.api_export import keras_export\n \n # Unique source of truth for the version number.\n-__version__ = \"3.2.1\"\n+__version__ = \"3.3.0\"\n \n \n @keras_export(\"keras.version\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e57b138cec3c9a204f7f686355a1a5bf76fe63e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 125 | Lines Deleted: 19 | Files Changed: 6 | Hunks: 20 | Methods Changed: 14 | Complexity Δ (Sum/Max): 12/9 | Churn Δ: 144 | Churn Cumulative: 286 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 12 | DMM Complexity: 0.9253731343283582\n\nDIFF:\n@@ -437,7 +437,8 @@ class JAXTrainer(base_trainer.Trainer):\n                 }\n \n                 # Callbacks\n-                callbacks.on_train_batch_end(step, self._pythonify_logs(logs))\n+                logs = self._pythonify_logs(logs)\n+                callbacks.on_train_batch_end(step, logs)\n                 if self.stop_training:\n                     break\n \n@@ -446,12 +447,12 @@ class JAXTrainer(base_trainer.Trainer):\n             # bottleneck.\n             self.jax_state_sync()\n \n-            # Override with model metrics instead of last step logs\n+            # Override with model metrics instead of last step logs if needed.\n             # The jax spmd_mode is need for multi-process context, since the\n             # metrics values are replicated, and we don't want to do a all\n             # gather, and only need the local copy of the value.\n             with jax.spmd_mode(\"allow_all\"):\n-                epoch_logs = self.get_metrics_result()\n+                epoch_logs = dict(self._get_metrics_result_or_logs(logs))\n \n             # Run validation.\n             if validation_data is not None and self._should_eval(\n@@ -585,7 +586,8 @@ class JAXTrainer(base_trainer.Trainer):\n                 \"non_trainable_variables\": non_trainable_variables,\n                 \"metrics_variables\": metrics_variables,\n             }\n-            callbacks.on_test_batch_end(step, self._pythonify_logs(logs))\n+            logs = self._pythonify_logs(logs)\n+            callbacks.on_test_batch_end(step, logs)\n             if self.stop_evaluating:\n                 break\n \n@@ -596,7 +598,7 @@ class JAXTrainer(base_trainer.Trainer):\n         # metrics values are replicated, and we don't want to do a all\n         # gather, and only need the local copy of the value.\n         with jax.spmd_mode(\"allow_all\"):\n-            logs = self.get_metrics_result()\n+            logs = self._get_metrics_result_or_logs(logs)\n         callbacks.on_test_end(logs)\n         self._jax_state = None\n         if return_dict:\n\n@@ -269,10 +269,11 @@ class NumpyTrainer(base_trainer.Trainer):\n         for step, data in epoch_iterator.enumerate_epoch():\n             callbacks.on_test_batch_begin(step)\n             logs = self.test_function(data)\n-            callbacks.on_test_batch_end(step, self._pythonify_logs(logs))\n+            logs = self._pythonify_logs(logs)\n+            callbacks.on_test_batch_end(step, logs)\n             if self.stop_evaluating:\n                 break\n-        logs = self.get_metrics_result()\n+        logs = self._get_metrics_result_or_logs(logs)\n         callbacks.on_test_end(logs)\n \n         if return_dict:\n\n@@ -312,14 +312,13 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 for step, iterator in epoch_iterator.enumerate_epoch():\n                     callbacks.on_train_batch_begin(step)\n                     logs = self.train_function(iterator)\n-                    callbacks.on_train_batch_end(\n-                        step, self._pythonify_logs(logs)\n-                    )\n+                    logs = self._pythonify_logs(logs)\n+                    callbacks.on_train_batch_end(step, logs)\n                     if self.stop_training:\n                         break\n \n-            # Override with model metrics instead of last step logs\n-            epoch_logs = self.get_metrics_result()\n+            # Override with model metrics instead of last step logs if needed.\n+            epoch_logs = dict(self._get_metrics_result_or_logs(logs))\n \n             # Run validation.\n             if validation_data is not None and self._should_eval(\n@@ -424,10 +423,11 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             for step, iterator in epoch_iterator.enumerate_epoch():\n                 callbacks.on_test_batch_begin(step)\n                 logs = self.test_function(iterator)\n-                callbacks.on_test_batch_end(step, self._pythonify_logs(logs))\n+                logs = self._pythonify_logs(logs)\n+                callbacks.on_test_batch_end(step, logs)\n                 if self.stop_evaluating:\n                     break\n-        logs = self.get_metrics_result()\n+        logs = self._get_metrics_result_or_logs(logs)\n         callbacks.on_test_end(logs)\n \n         if return_dict:\n\n@@ -252,14 +252,15 @@ class TorchTrainer(base_trainer.Trainer):\n                 callbacks.on_train_batch_begin(step)\n \n                 logs = self.train_function(data)\n+                logs = self._pythonify_logs(logs)\n \n                 # Callbacks\n-                callbacks.on_train_batch_end(step, self._pythonify_logs(logs))\n+                callbacks.on_train_batch_end(step, logs)\n                 if self.stop_training:\n                     break\n \n-            # Override with model metrics instead of last step logs\n-            epoch_logs = self.get_metrics_result()\n+            # Override with model metrics instead of last step logs if needed.\n+            epoch_logs = dict(self._get_metrics_result_or_logs(logs))\n \n             # Switch the torch Module back to testing mode.\n             self.eval()\n@@ -368,10 +369,11 @@ class TorchTrainer(base_trainer.Trainer):\n         for step, data in epoch_iterator.enumerate_epoch():\n             callbacks.on_test_batch_begin(step)\n             logs = self.test_function(data)\n-            callbacks.on_test_batch_end(step, self._pythonify_logs(logs))\n+            logs = self._pythonify_logs(logs)\n+            callbacks.on_test_batch_end(step, logs)\n             if self.stop_evaluating:\n                 break\n-        logs = self.get_metrics_result()\n+        logs = self._get_metrics_result_or_logs(logs)\n         callbacks.on_test_end(logs)\n \n         if return_dict:\n\n@@ -899,6 +899,37 @@ class Trainer:\n                 result[key] = value\n         return result\n \n+    def _get_metrics_result_or_logs(self, logs):\n+        \"\"\"Returns model metrics as a dict if the keys match with input logs.\n+\n+        When the training / evalution is performed with an asynchronous steps,\n+        the last scheduled `train / test_step` may not give the latest metrics\n+        because it is not guaranteed to be executed the last. This method gets\n+        metrics from the model directly instead of relying on the return from\n+        last step function.\n+\n+        When the user has custom train / test step functions, the metrics\n+        returned may be different from `Model.metrics`. In those instances,\n+        this function will be no-op and return the logs passed in.\n+\n+        Args:\n+            logs: A `dict` of metrics returned by train / test step function.\n+\n+        Returns:\n+            A `dict` containing values of the metrics listed in `self.metrics`\n+            when logs and model metrics keys match. Otherwise it returns input\n+            `logs`.\n+        \"\"\"\n+        metric_logs = self.get_metrics_result()\n+        # Verify that train / test step logs passed and metric logs have\n+        # matching keys. It could be different when using custom step functions,\n+        # in which case we return the logs from the last step.\n+        if isinstance(logs, dict) and set(logs.keys()) == set(\n+            metric_logs.keys()\n+        ):\n+            return metric_logs\n+        return logs\n+\n     def _flatten_metrics_in_order(self, logs):\n         \"\"\"Turns `logs` dict into a list as per key order of `metrics_names`.\"\"\"\n         metric_names = []\n\n@@ -44,6 +44,30 @@ class ExampleModel(Trainer, layers.Dense):\n         Trainer.__init__(self)\n \n \n+class CustomTrainTestStepModel(ExampleModel):\n+    def train_step(self, data):\n+        logs = super().train_step(data)\n+        logs[\"my_custom_metric\"] = 10.0\n+        return logs\n+\n+    def test_step(self, data):\n+        logs = super().test_step(data)\n+        logs[\"my_custom_metric\"] = 5.0\n+        return logs\n+\n+\n+class JaxCustomTrainTestStepModel(ExampleModel):\n+    def train_step(self, state, data):\n+        logs, state = super().train_step(state, data)\n+        logs[\"my_custom_metric\"] = 10.0\n+        return logs, state\n+\n+    def test_step(self, state, data):\n+        logs, state = super().test_step(state, data)\n+        logs[\"my_custom_metric\"] = 5.0\n+        return logs, state\n+\n+\n class StructModel(Trainer, layers.Layer):\n     def __init__(self, units):\n         layers.Layer.__init__(self)\n@@ -308,6 +332,27 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         self.assertIn(\"loss\", history)\n         self.assertIn(\"val_loss\", history)\n \n+    @pytest.mark.requires_trainable_backend\n+    def test_fit_with_custom_train_step(self):\n+        if backend.backend() == \"jax\":\n+            model = JaxCustomTrainTestStepModel(units=3)\n+        else:\n+            model = CustomTrainTestStepModel(units=3)\n+        x = np.ones((100, 4))\n+        y = np.zeros((100, 3))\n+        batch_size = 16\n+\n+        model.compile(\n+            optimizer=optimizers.SGD(),\n+            loss=losses.MeanSquaredError(),\n+            metrics=[metrics.MeanSquaredError()],\n+        )\n+        history = model.fit(x, y, batch_size=batch_size)\n+        history = history.history\n+        self.assertIn(\"loss\", history)\n+        self.assertIn(\"mean_squared_error\", history)\n+        self.assertAllClose(history[\"my_custom_metric\"], 10.0)\n+\n     @parameterized.named_parameters(\n         named_product(\n             generator_type=[\"tf\", \"jax\", \"scipy\"], mode=[\"eager\", \"graph\"]\n@@ -375,6 +420,31 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         self.assertIn(\"mean_squared_error\", output)\n         self.assertAllClose(output[\"mean_squared_error\"], 16.0)\n \n+    @parameterized.named_parameters([(\"flat\", False), (\"dict\", True)])\n+    @pytest.mark.requires_trainable_backend\n+    def test_evaluate_with_custom_test_step(self, return_dict):\n+        if backend.backend() == \"jax\":\n+            model = JaxCustomTrainTestStepModel(units=3)\n+        else:\n+            model = CustomTrainTestStepModel(units=3)\n+        x = np.ones((100, 4))\n+        y = np.zeros((100, 3))\n+        batch_size = 16\n+\n+        model.compile(\n+            optimizer=optimizers.SGD(),\n+            loss=losses.MeanSquaredError(),\n+            metrics=[metrics.MeanSquaredError()],\n+        )\n+        output = model.evaluate(\n+            x, y, batch_size=batch_size, return_dict=return_dict\n+        )\n+        self.assertLen(output, 3)\n+        if return_dict:\n+            self.assertAllClose(output[\"my_custom_metric\"], 5.0)\n+        else:\n+            self.assertAllClose(output[-1], 5.0)  # Custom metrics go last.\n+\n     @parameterized.named_parameters(\n         named_product(\n             generator_type=[\"tf\", \"jax\", \"scipy\"], mode=[\"eager\", \"graph\"]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e640584c7c2bb0dbac12a637625f6492fb06431", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 121 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -38,8 +38,6 @@ from keras.src.backend.exports import device\n from keras.src.backend.exports import name_scope\n from keras.src.dtype_policies.dtype_policy import DTypePolicy\n from keras.src.dtype_policies.dtype_policy import FloatDTypePolicy\n-from keras.src.dtype_policies.dtype_policy import QuantizedDTypePolicy\n-from keras.src.dtype_policies.dtype_policy import QuantizedFloat8DTypePolicy\n from keras.src.initializers.initializer import Initializer\n from keras.src.layers.core.input_layer import Input\n from keras.src.layers.input_spec import InputSpec\n@@ -51,7 +49,6 @@ from keras.src.models.sequential import Sequential\n from keras.src.ops.function import Function\n from keras.src.ops.operation import Operation\n from keras.src.optimizers.optimizer import Optimizer\n-from keras.src.quantizers.quantizers import AbsMaxQuantizer\n from keras.src.quantizers.quantizers import Quantizer\n from keras.src.regularizers.regularizers import Regularizer\n from keras.src.version import __version__\n\n@@ -37,8 +37,6 @@ from keras.src.backend.exports import device\n from keras.src.backend.exports import name_scope\n from keras.src.dtype_policies.dtype_policy import DTypePolicy\n from keras.src.dtype_policies.dtype_policy import FloatDTypePolicy\n-from keras.src.dtype_policies.dtype_policy import QuantizedDTypePolicy\n-from keras.src.dtype_policies.dtype_policy import QuantizedFloat8DTypePolicy\n from keras.src.initializers.initializer import Initializer\n from keras.src.layers.core.input_layer import Input\n from keras.src.layers.input_spec import InputSpec\n@@ -50,7 +48,6 @@ from keras.src.models.sequential import Sequential\n from keras.src.ops.function import Function\n from keras.src.ops.operation import Operation\n from keras.src.optimizers.optimizer import Optimizer\n-from keras.src.quantizers.quantizers import AbsMaxQuantizer\n from keras.src.quantizers.quantizers import Quantizer\n from keras.src.regularizers.regularizers import Regularizer\n from keras.src.version import __version__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4adfbd4a2968a0e4c18dc3a1222396893822b026", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 147 | Lines Deleted: 34 | Files Changed: 16 | Hunks: 39 | Methods Changed: 26 | Complexity Δ (Sum/Max): 3/1 | Churn Δ: 181 | Churn Cumulative: 295 | Contributors (this commit): 2 | Commits (past 90d): 32 | Contributors (cumulative): 32 | DMM Complexity: 0.9900990099009901\n\nDIFF:\n@@ -25,23 +25,24 @@ class JaxOptimizer(base_optimizer.BaseOptimizer):\n             ]\n             current_optimizer_vars_value = [v.value for v in self.variables]\n \n+            # `trainable_variables` might have been filtered in previous\n+            # processing steps, so we need to ensure the correct mapping between\n+            # `self._accumulated_gradients` and `trainable_variables`\n+            acc_grads = [\n+                self._accumulated_gradients[self._get_variable_index(v)]\n+                for v in trainable_variables\n+            ]\n+\n             new_g_accs = jax.lax.cond(\n                 is_update_step,\n-                lambda: [\n-                    jnp.zeros(x.shape, dtype=x.dtype)\n-                    for x in self._accumulated_gradients\n-                ],\n-                lambda: [\n-                    grads[i] + self._accumulated_gradients[i]\n-                    for i in range(len(grads))\n-                ],\n+                lambda: [jnp.zeros(g.shape, dtype=g.dtype) for g in acc_grads],\n+                lambda: [g + acc_g for g, acc_g in zip(grads, acc_grads)],\n             )\n \n             grads = jax.lax.cond(\n                 is_update_step,\n                 lambda: [\n-                    (grads[i] + self._accumulated_gradients[i]) / steps\n-                    for i in range(len(grads))\n+                    (g + acc_g) / steps for g, acc_g in zip(grads, acc_grads)\n                 ],\n                 lambda: list(grads),\n             )\n@@ -66,7 +67,7 @@ class JaxOptimizer(base_optimizer.BaseOptimizer):\n             for value, v in zip(new_opt_vars, self.variables):\n                 v.assign(value)\n \n-            for n_g_acc, g_acc in zip(new_g_accs, self._accumulated_gradients):\n+            for n_g_acc, g_acc in zip(new_g_accs, acc_grads):\n                 g_acc.assign(n_g_acc)\n \n         else:\n\n@@ -196,11 +196,11 @@ class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n                 var, lambda a, b: a.assign(b), args=(average_var,)\n             )\n \n-    def _backend_increment_gradient_accumulators(self, grads):\n+    def _backend_increment_gradient_accumulators(self, grads, acc_grads):\n         def update_accumulator(var, grad):\n             var.assign(var + grad)\n \n-        accumulators = [v.value for v in self._accumulated_gradients]\n+        accumulators = [v.value for v in acc_grads]\n \n         def _distributed_tf_increment_grad_acc(\n             distribution, grads, accumulators\n\n@@ -19,6 +19,6 @@ class TorchParallelOptimizer(BaseOptimizer):\n         torch._foreach_mul_(acc_list, 0.0)\n \n     @torch_utils.no_grad\n-    def _backend_increment_gradient_accumulators(self, grads):\n-        acc_list = [v.value for v in self._accumulated_gradients]\n+    def _backend_increment_gradient_accumulators(self, grads, acc_grads):\n+        acc_list = [v.value for v in acc_grads]\n         torch._foreach_add_(acc_list, grads, alpha=1.0)\n\n@@ -49,6 +49,8 @@ class Adadelta(optimizer.Optimizer):\n         use_ema=False,\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n+        loss_scale_factor=None,\n+        gradient_accumulation_steps=None,\n         name=\"adadelta\",\n         **kwargs,\n     ):\n@@ -61,6 +63,8 @@ class Adadelta(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            loss_scale_factor=loss_scale_factor,\n+            gradient_accumulation_steps=gradient_accumulation_steps,\n             name=name,\n             **kwargs,\n         )\n\n@@ -56,6 +56,8 @@ class Adafactor(optimizer.Optimizer):\n         use_ema=False,\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n+        loss_scale_factor=None,\n+        gradient_accumulation_steps=None,\n         name=\"adafactor\",\n         **kwargs,\n     ):\n@@ -69,6 +71,8 @@ class Adafactor(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            loss_scale_factor=loss_scale_factor,\n+            gradient_accumulation_steps=gradient_accumulation_steps,\n             **kwargs,\n         )\n         self.beta_2_decay = beta_2_decay\n\n@@ -44,6 +44,8 @@ class Adagrad(optimizer.Optimizer):\n         use_ema=False,\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n+        loss_scale_factor=None,\n+        gradient_accumulation_steps=None,\n         name=\"adagrad\",\n         **kwargs,\n     ):\n@@ -56,6 +58,8 @@ class Adagrad(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            loss_scale_factor=loss_scale_factor,\n+            gradient_accumulation_steps=gradient_accumulation_steps,\n             name=name,\n             **kwargs,\n         )\n\n@@ -54,6 +54,8 @@ class Adam(optimizer.Optimizer):\n         use_ema=False,\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n+        loss_scale_factor=None,\n+        gradient_accumulation_steps=None,\n         name=\"adam\",\n         **kwargs,\n     ):\n@@ -67,6 +69,8 @@ class Adam(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            loss_scale_factor=loss_scale_factor,\n+            gradient_accumulation_steps=gradient_accumulation_steps,\n             **kwargs,\n         )\n         self.beta_1 = beta_1\n\n@@ -63,6 +63,8 @@ class Adamax(optimizer.Optimizer):\n         use_ema=False,\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n+        loss_scale_factor=None,\n+        gradient_accumulation_steps=None,\n         name=\"adamax\",\n         **kwargs,\n     ):\n@@ -76,6 +78,8 @@ class Adamax(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            loss_scale_factor=loss_scale_factor,\n+            gradient_accumulation_steps=gradient_accumulation_steps,\n             **kwargs,\n         )\n         self.beta_1 = beta_1\n\n@@ -64,6 +64,8 @@ class AdamW(adam.Adam):\n         use_ema=False,\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n+        loss_scale_factor=None,\n+        gradient_accumulation_steps=None,\n         name=\"adamw\",\n         **kwargs,\n     ):\n@@ -81,6 +83,8 @@ class AdamW(adam.Adam):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            loss_scale_factor=loss_scale_factor,\n+            gradient_accumulation_steps=gradient_accumulation_steps,\n             **kwargs,\n         )\n \n\n@@ -1,8 +1,6 @@\n import re\n import warnings\n \n-import numpy as np\n-\n from keras.src import backend\n from keras.src import initializers\n from keras.src import ops\n@@ -375,27 +373,31 @@ class BaseOptimizer:\n             is_update_step = (\n                 self.iterations + 1\n             ) % self.gradient_accumulation_steps == 0\n+            # `trainable_variables` might have been filtered in previous\n+            # processing steps, so we need to ensure the correct mapping between\n+            # `self._accumulated_gradients` and `trainable_variables`\n+            acc_grads = [\n+                self._accumulated_gradients[self._get_variable_index(v)]\n+                for v in trainable_variables\n+            ]\n \n-            def _update_step_fn(self, grads, trainable_variables):\n+            def _update_step_fn(grads, trainable_variables):\n                 # Run update step with accumulated grads + reset accumulators\n                 steps = self.gradient_accumulation_steps\n                 grads = [\n-                    (grads[i] + self._accumulated_gradients[i]) / steps\n-                    for i in range(len(grads))\n+                    (g + acc_g) / steps for g, acc_g in zip(grads, acc_grads)\n                 ]\n                 self._backend_update_step(\n                     grads, trainable_variables, self.learning_rate\n                 )\n                 self._backend_reset_gradient_accumulators()\n \n-            def _grad_accumulation_fn(self, grads):\n-                # Update gradient accumulators\n-                self._backend_increment_gradient_accumulators(grads)\n-\n             ops.cond(\n                 is_update_step,\n-                lambda: _update_step_fn(self, grads, trainable_variables),\n-                lambda: _grad_accumulation_fn(self, grads),\n+                lambda: _update_step_fn(grads, trainable_variables),\n+                lambda: self._backend_increment_gradient_accumulators(\n+                    grads, acc_grads\n+                ),\n             )\n         else:\n             # Run udpate step.\n@@ -434,14 +436,11 @@ class BaseOptimizer:\n \n     def _backend_reset_gradient_accumulators(self):\n         for g_acc in self._accumulated_gradients:\n-            g_acc.assign(np.zeros(g_acc.shape, dtype=g_acc.dtype))\n+            g_acc.assign(ops.zeros(g_acc.shape, dtype=g_acc.dtype))\n \n-    def _backend_increment_gradient_accumulators(self, grads):\n-        new_g_accs = [\n-            (grads[i] + self._accumulated_gradients[i])\n-            for i in range(len(grads))\n-        ]\n-        for n_g_acc, g_acc in zip(new_g_accs, self._accumulated_gradients):\n+    def _backend_increment_gradient_accumulators(self, grads, acc_grads):\n+        new_g_accs = [(g + acc_g) for g, acc_g in zip(grads, acc_grads)]\n+        for n_g_acc, g_acc in zip(new_g_accs, acc_grads):\n             g_acc.assign(n_g_acc)\n \n     def stateless_apply(self, optimizer_variables, grads, trainable_variables):\n@@ -616,6 +615,31 @@ class BaseOptimizer:\n         for i in range(len(filtered_grads) - 1, -1, -1):\n             g, v = filtered_grads[i], filtered_vars[i]\n             if v.overwrite_with_gradient:\n+                if self.gradient_accumulation_steps:\n+                    # Utilize a stateless manner for JAX compatibility\n+                    steps = self.gradient_accumulation_steps\n+                    is_update_step = (self.iterations + 1) % steps == 0\n+                    acc_g = self._accumulated_gradients[\n+                        self._get_variable_index(v)\n+                    ]\n+                    # `ops.maximum` is utilized for gradient accumulation for\n+                    # `overwrite_with_gradient=True` variables\n+                    new_g_acc = ops.cond(\n+                        is_update_step,\n+                        lambda: ops.zeros(g.shape, dtype=g.dtype),\n+                        lambda: ops.maximum(g, acc_g),\n+                    )\n+                    new_g = ops.cond(\n+                        is_update_step,\n+                        lambda: ops.maximum(g, acc_g),\n+                        lambda: g,\n+                    )\n+                    new_v = ops.cond(\n+                        is_update_step, lambda: new_g, lambda: v.value\n+                    )\n+                    v.assign(new_v)\n+                    acc_g.assign(new_g_acc)\n+                else:\n                     v.assign(g)\n                 filtered_grads.pop(i)\n                 filtered_vars.pop(i)\n\n@@ -91,6 +91,8 @@ class Ftrl(optimizer.Optimizer):\n         use_ema=False,\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n+        loss_scale_factor=None,\n+        gradient_accumulation_steps=None,\n         name=\"ftrl\",\n         **kwargs,\n     ):\n@@ -104,6 +106,8 @@ class Ftrl(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            loss_scale_factor=loss_scale_factor,\n+            gradient_accumulation_steps=gradient_accumulation_steps,\n             **kwargs,\n         )\n \n\n@@ -53,6 +53,8 @@ class Lion(optimizer.Optimizer):\n         use_ema=False,\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n+        loss_scale_factor=None,\n+        gradient_accumulation_steps=None,\n         name=\"lion\",\n         **kwargs,\n     ):\n@@ -66,6 +68,8 @@ class Lion(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            loss_scale_factor=loss_scale_factor,\n+            gradient_accumulation_steps=gradient_accumulation_steps,\n             **kwargs,\n         )\n         self.beta_1 = beta_1\n\n@@ -49,6 +49,8 @@ class Nadam(optimizer.Optimizer):\n         use_ema=False,\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n+        loss_scale_factor=None,\n+        gradient_accumulation_steps=None,\n         name=\"nadam\",\n         **kwargs,\n     ):\n@@ -62,6 +64,8 @@ class Nadam(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            loss_scale_factor=loss_scale_factor,\n+            gradient_accumulation_steps=gradient_accumulation_steps,\n             **kwargs,\n         )\n         self.beta_1 = beta_1\n\n@@ -267,6 +267,50 @@ class OptimizerTest(testing.TestCase):\n         self.assertAllClose(v, [[1.0, 1.0], [1.0, 1.0]])\n         self.assertAllClose(v2, [[0.0, 1.0], [2.0, 3.0]])\n \n+    def test_overwrite_with_gradient_with_gradient_accumulation(self):\n+        v = backend.Variable([[1.0, 2.0], [3.0, 4.0]])\n+        v.overwrite_with_gradient = True\n+        v2 = backend.Variable([[1.0, 2.0], [3.0, 4.0]])\n+        grad_ones = backend.convert_to_tensor([[1.0, 1.0], [1.0, 1.0]])\n+        grad_twos = backend.convert_to_tensor([[2.0, 2.0], [2.0, 2.0]])\n+        optimizer = optimizers.SGD(\n+            learning_rate=1.0, gradient_accumulation_steps=2\n+        )\n+\n+        # Iteration 1\n+        optimizer.apply_gradients([(grad_ones, v), (grad_ones, v2)])\n+        self.assertAllClose(optimizer.iterations, 1)\n+        self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]])\n+        self.assertAllClose(v2, [[1.0, 2.0], [3.0, 4.0]])\n+        self.assertAllClose(\n+            optimizer._accumulated_gradients[0], [[1.0, 1.0], [1.0, 1.0]]\n+        )\n+        self.assertAllClose(\n+            optimizer._accumulated_gradients[1], [[1.0, 1.0], [1.0, 1.0]]\n+        )\n+        # Iteration 2\n+        optimizer.apply_gradients([(grad_twos, v), (grad_twos, v2)])\n+        self.assertAllClose(optimizer.iterations, 2)\n+        self.assertAllClose(v, [[2.0, 2.0], [2.0, 2.0]])\n+        self.assertAllClose(v2, [[-0.5, 0.5], [1.5, 2.5]])\n+        self.assertAllClose(\n+            optimizer._accumulated_gradients[0], [[0.0, 0.0], [0.0, 0.0]]\n+        )\n+        self.assertAllClose(\n+            optimizer._accumulated_gradients[1], [[0.0, 0.0], [0.0, 0.0]]\n+        )\n+        # Iteration 3\n+        optimizer.apply_gradients([(grad_ones, v), (grad_ones, v2)])\n+        self.assertAllClose(optimizer.iterations, 3)\n+        self.assertAllClose(v, [[2.0, 2.0], [2.0, 2.0]])\n+        self.assertAllClose(v2, [[-0.5, 0.5], [1.5, 2.5]])\n+        self.assertAllClose(\n+            optimizer._accumulated_gradients[0], [[1.0, 1.0], [1.0, 1.0]]\n+        )\n+        self.assertAllClose(\n+            optimizer._accumulated_gradients[1], [[1.0, 1.0], [1.0, 1.0]]\n+        )\n+\n     def test_setting_lr_to_callable_untracks_lr_var(self):\n         adam = optimizers.Adam(learning_rate=0.001)\n         self.assertLen(adam.variables, 2)\n\n@@ -63,7 +63,9 @@ class RMSprop(optimizer.Optimizer):\n         global_clipnorm=None,\n         use_ema=False,\n         ema_momentum=0.99,\n-        ema_overwrite_frequency=100,\n+        ema_overwrite_frequency=None,\n+        loss_scale_factor=None,\n+        gradient_accumulation_steps=None,\n         name=\"rmsprop\",\n         **kwargs,\n     ):\n@@ -76,6 +78,8 @@ class RMSprop(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            loss_scale_factor=loss_scale_factor,\n+            gradient_accumulation_steps=gradient_accumulation_steps,\n             name=name,\n             **kwargs,\n         )\n\n@@ -52,6 +52,8 @@ class SGD(optimizer.Optimizer):\n         use_ema=False,\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n+        loss_scale_factor=None,\n+        gradient_accumulation_steps=None,\n         name=\"SGD\",\n         **kwargs,\n     ):\n@@ -65,6 +67,8 @@ class SGD(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            loss_scale_factor=loss_scale_factor,\n+            gradient_accumulation_steps=gradient_accumulation_steps,\n             **kwargs,\n         )\n         if not isinstance(momentum, float) or momentum < 0 or momentum > 1:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#69f3abd176afd664e7242be5f5666cd5068822be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 24 | Churn Cumulative: 38 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -72,9 +72,10 @@ def make_layer_label(layer, **kwargs):\n         '<<table border=\"0\" cellborder=\"1\" bgcolor=\"black\" cellpadding=\"10\">'\n     )\n \n-    colspan = max(\n-        1, sum(int(x) for x in (show_dtype, show_shapes, show_trainable))\n-    )\n+    colspan_max = sum(int(x) for x in (show_dtype, show_trainable))\n+    if show_shapes:\n+        colspan_max += 2\n+    colspan = max(1, colspan_max)\n \n     if show_layer_names:\n         table += (\n@@ -104,15 +105,25 @@ def make_layer_label(layer, **kwargs):\n \n     cols = []\n     if show_shapes:\n-        shape = None\n+        input_shape = None\n+        output_shape = None\n         try:\n-            shape = tree.map_structure(lambda x: x.shape, layer.output)\n+            input_shape = tree.map_structure(lambda x: x.shape, layer.input)\n+            output_shape = tree.map_structure(lambda x: x.shape, layer.output)\n         except (ValueError, AttributeError):\n             pass\n+        if class_name != \"InputLayer\":\n             cols.append(\n                 (\n                     '<td bgcolor=\"white\"><font point-size=\"14\">'\n-                f'Output shape: <b>{shape or \"?\"}</b>'\n+                    f'Input shape: <b>{input_shape or \"?\"}</b>'\n+                    \"</font></td>\"\n+                )\n+            )\n+        cols.append(\n+            (\n+                '<td bgcolor=\"white\"><font point-size=\"14\">'\n+                f'Output shape: <b>{output_shape or \"?\"}</b>'\n                 \"</font></td>\"\n             )\n         )\n@@ -248,7 +259,6 @@ def model_to_dot(\n     }\n \n     if isinstance(model, sequential.Sequential):\n-        # TODO\n         layers = model.layers\n     elif not isinstance(model, functional.Functional):\n         # We treat subclassed models as a single node.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1228acbbf175bffa49103d8242412f8ccdf06df1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 543 | Lines Deleted: 1 | Files Changed: 7 | Hunks: 23 | Methods Changed: 9 | Complexity Δ (Sum/Max): 44/14 | Churn Δ: 544 | Churn Cumulative: 586 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 14 | DMM Complexity: 0.06627680311890838\n\nDIFF:\n@@ -43,6 +43,9 @@ def resize(\n     interpolation=\"bilinear\",\n     antialias=False,\n     crop_to_aspect_ratio=False,\n+    pad_to_aspect_ratio=False,\n+    fill_mode=\"constant\",\n+    fill_value=0.0,\n     data_format=\"channels_last\",\n ):\n     if interpolation not in RESIZE_INTERPOLATIONS:\n@@ -50,6 +53,16 @@ def resize(\n             \"Invalid value for argument `interpolation`. Expected of one \"\n             f\"{RESIZE_INTERPOLATIONS}. Received: interpolation={interpolation}\"\n         )\n+    if fill_mode != \"constant\":\n+        raise ValueError(\n+            \"Invalid value for argument `fill_mode`. Only `'constant'` \"\n+            f\"is supported. Received: fill_mode={fill_mode}\"\n+        )\n+    if pad_to_aspect_ratio and crop_to_aspect_ratio:\n+        raise ValueError(\n+            \"Only one of `pad_to_aspect_ratio` & `crop_to_aspect_ratio` \"\n+            \"can be `True`.\"\n+        )\n     if not len(size) == 2:\n         raise ValueError(\n             \"Argument `size` must be a tuple of two elements \"\n@@ -62,6 +75,7 @@ def resize(\n             size = (image.shape[0],) + size + (image.shape[-1],)\n         else:\n             size = (image.shape[0], image.shape[1]) + size\n+        batch_size = image.shape[0]\n     elif len(image.shape) == 3:\n         if data_format == \"channels_last\":\n             size = size + (image.shape[-1],)\n@@ -114,6 +128,150 @@ def resize(\n                     crop_box_hstart : crop_box_hstart + crop_height,\n                     crop_box_wstart : crop_box_wstart + crop_width,\n                 ]\n+    elif pad_to_aspect_ratio:\n+        shape = image.shape\n+        if data_format == \"channels_last\":\n+            height, width, channels = shape[-3], shape[-2], shape[-1]\n+        else:\n+            height, width, channels = shape[-2], shape[-1], shape[-3]\n+\n+        pad_height = int(float(width * target_height) / target_width)\n+        pad_height = max(height, pad_height)\n+        pad_width = int(float(height * target_width) / target_height)\n+        pad_width = max(width, pad_width)\n+        img_box_hstart = int(float(pad_height - height) / 2)\n+        img_box_wstart = int(float(pad_width - width) / 2)\n+        if data_format == \"channels_last\":\n+            if img_box_hstart > 0:\n+                if len(image.shape) == 4:\n+                    padded_img = jnp.concatenate(\n+                        [\n+                            jnp.ones(\n+                                (batch_size, img_box_hstart, width, channels),\n+                                dtype=image.dtype,\n+                            )\n+                            * fill_value,\n+                            image,\n+                            jnp.ones(\n+                                (batch_size, img_box_hstart, width, channels),\n+                                dtype=image.dtype,\n+                            )\n+                            * fill_value,\n+                        ],\n+                        axis=1,\n+                    )\n+                else:\n+                    padded_img = jnp.concatenate(\n+                        [\n+                            jnp.ones(\n+                                (img_box_hstart, width, channels),\n+                                dtype=image.dtype,\n+                            )\n+                            * fill_value,\n+                            image,\n+                            jnp.ones(\n+                                (img_box_hstart, width, channels),\n+                                dtype=image.dtype,\n+                            )\n+                            * fill_value,\n+                        ],\n+                        axis=0,\n+                    )\n+            elif img_box_wstart > 0:\n+                if len(image.shape) == 4:\n+                    padded_img = jnp.concatenate(\n+                        [\n+                            jnp.ones(\n+                                (batch_size, height, img_box_wstart, channels),\n+                                dtype=image.dtype,\n+                            )\n+                            * fill_value,\n+                            image,\n+                            jnp.ones(\n+                                (batch_size, height, img_box_wstart, channels),\n+                                dtype=image.dtype,\n+                            )\n+                            * fill_value,\n+                        ],\n+                        axis=2,\n+                    )\n+                else:\n+                    padded_img = jnp.concatenate(\n+                        [\n+                            jnp.ones(\n+                                (height, img_box_wstart, channels),\n+                                dtype=image.dtype,\n+                            )\n+                            * fill_value,\n+                            image,\n+                            jnp.ones(\n+                                (height, img_box_wstart, channels),\n+                                dtype=image.dtype,\n+                            )\n+                            * fill_value,\n+                        ],\n+                        axis=1,\n+                    )\n+            else:\n+                padded_img = image\n+        else:\n+            if img_box_hstart > 0:\n+                if len(image.shape) == 4:\n+                    padded_img = jnp.concatenate(\n+                        [\n+                            jnp.ones(\n+                                (batch_size, channels, img_box_hstart, width)\n+                            )\n+                            * fill_value,\n+                            image,\n+                            jnp.ones(\n+                                (batch_size, channels, img_box_hstart, width)\n+                            )\n+                            * fill_value,\n+                        ],\n+                        axis=2,\n+                    )\n+                else:\n+                    padded_img = jnp.concatenate(\n+                        [\n+                            jnp.ones((channels, img_box_hstart, width))\n+                            * fill_value,\n+                            image,\n+                            jnp.ones((channels, img_box_hstart, width))\n+                            * fill_value,\n+                        ],\n+                        axis=1,\n+                    )\n+            elif img_box_wstart > 0:\n+                if len(image.shape) == 4:\n+                    padded_img = jnp.concatenate(\n+                        [\n+                            jnp.ones(\n+                                (batch_size, channels, height, img_box_wstart)\n+                            )\n+                            * fill_value,\n+                            image,\n+                            jnp.ones(\n+                                (batch_size, channels, height, img_box_wstart)\n+                            )\n+                            * fill_value,\n+                        ],\n+                        axis=3,\n+                    )\n+                else:\n+                    padded_img = jnp.concatenate(\n+                        [\n+                            jnp.ones((channels, height, img_box_wstart))\n+                            * fill_value,\n+                            image,\n+                            jnp.ones((channels, height, img_box_wstart))\n+                            * fill_value,\n+                        ],\n+                        axis=2,\n+                    )\n+            else:\n+                padded_img = image\n+        image = padded_img\n \n     return jax.image.resize(\n         image, size, method=interpolation, antialias=antialias\n\n@@ -42,6 +42,9 @@ def resize(\n     interpolation=\"bilinear\",\n     antialias=False,\n     crop_to_aspect_ratio=False,\n+    pad_to_aspect_ratio=False,\n+    fill_mode=\"constant\",\n+    fill_value=0.0,\n     data_format=\"channels_last\",\n ):\n     if interpolation not in RESIZE_INTERPOLATIONS:\n@@ -49,6 +52,16 @@ def resize(\n             \"Invalid value for argument `interpolation`. Expected of one \"\n             f\"{RESIZE_INTERPOLATIONS}. Received: interpolation={interpolation}\"\n         )\n+    if fill_mode != \"constant\":\n+        raise ValueError(\n+            \"Invalid value for argument `fill_mode`. Only `'constant'` \"\n+            f\"is supported. Received: fill_mode={fill_mode}\"\n+        )\n+    if pad_to_aspect_ratio and crop_to_aspect_ratio:\n+        raise ValueError(\n+            \"Only one of `pad_to_aspect_ratio` & `crop_to_aspect_ratio` \"\n+            \"can be `True`.\"\n+        )\n     if not len(size) == 2:\n         raise ValueError(\n             \"Argument `size` must be a tuple of two elements \"\n@@ -113,6 +126,86 @@ def resize(\n                     crop_box_hstart : crop_box_hstart + crop_height,\n                     crop_box_wstart : crop_box_wstart + crop_width,\n                 ]\n+    elif pad_to_aspect_ratio:\n+        shape = image.shape\n+        batch_size = image.shape[0]\n+        if data_format == \"channels_last\":\n+            height, width, channels = shape[-3], shape[-2], shape[-1]\n+        else:\n+            channels, height, width = shape[-3], shape[-2], shape[-1]\n+        pad_height = int(float(width * target_height) / target_width)\n+        pad_height = max(height, pad_height)\n+        pad_width = int(float(height * target_width) / target_height)\n+        pad_width = max(width, pad_width)\n+        img_box_hstart = int(float(pad_height - height) / 2)\n+        img_box_wstart = int(float(pad_width - width) / 2)\n+        if data_format == \"channels_last\":\n+            if len(image.shape) == 4:\n+                padded_img = (\n+                    np.ones(\n+                        (\n+                            batch_size,\n+                            pad_height + height,\n+                            pad_width + width,\n+                            channels,\n+                        ),\n+                        dtype=image.dtype,\n+                    )\n+                    * fill_value\n+                )\n+                padded_img[\n+                    :,\n+                    img_box_hstart : img_box_hstart + height,\n+                    img_box_wstart : img_box_wstart + width,\n+                    :,\n+                ] = image\n+            else:\n+                padded_img = (\n+                    np.ones(\n+                        (pad_height + height, pad_width + width, channels),\n+                        dtype=image.dtype,\n+                    )\n+                    * fill_value\n+                )\n+                padded_img[\n+                    img_box_hstart : img_box_hstart + height,\n+                    img_box_wstart : img_box_wstart + width,\n+                    :,\n+                ] = image\n+        else:\n+            if len(image.shape) == 4:\n+                padded_img = (\n+                    np.ones(\n+                        (\n+                            batch_size,\n+                            channels,\n+                            pad_height + height,\n+                            pad_width + width,\n+                        ),\n+                        dtype=image.dtype,\n+                    )\n+                    * fill_value\n+                )\n+                padded_img[\n+                    :,\n+                    :,\n+                    img_box_hstart : img_box_hstart + height,\n+                    img_box_wstart : img_box_wstart + width,\n+                ] = image\n+            else:\n+                padded_img = (\n+                    np.ones(\n+                        (channels, pad_height + height, pad_width + width),\n+                        dtype=image.dtype,\n+                    )\n+                    * fill_value\n+                )\n+                padded_img[\n+                    :,\n+                    img_box_hstart : img_box_hstart + height,\n+                    img_box_wstart : img_box_wstart + width,\n+                ] = image\n+        image = padded_img\n \n     return np.array(\n         jax.image.resize(image, size, method=interpolation, antialias=antialias)\n\n@@ -42,6 +42,9 @@ def resize(\n     interpolation=\"bilinear\",\n     antialias=False,\n     crop_to_aspect_ratio=False,\n+    pad_to_aspect_ratio=False,\n+    fill_mode=\"constant\",\n+    fill_value=0.0,\n     data_format=\"channels_last\",\n ):\n     if interpolation not in RESIZE_INTERPOLATIONS:\n@@ -49,6 +52,16 @@ def resize(\n             \"Invalid value for argument `interpolation`. Expected of one \"\n             f\"{RESIZE_INTERPOLATIONS}. Received: interpolation={interpolation}\"\n         )\n+    if fill_mode != \"constant\":\n+        raise ValueError(\n+            \"Invalid value for argument `fill_mode`. Only `'constant'` \"\n+            f\"is supported. Received: fill_mode={fill_mode}\"\n+        )\n+    if pad_to_aspect_ratio and crop_to_aspect_ratio:\n+        raise ValueError(\n+            \"Only one of `pad_to_aspect_ratio` & `crop_to_aspect_ratio` \"\n+            \"can be `True`.\"\n+        )\n     if not len(size) == 2:\n         raise ValueError(\n             \"Argument `size` must be a tuple of two elements \"\n@@ -102,6 +115,115 @@ def resize(\n                 crop_box_wstart : crop_box_wstart + crop_width,\n                 :,\n             ]\n+    elif pad_to_aspect_ratio:\n+        shape = tf.shape(image)\n+        height, width = shape[-3], shape[-2]\n+        target_height, target_width = size\n+        pad_height = tf.cast(\n+            tf.cast(width * target_height, \"float32\") / target_width,\n+            \"int32\",\n+        )\n+        pad_height = tf.maximum(height, pad_height)\n+        pad_height = tf.cast(pad_height, \"int32\")\n+        pad_width = tf.cast(\n+            tf.cast(height * target_width, \"float32\") / target_height,\n+            \"int32\",\n+        )\n+        pad_width = tf.maximum(width, pad_width)\n+        pad_width = tf.cast(pad_width, \"int32\")\n+\n+        img_box_hstart = tf.cast(\n+            tf.cast(pad_height - height, \"float32\") / 2, \"int32\"\n+        )\n+        img_box_wstart = tf.cast(\n+            tf.cast(pad_width - width, \"float32\") / 2, \"int32\"\n+        )\n+        if len(image.shape) == 4:\n+            batch_size = tf.shape(image)[0]\n+            channels = tf.shape(image)[3]\n+            padded_img = tf.cond(\n+                img_box_hstart > 0,\n+                lambda: tf.concat(\n+                    [\n+                        tf.ones(\n+                            (batch_size, img_box_hstart, width, channels),\n+                            dtype=image.dtype,\n+                        )\n+                        * fill_value,\n+                        image,\n+                        tf.ones(\n+                            (batch_size, img_box_hstart, width, channels),\n+                            dtype=image.dtype,\n+                        )\n+                        * fill_value,\n+                    ],\n+                    axis=1,\n+                ),\n+                lambda: image,\n+            )\n+            padded_img = tf.cond(\n+                img_box_wstart > 0,\n+                lambda: tf.concat(\n+                    [\n+                        tf.ones(\n+                            (batch_size, height, img_box_wstart, channels),\n+                            dtype=image.dtype,\n+                        )\n+                        * fill_value,\n+                        padded_img,\n+                        tf.ones(\n+                            (batch_size, height, img_box_wstart, channels),\n+                            dtype=image.dtype,\n+                        )\n+                        * fill_value,\n+                    ],\n+                    axis=2,\n+                ),\n+                lambda: padded_img,\n+            )\n+        else:\n+            channels = tf.shape(image)[2]\n+            padded_img = tf.cond(\n+                img_box_hstart > 0,\n+                lambda: tf.concat(\n+                    [\n+                        tf.ones(\n+                            (img_box_hstart, width, channels),\n+                            dtype=image.dtype,\n+                        )\n+                        * fill_value,\n+                        image,\n+                        tf.ones(\n+                            (img_box_hstart, width, channels),\n+                            dtype=image.dtype,\n+                        )\n+                        * fill_value,\n+                    ],\n+                    axis=0,\n+                ),\n+                lambda: image,\n+            )\n+            padded_img = tf.cond(\n+                img_box_wstart > 0,\n+                lambda: tf.concat(\n+                    [\n+                        tf.ones(\n+                            (height, img_box_wstart, channels),\n+                            dtype=image.dtype,\n+                        )\n+                        * fill_value,\n+                        padded_img,\n+                        tf.ones(\n+                            (height, img_box_wstart, channels),\n+                            dtype=image.dtype,\n+                        )\n+                        * fill_value,\n+                    ],\n+                    axis=1,\n+                ),\n+                lambda: padded_img,\n+            )\n+        image = padded_img\n \n     resized = tf.image.resize(\n         image, size, method=interpolation, antialias=antialias\n\n@@ -52,6 +52,9 @@ def resize(\n     interpolation=\"bilinear\",\n     antialias=False,\n     crop_to_aspect_ratio=False,\n+    pad_to_aspect_ratio=False,\n+    fill_mode=\"constant\",\n+    fill_value=0.0,\n     data_format=\"channels_last\",\n ):\n     try:\n@@ -81,6 +84,16 @@ def resize(\n             \"Invalid value for argument `interpolation`. Expected of one \"\n             f\"{RESIZE_INTERPOLATIONS}. Received: interpolation={interpolation}\"\n         )\n+    if fill_mode != \"constant\":\n+        raise ValueError(\n+            \"Invalid value for argument `fill_mode`. Only `'constant'` \"\n+            f\"is supported. Received: fill_mode={fill_mode}\"\n+        )\n+    if pad_to_aspect_ratio and crop_to_aspect_ratio:\n+        raise ValueError(\n+            \"Only one of `pad_to_aspect_ratio` & `crop_to_aspect_ratio` \"\n+            \"can be `True`.\"\n+        )\n     if not len(size) == 2:\n         raise ValueError(\n             \"Argument `size` must be a tuple of two elements \"\n@@ -123,6 +136,52 @@ def resize(\n                 crop_box_hstart : crop_box_hstart + crop_height,\n                 crop_box_wstart : crop_box_wstart + crop_width,\n             ]\n+    elif pad_to_aspect_ratio:\n+        shape = image.shape\n+        height, width = shape[-2], shape[-1]\n+        target_height, target_width = size\n+        pad_height = int(float(width * target_height) / target_width)\n+        pad_height = max(height, pad_height)\n+        pad_width = int(float(height * target_width) / target_height)\n+        pad_width = max(width, pad_width)\n+        img_box_hstart = int(float(pad_height - height) / 2)\n+        img_box_wstart = int(float(pad_width - width) / 2)\n+        if len(image.shape) == 4:\n+            batch_size = image.shape[0]\n+            channels = image.shape[1]\n+            padded_img = (\n+                torch.ones(\n+                    (\n+                        batch_size,\n+                        channels,\n+                        pad_height + height,\n+                        pad_width + width,\n+                    ),\n+                    dtype=image.dtype,\n+                )\n+                * fill_value\n+            )\n+            padded_img[\n+                :,\n+                :,\n+                img_box_hstart : img_box_hstart + height,\n+                img_box_wstart : img_box_wstart + width,\n+            ] = image\n+        else:\n+            channels = image.shape[0]\n+            padded_img = (\n+                torch.ones(\n+                    (channels, pad_height + height, pad_width + width),\n+                    dtype=image.dtype,\n+                )\n+                * fill_value\n+            )\n+            padded_img[\n+                :,\n+                img_box_hstart : img_box_hstart + height,\n+                img_box_wstart : img_box_wstart + width,\n+            ] = image\n+        image = padded_img\n \n     resized = torchvision.transforms.functional.resize(\n         img=image,\n\n@@ -68,7 +68,7 @@ class RandomZoom(TFDataLayer):\n         interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n             `\"bilinear\"`.\n         seed: Integer. Used to create a random seed.\n-        fill_value: a float represents the value to be filled outside\n+        fill_value: a float that represents the value to be filled outside\n             the boundaries when `fill_mode=\"constant\"`.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n\n@@ -111,6 +111,9 @@ class Resize(Operation):\n         interpolation=\"bilinear\",\n         antialias=False,\n         crop_to_aspect_ratio=False,\n+        pad_to_aspect_ratio=False,\n+        fill_mode=\"constant\",\n+        fill_value=0.0,\n         data_format=\"channels_last\",\n     ):\n         super().__init__()\n@@ -119,6 +122,9 @@ class Resize(Operation):\n         self.antialias = antialias\n         self.data_format = data_format\n         self.crop_to_aspect_ratio = crop_to_aspect_ratio\n+        self.pad_to_aspect_ratio = pad_to_aspect_ratio\n+        self.fill_mode = fill_mode\n+        self.fill_value = fill_value\n \n     def call(self, image):\n         return backend.image.resize(\n@@ -128,6 +134,9 @@ class Resize(Operation):\n             antialias=self.antialias,\n             data_format=self.data_format,\n             crop_to_aspect_ratio=self.crop_to_aspect_ratio,\n+            pad_to_aspect_ratio=self.pad_to_aspect_ratio,\n+            fill_mode=self.fill_mode,\n+            fill_value=self.fill_value,\n         )\n \n     def compute_output_spec(self, image):\n@@ -160,6 +169,9 @@ def resize(\n     interpolation=\"bilinear\",\n     antialias=False,\n     crop_to_aspect_ratio=False,\n+    pad_to_aspect_ratio=False,\n+    fill_mode=\"constant\",\n+    fill_value=0.0,\n     data_format=\"channels_last\",\n ):\n     \"\"\"Resize images to size using the specified interpolation method.\n@@ -178,6 +190,15 @@ def resize(\n             largest possible window in the image (of size `(height, width)`)\n             that matches the target aspect ratio. By default\n             (`crop_to_aspect_ratio=False`), aspect ratio may not be preserved.\n+        pad_to_aspect_ratio: If `True`, pad the image without aspect\n+            ratio distortion. When the original aspect ratio differs\n+            from the target aspect ratio, the output image will be\n+            evenly padded on the short side.\n+        fill_mode: When using `pad_to_aspect_ratio=True`, padded areas\n+            are filled according to the given mode. Only `\"constant\"` is\n+            supported at this time\n+            (fill with constant value, equal to `fill_value`).\n+        fill_value: Float. Padding value to use when `pad_to_aspect_ratio=True`.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n             corresponds to inputs with shape `(batch, height, width, channels)`\n@@ -219,6 +240,11 @@ def resize(\n             \"channels)`, or `(batch_size, height, width, channels)`, but \"\n             f\"got input with incorrect rank, of shape {image.shape}.\"\n         )\n+    if pad_to_aspect_ratio and crop_to_aspect_ratio:\n+        raise ValueError(\n+            \"Only one of `pad_to_aspect_ratio` & `crop_to_aspect_ratio` \"\n+            \"can be `True`.\"\n+        )\n     if any_symbolic_tensors((image,)):\n         return Resize(\n             size,\n@@ -226,6 +252,9 @@ def resize(\n             antialias=antialias,\n             data_format=data_format,\n             crop_to_aspect_ratio=crop_to_aspect_ratio,\n+            pad_to_aspect_ratio=pad_to_aspect_ratio,\n+            fill_mode=fill_mode,\n+            fill_value=fill_value,\n         ).symbolic_call(image)\n     return backend.image.resize(\n         image,\n@@ -234,6 +263,9 @@ def resize(\n         antialias=antialias,\n         crop_to_aspect_ratio=crop_to_aspect_ratio,\n         data_format=data_format,\n+        pad_to_aspect_ratio=pad_to_aspect_ratio,\n+        fill_mode=fill_mode,\n+        fill_value=fill_value,\n     )\n \n \n\n@@ -317,6 +317,84 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n         self.assertAllClose(ref_out, out, atol=0.3)\n \n+    @parameterized.parameters(\n+        [\n+            (\"channels_last\",),\n+            (\"channels_first\",),\n+        ]\n+    )\n+    def test_resize_with_crop(self, data_format):\n+        if data_format == \"channels_first\":\n+            x = np.random.random((3, 60, 50)) * 255\n+        else:\n+            x = np.random.random((60, 50, 3)) * 255\n+        out = kimage.resize(\n+            x,\n+            size=(25, 25),\n+            crop_to_aspect_ratio=True,\n+            data_format=data_format,\n+        )\n+        if data_format == \"channels_first\":\n+            self.assertEqual(out.shape, (3, 25, 25))\n+        else:\n+            self.assertEqual(out.shape, (25, 25, 3))\n+\n+        # Batched case\n+        if data_format == \"channels_first\":\n+            x = np.random.random((2, 3, 50, 60)) * 255\n+        else:\n+            x = np.random.random((2, 50, 60, 3)) * 255\n+        out = kimage.resize(\n+            x,\n+            size=(25, 25),\n+            crop_to_aspect_ratio=True,\n+            data_format=data_format,\n+        )\n+        if data_format == \"channels_first\":\n+            self.assertEqual(out.shape, (2, 3, 25, 25))\n+        else:\n+            self.assertEqual(out.shape, (2, 25, 25, 3))\n+\n+    @parameterized.parameters(\n+        [\n+            (\"channels_last\", 2.0),\n+            (\"channels_first\", 2.0),\n+        ]\n+    )\n+    def test_resize_with_pad(self, data_format, fill_value):\n+        if data_format == \"channels_first\":\n+            x = np.random.random((3, 60, 50)) * 255\n+        else:\n+            x = np.random.random((60, 50, 3)) * 255\n+        out = kimage.resize(\n+            x,\n+            size=(25, 25),\n+            pad_to_aspect_ratio=True,\n+            data_format=data_format,\n+            fill_value=fill_value,\n+        )\n+        if data_format == \"channels_first\":\n+            self.assertEqual(out.shape, (3, 25, 25))\n+        else:\n+            self.assertEqual(out.shape, (25, 25, 3))\n+\n+        # Batched case\n+        if data_format == \"channels_first\":\n+            x = np.random.random((2, 3, 50, 60)) * 255\n+        else:\n+            x = np.random.random((2, 50, 60, 3)) * 255\n+        out = kimage.resize(\n+            x,\n+            size=(25, 25),\n+            pad_to_aspect_ratio=True,\n+            data_format=data_format,\n+            fill_value=fill_value,\n+        )\n+        if data_format == \"channels_first\":\n+            self.assertEqual(out.shape, (2, 3, 25, 25))\n+        else:\n+            self.assertEqual(out.shape, (2, 25, 25, 3))\n+\n     @parameterized.parameters(\n         [\n             (\"bilinear\", \"constant\", \"channels_last\"),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
