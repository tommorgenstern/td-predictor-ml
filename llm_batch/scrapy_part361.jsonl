{"custom_id": "scrapy#1a0572ad02446c0dacc682e503baffc2b5e67e98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 2318 | Contributors (this commit): 31 | Commits (past 90d): 15 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -72,18 +72,16 @@ class Crawler:\n         self.spidercls: Type[Spider] = spidercls\n         self.settings: Settings = settings.copy()\n         self.spidercls.update_settings(self.settings)\n+        self._update_root_log_handler()\n \n         self.addons: AddonManager = AddonManager(self)\n         self.signals: SignalManager = SignalManager(self)\n \n-        if get_scrapy_root_handler() is not None:\n-            # scrapy root handler already installed: update it with new settings\n-            install_scrapy_root_handler(self.settings)\n-\n         self._init_reactor: bool = init_reactor\n         self.crawling: bool = False\n         self._settings_loaded: bool = False\n         self._started: bool = False\n+\n         self.extensions: Optional[ExtensionManager] = None\n         self.stats: Optional[StatsCollector] = None\n         self.logformatter: Optional[LogFormatter] = None\n@@ -91,6 +89,11 @@ class Crawler:\n         self.spider: Optional[Spider] = None\n         self.engine: Optional[ExecutionEngine] = None\n \n+    def _update_root_log_handler(self) -> None:\n+        if get_scrapy_root_handler() is not None:\n+            # scrapy root handler already installed: update it with new settings\n+            install_scrapy_root_handler(self.settings)\n+\n     def _load_settings(self) -> None:\n         if self._settings_loaded:\n             return\n@@ -153,6 +156,7 @@ class Crawler:\n         try:\n             self.spider = self._create_spider(*args, **kwargs)\n             self._load_settings()\n+            self._update_root_log_handler()\n             self.engine = self._create_engine()\n             start_requests = iter(self.spider.start_requests())\n             yield self.engine.open_spider(self.spider, start_requests)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#77f39be4073cdce9e1b52ea0ee846881b13e6f23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 51 | Churn Cumulative: 4252 | Contributors (this commit): 36 | Commits (past 90d): 15 | Contributors (cumulative): 52 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,24 @@\n+from typing import Any\n+\n+import scrapy\n+from scrapy.crawler import Crawler, CrawlerProcess\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = \"no_request\"\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any):\n+        spider = super().from_crawler(crawler, *args, **kwargs)\n+        spider.settings.set(\"FOO\", kwargs.get(\"foo\"))\n+        return spider\n+\n+    def start_requests(self):\n+        self.logger.info(f\"The value of FOO is {self.settings.getint('FOO')}\")\n+        return []\n+\n+\n+process = CrawlerProcess(settings={})\n+\n+process.crawl(NoRequestsSpider, foo=42)\n+process.start()\n\n@@ -965,6 +965,28 @@ class MySpider(scrapy.Spider):\n             log,\n         )\n \n+    def test_args_change_settings(self):\n+        spider_code = \"\"\"\n+import scrapy\n+\n+class MySpider(scrapy.Spider):\n+    name = 'myspider'\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, *args, **kwargs):\n+        spider = super().from_crawler(crawler, *args, **kwargs)\n+        spider.settings.set(\"FOO\", kwargs.get(\"foo\"))\n+        return spider\n+\n+    def start_requests(self):\n+        self.logger.info(f\"The value of FOO is {self.settings.getint('FOO')}\")\n+        return []\n+\"\"\"\n+        args = [\"-a\", \"foo=42\"]\n+        log = self.get_log(spider_code, args=args)\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertIn(\"The value of FOO is 42\", log)\n+\n \n @skipIf(platform.system() != \"Windows\", \"Windows required for .pyw files\")\n class WindowsRunSpiderCommandTest(RunSpiderCommandTest):\n\n@@ -481,6 +481,11 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertNotIn(\"Using asyncio event loop: uvloop.Loop\", log)\n         self.assertIn(\"async pipeline opened!\", log)\n \n+    def test_args_change_settings(self):\n+        log = self.run_script(\"args_settings.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertIn(\"The value of FOO is 42\", log)\n+\n \n class CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):\n     script_dir = Path(__file__).parent.resolve() / \"CrawlerRunner\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#028a56b9a2e090bde761a0487a8504a3b263eb97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 38 | Files Changed: 3 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 92 | Churn Cumulative: 3605 | Contributors (this commit): 29 | Commits (past 90d): 25 | Contributors (cumulative): 44 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,8 @@ import unittest\n from typing import Any, Dict\n from unittest.mock import patch\n \n+from twisted.internet.defer import inlineCallbacks\n+\n from scrapy import Spider\n from scrapy.crawler import Crawler, CrawlerRunner\n from scrapy.exceptions import NotConfigured\n@@ -177,3 +179,22 @@ class AddonManagerTest(unittest.TestCase):\n                     {\"addons\": [addon]},\n                     extra={\"crawler\": crawler},\n                 )\n+\n+    @inlineCallbacks\n+    def test_enable_addon_in_spider(self):\n+        class MySpider(Spider):\n+            name = \"myspider\"\n+\n+            @classmethod\n+            def from_crawler(cls, crawler, *args, **kwargs):\n+                spider = super().from_crawler(crawler, *args, **kwargs)\n+                addon_config = {\"KEY\": \"addon\"}\n+                addon_cls = get_addon_cls(addon_config)\n+                spider.settings.set(\"ADDONS\", {addon_cls: 1}, priority=\"spider\")\n+                return spider\n+\n+        runner = CrawlerRunner({\"KEY\": \"project\"})\n+        crawler = runner.create_crawler(MySpider)\n+        self.assertEqual(crawler.settings.get(\"KEY\"), \"project\")\n+        yield crawler.crawl()\n+        self.assertEqual(crawler.settings.get(\"KEY\"), \"addon\")\n\n@@ -1,7 +1,6 @@\n import json\n import logging\n import unittest\n-import warnings\n from ipaddress import IPv4Address\n from socket import gethostbyname\n from urllib.parse import urlparse\n@@ -14,14 +13,11 @@ from twisted.python.failure import Failure\n from twisted.trial.unittest import TestCase\n \n from scrapy import signals\n-from scrapy.crawler import Crawler, CrawlerRunner\n-from scrapy.exceptions import ScrapyDeprecationWarning, StopDownload\n-from scrapy.extensions.throttle import AutoThrottle\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.exceptions import StopDownload\n from scrapy.http import Request\n from scrapy.http.response import Response\n-from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n-from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.test import get_crawler\n from tests import NON_EXISTING_RESOLVABLE\n from tests.mockserver import MockServer\n@@ -414,38 +410,6 @@ with multiples lines\n         self._assert_retried(log)\n         self.assertIn(\"Got response 200\", str(log))\n \n-    @defer.inlineCallbacks\n-    def test_populate_spidercls_settings(self):\n-        spider_settings = {\n-            \"TEST1\": \"spider\",\n-            \"TEST2\": \"spider\",\n-            \"AUTOTHROTTLE_ENABLED\": True,\n-        }\n-        project_settings = {\"TEST1\": \"project\", \"TEST3\": \"project\"}\n-\n-        class CustomSettingsSpider(DefaultSpider):\n-            custom_settings = spider_settings\n-\n-            def parse(self, response):\n-                return\n-\n-        settings = Settings()\n-        settings.setdict(project_settings, priority=\"project\")\n-        with warnings.catch_warnings():\n-            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n-            crawler = Crawler(CustomSettingsSpider, settings)\n-        yield crawler.crawl()\n-\n-        self.assertEqual(crawler.settings.get(\"TEST1\"), \"spider\")\n-        self.assertEqual(crawler.settings.get(\"TEST2\"), \"spider\")\n-        self.assertEqual(crawler.settings.get(\"TEST3\"), \"project\")\n-\n-        enabled_exts = [e.__class__ for e in crawler.extensions.middlewares]\n-        self.assertIn(AutoThrottle, enabled_exts)\n-\n-        self.assertFalse(settings.frozen)\n-        self.assertTrue(crawler.settings.frozen)\n-\n \n class CrawlSpiderTestCase(TestCase):\n     def setUp(self):\n\n@@ -17,6 +17,7 @@ import scrapy\n from scrapy.crawler import Crawler, CrawlerProcess, CrawlerRunner\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.extensions import telnet\n+from scrapy.extensions.throttle import AutoThrottle\n from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n from scrapy.utils.log import configure_logging, get_scrapy_root_handler\n@@ -32,6 +33,25 @@ class BaseCrawlerTest(unittest.TestCase):\n \n \n class CrawlerTestCase(BaseCrawlerTest):\n+    def test_populate_spidercls_settings(self):\n+        spider_settings = {\"TEST1\": \"spider\", \"TEST2\": \"spider\"}\n+        project_settings = {\"TEST1\": \"project\", \"TEST3\": \"project\"}\n+\n+        class CustomSettingsSpider(DefaultSpider):\n+            custom_settings = spider_settings\n+\n+        settings = Settings()\n+        settings.setdict(project_settings, priority=\"project\")\n+        crawler = Crawler(CustomSettingsSpider, settings)\n+        crawler._load_settings()\n+\n+        self.assertEqual(crawler.settings.get(\"TEST1\"), \"spider\")\n+        self.assertEqual(crawler.settings.get(\"TEST2\"), \"spider\")\n+        self.assertEqual(crawler.settings.get(\"TEST3\"), \"project\")\n+\n+        self.assertFalse(settings.frozen)\n+        self.assertTrue(crawler.settings.frozen)\n+\n     def test_crawler_accepts_dict(self):\n         crawler = get_crawler(DefaultSpider, {\"foo\": \"bar\"})\n         self.assertEqual(crawler.settings[\"foo\"], \"bar\")\n@@ -58,6 +78,17 @@ class CrawlerTestCase(BaseCrawlerTest):\n             yield crawler.crawl()\n \n \n+class SpiderSettingsTestCase(unittest.TestCase):\n+    def test_spider_custom_settings(self):\n+        class MySpider(scrapy.Spider):\n+            name = \"spider\"\n+            custom_settings = {\"AUTOTHROTTLE_ENABLED\": True}\n+\n+        crawler = get_crawler(MySpider)\n+        enabled_exts = [e.__class__ for e in crawler.extensions.middlewares]\n+        self.assertIn(AutoThrottle, enabled_exts)\n+\n+\n class CrawlerLoggingTestCase(unittest.TestCase):\n     def test_no_root_handler_installed(self):\n         handler = get_scrapy_root_handler()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#619140717fd74aff0c8119217f4d11f688d4dac6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 430 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,9 @@\n import itertools\n-import unittest\n from typing import Any, Dict\n from unittest.mock import patch\n \n from twisted.internet.defer import inlineCallbacks\n+from twisted.trial import unittest\n \n from scrapy import Spider\n from scrapy.crawler import Crawler, CrawlerRunner\n@@ -193,8 +193,10 @@ class AddonManagerTest(unittest.TestCase):\n                 spider.settings.set(\"ADDONS\", {addon_cls: 1}, priority=\"spider\")\n                 return spider\n \n-        runner = CrawlerRunner({\"KEY\": \"project\"})\n+        settings = Settings()\n+        settings.set(\"KEY\", \"default\", priority=\"default\")\n+        runner = CrawlerRunner(settings)\n         crawler = runner.create_crawler(MySpider)\n-        self.assertEqual(crawler.settings.get(\"KEY\"), \"project\")\n+        self.assertEqual(crawler.settings.get(\"KEY\"), \"default\")\n         yield crawler.crawl()\n         self.assertEqual(crawler.settings.get(\"KEY\"), \"addon\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#37562163393cc145171b802699c84467781c701b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 7 | Hunks: 9 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 6472 | Contributors (this commit): 50 | Commits (past 90d): 70 | Contributors (cumulative): 95 | DMM Complexity: None\n\nDIFF:\n@@ -19,8 +19,8 @@ class AddonManager:\n         self.crawler: \"Crawler\" = crawler\n         self.addons: List[Any] = []\n \n-    def load_settings(self, settings: Settings) -> None:\n-        \"\"\"Load add-ons and configurations from a settings object.\n+    def apply_settings(self, settings: Settings) -> None:\n+        \"\"\"Load add-ons and configurations from a settings object and apply them.\n \n         This will load the add-on for every add-on path in the\n         ``ADDONS`` setting and execute their ``update_settings`` methods.\n\n@@ -77,7 +77,7 @@ class Command(ScrapyCommand):\n         # The crawler is created this way since the Shell manually handles the\n         # crawling engine, so the set up in the crawl method won't work\n         crawler = self.crawler_process._create_crawler(spidercls)\n-        crawler._load_settings()\n+        crawler._apply_settings()\n         # The Shell class needs a persistent engine in the crawler\n         crawler.engine = crawler._create_engine()\n         crawler.engine.start()\n\n@@ -94,12 +94,12 @@ class Crawler:\n             # scrapy root handler already installed: update it with new settings\n             install_scrapy_root_handler(self.settings)\n \n-    def _load_settings(self) -> None:\n+    def _apply_settings(self) -> None:\n         if self._settings_loaded:\n             return\n         self._settings_loaded = True\n \n-        self.addons.load_settings(self.settings)\n+        self.addons.apply_settings(self.settings)\n         self.stats = load_object(self.settings[\"STATS_CLASS\"])(self)\n \n         handler = LogCounterHandler(self, level=self.settings.get(\"LOG_LEVEL\"))\n@@ -155,7 +155,7 @@ class Crawler:\n \n         try:\n             self.spider = self._create_spider(*args, **kwargs)\n-            self._load_settings()\n+            self._apply_settings()\n             self._update_root_log_handler()\n             self.engine = self._create_engine()\n             start_requests = iter(self.spider.start_requests())\n\n@@ -91,7 +91,7 @@ def get_crawler(\n     settings.update(settings_dict or {})\n     runner = CrawlerRunner(settings)\n     crawler = runner.create_crawler(spidercls or TestSpider)\n-    crawler._load_settings()\n+    crawler._apply_settings()\n     return crawler\n \n \n\n@@ -113,7 +113,7 @@ class AddonManagerTest(unittest.TestCase):\n         settings.set(\"KEY\", 0, priority=\"default\")\n         runner = CrawlerRunner(settings)\n         crawler = runner.create_crawler(Spider)\n-        crawler._load_settings()\n+        crawler._apply_settings()\n         self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n \n         settings_dict = {\n\n@@ -43,7 +43,7 @@ class CrawlerTestCase(BaseCrawlerTest):\n         settings = Settings()\n         settings.setdict(project_settings, priority=\"project\")\n         crawler = Crawler(CustomSettingsSpider, settings)\n-        crawler._load_settings()\n+        crawler._apply_settings()\n \n         self.assertEqual(crawler.settings.get(\"TEST1\"), \"spider\")\n         self.assertEqual(crawler.settings.get(\"TEST2\"), \"spider\")\n\n@@ -61,7 +61,7 @@ class TestExtPeriodicLog(PeriodicLog):\n \n def extension(settings=None):\n     crawler = Crawler(MetaSpider, settings=settings)\n-    crawler._load_settings()\n+    crawler._apply_settings()\n     return TestExtPeriodicLog.from_crawler(crawler)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ac201d310b812c53465ad6a27b033f22206dae4b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 3024 | Contributors (this commit): 32 | Commits (past 90d): 33 | Contributors (cumulative): 33 | DMM Complexity: 0.5\n\nDIFF:\n@@ -19,7 +19,7 @@ class AddonManager:\n         self.crawler: \"Crawler\" = crawler\n         self.addons: List[Any] = []\n \n-    def apply_settings(self, settings: Settings) -> None:\n+    def load_settings(self, settings: Settings) -> None:\n         \"\"\"Load add-ons and configurations from a settings object and apply them.\n \n         This will load the add-on for every add-on path in the\n\n@@ -79,7 +79,6 @@ class Crawler:\n \n         self._init_reactor: bool = init_reactor\n         self.crawling: bool = False\n-        self._settings_loaded: bool = False\n         self._started: bool = False\n \n         self.extensions: Optional[ExtensionManager] = None\n@@ -95,11 +94,10 @@ class Crawler:\n             install_scrapy_root_handler(self.settings)\n \n     def _apply_settings(self) -> None:\n-        if self._settings_loaded:\n+        if self.settings.frozen:\n             return\n-        self._settings_loaded = True\n \n-        self.addons.apply_settings(self.settings)\n+        self.addons.load_settings(self.settings)\n         self.stats = load_object(self.settings[\"STATS_CLASS\"])(self)\n \n         handler = LogCounterHandler(self, level=self.settings.get(\"LOG_LEVEL\"))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#be0e33af92d1c84cdca6068e54fc736bb524342f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 166 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -24,5 +24,6 @@ process = CrawlerProcess(\n         \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n     }\n )\n-process.crawl(NoRequestsSpider)\n+d = process.crawl(NoRequestsSpider)\n+d.addErrback(lambda failure: failure.printTraceback())\n process.start()\n\n@@ -17,5 +17,6 @@ process = CrawlerProcess(\n     }\n )\n \n-process.crawl(NoRequestsSpider)\n+d = process.crawl(NoRequestsSpider)\n+d.addErrback(lambda failure: failure.printTraceback())\n process.start()\n\n@@ -26,5 +26,6 @@ process = CrawlerProcess(\n     }\n )\n \n-process.crawl(NoRequestsSpider)\n+d = process.crawl(NoRequestsSpider)\n+d.addErrback(lambda failure: failure.printTraceback())\n process.start()\n\n@@ -17,6 +17,8 @@ class AsyncioReactorSpider(scrapy.Spider):\n \n \n process = CrawlerProcess()\n-process.crawl(SelectReactorSpider)\n-process.crawl(AsyncioReactorSpider)\n+d1 = process.crawl(SelectReactorSpider)\n+d1.addErrback(lambda failure: failure.printTraceback())\n+d2 = process.crawl(AsyncioReactorSpider)\n+d2.addErrback(lambda failure: failure.printTraceback())\n process.start()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6428356584c05e1a5074f54ad7f6722cc8a69fc7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 9 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 181 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -2,6 +2,7 @@ import asyncio\n import sys\n \n from twisted.internet import asyncioreactor\n+from twisted.python import log\n \n if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n@@ -25,5 +26,5 @@ process = CrawlerProcess(\n     }\n )\n d = process.crawl(NoRequestsSpider)\n-d.addErrback(lambda failure: failure.printTraceback())\n+d.addErrback(log.err)\n process.start()\n\n@@ -1,4 +1,5 @@\n from twisted.internet import reactor  # noqa: F401\n+from twisted.python import log\n \n import scrapy\n from scrapy.crawler import CrawlerProcess\n@@ -18,5 +19,5 @@ process = CrawlerProcess(\n )\n \n d = process.crawl(NoRequestsSpider)\n-d.addErrback(lambda failure: failure.printTraceback())\n+d.addErrback(log.err)\n process.start()\n\n@@ -1,5 +1,6 @@\n from twisted.internet.main import installReactor\n from twisted.internet.selectreactor import SelectReactor\n+from twisted.python import log\n \n import scrapy\n from scrapy.crawler import CrawlerProcess\n@@ -27,5 +28,5 @@ process = CrawlerProcess(\n )\n \n d = process.crawl(NoRequestsSpider)\n-d.addErrback(lambda failure: failure.printTraceback())\n+d.addErrback(log.err)\n process.start()\n\n@@ -1,3 +1,5 @@\n+from twisted.python import log\n+\n import scrapy\n from scrapy.crawler import CrawlerProcess\n \n@@ -18,7 +20,7 @@ class AsyncioReactorSpider(scrapy.Spider):\n \n process = CrawlerProcess()\n d1 = process.crawl(SelectReactorSpider)\n-d1.addErrback(lambda failure: failure.printTraceback())\n+d1.addErrback(log.err)\n d2 = process.crawl(AsyncioReactorSpider)\n-d2.addErrback(lambda failure: failure.printTraceback())\n+d2.addErrback(log.err)\n process.start()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#922ff5738448205c5b7e5ced533bb8820b168480", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 44 | Churn Cumulative: 1899 | Contributors (this commit): 20 | Commits (past 90d): 7 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,12 +23,13 @@ from scrapy.utils.response import response_status_message\n \n retry_logger = getLogger(__name__)\n \n+DEPRECATED_ATTRIBUTE = \"EXCEPTIONS_TO_RETRY\"\n \n-class BackwardsCompatibilityMetaclass(type):\n-    @property\n-    def EXCEPTIONS_TO_RETRY(cls):\n+\n+def backwards_compatibility_getattr(self, name):\n+    if name == DEPRECATED_ATTRIBUTE:\n         warnings.warn(\n-            \"Attribute RetryMiddleware.EXCEPTIONS_TO_RETRY is deprecated. \"\n+            f\"Attribute RetryMiddleware.{DEPRECATED_ATTRIBUTE} is deprecated. \"\n             \"Use the RETRY_EXCEPTIONS setting instead.\",\n             ScrapyDeprecationWarning,\n             stacklevel=2,\n@@ -37,6 +38,13 @@ class BackwardsCompatibilityMetaclass(type):\n             load_object(x) if isinstance(x, str) else x\n             for x in Settings().getlist(\"RETRY_EXCEPTIONS\")\n         )\n+    raise AttributeError(\n+        f\"{self.__class__.__name__!r} object has no attribute {name!r}\"\n+    )\n+\n+\n+class BackwardsCompatibilityMetaclass(type):\n+    __getattr__ = backwards_compatibility_getattr\n \n \n def get_retry_request(\n@@ -137,15 +145,14 @@ class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n         )\n         self.priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n \n-        if not hasattr(\n-            self, \"EXCEPTIONS_TO_RETRY\"\n-        ):  # If EXCEPTIONS_TO_RETRY is not \"overriden\"\n+        try:\n+            self.exceptions_to_retry = self.__getattribute__(DEPRECATED_ATTRIBUTE)\n+        except AttributeError:\n+            # If EXCEPTIONS_TO_RETRY is not \"overridden\"\n             self.exceptions_to_retry = tuple(\n                 load_object(x) if isinstance(x, str) else x\n                 for x in settings.getlist(\"RETRY_EXCEPTIONS\")\n             )\n-        else:\n-            self.exceptions_to_retry = self.EXCEPTIONS_TO_RETRY\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -175,3 +182,5 @@ class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n             max_retry_times=max_retry_times,\n             priority_adjust=priority_adjust,\n         )\n+\n+    __getattr__ = backwards_compatibility_getattr\n\n@@ -122,7 +122,7 @@ class RetryTest(unittest.TestCase):\n         req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n         self._test_retry_exception(req, exc(\"foo\"), mw)\n \n-    def test_exception_to_retry_customMiddleware(self):\n+    def test_exception_to_retry_custom_middleware(self):\n         exc = ValueError\n \n         with warnings.catch_warnings(record=True) as warns:\n@@ -138,6 +138,21 @@ class RetryTest(unittest.TestCase):\n         assert isinstance(req, Request)\n         self.assertEqual(req.meta[\"retry_times\"], 1)\n \n+    def test_exception_to_retry_custom_middleware_self(self):\n+        class MyRetryMiddleware(RetryMiddleware):\n+            def process_exception(self, request, exception, spider):\n+                if isinstance(exception, self.EXCEPTIONS_TO_RETRY):\n+                    return self._retry(request, exception, spider)\n+\n+        exc = OSError\n+        mw2 = MyRetryMiddleware.from_crawler(self.crawler)\n+        req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n+        with warnings.catch_warnings(record=True) as warns:\n+            req = mw2.process_exception(req, exc(\"foo\"), self.spider)\n+        assert isinstance(req, Request)\n+        self.assertEqual(req.meta[\"retry_times\"], 1)\n+        self.assertEqual(len(warns), 1)\n+\n     def _test_retry_exception(self, req, exception, mw=None):\n         if mw is None:\n             mw = self.mw\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#800c1f112e883070d97aa679b4bae10f618633a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 574 | Contributors (this commit): 16 | Commits (past 90d): 3 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -23,13 +23,11 @@ from scrapy.utils.response import response_status_message\n \n retry_logger = getLogger(__name__)\n \n-DEPRECATED_ATTRIBUTE = \"EXCEPTIONS_TO_RETRY\"\n-\n \n def backwards_compatibility_getattr(self, name):\n-    if name == DEPRECATED_ATTRIBUTE:\n+    if name == \"EXCEPTIONS_TO_RETRY\":\n         warnings.warn(\n-            f\"Attribute RetryMiddleware.{DEPRECATED_ATTRIBUTE} is deprecated. \"\n+            \"Attribute RetryMiddleware.EXCEPTIONS_TO_RETRY is deprecated. \"\n             \"Use the RETRY_EXCEPTIONS setting instead.\",\n             ScrapyDeprecationWarning,\n             stacklevel=2,\n@@ -146,7 +144,7 @@ class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n         self.priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n \n         try:\n-            self.exceptions_to_retry = self.__getattribute__(DEPRECATED_ATTRIBUTE)\n+            self.exceptions_to_retry = self.__getattribute__(\"EXCEPTIONS_TO_RETRY\")\n         except AttributeError:\n             # If EXCEPTIONS_TO_RETRY is not \"overridden\"\n             self.exceptions_to_retry = tuple(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e84fb6d5cb15a36022dbb9ef27f9924383d317fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 832 | Contributors (this commit): 37 | Commits (past 90d): 3 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -48,6 +48,7 @@ setup(\n     license=\"BSD\",\n     packages=find_packages(exclude=(\"tests\", \"tests.*\")),\n     include_package_data=True,\n+    package_data={\"scrapy\": \"py.typed\"},\n     zip_safe=False,\n     entry_points={\"console_scripts\": [\"scrapy = scrapy.cmdline:execute\"]},\n     classifiers=[\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#11bdc3df590d461634865027d6911cc5f8116cfe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 833 | Contributors (this commit): 37 | Commits (past 90d): 4 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -48,7 +48,6 @@ setup(\n     license=\"BSD\",\n     packages=find_packages(exclude=(\"tests\", \"tests.*\")),\n     include_package_data=True,\n-    package_data={\"scrapy\": \"py.typed\"},\n     zip_safe=False,\n     entry_points={\"console_scripts\": [\"scrapy = scrapy.cmdline:execute\"]},\n     classifiers=[\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#720f351a3eea5e5bfa83a6eaf50210cd1fa43992", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 2411 | Contributors (this commit): 32 | Commits (past 90d): 19 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -416,15 +416,17 @@ class CrawlerProcess(CrawlerRunner):\n                 return\n             d.addBoth(self._stop_reactor)\n \n-        if install_signal_handlers:\n-            install_shutdown_handlers(self._signal_shutdown)\n         resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n         resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)\n         resolver.install_on_reactor()\n         tp = reactor.getThreadPool()\n         tp.adjustPoolsize(maxthreads=self.settings.getint(\"REACTOR_THREADPOOL_MAXSIZE\"))\n         reactor.addSystemEventTrigger(\"before\", \"shutdown\", self.stop)\n-        reactor.run(installSignalHandlers=False)  # blocking call\n+        if install_signal_handlers:\n+            reactor.addSystemEventTrigger(\n+                \"after\", \"startup\", install_shutdown_handlers, self._signal_shutdown\n+            )\n+        reactor.run()  # blocking call\n \n     def _graceful_stop_reactor(self) -> Deferred:\n         d = self.stop()\n\n@@ -19,13 +19,10 @@ def install_shutdown_handlers(\n     function: SignalHandlerT, override_sigint: bool = True\n ) -> None:\n     \"\"\"Install the given function as a signal handler for all common shutdown\n-    signals (such as SIGINT, SIGTERM, etc). If override_sigint is ``False`` the\n-    SIGINT handler won't be install if there is already a handler in place\n+    signals (such as SIGINT, SIGTERM, etc). If ``override_sigint`` is ``False`` the\n+    SIGINT handler won't be installed if there is already a handler in place\n     (e.g. Pdb)\n     \"\"\"\n-    from twisted.internet import reactor\n-\n-    reactor._handleSignals()\n     signal.signal(signal.SIGTERM, function)\n     if signal.getsignal(signal.SIGINT) == signal.default_int_handler or override_sigint:\n         signal.signal(signal.SIGINT, function)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4b14215f83996a18d34c95bec953ef9cfba05245", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 836 | Contributors (this commit): 37 | Commits (past 90d): 5 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -6,8 +6,7 @@ version = (Path(__file__).parent / \"scrapy/VERSION\").read_text(\"ascii\").strip()\n \n \n install_requires = [\n-    # 23.8.0 incompatibility: https://github.com/scrapy/scrapy/issues/6024\n-    \"Twisted>=18.9.0,<23.8.0\",\n+    \"Twisted>=18.9.0\",\n     \"cryptography>=36.0.0\",\n     \"cssselect>=0.9.1\",\n     \"itemloaders>=1.0.1\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0c6440a427a6f3ed2920de38f97d15d692833851", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 135 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import os\n import sys\n-from typing import Iterable, Optional, Tuple, cast\n+from typing import Iterable, List, Optional, Tuple, cast\n \n from twisted.internet.defer import Deferred\n from twisted.internet.error import ProcessTerminated\n@@ -26,14 +26,15 @@ class ProcessTest:\n         env = os.environ.copy()\n         if settings is not None:\n             env[\"SCRAPY_SETTINGS_MODULE\"] = settings\n+        assert self.command\n         cmd = self.prefix + [self.command] + list(args)\n         pp = TestProcessProtocol()\n-        pp.deferred.addBoth(self._process_finished, cmd, check_code)\n+        pp.deferred.addCallback(self._process_finished, cmd, check_code)\n         reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)\n         return pp.deferred\n \n     def _process_finished(\n-        self, pp: TestProcessProtocol, cmd: str, check_code: bool\n+        self, pp: TestProcessProtocol, cmd: List[str], check_code: bool\n     ) -> Tuple[int, bytes, bytes]:\n         if pp.exitcode and check_code:\n             msg = f\"process {cmd} exit with code {pp.exitcode}\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0630e4aaa10c3fb8c79c2542a229f5c0632cddde", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2344 | Contributors (this commit): 31 | Commits (past 90d): 17 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -404,8 +404,8 @@ class CrawlerProcess(CrawlerRunner):\n         :param bool stop_after_crawl: stop or not the reactor when all\n             crawlers have finished\n \n-        :param bool install_signal_handlers: whether to install the shutdown\n-            handlers (default: True)\n+        :param bool install_signal_handlers: whether to install the OS signal\n+            handlers from Twisted and Scrapy (default: True)\n         \"\"\"\n         from twisted.internet import reactor\n \n@@ -426,7 +426,7 @@ class CrawlerProcess(CrawlerRunner):\n             reactor.addSystemEventTrigger(\n                 \"after\", \"startup\", install_shutdown_handlers, self._signal_shutdown\n             )\n-        reactor.run()  # blocking call\n+        reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n \n     def _graceful_stop_reactor(self) -> Deferred:\n         d = self.stop()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d19e315b0b07b070f45cf3c082d4dfe0e6a87186", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 26 | Churn Cumulative: 786 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,11 +1,15 @@\n+import sys\n+from io import BytesIO\n from pathlib import Path\n \n+from pexpect.popen_spawn import PopenSpawn\n from twisted.internet import defer\n from twisted.trial import unittest\n \n from scrapy.utils.testproc import ProcessTest\n from scrapy.utils.testsite import SiteTest\n from tests import NON_EXISTING_RESOLVABLE, tests_datadir\n+from tests.mockserver import MockServer\n \n \n class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n@@ -133,3 +137,25 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n         args = [\"-c\", code, \"--set\", f\"TWISTED_REACTOR={reactor_path}\"]\n         _, _, err = yield self.execute(args, check_code=True)\n         self.assertNotIn(b\"RuntimeError: There is no current event loop in thread\", err)\n+\n+\n+class InteractiveShellTest(unittest.TestCase):\n+    def test_fetch(self):\n+        args = (\n+            sys.executable,\n+            \"-m\",\n+            \"scrapy.cmdline\",\n+            \"shell\",\n+        )\n+        logfile = BytesIO()\n+        p = PopenSpawn(args, timeout=5)\n+        p.logfile_read = logfile\n+        p.expect_exact(\"Available Scrapy objects\")\n+        with MockServer() as mockserver:\n+            p.sendline(f\"fetch('{mockserver.url('/')}')\")\n+            p.sendline(\"type(response)\")\n+            p.expect_exact(\"HtmlResponse\")\n+        p.sendeof()\n+        p.wait()\n+        logfile.seek(0)\n+        self.assertNotIn(\"Traceback\", logfile.read().decode())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4b5fb9b5a6c7738aee22b5080fd96588feea39a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 55 | Churn Cumulative: 1751 | Contributors (this commit): 22 | Commits (past 90d): 11 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,24 @@\n+from twisted.internet.defer import Deferred\n+\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+from scrapy.utils.defer import maybe_deferred_to_future\n+\n+\n+class SleepingSpider(scrapy.Spider):\n+    name = \"sleeping\"\n+\n+    start_urls = [\"data:,;\"]\n+\n+    async def parse(self, response):\n+        from twisted.internet import reactor\n+\n+        d = Deferred()\n+        reactor.callLater(2, d.callback, None)\n+        await maybe_deferred_to_future(d)\n+\n+\n+process = CrawlerProcess(settings={})\n+\n+process.crawl(SleepingSpider)\n+process.start()\n\n@@ -1,13 +1,16 @@\n import logging\n import os\n import platform\n+import signal\n import subprocess\n import sys\n import warnings\n from pathlib import Path\n+from typing import List\n \n import pytest\n from packaging.version import parse as parse_version\n+from pexpect.popen_spawn import PopenSpawn\n from pytest import mark, raises\n from twisted.internet import defer\n from twisted.trial import unittest\n@@ -289,9 +292,12 @@ class ScriptRunnerMixin:\n     script_dir: Path\n     cwd = os.getcwd()\n \n-    def run_script(self, script_name: str, *script_args):\n+    def get_script_args(self, script_name: str, *script_args: str) -> List[str]:\n         script_path = self.script_dir / script_name\n-        args = [sys.executable, str(script_path)] + list(script_args)\n+        return [sys.executable, str(script_path)] + list(script_args)\n+\n+    def run_script(self, script_name: str, *script_args: str) -> str:\n+        args = self.get_script_args(script_name, *script_args)\n         p = subprocess.Popen(\n             args,\n             env=get_mockserver_env(),\n@@ -517,6 +523,27 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn(\"Spider closed (finished)\", log)\n         self.assertIn(\"The value of FOO is 42\", log)\n \n+    def test_shutdown_graceful(self):\n+        args = self.get_script_args(\"sleeping.py\")\n+        p = PopenSpawn(args, timeout=5)\n+        p.expect_exact(\"Spider opened\")\n+        p.expect_exact(\"Crawled (200)\")\n+        p.kill(signal.SIGTERM)\n+        p.expect_exact(\"shutting down gracefully\")\n+        p.expect_exact(\"Spider closed (shutdown)\")\n+        p.wait()\n+\n+    def test_shutdown_forced(self):\n+        args = self.get_script_args(\"sleeping.py\")\n+        p = PopenSpawn(args, timeout=5)\n+        p.expect_exact(\"Spider opened\")\n+        p.expect_exact(\"Crawled (200)\")\n+        p.kill(signal.SIGTERM)\n+        p.expect_exact(\"shutting down gracefully\")\n+        p.kill(signal.SIGTERM)\n+        p.expect_exact(\"forcing unclean shutdown\")\n+        p.wait()\n+\n \n class CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):\n     script_dir = Path(__file__).parent.resolve() / \"CrawlerRunner\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb5e2e79ba8d2c492737a6e2ab8e0d4de2bd1a6e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1733 | Contributors (this commit): 22 | Commits (past 90d): 11 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -528,7 +528,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         p = PopenSpawn(args, timeout=5)\n         p.expect_exact(\"Spider opened\")\n         p.expect_exact(\"Crawled (200)\")\n-        p.kill(signal.SIGTERM)\n+        p.kill(signal.SIGINT)\n         p.expect_exact(\"shutting down gracefully\")\n         p.expect_exact(\"Spider closed (shutdown)\")\n         p.wait()\n@@ -538,9 +538,9 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         p = PopenSpawn(args, timeout=5)\n         p.expect_exact(\"Spider opened\")\n         p.expect_exact(\"Crawled (200)\")\n-        p.kill(signal.SIGTERM)\n+        p.kill(signal.SIGINT)\n         p.expect_exact(\"shutting down gracefully\")\n-        p.kill(signal.SIGTERM)\n+        p.kill(signal.SIGINT)\n         p.expect_exact(\"forcing unclean shutdown\")\n         p.wait()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#83fff6c9511fdde1024d8cd33d99314ddf4eb2ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 305 | Contributors (this commit): 19 | Commits (past 90d): 2 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -73,6 +73,7 @@ IGNORED_EXTENSIONS = [\n     \"pps\",\n     \"doc\",\n     \"docx\",\n+    \"dotx\",\n     \"odt\",\n     \"ods\",\n     \"odg\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#da6e75d00a8a64fea0073816d59eef2ae3b4c672", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 314 | Contributors (this commit): 19 | Commits (past 90d): 3 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -67,12 +67,21 @@ IGNORED_EXTENSIONS = [\n     \"webm\",\n     # office suites\n     \"xls\",\n+    \"xlsm\",\n     \"xlsx\",\n+    \"xltm\",\n+    \"xltx\",\n+    \"potm\",\n+    \"potx\",\n     \"ppt\",\n+    \"pptm\",\n     \"pptx\",\n     \"pps\",\n     \"doc\",\n+    \"docb\",\n+    \"docm\",\n     \"docx\",\n+    \"dotm\",\n     \"dotx\",\n     \"odt\",\n     \"ods\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1ed9ed4f923ca40e0b43a72bff9e90d38db3a3bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 8568 | Contributors (this commit): 57 | Commits (past 90d): 13 | Contributors (cumulative): 71 | DMM Complexity: None\n\nDIFF:\n@@ -125,9 +125,6 @@ class FileFeedStorageTest(unittest.TestCase):\n             path.unlink()\n \n \n-@pytest.mark.skipif(\n-    sys.version_info >= (3, 12), reason=\"pyftpdlib doesn't support Python 3.12 yet\"\n-)\n class FTPFeedStorageTest(unittest.TestCase):\n     def get_test_spider(self, settings=None):\n         class TestSpider(scrapy.Spider):\n\n@@ -1,7 +1,6 @@\n import dataclasses\n import os\n import random\n-import sys\n import time\n from datetime import datetime\n from io import BytesIO\n@@ -12,7 +11,6 @@ from unittest import mock\n from urllib.parse import urlparse\n \n import attr\n-import pytest\n from itemadapter import ItemAdapter\n from twisted.internet import defer\n from twisted.trial import unittest\n@@ -648,9 +646,6 @@ class TestGCSFilesStore(unittest.TestCase):\n                     store.bucket.get_blob.assert_called_with(expected_blob_path)\n \n \n-@pytest.mark.skipif(\n-    sys.version_info >= (3, 12), reason=\"pyftpdlib doesn't support Python 3.12 yet\"\n-)\n class TestFTPFileStore(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_persist(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fb4debda04cff4e33daf62d95c71ab2919c37c07", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2932 | Contributors (this commit): 34 | Commits (past 90d): 3 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -127,7 +127,7 @@ class FileTestCase(unittest.TestCase):\n         return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n \n     def test_non_existent(self):\n-        request = Request(f\"file://{self.mktemp()}\")\n+        request = Request(path_to_file_uri(self.mktemp()))\n         d = self.download_request(request, Spider(\"foo\"))\n         return self.assertFailure(d, OSError)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c65567988da2f6dd8ad894cf0cf57f2c074be10f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 419 | Contributors (this commit): 24 | Commits (past 90d): 1 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -276,7 +276,7 @@ coverage_ignore_pyobjects = [\n \n intersphinx_mapping = {\n     \"attrs\": (\"https://www.attrs.org/en/stable/\", None),\n-    \"coverage\": (\"https://coverage.readthedocs.io/en/stable\", None),\n+    \"coverage\": (\"https://coverage.readthedocs.io/en/latest\", None),\n     \"cryptography\": (\"https://cryptography.io/en/latest/\", None),\n     \"cssselect\": (\"https://cssselect.readthedocs.io/en/latest\", None),\n     \"itemloaders\": (\"https://itemloaders.readthedocs.io/en/latest/\", None),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
