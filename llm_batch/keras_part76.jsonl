{"custom_id": "keras#80ddb5b3b84be2ca26ba6c144841c7cb882ef316", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 7779 | Contributors (this commit): 41 | Commits (past 90d): 67 | Contributors (cumulative): 41 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1465,7 +1465,8 @@ class Lambda(Layer):\n         if self._output_shape is None:\n             return self.input_shape\n         elif type(self._output_shape) == tuple:\n-            return (self.input_shape[0], ) + self._output_shape\n+            nb_samples = self.input_shape[0] if self.input_shape else None\n+            return (nb_samples, ) + self._output_shape\n         else:\n             output_shape_func = marshal.loads(self._output_shape)\n             output_shape_func = types.FunctionType(output_shape_func, globals())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#089fa1175260e06f18c249c9ab12a1df3a586795", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 18 | Churn Cumulative: 7797 | Contributors (this commit): 41 | Commits (past 90d): 73 | Contributors (cumulative): 41 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1121,6 +1121,24 @@ class TimeDistributedDense(MaskedLayer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n+        input_length = self.input_shape[1]\n+        if input_length and K._BACKEND == 'theano':\n+            import theano.tensor as T\n+            #X: (nb_samples, timesteps, input_dim)\n+            X = K.permute_dimensions(X, (1, 0, 2))\n+            #X: (timesteps, nb_samples, input_dim)\n+            W = [self.W] * input_length\n+            W = T.stack(*W)\n+            #W: (timesteps, input_dim, output_dim)\n+            z = T.batched_tensordot(X, W, axes=[(2), (1)])\n+            #z: (timesteps, nb_samples, output_dim)\n+            z = K.permute_dimensions(z, (1, 0, 2))\n+            #z: (nb_samples, timesteps, output_dim)\n+            b = [self.b] * input_length\n+            b = T.stack(*b)\n+            #b: (timesteps, output_dim)\n+            Y = self.activation(z + b)\n+            return Y\n             \n         def step(x, states):\n             output = K.dot(x, self.W) + self.b\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3aa807a0c89279b065818a3db95e3755cdf614e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 33 | Files Changed: 4 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 8/9 | Churn Δ: 51 | Churn Cumulative: 10215 | Contributors (this commit): 52 | Commits (past 90d): 153 | Contributors (cumulative): 69 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,13 +7,9 @@ def softmax(x):\n     if ndim == 2:\n         return K.softmax(x)\n     elif ndim == 3:\n-        # apply softmax to each timestep\n-        def step(x, states):\n-            return K.softmax(x), []\n-        last_output, outputs, states = K.rnn(step, x,\n-                                             [],\n-                                             mask=None)\n-        return outputs\n+        e = K.exp(x)\n+        s = K.sum(e, axis=-1, keepdims=True)\n+        return e / s\n     else:\n         raise Exception('Cannot apply softmax to a tensor that is not 2D or 3D. ' +\n                         'Here, ndim=' + str(ndim))\n\n@@ -353,6 +353,8 @@ def spatial_2d_padding(x, padding=(1, 1), dim_ordering='th'):\n                    [0, 0]]\n     return tf.pad(x, pattern)\n \n+def pack(x):\n+    return tf.pack(x)\n \n # VALUE MANIPULATION\n \n\n@@ -415,6 +415,8 @@ def spatial_3d_padding(x, padding=(1, 1, 1), dim_ordering='th'):\n         raise Exception('Invalid dim_ordering: ' + dim_ordering)\n     return T.set_subtensor(output[indices], x)\n \n+def pack(x):\n+    return T.stack(*x)\n \n # VALUE MANIPULATION\n \n\n@@ -1121,34 +1121,19 @@ class TimeDistributedDense(MaskedLayer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        input_length = self.input_shape[1]\n-        if input_length and K._BACKEND == 'theano':\n+        if K._BACKEND == 'theano':\n             import theano.tensor as T\n-            #X: (nb_samples, timesteps, input_dim)\n-            X = K.permute_dimensions(X, (1, 0, 2))\n-            #X: (timesteps, nb_samples, input_dim)\n-            W = [self.W] * input_length\n-            W = T.stack(*W)\n-            #W: (timesteps, input_dim, output_dim)\n-            z = T.batched_tensordot(X, W, axes=[(2), (1)])\n-            #z: (timesteps, nb_samples, output_dim)\n-            z = K.permute_dimensions(z, (1, 0, 2))\n-            #z: (nb_samples, timesteps, output_dim)\n-            b = [self.b] * input_length\n-            b = T.stack(*b)\n-            #b: (timesteps, output_dim)\n-            Y = self.activation(z + b)\n+            z = T.tensordot(X, self.W, axes=[(2), (0)])\n+            Y = z + self.b\n+            Y = self.activation(Y)\n+            return Y\n+        elif K._BACKEND == 'tensorflow':\n+            shape = self.input_shape\n+            X = K.reshape(X, (-1, shape[-1]))\n+            Y = K.dot(X, self.W) + self.b\n+            Y = K.reshape(Y, (-1, shape[1], shape[2]))\n+            Y = self.activation(Y)\n             return Y\n-            \n-        def step(x, states):\n-            output = K.dot(x, self.W) + self.b\n-            return output, []\n-\n-        last_output, outputs, states = K.rnn(step, X,\n-                                             initial_states=[],\n-                                             mask=None)\n-        outputs = self.activation(outputs)\n-        return outputs\n \n     def get_config(self):\n         config = {'name': self.__class__.__name__,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#abca83373d6d7df2747c5cdcc2bb7234395604e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 137 | Lines Deleted: 74 | Files Changed: 2 | Hunks: 24 | Methods Changed: 9 | Complexity Δ (Sum/Max): 16/16 | Churn Δ: 211 | Churn Cumulative: 3639 | Contributors (this commit): 21 | Commits (past 90d): 38 | Contributors (cumulative): 25 | DMM Complexity: 0.8235294117647058\n\nDIFF:\n@@ -47,7 +47,7 @@ print('X_test shape:', X_test.shape)\n print('Build model...')\n model = Sequential()\n model.add(Embedding(max_features, 128, input_length=maxlen, dropout=0.5))\n-model.add(LSTM(128, dropout_W=0.5, dropout_U=0.5))  # try using a GRU instead, for fun\n+model.add(LSTM(128, dropout_W=0.5, dropout_U=0.1))  # try using a GRU instead, for fun\n model.add(Dropout(0.5))\n model.add(Dense(1))\n model.add(Activation('sigmoid'))\n\n@@ -7,6 +7,26 @@ from .. import activations, initializations, regularizers\n from ..layers.core import MaskedLayer\n \n \n+def time_distributed_dense(x, w, b=None, dropout=None,\n+                           input_dim=None, output_dim=None, timesteps=None):\n+    if not input_dim:\n+        # won't work with TensorFlow\n+        input_dim = K.shape(x)[2]\n+    if not timesteps:\n+        # won't work with TensorFlow\n+        timesteps = K.shape(x)[1]\n+    if not output_dim:\n+        output_dim = K.shape(w)[1]\n+    x = K.reshape(x, (-1, input_dim))\n+    if dropout:\n+        x *= K.concatenate([dropout] * timesteps, 0)\n+    x = K.dot(x, w)\n+    if b:\n+        x = x + b\n+    x = K.reshape(x, (-1, timesteps, output_dim))\n+    return x\n+\n+\n class Recurrent(MaskedLayer):\n     '''Abstract base class for recurrent layers.\n     Do not use in a model -- it's not a functional layer!\n@@ -116,23 +136,25 @@ class Recurrent(MaskedLayer):\n     def step(self, x, states):\n         raise NotImplementedError\n \n-    def get_constants(self, X, train=False):\n-        return None\n+    def get_constants(self, x, train=False):\n+        return []\n \n-    def get_initial_states(self, X):\n+    def get_initial_states(self, x):\n         # build an all-zero tensor of shape (samples, output_dim)\n-        initial_state = K.zeros_like(X)  # (samples, timesteps, input_dim)\n+        initial_state = K.zeros_like(x)  # (samples, timesteps, input_dim)\n         initial_state = K.sum(initial_state, axis=1)  # (samples, input_dim)\n         reducer = K.zeros((self.input_dim, self.output_dim))\n         initial_state = K.dot(initial_state, reducer)  # (samples, output_dim)\n         initial_states = [initial_state for _ in range(len(self.states))]\n         return initial_states\n \n+    def preprocess_input(self, x, train=False):\n+        return x\n+\n     def get_output(self, train=False):\n         # input shape: (nb_samples, time (padded with zeros), input_dim)\n         X = self.get_input(train)\n         mask = self.get_input_mask(train)\n-        constants = self.get_constants(X, train)\n \n         assert K.ndim(X) == 3\n         if K._BACKEND == 'tensorflow':\n@@ -150,8 +172,10 @@ class Recurrent(MaskedLayer):\n             initial_states = self.states\n         else:\n             initial_states = self.get_initial_states(X)\n+        constants = self.get_constants(X, train)\n+        preprocessed_input = self.preprocess_input(X, train)\n \n-        last_output, outputs, states = K.rnn(self.step, X,\n+        last_output, outputs, states = K.rnn(self.step, preprocessed_input,\n                                              initial_states,\n                                              go_backwards=self.go_backwards,\n                                              mask=mask,\n@@ -263,33 +287,39 @@ class SimpleRNN(Recurrent):\n         else:\n             self.states = [K.zeros((input_shape[0], self.output_dim))]\n \n-    def step(self, x, states):\n-        # states contains the previous output,\n-        # and the two dropout matrices from self.get_constants()\n-        assert len(states) == 3  # 1 state and 2 constants\n+    def preprocess_input(self, x, train=False):\n+        if train and (0 < self.dropout_W < 1):\n+            ones = K.ones_like(K.reshape(x[:, 0, :], (-1, self.input_dim)))\n+            B_W = K.dropout(ones, self.dropout_W)\n+        else:\n+            B_W = None\n+        input_shape = self.input_shape\n+        input_dim = input_shape[2]\n+        if input_shape[1]:\n+            timesteps = input_shape[1]\n+        else:\n+            # this won't work with TensorFlow\n+            timesteps = K.shape(x)[1]\n+        return time_distributed_dense(x, self.W, self.b, B_W,\n+                                      input_dim, self.output_dim, timesteps)\n+\n+    def step(self, h, states):\n         prev_output = states[0]\n-        B_W = states[1]\n-        B_U = states[2]\n-        h = K.dot(x * B_W, self.W) + self.b\n+        if len(states) == 2:\n+            B_U = states[1]\n+        else:\n+            B_U = 1.\n         output = self.activation(h + K.dot(prev_output * B_U, self.U))\n         return output, [output]\n \n-    def get_constants(self, X, train=False):\n-        retain_p_W = 1. - self.dropout_W\n-        retain_p_U = 1. - self.dropout_U\n-        if train and (self.dropout_W > 0 or self.dropout_U > 0):\n-            nb_samples = K.shape(X)[0]\n-            if K._BACKEND == 'tensorflow':\n-                if not self.input_shape[0]:\n-                    raise Exception('For RNN dropout in tensorflow, a complete ' +\n-                                    'input_shape must be provided (including batch size).')\n-                nb_samples = self.input_shape[0]\n-            B_W = K.random_binomial((nb_samples, self.input_dim), p=retain_p_W)\n-            B_U = K.random_binomial((nb_samples, self.output_dim), p=retain_p_U)\n+    def get_constants(self, x, train=False):\n+        if train and (0 < self.dropout_U < 1):\n+            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n+            ones = K.concatenate([ones] * self.output_dim, 1)\n+            B_U = K.dropout(ones, self.dropout_U)\n+            return [B_U]\n         else:\n-            B_W = np.ones(1, dtype=K.floatx()) * retain_p_W\n-            B_U = np.ones(1, dtype=K.floatx()) * retain_p_U\n-        return [B_W, B_U]\n+            B_U = []\n \n     def get_config(self):\n         config = {\"output_dim\": self.output_dim,\n@@ -405,15 +435,38 @@ class GRU(Recurrent):\n         else:\n             self.states = [K.zeros((input_shape[0], self.output_dim))]\n \n-    def step(self, x, states):\n-        assert len(states) == 3  # 1 state and 2 constants\n-        h_tm1 = states[0]  # previous memory\n-        B_W = states[1]  # dropout matrix for input units\n-        B_U = states[2]  # dropout matrix for recurrent units\n+    def preprocess_input(self, x, train=False):\n+        if train and (0 < self.dropout_W < 1):\n+            ones = K.ones_like(K.reshape(x[:, 0, :], (-1, self.input_dim)))\n+            B_W = [K.dropout(ones, self.dropout_W) for _ in range(3)]\n+        else:\n+            B_W = [None for _ in range(3)]\n+        input_shape = self.input_shape\n+        input_dim = input_shape[2]\n+        if input_shape[1]:\n+            timesteps = input_shape[1]\n+        else:\n+            # this won't work with TensorFlow\n+            timesteps = K.shape(x)[1]\n \n-        x_z = K.dot(x * B_W[0], self.W_z) + self.b_z\n-        x_r = K.dot(x * B_W[1], self.W_r) + self.b_r\n-        x_h = K.dot(x * B_W[2], self.W_h) + self.b_h\n+        x_z = time_distributed_dense(x, self.W_z, self.b_z,\n+                                     B_W[0], input_dim, self.output_dim, timesteps)\n+        x_r = time_distributed_dense(x, self.W_r, self.b_r,\n+                                     B_W[1], input_dim, self.output_dim, timesteps)\n+        x_h = time_distributed_dense(x, self.W_h, self.b_h,\n+                                     B_W[2], input_dim, self.output_dim, timesteps)\n+        return K.concatenate([x_z, x_r, x_h], axis=2)\n+\n+    def step(self, x, states):\n+        h_tm1 = states[0]  # previous memory\n+        if len(states) == 2:\n+            B_U = states[1]  # dropout matrices for recurrent units\n+        else:\n+            B_U = [1., 1., 1.]\n+\n+        x_z = x[:, :self.output_dim]\n+        x_r = x[:, self.output_dim: 2 * self.output_dim]\n+        x_h = x[:, 2 * self.output_dim:]\n \n         z = self.inner_activation(x_z + K.dot(h_tm1 * B_U[0], self.U_z))\n         r = self.inner_activation(x_r + K.dot(h_tm1 * B_U[1], self.U_r))\n@@ -422,22 +475,14 @@ class GRU(Recurrent):\n         h = z * h_tm1 + (1 - z) * hh\n         return h, [h]\n \n-    def get_constants(self, X, train=False):\n-        retain_p_W = 1. - self.dropout_W\n-        retain_p_U = 1. - self.dropout_U\n-        if train and (self.dropout_W > 0 or self.dropout_U > 0):\n-            nb_samples = K.shape(X)[0]\n-            if K._BACKEND == 'tensorflow':\n-                if not self.input_shape[0]:\n-                    raise Exception('For RNN dropout in tensorflow, a complete ' +\n-                                    'input_shape must be provided (including batch size).')\n-                nb_samples = self.input_shape[0]\n-            B_W = [K.random_binomial((nb_samples, self.input_dim), p=retain_p_W) for _ in range(3)]\n-            B_U = [K.random_binomial((nb_samples, self.output_dim), p=retain_p_U) for _ in range(3)]\n+    def get_constants(self, x, train=False):\n+        if train and (0 < self.dropout_U < 1):\n+            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n+            ones = K.concatenate([ones] * self.output_dim, 1)\n+            B_U = [K.dropout(ones, self.dropout_U) for _ in range(3)]\n+            return [B_U]\n         else:\n-            B_W = np.ones(3, dtype=K.floatx()) * retain_p_W\n-            B_U = np.ones(3, dtype=K.floatx()) * retain_p_U\n-        return [B_W, B_U]\n+            B_U = []\n \n     def get_config(self):\n         config = {\"output_dim\": self.output_dim,\n@@ -572,41 +617,59 @@ class LSTM(Recurrent):\n             self.states = [K.zeros((input_shape[0], self.output_dim)),\n                            K.zeros((input_shape[0], self.output_dim))]\n \n+    def preprocess_input(self, x, train=False):\n+        if train and (0 < self.dropout_W < 1):\n+            ones = K.ones_like(K.reshape(x[:, 0, :], (-1, self.input_dim)))\n+            B_W = [K.dropout(ones, self.dropout_W) for _ in range(4)]\n+        else:\n+            B_W = [None for _ in range(4)]\n+        input_shape = self.input_shape\n+        input_dim = input_shape[2]\n+        if input_shape[1]:\n+            timesteps = input_shape[1]\n+        else:\n+            # this won't work with TensorFlow\n+            timesteps = K.shape(x)[1]\n+\n+        x_i = time_distributed_dense(x, self.W_i, self.b_i,\n+                                     B_W[0], input_dim, self.output_dim, timesteps)\n+        x_f = time_distributed_dense(x, self.W_f, self.b_f,\n+                                     B_W[1], input_dim, self.output_dim, timesteps)\n+        x_c = time_distributed_dense(x, self.W_c, self.b_c,\n+                                     B_W[2], input_dim, self.output_dim, timesteps)\n+        x_o = time_distributed_dense(x, self.W_o, self.b_o,\n+                                     B_W[3], input_dim, self.output_dim, timesteps)\n+        return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n+\n     def step(self, x, states):\n-        assert len(states) == 4  # 2 states and 2 constants\n         h_tm1 = states[0]\n         c_tm1 = states[1]\n-        B_W = states[2]\n-        B_U = states[3]\n+        if len(states) == 3:\n+            B_U = states[2]\n+        else:\n+            B_U = [1. for _ in range(4)]\n \n-        x_i = K.dot(x * B_W[0], self.W_i) + self.b_i\n-        x_f = K.dot(x * B_W[1], self.W_f) + self.b_f\n-        x_c = K.dot(x * B_W[2], self.W_c) + self.b_c\n-        x_o = K.dot(x * B_W[3], self.W_o) + self.b_o\n+        x_i = x[:, :self.output_dim]\n+        x_f = x[:, self.output_dim: 2 * self.output_dim]\n+        x_c = x[:, 2 * self.output_dim: 3 * self.output_dim]\n+        x_o = x[:, 3 * self.output_dim:]\n \n         i = self.inner_activation(x_i + K.dot(h_tm1 * B_U[0], self.U_i))\n         f = self.inner_activation(x_f + K.dot(h_tm1 * B_U[1], self.U_f))\n         c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * B_U[2], self.U_c))\n         o = self.inner_activation(x_o + K.dot(h_tm1 * B_U[3], self.U_o))\n+\n         h = o * self.activation(c)\n         return h, [h, c]\n \n-    def get_constants(self, X, train=False):\n-        retain_p_W = 1. - self.dropout_W\n-        retain_p_U = 1. - self.dropout_U\n-        if train and (self.dropout_W > 0 or self.dropout_U > 0):\n-            nb_samples = K.shape(X)[0]\n-            if K._BACKEND == 'tensorflow':\n-                if not self.input_shape[0]:\n-                    raise Exception('For RNN dropout in tensorflow, a complete ' +\n-                                    'input_shape must be provided (including batch size).')\n-                nb_samples = self.input_shape[0]\n-            B_W = [K.random_binomial((nb_samples, self.input_dim), p=retain_p_W) for _ in range(4)]\n-            B_U = [K.random_binomial((nb_samples, self.output_dim), p=retain_p_U) for _ in range(4)]\n+    def get_constants(self, x, train=False):\n+        if train and (0 < self.dropout_U < 1):\n+            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n+            ones = K.concatenate([ones] * self.output_dim, 1)\n+            B_U = [K.dropout(ones, self.dropout_U) for _ in range(4)]\n+            return [B_U]\n         else:\n-            B_W = np.ones(4, dtype=K.floatx()) * retain_p_W\n-            B_U = np.ones(4, dtype=K.floatx()) * retain_p_U\n-        return [B_W, B_U]\n+            return []\n \n     def get_config(self):\n         config = {\"output_dim\": self.output_dim,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9f2aa1b6ae27192f35fd0fbbed992a5b6ea9b06b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -14/0 | Churn Δ: 1 | Churn Cumulative: 1301 | Contributors (this commit): 20 | Commits (past 90d): 33 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -488,6 +488,7 @@ class Graph(Layer):\n                 sh = SiameseHead(i)\n                 sh.previous = s\n                 sh_name = outputs[i]\n+                sh.name = sh_name\n                 self.namespace.add(sh_name)\n                 self.nodes[sh_name] = sh\n                 self.node_config.append({'name': sh_name,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f4af11c7300816ca28b6b707fdf7d64b00430074", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 210 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -52,7 +52,7 @@ def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncati\n         elif truncating == 'post':\n             trunc = s[:maxlen]\n         else:\n-            raise ValueError(\"Truncating type '%s' not understood\" % padding)\n+            raise ValueError(\"Truncating type '%s' not understood\" % truncating)\n \n         # check `trunc` has expected shape\n         trunc = np.asarray(trunc, dtype=dtype)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#98e2789db94aefdea5151be570948c0f2c2fe865", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 42 | Files Changed: 1 | Hunks: 22 | Methods Changed: 3 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 74 | Churn Cumulative: 3506 | Contributors (this commit): 20 | Commits (past 90d): 35 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,6 +9,8 @@ from ..layers.core import MaskedLayer\n \n def time_distributed_dense(x, w, b=None, dropout=None,\n                            input_dim=None, output_dim=None, timesteps=None):\n+    '''Apply y.w + b for every temporal slice y of x.\n+    '''\n     if not input_dim:\n         # won't work with TensorFlow\n         input_dim = K.shape(x)[2]\n@@ -16,13 +18,19 @@ def time_distributed_dense(x, w, b=None, dropout=None,\n         # won't work with TensorFlow\n         timesteps = K.shape(x)[1]\n     if not output_dim:\n+        # won't work with TensorFlow\n         output_dim = K.shape(w)[1]\n+    if dropout:\n+        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n+        dropout_matrix = K.dropout(ones, dropout)\n+    # collapse time dimension and batch dimension together\n     x = K.reshape(x, (-1, input_dim))\n     if dropout:\n-        x *= K.concatenate([dropout] * timesteps, 0)\n+        x *= K.concatenate([dropout_matrix] * timesteps, 0)\n     x = K.dot(x, w)\n     if b:\n         x = x + b\n+    # reshape to 3D tensor\n     x = K.reshape(x, (-1, timesteps, output_dim))\n     return x\n \n@@ -289,18 +297,13 @@ class SimpleRNN(Recurrent):\n \n     def preprocess_input(self, x, train=False):\n         if train and (0 < self.dropout_W < 1):\n-            ones = K.ones_like(K.reshape(x[:, 0, :], (-1, self.input_dim)))\n-            B_W = K.dropout(ones, self.dropout_W)\n+            dropout = self.dropout_W\n         else:\n-            B_W = None\n+            dropout = 0\n         input_shape = self.input_shape\n         input_dim = input_shape[2]\n-        if input_shape[1]:\n         timesteps = input_shape[1]\n-        else:\n-            # this won't work with TensorFlow\n-            timesteps = K.shape(x)[1]\n-        return time_distributed_dense(x, self.W, self.b, B_W,\n+        return time_distributed_dense(x, self.W, self.b, dropout,\n                                       input_dim, self.output_dim, timesteps)\n \n     def step(self, h, states):\n@@ -318,8 +321,7 @@ class SimpleRNN(Recurrent):\n             ones = K.concatenate([ones] * self.output_dim, 1)\n             B_U = K.dropout(ones, self.dropout_U)\n             return [B_U]\n-        else:\n-            B_U = []\n+        return []\n \n     def get_config(self):\n         config = {\"output_dim\": self.output_dim,\n@@ -437,24 +439,19 @@ class GRU(Recurrent):\n \n     def preprocess_input(self, x, train=False):\n         if train and (0 < self.dropout_W < 1):\n-            ones = K.ones_like(K.reshape(x[:, 0, :], (-1, self.input_dim)))\n-            B_W = [K.dropout(ones, self.dropout_W) for _ in range(3)]\n+            dropout = self.dropout_W\n         else:\n-            B_W = [None for _ in range(3)]\n+            dropout = 0\n         input_shape = self.input_shape\n         input_dim = input_shape[2]\n-        if input_shape[1]:\n         timesteps = input_shape[1]\n-        else:\n-            # this won't work with TensorFlow\n-            timesteps = K.shape(x)[1]\n \n-        x_z = time_distributed_dense(x, self.W_z, self.b_z,\n-                                     B_W[0], input_dim, self.output_dim, timesteps)\n-        x_r = time_distributed_dense(x, self.W_r, self.b_r,\n-                                     B_W[1], input_dim, self.output_dim, timesteps)\n-        x_h = time_distributed_dense(x, self.W_h, self.b_h,\n-                                     B_W[2], input_dim, self.output_dim, timesteps)\n+        x_z = time_distributed_dense(x, self.W_z, self.b_z, dropout,\n+                                     input_dim, self.output_dim, timesteps)\n+        x_r = time_distributed_dense(x, self.W_r, self.b_r, dropout,\n+                                     input_dim, self.output_dim, timesteps)\n+        x_h = time_distributed_dense(x, self.W_h, self.b_h, dropout,\n+                                     input_dim, self.output_dim, timesteps)\n         return K.concatenate([x_z, x_r, x_h], axis=2)\n \n     def step(self, x, states):\n@@ -481,8 +478,7 @@ class GRU(Recurrent):\n             ones = K.concatenate([ones] * self.output_dim, 1)\n             B_U = [K.dropout(ones, self.dropout_U) for _ in range(3)]\n             return [B_U]\n-        else:\n-            B_U = []\n+        return []\n \n     def get_config(self):\n         config = {\"output_dim\": self.output_dim,\n@@ -619,26 +615,21 @@ class LSTM(Recurrent):\n \n     def preprocess_input(self, x, train=False):\n         if train and (0 < self.dropout_W < 1):\n-            ones = K.ones_like(K.reshape(x[:, 0, :], (-1, self.input_dim)))\n-            B_W = [K.dropout(ones, self.dropout_W) for _ in range(4)]\n+            dropout = self.dropout_W\n         else:\n-            B_W = [None for _ in range(4)]\n+            dropout = 0\n         input_shape = self.input_shape\n         input_dim = input_shape[2]\n-        if input_shape[1]:\n         timesteps = input_shape[1]\n-        else:\n-            # this won't work with TensorFlow\n-            timesteps = K.shape(x)[1]\n \n-        x_i = time_distributed_dense(x, self.W_i, self.b_i,\n-                                     B_W[0], input_dim, self.output_dim, timesteps)\n-        x_f = time_distributed_dense(x, self.W_f, self.b_f,\n-                                     B_W[1], input_dim, self.output_dim, timesteps)\n-        x_c = time_distributed_dense(x, self.W_c, self.b_c,\n-                                     B_W[2], input_dim, self.output_dim, timesteps)\n-        x_o = time_distributed_dense(x, self.W_o, self.b_o,\n-                                     B_W[3], input_dim, self.output_dim, timesteps)\n+        x_i = time_distributed_dense(x, self.W_i, self.b_i, dropout,\n+                                     input_dim, self.output_dim, timesteps)\n+        x_f = time_distributed_dense(x, self.W_f, self.b_f, dropout,\n+                                     input_dim, self.output_dim, timesteps)\n+        x_c = time_distributed_dense(x, self.W_c, self.b_c, dropout,\n+                                     input_dim, self.output_dim, timesteps)\n+        x_o = time_distributed_dense(x, self.W_o, self.b_o, dropout,\n+                                     input_dim, self.output_dim, timesteps)\n         return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n \n     def step(self, x, states):\n@@ -668,7 +659,6 @@ class LSTM(Recurrent):\n             ones = K.concatenate([ones] * self.output_dim, 1)\n             B_U = [K.dropout(ones, self.dropout_U) for _ in range(4)]\n             return [B_U]\n-        else:\n         return []\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#73e563ecaf915d073d8b8fabc4a568ecddb0ea11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -11/0 | Churn Δ: 31 | Churn Cumulative: 11472 | Contributors (this commit): 53 | Commits (past 90d): 115 | Contributors (cumulative): 68 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,7 +7,7 @@ def softmax(x):\n     if ndim == 2:\n         return K.softmax(x)\n     elif ndim == 3:\n-        e = K.exp(x)\n+        e = K.exp(x - K.max(x, axis=-1, keepdims=True))\n         s = K.sum(e, axis=-1, keepdims=True)\n         return e / s\n     else:\n\n@@ -1120,18 +1120,16 @@ class TimeDistributedDense(MaskedLayer):\n         return (input_shape[0], input_shape[1], self.output_dim)\n \n     def get_output(self, train=False):\n-        X = self.get_input(train)\n-        if K._BACKEND == 'theano':\n-            import theano.tensor as T\n-            z = T.tensordot(X, self.W, axes=[(2), (0)])\n-            Y = z + self.b\n-            Y = self.activation(Y)\n-            return Y\n-        elif K._BACKEND == 'tensorflow':\n-            shape = self.input_shape\n-            X = K.reshape(X, (-1, shape[-1]))\n-            Y = K.dot(X, self.W) + self.b\n-            Y = K.reshape(Y, (-1, shape[1], shape[2]))\n+        X = self.get_input(train)  # (samples, timesteps, input_dim)\n+        # Squash samples and timesteps into a single axis\n+        x = K.reshape(X, (-1, self.input_shape[-1]))  # (samples * timesteps, input_dim)\n+        Y = K.dot(x, self.W) + self.b  # (samples * timesteps, output_dim)\n+        # We have to reshape Y to (samples, timesteps, output_dim)\n+        input_length = self.input_shape[1]\n+        # Note: input_length will always be provided when using tensorflow backend.\n+        if not input_length:\n+            input_length = K.shape(X)[1]\n+        Y = K.reshape(Y, (-1, input_length, self.output_shape[-1]))  # (samples, timesteps, output_dim)\n         Y = self.activation(Y)\n         return Y\n \n\n@@ -121,10 +121,9 @@ class Recurrent(MaskedLayer):\n \n     def get_initial_states(self, X):\n         # build an all-zero tensor of shape (samples, output_dim)\n-        initial_state = K.zeros_like(X)  # (samples, timesteps, input_dim)\n-        initial_state = K.sum(initial_state, axis=1)  # (samples, input_dim)\n-        reducer = K.zeros((self.input_dim, self.output_dim))\n-        initial_state = K.dot(initial_state, reducer)  # (samples, output_dim)\n+        initial_state = X[:, 0, 0] * 0  # (samples, )\n+        initial_state = K.pack([initial_state] * self.output_dim)  # (output_dim, samples)\n+        initial_state = K.permute_dimensions(initial_state, (1, 0))  # (samples, output_dim)\n         initial_states = [initial_state for _ in range(len(self.states))]\n         return initial_states\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c6c150b0427c19f17e45471e450ddf87ac046419", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 2 | Churn Cumulative: 7858 | Contributors (this commit): 42 | Commits (past 90d): 70 | Contributors (cumulative): 42 | DMM Complexity: None\n\nDIFF:\n@@ -1743,7 +1743,7 @@ class Siamese(Layer):\n     def get_output_at(self, head, train=False):\n         X = self.inputs[head].get_output(train)\n         mask = self.inputs[head].get_output_mask(train)\n-        Y = self.layer(X, mask)\n+        Y = self.layer(X, mask=mask, train=train)\n         return Y\n \n     def get_output_shape(self, head, train=False):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#22c091ae3ff1048c0a6d28edca7c57bb93378850", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 287 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -7,8 +7,8 @@ http://arxiv.org/abs/1502.05698\n \n Task Number                  | FB LSTM Baseline | Keras QA\n ---                          | ---              | ---\n-QA1 - Single Supporting Fact | 50               | 52.1\n-QA2 - Two Supporting Facts   | 20               | 37.0\n+QA1 - Single Supporting Fact | 50               | 100.0\n+QA2 - Two Supporting Facts   | 20               | 50.0\n QA3 - Three Supporting Facts | 20               | 20.5\n QA4 - Two Arg. Relations     | 61               | 62.9\n QA5 - Three Arg. Relations   | 70               | 61.9\n@@ -34,8 +34,8 @@ https://research.facebook.com/researchers/1543934539189348\n Notes:\n \n - With default word, sentence, and query vector sizes, the GRU model achieves:\n-  - 52.1% test accuracy on QA1 in 20 epochs (2 seconds per epoch on CPU)\n-  - 37.0% test accuracy on QA2 in 20 epochs (16 seconds per epoch on CPU)\n+  - 100% test accuracy on QA1 in 20 epochs (2 seconds per epoch on CPU)\n+  - 50% test accuracy on QA2 in 20 epochs (16 seconds per epoch on CPU)\n In comparison, the Facebook paper achieves 50% and 20% for the LSTM baseline.\n \n - The task does not traditionally parse the question separately. This likely\n@@ -138,12 +138,12 @@ def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n         Y.append(y)\n     return pad_sequences(X, maxlen=story_maxlen), pad_sequences(Xq, maxlen=query_maxlen), np.array(Y)\n \n-RNN = recurrent.GRU\n+RNN = recurrent.LSTM\n EMBED_HIDDEN_SIZE = 50\n SENT_HIDDEN_SIZE = 100\n QUERY_HIDDEN_SIZE = 100\n BATCH_SIZE = 32\n-EPOCHS = 20\n+EPOCHS = 40\n print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE, QUERY_HIDDEN_SIZE))\n \n path = get_file('babi-tasks-v1-2.tar.gz', origin='http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz')\n@@ -178,15 +178,19 @@ print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n print('Build model...')\n \n sentrnn = Sequential()\n-sentrnn.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE, mask_zero=True))\n-sentrnn.add(RNN(SENT_HIDDEN_SIZE, return_sequences=False))\n+sentrnn.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE, input_length=story_maxlen, mask_zero=True))\n+sentrnn.add(Dropout(0.3))\n \n qrnn = Sequential()\n-qrnn.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE))\n-qrnn.add(RNN(QUERY_HIDDEN_SIZE, return_sequences=False))\n+qrnn.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE, input_length=query_maxlen))\n+qrnn.add(Dropout(0.3))\n+qrnn.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n+qrnn.add(RepeatVector(story_maxlen))\n \n model = Sequential()\n-model.add(Merge([sentrnn, qrnn], mode='concat'))\n+model.add(Merge([sentrnn, qrnn], mode='sum'))\n+model.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n+model.add(Dropout(0.3))\n model.add(Dense(vocab_size, activation='softmax'))\n \n model.compile(optimizer='adam', loss='categorical_crossentropy', class_mode='categorical')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#045e47174f46154229f6217f7520398f92223829", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 111 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 5 | Methods Changed: 10 | Complexity Δ (Sum/Max): 11/8 | Churn Δ: 111 | Churn Cumulative: 505 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 13 | DMM Complexity: 0.9253731343283582\n\nDIFF:\n@@ -0,0 +1,54 @@\n+from .core import MaskedLayer\n+from .. import backend as K\n+\n+\n+class TimeDistributed(MaskedLayer):\n+\n+    def __init__(self, layer, **kwargs):\n+        self.layer = layer\n+        super(TimeDistributed, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_shape = self.input_shape\n+        assert len(input_shape) >= 3\n+        child_input_shape = (input_shape[0],) + input_shape[2:]\n+        self.layer.set_input_shape(child_input_shape)\n+        self.layer.build()\n+\n+        trainable_weights, regularizers, constraints, updates = self.layer.get_params()\n+        self.trainable_weights = trainable_weights\n+        self.regularizers = regularizers\n+        self.constraints = constraints\n+        self.updates = updates\n+\n+    @property\n+    def output_shape(self):\n+        child_output_shape = self.layer.output_shape\n+        timesteps = self.input_shape[1]\n+        return (child_output_shape[0], timesteps) + child_output_shape[1:]\n+\n+    def get_output(self, train=False):\n+        X = self.get_input(train)\n+        mask = self.get_input_mask(train)\n+\n+        def step(x, states):\n+            output = self.layer(x, train=train)\n+            return output, []\n+\n+        last_output, outputs, states = K.rnn(step, X,\n+                                             initial_states=[],\n+                                             mask=mask)\n+        return outputs\n+\n+    def get_weights(self):\n+        weights = self.layer.get_weights()\n+        return weights\n+\n+    def set_weights(self, weights):\n+        self.layer.set_weights(weights)\n+\n+    def get_config(self):\n+        config = {'name': self.__class__.__name__,\n+                  'layer': self.layer.get_config()}\n+        base_config = super(TimeDistributed, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -10,6 +10,7 @@ from ..layers.embeddings import *\n from ..layers.noise import *\n from ..layers.normalization import *\n from ..layers.recurrent import *\n+from ..layers.wrappers import *\n from ..layers import containers\n from .. import regularizers\n from .. import constraints\n@@ -71,6 +72,13 @@ def container_from_config(original_layer_dict, custom_objects={}):\n                 kwargs[kwarg] = layer_dict[kwarg]\n         return AutoEncoder(**kwargs)\n \n+    elif name == 'TimeDistributed':\n+        child_layer = container_from_config(layer_dict.pop('layer'))\n+        # the \"name\" keyword argument of layers is saved as \"custom_name\"\n+        if 'custom_name' in layer_dict:\n+            layer_dict['name'] = layer_dict.pop('custom_name')\n+        return TimeDistributed(child_layer, **layer_dict)\n+\n     else:  # this is a non-topological layer (e.g. Dense, etc.)\n         layer_dict.pop('name')\n \n@@ -89,6 +97,7 @@ def container_from_config(original_layer_dict, custom_objects={}):\n         # the \"name\" keyword argument of layers is saved as \"custom_name\"\n         if 'custom_name' in layer_dict:\n             layer_dict['name'] = layer_dict.pop('custom_name')\n+\n         base_layer = get_layer(name, layer_dict)\n         return base_layer\n \n\n@@ -0,0 +1,48 @@\n+import pytest\n+import numpy as np\n+from numpy.testing import assert_allclose\n+\n+from keras.layers import wrappers\n+from keras.layers import core, convolutional\n+from keras.models import Sequential, model_from_json\n+\n+\n+def test_TimeDistributed():\n+    # first, test with Dense layer\n+    model = Sequential()\n+    model.add(wrappers.TimeDistributed(core.Dense(2), input_shape=(3, 4)))\n+    model.add(core.Activation('relu'))\n+\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 2)), nb_epoch=1, batch_size=10)\n+\n+    model.get_config()\n+\n+    # compare to TimeDistributedDense\n+    test_input = np.random.random((1, 3, 4))\n+    test_output = model.predict(test_input)\n+    weights = model.layers[0].get_weights()\n+\n+    reference = Sequential()\n+    reference.add(core.TimeDistributedDense(2, input_shape=(3, 4), weights=weights))\n+    reference.add(core.Activation('relu'))\n+\n+    reference.compile(optimizer='rmsprop', loss='mse')\n+\n+    reference_output = reference.predict(test_input)\n+    assert_allclose(test_output, reference_output, atol=1e-05)\n+\n+    # test with Convolution2D\n+    model = Sequential()\n+    model.add(wrappers.TimeDistributed(convolutional.Convolution2D(5, 2, 2, border_mode='same'), input_shape=(2, 3, 4, 4)))\n+    model.add(core.Activation('relu'))\n+\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.train_on_batch(np.random.random((1, 2, 3, 4, 4)), np.random.random((1, 2, 5, 4, 4)))\n+\n+    model = model_from_json(model.to_json())\n+    model.summary()\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#47c67ac19a7d0358ca2ebec433dc0b067e659101", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 1287 | Contributors (this commit): 14 | Commits (past 90d): 42 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -147,7 +147,10 @@ def prod(x, axis=None, keepdims=False):\n \n \n def mean(x, axis=None, keepdims=False):\n-    return T.mean(x, axis=axis, keepdims=keepdims)\n+    dtype = None\n+    if 'int' in x.dtype:\n+        dtype = _FLOATX\n+    return T.mean(x, axis=axis, keepdims=keepdims, dtype=dtype)\n \n \n def std(x, axis=None, keepdims=False):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0eea5f8867bb2ef68c116e7c827436f59ef11a2e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 289 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -66,7 +66,7 @@ np.random.seed(1337)  # for reproducibility\n \n from keras.datasets.data_utils import get_file\n from keras.layers.embeddings import Embedding\n-from keras.layers.core import Dense, Merge\n+from keras.layers.core import Dense, Merge, Dropout, RepeatVector\n from keras.layers import recurrent\n from keras.models import Sequential\n from keras.preprocessing.sequence import pad_sequences\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ff2f8ac69b8bcf013a42add480b662daa4cde656", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 89 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -68,7 +68,8 @@ for i in range(epochs):\n               expected_output,\n               batch_size=batch_size,\n               verbose=1,\n-              nb_epoch=1)\n+              nb_epoch=1,\n+              shuffle=False)\n     model.reset_states()\n \n print('Predicting')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#567fdccd0b5938276b517a16ddd148ad8fdc295f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 4 | Churn Cumulative: 7862 | Contributors (this commit): 42 | Commits (past 90d): 71 | Contributors (cumulative): 42 | DMM Complexity: 1.0\n\nDIFF:\n@@ -517,6 +517,10 @@ class Merge(Layer):\n                     self.constraints.append(c)\n         super(Merge, self).__init__()\n \n+    @property\n+    def input_shape(self):\n+        return [layer.input_shape for layer in self.layers]\n+\n     @property\n     def output_shape(self):\n         input_shapes = [layer.output_shape for layer in self.layers]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9b4f973d5779f79369bd3ae0fb982c7da49bd4f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 6 | Churn Cumulative: 3519 | Contributors (this commit): 20 | Commits (past 90d): 37 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -581,11 +581,11 @@ class LSTM(Recurrent):\n                 regularizers_list.append(regulariser)\n \n         self.regularizers = []\n-        for W in [self.W_i, self.W_f, self.W_i, self.W_o]:\n+        for W in [self.W_i, self.W_f, self.W_c, self.W_o]:\n             append_regulariser(self.W_regularizer, W, self.regularizers)\n-        for U in [self.U_i, self.U_f, self.U_i, self.U_o]:\n+        for U in [self.U_i, self.U_f, self.U_c, self.U_o]:\n             append_regulariser(self.U_regularizer, U, self.regularizers)\n-        for b in [self.b_i, self.b_f, self.b_i, self.b_o]:\n+        for b in [self.b_i, self.b_f, self.b_c, self.b_o]:\n             append_regulariser(self.b_regularizer, b, self.regularizers)\n \n         self.trainable_weights = [self.W_i, self.U_i, self.b_i,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#379966050454151db016f427dd7dae24a35e45e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 1301 | Contributors (this commit): 14 | Commits (past 90d): 42 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n import theano\n from theano import tensor as T\n from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n-from theano.tensor.signal import downsample\n+from theano.tensor.signal import pool\n from theano.tensor.nnet import conv3d2d\n import numpy as np\n from .common import _FLOATX, _EPSILON\n@@ -788,12 +788,12 @@ def pool2d(x, pool_size, strides=(1, 1), border_mode='valid',\n         x = x.dimshuffle((0, 3, 1, 2))\n \n     if pool_mode == 'max':\n-        pool_out = downsample.max_pool_2d(x, ds=pool_size, st=strides,\n+        pool_out = pool.max_pool_2d(x, ds=pool_size, st=strides,\n                                     ignore_border=True,\n                                     padding=padding,\n                                     mode='max')\n     elif pool_mode == 'avg':\n-        pool_out = downsample.max_pool_2d(x, ds=pool_size, st=strides,\n+        pool_out = pool.max_pool_2d(x, ds=pool_size, st=strides,\n                                     ignore_border=True,\n                                     padding=padding,\n                                     mode='average_exc_pad')\n@@ -832,7 +832,7 @@ def pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n \n     if pool_mode == 'max':\n         # pooling over conv_dim2, conv_dim1 (last two channels)\n-        output = downsample.max_pool_2d(input=x.dimshuffle(0, 1, 4, 3, 2),\n+        output = pool.max_pool_2d(input=x.dimshuffle(0, 1, 4, 3, 2),\n                                   ds=(pool_size[1], pool_size[0]),\n                                   st=(strides[1], strides[0]),\n                                   ignore_border=ignore_border,\n@@ -840,7 +840,7 @@ def pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n                                  mode='max')\n \n         # pooling over conv_dim3\n-        pool_out = downsample.max_pool_2d(input=output.dimshuffle(0, 1, 4, 3, 2),\n+        pool_out = pool.max_pool_2d(input=output.dimshuffle(0, 1, 4, 3, 2),\n                                     ds=(1, pool_size[2]),\n                                     st=(1, strides[2]),\n                                     ignore_border=ignore_border,\n@@ -849,7 +849,7 @@ def pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n \n     elif pool_mode == 'avg':\n         # pooling over conv_dim2, conv_dim1 (last two channels)\n-        output = downsample.max_pool_2d(input=x.dimshuffle(0, 1, 4, 3, 2),\n+        output = pool.max_pool_2d(input=x.dimshuffle(0, 1, 4, 3, 2),\n                                   ds=(pool_size[1], pool_size[0]),\n                                   st=(strides[1], strides[0]),\n                                   ignore_border=ignore_border,\n@@ -857,7 +857,7 @@ def pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n                                   mode='average_exc_pad')\n \n         # pooling over conv_dim3\n-        pool_out = downsample.max_pool_2d(input=output.dimshuffle(0, 1, 4, 3, 2),\n+        pool_out = pool.max_pool_2d(input=output.dimshuffle(0, 1, 4, 3, 2),\n                                     ds=(1, pool_size[2]),\n                                     st=(1, strides[2]),\n                                     ignore_border=ignore_border,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
