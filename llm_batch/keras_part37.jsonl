{"custom_id": "keras#e8e56d9013da401fc6260a6f7675c70ce03fba28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 28 | Files Changed: 4 | Hunks: 22 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 60 | Churn Cumulative: 1336 | Contributors (this commit): 11 | Commits (past 90d): 37 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -97,7 +97,7 @@ print(\"Building model...\")\n model = Sequential()\n model.add(Dense(512, input_shape=(dims,)))\n model.add(PReLU())\n-model.add(BatchNormalization((512,)))\n+model.add(BatchNormalization())\n model.add(Dropout(0.5))\n \n model.add(Dense(512))\n\n@@ -166,12 +166,14 @@ class Graph(Layer):\n         else:\n             return dict([(k, v.get_output(train)) for k, v in self.outputs.items()])\n \n-    def add_input(self, name, ndim=2, dtype='float'):\n+    def add_input(self, name, input_shape, dtype='float'):\n         if name in self.namespace:\n             raise Exception('Duplicate node identifier: ' + name)\n         self.namespace.add(name)\n         self.input_order.append(name)\n         layer = Layer()  # empty layer\n+        layer.set_input_shape(input_shape)\n+        ndim = len(input_shape) + 1\n         if dtype == 'float':\n             layer.input = ndim_tensor(ndim)\n         else:\n@@ -181,7 +183,9 @@ class Graph(Layer):\n                 raise Exception('Type \"int\" can only be used with ndim==2 (Embedding).')\n         layer.input.name = name\n         self.inputs[name] = layer\n-        self.input_config.append({'name': name, 'ndim': ndim, 'dtype': dtype})\n+        self.input_config.append({'name': name,\n+                                  'input_shape': input_shape,\n+                                  'dtype': dtype})\n \n     def add_node(self, layer, name, input=None, inputs=[],\n                  merge_mode='concat', concat_axis=-1, create_output=False):\n\n@@ -23,9 +23,9 @@ class TestGraph(unittest.TestCase):\n     def test_1o_1i(self):\n         print('test a non-sequential graph with 1 input and 1 output')\n         graph = Graph()\n-        graph.add_input(name='input1', ndim=2)\n+        graph.add_input(name='input1', input_shape=(32,))\n \n-        graph.add_node(Dense(16, input_shape=(32,)), name='dense1', input='input1')\n+        graph.add_node(Dense(16), name='dense1', input='input1')\n         graph.add_node(Dense(4), name='dense2', input='input1')\n         graph.add_node(Dense(4), name='dense3', input='dense1')\n \n@@ -45,9 +45,9 @@ class TestGraph(unittest.TestCase):\n     def test_1o_1i_2(self):\n         print('test a more complex non-sequential graph with 1 input and 1 output')\n         graph = Graph()\n-        graph.add_input(name='input1', ndim=2)\n+        graph.add_input(name='input1', input_shape=(32,))\n \n-        graph.add_node(Dense(16, input_shape=(32,)), name='dense1', input='input1')\n+        graph.add_node(Dense(16), name='dense1', input='input1')\n         graph.add_node(Dense(4), name='dense2-0', input='input1')\n         graph.add_node(Activation('relu'), name='dense2', input='dense2-0')\n \n@@ -71,10 +71,10 @@ class TestGraph(unittest.TestCase):\n     def test_1o_2i(self):\n         print('test a non-sequential graph with 2 inputs and 1 output')\n         graph = Graph()\n-        graph.add_input(name='input1', ndim=2)\n-        graph.add_input(name='input2', ndim=2)\n+        graph.add_input(name='input1', input_shape=(32,))\n+        graph.add_input(name='input2', input_shape=(32,))\n \n-        graph.add_node(Dense(16, input_shape=(32,)), name='dense1', input='input1')\n+        graph.add_node(Dense(16), name='dense1', input='input1')\n         graph.add_node(Dense(4), name='dense2', input='input2')\n         graph.add_node(Dense(4), name='dense3', input='dense1')\n \n@@ -95,9 +95,9 @@ class TestGraph(unittest.TestCase):\n     def test_2o_1i_weights(self):\n         print('test a non-sequential graph with 1 input and 2 outputs')\n         graph = Graph()\n-        graph.add_input(name='input1', ndim=2)\n+        graph.add_input(name='input1', input_shape=(32,))\n \n-        graph.add_node(Dense(16, input_shape=(32,)), name='dense1', input='input1')\n+        graph.add_node(Dense(16), name='dense1', input='input1')\n         graph.add_node(Dense(4), name='dense2', input='input1')\n         graph.add_node(Dense(1), name='dense3', input='dense1')\n \n@@ -118,8 +118,8 @@ class TestGraph(unittest.TestCase):\n         print('test weight saving')\n         graph.save_weights('temp.h5', overwrite=True)\n         graph = Graph()\n-        graph.add_input(name='input1', ndim=2)\n-        graph.add_node(Dense(16, input_shape=(32,)), name='dense1', input='input1')\n+        graph.add_input(name='input1', input_shape=(32,))\n+        graph.add_node(Dense(16), name='dense1', input='input1')\n         graph.add_node(Dense(4), name='dense2', input='input1')\n         graph.add_node(Dense(1), name='dense3', input='dense1')\n         graph.add_output(name='output1', input='dense2')\n@@ -133,9 +133,9 @@ class TestGraph(unittest.TestCase):\n     def test_2o_1i_sample_weights(self):\n         print('test a non-sequential graph with 1 input and 2 outputs with sample weights')\n         graph = Graph()\n-        graph.add_input(name='input1', ndim=2)\n+        graph.add_input(name='input1', input_shape=(32,))\n \n-        graph.add_node(Dense(16, input_shape=(32,)), name='dense1', input='input1')\n+        graph.add_node(Dense(16), name='dense1', input='input1')\n         graph.add_node(Dense(4), name='dense2', input='input1')\n         graph.add_node(Dense(1), name='dense3', input='dense1')\n \n@@ -166,8 +166,8 @@ class TestGraph(unittest.TestCase):\n         print('test layer-like API')\n \n         graph = containers.Graph()\n-        graph.add_input(name='input1', ndim=2)\n-        graph.add_node(Dense(16, input_shape=(32,)), name='dense1', input='input1')\n+        graph.add_input(name='input1', input_shape=(32,))\n+        graph.add_node(Dense(16), name='dense1', input='input1')\n         graph.add_node(Dense(4), name='dense2', input='input1')\n         graph.add_node(Dense(4), name='dense3', input='dense1')\n         graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n@@ -191,9 +191,9 @@ class TestGraph(unittest.TestCase):\n     def test_create_output(self):\n         print('test create_output argument')\n         graph = Graph()\n-        graph.add_input(name='input1', ndim=2)\n+        graph.add_input(name='input1', input_shape=(32,))\n \n-        graph.add_node(Dense(16, input_shape=(32,)), name='dense1', input='input1')\n+        graph.add_node(Dense(16), name='dense1', input='input1')\n         graph.add_node(Dense(4), name='dense2', input='input1')\n         graph.add_node(Dense(4), name='dense3', input='dense1')\n         graph.add_node(Dense(4), name='output1', inputs=['dense2', 'dense3'], merge_mode='sum', create_output=True)\n@@ -216,19 +216,19 @@ class TestGraph(unittest.TestCase):\n         nb_classes = 2\n \n         graph = Graph()\n-        graph.add_input(name='input1', ndim=2)\n-        graph.add_input(name='input2', ndim=2)\n-        graph.add_node(Dense(nb_units, input_shape=(nb_units,)),\n+        graph.add_input(name='input1', input_shape=(32,))\n+        graph.add_input(name='input2', input_shape=(32,))\n+        graph.add_node(Dense(nb_units),\n                        name='dense1', input='input1')\n-        graph.add_node(Dense(nb_classes, input_shape=(nb_units,)),\n+        graph.add_node(Dense(nb_classes),\n                        name='dense2', input='input2')\n         graph.add_node(Dense(nb_classes),\n                        name='dense3', input='dense1')\n         graph.add_output(name='output', inputs=['dense2', 'dense3'],\n                          merge_mode='sum')\n \n-        n = nb_units * nb_units + nb_units\n-        n += nb_units * nb_classes + nb_classes\n+        n = 32 * nb_units + nb_units\n+        n += 32 * nb_classes + nb_classes\n         n += nb_units * nb_classes + nb_classes\n \n         self.assertEqual(n, graph.count_params())\n\n@@ -51,8 +51,8 @@ def create_sequential_model():\n \n def create_graph_model():\n     model = Graph()\n-    model.add_input(name='input')\n-    model.add_node(Dense(50, activation='relu', input_shape=(784,)), name='d1', input='input')\n+    model.add_input(name='input', input_shape=(784,))\n+    model.add_node(Dense(50, activation='relu'), name='d1', input='input')\n     model.add_node(Dense(10, activation='softmax'), name='d2', input='d1')\n     model.add_output(name='output', input='d2')\n     return model\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cb77f7d7e2933078838e836018fdfa7075e5385f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1228 | Contributors (this commit): 12 | Commits (past 90d): 41 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -47,7 +47,7 @@ model = Sequential()\n \n # we start off with an efficient embedding layer which maps\n # our vocab indices into embedding_dims dimensions\n-model.add(Embedding(max_features, embedding_dims, max_length=maxlen))\n+model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n model.add(Dropout(0.25))\n \n # we add a Convolution1D, which will learn nb_filter\n\n@@ -136,9 +136,13 @@ class Convolution1D(Layer):\n                 assert(self.subsample_length == 1)\n                 border_mode = 'full'\n \n+            input_shape = self.input_shape\n+            image_shape = (input_shape[0], input_shape[2], input_shape[1], 1)\n             conv_out = T.nnet.conv.conv2d(X, self.W,\n                                           border_mode=border_mode,\n-                                          subsample=self.subsample)\n+                                          subsample=self.subsample,\n+                                          image_shape=image_shape,\n+                                          filter_shape=self.W_shape)\n             if self.border_mode == 'same':\n                 shift_x = (self.filter_length - 1) // 2\n                 conv_out = conv_out[:, :, shift_x:X.shape[2] + shift_x, :]\n@@ -245,7 +249,9 @@ class Convolution2D(Layer):\n                 conv_out = dnn.dnn_conv(img=X,\n                                         kerns=self.W,\n                                         border_mode=border_mode,\n-                                        subsample=self.subsample)\n+                                        subsample=self.subsample,\n+                                        image_shape=self.input_shape,\n+                                        filter_shape=self.W_shape)\n         else:\n             if border_mode == 'same':\n                 border_mode = 'full'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cfd2763514fc4d370e44b895d26726bc2f586134", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 218 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -56,7 +56,7 @@ class TestTasks(unittest.TestCase):\n \n     def test_temporal_clf(self):\n         print('temporal classification data:')\n-        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(5, 10),\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(3, 5),\n                                                              classification=True, nb_class=2)\n         print('X_train:', X_train.shape)\n         print('X_test:', X_test.shape)\n@@ -75,7 +75,7 @@ class TestTasks(unittest.TestCase):\n \n     def test_temporal_reg(self):\n         print('temporal regression data:')\n-        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(5, 10), output_shape=(2,),\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(3, 5), output_shape=(2,),\n                                                              classification=False)\n         print('X_train:', X_train.shape)\n         print('X_test:', X_test.shape)\n@@ -90,7 +90,7 @@ class TestTasks(unittest.TestCase):\n \n     def test_seq_to_seq(self):\n         print('sequence to sequence data:')\n-        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(5, 10), output_shape=(5, 10),\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(3, 5), output_shape=(3, 5),\n                                                              classification=False)\n         print('X_train:', X_train.shape)\n         print('X_test:', X_test.shape)\n@@ -105,7 +105,7 @@ class TestTasks(unittest.TestCase):\n \n     def test_img_clf(self):\n         print('image classification data:')\n-        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(3, 32, 32),\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(3, 8, 8),\n                                                              classification=True, nb_class=2)\n         print('X_train:', X_train.shape)\n         print('X_test:', X_test.shape)\n@@ -116,7 +116,7 @@ class TestTasks(unittest.TestCase):\n         y_test = to_categorical(y_test)\n \n         model = Sequential()\n-        model.add(Convolution2D(32, 32, 32, input_shape=(3, 32, 32)))\n+        model.add(Convolution2D(8, 8, 8, input_shape=(3, 8, 8)))\n         model.add(Activation('sigmoid'))\n         model.add(Flatten())\n         model.add(Dense(y_test.shape[-1]))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e839a4bdac9d664f6eb9acdb4b924eba6d428842", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 38 | Files Changed: 7 | Hunks: 38 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 76 | Churn Cumulative: 5615 | Contributors (this commit): 36 | Commits (past 90d): 121 | Contributors (cumulative): 64 | DMM Complexity: None\n\nDIFF:\n@@ -18,7 +18,7 @@ class LeakyReLU(MaskedLayer):\n         config = {\"name\": self.__class__.__name__,\n                   \"alpha\": self.alpha}\n         base_config = super(LeakyReLU, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class PReLU(MaskedLayer):\n@@ -51,7 +51,7 @@ class PReLU(MaskedLayer):\n         config = {\"name\": self.__class__.__name__,\n                   \"init\": self.init.__name__}\n         base_config = super(PReLU, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class ParametricSoftplus(MaskedLayer):\n@@ -89,7 +89,7 @@ class ParametricSoftplus(MaskedLayer):\n                   \"alpha_init\": self.alpha_init,\n                   \"beta_init\": self.beta_init}\n         base_config = super(ParametricSoftplus, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class ThresholdedLinear(MaskedLayer):\n@@ -112,7 +112,7 @@ class ThresholdedLinear(MaskedLayer):\n         config = {\"name\": self.__class__.__name__,\n                   \"theta\": self.theta}\n         base_config = super(ThresholdedLinear, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class ThresholdedReLU(MaskedLayer):\n@@ -135,4 +135,4 @@ class ThresholdedReLU(MaskedLayer):\n         config = {\"name\": self.__class__.__name__,\n                   \"theta\": self.theta}\n         base_config = super(ThresholdedReLU, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -167,7 +167,7 @@ class Convolution1D(Layer):\n                   \"input_dim\": self.input_dim,\n                   \"input_length\": self.input_length}\n         base_config = super(Convolution1D, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class Convolution2D(Layer):\n@@ -282,7 +282,7 @@ class Convolution2D(Layer):\n                   \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n                   \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None}\n         base_config = super(Convolution2D, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class MaxPooling1D(Layer):\n@@ -319,7 +319,7 @@ class MaxPooling1D(Layer):\n                   \"pool_length\": self.pool_length,\n                   \"ignore_border\": self.ignore_border}\n         base_config = super(MaxPooling1D, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class MaxPooling2D(Layer):\n@@ -352,7 +352,7 @@ class MaxPooling2D(Layer):\n                   \"ignore_border\": self.ignore_border,\n                   \"stride\": self.stride}\n         base_config = super(MaxPooling2D, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class UpSample1D(Layer):\n@@ -377,7 +377,7 @@ class UpSample1D(Layer):\n         config = {\"name\": self.__class__.__name__,\n                   \"length\": self.length}\n         base_config = super(UpSample1D, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class UpSample2D(Layer):\n@@ -403,7 +403,7 @@ class UpSample2D(Layer):\n         config = {\"name\": self.__class__.__name__,\n                   \"size\": self.size}\n         base_config = super(UpSample2D, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class ZeroPadding1D(Layer):\n@@ -448,7 +448,7 @@ class ZeroPadding1D(Layer):\n         config = {\"name\": self.__class__.__name__,\n                   \"padding\": self.padding}\n         base_config = super(ZeroPadding1D, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class ZeroPadding2D(Layer):\n@@ -501,4 +501,4 @@ class ZeroPadding2D(Layer):\n         config = {\"name\": self.__class__.__name__,\n                   \"padding\": self.padding}\n         base_config = super(ZeroPadding2D, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -216,7 +216,7 @@ class Masking(MaskedLayer):\n         config = {\"name\": self.__class__.__name__,\n                   \"mask_value\": self.mask_value}\n         base_config = super(Masking, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class TimeDistributedMerge(Layer):\n@@ -257,7 +257,7 @@ class TimeDistributedMerge(Layer):\n         config = {\"name\": self.__class__.__name__,\n                   \"mode\": self.mode}\n         base_config = super(TimeDistributedMerge, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class Merge(Layer):\n@@ -367,7 +367,7 @@ class Merge(Layer):\n                   \"mode\": self.mode,\n                   \"concat_axis\": self.concat_axis}\n         base_config = super(Merge, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class Dropout(MaskedLayer):\n@@ -393,7 +393,7 @@ class Dropout(MaskedLayer):\n         config = {\"name\": self.__class__.__name__,\n                   \"p\": self.p}\n         base_config = super(Dropout, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class Activation(MaskedLayer):\n@@ -416,7 +416,7 @@ class Activation(MaskedLayer):\n                   \"target\": self.target,\n                   \"beta\": self.beta}\n         base_config = super(Activation, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class Reshape(Layer):\n@@ -442,7 +442,7 @@ class Reshape(Layer):\n         config = {\"name\": self.__class__.__name__,\n                   \"dims\": self.dims}\n         base_config = super(Reshape, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class Permute(Layer):\n@@ -470,7 +470,7 @@ class Permute(Layer):\n         config = {\"name\": self.__class__.__name__,\n                   \"dims\": self.dims}\n         base_config = super(Permute, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class Flatten(Layer):\n@@ -519,7 +519,7 @@ class RepeatVector(Layer):\n         config = {\"name\": self.__class__.__name__,\n                   \"n\": self.n}\n         base_config = super(RepeatVector, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class Dense(Layer):\n@@ -597,7 +597,7 @@ class Dense(Layer):\n                   \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n                   \"input_dim\": self.input_dim}\n         base_config = super(Dense, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class ActivityRegularization(Layer):\n@@ -622,7 +622,7 @@ class ActivityRegularization(Layer):\n                   \"l1\": self.l1,\n                   \"l2\": self.l2}\n         base_config = super(ActivityRegularization, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class TimeDistributedDense(MaskedLayer):\n@@ -707,7 +707,7 @@ class TimeDistributedDense(MaskedLayer):\n                   \"input_dim\": self.input_dim,\n                   \"input_length\": self.input_length}\n         base_config = super(TimeDistributedDense, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class AutoEncoder(Layer):\n@@ -869,4 +869,4 @@ class MaxoutDense(Layer):\n                   \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n                   \"input_dim\": self.input_dim}\n         base_config = super(MaxoutDense, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -81,7 +81,7 @@ class Embedding(Layer):\n                   \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n                   \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None}\n         base_config = super(Embedding, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class WordContextProduct(Layer):\n@@ -150,4 +150,4 @@ class WordContextProduct(Layer):\n                   \"init\": self.init.__name__,\n                   \"activation\": self.activation.__name__}\n         base_config = super(WordContextProduct, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -27,7 +27,7 @@ class GaussianNoise(MaskedLayer):\n         config = {\"name\": self.__class__.__name__,\n                   \"sigma\": self.sigma}\n         base_config = super(GaussianNoise, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class GaussianDropout(MaskedLayer):\n@@ -54,4 +54,4 @@ class GaussianDropout(MaskedLayer):\n         config = {\"name\": self.__class__.__name__,\n                   \"p\": self.p}\n         base_config = super(GaussianDropout, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -76,7 +76,7 @@ class BatchNormalization(Layer):\n                   \"mode\": self.mode,\n                   \"momentum\": self.momentum}\n         base_config = super(BatchNormalization, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class LRN2D(Layer):\n@@ -114,4 +114,4 @@ class LRN2D(Layer):\n                   \"beta\": self.beta,\n                   \"n\": self.n}\n         base_config = super(LRN2D, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -124,7 +124,7 @@ class SimpleRNN(Recurrent):\n                   \"input_dim\": self.input_dim,\n                   \"input_length\": self.input_length}\n         base_config = super(SimpleRNN, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class SimpleDeepRNN(Recurrent):\n@@ -222,7 +222,7 @@ class SimpleDeepRNN(Recurrent):\n                   \"input_dim\": self.input_dim,\n                   \"input_length\": self.input_length}\n         base_config = super(SimpleDeepRNN, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class GRU(Recurrent):\n@@ -335,7 +335,7 @@ class GRU(Recurrent):\n                   \"input_dim\": self.input_dim,\n                   \"input_length\": self.input_length}\n         base_config = super(GRU, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class LSTM(Recurrent):\n@@ -466,7 +466,7 @@ class LSTM(Recurrent):\n                   \"input_dim\": self.input_dim,\n                   \"input_length\": self.input_length}\n         base_config = super(LSTM, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class JZS1(Recurrent):\n@@ -583,7 +583,7 @@ class JZS1(Recurrent):\n                   \"input_dim\": self.input_dim,\n                   \"input_length\": self.input_length}\n         base_config = super(JZS1, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class JZS2(Recurrent):\n@@ -701,7 +701,7 @@ class JZS2(Recurrent):\n                   \"input_dim\": self.input_dim,\n                   \"input_length\": self.input_length}\n         base_config = super(JZS2, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class JZS3(Recurrent):\n@@ -812,4 +812,4 @@ class JZS3(Recurrent):\n                   \"input_dim\": self.input_dim,\n                   \"input_length\": self.input_length}\n         base_config = super(JZS3, self).get_config()\n-        return dict(base_config.items() + config.items())\n+        return dict(list(base_config.items()) + list(config.items()))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f463d23b38aa05992e4436eda44d4b87f39c0562", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1230 | Contributors (this commit): 12 | Commits (past 90d): 38 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -249,9 +249,7 @@ class Convolution2D(Layer):\n                 conv_out = dnn.dnn_conv(img=X,\n                                         kerns=self.W,\n                                         border_mode=border_mode,\n-                                        subsample=self.subsample,\n-                                        image_shape=self.input_shape,\n-                                        filter_shape=self.W_shape)\n+                                        subsample=self.subsample)\n         else:\n             if border_mode == 'same':\n                 border_mode = 'full'\n@@ -259,7 +257,9 @@ class Convolution2D(Layer):\n \n             conv_out = T.nnet.conv.conv2d(X, self.W,\n                                           border_mode=border_mode,\n-                                          subsample=self.subsample)\n+                                          subsample=self.subsample,\n+                                          image_shape=self.input_shape,\n+                                          filter_shape=self.W_shape)\n             if self.border_mode == 'same':\n                 shift_x = (self.nb_row - 1) // 2\n                 shift_y = (self.nb_col - 1) // 2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9dbf04b69902a7135e3f064d08f596919175cfa1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 80 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -135,12 +135,12 @@ class DrawActivations(Callback):\n model = Sequential()\n model.add(Convolution2D(32, 1, 3, 3, border_mode='full'))\n model.add(Activation('relu'))\n-model.add(MaxPooling2D(poolsize=(2, 2)))\n+model.add(MaxPooling2D(pool_size=(2, 2)))\n model.add(Dropout(0.25))\n \n model.add(Convolution2D(64, 32, 3, 3, border_mode='full'))\n model.add(Activation('relu'))\n-model.add(MaxPooling2D(poolsize=(2, 2)))\n+model.add(MaxPooling2D(pool_size=(2, 2)))\n model.add(Dropout(0.25))\n \n model.add(Flatten())\n\n@@ -8,7 +8,7 @@ import keras.utils.layer_utils as layer_utils\n print('-- Sequential model')\n left = Sequential()\n left.add(Convolution2D(32, 1, 3, 3, border_mode='valid'))\n-left.add(MaxPooling2D(poolsize=(2, 2)))\n+left.add(MaxPooling2D(pool_size=(2, 2)))\n left.add(Flatten())\n left.add(Dense(32 * 13 * 13, 50))\n left.add(Activation('relu'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4c1a6fc27ebbcdda62febfd54f1e21426596d17d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 439 | Contributors (this commit): 8 | Commits (past 90d): 10 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -48,7 +48,7 @@ print('X_test shape:', X_test.shape)\n \n print('Build model...')\n model = Sequential()\n-model.add(Embedding(max_features, 128))\n+model.add(Embedding(max_features, 128, input_length=maxlen))\n model.add(LSTM(128))  # try using a GRU instead, for fun\n model.add(Dropout(0.5))\n model.add(Dense(1))\n\n@@ -75,7 +75,7 @@ class Embedding(Layer):\n                   \"input_dim\": self.input_dim,\n                   \"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n-                  \"max_lenght\": self.max_lenght,\n+                  \"input_length\": self.input_length,\n                   \"mask_zero\": self.mask_zero,\n                   \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                   \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#815f7064a2a77cbaed4cff6e2b00b08ac14c04f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 225 | Contributors (this commit): 6 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ import theano\n import copy\n \n from ..layers.advanced_activations import LeakyReLU, PReLU\n-from ..layers.core import Dense, Merge, Dropout, Activation, Reshape, Flatten, RepeatVector, Layer, AutoEncoder, Masking\n+from ..layers.core import Dense, Merge, Dropout, Activation, Reshape, Flatten, RepeatVector, Layer, AutoEncoder, Masking, Permute\n from ..layers.core import ActivityRegularization, TimeDistributedDense, AutoEncoder, MaxoutDense\n from ..layers.convolutional import Convolution1D, Convolution2D, MaxPooling1D, MaxPooling2D, ZeroPadding2D\n from ..layers.embeddings import Embedding, WordContextProduct\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d2f3101aeec10b0ef14a4d09a32479114e814c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 76 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,7 +29,8 @@ class MaxNorm(Constraint):\n \n class NonNeg(Constraint):\n     def __call__(self, p):\n-        p *= T.ge(p, 0)\n+        p = theano.shared(p)\n+        p *= T.ge(p, 0.)\n         return p\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d2189fef32470b3dce05b4765a40b501174bd0d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 93 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -3,12 +3,12 @@ from setuptools import find_packages\n \n \n setup(name='Keras',\n-      version='0.1.2',\n+      version='0.2.0',\n       description='Theano-based Deep Learning library',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n-      download_url='https://github.com/fchollet/keras/tarball/0.1.2',\n+      download_url='https://github.com/fchollet/keras/tarball/0.2.0',\n       license='MIT',\n       install_requires=['theano', 'pyyaml'],\n       extras_require={\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#38a3228999373b3ee07e87af8d1e9b2ec4a684cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 253 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -71,13 +71,13 @@ model.add(Activation('softmax'))\n sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n model.compile(loss='categorical_crossentropy', optimizer=sgd)\n \n-if not data_augmentation:\n-    print(\"Not using data augmentation or normalization\")\n-\n X_train = X_train.astype(\"float32\")\n X_test = X_test.astype(\"float32\")\n X_train /= 255\n X_test /= 255\n+\n+if not data_augmentation:\n+    print(\"Not using data augmentation or normalization\")\n     model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch)\n     score = model.evaluate(X_test, Y_test, batch_size=batch_size)\n     print('Test score:', score)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f62c03bdea7ff95a1360cbc38c2523bee4c59204", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 674 | Contributors (this commit): 11 | Commits (past 90d): 16 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -82,7 +82,7 @@ class SGD(Optimizer):\n         return {\"name\": self.__class__.__name__,\n                 \"lr\": float(self.lr.get_value()),\n                 \"momentum\": float(self.momentum.get_value()),\n-                \"decay\": float(self.decay.get_value()),\n+                \"decay\": float(self.decay),\n                 \"nesterov\": self.nesterov}\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#68f619b9f9e42a2baae5fdbbc904ec324d580928", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 227 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ from ..layers.core import ActivityRegularization, TimeDistributedDense, AutoEnco\n from ..layers.convolutional import Convolution1D, Convolution2D, MaxPooling1D, MaxPooling2D, ZeroPadding2D\n from ..layers.embeddings import Embedding, WordContextProduct\n from ..layers.noise import GaussianNoise, GaussianDropout\n-from ..layers.normalization import BatchNormalization\n+from ..layers.normalization import BatchNormalization, LRN2D\n from ..layers.recurrent import SimpleRNN, SimpleDeepRNN, GRU, LSTM, JZS1, JZS2, JZS3\n from ..layers import containers\n from .. import regularizers\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#11e4c4b90f3ce07b10aa06a0f117e122b0c7b368", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 676 | Contributors (this commit): 12 | Commits (past 90d): 15 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -82,7 +82,7 @@ class SGD(Optimizer):\n         return {\"name\": self.__class__.__name__,\n                 \"lr\": float(self.lr.get_value()),\n                 \"momentum\": float(self.momentum.get_value()),\n-                \"decay\": float(self.decay),\n+                \"decay\": float(self.decay.get_value()),\n                 \"nesterov\": self.nesterov}\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6f620712b5448643aceb4961475a41ffe6157746", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 2337 | Contributors (this commit): 23 | Commits (past 90d): 27 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -267,7 +267,7 @@ class Merge(Layer):\n         '''\n         if len(layers) < 2:\n             raise Exception(\"Please specify two or more input layers (or containers) to merge\")\n-        if mode not in {'sum', 'mul', 'concat', 'ave'}:\n+        if mode not in {'sum', 'mul', 'concat', 'ave', 'join'}:\n             raise Exception(\"Invalid merge mode: \" + str(mode))\n         self.mode = mode\n         self.concat_axis = concat_axis\n@@ -296,6 +296,8 @@ class Merge(Layer):\n             for shape in input_shapes[1:]:\n                 output_shape[self.concat_axis] += shape[self.concat_axis]\n             return tuple(output_shape)\n+        elif self.mode == 'join':\n+            return None \n \n     def get_params(self):\n         return self.params, self.regularizers, self.constraints, self.updates\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#569bb74a088bf4f2ddc5b97fc504f52225da19f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2339 | Contributors (this commit): 24 | Commits (past 90d): 28 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -263,7 +263,7 @@ class TimeDistributedMerge(Layer):\n class Merge(Layer):\n     def __init__(self, layers, mode='sum', concat_axis=-1):\n         ''' Merge the output of a list of layers or containers into a single tensor.\n-            mode: {'sum', 'mul', 'concat', 'ave'}\n+            mode: {'sum', 'mul', 'concat', 'ave', 'join'}\n         '''\n         if len(layers) < 2:\n             raise Exception(\"Please specify two or more input layers (or containers) to merge\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a78c43033ebf3ce9b4c5e9a4d13f20ddd028ac13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 195 | Lines Deleted: 23 | Files Changed: 3 | Hunks: 13 | Methods Changed: 13 | Complexity Δ (Sum/Max): 29/24 | Churn Δ: 218 | Churn Cumulative: 3126 | Contributors (this commit): 27 | Commits (past 90d): 44 | Contributors (cumulative): 35 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,114 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+import numpy as np\n+import datetime\n+\n+np.random.seed(1337)  # for reproducibility\n+\n+from keras.datasets import mnist\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Dropout, Activation, Flatten\n+from keras.layers.convolutional import Convolution2D, MaxPooling2D\n+from keras.utils import np_utils\n+\n+'''\n+    Transfer learning toy example:\n+        1- Train a simple convnet on the MNIST dataset the first 5 digits [0..4].\n+        2- Freeze convolutional layers and fine-tune dense layers for the classification of digits [5..9].\n+\n+    Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_cnn.py\n+\n+    Get to 99.8% test accuracy after 5 epochs for the first five digits classifier\n+    and 99.2% for the last five digits after transfer + fine-tuning.\n+'''\n+\n+now = datetime.datetime.now\n+\n+batch_size = 128\n+nb_classes = 5\n+nb_epoch = 5\n+\n+# input image dimensions\n+img_rows, img_cols = 28, 28\n+# number of convolutional filters to use\n+nb_filters = 32\n+# size of pooling area for max pooling\n+nb_pool = 2\n+# convolution kernel size\n+nb_conv = 3\n+\n+\n+def train_model(model, train, test, nb_classes):\n+    X_train = train[0].reshape(train[0].shape[0], 1, img_rows, img_cols)\n+    X_test = test[0].reshape(test[0].shape[0], 1, img_rows, img_cols)\n+    X_train = X_train.astype(\"float32\")\n+    X_test = X_test.astype(\"float32\")\n+    X_train /= 255\n+    X_test /= 255\n+    print('X_train shape:', X_train.shape)\n+    print(X_train.shape[0], 'train samples')\n+    print(X_test.shape[0], 'test samples')\n+\n+    # convert class vectors to binary class matrices\n+    Y_train = np_utils.to_categorical(train[1], nb_classes)\n+    Y_test = np_utils.to_categorical(test[1], nb_classes)\n+\n+    model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n+\n+    t = now()\n+    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1,\n+              validation_data=(X_test, Y_test))\n+    print('Training time: %s' % (now() - t))\n+    score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n+    print('Test score:', score[0])\n+    print('Test accuracy:', score[1])\n+\n+\n+# the data, shuffled and split between train and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+\n+# create two datasets one with digits below 5 and one with 5 and above\n+X_train_lt5 = X_train[y_train < 5]\n+y_train_lt5 = y_train[y_train < 5]\n+X_test_lt5 = X_test[y_test < 5]\n+y_test_lt5 = y_test[y_test < 5]\n+\n+X_train_gte5 = X_train[y_train >= 5]\n+y_train_gte5 = y_train[y_train >= 5] - 5  # make classes start at 0 for\n+X_test_gte5 = X_test[y_test >= 5]         # np_utils.to_categorical\n+y_test_gte5 = y_test[y_test >= 5] - 5\n+\n+# define two groups of layers: feature (convolutions) and classification (dense)\n+feature_layers = [\n+    Convolution2D(nb_filters, nb_conv, nb_conv,\n+                  border_mode='full',\n+                  input_shape=(1, img_rows, img_cols)),\n+    Activation('relu'),\n+    Convolution2D(nb_filters, nb_conv, nb_conv),\n+    Activation('relu'),\n+    MaxPooling2D(pool_size=(nb_pool, nb_pool)),\n+    Dropout(0.25),\n+    Flatten(),\n+]\n+classification_layers = [\n+    Dense(128),\n+    Activation('relu'),\n+    Dropout(0.5),\n+    Dense(nb_classes),\n+    Activation('softmax')\n+]\n+\n+# create complete model\n+model = Sequential()\n+for l in feature_layers + classification_layers:\n+    model.add(l)\n+\n+# train model for 5-digit classification [0..4]\n+train_model(model, (X_train_lt5, y_train_lt5), (X_test_lt5, y_test_lt5), nb_classes)\n+\n+# freeze feature layers and rebuild model\n+for l in feature_layers:\n+    l.trainable = False\n+\n+# transfer: train dense layers for new classification task [5..9]\n+train_model(model, (X_train_gte5, y_train_gte5), (X_test_gte5, y_test_gte5), nb_classes)\n\n@@ -2,6 +2,7 @@\n from __future__ import absolute_import\n from __future__ import print_function\n \n+from collections import OrderedDict\n import theano.tensor as T\n from ..layers.core import Layer, Merge\n from ..utils.theano_utils import ndim_tensor\n@@ -20,11 +21,6 @@ class Sequential(Layer):\n \n     def __init__(self, layers=[]):\n         self.layers = []\n-        self.params = []\n-        self.regularizers = []\n-        self.constraints = []\n-        self.updates = []\n-\n         for layer in layers:\n             self.add(layer)\n \n@@ -38,11 +34,37 @@ class Sequential(Layer):\n             if not hasattr(self.layers[0], 'input'):\n                 self.set_input()\n \n-        params, regularizers, constraints, updates = layer.get_params()\n-        self.params += params\n-        self.regularizers += regularizers\n-        self.constraints += constraints\n-        self.updates += updates\n+    @property\n+    def params(self):\n+        params = []\n+        for l in self.layers:\n+            if l.trainable:\n+                params += l.get_params()[0]\n+        return params\n+\n+    @property\n+    def regularizers(self):\n+        regularizers = []\n+        for l in self.layers:\n+            if l.trainable:\n+                regularizers += l.get_params()[1]\n+        return regularizers\n+\n+    @property\n+    def constraints(self):\n+        constraints = []\n+        for l in self.layers:\n+            if l.trainable:\n+                constraints += l.get_params()[2]\n+        return constraints\n+\n+    @property\n+    def updates(self):\n+        updates = []\n+        for l in self.layers:\n+            if l.trainable:\n+                updates += l.get_params()[3]\n+        return updates\n \n     @property\n     def output_shape(self):\n@@ -97,7 +119,6 @@ class Graph(Layer):\n         when it has exactly one input and one output.\n \n         inherited from Layer:\n-            - get_params\n             - get_output_mask\n             - supports_masked_input\n             - get_weights\n@@ -105,7 +126,7 @@ class Graph(Layer):\n     '''\n     def __init__(self):\n         self.namespace = set()  # strings\n-        self.nodes = {}  # layer-like\n+        self.nodes = OrderedDict()  # layer-like\n         self.inputs = {}  # layer-like\n         self.input_order = []  # strings\n         self.outputs = {}  # layer-like\n@@ -114,11 +135,6 @@ class Graph(Layer):\n         self.output_config = []  # dicts\n         self.node_config = []  # dicts\n \n-        self.params = []\n-        self.regularizers = []\n-        self.constraints = []\n-        self.updates = []\n-\n     @property\n     def nb_input(self):\n         return len(self.inputs)\n@@ -127,6 +143,38 @@ class Graph(Layer):\n     def nb_output(self):\n         return len(self.outputs)\n \n+    @property\n+    def params(self):\n+        params = []\n+        for l in self.nodes.values():\n+            if l.trainable:\n+                params += l.get_params()[0]\n+        return params\n+\n+    @property\n+    def regularizers(self):\n+        regularizers = []\n+        for l in self.nodes.values():\n+            if l.trainable:\n+                regularizers += l.get_params()[1]\n+        return regularizers\n+\n+    @property\n+    def constraints(self):\n+        constraints = []\n+        for l in self.nodes.values():\n+            if l.trainable:\n+                constraints += l.get_params()[2]\n+        return constraints\n+\n+    @property\n+    def updates(self):\n+        updates = []\n+        for l in self.nodes.values():\n+            if l.trainable:\n+                updates += l.get_params()[3]\n+        return updates\n+\n     def set_previous(self, layer, connection_map={}):\n         if self.nb_input != layer.nb_output:\n             raise Exception('Cannot connect layers: input count does not match output count.')\n@@ -220,11 +268,6 @@ class Graph(Layer):\n                                  'merge_mode': merge_mode,\n                                  'concat_axis': concat_axis,\n                                  'create_output': create_output})\n-        params, regularizers, constraints, updates = layer.get_params()\n-        self.params += params\n-        self.regularizers += regularizers\n-        self.constraints += constraints\n-        self.updates += updates\n \n         if create_output:\n             self.add_output(name, input=name)\n\n@@ -20,9 +20,11 @@ from six.moves import zip\n class Layer(object):\n     def __init__(self, **kwargs):\n         for kwarg in kwargs:\n-            assert kwarg in {'input_shape'}, \"Keyword argument not understood: \" + kwarg\n+            assert kwarg in {'input_shape', 'trainable'}, \"Keyword argument not understood: \" + kwarg\n         if 'input_shape' in kwargs:\n             self.set_input_shape(kwargs['input_shape'])\n+        if 'trainable' in kwargs:\n+            self._trainable = kwargs['trainable']\n         if not hasattr(self, 'params'):\n             self.params = []\n \n@@ -45,6 +47,17 @@ class Layer(object):\n         '''\n         pass\n \n+    @property\n+    def trainable(self):\n+        if hasattr(self, '_trainable'):\n+            return self._trainable\n+        else:\n+            return True\n+\n+    @trainable.setter\n+    def trainable(self, value):\n+        self._trainable = value\n+\n     @property\n     def nb_input(self):\n         return 1\n@@ -133,6 +146,8 @@ class Layer(object):\n         config = {\"name\": self.__class__.__name__}\n         if hasattr(self, '_input_shape'):\n             config['input_shape'] = self._input_shape[1:]\n+        if hasattr(self, '_trainable'):\n+            config['trainable'] = self._trainable\n         return config\n \n     def get_params(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
