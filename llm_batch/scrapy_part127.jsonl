{"custom_id": "scrapy#03bc21898759286e8cc99c08f1701403acd301f8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 144 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -43,7 +43,7 @@ def get_engine_status(engine=None):\n             status['global'][test] = eval(test)\n         except Exception, e:\n             status['global'][test] = \"%s (exception)\" % type(e).__name__\n-    for spider in engine.downloader.sites + engine.scraper.sites:\n+    for spider in set(engine.downloader.sites.keys() + engine.scraper.sites.keys()):\n         x = {}\n         for test in spider_tests:\n             try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2e74ccaa7e088e059cc261cfb8f00f6b48c18a80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 158 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ from functools import partial\n \n from scrapy.http import Request\n from scrapy.utils.spider import iterate_spider_output\n-from scrapy.contrib.spiders.init import InitSpider\n+from scrapy.spider import BaseSpider\n from scrapy.conf import settings\n \n def identity(x):\n@@ -29,7 +29,7 @@ class Rule(object):\n         else:\n             self.follow = follow\n \n-class CrawlSpider(InitSpider):\n+class CrawlSpider(BaseSpider):\n \n     rules = ()\n \n\n@@ -5,14 +5,14 @@ for scraping from an XML feed.\n See documentation in docs/topics/spiders.rst\n \"\"\"\n \n-from scrapy.contrib.spiders.init import InitSpider\n+from scrapy.spider import BaseSpider\n from scrapy.item import BaseItem\n from scrapy.http import Request\n from scrapy.utils.iterators import xmliter, csviter\n from scrapy.selector import XmlXPathSelector, HtmlXPathSelector\n from scrapy.exceptions import NotConfigured, NotSupported\n \n-class XMLFeedSpider(InitSpider):\n+class XMLFeedSpider(BaseSpider):\n     \"\"\"\n     This class intends to be the base class for spiders that scrape\n     from XML feeds.\n@@ -47,7 +47,7 @@ class XMLFeedSpider(InitSpider):\n         \"\"\"This method must be overriden with your custom spider functionality\"\"\"\n         if hasattr(self, 'parse_item'): # backward compatibility\n             return self.parse_item(response, selector)\n-        raise NotImplemented\n+        raise NotImplementedError\n         \n     def parse_nodes(self, response, nodes):\n         \"\"\"This method is called for the nodes matching the provided tag name\n@@ -90,7 +90,7 @@ class XMLFeedSpider(InitSpider):\n         for (prefix, uri) in self.namespaces:\n             selector.register_namespace(prefix, uri)\n \n-class CSVFeedSpider(InitSpider):\n+class CSVFeedSpider(BaseSpider):\n     \"\"\"Spider for parsing CSV feeds.\n     It receives a CSV file in a response; iterates through each of its rows,\n     and calls parse_row with a dict containing each field's data.\n@@ -110,9 +110,9 @@ class CSVFeedSpider(InitSpider):\n         \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n         return response\n \n-    def parse_row(self, row):\n+    def parse_row(self, response, row):\n         \"\"\"This method must be overriden with your custom spider functionality\"\"\"\n-        raise NotImplemented\n+        raise NotImplementedError\n \n     def parse_rows(self, response):\n         \"\"\"Receives a response and a dict (representing each row) with a key for\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0305ffdd6cb556d7f635ae05c605a5372154784a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 119 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -39,7 +39,22 @@ class SitemapTest(unittest.TestCase):\n         assert s.type == 'sitemapindex'\n         self.assertEqual(list(s), [{'loc': 'http://www.example.com/sitemap1.xml.gz', 'lastmod': '2004-10-01T18:23:17+00:00'}, {'loc': 'http://www.example.com/sitemap2.xml.gz', 'lastmod': '2005-01-01'}])\n \n-class RobotsTest(unittest.TestCase):\n+    def test_sitemap_strip(self):\n+        \"\"\"Assert we can deal with trailing spaces inside <loc> tags - we've\n+        seen those\n+        \"\"\"\n+        s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n+  <url>\n+    <loc> http://www.example.com/</loc>\n+    <lastmod>2009-08-16</lastmod>\n+    <changefreq>daily</changefreq>\n+    <priority>1</priority>\n+  </url>\n+</urlset>\n+\"\"\")\n+        self.assertEqual(list(s),\n+            [{'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'}])\n \n     def test_sitemap_urls_from_robots(self):\n         robots = \"\"\"User-agent: *\n\n@@ -23,7 +23,7 @@ class Sitemap(object):\n             d = {}\n             for el in elem.getchildren():\n                 _, name = el.tag.split('}', 1)\n-                d[name] = el.text\n+                d[name] = el.text.strip()\n             yield d\n \n def sitemap_urls_from_robots(robots_text):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d8775a7575b730b18565d46cb929f2011dc5cb10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 54 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): -14/0 | Churn Δ: 54 | Churn Cumulative: 132 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.7435897435897436\n\nDIFF:\n@@ -1,54 +0,0 @@\n-import warnings\n-warnings.warn(\"File export pipeline is deprecated and will be removed in Scrapy 0.11, use Feed exports instead\", \\\n-    DeprecationWarning, stacklevel=2)\n-\n-from scrapy.xlib.pydispatch import dispatcher\n-from scrapy import signals\n-from scrapy.exceptions import NotConfigured\n-from scrapy.contrib import exporter\n-from scrapy.conf import settings\n-\n-class FileExportPipeline(object):\n-\n-    def __init__(self):\n-        self.exporter, self.file = self.get_exporter_and_file()\n-        self.exporter.start_exporting()\n-        dispatcher.connect(self.engine_stopped, signals.engine_stopped)\n-\n-    def process_item(self, item, spider):\n-        self.exporter.export_item(item)\n-        return item\n-\n-    def engine_stopped(self):\n-        self.exporter.finish_exporting()\n-        self.file.close()\n-\n-    def get_exporter_and_file(self):\n-        format = settings['EXPORT_FORMAT']\n-        filename = settings['EXPORT_FILE']\n-        if not format or not filename:\n-            raise NotConfigured\n-        exp_kwargs = {\n-            'fields_to_export': settings.getlist('EXPORT_FIELDS') or None,\n-            'export_empty_fields': settings.getbool('EXPORT_EMPTY', False),\n-            'encoding': settings.get('EXPORT_ENCODING', 'utf-8'),\n-        }\n-        file = open(filename, 'wb')\n-        if format == 'xml':\n-            exp = exporter.XmlItemExporter(file, **exp_kwargs)\n-        elif format == 'csv':\n-            exp = exporter.CsvItemExporter(file, **exp_kwargs)\n-        elif format == 'csv_headers':\n-            exp = exporter.CsvItemExporter(file, include_headers_line=True, \\\n-                **exp_kwargs)\n-        elif format == 'pprint':\n-            exp = exporter.PprintItemExporter(file, **exp_kwargs)\n-        elif format == 'pickle':\n-            exp = exporter.PickleItemExporter(file, **exp_kwargs)\n-        elif format == 'json':\n-            exp = exporter.JsonLinesItemExporter(file, **exp_kwargs)\n-        elif format == 'jsonlines':\n-            exp = exporter.JsonItemExporter(file, **exp_kwargs)\n-        else:\n-            raise NotConfigured(\"Unsupported export format: %s\" % format)\n-        return exp, file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d197895d8fe54bc42228fba7ae23ca5eac76fce1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 473 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -81,8 +81,7 @@ class FilesystemCacheStorage(object):\n             body = f.read()\n         with open(join(rpath, 'response_headers'), 'rb') as f:\n             rawheaders = f.read()\n-        # We failback to metadata['url'] to support old generated caches. TODO: remove for Scrapy 0.11\n-        url = metadata.get('response_url') or metadata['url']\n+        url = metadata.get('response_url')\n         status = metadata['status']\n         headers = Headers(headers_raw_to_dict(rawheaders))\n         respcls = responsetypes.from_args(headers=headers, url=url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d97a9d87319b8fb662612e6d47e43fa72cfb422d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 30 | Churn Cumulative: 756 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -79,7 +79,11 @@ class ItemLoader(object):\n     def get_output_value(self, field_name):\n         proc = self.get_output_processor(field_name)\n         proc = wrap_loader_context(proc, self.context)\n+        try:\n             return proc(self._values[field_name])\n+        except Exception, e:\n+            raise ValueError(\"Error with output processor: field=%r value=%r error='%s: %s'\" % \\\n+                (field_name, self._values[field_name], type(e).__name__, str(e)))\n \n     def get_collected_values(self, field_name):\n         return self._values[field_name]\n\n@@ -215,6 +215,32 @@ class ItemLoaderTest(unittest.TestCase):\n         il.add_value('name', [u'mar', u'ta'])\n         self.assertEqual(il.get_output_value('name'), u'Mar Ta')\n \n+\n+    def test_output_processor_error(self):\n+        class TestItemLoader(ItemLoader):\n+            default_item_class = TestItem\n+            name_out = MapCompose(float)\n+\n+        il = TestItemLoader()\n+        il.add_value('name', [u'$10'])\n+        try:\n+            float('$10')\n+        except Exception, e:\n+            expected_exc_str = str(e)\n+\n+        exc = None\n+        try:\n+            il.load_item()\n+        except Exception, e:\n+            exc = e\n+        assert isinstance(exc, ValueError)\n+        s = str(exc)\n+        assert 'name' in s, s\n+        assert '$10' in s, s\n+        assert 'ValueError' in s, s\n+        assert expected_exc_str in s, s\n+\n+\n     def test_output_processor_using_classes(self):\n         il = TestItemLoader()\n         il.add_value('name', [u'mar', u'ta'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#db5cae7c03b803dfe35b1fc4b93788a5ab6dfeba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 24 | Churn Cumulative: 66 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,16 +8,16 @@ class SitemapSpider(BaseSpider):\n \n     sitemap_urls = ()\n     sitemap_rules = [('', 'parse')]\n+    sitemap_follow = ['']\n \n     def __init__(self, *a, **kw):\n         super(SitemapSpider, self).__init__(*a, **kw)\n         self._cbs = []\n         for r, c in self.sitemap_rules:\n-            if isinstance(r, basestring):\n-                r = re.compile(r)\n             if isinstance(c, basestring):\n                 c = getattr(self, c)\n-            self._cbs.append((r, c))\n+            self._cbs.append((regex(r), c))\n+        self._follow = [regex(x) for x in self.sitemap_follow]\n \n     def start_requests(self):\n         return [Request(x, callback=self._parse_sitemap) for x in self.sitemap_urls]\n@@ -29,12 +29,22 @@ class SitemapSpider(BaseSpider):\n         else:\n             s = Sitemap(response.body)\n             if s.type == 'sitemapindex':\n-                for sitemap in s:\n-                    yield Request(sitemap['loc'], callback=self._parse_sitemap)\n+                for loc in iterloc(s):\n+                    if any(x.search(loc) for x in self._follow):\n+                        yield Request(loc, callback=self._parse_sitemap)\n             elif s.type == 'urlset':\n-                for url in s:\n-                    loc = url['loc']\n+                for loc in iterloc(s):\n                     for r, c in self._cbs:\n                         if r.search(loc):\n                             yield Request(loc, callback=c)\n                             break\n+\n+\n+def regex(x):\n+    if isinstance(x, basestring):\n+        return re.compile(x)\n+    return x\n+\n+def iterloc(it):\n+    for d in it:\n+        yield d['loc']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7cd559eca54157d1bf37af0f651abfaaf22cdd5f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 72 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,8 +1,9 @@\n import re\n \n from scrapy.spider import BaseSpider\n-from scrapy.http import Request\n+from scrapy.http import Request, XmlResponse\n from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n+from scrapy import log\n \n class SitemapSpider(BaseSpider):\n \n@@ -27,6 +28,9 @@ class SitemapSpider(BaseSpider):\n             for url in sitemap_urls_from_robots(response.body):\n                 yield Request(url, callback=self._parse_sitemap)\n         else:\n+            if not isinstance(response, XmlResponse):\n+                log.msg(\"Ignoring non-XML sitemap: %s\" % response, log.WARNING)\n+                return\n             s = Sitemap(response.body)\n             if s.type == 'sitemapindex':\n                 for loc in iterloc(s):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5275343fa19404f10736923f32d4eb3e74efb35f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 530 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -74,6 +74,7 @@ class Shell(object):\n         else:\n             url = any_to_uri(request_or_url)\n             request = Request(url, dont_filter=True)\n+            request.meta['handle_httpstatus_all'] = True\n         response = None\n         response, spider = threads.blockingCallFromThread(reactor, \\\n             self._schedule, request, spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a5223881ee1fa615674fec130f7b09e9aacb3e80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 23 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -17,8 +17,6 @@ class HttpCompressionMiddleware(object):\n             content_encoding = response.headers.getlist('Content-Encoding')\n             if content_encoding:\n                 encoding = content_encoding.pop()\n-                with open('body', 'w') as f:\n-                    f.write(response.body)\n                 decoded_body = self._decode(response.body, encoding.lower())\n                 respcls = responsetypes.from_args(headers=response.headers, \\\n                     url=response.url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#81fbe8c9a4b3940cd1cdf27ff0848a7ec5360512", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 27 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ class HttpCompressionMiddleware(object):\n     sent/received from web sites\"\"\"\n \n     def process_request(self, request, spider):\n-        request.headers.setdefault('Accept-Encoding', 'gzip,deflate')\n+        request.headers.setdefault('Accept-Encoding', 'x-gzip,gzip,deflate')\n \n     def process_response(self, request, response, spider):\n         if isinstance(response, Response):\n@@ -32,7 +32,7 @@ class HttpCompressionMiddleware(object):\n         return response\n \n     def _decode(self, body, encoding):\n-        if encoding == 'gzip':\n+        if encoding == 'gzip' or encoding == 'x-gzip':\n             body = gunzip(body)\n \n         if encoding == 'deflate':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#57070513524c3d6837df733dff77f340dca4a686", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 56 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -16,6 +16,7 @@ SAMPLEDIR = join(tests_datadir, 'compressed')\n \n FORMAT = {\n         'gzip': ('html-gzip.bin', 'gzip'),\n+        'x-gzip': ('html-gzip.bin', 'gzip'),\n         'rawdeflate': ('html-rawdeflate.bin', 'deflate'),\n         'zlibdeflate': ('html-zlibdeflate.bin', 'deflate'),\n         }\n@@ -51,7 +52,7 @@ class HttpCompressionTest(TestCase):\n         request = Request('http://scrapytest.org')\n         assert 'Accept-Encoding' not in request.headers\n         self.mw.process_request(request, self.spider)\n-        self.assertEqual(request.headers.get('Accept-Encoding'), 'gzip,deflate')\n+        self.assertEqual(request.headers.get('Accept-Encoding'), 'x-gzip,gzip,deflate')\n \n     def test_process_response_gzip(self):\n         response = self._getresponse('gzip')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#949e11ee3147010f891eca82a98e9003547d9cc7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 85 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 0.42857142857142855\n\nDIFF:\n@@ -3,6 +3,7 @@ import re\n from scrapy.spider import BaseSpider\n from scrapy.http import Request, XmlResponse\n from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n+from scrapy.utils.gz import gunzip\n from scrapy import log\n \n class SitemapSpider(BaseSpider):\n@@ -28,10 +29,15 @@ class SitemapSpider(BaseSpider):\n             for url in sitemap_urls_from_robots(response.body):\n                 yield Request(url, callback=self._parse_sitemap)\n         else:\n-            if not isinstance(response, XmlResponse):\n+            if isinstance(response, XmlResponse):\n+                body = response.body\n+            elif is_gzipped(response):\n+                body = gunzip(response.body)\n+            else:\n                 log.msg(\"Ignoring non-XML sitemap: %s\" % response, log.WARNING)\n                 return\n-            s = Sitemap(response.body)\n+\n+            s = Sitemap(body)\n             if s.type == 'sitemapindex':\n                 for loc in iterloc(s):\n                     if any(x.search(loc) for x in self._follow):\n@@ -43,6 +49,9 @@ class SitemapSpider(BaseSpider):\n                             yield Request(loc, callback=c)\n                             break\n \n+def is_gzipped(response):\n+    ctype = response.headers.get('Content-Type', '')\n+    return ctype in ('application/x-gzip', 'applicatoin/gzip')\n \n def regex(x):\n     if isinstance(x, basestring):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7abc4b4c5a147c305016bf895e49f7ed299d71ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 87 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -51,7 +51,7 @@ class SitemapSpider(BaseSpider):\n \n def is_gzipped(response):\n     ctype = response.headers.get('Content-Type', '')\n-    return ctype in ('application/x-gzip', 'applicatoin/gzip')\n+    return ctype in ('application/x-gzip', 'application/gzip')\n \n def regex(x):\n     if isinstance(x, basestring):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ab9b786791dce0dcd126280102b5504bba14d55b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 32 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -16,7 +16,7 @@ def render_templatefile(path, **kwargs):\n     if path.endswith('.tmpl'):\n         os.remove(path)\n \n-CAMELCASE_INVALID_CHARS = re.compile('[^a-zA-Z]')\n+CAMELCASE_INVALID_CHARS = re.compile('[^a-zA-Z\\d]')\n def string_camelcase(string):\n     \"\"\" Convert a word  to its CamelCase version and remove invalid chars\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#574b070bb4574348c33a15b68de76030984c031d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 129 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -51,10 +51,16 @@ class SitemapTest(unittest.TestCase):\n     <changefreq>daily</changefreq>\n     <priority>1</priority>\n   </url>\n+  <url>\n+    <loc> http://www.example.com/2</loc>\n+    <lastmod />\n+  </url>\n </urlset>\n \"\"\")\n         self.assertEqual(list(s),\n-            [{'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'}])\n+            [{'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'},\n+             {'loc': 'http://www.example.com/2', 'lastmod': ''},\n+            ])\n \n     def test_sitemap_urls_from_robots(self):\n         robots = \"\"\"User-agent: *\n\n@@ -23,7 +23,7 @@ class Sitemap(object):\n             d = {}\n             for el in elem.getchildren():\n                 _, name = el.tag.split('}', 1)\n-                d[name] = el.text.strip()\n+                d[name] = el.text.strip() if el.text else ''\n             yield d\n \n def sitemap_urls_from_robots(robots_text):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#409aaade0b97f1b3a7a31ca4510b420e18d34617", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 74 | Files Changed: 4 | Hunks: 34 | Methods Changed: 29 | Complexity Δ (Sum/Max): -3/6 | Churn Δ: 153 | Churn Cumulative: 2222 | Contributors (this commit): 5 | Commits (past 90d): 18 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,7 +9,6 @@ from collections import deque\n from twisted.internet import reactor, defer\n from twisted.python.failure import Failure\n \n-from scrapy.exceptions import IgnoreRequest\n from scrapy.conf import settings\n from scrapy.utils.python import setattr_default\n from scrapy.utils.defer import mustbe_deferred\n@@ -34,7 +33,6 @@ class SpiderInfo(object):\n         self.active = set()\n         self.queue = deque()\n         self.transferring = set()\n-        self.closing = False\n         self.lastseen = 0\n         self.next_request_calls = set()\n \n@@ -77,13 +75,10 @@ class Downloader(object):\n         not be downloaded from site.\n         \"\"\"\n         site = self.sites[spider]\n-        if site.closing:\n-            raise IgnoreRequest('Cannot fetch on a closing spider')\n \n         site.active.add(request)\n         def _deactivate(response):\n             site.active.remove(request)\n-            self._close_if_idle(spider)\n             return response\n \n         dfd = self.middleware.download(self.enqueue, request, spider)\n@@ -92,8 +87,6 @@ class Downloader(object):\n     def enqueue(self, request, spider):\n         \"\"\"Enqueue a Request for a effective download from site\"\"\"\n         site = self.sites[spider]\n-        if site.closing:\n-            raise IgnoreRequest\n \n         def _downloaded(response):\n             send_catch_log(signal=signals.response_downloaded, \\\n@@ -128,20 +121,9 @@ class Downloader(object):\n         # Process enqueued requests if there are free slots to transfer for this site\n         while site.queue and site.free_transfer_slots() > 0:\n             request, deferred = site.queue.popleft()\n-            if site.closing:\n-                dfd = defer.fail(Failure(IgnoreRequest()))\n-            else:\n             dfd = self._download(site, request, spider)\n             dfd.chainDeferred(deferred)\n \n-        self._close_if_idle(spider)\n-\n-    def _close_if_idle(self, spider):\n-        site = self.sites.get(spider)\n-        if site and site.closing and not site.active:\n-            del self.sites[spider]\n-            site.closing.callback(None)\n-\n     def _download(self, site, request, spider):\n         # The order is very important for the following deferreds. Do not change!\n \n@@ -156,12 +138,6 @@ class Downloader(object):\n         def finish_transferring(_):\n             site.transferring.remove(request)\n             self._process_queue(spider)\n-            # avoid partially downloaded responses from propagating to the\n-            # downloader middleware, to speed-up the closing process\n-            if site.closing:\n-                log.msg(\"Crawled while closing spider: %s\" % request, \\\n-                    level=log.DEBUG, spider=spider)\n-                raise IgnoreRequest\n             return _\n         return dfd.addBoth(finish_transferring)\n \n@@ -173,11 +149,8 @@ class Downloader(object):\n     def close_spider(self, spider):\n         \"\"\"Free any resources associated with the given spider\"\"\"\n         assert spider in self.sites, \"Spider not opened: %s\" % spider\n-        site = self.sites.get(spider)\n-        site.closing = defer.Deferred()\n+        site = self.sites.pop(spider)\n         site.cancel_request_calls()\n-        self._process_queue(spider)\n-        return site.closing\n \n     def is_idle(self):\n         return not self.sites\n\n@@ -19,12 +19,34 @@ from scrapy.utils.misc import load_object\n from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n from scrapy.utils.defer import mustbe_deferred\n \n+class Slot(object):\n+\n+    def __init__(self):\n+        self.closing = False\n+        self.inprogress = set() # requests in progress\n+\n+    def add_request(self, request):\n+        self.inprogress.add(request)\n+\n+    def remove_request(self, request):\n+        self.inprogress.remove(request)\n+        self._maybe_fire_closing()\n+\n+    def close(self):\n+        self.closing = defer.Deferred()\n+        self._maybe_fire_closing()\n+        return self.closing\n+\n+    def _maybe_fire_closing(self):\n+        if self.closing and not self.inprogress:\n+            self.closing.callback(None)\n+\n+\n class ExecutionEngine(object):\n \n     def __init__(self, settings, spider_closed_callback):\n         self.settings = settings\n-        self.closing = {} # dict (spider -> reason) of spiders being closed\n-        self.closing_dfds = {} # dict (spider -> deferred) of spiders being closed\n+        self.slots = {}\n         self.running = False\n         self.paused = False\n         self._next_request_calls = {}\n@@ -88,7 +110,9 @@ class ExecutionEngine(object):\n             self._spider_idle(spider)\n \n     def _needs_backout(self, spider):\n+        slot = self.slots[spider]\n         return not self.running \\\n+            or slot.closing \\\n             or self.spider_is_closed(spider) \\\n             or self.downloader.sites[spider].needs_backout() \\\n             or self.scraper.sites[spider].needs_backout()\n@@ -99,6 +123,10 @@ class ExecutionEngine(object):\n             return\n         d = self._download(request, spider)\n         d.addBoth(self._handle_downloader_output, request, spider)\n+        d.addErrback(log.msg, spider=spider)\n+        slot = self.slots[spider]\n+        d.addBoth(lambda _: slot.remove_request(request))\n+        d.addErrback(log.msg, spider=spider)\n         d.addBoth(lambda _: self.next_request(spider))\n         return d\n \n@@ -131,11 +159,6 @@ class ExecutionEngine(object):\n         closing stage)\"\"\"\n         return spider not in self.downloader.sites\n \n-    def spider_is_open(self, spider):\n-        \"\"\"Return True if the spider is fully opened (ie. not in closing\n-        stage)\"\"\"\n-        return spider in self.downloader.sites and spider not in self.closing\n-\n     @property\n     def open_spiders(self):\n         return self.downloader.sites.keys()\n@@ -145,28 +168,31 @@ class ExecutionEngine(object):\n         return len(self.downloader.sites) < self.downloader.concurrent_spiders\n \n     def crawl(self, request, spider):\n-        if spider in self.closing: # ignore requests for spiders being closed\n-            return\n         assert spider in self.open_spiders, \\\n             \"Spider %r not opened when crawling: %s\" % (spider.name, request)\n         self.schedule(request, spider)\n         self.next_request(spider)\n-        # FIXME: we can't log errors because we would be preventing them from\n-        # propagating to the request errback. This should be fixed after the\n-        # next core refactoring.\n-        #schd.addErrback(log.err, \"Error on engine.crawl()\")\n \n     def schedule(self, request, spider):\n         return self.scheduler.enqueue_request(spider, request)\n \n     def download(self, request, spider):\n+        slot = self.slots[request]\n+        slot.add_request(request)\n         if isinstance(request, Response):\n             return request\n         d = self._download(request, spider)\n         d.addCallback(self.download, spider)\n+        d.addBoth(self._remove_request, slot, request)\n         return d\n \n+    def _remove_request(self, _, slot, request):\n+        slot.remove_request(request)\n+        return _\n+\n     def _download(self, request, spider):\n+        slot = self.slots[spider]\n+        slot.add_request(request)\n         def _on_success(response):\n             \"\"\"handle the result of a page download\"\"\"\n             assert isinstance(response, (Response, Request))\n@@ -207,6 +233,7 @@ class ExecutionEngine(object):\n         assert self.has_capacity(), \"No free spider slots when opening %r\" % \\\n             spider.name\n         log.msg(\"Spider opened\", spider=spider)\n+        self.slots[spider] = Slot()\n         yield self.scheduler.open_spider(spider)\n         self.downloader.open_spider(spider)\n         yield self.scraper.open_spider(spider)\n@@ -234,44 +261,54 @@ class ExecutionEngine(object):\n \n     def close_spider(self, spider, reason='cancelled'):\n         \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n-        if spider in self.closing:\n-            return defer.succeed(None)\n+\n+        slot = self.slots[spider]\n+        if slot.closing:\n+            return slot.closing\n         log.msg(\"Closing spider (%s)\" % reason, spider=spider)\n-        self.closing[spider] = reason\n+\n         self.scheduler.clear_pending_requests(spider)\n-        dfd = self.downloader.close_spider(spider)\n-        self.closing_dfds[spider] = dfd\n+\n+        dfd = slot.close()\n+\n         dfd.addBoth(lambda _: self.scheduler.close_spider(spider))\n-        dfd.addErrback(log.err, \"Unhandled error in scheduler.close_spider()\", \\\n-            spider=spider)\n+        dfd.addErrback(log.err, spider=spider)\n+\n+        dfd.addBoth(lambda _: self.downloader.close_spider(spider))\n+        dfd.addErrback(log.err, spider=spider)\n+\n         dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n-        dfd.addErrback(log.err, \"Unhandled error in scraper.close_spider()\", \\\n-            spider=spider)\n-        dfd.addBoth(lambda _: self._finish_closing_spider(spider))\n+        dfd.addErrback(log.err, spider=spider)\n+\n+        dfd.addBoth(lambda _: self._cancel_next_call(spider))\n+        dfd.addErrback(log.err, spider=spider)\n+\n+        dfd.addBoth(lambda _: send_catch_log_deferred(signal=signals.spider_closed, \\\n+            spider=spider, reason=reason))\n+        dfd.addErrback(log.err, spider=spider)\n+\n+        dfd.addBoth(lambda _: stats.close_spider(spider, reason=reason))\n+        dfd.addErrback(log.err, spider=spider)\n+\n+        dfd.addBoth(lambda _: log.msg(\"Spider closed (%s)\" % reason, spider=spider))\n+\n+        dfd.addBoth(lambda _: self.slots.pop(spider))\n+        dfd.addErrback(log.err, spider=spider)\n+\n+        dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n+\n         return dfd\n \n+    def _cancel_next_call(self, spider):\n+        call = self._next_request_calls.pop(spider, None)\n+        if call and call.active:\n+            call.cancel()\n+\n     def _close_all_spiders(self):\n         dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n-        dfds += self.closing_dfds.values()\n         dlist = defer.DeferredList(dfds)\n         return dlist\n \n-    def _finish_closing_spider(self, spider):\n-        \"\"\"This function is called after the spider has been closed\"\"\"\n-        reason = self.closing.pop(spider, 'finished')\n-        call = self._next_request_calls.pop(spider, None)\n-        if call and call.active():\n-            call.cancel()\n-        dfd = send_catch_log_deferred(signal=signals.spider_closed, \\\n-            spider=spider, reason=reason)\n-        dfd.addBoth(lambda _: stats.close_spider(spider, reason=reason))\n-        dfd.addErrback(log.err, \"Unhandled error in stats.close_spider()\",\n-            spider=spider)\n-        dfd.addBoth(lambda _: log.msg(\"Spider closed (%s)\" % reason, spider=spider))\n-        dfd.addBoth(lambda _: self.closing_dfds.pop(spider).callback(spider))\n-        dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n-        return dfd\n-\n     @defer.inlineCallbacks\n     def _finish_stopping_engine(self):\n         yield send_catch_log_deferred(signal=signals.engine_stopped)\n\n@@ -9,9 +9,6 @@ class Scheduler(object):\n         self.dfo = settings['SCHEDULER_ORDER'].upper() == 'DFO'\n         self.dupefilter = load_object(settings['DUPEFILTER_CLASS'])()\n \n-    def spider_is_open(self, spider):\n-        return spider in self.pending_requests\n-\n     def spider_has_pending_requests(self, spider):\n         if spider in self.pending_requests:\n             return bool(self.pending_requests[spider])\n\n@@ -22,13 +22,11 @@ def get_engine_status(engine=None):\n     ]\n     spider_tests = [\n         \"engine.spider_is_idle(spider)\",\n-        \"engine.closing.get(spider)\",\n-        \"engine.scheduler.spider_has_pending_requests(spider)\",\n+        \"engine.slots[spider].closing\",\n         \"len(engine.scheduler.pending_requests[spider])\",\n         \"len(engine.downloader.sites[spider].queue)\",\n         \"len(engine.downloader.sites[spider].active)\",\n         \"len(engine.downloader.sites[spider].transferring)\",\n-        \"engine.downloader.sites[spider].closing\",\n         \"engine.downloader.sites[spider].lastseen\",\n         \"len(engine.scraper.sites[spider].queue)\",\n         \"len(engine.scraper.sites[spider].active)\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#90b1ae694c00ddde7f338c735cee74a096b3d4f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 164 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -35,31 +35,31 @@ def get_engine_status(engine=None):\n         \"engine.scraper.sites[spider].needs_backout()\",\n     ]\n \n-    status = {'global': {}, 'spiders': {}}\n+    status = {'global': [], 'spiders': {}}\n     for test in global_tests:\n         try:\n-            status['global'][test] = eval(test)\n+            status['global'] += [(test, eval(test))]\n         except Exception, e:\n-            status['global'][test] = \"%s (exception)\" % type(e).__name__\n+            status['global'] += [(test, \"%s (exception)\" % type(e).__name__)]\n     for spider in set(engine.downloader.sites.keys() + engine.scraper.sites.keys()):\n-        x = {}\n+        x = []\n         for test in spider_tests:\n             try:\n-                x[test] = eval(test)\n+                x += [(test, eval(test))]\n             except Exception, e:\n-                x[test] = \"%s (exception)\" % type(e).__name__\n+                x += [(test, \"%s (exception)\" % type(e).__name__)]\n             status['spiders'][spider] = x\n     return status\n \n def format_engine_status(engine=None):\n     status = get_engine_status(engine)\n     s = \"Execution engine status\\n\\n\"\n-    for test, result in status['global'].items():\n+    for test, result in status['global']:\n         s += \"%-47s : %s\\n\" % (test, result)\n     s += \"\\n\"\n     for spider, tests in status['spiders'].items():\n         s += \"Spider: %s\\n\" % spider\n-        for test, result in tests.items():\n+        for test, result in tests:\n             s += \"  %-50s : %s\\n\" % (test, result)\n     return s\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#31a375bde738a5c9c0a4930075be4535ffff6baf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1696 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -271,15 +271,15 @@ class ExecutionEngine(object):\n \n         dfd = slot.close()\n \n-        dfd.addBoth(lambda _: self.scheduler.close_spider(spider))\n-        dfd.addErrback(log.err, spider=spider)\n-\n         dfd.addBoth(lambda _: self.downloader.close_spider(spider))\n         dfd.addErrback(log.err, spider=spider)\n \n         dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n         dfd.addErrback(log.err, spider=spider)\n \n+        dfd.addBoth(lambda _: self.scheduler.close_spider(spider))\n+        dfd.addErrback(log.err, spider=spider)\n+\n         dfd.addBoth(lambda _: self._cancel_next_call(spider))\n         dfd.addErrback(log.err, spider=spider)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#67213ce6735dd72ced73adcafe11f976e98ee3aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 24 | Churn Cumulative: 86 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,11 +9,11 @@ class LogFormatter(object):\n     def crawled(self, request, response, spider):\n         referer = request.headers.get('Referer')\n         flags = ' %s' % str(response.flags) if response.flags else ''\n-        return \"Crawled (%d) %s (referer: %s)%s\" % (response.status, \\\n+        return u\"Crawled (%d) %s (referer: %s)%s\" % (response.status, \\\n             request, referer, flags)\n \n     def scraped(self, item, response, spider):\n-        return \"Scraped from %s\" % response + os.linesep + str(item)\n+        return u\"Scraped from %s%s%s\" % (response, os.linesep, item)\n \n     def dropped(self, item, exception, response, spider):\n-        return \"Dropped: %s\" % unicode(exception) + os.linesep + str(item)\n+        return u\"Dropped: %s%s%s\" % (exception, os.linesep, item)\n\n@@ -2,9 +2,18 @@ import unittest\n \n from scrapy.spider import BaseSpider\n from scrapy.http import Request, Response\n+from scrapy.item import Item, Field\n from scrapy.logformatter import LogFormatter\n \n \n+class CustomItem(Item):\n+\n+    name = Field()\n+\n+    def __str__(self):\n+        return \"name: %s\" % self['name']\n+\n+\n class LoggingContribTest(unittest.TestCase):\n \n     def setUp(self):\n@@ -27,7 +36,16 @@ class LoggingContribTest(unittest.TestCase):\n         exception = Exception(u\"\\u2018\")\n         response = Response(\"http://www.example.com\")\n         lines = self.formatter.dropped(item, exception, response, self.spider).splitlines()\n+        assert all(isinstance(x, unicode) for x in lines)\n         self.assertEqual(lines, [u\"Dropped: \\u2018\", '{}'])\n \n+    def test_scraped(self):\n+        item = CustomItem()\n+        item['name'] = u'\\xa3'\n+        response = Response(\"http://www.example.com\")\n+        lines = self.formatter.scraped(item, response, self.spider).splitlines()\n+        assert all(isinstance(x, unicode) for x in lines)\n+        self.assertEqual(lines, [u\"Scraped from <200 http://www.example.com>\", u'name: \\xa3'])\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4bb409923cc3d9a9a9de194e4c50e927fb420c4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 357 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,15 +18,15 @@ class HtmlResponse(TextResponse):\n \n     _httpequiv_re = _template % ('http-equiv', 'Content-Type')\n     _content_re   = _template % ('content', r'(?P<mime>[^;]+);\\s*charset=(?P<charset>[\\w-]+)')\n-    _encoding_re  = _template % ('encoding', r'(?P<charset>[\\w-]+)')\n+    _content2_re   = _template % ('charset', r'(?P<charset>[\\w-]+)')\n \n     METATAG_RE  = re.compile(r'<meta\\s+%s\\s+%s' % (_httpequiv_re, _content_re), re.I)\n-    METATAG_RE2 = re.compile(r'<meta\\s+%s\\s+%s' % (_content_re, _httpequiv_re), re.I)\n+    METATAG2_RE  = re.compile(r'<meta\\s+%s' % _content2_re, re.I)\n \n     @memoizemethod_noargs\n     def _body_declared_encoding(self):\n         chunk = self.body[:5000]\n-        match = self.METATAG_RE.search(chunk) or self.METATAG_RE2.search(chunk)\n+        match = self.METATAG_RE.search(chunk) or self.METATAG2_RE.search(chunk)\n         return match.group('charset') if match else None\n \n \n\n@@ -260,6 +260,10 @@ class HtmlResponseTest(TextResponseTest):\n         r4 = r3.replace(body=body)\n         self._assert_response_values(r4, 'iso-8859-1', body)\n \n+    def test_html5_meta_charset(self):\n+        body = \"\"\"<html><head><meta charset=\"gb2312\" /><title>Some page</title><body>bla bla</body>\"\"\"\n+        r1 = self.response_class(\"http://www.example.com\", body=body)\n+        self._assert_response_values(r1, 'gb2312', body)\n \n \n class XmlResponseTest(TextResponseTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4fde1ef94d44418e09e2f65d55be60032184f33a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 12 | Churn Cumulative: 700 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,7 +10,7 @@ from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errba\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.misc import load_object\n from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n-from scrapy.exceptions import IgnoreRequest, DropItem\n+from scrapy.exceptions import CloseSpider, IgnoreRequest, DropItem\n from scrapy import signals\n from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n@@ -143,6 +143,10 @@ class Scraper(object):\n         return dfd.addCallback(iterate_spider_output)\n \n     def handle_spider_error(self, _failure, request, response, spider, propagated_failure=None):\n+        exc = _failure.value\n+        if isinstance(exc, CloseSpider):\n+            self.engine.close_spider(spider, exc.reason or 'cancelled')\n+            return\n         referer = request.headers.get('Referer', None)\n         msg = \"Spider error processing <%s> (referer: <%s>)\" % \\\n             (request.url, referer)\n\n@@ -20,6 +20,12 @@ class DontCloseSpider(Exception):\n     \"\"\"Request the spider not to be closed yet\"\"\"\n     pass\n \n+class CloseSpider(Exception):\n+    \"\"\"Raise this from callbacks to request the spider to be closed\"\"\"\n+\n+    def __init__(self, reason='cancelled'):\n+        self.reason = reason\n+\n # Items\n \n class DropItem(Exception):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
