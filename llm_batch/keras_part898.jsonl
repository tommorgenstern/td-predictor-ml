{"custom_id": "keras#7d7a6bb4e70c548492c4b73dab10886e2252d3c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 26 | Churn Cumulative: 3523 | Contributors (this commit): 11 | Commits (past 90d): 10 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -707,7 +707,7 @@ def map_coordinates(\n         gathered = tf.transpose(tf.gather_nd(input_arr, indices))\n \n         if fill_mode == \"constant\":\n-            all_valid = tf.reduce_all(validities)\n+            all_valid = tf.reduce_all(validities, axis=0)\n             gathered = tf.where(all_valid, gathered, fill_value)\n \n         contribution = gathered\n\n@@ -1865,6 +1865,30 @@ class ImageOpsCorrectnessTest(testing.TestCase):\n         )\n         self.assertAllClose(np.var(ref_out), np.var(out), atol=1e-2, rtol=1e-2)\n \n+    def test_map_coordinates_constant_padding(self):\n+        input_img = tf.ones((2, 2), dtype=tf.uint8)\n+        # one pixel outside of the input space around the edges\n+        grid = tf.stack(\n+            tf.meshgrid(\n+                tf.range(-1, 3, dtype=tf.float32),\n+                tf.range(-1, 3, dtype=tf.float32),\n+                indexing=\"ij\",\n+            ),\n+            axis=0,\n+        )\n+        out = backend.convert_to_numpy(\n+            kimage.map_coordinates(\n+                input_img, grid, order=0, fill_mode=\"constant\", fill_value=0\n+            )\n+        )\n+\n+        # check for ones in the middle and zeros around the edges\n+        self.assertTrue(np.all(out[:1] == 0))\n+        self.assertTrue(np.all(out[-1:] == 0))\n+        self.assertTrue(np.all(out[:, :1] == 0))\n+        self.assertTrue(np.all(out[:, -1:] == 0))\n+        self.assertTrue(np.all(out[1:3, 1:3] == 1))\n+\n \n class ImageOpsBehaviorTests(testing.TestCase):\n     def setUp(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#785c9b0b59be9d7921f48562998d4b0257e24ee9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 145 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n from keras.src import backend\n+from keras.src import tree\n from keras.src.api_export import keras_export\n from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n     BaseImagePreprocessingLayer,\n@@ -96,7 +97,12 @@ class RandomGrayscale(BaseImagePreprocessingLayer):\n         return input_shape\n \n     def compute_output_spec(self, inputs, **kwargs):\n-        return inputs\n+        return tree.map_structure(\n+            lambda x: backend.KerasTensor(\n+                x.shape, dtype=x.dtype, sparse=x.sparse\n+            ),\n+            inputs,\n+        )\n \n     def transform_bounding_boxes(self, bounding_boxes, **kwargs):\n         return bounding_boxes\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6810267d52517cc3ec752a4e5c5b8656f97a54de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 25 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -830,7 +830,7 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n     model.compile(\n         optimizer='sgd',\n         loss='binary_crossentropy',\n-        metrics=[keras.metrics.SpecificityAtSensitivity()])\n+        metrics=[keras.metrics.SpecificityAtSensitivity(sensitivity=0.3)])\n     ```\n     \"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b74cb0fffc202ce9d71d95a42d2a43629238016", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 0 | Files Changed: 10 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 11/4 | Churn Δ: 81 | Churn Cumulative: 8210 | Contributors (this commit): 29 | Commits (past 90d): 97 | Contributors (cumulative): 120 | DMM Complexity: 1.0\n\nDIFF:\n@@ -186,6 +186,7 @@ from keras.src.ops.numpy import get_item as get_item\n from keras.src.ops.numpy import greater as greater\n from keras.src.ops.numpy import greater_equal as greater_equal\n from keras.src.ops.numpy import hamming as hamming\n+from keras.src.ops.numpy import hanning as hanning\n from keras.src.ops.numpy import histogram as histogram\n from keras.src.ops.numpy import hstack as hstack\n from keras.src.ops.numpy import identity as identity\n\n@@ -76,6 +76,7 @@ from keras.src.ops.numpy import get_item as get_item\n from keras.src.ops.numpy import greater as greater\n from keras.src.ops.numpy import greater_equal as greater_equal\n from keras.src.ops.numpy import hamming as hamming\n+from keras.src.ops.numpy import hanning as hanning\n from keras.src.ops.numpy import histogram as histogram\n from keras.src.ops.numpy import hstack as hstack\n from keras.src.ops.numpy import identity as identity\n\n@@ -186,6 +186,7 @@ from keras.src.ops.numpy import get_item as get_item\n from keras.src.ops.numpy import greater as greater\n from keras.src.ops.numpy import greater_equal as greater_equal\n from keras.src.ops.numpy import hamming as hamming\n+from keras.src.ops.numpy import hanning as hanning\n from keras.src.ops.numpy import histogram as histogram\n from keras.src.ops.numpy import hstack as hstack\n from keras.src.ops.numpy import identity as identity\n\n@@ -76,6 +76,7 @@ from keras.src.ops.numpy import get_item as get_item\n from keras.src.ops.numpy import greater as greater\n from keras.src.ops.numpy import greater_equal as greater_equal\n from keras.src.ops.numpy import hamming as hamming\n+from keras.src.ops.numpy import hanning as hanning\n from keras.src.ops.numpy import histogram as histogram\n from keras.src.ops.numpy import hstack as hstack\n from keras.src.ops.numpy import identity as identity\n\n@@ -47,6 +47,11 @@ def hamming(x):\n     return jnp.hamming(x)\n \n \n+def hanning(x):\n+    x = convert_to_tensor(x)\n+    return jnp.hanning(x)\n+\n+\n def kaiser(x, beta):\n     x = convert_to_tensor(x)\n     return jnp.kaiser(x, beta)\n\n@@ -315,6 +315,11 @@ def hamming(x):\n     return np.hamming(x).astype(config.floatx())\n \n \n+def hanning(x):\n+    x = convert_to_tensor(x)\n+    return np.hanning(x).astype(config.floatx())\n+\n+\n def kaiser(x, beta):\n     x = convert_to_tensor(x)\n     return np.kaiser(x, beta).astype(config.floatx())\n\n@@ -151,6 +151,11 @@ def hamming(x):\n     return tf.signal.hamming_window(x, periodic=False)\n \n \n+def hanning(x):\n+    x = convert_to_tensor(x, dtype=tf.int32)\n+    return tf.signal.hann_window(x, periodic=False)\n+\n+\n def kaiser(x, beta):\n     x = convert_to_tensor(x, dtype=tf.int32)\n     return tf.signal.kaiser_window(x, beta=beta)\n\n@@ -440,6 +440,11 @@ def hamming(x):\n     return torch.signal.windows.hamming(x)\n \n \n+def hanning(x):\n+    x = convert_to_tensor(x)\n+    return torch.signal.windows.hann(x)\n+\n+\n def kaiser(x, beta):\n     x = convert_to_tensor(x)\n     return torch.signal.windows.kaiser(x, beta=beta)\n\n@@ -1270,6 +1270,37 @@ def hamming(x):\n     return backend.numpy.hamming(x)\n \n \n+class Hanning(Operation):\n+    def call(self, x):\n+        return backend.numpy.hanning(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=backend.floatx())\n+\n+\n+@keras_export([\"keras.ops.hanning\", \"keras.ops.numpy.hanning\"])\n+def hanning(x):\n+    \"\"\"Hanning window function.\n+\n+    The Hanning window is defined as:\n+    `w[n] = 0.5 - 0.5 * cos(2 * pi * n / (N - 1))` for `0 <= n <= N - 1`.\n+\n+    Args:\n+        x: Scalar or 1D Tensor. The window length.\n+\n+    Returns:\n+        A 1D tensor containing the Hanning window values.\n+\n+    Example:\n+    >>> x = keras.ops.convert_to_tensor(5)\n+    >>> keras.ops.hanning(x)\n+    array([0. , 0.5, 1. , 0.5, 0. ], dtype=float32)\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Hanning().symbolic_call(x)\n+    return backend.numpy.hanning(x)\n+\n+\n class Kaiser(Operation):\n     def __init__(self, beta):\n         super().__init__()\n\n@@ -1232,6 +1232,10 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         x = np.random.randint(1, 100 + 1)\n         self.assertEqual(knp.hamming(x).shape[0], x)\n \n+    def test_hanning(self):\n+        x = np.random.randint(1, 100 + 1)\n+        self.assertEqual(knp.hanning(x).shape[0], x)\n+\n     def test_kaiser(self):\n         x = np.random.randint(1, 100 + 1)\n         beta = float(np.random.randint(10, 20 + 1))\n@@ -3625,6 +3629,12 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n \n         self.assertAllClose(knp.Hamming()(x), np.hamming(x))\n \n+    def test_hanning(self):\n+        x = np.random.randint(1, 100 + 1)\n+        self.assertAllClose(knp.hanning(x), np.hanning(x))\n+\n+        self.assertAllClose(knp.Hanning()(x), np.hanning(x))\n+\n     def test_kaiser(self):\n         x = np.random.randint(1, 100 + 1)\n         beta = float(np.random.randint(10, 20 + 1))\n@@ -5639,6 +5649,22 @@ class NumpyDtypeTest(testing.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_hanning(self, dtype):\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((), dtype=dtype)\n+        x_jax = jnp.ones((), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnp.hanning(x_jax).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knp.hanning(x).dtype), expected_dtype\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knp.Hanning().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_kaiser(self, dtype):\n         import jax.numpy as jnp\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e38e0452ea613a4c626b3485eb78c3f09a12615", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 153 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,4 @@\n from keras.src import backend\n-from keras.src import tree\n from keras.src.api_export import keras_export\n from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n     BaseImagePreprocessingLayer,\n@@ -97,11 +96,8 @@ class RandomGrayscale(BaseImagePreprocessingLayer):\n         return input_shape\n \n     def compute_output_spec(self, inputs, **kwargs):\n-        return tree.map_structure(\n-            lambda x: backend.KerasTensor(\n-                x.shape, dtype=x.dtype, sparse=x.sparse\n-            ),\n-            inputs,\n+        return backend.KerasTensor(\n+            inputs.shape, dtype=inputs.dtype, sparse=inputs.sparse\n         )\n \n     def transform_bounding_boxes(self, bounding_boxes, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#240d4f4a641eca0bc265060e6b9517f06acbdd30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 12/8 | Churn Δ: 27 | Churn Cumulative: 1179 | Contributors (this commit): 14 | Commits (past 90d): 5 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,6 +14,7 @@ from keras.src.backend.common.keras_tensor import is_keras_tensor\n from keras.src.ops import operation_utils\n from keras.src.ops.operation import Operation\n from keras.src.ops.operation_utils import reduce_shape\n+from keras.src.utils.python_utils import is_continuous_axis\n \n \n class Relu(Operation):\n@@ -2772,6 +2773,14 @@ def rms_normalization(x, scale=1, axis=-1, epsilon=None):\n \n \n def _rms_normalization(x, scale=1, axis=-1, epsilon=None):\n+    if backend.backend() == \"torch\" and is_continuous_axis(axis):\n+        import torch.nn.functional as F\n+\n+        if isinstance(axis, (tuple, list)):\n+            normalized_shape = tuple([x.shape[dim] for dim in axis])\n+        else:\n+            normalized_shape = x.shape[axis]\n+        return F.rms_norm(x, normalized_shape, scale, epsilon)\n     x = backend.convert_to_tensor(x)\n     if len(x.shape) == 0:\n         x = backend.numpy.expand_dims(x, axis=0)\n\n@@ -5,6 +5,24 @@ import os\n import types as python_types\n \n \n+def is_continuous_axis(axis):\n+    # Used to determine whether the dimensions in an axis are continuous\n+    if isinstance(axis, int) or len(axis) == 1:\n+        return True\n+    positive_order_flag = True\n+    for i in range(len(axis) - 1):\n+        if axis[i + 1] - axis[i] != 1:\n+            positive_order_flag = False\n+            break\n+\n+    negative_order_flag = True\n+    for i in range(len(axis) - 1):\n+        if axis[i + 1] - axis[i] != 1:\n+            negative_order_flag = False\n+            break\n+    return positive_order_flag or negative_order_flag\n+\n+\n def default(method):\n     \"\"\"Decorates a method to detect overrides in subclasses.\"\"\"\n     method._is_default = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1b4431d29bd4e3dd608854eabd112f14914a1e6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 545 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -152,7 +152,16 @@ def build():\n     try:\n         os.chdir(build_dir)\n         open(build_api_init_fname, \"w\").close()\n-        namex.generate_api_files(\"keras\", code_directory=\"src\")\n+        namex.generate_api_files(\n+            \"keras\",\n+            code_directory=\"src\",\n+            exclude_directories=[\n+                os.path.join(\"src\", \"backend\", \"jax\"),\n+                os.path.join(\"src\", \"backend\", \"openvino\"),\n+                os.path.join(\"src\", \"backend\", \"tensorflow\"),\n+                os.path.join(\"src\", \"backend\", \"torch\"),\n+            ],\n+        )\n         # Add __version__ to `api/`.\n         export_version_string(build_api_init_fname)\n         # Creates `_tf_keras` with full keras API\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#86638c23fe7dbed05388272ca6a9d5268f015f62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 153 | Lines Deleted: 50 | Files Changed: 7 | Hunks: 9 | Methods Changed: 9 | Complexity Δ (Sum/Max): 12/19 | Churn Δ: 203 | Churn Cumulative: 4519 | Contributors (this commit): 26 | Commits (past 90d): 41 | Contributors (cumulative): 91 | DMM Complexity: 0.3793103448275862\n\nDIFF:\n@@ -83,6 +83,7 @@ from keras.src.ops.nn import hard_sigmoid as hard_sigmoid\n from keras.src.ops.nn import hard_silu as hard_silu\n from keras.src.ops.nn import hard_silu as hard_swish\n from keras.src.ops.nn import hard_tanh as hard_tanh\n+from keras.src.ops.nn import layer_normalization as layer_normalization\n from keras.src.ops.nn import leaky_relu as leaky_relu\n from keras.src.ops.nn import log_sigmoid as log_sigmoid\n from keras.src.ops.nn import log_softmax as log_softmax\n\n@@ -25,6 +25,7 @@ from keras.src.ops.nn import hard_sigmoid as hard_sigmoid\n from keras.src.ops.nn import hard_silu as hard_silu\n from keras.src.ops.nn import hard_silu as hard_swish\n from keras.src.ops.nn import hard_tanh as hard_tanh\n+from keras.src.ops.nn import layer_normalization as layer_normalization\n from keras.src.ops.nn import leaky_relu as leaky_relu\n from keras.src.ops.nn import log_sigmoid as log_sigmoid\n from keras.src.ops.nn import log_softmax as log_softmax\n\n@@ -83,6 +83,7 @@ from keras.src.ops.nn import hard_sigmoid as hard_sigmoid\n from keras.src.ops.nn import hard_silu as hard_silu\n from keras.src.ops.nn import hard_silu as hard_swish\n from keras.src.ops.nn import hard_tanh as hard_tanh\n+from keras.src.ops.nn import layer_normalization as layer_normalization\n from keras.src.ops.nn import leaky_relu as leaky_relu\n from keras.src.ops.nn import log_sigmoid as log_sigmoid\n from keras.src.ops.nn import log_softmax as log_softmax\n\n@@ -25,6 +25,7 @@ from keras.src.ops.nn import hard_sigmoid as hard_sigmoid\n from keras.src.ops.nn import hard_silu as hard_silu\n from keras.src.ops.nn import hard_silu as hard_swish\n from keras.src.ops.nn import hard_tanh as hard_tanh\n+from keras.src.ops.nn import layer_normalization as layer_normalization\n from keras.src.ops.nn import leaky_relu as leaky_relu\n from keras.src.ops.nn import log_sigmoid as log_sigmoid\n from keras.src.ops.nn import log_softmax as log_softmax\n\n@@ -1,4 +1,3 @@\n-from keras.src import backend\n from keras.src import constraints\n from keras.src import initializers\n from keras.src import ops\n@@ -180,56 +179,14 @@ class LayerNormalization(Layer):\n             self.beta = None\n \n     def call(self, inputs):\n-        # Compute the axes along which to reduce the mean / variance\n-        input_shape = inputs.shape\n-        ndims = len(input_shape)\n-\n-        # Broadcasting only necessary for norm when the axis is not just\n-        # the last dimension\n-        broadcast_shape = [1] * ndims\n-        for dim in self.axis:\n-            broadcast_shape[dim] = input_shape[dim]\n-\n-        def _broadcast(v):\n-            if (\n-                v is not None\n-                and len(v.shape) != ndims\n-                and self.axis != [ndims - 1]\n-            ):\n-                return ops.reshape(v, broadcast_shape)\n-            return v\n-\n-        compute_dtype = backend.result_type(inputs.dtype, \"float32\")\n-        # LN is prone to overflow with float16/bfloat16 inputs, so we upcast to\n-        # float32 for the subsequent computations.\n-        inputs = ops.cast(inputs, compute_dtype)\n-\n-        if self.rms_scaling:\n-            # Calculate outputs with only variance and gamma if rms scaling\n-            # is enabled\n-            # Calculate the variance along self.axis (layer activations).\n-            variance = ops.var(inputs, axis=self.axis, keepdims=True)\n-            inv = ops.rsqrt(variance + self.epsilon)\n-\n-            outputs = (\n-                inputs * inv * ops.cast(_broadcast(self.gamma), inputs.dtype)\n+        outputs = ops.layer_normalization(\n+            inputs,\n+            self.gamma,\n+            self.beta,\n+            self.axis,\n+            self.epsilon,\n+            self.rms_scaling,\n         )\n-        else:\n-            # Calculate the mean & variance along self.axis (layer activations).\n-            mean, variance = ops.moments(inputs, axes=self.axis, keepdims=True)\n-            gamma, beta = _broadcast(self.gamma), _broadcast(self.beta)\n-\n-            inv = ops.rsqrt(variance + self.epsilon)\n-            if gamma is not None:\n-                gamma = ops.cast(gamma, inputs.dtype)\n-                inv = inv * gamma\n-\n-            res = -mean * inv\n-            if beta is not None:\n-                beta = ops.cast(beta, inputs.dtype)\n-                res = res + beta\n-\n-            outputs = inputs * inv + res\n         return ops.cast(outputs, self.compute_dtype)\n \n     def compute_output_shape(self, input_shape):\n\n@@ -2799,6 +2799,142 @@ def _rms_normalization(x, scale=1, axis=-1, epsilon=None):\n     return (x * rrms) * scale\n \n \n+class LayerNorm(Operation):\n+    def __init__(\n+        self, gamma=None, beta=None, axis=-1, epsilon=None, rms_scaling=False\n+    ):\n+        super().__init__()\n+        self.axis = axis\n+        self.gamma = gamma\n+        self.beta = beta\n+        self.epsilon = epsilon\n+        self.rms_scaling = rms_scaling\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(shape=x.shape)\n+\n+    def call(self, x):\n+        return _rms_normalization(\n+            x,\n+            gamma=self.gamma,\n+            beta=self.beta,\n+            axis=self.axis,\n+            epsilon=self.epsilon,\n+            rms_scaling=self.rms_scaling,\n+        )\n+\n+\n+@keras_export(\n+    [\n+        \"keras.ops.layer_normalization\",\n+        \"keras.ops.nn.layer_normalization\",\n+    ]\n+)\n+def layer_normalization(\n+    x, gamma=None, beta=None, axis=-1, epsilon=None, rms_scaling=False\n+):\n+    \"\"\"Layer normalization layer (Ba et al., 2016).\n+\n+    Normalize the activations of the previous layer for each given example in a\n+    batch independently, rather than across a batch like Batch Normalization.\n+    i.e. applies a transformation that maintains the mean activation within each\n+    example close to 0 and the activation standard deviation close to 1.\n+    Args:\n+        x: Input tensor.\n+        axis: The axis or axes along which to perform normalization.\n+            Default to -1.\n+        gamma: Optional scaling factor for the normalization.\n+        beta: Optional add offset for the normalized tensor.\n+        rms_scaling:This is an approximate and faster\n+            approach that avoids ever computing the mean of the input. Note that\n+            this *isn't* equivalent to the computation that rms_normalization\n+        epsilon: A lower bound value for the norm.\n+            Defaults to `backend.epsilon()`.\n+\n+    Returns:\n+        The normalized array.\n+    >>> x = ops.arange(5,dtype = \"float32\")\n+    >>> x_norm = ops.layer_normalization(x)\n+    >>> print(x_norm)\n+    array([-1.4142135 , -0.70710677,  0.,  0.7071067 ,  1.4142135 ])\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return LayerNorm(\n+            gamma=gamma,\n+            beta=beta,\n+            axis=axis,\n+            epsilon=epsilon,\n+            rms_scaling=rms_scaling,\n+        ).symbolic_call(x)\n+    return _layer_normalization(\n+        x,\n+        gamma=gamma,\n+        beta=beta,\n+        axis=axis,\n+        epsilon=epsilon,\n+        rms_scaling=rms_scaling,\n+    )\n+\n+\n+def _layer_normalization(\n+    inputs, gamma=None, beta=None, axis=-1, epsilon=None, rms_scaling=False\n+):\n+    compute_dtype = backend.result_type(inputs.dtype, \"float32\")\n+    # LN is prone to overflow with float16/bfloat16 inputs, so we upcast to\n+    # float32 for the subsequent computations.\n+    x = backend.cast(inputs, compute_dtype)\n+    # Compute the axes along which to reduce the mean / variance\n+    input_shape = x.shape\n+    ndims = len(input_shape)\n+\n+    # Broadcasting only necessary for norm when the axis is not just\n+    # the last dimension\n+    broadcast_shape = [1] * ndims\n+    if isinstance(axis, int):\n+        axis = [axis]\n+    for dim in axis:\n+        broadcast_shape[dim] = input_shape[dim]\n+\n+    def _broadcast(v):\n+        if v is not None and len(v.shape) != ndims and axis != [ndims - 1]:\n+            return backend.numpy.reshape(v, broadcast_shape)\n+        return v\n+\n+    if epsilon is None:\n+        epsilon = backend.epsilon()\n+\n+    if rms_scaling:\n+        # Calculate outputs with only variance and gamma if rms scaling\n+        # is enabled\n+        # Calculate the variance along self.axis (layer activations).\n+        variance = backend.numpy.var(x, axis=axis, keepdims=True)\n+        inv = backend.math.rsqrt(variance + epsilon)\n+\n+        outputs = x * inv * backend.cast(_broadcast(gamma), x.dtype)\n+    elif backend.config.backend() == \"torch\" and is_continuous_axis(axis):\n+        # when using torch backend,use kernel to improve performance\n+        import torch.nn.functional as F\n+\n+        normalized_shape = tuple([input_shape[dim] for dim in axis])\n+        outputs = F.layer_norm(x, normalized_shape, gamma, beta, epsilon)\n+    else:\n+        # Calculate the mean & variance along self.axis (layer activations).\n+        mean, variance = moments(x, axes=axis, keepdims=True)\n+        gamma, beta = _broadcast(gamma), _broadcast(beta)\n+        inv = backend.math.rsqrt(variance + epsilon)\n+        if gamma is not None:\n+            gamma = backend.cast(gamma, x.dtype)\n+            inv = inv * gamma\n+\n+        res = -mean * inv\n+        if beta is not None:\n+            beta = backend.cast(beta, x.dtype)\n+            res = res + beta\n+\n+        outputs = x * inv + res\n+    return backend.cast(outputs, inputs.dtype)\n+\n+\n class Polar(Operation):\n     def __init__(self):\n         super().__init__()\n\n@@ -3178,3 +3178,9 @@ class NNOpsBehaviorTest(testing.TestCase):\n         self.assertEqual(\n             knn.rms_normalization(x, (None, 2, 3)).shape, (None, 2, 3)\n         )\n+\n+    def test_layer_normalization(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(\n+            knn.layer_normalization(x, (None, 2, 3)).shape, (None, 2, 3)\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#81821e02486886436d10bb59bdfdf1715ebcca1a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 171 | Files Changed: 1 | Hunks: 17 | Methods Changed: 3 | Complexity Δ (Sum/Max): -27/0 | Churn Δ: 198 | Churn Cumulative: 1341 | Contributors (this commit): 14 | Commits (past 90d): 6 | Contributors (cumulative): 14 | DMM Complexity: 0.9722222222222222\n\nDIFF:\n@@ -1126,17 +1126,16 @@ def wrap_flash_attention(\n     decoder_segment_ids,\n     custom_mask=None,\n     attn_logits_soft_cap=None,\n-    head_shards=1,\n-    q_seq_shards=1,\n ):\n     if decoder_segment_ids is not None:\n         assert query.shape[2] == decoder_segment_ids.q.shape[1], (\n-            \"Sharding along sequence dimension not allowed\"\n-            \" in TPU kernel attention\"\n+            \"Sharding along sequence dimension not allowed in tpu kernel \"\n+            \"attention\"\n         )\n \n     if custom_mask is not None:\n         mask = splash_attention_mask.NumpyMask(array=custom_mask)\n+\n     else:\n         mask = splash_attention_mask.CausalMask(\n             shape=(query.shape[2], query.shape[2])\n@@ -1148,8 +1147,8 @@ def wrap_flash_attention(\n     )\n     splash_kernel = splash_attention_kernel.make_splash_mha(\n         mask=multi_head_mask,\n-        head_shards=head_shards,\n-        q_seq_shards=q_seq_shards,\n+        head_shards=1,\n+        q_seq_shards=1,\n         attn_logits_soft_cap=attn_logits_soft_cap,\n     )\n \n@@ -1169,38 +1168,6 @@ def dot_product_attention(\n     flash_attention=None,\n     attn_logits_soft_cap=None,\n ):\n-    \"\"\"Computes dot-product attention given query, key, and value.\n-\n-    This is the core computation of attention that is used in transformers.\n-    For TPU platforms, flash attention optimizations are automatically applied\n-    when possible, and sharding parameters are inferred from the layout map\n-    in the current distribution context.\n-\n-    Args:\n-        query: Queries with shape `[batch, time, heads,\n-            depth_k]`.\n-        key: Keys with shape `[batch, time, heads,\n-            depth_k]`.\n-        value: Values with shape `[batch, time, heads,\n-            depth_v]`.\n-        bias: Optional bias with shape broadcastable to\n-            `[batch, heads, dest_time, source_time]`.\n-        mask: Optional mask with shape broadcastable to\n-            `[batch, heads, dest_time, source_time]`.\n-        scale: Float. Optional scale that is applied to the attention\n-            computation.\n-        is_causal: Boolean. Specifying whether causal masking is applied.\n-        flash_attention: Boolean. Whether to use flash attention optimization\n-            for increased performance. Default to None, which means it will\n-            be auto-determined based on the platform, input shapes and\n-            compatibility.\n-        attn_logits_soft_cap: Float. Optional float to softly cap attention\n-            logits to avoid numerical stability issues. Applied as:\n-            `logits = logits / (1.0 + abs(logits) / attn_logits_soft_cap)`.\n-\n-    Returns:\n-        JAX Array of shape `[batch, time, heads, depth_v]`.\n-    \"\"\"\n     query = convert_to_tensor(query)\n     key = convert_to_tensor(key)\n     value = convert_to_tensor(value)\n@@ -1210,131 +1177,37 @@ def dot_product_attention(\n             f\"Received: query.shape={query.shape}, key.shape={key.shape}, \"\n             f\"value.shape={value.shape}.\"\n         )\n-\n-    # Check platform\n-    platform = jax.devices()[0].platform\n-    is_tpu = platform == \"tpu\"\n-\n-    # Get sharding parameters from distribution context\n-    head_shards = 1\n-    q_seq_shards = 1\n-\n-    if is_tpu:\n-        try:\n-            from keras.src.distribution.distribution_lib import ModelParallel\n-            from keras.src.distribution.distribution_lib import (\n-                distribution as get_dist,\n-            )\n-\n-            # Get current distribution if available\n-            dist = get_dist()\n-            if dist and isinstance(dist, ModelParallel):\n-                mesh = dist.device_mesh\n-                if \"model\" in mesh.axis_names:\n-                    model_dim_index = mesh.axis_names.index(\"model\")\n-                    # Set head_shards based on the model dimension of the mesh\n-                    head_shards = mesh.shape[model_dim_index]\n-                    # Typically keep q_seq_shards=1 for best performance\n-                    q_seq_shards = 1\n-        except (ImportError, ValueError, AttributeError):\n-            # Use default values if detection fails\n-            head_shards = 1\n-            q_seq_shards = 1\n-\n-    # Check if inputs use partial sharding (not fully replicated)\n-    # Flash attention works well with fully replicated tensors on all platforms\n-    # but may have issues with certain partial sharding patterns on non-TPU\n-    # platforms\n-    partially_sharded_inputs = any(\n-        hasattr(t, \"sharding\") and not t.sharding.is_fully_replicated\n-        for t in (query, key, value)\n-    )\n-\n-    # Determine flash attention compatibility\n     if flash_attention is None:\n-        # Auto-detect flash attention availability\n-        if is_tpu:\n-            # TPUs have specialized hardware for attention that works with any\n-            # sharding pattern\n-            flash_attention = True\n-        else:\n-            # For GPU/CPU with partially sharded inputs, we need\n-            # multiple devices to efficiently handle the sharding\n-            if partially_sharded_inputs and len(jax.devices()) <= 1:\n-                flash_attention = False\n-            else:\n-                flash_attention = _can_use_flash_attention(\n-                    query, key, value, bias\n-                )\n-    elif flash_attention is True and not is_tpu:\n-        # If flash attention is explicitly requested, validate compatibility\n-        # Skip validation for TPU as it has specialized hardware support\n-        try:\n+        flash_attention = _can_use_flash_attention(query, key, value, bias)\n+    elif flash_attention is True:\n+        # Use `raise_error=True` to provide more details if the inputs failed to\n+        # use flash attention\n         _can_use_flash_attention(query, key, value, bias, raise_error=True)\n-        except Exception:\n-            # Only disable flash attention on non-TPU platforms\n-            # if validation fails\n-            flash_attention = False\n \n-    # TPU-specific flash attention path\n-    if is_tpu and flash_attention:\n-        # Transpose to ('batch', 'heads', 'length', 'head_dim')\n-        query_tpu_layout = jnp.transpose(query, axes=(0, 2, 1, 3))\n-        key_tpu_layout = jnp.transpose(key, axes=(0, 2, 1, 3))\n-        value_tpu_layout = jnp.transpose(value, axes=(0, 2, 1, 3))\n+    if jax.devices()[0].platform == \"tpu\":\n+        # Transpose to ('batch', 'heads', 'length', 'kv')\n+        query = jnp.transpose(query, axes=(0, 2, 1, 3))\n+        key = jnp.transpose(key, axes=(0, 2, 1, 3))\n+        value = jnp.transpose(value, axes=(0, 2, 1, 3))\n+        B, H, S, KV = query.shape\n \n-        bs, num_heads, q_len, head_dim = query_tpu_layout.shape\n-\n-        # Apply scale to query if provided\n-        if scale is not None:\n-            # TPU kernel applies 1/sqrt(head_dim) internally, to achieve\n-            # overall QK^T * scale, scale query by (scale * sqrt(head_dim))\n-            query_tpu_layout = query_tpu_layout * (scale * math.sqrt(head_dim))\n-\n-        # Create segment IDs for Splash Attention (for packing/batching)\n-        segment_ids = jnp.zeros([bs, q_len], dtype=jnp.int32)\n+        segment_ids = jnp.ones([B, S])\n+        # {token_ids, padding_mask, segment_ids} enable packing\n+        out = wrap_flash_attention(\n+            query,\n+            key,\n+            value,\n             decoder_segment_ids=splash_attention_kernel.SegmentIds(\n-            q=segment_ids, kv=segment_ids\n-        )\n-\n-        # Process mask for Splash Attention\n-        custom_mask = None\n-        if mask is not None:\n-            mask_bool = mask.astype(\"bool\") if mask.dtype != jnp.bool_ else mask\n-\n-            if mask_bool.ndim == 3 and mask_bool.shape[0] == bs:\n-                custom_mask = mask_bool[0]\n-            elif mask_bool.ndim == 4 and mask_bool.shape[0] == bs:\n-                custom_mask = mask_bool[0, 0]\n-\n-            if is_causal and custom_mask is not None:\n-                causal_mask = jnp.tril(\n-                    jnp.ones((q_len, q_len), dtype=jnp.bool_)\n-                )\n-                custom_mask = jnp.logical_and(custom_mask, causal_mask)\n-\n-        if custom_mask is None and is_causal:\n-            custom_mask = jnp.tril(jnp.ones((q_len, q_len), dtype=jnp.bool_))\n-\n-        try:\n-            output = wrap_flash_attention(\n-                query_tpu_layout,\n-                key_tpu_layout,\n-                value_tpu_layout,\n-                decoder_segment_ids=decoder_segment_ids,\n-                custom_mask=custom_mask,\n+                segment_ids, segment_ids\n+            ),\n+            custom_mask=mask,\n             attn_logits_soft_cap=attn_logits_soft_cap,\n-                head_shards=head_shards,\n-                q_seq_shards=q_seq_shards,\n         )\n-            # Transpose output back to Keras layout\n-            return jnp.transpose(output, axes=(0, 2, 1, 3))\n-        except Exception:\n-            flash_attention = False\n+        out = jnp.transpose(out, axes=(0, 2, 1, 3))\n+        return out\n \n-    # JAX native dot_product_attention for GPU or fallback for TPU\n+    # `dot_product_attention` is only available in jax>=0.4.31\n     if hasattr(jax.nn, \"dot_product_attention\"):\n-        try:\n         return jax.nn.dot_product_attention(\n             query,\n             key,\n@@ -1345,20 +1218,6 @@ def dot_product_attention(\n             is_causal=is_causal,\n             implementation=\"cudnn\" if flash_attention else \"xla\",\n         )\n-        except Exception:\n-            # If flash attention fails, fall back to XLA implementation\n-            if flash_attention:\n-                return jax.nn.dot_product_attention(\n-                    query,\n-                    key,\n-                    value,\n-                    bias=bias,\n-                    mask=mask,\n-                    scale=scale,\n-                    is_causal=is_causal,\n-                    implementation=\"xla\",\n-                )\n-            raise\n \n     if flash_attention:\n         raise RuntimeError(\n@@ -1369,9 +1228,6 @@ def dot_product_attention(\n     # Ref: jax.nn.dot_product_attention\n     # https://github.com/jax-ml/jax/blob/jax-v0.4.33/jax/_src/nn/functions.py#L886\n     # Not support `query_seq_lengths` and `key_value_seq_lengths` args\n-\n-    # Fallback to custom XLA implementation\n-    # This is the reference implementation from jax.nn.dot_product_attention\n     output_shape = query.shape\n     _, _, K, H = key.shape\n     scale = (1.0 / jnp.sqrt(H)) if scale is None else scale\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4cb5e38ff87b2eb43248d0792ca81ac19d249f97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 14/9 | Churn Δ: 45 | Churn Cumulative: 778 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -152,8 +152,24 @@ class PyDataset:\n         Returns:\n             A batch\n         \"\"\"\n+        del index\n         raise NotImplementedError\n \n+    def __iter__(self):\n+        index_range = None\n+        try:\n+            num_batches = self.num_batches\n+            if num_batches is not None:\n+                index_range = range(num_batches)\n+        except NotImplementedError:\n+            pass\n+\n+        if index_range is None:\n+            index_range = itertools.count()\n+\n+        for index in index_range:\n+            yield self[index]\n+\n     @property\n     def num_batches(self):\n         \"\"\"Number of batches in the PyDataset.\n\n@@ -422,3 +422,32 @@ class PyDatasetAdapterTest(testing.TestCase):\n             expected_exception_class, \"Expected exception\"\n         ):\n             next(it)\n+\n+    def test_iterate_finite(self):\n+        py_dataset = ExamplePyDataset(\n+            np.ones((6, 11), dtype=\"int32\"),\n+            np.zeros((6, 11), dtype=\"int32\"),\n+            batch_size=2,\n+        )\n+        batches = [batch for batch in py_dataset]\n+        self.assertLen(batches, 3)\n+\n+    def test_iterate_infinite_with_none_num_batches(self):\n+        py_dataset = ExamplePyDataset(\n+            np.ones((6, 11), dtype=\"int32\"),\n+            np.zeros((6, 11), dtype=\"int32\"),\n+            batch_size=2,\n+            infinite=True,\n+        )\n+        for index, _ in enumerate(py_dataset):\n+            if index >= 10:\n+                break\n+\n+    def test_iterate_infinite_with_no_len(self):\n+        class NoLenDataset(py_dataset_adapter.PyDataset):\n+            def __getitem__(self, idx):\n+                yield np.ones((2, 11), dtype=\"int32\")\n+\n+        for index, _ in enumerate(NoLenDataset()):\n+            if index >= 10:\n+                break\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6d70f49a9eaa4b297d584f4fa2e1ea9d65e20861", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 9 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -87,6 +87,9 @@ class Progbar:\n                 # called, which will cause 'current' and 'self._seen_so_far' to\n                 # have the same value. Force the minimal value to 1 here,\n                 # otherwise stateful_metric will be 0s.\n+                if finalize:\n+                    self._values[k] = [v, 1]\n+                else:\n                     value_base = max(current - self._seen_so_far, 1)\n                     if k not in self._values:\n                         self._values[k] = [v * value_base, value_base]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#de9cf2595b21b2166ce4997d2f6be8b0e4223762", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 15 | Churn Cumulative: 23 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -37,6 +37,7 @@ class CSVLogger(Callback):\n         self.writer = None\n         self.keys = None\n         self.append_header = True\n+        self.csv_file = None\n \n     def on_train_begin(self, logs=None):\n         if self.append:\n@@ -46,7 +47,13 @@ class CSVLogger(Callback):\n             mode = \"a\"\n         else:\n             mode = \"w\"\n+        # ensure csv_file is None or closed before reassigning\n+        if self.csv_file and not self.csv_file.closed:\n+            self.csv_file.close()\n         self.csv_file = file_utils.File(self.filename, mode)\n+        # Reset writer and keys\n+        self.writer = None\n+        self.keys = None\n \n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}\n@@ -65,14 +72,13 @@ class CSVLogger(Callback):\n \n         if self.keys is None:\n             self.keys = sorted(logs.keys())\n-            # When validation_freq > 1, `val_` keys are not in first epoch logs\n-            # Add the `val_` keys so that its part of the fieldnames of writer.\n+\n             val_keys_found = False\n             for key in self.keys:\n                 if key.startswith(\"val_\"):\n                     val_keys_found = True\n                     break\n-            if not val_keys_found:\n+            if not val_keys_found and self.keys:\n                 self.keys.extend([\"val_\" + k for k in self.keys])\n \n         if not self.writer:\n@@ -80,7 +86,7 @@ class CSVLogger(Callback):\n             class CustomDialect(csv.excel):\n                 delimiter = self.sep\n \n-            fieldnames = [\"epoch\"] + self.keys\n+            fieldnames = [\"epoch\"] + (self.keys or [])\n \n             self.writer = csv.DictWriter(\n                 self.csv_file, fieldnames=fieldnames, dialect=CustomDialect\n@@ -96,5 +102,6 @@ class CSVLogger(Callback):\n         self.csv_file.flush()\n \n     def on_train_end(self, logs=None):\n+        if self.csv_file and not self.csv_file.closed:\n             self.csv_file.close()\n         self.writer = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#802299e20563c7417bda4c70e2e6f5d8b18139d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 16 | Churn Cumulative: 859 | Contributors (this commit): 13 | Commits (past 90d): 9 | Contributors (cumulative): 22 | DMM Complexity: 0.0\n\nDIFF:\n@@ -621,6 +621,8 @@ def functional_from_config(cls, config, custom_objects=None):\n \n     input_tensors = map_tensors(functional_config[\"input_layers\"])\n     output_tensors = map_tensors(functional_config[\"output_layers\"])\n+    if isinstance(input_tensors, list) and len(input_tensors) == 1:\n+        input_tensors = input_tensors[0]\n     if isinstance(output_tensors, list) and len(output_tensors) == 1:\n         output_tensors = output_tensors[0]\n \n\n@@ -17,7 +17,6 @@ from keras.src.layers.input_spec import InputSpec\n from keras.src.models import Functional\n from keras.src.models import Model\n from keras.src.models import Sequential\n-from keras.src.models.model import model_from_json\n \n \n class FunctionalTest(testing.TestCase):\n@@ -274,19 +273,6 @@ class FunctionalTest(testing.TestCase):\n         out_val = model_restored(Input(shape=(3,), batch_size=2))\n         self.assertIsInstance(out_val, out_type)\n \n-    def test_restored_nested_input(self):\n-        input_a = Input(shape=(3,), batch_size=2, name=\"input_a\")\n-        x = layers.Dense(5)(input_a)\n-        outputs = layers.Dense(4)(x)\n-        model = Functional([[input_a]], outputs)\n-\n-        # Serialize and deserialize the model\n-        json_config = model.to_json()\n-        restored_json_config = model_from_json(json_config).to_json()\n-\n-        # Check that the serialized model is the same as the original\n-        self.assertEqual(json_config, restored_json_config)\n-\n     @pytest.mark.requires_trainable_backend\n     def test_layer_getters(self):\n         # Test mixing ops and layers\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#08b0fd9e5fe28eeed6f0f159b27557cf8d8725d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 85 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 6 | Methods Changed: 9 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 85 | Churn Cumulative: 123 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -52,6 +52,14 @@ class AdamWTest(testing.TestCase):\n         self.assertAlmostEqual(var2.numpy(), 2.0, decimal=6)\n         self.assertAlmostEqual(var3.numpy(), 2.0, decimal=6)\n \n+    def test_weight_decay_is_none(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Argument `weight_decay` must be a float. \"\n+            \"Received: weight_decay=None\",\n+        ):\n+            AdamW(learning_rate=1.0, weight_decay=None)\n+\n     def test_correctness_with_golden(self):\n         optimizer = AdamW(learning_rate=1.0, weight_decay=0.5, epsilon=2)\n \n\n@@ -2,6 +2,7 @@\n \n \n import numpy as np\n+from unittest import mock\n \n from keras.src import backend\n from keras.src import testing\n@@ -71,3 +72,43 @@ class FtrlTest(testing.TestCase):\n         grad = [np.array([100.0, 100.0])]\n         clipped_grad = optimizer._clip_gradients(grad)\n         self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n+\n+    def test_invalid_initial_accumulator_value(self):\n+        invalid_value = -0.1\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            f\"^`initial_accumulator_value` needs to be positive or zero. Received: initial_accumulator_value={invalid_value}.$\",\n+        ):\n+            Ftrl(initial_accumulator_value=invalid_value)\n+\n+    def test_invalid_learning_rate_power(self):\n+        invalid_value = 0.1\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            f\"^`learning_rate_power` needs to be negative or zero. Received: learning_rate_power={invalid_value}.$\",\n+        ):\n+            Ftrl(learning_rate_power=invalid_value)\n+\n+    def test_invalid_l1_regularization_strength(self):\n+        invalid_value = -0.1\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            f\"^`l1_regularization_strength` needs to be positive or zero. Received: l1_regularization_strength={invalid_value}.$\",\n+        ):\n+            Ftrl(l1_regularization_strength=invalid_value)\n+\n+    def test_invalid_l2_regularization_strength(self):\n+        invalid_value = -0.1\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            f\"^`l2_regularization_strength` needs to be positive or zero. Received: l2_regularization_strength={invalid_value}.$\",\n+        ):\n+            Ftrl(l2_regularization_strength=invalid_value)\n+\n+    def test_invalid_l2_shrinkage_regularization_strength(self):\n+        invalid_value = -0.1\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            f\"^`l2_shrinkage_regularization_strength` needs to be positive or zero. Received: l2_shrinkage_regularization_strength={invalid_value}.$\",\n+        ):\n+            Ftrl(l2_shrinkage_regularization_strength=invalid_value)\n\n@@ -9,6 +9,26 @@ from keras.src.optimizers.lion import Lion\n \n \n class LionTest(testing.TestCase):\n+    def test_invalid_beta_1(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Argument `beta_1` must be in the \\\\[0, 1\\\\] range. Otherwise, the \"\n+            \"optimizer degenerates to SignSGD. Received: beta_1=-0.1.\",\n+        ):\n+            Lion(beta_1=-0.1)\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Argument `beta_1` must be in the \\\\[0, 1\\\\] range. Otherwise, the \"\n+            \"optimizer degenerates to SignSGD. Received: beta_1=0.0.\",\n+        ):\n+            Lion(beta_1=0.0)\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Argument `beta_1` must be in the \\\\[0, 1\\\\] range. Otherwise, the \"\n+            \"optimizer degenerates to SignSGD. Received: beta_1=1.1.\",\n+        ):\n+            Lion(beta_1=1.1)\n+\n     def test_config(self):\n         optimizer = Lion(\n             learning_rate=0.5,\n\n@@ -19,6 +19,11 @@ class NadamTest(testing.TestCase):\n         )\n         self.run_class_serialization_test(optimizer)\n \n+    def test_build_with_empty_var_list(self):\n+        optimizer = Nadam()\n+        optimizer.build([])\n+        self.assertEqual(optimizer._u_product.dtype, backend.floatx())\n+\n     def test_single_step(self):\n         optimizer = Nadam(learning_rate=0.5)\n         grads = ops.array([1.0, 6.0, 7.0, 2.0])\n\n@@ -30,6 +30,17 @@ class SGDTest(testing.TestCase):\n         self.assertEqual(optimizer.variables[0], 1)\n         self.assertEqual(optimizer.variables[1], 0.5)\n \n+    def test_invalid_momentum(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"`momentum` must be a float between \\\\[0, 1\\\\].\"\n+        ):\n+            SGD(momentum=-1.0)\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"`momentum` must be a float between \\\\[0, 1\\\\].\"\n+        ):\n+            SGD(momentum=2.0)\n+\n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n             ops.zeros(()),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
