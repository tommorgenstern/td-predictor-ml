{"custom_id": "keras#86c8d1dd45530aef4f8af09251ad8290be568a5a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 6 | Churn Cumulative: 2724 | Contributors (this commit): 54 | Commits (past 90d): 17 | Contributors (cumulative): 54 | DMM Complexity: 0.0\n\nDIFF:\n@@ -523,8 +523,6 @@ class RemoteMonitor(Callback):\n         path: String; path relative to `root` to which the events will be sent.\n         field: String; JSON field under which the data will be stored.\n         headers: Dictionary; optional custom HTTP headers.\n-            Defaults to:\n-            `{'Accept': 'application/json', 'Content-Type': 'application/json'}`\n     \"\"\"\n \n     def __init__(self,\n@@ -533,9 +531,7 @@ class RemoteMonitor(Callback):\n                  field='data',\n                  headers=None):\n         super(RemoteMonitor, self).__init__()\n-        if headers is None:\n-            headers = {'Accept': 'application/json',\n-                       'Content-Type': 'application/json'}\n+\n         self.root = root\n         self.path = path\n         self.field = field\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1ddf23528e38d6ef47ad42d10010da90d8c21018", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 25 | Churn Cumulative: 8011 | Contributors (this commit): 43 | Commits (past 90d): 27 | Contributors (cumulative): 43 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2923,6 +2923,31 @@ def preprocess_weights_for_loading(layer, weights,\n                                                     (2, 3, 1, 0))\n                 weights = [kernel, recurrent_kernel, bias]\n \n+        if layer.__class__.__name__ in ['Model', 'Sequential']:\n+            new_weights = []\n+            # trainable weights\n+            for sublayer in layer.layers:\n+                num_weights = len(sublayer.trainable_weights)\n+                if num_weights > 0:\n+                    new_weights.extend(preprocess_weights_for_loading(\n+                        layer=sublayer,\n+                        weights=weights[:num_weights],\n+                        original_keras_version=original_keras_version,\n+                        original_backend=original_backend))\n+                    weights = weights[num_weights:]\n+\n+            # non-trainable weights\n+            for sublayer in layer.layers:\n+                num_weights = len([l for l in sublayer.weights if l not in sublayer.trainable_weights])\n+                if num_weights > 0:\n+                    new_weights.extend(preprocess_weights_for_loading(\n+                        layer=sublayer,\n+                        weights=weights[:num_weights],\n+                        original_keras_version=original_keras_version,\n+                        original_backend=original_backend))\n+                    weights = weights[num_weights:]\n+            weights = new_weights\n+\n     conv_layers = ['Conv1D',\n                    'Conv2D',\n                    'Conv3D',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#68145065284e645da12f364f3f853799d13c08c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 76 | Churn Cumulative: 516 | Contributors (this commit): 11 | Commits (past 90d): 5 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,11 +8,13 @@ from __future__ import division\n \n import string\n import sys\n+import warnings\n+from collections import OrderedDict\n+from hashlib import md5\n+\n import numpy as np\n from six.moves import range\n from six.moves import zip\n-from collections import OrderedDict\n-import warnings\n \n if sys.version_info < (3,):\n     maketrans = string.maketrans\n@@ -45,11 +47,58 @@ def one_hot(text, n,\n             filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n             lower=True,\n             split=' '):\n+    \"\"\"One-hot encodes a text into a list of word indexes of size n.\n+\n+    This is a wrapper to the `hashing_trick` function using `hash` as the\n+    hashing function, unicity of word to index mapping non-guaranteed.\n+    \"\"\"\n+    return hashing_trick(text, n,\n+                         hash_function=hash,\n+                         filters=filters,\n+                         lower=lower,\n+                         split=split)\n+\n+\n+def hashing_trick(text, n,\n+                  hash_function=None,\n+                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n+                  lower=True,\n+                  split=' '):\n+    \"\"\"Converts a text to a sequence of indexes in a fixed-size hashing space.\n+\n+    # Arguments\n+        text: Input text (string).\n+        n: Dimension of the hashing space.\n+        hash_function: if `None` uses python `hash` function, can be 'md5' or\n+            any function that takes in input a string and returns a int.\n+            Note that `hash` is not a stable hashing function, so\n+            it is not consistent across different runs, while 'md5'\n+            is a stable hashing function.\n+        filters: Sequence of characters to filter out.\n+        lower: Whether to convert the input to lowercase.\n+        split: Sentence split marker (string).\n+\n+    # Returns\n+        A list of integer word indices (unicity non-guaranteed).\n+\n+    `0` is a reserved index that won't be assigned to any word.\n+\n+    Two or more words may be assigned to the same index, due to possible\n+    collisions by the hashing function.\n+    The [probability](https://en.wikipedia.org/wiki/Birthday_problem#Probability_table)\n+    of a collision is in relation to the dimension of the hashing space and\n+    the number of distinct objects.\n+    \"\"\"\n+    if hash_function is None:\n+        hash_function = hash\n+    elif hash_function == 'md5':\n+        hash_function = lambda w: int(md5(w.encode()).hexdigest(), 16)\n+\n     seq = text_to_word_sequence(text,\n                                 filters=filters,\n                                 lower=lower,\n                                 split=split)\n-    return [(abs(hash(w)) % (n - 1) + 1) for w in seq]\n+    return [(hash_function(w) % (n - 1) + 1) for w in seq]\n \n \n class Tokenizer(object):\n\n@@ -1,6 +1,7 @@\n-from keras.preprocessing.text import Tokenizer, one_hot\n-import pytest\n import numpy as np\n+import pytest\n+\n+from keras.preprocessing.text import Tokenizer, one_hot, hashing_trick\n \n \n def test_one_hot():\n@@ -11,6 +12,22 @@ def test_one_hot():\n     assert np.min(encoded) >= 0\n \n \n+def test_hashing_trick_hash():\n+    text = 'The cat sat on the mat.'\n+    encoded = hashing_trick(text, 5)\n+    assert len(encoded) == 6\n+    assert np.max(encoded) <= 4\n+    assert np.min(encoded) >= 1\n+\n+\n+def test_hashing_trick_md5():\n+    text = 'The cat sat on the mat.'\n+    encoded = hashing_trick(text, 5, hash_function='md5')\n+    assert len(encoded) == 6\n+    assert np.max(encoded) <= 4\n+    assert np.min(encoded) >= 1\n+\n+\n def test_tokenizer():\n     texts = ['The cat sat on the mat.',\n              'The dog sat on the log.',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7425e68cd65b351a09e5c8b84c089cee948f3107", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 65 | Lines Deleted: 33 | Files Changed: 2 | Hunks: 29 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 98 | Churn Cumulative: 5432 | Contributors (this commit): 54 | Commits (past 90d): 27 | Contributors (cumulative): 59 | DMM Complexity: 0.3\n\nDIFF:\n@@ -1770,15 +1770,18 @@ class Model(Container):\n         is_sequence = isinstance(generator, Sequence)\n         if not is_sequence and use_multiprocessing:\n             warnings.warn(\n-                UserWarning('Using a generator with `use_multiprocessing=True` may duplicate your data.',\n-                            'Please consider using the `keras.utils.Sequence` object.'))\n+                UserWarning('Using a generator with `use_multiprocessing=True`'\n+                            ' may duplicate your data.Please consider using '\n+                            'the `keras.utils.Sequence` class.'))\n         enqueuer = None\n \n         try:\n             if is_sequence:\n-                enqueuer = OrderedEnqueuer(generator, use_multiprocessing=use_multiprocessing)\n+                enqueuer = OrderedEnqueuer(generator,\n+                                           use_multiprocessing=use_multiprocessing)\n             else:\n-                enqueuer = GeneratorEnqueuer(generator, use_multiprocessing=use_multiprocessing,\n+                enqueuer = GeneratorEnqueuer(generator,\n+                                             use_multiprocessing=use_multiprocessing,\n                                              wait_time=wait_time)\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n@@ -1871,7 +1874,9 @@ class Model(Container):\n \n     @interfaces.legacy_generator_methods_support\n     def evaluate_generator(self, generator, steps,\n-                           max_queue_size=10, workers=1, use_multiprocessing=False):\n+                           max_queue_size=10,\n+                           workers=1,\n+                           use_multiprocessing=False):\n         \"\"\"Evaluates the model on a data generator.\n \n         The generator should return the same kind of data\n@@ -1915,15 +1920,19 @@ class Model(Container):\n         is_sequence = isinstance(generator, Sequence)\n         if not is_sequence and use_multiprocessing:\n             warnings.warn(\n-                UserWarning('Using a generator with `use_multiprocessing=True` may duplicate your data.',\n-                            'Please consider using the `keras.utils.Sequence` object.'))\n+                UserWarning('Using a generator with `use_multiprocessing=True`'\n+                            ' may duplicate your data.Please consider using '\n+                            'the `keras.utils.Sequence` class.'))\n         enqueuer = None\n \n         try:\n             if is_sequence:\n-                enqueuer = OrderedEnqueuer(generator, use_multiprocessing=use_multiprocessing)\n+                enqueuer = OrderedEnqueuer(generator,\n+                                           use_multiprocessing=use_multiprocessing)\n             else:\n-                enqueuer = GeneratorEnqueuer(generator, use_multiprocessing=use_multiprocessing, wait_time=wait_time)\n+                enqueuer = GeneratorEnqueuer(generator,\n+                                             use_multiprocessing=use_multiprocessing,\n+                                             wait_time=wait_time)\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n \n@@ -1976,8 +1985,10 @@ class Model(Container):\n \n     @interfaces.legacy_generator_methods_support\n     def predict_generator(self, generator, steps,\n-                          max_queue_size=10, workers=1,\n-                          use_multiprocessing=False, verbose=0):\n+                          max_queue_size=10,\n+                          workers=1,\n+                          use_multiprocessing=False,\n+                          verbose=0):\n         \"\"\"Generates predictions for the input samples from a data generator.\n \n         The generator should return the same kind of data as accepted by\n@@ -2017,15 +2028,18 @@ class Model(Container):\n         is_sequence = isinstance(generator, Sequence)\n         if not is_sequence and use_multiprocessing:\n             warnings.warn(\n-                UserWarning('Using a generator with `use_multiprocessing=True` may duplicate your data.',\n-                            'Please consider using the `keras.utils.Sequence` object.'))\n+                UserWarning('Using a generator with `use_multiprocessing=True`'\n+                            ' may duplicate your data.Please consider using '\n+                            'the `keras.utils.Sequence` class.'))\n         enqueuer = None\n \n         try:\n             if is_sequence:\n-                enqueuer = OrderedEnqueuer(generator, use_multiprocessing=use_multiprocessing)\n+                enqueuer = OrderedEnqueuer(generator,\n+                                           use_multiprocessing=use_multiprocessing)\n             else:\n-                enqueuer = GeneratorEnqueuer(generator, use_multiprocessing=use_multiprocessing,\n+                enqueuer = GeneratorEnqueuer(generator,\n+                                             use_multiprocessing=use_multiprocessing,\n                                              wait_time=wait_time)\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n@@ -2043,7 +2057,7 @@ class Model(Container):\n                     elif len(generator_output) == 3:\n                         x, _, _ = generator_output\n                     else:\n-                        raise ValueError('output of generator should be '\n+                        raise ValueError('Output of generator should be '\n                                          'a tuple `(x, y, sample_weight)` '\n                                          'or `(x, y)`. Found: ' +\n                                          str(generator_output))\n\n@@ -299,6 +299,7 @@ def validate_file(fpath, file_hash, algorithm='auto', chunk_size=65535):\n \n class Sequence(object):\n     \"\"\"Base object for fitting to a sequence of data, such as a dataset.\n+\n     Every `Sequence` must implements the `__getitem__` and the `__len__` methods.\n \n     # Examples\n@@ -366,7 +367,8 @@ def get_index(ds, i):\n \n class SequenceEnqueuer(object):\n     \"\"\"Base class to enqueue inputs.\n-    The task of an Enqueuer is to use parallelism to speed up the preprocessing.\n+\n+    The task of an Enqueuer is to use parallelism to speed up preprocessing.\n     This is done with processes or threads.\n \n     # Examples\n@@ -395,7 +397,8 @@ class SequenceEnqueuer(object):\n \n         # Arguments\n             workers: number of worker threads\n-            max_queue_size: queue size (when full, threads could block on `put()`)\n+            max_queue_size: queue size\n+                (when full, threads could block on `put()`).\n         \"\"\"\n         raise NotImplemented\n \n@@ -412,11 +415,13 @@ class SequenceEnqueuer(object):\n \n     @abstractmethod\n     def get(self):\n-        \"\"\"Creates a generator to extract data from the queue. Skip the data if it's None.\n+        \"\"\"Creates a generator to extract data from the queue.\n+\n+        Skip the data if it is `None`.\n \n         # Returns\n-            Generator yielding tuples (inputs, targets)\n-                or (inputs, targets, sample_weights)\n+            Generator yielding tuples `(inputs, targets)`\n+                or `(inputs, targets, sample_weights)`.\n         \"\"\"\n         raise NotImplemented\n \n@@ -432,7 +437,9 @@ class OrderedEnqueuer(SequenceEnqueuer):\n         scheduling: Sequential querying of datas if 'sequential', random otherwise.\n     \"\"\"\n \n-    def __init__(self, sequence, use_multiprocessing=False, scheduling='sequential'):\n+    def __init__(self, sequence,\n+                 use_multiprocessing=False,\n+                 scheduling='sequential'):\n         self.sequence = sequence\n         self.use_multiprocessing = use_multiprocessing\n         self.scheduling = scheduling\n@@ -450,7 +457,8 @@ class OrderedEnqueuer(SequenceEnqueuer):\n \n         # Arguments\n             workers: number of worker threads\n-            max_queue_size: queue size (when full, workers could block on put())\n+            max_queue_size: queue size\n+                (when full, workers could block on `put()`)\n         \"\"\"\n         if self.use_multiprocessing:\n             self.executor = multiprocessing.Pool(workers)\n@@ -472,10 +480,13 @@ class OrderedEnqueuer(SequenceEnqueuer):\n                 if self.stop_signal.is_set():\n                     return\n                 self.queue.put(\n-                    self.executor.apply_async(get_index, (self.sequence, i)), block=True)\n+                    self.executor.apply_async(get_index,\n+                                              (self.sequence, i)), block=True)\n \n     def get(self):\n-        \"\"\"Creates a generator to extract data from the queue. Skip the data if it's None.\n+        \"\"\"Creates a generator to extract data from the queue.\n+\n+        Skip the data if it is `None`.\n \n         # Returns\n             Generator yielding tuples (inputs, targets)\n@@ -493,10 +504,10 @@ class OrderedEnqueuer(SequenceEnqueuer):\n     def stop(self, timeout=None):\n         \"\"\"Stops running threads and wait for them to exit, if necessary.\n \n-        Should be called by the same thread which called start().\n+        Should be called by the same thread which called `start()`.\n \n         # Arguments\n-            timeout: maximum time to wait on thread.join()\n+            timeout: maximum time to wait on `thread.join()`\n         \"\"\"\n         self.stop_signal.set()\n         with self.queue.mutex:\n@@ -516,10 +527,14 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n         generator: a generator function which endlessly yields data\n         use_multiprocessing: use multiprocessing if True, otherwise threading\n         wait_time: time to sleep in-between calls to `put()`\n-        random_seed: Initial seed for workers, will be incremented by one for each workers.\n+        random_seed: Initial seed for workers,\n+            will be incremented by one for each workers.\n     \"\"\"\n \n-    def __init__(self, generator, use_multiprocessing=False, wait_time=0.05, random_seed=None):\n+    def __init__(self, generator,\n+                 use_multiprocessing=False,\n+                 wait_time=0.05,\n+                 random_seed=None):\n         self.wait_time = wait_time\n         self._generator = generator\n         self._use_multiprocessing = use_multiprocessing\n@@ -533,7 +548,8 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n \n         # Arguments\n             workers: number of worker threads\n-            max_queue_size: queue size (when full, threads could block on put())\n+            max_queue_size: queue size\n+                (when full, threads could block on `put()`)\n         \"\"\"\n \n         def data_generator_task():\n@@ -579,10 +595,10 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n     def stop(self, timeout=None):\n         \"\"\"Stops running threads and wait for them to exit, if necessary.\n \n-        Should be called by the same thread which called start().\n+        Should be called by the same thread which called `start()`.\n \n         # Arguments\n-            timeout: maximum time to wait on thread.join()\n+            timeout: maximum time to wait on `thread.join()`.\n         \"\"\"\n         if self.is_running():\n             self._stop_event.set()\n@@ -603,7 +619,9 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n         self.queue = None\n \n     def get(self):\n-        \"\"\"Creates a generator to extract data from the queue. Skip the data if it's None.\n+        \"\"\"Creates a generator to extract data from the queue.\n+\n+        Skip the data if it is `None`.\n \n         # Returns\n             A generator\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#abf8691ade76c33da7585741e46d18531923e30f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 394 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -275,19 +275,19 @@ def test_multiprocessing_predict_error():\n     def custom_generator():\n         \"\"\"Raises an exception after a few good batches\"\"\"\n         for i in range(good_batches):\n-            yield (np.random.randint(1, 256, (2, 50)),\n-                   np.random.randint(1, 256, (2, 50)))\n+            yield (np.random.randint(1, 256, size=(2, 5)),\n+                   np.random.randint(1, 256, size=(2, 5)))\n         raise RuntimeError\n \n     model = Sequential()\n-    model.add(Dense(1, input_shape=(2, )))\n+    model.add(Dense(1, input_shape=(5,)))\n     model.compile(loss='mse', optimizer='adadelta')\n \n-    with pytest.raises(StopIteration):\n-        model.predict_generator(\n-            custom_generator(), good_batches + 1, 1,\n-            workers=4, use_multiprocessing=True,\n-        )\n+    # with pytest.raises(StopIteration):\n+    #     model.predict_generator(\n+    #         custom_generator(), good_batches + 1, 1,\n+    #         workers=4, use_multiprocessing=True,\n+    #     )\n \n     with pytest.raises(StopIteration):\n         model.predict_generator(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#58d1d0678f1f0dfb2dca976b84fc3d419d9f4618", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 405 | Contributors (this commit): 6 | Commits (past 90d): 7 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -271,6 +271,7 @@ def test_multiprocessing_evaluate_error():\n @keras_test\n def test_multiprocessing_predict_error():\n     good_batches = 3\n+    workers = 4\n \n     def custom_generator():\n         \"\"\"Raises an exception after a few good batches\"\"\"\n@@ -283,11 +284,11 @@ def test_multiprocessing_predict_error():\n     model.add(Dense(1, input_shape=(5,)))\n     model.compile(loss='mse', optimizer='adadelta')\n \n-    # with pytest.raises(StopIteration):\n-    #     model.predict_generator(\n-    #         custom_generator(), good_batches + 1, 1,\n-    #         workers=4, use_multiprocessing=True,\n-    #     )\n+    with pytest.raises(StopIteration):\n+        model.predict_generator(\n+            custom_generator(), good_batches * workers + 1, 1,\n+            workers=workers, use_multiprocessing=True,\n+        )\n \n     with pytest.raises(StopIteration):\n         model.predict_generator(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#04a20177cf332004a291cee626c20a0262427796", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 122 | Lines Deleted: 44 | Files Changed: 14 | Hunks: 47 | Methods Changed: 17 | Complexity Δ (Sum/Max): 18/12 | Churn Δ: 166 | Churn Cumulative: 40308 | Contributors (this commit): 203 | Commits (past 90d): 142 | Contributors (cumulative): 362 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -320,9 +320,11 @@ def get_classes_ancestors(classes):\n \n \n def get_function_signature(function, method=True):\n-    signature = getattr(function, '_legacy_support_signature', None)\n-    if signature is None:\n+    wrapped = getattr(function, '_original_function', None)\n+    if wrapped is None:\n         signature = inspect.getargspec(function)\n+    else:\n+        signature = inspect.getargspec(wrapped)\n     defaults = signature.defaults\n     if method:\n         args = signature.args[1:]\n\n@@ -1,5 +1,6 @@\n from __future__ import absolute_import\r\n \r\n+from . import utils\r\n from . import activations\r\n from . import applications\r\n from . import backend\r\n@@ -7,7 +8,6 @@ from . import datasets\n from . import engine\r\n from . import layers\r\n from . import preprocessing\r\n-from . import utils\r\n from . import wrappers\r\n from . import callbacks\r\n from . import constraints\r\n\n@@ -7,13 +7,14 @@ from tensorflow.python.ops import ctc_ops as ctc\n from tensorflow.python.ops import variables as tf_variables\n \n from collections import defaultdict\n-import inspect\n+\n import numpy as np\n import os\n \n from .common import floatx\n from .common import _EPSILON\n from .common import image_data_format\n+from ..utils.generic_utils import has_arg\n \n # Legacy functions\n from .common import set_image_dim_ordering\n@@ -2285,8 +2286,7 @@ def function(inputs, outputs, updates=None, **kwargs):\n     \"\"\"\n     if kwargs:\n         for key in kwargs:\n-            if (key not in inspect.getargspec(tf.Session.run)[0] and\n-                    key not in inspect.getargspec(Function.__init__)[0]):\n+            if not (has_arg(tf.Session.run, key, True) or has_arg(Function.__init__, key, True)):\n                 msg = 'Invalid argument \"%s\" passed to K.function with Tensorflow backend' % key\n                 raise ValueError(msg)\n     return Function(inputs, outputs, updates=updates, **kwargs)\n\n@@ -14,9 +14,10 @@ try:\n     from theano.tensor.nnet.nnet import softsign as T_softsign\n except ImportError:\n     from theano.sandbox.softsign import softsign as T_softsign\n-import inspect\n+\n import numpy as np\n from .common import _FLOATX, floatx, _EPSILON, image_data_format\n+from ..utils.generic_utils import has_arg\n # Legacy functions\n from .common import set_image_dim_ordering, image_dim_ordering\n \n@@ -1194,9 +1195,8 @@ class Function(object):\n \n def function(inputs, outputs, updates=[], **kwargs):\n     if len(kwargs) > 0:\n-        function_args = inspect.getargspec(theano.function)[0]\n         for key in kwargs.keys():\n-            if key not in function_args:\n+            if not has_arg(theano.function, key, True):\n                 msg = 'Invalid argument \"%s\" passed to K.function with Theano backend' % key\n                 raise ValueError(msg)\n     return Function(inputs, outputs, updates=updates, **kwargs)\n\n@@ -10,13 +10,13 @@ import warnings\n import copy\n import os\n import re\n-import inspect\n from six.moves import zip\n \n from .. import backend as K\n from .. import initializers\n from ..utils.io_utils import ask_to_proceed_with_overwrite\n from ..utils.layer_utils import print_summary as print_layer_summary\n+from ..utils.generic_utils import has_arg\n from ..utils import conv_utils\n from ..legacy import interfaces\n \n@@ -584,7 +584,7 @@ class Layer(object):\n             user_kwargs = copy.copy(kwargs)\n             if not _is_all_none(previous_mask):\n                 # The previous layer generated a mask.\n-                if 'mask' in inspect.getargspec(self.call).args:\n+                if has_arg(self.call, 'mask'):\n                     if 'mask' not in kwargs:\n                         # If mask is explicitly passed to __call__,\n                         # we should override the default mask.\n@@ -2206,7 +2206,7 @@ class Container(Layer):\n                             kwargs = {}\n                         if len(computed_data) == 1:\n                             computed_tensor, computed_mask = computed_data[0]\n-                            if 'mask' in inspect.getargspec(layer.call).args:\n+                            if has_arg(layer.call, 'mask'):\n                                 if 'mask' not in kwargs:\n                                     kwargs['mask'] = computed_mask\n                             output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\n@@ -2217,7 +2217,7 @@ class Container(Layer):\n                         else:\n                             computed_tensors = [x[0] for x in computed_data]\n                             computed_masks = [x[1] for x in computed_data]\n-                            if 'mask' in inspect.getargspec(layer.call).args:\n+                            if has_arg(layer.call, 'mask'):\n                                 if 'mask' not in kwargs:\n                                     kwargs['mask'] = computed_masks\n                             output_tensors = _to_list(layer.call(computed_tensors, **kwargs))\n\n@@ -5,7 +5,6 @@ from __future__ import division\n import numpy as np\n \n import copy\n-import inspect\n import types as python_types\n import warnings\n \n@@ -19,6 +18,7 @@ from ..engine import Layer\n from ..utils.generic_utils import func_dump\n from ..utils.generic_utils import func_load\n from ..utils.generic_utils import deserialize_keras_object\n+from ..utils.generic_utils import has_arg\n from ..legacy import interfaces\n \n \n@@ -642,8 +642,7 @@ class Lambda(Layer):\n \n     def call(self, inputs, mask=None):\n         arguments = self.arguments\n-        arg_spec = inspect.getargspec(self.function)\n-        if 'mask' in arg_spec.args:\n+        if has_arg(self.function, 'mask'):\n             arguments['mask'] = mask\n         return self.function(inputs, **arguments)\n \n\n@@ -2,9 +2,9 @@\n from __future__ import absolute_import\n \n import copy\n-import inspect\n from ..engine import Layer\n from ..engine import InputSpec\n+from ..utils.generic_utils import has_arg\n from .. import backend as K\n \n \n@@ -155,8 +155,7 @@ class TimeDistributed(Wrapper):\n \n     def call(self, inputs, training=None, mask=None):\n         kwargs = {}\n-        func_args = inspect.getargspec(self.layer.call).args\n-        if 'training' in func_args:\n+        if has_arg(self.layer.call, 'training'):\n             kwargs['training'] = training\n         uses_learning_phase = False\n \n@@ -272,10 +271,9 @@ class Bidirectional(Wrapper):\n \n     def call(self, inputs, training=None, mask=None):\n         kwargs = {}\n-        func_args = inspect.getargspec(self.layer.call).args\n-        if 'training' in func_args:\n+        if has_arg(self.layer.call, 'training'):\n             kwargs['training'] = training\n-        if 'mask' in func_args:\n+        if has_arg(self.layer.call, 'mask'):\n             kwargs['mask'] = mask\n \n         y = self.forward_layer.call(inputs, **kwargs)\n\n@@ -3,7 +3,6 @@\n import six\n import warnings\n import functools\n-import inspect\n import numpy as np\n \n \n@@ -86,7 +85,7 @@ def generate_legacy_interface(allowed_positional_args=None,\n                 warnings.warn('Update your `' + object_name +\n                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n             return func(*args, **kwargs)\n-        wrapper._legacy_support_signature = inspect.getargspec(func)\n+        wrapper._original_function = func\n         return wrapper\n     return legacy_support\n \n\n@@ -1,10 +1,9 @@\n-import inspect\n import types as python_types\n import warnings\n \n from ..engine.topology import Layer, InputSpec\n from .. import backend as K\n-from ..utils.generic_utils import func_dump, func_load\n+from ..utils.generic_utils import func_dump, func_load, has_arg\n from .. import regularizers\n from .. import constraints\n from .. import activations\n@@ -197,8 +196,7 @@ class Merge(Layer):\n         # Case: \"mode\" is a lambda or function.\n         if callable(self.mode):\n             arguments = self.arguments\n-            arg_spec = inspect.getargspec(self.mode)\n-            if 'mask' in arg_spec.args:\n+            if has_arg(self.mode, 'mask'):\n                 arguments['mask'] = mask\n             return self.mode(inputs, **arguments)\n \n\n@@ -1,9 +1,9 @@\n from __future__ import absolute_import\n from . import np_utils\n-from . import conv_utils\n-from . import data_utils\n from . import generic_utils\n+from . import data_utils\n from . import io_utils\n+from . import conv_utils\n \n # Globally-importable utils.\n from .io_utils import HDF5Matrix\n\n@@ -132,9 +132,8 @@ def deserialize_keras_object(identifier, module_objects=None,\n                 raise ValueError('Unknown ' + printable_module_name +\n                                  ': ' + class_name)\n         if hasattr(cls, 'from_config'):\n-            arg_spec = inspect.getargspec(cls.from_config)\n             custom_objects = custom_objects or {}\n-            if 'custom_objects' in arg_spec.args:\n+            if has_arg(cls.from_config, 'custom_objects'):\n                 return cls.from_config(config['config'],\n                                        custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n                                                            list(custom_objects.items())))\n@@ -207,6 +206,48 @@ def func_load(code, defaults=None, closure=None, globs=None):\n                                      closure=closure)\n \n \n+def has_arg(fn, name, accept_all=False):\n+    \"\"\"Checks if a callable accepts a given keyword argument.\n+\n+    For Python 2, checks if there is an argument with the given name.\n+\n+    For Python 3, checks if there is an argument with the given name, and\n+    also whether this argument can be called with a keyword (i.e. if it is\n+    not a positional-only argument).\n+\n+    # Arguments\n+        fn: Callable to inspect.\n+        name: Check if `fn` can be called with `name` as a keyword argument.\n+        accept_all: What to return if there is no parameter called `name`\n+                    but the function accepts a `**kwargs` argument.\n+\n+    # Returns\n+        bool, whether `fn` accepts a `name` keyword argument.\n+    \"\"\"\n+    if sys.version_info < (3,):\n+        arg_spec = inspect.getargspec(fn)\n+        if accept_all and arg_spec.keywords is not None:\n+            return True\n+        return (name in arg_spec.args)\n+    elif sys.version_info < (3, 3):\n+        arg_spec = inspect.getfullargspec(fn)\n+        if accept_all and arg_spec.varkw is not None:\n+            return True\n+        return (name in arg_spec.args or\n+                name in arg_spec.kwonlyargs)\n+    else:\n+        signature = inspect.signature(fn)\n+        parameter = signature.parameters.get(name)\n+        if parameter is None:\n+            if accept_all:\n+                for param in signature.parameters.values():\n+                    if param.kind == inspect.Parameter.VAR_KEYWORD:\n+                        return True\n+            return False\n+        return (parameter.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD,\n+                                   inspect.Parameter.KEYWORD_ONLY))\n+\n+\n class Progbar(object):\n     \"\"\"Displays a progress bar.\n \n\n@@ -1,9 +1,9 @@\n \"\"\"Utilities related to Keras unit tests.\"\"\"\n import numpy as np\n from numpy.testing import assert_allclose\n-import inspect\n import six\n \n+from .generic_utils import has_arg\n from ..engine import Model, Input\n from ..models import Sequential\n from ..models import model_from_json\n@@ -71,7 +71,9 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n     layer.set_weights(weights)\n \n     # test and instantiation from weights\n-    if 'weights' in inspect.getargspec(layer_cls.__init__):\n+    # Checking for empty weights array to avoid a problem where some\n+    # legacy layers return bad values from get_weights()\n+    if has_arg(layer_cls.__init__, 'weights') and len(weights):\n         kwargs['weights'] = weights\n         layer = layer_cls(**kwargs)\n \n\n@@ -1,12 +1,12 @@\n from __future__ import absolute_import\n \n import copy\n-import inspect\n import types\n \n import numpy as np\n \n from ..utils.np_utils import to_categorical\n+from ..utils.generic_utils import has_arg\n from ..models import Sequential\n \n \n@@ -75,13 +75,11 @@ class BaseWrapper(object):\n         else:\n             legal_params_fns.append(self.build_fn)\n \n-        legal_params = []\n-        for fn in legal_params_fns:\n-            legal_params += inspect.getargspec(fn)[0]\n-        legal_params = set(legal_params)\n-\n         for params_name in params:\n-            if params_name not in legal_params:\n+            for fn in legal_params_fns:\n+                if has_arg(fn, params_name):\n+                    break\n+            else:\n                 if params_name != 'nb_epoch':\n                     raise ValueError(\n                         '{} is not a legal parameter'.format(params_name))\n@@ -163,9 +161,8 @@ class BaseWrapper(object):\n         \"\"\"\n         override = override or {}\n         res = {}\n-        fn_args = inspect.getargspec(fn)[0]\n         for name, value in self.sk_params.items():\n-            if name in fn_args:\n+            if has_arg(fn, name):\n                 res.update({name: value})\n         res.update(override)\n         return res\n\n@@ -1,5 +1,6 @@\n+import sys\n import pytest\n-from keras.utils.generic_utils import custom_object_scope\n+from keras.utils.generic_utils import custom_object_scope, has_arg\n from keras import activations\n from keras import regularizers\n \n@@ -20,5 +21,46 @@ def test_custom_objects_scope():\n         assert cl.__class__ == CustomClass\n \n \n+@pytest.mark.parametrize('fn, name, accept_all, expected', [\n+    ('f(x)', 'x', False, True),\n+    ('f(x)', 'y', False, False),\n+    ('f(x)', 'y', True, False),\n+    ('f(x, y)', 'y', False, True),\n+    ('f(x, y=1)', 'y', False, True),\n+    ('f(x, **kwargs)', 'x', False, True),\n+    ('f(x, **kwargs)', 'y', False, False),\n+    ('f(x, **kwargs)', 'y', True, True),\n+    ('f(x, y=1, **kwargs)', 'y', False, True),\n+    # Keyword-only arguments (Python 3 only)\n+    ('f(x, *args, y=1)', 'y', False, True),\n+    ('f(x, *args, y=1)', 'z', True, False),\n+    ('f(x, *, y=1)', 'x', False, True),\n+    ('f(x, *, y=1)', 'y', False, True),\n+    # lambda\n+    (lambda x: x, 'x', False, True),\n+    (lambda x: x, 'y', False, False),\n+    (lambda x: x, 'y', True, False),\n+])\n+def test_has_arg(fn, name, accept_all, expected):\n+    if isinstance(fn, str):\n+        context = dict()\n+        try:\n+            exec('def {}: pass'.format(fn), context)\n+        except SyntaxError:\n+            if sys.version_info >= (3,):\n+                raise\n+            pytest.skip('Function is not compatible with Python 2')\n+        context.pop('__builtins__', None)  # Sometimes exec adds builtins to the context\n+        fn, = context.values()\n+\n+    assert has_arg(fn, name, accept_all) is expected\n+\n+\n+@pytest.mark.xfail(sys.version_info < (3, 3),\n+                   reason='inspect API does not reveal positional-only arguments')\n+def test_has_arg_positional_only():\n+    assert has_arg(pow, 'x') is False\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b5ad5334fcc98c147826ac84292ed0425b797b08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 36 | Churn Cumulative: 351 | Contributors (this commit): 12 | Commits (past 90d): 6 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -96,8 +96,40 @@ class HDF5Matrix(object):\n \n     @property\n     def shape(self):\n+        \"\"\"Gets a numpy-style shape tuple giving the dataset dimensions.\n+\n+        # Returns\n+            A numpy-style shape tuple.\n+        \"\"\"\n         return (self.end - self.start,) + self.data.shape[1:]\n \n+    @property\n+    def dtype(self):\n+        \"\"\"Gets the datatype of the dataset.\n+\n+        # Returns\n+            A numpy dtype string.\n+        \"\"\"\n+        return self.data.dtype\n+\n+    @property\n+    def ndim(self):\n+        \"\"\"Gets the number of dimensions (rank) of the dataset.\n+\n+        # Returns\n+            An integer denoting the number of dimensions (rank) of the dataset.\n+        \"\"\"\n+        return self.data.ndim\n+\n+    @property\n+    def size(self):\n+        \"\"\"Gets the total dataset size (number of elements).\n+\n+        # Returns\n+            An integer denoting the number of elements in the dataset.\n+        \"\"\"\n+        return np.prod(self.shape)\n+\n \n def ask_to_proceed_with_overwrite(filepath):\n     \"\"\"Produces a prompt asking about overwriting a file.\n\n@@ -53,6 +53,10 @@ def test_io_utils(in_tmpdir):\n     assert y_train.shape == (150, 1), 'HDF5Matrix shape should match input array'\n     # But they do not support negative indices, so don't try print(X_train[-1])\n \n+    assert y_train.dtype == np.dtype('i'), 'HDF5Matrix dtype should match input array'\n+    assert y_train.ndim == 2, 'HDF5Matrix ndim should match input array'\n+    assert y_train.size == 150, 'HDF5Matrix ndim should match input array'\n+\n     model = Sequential()\n     model.add(Dense(64, input_shape=(10,), activation='relu'))\n     model.add(Dense(1, activation='sigmoid'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#75470e380ff92fa52d600aa3ec0cb0be06773cc1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 840 | Contributors (this commit): 7 | Commits (past 90d): 8 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -515,6 +515,7 @@ class OrderedEnqueuer(SequenceEnqueuer):\n             self.queue.unfinished_tasks = 0\n             self.queue.not_full.notify()\n         self.executor.close()\n+        self.executor.join()\n         self.run_thread.join(timeout)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b713122e77a723d9be2c5652e7833ba68f3c56a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 112 | Contributors (this commit): 6 | Commits (past 90d): 7 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -93,11 +93,12 @@ def test_TimeDistributed():\n                     reason='cntk does not support dropout yet')\n def test_TimeDistributed_learning_phase():\n     # test layers that need learning_phase to be set\n+    np.random.seed(1234)\n     x = Input(shape=(3, 2))\n     y = wrappers.TimeDistributed(core.Dropout(.999))(x, training=True)\n     model = Model(x, y)\n     y = model.predict(np.random.random((10, 3, 2)))\n-    assert_allclose(0., y, atol=1e-2)\n+    assert_allclose(np.mean(y), 0., atol=1e-1, rtol=1e-1)\n \n \n @keras_test\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a2f6ae2c66f22a2415d069913479990ce73e98b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 379 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -104,7 +104,7 @@ def make_sampling_table(size, sampling_factor=1e-5):\n         is the probability that a word of rank i should be sampled.\n     \"\"\"\n     gamma = 0.577\n-    rank = np.array(list(range(size)))\n+    rank = np.arange(size)\n     rank[0] = 1\n     inv_fq = rank * (np.log(rank) + gamma) + 0.5 - 1. / (12. * rank)\n     f = sampling_factor * inv_fq\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f430de10fba7f02ff821a094b29db9a9d407eb4b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 30 | Files Changed: 8 | Hunks: 24 | Methods Changed: 15 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 75 | Churn Cumulative: 27811 | Contributors (this commit): 182 | Commits (past 90d): 71 | Contributors (cumulative): 243 | DMM Complexity: None\n\nDIFF:\n@@ -136,8 +136,8 @@ def shuffle_mats_or_lists(matrix_list, stop_ind=None):\n         elif isinstance(mat, list):\n             ret.append([mat[i] for i in a])\n         else:\n-            raise TypeError('shuffle_mats_or_lists only supports '\n-                            'numpy.array and list objects')\n+            raise TypeError('`shuffle_mats_or_lists` only supports '\n+                            'numpy.array and list objects.')\n     return ret\n \n \n\n@@ -77,7 +77,7 @@ def identity_block(input_tensor, kernel_size, filters, stage, block):\n \n \n def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n-    \"\"\"conv_block is the block that has a conv layer at shortcut\n+    \"\"\"A block that has a conv layer at shortcut.\n \n     # Arguments\n         input_tensor: input tensor\n\n@@ -652,16 +652,16 @@ def _normalize_axis(axis, x):\n \n     nones = _get_dynamic_axis_num(x)\n \n-    if type(axis) is tuple:\n+    if isinstance(axis, tuple):\n         _axis = list(axis)\n-    elif type(axis) is int:\n+    elif isinstance(axis, int):\n         _axis = [axis]\n-    elif type(axis) is list:\n+    elif isinstance(axis, list):\n         _axis = list(axis)\n     else:\n         _axis = axis\n \n-    if type(_axis) is list:\n+    if isinstance(_axis, list):\n         for i, a in enumerate(_axis):\n             if a is not None and a < 0:\n                 _axis[i] = (a % ndim)\n\n@@ -446,10 +446,14 @@ def transpose(x):\n \n \n def gather(reference, indices):\n-    \"\"\"reference: a tensor.\n-    indices: an int tensor of indices.\n+    \"\"\"Retrieves the elements of indices `indices` in the tensor `reference`.\n \n-    Return: a tensor of same type as reference.\n+    # Arguments\n+        reference: A tensor.\n+        indices: An integer tensor of indices.\n+\n+    # Returns\n+        A tensor of same type as `reference`.\n     \"\"\"\n     y = reference[indices]\n     if hasattr(reference, '_keras_shape') and hasattr(indices, '_keras_shape'):\n@@ -930,7 +934,7 @@ def tile(x, n):\n                     output_shape += (None,)\n                 else:\n                     output_shape += (i * j,)\n-        elif type(n) is int:\n+        elif isinstance(n, int):\n             output_shape = x._keras_shape[:-1]\n             if x._keras_shape[-1] is None:\n                 output_shape += (None,)\n@@ -1138,8 +1142,8 @@ def pattern_broadcast(x, broatcastable):\n \n def get_value(x):\n     if not hasattr(x, 'get_value'):\n-        raise TypeError('get_value() can only be called on a variable. '\n-                        'If you have an expression instead, use eval().')\n+        raise TypeError('`get_value` can only be called on a variable. '\n+                        'If you have an expression instead, use `eval()`.')\n     return x.get_value()\n \n \n@@ -1386,7 +1390,18 @@ def rnn(step_function, inputs, initial_states,\n \n \n def switch(condition, then_expression, else_expression):\n-    \"\"\"condition: scalar tensor.\n+    \"\"\"Switches between two operations depending on a scalar value.\n+\n+    Note that both `then_expression` and `else_expression`\n+    should be symbolic tensors of the *same shape*.\n+\n+    # Arguments\n+        condition: scalar tensor (`int` or `bool`).\n+        then_expression: either a tensor, or a callable that returns a tensor.\n+        else_expression: either a tensor, or a callable that returns a tensor.\n+\n+    # Returns\n+        The selected tensor.\n     \"\"\"\n     if callable(then_expression):\n         then_expression = then_expression()\n\n@@ -19,7 +19,7 @@ def load_data(label_mode='fine'):\n         ValueError: in case of invalid `label_mode`.\n     \"\"\"\n     if label_mode not in ['fine', 'coarse']:\n-        raise ValueError('label_mode must be one of \"fine\" \"coarse\".')\n+        raise ValueError('`label_mode` must be one of `\"fine\"`, `\"coarse\"`.')\n \n     dirname = 'cifar-100-python'\n     origin = 'http://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'\n\n@@ -563,7 +563,7 @@ def _standardize_weights(y, sample_weight=None, class_weight=None,\n         return sample_weight\n     elif isinstance(class_weight, dict):\n         if len(y.shape) > 2:\n-            raise ValueError('class_weight not supported for '\n+            raise ValueError('`class_weight` not supported for '\n                              '3+ dimensional targets.')\n         if y.shape[1] > 1:\n             y_classes = y.argmax(axis=1)\n@@ -1756,7 +1756,7 @@ class Model(Container):\n             elif len(validation_data) == 3:\n                 val_x, val_y, val_sample_weight = validation_data\n             else:\n-                raise ValueError('validation_data should be a tuple '\n+                raise ValueError('`validation_data` should be a tuple '\n                                  '`(val_x, val_y, val_sample_weight)` '\n                                  'or `(val_x, val_y)`. Found: ' +\n                                  str(validation_data))\n@@ -1795,7 +1795,7 @@ class Model(Container):\n                     generator_output = next(output_generator)\n \n                     if not hasattr(generator_output, '__len__'):\n-                        raise ValueError('output of generator should be '\n+                        raise ValueError('Output of generator should be '\n                                          'a tuple `(x, y, sample_weight)` '\n                                          'or `(x, y)`. Found: ' +\n                                          str(generator_output))\n@@ -1805,7 +1805,7 @@ class Model(Container):\n                     elif len(generator_output) == 3:\n                         x, y, sample_weight = generator_output\n                     else:\n-                        raise ValueError('output of generator should be '\n+                        raise ValueError('Output of generator should be '\n                                          'a tuple `(x, y, sample_weight)` '\n                                          'or `(x, y)`. Found: ' +\n                                          str(generator_output))\n@@ -1939,7 +1939,7 @@ class Model(Container):\n             while steps_done < steps:\n                 generator_output = next(output_generator)\n                 if not hasattr(generator_output, '__len__'):\n-                    raise ValueError('output of generator should be a tuple '\n+                    raise ValueError('Output of generator should be a tuple '\n                                      '(x, y, sample_weight) '\n                                      'or (x, y). Found: ' +\n                                      str(generator_output))\n@@ -1949,7 +1949,7 @@ class Model(Container):\n                 elif len(generator_output) == 3:\n                     x, y, sample_weight = generator_output\n                 else:\n-                    raise ValueError('output of generator should be a tuple '\n+                    raise ValueError('Output of generator should be a tuple '\n                                      '(x, y, sample_weight) '\n                                      'or (x, y). Found: ' +\n                                      str(generator_output))\n\n@@ -193,8 +193,8 @@ class SpatialDropout2D(Dropout):\n         if data_format is None:\n             data_format = K.image_data_format()\n         if data_format not in {'channels_last', 'channels_first'}:\n-            raise ValueError('data_format must be in '\n-                             '{\"channels_last\", \"channels_first\"}')\n+            raise ValueError('`data_format` must be in '\n+                             '{`\"channels_last\"`, `\"channels_first\"`}')\n         self.data_format = data_format\n         self.input_spec = InputSpec(ndim=4)\n \n@@ -246,8 +246,8 @@ class SpatialDropout3D(Dropout):\n         if data_format is None:\n             data_format = K.image_data_format()\n         if data_format not in {'channels_last', 'channels_first'}:\n-            raise ValueError('data_format must be in '\n-                             '{\"channels_last\", \"channels_first\"}')\n+            raise ValueError('`data_format` must be in '\n+                             '{`\"channels_last\"`, `\"channels_first\"`}')\n         self.data_format = data_format\n         self.input_spec = InputSpec(ndim=5)\n \n@@ -637,7 +637,7 @@ class Lambda(Layer):\n         else:\n             shape = self._output_shape(input_shape)\n             if not isinstance(shape, (list, tuple)):\n-                raise ValueError('output_shape function must return a tuple')\n+                raise ValueError('`output_shape` function must return a tuple.')\n             return tuple(shape)\n \n     def call(self, inputs, mask=None):\n\n@@ -139,7 +139,7 @@ def random_zoom(x, zoom_range, row_axis=1, col_axis=2, channel_axis=0,\n         ValueError: if `zoom_range` isn't a tuple.\n     \"\"\"\n     if len(zoom_range) != 2:\n-        raise ValueError('zoom_range should be a tuple or list of two floats. '\n+        raise ValueError('`zoom_range` should be a tuple or list of two floats. '\n                          'Received arg: ', zoom_range)\n \n     if zoom_range[0] == 1 and zoom_range[1] == 1:\n@@ -421,8 +421,8 @@ class ImageDataGenerator(object):\n         self.preprocessing_function = preprocessing_function\n \n         if data_format not in {'channels_last', 'channels_first'}:\n-            raise ValueError('data_format should be \"channels_last\" (channel after row and '\n-                             'column) or \"channels_first\" (channel before row and column). '\n+            raise ValueError('`data_format` should be `\"channels_last\"` (channel after row and '\n+                             'column) or `\"channels_first\"` (channel before row and column). '\n                              'Received arg: ', data_format)\n         self.data_format = data_format\n         if data_format == 'channels_first':\n@@ -443,7 +443,7 @@ class ImageDataGenerator(object):\n         elif len(zoom_range) == 2:\n             self.zoom_range = [zoom_range[0], zoom_range[1]]\n         else:\n-            raise ValueError('zoom_range should be a float or '\n+            raise ValueError('`zoom_range` should be a float or '\n                              'a tuple or list of two floats. '\n                              'Received arg: ', zoom_range)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
