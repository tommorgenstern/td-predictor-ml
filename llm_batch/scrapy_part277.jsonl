{"custom_id": "scrapy#b9a58798eed39bf543b9682d9ce43b13378a5074", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 5 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 13 | Churn Cumulative: 1957 | Contributors (this commit): 31 | Commits (past 90d): 95 | Contributors (cumulative): 50 | DMM Complexity: 1.0\n\nDIFF:\n@@ -145,7 +145,7 @@ class Scraper(object):\n     def call_spider(self, result, request, spider):\n         result.request = request\n         dfd = defer_result(result)\n-        callback = request.callback or spider.parse\n+        callback = request.callback or spider._parse\n         warn_on_generator_with_return_value(spider, callback)\n         warn_on_generator_with_return_value(spider, request.errback)\n         dfd.addCallbacks(callback=callback,\n\n@@ -80,6 +80,9 @@ class Spider(object_ref):\n         \"\"\" This method is deprecated. \"\"\"\n         return Request(url, dont_filter=True)\n \n+    def _parse(self, response):\n+        return self.parse(response)\n+\n     def parse(self, response):\n         raise NotImplementedError('{}.parse callback is not defined'.format(self.__class__.__name__))\n \n\n@@ -74,7 +74,7 @@ class CrawlSpider(Spider):\n         super(CrawlSpider, self).__init__(*a, **kw)\n         self._compile_rules()\n \n-    def parse(self, response):\n+    def _parse(self, response):\n         return self._parse_response(response, self.parse_start_url, cb_kwargs={}, follow=True)\n \n     def parse_start_url(self, response):\n\n@@ -61,7 +61,7 @@ class XMLFeedSpider(Spider):\n             for result_item in self.process_results(response, ret):\n                 yield result_item\n \n-    def parse(self, response):\n+    def _parse(self, response):\n         if not hasattr(self, 'parse_node'):\n             raise NotConfigured('You must define parse_node method in order to scrape this XML feed')\n \n@@ -128,7 +128,7 @@ class CSVFeedSpider(Spider):\n             for result_item in self.process_results(response, ret):\n                 yield result_item\n \n-    def parse(self, response):\n+    def _parse(self, response):\n         if not hasattr(self, 'parse_row'):\n             raise NotConfigured('You must define parse_row method in order to scrape this CSV feed')\n         response = self.adapt_response(response)\n\n@@ -142,7 +142,7 @@ class XMLFeedSpiderTest(SpiderTest):\n \n         for iterator in ('iternodes', 'xml'):\n             spider = _XMLSpider('example', iterator=iterator)\n-            output = list(spider.parse(response))\n+            output = list(spider._parse(response))\n             self.assertEqual(len(output), 2, iterator)\n             self.assertEqual(output, [\n                 {'loc': [u'http://www.example.com/Special-Offers.html'],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5982e3477c732f4dff5accea6eab486e4ab52c3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 6 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 317 | Contributors (this commit): 11 | Commits (past 90d): 9 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -80,10 +80,10 @@ class Spider(object_ref):\n         \"\"\" This method is deprecated. \"\"\"\n         return Request(url, dont_filter=True)\n \n-    def _parse(self, response):\n-        return self.parse(response)\n+    def _parse(self, response, **kwargs):\n+        return self.parse(response, **kwargs)\n \n-    def parse(self, response):\n+    def parse(self, response, **kwargs):\n         raise NotImplementedError('{}.parse callback is not defined'.format(self.__class__.__name__))\n \n     @classmethod\n\n@@ -74,10 +74,15 @@ class CrawlSpider(Spider):\n         super(CrawlSpider, self).__init__(*a, **kw)\n         self._compile_rules()\n \n-    def _parse(self, response):\n-        return self._parse_response(response, self.parse_start_url, cb_kwargs={}, follow=True)\n+    def _parse(self, response, **kwargs):\n+        return self._parse_response(\n+            response=response,\n+            callback=self.parse_start_url,\n+            cb_kwargs=kwargs,\n+            follow=True,\n+        )\n \n-    def parse_start_url(self, response):\n+    def parse_start_url(self, response, **kwargs):\n         return []\n \n     def process_results(self, response, results):\n\n@@ -61,7 +61,7 @@ class XMLFeedSpider(Spider):\n             for result_item in self.process_results(response, ret):\n                 yield result_item\n \n-    def _parse(self, response):\n+    def _parse(self, response, **kwargs):\n         if not hasattr(self, 'parse_node'):\n             raise NotConfigured('You must define parse_node method in order to scrape this XML feed')\n \n@@ -128,7 +128,7 @@ class CSVFeedSpider(Spider):\n             for result_item in self.process_results(response, ret):\n                 yield result_item\n \n-    def _parse(self, response):\n+    def _parse(self, response, **kwargs):\n         if not hasattr(self, 'parse_row'):\n             raise NotConfigured('You must define parse_row method in order to scrape this CSV feed')\n         response = self.adapt_response(response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2fb160e3bac9e09373a49cb0b2d764ddf782987c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 273 | Contributors (this commit): 9 | Commits (past 90d): 10 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -71,14 +71,14 @@ class FTPDownloadHandler:\n         \"default\": 503,\n     }\n \n-    def __init__(self, settings, crawler=None):\n+    def __init__(self, settings):\n         self.default_user = settings['FTP_USER']\n         self.default_password = settings['FTP_PASSWORD']\n         self.passive_mode = settings['FTP_PASSIVE_MODE']\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        return cls(crawler.settings, crawler)\n+        return cls(crawler.settings)\n \n     def download_request(self, request, spider):\n         parsed_url = urlparse_cached(request)\n\n@@ -10,8 +10,8 @@ class HTTP10DownloadHandler:\n     lazy = False\n \n     def __init__(self, settings, crawler=None):\n-        self.HTTPClientFactory = load_object(crawler.settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n-        self.ClientContextFactory = load_object(crawler.settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n+        self.HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n+        self.ClientContextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n         self._settings = settings\n         self._crawler = crawler\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9a75b46fb8322d27ffd45ee6c187bc84a565e26d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 2193 | Contributors (this commit): 26 | Commits (past 90d): 19 | Contributors (cumulative): 37 | DMM Complexity: 0.0\n\nDIFF:\n@@ -29,7 +29,7 @@ class HTTP10DownloadHandler:\n         host, port = to_unicode(factory.host), factory.port\n         if factory.scheme == b'https':\n             client_context_factory = create_instance(\n-                self.ClientContextFactory,\n+                objcls=self.ClientContextFactory,\n                 settings=self._settings,\n                 crawler=self._crawler,\n             )\n\n@@ -41,7 +41,7 @@ class HTTP11DownloadHandler:\n         # try method-aware context factory\n         try:\n             self._contextFactory = create_instance(\n-                self._contextFactoryClass,\n+                objcls=self._contextFactoryClass,\n                 settings=settings,\n                 crawler=crawler,\n                 method=self._sslMethod,\n@@ -49,7 +49,7 @@ class HTTP11DownloadHandler:\n         except TypeError:\n             # use context factory defaults\n             self._contextFactory = create_instance(\n-                self._contextFactoryClass,\n+                objcls=self._contextFactoryClass,\n                 settings=settings,\n                 crawler=crawler,\n             )\n\n@@ -68,7 +68,11 @@ class S3DownloadHandler:\n             except Exception as ex:\n                 raise NotConfigured(str(ex))\n \n-        _http_handler = create_instance(httpdownloadhandler, settings, crawler)\n+        _http_handler = create_instance(\n+            objcls=httpdownloadhandler,\n+            settings=settings,\n+            crawler=crawler,\n+        )\n         self._download_http = _http_handler.download_request\n \n     @classmethod\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#982a66f9fb627575d42f0ad4fb2eb38b0a55b784", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 32 | Files Changed: 1 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 41 | Churn Cumulative: 1513 | Contributors (this commit): 20 | Commits (past 90d): 18 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -104,8 +104,7 @@ class FileTestCase(unittest.TestCase):\n         self.tmpname = self.mktemp()\n         with open(self.tmpname + '^', 'w') as f:\n             f.write('0123456789')\n-        crawler = get_crawler()\n-        handler = create_instance(FileDownloadHandler, crawler.settings, crawler)\n+        handler = create_instance(FileDownloadHandler, None, get_crawler())\n         self.download_request = handler.download_request\n \n     def tearDown(self):\n@@ -239,12 +238,7 @@ class HttpTestCase(unittest.TestCase):\n         else:\n             self.port = reactor.listenTCP(0, self.wrapper, interface=self.host)\n         self.portno = self.port.getHost().port\n-        crawler = get_crawler()\n-        self.download_handler = create_instance(\n-            objcls=self.download_handler_cls,\n-            settings=crawler.settings,\n-            crawler=crawler\n-        )\n+        self.download_handler = create_instance(self.download_handler_cls, None, get_crawler())\n         self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n@@ -485,11 +479,7 @@ class Http11TestCase(HttpTestCase):\n \n     def test_download_broken_content_allow_data_loss_via_setting(self, url='broken'):\n         crawler = get_crawler(settings_dict={'DOWNLOAD_FAIL_ON_DATALOSS': False})\n-        download_handler = create_instance(\n-            objcls=self.download_handler_cls,\n-            settings=crawler.settings,\n-            crawler=crawler\n-        )\n+        download_handler = create_instance(self.download_handler_cls, None, crawler)\n         request = Request(self.getURL(url))\n         d = download_handler.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.flags)\n@@ -508,11 +498,7 @@ class Https11TestCase(Http11TestCase):\n     @defer.inlineCallbacks\n     def test_tls_logging(self):\n         crawler = get_crawler(settings_dict={'DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING': True})\n-        download_handler = create_instance(\n-            objcls=self.download_handler_cls,\n-            settings=crawler.settings,\n-            crawler=crawler\n-        )\n+        download_handler = create_instance(self.download_handler_cls, None, crawler)\n         try:\n             with LogCapture() as log_capture:\n                 request = Request(self.getURL('file'))\n@@ -580,11 +566,7 @@ class Https11CustomCiphers(unittest.TestCase):\n             interface=self.host)\n         self.portno = self.port.getHost().port\n         crawler = get_crawler(settings_dict={'DOWNLOADER_CLIENT_TLS_CIPHERS': 'CAMELLIA256-SHA'})\n-        self.download_handler = create_instance(\n-            objcls=self.download_handler_cls,\n-            settings=crawler.settings,\n-            crawler=crawler\n-        )\n+        self.download_handler = create_instance(self.download_handler_cls, None, crawler)\n         self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n@@ -680,12 +662,7 @@ class HttpProxyTestCase(unittest.TestCase):\n         wrapper = WrappingFactory(site)\n         self.port = reactor.listenTCP(0, wrapper, interface='127.0.0.1')\n         self.portno = self.port.getHost().port\n-        crawler = get_crawler()\n-        self.download_handler = create_instance(\n-            objcls=self.download_handler_cls,\n-            settings=crawler.settings,\n-            crawler=crawler\n-        )\n+        self.download_handler = create_instance(self.download_handler_cls, None, get_crawler())\n         self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n@@ -764,7 +741,7 @@ class S3AnonTestCase(unittest.TestCase):\n         crawler = get_crawler()\n         self.s3reqh = create_instance(\n             objcls=S3DownloadHandler,\n-            settings=crawler.settings,\n+            settings=None,\n             crawler=crawler,\n             httpdownloadhandler=HttpDownloadHandlerMock,\n             # anon=True, # implicit\n@@ -796,7 +773,7 @@ class S3TestCase(unittest.TestCase):\n         crawler = get_crawler()\n         s3reqh = create_instance(\n             objcls=S3DownloadHandler,\n-            settings=crawler.settings,\n+            settings=None,\n             crawler=crawler,\n             aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n             aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n@@ -824,7 +801,7 @@ class S3TestCase(unittest.TestCase):\n             crawler = get_crawler()\n             create_instance(\n                 objcls=S3DownloadHandler,\n-                settings=crawler.settings,\n+                settings=None,\n                 crawler=crawler,\n                 extra_kw=True,\n             )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ab54e0d33e2a461059a9f1e9759c95b18e38da28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 282 | Contributors (this commit): 11 | Commits (past 90d): 9 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -33,7 +33,8 @@ def _get_boto_connection():\n \n class S3DownloadHandler:\n \n-    def __init__(self, settings, crawler=None,\n+    def __init__(self, settings, *,\n+                 crawler=None,\n                  aws_access_key_id=None, aws_secret_access_key=None,\n                  httpdownloadhandler=HTTPDownloadHandler, **kw):\n         if not aws_access_key_id:\n@@ -76,8 +77,8 @@ class S3DownloadHandler:\n         self._download_http = _http_handler.download_request\n \n     @classmethod\n-    def from_crawler(cls, crawler, *args, **kwargs):\n-        return cls(crawler.settings, crawler, *args, **kwargs)\n+    def from_crawler(cls, crawler, **kwargs):\n+        return cls(crawler.settings, crawler=crawler, **kwargs)\n \n     def download_request(self, request, spider):\n         p = urlparse_cached(request)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#87ece066ca320b07acda57c99aee8a62992ec144", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 16 | Churn Cumulative: 61 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,25 +1,17 @@\n+import asyncio\n from contextlib import suppress\n \n+from twisted.internet import asyncioreactor\n from twisted.internet.error import ReactorAlreadyInstalledError\n \n \n def install_asyncio_reactor():\n     \"\"\" Tries to install AsyncioSelectorReactor\n     \"\"\"\n-    try:\n-        import asyncio\n-        from twisted.internet import asyncioreactor\n-    except ImportError:\n-        return\n-\n     with suppress(ReactorAlreadyInstalledError):\n         asyncioreactor.install(asyncio.get_event_loop())\n \n \n def is_asyncio_reactor_installed():\n-    try:\n-        import twisted.internet.reactor\n-        from twisted.internet import asyncioreactor\n-        return isinstance(twisted.internet.reactor, asyncioreactor.AsyncioSelectorReactor)\n-    except ImportError:\n-        return False\n+    from twisted.internet import reactor\n+    return isinstance(reactor, asyncioreactor.AsyncioSelectorReactor)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#37ac47ff8074959ea66566fcb8b0e9e62272f963", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 25 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ class AsyncioTest(TestCase):\n \n     def test_is_asyncio_reactor_installed(self):\n         # the result should depend only on the pytest --reactor argument\n-        self.assertEquals(is_asyncio_reactor_installed(), self.reactor_pytest == 'asyncio')\n+        self.assertEqual(is_asyncio_reactor_installed(), self.reactor_pytest == 'asyncio')\n \n     def test_install_asyncio_reactor(self):\n         # this should do nothing\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8d4948f6ca44a76ee7714c8b4b1c46ef73a8845e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 66 | Churn Cumulative: 827 | Contributors (this commit): 12 | Commits (past 90d): 8 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -186,13 +186,39 @@ class DuplicateStartRequestsSpider(MockServerSpider):\n         self.visited += 1\n \n \n-class CrawlSpiderWithErrback(MockServerSpider, CrawlSpider):\n-    name = 'crawl_spider_with_errback'\n+class CrawlSpiderWithParseMethod(MockServerSpider, CrawlSpider):\n+    \"\"\"\n+    A CrawlSpider which overrides the 'parse' method\n+    \"\"\"\n+    name = 'crawl_spider_with_parse_method'\n     custom_settings = {\n         'RETRY_HTTP_CODES': [],  # no need to retry\n     }\n     rules = (\n-        Rule(LinkExtractor(), callback='callback', errback='errback', follow=True),\n+        Rule(LinkExtractor(), callback='parse', follow=True),\n+    )\n+\n+    def start_requests(self):\n+        test_body = b\"\"\"\n+        <html>\n+            <head><title>Page title<title></head>\n+            <body>\n+                <p><a href=\"/status?n=200\">Item 200</a></p>  <!-- callback -->\n+                <p><a href=\"/status?n=201\">Item 201</a></p>  <!-- callback -->\n+            </body>\n+        </html>\n+        \"\"\"\n+        url = self.mockserver.url(\"/alpayload\")\n+        yield Request(url, method=\"POST\", body=test_body)\n+\n+    def parse(self, response):\n+        self.logger.info('[parse] status %i', response.status)\n+\n+\n+class CrawlSpiderWithErrback(CrawlSpiderWithParseMethod):\n+    name = 'crawl_spider_with_errback'\n+    rules = (\n+        Rule(LinkExtractor(), callback='parse', errback='errback', follow=True),\n     )\n \n     def start_requests(self):\n@@ -211,8 +237,5 @@ class CrawlSpiderWithErrback(MockServerSpider, CrawlSpider):\n         url = self.mockserver.url(\"/alpayload\")\n         yield Request(url, method=\"POST\", body=test_body)\n \n-    def callback(self, response):\n-        self.logger.info('[callback] status %i', response.status)\n-\n     def errback(self, failure):\n         self.logger.info('[errback] status %i', failure.value.response.status)\n\n@@ -9,8 +9,9 @@ from scrapy.crawler import CrawlerRunner\n from scrapy.http import Request\n from scrapy.utils.python import to_unicode\n from tests.mockserver import MockServer\n-from tests.spiders import (FollowAllSpider, DelaySpider, SimpleSpider, BrokenStartRequestsSpider,\n-                           SingleRequestSpider, DuplicateStartRequestsSpider, CrawlSpiderWithErrback)\n+from tests.spiders import (BrokenStartRequestsSpider, CrawlSpiderWithErrback,\n+                           CrawlSpiderWithParseMethod, DelaySpider, SimpleSpider,\n+                           DuplicateStartRequestsSpider, FollowAllSpider, SingleRequestSpider)\n \n \n class CrawlTestCase(TestCase):\n@@ -297,6 +298,27 @@ with multiples lines\n         self._assert_retried(log)\n         self.assertIn(\"Got response 200\", str(log))\n \n+\n+class CrawlSpiderTestCase(TestCase):\n+\n+    def setUp(self):\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n+        self.runner = CrawlerRunner()\n+\n+    def tearDown(self):\n+        self.mockserver.__exit__(None, None, None)\n+\n+    @defer.inlineCallbacks\n+    def test_crawlspider_with_parse(self):\n+        self.runner.crawl(CrawlSpiderWithParseMethod, mockserver=self.mockserver)\n+\n+        with LogCapture() as log:\n+            yield self.runner.join()\n+\n+        self.assertIn(\"[parse] status 200\", str(log))\n+        self.assertIn(\"[parse] status 201\", str(log))\n+\n     @defer.inlineCallbacks\n     def test_crawlspider_with_errback(self):\n         self.runner.crawl(CrawlSpiderWithErrback, mockserver=self.mockserver)\n@@ -304,7 +326,8 @@ with multiples lines\n         with LogCapture() as log:\n             yield self.runner.join()\n \n-        self.assertIn(\"[callback] status 200\", str(log))\n-        self.assertIn(\"[callback] status 201\", str(log))\n+        self.assertIn(\"[parse] status 200\", str(log))\n+        self.assertIn(\"[parse] status 201\", str(log))\n         self.assertIn(\"[errback] status 404\", str(log))\n         self.assertIn(\"[errback] status 500\", str(log))\n+        self.assertIn(\"[errback] status 501\", str(log))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c54df8253a67bb6863a25bf7d667096adc73a040", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 842 | Contributors (this commit): 12 | Commits (past 90d): 10 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -211,8 +211,9 @@ class CrawlSpiderWithParseMethod(MockServerSpider, CrawlSpider):\n         url = self.mockserver.url(\"/alpayload\")\n         yield Request(url, method=\"POST\", body=test_body)\n \n-    def parse(self, response):\n-        self.logger.info('[parse] status %i', response.status)\n+    def parse(self, response, foo=None):\n+        self.logger.info('[parse] status %i (foo: %s)', response.status, foo)\n+        yield Request(self.mockserver.url(\"/status?n=202\"), self.parse, cb_kwargs={\"foo\": \"bar\"})\n \n \n class CrawlSpiderWithErrback(CrawlSpiderWithParseMethod):\n\n@@ -316,8 +316,9 @@ class CrawlSpiderTestCase(TestCase):\n         with LogCapture() as log:\n             yield self.runner.join()\n \n-        self.assertIn(\"[parse] status 200\", str(log))\n-        self.assertIn(\"[parse] status 201\", str(log))\n+        self.assertIn(\"[parse] status 200 (foo: None)\", str(log))\n+        self.assertIn(\"[parse] status 201 (foo: None)\", str(log))\n+        self.assertIn(\"[parse] status 202 (foo: bar)\", str(log))\n \n     @defer.inlineCallbacks\n     def test_crawlspider_with_errback(self):\n@@ -326,8 +327,9 @@ class CrawlSpiderTestCase(TestCase):\n         with LogCapture() as log:\n             yield self.runner.join()\n \n-        self.assertIn(\"[parse] status 200\", str(log))\n-        self.assertIn(\"[parse] status 201\", str(log))\n+        self.assertIn(\"[parse] status 200 (foo: None)\", str(log))\n+        self.assertIn(\"[parse] status 201 (foo: None)\", str(log))\n+        self.assertIn(\"[parse] status 202 (foo: bar)\", str(log))\n         self.assertIn(\"[errback] status 404\", str(log))\n         self.assertIn(\"[errback] status 500\", str(log))\n         self.assertIn(\"[errback] status 501\", str(log))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f75ccc997aa75fadf96a8d4b836248397ef89802", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 164 | Contributors (this commit): 10 | Commits (past 90d): 8 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -48,4 +48,4 @@ def reactor_pytest(request):\n @pytest.fixture(autouse=True)\n def only_asyncio(request, reactor_pytest):\n     if request.node.get_closest_marker('only_asyncio') and reactor_pytest != 'asyncio':\n-        pytest.skip('This test is only run with --reactor-asyncio')\n+        pytest.skip('This test is only run with --reactor=asyncio')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dc1ee09481c7655a8ebf77a75ccc965a4ba5400d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 40 | Files Changed: 7 | Hunks: 23 | Methods Changed: 14 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 78 | Churn Cumulative: 3988 | Contributors (this commit): 60 | Commits (past 90d): 32 | Contributors (cumulative): 103 | DMM Complexity: 1.0\n\nDIFF:\n@@ -137,6 +137,7 @@ class CrawlerRunner(object):\n         self._crawlers = set()\n         self._active = set()\n         self.bootstrap_failed = False\n+        self._handle_asyncio_reactor()\n \n     @property\n     def spiders(self):\n@@ -230,6 +231,11 @@ class CrawlerRunner(object):\n         while self._active:\n             yield defer.DeferredList(self._active)\n \n+    def _handle_asyncio_reactor(self):\n+        if self.settings.getbool('ASYNCIO_REACTOR') and not is_asyncio_reactor_installed():\n+            raise Exception(\"ASYNCIO_REACTOR is on but the Twisted asyncio \"\n+                            \"reactor is not installed.\")\n+\n \n class CrawlerProcess(CrawlerRunner):\n     \"\"\"\n@@ -257,12 +263,6 @@ class CrawlerProcess(CrawlerRunner):\n \n     def __init__(self, settings=None, install_root_handler=True):\n         super(CrawlerProcess, self).__init__(settings)\n-        if self.settings.getbool('ASYNCIO_ENABLED'):\n-            install_asyncio_reactor()\n-            if not is_asyncio_reactor_installed():\n-                raise Exception(\"ASYNCIO_ENABLED is on but the Twisted asyncio \"\n-                                \"reactor is not installed, this is not supported.\")\n-\n         install_shutdown_handlers(self._signal_shutdown)\n         configure_logging(self.settings, install_root_handler)\n         log_scrapy_info(self.settings)\n@@ -333,6 +333,11 @@ class CrawlerProcess(CrawlerRunner):\n         except RuntimeError:  # raised if already stopped or in shutdown stage\n             pass\n \n+    def _handle_asyncio_reactor(self):\n+        if self.settings.getbool('ASYNCIO_REACTOR'):\n+            install_asyncio_reactor()\n+        super()._handle_asyncio_reactor()\n+\n \n def _get_spider_loader(settings):\n     \"\"\" Get SpiderLoader instance from settings \"\"\"\n\n@@ -19,7 +19,7 @@ from os.path import join, abspath, dirname\n \n AJAXCRAWL_ENABLED = False\n \n-ASYNCIO_ENABLED = False\n+ASYNCIO_REACTOR = False\n \n AUTOTHROTTLE_ENABLED = False\n AUTOTHROTTLE_DEBUG = False\n\n@@ -11,6 +11,7 @@ from twisted.python import log as twisted_log\n import scrapy\n from scrapy.settings import Settings\n from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.asyncio import is_asyncio_reactor_installed\n from scrapy.utils.versions import scrapy_components_versions\n \n \n@@ -148,8 +149,8 @@ def log_scrapy_info(settings):\n                 {'versions': \", \".join(\"%s %s\" % (name, version)\n                     for name, version in scrapy_components_versions()\n                     if name != \"Scrapy\")})\n-    if settings.getbool('ASYNCIO_ENABLED'):\n-        logger.debug(\"Asyncio support enabled\")\n+    if is_asyncio_reactor_installed():\n+        logger.debug(\"Asyncio reactor is installed\")\n \n \n class StreamLogger(object):\n\n@@ -10,7 +10,7 @@ class NoRequestsSpider(scrapy.Spider):\n \n \n process = CrawlerProcess(settings={\n-    'ASYNCIO_ENABLED': True,\n+    'ASYNCIO_REACTOR': True,\n })\n \n process.crawl(NoRequestsSpider)\n\n@@ -15,7 +15,7 @@ class NoRequestsSpider(scrapy.Spider):\n \n \n process = CrawlerProcess(settings={\n-    'ASYNCIO_ENABLED': True,\n+    'ASYNCIO_REACTOR': True,\n })\n \n process.crawl(NoRequestsSpider)\n\n@@ -296,12 +296,12 @@ class BadSpider(scrapy.Spider):\n         self.assertIn(\"badspider.py\", log)\n \n     def test_asyncio_enabled_true(self):\n-        log = self.get_log(self.debug_log_spider, args=['-s', 'ASYNCIO_ENABLED=True'])\n-        self.assertIn(\"DEBUG: Asyncio support enabled\", log)\n+        log = self.get_log(self.debug_log_spider, args=['-s', 'ASYNCIO_REACTOR=True'])\n+        self.assertIn(\"DEBUG: Asyncio reactor is installed\", log)\n \n     def test_asyncio_enabled_false(self):\n-        log = self.get_log(self.debug_log_spider, args=['-s', 'ASYNCIO_ENABLED=False'])\n-        self.assertNotIn(\"DEBUG: Asyncio support enabled\", log)\n+        log = self.get_log(self.debug_log_spider, args=['-s', 'ASYNCIO_REACTOR=False'])\n+        self.assertNotIn(\"DEBUG: Asyncio reactor is installed\", log)\n \n \n class BenchCommandTest(CommandTest):\n\n@@ -13,7 +13,6 @@ import scrapy\n from scrapy.crawler import Crawler, CrawlerRunner, CrawlerProcess\n from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n-from scrapy.utils.asyncio import is_asyncio_reactor_installed\n from scrapy.utils.log import configure_logging, get_scrapy_root_handler\n from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.misc import load_object\n@@ -209,14 +208,6 @@ class NoRequestsSpider(scrapy.Spider):\n         return []\n \n \n-class AsyncioSpider(scrapy.Spider):\n-    name = 'asyncio'\n-\n-    def start_requests(self):\n-        self.logger.info('Asyncio support: %s', is_asyncio_reactor_installed())\n-        return []\n-\n-\n @mark.usefixtures('reactor_pytest')\n class CrawlerRunnerHasSpider(unittest.TestCase):\n \n@@ -261,31 +252,32 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n \n         self.assertEqual(runner.bootstrap_failed, True)\n \n+    def test_crawler_runner_asyncio_enabled_true(self):\n+        if self.reactor_pytest == 'asyncio':\n+            runner = CrawlerRunner(settings={'ASYNCIO_REACTOR': True})\n+        else:\n+            msg = \"ASYNCIO_REACTOR is on but the Twisted asyncio reactor is not installed\"\n+            with self.assertRaisesRegex(Exception, msg):\n+                runner = CrawlerRunner(settings={'ASYNCIO_REACTOR': True})\n+\n     @defer.inlineCallbacks\n     def test_crawler_process_asyncio_enabled_true(self):\n         with LogCapture(level=logging.DEBUG) as log:\n             if self.reactor_pytest == 'asyncio':\n-                runner = CrawlerProcess(settings={'ASYNCIO_ENABLED': True})\n+                runner = CrawlerProcess(settings={'ASYNCIO_REACTOR': True})\n                 yield runner.crawl(NoRequestsSpider)\n-                self.assertIn(\"Asyncio support enabled\", str(log))\n+                self.assertIn(\"Asyncio reactor is installed\", str(log))\n             else:\n-                msg = \"ASYNCIO_ENABLED is on but the Twisted asyncio reactor is not installed\"\n+                msg = \"ASYNCIO_REACTOR is on but the Twisted asyncio reactor is not installed\"\n                 with self.assertRaisesRegex(Exception, msg):\n-                    runner = CrawlerProcess(settings={'ASYNCIO_ENABLED': True})\n+                    runner = CrawlerProcess(settings={'ASYNCIO_REACTOR': True})\n \n     @defer.inlineCallbacks\n     def test_crawler_process_asyncio_enabled_false(self):\n-        runner = CrawlerProcess(settings={'ASYNCIO_ENABLED': False})\n+        runner = CrawlerProcess(settings={'ASYNCIO_REACTOR': False})\n         with LogCapture(level=logging.DEBUG) as log:\n             yield runner.crawl(NoRequestsSpider)\n-            self.assertNotIn(\"Asyncio support enabled\", str(log))\n-\n-    @defer.inlineCallbacks\n-    def test_crawler_runner_asyncio_supported(self):\n-        runner = CrawlerRunner()\n-        with LogCapture() as log:\n-            yield runner.crawl(AsyncioSpider)\n-            log.check_present(('asyncio', 'INFO', 'Asyncio support: %s' % (self.reactor_pytest == 'asyncio')))\n+            self.assertNotIn(\"Asyncio reactor is installed\", str(log))\n \n \n class CrawlerProcessSubprocess(unittest.TestCase):\n@@ -302,14 +294,14 @@ class CrawlerProcessSubprocess(unittest.TestCase):\n     def test_simple(self):\n         log = self.run_script('simple.py')\n         self.assertIn('Spider closed (finished)', log)\n-        self.assertNotIn(\"DEBUG: Asyncio support enabled\", log)\n+        self.assertNotIn(\"DEBUG: Asyncio reactor is installed\", log)\n \n     def test_asyncio_enabled_no_reactor(self):\n         log = self.run_script('asyncio_enabled_no_reactor.py')\n         self.assertIn('Spider closed (finished)', log)\n-        self.assertIn(\"DEBUG: Asyncio support enabled\", log)\n+        self.assertIn(\"DEBUG: Asyncio reactor is installed\", log)\n \n     def test_asyncio_enabled_reactor(self):\n         log = self.run_script('asyncio_enabled_reactor.py')\n         self.assertIn('Spider closed (finished)', log)\n-        self.assertIn(\"DEBUG: Asyncio support enabled\", log)\n+        self.assertIn(\"DEBUG: Asyncio reactor is installed\", log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#21f50c795ac6978de9e73b64f31542a9df928ac5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 32 | Churn Cumulative: 377 | Contributors (this commit): 12 | Commits (past 90d): 8 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,7 @@ from twisted.internet import defer\n from scrapy.exceptions import _InvalidOutput\n from scrapy.http import Request, Response\n from scrapy.middleware import MiddlewareManager\n-from scrapy.utils.defer import mustbe_deferred\n+from scrapy.utils.defer import mustbe_deferred, deferred_from_coro\n from scrapy.utils.conf import build_component_list\n \n \n@@ -33,7 +33,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n         @defer.inlineCallbacks\n         def process_request(request):\n             for method in self.methods['process_request']:\n-                response = yield method(request=request, spider=spider)\n+                response = yield deferred_from_coro(method(request=request, spider=spider))\n                 if response is not None and not isinstance(response, (Response, Request)):\n                     raise _InvalidOutput('Middleware %s.process_request must return None, Response or Request, got %s' % \\\n                                          (method.__self__.__class__.__name__, response.__class__.__name__))\n@@ -48,7 +48,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                 defer.returnValue(response)\n \n             for method in self.methods['process_response']:\n-                response = yield method(request=request, response=response, spider=spider)\n+                response = yield deferred_from_coro(method(request=request, response=response, spider=spider))\n                 if not isinstance(response, (Response, Request)):\n                     raise _InvalidOutput('Middleware %s.process_response must return Response or Request, got %s' % \\\n                                          (method.__self__.__class__.__name__, type(response)))\n@@ -60,7 +60,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n         def process_exception(_failure):\n             exception = _failure.value\n             for method in self.methods['process_exception']:\n-                response = yield method(request=request, exception=exception, spider=spider)\n+                response = yield deferred_from_coro(method(request=request, exception=exception, spider=spider))\n                 if response is not None and not isinstance(response, (Response, Request)):\n                     raise _InvalidOutput('Middleware %s.process_exception must return None, Response or Request, got %s' % \\\n                                          (method.__self__.__class__.__name__, type(response)))\n\n@@ -1,3 +1,4 @@\n+import asyncio\n from unittest import mock\n \n from twisted.internet.defer import Deferred\n@@ -206,3 +207,26 @@ class MiddlewareUsingDeferreds(ManagerTestCase):\n \n         self.assertIs(results[0], resp)\n         self.assertFalse(download_func.called)\n+\n+\n+class MiddlewareUsingCoro(ManagerTestCase):\n+    \"\"\"Middlewares using asyncio coroutines should work\"\"\"\n+\n+    def test_asyncdef(self):\n+        resp = Response('http://example.com/index.html')\n+\n+        class CoroMiddleware:\n+            async def process_request(self, request, spider):\n+                await asyncio.sleep(0.1)\n+                return resp\n+\n+        self.mwman._add_middleware(CoroMiddleware())\n+        req = Request('http://example.com/index.html')\n+        download_func = mock.MagicMock()\n+        dfd = self.mwman.download(download_func, req, self.spider)\n+        results = []\n+        dfd.addBoth(results.append)\n+        self._wait(dfd)\n+\n+        self.assertIs(results[0], resp)\n+        self.assertFalse(download_func.called)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3603644552f8d15d203abca221edb56047119528", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 20 | Churn Cumulative: 188 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n import asyncio\n from unittest import mock\n \n+from twisted.internet import defer\n from twisted.internet.defer import Deferred\n from twisted.trial.unittest import TestCase\n from twisted.python.failure import Failure\n@@ -215,6 +216,25 @@ class MiddlewareUsingCoro(ManagerTestCase):\n     def test_asyncdef(self):\n         resp = Response('http://example.com/index.html')\n \n+        class CoroMiddleware:\n+            async def process_request(self, request, spider):\n+                await defer.succeed(42)\n+                return resp\n+\n+        self.mwman._add_middleware(CoroMiddleware())\n+        req = Request('http://example.com/index.html')\n+        download_func = mock.MagicMock()\n+        dfd = self.mwman.download(download_func, req, self.spider)\n+        results = []\n+        dfd.addBoth(results.append)\n+        self._wait(dfd)\n+\n+        self.assertIs(results[0], resp)\n+        self.assertFalse(download_func.called)\n+\n+    def test_asyncdef_asyncio(self):\n+        resp = Response('http://example.com/index.html')\n+\n         class CoroMiddleware:\n             async def process_request(self, request, spider):\n                 await asyncio.sleep(0.1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5cf1ac0005fa3a174b700d88c0e2536b689f13c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 195 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,7 @@\n import asyncio\n from unittest import mock\n \n+from pytest import mark\n from twisted.internet import defer\n from twisted.internet.defer import Deferred\n from twisted.trial.unittest import TestCase\n@@ -232,6 +233,12 @@ class MiddlewareUsingCoro(ManagerTestCase):\n         self.assertIs(results[0], resp)\n         self.assertFalse(download_func.called)\n \n+\n+@mark.only_asyncio()\n+class MiddlewareUsingCoroAsyncio(ManagerTestCase):\n+\n+    settings_dict = {'ASYNCIO_ENABLED': True}\n+\n     def test_asyncdef_asyncio(self):\n         resp = Response('http://example.com/index.html')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#16787f5bf4475fe1604c1e4cac7b491f1df6a1fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 200 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -233,12 +233,7 @@ class MiddlewareUsingCoro(ManagerTestCase):\n         self.assertIs(results[0], resp)\n         self.assertFalse(download_func.called)\n \n-\n     @mark.only_asyncio()\n-class MiddlewareUsingCoroAsyncio(ManagerTestCase):\n-\n-    settings_dict = {'ASYNCIO_ENABLED': True}\n-\n     def test_asyncdef_asyncio(self):\n         resp = Response('http://example.com/index.html')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2b9254c2bde76995e81c54ad112ae884a1499386", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 525 | Contributors (this commit): 17 | Commits (past 90d): 6 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n \"\"\"\n This module contains some assorted functions used in tests\n \"\"\"\n-\n+import asyncio\n import os\n \n from importlib import import_module\n@@ -96,3 +96,10 @@ def assert_samelines(testcase, text1, text2, msg=None):\n     line endings between platforms\n     \"\"\"\n     testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)\n+\n+\n+def get_from_asyncio_queue(value):\n+    q = asyncio.Queue()\n+    getter = q.get()\n+    q.put_nowait(value)\n+    return getter\n\n@@ -11,7 +11,7 @@ from scrapy.http import Request, Response\n from scrapy.spiders import Spider\n from scrapy.exceptions import _InvalidOutput\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n-from scrapy.utils.test import get_crawler\n+from scrapy.utils.test import get_crawler, get_from_asyncio_queue\n from scrapy.utils.python import to_bytes\n \n \n@@ -240,7 +240,8 @@ class MiddlewareUsingCoro(ManagerTestCase):\n         class CoroMiddleware:\n             async def process_request(self, request, spider):\n                 await asyncio.sleep(0.1)\n-                return resp\n+                result = await get_from_asyncio_queue(resp)\n+                return result\n \n         self.mwman._add_middleware(CoroMiddleware())\n         req = Request('http://example.com/index.html')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#81175669746737f1ab0bd0236a70f787ed1855a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 320 | Contributors (this commit): 14 | Commits (past 90d): 8 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@\n Helper functions for dealing with Twisted deferreds\n \"\"\"\n import asyncio\n+from functools import wraps\n import inspect\n \n from twisted.internet import defer, task\n@@ -140,3 +141,15 @@ def deferred_from_coro(o):\n             # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n             return defer.Deferred.fromFuture(asyncio.ensure_future(o))\n     return o\n+\n+\n+def deferred_f_from_coro_f(coro_f):\n+    \"\"\" Converts a coroutine function into a function that returns a Deferred.\n+\n+    The coroutine function will be called at the time when the wrapper is called. Wrapper args will be passed to it.\n+    This is useful for callback chains, as callback functions are called with the previous callback result.\n+    \"\"\"\n+    @wraps(coro_f)\n+    def f(*coro_args, **coro_kwargs):\n+        return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))\n+    return f\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1f9cef787d3ca0c12099f1b1b4c52efc510e381d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 17 | Churn Cumulative: 101 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,8 @@ See documentation in docs/item-pipeline.rst\n \n from scrapy.middleware import MiddlewareManager\n from scrapy.utils.conf import build_component_list\n+from scrapy.utils.defer import deferred_f_from_coro_f\n+\n \n \n class ItemPipelineManager(MiddlewareManager):\n@@ -19,7 +21,7 @@ class ItemPipelineManager(MiddlewareManager):\n     def _add_middleware(self, pipe):\n         super(ItemPipelineManager, self)._add_middleware(pipe)\n         if hasattr(pipe, 'process_item'):\n-            self.methods['process_item'].append(pipe.process_item)\n+            self.methods['process_item'].append(deferred_f_from_coro_f(pipe.process_item))\n \n     def process_item(self, item, spider):\n         return self._process_chain('process_item', item, spider)\n\n@@ -26,6 +26,13 @@ class DeferredPipeline:\n         return d\n \n \n+class AsyncDefPipeline:\n+    async def process_item(self, item, spider):\n+        await defer.succeed(42)\n+        item['pipeline_passed'] = True\n+        return item\n+\n+\n class ItemSpider(Spider):\n     name = 'itemspider'\n \n@@ -69,3 +76,9 @@ class PipelineTestCase(unittest.TestCase):\n         crawler = self._create_crawler(DeferredPipeline)\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertEqual(len(self.items), 1)\n+\n+    @defer.inlineCallbacks\n+    def test_asyncdef_pipeline(self):\n+        crawler = self._create_crawler(AsyncDefPipeline)\n+        yield crawler.crawl(mockserver=self.mockserver)\n+        self.assertEqual(len(self.items), 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
