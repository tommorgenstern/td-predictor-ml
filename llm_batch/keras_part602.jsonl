{"custom_id": "keras#b90efa284cb7c5297ef80e6ce9989cb4dc0bd026", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 18 | Files Changed: 4 | Hunks: 12 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 32 | Churn Cumulative: 4509 | Contributors (this commit): 6 | Commits (past 90d): 40 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,13 +2,10 @@ import numpy as np\n import torch\n \n from keras_core.backend.common import KerasVariable\n-from keras_core.backend.common import get_autocast_scope\n from keras_core.backend.common import standardize_dtype\n-from keras_core.backend.common import standardize_shape\n from keras_core.backend.common.stateless_scope import get_stateless_scope\n from keras_core.backend.common.stateless_scope import in_stateless_scope\n \n-\n DYNAMIC_SHAPES_OK = True\n \n \n\n@@ -1,7 +1,3 @@\n-import numpy as np\n-import torch\n-\n-\n def add(x1, x2):\n     pass\n     # return tfnp.add(x1, x2)\n@@ -617,7 +613,7 @@ def tril(x, k=0):\n \n def triu(x, k=0):\n     pass\n-    return tfnp.triu(x, k=k)\n+    # return tfnp.triu(x, k=k)\n \n \n def vdot(x1, x2):\n\n@@ -69,15 +69,15 @@ class ModelCheckpoint(Callback):\n     ```\n \n     Args:\n-        filepath: string or `PathLike`, path to save the model file. e.g.\n-            filepath = os.path.join(working_dir, 'ckpt', file_name). `filepath`\n-            can contain named formatting options, which will be filled the\n-            value of `epoch` and keys in `logs` (passed in `on_epoch_end`).\n+        filepath: string or `PathLike`, path to save the model file.\n+            `filepath` can contain named formatting options,\n+            which will be filled the value of `epoch` and keys in `logs`\n+            (passed in `on_epoch_end`).\n             The `filepath` name needs to end with `\".weights.h5\"` when\n             `save_weights_only=True` or should end with `\".keras\"` when\n-            checkpoint saving the whole model (default)\".\n+            checkpoint saving the whole model (default).\n             For example:\n-            if `filepath` is `{epoch:02d}-{val_loss:.2f}.keras`, then the\n+            if `filepath` is `\"{epoch:02d}-{val_loss:.2f}.keras\"`, then the\n             model checkpoints will be saved with the epoch number and the\n             validation loss in the filename. The directory of the filepath\n             should not be reused by any other callbacks to avoid conflicts.\n@@ -322,16 +322,17 @@ class ModelCheckpoint(Callback):\n         This utility function is best demonstrated via an example:\n \n         ```python\n-        file_pattern = 'f.batch{batch:02d}epoch{epoch:02d}.keras'\n+        file_pattern = 'batch{batch:02d}epoch{epoch:02d}.keras'\n         test_dir = self.get_temp_dir()\n         path_pattern = os.path.join(test_dir, file_pattern)\n         file_paths = [\n             os.path.join(test_dir, file_name) for file_name in\n-            ['f.batch03epoch02.keras',\n-             'f.batch02epoch02.keras', 'f.batch01epoch01.keras']\n+            ['batch03epoch02.keras',\n+             'batch02epoch02.keras', 'batch01epoch01.keras']\n         ]\n         for file_path in file_paths:\n             # Write something to each of the files\n+            ...\n         self.assertEqual(\n             _get_most_recently_modified_file_matching_pattern(path_pattern),\n             file_paths[-1])\n\n@@ -19,7 +19,9 @@ elif backend.backend() == \"jax\":\n elif backend.backend() == \"pytorch\":\n     from keras_core.backend.pytorch.trainer import PyTorchTrainer as Trainer\n else:\n-    raise RuntimeError(f\"Backend '{backend.backend()}' must implement the Trainer class.\")\n+    raise RuntimeError(\n+        f\"Backend '{backend.backend()}' must implement the Trainer class.\"\n+    )\n \n \n @keras_core_export([\"keras_core.Model\", \"keras_core.models.Model\"])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#87f59638dd33ea161439168a7aa196990fd50119", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 13 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 57 | Churn Cumulative: 3627 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,6 +8,27 @@ from keras_core.backend.common.stateless_scope import in_stateless_scope\n \n DYNAMIC_SHAPES_OK = True\n \n+TORCH_DTYPES = {\n+    \"float16\": torch.float16,\n+    \"float32\": torch.float32,\n+    \"float64\": torch.float64,\n+    \"uint8\": torch.uint8,\n+    \"int8\": torch.int8,\n+    \"int16\": torch.int16,\n+    \"int32\": torch.int32,\n+    \"int64\": torch.int64,\n+    \"bfloat16\": torch.bfloat16,\n+    \"bool\": torch.bool,\n+}\n+\n+\n+def to_torch_dtype(dtype):\n+    dtype = standardize_dtype(dtype)\n+    dtype = TORCH_DTYPES.get(dtype, None)\n+    if dtype is None:\n+        raise ValueError(f\"Unsupported dtype for PyTorch: {dtype}\")\n+    return dtype\n+\n \n class Variable(KerasVariable):\n     def _initialize(self, value):\n@@ -63,15 +84,11 @@ class Variable(KerasVariable):\n \n def convert_to_tensor(x, dtype=None):\n     # TODO: Need to address device placement arg of `as_tensor`\n-    if dtype is not None:\n-        dtype = standardize_dtype(dtype)\n+    dtype = to_torch_dtype(dtype)\n     if isinstance(x, Variable):\n         if dtype and dtype != x.dtype:\n-            # TODO: fix below\n-            return x.value.astype(dtype)\n+            return x.value.to(dtype)\n         return x.value\n-    if dtype == \"float32\":\n-        dtype = torch.float32\n     return torch.as_tensor(x, dtype=dtype)\n \n \n@@ -84,6 +101,7 @@ def shape(x):\n \n \n def cast(x, dtype):\n+    dtype = to_torch_dtype(dtype)\n     return x.to(dtype)\n \n \n\n@@ -1,6 +1,9 @@\n+import torch\n+from keras_core.backend.pytorch.core import to_torch_dtype\n+\n+\n def add(x1, x2):\n-    pass\n-    # return tfnp.add(x1, x2)\n+    return x1 + x2\n \n \n def einsum(subscripts, *operands, **kwargs):\n@@ -9,8 +12,7 @@ def einsum(subscripts, *operands, **kwargs):\n \n \n def subtract(x1, x2):\n-    pass\n-    # return tfnp.subtract(x1, x2)\n+    return x1 - x2\n \n \n def matmul(x1, x2):\n@@ -19,17 +21,16 @@ def matmul(x1, x2):\n \n \n def multiply(x1, x2):\n-    pass\n-    # return tfnp.multiply(x1, x2)\n+    return x1 * x2\n \n \n def mean(x, axis=None, keepdims=False):\n-    pass\n-    # return tfnp.mean(x, axis=axis, keepdims=keepdims)\n+    return torch.mean(x, dim=axis, keepdim=keepdims)\n \n \n def max(x, axis=None, keepdims=False, initial=None):\n-    pass\n+    # TODO: handle initial\n+    return torch.max(x, dim=axis, keepdim=keepdims)\n     # The TensorFlow numpy API implementation doesn't support `initial` so we\n     # handle it manually here.\n     # if initial is not None:\n@@ -49,13 +50,13 @@ def max(x, axis=None, keepdims=False, initial=None):\n \n \n def ones(shape, dtype=\"float32\"):\n-    pass\n-    # return tf.ones(shape, dtype=dtype)\n+    dtype = to_torch_dtype(dtype)\n+    torch.ones(*shape, dtype=dtype)\n \n \n def zeros(shape, dtype=\"float32\"):\n-    pass\n-    # return tf.zeros(shape, dtype=dtype)\n+    dtype = to_torch_dtype(dtype)\n+    torch.zeros(*shape, dtype=dtype)\n \n \n def absolute(x):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0f419b37f6e94c768edbf6e5a0ebabec5d6bfdeb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 3776 | Contributors (this commit): 6 | Commits (past 90d): 27 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,8 @@ import numpy as np\n from keras_core.backend.config import backend\n \n if backend() == \"pytorch\":\n-    # TODO: comment explanation\n+    # torch needs to be imported first, otherwise it will\n+    # segfault upon import.\n     import torch\n \n from keras_core.backend.common.keras_tensor import KerasTensor\n\n@@ -1,4 +1,5 @@\n import torch\n+\n from keras_core.backend.pytorch.core import to_torch_dtype\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b2b8e1424c337a31c0ea2bff3c2df949444889b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 34 | Files Changed: 12 | Hunks: 9 | Methods Changed: 0 | Complexity Δ (Sum/Max): 124/123 | Churn Δ: 64 | Churn Cumulative: 1659 | Contributors (this commit): 7 | Commits (past 90d): 68 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -1,13 +1,9 @@\n-import json\n-import os\n-\n-import numpy as np\n-\n from keras_core.backend.config import backend\n \n-if backend() == \"pytorch\":\n-    # torch needs to be imported first, otherwise it will\n-    # segfault upon import.\n+if backend() == \"torch\":\n+    # When using the torch backend,\n+    # torch needs to be imported first, otherwise it will segfault\n+    # upon import.\n     import torch\n \n from keras_core.backend.common.keras_tensor import KerasTensor\n@@ -37,8 +33,8 @@ if backend() == \"tensorflow\":\n elif backend() == \"jax\":\n     print_msg(\"Using JAX backend.\")\n     from keras_core.backend.jax import *  # noqa: F403\n-elif backend() == \"pytorch\":\n+elif backend() == \"torch\":\n     print_msg(\"Using PyTorch backend.\")\n-    from keras_core.backend.pytorch import *  # noqa: F403\n+    from keras_core.backend.torch import *  # noqa: F403\n else:\n     raise ValueError(f\"Unable to import backend : {backend()}\")\n\n@@ -1,20 +0,0 @@\n-from keras_core.backend.pytorch import core\n-from keras_core.backend.pytorch import image\n-from keras_core.backend.pytorch import math\n-from keras_core.backend.pytorch import nn\n-from keras_core.backend.pytorch import numpy\n-from keras_core.backend.pytorch import random\n-from keras_core.backend.pytorch.core import DYNAMIC_SHAPES_OK\n-from keras_core.backend.pytorch.core import Variable\n-from keras_core.backend.pytorch.core import cast\n-from keras_core.backend.pytorch.core import compute_output_spec\n-from keras_core.backend.pytorch.core import cond\n-from keras_core.backend.pytorch.core import convert_to_tensor\n-from keras_core.backend.pytorch.core import is_tensor\n-from keras_core.backend.pytorch.core import name_scope\n-from keras_core.backend.pytorch.core import scatter\n-from keras_core.backend.pytorch.core import shape\n-from keras_core.backend.pytorch.core import vectorized_map\n-from keras_core.backend.pytorch.rnn import gru\n-from keras_core.backend.pytorch.rnn import lstm\n-from keras_core.backend.pytorch.rnn import rnn\n\n@@ -0,0 +1,20 @@\n+from keras_core.backend.torch import core\n+from keras_core.backend.torch import image\n+from keras_core.backend.torch import math\n+from keras_core.backend.torch import nn\n+from keras_core.backend.torch import numpy\n+from keras_core.backend.torch import random\n+from keras_core.backend.torch.core import DYNAMIC_SHAPES_OK\n+from keras_core.backend.torch.core import Variable\n+from keras_core.backend.torch.core import cast\n+from keras_core.backend.torch.core import compute_output_spec\n+from keras_core.backend.torch.core import cond\n+from keras_core.backend.torch.core import convert_to_tensor\n+from keras_core.backend.torch.core import is_tensor\n+from keras_core.backend.torch.core import name_scope\n+from keras_core.backend.torch.core import scatter\n+from keras_core.backend.torch.core import shape\n+from keras_core.backend.torch.core import vectorized_map\n+from keras_core.backend.torch.rnn import gru\n+from keras_core.backend.torch.rnn import lstm\n+from keras_core.backend.torch.rnn import rnn\n\n\n\n\n\n@@ -1,6 +1,6 @@\n import torch\n \n-from keras_core.backend.pytorch.core import to_torch_dtype\n+from keras_core.backend.torch.core import to_torch_dtype\n \n \n def add(x1, x2):\n\n\n\n@@ -1,6 +1,6 @@\n from keras_core.trainers import trainer as base_trainer\n \n \n-class PyTorchTrainer(base_trainer.Trainer):\n+class TorchTrainer(base_trainer.Trainer):\n     def __init__(self):\n         super().__init__()\n\n@@ -16,8 +16,8 @@ if backend.backend() == \"tensorflow\":\n     )\n elif backend.backend() == \"jax\":\n     from keras_core.backend.jax.trainer import JAXTrainer as Trainer\n-elif backend.backend() == \"pytorch\":\n-    from keras_core.backend.pytorch.trainer import PyTorchTrainer as Trainer\n+elif backend.backend() == \"torch\":\n+    from keras_core.backend.torch.trainer import TorchTrainer as Trainer\n else:\n     raise RuntimeError(\n         f\"Backend '{backend.backend()}' must implement the Trainer class.\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#93715d04cc596d05c7486362bbd4b535939d2ea7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 130 | Lines Deleted: 19 | Files Changed: 13 | Hunks: 21 | Methods Changed: 5 | Complexity Δ (Sum/Max): 52/35 | Churn Δ: 149 | Churn Cumulative: 1298 | Contributors (this commit): 6 | Commits (past 90d): 85 | Contributors (cumulative): 29 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -1,8 +1,8 @@\n from keras_core.backend.common import backend_utils\n-from keras_core.backend.common import random\n from keras_core.backend.common.variables import AutocastScope\n from keras_core.backend.common.variables import KerasVariable\n from keras_core.backend.common.variables import get_autocast_scope\n from keras_core.backend.common.variables import is_float_dtype\n from keras_core.backend.common.variables import standardize_dtype\n from keras_core.backend.common.variables import standardize_shape\n+from keras_core.random import random\n\n@@ -4,6 +4,7 @@ from keras_core.api_export import keras_core_export\n from keras_core.utils.naming import auto_name\n \n \n+@keras_core_export(\"keras_core.KerasTensor\")\n class KerasTensor:\n     def __init__(self, shape, dtype=\"float32\", record_history=True, name=None):\n         from keras_core import backend\n\n@@ -1,8 +1,10 @@\n+from keras_core.api_export import keras_core_export\n from keras_core.backend.common import global_state\n from keras_core.backend.common.variables import KerasVariable\n from keras_core.backend.common.variables import initialize_all_variables\n \n \n+@keras_core_export(\"keras_core.StatelessScope\")\n class StatelessScope:\n     def __init__(self, state_mapping=None, collect_losses=False):\n         from keras_core import backend\n\n@@ -33,11 +33,18 @@ class KerasVariable:\n                     \"You are attempting to create a variable \"\n                     \"while in a stateless scope. This is disallowed. \"\n                     \"Make sure that all variables are created \"\n-                    \"before you start using your layer/model objects. \"\n-                    \"Most of this time, this means you need to \"\n+                    \"before you start using your layer/model objects.\\n\\n\"\n+                    \"In some cases, you might be seeing this error \"\n+                    \"because you need to \"\n                     \"implement a `def build(self, input_shape)` method \"\n                     \"on your layer/model, which will \"\n-                    \"create its variables.\"\n+                    \"create its variables.\\n\\n\"\n+                    \"In some other cases, you might be seeing this error \"\n+                    \"because you are instantiating a `Variable` and \"\n+                    \"assigning it to a layer without going through \"\n+                    \"self.add_variable()/self.add_weight(). Always prefer \"\n+                    \"using these methods \"\n+                    \"(with a `shape` and `initializer` argument).\"\n                 )\n         else:\n             if callable(initializer):\n\n@@ -1,9 +1,9 @@\n import jax\n \n-from keras_core.backend.common.random import SeedGenerator\n-from keras_core.backend.common.random import draw_seed\n-from keras_core.backend.common.random import make_default_seed\n from keras_core.backend.config import floatx\n+from keras_core.random.seed_generator import SeedGenerator\n+from keras_core.random.seed_generator import draw_seed\n+from keras_core.random.seed_generator import make_default_seed\n \n \n def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n\n@@ -1,9 +1,9 @@\n import tensorflow as tf\n \n-from keras_core.backend.common.random import SeedGenerator\n-from keras_core.backend.common.random import draw_seed\n-from keras_core.backend.common.random import make_default_seed\n from keras_core.backend.config import floatx\n+from keras_core.random.seed_generator import SeedGenerator\n+from keras_core.random.seed_generator import draw_seed\n+from keras_core.random.seed_generator import make_default_seed\n \n \n def tf_draw_seed(seed):\n\n@@ -1,7 +1,7 @@\n-from keras_core.backend.common.random import SeedGenerator\n-from keras_core.backend.common.random import draw_seed\n-from keras_core.backend.common.random import make_default_seed\n from keras_core.backend.config import floatx\n+from keras_core.random.seed_generator import SeedGenerator\n+from keras_core.random.seed_generator import draw_seed\n+from keras_core.random.seed_generator import make_default_seed\n \n \n def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n\n@@ -2,10 +2,12 @@ import collections\n \n from tensorflow import nest\n \n+from keras_core.api_export import keras_core_export\n from keras_core.backend import KerasTensor\n from keras_core.operations.operation import Operation\n \n \n+@keras_core_export(\"keras_core.Function\")\n class Function(Operation):\n     def __init__(self, inputs, outputs, name=None):\n         super().__init__(name=name)\n\n@@ -4,6 +4,7 @@ import textwrap\n from tensorflow import nest\n \n from keras_core import backend\n+from keras_core.api_export import keras_core_export\n from keras_core.backend.common.keras_tensor import any_symbolic_tensors\n from keras_core.operations.node import Node\n from keras_core.utils import python_utils\n@@ -11,6 +12,7 @@ from keras_core.utils import traceback_utils\n from keras_core.utils.naming import auto_name\n \n \n+@keras_core_export(\"keras_core.Operation\")\n class Operation:\n     def __init__(self, name=None):\n         if name is None:\n\n@@ -1,6 +0,0 @@\n-from keras_core import backend\n-\n-dropout = backend.random.dropout\n-normal = backend.random.normal\n-truncated_normal = backend.random.truncated_normal\n-uniform = backend.random.uniform\n\n\n@@ -0,0 +1,100 @@\n+from keras_core import backend\n+from keras_core.api_export import keras_core_export\n+\n+\n+@keras_core_export(\"keras_core.random.normal\")\n+def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n+    \"\"\"Draw random samples from a normal (Gaussian) distribution.\n+\n+    Args:\n+        shape: The shape of the random values to generate.\n+        mean: Floats, defaults to 0. Mean of the random values to generate.\n+        stddev: Floats, defaults to 1. Standard deviation of the random values\n+            to generate.\n+        dtype: Optional dtype of the tensor. Only floating point types are\n+            supported. If not specified, `keras_core.config.floatx()` is used,\n+            which defaults to `float32` unless you configured it otherwise (via\n+            `keras_core.config.set_floatx(float_dtype)`).\n+        seed: A Python integer or instance of\n+            `keras_core.random.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.random.SeedGenerator`.\n+    \"\"\"\n+    return normal(shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed)\n+\n+\n+@keras_core_export(\"keras_core.random.uniform\")\n+def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n+    \"\"\"Draw samples from a uniform distribution.\n+\n+    The generated values follow a uniform distribution in the range\n+    `[minval, maxval)`. The lower bound `minval` is included in the range,\n+    while the upper bound `maxval` is excluded.\n+\n+    For floats, the default range is `[0, 1)`.  For ints, at least `maxval`\n+    must be specified explicitly.\n+\n+    Args:\n+        shape: The shape of the random values to generate.\n+        minval: Floats, defaults to 0. Lower bound of the range of\n+            random values to generate (inclusive).\n+        maxval: Floats, defaults to 1. Upper bound of the range of\n+            random values to generate (exclusive).\n+        dtype: Optional dtype of the tensor. Only floating point types are\n+            supported. If not specified, `keras_core.config.floatx()` is used,\n+            which defaults to `float32` unless you configured it otherwise (via\n+            `keras_core.config.set_floatx(float_dtype)`)\n+        seed: A Python integer or instance of\n+            `keras_core.random.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.random.SeedGenerator`.\n+    \"\"\"\n+    return backend.random.uniform(\n+        shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed\n+    )\n+\n+\n+@keras_core_export(\"keras_core.random.truncated_normal\")\n+def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n+    \"\"\"Draw samples from a truncated normal distribution.\n+\n+    The values are drawn from a normal distribution with specified mean and\n+    standard deviation, discarding and re-drawing any samples that are more\n+    than two standard deviations from the mean.\n+\n+    Args:\n+        shape: The shape of the random values to generate.\n+        mean: Floats, defaults to 0. Mean of the random values to generate.\n+        stddev: Floats, defaults to 1. Standard deviation of the random values\n+            to generate.\n+        dtype: Optional dtype of the tensor. Only floating point types are\n+            supported. If not specified, `keras.config.floatx()` is used,\n+            which defaults to `float32` unless you configured it otherwise (via\n+            `keras.config.set_floatx(float_dtype)`)\n+        seed: A Python integer or instance of\n+            `keras_core.random.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.random.SeedGenerator`.\n+    \"\"\"\n+    return backend.random.truncated_normal(\n+        shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed\n+    )\n+\n+\n+@keras_core_export(\"keras_core.random.dropout\")\n+def dropout(inputs, rate, noise_shape=None, seed=None):\n+    return backend.random.dropout(\n+        inputs, rate, noise_shape=noise_shape, seed=seed\n+    )\n\n@@ -1,6 +1,9 @@\n import random as python_random\n \n+from keras_core.api_export import keras_core_export\n \n+\n+@keras_core_export(\"keras_core.random.SeedGenerator\")\n class SeedGenerator:\n     def __init__(self, seed):\n         from keras_core.backend import Variable\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d07404036c2a6a15ea9b9c5f15b22b983308a31a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 88 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 88 | Churn Cumulative: 88 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,88 @@\n+import numpy as np\n+from tensorflow import keras\n+\n+import keras_core\n+\n+NUM_CLASSES = 10\n+\n+\n+def build_mnist_data(num_classes):\n+    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n+\n+    # Scale images to the [0, 1] range\n+    x_train = x_train.astype(\"float32\") / 255\n+    x_test = x_test.astype(\"float32\") / 255\n+    # Make sure images have shape (28, 28, 1)\n+    x_train = np.expand_dims(x_train, -1)\n+    x_test = np.expand_dims(x_test, -1)\n+\n+    # convert class vectors to binary class matrices\n+    y_train = keras.utils.to_categorical(y_train, num_classes)\n+    y_test = keras.utils.to_categorical(y_test, num_classes)\n+\n+    print(\"x_train shape:\", x_train.shape)\n+    print(x_train.shape[0], \"train samples\")\n+    print(x_test.shape[0], \"test samples\")\n+    \n+    return x_train, y_train, x_test, y_test\n+\n+\n+def build_keras_model(keras_module, num_classes):\n+\n+    input_shape = (28, 28, 1)\n+\n+    model = keras_module.Sequential(\n+        [\n+            keras_module.Input(shape=input_shape),\n+            keras_module.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n+            keras_module.layers.MaxPooling2D(pool_size=(2, 2)),\n+            keras_module.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n+            keras_module.layers.MaxPooling2D(pool_size=(2, 2)),\n+            keras_module.layers.Flatten(),\n+            keras_module.layers.Dropout(0.5),\n+            keras_module.layers.Dense(num_classes, activation=\"softmax\"),\n+        ]\n+    )\n+\n+    model.summary()\n+    return model\n+\n+\n+def train_model(model, x, y):\n+    batch_size = 128\n+    epochs = 1\n+\n+    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n+\n+    return model.fit(x, y, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n+\n+\n+def eval_model(model, x, y):\n+    score = model.evaluate(x, y, verbose=0)\n+    print(\"Test loss:\", score[0])\n+    print(\"Test accuracy:\", score[1])\n+    return score\n+\n+\n+def numerical_test():\n+    x_train, y_train, x_test, y_test = build_mnist_data(NUM_CLASSES)\n+    keras_model = build_keras_model(keras, NUM_CLASSES)\n+    keras_core_model = build_keras_model(keras_core, NUM_CLASSES)\n+\n+    # Make sure both model have same weights before training\n+    keras_core_model.set_weights(keras_model.weights)\n+\n+    for kw, kcw in zip(keras_model.weights, keras_core_model.weights):\n+        np.testing.assert_allclose(kw, kcw)\n+\n+    keras_history = train_model(keras_model, x_train, y_train)\n+    keras_core_history = train_model(keras_core_model, x_train, y_train)\n+\n+    for h, ch in zip(keras_history.history.items(), keras_core_history.history.items()):\n+        # They are not exactly equal at the moment.\n+        print(h)\n+        print(ch)\n+\n+\n+if __name__ == \"__main__\":\n+    numerical_test()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#782da4675228409fdb3842a4454bf25ce429a560", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 10 | Files Changed: 4 | Hunks: 12 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 907 | Contributors (this commit): 9 | Commits (past 90d): 41 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,7 +10,6 @@ from keras_core.utils import rng_utils\n \n \n def test_model_fit():\n-\n     cpus = tf.config.list_physical_devices(\"CPU\")\n     tf.config.set_logical_device_configuration(\n         cpus[0],\n@@ -22,7 +21,7 @@ def test_model_fit():\n \n     rng_utils.set_random_seed(1337)\n \n-    strategy = tf.distribute.MirroredStrategy(['CPU:0', 'CPU:1'])\n+    strategy = tf.distribute.MirroredStrategy([\"CPU:0\", \"CPU:1\"])\n     with strategy.scope():\n         inputs = layers.Input((100,), batch_size=32)\n         x = layers.Dense(256, activation=\"relu\")(inputs)\n@@ -55,5 +54,6 @@ def test_model_fit():\n     print(\"History:\")\n     print(history.history)\n \n+\n if __name__ == \"__main__\":\n     test_model_fit()\n\n@@ -28,15 +28,18 @@ def build_mnist_data(num_classes):\n \n \n def build_keras_model(keras_module, num_classes):\n-\n     input_shape = (28, 28, 1)\n \n     model = keras_module.Sequential(\n         [\n             keras_module.Input(shape=input_shape),\n-            keras_module.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n+            keras_module.layers.Conv2D(\n+                32, kernel_size=(3, 3), activation=\"relu\"\n+            ),\n             keras_module.layers.MaxPooling2D(pool_size=(2, 2)),\n-            keras_module.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n+            keras_module.layers.Conv2D(\n+                64, kernel_size=(3, 3), activation=\"relu\"\n+            ),\n             keras_module.layers.MaxPooling2D(pool_size=(2, 2)),\n             keras_module.layers.Flatten(),\n             keras_module.layers.Dropout(0.5),\n@@ -52,9 +55,13 @@ def train_model(model, x, y):\n     batch_size = 128\n     epochs = 1\n \n-    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n+    model.compile(\n+        loss=\"mse\", optimizer=\"adam\", metrics=[\"accuracy\"], jit_compile=False\n+    )\n \n-    return model.fit(x, y, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n+    return model.fit(\n+        x, y, batch_size=batch_size, epochs=epochs, validation_split=0.1\n+    )\n \n \n def eval_model(model, x, y):\n@@ -78,11 +85,15 @@ def numerical_test():\n     keras_history = train_model(keras_model, x_train, y_train)\n     keras_core_history = train_model(keras_core_model, x_train, y_train)\n \n-    for h, ch in zip(keras_history.history.items(), keras_core_history.history.items()):\n+    for h, ch in zip(\n+        keras_history.history.items(), keras_core_history.history.items()\n+    ):\n         # They are not exactly equal at the moment.\n         print(h)\n         print(ch)\n \n \n if __name__ == \"__main__\":\n+    keras.utils.set_random_seed(1337)\n+    keras_core.utils.set_random_seed(1337)\n     numerical_test()\n\n@@ -155,7 +155,7 @@ def apply_mask(sample_weight, mask, dtype, reduction):\n             #   = sum(loss * sample_weight) / valid\n             total = ops.cast(ops.shape(mask)[0], dtype=dtype)\n             valid = ops.sum(mask)  # May be 0!\n-            mask *= total / (valid + backend.epsilon())\n+            mask *= total / ops.maximum(valid, backend.epsilon())\n \n         if sample_weight is not None:\n             sample_weight = ops.cast(sample_weight, dtype=dtype)\n\n@@ -138,7 +138,9 @@ class Mean(Metric):\n \n     def result(self):\n         return self.total / (\n-            ops.cast(self.count, dtype=self.dtype) + backend.epsilon()\n+            ops.maximum(\n+                ops.cast(self.count, dtype=self.dtype), backend.epsilon()\n+            )\n         )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#39f8ef5e3c011cf3a80b2002e4e880573957d62b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 571 | Contributors (this commit): 6 | Commits (past 90d): 26 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -42,7 +42,6 @@ def build_keras_model(keras_module, num_classes):\n             ),\n             keras_module.layers.MaxPooling2D(pool_size=(2, 2)),\n             keras_module.layers.Flatten(),\n-            keras_module.layers.Dropout(0.5),\n             keras_module.layers.Dense(num_classes, activation=\"softmax\"),\n         ]\n     )\n@@ -52,7 +51,7 @@ def build_keras_model(keras_module, num_classes):\n \n \n def train_model(model, x, y):\n-    batch_size = 128\n+    batch_size = 256\n     epochs = 1\n \n     model.compile(\n@@ -60,7 +59,12 @@ def train_model(model, x, y):\n     )\n \n     return model.fit(\n-        x, y, batch_size=batch_size, epochs=epochs, validation_split=0.1\n+        x,\n+        y,\n+        batch_size=batch_size,\n+        epochs=epochs,\n+        validation_split=0.1,\n+        shuffle=False,\n     )\n \n \n@@ -85,12 +89,12 @@ def numerical_test():\n     keras_history = train_model(keras_model, x_train, y_train)\n     keras_core_history = train_model(keras_core_model, x_train, y_train)\n \n-    for h, ch in zip(\n-        keras_history.history.items(), keras_core_history.history.items()\n-    ):\n-        # They are not exactly equal at the moment.\n-        print(h)\n-        print(ch)\n+    for key in keras_history.history.keys():\n+        np.testing.assert_allclose(\n+            keras_history.history[key],\n+            keras_core_history.history[key],\n+            atol=1e-3,\n+        )\n \n \n if __name__ == \"__main__\":\n\n@@ -155,7 +155,7 @@ def apply_mask(sample_weight, mask, dtype, reduction):\n             #   = sum(loss * sample_weight) / valid\n             total = ops.cast(ops.shape(mask)[0], dtype=dtype)\n             valid = ops.sum(mask)  # May be 0!\n-            mask *= total / ops.maximum(valid, backend.epsilon())\n+            mask *= total / (valid + backend.epsilon())\n \n         if sample_weight is not None:\n             sample_weight = ops.cast(sample_weight, dtype=dtype)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e989cb7a0598aa528dafa6c3c1db0c73a92f59e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 138 | Lines Deleted: 13 | Files Changed: 4 | Hunks: 48 | Methods Changed: 15 | Complexity Δ (Sum/Max): 9/8 | Churn Δ: 151 | Churn Cumulative: 3910 | Contributors (this commit): 8 | Commits (past 90d): 41 | Contributors (cumulative): 16 | DMM Complexity: 0.43157894736842106\n\nDIFF:\n@@ -1,4 +1,4 @@\n-def _compute_conv_transpose_output_length(\n+def compute_conv_transpose_output_length(\n     input_length,\n     kernel_size,\n     padding,\n@@ -63,13 +63,13 @@ def compute_conv_transpose_output_shape(\n             None if output_padding is None else output_padding[i]\n         )\n         output_shape.append(\n-            _compute_conv_transpose_output_length(\n+            compute_conv_transpose_output_length(\n                 input_spatial_shape[i],\n                 kernel_spatial_shape[i],\n                 padding=padding,\n                 output_padding=current_output_padding,\n                 stride=strides[i],\n-                dilation=dilation_rate[0],\n+                dilation=dilation_rate[i],\n             )\n         )\n \n\n@@ -4,6 +4,9 @@ import numpy as np\n from jax import lax\n from jax import nn as jnn\n \n+from keras_core.backend.common.backend_utils import (\n+    compute_conv_transpose_output_length,\n+)\n from keras_core.backend.config import epsilon\n from keras_core.backend.jax.core import convert_to_tensor\n \n@@ -329,6 +332,81 @@ def separable_conv(\n     )\n \n \n+def _compute_padding_value_one_dim(\n+    input_length,\n+    output_length,\n+    kernel_size,\n+    stride,\n+    padding,\n+    dilation_rate,\n+):\n+    \"\"\"Computes adjusted padding for `conv_transpose` in one dim.\"\"\"\n+    kernel_size = (kernel_size - 1) * dilation_rate + 1\n+    if padding == \"valid\":\n+        padding_before = 0\n+    else:\n+        # padding == \"same\".\n+        padding_needed = max(\n+            0, (input_length - 1) * stride + kernel_size - output_length\n+        )\n+        padding_before = padding_needed // 2\n+\n+    expanded_input_length = (input_length - 1) * stride + 1\n+    padded_out_length = output_length + kernel_size - 1\n+    pad_before = kernel_size - 1 - padding_before\n+    pad_after = padded_out_length - expanded_input_length - pad_before\n+    return (pad_before, pad_after)\n+\n+\n+def _compute_padding_values(\n+    input_shape,\n+    kernel_shape,\n+    strides=1,\n+    padding=\"valid\",\n+    output_padding=None,\n+    data_format=\"channels_last\",\n+    dilation_rate=1,\n+):\n+    \"\"\"Computes adjusted padding for `conv_transpose`.\"\"\"\n+    num_spatial_dims = len(input_shape) - 2\n+    if isinstance(output_padding, int):\n+        output_padding = (output_padding,) * num_spatial_dims\n+    if isinstance(strides, int):\n+        strides = (strides,) * num_spatial_dims\n+    if isinstance(dilation_rate, int):\n+        dilation_rate = (dilation_rate,) * num_spatial_dims\n+\n+    kernel_spatial_shape = kernel_shape[:-2]\n+    if data_format == \"channels_last\":\n+        input_spatial_shape = input_shape[1:-1]\n+    else:\n+        input_spatial_shape = input_shape[2:]\n+    padding_values = []\n+    for i in range(num_spatial_dims):\n+        input_length = input_spatial_shape[i]\n+        current_output_padding = (\n+            None if output_padding is None else output_padding[i]\n+        )\n+        output_length = compute_conv_transpose_output_length(\n+            input_spatial_shape[i],\n+            kernel_spatial_shape[i],\n+            padding=padding,\n+            output_padding=current_output_padding,\n+            stride=strides[i],\n+            dilation=dilation_rate[i],\n+        )\n+        padding_value = _compute_padding_value_one_dim(\n+            input_length,\n+            output_length,\n+            kernel_spatial_shape[i],\n+            strides[i],\n+            padding=padding,\n+            dilation_rate=dilation_rate[i],\n+        )\n+        padding_values.append(padding_value)\n+    return padding_values\n+\n+\n def conv_transpose(\n     inputs,\n     kernel,\n@@ -339,6 +417,15 @@ def conv_transpose(\n     dilation_rate=1,\n ):\n     num_spatial_dims = inputs.ndim - 2\n+    padding_values = _compute_padding_values(\n+        inputs.shape,\n+        kernel.shape,\n+        strides,\n+        padding,\n+        output_padding,\n+        data_format,\n+        dilation_rate,\n+    )\n     dimension_numbers = _convert_to_lax_conv_dimension_numbers(\n         num_spatial_dims,\n         data_format,\n@@ -357,17 +444,11 @@ def conv_transpose(\n         include_batch_and_channels=False,\n     )\n \n-    if output_padding is not None:\n-        raise ValueError(\n-            \"Custom `output_padding` is not supported yet, please set \"\n-            \"`output_padding=None`.\"\n-        )\n-    padding = padding.upper()\n     return jax.lax.conv_transpose(\n         inputs,\n         kernel,\n         strides,\n-        padding,\n+        padding=padding_values,\n         rhs_dilation=dilation_rate,\n         dimension_numbers=dimension_numbers,\n         transpose_kernel=True,\n\n@@ -74,6 +74,7 @@ class BaseConvTranspose(Layer):\n         kernel_size,\n         strides=1,\n         padding=\"valid\",\n+        output_padding=None,\n         data_format=None,\n         dilation_rate=1,\n         activation=None,\n@@ -103,6 +104,14 @@ class BaseConvTranspose(Layer):\n             dilation_rate, rank, \"dilation_rate\"\n         )\n         self.padding = padding\n+        if output_padding is None:\n+            self.output_padding = None\n+        else:\n+            self.output_padding = standardize_tuple(\n+                output_padding,\n+                rank,\n+                \"output_padding\",\n+            )\n         self.data_format = standardize_data_format(data_format)\n         self.activation = activations.get(activation)\n         self.use_bias = use_bias\n@@ -184,6 +193,7 @@ class BaseConvTranspose(Layer):\n             self.kernel,\n             strides=list(self.strides),\n             padding=self.padding,\n+            output_padding=self.output_padding,\n             dilation_rate=self.dilation_rate,\n             data_format=self.data_format,\n         )\n@@ -207,6 +217,7 @@ class BaseConvTranspose(Layer):\n             self.filters,\n             strides=self.strides,\n             padding=self.padding,\n+            output_padding=self.output_padding,\n             data_format=self.data_format,\n             dilation_rate=self.dilation_rate,\n         )\n\n@@ -13,6 +13,7 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n             \"kernel_size\": 2,\n             \"strides\": 2,\n             \"padding\": \"valid\",\n+            \"output_padding\": None,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": 1,\n             \"input_shape\": (2, 8, 4),\n@@ -23,16 +24,18 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n             \"kernel_size\": 2,\n             \"strides\": 3,\n             \"padding\": \"same\",\n+            \"output_padding\": 2,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": (1,),\n             \"input_shape\": (2, 8, 4),\n-            \"output_shape\": (2, 24, 6),\n+            \"output_shape\": (2, 23, 6),\n         },\n         {\n             \"filters\": 6,\n             \"kernel_size\": (2,),\n             \"strides\": (2,),\n             \"padding\": \"valid\",\n+            \"output_padding\": None,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": 1,\n             \"input_shape\": (2, 8, 4),\n@@ -45,6 +48,7 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n         kernel_size,\n         strides,\n         padding,\n+        output_padding,\n         data_format,\n         dilation_rate,\n         input_shape,\n@@ -57,6 +61,7 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n                 \"kernel_size\": kernel_size,\n                 \"strides\": strides,\n                 \"padding\": padding,\n+                \"output_padding\": output_padding,\n                 \"data_format\": data_format,\n                 \"dilation_rate\": dilation_rate,\n             },\n@@ -74,6 +79,7 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n             \"kernel_size\": 2,\n             \"strides\": 2,\n             \"padding\": \"valid\",\n+            \"output_padding\": None,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": 1,\n             \"input_shape\": (2, 8, 8, 4),\n@@ -84,16 +90,18 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n             \"kernel_size\": 2,\n             \"strides\": 3,\n             \"padding\": \"same\",\n+            \"output_padding\": 2,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": (1, 1),\n             \"input_shape\": (2, 8, 8, 4),\n-            \"output_shape\": (2, 24, 24, 6),\n+            \"output_shape\": (2, 23, 23, 6),\n         },\n         {\n             \"filters\": 6,\n             \"kernel_size\": (2, 3),\n             \"strides\": (2, 1),\n             \"padding\": \"valid\",\n+            \"output_padding\": None,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": (1, 1),\n             \"input_shape\": (2, 8, 8, 4),\n@@ -106,6 +114,7 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n         kernel_size,\n         strides,\n         padding,\n+        output_padding,\n         data_format,\n         dilation_rate,\n         input_shape,\n@@ -118,6 +127,7 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n                 \"kernel_size\": kernel_size,\n                 \"strides\": strides,\n                 \"padding\": padding,\n+                \"output_padding\": output_padding,\n                 \"data_format\": data_format,\n                 \"dilation_rate\": dilation_rate,\n             },\n@@ -135,6 +145,7 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n             \"kernel_size\": 2,\n             \"strides\": 2,\n             \"padding\": \"valid\",\n+            \"output_padding\": None,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": 1,\n             \"input_shape\": (2, 8, 8, 8, 4),\n@@ -145,16 +156,18 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n             \"kernel_size\": 2,\n             \"strides\": 3,\n             \"padding\": \"same\",\n+            \"output_padding\": 2,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": (1, 1, 1),\n             \"input_shape\": (2, 8, 8, 8, 4),\n-            \"output_shape\": (2, 24, 24, 24, 6),\n+            \"output_shape\": (2, 23, 23, 23, 6),\n         },\n         {\n             \"filters\": 6,\n             \"kernel_size\": (2, 2, 3),\n             \"strides\": (2, 1, 2),\n             \"padding\": \"valid\",\n+            \"output_padding\": None,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": (1, 1, 1),\n             \"input_shape\": (2, 8, 8, 8, 4),\n@@ -167,6 +180,7 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n         kernel_size,\n         strides,\n         padding,\n+        output_padding,\n         data_format,\n         dilation_rate,\n         input_shape,\n@@ -179,6 +193,7 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n                 \"kernel_size\": kernel_size,\n                 \"strides\": strides,\n                 \"padding\": padding,\n+                \"output_padding\": output_padding,\n                 \"data_format\": data_format,\n                 \"dilation_rate\": dilation_rate,\n             },\n@@ -219,6 +234,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             \"kernel_size\": 2,\n             \"strides\": 2,\n             \"padding\": \"valid\",\n+            \"output_padding\": None,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": 1,\n         },\n@@ -227,6 +243,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             \"kernel_size\": 2,\n             \"strides\": 3,\n             \"padding\": \"same\",\n+            \"output_padding\": 2,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": (1,),\n         },\n@@ -235,6 +252,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             \"kernel_size\": (2,),\n             \"strides\": (2,),\n             \"padding\": \"valid\",\n+            \"output_padding\": None,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": 1,\n         },\n@@ -245,6 +263,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         kernel_size,\n         strides,\n         padding,\n+        output_padding,\n         data_format,\n         dilation_rate,\n     ):\n@@ -253,6 +272,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             kernel_size=kernel_size,\n             strides=strides,\n             padding=padding,\n+            output_padding=output_padding,\n             data_format=data_format,\n             dilation_rate=dilation_rate,\n         )\n@@ -261,6 +281,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             kernel_size=kernel_size,\n             strides=strides,\n             padding=padding,\n+            output_padding=output_padding,\n             data_format=data_format,\n             dilation_rate=dilation_rate,\n         )\n@@ -288,6 +309,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             \"kernel_size\": 2,\n             \"strides\": 2,\n             \"padding\": \"valid\",\n+            \"output_padding\": None,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": 1,\n         },\n@@ -296,6 +318,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             \"kernel_size\": 2,\n             \"strides\": 3,\n             \"padding\": \"same\",\n+            \"output_padding\": 2,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": (1, 1),\n         },\n@@ -304,6 +327,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             \"kernel_size\": (2, 3),\n             \"strides\": (2, 1),\n             \"padding\": \"valid\",\n+            \"output_padding\": None,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": (1, 1),\n         },\n@@ -314,6 +338,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         kernel_size,\n         strides,\n         padding,\n+        output_padding,\n         data_format,\n         dilation_rate,\n     ):\n@@ -322,6 +347,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             kernel_size=kernel_size,\n             strides=strides,\n             padding=padding,\n+            output_padding=output_padding,\n             data_format=data_format,\n             dilation_rate=dilation_rate,\n         )\n@@ -330,6 +356,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             kernel_size=kernel_size,\n             strides=strides,\n             padding=padding,\n+            output_padding=output_padding,\n             data_format=data_format,\n             dilation_rate=dilation_rate,\n         )\n@@ -357,6 +384,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             \"kernel_size\": 2,\n             \"strides\": 2,\n             \"padding\": \"valid\",\n+            \"output_padding\": None,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": 1,\n         },\n@@ -365,6 +393,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             \"kernel_size\": 2,\n             \"strides\": 3,\n             \"padding\": \"same\",\n+            \"output_padding\": 2,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": (1, 1, 1),\n         },\n@@ -373,6 +402,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             \"kernel_size\": (2, 2, 3),\n             \"strides\": (2, 1, 2),\n             \"padding\": \"valid\",\n+            \"output_padding\": None,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": (1, 1, 1),\n         },\n@@ -383,6 +413,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         kernel_size,\n         strides,\n         padding,\n+        output_padding,\n         data_format,\n         dilation_rate,\n     ):\n@@ -391,6 +422,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             kernel_size=kernel_size,\n             strides=strides,\n             padding=padding,\n+            output_padding=output_padding,\n             data_format=data_format,\n             dilation_rate=dilation_rate,\n         )\n@@ -399,6 +431,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             kernel_size=kernel_size,\n             strides=strides,\n             padding=padding,\n+            output_padding=output_padding,\n             data_format=data_format,\n             dilation_rate=dilation_rate,\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
