{"custom_id": "keras#2f1b7adab0bb8b09ec928b2e83ee49a12b743c23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 3898 | Contributors (this commit): 8 | Commits (past 90d): 20 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -645,6 +645,9 @@ class RandomFlip(BaseImageAugmentationLayer):\n     self.seed = seed\n     self.auto_vectorize = False\n \n+  def augment_label(self, label, transformation=None):\n+    return label\n+\n   def augment_image(self, image, transformation=None):\n     flipped_outputs = image\n     if self.horizontal:\n@@ -838,6 +841,9 @@ class RandomTranslation(BaseImageAugmentationLayer):\n     # issue for different tensorspec between inputs and outputs.\n     return tf.vectorized_map(self._augment, inputs)\n \n+  def augment_label(self, label, transformation=None):\n+    return label\n+\n   def compute_output_shape(self, input_shape):\n     return input_shape\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#411a79c4da1c771a7c95556bf3d243a4f26d4d95", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 38 | Churn Cumulative: 3936 | Contributors (this commit): 8 | Commits (past 90d): 21 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -25,6 +25,8 @@ from keras.utils import image_utils\n from keras.utils import tf_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.ops import stateless_random_ops\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n \n@@ -1365,7 +1367,7 @@ def get_zoom_matrix(zooms, image_height, image_width, name=None):\n @keras_export('keras.layers.RandomContrast',\n               'keras.layers.experimental.preprocessing.RandomContrast',\n               v1=[])\n-class RandomContrast(base_layer.BaseRandomLayer):\n+class RandomContrast(BaseImageAugmentationLayer):\n   \"\"\"A preprocessing layer which randomly adjusts contrast during training.\n \n   This layer will randomly adjust the contrast of an image or images by a random\n@@ -1418,25 +1420,23 @@ class RandomContrast(base_layer.BaseRandomLayer):\n                        ' got {}'.format(factor))\n     self.seed = seed\n \n-  def call(self, inputs, training=True):\n-    inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n-    def random_contrasted_inputs(inputs):\n-      seed = self._random_generator.make_seed_for_stateless_op()\n-      if seed is not None:\n-        output = tf.image.stateless_random_contrast(\n-            inputs, 1. - self.lower, 1. + self.upper, seed=seed)\n-      else:\n-        output = tf.image.random_contrast(\n-            inputs, 1. - self.lower, 1. + self.upper,\n-            seed=self._random_generator.make_legacy_seed())\n-      output = tf.clip_by_value(output, 0, 255)\n-      output.set_shape(inputs.shape)\n-      return output\n+  def get_random_transformation(self,\n+                                image=None,\n+                                label=None,\n+                                bounding_box=None):\n+    lower = 1. - self.lower\n+    upper = 1. + self.upper\n+    random_seed = self._random_generator.make_seed_for_stateless_op()\n+    contrast_factor = stateless_random_ops.stateless_random_uniform(\n+        shape=[], minval=lower, maxval=upper, seed=random_seed)\n+    return {'contrast_factor': contrast_factor}\n \n-    if training:\n-      return random_contrasted_inputs(inputs)\n-    else:\n-      return inputs\n+  def augment_image(self, image, transformation=None):\n+    contrast_factor = transformation['contrast_factor']\n+    output = tf.image.adjust_contrast(image, contrast_factor=contrast_factor)\n+    output = tf.clip_by_value(output, 0, 255)\n+    output.set_shape(image.shape)\n+    return output\n \n   def compute_output_shape(self, input_shape):\n     return input_shape\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a20cd6db7c52c293b1a586dd3eb4e53dcef71baf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 28 | Files Changed: 3 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 54 | Churn Cumulative: 433 | Contributors (this commit): 4 | Commits (past 90d): 24 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -89,8 +89,9 @@ class Adagrad(optimizer.Optimizer):\n     for var in var_list:\n       self._accumulators.append(\n           self.add_variable_from_reference(\n-              var, 'accumulator', initializer(shape=var.shape,\n-                                              dtype=var.dtype)))\n+              var,\n+              'accumulator',\n+              initial_value=initializer(shape=var.shape, dtype=var.dtype)))\n \n   def update_step(self, grad, variable):\n     \"\"\"Update step given gradient and the associated model variable.\"\"\"\n\n@@ -343,6 +343,7 @@ class _BaseOptimizer(tf.Module):\n   def add_variable_from_reference(self,\n                                   model_variable,\n                                   variable_name,\n+                                  shape=None,\n                                   initial_value=None):\n     \"\"\"Create an optimizer variable from model variable.\n \n@@ -351,20 +352,27 @@ class _BaseOptimizer(tf.Module):\n     corresponding momemtum variable is created of the same shape and dtype.\n \n     Args:\n-      model_variable: The corresponding model variable to the optimizer variable\n-        to be created.\n-      variable_name: The name prefix of the optimizer variable to be created.\n-        The create variables name will follow the pattern\n+      model_variable: tf.Variable. The corresponding model variable to the\n+        optimizer variable to be created.\n+      variable_name: String. The name prefix of the optimizer variable to be\n+        created. The create variables name will follow the pattern\n         `{variable_name}/{model_variable.name}`, e.g., `momemtum/dense_1`.\n-      initial_value: The initial value of the optimizer variable, if None, the\n-        value will be default to 0.\n+      shape: List or Tuple, defaults to None. The shape of the optimizer\n+        variable to be created. If None, the created variable will have the\n+        same shape as `model_variable`.\n+      initial_value: A Tensor, or Python object convertible to a Tensor,\n+        defaults to None. The initial value of the optimizer variable, if None,\n+        the initial value will be default to 0.\n \n     Returns:\n       An optimizer variable.\n     \"\"\"\n     if initial_value is None:\n+      if shape is None:\n         initial_value = tf.zeros(\n             shape=model_variable.shape, dtype=model_variable.dtype)\n+      else:\n+        initial_value = tf.zeros(shape, dtype=model_variable.dtype)\n     return tf.Variable(\n         initial_value=initial_value,\n         name=f\"{variable_name}/{model_variable._shared_name}\",  # pylint: disable=protected-access\n@@ -752,29 +760,12 @@ class Optimizer(_BaseOptimizer):\n   def add_variable_from_reference(self,\n                                   model_variable,\n                                   variable_name,\n+                                  shape=None,\n                                   initial_value=None):\n-    \"\"\"Create an optimizer variable.\n-\n-    Create an optimizer variable based on the information of model variable.\n-    The created optimizer variable will have the same shape and dtype as the\n-    model variable, and placed at the same device.\n-\n-    Args:\n-      model_variable: The corresponding model variable to the optimizer variable\n-        to be created.\n-      variable_name: The name prefix of the optimizer variable to be created.\n-      initial_value: The initial value of the optimizer variable, if None, the\n-        value will be default to 0.\n-\n-    Returns:\n-      An optimizer variable.\n-    \"\"\"\n     strategy = tf.distribute.get_strategy()\n     with strategy.extended.colocate_vars_with(model_variable):\n-      return super(Optimizer,\n-                   self).add_variable_from_reference(model_variable,\n-                                                     variable_name,\n-                                                     initial_value)\n+      return super().add_variable_from_reference(model_variable, variable_name,\n+                                                 shape, initial_value)\n \n   def _var_key(self, variable):\n     \"\"\"Get a unique identifier of the given variable.\"\"\"\n\n@@ -93,6 +93,12 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(variable._shared_name, \"test/tmp\")\n     self.assertEqual(self.evaluate(variable), 0)\n \n+  def testAddVarialeWithCustomShape(self):\n+    optimizer = adam_new.Adam()\n+    variable = optimizer.add_variable_from_reference(\n+        tf.Variable([1.0, 2.0], name=\"tmp\"), \"test\", shape=[])\n+    self.assertEqual(variable, tf.Variable(0.))\n+\n   def testBuildIndexDict(self):\n     optimizer = adam_new.Adam()\n     var_list = [tf.Variable(0, name=f\"var{i}\") for i in range(10)]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4c15bb3f08fdca36c3d9a55fbf6bdac5cd06d48c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2049 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -122,7 +122,7 @@ class DataAdapter(object, metaclass=abc.ABCMeta):\n     epoch. This behavior might change in the future.\n \n     Returns:\n-      An tf.dataset.Dataset. Caller might use the dataset in different\n+      A tf.dataset.Dataset. Caller might use the dataset in different\n       context, e.g. iter(dataset) in eager to get the value directly, or in graph\n       mode, provide the iterator tensor to Keras model function.\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1fa5209dd2af716e8dcc7bfccf37ef7313a00f9b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2051 | Contributors (this commit): 15 | Commits (past 90d): 6 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -122,7 +122,7 @@ class DataAdapter(object, metaclass=abc.ABCMeta):\n     epoch. This behavior might change in the future.\n \n     Returns:\n-      A tf.dataset.Dataset. Caller might use the dataset in different\n+      A `tf.data.Dataset`. Caller might use the dataset in different\n       context, e.g. iter(dataset) in eager to get the value directly, or in graph\n       mode, provide the iterator tensor to Keras model function.\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c141b3c7e98bc572427dbf3c50747e00348e4cb3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 32 | Files Changed: 1 | Hunks: 19 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 51 | Churn Cumulative: 1496 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -17,7 +17,6 @@\n import tensorflow.compat.v2 as tf\n \n import math\n-from typing import List, Union, Text, Optional, Any, Tuple, Dict, ItemsView, overload, DefaultDict\n \n from absl.testing import parameterized\n import numpy as np\n@@ -32,37 +31,31 @@ from tensorflow.python.eager import context\n \n class TestBatchSequence(data_utils.Sequence):\n \n-  def __init__(\n-    self,\n-    batch_size: Union[int, List[int]],\n-    feature_shape,\n-    epochs: int = 2,\n-  ):\n-    print(\"TestBatchSequence.__init__\")\n+  def __init__(self, batch_size, feature_shape, epochs=2):\n+    \"\"\"Create a sequence with\n+\n+    Args:\n+        batch_size (Union[int, List[int]]): Can be a list containing two values:\n+                                            start and end batch_size\n+        feature_shape (int): Number of features in a sample\n+        epochs (int, optional): Number of epochs\n+    \"\"\"\n     self.batch_size = batch_size\n     self.feature_shape = feature_shape\n \n-    # if isinstance(batch_size, list):\n-    #     logger.debug(\n-    #         \"The provided batch size is a list, this data generator will use a \"\n-    #         \"linear increasing batch size.\"\n-    #     )\n-\n     self._epochs = epochs\n     # we use `on_epoch_end` method to prepare data for the next epoch\n     # set current epoch to `-1`, so that `on_epoch_end` will increase it to `0`\n     self._current_epoch = -1\n     # actual batch size will be set inside `on_epoch_end`\n     self._current_batch_size = 0\n-    # create separate data variable that will store modified data for each batch\n-    # self._data: Data = {}\n \n     self.on_epoch_end()\n \n-  def __len__(self) -> int:\n+  def __len__(self):\n       \"\"\"Number of batches in the Sequence.\n \n-      Returns:\n+      Returns: int\n           The number of batches in the Sequence.\n       \"\"\"\n       # data was rebalanced, so need to recalculate number of examples\n@@ -70,35 +63,32 @@ class TestBatchSequence(data_utils.Sequence):\n       batch_size = self._current_batch_size\n       return num_examples // batch_size + int(num_examples % batch_size > 0)  # = math.ceil(num_examples / batch_size )\n \n-  def __getitem__(self, index: int) -> Tuple[Any, Any]:\n+  def __getitem__(self, index):\n       \"\"\"Gets batch at position `index`.\n \n       Arguments:\n-          index: position of the batch in the Sequence.\n+          index (int): position of the batch in the Sequence.\n \n-      Returns:\n+      Returns: Tuple[Any, Any]\n           A batch (tuple of input data and target data).\n       \"\"\"\n-      print(f\"__getitem__ : self._current_epoch: {self._current_epoch}, self._current_batch_size: {self._current_batch_size}\")\n       # return input and target data, as our target data is inside the input\n       # data return None for the target data\n       return (np.zeros((self._current_batch_size, self.feature_shape)),\n               np.ones((self._current_batch_size,)))\n \n-  def on_epoch_end(self) -> None:\n+  def on_epoch_end(self):\n     \"\"\"Update the data after every epoch.\"\"\"\n     self._current_epoch += 1\n     if self._current_epoch < self._epochs:\n       self._current_batch_size = self._linearly_increasing_batch_size()\n-    print(f\"on_epoch_end: self._current_epoch: {self._current_epoch}, self._current_batch_size: {self._current_batch_size}\")\n-    # self._data = self._shuffle_and_balance(self._current_batch_size)\n \n-  def _linearly_increasing_batch_size(self) -> int:\n+  def _linearly_increasing_batch_size(self):\n       \"\"\"Linearly increase batch size with every epoch.\n \n       The idea comes from https://arxiv.org/abs/1711.00489.\n \n-      Returns:\n+      Returns: int\n           The batch size to use in this epoch.\n       \"\"\"\n       if not isinstance(self.batch_size, list):\n@@ -615,7 +605,6 @@ class GenericArrayLikeDataAdapterTest(DataAdapterTestBase):\n     self.assertEqual(my_data_handler.inferred_steps, 4)  # 20 samples / 5 bs = 4\n \n     self.model.fit(self.sequence_input_increasing_batch_size, epochs=epochs)\n-    # TODO call things inside fit step by step and see how data_handler behaves while doing this\n \n     # Check state after fit()\n     self.assertEqual(self.sequence_input_increasing_batch_size._current_epoch, 2)\n@@ -652,18 +641,16 @@ class GenericArrayLikeDataAdapterTest(DataAdapterTestBase):\n       self.model.reset_metrics()\n       with data_handler.catch_stop_iteration():\n           for step in data_handler.steps():\n+              cur_bs = self.sequence_input_increasing_batch_size._current_batch_size\n               with tf.profiler.experimental.Trace(\n                   \"train\",\n                   epoch_num=epoch,\n                   step_num=step,\n-                  batch_size=self.sequence_input_increasing_batch_size._current_batch_size,\n+                  batch_size=cur_bs,\n                   _r=1,\n               ):\n-                  tmp_logs = self.model.train_function(iterator)\n                   if data_handler.should_sync:\n                       context.async_wait()\n-                  logs = tmp_logs  # No error, now safe to assign to logs.\n-                  end_step = step + data_handler.step_increment\n                   if self.model.stop_training:\n                       break\n     self.assertEqual(data_handler.inferred_steps, 2)  # 20 samples / 10 bs = 2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fabcd8ac0a7c6d4bc1e8bf01c60deb02814c4c64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 75 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 157 | Churn Cumulative: 1653 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -156,13 +156,6 @@ class DataAdapterTestBase(test_combinations.TestCase):\n     self.sequence_input = TestSequence(batch_size=self.batch_size,\n                                        feature_shape=10)\n \n-    self.increasing_batch_size = [5, 10]\n-    self.sequence_input_increasing_batch_size = TestBatchSequence(\n-      batch_size=self.increasing_batch_size,\n-      feature_shape=10,\n-      epochs=2,\n-      )\n-\n     self.text_input = [['abc']]\n     self.bytes_input = [[b'abc']]\n     self.model = keras.models.Sequential(\n@@ -487,6 +480,88 @@ class TensorLikeDataAdapterTest(DataAdapterTestBase):\n     self.assertEqual(adapter.partial_batch_size(), partial_batch_size or None)\n \n \n+class IncreasingBatchSizeAdapterTest(test_combinations.TestCase):\n+  def setUp(self):\n+    super(IncreasingBatchSizeAdapterTest, self).setUp()\n+    self.adapter_cls = data_adapter.KerasSequenceAdapter\n+\n+    self.increasing_batch_size = [5, 10]\n+    self.sequence_input = TestBatchSequence(\n+      batch_size=self.increasing_batch_size,\n+      feature_shape=10,\n+      epochs=2,\n+      )\n+    self.model = keras.models.Sequential(\n+        [keras.layers.Dense(8, input_shape=(10,), activation='softmax')])\n+\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+  def test_training_increasing_batch_size_fit_call(self):\n+    epochs = self.sequence_input._epochs  # 2\n+\n+    self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n+                       run_eagerly=test_utils.should_run_eagerly())\n+\n+    # Check state before fit()\n+    self.assertEqual(self.sequence_input._current_epoch, 0)\n+    self.assertEqual(self.sequence_input._current_batch_size, 5)\n+    my_data_handler = data_adapter.get_data_handler(\n+      self.sequence_input,\n+      epochs=epochs,\n+      model=self.model,\n+    )\n+    self.assertEqual(my_data_handler.inferred_steps, 4)  # 20 samples / 5 bs = 4\n+\n+    self.model.fit(self.sequence_input, epochs=epochs)\n+\n+    # Check state after fit()\n+    self.assertEqual(self.sequence_input._current_epoch, 2)\n+    self.assertEqual(self.sequence_input._current_batch_size, 10)\n+    my_new_data_handler = data_adapter.get_data_handler(\n+      self.sequence_input,\n+      epochs=epochs,\n+      model=self.model,\n+    )\n+    self.assertEqual(my_data_handler.inferred_steps, 4)  # 20 samples / 5 bs = 4\n+    self.assertEqual(my_new_data_handler.inferred_steps, 2)  # 20 samples / 10 bs = 2\n+\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+  def test_training_increasing_batch_size_fit_internals(self):\n+    epochs = self.sequence_input._epochs  # 2\n+\n+    self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n+                       run_eagerly=test_utils.should_run_eagerly())\n+    self.model.stop_training = False\n+    self.model.train_function = self.model.make_train_function()\n+\n+    # Check state before fit()\n+    self.assertEqual(self.sequence_input._current_epoch, 0)\n+    self.assertEqual(self.sequence_input._current_batch_size, 5)\n+    data_handler = data_adapter.get_data_handler(\n+      self.sequence_input,\n+      epochs=epochs,\n+      model=self.model,\n+    )\n+    self.assertEqual(data_handler.inferred_steps, 4)  # 20 samples / 5 bs = 4\n+\n+    # Execute fit()-loop and see how data_handler behaves while doing this\n+    for epoch, iterator in data_handler.enumerate_epochs():\n+      self.model.reset_metrics()\n+      with data_handler.catch_stop_iteration():\n+        for step in data_handler.steps():\n+          with tf.profiler.experimental.Trace(\n+            \"train\",\n+            epoch_num=epoch,\n+            step_num=step,\n+            batch_size=self.sequence_input._current_batch_size,\n+            _r=1,\n+          ):\n+            if data_handler.should_sync:\n+              context.async_wait()\n+            if self.model.stop_training:\n+              break\n+    self.assertEqual(data_handler.inferred_steps, 2)  # 20 samples / 10 bs = 2\n+\n+\n class GenericArrayLikeDataAdapterTest(DataAdapterTestBase):\n \n   def setUp(self):\n@@ -587,74 +662,6 @@ class GenericArrayLikeDataAdapterTest(DataAdapterTestBase):\n     self.model.evaluate(self.arraylike_input,\n                         self.numpy_target, batch_size=5)\n \n-  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n-  def test_training_increasing_batch_size_fit_call(self):\n-    epochs = self.sequence_input_increasing_batch_size._epochs  # 2\n-\n-    self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n-                       run_eagerly=test_utils.should_run_eagerly())\n-\n-    # Check state before fit()\n-    self.assertEqual(self.sequence_input_increasing_batch_size._current_epoch, 0)\n-    self.assertEqual(self.sequence_input_increasing_batch_size._current_batch_size, 5)\n-    my_data_handler = data_adapter.get_data_handler(\n-      self.sequence_input_increasing_batch_size,\n-      epochs=epochs,\n-      model=self.model,\n-    )\n-    self.assertEqual(my_data_handler.inferred_steps, 4)  # 20 samples / 5 bs = 4\n-\n-    self.model.fit(self.sequence_input_increasing_batch_size, epochs=epochs)\n-\n-    # Check state after fit()\n-    self.assertEqual(self.sequence_input_increasing_batch_size._current_epoch, 2)\n-    self.assertEqual(self.sequence_input_increasing_batch_size._current_batch_size, 10)\n-    my_new_data_handler = data_adapter.get_data_handler(\n-      self.sequence_input_increasing_batch_size,\n-      epochs=epochs,\n-      model=self.model,\n-    )\n-    self.assertEqual(my_data_handler.inferred_steps, 4)  # 20 samples / 5 bs = 4\n-    self.assertEqual(my_new_data_handler.inferred_steps, 2)  # 20 samples / 10 bs = 2\n-\n-  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n-  def test_training_increasing_batch_size_fit_internals(self):\n-    epochs = self.sequence_input_increasing_batch_size._epochs  # 2\n-\n-    self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n-                       run_eagerly=test_utils.should_run_eagerly())\n-    self.model.stop_training = False\n-    self.model.train_function = self.model.make_train_function()\n-\n-    # Check state before fit()\n-    self.assertEqual(self.sequence_input_increasing_batch_size._current_epoch, 0)\n-    self.assertEqual(self.sequence_input_increasing_batch_size._current_batch_size, 5)\n-    data_handler = data_adapter.get_data_handler(\n-      self.sequence_input_increasing_batch_size,\n-      epochs=epochs,\n-      model=self.model,\n-    )\n-    self.assertEqual(data_handler.inferred_steps, 4)  # 20 samples / 5 bs = 4\n-\n-    # Execute fit()-loop and see how data_handler behaves while doing this\n-    for epoch, iterator in data_handler.enumerate_epochs():\n-      self.model.reset_metrics()\n-      with data_handler.catch_stop_iteration():\n-        for step in data_handler.steps():\n-          cur_bs = self.sequence_input_increasing_batch_size._current_batch_size\n-          with tf.profiler.experimental.Trace(\n-            \"train\",\n-            epoch_num=epoch,\n-            step_num=step,\n-            batch_size=cur_bs,\n-            _r=1,\n-          ):\n-            if data_handler.should_sync:\n-              context.async_wait()\n-            if self.model.stop_training:\n-              break\n-    self.assertEqual(data_handler.inferred_steps, 2)  # 20 samples / 10 bs = 2\n-\n   @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_training_tensor_target(self):\n     self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2135b7d2a073fff320e2352b3954f8185f09355e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 88 | Lines Deleted: 99 | Files Changed: 1 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 187 | Churn Cumulative: 1840 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -29,82 +29,6 @@ from keras.utils import data_utils\n from tensorflow.python.eager import context\n \n \n-class TestBatchSequence(data_utils.Sequence):\n-\n-  def __init__(self, batch_size, feature_shape, epochs=2):\n-    \"\"\"Create a sequence with\n-\n-    Args:\n-        batch_size (Union[int, List[int]]): Can be a list containing two values:\n-                                            start and end batch_size\n-        feature_shape (int): Number of features in a sample\n-        epochs (int, optional): Number of epochs\n-    \"\"\"\n-    self.batch_size = batch_size\n-    self.feature_shape = feature_shape\n-\n-    self._epochs = epochs\n-    # we use `on_epoch_end` method to prepare data for the next epoch\n-    # set current epoch to `-1`, so that `on_epoch_end` will increase it to `0`\n-    self._current_epoch = -1\n-    # actual batch size will be set inside `on_epoch_end`\n-    self._current_batch_size = 0\n-\n-    self.on_epoch_end()\n-\n-  def __len__(self):\n-    \"\"\"Number of batches in the Sequence.\n-\n-    Returns: int\n-        The number of batches in the Sequence.\n-    \"\"\"\n-    # data was rebalanced, so need to recalculate number of examples\n-    num_examples = 20\n-    batch_size = self._current_batch_size\n-    return num_examples // batch_size + int(num_examples % batch_size > 0)  # = math.ceil(num_examples / batch_size )\n-\n-  def __getitem__(self, index):\n-    \"\"\"Gets batch at position `index`.\n-\n-    Arguments:\n-        index (int): position of the batch in the Sequence.\n-\n-    Returns: Tuple[Any, Any]\n-        A batch (tuple of input data and target data).\n-    \"\"\"\n-    # return input and target data, as our target data is inside the input\n-    # data return None for the target data\n-    return (np.zeros((self._current_batch_size, self.feature_shape)),\n-            np.ones((self._current_batch_size,)))\n-\n-  def on_epoch_end(self):\n-    \"\"\"Update the data after every epoch.\"\"\"\n-    self._current_epoch += 1\n-    if self._current_epoch < self._epochs:\n-      self._current_batch_size = self._linearly_increasing_batch_size()\n-\n-  def _linearly_increasing_batch_size(self):\n-    \"\"\"Linearly increase batch size with every epoch.\n-\n-    The idea comes from https://arxiv.org/abs/1711.00489.\n-\n-    Returns: int\n-        The batch size to use in this epoch.\n-    \"\"\"\n-    if not isinstance(self.batch_size, list):\n-      return int(self.batch_size)\n-\n-    if self._epochs > 1:\n-      return int(\n-        self.batch_size[0]\n-        + self._current_epoch\n-        * (self.batch_size[1] - self.batch_size[0])\n-        / (self._epochs - 1)\n-      )\n-    else:\n-      return int(self.batch_size[0])\n-\n-\n class DummyArrayLike:\n   \"\"\"Dummy array-like object.\"\"\"\n \n@@ -195,6 +119,82 @@ class TestRaggedSequence(TestSequence):\n     return (rt, np.ones((self.batch_size,)))\n \n \n+class TestBatchSequence(data_utils.Sequence):\n+\n+  def __init__(self, batch_size, feature_shape, epochs=2):\n+    \"\"\"Creates a keras.utils.Sequence with increasing batch_size.\n+\n+    Args:\n+        batch_size (Union[int, List[int]]): Can be a list containing two values:\n+                                            start and end batch_size\n+        feature_shape (int): Number of features in a sample\n+        epochs (int, optional): Number of epochs\n+    \"\"\"\n+    self.batch_size = batch_size\n+    self.feature_shape = feature_shape\n+\n+    self._epochs = epochs\n+    # we use `on_epoch_end` method to prepare data for the next epoch\n+    # set current epoch to `-1`, so that `on_epoch_end` will increase it to `0`\n+    self._current_epoch = -1\n+    # actual batch size will be set inside `on_epoch_end`\n+    self._current_batch_size = 0\n+\n+    self.on_epoch_end()\n+\n+  def __len__(self):\n+    \"\"\"Number of batches in the Sequence.\n+\n+    Returns: int\n+        The number of batches in the Sequence.\n+    \"\"\"\n+    # data was rebalanced, so need to recalculate number of examples\n+    num_examples = 20\n+    batch_size = self._current_batch_size\n+    return num_examples // batch_size + int(num_examples % batch_size > 0)  # = math.ceil(num_examples / batch_size )\n+\n+  def __getitem__(self, index):\n+    \"\"\"Gets batch at position `index`.\n+\n+    Arguments:\n+        index (int): position of the batch in the Sequence.\n+\n+    Returns: Tuple[Any, Any]\n+        A batch (tuple of input data and target data).\n+    \"\"\"\n+    # return input and target data, as our target data is inside the input\n+    # data return None for the target data\n+    return (np.zeros((self._current_batch_size, self.feature_shape)),\n+            np.ones((self._current_batch_size,)))\n+\n+  def on_epoch_end(self):\n+    \"\"\"Updates the data after every epoch.\"\"\"\n+    self._current_epoch += 1\n+    if self._current_epoch < self._epochs:\n+      self._current_batch_size = self._linearly_increasing_batch_size()\n+\n+  def _linearly_increasing_batch_size(self):\n+    \"\"\"Linearly increase batch size with every epoch.\n+\n+    The idea comes from https://arxiv.org/abs/1711.00489.\n+\n+    Returns: int\n+        The batch size to use in this epoch.\n+    \"\"\"\n+    if not isinstance(self.batch_size, list):\n+      return int(self.batch_size)\n+\n+    if self._epochs > 1:\n+      return int(\n+        self.batch_size[0]\n+        + self._current_epoch\n+        * (self.batch_size[1] - self.batch_size[0])\n+        / (self._epochs - 1)\n+      )\n+    else:\n+      return int(self.batch_size[0])\n+\n+\n class TensorLikeDataAdapterTest(DataAdapterTestBase):\n \n   def setUp(self):\n@@ -485,49 +485,36 @@ class IncreasingBatchSizeAdapterTest(test_combinations.TestCase):\n     super(IncreasingBatchSizeAdapterTest, self).setUp()\n     self.adapter_cls = data_adapter.KerasSequenceAdapter\n \n+    self.epochs = 2\n     self.increasing_batch_size = [5, 10]\n     self.sequence_input = TestBatchSequence(\n       batch_size=self.increasing_batch_size,\n       feature_shape=10,\n-      epochs=2,\n+      epochs=self.epochs,\n       )\n     self.model = keras.models.Sequential(\n         [keras.layers.Dense(8, input_shape=(10,), activation='softmax')])\n \n   @test_combinations.run_all_keras_modes(always_skip_v1=True)\n-  def test_training_increasing_batch_size_fit_call(self):\n-    epochs = self.sequence_input._epochs  # 2\n-\n+  def test_training_with_test_batch_sequence(self):\n+    \"\"\"Ensures TestBatchSequence works as expected.\"\"\"\n     self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n                        run_eagerly=test_utils.should_run_eagerly())\n \n     # Check state before fit()\n     self.assertEqual(self.sequence_input._current_epoch, 0)\n     self.assertEqual(self.sequence_input._current_batch_size, 5)\n-    my_data_handler = data_adapter.get_data_handler(\n-      self.sequence_input,\n-      epochs=epochs,\n-      model=self.model,\n-    )\n-    self.assertEqual(my_data_handler.inferred_steps, 4)  # 20 samples / 5 bs = 4\n \n-    self.model.fit(self.sequence_input, epochs=epochs)\n+    # Execute fit()\n+    self.model.fit(self.sequence_input, epochs=self.epochs)\n \n     # Check state after fit()\n     self.assertEqual(self.sequence_input._current_epoch, 2)\n     self.assertEqual(self.sequence_input._current_batch_size, 10)\n-    my_new_data_handler = data_adapter.get_data_handler(\n-      self.sequence_input,\n-      epochs=epochs,\n-      model=self.model,\n-    )\n-    self.assertEqual(my_data_handler.inferred_steps, 4)  # 20 samples / 5 bs = 4\n-    self.assertEqual(my_new_data_handler.inferred_steps, 2)  # 20 samples / 10 bs = 2\n \n   @test_combinations.run_all_keras_modes(always_skip_v1=True)\n-  def test_training_increasing_batch_size_fit_internals(self):\n-    epochs = self.sequence_input._epochs  # 2\n-\n+  def test_training_with_increasing_batch_size(self):\n+    \"\"\"Ensures data_adapters DataHandler & DataAdapter work as expected.\"\"\"\n     self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n                        run_eagerly=test_utils.should_run_eagerly())\n     self.model.stop_training = False\n@@ -538,12 +525,12 @@ class IncreasingBatchSizeAdapterTest(test_combinations.TestCase):\n     self.assertEqual(self.sequence_input._current_batch_size, 5)\n     data_handler = data_adapter.get_data_handler(\n       self.sequence_input,\n-      epochs=epochs,\n+      epochs=self.epochs,\n       model=self.model,\n     )\n     self.assertEqual(data_handler.inferred_steps, 4)  # 20 samples / 5 bs = 4\n \n-    # Execute fit()-loop and see how data_handler behaves while doing this\n+    # Execute fit()-loop\n     for epoch, iterator in data_handler.enumerate_epochs():\n       self.model.reset_metrics()\n       with data_handler.catch_stop_iteration():\n@@ -559,6 +546,8 @@ class IncreasingBatchSizeAdapterTest(test_combinations.TestCase):\n               context.async_wait()\n             if self.model.stop_training:\n               break\n+\n+    # Check state after fit()\n     self.assertEqual(data_handler.inferred_steps, 2)  # 20 samples / 10 bs = 2\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3abad9a1eed1a334042dc66f338dfeda71d76d8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1841 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -79,7 +79,6 @@ class DataAdapterTestBase(test_combinations.TestCase):\n     self.iterator_input = data_utils.threadsafe_generator(generator)()\n     self.sequence_input = TestSequence(batch_size=self.batch_size,\n                                        feature_shape=10)\n-\n     self.text_input = [['abc']]\n     self.bytes_input = [[b'abc']]\n     self.model = keras.models.Sequential(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#55476a86ed482d2e1f473dc629848d2068225c73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 44 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -19,11 +19,8 @@ _DTENSOR_API_ENABLED = False\n \n # Conditional import the dtensor API, since it is currently broken in OSS.\n if _DTENSOR_API_ENABLED:\n-  # pylint: disable=g-direct-tensorflow-import, g-import-not-at-top\n-  from tensorflow.dtensor import python as dtensor_api\n+  from tensorflow.compat.v2.experimental import dtensor as dtensor_api  # pylint: disable=g-import-not-at-top\n else:\n   # Leave it with a placeholder, so that the import line from other python file\n   # will not break.\n   dtensor_api = None\n-\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8abffb29aba7aa214dc2a34c26cc6be302907d39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 398 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -29,6 +29,6 @@ from keras.engine.training import Model\n \n from tensorflow.python.util.tf_export import keras_export\n \n-__version__ = '2.9.0'\n+__version__ = '2.10.0'\n \n keras_export('keras.__version__').export_constant(__name__, '__version__')\n\n@@ -30,7 +30,7 @@ DOCLINES = __doc__.split('\\n')\n # This version string is semver compatible, but incompatible with pip.\n # For pip, we will remove all '-' characters from this string, and use the\n # result for pip.\n-_VERSION = '2.9.0'\n+_VERSION = '2.10.0'\n \n REQUIRED_PACKAGES = [\n     # We depend on TensorFlow's declared pip dependencies.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8e7cc8176b6bb8218a85a86f1861b97b4feb5a07", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 25 | Files Changed: 1 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 45 | Churn Cumulative: 3981 | Contributors (this commit): 8 | Commits (past 90d): 22 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1158,7 +1158,7 @@ class RandomRotation(base_layer.BaseRandomLayer):\n @keras_export('keras.layers.RandomZoom',\n               'keras.layers.experimental.preprocessing.RandomZoom',\n               v1=[])\n-class RandomZoom(base_layer.BaseRandomLayer):\n+class RandomZoom(BaseImageAugmentationLayer):\n   \"\"\"A preprocessing layer which randomly zooms images during training.\n \n   This layer will randomly zoom in or out on each axis of an image\n@@ -1261,51 +1261,46 @@ class RandomZoom(base_layer.BaseRandomLayer):\n     self.interpolation = interpolation\n     self.seed = seed\n \n-  def call(self, inputs, training=True):\n-    inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n-\n-    def random_zoomed_inputs(inputs):\n-      \"\"\"Zoomed inputs with random ops.\"\"\"\n-      original_shape = inputs.shape\n-      unbatched = inputs.shape.rank == 3\n-      # The transform op only accepts rank 4 inputs, so if we have an unbatched\n-      # image, we need to temporarily expand dims to a batch.\n-      if unbatched:\n-        inputs = tf.expand_dims(inputs, 0)\n-      inputs_shape = tf.shape(inputs)\n-      batch_size = inputs_shape[0]\n-      img_hd = tf.cast(inputs_shape[H_AXIS], tf.float32)\n-      img_wd = tf.cast(inputs_shape[W_AXIS], tf.float32)\n+  def get_random_transformation(self,\n+                                image=None,\n+                                label=None,\n+                                bounding_box=None):\n     height_zoom = self._random_generator.random_uniform(\n-          shape=[batch_size, 1],\n+        shape=[1, 1],\n         minval=1. + self.height_lower,\n         maxval=1. + self.height_upper)\n     if self.width_factor is not None:\n       width_zoom = self._random_generator.random_uniform(\n-            shape=[batch_size, 1],\n+          shape=[1, 1],\n           minval=1. + self.width_lower,\n           maxval=1. + self.width_upper)\n     else:\n       width_zoom = height_zoom\n+\n+    return {'height_zoom': height_zoom, 'width_zoom': width_zoom}\n+\n+  def augment_image(self, image, transformation=None):\n+    image = utils.ensure_tensor(image, self.compute_dtype)\n+    original_shape = image.shape\n+    image = tf.expand_dims(image, 0)\n+    image_shape = tf.shape(image)\n+    img_hd = tf.cast(image_shape[H_AXIS], tf.float32)\n+    img_wd = tf.cast(image_shape[W_AXIS], tf.float32)\n+    width_zoom = transformation['width_zoom']\n+    height_zoom = transformation['height_zoom']\n     zooms = tf.cast(\n         tf.concat([width_zoom, height_zoom], axis=1),\n         dtype=tf.float32)\n     output = transform(\n-          inputs,\n+        image,\n         get_zoom_matrix(zooms, img_hd, img_wd),\n         fill_mode=self.fill_mode,\n         fill_value=self.fill_value,\n         interpolation=self.interpolation)\n-      if unbatched:\n     output = tf.squeeze(output, 0)\n     output.set_shape(original_shape)\n     return output\n \n-    if training:\n-      return random_zoomed_inputs(inputs)\n-    else:\n-      return inputs\n-\n   def compute_output_shape(self, input_shape):\n     return input_shape\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2d7dc6080f0824200e317f255e3290da60e0f98a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 45 | Churn Cumulative: 1884 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -321,7 +321,7 @@ def validate_per_replica_inputs(distribution_strategy, x):\n \n   \"\"\"\n   # Convert the inputs and targets into a list of PerReplica objects.\n-  per_replica_list = tf.nest.flatten(x, expand_composites=True)\n+  per_replica_list = tf.nest.flatten(x)\n   x_values_list = []\n   for x in per_replica_list:\n     # At this point x should contain only tensors.\n\n@@ -173,50 +173,65 @@ class TestDistributionStrategyErrorCases(tf.test.TestCase, parameterized.TestCas\n   @tf.__internal__.distribute.combinations.generate(\n       tf.__internal__.test.combinations.combine(\n           distribution=[\n-              tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n+              tf.__internal__.distribute.combinations.\n+              mirrored_strategy_with_gpu_and_cpu,\n           ],\n           mode=['graph']))\n   def test_validating_dataset_input_tensors_with_shape_mismatch(\n       self, distribution):\n     with self.cached_session():\n-      a = tf.constant([1, 2], shape=(1, 2))\n-      b = tf.constant([[1, 2], [1, 2]], shape=(2, 2))\n-      x = tf.distribute.DistributedValues((a, b))\n-      y = tf.distribute.DistributedValues((a, a))\n+      @tf.function\n+      def run():\n+        ctx = tf.distribute.get_replica_context()\n+        if ctx.replica_id_in_sync_group.device.endswith('GPU:0'):\n+          return tf.constant([[1, 2]])\n+        else:\n+          return tf.constant([[1, 2], [1, 2]])\n+\n+      x = distribution.run(run)\n+\n       # Removed device and input tensor shape details from the error message\n       # since the order of the device and the corresponding input tensor shape\n       # is not deterministic over different runs.\n       with self.assertRaisesRegex(\n           ValueError, 'Input tensor shapes do not match for '\n           'distributed tensor inputs '\n-          'DistributedValues:.+'):\n+          'PerReplica:.+'):\n         with distribution.scope():\n           distributed_training_utils_v1.validate_distributed_dataset_inputs(\n-              distribution, x, y)\n+              distribution, x, None)\n \n   @tf.__internal__.distribute.combinations.generate(\n       tf.__internal__.test.combinations.combine(\n           distribution=[\n-              tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n+              tf.__internal__.distribute.combinations\n+              .mirrored_strategy_with_gpu_and_cpu,\n           ],\n           mode=['graph', 'eager']))\n   def test_validating_dataset_input_tensors_with_dtype_mismatch(\n       self, distribution):\n     with self.cached_session():\n-      a = tf.constant([1, 2], shape=(1, 2), dtype=tf.int32)\n-      b = tf.constant([1, 2], shape=(1, 2), dtype=tf.float64)\n-      x = tf.distribute.DistributedValues((a, b))\n-      y = tf.distribute.DistributedValues((a, a))\n+\n+      @tf.function\n+      def run():\n+        ctx = tf.distribute.get_replica_context()\n+        if ctx.replica_id_in_sync_group.device.endswith('GPU:0'):\n+          return tf.constant([[1, 2]], dtype=tf.int32)\n+        else:\n+          return tf.constant([[1, 2]], dtype=tf.float64)\n+\n+      x = distribution.run(run)\n+\n       # Removed device and input tensor dtype details from the error message\n       # since the order of the device and the corresponding input tensor dtype\n       # is not deterministic over different runs.\n       with self.assertRaisesRegex(\n           ValueError, 'Input tensor dtypes do not match for '\n           'distributed tensor inputs '\n-          'DistributedValues:.+'):\n+          'PerReplica:.+'):\n         with distribution.scope():\n           distributed_training_utils_v1.validate_distributed_dataset_inputs(\n-              distribution, x, y)\n+              distribution, x, None)\n \n   @tf.__internal__.distribute.combinations.generate(\n       tf.__internal__.test.combinations.combine(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9258d026f7782084cd75b78e13872bc6b3f65c8d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 46 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@\n # ==============================================================================\n \"\"\"Keras' DTensor library.\"\"\"\n \n-_DTENSOR_API_ENABLED = False\n+_DTENSOR_API_ENABLED = True\n \n \n # Conditional import the dtensor API, since it is currently broken in OSS.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2db5acf3e3c5904b014cb409d3c514bef44f9640", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 186 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -128,7 +128,10 @@ class ApplicationsLoadWeightTest(tf.test.TestCase, parameterized.TestCase):\n     app_module = ARG_TO_MODEL[FLAGS.module][0]\n     apps = ARG_TO_MODEL[FLAGS.module][1]\n     for app in apps:\n+      try:\n         model = app(weights='imagenet')\n+      except Exception:  # pylint: disable=broad-except\n+        self.skipTest('TODO(b/227700184): Re-enable.')\n       self.assertShapeEqual(model.output_shape, (None, _IMAGENET_CLASSES))\n       x = _get_elephant(model.input_shape[1:3])\n       x = app_module.preprocess_input(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5eaca086dd22d7aabcad703a330cd12247f2a15e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 739 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 32 | Complexity Δ (Sum/Max): 73/42 | Churn Δ: 739 | Churn Cumulative: 739 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.8211764705882353\n\nDIFF:\n@@ -0,0 +1,326 @@\n+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Denormalization preprocessing layer.\"\"\"\n+\n+# pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine import base_preprocessing_layer\n+from keras.layers.preprocessing import preprocessing_utils as utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Denormalization',\n+              'keras.layers.experimental.preprocessing.Denormalization')\n+class Denormalization(base_preprocessing_layer.PreprocessingLayer):\n+  \"\"\"A preprocessing layer which denormalizes continuous features.\n+\n+  This layer will shift and scale inputs into a distribution of training data. It accomplishes this by precomputing the mean and\n+  variance of the data, and calling `(input + mean) * sqrt(var)` at runtime.\n+\n+  The mean and variance values for the layer must be either supplied on\n+  construction or learned via `adapt()`. `adapt()` will compute the mean and\n+  variance of the data and store them as the layer's weights. `adapt()` should\n+  be called before `fit()`, `evaluate()`, or `predict()`.\n+\n+  For an overview and full list of preprocessing layers, see the preprocessing\n+  [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n+\n+  Args:\n+      axis: Integer, tuple of integers, or None. The axis or axes that should\n+        have a separate mean and variance for each index in the shape. For\n+        example, if shape is `(None, 5)` and `axis=1`, the layer will track 5\n+        separate mean and variance values for the last axis. If `axis` is set to\n+        `None`, the layer will normalize all elements in the input by a scalar\n+        mean and variance. Defaults to -1, where the last axis of the input is\n+        assumed to be a feature dimension and is normalized per index. Note that\n+        in the specific case of batched scalar inputs where the only axis is the\n+        batch axis, the default will normalize each index in the batch\n+        separately. In this case, consider passing `axis=None`.\n+      mean: The mean value(s) to use during denormalization. The passed value(s)\n+        will be broadcast to the shape of the kept axes above; if the value(s)\n+        cannot be broadcast, an error will be raised when this layer's `build()`\n+        method is called.\n+      variance: The variance value(s) to use during denormalization. The passed\n+        value(s) will be broadcast to the shape of the kept axes above; if the\n+        value(s) cannot be broadcast, an error will be raised when this layer's\n+        `build()` method is called.\n+\n+  Examples:\n+\n+  Calculate a global mean and variance by analyzing the dataset in `adapt()`.\n+\n+  >>> adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')\n+  >>> input_data = np.array([1., 2., 3.], dtype='float32')\n+  >>> layer = tf.keras.layers.Denormalization(axis=None)\n+  >>> layer.adapt(adapt_data)\n+  >>> layer(input_data)\n+  <tf.Tensor: shape=(3,), dtype=float32, numpy=\n+  array([-1.4142135, -0.70710677, 0.], dtype=float32)>\n+\n+  Calculate a mean and variance for each index on the last axis.\n+\n+  >>> adapt_data = np.array([[0., 7., 4.],\n+  ...                        [2., 9., 6.],\n+  ...                        [0., 7., 4.],\n+  ...                        [2., 9., 6.]], dtype='float32')\n+  >>> input_data = np.array([[0., 7., 4.]], dtype='float32')\n+  >>> layer = tf.keras.layers.Denormalization(axis=-1)\n+  >>> layer.adapt(adapt_data)\n+  >>> layer(input_data)\n+  <tf.Tensor: shape=(1, 3), dtype=float32, numpy=\n+  array([-1., -1., -1.], dtype=float32)>\n+\n+  Pass the mean and variance directly.\n+\n+  >>> input_data = np.array([[1.], [2.], [3.]], dtype='float32')\n+  >>> layer = tf.keras.layers.Denormalization(mean=3., variance=2.)\n+  >>> layer(input_data)\n+  <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n+  array([[-1.4142135 ],\n+         [-0.70710677],\n+         [ 0.        ]], dtype=float32)>\n+  \"\"\"\n+\n+  def __init__(self, axis=-1, mean=None, variance=None, **kwargs):\n+    super().__init__(**kwargs)\n+    base_preprocessing_layer.keras_kpl_gauge.get_cell('Denormalization').set(True)\n+\n+    # Standardize `axis` to a tuple.\n+    if axis is None:\n+      axis = ()\n+    elif isinstance(axis, int):\n+      axis = (axis,)\n+    else:\n+      axis = tuple(axis)\n+    self.axis = axis\n+\n+    # Set `mean` and `variance` if passed.\n+    if isinstance(mean, tf.Variable):\n+      raise ValueError('Denormalization does not support passing a Variable '\n+                       'for the `mean` init arg.')\n+    if isinstance(variance, tf.Variable):\n+      raise ValueError('Denormalization does not support passing a Variable '\n+                       'for the `variance` init arg.')\n+    if (mean is not None) != (variance is not None):\n+      raise ValueError(\n+          'When setting values directly, both `mean` and `variance` '\n+          'must be set. Got mean: {} and variance: {}'.format(mean, variance))\n+    self.input_mean = mean\n+    self.input_variance = variance\n+\n+  def build(self, input_shape):\n+    super().build(input_shape)\n+\n+    if (isinstance(input_shape, (list, tuple)) and\n+        all(isinstance(shape, tf.TensorShape) for shape in input_shape)):\n+      raise ValueError('Denormalization only accepts a single input. If you are '\n+                       'passing a python list or tuple as a single input, '\n+                       'please convert to a numpy array or `tf.Tensor`.')\n+\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    ndim = len(input_shape)\n+\n+    if any(a < -ndim or a >= ndim for a in self.axis):\n+      raise ValueError('All `axis` values must be in the range [-ndim, ndim). '\n+                       'Found ndim: `{}`, axis: {}'.format(ndim, self.axis))\n+\n+    # Axes to be kept, replacing negative values with positive equivalents.\n+    # Sorted to avoid transposing axes.\n+    self._keep_axis = sorted([d if d >= 0 else d + ndim for d in self.axis])\n+    # All axes to be kept should have known shape.\n+    for d in self._keep_axis:\n+      if input_shape[d] is None:\n+        raise ValueError(\n+            'All `axis` values to be kept must have known shape. Got axis: {}, '\n+            'input shape: {}, with unknown axis at index: {}'.format(\n+                self.axis, input_shape, d))\n+    # Axes to be reduced.\n+    self._reduce_axis = [d for d in range(ndim) if d not in self._keep_axis]\n+    # 1 if an axis should be reduced, 0 otherwise.\n+    self._reduce_axis_mask = [\n+        0 if d in self._keep_axis else 1 for d in range(ndim)\n+    ]\n+    # Broadcast any reduced axes.\n+    self._broadcast_shape = [\n+        input_shape[d] if d in self._keep_axis else 1 for d in range(ndim)\n+    ]\n+    mean_and_var_shape = tuple(input_shape[d] for d in self._keep_axis)\n+\n+    if self.input_mean is None:\n+      self.adapt_mean = self.add_weight(\n+          name='mean',\n+          shape=mean_and_var_shape,\n+          dtype=self.compute_dtype,\n+          initializer='zeros',\n+          trainable=False)\n+      self.adapt_variance = self.add_weight(\n+          name='variance',\n+          shape=mean_and_var_shape,\n+          dtype=self.compute_dtype,\n+          initializer='ones',\n+          trainable=False)\n+      self.count = self.add_weight(\n+          name='count',\n+          shape=(),\n+          dtype=tf.int64,\n+          initializer='zeros',\n+          trainable=False)\n+      self.finalize_state()\n+    else:\n+      # In the no adapt case, make constant tensors for mean and variance with\n+      # proper broadcast shape for use during call.\n+      mean = self.input_mean * np.ones(mean_and_var_shape)\n+      variance = self.input_variance * np.ones(mean_and_var_shape)\n+      mean = tf.reshape(mean, self._broadcast_shape)\n+      variance = tf.reshape(variance, self._broadcast_shape)\n+      self.mean = tf.cast(mean, self.compute_dtype)\n+      self.variance = tf.cast(variance, self.compute_dtype)\n+\n+  # We override this method solely to generate a docstring.\n+  def adapt(self, data, batch_size=None, steps=None):\n+    \"\"\"Computes the mean and variance of values in a dataset.\n+\n+    Calling `adapt()` on a `Denormalization` layer is an alternative to passing in\n+    `mean` and `variance` arguments during layer construction. A `Denormalization`\n+    layer should always either be adapted over a dataset or passed `mean` and\n+    `variance`.\n+\n+    During `adapt()`, the layer will compute a `mean` and `variance` separately\n+    for each position in each axis specified by the `axis` argument. To\n+    calculate a single `mean` and `variance` over the input data, simply pass\n+    `axis=None`.\n+\n+    In order to make `Denormalization` efficient in any distribution context, the\n+    computed mean and variance are kept static with respect to any compiled\n+    `tf.Graph`s that call the layer. As a consequence, if the layer is adapted a\n+    second time, any models using the layer should be re-compiled. For more\n+    information see\n+    `tf.keras.layers.experimental.preprocessing.PreprocessingLayer.adapt`.\n+\n+    `adapt()` is meant only as a single machine utility to compute layer state.\n+    To analyze a dataset that cannot fit on a single machine, see\n+    [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started)\n+    for a multi-machine, map-reduce solution.\n+\n+    Arguments:\n+      data: The data to train on. It can be passed either as a\n+          `tf.data.Dataset`, or as a numpy array.\n+      batch_size: Integer or `None`.\n+          Number of samples per state update.\n+          If unspecified, `batch_size` will default to 32.\n+          Do not specify the `batch_size` if your data is in the\n+          form of datasets, generators, or `keras.utils.Sequence` instances\n+          (since they generate batches).\n+      steps: Integer or `None`.\n+          Total number of steps (batches of samples)\n+          When training with input tensors such as\n+          TensorFlow data tensors, the default `None` is equal to\n+          the number of samples in your dataset divided by\n+          the batch size, or 1 if that cannot be determined. If x is a\n+          `tf.data` dataset, and 'steps' is None, the epoch will run until\n+          the input dataset is exhausted. When passing an infinitely\n+          repeating dataset, you must specify the `steps` argument. This\n+          argument is not supported with array inputs.\n+    \"\"\"\n+    super().adapt(data, batch_size=batch_size, steps=steps)\n+\n+  def update_state(self, data):\n+    if self.input_mean is not None:\n+      raise ValueError(\n+          'Cannot `adapt` a Denormalization layer that is initialized with '\n+          'static `mean` and `variance`, you passed mean {} and variance {}.'\n+          .format(self.input_mean, self.input_variance))\n+\n+    if not self.built:\n+      raise RuntimeError('`build` must be called before `update_state`.')\n+\n+    data = self._standardize_inputs(data)\n+    data = tf.cast(data, self.adapt_mean.dtype)\n+    batch_mean, batch_variance = tf.nn.moments(data, axes=self._reduce_axis)\n+    batch_shape = tf.shape(data, out_type=self.count.dtype)\n+    if self._reduce_axis:\n+      batch_reduce_shape = tf.gather(batch_shape, self._reduce_axis)\n+      batch_count = tf.reduce_prod(batch_reduce_shape)\n+    else:\n+      batch_count = 1\n+\n+    total_count = batch_count + self.count\n+    batch_weight = (\n+        tf.cast(batch_count, dtype=self.compute_dtype) /\n+        tf.cast(total_count, dtype=self.compute_dtype))\n+    existing_weight = 1. - batch_weight\n+\n+    total_mean = self.adapt_mean * existing_weight + batch_mean * batch_weight\n+    # The variance is computed using the lack-of-fit sum of squares\n+    # formula (see https://en.wikipedia.org/wiki/Lack-of-fit_sum_of_squares).\n+    total_variance = ((self.adapt_variance +\n+                       (self.adapt_mean - total_mean)**2) * existing_weight +\n+                      (batch_variance +\n+                       (batch_mean - total_mean)**2) * batch_weight)\n+    self.adapt_mean.assign(total_mean)\n+    self.adapt_variance.assign(total_variance)\n+    self.count.assign(total_count)\n+\n+  def reset_state(self):  # pylint: disable=method-hidden\n+    if self.input_mean is not None or not self.built:\n+      return\n+\n+    self.adapt_mean.assign(tf.zeros_like(self.adapt_mean))\n+    self.adapt_variance.assign(tf.ones_like(self.adapt_variance))\n+    self.count.assign(tf.zeros_like(self.count))\n+\n+  def finalize_state(self):\n+    if self.input_mean is not None or not self.built:\n+      return\n+\n+    # In the adapt case, we make constant tensors for mean and variance with\n+    # proper broadcast shape and dtype each time `finalize_state` is called.\n+    self.mean = tf.reshape(self.adapt_mean, self._broadcast_shape)\n+    self.mean = tf.cast(self.mean, self.compute_dtype)\n+    self.variance = tf.reshape(self.adapt_variance, self._broadcast_shape)\n+    self.variance = tf.cast(self.variance, self.compute_dtype)\n+\n+  def call(self, inputs):\n+    inputs = self._standardize_inputs(inputs)\n+    # The base layer automatically casts floating-point inputs, but we\n+    # explicitly cast here to also allow integer inputs to be passed\n+    inputs = tf.cast(inputs, self.compute_dtype)\n+    return ((inputs + self.mean) *\n+            tf.maximum(tf.sqrt(self.variance), backend.epsilon()))\n+\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n+\n+  def compute_output_signature(self, input_spec):\n+    return input_spec\n+\n+  def get_config(self):\n+    config = super().get_config()\n+    config.update({\n+        'axis': self.axis,\n+        'mean': utils.listify_tensors(self.input_mean),\n+        'variance': utils.listify_tensors(self.input_variance),\n+    })\n+    return config\n+\n+  def _standardize_inputs(self, inputs):\n+    inputs = tf.convert_to_tensor(inputs)\n+    if inputs.dtype != self.compute_dtype:\n+      inputs = tf.cast(inputs, self.compute_dtype)\n+    return inputs\n\n@@ -0,0 +1,413 @@\n+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for keras.layers.preprocessing.denormalization.\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+\n+import os\n+\n+from absl.testing import parameterized\n+\n+import numpy as np\n+\n+import keras\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n+from keras.layers.preprocessing import denormalization\n+from keras.layers.preprocessing import preprocessing_test_utils\n+from keras.mixed_precision import policy\n+\n+\n+def _get_layer_computation_test_cases():\n+  test_cases = ({\n+      \"adapt_data\": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),\n+      \"axis\": -1,\n+      \"test_data\": np.array([[1.], [2.], [3.]], np.float32),\n+      \"expected\": np.array([[-1.414214], [-.707107], [0]], np.float32),\n+      \"testcase_name\": \"2d_single_element\"\n+  }, {\n+      \"adapt_data\": np.array([[1], [2], [3], [4], [5]], dtype=np.int32),\n+      \"axis\": -1,\n+      \"test_data\": np.array([[1], [2], [3]], np.int32),\n+      \"expected\": np.array([[-1.414214], [-.707107], [0]], np.float32),\n+      \"testcase_name\": \"2d_int_data\"\n+  }, {\n+      \"adapt_data\": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),\n+      \"axis\": None,\n+      \"test_data\": np.array([[1.], [2.], [3.]], np.float32),\n+      \"expected\": np.array([[-1.414214], [-.707107], [0]], np.float32),\n+      \"testcase_name\": \"2d_single_element_none_axis\"\n+  }, {\n+      \"adapt_data\": np.array([[1., 2., 3., 4., 5.]], dtype=np.float32),\n+      \"axis\": None,\n+      \"test_data\": np.array([[1.], [2.], [3.]], np.float32),\n+      \"expected\": np.array([[-1.414214], [-.707107], [0]], np.float32),\n+      \"testcase_name\": \"2d_single_element_none_axis_flat_data\"\n+  }, {\n+      \"adapt_data\":\n+          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],\n+                   np.float32),\n+      \"axis\":\n+          1,\n+      \"test_data\":\n+          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],\n+                   np.float32),\n+      \"expected\":\n+          np.array([[[-1.549193, -0.774597, 0.], [-1.549193, -0.774597, 0.]],\n+                    [[0., 0.774597, 1.549193], [0., 0.774597, 1.549193]]],\n+                   np.float32),\n+      \"testcase_name\":\n+          \"3d_internal_axis\"\n+  }, {\n+      \"adapt_data\":\n+          np.array(\n+              [[[1., 0., 3.], [2., 3., 4.]], [[3., -1., 5.], [4., 5., 8.]]],\n+              np.float32),\n+      \"axis\": (1, 2),\n+      \"test_data\":\n+          np.array(\n+              [[[3., 1., -1.], [2., 5., 4.]], [[3., 0., 5.], [2., 5., 8.]]],\n+              np.float32),\n+      \"expected\":\n+          np.array(\n+              [[[1., 3., -5.], [-1., 1., -1.]], [[1., 1., 1.], [-1., 1., 1.]]],\n+              np.float32),\n+      \"testcase_name\":\n+          \"3d_multiple_axis\"\n+  }, {\n+      \"adapt_data\":\n+          np.zeros((3, 4)),\n+      \"axis\": -1,\n+      \"test_data\":\n+          np.zeros((3, 4)),\n+      \"expected\":\n+          np.zeros((3, 4)),\n+      \"testcase_name\":\n+          \"zero_variance\"\n+  })\n+\n+  crossed_test_cases = []\n+  # Cross above test cases with use_dataset in (True, False)\n+  for use_dataset in (True, False):\n+    for case in test_cases:\n+      case = case.copy()\n+      if use_dataset:\n+        case[\"testcase_name\"] = case[\"testcase_name\"] + \"_with_dataset\"\n+      case[\"use_dataset\"] = use_dataset\n+      crossed_test_cases.append(case)\n+\n+  return crossed_test_cases\n+\n+\n+@test_combinations.run_all_keras_modes\n+class NormalizationTest(test_combinations.TestCase,\n+                        preprocessing_test_utils.PreprocessingLayerTest):\n+\n+  def test_broadcasting_during_direct_setting(self):\n+    layer = denormalization.Denormalization(axis=-1, mean=[1.0], variance=[2.0])\n+    output = layer(np.array([[1., 2.]]))\n+    expected_output = [[4., 6.]]\n+    self.assertAllClose(output, expected_output)\n+    self.assertAllClose(layer.get_weights(), [])\n+\n+  def test_broadcasting_during_direct_setting_with_tensors(self):\n+    if not tf.executing_eagerly():\n+      self.skipTest(\"Only supported in TF2.\")\n+\n+    layer = denormalization.Denormalization(\n+        axis=-1,\n+        mean=tf.constant([1.0]),\n+        variance=tf.constant([2.0]))\n+    output = layer(np.array([[1., 2.]]))\n+    expected_output = [[4., 6.]]\n+    self.assertAllClose(output, expected_output)\n+    self.assertAllClose(layer.get_weights(), [])\n+\n+  def test_1d_data(self):\n+    data = np.array([0., 2., 0., 2.])\n+    layer = denormalization.Denormalization(mean=1.0, variance=2.0)\n+    output = layer(data)\n+    self.assertListEqual(output.shape.as_list(), [4])\n+    self.assertAllClose(output, [2, 6, -2, 6])\n+\n+  def test_0d_data(self):\n+    layer = denormalization.Denormalization(axis=None, mean=1.0, variance=2.0)\n+    output = layer(0.)\n+    self.assertListEqual(output.shape.as_list(), [])\n+    self.assertAllClose(output, 2)\n+\n+  def test_broadcasting_during_direct_setting_with_variables_fails(self):\n+    with self.assertRaisesRegex(ValueError, \"passing a Variable\"):\n+      _ = denormalization.Denormalization(\n+          axis=-1,\n+          mean=tf.Variable([1.0]),\n+          variance=tf.Variable([2.0]))\n+\n+  def test_keeping_an_unknown_axis_fails(self):\n+    layer = denormalization.Denormalization(axis=-1)\n+    with self.assertRaisesRegex(ValueError, \"axis.*must have known shape\"):\n+      layer.build([None])\n+\n+  @parameterized.parameters(\n+      # Out of bounds\n+      {\"axis\": 3},\n+      {\"axis\": -4},\n+      # In a tuple\n+      {\"axis\": (1, 3)},\n+      {\"axis\": (1, -4)},\n+  )\n+  def test_bad_axis_fail_build(self, axis):\n+    layer = denormalization.Denormalization(axis=axis)\n+    with self.assertRaisesRegex(ValueError, \"in the range\"):\n+      layer.build([None, 2, 3])\n+\n+  def test_list_input(self):\n+    with self.assertRaisesRegex(\n+        ValueError, (\"Normalization only accepts a single input. If you are \"\n+                     \"passing a python list or tuple as a single input, \"\n+                     \"please convert to a numpy array or `tf.Tensor`.\")):\n+      denormalization.Denormalization()([1, 2, 3])\n+\n+  def test_scalar_input(self):\n+    with self.assertRaisesRegex(ValueError,\n+                                \"axis.*values must be in the range\"):\n+      denormalization.Denormalization()(1)\n+\n+  def test_output_dtype(self):\n+    if not tf.__internal__.tf2.enabled():\n+      self.skipTest(\"set_global_policy only supported in TF2.\")\n+    # Output should respect an explicit dtype, and default to the global policy.\n+    policy.set_global_policy(\"float64\")\n+    input_data = keras.Input(batch_size=16, shape=(1,))\n+    layer = denormalization.Denormalization(mean=1.0, variance=2.0, dtype=\"float16\")\n+    output = layer(input_data)\n+    self.assertAllEqual(output.dtype, tf.float16)\n+    layer = denormalization.Denormalization(mean=1.0, variance=2.0)\n+    output = layer(input_data)\n+    self.assertAllEqual(output.dtype, tf.float64)\n+\n+\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class NormalizationAdaptTest(test_combinations.TestCase,\n+                             preprocessing_test_utils.PreprocessingLayerTest):\n+\n+  def test_layer_api_compatibility(self):\n+    cls = denormalization.Denormalization\n+    output_data = test_utils.layer_test(\n+        cls,\n+        kwargs={\"axis\": -1},\n+        input_shape=(None, 3),\n+        input_data=np.array([[3, 1, 2], [6, 5, 4]], dtype=np.float32),\n+        validate_training=False,\n+        adapt_data=np.array([[1, 2, 1], [2, 3, 4], [1, 2, 1], [2, 3, 4]]))\n+    expected = np.array([[3., -3., -0.33333333], [9., 5., 1.]])\n+    self.assertAllClose(expected, output_data)\n+\n+  @parameterized.named_parameters(*_get_layer_computation_test_cases())\n+  def test_layer_computation(self, adapt_data, axis, test_data, use_dataset,\n+                             expected):\n+    input_shape = tuple([test_data.shape[i] for i in range(1, test_data.ndim)])\n+    if use_dataset:\n+      # Keras APIs expect batched datasets\n+      adapt_data = tf.data.Dataset.from_tensor_slices(adapt_data).batch(\n+          test_data.shape[0] // 2)\n+      test_data = tf.data.Dataset.from_tensor_slices(test_data).batch(\n+          test_data.shape[0] // 2)\n+\n+    layer = denormalization.Denormalization(axis=axis)\n+    layer.adapt(adapt_data)\n+\n+    input_data = keras.Input(shape=input_shape)\n+    output = layer(input_data)\n+    model = keras.Model(input_data, output)\n+    model._run_eagerly = test_utils.should_run_eagerly()\n+    output_data = model.predict(test_data)\n+    self.assertAllClose(expected, output_data)\n+\n+  def test_1d_unbatched_adapt(self):\n+    ds = tf.data.Dataset.from_tensor_slices([\n+        [2., 0., 2., 0.],\n+        [0., 2., 0., 2.],\n+    ])\n+    layer = denormalization.Denormalization(axis=-1)\n+    layer.adapt(ds)\n+    output_ds = ds.map(layer)\n+    self.assertAllClose(\n+        list(output_ds.as_numpy_iterator()), [\n+            [1., -1., 1., -1.],\n+            [-1., 1., -1., 1.],\n+        ])\n+\n+  def test_0d_unbatched_adapt(self):\n+    ds = tf.data.Dataset.from_tensor_slices([2., 0., 2., 0.])\n+    layer = denormalization.Denormalization(axis=None)\n+    layer.adapt(ds)\n+    output_ds = ds.map(layer)\n+    self.assertAllClose(list(output_ds.as_numpy_iterator()), [1., -1., 1., -1.])\n+\n+  @parameterized.parameters(\n+      # Results should be identical no matter how the axes are specified (3d).\n+      {\"axis\": (1, 2)},\n+      {\"axis\": (2, 1)},\n+      {\"axis\": (1, -1)},\n+      {\"axis\": (-1, 1)},\n+  )\n+  def test_axis_permutations(self, axis):\n+    layer = denormalization.Denormalization(axis=axis)\n+    # data.shape = [2, 2, 3]\n+    data = np.array([[[0., 1., 2.], [0., 2., 6.]],\n+                     [[2., 3., 4.], [3., 6., 10.]]])\n+    expect = np.array([[[-1., -1., -1.], [-1., -1., -1.]],\n+                       [[1., 1., 1.], [1., 1., 1.]]])\n+    layer.adapt(data)\n+    self.assertAllClose(expect, layer(data))\n+\n+  def test_model_summary_after_layer_adapt(self):\n+    data = np.array([[[0., 1., 2.], [0., 2., 6.]],\n+                     [[2., 3., 4.], [3., 6., 10.]]])\n+    layer = denormalization.Denormalization(axis=-1)\n+    layer.adapt(data)\n+    model = keras.Sequential(\n+        [layer,\n+         keras.layers.Dense(64, activation=\"relu\"),\n+         keras.layers.Dense(1)])\n+    model.summary()\n+\n+  def test_multiple_adapts(self):\n+    first_adapt = [[0], [2], [0], [2]]\n+    second_adapt = [[2], [4], [2], [4]]\n+    predict_input = [[2], [2]]\n+    expected_first_output = [[1], [1]]\n+    expected_second_output = [[-1], [-1]]\n+\n+    inputs = keras.Input(shape=(1,), dtype=tf.int32)\n+    layer = denormalization.Denormalization(axis=-1)\n+    layer.adapt(first_adapt)\n+    outputs = layer(inputs)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+\n+    actual_output = model.predict(predict_input)\n+    self.assertAllClose(actual_output, expected_first_output)\n+\n+    # Re-adapt the layer on new inputs.\n+    layer.adapt(second_adapt)\n+    # Re-compile the model.\n+    model.compile()\n+    # `predict` should now use the new model state.\n+    actual_output = model.predict(predict_input)\n+    self.assertAllClose(actual_output, expected_second_output)\n+\n+  @parameterized.parameters(\n+      {\"adapted\": True},\n+      {\"adapted\": False},\n+  )\n+  def test_saved_model_tf(self, adapted):\n+    input_data = [[0.], [2.], [0.], [2.]]\n+    expected_output = [[-1.], [1.], [-1.], [1.]]\n+\n+    inputs = keras.Input(shape=(1,), dtype=tf.float32)\n+    if adapted:\n+      layer = denormalization.Normalization(axis=-1)\n+      layer.adapt(input_data)\n+    else:\n+      layer = denormalization.Normalization(mean=1., variance=2.)\n+    outputs = layer(inputs)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+\n+    output_data = model.predict(input_data)\n+    self.assertAllClose(output_data, expected_output)\n+\n+    # Save the model to disk.\n+    output_path = os.path.join(self.get_temp_dir(), \"tf_saved_model\")\n+    tf.saved_model.save(model, output_path)\n+    loaded_model = tf.saved_model.load(output_path)\n+    f = loaded_model.signatures[\"serving_default\"]\n+\n+    # Ensure that the loaded model is unique (so that the save/load is real)\n+    self.assertIsNot(model, loaded_model)\n+\n+    # Validate correctness of the new model.\n+    new_output_data = f(tf.constant(input_data))[\"normalization\"]\n+    self.assertAllClose(new_output_data, expected_output)\n+\n+  @parameterized.product(\n+      save_format=[\"tf\", \"h5\"],\n+      adapt=[True, False],\n+  )\n+  def test_saved_model_keras(self, save_format, adapt):\n+    input_data = [[0.], [2.], [0.], [2.]]\n+    expected_output = [[-1.], [1.], [-1.], [1.]]\n+\n+    cls = denormalization.Denormalization\n+    inputs = keras.Input(shape=(1,), dtype=tf.float32)\n+    if adapt:\n+      layer = cls(axis=-1)\n+      layer.adapt(input_data)\n+    else:\n+      layer = cls(mean=1., variance=2.)\n+    outputs = layer(inputs)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+\n+    output_data = model.predict(input_data)\n+    self.assertAllClose(output_data, expected_output)\n+\n+    # Save the model to disk.\n+    output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n+    model.save(output_path, save_format=format)\n+    loaded_model = keras.models.load_model(\n+        output_path, custom_objects={\"Normalization\": cls})\n+\n+    # Ensure that the loaded model is unique (so that the save/load is real)\n+    self.assertIsNot(model, loaded_model)\n+\n+    # Validate correctness of the new model.\n+    new_output_data = loaded_model.predict(input_data)\n+    self.assertAllClose(new_output_data, expected_output)\n+\n+  @parameterized.parameters(\n+      {\"adapted\": True},\n+      {\"adapted\": False},\n+  )\n+  def test_saved_weights_keras(self, adapted):\n+    input_data = [[0.], [2.], [0.], [2.]]\n+    expected_output = [[-1.], [1.], [-1.], [1.]]\n+\n+    cls = denormalization.Denormalization\n+    inputs = keras.Input(shape=(1,), dtype=tf.float32)\n+    if adapted:\n+      layer = cls(axis=-1)\n+      layer.adapt(input_data)\n+    else:\n+      layer = cls(mean=1., variance=2.)\n+    outputs = layer(inputs)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+\n+    output_data = model.predict(input_data)\n+    self.assertAllClose(output_data, expected_output)\n+\n+    # Save the model to disk.\n+    output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_weights\")\n+    model.save_weights(output_path, save_format=\"tf\")\n+    new_model = keras.Model.from_config(\n+        model.get_config(), custom_objects={\"Normalization\": cls})\n+    new_model.load_weights(output_path)\n+\n+    # Validate correctness of the new model.\n+    new_output_data = new_model.predict(input_data)\n+    self.assertAllClose(new_output_data, expected_output)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
