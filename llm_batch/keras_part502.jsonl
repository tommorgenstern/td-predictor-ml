{"custom_id": "keras#3c46a96ddafac50ff3640c1fe110b448bded0153", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 70 | Files Changed: 2 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): -13/0 | Churn Δ: 70 | Churn Cumulative: 694 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 0.23529411764705882\n\nDIFF:\n@@ -19,7 +19,6 @@ import os\n import pathlib\n import shutil\n import tempfile\n-import warnings\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -1322,55 +1321,6 @@ class TestWholeModelSaving(test_combinations.TestCase):\n         # loaded model.\n         self.assertSequenceEqual(model.metrics_names, loaded.metrics_names)\n \n-    @test_combinations.generate(\n-        test_combinations.combine(mode=[\"graph\", \"eager\"])\n-    )\n-    def test_warning_when_saving_invalid_custom_mask_layer(self):\n-        class MyMasking(keras.layers.Layer):\n-            def call(self, inputs):\n-                return inputs\n-\n-            def compute_mask(self, inputs, mask=None):\n-                mask = tf.not_equal(inputs, 0)\n-                return mask\n-\n-        class MyLayer(keras.layers.Layer):\n-            def call(self, inputs, mask=None):\n-                return tf.identity(inputs)\n-\n-        samples = np.random.random((2, 2))\n-        model = keras.Sequential([MyMasking(), MyLayer()])\n-        model.predict(samples)\n-        with warnings.catch_warnings(record=True) as w:\n-            model.save(self._save_model_dir(), test_utils.get_save_format())\n-        self.assertIn(\n-            serialization.CustomMaskWarning, {warning.category for warning in w}\n-        )\n-\n-        # Test that setting up a custom mask correctly does not issue a warning.\n-        class MyCorrectMasking(keras.layers.Layer):\n-            def call(self, inputs):\n-                return inputs\n-\n-            def compute_mask(self, inputs, mask=None):\n-                mask = tf.not_equal(inputs, 0)\n-                return mask\n-\n-            # This get_config doesn't actually do anything because our mask is\n-            # static and doesn't need any external information to work. We do\n-            # need a dummy get_config method to prevent the warning from\n-            # appearing, however.\n-            def get_config(self, *args, **kwargs):\n-                return {}\n-\n-        model = keras.Sequential([MyCorrectMasking(), MyLayer()])\n-        model.predict(samples)\n-        with warnings.catch_warnings(record=True) as w:\n-            model.save(self._save_model_dir(), test_utils.get_save_format())\n-        self.assertNotIn(\n-            serialization.CustomMaskWarning, {warning.category for warning in w}\n-        )\n-\n     # Test only in eager mode because ragged tensor inputs\n     # cannot be used in graph mode.\n     @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n\n@@ -15,7 +15,6 @@\n \"\"\"Legacy serialization logic for Keras models.\"\"\"\n \n import threading\n-import warnings\n import weakref\n \n import tensorflow.compat.v2 as tf\n@@ -278,10 +277,6 @@ def skip_failed_serialization():\n         _SKIP_FAILED_SERIALIZATION = prev\n \n \n-class CustomMaskWarning(Warning):\n-    pass\n-\n-\n @keras_export(\"keras.utils.serialize_keras_object\")\n def serialize_keras_object(instance):\n     \"\"\"Serialize a Keras object into a JSON-compatible representation.\n@@ -303,21 +298,6 @@ def serialize_keras_object(instance):\n     if instance is None:\n         return None\n \n-    # For v1 layers, checking supports_masking is not enough. We have to also\n-    # check whether compute_mask has been overridden.\n-    supports_masking = getattr(instance, \"supports_masking\", False) or (\n-        hasattr(instance, \"compute_mask\")\n-        and not is_default(instance.compute_mask)\n-    )\n-    if supports_masking and is_default(instance.get_config):\n-        warnings.warn(\n-            \"Custom mask layers require a config and must override \"\n-            \"get_config. When loading, the custom mask layer must be \"\n-            \"passed to the custom_objects argument.\",\n-            category=CustomMaskWarning,\n-            stacklevel=2,\n-        )\n-\n     if hasattr(instance, \"get_config\"):\n         name = object_registration.get_registered_name(instance.__class__)\n         try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e69dd22bc51b28b9f311c81abed92dfe46e82960", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 147 | Lines Deleted: 42 | Files Changed: 5 | Hunks: 26 | Methods Changed: 12 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 189 | Churn Cumulative: 30669 | Contributors (this commit): 142 | Commits (past 90d): 84 | Contributors (cumulative): 171 | DMM Complexity: 0.6770833333333334\n\nDIFF:\n@@ -2898,56 +2898,55 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     @traceback_utils.filter_traceback\n     def load_weights(\n-        self, filepath, by_name=False, skip_mismatch=False, options=None\n+        self, filepath, skip_mismatch=False, by_name=False, options=None\n     ):\n-        \"\"\"Loads all layer weights, either from a SavedModel or H5 weights file.\n+        \"\"\"Loads all layer weights from a saved files.\n \n-        If `by_name` is False weights are loaded based on the network's\n+        The saved file could be a SavedModel file, a `.keras` file (v3 saving\n+        format), or a file created via `model.save_weights()`.\n+\n+        By default, weights are loaded based on the network's\n         topology. This means the architecture should be the same as when the\n         weights were saved. Note that layers that don't have weights are not\n         taken into account in the topological ordering, so adding or removing\n         layers is fine as long as they don't have weights.\n \n-        If `by_name` is True, weights are loaded into layers only if they share\n+        **Partial weight loading**\n+\n+        If you have modified your model, for instance by adding a new layer\n+        (with weights) or by changing the shape of the weights of a layer,\n+        you can choose to ignore errors and continue loading\n+        by setting `skip_mismatch=True`. In this case any layer with\n+        mismatching weights will be skipped. A warning will be displayed\n+        for each skipped layer.\n+\n+        **Weight loading by name**\n+\n+        If your weights are saved as a `.h5` file created\n+        via `model.save_weights()`, you can use the argument `by_name=True`.\n+\n+        In this case, weights are loaded into layers only if they share\n         the same name. This is useful for fine-tuning or transfer-learning\n         models where some of the layers have changed.\n \n-        Only topological loading (`by_name=False`) is supported when loading\n-        weights from the TensorFlow format. Note that topological loading\n-        differs slightly between TensorFlow and HDF5 formats for user-defined\n-        classes inheriting from `tf.keras.Model`: HDF5 loads based on a\n-        flattened list of weights, while the TensorFlow format loads based on\n-        the object-local names of attributes to which layers are assigned in the\n-        `Model`'s constructor.\n+        Note that only topological loading (`by_name=False`) is supported when\n+        loading weights from the `.keras` v3 format or from the TensorFlow\n+        SavedModel format.\n \n         Args:\n             filepath: String, path to the weights file to load. For weight files\n                 in TensorFlow format, this is the file prefix (the same as was\n-                passed to `save_weights`). This can also be a path to a\n-                SavedModel saved from `model.save`.\n-            by_name: Boolean, whether to load weights by name or by topological\n-                order. Only topological loading is supported for weight files in\n-                TensorFlow format.\n+                passed to `save_weights()`). This can also be a path to a\n+                SavedModel or a `.keras` file (v3 saving format) saved\n+                via `model.save()`.\n             skip_mismatch: Boolean, whether to skip loading of layers where\n                 there is a mismatch in the number of weights, or a mismatch in\n-                the shape of the weight (only valid when `by_name=True`).\n+                the shape of the weights.\n+            by_name: Boolean, whether to load weights by name or by topological\n+                order. Only topological loading is supported for weight files in\n+                the `.keras` v3 format or in the TensorFlow SavedModel format.\n             options: Optional `tf.train.CheckpointOptions` object that specifies\n-                options for loading weights.\n-\n-        Returns:\n-            When loading a weight file in TensorFlow format, returns the same\n-            status object as `tf.train.Checkpoint.restore`. When graph building,\n-            restore ops are run automatically as soon as the network is built\n-            (on first call for user-defined classes inheriting from `Model`,\n-            immediately if it is already built).\n-\n-            When loading weights in HDF5 format, returns `None`.\n-\n-        Raises:\n-            ImportError: If `h5py` is not available and the weight file is in\n-              HDF5 format.\n-            ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n-              `False`.\n+                options for loading weights (only valid for a SavedModel file).\n         \"\"\"\n         return saving_api.load_weights(\n             self,\n\n@@ -219,7 +219,9 @@ class Model(training_lib.Model):\n                     \"Load weights is not yet supported with TPUStrategy \"\n                     \"with steps_per_run greater than 1.\"\n                 )\n-        return super().load_weights(filepath, by_name, skip_mismatch)\n+        return super().load_weights(\n+            filepath, by_name=by_name, skip_mismatch=skip_mismatch\n+        )\n \n     @tf.__internal__.tracking.no_automatic_dependency_tracking\n     def compile(\n\n@@ -299,7 +299,7 @@ def save_weights_only(model, filepath):\n     weights_store.close()\n \n \n-def load_weights_only(model, filepath):\n+def load_weights_only(model, filepath, skip_mismatch=False):\n     \"\"\"Load the weights of a model from a filepath (.keras or .weights.h5).\n \n     Note: only supports h5 for now.\n@@ -321,6 +321,7 @@ def load_weights_only(model, filepath):\n         weights_store=weights_store,\n         assets_store=None,\n         inner_path=\"\",\n+        skip_mismatch=skip_mismatch,\n         visited_trackables=set(),\n     )\n     weights_store.close()\n@@ -390,16 +391,45 @@ def _save_state(\n \n \n def _load_state(\n-    trackable, weights_store, assets_store, inner_path, visited_trackables\n+    trackable,\n+    weights_store,\n+    assets_store,\n+    inner_path,\n+    skip_mismatch=False,\n+    visited_trackables=None,\n ):\n-    if id(trackable) in visited_trackables:\n+    if visited_trackables and id(trackable) in visited_trackables:\n         return\n \n     if hasattr(trackable, \"_load_own_variables\") and weights_store:\n+        if skip_mismatch:\n+            try:\n                 trackable._load_own_variables(weights_store.get(inner_path))\n+            except Exception as e:\n+                warnings.warn(\n+                    f\"Could not load weights in object {trackable}. \"\n+                    \"Skipping object. \"\n+                    f\"Exception encountered: {e}\",\n+                    stacklevel=2,\n+                )\n+        else:\n+            trackable._load_own_variables(weights_store.get(inner_path))\n+\n     if hasattr(trackable, \"_load_assets\") and assets_store:\n+        if skip_mismatch:\n+            try:\n+                trackable._load_assets(assets_store.get(inner_path))\n+            except Exception as e:\n+                warnings.warn(\n+                    f\"Could not load assets in object {trackable}. \"\n+                    \"Skipping object. \"\n+                    f\"Exception encountered: {e}\",\n+                    stacklevel=2,\n+                )\n+        else:\n             trackable._load_assets(assets_store.get(inner_path))\n \n+    if visited_trackables is not None:\n         visited_trackables.add(id(trackable))\n \n     # Recursively load states for Keras trackables such as layers/optimizers.\n@@ -410,6 +440,7 @@ def _load_state(\n                 weights_store,\n                 assets_store,\n                 inner_path=tf.io.gfile.join(inner_path, child_attr),\n+                skip_mismatch=skip_mismatch,\n                 visited_trackables=visited_trackables,\n             )\n         elif isinstance(child_obj, (list, dict, tuple, set)):\n@@ -418,6 +449,7 @@ def _load_state(\n                 weights_store,\n                 assets_store,\n                 inner_path=tf.io.gfile.join(inner_path, child_attr),\n+                skip_mismatch=skip_mismatch,\n                 visited_trackables=visited_trackables,\n             )\n \n@@ -447,7 +479,12 @@ def _save_container_state(\n \n \n def _load_container_state(\n-    container, weights_store, assets_store, inner_path, visited_trackables\n+    container,\n+    weights_store,\n+    assets_store,\n+    inner_path,\n+    skip_mismatch,\n+    visited_trackables,\n ):\n     used_names = {}\n     for trackable in container:\n@@ -463,6 +500,7 @@ def _load_container_state(\n                 weights_store,\n                 assets_store,\n                 inner_path=tf.io.gfile.join(inner_path, name),\n+                skip_mismatch=skip_mismatch,\n                 visited_trackables=visited_trackables,\n             )\n \n\n@@ -615,6 +615,66 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n         with self.assertRaises(EOFError):\n             model.save_weights(temp_filepath, overwrite=False)\n \n+    def test_partial_load(self):\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"mymodel.keras\")\n+        original_model = keras.Sequential(\n+            [\n+                keras.Input(shape=(3,)),\n+                keras.layers.Dense(4),\n+                keras.layers.Dense(5),\n+            ]\n+        )\n+        original_model.save(temp_filepath, save_format=\"keras_v3\")\n+\n+        # Test with a model that has a differently shaped layer\n+        new_model = keras.Sequential(\n+            [\n+                keras.Input(shape=(3,)),\n+                keras.layers.Dense(4),\n+                keras.layers.Dense(6),\n+            ]\n+        )\n+        new_layer_kernel_value = new_model.layers[1].kernel.numpy()\n+        with self.assertRaisesRegex(ValueError, \"Shape mismatch\"):\n+            # Doesn't work by default\n+            new_model.load_weights(temp_filepath)\n+        # Now it works\n+        new_model.load_weights(temp_filepath, skip_mismatch=True)\n+        self.assertAllClose(\n+            original_model.layers[0].get_weights(),\n+            new_model.layers[0].get_weights(),\n+        )\n+        self.assertAllClose(\n+            new_model.layers[1].kernel.numpy(), new_layer_kernel_value\n+        )\n+\n+        # Test with a model that has a new layer\n+        new_model = keras.Sequential(\n+            [\n+                keras.Input(shape=(3,)),\n+                keras.layers.Dense(4),\n+                keras.layers.Dense(5),\n+                keras.layers.Dense(5),\n+            ]\n+        )\n+        new_layer_kernel_value = new_model.layers[2].kernel.numpy()\n+        with self.assertRaisesRegex(ValueError, \"received 0 variables\"):\n+            # Doesn't work by default\n+            new_model.load_weights(temp_filepath)\n+        # Now it works\n+        new_model.load_weights(temp_filepath, skip_mismatch=True)\n+        self.assertAllClose(\n+            original_model.layers[0].get_weights(),\n+            new_model.layers[0].get_weights(),\n+        )\n+        self.assertAllClose(\n+            original_model.layers[1].get_weights(),\n+            new_model.layers[1].get_weights(),\n+        )\n+        self.assertAllClose(\n+            new_model.layers[2].kernel.numpy(), new_layer_kernel_value\n+        )\n+\n     def test_api_errors(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"mymodel.notkeras\")\n         model = self._get_functional_model()\n\n@@ -223,13 +223,19 @@ def save_weights(model, filepath, overwrite=True, **kwargs):\n         )\n \n \n-def load_weights(model, filepath, **kwargs):\n+def load_weights(model, filepath, skip_mismatch=False, **kwargs):\n     if str(filepath).endswith(\".keras\") and zipfile.is_zipfile(filepath):\n-        saving_lib.load_weights_only(model, filepath)\n+        saving_lib.load_weights_only(\n+            model, filepath, skip_mismatch=skip_mismatch\n+        )\n     elif str(filepath).endswith(\".weights.h5\"):\n-        saving_lib.load_weights_only(model, filepath)\n+        saving_lib.load_weights_only(\n+            model, filepath, skip_mismatch=skip_mismatch\n+        )\n     else:\n-        return legacy_sm_saving_lib.load_weights(model, filepath, **kwargs)\n+        return legacy_sm_saving_lib.load_weights(\n+            model, filepath, skip_mismatch=skip_mismatch, **kwargs\n+        )\n \n \n def get_save_format(filepath, save_format):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1fa82c44a1e49b7fd6f8abf52bc915b6c7cc7750", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 4399 | Contributors (this commit): 44 | Commits (past 90d): 1 | Contributors (cumulative): 44 | DMM Complexity: None\n\nDIFF:\n@@ -447,15 +447,14 @@ class Sequence:\n     \"\"\"Base object for fitting to a sequence of data, such as a dataset.\n \n     Every `Sequence` must implement the `__getitem__` and the `__len__` methods.\n-    If you want to modify your dataset between epochs you may implement\n-    `on_epoch_end`.\n-    The method `__getitem__` should return a complete batch.\n+    If you want to modify your dataset between epochs, you may implement\n+    `on_epoch_end`. The method `__getitem__` should return a complete batch.\n \n     Notes:\n \n-    `Sequence` are a safer way to do multiprocessing. This structure guarantees\n-    that the network will only train once\n-     on each sample per epoch which is not the case with generators.\n+    `Sequence` is a safer way to do multiprocessing. This structure guarantees\n+    that the network will only train once on each sample per epoch, which is not\n+    the case with generators.\n \n     Examples:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ae7cee57b1c82ab26b1b1aec2a28f9f0d5ede00d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 4407 | Contributors (this commit): 44 | Commits (past 90d): 2 | Contributors (cumulative): 44 | DMM Complexity: None\n\nDIFF:\n@@ -477,10 +477,10 @@ class Sequence:\n             return math.ceil(len(self.x) / self.batch_size)\n \n         def __getitem__(self, idx):\n-            batch_x = self.x[idx * self.batch_size:(idx + 1) *\n-            self.batch_size]\n-            batch_y = self.y[idx * self.batch_size:(idx + 1) *\n-            self.batch_size]\n+            low = idx * self.batch_size\n+            high = (idx + 1) * self.batch_size\n+            batch_x = self.x[low:high]\n+            batch_y = self.y[low:high]\n \n             return np.array([\n                 resize(imread(file_name), (200, 200))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5f943bc65ba93f10786dcb0b03cf27fd4e89e9ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 283 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -132,6 +132,7 @@ def image_dataset_from_directory(\n \n     Returns:\n       A `tf.data.Dataset` object.\n+\n         - If `label_mode` is None, it yields `float32` tensors of shape\n           `(batch_size, image_size[0], image_size[1], num_channels)`,\n           encoding images (see below for rules regarding `num_channels`).\n@@ -140,6 +141,7 @@ def image_dataset_from_directory(\n           and `labels` follows the format described below.\n \n     Rules regarding labels format:\n+\n       - if `label_mode` is `int`, the labels are an `int32` tensor of shape\n         `(batch_size,)`.\n       - if `label_mode` is `binary`, the labels are a `float32` tensor of\n@@ -149,6 +151,7 @@ def image_dataset_from_directory(\n         encoding of the class index.\n \n     Rules regarding number of channels in the yielded images:\n+\n       - if `color_mode` is `grayscale`,\n         there's 1 channel in the image tensors.\n       - if `color_mode` is `rgb`,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5e957b52700ddefc4751191e8f5dd3eb5a9b789b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 53 | Churn Cumulative: 1246 | Contributors (this commit): 12 | Commits (past 90d): 11 | Contributors (cumulative): 16 | DMM Complexity: 0.2127659574468085\n\nDIFF:\n@@ -22,6 +22,7 @@ from keras.distribute import distributed_training_utils\n from keras.engine import base_layer\n from keras.engine import keras_tensor\n from keras.engine import node as node_module\n+from keras.saving import serialization_lib\n from keras.saving.legacy.saved_model import layer_serialization\n from keras.utils import tf_utils\n from keras.utils import traceback_utils\n@@ -201,6 +202,12 @@ class InputLayer(base_layer.Layer):\n                     \"Creating Keras inputs from a type_spec is only \"\n                     \"supported when eager execution is enabled.\"\n                 )\n+            # Needed for type_spec deserialization since TypeSpec objects\n+            # are not Keras-native (not automatically deserialized).\n+            if isinstance(type_spec, dict):\n+                type_spec = serialization_lib.deserialize_keras_object(\n+                    type_spec\n+                )\n             input_tensor = keras_tensor.keras_tensor_from_type_spec(type_spec)\n             if isinstance(input_tensor, keras_tensor.SparseKerasTensor):\n                 self.sparse = True\n\n@@ -133,6 +133,24 @@ def serialize_keras_object(obj):\n                 \"value\": generic_utils.func_dump(obj),\n             },\n         }\n+    if isinstance(obj, tf.TypeSpec):\n+        ts_config = obj._serialize()\n+        # TensorShape and tf.DType conversion\n+        ts_config = list(\n+            map(\n+                lambda x: x.as_list()\n+                if isinstance(x, tf.TensorShape)\n+                else (x.name if isinstance(x, tf.DType) else x),\n+                ts_config,\n+            )\n+        )\n+        return {\n+            \"class_name\": \"__typespec__\",\n+            \"spec_name\": obj.__class__.__name__,\n+            \"module\": obj.__class__.__module__,\n+            \"config\": ts_config,\n+            \"registered_name\": None,\n+        }\n \n     # This gets the `keras.*` exported name, such as \"keras.optimizers.Adam\".\n     keras_api_name = tf_export.get_canonical_name_for_symbol(\n@@ -333,6 +351,23 @@ def deserialize_keras_object(config, custom_objects=None):\n         return inner_config[\"value\"].encode(\"utf-8\")\n     if config[\"class_name\"] == \"__lambda__\":\n         return generic_utils.func_load(inner_config[\"value\"])\n+    if config[\"class_name\"] == \"__typespec__\":\n+        obj = _retrieve_class_or_fn(\n+            config[\"spec_name\"],\n+            config[\"registered_name\"],\n+            config[\"module\"],\n+            obj_type=\"class\",\n+            full_config=config,\n+            custom_objects=custom_objects,\n+        )\n+        # Conversion to TensorShape and tf.DType\n+        inner_config = map(\n+            lambda x: tf.TensorShape(x)\n+            if isinstance(x, list)\n+            else (getattr(tf, x) if hasattr(tf.dtypes, str(x)) else x),\n+            inner_config,\n+        )\n+        return obj._deserialize(tuple(inner_config))\n     # TODO(fchollet): support for TypeSpec, CompositeTensor, tf.Dtype\n     # TODO(fchollet): consider special-casing tuples (which are currently\n     # deserialized as lists).\n\n@@ -180,6 +180,17 @@ class SerializationLibTest(tf.test.TestCase, parameterized.TestCase):\n         y2 = new_lmbda(x)\n         self.assertAllClose(y1, y2, atol=1e-5)\n \n+    def test_tensorspec(self):\n+        inputs = keras.Input(type_spec=tf.TensorSpec((2, 2), tf.float32))\n+        outputs = keras.layers.Dense(1)(inputs)\n+        model = keras.Model(inputs, outputs)\n+        _, new_model, _ = self.roundtrip(model)\n+        x = tf.random.normal((2, 2))\n+        y1 = model(x)\n+        new_model.set_weights(model.get_weights())\n+        y2 = new_model(x)\n+        self.assertAllClose(y1, y2, atol=1e-5)\n+\n     def shared_inner_layer(self):\n         input_1 = keras.Input((2,))\n         input_2 = keras.Input((2,))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4f567d109ad995b7d7d28141be48c3d1846219ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 4411 | Contributors (this commit): 44 | Commits (past 90d): 3 | Contributors (cumulative): 44 | DMM Complexity: None\n\nDIFF:\n@@ -478,7 +478,9 @@ class Sequence:\n \n         def __getitem__(self, idx):\n             low = idx * self.batch_size\n-            high = (idx + 1) * self.batch_size\n+            # Cap upper bound at array length; the last batch may be smaller\n+            # if the total number of items is not a multiple of batch size.\n+            high = min(low + self.batch_size, len(self.x))\n             batch_x = self.x[low:high]\n             batch_y = self.y[low:high]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6d7e23d98e516988e04455d93bde9a66d8e79566", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 7 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 51/51 | Churn Δ: 12 | Churn Cumulative: 28648 | Contributors (this commit): 158 | Commits (past 90d): 57 | Contributors (cumulative): 191 | DMM Complexity: None\n\nDIFF:\n@@ -22,7 +22,7 @@ import tensorflow.compat.v2 as tf\n \n from keras import losses as losses_mod\n from keras import metrics as metrics_mod\n-from keras.saving.experimental import saving_lib\n+from keras.saving import saving_lib\n from keras.utils import generic_utils\n from keras.utils import losses_utils\n from keras.utils import tf_utils\n\n@@ -38,7 +38,7 @@ from keras.optimizers import optimizer\n from keras.optimizers import optimizer_v1\n from keras.saving import pickle_utils\n from keras.saving import saving_api\n-from keras.saving.experimental import saving_lib\n+from keras.saving import saving_lib\n from keras.saving.legacy import serialization\n from keras.saving.legacy.saved_model import json_utils\n from keras.saving.legacy.saved_model import model_serialization\n\n@@ -22,7 +22,7 @@ import functools\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n-from keras.saving.experimental import saving_lib\n+from keras.saving import saving_lib\n from keras.saving.legacy.serialization import deserialize_keras_object\n from keras.saving.legacy.serialization import serialize_keras_object\n from keras.utils import losses_utils\n\n@@ -18,7 +18,7 @@ import tempfile\n \n import tensorflow.compat.v2 as tf\n \n-from keras.saving.experimental import saving_lib\n+from keras.saving import saving_lib\n \n \n def deserialize_model_from_bytecode(serialized_model):\n\n@@ -20,7 +20,7 @@ import zipfile\n import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n-from keras.saving.experimental import saving_lib\n+from keras.saving import saving_lib\n from keras.saving.legacy import save as legacy_sm_saving_lib\n from keras.utils import io_utils\n \n\n\n@@ -28,7 +28,7 @@ import keras\n from keras import backend\n from keras.optimizers import adam\n from keras.saving import object_registration\n-from keras.saving.experimental import saving_lib\n+from keras.saving import saving_lib\n from keras.saving.legacy.saved_model import json_utils\n from keras.testing_infra import test_utils\n from keras.utils import io_utils\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1c21178247e21000d5c720bf139c4a6e925eb8b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 229 | Lines Deleted: 251 | Files Changed: 85 | Hunks: 142 | Methods Changed: 59 | Complexity Δ (Sum/Max): 671/122 | Churn Δ: 480 | Churn Cumulative: 65544 | Contributors (this commit): 64 | Commits (past 90d): 180 | Contributors (cumulative): 564 | DMM Complexity: 0.0\n\nDIFF:\n@@ -17,7 +17,7 @@\n import tensorflow.compat.v2 as tf\n \n from keras.benchmarks import benchmark_util\n-from keras.optimizers.optimizer_v2 import adam\n+from keras.optimizers.legacy import adam\n \n # isort: off\n from tensorflow.python.platform.benchmark import (\n\n@@ -38,8 +38,8 @@ from keras.callbacks import Callback\n from keras.engine import sequential\n from keras.layers import Activation\n from keras.layers import Dense\n-from keras.optimizers import sgd_experimental\n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers import sgd\n+from keras.optimizers.legacy import gradient_descent\n from keras.optimizers.schedules import learning_rate_schedule\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n@@ -429,7 +429,7 @@ class KerasCallbacksTest(test_combinations.TestCase):\n                     raise RuntimeError(\"Interruption\")\n \n         model = keras.Sequential([keras.layers.Dense(10)])\n-        optimizer = sgd_experimental.SGD()\n+        optimizer = sgd.SGD()\n         model.compile(optimizer, loss=\"mse\")\n \n         x = tf.random.uniform((24, 10))\n@@ -506,7 +506,7 @@ class KerasCallbacksTest(test_combinations.TestCase):\n                     )\n \n         model = keras.Sequential([keras.layers.Dense(10)])\n-        optimizer = sgd_experimental.SGD()\n+        optimizer = sgd.SGD()\n         model.compile(optimizer, loss=\"mse\")\n \n         x = tf.random.uniform((24, 10))\n\n@@ -18,7 +18,7 @@ import os\n import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n-from keras.optimizers.optimizer_v2 import adam\n+from keras.optimizers.legacy import adam\n \n \n class TrainingCheckpointTests(tf.test.TestCase, parameterized.TestCase):\n\n@@ -19,9 +19,7 @@ from absl.testing import parameterized\n \n from keras import layers\n from keras.engine import training\n-from keras.optimizers.optimizer_v2 import (\n-    gradient_descent as gradient_descent_keras,\n-)\n+from keras.optimizers.legacy import gradient_descent as gradient_descent_keras\n from keras.testing_infra import test_utils\n \n \n\n@@ -443,7 +443,7 @@ class TestDistributionStrategyDnnCorrectness(\n \n             model = create_model()\n \n-            optimizer = optimizers.adam_v2.Adam()\n+            optimizer = optimizers.adam_legacy.Adam()\n \n         def train_step(inputs):\n             images, labels = inputs\n\n@@ -23,7 +23,7 @@ from absl.testing import parameterized\n import keras\n from keras.distribute import strategy_combinations\n from keras.layers import core\n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import gradient_descent\n \n \n class CustomModel(tf.Module):\n@@ -85,7 +85,7 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n         with distribution.scope():\n             model = _get_model()\n-            optimizer = keras.optimizers.optimizer_v2.rmsprop.RMSprop()\n+            optimizer = keras.optimizers.legacy.rmsprop.RMSprop()\n \n         @tf.function\n         def train_step(replicated_inputs):\n@@ -125,7 +125,7 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n         with distribution.scope():\n             model = get_subclass_model()\n-            optimizer = keras.optimizers.optimizer_v2.rmsprop.RMSprop()\n+            optimizer = keras.optimizers.legacy.rmsprop.RMSprop()\n \n         @tf.function\n         def train_step(iterator):\n@@ -153,7 +153,7 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n         with distribution.scope():\n             model = _get_model()\n-            optimizer = keras.optimizers.optimizer_v2.rmsprop.RMSprop()\n+            optimizer = keras.optimizers.legacy.rmsprop.RMSprop()\n \n         @tf.function\n         def train_step(iterator):\n@@ -187,7 +187,7 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n             y = keras.layers.Flatten()(y)\n             y = keras.layers.Dense(4, name=\"dense\")(y)\n             model = keras.Model(x, y)\n-            optimizer = keras.optimizers.optimizer_v2.rmsprop.RMSprop()\n+            optimizer = keras.optimizers.legacy.rmsprop.RMSprop()\n \n         @tf.function\n         def train_step(iterator):\n@@ -237,7 +237,7 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n         with distribution.scope():\n             model = create_lstm_model()\n-            optimizer = keras.optimizers.optimizer_v2.gradient_descent.SGD()\n+            optimizer = keras.optimizers.legacy.gradient_descent.SGD()\n \n         @tf.function\n         def train_step(input_iterator):\n@@ -281,7 +281,7 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n         with distribution.scope():\n             model = get_model()\n-            optimizer = keras.optimizers.optimizer_v2.gradient_descent.SGD(\n+            optimizer = keras.optimizers.legacy.gradient_descent.SGD(\n                 0.1, momentum=0.01\n             )\n             weights_file = os.path.join(self.get_temp_dir(), \".h5\")\n@@ -350,7 +350,7 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n         with distribution.scope():\n             model = get_model()\n-            optimizer = keras.optimizers.optimizer_v2.gradient_descent.SGD(\n+            optimizer = keras.optimizers.legacy.gradient_descent.SGD(\n                 0.1, momentum=0.01\n             )\n \n@@ -394,7 +394,7 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n         with distribution.scope():\n             model = get_model()\n-            optimizer = keras.optimizers.optimizer_v2.gradient_descent.SGD(\n+            optimizer = keras.optimizers.legacy.gradient_descent.SGD(\n                 0.1, momentum=0.01\n             )\n \n\n@@ -20,7 +20,7 @@ from absl.testing import parameterized\n from keras.distribute import (\n     strategy_combinations as keras_strategy_combinations,\n )\n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import gradient_descent\n \n # isort: off\n from tensorflow.python.distribute import values\n\n@@ -25,7 +25,7 @@ from keras import callbacks as callbacks_lib\n from keras.engine import sequential\n from keras.layers import core as core_layers\n from keras.layers.preprocessing import string_lookup\n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import gradient_descent\n from keras.utils import dataset_creator\n \n # isort: off\n\n@@ -38,9 +38,7 @@ from keras.distribute.strategy_combinations import tpu_strategies\n from keras.engine import base_layer_utils\n from keras.mixed_precision import policy\n from keras.optimizers import optimizer as optimizer_base\n-from keras.optimizers.optimizer_v2 import (\n-    gradient_descent as gradient_descent_keras,\n-)\n+from keras.optimizers.legacy import gradient_descent as gradient_descent_keras\n from keras.testing_infra import test_utils\n from keras.utils import losses_utils\n from keras.utils import np_utils\n@@ -2879,7 +2877,7 @@ class TestDistributionStrategyWithMultipleAddLossAndMetricCalls(\n         with distribution.scope():\n             model = model_fn(input_shape, 10, l1, l2)\n             model.compile(\n-                optimizer=keras.optimizers.adam_v2.Adam(1e-4),\n+                optimizer=keras.optimizers.adam_legacy.Adam(1e-4),\n                 loss=keras.losses.SparseCategoricalCrossentropy(\n                     from_logits=True,\n                     reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n@@ -2947,7 +2945,7 @@ class TestModelCapturesStrategy(tf.test.TestCase, parameterized.TestCase):\n         # Make model with distribution strategy\n         with distribution.scope():\n             model = DeterministicModel(distribution)\n-            optimizer = keras.optimizers.adam_v2.Adam(1e-4)\n+            optimizer = keras.optimizers.adam_legacy.Adam(1e-4)\n \n         # Compile & evaluate the model outside of the distribution strategy\n         # scope\n\n@@ -18,7 +18,7 @@ import tensorflow.compat.v2 as tf\n \n from keras import callbacks\n from keras.distribute import distributed_training_utils_v1\n-from keras.optimizers.optimizer_v2 import adam\n+from keras.optimizers.legacy import adam\n \n \n class DistributedTrainingUtilsTest(tf.test.TestCase):\n\n@@ -26,7 +26,7 @@ from keras import optimizers\n from keras.distribute import distribute_coordinator_utils as dc\n from keras.distribute import distributed_training_utils as dist_utils\n from keras.engine import training_utils_v1\n-from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.legacy import optimizer_v2\n from keras.utils import tf_contextlib\n from keras.utils.mode_keys import ModeKeys\n \n\n@@ -21,9 +21,7 @@ import keras\n from keras import backend\n from keras.distribute import keras_correctness_test_base\n from keras.distribute import strategy_combinations\n-from keras.optimizers.optimizer_v2 import (\n-    gradient_descent as gradient_descent_keras,\n-)\n+from keras.optimizers.legacy import gradient_descent as gradient_descent_keras\n from keras.testing_infra import test_utils\n \n \n\n@@ -19,9 +19,7 @@ import tensorflow.compat.v2 as tf\n \n import keras\n from keras.distribute import keras_correctness_test_base\n-from keras.optimizers.optimizer_v2 import (\n-    gradient_descent as gradient_descent_keras,\n-)\n+from keras.optimizers.legacy import gradient_descent as gradient_descent_keras\n \n \n class DistributionStrategyEmbeddingModelCorrectnessTest(\n\n@@ -19,7 +19,7 @@ import tensorflow.compat.v2 as tf\n \n import keras\n from keras.distribute import keras_correctness_test_base\n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import gradient_descent\n from keras.testing_infra import test_utils\n \n \n\n@@ -19,8 +19,8 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n import keras\n-from keras.optimizers.optimizer_v2 import adam\n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import adam\n+from keras.optimizers.legacy import gradient_descent\n \n \n def get_model():\n\n@@ -20,8 +20,8 @@ from absl.testing import parameterized\n \n from keras.engine import sequential\n from keras.layers import core\n-from keras.optimizers.optimizer_v2 import adagrad\n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import adagrad\n+from keras.optimizers.legacy import gradient_descent\n from keras.premade_models import linear\n from keras.premade_models import wide_deep\n from keras.utils import dataset_creator\n\n@@ -24,9 +24,7 @@ from keras.layers.rnn import gru_v1\n from keras.layers.rnn import lstm\n from keras.layers.rnn import lstm_v1\n from keras.mixed_precision import policy\n-from keras.optimizers.optimizer_v2 import (\n-    gradient_descent as gradient_descent_keras,\n-)\n+from keras.optimizers.legacy import gradient_descent as gradient_descent_keras\n from keras.testing_infra import test_utils\n \n \n\n@@ -19,9 +19,7 @@ import tensorflow.compat.v2 as tf\n \n import keras\n from keras.distribute import keras_correctness_test_base\n-from keras.optimizers.optimizer_v2 import (\n-    gradient_descent as gradient_descent_keras,\n-)\n+from keras.optimizers.legacy import gradient_descent as gradient_descent_keras\n \n \n def strategies_for_stateful_embedding_model():\n\n@@ -23,7 +23,7 @@ from keras.distribute import optimizer_combinations\n from keras.distribute.test_example import batchnorm_example\n from keras.distribute.test_example import minimize_loss_example\n from keras.layers import core\n-from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.legacy import optimizer_v2\n \n VAR_MAP_V1 = {\n     \"GradientDescent\": (\"dense/kernel\", \"dense/bias\"),\n\n@@ -21,7 +21,7 @@ from absl.testing import parameterized\n import keras\n from keras.engine import training as keras_training\n from keras.layers import core as keras_core\n-from keras.optimizers.optimizer_v2 import rmsprop\n+from keras.optimizers.legacy import rmsprop\n from keras.utils import kpl_test_utils\n \n # isort: off\n\n@@ -32,7 +32,7 @@ from keras import metrics as metrics_module\n from keras import models\n from keras.distribute import multi_worker_testing_utils\n from keras.optimizers import optimizer_v1\n-from keras.optimizers.optimizer_v2 import rmsprop\n+from keras.optimizers.legacy import rmsprop\n from keras.utils import kpl_test_utils\n \n \n\n@@ -20,7 +20,7 @@ import unittest\n import tensorflow.compat.v2 as tf\n \n import keras\n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import gradient_descent\n \n # isort: off\n from tensorflow.python.distribute.cluster_resolver import (\n\n@@ -17,16 +17,16 @@\n import tensorflow.compat.v2 as tf\n \n from keras.optimizers import adam as adam_experimental\n-from keras.optimizers.optimizer_v2 import adadelta as adadelta_keras_v2\n-from keras.optimizers.optimizer_v2 import adagrad as adagrad_keras_v2\n-from keras.optimizers.optimizer_v2 import adam as adam_keras_v2\n-from keras.optimizers.optimizer_v2 import adamax as adamax_keras_v2\n-from keras.optimizers.optimizer_v2 import ftrl as ftrl_keras_v2\n-from keras.optimizers.optimizer_v2 import (\n+from keras.optimizers.legacy import adadelta as adadelta_keras_v2\n+from keras.optimizers.legacy import adagrad as adagrad_keras_v2\n+from keras.optimizers.legacy import adam as adam_keras_v2\n+from keras.optimizers.legacy import adamax as adamax_keras_v2\n+from keras.optimizers.legacy import ftrl as ftrl_keras_v2\n+from keras.optimizers.legacy import (\n     gradient_descent as gradient_descent_keras_v2,\n )\n-from keras.optimizers.optimizer_v2 import nadam as nadam_keras_v2\n-from keras.optimizers.optimizer_v2 import rmsprop as rmsprop_keras_v2\n+from keras.optimizers.legacy import nadam as nadam_keras_v2\n+from keras.optimizers.legacy import rmsprop as rmsprop_keras_v2\n \n gradient_descent_optimizer_v1_fn = (\n     tf.__internal__.test.combinations.NamedObject(\n\n@@ -298,7 +298,7 @@ class ShardedVariableTest(tf.test.TestCase, parameterized.TestCase):\n             # keying\n             var = tf.Variable([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name=\"test\")\n \n-        opt = keras.optimizers.optimizer_v2.adam.Adam()\n+        opt = keras.optimizers.legacy.adam.Adam()\n \n         # Run once to trigger apply_gradients to populate optimizer slot\n         # variables.\n@@ -357,7 +357,7 @@ class ShardedVariableTest(tf.test.TestCase, parameterized.TestCase):\n             # keying\n             var = tf.Variable([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name=\"test\")\n \n-        opt = keras.optimizers.optimizer_v2.adam.Adam()\n+        opt = keras.optimizers.legacy.adam.Adam()\n \n         # Run once to trigger apply_gradients to populate optimizer slot\n         # variables.\n@@ -393,7 +393,7 @@ class ShardedVariableTest(tf.test.TestCase, parameterized.TestCase):\n         with strategy2.scope():\n             var = tf.Variable([0.0, 1.0, 2.0, 3.0, 4.0, 5.0], name=\"test\")\n \n-        opt = keras.optimizers.optimizer_v2.adam.Adam()\n+        opt = keras.optimizers.legacy.adam.Adam()\n         # Run once to trigger apply_gradients to populate optimizer slot\n         # variables.\n         strategy2.run(train_step)\n\n@@ -19,7 +19,7 @@ import tensorflow.compat.v2 as tf\n \n import keras\n from keras.distribute import model_collection_base\n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import gradient_descent\n \n _BATCH_SIZE = 10\n \n\n@@ -18,7 +18,7 @@ import tensorflow.compat.v2 as tf\n \n from keras.legacy_tf_layers import core\n from keras.legacy_tf_layers import normalization\n-from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.legacy import optimizer_v2\n \n \n def minimize_loss_example(optimizer, use_bias=False, use_callable_loss=True):\n\n@@ -27,7 +27,7 @@ from keras.engine import input_layer\n from keras.engine import sequential\n from keras.engine import training as training_lib\n from keras.legacy_tf_layers import core as legacy_core\n-from keras.optimizers.optimizer_v2 import rmsprop\n+from keras.optimizers.legacy import rmsprop\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import control_flow_util\n\n@@ -20,7 +20,7 @@ from absl.testing import parameterized\n \n import keras\n from keras.engine import base_layer\n-from keras.optimizers.optimizer_v2 import rmsprop\n+from keras.optimizers.legacy import rmsprop\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n\n@@ -54,7 +54,7 @@ class SimpleBiasTest(test_combinations.TestCase):\n             [test_utils.Bias()], input_shape=(1,)\n         )\n         model.compile(\n-            keras.optimizers.optimizer_v2.gradient_descent.SGD(0.1),\n+            keras.optimizers.legacy.gradient_descent.SGD(0.1),\n             \"mae\",\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n@@ -92,7 +92,7 @@ class MultipleInputTest(test_combinations.TestCase):\n         else:\n             model = multi_input_functional()\n         model.compile(\n-            keras.optimizers.optimizer_v2.gradient_descent.SGD(0.1),\n+            keras.optimizers.legacy.gradient_descent.SGD(0.1),\n             \"mae\",\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n\n@@ -20,7 +20,7 @@ from absl.testing import parameterized\n \n import keras\n from keras import metrics as metrics_module\n-from keras.optimizers.optimizer_v2 import rmsprop\n+from keras.optimizers.legacy import rmsprop\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n\n@@ -26,7 +26,7 @@ from keras import metrics as metrics_module\n from keras.engine import input_layer\n from keras.engine import training\n from keras.engine import training_generator_v1\n-from keras.optimizers.optimizer_v2 import rmsprop\n+from keras.optimizers.legacy import rmsprop\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import data_utils\n\n@@ -36,7 +36,7 @@ from keras.engine import training as training_module\n from keras.engine import training_utils_v1\n from keras.layers.preprocessing import string_lookup\n from keras.mixed_precision import policy\n-from keras.optimizers import optimizer_v2\n+from keras.optimizers import legacy as optimizer_legacy\n from keras.optimizers import rmsprop\n from keras.optimizers import sgd as sgd_experimental\n from keras.testing_infra import test_combinations\n@@ -1200,7 +1200,7 @@ class TrainingTest(test_combinations.TestCase):\n         # while the correct case will drop very quickly.\n         model.compile(\n             loss=\"mse\",\n-            optimizer=optimizer_v2.gradient_descent.SGD(0.24),\n+            optimizer=optimizer_legacy.gradient_descent.SGD(0.24),\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n \n@@ -1504,7 +1504,9 @@ class TrainingTest(test_combinations.TestCase):\n             outputs = layers_module.Dense(1, activation=\"sigmoid\")(inputs)\n             model = training_module.Model(inputs, outputs)\n \n-            model.compile(optimizer_v2.adam.Adam(0.001), \"binary_crossentropy\")\n+            model.compile(\n+                optimizer_legacy.adam.Adam(0.001), \"binary_crossentropy\"\n+            )\n             counter = Counter()\n             model.fit(x, y, callbacks=[counter])\n             self.assertEqual(counter.batches, expected_batches)\n@@ -1512,7 +1514,9 @@ class TrainingTest(test_combinations.TestCase):\n             model = sequential.Sequential(\n                 [layers_module.Dense(1, batch_input_shape=(batch_size, 10))]\n             )\n-            model.compile(optimizer_v2.adam.Adam(0.001), \"binary_crossentropy\")\n+            model.compile(\n+                optimizer_legacy.adam.Adam(0.001), \"binary_crossentropy\"\n+            )\n             counter = Counter()\n             model.fit(x, y, callbacks=[counter])\n             self.assertEqual(counter.batches, expected_batches)\n@@ -1528,7 +1532,7 @@ class TrainingTest(test_combinations.TestCase):\n         inputs = input_layer.Input(batch_size=2, shape=(10,))\n         outputs = layers_module.Dense(1, activation=\"sigmoid\")(inputs)\n         model = training_module.Model(inputs, outputs)\n-        model.compile(optimizer_v2.adam.Adam(0.001), \"binary_crossentropy\")\n+        model.compile(optimizer_legacy.adam.Adam(0.001), \"binary_crossentropy\")\n         with self.assertRaisesRegex(\n             ValueError, \"incompatible with the specified batch size\"\n         ):\n@@ -1930,7 +1934,7 @@ class TrainingTest(test_combinations.TestCase):\n \n     @test_combinations.run_all_keras_modes\n     def test_calling_aggregate_gradient(self):\n-        class _Optimizer(optimizer_v2.gradient_descent.SGD):\n+        class _Optimizer(optimizer_legacy.gradient_descent.SGD):\n             \"\"\"Mock optimizer to check if _aggregate_gradient is called.\"\"\"\n \n             _HAS_AGGREGATE_GRAD = True\n@@ -1992,7 +1996,7 @@ class TrainingTest(test_combinations.TestCase):\n             [DenseWithExtraWeight(4, input_shape=(4,))]\n         )\n         # Test clipping can handle None gradients\n-        opt = optimizer_v2.adam.Adam(clipnorm=1.0, clipvalue=1.0)\n+        opt = optimizer_legacy.adam.Adam(clipnorm=1.0, clipvalue=1.0)\n         model.compile(opt, \"mse\", run_eagerly=test_utils.should_run_eagerly())\n         inputs = np.random.normal(size=(64, 4))\n         targets = np.random.normal(size=(64, 4))\n@@ -2248,7 +2252,7 @@ class TrainingTest(test_combinations.TestCase):\n         model = MyModel([layers_module.Dense(10)])\n         model.custom_metric = CustomMetric(\"my_metric\")\n         initial_result = model.custom_metric.result()\n-        optimizer = optimizer_v2.gradient_descent.SGD()\n+        optimizer = optimizer_legacy.gradient_descent.SGD()\n         model.compile(optimizer, loss=\"mse\", steps_per_execution=10)\n         model.fit(dataset, epochs=2, steps_per_epoch=10, verbose=2)\n         after_fit_result = model.custom_metric.result()\n@@ -2286,7 +2290,7 @@ class TrainingTest(test_combinations.TestCase):\n         model = MyModel(inputs, outputs)\n         model.add_loss(tf.reduce_sum(outputs))\n \n-        optimizer = optimizer_v2.gradient_descent.SGD()\n+        optimizer = optimizer_legacy.gradient_descent.SGD()\n         model.compile(optimizer, loss=\"mse\", steps_per_execution=10)\n         history = model.fit(dataset, epochs=2, steps_per_epoch=10)\n         self.assertLen(history.history[\"loss\"], 2)\n@@ -4265,7 +4269,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n \n         model.compile(\n             loss=\"mae\",\n-            optimizer=optimizer_v2.gradient_descent.SGD(0.1),\n+            optimizer=optimizer_legacy.gradient_descent.SGD(0.1),\n             metrics=[metrics_module.MeanAbsoluteError(name=\"mae_3\")],\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n@@ -4820,7 +4824,7 @@ class ScalarDataModelTest(test_combinations.TestCase):\n \n         model = MyModel()\n         model.compile(\n-            optimizer_v2.gradient_descent.SGD(1e-2),\n+            optimizer_legacy.gradient_descent.SGD(1e-2),\n             loss=\"mse\",\n             metrics=[\"binary_accuracy\"],\n         )\n\n@@ -35,7 +35,7 @@ from keras.engine import training_utils\n from keras.engine import training_utils_v1\n from keras.mixed_precision import loss_scale_optimizer\n from keras.optimizers import optimizer_v1\n-from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.legacy import optimizer_v2\n from keras.saving.legacy import saving_utils\n from keras.saving.legacy.saved_model import model_serialization\n from keras.utils import data_utils\n\n@@ -331,7 +331,7 @@ class TestStatefulLambda(test_combinations.TestCase):\n \n         model = test_utils.get_model_from_layers([layer], input_shape=(10,))\n         model.compile(\n-            keras.optimizers.optimizer_v2.gradient_descent.SGD(0.1),\n+            keras.optimizers.legacy.gradient_descent.SGD(0.1),\n             \"mae\",\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n@@ -437,7 +437,7 @@ class TestStatefulLambda(test_combinations.TestCase):\n \n         model = test_utils.get_model_from_layers([layer], input_shape=(10,))\n         model.compile(\n-            keras.optimizers.optimizer_v2.gradient_descent.SGD(0.1),\n+            keras.optimizers.legacy.gradient_descent.SGD(0.1),\n             \"mae\",\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n\n@@ -23,7 +23,7 @@ from absl.testing import parameterized\n \n import keras\n from keras.layers.locally_connected import locally_connected_utils\n-from keras.optimizers.optimizer_v2 import rmsprop\n+from keras.optimizers.legacy import rmsprop\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n\n@@ -22,7 +22,7 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n import keras\n-from keras.optimizers.optimizer_v2.rmsprop import RMSprop\n+from keras.optimizers.legacy.rmsprop import RMSprop\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n\n@@ -22,7 +22,7 @@ from absl.testing import parameterized\n \n import keras\n from keras.engine import keras_tensor\n-from keras.optimizers.optimizer_v2 import adam\n+from keras.optimizers.legacy import adam\n from keras.saving.legacy import model_config\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n\n@@ -22,16 +22,14 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n from keras.mixed_precision import autocast_variable\n-from keras.optimizers.optimizer_v2 import adadelta\n-from keras.optimizers.optimizer_v2 import adagrad\n-from keras.optimizers.optimizer_v2 import adam\n-from keras.optimizers.optimizer_v2 import adamax\n-from keras.optimizers.optimizer_v2 import ftrl\n-from keras.optimizers.optimizer_v2 import (\n-    gradient_descent as gradient_descent_v2,\n-)\n-from keras.optimizers.optimizer_v2 import nadam\n-from keras.optimizers.optimizer_v2 import rmsprop\n+from keras.optimizers.legacy import adadelta\n+from keras.optimizers.legacy import adagrad\n+from keras.optimizers.legacy import adam\n+from keras.optimizers.legacy import adamax\n+from keras.optimizers.legacy import ftrl\n+from keras.optimizers.legacy import gradient_descent as gradient_descent_v2\n+from keras.optimizers.legacy import nadam\n+from keras.optimizers.legacy import rmsprop\n \n maybe_distribute = tf.__internal__.test.combinations.combine(\n     distribution=[\n\n@@ -27,7 +27,7 @@ from keras.engine import base_layer_utils\n from keras.engine import input_spec\n from keras.mixed_precision import policy\n from keras.mixed_precision import test_util as mp_test_util\n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import gradient_descent\n from keras.testing_infra import test_combinations\n \n \n\n@@ -20,7 +20,7 @@ from keras import backend\n from keras import optimizers\n from keras.optimizers import optimizer\n from keras.optimizers import utils as optimizer_utils\n-from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.legacy import optimizer_v2\n from keras.saving.legacy import serialization\n \n # isort: off\n\n@@ -27,9 +27,9 @@ from keras.mixed_precision import test_util as mp_test_util\n from keras.optimizers import adam as adam_experimental\n from keras.optimizers import optimizer as optimizer_experimental\n from keras.optimizers import sgd as sgd_experimental\n-from keras.optimizers.optimizer_v2 import adam\n-from keras.optimizers.optimizer_v2 import gradient_descent\n-from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.legacy import adam\n+from keras.optimizers.legacy import gradient_descent\n+from keras.optimizers.legacy import optimizer_v2\n from keras.testing_infra import test_combinations\n \n # isort: off\n\n@@ -22,9 +22,7 @@ from keras.mixed_precision import (\n     loss_scale_optimizer as loss_scale_optimizer_v2,\n )\n from keras.mixed_precision import policy\n-from keras.optimizers.optimizer_v2 import (\n-    gradient_descent as gradient_descent_v2,\n-)\n+from keras.optimizers.legacy import gradient_descent as gradient_descent_v2\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n\n@@ -41,7 +41,7 @@ from keras.mixed_precision import loss_scale_optimizer\n from keras.mixed_precision import policy\n from keras.mixed_precision import test_util as mp_test_util\n from keras.optimizers import optimizer_v1\n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import gradient_descent\n from keras.saving import object_registration\n from keras.saving.legacy import save\n from keras.testing_infra import test_combinations\n\n@@ -20,7 +20,7 @@ from absl.testing import parameterized\n from keras.engine import base_layer_utils\n from keras.mixed_precision import device_compatibility_check\n from keras.mixed_precision import policy as mp_policy\n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import gradient_descent\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n\n@@ -320,7 +320,7 @@ class TestModelCloning(test_combinations.TestCase):\n         model = keras.Model(inputs=inputs, outputs=outputs)\n         model.compile(\n             loss=keras.losses.CategoricalCrossentropy(),\n-            optimizer=keras.optimizers.optimizer_v2.rmsprop.RMSprop(lr=0.01),\n+            optimizer=keras.optimizers.legacy.rmsprop.RMSprop(lr=0.01),\n             metrics=[\"accuracy\"],\n         )\n         keras.models.clone_model(model)\n@@ -524,7 +524,7 @@ class TestCloneAndBuildModel(test_combinations.TestCase):\n             model.optimizer,\n             (\n                 optimizer_v1.RMSprop,\n-                keras.optimizers.optimizer_v2.rmsprop.RMSprop,\n+                keras.optimizers.legacy.rmsprop.RMSprop,\n             ),\n         )\n \n@@ -636,7 +636,7 @@ class TestCloneAndBuildModel(test_combinations.TestCase):\n         with tf.Graph().as_default():\n             with self.session():\n                 model = test_utils.get_small_sequential_mlp(3, 4)\n-                optimizer = keras.optimizers.optimizer_v2.adam.Adam()\n+                optimizer = keras.optimizers.legacy.adam.Adam()\n                 model.compile(\n                     optimizer,\n                     \"mse\",\n\n@@ -22,41 +22,39 @@ For more examples see the base class `tf.keras.optimizers.Optimizer`.\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n-from keras.optimizers import adadelta as adadelta_experimental\n+from keras.optimizers import adadelta\n from keras.optimizers import adafactor\n-from keras.optimizers import adagrad as adagrad_experimental\n-from keras.optimizers import adam as adam_experimental\n-from keras.optimizers import adamax as adamax_experimental\n-from keras.optimizers import adamw as adamw_experimental\n-from keras.optimizers import ftrl as ftrl_experimental\n-from keras.optimizers import nadam as nadam_experimental\n+from keras.optimizers import adagrad\n+from keras.optimizers import adam\n+from keras.optimizers import adamax\n+from keras.optimizers import adamw\n+from keras.optimizers import ftrl\n+from keras.optimizers import nadam\n from keras.optimizers import optimizer as base_optimizer\n-from keras.optimizers import rmsprop as rmsprop_experimental\n-from keras.optimizers import sgd as sgd_experimental\n-from keras.optimizers.optimizer_v1 import Optimizer\n-from keras.optimizers.optimizer_v1 import TFOptimizer\n-from keras.optimizers.optimizer_v2 import adadelta as adadelta_v2\n-from keras.optimizers.optimizer_v2 import adagrad as adagrad_v2\n-from keras.optimizers.optimizer_v2 import adam as adam_v2\n-from keras.optimizers.optimizer_v2 import adamax as adamax_v2\n-from keras.optimizers.optimizer_v2 import ftrl as ftrl_v2\n-from keras.optimizers.optimizer_v2 import (\n-    gradient_descent as gradient_descent_v2,\n-)\n-from keras.optimizers.optimizer_v2 import nadam as nadam_v2\n-from keras.optimizers.optimizer_v2 import optimizer_v2 as base_optimizer_v2\n-from keras.optimizers.optimizer_v2 import rmsprop as rmsprop_v2\n-from keras.optimizers.optimizer_v2.adadelta import Adadelta\n-from keras.optimizers.optimizer_v2.adagrad import Adagrad\n-from keras.optimizers.optimizer_v2.adam import Adam\n-from keras.optimizers.optimizer_v2.adamax import Adamax\n-from keras.optimizers.optimizer_v2.ftrl import Ftrl\n+from keras.optimizers import rmsprop\n+from keras.optimizers import sgd\n+from keras.optimizers.legacy import adadelta as adadelta_legacy\n+from keras.optimizers.legacy import adagrad as adagrad_legacy\n+from keras.optimizers.legacy import adam as adam_legacy\n+from keras.optimizers.legacy import adamax as adamax_legacy\n+from keras.optimizers.legacy import ftrl as ftrl_legacy\n+from keras.optimizers.legacy import gradient_descent as gradient_descent_legacy\n+from keras.optimizers.legacy import nadam as nadam_legacy\n+from keras.optimizers.legacy import optimizer_v2 as base_optimizer_legacy\n+from keras.optimizers.legacy import rmsprop as rmsprop_legacy\n+from keras.optimizers.legacy.adadelta import Adadelta\n+from keras.optimizers.legacy.adagrad import Adagrad\n+from keras.optimizers.legacy.adam import Adam\n+from keras.optimizers.legacy.adamax import Adamax\n+from keras.optimizers.legacy.ftrl import Ftrl\n \n # Symbols to be accessed under keras.optimizers. To be replaced with\n # optimizers v2022 when they graduate out of experimental.\n-from keras.optimizers.optimizer_v2.gradient_descent import SGD\n-from keras.optimizers.optimizer_v2.nadam import Nadam\n-from keras.optimizers.optimizer_v2.rmsprop import RMSprop\n+from keras.optimizers.legacy.gradient_descent import SGD\n+from keras.optimizers.legacy.nadam import Nadam\n+from keras.optimizers.legacy.rmsprop import RMSprop\n+from keras.optimizers.optimizer_v1 import Optimizer\n+from keras.optimizers.optimizer_v1 import TFOptimizer\n from keras.optimizers.schedules import learning_rate_schedule\n from keras.saving.legacy.serialization import deserialize_keras_object\n from keras.saving.legacy.serialization import serialize_keras_object\n@@ -118,18 +116,18 @@ def deserialize(config, custom_objects=None, **kwargs):\n         and not use_legacy_optimizer\n     ):\n         all_classes = {\n-            \"adadelta\": adadelta_experimental.Adadelta,\n-            \"adagrad\": adagrad_experimental.Adagrad,\n-            \"adam\": adam_experimental.Adam,\n-            \"adamax\": adamax_experimental.Adamax,\n-            \"experimentaladadelta\": adadelta_experimental.Adadelta,\n-            \"experimentaladagrad\": adagrad_experimental.Adagrad,\n-            \"experimentaladam\": adam_experimental.Adam,\n-            \"experimentalsgd\": sgd_experimental.SGD,\n-            \"nadam\": nadam_experimental.Nadam,\n-            \"rmsprop\": rmsprop_experimental.RMSprop,\n-            \"sgd\": sgd_experimental.SGD,\n-            \"ftrl\": ftrl_experimental.Ftrl,\n+            \"adadelta\": adadelta.Adadelta,\n+            \"adagrad\": adagrad.Adagrad,\n+            \"adam\": adam.Adam,\n+            \"adamax\": adamax.Adamax,\n+            \"experimentaladadelta\": adadelta.Adadelta,\n+            \"experimentaladagrad\": adagrad.Adagrad,\n+            \"experimentaladam\": adam.Adam,\n+            \"experimentalsgd\": sgd.SGD,\n+            \"nadam\": nadam.Nadam,\n+            \"rmsprop\": rmsprop.RMSprop,\n+            \"sgd\": sgd.SGD,\n+            \"ftrl\": ftrl.Ftrl,\n             \"lossscaleoptimizer\": loss_scale_optimizer.LossScaleOptimizerV3,\n             \"lossscaleoptimizerv3\": loss_scale_optimizer.LossScaleOptimizerV3,\n             # LossScaleOptimizerV1 was an old version of LSO that was removed.\n@@ -138,18 +136,18 @@ def deserialize(config, custom_objects=None, **kwargs):\n         }\n     else:\n         all_classes = {\n-            \"adadelta\": adadelta_v2.Adadelta,\n-            \"adagrad\": adagrad_v2.Adagrad,\n-            \"adam\": adam_v2.Adam,\n-            \"adamax\": adamax_v2.Adamax,\n-            \"experimentaladadelta\": adadelta_experimental.Adadelta,\n-            \"experimentaladagrad\": adagrad_experimental.Adagrad,\n-            \"experimentaladam\": adam_experimental.Adam,\n-            \"experimentalsgd\": sgd_experimental.SGD,\n-            \"nadam\": nadam_v2.Nadam,\n-            \"rmsprop\": rmsprop_v2.RMSprop,\n-            \"sgd\": gradient_descent_v2.SGD,\n-            \"ftrl\": ftrl_v2.Ftrl,\n+            \"adadelta\": adadelta_legacy.Adadelta,\n+            \"adagrad\": adagrad_legacy.Adagrad,\n+            \"adam\": adam_legacy.Adam,\n+            \"adamax\": adamax_legacy.Adamax,\n+            \"experimentaladadelta\": adadelta.Adadelta,\n+            \"experimentaladagrad\": adagrad.Adagrad,\n+            \"experimentaladam\": adam.Adam,\n+            \"experimentalsgd\": sgd.SGD,\n+            \"nadam\": nadam_legacy.Nadam,\n+            \"rmsprop\": rmsprop_legacy.RMSprop,\n+            \"sgd\": gradient_descent_legacy.SGD,\n+            \"ftrl\": ftrl_legacy.Ftrl,\n             \"lossscaleoptimizer\": loss_scale_optimizer.LossScaleOptimizer,\n             \"lossscaleoptimizerv3\": loss_scale_optimizer.LossScaleOptimizerV3,\n             # LossScaleOptimizerV1 was an old version of LSO that was removed.\n@@ -236,7 +234,7 @@ def get(identifier, **kwargs):\n         identifier,\n         (\n             Optimizer,\n-            base_optimizer_v2.OptimizerV2,\n+            base_optimizer_legacy.OptimizerV2,\n         ),\n     ):\n         return identifier\n\n\n@@ -18,7 +18,7 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n \n from keras import backend_config\n-from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.legacy import optimizer_v2\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n\n@@ -18,7 +18,7 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n-from keras.optimizers.optimizer_v2 import adadelta\n+from keras.optimizers.legacy import adadelta\n from keras.testing_infra import test_combinations\n \n _DATA_TYPES = [tf.half, tf.float32, tf.float64, tf.complex64, tf.complex128]\n\n@@ -18,7 +18,7 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n \n from keras import backend_config\n-from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.legacy import optimizer_v2\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n\n@@ -20,7 +20,7 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n-from keras.optimizers.optimizer_v2 import adagrad\n+from keras.optimizers.legacy import adagrad\n from keras.optimizers.schedules import learning_rate_schedule\n from keras.testing_infra import test_combinations\n \n\n@@ -17,7 +17,7 @@\n import tensorflow.compat.v2 as tf\n \n from keras import backend_config\n-from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.legacy import optimizer_v2\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n\n@@ -19,7 +19,7 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n from keras.optimizers import optimizer_v1\n-from keras.optimizers.optimizer_v2 import adam\n+from keras.optimizers.legacy import adam\n from keras.optimizers.schedules import learning_rate_schedule\n from keras.testing_infra import test_combinations\n \n\n@@ -17,7 +17,7 @@\n import tensorflow.compat.v2 as tf\n \n from keras import backend_config\n-from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.legacy import optimizer_v2\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n\n@@ -18,7 +18,7 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n-from keras.optimizers.optimizer_v2 import adamax\n+from keras.optimizers.legacy import adamax\n from keras.testing_infra import test_combinations\n \n \n\n@@ -17,7 +17,7 @@\n \n import tensorflow.compat.v2 as tf\n \n-from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.legacy import optimizer_v2\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n\n@@ -17,7 +17,7 @@\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n-from keras.optimizers.optimizer_v2 import ftrl\n+from keras.optimizers.legacy import ftrl\n \n \n class FtrlOptimizerTest(tf.test.TestCase):\n\n@@ -17,7 +17,7 @@\n \n import tensorflow.compat.v2 as tf\n \n-from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.legacy import optimizer_v2\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n@@ -30,7 +30,7 @@ from tensorflow.python.util.tf_export import keras_export\n class SGD(optimizer_v2.OptimizerV2):\n     r\"\"\"Gradient descent (with momentum) optimizer.\n \n-    Update rule for parameter `w` with gradient `g` when `momentum` is 0:\n+    Update rule for parameter `w` with gradient `g` when `momentum=0`:\n \n     ```python\n     w = w - learning_rate * g\n\n@@ -18,7 +18,7 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import gradient_descent\n from keras.optimizers.schedules import learning_rate_schedule\n from keras.testing_infra import test_combinations\n \n\n@@ -17,7 +17,7 @@\n import tensorflow.compat.v2 as tf\n \n from keras import backend_config\n-from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.legacy import optimizer_v2\n from keras.optimizers.schedules import learning_rate_schedule\n \n # isort: off\n\n@@ -17,7 +17,7 @@\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n-from keras.optimizers.optimizer_v2 import nadam\n+from keras.optimizers.legacy import nadam\n \n \n def get_beta_accumulators(opt, dtype):\n\n\n@@ -30,15 +30,15 @@ from keras.engine import training\n from keras.layers import core\n from keras.layers import regularization\n from keras.optimizers import optimizer_v1\n-from keras.optimizers.optimizer_v2 import adadelta\n-from keras.optimizers.optimizer_v2 import adagrad\n-from keras.optimizers.optimizer_v2 import adam\n-from keras.optimizers.optimizer_v2 import adamax\n-from keras.optimizers.optimizer_v2 import ftrl\n-from keras.optimizers.optimizer_v2 import gradient_descent\n-from keras.optimizers.optimizer_v2 import nadam\n-from keras.optimizers.optimizer_v2 import optimizer_v2\n-from keras.optimizers.optimizer_v2 import rmsprop\n+from keras.optimizers.legacy import adadelta\n+from keras.optimizers.legacy import adagrad\n+from keras.optimizers.legacy import adam\n+from keras.optimizers.legacy import adamax\n+from keras.optimizers.legacy import ftrl\n+from keras.optimizers.legacy import gradient_descent\n+from keras.optimizers.legacy import nadam\n+from keras.optimizers.legacy import optimizer_v2\n+from keras.optimizers.legacy import rmsprop\n from keras.optimizers.schedules import learning_rate_schedule\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n\n@@ -18,7 +18,7 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n \n from keras import backend_config\n-from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.legacy import optimizer_v2\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n\n@@ -22,7 +22,7 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n-from keras.optimizers.optimizer_v2 import rmsprop\n+from keras.optimizers.legacy import rmsprop\n from keras.optimizers.schedules import learning_rate_schedule\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n\n@@ -20,12 +20,12 @@ from keras.optimizers import ftrl as ftrl_new\n from keras.optimizers import nadam as nadam_new\n from keras.optimizers import rmsprop as rmsprop_new\n from keras.optimizers import sgd as sgd_new\n-from keras.optimizers.optimizer_v2 import adadelta as adadelta_old\n-from keras.optimizers.optimizer_v2 import adagrad as adagrad_old\n-from keras.optimizers.optimizer_v2 import adam as adam_old\n-from keras.optimizers.optimizer_v2 import ftrl as ftrl_old\n-from keras.optimizers.optimizer_v2 import gradient_descent as sgd_old\n-from keras.optimizers.optimizer_v2 import rmsprop as rmsprop_old\n+from keras.optimizers.legacy import adadelta as adadelta_old\n+from keras.optimizers.legacy import adagrad as adagrad_old\n+from keras.optimizers.legacy import adam as adam_old\n+from keras.optimizers.legacy import ftrl as ftrl_old\n+from keras.optimizers.legacy import gradient_descent as sgd_old\n+from keras.optimizers.legacy import rmsprop as rmsprop_old\n from keras.optimizers.schedules import learning_rate_schedule\n from keras.testing_infra import test_utils\n from keras.utils import losses_utils\n\n@@ -20,7 +20,7 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import gradient_descent\n from keras.optimizers.schedules import learning_rate_schedule\n from keras.testing_infra import test_combinations\n \n\n@@ -24,7 +24,7 @@ from keras.engine import sequential\n from keras.engine import training\n from keras.feature_column import dense_features_v2\n from keras.layers import core\n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import gradient_descent\n from keras.premade_models import linear\n from keras.testing_infra import test_combinations\n \n\n@@ -22,7 +22,7 @@ from keras.engine import sequential\n from keras.engine import training\n from keras.feature_column import dense_features_v2\n from keras.layers import core\n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import gradient_descent\n from keras.premade_models import linear\n from keras.premade_models import wide_deep\n from keras.testing_infra import test_combinations\n\n@@ -24,7 +24,7 @@ from absl.testing import parameterized\n import keras\n from keras import layers\n from keras import losses\n-from keras.optimizers import optimizer_v2\n+from keras.optimizers import legacy as optimizer_legacy\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import losses_utils\n@@ -134,7 +134,7 @@ class LossesSerialization(test_combinations.TestCase):\n         ):\n             model = _get_multi_io_model()\n             model.compile(\n-                optimizer_v2.gradient_descent.SGD(0.1),\n+                optimizer_legacy.gradient_descent.SGD(0.1),\n                 loss=value,\n                 run_eagerly=test_utils.should_run_eagerly(),\n             )\n@@ -171,7 +171,7 @@ class LossesSerialization(test_combinations.TestCase):\n     def test_serializing_model_with_loss_with_custom_objects(self, value):\n         model = _get_multi_io_model()\n         model.compile(\n-            optimizer_v2.gradient_descent.SGD(0.1),\n+            optimizer_legacy.gradient_descent.SGD(0.1),\n             loss=value,\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n\n@@ -24,7 +24,7 @@ from absl.testing import parameterized\n import keras\n from keras import layers\n from keras import metrics\n-from keras.optimizers import optimizer_v2\n+from keras.optimizers import legacy as optimizer_legacy\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import custom_object_scope\n@@ -185,7 +185,7 @@ class MetricsSerialization(test_combinations.TestCase):\n         ):\n             model = _get_multi_io_model()\n             model.compile(\n-                optimizer_v2.gradient_descent.SGD(0.1),\n+                optimizer_legacy.gradient_descent.SGD(0.1),\n                 \"mae\",\n                 metrics=metric_input,\n                 weighted_metrics=weighted_metric_input,\n@@ -234,7 +234,7 @@ class MetricsSerialization(test_combinations.TestCase):\n \n         model = _get_multi_io_model()\n         model.compile(\n-            optimizer_v2.gradient_descent.SGD(0.1),\n+            optimizer_legacy.gradient_descent.SGD(0.1),\n             \"mae\",\n             metrics=metric_input,\n             weighted_metrics=weighted_metric_input,\n\n@@ -532,9 +532,7 @@ class TestWholeModelSaving(test_combinations.TestCase):\n             )\n             model.compile(\n                 loss=keras.losses.MSE,\n-                optimizer=keras.optimizers.optimizer_v2.rmsprop.RMSprop(\n-                    lr=0.0001\n-                ),\n+                optimizer=keras.optimizers.legacy.rmsprop.RMSprop(lr=0.0001),\n                 metrics=[\n                     keras.metrics.categorical_accuracy,\n                     keras.metrics.CategoricalCrossentropy(\n@@ -1011,7 +1009,7 @@ class TestWholeModelSaving(test_combinations.TestCase):\n             model = _make_model()\n             model.compile(\n                 loss=keras.losses.SparseCategoricalCrossentropy(),\n-                optimizer=optimizers.gradient_descent_v2.SGD(),\n+                optimizer=optimizers.gradient_descent_legacy.SGD(),\n                 metrics=[keras.metrics.SparseCategoricalCrossentropy()],\n             )\n             x = np.random.normal(size=(32, 4))\n@@ -1051,9 +1049,7 @@ class TestWholeModelSaving(test_combinations.TestCase):\n             )\n             # Set the model's optimizer but don't compile. This can happen if\n             # the model is trained with a custom training loop.\n-            model.optimizer = keras.optimizers.optimizer_v2.rmsprop.RMSprop(\n-                lr=0.0001\n-            )\n+            model.optimizer = keras.optimizers.legacy.rmsprop.RMSprop(lr=0.0001)\n             if not tf.executing_eagerly():\n                 session.run([v.initializer for v in model.variables])\n             model.save(saved_model_dir, save_format=save_format)\n@@ -1062,7 +1058,7 @@ class TestWholeModelSaving(test_combinations.TestCase):\n                 loaded = keras.models.load_model(saved_model_dir)\n                 self.assertIsInstance(\n                     loaded.optimizer,\n-                    keras.optimizers.optimizer_v2.optimizer_v2.OptimizerV2,\n+                    keras.optimizers.legacy.optimizer_v2.OptimizerV2,\n                 )\n \n     @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n\n@@ -25,7 +25,7 @@ from google.protobuf import message\n from keras import backend\n from keras import regularizers\n from keras.engine import input_spec\n-from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.legacy import optimizer_v2\n from keras.protobuf import saved_metadata_pb2\n from keras.protobuf import versions_pb2\n from keras.saving import object_registration\n\n@@ -161,7 +161,7 @@ def trace_model_call(model, input_signature=None):\n def model_metadata(model, include_optimizer=True, require_config=True):\n     \"\"\"Returns a dictionary containing the model metadata.\"\"\"\n     from keras import __version__ as keras_version\n-    from keras.optimizers.optimizer_v2 import optimizer_v2\n+    from keras.optimizers.legacy import optimizer_v2\n \n     model_config = {\"class_name\": model.__class__.__name__}\n     try:\n\n@@ -23,7 +23,7 @@ import keras\n from keras import backend\n from keras.engine import sequential\n from keras.feature_column import dense_features\n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import gradient_descent\n from keras.saving.legacy import saving_utils\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n\n@@ -29,15 +29,13 @@ from keras import backend\n from keras import layers\n from keras import models\n from keras.engine import base_layer_utils\n-from keras.optimizers.optimizer_v2 import adadelta as adadelta_v2\n-from keras.optimizers.optimizer_v2 import adagrad as adagrad_v2\n-from keras.optimizers.optimizer_v2 import adam as adam_v2\n-from keras.optimizers.optimizer_v2 import adamax as adamax_v2\n-from keras.optimizers.optimizer_v2 import (\n-    gradient_descent as gradient_descent_v2,\n-)\n-from keras.optimizers.optimizer_v2 import nadam as nadam_v2\n-from keras.optimizers.optimizer_v2 import rmsprop as rmsprop_v2\n+from keras.optimizers.legacy import adadelta as adadelta_v2\n+from keras.optimizers.legacy import adagrad as adagrad_v2\n+from keras.optimizers.legacy import adam as adam_v2\n+from keras.optimizers.legacy import adamax as adamax_v2\n+from keras.optimizers.legacy import gradient_descent as gradient_descent_v2\n+from keras.optimizers.legacy import nadam as nadam_v2\n+from keras.optimizers.legacy import rmsprop as rmsprop_v2\n from keras.utils import tf_contextlib\n from keras.utils import tf_inspect\n \n\n@@ -22,7 +22,7 @@ from keras import Model\n from keras import Sequential\n from keras import layers\n from keras import losses\n-from keras.optimizers import optimizer_v2\n+from keras.optimizers import legacy as optimizer_legacy\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n@@ -37,7 +37,7 @@ mae = losses.mean_absolute_error\n \n \n def get_ctl_train_step(model):\n-    optimizer = optimizer_v2.gradient_descent.SGD(0.05)\n+    optimizer = optimizer_legacy.gradient_descent.SGD(0.05)\n \n     def train_step(x, y, w=None):\n         with tf.GradientTape() as tape:\n@@ -73,7 +73,7 @@ class TestAddLossCorrectness(test_combinations.TestCase):\n         model.add_loss(MAE()(targets, outputs))\n         model.add_loss(tf.reduce_mean(mae(targets, outputs)))\n         model.compile(\n-            optimizer_v2.gradient_descent.SGD(0.05),\n+            optimizer_legacy.gradient_descent.SGD(0.05),\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n \n@@ -94,7 +94,7 @@ class TestAddLossCorrectness(test_combinations.TestCase):\n \n         model.add_loss(callable_loss)\n         model.compile(\n-            optimizer_v2.gradient_descent.SGD(0.1),\n+            optimizer_legacy.gradient_descent.SGD(0.1),\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n \n@@ -154,7 +154,7 @@ class TestAddLossCorrectness(test_combinations.TestCase):\n         model.add_loss(MAE()(targets, outputs, sw))\n         model.add_loss(3 * tf.reduce_mean(sw * mae(targets, outputs)))\n         model.compile(\n-            optimizer_v2.gradient_descent.SGD(0.025),\n+            optimizer_legacy.gradient_descent.SGD(0.025),\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n \n@@ -201,7 +201,7 @@ class TestAddLossCorrectness(test_combinations.TestCase):\n         model = MyModel()\n         model.predict([self.x, self.y, self.w])\n         model.compile(\n-            optimizer_v2.gradient_descent.SGD(0.05),\n+            optimizer_legacy.gradient_descent.SGD(0.05),\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n \n@@ -235,7 +235,7 @@ class TestAddLossCorrectness(test_combinations.TestCase):\n         model = Model([inputs, targets, sw], outputs)\n         model.predict([self.x, self.y, self.w])\n         model.compile(\n-            optimizer_v2.gradient_descent.SGD(0.05),\n+            optimizer_legacy.gradient_descent.SGD(0.05),\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n \n\n@@ -59,7 +59,7 @@ class LayerWithTrainingArg(keras.layers.Layer):\n \n \n def add_loss_step(defun):\n-    optimizer = keras.optimizers.optimizer_v2.adam.Adam()\n+    optimizer = keras.optimizers.legacy.adam.Adam()\n     model = test_utils.get_model_from_layers(\n         [LayerWithLosses()], input_shape=(10,)\n     )\n@@ -81,7 +81,7 @@ def add_loss_step(defun):\n \n \n def batch_norm_step(defun):\n-    optimizer = keras.optimizers.optimizer_v2.adadelta.Adadelta()\n+    optimizer = keras.optimizers.legacy.adadelta.Adadelta()\n     model = test_utils.get_model_from_layers(\n         [\n             keras.layers.BatchNormalization(momentum=0.9),\n@@ -108,7 +108,7 @@ def batch_norm_step(defun):\n \n \n def add_metric_step(defun):\n-    optimizer = keras.optimizers.optimizer_v2.rmsprop.RMSprop()\n+    optimizer = keras.optimizers.legacy.rmsprop.RMSprop()\n     model = test_utils.get_model_from_layers(\n         [\n             LayerWithMetrics(),\n\n@@ -68,7 +68,7 @@ class VectorClassificationIntegrationTest(test_combinations.TestCase):\n         )\n         model.compile(\n             loss=\"categorical_crossentropy\",\n-            optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n+            optimizer=keras.optimizers.legacy.adam.Adam(0.005),\n             metrics=[\"acc\"],\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n@@ -113,7 +113,7 @@ class VectorClassificationIntegrationTest(test_combinations.TestCase):\n         model = keras.models.Model(x, y)\n         model.compile(\n             loss=\"categorical_crossentropy\",\n-            optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n+            optimizer=keras.optimizers.legacy.adam.Adam(0.005),\n             metrics=[\"acc\"],\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n@@ -158,7 +158,7 @@ class SequentialIntegrationTest(KerasIntegrationTest):\n         )\n         model.compile(\n             loss=\"categorical_crossentropy\",\n-            optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n+            optimizer=keras.optimizers.legacy.adam.Adam(0.005),\n             metrics=[\"acc\"],\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n@@ -177,7 +177,7 @@ class SequentialIntegrationTest(KerasIntegrationTest):\n \n         model.compile(\n             loss=\"categorical_crossentropy\",\n-            optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n+            optimizer=keras.optimizers.legacy.adam.Adam(0.005),\n             metrics=[\"acc\"],\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n@@ -220,7 +220,7 @@ class TimeseriesClassificationIntegrationTest(test_combinations.TestCase):\n         )\n         model.compile(\n             loss=\"categorical_crossentropy\",\n-            optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n+            optimizer=keras.optimizers.legacy.adam.Adam(0.005),\n             metrics=[\"acc\"],\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n@@ -268,7 +268,7 @@ class TimeseriesClassificationIntegrationTest(test_combinations.TestCase):\n             )\n             model.compile(\n                 loss=\"categorical_crossentropy\",\n-                optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n+                optimizer=keras.optimizers.legacy.adam.Adam(0.005),\n                 metrics=[\"acc\"],\n                 run_eagerly=test_utils.should_run_eagerly(),\n             )\n@@ -314,7 +314,7 @@ class ImageClassificationIntegrationTest(test_combinations.TestCase):\n         )\n         model.compile(\n             loss=\"categorical_crossentropy\",\n-            optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n+            optimizer=keras.optimizers.legacy.adam.Adam(0.005),\n             metrics=[\"acc\"],\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n@@ -368,7 +368,7 @@ class ActivationV2IntegrationTest(test_combinations.TestCase):\n \n         model.compile(\n             loss=\"categorical_crossentropy\",\n-            optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n+            optimizer=keras.optimizers.legacy.adam.Adam(0.005),\n             metrics=[\"accuracy\"],\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n\n@@ -62,9 +62,7 @@ class MemoryCheckerTest(tf.test.TestCase):\n             )\n \n             model.compile(\n-                optimizer=keras.optimizers.optimizer_v2.gradient_descent.SGD(\n-                    lr=0.01\n-                ),\n+                optimizer=keras.optimizers.legacy.gradient_descent.SGD(lr=0.01),\n                 loss=\"mean_squared_error\",\n                 metrics=[\"accuracy\"],\n             )\n\n@@ -19,7 +19,7 @@ import os\n import tensorflow.compat.v2 as tf\n \n from keras.layers import core\n-from keras.optimizers.optimizer_v2 import adam\n+from keras.optimizers.legacy import adam\n \n # isort: off\n from tensorflow.python.framework import (\n\n@@ -19,7 +19,7 @@ import tensorflow.compat.v2 as tf\n \n from keras import layers\n from keras import metrics\n-from keras.optimizers import optimizer_v2\n+from keras.optimizers import legacy as optimizer_legacy\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n@@ -53,7 +53,7 @@ def get_multi_io_temporal_model():\n def get_compiled_multi_io_model_temporal(sample_weight_mode):\n     model = get_multi_io_temporal_model()\n     model.compile(\n-        optimizer=optimizer_v2.gradient_descent.SGD(0.1),\n+        optimizer=optimizer_legacy.gradient_descent.SGD(0.1),\n         loss=\"mae\",\n         metrics=[metrics.MeanAbsoluteError(name=\"mae\")],\n         weighted_metrics=[metrics.MeanAbsoluteError(name=\"mae_2\")],\n\n@@ -24,7 +24,7 @@ from keras.engine import sequential\n from keras.engine import training\n from keras.layers import core\n from keras.layers import reshaping\n-from keras.optimizers.optimizer_v2 import adam\n+from keras.optimizers.legacy import adam\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n\n@@ -17,7 +17,7 @@ import tensorflow.compat.v2 as tf\n \n from keras.engine import training\n from keras.layers import core\n-from keras.optimizers.optimizer_v2 import adam\n+from keras.optimizers.legacy import adam\n \n # isort: off\n from tensorflow.compiler.tests import xla_test\n\n@@ -21,7 +21,7 @@ from keras.distribute import multi_worker_testing_utils\n from keras.engine import data_adapter\n from keras.engine import sequential\n from keras.layers import core as core_layers\n-from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.legacy import gradient_descent\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import dataset_creator\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
