{"custom_id": "keras#bf734b0eece996d568ad20d7140d58039c4817c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 69 | Churn Cumulative: 645 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -473,4 +473,4 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         outputs = layer(inputs)\n \n         expected = tf_keras_layer(inputs)\n-        self.assertAllClose(outputs, expected)\n+        self.assertAllClose(outputs, expected, rtol=1e-5)\n\n@@ -24,7 +24,9 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n             across multiple calls, use as seed an instance\n             of `keras_core.random.SeedGenerator`.\n     \"\"\"\n-    return normal(shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed)\n+    return backend.random.normal(\n+        shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed\n+    )\n \n \n @keras_core_export(\"keras_core.random.uniform\")\n\n@@ -0,0 +1,63 @@\n+import numpy as np\n+from absl.testing import parameterized\n+\n+from keras_core import testing\n+from keras_core.operations import numpy as knp\n+from keras_core.random import random\n+\n+\n+class RandomTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.parameters(\n+        {\"seed\": 10, \"shape\": (5,), \"mean\": 0, \"stddev\": 1},\n+        {\"seed\": 10, \"shape\": (2, 3), \"mean\": 0, \"stddev\": 1},\n+        {\"seed\": 10, \"shape\": (2, 3, 4), \"mean\": 0, \"stddev\": 1},\n+        {\"seed\": 10, \"shape\": (2, 3), \"mean\": 10, \"stddev\": 1},\n+        {\"seed\": 10, \"shape\": (2, 3), \"mean\": 10, \"stddev\": 3},\n+    )\n+    def test_normal(self, seed, shape, mean, stddev):\n+        np.random.seed(seed)\n+        np_res = np.random.normal(loc=mean, scale=stddev, size=shape)\n+        res = random.normal(shape, mean=mean, stddev=stddev, seed=seed)\n+        self.assertEqual(res.shape, shape)\n+        self.assertEqual(res.shape, np_res.shape)\n+\n+    @parameterized.parameters(\n+        {\"seed\": 10, \"shape\": (5,), \"minval\": 0, \"maxval\": 1},\n+        {\"seed\": 10, \"shape\": (2, 3), \"minval\": 0, \"maxval\": 1},\n+        {\"seed\": 10, \"shape\": (2, 3, 4), \"minval\": 0, \"maxval\": 2},\n+        {\"seed\": 10, \"shape\": (2, 3), \"minval\": -1, \"maxval\": 1},\n+        {\"seed\": 10, \"shape\": (2, 3), \"minval\": 1, \"maxval\": 3},\n+    )\n+    def test_uniform(self, seed, shape, minval, maxval):\n+        np.random.seed(seed)\n+        np_res = np.random.uniform(low=minval, high=maxval, size=shape)\n+        res = random.uniform(shape, minval=minval, maxval=maxval, seed=seed)\n+        self.assertEqual(res.shape, shape)\n+        self.assertEqual(res.shape, np_res.shape)\n+        self.assertLessEqual(knp.max(res), maxval)\n+        self.assertGreaterEqual(knp.max(res), minval)\n+\n+    @parameterized.parameters(\n+        {\"seed\": 10, \"shape\": (5,), \"mean\": 0, \"stddev\": 1},\n+        {\"seed\": 10, \"shape\": (2, 3), \"mean\": 0, \"stddev\": 1},\n+        {\"seed\": 10, \"shape\": (2, 3, 4), \"mean\": 0, \"stddev\": 1},\n+        {\"seed\": 10, \"shape\": (2, 3), \"mean\": 10, \"stddev\": 1},\n+        {\"seed\": 10, \"shape\": (2, 3), \"mean\": 10, \"stddev\": 3},\n+    )\n+    def test_truncated_normal(self, seed, shape, mean, stddev):\n+        np.random.seed(seed)\n+        np_res = np.random.normal(loc=mean, scale=stddev, size=shape)\n+        res = random.truncated_normal(\n+            shape, mean=mean, stddev=stddev, seed=seed\n+        )\n+        self.assertEqual(res.shape, shape)\n+        self.assertEqual(res.shape, np_res.shape)\n+        self.assertLessEqual(knp.max(res), mean + 2 * stddev)\n+        self.assertGreaterEqual(knp.max(res), mean - 2 * stddev)\n+\n+    def test_dropout(self):\n+        x = knp.ones((3, 5))\n+        self.assertAllClose(random.dropout(x, rate=0, seed=0), x)\n+        x_res = random.dropout(x, rate=0.8, seed=0)\n+        self.assertGreater(knp.max(x_res), knp.max(x))\n+        self.assertGreater(knp.sum(x_res == 0), 2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0dc71f879d53af2663bfad2076eab812df9e4acd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 298 | Lines Deleted: 121 | Files Changed: 12 | Hunks: 54 | Methods Changed: 33 | Complexity Δ (Sum/Max): 26/35 | Churn Δ: 419 | Churn Cumulative: 9004 | Contributors (this commit): 9 | Commits (past 90d): 113 | Contributors (cumulative): 32 | DMM Complexity: 0.44919786096256686\n\nDIFF:\n@@ -1,5 +1,199 @@\n-def rnn(*args, **kwargs):\n-    raise NotImplementedError\n+from jax import lax\n+from jax import numpy as jnp\n+from tensorflow import nest\n+\n+\n+def rnn(\n+    step_function,\n+    inputs,\n+    initial_states,\n+    go_backwards=False,\n+    mask=None,\n+    constants=None,\n+    unroll=False,\n+    input_length=None,\n+    time_major=False,\n+    zero_output_for_mask=False,\n+    return_all_outputs=True,\n+):\n+    def swap_batch_timestep(input_t):\n+        # Swap the batch and timestep dim for the incoming tensor.\n+        axes = list(range(len(input_t.shape)))\n+        axes[0], axes[1] = 1, 0\n+        return jnp.transpose(input_t, axes)\n+\n+    if not time_major:\n+        inputs = nest.map_structure(swap_batch_timestep, inputs)\n+\n+    flattened_inputs = nest.flatten(inputs)\n+    time_steps = flattened_inputs[0].shape[0]\n+\n+    if mask is not None:\n+        if mask.dtype != \"bool\":\n+            mask = mask.astype(\"bool\")\n+        if len(mask.shape) == 2:\n+            mask = jnp.expand_dims(mask, axis=-1)\n+        if not time_major:\n+            mask = swap_batch_timestep(mask)\n+\n+    if constants is None:\n+        constants = []\n+\n+    def _expand_mask(mask_t, input_t, fixed_dim=1):\n+        if nest.is_nested(mask_t):\n+            raise ValueError(\n+                f\"mask_t is expected to be tensor, but got {mask_t}\"\n+            )\n+        if nest.is_nested(input_t):\n+            raise ValueError(\n+                f\"input_t is expected to be tensor, but got {input_t}\"\n+            )\n+        rank_diff = len(input_t.shape) - len(mask_t.shape)\n+        for _ in range(rank_diff):\n+            mask_t = jnp.expand_dims(mask_t, -1)\n+        multiples = [1] * fixed_dim + input_t.shape[fixed_dim:]\n+        return jnp.tile(mask_t, multiples)\n+\n+    if unroll:\n+        if not time_steps:\n+            raise ValueError(\"Unrolling requires a fixed number of timesteps.\")\n+        states = tuple(initial_states)\n+        successive_states = []\n+        successive_outputs = []\n+\n+        # Process the input tensors. The input tensor need to be split on the\n+        # time_step dim, and reverse if go_backwards is True. In the case of\n+        # nested input, the input is flattened and then transformed\n+        # individually.  The result of this will be a tuple of lists, each of\n+        # the item in tuple is list of the tensor with shape (batch, feature)\n+        def _process_single_input_t(input_t):\n+            input_t = unstack(input_t)  # unstack for time_step dim\n+            if go_backwards:\n+                input_t.reverse()\n+            return input_t\n+\n+        if nest.is_nested(inputs):\n+            processed_input = nest.map_structure(\n+                _process_single_input_t, inputs\n+            )\n+        else:\n+            processed_input = (_process_single_input_t(inputs),)\n+\n+        def _get_input_tensor(time):\n+            inp = [t_[time] for t_ in processed_input]\n+            return nest.pack_sequence_as(inputs, inp)\n+\n+        if mask is not None:\n+            mask_list = unstack(mask)\n+            if go_backwards:\n+                mask_list.reverse()\n+\n+            for i in range(time_steps):\n+                inp = _get_input_tensor(i)\n+                mask_t = mask_list[i]\n+                output, new_states = step_function(\n+                    inp, tuple(states) + tuple(constants)\n+                )\n+                tiled_mask_t = _expand_mask(mask_t, output)\n+\n+                if not successive_outputs:\n+                    prev_output = jnp.zeros_like(output)\n+                else:\n+                    prev_output = successive_outputs[-1]\n+\n+                output = jnp.where(tiled_mask_t, output, prev_output)\n+\n+                flat_states = nest.flatten(states)\n+                flat_new_states = nest.flatten(new_states)\n+                tiled_mask_t = tuple(\n+                    _expand_mask(mask_t, s) for s in flat_states\n+                )\n+                flat_final_states = tuple(\n+                    jnp.where(m, s, ps)\n+                    for m, s, ps in zip(\n+                        tiled_mask_t, flat_new_states, flat_states\n+                    )\n+                )\n+                states = nest.pack_sequence_as(states, flat_final_states)\n+\n+                if return_all_outputs:\n+                    successive_outputs.append(output)\n+                    successive_states.append(states)\n+                else:\n+                    successive_outputs = [output]\n+                    successive_states = [states]\n+            last_output = successive_outputs[-1]\n+            new_states = successive_states[-1]\n+            outputs = jnp.stack(successive_outputs)\n+\n+        else:  # mask is None\n+            for i in range(time_steps):\n+                inp = _get_input_tensor(i)\n+                output, states = step_function(\n+                    inp, tuple(states) + tuple(constants)\n+                )\n+                if return_all_outputs:\n+                    successive_outputs.append(output)\n+                    successive_states.append(states)\n+                else:\n+                    successive_outputs = [output]\n+                    successive_states = [states]\n+            last_output = successive_outputs[-1]\n+            new_states = successive_states[-1]\n+            outputs = jnp.stack(successive_outputs)\n+\n+    else:  # Unroll == False\n+        if mask is not None:\n+\n+            def _step(states, current_input):\n+                current_input, current_mask = current_input\n+                is_masked = jnp.all(jnp.logical_not(current_mask))\n+\n+                output_t, new_states = step_function(current_input, states)\n+                return lax.cond(\n+                    is_masked,\n+                    lambda: states,\n+                    current_input,\n+                    lambda: new_states,\n+                    output_t,\n+                )\n+\n+            scan_xs = (inputs, mask)\n+\n+        else:\n+\n+            def _step(states, current_input):\n+                output_t, new_states = step_function(current_input, states)\n+                return new_states, output_t\n+\n+            scan_xs = inputs\n+\n+        new_states, outputs = lax.scan(\n+            f=_step,\n+            init=initial_states,\n+            xs=scan_xs,\n+            reverse=go_backwards,\n+        )\n+        if go_backwards:\n+            outputs = jnp.flip(outputs, axis=0)\n+        last_output = outputs[-1]\n+\n+    if zero_output_for_mask:\n+        last_output = jnp.where(\n+            _expand_mask(mask_list[-1], last_output),\n+            last_output,\n+            jnp.zeros_like(last_output),\n+        )\n+        outputs = jnp.where(\n+            _expand_mask(mask, outputs, fixed_dim=2),\n+            outputs,\n+            jnp.zeros_like(outputs),\n+        )\n+\n+    if not time_major:\n+        outputs = nest.map_structure(swap_batch_timestep, outputs)\n+\n+    return last_output, outputs, new_states\n \n \n def lstm(*args, **kwargs):\n@@ -8,3 +202,10 @@ def lstm(*args, **kwargs):\n \n def gru(*args, **kwargs):\n     raise NotImplementedError\n+\n+\n+def unstack(x, axis=0):\n+    return [\n+        lax.index_in_dim(x, i, axis, keepdims=False)\n+        for i in range(x.shape[axis])\n+    ]\n\n@@ -119,7 +119,6 @@ class JAXTrainer(base_trainer.Trainer):\n         )\n \n         def _train_step(state, data):\n-            data = data[0]\n             (\n                 trainable_variables,\n                 non_trainable_variables,\n@@ -173,22 +172,19 @@ class JAXTrainer(base_trainer.Trainer):\n \n         def _train_multi_step(state, data):\n             for single_step_data in data:\n-                logs, state = _train_step(state, [single_step_data])\n+                logs, state = _train_step(state, single_step_data)\n             return logs, state\n \n-        if self.steps_per_execution > 1:\n-            _train_function = _train_multi_step\n-        else:\n-            _train_function = _train_step\n-\n         if not self.run_eagerly and self.jit_compile:\n \n             @jax.jit\n             def train_step(state, data):\n-                return _train_function(state, data)\n+                if self.steps_per_execution > 1:\n+                    return _train_multi_step(state, data)\n+                return _train_step(state, data[0])\n \n         else:\n-            train_step = _train_function\n+            train_step = _train_multi_step\n \n         self.stop_training = False\n         callbacks.on_train_begin()\n@@ -346,7 +342,6 @@ class JAXTrainer(base_trainer.Trainer):\n             )\n \n         def _test_step(state, data):\n-            data = data[0]\n             (\n                 trainable_variables,\n                 non_trainable_variables,\n@@ -386,7 +381,6 @@ class JAXTrainer(base_trainer.Trainer):\n             metrics_variables = new_metrics_variables\n \n             state = (\n-                trainable_variables,\n                 non_trainable_variables,\n                 metrics_variables,\n             )\n@@ -394,22 +388,19 @@ class JAXTrainer(base_trainer.Trainer):\n \n         def _test_multi_step(state, data):\n             for single_step_data in data:\n-                logs, state = _test_step(state, [single_step_data])\n+                logs, state = _test_step(state, single_step_data)\n             return logs, state\n \n-        if self.steps_per_execution > 1:\n-            _test_function = _test_multi_step\n-        else:\n-            _test_function = _test_step\n-\n         if not self.run_eagerly and self.jit_compile:\n \n             @jax.jit\n             def test_step(state, data):\n-                return _test_function(state, data)\n+                if self.steps_per_execution > 1:\n+                    return _test_multi_step(state, data)\n+                return _test_step(state, data[0])\n \n         else:\n-            test_step = _test_function\n+            test_step = _test_multi_step\n \n         callbacks.on_test_begin()\n         logs = None\n@@ -430,7 +421,7 @@ class JAXTrainer(base_trainer.Trainer):\n             logs, state = test_step(state, data)\n             # Note that trainable variables are not returned since they're\n             # immutable here.\n-            _, non_trainable_variables, metrics_variables = state\n+            non_trainable_variables, metrics_variables = state\n \n             callbacks.on_test_batch_end(step, logs)\n \n@@ -480,29 +471,17 @@ class JAXTrainer(base_trainer.Trainer):\n                 model=self,\n             )\n \n-        def _predict_step(trainable_variables, non_trainable_variables, data):\n-            return [\n-                self.stateless_call(\n-                    trainable_variables, non_trainable_variables, data[0]\n-                )\n-            ]\n-\n         def _predict_multi_step(\n             trainable_variables, non_trainable_variables, data\n         ):\n-            outputs = []\n-            for single_step_data in data:\n-                outputs += _predict_step(\n+            return [\n+                self.stateless_call(\n                     trainable_variables,\n                     non_trainable_variables,\n-                    [single_step_data],\n+                    single_step_data,\n                 )\n-            return outputs\n-\n-        if self.steps_per_execution > 1:\n-            _predict_function = _predict_multi_step\n-        else:\n-            _predict_function = _predict_step\n+                for single_step_data in data\n+            ]\n \n         if not self.run_eagerly and self.jit_compile:\n \n@@ -510,12 +489,18 @@ class JAXTrainer(base_trainer.Trainer):\n             def predict_step(\n                 trainable_variables, non_trainable_variables, data\n             ):\n-                return _predict_function(\n+                if self.steps_per_execution > 1:\n+                    return _predict_multi_step(\n                         trainable_variables, non_trainable_variables, data\n                     )\n+                return [\n+                    self.stateless_call(\n+                        trainable_variables, non_trainable_variables, data[0]\n+                    )\n+                ]\n \n         else:\n-            predict_step = _predict_function\n+            predict_step = _predict_multi_step\n \n         callbacks.on_predict_begin()\n \n\n@@ -84,12 +84,12 @@ def rnn(\n     if not time_major:\n         inputs = tf.nest.map_structure(swap_batch_timestep, inputs)\n \n-    flatted_inputs = tf.nest.flatten(inputs)\n-    time_steps = flatted_inputs[0].shape[0]\n-    batch = flatted_inputs[0].shape[1]\n-    time_steps_t = tf.shape(flatted_inputs[0])[0]\n+    flattened_inputs = tf.nest.flatten(inputs)\n+    time_steps = flattened_inputs[0].shape[0]\n+    batch = flattened_inputs[0].shape[1]\n+    time_steps_t = tf.shape(flattened_inputs[0])[0]\n \n-    for input_ in flatted_inputs:\n+    for input_ in flattened_inputs:\n         input_.shape.with_rank_at_least(3)\n \n     if mask is not None:\n@@ -236,20 +236,20 @@ def rnn(\n                 size=time_steps_t,\n                 tensor_array_name=f\"input_ta_{i}\",\n             )\n-            for i, inp in enumerate(flatted_inputs)\n+            for i, inp in enumerate(flattened_inputs)\n         )\n         input_ta = tuple(\n             ta.unstack(input_)\n             if not go_backwards\n             else ta.unstack(tf.reverse(input_, [0]))\n-            for ta, input_ in zip(input_ta, flatted_inputs)\n+            for ta, input_ in zip(input_ta, flattened_inputs)\n         )\n \n         # Get the time(0) input and compute the output for that, the output will\n         # be used to determine the dtype of output tensor array. Don't read from\n         # input_ta due to TensorArray clear_after_read default to True.\n         input_time_zero = tf.nest.pack_sequence_as(\n-            inputs, [inp[0] for inp in flatted_inputs]\n+            inputs, [inp[0] for inp in flattened_inputs]\n         )\n         # output_time_zero is used to determine the cell output shape and its\n         # dtype.  the value is discarded.\n\n@@ -115,16 +115,12 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             )\n             return outputs\n \n-        def mutli_step_on_iterator(iterator):\n+        def train_function(iterator):\n+            \"\"\"Runs a training execution with multiple steps.\"\"\"\n             for _ in tf.range(self.steps_per_execution):\n                 outputs = one_step_on_iterator(iterator)\n             return outputs\n \n-        if self.steps_per_execution > 1:\n-            train_function = mutli_step_on_iterator\n-        else:\n-            train_function = one_step_on_iterator\n-\n         if not self.run_eagerly:\n             train_function = tf.function(train_function, reduce_retracing=True)\n \n@@ -157,19 +153,12 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             )\n             return outputs\n \n-        def mutli_step_on_iterator(iterator):\n-            for _ in tf.range(self.steps_per_execution):\n-                outputs = one_step_on_iterator(iterator)\n-            return outputs\n-\n-        if self.steps_per_execution > 1:\n-            test_function = mutli_step_on_iterator\n+        if not self.run_eagerly:\n+            test_function = tf.function(\n+                one_step_on_iterator, reduce_retracing=True\n+            )\n         else:\n             test_function = one_step_on_iterator\n-\n-        if not self.run_eagerly:\n-            test_function = tf.function(test_function, reduce_retracing=True)\n-\n         self.test_function = test_function\n \n     def make_predict_function(self, force=False):\n@@ -197,26 +186,14 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 self.distribute_strategy,\n                 reduction=self.distribute_reduction_method,\n             )\n-            return [outputs]\n-\n-        def mutli_step_on_iterator(iterator):\n-            outputs = []\n-            for _ in tf.range(self.steps_per_execution):\n-                outputs += one_step_on_iterator(iterator)\n             return outputs\n \n-        if self.steps_per_execution > 1:\n-            # TODO(haifengj): Use multi_step_on_iterator.\n-            # predict_function = mutli_step_on_iterator\n-            predict_function = one_step_on_iterator\n-        else:\n-            predict_function = one_step_on_iterator\n-\n         if not self.run_eagerly:\n             predict_function = tf.function(\n-                predict_function, reduce_retracing=True\n+                one_step_on_iterator, reduce_retracing=True\n             )\n-\n+        else:\n+            predict_function = one_step_on_iterator\n         self.predict_function = predict_function\n \n     def fit(\n@@ -437,8 +414,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         with epoch_iterator.catch_stop_iteration():\n             for step, iterator in epoch_iterator.enumerate_epoch():\n                 callbacks.on_predict_batch_begin(step)\n-                multi_batch_outputs = self.predict_function(iterator)\n-                for batch_outputs in multi_batch_outputs:\n+                batch_outputs = self.predict_function(iterator)\n                 if outputs is None:\n                     outputs = tf.nest.map_structure(\n                         lambda batch_output: [batch_output],\n\n@@ -1,5 +1,3 @@\n-import pytest\n-\n from keras_core import backend\n from keras_core import layers\n from keras_core import operations as ops\n@@ -45,10 +43,6 @@ class RNNCellWithDropout(layers.Layer, DropoutRNNCell):\n         return output, [output]\n \n \n-@pytest.mark.skipif(\n-    backend.backend() != \"tensorflow\",\n-    reason=\"Only implemented for TF for now.\",\n-)\n class DropoutRNNCellTest(testing.TestCase):\n     def test_basics(self):\n         self.run_layer_test(\n\n@@ -1,17 +1,11 @@\n import numpy as np\n-import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n from keras_core import initializers\n from keras_core import layers\n from keras_core import testing\n \n \n-@pytest.mark.skipif(\n-    backend.backend() != \"tensorflow\",\n-    reason=\"Only implemented for TF for now.\",\n-)\n class GRUTest(testing.TestCase, parameterized.TestCase):\n     def test_basics(self):\n         self.run_layer_test(\n\n@@ -1,17 +1,11 @@\n import numpy as np\n-import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n from keras_core import initializers\n from keras_core import layers\n from keras_core import testing\n \n \n-@pytest.mark.skipif(\n-    backend.backend() != \"tensorflow\",\n-    reason=\"Only implemented for TF for now.\",\n-)\n class LSTMTest(testing.TestCase, parameterized.TestCase):\n     def test_basics(self):\n         self.run_layer_test(\n\n@@ -68,10 +68,6 @@ class TwoStatesRNNCell(layers.Layer):\n         return output, [output_1, output_2]\n \n \n-@pytest.mark.skipif(\n-    backend.backend() != \"tensorflow\",\n-    reason=\"Only implemented for TF for now.\",\n-)\n class RNNTest(testing.TestCase):\n     def test_basics(self):\n         self.run_layer_test(\n\n@@ -1,16 +1,10 @@\n import numpy as np\n-import pytest\n \n-from keras_core import backend\n from keras_core import initializers\n from keras_core import layers\n from keras_core import testing\n \n \n-@pytest.mark.skipif(\n-    backend.backend() != \"tensorflow\",\n-    reason=\"Only implemented for TF for now.\",\n-)\n class SimpleRNNTest(testing.TestCase):\n     def test_basics(self):\n         self.run_layer_test(\n@@ -58,6 +52,59 @@ class SimpleRNNTest(testing.TestCase):\n             ),\n             output,\n         )\n+        layer = layers.SimpleRNN(\n+            4,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            unroll=True,\n+        )\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.405432, 0.405432, 0.405432, 0.405432],\n+                    [0.73605347, 0.73605347, 0.73605347, 0.73605347],\n+                ]\n+            ),\n+            output,\n+        )\n+\n+        layer = layers.SimpleRNN(\n+            4,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            go_backwards=True,\n+        )\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.11144729, 0.11144729, 0.11144729, 0.11144729],\n+                    [0.5528889, 0.5528889, 0.5528889, 0.5528889],\n+                ]\n+            ),\n+            output,\n+        )\n+        layer = layers.SimpleRNN(\n+            4,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            go_backwards=True,\n+            unroll=True,\n+        )\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.11144729, 0.11144729, 0.11144729, 0.11144729],\n+                    [0.5528889, 0.5528889, 0.5528889, 0.5528889],\n+                ]\n+            ),\n+            output,\n+        )\n \n     def test_statefulness(self):\n         sequence = np.arange(24).reshape((2, 3, 4)).astype(\"float32\")\n\n@@ -1,17 +1,11 @@\n import numpy as np\n-import pytest\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n from keras_core.layers.rnn.rnn_test import OneStateRNNCell\n from keras_core.layers.rnn.rnn_test import TwoStatesRNNCell\n \n \n-@pytest.mark.skipif(\n-    backend.backend() != \"tensorflow\",\n-    reason=\"Only implemented for TF for now.\",\n-)\n class StackedRNNTest(testing.TestCase):\n     def test_basics(self):\n         self.run_layer_test(\n\n@@ -377,7 +377,6 @@ class BinaryIoU(IoU):\n         name=None,\n         dtype=None,\n     ):\n-\n         super().__init__(\n             num_classes=2,\n             target_class_ids=target_class_ids,\n\n@@ -241,7 +241,4 @@ class TestTrainer(testing.TestCase):\n         model_2 = ExampleModel(units=1)\n         model_2.compile(loss=\"mse\", optimizer=\"adam\", steps_per_execution=1)\n         model_2.fit(x=x, y=y, batch_size=16, verbose=0)\n-\n         self.assertAllClose(model.get_weights(), model_2.get_weights())\n-        self.assertAllClose(model.predict(x), model_2.predict(x))\n-        self.assertAllClose(model.evaluate(x, y), model_2.evaluate(x, y))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c4be4608eab2bbe3666a67541998ab71f2457c16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 221 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -152,10 +152,8 @@ def rnn(\n                 output_t, new_states = step_function(current_input, states)\n                 return lax.cond(\n                     is_masked,\n-                    lambda: states,\n-                    current_input,\n-                    lambda: new_states,\n-                    output_t,\n+                    lambda: (states, current_input),\n+                    lambda: (new_states, output_t),\n                 )\n \n             scan_xs = (inputs, mask)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#74b7675be649123cec83fe6db9b49d1276062db2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 162 | Lines Deleted: 39 | Files Changed: 5 | Hunks: 13 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 201 | Churn Cumulative: 5534 | Contributors (this commit): 3 | Commits (past 90d): 31 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -51,7 +51,7 @@ def rnn(\n         rank_diff = len(input_t.shape) - len(mask_t.shape)\n         for _ in range(rank_diff):\n             mask_t = jnp.expand_dims(mask_t, -1)\n-        multiples = [1] * fixed_dim + input_t.shape[fixed_dim:]\n+        multiples = [1] * fixed_dim + list(input_t.shape[fixed_dim:])\n         return jnp.tile(mask_t, multiples)\n \n     if unroll:\n@@ -147,14 +147,26 @@ def rnn(\n \n             def _step(states, current_input):\n                 current_input, current_mask = current_input\n-                is_masked = jnp.all(jnp.logical_not(current_mask))\n+                is_masked = jnp.all(\n+                    jnp.logical_not(current_mask), axis=-1, keepdims=True\n+                )\n \n                 output_t, new_states = step_function(current_input, states)\n-                return lax.cond(\n-                    is_masked,\n-                    lambda: (states, current_input),\n-                    lambda: (new_states, output_t),\n+\n+                if zero_output_for_mask:\n+                    masked_outs = jnp.where(\n+                        is_masked, jnp.zeros_like(output_t), output_t\n                     )\n+                else:\n+                    # Assume the first state is the previous output.\n+                    output_tm1 = states[0]\n+                    masked_outs = jnp.where(is_masked, output_tm1, output_t)\n+\n+                new_states = [\n+                    jnp.where(is_masked, s, ns)\n+                    for s, ns in zip(states, new_states)\n+                ]\n+                return (new_states, masked_outs)\n \n             scan_xs = (inputs, mask)\n \n@@ -176,18 +188,6 @@ def rnn(\n             outputs = jnp.flip(outputs, axis=0)\n         last_output = outputs[-1]\n \n-    if zero_output_for_mask:\n-        last_output = jnp.where(\n-            _expand_mask(mask_list[-1], last_output),\n-            last_output,\n-            jnp.zeros_like(last_output),\n-        )\n-        outputs = jnp.where(\n-            _expand_mask(mask, outputs, fixed_dim=2),\n-            outputs,\n-            jnp.zeros_like(outputs),\n-        )\n-\n     if not time_major:\n         outputs = nest.map_structure(swap_batch_timestep, outputs)\n \n\n@@ -86,7 +86,6 @@ def rnn(\n \n     flattened_inputs = tf.nest.flatten(inputs)\n     time_steps = flattened_inputs[0].shape[0]\n-    batch = flattened_inputs[0].shape[1]\n     time_steps_t = tf.shape(flattened_inputs[0])[0]\n \n     for input_ in flattened_inputs:\n@@ -436,20 +435,6 @@ def rnn(\n         outputs = tf.nest.pack_sequence_as(output_time_zero, outputs)\n         last_output = tf.nest.pack_sequence_as(output_time_zero, last_output)\n \n-    # static shape inference\n-    def set_shape(output_):\n-        if isinstance(output_, tf.Tensor):\n-            shape = output_.shape.as_list()\n-            if return_all_outputs:\n-                shape[0] = time_steps\n-            else:\n-                shape[0] = 1\n-            shape[1] = batch\n-            output_.set_shape(shape)\n-        return output_\n-\n-    outputs = tf.nest.map_structure(set_shape, outputs)\n-\n     if not time_major:\n         outputs = tf.nest.map_structure(swap_batch_timestep, outputs)\n \n\n@@ -188,6 +188,7 @@ class RNN(Layer):\n         go_backwards=False,\n         stateful=False,\n         unroll=False,\n+        zero_output_for_mask=False,\n         **kwargs,\n     ):\n         if isinstance(cell, (list, tuple)):\n@@ -204,12 +205,11 @@ class RNN(Layer):\n                 \"one integer per RNN state). \"\n                 f\"Received: cell={cell}\"\n             )\n+        super().__init__(**kwargs)\n+\n         # If True, the output for masked timestep will be zeros, whereas in the\n         # False case, output from previous timestep is returned for masked\n         # timestep.\n-        zero_output_for_mask = kwargs.pop(\"zero_output_for_mask\", False)\n-        super().__init__(**kwargs)\n-\n         self.zero_output_for_mask = zero_output_for_mask\n         self.cell = cell\n         self.return_sequences = return_sequences\n\n@@ -349,9 +349,9 @@ class SimpleRNN(RNN):\n         )\n         self.input_spec = [InputSpec(ndim=3)]\n \n-    def call(self, inputs, initial_state=None, mask=None, training=None):\n+    def call(self, sequence, initial_state=None, mask=None, training=None):\n         return super().call(\n-            inputs, mask=mask, training=training, initial_state=initial_state\n+            sequence, mask=mask, training=training, initial_state=initial_state\n         )\n \n     @property\n\n@@ -139,4 +139,142 @@ class SimpleRNNTest(testing.TestCase):\n             output,\n         )\n \n-    # TODO: test masking\n+    def test_pass_initial_state(self):\n+        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n+        initial_state = np.arange(8).reshape((2, 4)).astype(\"float32\")\n+        layer = layers.SimpleRNN(\n+            4,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+        )\n+        output = layer(sequence, initial_state=initial_state)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.33621645, 0.33621645, 0.33621645, 0.33621645],\n+                    [0.6262637, 0.6262637, 0.6262637, 0.6262637],\n+                ]\n+            ),\n+            output,\n+        )\n+\n+        layer = layers.SimpleRNN(\n+            4,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            go_backwards=True,\n+        )\n+        output = layer(sequence, initial_state=initial_state)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.07344437, 0.07344437, 0.07344437, 0.07344437],\n+                    [0.43043602, 0.43043602, 0.43043602, 0.43043602],\n+                ]\n+            ),\n+            output,\n+        )\n+\n+    def test_masking(self):\n+        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n+        mask = np.array([[True, True, False, True], [True, False, False, True]])\n+        layer = layers.SimpleRNN(\n+            4,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            unroll=True,\n+        )\n+        output = layer(sequence, mask=mask)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.32951632, 0.32951632, 0.32951632, 0.32951632],\n+                    [0.61799484, 0.61799484, 0.61799484, 0.61799484],\n+                ]\n+            ),\n+            output,\n+        )\n+\n+        layer = layers.SimpleRNN(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            return_sequences=True,\n+        )\n+        output = layer(sequence, mask=mask)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.0599281, 0.0599281],\n+                    [0.15122814, 0.15122814],\n+                    [0.15122814, 0.15122814],\n+                    [0.32394567, 0.32394567],\n+                ],\n+            ),\n+            output[0],\n+        )\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.3969304, 0.3969304],\n+                    [0.3969304, 0.3969304],\n+                    [0.3969304, 0.3969304],\n+                    [0.608085, 0.608085],\n+                ],\n+            ),\n+            output[1],\n+        )\n+\n+        layer = layers.SimpleRNN(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            return_sequences=True,\n+            zero_output_for_mask=True,\n+        )\n+        output = layer(sequence, mask=mask)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.0599281, 0.0599281],\n+                    [0.15122814, 0.15122814],\n+                    [0.0, 0.0],\n+                    [0.32394567, 0.32394567],\n+                ],\n+            ),\n+            output[0],\n+        )\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.3969304, 0.3969304],\n+                    [0.0, 0.0],\n+                    [0.0, 0.0],\n+                    [0.608085, 0.608085],\n+                ],\n+            ),\n+            output[1],\n+        )\n+\n+        layer = layers.SimpleRNN(\n+            4,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            go_backwards=True,\n+        )\n+        output = layer(sequence, mask=mask)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.07376196, 0.07376196, 0.07376196, 0.07376196],\n+                    [0.43645123, 0.43645123, 0.43645123, 0.43645123],\n+                ]\n+            ),\n+            output,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca006ece74cae9fcfbb3a748d527b725b2de5889", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 250 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 254 | Churn Cumulative: 4235 | Contributors (this commit): 3 | Commits (past 90d): 19 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -574,7 +574,12 @@ def _is_sequence_right_padded(mask):\n     max_seq_length = tf.shape(mask)[1]\n     count_of_true = tf.reduce_sum(tf.cast(mask, tf.int32), axis=1)\n     right_padded_mask = tf.sequence_mask(count_of_true, maxlen=max_seq_length)\n-    return tf.reduce_all(tf.equal(mask, right_padded_mask))\n+    return tf.reduce_all(\n+        tf.equal(\n+            tf.cast(mask, dtype=\"bool\"),\n+            tf.cast(right_padded_mask, dtype=\"bool\"),\n+        )\n+    )\n \n \n def _has_fully_masked_sequence(mask):\n@@ -583,7 +588,9 @@ def _has_fully_masked_sequence(mask):\n     # to standard kernel, until the issue on cudnn side has been fixed.  For a\n     # fully masked sequence, it will contain all Falses. To make it easy to\n     # check, we inverse the boolean, check if any of the sequence has all True.\n-    return tf.reduce_any(tf.reduce_all(tf.logical_not(mask), axis=1))\n+    return tf.reduce_any(\n+        tf.reduce_all(tf.logical_not(tf.cast(mask, dtype=\"bool\")), axis=1)\n+    )\n \n \n def _standardize_cudnn_weights(weights, biases, shape, transpose_weights=False):\n\n@@ -165,4 +165,122 @@ class GRUTest(testing.TestCase, parameterized.TestCase):\n             output,\n         )\n \n-    # TODO: test masking\n+    def test_pass_initial_state(self):\n+        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n+        initial_state = np.arange(4).reshape((2, 2)).astype(\"float32\")\n+        layer = layers.GRU(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+        )\n+        output = layer(sequence, initial_state=initial_state)\n+        self.assertAllClose(\n+            np.array([[0.23774096, 0.33508456], [0.83659905, 1.0227708]]),\n+            output,\n+        )\n+\n+        layer = layers.GRU(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            go_backwards=True,\n+        )\n+        output = layer(sequence, initial_state=initial_state)\n+        self.assertAllClose(\n+            np.array([[0.13486053, 0.23261218], [0.78257304, 0.9691353]]),\n+            output,\n+        )\n+\n+    def test_masking(self):\n+        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n+        mask = np.array([[True, True, False, True], [True, False, False, True]])\n+        layer = layers.GRU(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            unroll=True,\n+        )\n+        output = layer(sequence, mask=mask)\n+        self.assertAllClose(\n+            np.array([[0.19393763, 0.19393763], [0.30818558, 0.30818558]]),\n+            output,\n+        )\n+\n+        layer = layers.GRU(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            return_sequences=True,\n+        )\n+        output = layer(sequence, mask=mask)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.03606692, 0.03606692],\n+                    [0.09497581, 0.09497581],\n+                    [0.09497581, 0.09497581],\n+                    [0.19393763, 0.19393763],\n+                ],\n+            ),\n+            output[0],\n+        )\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.16051409, 0.16051409],\n+                    [0.16051409, 0.16051409],\n+                    [0.16051409, 0.16051409],\n+                    [0.30818558, 0.30818558],\n+                ],\n+            ),\n+            output[1],\n+        )\n+\n+        layer = layers.GRU(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            return_sequences=True,\n+            zero_output_for_mask=True,\n+        )\n+        output = layer(sequence, mask=mask)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.03606692, 0.03606692],\n+                    [0.09497581, 0.09497581],\n+                    [0.0, 0.0],\n+                    [0.19393763, 0.19393763],\n+                ],\n+            ),\n+            output[0],\n+        )\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.16051409, 0.16051409],\n+                    [0.0, 0.0],\n+                    [0.0, 0.0],\n+                    [0.30818558, 0.30818558],\n+                ],\n+            ),\n+            output[1],\n+        )\n+\n+        layer = layers.GRU(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            go_backwards=True,\n+        )\n+        output = layer(sequence, mask=mask)\n+        self.assertAllClose(\n+            np.array([[0.11669192, 0.11669192], [0.28380975, 0.28380975]]),\n+            output,\n+        )\n\n@@ -170,4 +170,125 @@ class LSTMTest(testing.TestCase, parameterized.TestCase):\n             output,\n         )\n \n-    # TODO: test masking\n+    def test_pass_initial_state(self):\n+        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n+        initial_state = [\n+            np.arange(4).reshape((2, 2)).astype(\"float32\"),\n+            np.arange(4).reshape((2, 2)).astype(\"float32\"),\n+        ]\n+        layer = layers.LSTM(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+        )\n+        output = layer(sequence, initial_state=initial_state)\n+        self.assertAllClose(\n+            np.array([[0.20574439, 0.3558822], [0.64930826, 0.66276]]),\n+            output,\n+        )\n+\n+        layer = layers.LSTM(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            go_backwards=True,\n+        )\n+        output = layer(sequence, initial_state=initial_state)\n+        self.assertAllClose(\n+            np.array([[0.13281618, 0.2790356], [0.5839337, 0.5992567]]),\n+            output,\n+        )\n+\n+    def test_masking(self):\n+        sequence = np.arange(24).reshape((2, 4, 3)).astype(\"float32\")\n+        mask = np.array([[True, True, False, True], [True, False, False, True]])\n+        layer = layers.LSTM(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            unroll=True,\n+        )\n+        output = layer(sequence, mask=mask)\n+        self.assertAllClose(\n+            np.array([[0.1524914, 0.1524914], [0.35969394, 0.35969394]]),\n+            output,\n+        )\n+\n+        layer = layers.LSTM(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            return_sequences=True,\n+        )\n+        output = layer(sequence, mask=mask)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.0158891, 0.0158891],\n+                    [0.05552047, 0.05552047],\n+                    [0.05552047, 0.05552047],\n+                    [0.1524914, 0.1524914],\n+                ],\n+            ),\n+            output[0],\n+        )\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.14185596, 0.14185596],\n+                    [0.14185596, 0.14185596],\n+                    [0.14185596, 0.14185596],\n+                    [0.35969394, 0.35969394],\n+                ],\n+            ),\n+            output[1],\n+        )\n+\n+        layer = layers.LSTM(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            return_sequences=True,\n+            zero_output_for_mask=True,\n+        )\n+        output = layer(sequence, mask=mask)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.0158891, 0.0158891],\n+                    [0.05552047, 0.05552047],\n+                    [0.0, 0.0],\n+                    [0.1524914, 0.1524914],\n+                ],\n+            ),\n+            output[0],\n+        )\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [0.14185596, 0.14185596],\n+                    [0.0, 0.0],\n+                    [0.0, 0.0],\n+                    [0.35969394, 0.35969394],\n+                ],\n+            ),\n+            output[1],\n+        )\n+\n+        layer = layers.LSTM(\n+            2,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+            go_backwards=True,\n+        )\n+        output = layer(sequence, mask=mask)\n+        self.assertAllClose(\n+            np.array([[0.10056866, 0.10056866], [0.31006062, 0.31006062]]),\n+            output,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
