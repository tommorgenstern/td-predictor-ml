{"custom_id": "keras#6f53d0ab98e0fe0b7d51eb5a14eaa5b04dd91925", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 32 | Files Changed: 1 | Hunks: 26 | Methods Changed: 5 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 74 | Churn Cumulative: 4901 | Contributors (this commit): 29 | Commits (past 90d): 19 | Contributors (cumulative): 29 | DMM Complexity: 0.0\n\nDIFF:\n@@ -260,17 +260,19 @@ class TestBackend(object):\n                                          'squeeze', {'axis': 2},\n                                          (4, 3, 1, 1), WITH_NP)\n \n+    @pytest.mark.skipif(K.backend() != 'theano',\n+                        reason='We only test the shape inference of the '\n+                               'theano backend.')\n     def test_none_shape_operations(self):\n         # Test shape inference when input\n         # shape has `None` entries\n-        if K.backend() == 'theano':\n-            x = KTH.placeholder((3, None, 4))\n+        x = K.placeholder((3, None, 4))\n \n-            y = KTH.batch_flatten(x)\n+        y = K.batch_flatten(x)\n         if hasattr(y, '_keras_shape'):\n             assert y._keras_shape == (3, None)\n \n-            y = KTH.flatten(x)\n+        y = K.flatten(x)\n         if hasattr(y, '_keras_shape'):\n             assert y._keras_shape == (None, )\n \n@@ -417,7 +419,8 @@ class TestBackend(object):\n     def test_log(self):\n         check_single_tensor_operation('log', (4, 2), WITH_NP)\n \n-    # cntk doesn't support gradient in this way\n+    @pytest.mark.skipif(K.backend() == 'cntk',\n+                        reason='cntk doesn\\'t support gradient in this way.')\n     def test_gradient(self):\n         val = np.random.random((4, 2))\n         x_list = [k.variable(val) for k in [KTH, KTF]]\n@@ -447,7 +450,9 @@ class TestBackend(object):\n         c, d = K.stop_gradient([a, b])\n         e = K.stop_gradient(b)\n \n-    # cntk currently not support function in this way, so can't test as this\n+    @pytest.mark.skipif(K.backend() == 'cntk',\n+                        reason='cntk currently not support function in this '\n+                               'way, so can\\'t test as this.')\n     def test_function(self):\n         test_backend = [KTH, KTF]\n         val = np.random.random((4, 2))\n@@ -470,6 +475,8 @@ class TestBackend(object):\n         new_val_list = [k.get_value(x) for x, k in zip(x_list, test_backend)]\n         assert_list_pairwise(new_val_list)\n \n+    @pytest.mark.skipif(K.backend() != 'tensorflow',\n+                        reason='Uses the `fetches` argument.')\n     def test_function_tf_fetches(self):\n         # Additional operations can be passed to tf.Session().run() via its\n         # `fetches` arguments. In contrast to `updates` argument of\n@@ -477,19 +484,21 @@ class TestBackend(object):\n         # they can run in parallel. Also they should not contribute to output of\n         # KTF.function().\n \n-        x = KTF.variable(0.)\n-        y = KTF.variable(0.)\n-        x_placeholder = KTF.placeholder(shape=())\n-        y_placeholder = KTF.placeholder(shape=())\n+        x = K.variable(0.)\n+        y = K.variable(0.)\n+        x_placeholder = K.placeholder(shape=())\n+        y_placeholder = K.placeholder(shape=())\n \n-        f = KTF.function(inputs=[x_placeholder, y_placeholder],\n+        f = K.function(inputs=[x_placeholder, y_placeholder],\n                        outputs=[x_placeholder + y_placeholder],\n                        updates=[(x, x_placeholder + 1.)],\n-                         fetches=[KTF.update(y, 5.)])\n+                       fetches=[K.update(y, 5.)])\n         output = f([10., 20.])\n         assert output == [30.]\n-        assert KTF.get_session().run(fetches=[x, y]) == [11., 5.]\n+        assert K.get_session().run(fetches=[x, y]) == [11., 5.]\n \n+    @pytest.mark.skipif(K.backend() != 'tensorflow',\n+                        reason='Uses the `feed_dict` argument.')\n     def test_function_tf_feed_dict(self):\n         # Additional substitutions can be passed to `tf.Session().run()` via its\n         # `feed_dict` arguments. Note that the feed_dict is passed once in the\n@@ -497,35 +506,37 @@ class TestBackend(object):\n         # this feed_dict we can provide additional substitutions besides Keras\n         # inputs.\n \n-        x = KTF.variable(0.)\n-        y = KTF.variable(0.)\n-        x_placeholder = KTF.placeholder(shape=())\n-        y_placeholder = KTF.placeholder(shape=())\n+        x = K.variable(0.)\n+        y = K.variable(0.)\n+        x_placeholder = K.placeholder(shape=())\n+        y_placeholder = K.placeholder(shape=())\n \n         feed_dict = {y_placeholder: 3.}\n \n-        f = KTF.function(inputs=[x_placeholder],\n+        f = K.function(inputs=[x_placeholder],\n                        outputs=[x_placeholder + 1.],\n                        updates=[(x, x_placeholder + 10.)],\n                        feed_dict=feed_dict,\n-                         fetches=[KTF.update(y, y_placeholder * 10.)])\n+                       fetches=[K.update(y, y_placeholder * 10.)])\n         output = f([10.])\n         assert output == [11.]\n-        assert KTF.get_session().run(fetches=[x, y]) == [20., 30.]\n+        assert K.get_session().run(fetches=[x, y]) == [20., 30.]\n \n         # updated value in feed_dict will be modified within the K.function()\n         feed_dict[y_placeholder] = 4.\n         output = f([20.])\n         assert output == [21.]\n-        assert KTF.get_session().run(fetches=[x, y]) == [30., 40.]\n+        assert K.get_session().run(fetches=[x, y]) == [30., 40.]\n \n+    @pytest.mark.skipif(K.backend() != 'tensorflow',\n+                        reason='Uses the `string` type for a tensor.')\n     def test_function_tf_string_input(self):\n         # Test functions with string inputs.\n \n-        x_placeholder = KTF.placeholder(shape=(), dtype=\"string\")\n-        x_identity = KTF.identity(x_placeholder)\n+        x_placeholder = K.placeholder(shape=(), dtype=\"string\")\n+        x_identity = K.identity(x_placeholder)\n \n-        f = KTF.function(inputs=[x_placeholder], outputs=[x_identity])\n+        f = K.function(inputs=[x_placeholder], outputs=[x_identity])\n         output = f([b'test'])\n         assert output == [b'test']\n \n@@ -1470,10 +1481,9 @@ class TestBackend(object):\n         res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n         assert_allclose(res[0, :] if K.backend() == 'theano' else res[:, 0], ref, atol=1e-05)\n \n-    '''only tensorflow tested, need special handle'''\n-\n+    @pytest.mark.skipif(K.backend() != 'tensorflow',\n+                        reason='Test adapted from tensorflow.')\n     def test_ctc_decode_greedy(self):\n-        # Test adapted from tensorflow\n         \"\"\"Test two batch entries - best path decoder.\"\"\"\n         max_time_steps = 6\n \n@@ -1506,9 +1516,9 @@ class TestBackend(object):\n                   for t in range(max_time_steps)]\n \n         # change tensorflow order to keras backend order\n-        inputs = KTF.variable(np.asarray(inputs).transpose((1, 0, 2)))\n+        inputs = K.variable(np.asarray(inputs).transpose((1, 0, 2)))\n         # batch_size length vector of sequence_lengths\n-        input_length = KTF.variable(np.array([seq_len_0, seq_len_1], dtype=np.int32))\n+        input_length = K.variable(np.array([seq_len_0, seq_len_1], dtype=np.int32))\n \n         # batch_size length vector of negative log probabilities\n         log_prob_truth = np.array([\n@@ -1519,14 +1529,14 @@ class TestBackend(object):\n         # keras output, unlike tensorflow, is a dense (not sparse) tensor\n         decode_truth = np.array([[0, 1, -1], [1, 1, 0]])\n \n-        decode_pred_tf, log_prob_pred_tf = KTF.ctc_decode(inputs,\n+        decode_pred_tf, log_prob_pred_tf = K.ctc_decode(inputs,\n                                                         input_length,\n                                                         greedy=True)\n \n         assert len(decode_pred_tf) == 1\n \n-        decode_pred = KTF.eval(decode_pred_tf[0])\n-        log_prob_pred = KTF.eval(log_prob_pred_tf)\n+        decode_pred = K.eval(decode_pred_tf[0])\n+        log_prob_pred = K.eval(log_prob_pred_tf)\n \n         assert np.alltrue(decode_truth == decode_pred)\n         assert np.allclose(log_prob_truth, log_prob_pred)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#545acc4b66b5fb0ed5457ef7288d836f022c118d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 14 | Methods Changed: 16 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 90 | Churn Cumulative: 5445 | Contributors (this commit): 29 | Commits (past 90d): 27 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -195,12 +195,11 @@ class TestBackend(object):\n             K.set_learning_phase(2)\n \n     def test_eye(self):\n-        z_list = [k.eval(k.eye(3)) for k in WITH_NP]\n-        assert_list_pairwise(z_list)\n+        check_single_tensor_operation('eye', 3, WITH_NP, shape_or_val=False)\n \n     def test_linear_operations(self):\n-        check_two_tensor_operation('dot', (4, 2), (2, 4), BACKENDS)\n-        check_two_tensor_operation('dot', (4, 2), (5, 2, 3), BACKENDS)\n+        check_two_tensor_operation('dot', (4, 2), (2, 4), WITH_NP)\n+        check_two_tensor_operation('dot', (4, 2), (5, 2, 3), WITH_NP)\n \n         check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3),\n                                    BACKENDS, cntk_two_dynamicity=True, axes=(2, 2))\n@@ -213,15 +212,16 @@ class TestBackend(object):\n         check_two_tensor_operation('batch_dot', (32, 20), (32, 20),\n                                    BACKENDS, cntk_two_dynamicity=True, axes=(1, 1))\n \n-        check_single_tensor_operation('transpose', (4, 2), BACKENDS)\n-        check_single_tensor_operation('reverse', (4, 3, 2), BACKENDS, axes=1)\n-        check_single_tensor_operation('reverse', (4, 3, 2), [KTH, KTF], axes=(1, 2))\n+        check_single_tensor_operation('transpose', (4, 2), WITH_NP)\n+        check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=1)\n+        if K.backend() != 'cntk':\n+            check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=(1, 2))\n \n     def test_random_variables(self):\n-        check_single_tensor_operation('random_uniform_variable', (2, 3), BACKENDS,\n+        check_single_tensor_operation('random_uniform_variable', (2, 3), WITH_NP,\n                                       low=0., high=1.,\n                                       shape_or_val=False, assert_value_equality=False)\n-        check_single_tensor_operation('random_normal_variable', (2, 3), BACKENDS,\n+        check_single_tensor_operation('random_normal_variable', (2, 3), WITH_NP,\n                                       mean=0., scale=1.,\n                                       shape_or_val=False, assert_value_equality=False)\n \n@@ -246,7 +246,7 @@ class TestBackend(object):\n                                    axis=-1, concat_args=True)\n \n         check_single_tensor_operation('reshape', (4, 2), WITH_NP, shape=(8, 1))\n-        check_single_tensor_operation('permute_dimensions', (4, 2, 3), BACKENDS,\n+        check_single_tensor_operation('permute_dimensions', (4, 2, 3), WITH_NP,\n                                       pattern=(2, 0, 1))\n         check_single_tensor_operation('repeat', (4, 1), WITH_NP, n=3)\n         check_single_tensor_operation('flatten', (4, 1), WITH_NP)\n@@ -1271,7 +1271,7 @@ class TestBackend(object):\n             elif data_format == 'channels_last':\n                 x_shape = (2,) + shape + (3,)\n             check_single_tensor_operation('resize_images', x_shape,\n-                                          BACKENDS, cntk_dynamicity=True,\n+                                          WITH_NP, cntk_dynamicity=True,\n                                           height_factor=2,\n                                           width_factor=2,\n                                           data_format=data_format)\n@@ -1307,7 +1307,7 @@ class TestBackend(object):\n             elif data_format == 'channels_last':\n                 x_shape = (2,) + shape + (3,)\n             check_single_tensor_operation('resize_volumes', x_shape,\n-                                          BACKENDS, cntk_dynamicity=True,\n+                                          WITH_NP, cntk_dynamicity=True,\n                                           depth_factor=2,\n                                           height_factor=2,\n                                           width_factor=2,\n@@ -1378,7 +1378,7 @@ class TestBackend(object):\n                     x_shape = (1,) + shape + (4,)\n                 bias_shape = (4,)\n                 check_two_tensor_operation('bias_add', x_shape, bias_shape,\n-                                           BACKENDS, cntk_dynamicity=True,\n+                                           WITH_NP, cntk_dynamicity=True,\n                                            data_format=data_format)\n \n             if data_format == 'channels_first':\n@@ -1386,7 +1386,7 @@ class TestBackend(object):\n             else:\n                 x_shape = (20, 10, 6)\n             check_two_tensor_operation('bias_add', x_shape, (10, 6),\n-                                       BACKENDS, cntk_dynamicity=True,\n+                                       WITH_NP, cntk_dynamicity=True,\n                                        data_format=data_format)\n \n         # Test invalid use cases\n\n@@ -142,6 +142,18 @@ pool2d = pool\n pool3d = pool\n \n \n+def bias_add(x, y, data_format):\n+    if data_format == 'channels_first':\n+        if y.ndim > 1:\n+            y = np.reshape(y, y.shape[::-1])\n+        for _ in range(x.ndim - y.ndim - 1):\n+            y = np.expand_dims(y, -1)\n+    else:\n+        for _ in range(x.ndim - y.ndim - 1):\n+            y = np.expand_dims(y, 0)\n+    return x + y\n+\n+\n def rnn(x, w, init, go_backwards=False, mask=None, unroll=False, input_length=None):\n     w_i, w_h, w_o = w\n     h = []\n@@ -334,6 +346,10 @@ def concatenate(tensors, axis=-1):\n     return np.concatenate(tensors, axis)\n \n \n+def permute_dimensions(x, pattern):\n+    return np.transpose(x, pattern)\n+\n+\n def reshape(x, shape):\n     return np.reshape(x, shape)\n \n@@ -387,6 +403,22 @@ def eye(size, dtype=None, name=None):\n     return np.eye(size, dtype=dtype)\n \n \n+def dot(x, y):\n+    return np.dot(x, y)\n+\n+\n+def transpose(x):\n+    return np.transpose(x)\n+\n+\n+def reverse(x, axes):\n+    if isinstance(axes, int):\n+        axes = [axes]\n+    for a in axes:\n+        x = np.flip(x, a)\n+    return x\n+\n+\n def variable(value, dtype=None, name=None, constraint=None):\n     if constraint is not None:\n         raise TypeError(\"Constraint must be None when \"\n@@ -426,6 +458,36 @@ def minimum(x, y):\n     return np.minimum(x, y)\n \n \n+def random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None):\n+    return (high - low) * np.random.random(shape).astype(dtype) + low\n+\n+\n+def random_normal_variable(shape, mean, scale, dtype=None, name=None, seed=None):\n+    return scale * np.random.randn(*shape).astype(dtype) + mean\n+\n+\n+def resize_images(x, height_factor, width_factor, data_format):\n+    if data_format == 'channels_first':\n+        x = repeat_elements(x, height_factor, axis=2)\n+        x = repeat_elements(x, width_factor, axis=3)\n+    elif data_format == 'channels_last':\n+        x = repeat_elements(x, height_factor, axis=1)\n+        x = repeat_elements(x, width_factor, axis=2)\n+    return x\n+\n+\n+def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n+    if data_format == 'channels_first':\n+        x = repeat_elements(x, depth_factor, axis=2)\n+        x = repeat_elements(x, height_factor, axis=3)\n+        x = repeat_elements(x, width_factor, axis=4)\n+    elif data_format == 'channels_last':\n+        x = repeat_elements(x, depth_factor, axis=1)\n+        x = repeat_elements(x, height_factor, axis=2)\n+        x = repeat_elements(x, width_factor, axis=3)\n+    return x\n+\n+\n square = np.square\n abs = np.abs\n exp = np.exp\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f899d0fb336cce4061093a575a573c4f897106e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 21 | Files Changed: 2 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 13569 | Contributors (this commit): 73 | Commits (past 90d): 21 | Contributors (cumulative): 80 | DMM Complexity: None\n\nDIFF:\n@@ -1424,9 +1424,6 @@ class SeparableConv1D(_SeparableConv):\n             `(batch, steps, channels)` while `\"channels_first\"`\n             corresponds to inputs with shape\n             `(batch, channels, steps)`.\n-            It defaults to the `image_data_format` value found in your\n-            Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"channels_last\".\n         dilation_rate: An integer or tuple/list of a single integer, specifying\n             the dilation rate to use for dilated convolution.\n             Currently, specifying any `dilation_rate` value != 1 is\n@@ -1488,7 +1485,7 @@ class SeparableConv1D(_SeparableConv):\n                  kernel_size,\n                  strides=1,\n                  padding='valid',\n-                 data_format=None,\n+                 data_format='channels_last',\n                  dilation_rate=1,\n                  depth_multiplier=1,\n                  activation=None,\n\n@@ -17,7 +17,7 @@ class _Pooling1D(Layer):\n     \"\"\"\n \n     def __init__(self, pool_size=2, strides=None,\n-                 padding='valid', data_format=None, **kwargs):\n+                 padding='valid', data_format='channels_last', **kwargs):\n         super(_Pooling1D, self).__init__(**kwargs)\n         if strides is None:\n             strides = pool_size\n@@ -82,9 +82,6 @@ class MaxPooling1D(_Pooling1D):\n             `(batch, steps, features)` while `channels_first`\n             corresponds to inputs with shape\n             `(batch, features, steps)`.\n-            It defaults to the `image_data_format` value found in your\n-            Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"channels_last\".\n \n     # Input shape\n         - If `data_format='channels_last'`:\n@@ -105,7 +102,7 @@ class MaxPooling1D(_Pooling1D):\n \n     @interfaces.legacy_pooling1d_support\n     def __init__(self, pool_size=2, strides=None,\n-                 padding='valid', data_format=None, **kwargs):\n+                 padding='valid', data_format='channels_last', **kwargs):\n         super(MaxPooling1D, self).__init__(pool_size, strides,\n                                            padding, data_format,\n                                            **kwargs)\n@@ -133,9 +130,6 @@ class AveragePooling1D(_Pooling1D):\n             `(batch, steps, features)` while `channels_first`\n             corresponds to inputs with shape\n             `(batch, features, steps)`.\n-            It defaults to the `image_data_format` value found in your\n-            Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"channels_last\".\n \n     # Input shape\n         - If `data_format='channels_last'`:\n@@ -156,7 +150,7 @@ class AveragePooling1D(_Pooling1D):\n \n     @interfaces.legacy_pooling1d_support\n     def __init__(self, pool_size=2, strides=None,\n-                 padding='valid', data_format=None, **kwargs):\n+                 padding='valid', data_format='channels_last', **kwargs):\n         super(AveragePooling1D, self).__init__(pool_size, strides,\n                                                padding, data_format,\n                                                **kwargs)\n@@ -494,7 +488,7 @@ class _GlobalPooling1D(Layer):\n     \"\"\"Abstract class for different global pooling 1D layers.\n     \"\"\"\n \n-    def __init__(self, data_format=None, **kwargs):\n+    def __init__(self, data_format='channels_last', **kwargs):\n         super(_GlobalPooling1D, self).__init__(**kwargs)\n         self.input_spec = InputSpec(ndim=3)\n         self.data_format = K.normalize_data_format(data_format)\n@@ -525,9 +519,6 @@ class GlobalAveragePooling1D(_GlobalPooling1D):\n             `(batch, steps, features)` while `channels_first`\n             corresponds to inputs with shape\n             `(batch, features, steps)`.\n-            It defaults to the `image_data_format` value found in your\n-            Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"channels_last\".\n \n     # Input shape\n         - If `data_format='channels_last'`:\n@@ -542,7 +533,7 @@ class GlobalAveragePooling1D(_GlobalPooling1D):\n         `(batch_size, features)`\n     \"\"\"\n \n-    def __init__(self, data_format=None, **kwargs):\n+    def __init__(self, data_format='channels_last', **kwargs):\n         super(GlobalAveragePooling1D, self).__init__(data_format,\n                                                      **kwargs)\n         self.supports_masking = True\n@@ -574,9 +565,6 @@ class GlobalMaxPooling1D(_GlobalPooling1D):\n             `(batch, steps, features)` while `channels_first`\n             corresponds to inputs with shape\n             `(batch, features, steps)`.\n-            It defaults to the `image_data_format` value found in your\n-            Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"channels_last\".\n \n     # Input shape\n         - If `data_format='channels_last'`:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1915f106fe94a65cab3887c5842cf520f8adedf3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 5 | Methods Changed: 8 | Complexity Δ (Sum/Max): 7/9 | Churn Δ: 48 | Churn Cumulative: 5493 | Contributors (this commit): 29 | Commits (past 90d): 29 | Contributors (cumulative): 31 | DMM Complexity: 0.2777777777777778\n\nDIFF:\n@@ -1741,18 +1741,18 @@ class TestBackend(object):\n             t = k.arange(start)\n             assert len(k.eval(t)) == 0\n \n-    def test_in_train_phase(self):\n-        for training in [True, False]:\n-            check_two_tensor_operation('in_train_phase', (3, 3), (2, 2), [KTH, KTF],\n+    @pytest.mark.parametrize('training', [True, False])\n+    def test_in_train_phase(self, training):\n+        check_two_tensor_operation('in_train_phase', (3, 3), (2, 2), WITH_NP,\n                                    training=training)\n-            check_two_tensor_operation('in_train_phase', (2, 3), (2, 3), BACKENDS,\n+        check_two_tensor_operation('in_train_phase', (2, 3), (2, 3), WITH_NP,\n                                    training=training)\n \n-    def test_in_test_phase(self):\n-        for training in [True, False]:\n-            check_two_tensor_operation('in_test_phase', (3, 3), (2, 2), [KTH, KTF],\n+    @pytest.mark.parametrize('training', [True, False])\n+    def test_in_test_phase(self, training):\n+        check_two_tensor_operation('in_test_phase', (3, 3), (2, 2), WITH_NP,\n                                    training=training)\n-            check_two_tensor_operation('in_test_phase', (2, 3), (2, 3), BACKENDS,\n+        check_two_tensor_operation('in_test_phase', (2, 3), (2, 3), WITH_NP,\n                                    training=training)\n \n     def test_setfloatx_incorrect_values(self):\n\n@@ -193,6 +193,38 @@ def rnn(x, w, init, go_backwards=False, mask=None, unroll=False, input_length=No\n     return o[-1], np.stack(o, axis=1), np.stack(h, axis=1)\n \n \n+_LEARNING_PHASE = True\n+\n+\n+def learning_phase():\n+    return _LEARNING_PHASE\n+\n+\n+def set_learning_phase(value):\n+    global _LEARNING_PHASE\n+    _LEARNING_PHASE = value\n+\n+\n+def in_train_phase(x, alt, training=None):\n+    if training is None:\n+        training = learning_phase()\n+\n+    if training is 1 or training is True:\n+        if callable(x):\n+            return x()\n+        else:\n+            return x\n+    else:\n+        if callable(alt):\n+            return alt()\n+        else:\n+            return alt\n+\n+\n+def in_test_phase(x, alt, training=None):\n+    return in_train_phase(alt, x, training=training)\n+\n+\n def relu(x, alpha=0., max_value=None):\n     y = x * (x > 0) + alpha * x * (x < 0)\n     if max_value is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f9210387088fe91b5bc8999cf0cb41a0fe9eacf6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 11 | Churn Cumulative: 5504 | Contributors (this commit): 29 | Commits (past 90d): 31 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -885,7 +885,7 @@ class TestBackend(object):\n         # scalar\n         val = np.random.random()\n         z_list = []\n-        for k in BACKENDS:\n+        for k in WITH_NP:\n             x = k.variable(val)\n             x = k.switch(k.greater_equal(x, 0.5), x * 0.1, x * 0.2)\n             z_list.append(k.eval(x))\n@@ -898,7 +898,7 @@ class TestBackend(object):\n         for s in shapes:\n             z_list = []\n             arrays = list(map(np.random.random, s))\n-            for k in BACKENDS:\n+            for k in WITH_NP:\n                 x, then_expr, else_expr = map(k.variable, arrays)\n                 cond = k.greater_equal(x, 0.5)\n                 z_list.append(k.eval(k.switch(cond, then_expr, else_expr)))\n\n@@ -232,6 +232,13 @@ def relu(x, alpha=0., max_value=None):\n     return y\n \n \n+def switch(condition, then_expression, else_expression):\n+    cond_float = condition.astype(floatx())\n+    while cond_float.ndim < then_expression.ndim:\n+        cond_float = cond_float[..., None]\n+    return cond_float * then_expression + (1 - cond_float) * else_expression\n+\n+\n def softplus(x):\n     return np.log(1. + np.exp(x))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#66f8cc7ac4942f7f9fe0164a2a854a6264b87735", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 108 | Lines Deleted: 18 | Files Changed: 3 | Hunks: 18 | Methods Changed: 17 | Complexity Δ (Sum/Max): 15/10 | Churn Δ: 126 | Churn Cumulative: 14070 | Contributors (this commit): 76 | Commits (past 90d): 16 | Contributors (cumulative): 99 | DMM Complexity: 0.8857142857142857\n\nDIFF:\n@@ -1439,7 +1439,7 @@ def rnn(step_function, inputs, initial_states,\n             for o, p in zip(new_states, place_holders):\n                 n_s.append(o.replace_placeholders({p: o.output}))\n             if len(n_s) > 0:\n-                new_output = n_s[0]\n+                new_output = n_s[-1]\n             return new_output, n_s\n \n         final_output, final_states = _recurrence(rnn_inputs, states, mask)\n\n@@ -54,35 +54,55 @@ class StackedRNNCells(Layer):\n                                  '`state_size` attribute. '\n                                  'received cells:', cells)\n         self.cells = cells\n+        # reverse_state_order determines whether the state size will be in a\n+        # reverse order of the cells' state. User might want to set this to True\n+        # to keep the existing behavior. This is only useful when use\n+        # `RNN(return_state=True)` since the state will be returned as the same\n+        # order of state_size.\n+        self.reverse_state_order = kwargs.pop('reverse_state_order', False)\n+        if self.reverse_state_order:\n+            warnings.warn('`reverse_state_order=True` in `StackedRNNCells` '\n+                          'will soon be deprecated. Please update the code to '\n+                          'work with the natural order of states if you '\n+                          'reply on the RNN states, '\n+                          'eg `RNN(return_state=True)`.')\n         super(StackedRNNCells, self).__init__(**kwargs)\n \n     @property\n     def state_size(self):\n-        # States are a flat list\n-        # in reverse order of the cell stack.\n-        # This allows to preserve the requirement\n-        # `stack.state_size[0] == output_dim`.\n-        # e.g. states of a 2-layer LSTM would be\n-        # `[h2, c2, h1, c1]`\n+        # States are a flat list of the individual cell state size.\n+        # e.g. states of a 2-layer LSTM would be `[h1, c1, h2, c2]`.\n         # (assuming one LSTM has states [h, c])\n+        # In the case of reverse_state_order=True, the state_size will be\n+        # `[h2, c2, h1, c1]`.\n         state_size = []\n-        for cell in self.cells[::-1]:\n+        for cell in self.cells[::-1] if self.reverse_state_order else self.cells:\n             if hasattr(cell.state_size, '__len__'):\n                 state_size += list(cell.state_size)\n             else:\n                 state_size.append(cell.state_size)\n         return tuple(state_size)\n \n+    @property\n+    def output_size(self):\n+        if getattr(self.cells[-1], 'output_size', None) is not None:\n+            return self.cells[-1].output_size\n+        if hasattr(self.cells[-1].state_size, '__len__'):\n+            return self.cells[-1].state_size[0]\n+        else:\n+            return self.cells[-1].state_size\n+\n     def call(self, inputs, states, constants=None, **kwargs):\n         # Recover per-cell states.\n         nested_states = []\n-        for cell in self.cells[::-1]:\n+        for cell in self.cells[::-1] if self.reverse_state_order else self.cells:\n             if hasattr(cell.state_size, '__len__'):\n                 nested_states.append(states[:len(cell.state_size)])\n                 states = states[len(cell.state_size):]\n             else:\n                 nested_states.append([states[0]])\n                 states = states[1:]\n+        if self.reverse_state_order:\n             nested_states = nested_states[::-1]\n \n         # Call the cells in order and store the returned states.\n@@ -98,10 +118,12 @@ class StackedRNNCells(Layer):\n \n         # Format the new states as a flat list\n         # in reverse cell order.\n-        states = []\n-        for cell_states in new_nested_states[::-1]:\n-            states += cell_states\n-        return inputs, states\n+        new_states = []\n+        if self.reverse_state_order:\n+            new_nested_states = new_nested_states[::-1]\n+        for cell_states in new_nested_states:\n+            new_states += cell_states\n+        return inputs, new_states\n \n     def build(self, input_shape):\n         if isinstance(input_shape, list):\n@@ -113,7 +135,9 @@ class StackedRNNCells(Layer):\n                     cell.build([input_shape] + constants_shape)\n                 else:\n                     cell.build(input_shape)\n-            if hasattr(cell.state_size, '__len__'):\n+            if getattr(cell, 'output_size', None) is not None:\n+                output_dim = cell.output_size\n+            elif hasattr(cell.state_size, '__len__'):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size\n@@ -223,9 +247,12 @@ class RNN(Layer):\n                 the size of the recurrent state\n                 (which should be the same as the size of the cell output).\n                 This can also be a list/tuple of integers\n-                (one size per state). In this case, the first entry\n-                (`state_size[0]`) should be the same as\n-                the size of the cell output.\n+                (one size per state).\n+            - a `output_size` attribute. This can be a single integer or a\n+                TensorShape, which represent the shape of the output. For\n+                backward compatible reason, if this attribute is not available\n+                for the cell, the value will be inferred by the first element\n+                of the `state_size`.\n             It is also possible for `cell` to be a list of RNN cell instances,\n             in which cases the cells get stacked on after the other in the RNN,\n             implementing an efficient stacked RNN.\n@@ -414,6 +441,10 @@ class RNN(Layer):\n             state_size = self.cell.state_size\n         else:\n             state_size = [self.cell.state_size]\n+\n+        if getattr(self.cell, 'output_size', None) is not None:\n+            output_dim = self.cell.output_size\n+        else:\n             output_dim = state_size[0]\n \n         if self.return_sequences:\n@@ -827,6 +858,7 @@ class SimpleRNNCell(Layer):\n         self.dropout = min(1., max(0., dropout))\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.state_size = self.units\n+        self.output_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None\n \n@@ -1220,6 +1252,7 @@ class GRUCell(Layer):\n         self.implementation = implementation\n         self.reset_after = reset_after\n         self.state_size = self.units\n+        self.output_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None\n \n@@ -1795,6 +1828,7 @@ class LSTMCell(Layer):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.implementation = implementation\n         self.state_size = (self.units, self.units)\n+        self.output_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None\n \n\n@@ -499,7 +499,7 @@ def test_minimal_rnn_cell_non_layer_multiple_states():\n              MinimalRNNCell(16, 8),\n              MinimalRNNCell(32, 16)]\n     layer = recurrent.RNN(cells)\n-    assert layer.cell.state_size == (32, 32, 16, 16, 8, 8)\n+    assert layer.cell.state_size == (8, 8, 16, 16, 32, 32)\n     y = layer(x)\n     model = keras.models.Model(x, y)\n     model.compile(optimizer='rmsprop', loss='mse')\n@@ -678,6 +678,19 @@ def test_stacked_rnn_compute_output_shape():\n              recurrent.LSTMCell(6)]\n     layer = recurrent.RNN(cells, return_state=True, return_sequences=True)\n     output_shape = layer.compute_output_shape((None, timesteps, embedding_dim))\n+    expected_output_shape = [(None, timesteps, 6),\n+                             (None, 3),\n+                             (None, 3),\n+                             (None, 6),\n+                             (None, 6)]\n+    assert output_shape == expected_output_shape\n+\n+    # Test reverse_state_order = True for stacked cell.\n+    stacked_cell = recurrent.StackedRNNCells(\n+        cells, reverse_state_order=True)\n+    layer = recurrent.RNN(\n+        stacked_cell, return_state=True, return_sequences=True)\n+    output_shape = layer.compute_output_shape((None, timesteps, embedding_dim))\n     expected_output_shape = [(None, timesteps, 6),\n                              (None, 6),\n                              (None, 6),\n@@ -907,5 +920,48 @@ def test_rnn_cell_identity_initializer(layer_class):\n                           np.concatenate([np.identity(units)] * num_kernels, axis=1))\n \n \n+@keras_test\n+@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\n+def test_inconsistent_output_state_size():\n+\n+    class PlusOneRNNCell(keras.layers.Layer):\n+        \"\"\"Add one to the input and state.\n+\n+        This cell is used for testing state_size and output_size.\"\"\"\n+\n+        def __init__(self, num_unit, **kwargs):\n+            self.state_size = num_unit\n+            super(PlusOneRNNCell, self).__init__(**kwargs)\n+\n+        def build(self, input_shape):\n+            self.output_size = input_shape[-1]\n+\n+        def call(self, inputs, states):\n+            return inputs + 1, [states[0] + 1]\n+\n+    batch = 32\n+    time_step = 4\n+    state_size = 5\n+    input_size = 6\n+    cell = PlusOneRNNCell(state_size)\n+    x = keras.Input((None, input_size))\n+    layer = recurrent.RNN(cell)\n+    y = layer(x)\n+\n+    assert cell.state_size == state_size\n+    init_state = layer.get_initial_state(x)\n+    assert len(init_state) == 1\n+    if K.backend() != 'theano':\n+        # theano does not support static shape inference.\n+        assert K.int_shape(init_state[0]) == (None, state_size)\n+\n+    model = keras.models.Model(x, y)\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.train_on_batch(\n+        np.zeros((batch, time_step, input_size)),\n+        np.zeros((batch, input_size)))\n+    assert model.output_shape == (None, input_size)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#105f6e2964b8f28fd4d05c0e538b492ee7fc5020", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 860 | Contributors (this commit): 12 | Commits (past 90d): 1 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -218,7 +218,7 @@ if __name__ == '__main__':\n     plot_model(vae, to_file='vae_cnn.png', show_shapes=True)\n \n     if args.weights:\n-        vae = vae.load_weights(args.weights)\n+        vae.load_weights(args.weights)\n     else:\n         # train the autoencoder\n         vae.fit(x_train,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e1a0df3cd1d1bbbe7f9010c4c943bedfdfc3487", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 471 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -102,7 +102,9 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n             _output = recovered_model.predict(input_data)\n             assert_allclose(_output, actual_output, rtol=1e-3)\n \n-        # test training mode (e.g. useful for dropout tests)\n+        # test training mode (e.g. useful when the layer has a\n+        # different behavior at training and testing time).\n+        if has_arg(layer.call, 'training'):\n             model.compile('rmsprop', 'mse')\n             model.train_on_batch(input_data, actual_output)\n         return actual_output\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2780ab0c9712b41c34693758141f30ae3d3c2e7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 92 | Lines Deleted: 169 | Files Changed: 1 | Hunks: 26 | Methods Changed: 13 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 261 | Churn Cumulative: 1974 | Contributors (this commit): 21 | Commits (past 90d): 6 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -422,30 +422,91 @@ class SequenceEnqueuer(object):\n     The `enqueuer.get()` should be an infinite stream of datas.\n \n     \"\"\"\n+    def __init__(self, sequence,\n+                 use_multiprocessing=False):\n+        self.sequence = sequence\n+        self.use_multiprocessing = use_multiprocessing\n+\n+        global _SEQUENCE_COUNTER\n+        if _SEQUENCE_COUNTER is None:\n+            try:\n+                _SEQUENCE_COUNTER = mp.Value('i', 0)\n+            except OSError:\n+                # In this case the OS does not allow us to use\n+                # multiprocessing. We resort to an int\n+                # for enqueuer indexing.\n+                _SEQUENCE_COUNTER = 0\n+\n+        if isinstance(_SEQUENCE_COUNTER, int):\n+            self.uid = _SEQUENCE_COUNTER\n+            _SEQUENCE_COUNTER += 1\n+        else:\n+            # Doing Multiprocessing.Value += x is not process-safe.\n+            with _SEQUENCE_COUNTER.get_lock():\n+                self.uid = _SEQUENCE_COUNTER.value\n+                _SEQUENCE_COUNTER.value += 1\n+\n+        self.workers = 0\n+        self.executor_fn = None\n+        self.queue = None\n+        self.run_thread = None\n+        self.stop_signal = None\n \n-    @abstractmethod\n     def is_running(self):\n-        raise NotImplementedError\n+        return self.stop_signal is not None and not self.stop_signal.is_set()\n \n-    @abstractmethod\n     def start(self, workers=1, max_queue_size=10):\n-        \"\"\"Starts the handler's workers.\n+        \"\"\"Start the handler's workers.\n \n         # Arguments\n             workers: number of worker threads\n             max_queue_size: queue size\n-                (when full, threads could block on `put()`).\n+                (when full, workers could block on `put()`)\n         \"\"\"\n+        if self.use_multiprocessing:\n+            self.executor_fn = self._get_executor_init(workers)\n+        else:\n+            # We do not need the init since it's threads.\n+            self.executor_fn = lambda _: ThreadPool(workers)\n+        self.workers = workers\n+        self.queue = queue.Queue(max_queue_size)\n+        self.stop_signal = threading.Event()\n+        self.run_thread = threading.Thread(target=self._run)\n+        self.run_thread.daemon = True\n+        self.run_thread.start()\n+\n+    def _send_sequence(self):\n+        \"\"\"Send current Iterable to all workers.\"\"\"\n+        # For new processes that may spawn\n+        _SHARED_SEQUENCES[self.uid] = self.sequence\n+\n+    def stop(self, timeout=None):\n+        \"\"\"Stops running threads and wait for them to exit, if necessary.\n+\n+        Should be called by the same thread which called `start()`.\n+\n+        # Arguments\n+            timeout: maximum time to wait on `thread.join()`\n+        \"\"\"\n+        self.stop_signal.set()\n+        with self.queue.mutex:\n+            self.queue.queue.clear()\n+            self.queue.unfinished_tasks = 0\n+            self.queue.not_full.notify()\n+        self.run_thread.join(timeout)\n+        _SHARED_SEQUENCES[self.uid] = None\n+\n+    @abstractmethod\n+    def _run(self):\n+        \"\"\"Submits request to the executor and queue the `Future` objects.\"\"\"\n         raise NotImplementedError\n \n     @abstractmethod\n-    def stop(self, timeout=None):\n-        \"\"\"Stop running threads and wait for them to exit, if necessary.\n+    def _get_executor_init(self, workers):\n+        \"\"\"Get the Pool initializer for multiprocessing.\n \n-        Should be called by the same thread which called start().\n-\n-        # Arguments\n-            timeout: maximum time to wait on thread.join()\n+        # Returns\n+            Function, a Function to initialize the pool\n         \"\"\"\n         raise NotImplementedError\n \n@@ -472,63 +533,19 @@ class OrderedEnqueuer(SequenceEnqueuer):\n         use_multiprocessing: use multiprocessing if True, otherwise threading\n         shuffle: whether to shuffle the data at the beginning of each epoch\n     \"\"\"\n-\n-    def __init__(self, sequence,\n-                 use_multiprocessing=False,\n-                 shuffle=False):\n-        self.sequence = sequence\n-        self.use_multiprocessing = use_multiprocessing\n-\n-        global _SEQUENCE_COUNTER\n-        if _SEQUENCE_COUNTER is None:\n-            try:\n-                _SEQUENCE_COUNTER = mp.Value('i', 0)\n-            except OSError:\n-                # In this case the OS does not allow us to use\n-                # multiprocessing. We resort to an int\n-                # for enqueuer indexing.\n-                _SEQUENCE_COUNTER = 0\n-\n-        if isinstance(_SEQUENCE_COUNTER, int):\n-            self.uid = _SEQUENCE_COUNTER\n-            _SEQUENCE_COUNTER += 1\n-        else:\n-            # Doing Multiprocessing.Value += x is not process-safe.\n-            with _SEQUENCE_COUNTER.get_lock():\n-                self.uid = _SEQUENCE_COUNTER.value\n-                _SEQUENCE_COUNTER.value += 1\n-\n+    def __init__(self, sequence, use_multiprocessing=False, shuffle=False):\n+        super(OrderedEnqueuer, self).__init__(sequence, use_multiprocessing)\n         self.shuffle = shuffle\n-        self.workers = 0\n-        self.executor_fn = None\n-        self.queue = None\n-        self.run_thread = None\n-        self.stop_signal = None\n \n-    def is_running(self):\n-        return self.stop_signal is not None and not self.stop_signal.is_set()\n+    def _get_executor_init(self, workers):\n+        \"\"\"Get the Pool initializer for multiprocessing.\n \n-    def start(self, workers=1, max_queue_size=10):\n-        \"\"\"Start the handler's workers.\n-\n-        # Arguments\n-            workers: number of worker threads\n-            max_queue_size: queue size\n-                (when full, workers could block on `put()`)\n+        # Returns\n+            Function, a Function to initialize the pool\n         \"\"\"\n-        if self.use_multiprocessing:\n-            self.executor_fn = lambda seqs: mp.Pool(workers,\n+        return lambda seqs: mp.Pool(workers,\n                                     initializer=init_pool,\n                                     initargs=(seqs,))\n-        else:\n-            # We do not need the init since it's threads.\n-            self.executor_fn = lambda _: ThreadPool(workers)\n-        self.workers = workers\n-        self.queue = queue.Queue(max_queue_size)\n-        self.stop_signal = threading.Event()\n-        self.run_thread = threading.Thread(target=self._run)\n-        self.run_thread.daemon = True\n-        self.run_thread.start()\n \n     def _wait_queue(self):\n         \"\"\"Wait for the queue to be empty.\"\"\"\n@@ -583,37 +600,10 @@ class OrderedEnqueuer(SequenceEnqueuer):\n             self.stop()\n             six.reraise(*sys.exc_info())\n \n-    def _send_sequence(self):\n-        \"\"\"Send current Sequence to all workers.\"\"\"\n-        # For new processes that may spawn\n-        _SHARED_SEQUENCES[self.uid] = self.sequence\n-\n-    def stop(self, timeout=None):\n-        \"\"\"Stops running threads and wait for them to exit, if necessary.\n-\n-        Should be called by the same thread which called `start()`.\n-\n-        # Arguments\n-            timeout: maximum time to wait on `thread.join()`\n-        \"\"\"\n-        self.stop_signal.set()\n-        with self.queue.mutex:\n-            self.queue.queue.clear()\n-            self.queue.unfinished_tasks = 0\n-            self.queue.not_full.notify()\n-        self.run_thread.join(timeout)\n-        _SHARED_SEQUENCES[self.uid] = None\n-\n-\n-# Global variables to be shared across processes\n-_SHARED_GENERATOR = {}\n-# We use a Value to provide unique id to different processes.\n-_GENERATOR_COUNTER = None\n-\n \n def init_pool_generator(gens, random_seed=None):\n-    global _SHARED_GENERATOR\n-    _SHARED_GENERATOR = gens\n+    global _SHARED_SEQUENCES\n+    _SHARED_SEQUENCES = gens\n \n     if random_seed is not None:\n         ident = mp.current_process().ident\n@@ -633,7 +623,7 @@ def next_sample(uid):\n     # Returns\n         The next value of generator `uid`.\n     \"\"\"\n-    return six.next(_SHARED_GENERATOR[uid])\n+    return six.next(_SHARED_SEQUENCES[uid])\n \n \n class GeneratorEnqueuer(SequenceEnqueuer):\n@@ -652,72 +642,28 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n             will be incremented by one for each worker.\n     \"\"\"\n \n-    def __init__(self, generator,\n-                 use_multiprocessing=False,\n-                 wait_time=None,\n+    def __init__(self, sequence, use_multiprocessing=False, wait_time=None,\n                  random_seed=None):\n-        self.generator = generator\n-        self.use_multiprocessing = use_multiprocessing\n+        super(GeneratorEnqueuer, self).__init__(sequence, use_multiprocessing)\n         self.random_seed = random_seed\n         if wait_time is not None:\n             warnings.warn('`wait_time` is not used anymore.',\n                           DeprecationWarning)\n \n-        global _GENERATOR_COUNTER\n-        if _GENERATOR_COUNTER is None:\n-            try:\n-                _GENERATOR_COUNTER = mp.Value('i', 0)\n-            except OSError:\n-                # In this case the OS does not allow us to use\n-                # multiprocessing. We resort to an int\n-                # for enqueuer indexing.\n-                _GENERATOR_COUNTER = 0\n+    def _get_executor_init(self, workers):\n+        \"\"\"Get the Pool initializer for multiprocessing.\n \n-        if isinstance(_GENERATOR_COUNTER, int):\n-            self.uid = _GENERATOR_COUNTER\n-            _GENERATOR_COUNTER += 1\n-        else:\n-            # Doing Multiprocessing.Value += x is not process-safe.\n-            with _GENERATOR_COUNTER.get_lock():\n-                self.uid = _GENERATOR_COUNTER.value\n-                _GENERATOR_COUNTER.value += 1\n-\n-        self.workers = 0\n-        self.executor_fn = None\n-        self.queue = None\n-        self.run_thread = None\n-        self.stop_signal = None\n-\n-    def is_running(self):\n-        return self.stop_signal is not None and not self.stop_signal.is_set()\n-\n-    def start(self, workers=1, max_queue_size=10):\n-        \"\"\"Start the handler's workers.\n-\n-        # Arguments\n-            workers: number of worker threads\n-            max_queue_size: queue size\n-                (when full, workers could block on `put()`)\n+        # Returns\n+            Function, a Function to initialize the pool\n         \"\"\"\n-        if self.use_multiprocessing:\n-            self.executor_fn = lambda gens: mp.Pool(workers,\n+        return lambda seqs: mp.Pool(workers,\n                                     initializer=init_pool_generator,\n-                                                    initargs=(gens,\n-                                                              self.random_seed))\n-        else:\n-            # We do not need the init since it's threads.\n-            self.executor_fn = lambda _: ThreadPool(workers)\n-        self.workers = workers\n-        self.queue = queue.Queue(max_queue_size)\n-        self.stop_signal = threading.Event()\n-        self.run_thread = threading.Thread(target=self._run)\n-        self.run_thread.daemon = True\n-        self.run_thread.start()\n+                                    initargs=(seqs, self.random_seed))\n \n     def _run(self):\n         \"\"\"Submits request to the executor and queue the `Future` objects.\"\"\"\n-        self._send_generator()  # Share the initial generator\n-        with closing(self.executor_fn(_SHARED_GENERATOR)) as executor:\n+        self._send_sequence()  # Share the initial generator\n+        with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n             while True:\n                 if self.stop_signal.is_set():\n                     return\n@@ -761,26 +707,3 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n                     \"`use_multiprocessing=False, workers > 1`.\"\n                     \"For more information see issue #1638.\")\n             six.reraise(*sys.exc_info())\n-\n-    def _send_generator(self):\n-        \"\"\"Send current generator to all workers.\"\"\"\n-        # For new processes that may spawn\n-        _SHARED_GENERATOR[self.uid] = self.generator\n-\n-    def stop(self, timeout=None):\n-        \"\"\"Stops running threads and wait for them to exit, if necessary.\n-\n-        Should be called by the same thread which called `start()`.\n-\n-        # Arguments\n-            timeout: maximum time to wait on `thread.join()`\n-        \"\"\"\n-        self.stop_signal.set()\n-\n-        with self.queue.mutex:\n-            self.queue.queue.clear()\n-            self.queue.unfinished_tasks = 0\n-            self.queue.not_full.notify()\n-\n-        self.run_thread.join(timeout)\n-        _SHARED_GENERATOR[self.uid] = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
