{"custom_id": "scrapy#f4fc05c2a1a676f711650d1661f7f5f78a27b501", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 144 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -63,6 +63,7 @@ class CookiesMiddlewareTest(TestCase):\n         crawler = get_crawler(settings_dict={'COOKIES_DEBUG': True})\n         mw = CookiesMiddleware.from_crawler(crawler)\n         with LogCapture('scrapy.downloadermiddlewares.cookies',\n+                        propagate=False,\n                         level=logging.DEBUG) as l:\n             req = Request('http://scrapytest.org/')\n             res = Response('http://scrapytest.org/',\n@@ -86,6 +87,7 @@ class CookiesMiddlewareTest(TestCase):\n         crawler = get_crawler(settings_dict={'COOKIES_DEBUG': False})\n         mw = CookiesMiddleware.from_crawler(crawler)\n         with LogCapture('scrapy.downloadermiddlewares.cookies',\n+                        propagate=False,\n                         level=logging.DEBUG) as l:\n             req = Request('http://scrapytest.org/')\n             res = Response('http://scrapytest.org/',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#91991e0bdcb9342813523bf393c3ed61c3a12def", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 15 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 103 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,11 @@\n \"\"\"\n Helper functions for serializing (and deserializing) requests.\n \"\"\"\n+import six\n \n from scrapy.http import Request\n+from scrapy.utils.python import to_unicode, to_native_str\n+\n \n def request_to_dict(request, spider=None):\n     \"\"\"Convert Request object to a dict.\n@@ -17,7 +20,7 @@ def request_to_dict(request, spider=None):\n     if callable(eb):\n         eb = _find_method(spider, eb)\n     d = {\n-        'url': request.url.decode('ascii'), # urls should be safe (safe_string_url)\n+        'url': to_unicode(request.url),  # urls should be safe (safe_string_url)\n         'callback': cb,\n         'errback': eb,\n         'method': request.method,\n@@ -45,7 +48,7 @@ def request_from_dict(d, spider=None):\n     if eb and spider:\n         eb = _get_method(spider, eb)\n     return Request(\n-        url=d['url'].encode('ascii'),\n+        url=to_native_str(d['url']),\n         callback=cb,\n         errback=eb,\n         method=d['method'],\n@@ -59,11 +62,17 @@ def request_from_dict(d, spider=None):\n \n \n def _find_method(obj, func):\n-    if obj and hasattr(func, 'im_self') and func.im_self is obj:\n-        return func.im_func.__name__\n+    if obj:\n+        try:\n+            func_self = six.get_method_self(func)\n+        except AttributeError:  # func has no __self__\n+            pass\n         else:\n+            if func_self is obj:\n+                return six.get_method_function(func).__name__\n     raise ValueError(\"Function %s is not a method of: %s\" % (func, obj))\n \n+\n def _get_method(obj, name):\n     name = str(name)\n     try:\n\n@@ -1,9 +1,11 @@\n+# -*- coding: utf-8 -*-\n import unittest\n \n from scrapy.http import Request\n from scrapy.spiders import Spider\n from scrapy.utils.reqser import request_to_dict, request_from_dict\n \n+\n class RequestSerializationTest(unittest.TestCase):\n \n     def setUp(self):\n@@ -20,18 +22,18 @@ class RequestSerializationTest(unittest.TestCase):\n             method=\"POST\",\n             body=\"some body\",\n             headers={'content-encoding': 'text/html; charset=latin-1'},\n-            cookies={'currency': 'usd'},\n+            cookies={'currency': u'руб'},\n             encoding='latin-1',\n             priority=20,\n             meta={'a': 'b'})\n         self._assert_serializes_ok(r)\n \n     def test_latin1_body(self):\n-        r = Request(\"http://www.example.com\", body=\"\\xa3\")\n+        r = Request(\"http://www.example.com\", body=b\"\\xa3\")\n         self._assert_serializes_ok(r)\n \n     def test_utf8_body(self):\n-        r = Request(\"http://www.example.com\", body=\"\\xc2\\xa3\")\n+        r = Request(\"http://www.example.com\", body=b\"\\xc2\\xa3\")\n         self._assert_serializes_ok(r)\n \n     def _assert_serializes_ok(self, request, spider=None):\n@@ -53,7 +55,7 @@ class RequestSerializationTest(unittest.TestCase):\n         self.assertEqual(r1.dont_filter, r2.dont_filter)\n \n     def test_callback_serialization(self):\n-        r = Request(\"http://www.example.com\", callback=self.spider.parse_item, \\\n+        r = Request(\"http://www.example.com\", callback=self.spider.parse_item,\n                     errback=self.spider.handle_error)\n         self._assert_serializes_ok(r, spider=self.spider)\n \n@@ -69,7 +71,9 @@ class RequestSerializationTest(unittest.TestCase):\n \n class TestSpider(Spider):\n     name = 'test'\n+\n     def parse_item(self, response):\n         pass\n+\n     def handle_error(self, failure):\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3e6d6c43ac0763adf2cd92efdb4a1dc2ba165440", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 15 | Churn Cumulative: 649 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,7 +18,7 @@ def _iter_command_classes(module_name):\n     # TODO: add `name` attribute to commands and and merge this function with\n     # scrapy.utils.spider.iter_spider_classes\n     for module in walk_modules(module_name):\n-        for obj in vars(module).itervalues():\n+        for obj in vars(module).values():\n             if inspect.isclass(obj) and \\\n                     issubclass(obj, ScrapyCommand) and \\\n                     obj.__module__ == module.__name__:\n\n@@ -35,8 +35,8 @@ class TestProcessProtocol(protocol.ProcessProtocol):\n \n     def __init__(self):\n         self.deferred = defer.Deferred()\n-        self.out = ''\n-        self.err = ''\n+        self.out = b''\n+        self.err = b''\n         self.exitcode = None\n \n     def outReceived(self, data):\n\n@@ -11,10 +11,11 @@ class CmdlineTest(unittest.TestCase):\n         self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'\n \n     def _execute(self, *new_args, **kwargs):\n+        encoding = getattr(sys.stdout, 'encoding') or 'utf-8'\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n-        comm = proc.communicate()\n-        return comm[0].strip()\n+        comm = proc.communicate()[0].strip()\n+        return comm.decode(encoding)\n \n     def test_default_settings(self):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\\n\n@@ -1,3 +1,4 @@\n+import sys\n from twisted.trial import unittest\n from twisted.internet import defer\n \n@@ -11,5 +12,6 @@ class VersionTest(ProcessTest, unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_output(self):\n+        encoding = getattr(sys.stdout, 'encoding') or 'utf-8'\n         _, out, _ = yield self.execute([])\n-        self.assertEqual(out.strip(), \"Scrapy %s\" % scrapy.__version__)\n+        self.assertEqual(out.strip().decode(encoding), \"Scrapy %s\" % scrapy.__version__)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6e762ce25cb15ed16f10bc218f38133801548604", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 9 | Files Changed: 5 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 527 | Contributors (this commit): 9 | Commits (past 90d): 8 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,4 @@\n-from urlparse import unquote\n+from six.moves.urllib.parse import unquote\n \n from scrapy.exceptions import NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n\n@@ -3,7 +3,7 @@ Downloader Middleware manager\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n-\n+import six\n from scrapy.http import Request, Response\n from scrapy.middleware import MiddlewareManager\n from scrapy.utils.defer import mustbe_deferred\n@@ -32,7 +32,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                 response = method(request=request, spider=spider)\n                 assert response is None or isinstance(response, (Response, Request)), \\\n                         'Middleware %s.process_request must return None, Response or Request, got %s' % \\\n-                        (method.im_self.__class__.__name__, response.__class__.__name__)\n+                        (six.get_method_self(method).__class__.__name__, response.__class__.__name__)\n                 if response:\n                     return response\n             return download_func(request=request, spider=spider)\n@@ -46,7 +46,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                 response = method(request=request, response=response, spider=spider)\n                 assert isinstance(response, (Response, Request)), \\\n                     'Middleware %s.process_response must return Response or Request, got %s' % \\\n-                    (method.im_self.__class__.__name__, type(response))\n+                    (six.get_method_self(method).__class__.__name__, type(response))\n                 if isinstance(response, Request):\n                     return response\n             return response\n@@ -57,7 +57,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                 response = method(request=request, exception=exception, spider=spider)\n                 assert response is None or isinstance(response, (Response, Request)), \\\n                     'Middleware %s.process_exception must return None, Response or Request, got %s' % \\\n-                    (method.im_self.__class__.__name__, type(response))\n+                    (six.get_method_self(method).__class__.__name__, type(response))\n                 if response:\n                     return response\n             return _failure\n\n@@ -3,7 +3,7 @@ Spider Middleware manager\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n-\n+import six\n from twisted.python.failure import Failure\n from scrapy.middleware import MiddlewareManager\n from scrapy.utils.defer import mustbe_deferred\n@@ -33,7 +33,9 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             self.methods['process_start_requests'].insert(0, mw.process_start_requests)\n \n     def scrape_response(self, scrape_func, response, request, spider):\n-        fname = lambda f:'%s.%s' % (f.im_self.__class__.__name__, f.im_func.__name__)\n+        fname = lambda f:'%s.%s' % (\n+                six.get_method_self(f).__class__.__name__,\n+                six.get_method_function(f).__name__)\n \n         def process_spider_input(response):\n             for method in self.methods['process_spider_input']:\n\n@@ -3,7 +3,7 @@ HTMLParser-based link extractor\n \"\"\"\n \n import warnings\n-from HTMLParser import HTMLParser\n+from six.moves.html_parser import HTMLParser\n from six.moves.urllib.parse import urljoin\n \n from w3lib.url import safe_url_string\n\n@@ -141,7 +141,7 @@ class CrawlTestCase(TestCase):\n     def test_unbounded_response(self):\n         # Completeness of responses without Content-Length or Transfer-Encoding\n         # can not be determined, we treat them as valid but flagged as \"partial\"\n-        from urllib import urlencode\n+        from six.moves.urllib.parse import urlencode\n         query = urlencode({'raw': '''\\\n HTTP/1.1 200 OK\n Server: Apache-Coyote/1.1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#991197003bdf8e908aebe5ce39ceff353af7e016", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 54 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -26,6 +26,7 @@ from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.misc import md5sum\n from scrapy.utils.log import failure_to_exc_info\n+from scrapy.utils.python import to_bytes\n \n logger = logging.getLogger(__name__)\n \n@@ -330,7 +331,7 @@ class FilesPipeline(MediaPipeline):\n             return self.file_key(url)\n         ## end of deprecation warning block\n \n-        media_guid = hashlib.sha1(url).hexdigest()  # change to request.url after deprecation\n+        media_guid = hashlib.sha1(to_bytes(url)).hexdigest()  # change to request.url after deprecation\n         media_ext = os.path.splitext(url)[1]  # change to request.url after deprecation\n         return 'full/%s%s' % (media_guid, media_ext)\n \n\n@@ -12,6 +12,7 @@ from scrapy.pipelines.files import FilesPipeline, FSFilesStore\n from scrapy.item import Item, Field\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n+from scrapy.utils.python import to_bytes\n \n from tests import mock\n \n@@ -103,7 +104,7 @@ class FilesPipelineTestCase(unittest.TestCase):\n \n class DeprecatedFilesPipeline(FilesPipeline):\n     def file_key(self, url):\n-        media_guid = hashlib.sha1(url).hexdigest()\n+        media_guid = hashlib.sha1(to_bytes(url)).hexdigest()\n         media_ext = os.path.splitext(url)[1]\n         return 'empty/%s%s' % (media_guid, media_ext)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#34eced0ee822ac395fde457984faf4afcc77f713", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 33 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -15,6 +15,7 @@ except ImportError:\n from PIL import Image\n \n from scrapy.utils.misc import md5sum\n+from scrapy.utils.python import to_bytes\n from scrapy.http import Request\n from scrapy.exceptions import DropItem\n #TODO: from scrapy.pipelines.media import MediaPipeline\n@@ -138,7 +139,7 @@ class ImagesPipeline(FilesPipeline):\n             return self.image_key(url)\n         ## end of deprecation warning block\n \n-        image_guid = hashlib.sha1(url).hexdigest()  # change to request.url after deprecation\n+        image_guid = hashlib.sha1(to_bytes(url)).hexdigest()  # change to request.url after deprecation\n         return 'full/%s.jpg' % (image_guid)\n \n     def thumb_path(self, request, thumb_id, response=None, info=None):\n@@ -163,7 +164,7 @@ class ImagesPipeline(FilesPipeline):\n             return self.thumb_key(url, thumb_id)\n         ## end of deprecation warning block\n \n-        thumb_guid = hashlib.sha1(url).hexdigest()  # change to request.url after deprecation\n+        thumb_guid = hashlib.sha1(to_bytes(url)).hexdigest()  # change to request.url after deprecation\n         return 'thumbs/%s/%s.jpg' % (thumb_id, thumb_guid)\n \n     # deprecated\n\n@@ -10,6 +10,7 @@ from scrapy.item import Item, Field\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n from scrapy.pipelines.images import ImagesPipeline\n+from scrapy.utils.python import to_bytes\n \n skip = False\n try:\n@@ -100,11 +101,11 @@ class DeprecatedImagesPipeline(ImagesPipeline):\n         return self.image_key(url)\n \n     def image_key(self, url):\n-        image_guid = hashlib.sha1(url).hexdigest()\n+        image_guid = hashlib.sha1(to_bytes(url)).hexdigest()\n         return 'empty/%s.jpg' % (image_guid)\n \n     def thumb_key(self, url, thumb_id):\n-        thumb_guid = hashlib.sha1(url).hexdigest()\n+        thumb_guid = hashlib.sha1(to_bytes(url)).hexdigest()\n         return 'thumbsup/%s/%s.jpg' % (thumb_id, thumb_guid)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#45d441d444ed1d1e2f94739e574ff9f1290cf3dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 19 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n import unittest\n+import six\n from functools import partial\n \n from scrapy.loader import ItemLoader\n@@ -141,7 +142,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n \n     def test_get_value(self):\n         il = NameItemLoader()\n-        self.assertEqual(u'FOO', il.get_value([u'foo', u'bar'], TakeFirst(), unicode.upper))\n+        self.assertEqual(u'FOO', il.get_value([u'foo', u'bar'], TakeFirst(), six.text_type.upper))\n         self.assertEqual([u'foo', u'bar'], il.get_value([u'name:foo', u'name:bar'], re=u'name:(.*)$'))\n         self.assertEqual(u'foo', il.get_value([u'name:foo', u'name:bar'], TakeFirst(), re=u'name:(.*)$'))\n \n@@ -242,7 +243,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n \n     def test_extend_custom_input_processors(self):\n         class ChildItemLoader(TestItemLoader):\n-            name_in = MapCompose(TestItemLoader.name_in, unicode.swapcase)\n+            name_in = MapCompose(TestItemLoader.name_in, six.text_type.swapcase)\n \n         il = ChildItemLoader()\n         il.add_value('name', u'marta')\n@@ -250,7 +251,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n \n     def test_extend_default_input_processors(self):\n         class ChildDefaultedItemLoader(DefaultedItemLoader):\n-            name_in = MapCompose(DefaultedItemLoader.default_input_processor, unicode.swapcase)\n+            name_in = MapCompose(DefaultedItemLoader.default_input_processor, six.text_type.swapcase)\n \n         il = ChildDefaultedItemLoader()\n         il.add_value('name', u'marta')\n@@ -423,7 +424,7 @@ class ProcessorsTest(unittest.TestCase):\n         self.assertRaises(TypeError, proc, [None, '', 'hello', 'world'])\n         self.assertEqual(proc(['', 'hello', 'world']), u' hello world')\n         self.assertEqual(proc(['hello', 'world']), u'hello world')\n-        self.assert_(isinstance(proc(['hello', 'world']), unicode))\n+        self.assert_(isinstance(proc(['hello', 'world']), six.text_type))\n \n     def test_compose(self):\n         proc = Compose(lambda v: v[0], str.upper)\n@@ -435,13 +436,13 @@ class ProcessorsTest(unittest.TestCase):\n \n     def test_mapcompose(self):\n         filter_world = lambda x: None if x == 'world' else x\n-        proc = MapCompose(filter_world, unicode.upper)\n+        proc = MapCompose(filter_world, six.text_type.upper)\n         self.assertEqual(proc([u'hello', u'world', u'this', u'is', u'scrapy']),\n                          [u'HELLO', u'THIS', u'IS', u'SCRAPY'])\n \n \n class SelectortemLoaderTest(unittest.TestCase):\n-    response = HtmlResponse(url=\"\", body=\"\"\"\n+    response = HtmlResponse(url=\"\", encoding='utf-8', body=b\"\"\"\n     <html>\n     <body>\n     <div id=\"id\">marta</div>\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#17b5e9fb86b3969884213cde9e5a44647396f560", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 4 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 25/23 | Churn Δ: 18 | Churn Cumulative: 108 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -9,7 +9,7 @@ from cssselect.parser import SelectorSyntaxError\n from cssselect.xpath import ExpressionError\n \n \n-HTMLBODY = '''\n+HTMLBODY = b'''\n <html>\n <body>\n <div>\n\n@@ -6,7 +6,7 @@ from scrapy.http import TextResponse, HtmlResponse\n class LxmlDocumentTest(unittest.TestCase):\n \n     def test_caching(self):\n-        r1 = HtmlResponse('http://www.example.com', body='<html><head></head><body></body></html>')\n+        r1 = HtmlResponse('http://www.example.com', body=b'<html><head></head><body></body></html>')\n         r2 = r1.copy()\n \n         doc1 = LxmlDocument(r1)\n@@ -19,7 +19,7 @@ class LxmlDocumentTest(unittest.TestCase):\n \n     def test_null_char(self):\n         # make sure bodies with null char ('\\x00') don't raise a TypeError exception\n-        body = 'test problematic \\x00 body'\n+        body = b'test problematic \\x00 body'\n         response = TextResponse('http://example.com/catalog/product/blabla-123',\n                                 headers={'Content-Type': 'text/plain; charset=utf-8'},\n                                 body=body)\n\n@@ -13,7 +13,7 @@ class XmliterTestCase(unittest.TestCase):\n     xmliter = staticmethod(xmliter)\n \n     def test_xmliter(self):\n-        body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\\n+        body = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\\n             <products xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"someschmea.xsd\">\\\n               <product id=\"001\">\\\n                 <type>Type 1</type>\\\n@@ -40,7 +40,7 @@ class XmliterTestCase(unittest.TestCase):\n                          [[u'one'], [u'two']])\n \n     def test_xmliter_namespaces(self):\n-        body = \"\"\"\\\n+        body = b\"\"\"\\\n             <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n             <rss version=\"2.0\" xmlns:g=\"http://base.google.com/ns/1.0\">\n                 <channel>\n@@ -83,7 +83,7 @@ class XmliterTestCase(unittest.TestCase):\n         self.assertRaises(StopIteration, next, iter)\n \n     def test_xmliter_encoding(self):\n-        body = '<?xml version=\"1.0\" encoding=\"ISO-8859-9\"?>\\n<xml>\\n    <item>Some Turkish Characters \\xd6\\xc7\\xde\\xdd\\xd0\\xdc \\xfc\\xf0\\xfd\\xfe\\xe7\\xf6</item>\\n</xml>\\n\\n'\n+        body = b'<?xml version=\"1.0\" encoding=\"ISO-8859-9\"?>\\n<xml>\\n    <item>Some Turkish Characters \\xd6\\xc7\\xde\\xdd\\xd0\\xdc \\xfc\\xf0\\xfd\\xfe\\xe7\\xf6</item>\\n</xml>\\n\\n'\n         response = XmlResponse('http://www.example.com', body=body)\n         self.assertEqual(\n             self.xmliter(response, 'item').next().extract(),\n@@ -95,7 +95,7 @@ class LxmlXmliterTestCase(XmliterTestCase):\n     xmliter = staticmethod(xmliter_lxml)\n \n     def test_xmliter_iterate_namespace(self):\n-        body = \"\"\"\\\n+        body = b\"\"\"\\\n             <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n             <rss version=\"2.0\" xmlns=\"http://base.google.com/ns/1.0\">\n                 <channel>\n@@ -124,7 +124,7 @@ class LxmlXmliterTestCase(XmliterTestCase):\n         self.assertEqual(node.xpath('text()').extract(), ['http://www.mydummycompany.com/images/item2.jpg'])\n \n     def test_xmliter_namespaces_prefix(self):\n-        body = \"\"\"\\\n+        body = b\"\"\"\\\n         <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n         <root>\n             <h:table xmlns:h=\"http://www.w3.org/TR/html4/\">\n\n@@ -20,7 +20,7 @@ class RequestSerializationTest(unittest.TestCase):\n             callback='parse_item',\n             errback='handle_error',\n             method=\"POST\",\n-            body=\"some body\",\n+            body=b\"some body\",\n             headers={'content-encoding': 'text/html; charset=latin-1'},\n             cookies={'currency': u'руб'},\n             encoding='latin-1',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#56be610e6e26ec7a17ec16dccc09fa832facb0fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 24 | Churn Cumulative: 31 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,18 @@\n+import os\n import sys\n+import shutil\n+import pstats\n+import tempfile\n from subprocess import Popen, PIPE\n import unittest\n+try:\n+    from cStringIO import StringIO\n+except ImportError:\n+    from io import StringIO\n \n from scrapy.utils.test import get_testenv\n \n+\n class CmdlineTest(unittest.TestCase):\n \n     def setUp(self):\n@@ -30,3 +39,18 @@ class CmdlineTest(unittest.TestCase):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\\n                          'override')\n \n+    def test_profiling(self):\n+        path = tempfile.mkdtemp()\n+        filename = os.path.join(path, 'res.prof')\n+        try:\n+            self._execute('version', '--profile', filename)\n+            self.assertTrue(os.path.exists(filename))\n+            out = StringIO()\n+            stats = pstats.Stats(filename, stream=out)\n+            stats.print_stats()\n+            out.seek(0)\n+            stats = out.read()\n+            self.assertIn('scrapy/commands/version.py', stats)\n+            self.assertIn('tottime', stats)\n+        finally:\n+            shutil.rmtree(path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#93accb7fb346e47feda24b70d4af35ae0ae4f069", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 43 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,7 +26,7 @@ from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.misc import md5sum\n from scrapy.utils.log import failure_to_exc_info\n-from scrapy.utils.python import to_bytes\n+from scrapy.utils.python import to_bytes, to_native_str\n \n logger = logging.getLogger(__name__)\n \n@@ -199,7 +199,7 @@ class FilesPipeline(MediaPipeline):\n             if age_days > self.EXPIRES:\n                 return  # returning None force download\n \n-            referer = request.headers.get('Referer')\n+            referer = _get_referer(request)\n             logger.debug(\n                 'File (uptodate): Downloaded %(medianame)s from %(request)s '\n                 'referred in <%(referer)s>',\n@@ -225,7 +225,7 @@ class FilesPipeline(MediaPipeline):\n \n     def media_failed(self, failure, request, info):\n         if not isinstance(failure.value, IgnoreRequest):\n-            referer = request.headers.get('Referer')\n+            referer = _get_referer(request)\n             logger.warning(\n                 'File (unknown-error): Error downloading %(medianame)s from '\n                 '%(request)s referred in <%(referer)s>: %(exception)s',\n@@ -237,7 +237,7 @@ class FilesPipeline(MediaPipeline):\n         raise FileException\n \n     def media_downloaded(self, response, request, info):\n-        referer = request.headers.get('Referer')\n+        referer = _get_referer(request)\n \n         if response.status != 200:\n             logger.warning(\n@@ -339,3 +339,11 @@ class FilesPipeline(MediaPipeline):\n     def file_key(self, url):\n         return self.file_path(url)\n     file_key._base = True\n+\n+\n+def _get_referer(request):\n+    \"\"\" Return Referer HTTP header suitable for logging \"\"\"\n+    referrer = request.headers.get('Referer')\n+    if referrer is None:\n+        return referrer\n+    return to_native_str(referrer, errors='replace')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8d45b3c4810cb5304ba1193b45697a0df1157326", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 7 | Churn Cumulative: 143 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -79,7 +79,7 @@ def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n-    s += b\"Host: \" + to_bytes(parsed.hostname) + b\"\\r\\n\"\n+    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\"\n\n@@ -71,5 +71,10 @@ class UtilsRequestTest(unittest.TestCase):\n         r1 = Request(\"http://www.example.com\", method='POST', headers={\"Content-type\": b\"text/html\"}, body=b\"Some body\")\n         self.assertEqual(request_httprepr(r1), b'POST / HTTP/1.1\\r\\nHost: www.example.com\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n \n+    def test_request_httprepr_for_non_http_request(self):\n+        # the representation is not important but it must not fail.\n+        request_httprepr(Request(\"file:///tmp/foo.txt\"))\n+        request_httprepr(Request(\"ftp://localhost/tmp/foo.txt\"))\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#accd28cc71e20a194c7e2497b395cd3b0910c7e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 88 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 13 | Methods Changed: 8 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 97 | Churn Cumulative: 171 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,15 +10,17 @@ alias to object in that case).\n \"\"\"\n \n from __future__ import print_function\n-import weakref, os, six\n-from collections import defaultdict\n+import weakref\n from time import time\n from operator import itemgetter\n+from collections import defaultdict\n+import six\n+\n \n NoneType = type(None)\n-\n live_refs = defaultdict(weakref.WeakKeyDictionary)\n \n+\n class object_ref(object):\n     \"\"\"Inherit from this class (instead of object) to a keep a record of live\n     instances\"\"\"\n@@ -30,28 +32,35 @@ class object_ref(object):\n         live_refs[cls][obj] = time()\n         return obj\n \n+\n def format_live_refs(ignore=NoneType):\n-    s = \"Live References\" + os.linesep + os.linesep\n+    s = \"Live References\\n\\n\"\n     now = time()\n-    for cls, wdict in six.iteritems(live_refs):\n+    for cls, wdict in sorted(six.iteritems(live_refs),\n+                             key=lambda x: x[0].__name__):\n         if not wdict:\n             continue\n         if issubclass(cls, ignore):\n             continue\n-        oldest = min(wdict.itervalues())\n-        s += \"%-30s %6d   oldest: %ds ago\" % (cls.__name__, len(wdict), \\\n-            now-oldest) + os.linesep\n+        oldest = min(six.itervalues(wdict))\n+        s += \"%-30s %6d   oldest: %ds ago\\n\" % (\n+            cls.__name__, len(wdict), now - oldest\n+        )\n     return s\n \n+\n def print_live_refs(*a, **kw):\n     print(format_live_refs(*a, **kw))\n \n+\n def get_oldest(class_name):\n     for cls, wdict in six.iteritems(live_refs):\n         if cls.__name__ == class_name:\n-            if wdict:\n+            if not wdict:\n+                break\n             return min(six.iteritems(wdict), key=itemgetter(1))[0]\n \n+\n def iter_all(class_name):\n     for cls, wdict in six.iteritems(live_refs):\n         if cls.__name__ == class_name:\n\n@@ -0,0 +1,70 @@\n+import six\n+import unittest\n+from scrapy.utils import trackref\n+from tests import mock\n+\n+\n+class Foo(trackref.object_ref):\n+    pass\n+\n+\n+class Bar(trackref.object_ref):\n+    pass\n+\n+\n+class TrackrefTestCase(unittest.TestCase):\n+\n+    def setUp(self):\n+        trackref.live_refs.clear()\n+\n+    def test_format_live_refs(self):\n+        o1 = Foo()  # NOQA\n+        o2 = Bar()  # NOQA\n+        o3 = Foo()  # NOQA\n+        self.assertEqual(\n+            trackref.format_live_refs(),\n+            '''\\\n+Live References\n+\n+Bar                                 1   oldest: 0s ago\n+Foo                                 2   oldest: 0s ago\n+''')\n+\n+        self.assertEqual(\n+            trackref.format_live_refs(ignore=Foo),\n+            '''\\\n+Live References\n+\n+Bar                                 1   oldest: 0s ago\n+''')\n+\n+    @mock.patch('sys.stdout', new_callable=six.StringIO)\n+    def test_print_live_refs_empty(self, stdout):\n+        trackref.print_live_refs()\n+        self.assertEqual(stdout.getvalue(), 'Live References\\n\\n\\n')\n+\n+    @mock.patch('sys.stdout', new_callable=six.StringIO)\n+    def test_print_live_refs_with_objects(self, stdout):\n+        o1 = Foo()  # NOQA\n+        trackref.print_live_refs()\n+        self.assertEqual(stdout.getvalue(), '''\\\n+Live References\n+\n+Foo                                 1   oldest: 0s ago\\n\\n''')\n+\n+    def test_get_oldest(self):\n+        o1 = Foo()  # NOQA\n+        o2 = Bar()  # NOQA\n+        o3 = Foo()  # NOQA\n+        self.assertIs(trackref.get_oldest('Foo'), o1)\n+        self.assertIs(trackref.get_oldest('Bar'), o2)\n+        self.assertIsNone(trackref.get_oldest('XXX'))\n+\n+    def test_iter_all(self):\n+        o1 = Foo()  # NOQA\n+        o2 = Bar()  # NOQA\n+        o3 = Foo()  # NOQA\n+        self.assertEqual(\n+            set(trackref.iter_all('Foo')),\n+            {o1, o3},\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#666ebfa1d97264bc4e6adb78fe4ce1a9ea15cc1f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 754 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -5,6 +5,7 @@ from time import time\n from datetime import datetime\n from collections import deque\n \n+import six\n from twisted.internet import reactor, defer, task\n \n from scrapy.utils.defer import mustbe_deferred\n@@ -188,7 +189,7 @@ class Downloader(object):\n \n     def close(self):\n         self._slot_gc_loop.stop()\n-        for slot in self.slots.itervalues():\n+        for slot in six.itervalues(self.slots):\n             slot.close()\n \n     def _slot_gc(self, age=60):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8c7997083fc1455daeda8d13d8037325f4b74909", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 60 | Churn Cumulative: 312 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,8 +11,10 @@ from scrapy import signals\n class DownloadHandlers(object):\n \n     def __init__(self, crawler):\n-        self._handlers = {}\n-        self._notconfigured = {}\n+        self._crawler_settings = crawler.settings\n+        self._schemes = {} # stores acceptable schemes on instancing\n+        self._handlers = {} # stores instanced handlers for schemes\n+        self._notconfigured = {} # remembers failed handlers\n         handlers = crawler.settings.get('DOWNLOAD_HANDLERS_BASE')\n         handlers.update(crawler.settings.get('DOWNLOAD_HANDLERS', {}))\n         for scheme, clspath in six.iteritems(handlers):\n@@ -20,25 +22,40 @@ class DownloadHandlers(object):\n             # component (extension, middleware, etc).\n             if clspath is None:\n                 continue\n-            cls = load_object(clspath)\n-            try:\n-                dh = cls(crawler.settings)\n-            except NotConfigured as ex:\n-                self._notconfigured[scheme] = str(ex)\n-            else:\n-                self._handlers[scheme] = dh\n+            self._schemes[scheme] = clspath\n \n         crawler.signals.connect(self._close, signals.engine_stopped)\n \n+    def _get_handler(self, scheme):\n+        \"\"\"Lazy-load the downloadhandler for a scheme\n+        only on the first request for that scheme.\n+        \"\"\"\n+        if scheme in self._handlers:\n+            return self._handlers[scheme]\n+        if scheme in self._notconfigured:\n+            return None\n+        if scheme not in self._schemes:\n+            self._notconfigured[scheme] = \\\n+                    'no handler available for that scheme'\n+            return None\n+\n+        dhcls = load_object(self._schemes[scheme])\n+        try:\n+            dh = dhcls(self._crawler_settings)\n+        except NotConfigured as ex:\n+            self._notconfigured[scheme] = str(ex)\n+            return None\n+        else:\n+            self._handlers[scheme] = dh\n+        return self._handlers[scheme]\n+\n     def download_request(self, request, spider):\n         scheme = urlparse_cached(request).scheme\n-        try:\n-            handler = self._handlers[scheme].download_request\n-        except KeyError:\n-            msg = self._notconfigured.get(scheme, \\\n-                    'no handler available for that scheme')\n-            raise NotSupported(\"Unsupported URL scheme '%s': %s\" % (scheme, msg))\n-        return handler(request, spider)\n+        handler = self._get_handler(scheme)\n+        if not handler:\n+            raise NotSupported(\"Unsupported URL scheme '%s': %s\" %\n+                    (scheme, self._notconfigured[scheme]))\n+        return handler.download_request(request, spider)\n \n     @defer.inlineCallbacks\n     def _close(self, *_a, **_kw):\n\n@@ -52,6 +52,9 @@ class LoadTestCase(unittest.TestCase):\n         handlers = {'scheme': 'tests.test_downloader_handlers.DummyDH'}\n         crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})\n         dh = DownloadHandlers(crawler)\n+        self.assertIn('scheme', dh._schemes)\n+        for scheme in handlers: # force load handlers\n+            dh._get_handler(scheme)\n         self.assertIn('scheme', dh._handlers)\n         self.assertNotIn('scheme', dh._notconfigured)\n \n@@ -59,6 +62,9 @@ class LoadTestCase(unittest.TestCase):\n         handlers = {'scheme': 'tests.test_downloader_handlers.OffDH'}\n         crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})\n         dh = DownloadHandlers(crawler)\n+        self.assertIn('scheme', dh._schemes)\n+        for scheme in handlers: # force load handlers\n+            dh._get_handler(scheme)\n         self.assertNotIn('scheme', dh._handlers)\n         self.assertIn('scheme', dh._notconfigured)\n \n@@ -66,8 +72,11 @@ class LoadTestCase(unittest.TestCase):\n         handlers = {'scheme': None}\n         crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})\n         dh = DownloadHandlers(crawler)\n+        self.assertNotIn('scheme', dh._schemes)\n+        for scheme in handlers: # force load handlers\n+            dh._get_handler(scheme)\n         self.assertNotIn('scheme', dh._handlers)\n-        self.assertNotIn('scheme', dh._notconfigured)\n+        self.assertIn('scheme', dh._notconfigured)\n \n \n class FileTestCase(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d3804b3439d72e718cf68ff9fc1798bb9232625c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 152 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n \"\"\"Download handlers for different schemes\"\"\"\n \n+import logging\n from twisted.internet import defer\n import six\n from scrapy.exceptions import NotSupported, NotConfigured\n@@ -8,6 +9,9 @@ from scrapy.utils.misc import load_object\n from scrapy import signals\n \n \n+logger = logging.getLogger(__name__)\n+\n+\n class DownloadHandlers(object):\n \n     def __init__(self, crawler):\n@@ -39,12 +43,16 @@ class DownloadHandlers(object):\n                     'no handler available for that scheme'\n             return None\n \n-        dhcls = load_object(self._schemes[scheme])\n         try:\n+            dhcls = load_object(self._schemes[scheme])\n             dh = dhcls(self._crawler_settings)\n         except NotConfigured as ex:\n             self._notconfigured[scheme] = str(ex)\n             return None\n+        except Exception as ex:\n+            logger.exception()\n+            self._notconfigured[scheme] = str(ex)\n+            return None\n         else:\n             self._handlers[scheme] = dh\n         return self._handlers[scheme]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#15ccf79cad46385041fb8c3a7bf697d9d7ee7c55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 158 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -43,14 +43,16 @@ class DownloadHandlers(object):\n                     'no handler available for that scheme'\n             return None\n \n+        path = self._schemes[scheme]\n         try:\n-            dhcls = load_object(self._schemes[scheme])\n+            dhcls = load_object(path)\n             dh = dhcls(self._crawler_settings)\n         except NotConfigured as ex:\n             self._notconfigured[scheme] = str(ex)\n             return None\n         except Exception as ex:\n-            logger.exception()\n+            logger.exception('Loading \"{}\" for scheme \"{}\" handler'\\\n+                             .format(path, scheme))\n             self._notconfigured[scheme] = str(ex)\n             return None\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb44152a585a54c5c2ee27c9eaed32aa2f9b15a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 163 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -39,8 +39,7 @@ class DownloadHandlers(object):\n         if scheme in self._notconfigured:\n             return None\n         if scheme not in self._schemes:\n-            self._notconfigured[scheme] = \\\n-                    'no handler available for that scheme'\n+            self._notconfigured[scheme] = 'no handler available for that scheme'\n             return None\n \n         path = self._schemes[scheme]\n@@ -51,7 +50,7 @@ class DownloadHandlers(object):\n             self._notconfigured[scheme] = str(ex)\n             return None\n         except Exception as ex:\n-            logger.exception('Loading \"{}\" for scheme \"{}\" handler'\\\n+            logger.exception('Loading \"{}\" for scheme \"{}\" handler'\n                              .format(path, scheme))\n             self._notconfigured[scheme] = str(ex)\n             return None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8738521d0a221e8800909121660746373dc5f823", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 105 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -34,6 +34,7 @@ class object_ref(object):\n \n \n def format_live_refs(ignore=NoneType):\n+    \"\"\"Return a tabular representation of tracked objects\"\"\"\n     s = \"Live References\\n\\n\"\n     now = time()\n     for cls, wdict in sorted(six.iteritems(live_refs),\n@@ -50,10 +51,12 @@ def format_live_refs(ignore=NoneType):\n \n \n def print_live_refs(*a, **kw):\n+    \"\"\"Print tracked objects\"\"\"\n     print(format_live_refs(*a, **kw))\n \n \n def get_oldest(class_name):\n+    \"\"\"Get the oldest object for a specific class name\"\"\"\n     for cls, wdict in six.iteritems(live_refs):\n         if cls.__name__ == class_name:\n             if not wdict:\n@@ -62,6 +65,7 @@ def get_oldest(class_name):\n \n \n def iter_all(class_name):\n+    \"\"\"Iterate over all objects of the same class by its class name\"\"\"\n     for cls, wdict in six.iteritems(live_refs):\n         if cls.__name__ == class_name:\n             return six.iterkeys(wdict)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7717501ab28fa2760da6b1b9be998eb589988821", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 172 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,7 +15,7 @@ logger = logging.getLogger(__name__)\n class DownloadHandlers(object):\n \n     def __init__(self, crawler):\n-        self._crawler_settings = crawler.settings\n+        self._crawler = crawler\n         self._schemes = {}  # stores acceptable schemes on instancing\n         self._handlers = {}  # stores instanced handlers for schemes\n         self._notconfigured = {}  # remembers failed handlers\n@@ -45,13 +45,14 @@ class DownloadHandlers(object):\n         path = self._schemes[scheme]\n         try:\n             dhcls = load_object(path)\n-            dh = dhcls(self._crawler_settings)\n+            dh = dhcls(self._crawler.settings)\n         except NotConfigured as ex:\n             self._notconfigured[scheme] = str(ex)\n             return None\n         except Exception as ex:\n-            logger.exception('Loading \"{}\" for scheme \"{}\" handler'\n-                             .format(path, scheme))\n+            logger.error('Loading \"%(clspath)s\" for scheme \"%(scheme)s\"',\n+                         {\"clspath\": path, \"scheme\": scheme},\n+                         exc_info=True,  extra={'crawler': self._crawler})\n             self._notconfigured[scheme] = str(ex)\n             return None\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
