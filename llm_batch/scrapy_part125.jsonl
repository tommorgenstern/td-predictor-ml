{"custom_id": "scrapy#61cc95df7c586129f6d006fa9f52a3915f4f30d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1002 | Files Changed: 6 | Hunks: 6 | Methods Changed: 48 | Complexity Δ (Sum/Max): -81/0 | Churn Δ: 1002 | Churn Cumulative: 2044 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 13 | DMM Complexity: 0.05819295558958652\n\nDIFF:\n@@ -1,94 +0,0 @@\n-from twisted.trial import unittest\n-\n-from scrapy.http import Request\n-from scrapy.http import Response\n-\n-from scrapy.contrib_exp.crawlspider.matchers import BaseMatcher\n-from scrapy.contrib_exp.crawlspider.matchers import UrlMatcher\n-from scrapy.contrib_exp.crawlspider.matchers import UrlRegexMatcher\n-from scrapy.contrib_exp.crawlspider.matchers import UrlListMatcher\n-\n-import re\n-\n-class MatchersTest(unittest.TestCase):\n-\n-    def setUp(self):\n-        pass\n-\n-    def test_base_matcher(self):\n-        matcher = BaseMatcher()\n-\n-        request = Request('http://example.com')\n-        response = Response('http://example.com')\n-\n-        self.assertTrue(matcher.matches_request(request))\n-        self.assertTrue(matcher.matches_response(response))\n-\n-    def test_url_matcher(self):\n-        matcher = UrlMatcher('http://example.com')\n-\n-        request = Request('http://example.com')\n-        response = Response('http://example.com')\n-\n-        self.failUnless(matcher.matches_request(request))\n-        self.failUnless(matcher.matches_request(response))\n-\n-        request = Request('http://example2.com')\n-        response = Response('http://example2.com')\n-\n-        self.failIf(matcher.matches_request(request))\n-        self.failIf(matcher.matches_request(response))\n-\n-    def test_url_regex_matcher(self):\n-        matcher = UrlRegexMatcher(r'sample')\n-        urls = (\n-            'http://example.com/sample1.html',\n-            'http://example.com/sample2.html',\n-            'http://example.com/sample3.html',\n-            'http://example.com/sample4.html',\n-            )\n-        for url in urls:\n-            request, response = Request(url), Response(url)\n-            self.failUnless(matcher.matches_request(request))\n-            self.failUnless(matcher.matches_response(response))\n-\n-        matcher = UrlRegexMatcher(r'sample_fail')\n-        for url in urls:\n-            request, response = Request(url), Response(url)\n-            self.failIf(matcher.matches_request(request))\n-            self.failIf(matcher.matches_response(response))\n-\n-        matcher = UrlRegexMatcher(r'SAMPLE\\d+', re.IGNORECASE)\n-        for url in urls:\n-            request, response = Request(url), Response(url)\n-            self.failUnless(matcher.matches_request(request))\n-            self.failUnless(matcher.matches_response(response))\n-\n-    def test_url_list_matcher(self):\n-        urls = (\n-            'http://example.com/sample1.html',\n-            'http://example.com/sample2.html',\n-            'http://example.com/sample3.html',\n-            'http://example.com/sample4.html',\n-            )\n-        urls2 = (\n-            'http://example.com/sample5.html',\n-            'http://example.com/sample6.html',\n-            'http://example.com/sample7.html',\n-            'http://example.com/sample8.html',\n-            'http://example.com/',\n-            )\n-        matcher = UrlListMatcher(urls)\n-\n-        # match urls\n-        for url in urls:\n-            request, response = Request(url), Response(url)\n-            self.failUnless(matcher.matches_request(request))\n-            self.failUnless(matcher.matches_response(response))\n-\n-        # non-match urls\n-        for url in urls2:\n-            request, response = Request(url), Response(url)\n-            self.failIf(matcher.matches_request(request))\n-            self.failIf(matcher.matches_response(response))\n-\n\n@@ -1,156 +0,0 @@\n-from twisted.trial import unittest\n-\n-from scrapy.http import Request\n-from scrapy.http import HtmlResponse\n-from scrapy.tests import get_testdata\n-\n-from scrapy.contrib_exp.crawlspider.reqext import BaseSgmlRequestExtractor\n-from scrapy.contrib_exp.crawlspider.reqext import SgmlRequestExtractor\n-from scrapy.contrib_exp.crawlspider.reqext import XPathRequestExtractor\n-\n-class AbstractRequestExtractorTest(unittest.TestCase):\n-\n-    def _requests_equals(self, list1, list2):\n-        \"\"\"Compares request's urls and link_text\"\"\"\n-        for (r1, r2) in zip(list1, list2):\n-            if r1.url != r2.url:\n-                return False\n-            if r1.meta['link_text'] != r2.meta['link_text']:\n-                return False\n-        # all equal\n-        return True\n-\n-\n-class RequestExtractorTest(AbstractRequestExtractorTest):\n-\n-    def test_basic(self):\n-        base_url = 'http://example.org/somepage/index.html'\n-        html = \"\"\"<html><head><title>Page title<title>\n-        <body><p><a href=\"item/12.html\">Item 12</a></p>\n-        <p><a href=\"/about.html\">About us</a></p>\n-        <img src=\"/logo.png\" alt=\"Company logo (not a link)\" />\n-        <p><a href=\"../othercat.html\">Other category</a></p>\n-        <p><a href=\"/\" /></p></body></html>\"\"\"\n-        requests = [\n-            Request('http://example.org/somepage/item/12.html',\n-                    meta={'link_text': 'Item 12'}),\n-            Request('http://example.org/about.html',\n-                    meta={'link_text': 'About us'}),\n-            Request('http://example.org/othercat.html',\n-                    meta={'link_text': 'Other category'}),\n-            Request('http://example.org/',\n-                    meta={'link_text': ''}),\n-            ]\n-\n-        response = HtmlResponse(base_url, body=html)\n-        reqx = BaseSgmlRequestExtractor() # default: tag=a, attr=href\n-\n-        self.failUnless(\n-            self._requests_equals(requests, reqx.extract_requests(response))\n-            )\n-\n-    def test_base_url(self):\n-        reqx = BaseSgmlRequestExtractor()\n-\n-        html = \"\"\"<html><head><title>Page title<title>\n-        <base href=\"http://otherdomain.com/base/\" />\n-        <body><p><a href=\"item/12.html\">Item 12</a></p>\n-        </body></html>\"\"\"\n-        response = HtmlResponse(\"https://example.org/p/index.html\", body=html)\n-        reqs = reqx.extract_requests(response)\n-        self.failUnless(self._requests_equals( \\\n-            [Request('http://otherdomain.com/base/item/12.html', \\\n-                    meta={'link_text': 'Item 12'})], reqs), reqs)\n-\n-        # base url is an absolute path and relative to host\n-        html = \"\"\"<html><head><title>Page title<title>\n-        <base href=\"/\" />\n-        <body><p><a href=\"item/12.html\">Item 12</a></p>\n-        </body></html>\"\"\"\n-        response = HtmlResponse(\"https://example.org/p/index.html\", body=html)\n-        reqs = reqx.extract_requests(response)\n-        self.failUnless(self._requests_equals( \\\n-            [Request('https://example.org/item/12.html', \\\n-                    meta={'link_text': 'Item 12'})], reqs), reqs)\n-\n-        # base url has no scheme\n-        html = \"\"\"<html><head><title>Page title<title>\n-        <base href=\"//noscheme.com/base/\" />\n-        <body><p><a href=\"item/12.html\">Item 12</a></p>\n-        </body></html>\"\"\"\n-        response = HtmlResponse(\"https://example.org/p/index.html\", body=html)\n-        reqs = reqx.extract_requests(response)\n-        self.failUnless(self._requests_equals( \\\n-            [Request('https://noscheme.com/base/item/12.html', \\\n-                    meta={'link_text': 'Item 12'})], reqs), reqs)\n-\n-    def test_extraction_encoding(self):\n-        #TODO: use own fixtures\n-        body = get_testdata('link_extractor', 'linkextractor_noenc.html')\n-        response_utf8 = HtmlResponse(url='http://example.com/utf8', body=body,\n-                        headers={'Content-Type': ['text/html; charset=utf-8']})\n-        response_noenc = HtmlResponse(url='http://example.com/noenc',\n-                            body=body)\n-        body = get_testdata('link_extractor', 'linkextractor_latin1.html')\n-        response_latin1 = HtmlResponse(url='http://example.com/latin1',\n-                            body=body)\n-\n-        reqx = BaseSgmlRequestExtractor()\n-        self.failUnless(\n-            self._requests_equals(\n-                reqx.extract_requests(response_utf8),\n-                [ Request(url='http://example.com/sample_%C3%B1.html',\n-                          meta={'link_text': ''}),\n-                  Request(url='http://example.com/sample_%E2%82%AC.html',\n-                          meta={'link_text':\n-                                'sample \\xe2\\x82\\xac text'.decode('utf-8')}) ]\n-                )\n-            )\n-\n-        self.failUnless(\n-            self._requests_equals(\n-                reqx.extract_requests(response_noenc),\n-                [ Request(url='http://example.com/sample_%C3%B1.html',\n-                          meta={'link_text': ''}),\n-                  Request(url='http://example.com/sample_%E2%82%AC.html',\n-                          meta={'link_text':\n-                                'sample \\xe2\\x82\\xac text'.decode('utf-8')}) ]\n-                )\n-            )\n-\n-        self.failUnless(\n-            self._requests_equals(\n-                reqx.extract_requests(response_latin1),\n-                [ Request(url='http://example.com/sample_%F1.html',\n-                          meta={'link_text': ''}),\n-                  Request(url='http://example.com/sample_%E1.html',\n-                          meta={'link_text':\n-                                'sample \\xe1 text'.decode('latin1')}) ]\n-                )\n-            )\n-\n-\n-class SgmlRequestExtractorTest(AbstractRequestExtractorTest):\n-    pass\n-\n-\n-class XPathRequestExtractorTest(AbstractRequestExtractorTest):\n-\n-    def setUp(self):\n-        # TODO: use own fixtures\n-        body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n-        self.response = HtmlResponse(url='http://example.com/index', body=body)\n-\n-\n-    def test_restrict_xpaths(self):\n-        reqx = XPathRequestExtractor('//div[@id=\"subwrapper\"]')\n-        self.failUnless(\n-            self._requests_equals(\n-                reqx.extract_requests(self.response),\n-                [ Request(url='http://example.com/sample1.html',\n-                          meta={'link_text': ''}),\n-                  Request(url='http://example.com/sample2.html',\n-                          meta={'link_text': 'sample 2'}) ]\n-                )\n-            )\n-\n\n@@ -1,124 +0,0 @@\n-from twisted.internet import defer\n-from twisted.trial import unittest\n-\n-from scrapy.http import Request\n-from scrapy.http import HtmlResponse\n-from scrapy.utils.python import equal_attributes\n-\n-from scrapy.contrib_exp.crawlspider.reqext import SgmlRequestExtractor\n-from scrapy.contrib_exp.crawlspider.reqgen import RequestGenerator\n-from scrapy.contrib_exp.crawlspider.reqproc import Canonicalize\n-from scrapy.contrib_exp.crawlspider.reqproc import FilterDomain\n-from scrapy.contrib_exp.crawlspider.reqproc import FilterUrl\n-from scrapy.contrib_exp.crawlspider.reqproc import FilterDupes\n-\n-class RequestGeneratorTest(unittest.TestCase):\n-\n-    def setUp(self):\n-        url = 'http://example.org/somepage/index.html'\n-        html = \"\"\"<html><head><title>Page title<title>\n-        <body><p><a href=\"item/12.html\">Item 12</a></p>\n-        <p><a href=\"/about.html\">About us</a></p>\n-        <img src=\"/logo.png\" alt=\"Company logo (not a link)\" />\n-        <p><a href=\"../othercat.html\">Other category</a></p>\n-        <p><a href=\"/\" /></p></body></html>\"\"\"\n-\n-        self.response = HtmlResponse(url, body=html)\n-        self.deferred = defer.Deferred()\n-        self.requests = [\n-            Request('http://example.org/somepage/item/12.html',\n-                    meta={'link_text': 'Item 12'}),\n-            Request('http://example.org/about.html',\n-                    meta={'link_text': 'About us'}),\n-            Request('http://example.org/othercat.html',\n-                    meta={'link_text': 'Other category'}),\n-            Request('http://example.org/',\n-                    meta={'link_text': ''}),\n-            ]\n-\n-    def _equal_requests_list(self, list1, list2):\n-        list1 = list(list1)\n-        list2 = list(list2)\n-        if not len(list1) == len(list2):\n-            return False\n-\n-        for (req1, req2) in zip(list1, list2):\n-            if not equal_attributes(req1, req2, ['url']):\n-                return False\n-        return True\n-\n-    def test_basic(self):\n-        reqgen = RequestGenerator([], [], callback=self.deferred)\n-        # returns generator\n-        requests = reqgen.generate_requests(self.response)\n-        self.failUnlessEqual(list(requests), [])\n-\n-    def test_request_extractor(self):\n-        extractors = [\n-            SgmlRequestExtractor()\n-            ]\n-\n-        # extract all requests\n-        reqgen = RequestGenerator(extractors, [], callback=self.deferred)\n-        requests = reqgen.generate_requests(self.response)\n-        self.failUnless(self._equal_requests_list(requests, self.requests))\n-\n-    def test_request_processor(self):\n-        extractors = [\n-            SgmlRequestExtractor()\n-            ]\n-\n-        processors = [\n-            Canonicalize(),\n-            FilterDupes(),\n-            ]\n-\n-        reqgen = RequestGenerator(extractors, processors, callback=self.deferred)\n-        requests = reqgen.generate_requests(self.response)\n-        self.failUnless(self._equal_requests_list(requests, self.requests))\n-\n-        # filter domain\n-        processors = [\n-            Canonicalize(),\n-            FilterDupes(),\n-            FilterDomain(deny='example.org'),\n-            ]\n-\n-        reqgen = RequestGenerator(extractors, processors, callback=self.deferred)\n-        requests = reqgen.generate_requests(self.response)\n-        self.failUnlessEqual(list(requests), [])\n-\n-        # filter url\n-        processors = [\n-            Canonicalize(),\n-            FilterDupes(),\n-            FilterUrl(deny=(r'about', r'othercat')),\n-            ]\n-\n-        reqgen = RequestGenerator(extractors, processors, callback=self.deferred)\n-        requests = reqgen.generate_requests(self.response)\n-\n-        self.failUnless(self._equal_requests_list(requests, [\n-                Request('http://example.org/somepage/item/12.html',\n-                        meta={'link_text': 'Item 12'}),\n-                Request('http://example.org/',\n-                        meta={'link_text': ''}),\n-                ]))\n-\n-        processors = [\n-            Canonicalize(),\n-            FilterDupes(),\n-            FilterUrl(allow=r'/somepage/'),\n-            ]\n-\n-        reqgen = RequestGenerator(extractors, processors, callback=self.deferred)\n-        requests = reqgen.generate_requests(self.response)\n-\n-        self.failUnless(self._equal_requests_list(requests, [\n-                Request('http://example.org/somepage/item/12.html',\n-                        meta={'link_text': 'Item 12'}),\n-                ]))\n-\n- \n- \n-        \n\n@@ -1,144 +0,0 @@\n-from twisted.trial import unittest\n-\n-from scrapy.http import Request\n-\n-from scrapy.contrib_exp.crawlspider.reqproc import Canonicalize\n-from scrapy.contrib_exp.crawlspider.reqproc import FilterDomain\n-from scrapy.contrib_exp.crawlspider.reqproc import FilterUrl\n-from scrapy.contrib_exp.crawlspider.reqproc import FilterDupes\n-\n-import copy\n-\n-class RequestProcessorsTest(unittest.TestCase):\n-\n-    def test_canonicalize_requests(self):\n-        urls = [\n-            'http://example.com/do?&b=1&a=2&c=3',\n-            'http://example.com/do?123,&q=a space',\n-            ]\n-        urls_after = [\n-            'http://example.com/do?a=2&b=1&c=3',\n-            'http://example.com/do?123%2C=&q=a+space',\n-            ]\n-\n-        proc = Canonicalize()\n-        results = [req.url for req in proc(Request(url) for url in urls)]\n-        self.failUnlessEquals(results, urls_after)\n-\n-    def test_unique_requests(self):\n-        urls = [\n-            'http://example.com/sample1.html',\n-            'http://example.com/sample2.html',\n-            'http://example.com/sample3.html',\n-            'http://example.com/sample1.html',\n-            'http://example.com/sample2.html',\n-            ]\n-        urls_unique = [\n-            'http://example.com/sample1.html',\n-            'http://example.com/sample2.html',\n-            'http://example.com/sample3.html',\n-            ]\n-\n-        proc = FilterDupes()\n-        results = [req.url for req in proc(Request(url) for url in urls)]\n-        self.failUnlessEquals(results, urls_unique)\n-\n-        # Check custom attributes\n-        requests = [\n-            Request('http://example.com', method='GET'),\n-            Request('http://example.com', method='POST'),\n-            ]\n-        proc = FilterDupes('url', 'method')\n-        self.failUnlessEqual(len(list(proc(requests))), 2)\n-\n-        proc = FilterDupes('url')\n-        self.failUnlessEqual(len(list(proc(requests))), 1)\n-\n-    def test_filter_domain(self):\n-        urls = [\n-            'http://blah1.com/index',\n-            'http://blah2.com/index',\n-            'http://blah1.com/section',\n-            'http://blah2.com/section',\n-            ]\n-\n-        proc = FilterDomain(allow=('blah1.com'), deny=('blah2.com'))\n-        filtered = [req.url for req in proc(Request(url) for url in urls)]\n-        self.failUnlessEquals(filtered, [\n-                    'http://blah1.com/index',\n-                    'http://blah1.com/section',\n-                ])\n-\n-        proc = FilterDomain(deny=('blah1.com', 'blah2.com'))\n-        filtered = [req.url for req in proc(Request(url) for url in urls)]\n-        self.failUnlessEquals(filtered, [])\n-\n-        proc = FilterDomain(allow=('blah1.com', 'blah2.com'))\n-        filtered = [req.url for req in proc(Request(url) for url in urls)]\n-        self.failUnlessEquals(filtered, urls)\n-\n-    def test_filter_url(self):\n-        urls = [\n-            'http://blah1.com/index',\n-            'http://blah2.com/index',\n-            'http://blah1.com/section',\n-            'http://blah2.com/section',\n-            ]\n-\n-        proc = FilterUrl(allow=(r'blah1'), deny=(r'blah2'))\n-        filtered = [req.url for req in proc(Request(url) for url in urls)]\n-        self.failUnlessEquals(filtered, [\n-                    'http://blah1.com/index',\n-                    'http://blah1.com/section',\n-                ])\n-\n-        proc = FilterUrl(deny=('blah1', 'blah2'))\n-        filtered = [req.url for req in proc(Request(url) for url in urls)]\n-        self.failUnlessEquals(filtered, [])\n-\n-        proc = FilterUrl(allow=('index$', 'section$'))\n-        filtered = [req.url for req in proc(Request(url) for url in urls)]\n-        self.failUnlessEquals(filtered, urls)\n-\n-\n-\n-    def test_all_processors(self):\n-        urls = [\n-            'http://example.com/sample1.html',\n-            'http://example.com/sample2.html',\n-            'http://example.com/sample3.html',\n-            'http://example.com/sample1.html',\n-            'http://example.com/sample2.html',\n-            'http://example.com/do?&b=1&a=2&c=3',\n-            'http://example.com/do?123,&q=a space',\n-            ]\n-        urls_processed = [\n-            'http://example.com/sample1.html',\n-            'http://example.com/sample2.html',\n-            'http://example.com/sample3.html',\n-            'http://example.com/do?a=2&b=1&c=3',\n-            'http://example.com/do?123%2C=&q=a+space',\n-            ]\n-\n-        processors = [\n-            Canonicalize(),\n-            FilterDupes(),\n-            ]\n-\n-        def _process(requests):\n-            \"\"\"Apply all processors\"\"\"\n-            # copy list\n-            processed = [copy.copy(req) for req in requests]\n-            for proc in processors:\n-                processed = proc(processed)\n-            return processed\n-\n-        # empty requests\n-        results1 = [r.url for r in _process([])]\n-        self.failUnlessEquals(results1, [])\n-\n-        # try urls\n-        requests = (Request(url) for url in urls)\n-        results2 = [r.url for r in _process(requests)]\n-        self.failUnlessEquals(results2, urls_processed)\n-\n\n@@ -1,262 +0,0 @@\n-from twisted.trial import unittest\n-\n-from scrapy.http import HtmlResponse\n-from scrapy.spider import BaseSpider\n-from scrapy.contrib_exp.crawlspider.matchers import BaseMatcher\n-from scrapy.contrib_exp.crawlspider.matchers import UrlMatcher\n-from scrapy.contrib_exp.crawlspider.matchers import UrlRegexMatcher\n-\n-from scrapy.contrib_exp.crawlspider.rules import CompiledRule\n-from scrapy.contrib_exp.crawlspider.rules import Rule\n-from scrapy.contrib_exp.crawlspider.rules import RulesManager\n-\n-from functools import partial\n-\n-class RuleInitializationTest(unittest.TestCase):\n-\n-    def test_fail_if_rule_null(self):\n-        # fail on empty rule\n-        self.failUnlessRaises(ValueError, Rule)\n-        self.failUnlessRaises(ValueError, Rule,\n-                                **dict(callback=None, follow=None))\n-        self.failUnlessRaises(ValueError, Rule,\n-                                **dict(callback=None, follow=False))\n-\n-    def test_minimal_arguments_to_instantiation(self):\n-        # not fail if callback set\n-        self.failUnless(Rule(callback=lambda: True))\n-        # not fail if follow set\n-        self.failUnless(Rule(follow=True))\n-\n-    def test_validate_default_attributes(self):\n-        # test null Rule\n-        rule = Rule(follow=True)\n-        self.failUnlessEqual(None, rule.matcher)\n-        self.failUnlessEqual(None, rule.callback)\n-        self.failUnlessEqual({}, rule.cb_kwargs)\n-        # follow default False\n-        self.failUnlessEqual(True, rule.follow)\n-\n-    def test_validate_attributes_set(self):\n-        matcher = BaseMatcher()\n-        callback = lambda: True\n-        rule = Rule(matcher, callback, True, a=1)\n-        # test attributes\n-        self.failUnlessEqual(matcher, rule.matcher)\n-        self.failUnlessEqual(callback, rule.callback)\n-        self.failUnlessEqual({'a': 1}, rule.cb_kwargs)\n-        self.failUnlessEqual(True, rule.follow)\n-\n-class CompiledRuleInitializationTest(unittest.TestCase):\n-\n-    def test_fail_on_invalid_matcher(self):\n-        # pass with valid matcher\n-        self.failUnless(CompiledRule(BaseMatcher()),\n-                \"Failed CompiledRule instantiation\")\n-\n-        # at least needs valid matcher\n-        self.assertRaises(AssertionError, CompiledRule, None)\n-        self.assertRaises(AssertionError, CompiledRule, False)\n-        self.assertRaises(AssertionError, CompiledRule, True)\n-\n-    def test_fail_on_invalid_callback(self):\n-        # pass with valid callback\n-        callback = lambda: True\n-        self.failUnless(CompiledRule(BaseMatcher(), callback))\n-        # pass with callback none\n-        self.failUnless(CompiledRule(BaseMatcher(), None))\n-\n-        # assert on invalid callback\n-        self.assertRaises(AssertionError, CompiledRule, BaseMatcher(),\n-                          'myfunc')\n-\n-        # numeric variable\n-        var = 123\n-        self.assertRaises(AssertionError, CompiledRule, BaseMatcher(),\n-                          var)\n-\n-        class A:\n-            pass\n-\n-        # random instance\n-        self.assertRaises(AssertionError, CompiledRule, BaseMatcher(),\n-                          A())\n-\n-\n-    def test_fail_on_invalid_follow_value(self):\n-        callback = lambda: True\n-        matcher = BaseMatcher()\n-        # pass bool\n-        self.failUnless(CompiledRule(matcher, callback, True))\n-        self.failUnless(CompiledRule(matcher, callback, False))\n-\n-        # assert with non-bool\n-        self.assertRaises(AssertionError, CompiledRule, matcher,\n-                          callback, None)\n-        self.assertRaises(AssertionError, CompiledRule, matcher,\n-                          callback, 1)\n-\n-    def test_validate_default_attributes(self):\n-        callback = lambda: True\n-        matcher = BaseMatcher()\n-        rule = CompiledRule(matcher, callback, True)\n-\n-        # test attributes\n-        self.failUnlessEqual(matcher, rule.matcher)\n-        self.failUnlessEqual(callback, rule.callback)\n-        self.failUnlessEqual(True, rule.follow)\n-\n-\n-class RulesTest(unittest.TestCase):\n-    def test_rules_manager_basic(self):\n-        spider = BaseSpider('foo')\n-        response1 = HtmlResponse('http://example.org')\n-        response2 = HtmlResponse('http://othersite.org')\n-        rulesman = RulesManager([], spider)\n-\n-        # should return none\n-        self.failIf(rulesman.get_rule_from_response(response1))\n-        self.failIf(rulesman.get_rule_from_response(response2))\n-\n-        # rules manager with match-all rule\n-        rulesman = RulesManager([\n-                Rule(BaseMatcher(), follow=True),\n-                ], spider)\n-\n-        # returns CompiledRule\n-        rule1 = rulesman.get_rule_from_response(response1)\n-        rule2 = rulesman.get_rule_from_response(response2)\n-\n-        self.failUnless(isinstance(rule1, CompiledRule))\n-        self.failUnless(isinstance(rule2, CompiledRule))\n-        self.assert_(rule1 is rule2)\n-        self.failUnlessEqual(rule1.callback, None)\n-        self.failUnlessEqual(rule1.follow, True)\n-\n-    def test_rules_manager_empty_rule(self):\n-        spider = BaseSpider('foo')\n-        response = HtmlResponse('http://example.org')\n-\n-        rulesman = RulesManager([Rule(follow=True)], spider)\n-\n-        rule = rulesman.get_rule_from_response(response)\n-        # default matcher if None: BaseMatcher\n-        self.failUnless(isinstance(rule.matcher, BaseMatcher))\n-\n-    def test_rules_manager_default_matcher(self):\n-        spider = BaseSpider('foo')\n-        response = HtmlResponse('http://example.org')\n-        callback = lambda x: None\n-\n-        rulesman = RulesManager([\n-            Rule('http://example.org', callback),\n-                ], spider, default_matcher=UrlMatcher)\n-\n-        rule = rulesman.get_rule_from_response(response)\n-        self.failUnless(isinstance(rule.matcher, UrlMatcher))\n-\n-    def test_rules_manager_matchers(self):\n-        spider = BaseSpider('foo')\n-        response1 = HtmlResponse('http://example.org')\n-        response2 = HtmlResponse('http://othersite.org')\n-\n-        urlmatcher = UrlMatcher('http://example.org')\n-        basematcher = BaseMatcher()\n-        # callback needed for Rule\n-        callback = lambda x: None\n-\n-        # test fail matcher resolve\n-        self.assertRaises(ValueError, RulesManager,\n-                          [Rule(False, callback)], spider)\n-        self.assertRaises(ValueError, RulesManager,\n-                          [Rule(spider, callback)], spider)\n-\n-        rulesman = RulesManager([\n-            Rule(urlmatcher, callback),\n-            Rule(basematcher, callback),\n-            ], spider)\n-\n-        # response1 matches example.org\n-        rule1 = rulesman.get_rule_from_response(response1)\n-        # response2 is catch by BaseMatcher()\n-        rule2 = rulesman.get_rule_from_response(response2)\n-\n-        self.failUnlessEqual(rule1.matcher, urlmatcher)\n-        self.failUnlessEqual(rule2.matcher, basematcher)\n-\n-        # reverse order. BaseMatcher should match all\n-        rulesman = RulesManager([\n-            Rule(basematcher, callback),\n-            Rule(urlmatcher, callback),\n-            ], spider)\n-\n-        rule1 = rulesman.get_rule_from_response(response1)\n-        rule2 = rulesman.get_rule_from_response(response2)\n-\n-        self.failUnlessEqual(rule1.matcher, basematcher)\n-        self.failUnlessEqual(rule2.matcher, basematcher)\n-        self.failUnless(rule1 is rule2)\n-\n-    def test_rules_manager_callbacks(self):\n-        mycallback = lambda: True\n-\n-        spider = BaseSpider('foo')\n-        spider.parse_item = lambda: True\n-\n-        response1 = HtmlResponse('http://example.org')\n-        response2 = HtmlResponse('http://othersite.org')\n-\n-        rulesman = RulesManager([\n-            Rule('example', mycallback),\n-            Rule('othersite', 'parse_item'),\n-                ], spider, default_matcher=UrlRegexMatcher)\n-\n-        rule1 = rulesman.get_rule_from_response(response1)\n-        rule2 = rulesman.get_rule_from_response(response2)\n-\n-        self.failUnlessEqual(rule1.callback, mycallback)\n-        self.failUnlessEqual(rule2.callback, spider.parse_item)\n-\n-        # fail unknown callback\n-        self.assertRaises(AttributeError, RulesManager, [\n-                            Rule(BaseMatcher(), 'mycallback')\n-                            ], spider)\n-        # fail not callable\n-        spider.not_callable = True\n-        self.assertRaises(AttributeError, RulesManager, [\n-                            Rule(BaseMatcher(), 'not_callable')\n-                            ], spider)\n-\n-\n-    def test_rules_manager_callback_with_arguments(self):\n-        spider = BaseSpider('foo')\n-        response = HtmlResponse('http://example.org')\n-\n-        kwargs = {'a': 1}\n-        \n-        def myfunc(**mykwargs):\n-            return mykwargs\n-        \n-        # verify return validation\n-        self.failUnlessEquals(kwargs, myfunc(**kwargs))\n-\n-        # test callback w/o arguments\n-        rulesman = RulesManager([\n-            Rule(BaseMatcher(), myfunc),\n-            ], spider)\n-        rule = rulesman.get_rule_from_response(response)\n-\n-        # without arguments should return same callback\n-        self.failUnlessEqual(rule.callback, myfunc)\n-\n-        # test callback w/ arguments\n-        rulesman = RulesManager([\n-            Rule(BaseMatcher(), myfunc, **kwargs),\n-            ], spider)\n-        rule = rulesman.get_rule_from_response(response)\n-\n-        # with argument should return partial applied callback\n-        self.failUnless(isinstance(rule.callback, partial))\n-        self.failUnlessEquals(kwargs, rule.callback())\n-\n-\n\n@@ -1,222 +0,0 @@\n-from twisted.trial import unittest\n-\n-from scrapy.http import Request\n-from scrapy.http import HtmlResponse\n-from scrapy.item import BaseItem\n-from scrapy.utils.spider import iterate_spider_output\n-\n-# basics\n-from scrapy.contrib_exp.crawlspider import CrawlSpider\n-from scrapy.contrib_exp.crawlspider import Rule\n-\n-# matchers\n-from scrapy.contrib_exp.crawlspider.matchers import BaseMatcher\n-from scrapy.contrib_exp.crawlspider.matchers import UrlRegexMatcher\n-from scrapy.contrib_exp.crawlspider.matchers import UrlListMatcher\n-\n-# extractors\n-from scrapy.contrib_exp.crawlspider.reqext import SgmlRequestExtractor\n-\n-# processors\n-from scrapy.contrib_exp.crawlspider.reqproc import Canonicalize\n-from scrapy.contrib_exp.crawlspider.reqproc import FilterDupes\n-\n-\n-# mock items\n-class Item1(BaseItem):\n-    pass\n-\n-class Item2(BaseItem):\n-    pass\n-\n-class Item3(BaseItem):\n-    pass\n-\n-\n-class CrawlSpiderTest(unittest.TestCase):\n-\n-    def spider_factory(self, rules=[],\n-                       extractors=[], processors=[],\n-                       start_urls=[]):\n-        # mock spider\n-        class Spider(CrawlSpider):\n-            def parse_item1(self, response):\n-                return Item1()\n-            \n-            def parse_item2(self, response):\n-                return Item2()\n-\n-            def parse_item3(self, response):\n-                return Item3()\n-\n-            def parse_request1(self, response):\n-                return Request('http://example.org/request1')\n-\n-            def parse_request2(self, response):\n-                return Request('http://example.org/request2')\n-\n-        Spider.start_urls = start_urls\n-        Spider.rules = rules\n-        Spider.request_extractors = extractors\n-        Spider.request_processors = processors\n-\n-        return Spider('foo')\n-\n-    def test_start_url_auto_rule(self):\n-        spider = self.spider_factory()\n-        # zero spider rules\n-        self.failUnlessEqual(len(spider.rules), 0)\n-        self.failUnlessEqual(len(spider._rulesman._rules), 0)\n-\n-        spider = self.spider_factory(start_urls=['http://example.org'])\n-\n-        self.failUnlessEqual(len(spider.rules), 0)\n-        self.failUnlessEqual(len(spider._rulesman._rules), 1)\n-\n-    def test_start_url_matcher(self):\n-        url = 'http://example.org'\n-        spider = self.spider_factory(start_urls=[url])\n-\n-        response = HtmlResponse(url)\n-\n-        rule = spider._rulesman.get_rule_from_response(response)\n-        self.failUnless(isinstance(rule.matcher, UrlListMatcher))\n-\n-        response = HtmlResponse(url + '/item.html')\n-\n-        rule = spider._rulesman.get_rule_from_response(response)\n-        self.failUnless(rule is None)\n-\n-        # TODO: remove this block\n-        # in previous version get_rule returns rule from response.request\n-        response.request = Request(url)\n-        rule = spider._rulesman.get_rule_from_response(response.request)\n-        self.failUnless(isinstance(rule.matcher, UrlListMatcher))\n-        self.failUnlessEqual(rule.follow, True)\n-\n-    def test_parse_callback(self):\n-        response = HtmlResponse('http://example.org')\n-        rules = (\n-            Rule(BaseMatcher(), 'parse_item1'),\n-            )\n-        spider = self.spider_factory(rules)\n-\n-        result = list(spider.parse(response))\n-        self.failUnlessEqual(len(result), 1)\n-        self.failUnless(isinstance(result[0], Item1))\n-\n-    def test_crawling_start_url(self):\n-        url = 'http://example.org/'\n-        html = \"\"\"<html><head><title>Page title<title>\n-        <body><p><a href=\"item/12.html\">Item 12</a></p>\n-        <p><a href=\"/about.html\">About us</a></p>\n-        <img src=\"/logo.png\" alt=\"Company logo (not a link)\" />\n-        <p><a href=\"../othercat.html\">Other category</a></p>\n-        <p><a href=\"/\" /></p></body></html>\"\"\"\n-        response = HtmlResponse(url, body=html)\n-\n-        extractors = (SgmlRequestExtractor(), )\n-        spider = self.spider_factory(start_urls=[url],\n-                                     extractors=extractors)\n-        result = list(spider.parse(response))\n-\n-        # 1 request extracted: example.org/ \n-        # because requests returns only matching\n-        self.failUnlessEqual(len(result), 1)\n-\n-        # we will add catch-all rule to extract all\n-        callback = lambda x: None\n-        rules = [Rule(r'\\.html$', callback=callback)]\n-        spider = self.spider_factory(rules, start_urls=[url],\n-                                     extractors=extractors)\n-        result = list(spider.parse(response))\n-\n-        # 4 requests extracted\n-        # 3 of .html pattern\n-        # 1 of start url patter\n-        self.failUnlessEqual(len(result), 4)\n-\n-    def test_crawling_simple_rule(self):\n-        url = 'http://example.org/somepage/index.html'\n-        html = \"\"\"<html><head><title>Page title<title>\n-        <body><p><a href=\"item/12.html\">Item 12</a></p>\n-        <p><a href=\"/about.html\">About us</a></p>\n-        <img src=\"/logo.png\" alt=\"Company logo (not a link)\" />\n-        <p><a href=\"../othercat.html\">Other category</a></p>\n-        <p><a href=\"/\" /></p></body></html>\"\"\"\n-\n-        response = HtmlResponse(url, body=html)\n-\n-        rules = (\n-            # first response callback\n-            Rule(r'index\\.html', 'parse_item1'),\n-            )\n-        spider = self.spider_factory(rules)\n-        result = list(spider.parse(response))\n-\n-        # should return Item1\n-        self.failUnlessEqual(len(result), 1)\n-        self.failUnless(isinstance(result[0], Item1))\n-\n-        # test request generation\n-        rules = (\n-            # first response without callback and follow flag\n-            Rule(r'index\\.html', follow=True),\n-            Rule(r'(\\.html|/)$', 'parse_item1'),\n-            )\n-        spider = self.spider_factory(rules)\n-        result = list(spider.parse(response))\n-\n-        # 0 because spider does not have extractors\n-        self.failUnlessEqual(len(result), 0)\n-\n-        extractors = (SgmlRequestExtractor(), )\n-\n-        # instance spider with extractor\n-        spider = self.spider_factory(rules, extractors)\n-        result = list(spider.parse(response))\n-        # 4 requests extracted\n-        self.failUnlessEqual(len(result), 4)\n-\n-    def test_crawling_multiple_rules(self):\n-        html = \"\"\"<html><head><title>Page title<title>\n-        <body><p><a href=\"item/12.html\">Item 12</a></p>\n-        <p><a href=\"/about.html\">About us</a></p>\n-        <img src=\"/logo.png\" alt=\"Company logo (not a link)\" />\n-        <p><a href=\"../othercat.html\">Other category</a></p>\n-        <p><a href=\"/\" /></p></body></html>\"\"\"\n-\n-        response = HtmlResponse('http://example.org/index.html', body=html)\n-        response1 = HtmlResponse('http://example.org/1.html')\n-        response2 = HtmlResponse('http://example.org/othercat.html')\n-\n-        rules = (\n-            Rule(r'\\d+\\.html$', 'parse_item1'),\n-            Rule(r'othercat\\.html$', 'parse_item2'),\n-            # follow-only rules\n-            Rule(r'index\\.html', 'parse_item3', follow=True)\n-            )\n-        extractors = [SgmlRequestExtractor()]\n-        spider = self.spider_factory(rules, extractors)\n-\n-        result = list(spider.parse(response))\n-        # 1 Item 2 Requests\n-        self.failUnlessEqual(len(result), 3)\n-        # parse_item3\n-        self.failUnless(isinstance(result[0], Item3))\n-        only_requests = lambda r: isinstance(r, Request)\n-        requests = filter(only_requests, result[1:])\n-        self.failUnlessEqual(len(requests), 2)\n-        self.failUnless(all(requests))\n-\n-        result1 = list(spider.parse(response1))\n-        # parse_item1\n-        self.failUnlessEqual(len(result1), 1)\n-        self.failUnless(isinstance(result1[0], Item1))\n-\n-        result2 = list(spider.parse(response2))\n-        # parse_item2\n-        self.failUnlessEqual(len(result2), 1)\n-        self.failUnless(isinstance(result2[0], Item2))\n-\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3d823d6f4525518bce26ee7550940dc669ef3108", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 18 | Churn Cumulative: 63 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,26 +1,16 @@\n \"\"\"\n-Scrapy extension for collecting scraping stats\n+Extension for collecting core stats like items scraped and start/finish times\n \"\"\"\n-import os\n-import getpass\n-import socket\n import datetime\n \n from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import signals\n from scrapy.stats import stats\n-from scrapy.conf import settings\n \n class CoreStats(object):\n-    \"\"\"Scrapy core stats collector\"\"\"\n \n     def __init__(self):\n-        stats.set_value('envinfo/user', getpass.getuser())\n-        stats.set_value('envinfo/host', socket.gethostname())\n-        stats.set_value('envinfo/logfile', settings['LOG_FILE'])\n-        stats.set_value('envinfo/pid', os.getpid())\n-\n         dispatcher.connect(self.stats_spider_opened, signal=signals.stats_spider_opened)\n         dispatcher.connect(self.stats_spider_closing, signal=signals.stats_spider_closing)\n         dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n@@ -28,19 +18,15 @@ class CoreStats(object):\n \n     def stats_spider_opened(self, spider):\n         stats.set_value('start_time', datetime.datetime.utcnow(), spider=spider)\n-        stats.set_value('envinfo/host', stats.get_value('envinfo/host'), spider=spider)\n \n     def stats_spider_closing(self, spider, reason):\n         stats.set_value('finish_time', datetime.datetime.utcnow(), spider=spider)\n-        stats.set_value('finish_status', 'OK' if reason == 'finished' else reason, spider=spider)\n-        stats.inc_value('spiders_closed_count/%s' % reason)\n+        stats.set_value('finish_reason', reason, spider=spider)\n \n     def item_scraped(self, item, spider):\n         stats.inc_value('item_scraped_count', spider=spider)\n-        stats.inc_value('item_scraped_count')\n \n     def item_dropped(self, item, spider, exception):\n         reason = exception.__class__.__name__\n         stats.inc_value('item_dropped_count', spider=spider)\n         stats.inc_value('item_dropped_reasons_count/%s' % reason, spider=spider)\n-        stats.inc_value('item_dropped_count')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ffbc9295f6c8836fbedae002e9ca79699cdcdc3c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 79 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -11,24 +11,19 @@ class DownloaderStats(object):\n             raise NotConfigured\n \n     def process_request(self, request, spider):\n-        stats.inc_value('downloader/request_count')\n         stats.inc_value('downloader/request_count', spider=spider)\n         stats.inc_value('downloader/request_method_count/%s' % request.method, spider=spider)\n         reqlen = len(request_httprepr(request))\n         stats.inc_value('downloader/request_bytes', reqlen, spider=spider)\n-        stats.inc_value('downloader/request_bytes', reqlen)\n \n     def process_response(self, request, response, spider):\n-        stats.inc_value('downloader/response_count')\n         stats.inc_value('downloader/response_count', spider=spider)\n         stats.inc_value('downloader/response_status_count/%s' % response.status, spider=spider)\n         reslen = len(response_httprepr(response))\n         stats.inc_value('downloader/response_bytes', reslen, spider=spider)\n-        stats.inc_value('downloader/response_bytes', reslen)\n         return response\n \n     def process_exception(self, request, exception, spider):\n         ex_class = \"%s.%s\" % (exception.__class__.__module__, exception.__class__.__name__)\n-        stats.inc_value('downloader/exception_count')\n         stats.inc_value('downloader/exception_count', spider=spider)\n         stats.inc_value('downloader/exception_type_count/%s' % ex_class, spider=spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d9c8877daba0ea49649935bcbeefbf291cb70cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 44 | Churn Cumulative: 60 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,35 @@\n+import sys, os\n+\n+from scrapy.command import ScrapyCommand\n+from scrapy.exceptions import UsageError\n+\n+class Command(ScrapyCommand):\n+\n+    requires_project = True\n+    default_settings = {'LOG_ENABLED': False}\n+\n+    def syntax(self):\n+        return \"<spider>\"\n+\n+    def short_desc(self):\n+        return \"Edit spider\"\n+\n+    def long_desc(self):\n+        return \"Edit a spider using the editor defined in EDITOR setting\"\n+\n+    def _err(self, msg):\n+        sys.stderr.write(msg + os.linesep)\n+        self.exitcode = 1\n+\n+    def run(self, args, opts):\n+        if len(args) != 1:\n+            raise UsageError()\n+        editor = self.crawler.settings['EDITOR']\n+        try:\n+            spider = self.crawler.spiders.create(args[0])\n+        except KeyError:\n+            return self._err(\"Spider not found: %s\" % args[0])\n+\n+        sfile = sys.modules[spider.__module__].__file__\n+        sfile = sfile.replace('.pyc', '.py')\n+        self.exitcode = os.system('%s \"%s\"' % (editor, sfile))\n\n@@ -13,6 +13,7 @@ Scrapy developers, if you add a setting here remember to:\n \n \"\"\"\n \n+import sys, os\n from os.path import join, abspath, dirname\n \n BOT_NAME = 'scrapybot'\n@@ -82,6 +83,14 @@ DOWNLOADER_STATS = True\n \n DUPEFILTER_CLASS = 'scrapy.contrib.dupefilter.RequestFingerprintDupeFilter'\n \n+try:\n+    EDITOR = os.environ['EDITOR']\n+except KeyError:\n+    if sys.platform == 'win32':\n+        EDITOR = '%s -m idlelib.idle'\n+    else:\n+        EDITOR = 'vi'\n+\n ENCODING_ALIASES = {}\n \n ENCODING_ALIASES_BASE = {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#35b52fcdf08c0a15ed469fbfa2ac9453221b9d1a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 3 | Churn Cumulative: 178 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,8 +13,6 @@ class DepthMiddleware(object):\n         self.maxdepth = maxdepth\n         self.stats = stats\n         self.verbose_stats = verbose_stats\n-        if self.stats and self.maxdepth:\n-            stats.set_value('envinfo/request_depth_limit', maxdepth)\n \n     @classmethod\n     def from_settings(cls, settings):\n\n@@ -15,7 +15,6 @@ class TestDepthMiddleware(TestCase):\n         self.stats.open_spider(self.spider)\n \n         self.mw = DepthMiddleware(1, self.stats, True)\n-        self.assertEquals(self.stats.get_value('envinfo/request_depth_limit'), 1)\n \n     def test_process_spider_output(self):\n         req = Request('http://scrapytest.org')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5fbc32c0156a5cf774bb9aa2be8a80f884ce3db2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1527 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -260,3 +260,4 @@ class ExecutionEngine(object):\n     @defer.inlineCallbacks\n     def _finish_stopping_engine(self):\n         yield send_catch_log_deferred(signal=signals.engine_stopped)\n+        yield stats.engine_stopped()\n\n@@ -17,7 +17,6 @@ class StatsCollector(object):\n     def __init__(self):\n         self._dump = settings.getbool('STATS_DUMP')\n         self._stats = {None: {}} # None is for global stats\n-        dispatcher.connect(self._engine_stopped, signal=signals.engine_stopped)\n \n     def get_value(self, key, default=None, spider=None):\n         return self._stats[spider].get(key, default)\n@@ -63,7 +62,7 @@ class StatsCollector(object):\n                 spider=spider)\n         self._persist_stats(stats, spider)\n \n-    def _engine_stopped(self):\n+    def engine_stopped(self):\n         stats = self.get_stats()\n         if self._dump:\n             log.msg(\"Dumping global stats:\\n\" + pprint.pformat(stats))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#474cba512c2b9697f836167fa50f5258d777ccbf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 34 | Files Changed: 1 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 43 | Churn Cumulative: 122 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,18 +4,15 @@ MemoryDebugger extension\n See documentation in docs/topics/extensions.rst\n \"\"\"\n \n-import os\n import gc\n-import socket\n \n from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import signals\n from scrapy.exceptions import NotConfigured\n-from scrapy.mail import MailSender\n-from scrapy.utils.trackref import format_live_refs\n from scrapy.conf import settings\n-from scrapy import log\n+from scrapy.stats import stats\n+from scrapy.utils.trackref import live_refs\n \n class MemoryDebugger(object):\n \n@@ -28,9 +25,6 @@ class MemoryDebugger(object):\n         if not settings.getbool('MEMDEBUG_ENABLED'):\n             raise NotConfigured\n \n-        self.mail = MailSender()\n-        self.rcpts = settings.getlist('MEMDEBUG_NOTIFY')\n-\n         dispatcher.connect(self.engine_started, signals.engine_started)\n         dispatcher.connect(self.engine_stopped, signals.engine_stopped)\n \n@@ -39,32 +33,13 @@ class MemoryDebugger(object):\n             self.libxml2.debugMemory(1)\n \n     def engine_stopped(self):\n-        figures = self.collect_figures()\n-        report = self.create_report(figures)\n-        self.log_or_send_report(report)\n-\n-    def collect_figures(self):\n-        gc.collect()\n-\n-        figures = []\n-        figures.append((\"Objects in gc.garbage\", len(gc.garbage), \"\"))\n         if self.libxml2:\n             self.libxml2.cleanupParser()\n-            figures.append((\"libxml2 memory leak\", self.libxml2.debugMemory(1), \"bytes\"))\n-        return figures\n-\n-    def create_report(self, figures):\n-        s = \"\"\n-        s += \"SCRAPY MEMORY DEBUGGER RESULTS\\n\\n\"\n-        for f in figures:\n-            s += \"%-30s : %d %s\\n\" % f\n+            stats.set_value('memdebug/libxml2_leaked_bytes', self.libxml2.debugMemory(1))\n+        gc.collect()\n+        stats.set_value('memdebug/gc_garbage_count', len(gc.garbage))\n         if settings.getbool('TRACK_REFS'):\n-            s += os.linesep\n-            s += format_live_refs()\n-        return s\n-\n-    def log_or_send_report(self, report):\n-        if self.rcpts:\n-            self.mail.send(self.rcpts, \"Scrapy Memory Debugger results at %s\" % \\\n-                socket.gethostname(), report)\n-        log.msg(report)\n+            for cls, wdict in live_refs.iteritems():\n+                if not wdict:\n+                    continue\n+                stats.set_value('memdebug/live_refs/%s' % cls.__name__, len(wdict))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
