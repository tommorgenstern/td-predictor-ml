{"custom_id": "keras#e092fa8b5776ee8c7bc80227d046036de900412c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 43 | Churn Cumulative: 7313 | Contributors (this commit): 41 | Commits (past 90d): 42 | Contributors (cumulative): 44 | DMM Complexity: 0.2962962962962963\n\nDIFF:\n@@ -2324,10 +2324,9 @@ class Container(Layer):\n         import yaml\n         return yaml.dump(self._updated_config(), **kwargs)\n \n-    def summary(self, line_length=100, positions=[.33, .55, .67, 1.]):\n+    def summary(self, line_length=None, positions=None):\n         from keras.utils.layer_utils import print_summary\n-        print_summary(self.layers,\n-                      getattr(self, 'container_nodes', None),\n+        print_summary(self,\n                       line_length=line_length,\n                       positions=positions)\n \n\n@@ -29,13 +29,11 @@ def layer_from_config(config, custom_objects=None):\n                                     printable_module_name='layer')\n \n \n-def print_summary(layers, relevant_nodes=None,\n-                  line_length=100, positions=None):\n+def print_summary(model, line_length=None, positions=None):\n     \"\"\"Prints a summary of a layer.\n \n     # Arguments\n-        layers: list of layers to print summaries of\n-        relevant_nodes: list of relevant nodes\n+        model: Keras model instance.\n         line_length: total length of printed lines\n         positions: relative or absolute positions of log elements in each line.\n             If not provided, defaults to `[.33, .55, .67, 1.]`.\n@@ -43,6 +41,24 @@ def print_summary(layers, relevant_nodes=None,\n     # TODO: don't print connectivity for sequential models\n     maybe change API to accept a model instance\n     \"\"\"\n+\n+    if isinstance(model, Sequential):\n+        sequential_like = True\n+    else:\n+        sequential_like = True\n+        for k, v in model.nodes_by_depth.items():\n+            if len(v) > 1:\n+                sequential_like = False\n+\n+    if sequential_like:\n+        line_length = line_length or 65\n+        positions = positions or [.45, .85, 1.]\n+        if positions[-1] <= 1:\n+            positions = [int(line_length * p) for p in positions]\n+        # header names for the different log elements\n+        to_display = ['Layer (type)', 'Output Shape', 'Param #']\n+    else:\n+        line_length = line_length or 100\n         positions = positions or [.33, .55, .67, 1.]\n         if positions[-1] <= 1:\n             positions = [int(line_length * p) for p in positions]\n@@ -64,6 +80,16 @@ def print_summary(layers, relevant_nodes=None,\n     print('=' * line_length)\n \n     def print_layer_summary(layer):\n+        try:\n+            output_shape = layer.output_shape\n+        except AttributeError:\n+            output_shape = 'multiple'\n+        name = layer.name\n+        cls_name = layer.__class__.__name__\n+        fields = [name + ' (' + cls_name + ')', output_shape, layer.count_params()]\n+        print_row(fields, positions)\n+\n+    def print_layer_summary_with_connections(layer):\n         \"\"\"Prints a summary for a single layer.\n \n         # Arguments\n@@ -99,8 +125,12 @@ def print_summary(layers, relevant_nodes=None,\n                 fields = ['', '', '', connections[i]]\n                 print_row(fields, positions)\n \n+    layers = model.layers\n     for i in range(len(layers)):\n+        if sequential_like:\n             print_layer_summary(layers[i])\n+        else:\n+            print_layer_summary_with_connections(layers[i])\n         if i == len(layers) - 1:\n             print('=' * line_length)\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#803e2869c76c5c73bfe1fbf6ab4af269b4a0d551", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 120 | Lines Deleted: 121 | Files Changed: 7 | Hunks: 65 | Methods Changed: 7 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 241 | Churn Cumulative: 2349 | Contributors (this commit): 31 | Commits (past 90d): 26 | Contributors (cumulative): 44 | DMM Complexity: 0.0\n\nDIFF:\n@@ -23,12 +23,10 @@ Four digits inverted:\n \n Five digits inverted:\n + One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n-\n '''\n \n from __future__ import print_function\n from keras.models import Sequential\n-from keras.engine.training import slice_X\n from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, recurrent\n import numpy as np\n from six.moves import range\n@@ -57,15 +55,15 @@ class CharacterTable(object):\n             num_rows: Number of rows in the returned one hot encoding. This is\n                 used to keep the # of rows for each data the same.\n         \"\"\"\n-        X = np.zeros((num_rows, len(self.chars)))\n+        x = np.zeros((num_rows, len(self.chars)))\n         for i, c in enumerate(C):\n-            X[i, self.char_indices[c]] = 1\n-        return X\n+            x[i, self.char_indices[c]] = 1\n+        return x\n \n-    def decode(self, X, calc_argmax=True):\n+    def decode(self, x, calc_argmax=True):\n         if calc_argmax:\n-            X = X.argmax(axis=-1)\n-        return ''.join(self.indices_char[x] for x in X)\n+            x = x.argmax(axis=-1)\n+        return ''.join(self.indices_char[x] for x in x)\n \n \n class colors:\n@@ -80,7 +78,7 @@ INVERT = True\n \n # Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n # int is DIGITS.\n-MAXLEN = DIGITS + 1 + DIGITS\n+MAxLEN = DIGITS + 1 + DIGITS\n \n # All the numbers, plus sign and space for padding.\n chars = '0123456789+ '\n@@ -95,14 +93,14 @@ while len(questions) < TRAINING_SIZE:\n                     for i in range(np.random.randint(1, DIGITS + 1))))\n     a, b = f(), f()\n     # Skip any addition questions we've already seen\n-    # Also skip any such that X+Y == Y+X (hence the sorting).\n+    # Also skip any such that x+Y == Y+x (hence the sorting).\n     key = tuple(sorted((a, b)))\n     if key in seen:\n         continue\n     seen.add(key)\n-    # Pad the data with spaces such that it is always MAXLEN.\n+    # Pad the data with spaces such that it is always MAxLEN.\n     q = '{}+{}'.format(a, b)\n-    query = q + ' ' * (MAXLEN - len(q))\n+    query = q + ' ' * (MAxLEN - len(q))\n     ans = str(a + b)\n     # Answers can be of maximum size DIGITS + 1.\n     ans += ' ' * (DIGITS + 1 - len(ans))\n@@ -115,31 +113,31 @@ while len(questions) < TRAINING_SIZE:\n print('Total addition questions:', len(questions))\n \n print('Vectorization...')\n-X = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n+x = np.zeros((len(questions), MAxLEN, len(chars)), dtype=np.bool)\n y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n for i, sentence in enumerate(questions):\n-    X[i] = ctable.encode(sentence, MAXLEN)\n+    x[i] = ctable.encode(sentence, MAxLEN)\n for i, sentence in enumerate(expected):\n     y[i] = ctable.encode(sentence, DIGITS + 1)\n \n-# Shuffle (X, y) in unison as the later parts of X will almost all be larger\n+# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n # digits.\n indices = np.arange(len(y))\n np.random.shuffle(indices)\n-X = X[indices]\n+x = x[indices]\n y = y[indices]\n \n # Explicitly set apart 10% for validation data that we never train over.\n-split_at = len(X) - len(X) // 10\n-(X_train, X_val) = (slice_X(X, 0, split_at), slice_X(X, split_at))\n-(y_train, y_val) = (y[:split_at], y[split_at:])\n+split_at = len(x) - len(x) // 10\n+(x_train, x_val) = x[:split_at], x[split_at:]\n+(y_train, y_val) = y[:split_at], y[split_at:]\n \n print('Training Data:')\n-print(X_train.shape)\n+print(x_train.shape)\n print(y_train.shape)\n \n print('Validation Data:')\n-print(X_val.shape)\n+print(x_val.shape)\n print(y_val.shape)\n \n # Try replacing GRU, or SimpleRNN.\n@@ -153,7 +151,7 @@ model = Sequential()\n # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n # Note: In a situation where your input sequences have a variable length,\n # use input_shape=(None, num_feature).\n-model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n+model.add(RNN(HIDDEN_SIZE, input_shape=(MAxLEN, len(chars))))\n # As the decoder RNN's input, repeatedly provide with the last hidden state of\n # RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n # length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n@@ -181,15 +179,15 @@ for iteration in range(1, 200):\n     print()\n     print('-' * 50)\n     print('Iteration', iteration)\n-    model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=1,\n-              validation_data=(X_val, y_val))\n+    model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=1,\n+              validation_data=(x_val, y_val))\n     # Select 10 samples from the validation set at random so we can visualize\n     # errors.\n     for i in range(10):\n-        ind = np.random.randint(0, len(X_val))\n-        rowX, rowy = X_val[np.array([ind])], y_val[np.array([ind])]\n-        preds = model.predict_classes(rowX, verbose=0)\n-        q = ctable.decode(rowX[0])\n+        ind = np.random.randint(0, len(x_val))\n+        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n+        preds = model.predict_classes(rowx, verbose=0)\n+        q = ctable.decode(rowx[0])\n         correct = ctable.decode(rowy[0])\n         guess = ctable.decode(preds[0], calc_argmax=False)\n         print('Q', q[::-1] if INVERT else q)\n\n@@ -52,11 +52,11 @@ class Antirectifier(Layer):\n         shape[-1] *= 2\n         return tuple(shape)\n \n-    def call(self, x, mask=None):\n-        x -= K.mean(x, axis=1, keepdims=True)\n-        x = K.l2_normalize(x, axis=1)\n-        pos = K.relu(x)\n-        neg = K.relu(-x)\n+    def call(self, inputs):\n+        inputs -= K.mean(inputs, axis=1, keepdims=True)\n+        inputs = K.l2_normalize(inputs, axis=1)\n+        pos = K.relu(inputs)\n+        neg = K.relu(-inputs)\n         return K.concatenate([pos, neg], axis=1)\n \n # global parameters\n@@ -65,16 +65,16 @@ num_classes = 10\n epochs = 40\n \n # the data, shuffled and split between train and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+(x_train, y_train), (x_test, y_test) = mnist.load_data()\n \n-X_train = X_train.reshape(60000, 784)\n-X_test = X_test.reshape(10000, 784)\n-X_train = X_train.astype('float32')\n-X_test = X_test.astype('float32')\n-X_train /= 255\n-X_test /= 255\n-print(X_train.shape[0], 'train samples')\n-print(X_test.shape[0], 'test samples')\n+x_train = x_train.reshape(60000, 784)\n+x_test = x_test.reshape(10000, 784)\n+x_train = x_train.astype('float32')\n+x_test = x_test.astype('float32')\n+x_train /= 255\n+x_test /= 255\n+print(x_train.shape[0], 'train samples')\n+print(x_test.shape[0], 'test samples')\n \n # convert class vectors to binary class matrices\n Y_train = np_utils.to_categorical(y_train, num_classes)\n@@ -97,9 +97,9 @@ model.compile(loss='categorical_crossentropy',\n               metrics=['accuracy'])\n \n # train the model\n-model.fit(X_train, Y_train,\n+model.fit(x_train, Y_train,\n           batch_size=batch_size, epochs=epochs,\n-          verbose=1, validation_data=(X_test, Y_test))\n+          verbose=1, validation_data=(x_test, Y_test))\n \n # next, compare with an equivalent network\n # with2x bigger Dense layers and ReLU\n\n@@ -12,7 +12,7 @@ QA2 - Two Supporting Facts   | 20               | 50.0\n QA3 - Three Supporting Facts | 20               | 20.5\n QA4 - Two Arg. Relations     | 61               | 62.9\n QA5 - Three Arg. Relations   | 70               | 61.9\n-QA6 - Yes/No Questions       | 48               | 50.7\n+QA6 - yes/No Questions       | 48               | 50.7\n QA7 - Counting               | 49               | 78.9\n QA8 - Lists/Sets             | 45               | 77.2\n QA9 - Simple Negation        | 64               | 64.0\n@@ -62,13 +62,12 @@ import re\n import tarfile\n \n import numpy as np\n-np.random.seed(1337)  # for reproducibility\n \n from keras.utils.data_utils import get_file\n from keras.layers.embeddings import Embedding\n-from keras.layers import Dense, Merge, Dropout, RepeatVector\n+from keras import layers\n from keras.layers import recurrent\n-from keras.models import Sequential\n+from keras.models import Model\n from keras.preprocessing.sequence import pad_sequences\n \n \n@@ -125,26 +124,26 @@ def get_stories(f, only_supporting=False, max_length=None):\n \n \n def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n-    X = []\n-    Xq = []\n-    Y = []\n+    xs = []\n+    xqs = []\n+    ys = []\n     for story, query, answer in data:\n         x = [word_idx[w] for w in story]\n         xq = [word_idx[w] for w in query]\n         y = np.zeros(len(word_idx) + 1)  # let's not forget that index 0 is reserved\n         y[word_idx[answer]] = 1\n-        X.append(x)\n-        Xq.append(xq)\n-        Y.append(y)\n-    return pad_sequences(X, maxlen=story_maxlen), pad_sequences(Xq, maxlen=query_maxlen), np.array(Y)\n+        xs.append(x)\n+        xqs.append(xq)\n+        ys.append(y)\n+    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)\n \n RNN = recurrent.LSTM\n EMBED_HIDDEN_SIZE = 50\n SENT_HIDDEN_SIZE = 100\n-QUERY_HIDDEN_SIZE = 100\n+QUERy_HIDDEN_SIZE = 100\n BATCH_SIZE = 32\n EPOCHS = 40\n-print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE, QUERY_HIDDEN_SIZE))\n+print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE, QUERy_HIDDEN_SIZE))\n \n try:\n     path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n@@ -172,40 +171,38 @@ word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n \n-X, Xq, Y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\n-tX, tXq, tY = vectorize_stories(test, word_idx, story_maxlen, query_maxlen)\n+x, xq, y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\n+tx, txq, ty = vectorize_stories(test, word_idx, story_maxlen, query_maxlen)\n \n print('vocab = {}'.format(vocab))\n-print('X.shape = {}'.format(X.shape))\n-print('Xq.shape = {}'.format(Xq.shape))\n-print('Y.shape = {}'.format(Y.shape))\n+print('x.shape = {}'.format(x.shape))\n+print('xq.shape = {}'.format(xq.shape))\n+print('y.shape = {}'.format(y.shape))\n print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n \n print('Build model...')\n \n-sentrnn = Sequential()\n-sentrnn.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE,\n-                      input_length=story_maxlen))\n-sentrnn.add(Dropout(0.3))\n+sentence = layers.Input(shape=(story_maxlen,), dtype='int32')\n+encoded_sentence = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(sentence)\n+encoded_sentence = layers.Dropout(0.3)(encoded_sentence)\n \n-qrnn = Sequential()\n-qrnn.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE,\n-                   input_length=query_maxlen))\n-qrnn.add(Dropout(0.3))\n-qrnn.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n-qrnn.add(RepeatVector(story_maxlen))\n+question = layers.Input(shape=(query_maxlen,), dtype='int32')\n+encoded_question = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question)\n+encoded_question = layers.Dropout(0.3)(encoded_question)\n+encoded_question = RNN(EMBED_HIDDEN_SIZE)(encoded_question)\n+encoded_question = layers.RepeatVector(story_maxlen)(encoded_question)\n \n-model = Sequential()\n-model.add(Merge([sentrnn, qrnn], mode='sum'))\n-model.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n-model.add(Dropout(0.3))\n-model.add(Dense(vocab_size, activation='softmax'))\n+merged = layers.sum([encoded_sentence, encoded_question])\n+merged = RNN(EMBED_HIDDEN_SIZE)(merged)\n+merged = layers.Dropout(0.3)(merged)\n+preds = layers.Dense(vocab_size, activation='softmax')(merged)\n \n+model = Model([sentence, question], preds)\n model.compile(optimizer='adam',\n               loss='categorical_crossentropy',\n               metrics=['accuracy'])\n \n print('Training')\n-model.fit([X, Xq], Y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.05)\n-loss, acc = model.evaluate([tX, tXq], tY, batch_size=BATCH_SIZE)\n+model.fit([x, xq], y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.05)\n+loss, acc = model.evaluate([tx, txq], ty, batch_size=BATCH_SIZE)\n print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n\n@@ -1,7 +1,7 @@\n '''Train a simple deep CNN on the CIFAR10 small images dataset.\n \n GPU run command with Theano backend (with TensorFlow, the GPU is automatically used):\n-    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python cifar10_cnn.py\n+    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatx=float32 python cifar10_cnn.py\n \n It gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs.\n (it's still underfitting at that point, though).\n@@ -12,7 +12,7 @@ from keras.datasets import cifar10\n from keras.preprocessing.image import ImageDataGenerator\n from keras.models import Sequential\n from keras.layers import Dense, Dropout, Activation, Flatten\n-from keras.layers import Convolution2D, MaxPooling2D\n+from keras.layers import Conv2D, MaxPooling2D\n from keras.utils import np_utils\n \n batch_size = 32\n@@ -26,28 +26,28 @@ img_rows, img_cols = 32, 32\n img_channels = 3\n \n # The data, shuffled and split between train and test sets:\n-(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n-print('X_train shape:', X_train.shape)\n-print(X_train.shape[0], 'train samples')\n-print(X_test.shape[0], 'test samples')\n+(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n+print('x_train shape:', x_train.shape)\n+print(x_train.shape[0], 'train samples')\n+print(x_test.shape[0], 'test samples')\n \n # Convert class vectors to binary class matrices.\n-Y_train = np_utils.to_categorical(y_train, num_classes)\n-Y_test = np_utils.to_categorical(y_test, num_classes)\n+y_train = np_utils.to_categorical(y_train, num_classes)\n+y_test = np_utils.to_categorical(y_test, num_classes)\n \n model = Sequential()\n \n-model.add(Convolution2D(32, 3, 3, border_mode='same',\n-                        input_shape=X_train.shape[1:]))\n+model.add(Conv2D(32, (3, 3), padding='same',\n+                 input_shape=x_train.shape[1:]))\n model.add(Activation('relu'))\n-model.add(Convolution2D(32, 3, 3))\n+model.add(Conv2D(32, (3, 3)))\n model.add(Activation('relu'))\n model.add(MaxPooling2D(pool_size=(2, 2)))\n model.add(Dropout(0.25))\n \n-model.add(Convolution2D(64, 3, 3, border_mode='same'))\n+model.add(Conv2D(64, (3, 3), padding='same'))\n model.add(Activation('relu'))\n-model.add(Convolution2D(64, 3, 3))\n+model.add(Conv2D(64, (3, 3)))\n model.add(Activation('relu'))\n model.add(MaxPooling2D(pool_size=(2, 2)))\n model.add(Dropout(0.25))\n@@ -64,17 +64,17 @@ model.compile(loss='categorical_crossentropy',\n               optimizer='rmsprop',\n               metrics=['accuracy'])\n \n-X_train = X_train.astype('float32')\n-X_test = X_test.astype('float32')\n-X_train /= 255\n-X_test /= 255\n+x_train = x_train.astype('float32')\n+x_test = x_test.astype('float32')\n+x_train /= 255\n+x_test /= 255\n \n if not data_augmentation:\n     print('Not using data augmentation.')\n-    model.fit(X_train, Y_train,\n+    model.fit(x_train, y_train,\n               batch_size=batch_size,\n               epochs=epochs,\n-              validation_data=(X_test, Y_test),\n+              validation_data=(x_test, y_test),\n               shuffle=True)\n else:\n     print('Using real-time data augmentation.')\n@@ -93,11 +93,11 @@ else:\n \n     # Compute quantities required for featurewise normalization\n     # (std, mean, and principal components if ZCA whitening is applied).\n-    datagen.fit(X_train)\n+    datagen.fit(x_train)\n \n     # Fit the model on the batches generated by datagen.flow().\n-    model.fit_generator(datagen.flow(X_train, Y_train,\n+    model.fit_generator(datagen.flow(x_train, y_train,\n                                      batch_size=batch_size),\n-                        samples_per_epoch=X_train.shape[0],\n+                        samples_per_epoch=x_train.shape[0],\n                         epochs=epochs,\n-                        validation_data=(X_test, Y_test))\n+                        validation_data=(x_test, y_test))\n\n@@ -3,7 +3,7 @@ This network is used to predict the next frame of an artificially\n generated movie which contains moving squares.\n \"\"\"\n from keras.models import Sequential\n-from keras.layers.convolutional import Convolution3D\n+from keras.layers.convolutional import Conv3D\n from keras.layers.convolutional_recurrent import ConvLSTM2D\n from keras.layers.normalization import BatchNormalization\n import numpy as np\n@@ -14,27 +14,26 @@ import pylab as plt\n # of identical shape.\n \n seq = Sequential()\n-seq.add(ConvLSTM2D(filters=40, num_row=3, num_col=3,\n+seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n                    input_shape=(None, 40, 40, 1),\n-                   border_mode='same', return_sequences=True))\n+                   padding='same', return_sequences=True))\n seq.add(BatchNormalization())\n \n-seq.add(ConvLSTM2D(filters=40, num_row=3, num_col=3,\n-                   border_mode='same', return_sequences=True))\n+seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n+                   padding='same', return_sequences=True))\n seq.add(BatchNormalization())\n \n-seq.add(ConvLSTM2D(filters=40, num_row=3, num_col=3,\n-                   border_mode='same', return_sequences=True))\n+seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n+                   padding='same', return_sequences=True))\n seq.add(BatchNormalization())\n \n-seq.add(ConvLSTM2D(filters=40, num_row=3, num_col=3,\n-                   border_mode='same', return_sequences=True))\n+seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n+                   padding='same', return_sequences=True))\n seq.add(BatchNormalization())\n \n-seq.add(Convolution3D(filters=1, kernel_dim1=1, kernel_dim2=3,\n-                      kernel_dim3=3, activation='sigmoid',\n-                      border_mode='same', data_format='channels_last'))\n-\n+seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n+               activation='sigmoid',\n+               padding='same', data_format='channels_last'))\n seq.compile(loss='binary_crossentropy', optimizer='adadelta')\n \n \n\n@@ -12,7 +12,6 @@ from what you see with CNNs/MLPs/etc.\n '''\n from __future__ import print_function\n import numpy as np\n-np.random.seed(1337)  # for reproducibility\n \n from keras.preprocessing import sequence\n from keras.models import Sequential\n@@ -37,10 +36,9 @@ print('x_test shape:', x_test.shape)\n \n print('Build model...')\n model = Sequential()\n-model.add(Embedding(max_features, 128, dropout=0.2))\n-model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n-model.add(Dense(1))\n-model.add(Activation('sigmoid'))\n+model.add(Embedding(max_features, 128))\n+model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n+model.add(Dense(1, activation='sigmoid'))\n \n # try using different optimizers and different optimizer configs\n model.compile(loss='binary_crossentropy',\n\n@@ -33,7 +33,14 @@ def pad_sequences(sequences, maxlen=None, dtype='int32',\n         ValueError: in case of invalid values for `truncating` or `padding`,\n             or in case of invalid shape for a `sequences` entry.\n     \"\"\"\n-    lengths = [len(s) for s in sequences]\n+    if not hasattr(sequences, '__len__'):\n+        raise ValueError('`sequences` must be iterable.')\n+    lengths = []\n+    for x in sequences:\n+        if not hasattr(x, '__len__'):\n+            raise ValueError('`sequences` must be a list of iterables. '\n+                             'Found non-iterable: ' + str(x))\n+        lengths.append(len(x))\n \n     num_samples = len(sequences)\n     if maxlen is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#451d74c56d1b2f498e5c9a4856309d7f454b39e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1298 | Lines Deleted: 256 | Files Changed: 12 | Hunks: 25 | Methods Changed: 62 | Complexity Δ (Sum/Max): 79/20 | Churn Δ: 1554 | Churn Cumulative: 26509 | Contributors (this commit): 106 | Commits (past 90d): 94 | Contributors (cumulative): 152 | DMM Complexity: 0.6288659793814433\n\nDIFF:\n@@ -591,11 +591,14 @@ class Lambda(Layer):\n         (or auto-inferred when using TensorFlow).\n     \"\"\"\n \n-    def __init__(self, function, output_shape=None, arguments=None, **kwargs):\n+    def __init__(self, function, output_shape=None,\n+                 mask=None, arguments=None, **kwargs):\n         super(Lambda, self).__init__(**kwargs)\n         self.function = function\n         self.arguments = arguments if arguments else {}\n-        self.supports_masking = False\n+        if mask is not None:\n+            self.supports_masking = True\n+        self.mask = mask\n \n         if output_shape is None:\n             self._output_shape = None\n@@ -650,6 +653,11 @@ class Lambda(Layer):\n             arguments['mask'] = mask\n         return self.function(inputs, **arguments)\n \n+    def compute_mask(self, inputs, mask=None):\n+        if callable(self.mask):\n+            return self.mask(inputs, mask)\n+        return self.mask\n+\n     def get_config(self):\n         if isinstance(self.function, python_types.LambdaType):\n             function = func_dump(self.function)\n\n@@ -22,6 +22,8 @@ class _Merge(Layer):\n             raise ValueError('A merge layer should be called '\n                              'on a list of at least 2 inputs. '\n                              'Got ' + str(len(input_shape)) + ' inputs.')\n+        if all([shape is None for shape in input_shape]):\n+            return\n         # TODO: handle shapes with None entries.\n         input_shapes_set = set(input_shape)\n         if None in input_shapes_set:\n@@ -108,6 +110,8 @@ class Concatenate(_Merge):\n         if not isinstance(input_shape, list):\n             raise ValueError('`Concatenate` layer should be called '\n                              'on a list of inputs')\n+        if all([shape is None for shape in input_shape]):\n+            return\n         reduced_inputs_shapes = [list(shape) for shape in input_shape]\n         shape_set = set()\n         for i in range(len(reduced_inputs_shapes)):\n\n@@ -73,12 +73,12 @@ class TimeDistributed(Wrapper):\n     The output will then have shape `(32, 10, 8)`.\n \n     `TimeDistributed` can be used with arbitrary layers, not just `Dense`,\n-    for instance with a `Convolution2D` layer:\n+    for instance with a `Conv2D` layer:\n \n     ```python\n         model = Sequential()\n-        model.add(TimeDistributed(Convolution2D(64, 3, 3),\n-                                  input_shape=(10, 3, 299, 299)))\n+        model.add(TimeDistributed(Conv2D(64, (3, 3)),\n+                                  input_shape=(10, 299, 299, 3)))\n     ```\n \n     # Arguments\n@@ -86,8 +86,8 @@ class TimeDistributed(Wrapper):\n     \"\"\"\n \n     def __init__(self, layer, **kwargs):\n-        self.supports_masking = True\n         super(TimeDistributed, self).__init__(layer, **kwargs)\n+        self.supports_masking = True\n \n     def build(self, input_shape):\n         assert len(input_shape) >= 3\n\n@@ -14,6 +14,7 @@ from .utils.io_utils import ask_to_proceed_with_overwrite\n from .engine.training import Model\n from .engine.topology import get_source_inputs, Node, Layer, Input\n from .legacy import layers as legacy_layers\n+from .legacy import models as legacy_models\n \n \n def save_model(model, filepath, overwrite=True):\n@@ -423,6 +424,39 @@ class Sequential(Model):\n             self.build()\n         return self.model.uses_learning_phase\n \n+    @property\n+    def _flattened_layers(self):\n+        layers = []\n+        if self.layers:\n+            # Support for legacy models\n+            if isinstance(self.layers[0], legacy_layers.Merge):\n+                merge = self.layers[0]\n+                for layer in merge.layers:\n+                    if hasattr(layer, '_flattened_layers'):\n+                        for sublayer in layer._flattened_layers:\n+                            if sublayer not in layers:\n+                                layers.append(sublayer)\n+                    elif hasattr(layer, 'layers'):\n+                        for sublayer in layer.layers:\n+                            if sublayer not in layers:\n+                                layers.append(sublayer)\n+                    else:\n+                        if layer not in layers:\n+                            layers.append(layer)\n+            else:\n+                if self.layers[0] not in layers:\n+                    layers.append(self.layers[0])\n+            for layer in self.layers[1:]:\n+                if layer not in layers:\n+                    layers.append(layer)\n+        return layers\n+\n+    def _gather_list_attr(self, attr):\n+        all_attrs = []\n+        for layer in self._flattened_layers:\n+            all_attrs += getattr(layer, attr, [])\n+        return all_attrs\n+\n     @property\n     def trainable(self):\n         return self._trainable\n@@ -435,15 +469,19 @@ class Sequential(Model):\n \n     @property\n     def trainable_weights(self):\n-        if self.model is None:\n-            self.build()\n-        return self.model.trainable_weights\n+        if not self.trainable:\n+            return []\n+        # Support for legacy behavior\n+        return self._gather_list_attr('trainable_weights')\n \n     @property\n     def non_trainable_weights(self):\n-        if self.model is None:\n-            self.build()\n-        return self.model.non_trainable_weights\n+        # Support for legacy behavior\n+        weights = self._gather_list_attr('non_trainable_weights')\n+        if not self.trainable:\n+            trainable_weights = self._gather_list_attr('trainable_weights')\n+            return trainable_weights + weights\n+        return weights\n \n     @property\n     def updates(self):\n@@ -489,6 +527,14 @@ class Sequential(Model):\n         \"\"\"Returns the weights of the model,\n         as a flat list of Numpy arrays.\n         \"\"\"\n+        # Legacy support\n+        if legacy_models.needs_legacy_support(self):\n+            layers = legacy_models.legacy_sequential_layers(self)\n+            weights = []\n+            for layer in layers:\n+                weights.append(layer.get_weights())\n+            return weights\n+\n         if self.model is None:\n             self.build()\n         return self.model.get_weights()\n@@ -499,6 +545,14 @@ class Sequential(Model):\n         of Numpy arrays with shapes and types matching\n         the output of `model.get_weights()`.\n         \"\"\"\n+        # Legacy support\n+        if legacy_models.needs_legacy_support(self):\n+            layers = legacy_models.legacy_sequential_layers(self)\n+            for layer in layers:\n+                nb_param = len(layer.weights)\n+                layer.set_weights(weights[:nb_param])\n+                weights = weights[nb_param:]\n+\n         if self.model is None:\n             self.build()\n         self.model.set_weights(weights)\n\n@@ -1,7 +1,5 @@\n from __future__ import print_function\n-import inspect\n \n-from .generic_utils import get_custom_objects\n from .generic_utils import deserialize_keras_object\n from .conv_utils import convert_kernel\n py_sum = sum\n@@ -64,6 +62,9 @@ def print_summary(model, line_length=None, positions=None):\n             positions = [int(line_length * p) for p in positions]\n         # header names for the different log elements\n         to_display = ['Layer (type)', 'Output Shape', 'Param #', 'Connected to']\n+        relevant_nodes = []\n+        for k, v in model.nodes_by_depth.items():\n+            relevant_nodes += v\n \n     def print_row(fields, positions):\n         line = ''\n\n@@ -0,0 +1,92 @@\n+import pytest\n+from keras.utils.test_utils import keras_test\n+from keras import applications\n+from keras import backend as K\n+\n+\n+@keras_test\n+def test_vgg16():\n+    model = applications.VGG16(weights=None)\n+    assert model.output_shape == (None, 1000)\n+\n+\n+@keras_test\n+def test_vgg16_notop():\n+    model = applications.VGG16(weights=None, include_top=False)\n+    assert model.output_shape == (None, None, None, 512)\n+\n+\n+@keras_test\n+def test_vgg16_pooling():\n+    model = applications.VGG16(weights=None, include_top=False, pooling='avg')\n+    assert model.output_shape == (None, 512)\n+\n+\n+@keras_test\n+def test_vgg19():\n+    model = applications.VGG19(weights=None)\n+    assert model.output_shape == (None, 1000)\n+\n+\n+@keras_test\n+def test_vgg19_notop():\n+    model = applications.VGG16(weights=None, include_top=False)\n+    assert model.output_shape == (None, None, None, 512)\n+\n+\n+@keras_test\n+def test_vgg19_pooling():\n+    model = applications.VGG16(weights=None, include_top=False, pooling='avg')\n+    assert model.output_shape == (None, 512)\n+\n+\n+@keras_test\n+@pytest.mark.skipif((K.backend() != 'tensorflow'),\n+                    reason='Requires tensorflow backend')\n+def test_xception():\n+    model = applications.Xception(weights=None)\n+    assert model.output_shape == (None, 1000)\n+\n+\n+@keras_test\n+@pytest.mark.skipif((K.backend() != 'tensorflow'),\n+                    reason='Requires tensorflow backend')\n+def test_xception_notop():\n+    model = applications.Xception(weights=None, include_top=False)\n+    assert model.output_shape == (None, None, None, 2048)\n+\n+\n+@keras_test\n+@pytest.mark.skipif((K.backend() != 'tensorflow'),\n+                    reason='Requires tensorflow backend')\n+def test_xception_pooling():\n+    model = applications.Xception(weights=None, include_top=False, pooling='avg')\n+    assert model.output_shape == (None, 2048)\n+\n+\n+@keras_test\n+def test_inceptionv3():\n+    model = applications.InceptionV3(weights=None)\n+    assert model.output_shape == (None, 1000)\n+\n+\n+@keras_test\n+def test_inceptionv3_notop():\n+    model = applications.InceptionV3(weights=None, include_top=False)\n+    assert model.output_shape == (None, None, None, 2048)\n+\n+\n+@keras_test\n+def test_inceptionv3_pooling():\n+    model = applications.InceptionV3(weights=None, include_top=False, pooling='avg')\n+    assert model.output_shape == (None, 2048)\n+\n+\n+@keras_test\n+def test_music_tagger_rcnn():\n+    model = applications.MusicTaggerCRNN(weights=None)\n+    assert model.output_shape == (None, 50)\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n@@ -454,8 +454,8 @@ def test_recursion():\n         j_tf = tf.placeholder(dtype=K.floatx())\n         k_tf = tf.placeholder(dtype=K.floatx())\n         m_tf, n_tf = tf_model([j_tf, k_tf])\n-        assert K.int_shape(m_tf) == (None, 64)\n-        assert K.int_shape(n_tf) == (None, 5)\n+        assert m_tf.get_shape().as_list() == [None, 64]\n+        assert n_tf.get_shape().as_list() == [None, 5]\n \n         # test merge\n         layers.concatenate([j_tf, k_tf], axis=1)\n\n@@ -0,0 +1,118 @@\n+import pytest\n+import numpy as np\n+from numpy.testing import assert_allclose\n+\n+from keras import backend as K\n+from keras.models import Sequential\n+from keras.layers import convolutional_recurrent\n+from keras.utils.test_utils import layer_test\n+from keras import regularizers\n+\n+\n+def test_convolutional_recurrent():\n+    num_row = 3\n+    num_col = 3\n+    filters = 5\n+    num_samples = 2\n+    input_channel = 2\n+    input_num_row = 5\n+    input_num_col = 5\n+    sequence_len = 2\n+    for data_format in ['channels_first', 'channels_last']:\n+\n+        if data_format == 'channels_first':\n+            inputs = np.random.rand(num_samples, sequence_len,\n+                                    input_channel,\n+                                    input_num_row, input_num_col)\n+        else:\n+            inputs = np.random.rand(num_samples, sequence_len,\n+                                    input_num_row, input_num_col,\n+                                    input_channel)\n+\n+        for return_sequences in [True, False]:\n+            # test for output shape:\n+            output = layer_test(convolutional_recurrent.ConvLSTM2D,\n+                                kwargs={'data_format': data_format,\n+                                        'return_sequences': return_sequences,\n+                                        'filters': filters,\n+                                        'kernel_size': (num_row, num_col),\n+                                        'padding': 'valid'},\n+                                input_shape=inputs.shape)\n+\n+            # No need to check following tests for both data formats\n+            if data_format == 'channels_first' or return_sequences:\n+                continue\n+\n+            # Tests for statefulness\n+            model = Sequential()\n+            kwargs = {'data_format': data_format,\n+                      'return_sequences': return_sequences,\n+                      'filters': filters,\n+                      'kernel_size': (num_row, num_col),\n+                      'stateful': True,\n+                      'batch_input_shape': inputs.shape,\n+                      'padding': 'same'}\n+            layer = convolutional_recurrent.ConvLSTM2D(**kwargs)\n+\n+            model.add(layer)\n+            model.compile(optimizer='sgd', loss='mse')\n+            out1 = model.predict(np.ones_like(inputs))\n+\n+            # train once so that the states change\n+            model.train_on_batch(np.ones_like(inputs),\n+                                 np.random.random(out1.shape))\n+            out2 = model.predict(np.ones_like(inputs))\n+\n+            # if the state is not reset, output should be different\n+            assert(out1.max() != out2.max())\n+\n+            # check that output changes after states are reset\n+            # (even though the model itself didn't change)\n+            layer.reset_states()\n+            out3 = model.predict(np.ones_like(inputs))\n+            assert(out2.max() != out3.max())\n+\n+            # check that container-level reset_states() works\n+            model.reset_states()\n+            out4 = model.predict(np.ones_like(inputs))\n+            assert_allclose(out3, out4, atol=1e-5)\n+\n+            # check that the call to `predict` updated the states\n+            out5 = model.predict(np.ones_like(inputs))\n+            assert(out4.max() != out5.max())\n+\n+            # check regularizers\n+            kwargs = {'data_format': data_format,\n+                      'return_sequences': return_sequences,\n+                      'kernel_size': (num_row, num_col),\n+                      'stateful': True,\n+                      'filters': filters,\n+                      'batch_input_shape': inputs.shape,\n+                      'kernel_regularizer': regularizers.L1L2(l1=0.01),\n+                      'recurrent_regularizer': regularizers.L1L2(l1=0.01),\n+                      'bias_regularizer': 'l2',\n+                      'kernel_constraint': 'max_norm',\n+                      'recurrent_constraint': 'max_norm',\n+                      'bias_constraint': 'max_norm',\n+                      'padding': 'same'}\n+\n+            layer = convolutional_recurrent.ConvLSTM2D(**kwargs)\n+            layer.build(inputs.shape)\n+            assert len(layer.losses) == 3\n+            output = layer(K.variable(np.ones(inputs.shape)))\n+            assert len(layer.losses) == 3\n+            K.eval(output)\n+\n+            # check dropout\n+            layer_test(convolutional_recurrent.ConvLSTM2D,\n+                       kwargs={'data_format': data_format,\n+                               'return_sequences': return_sequences,\n+                               'filters': filters,\n+                               'kernel_size': (num_row, num_col),\n+                               'padding': 'same',\n+                               'dropout': 0.1,\n+                               'recurrent_dropout': 0.1},\n+                       input_shape=inputs.shape)\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n@@ -0,0 +1,169 @@\n+import pytest\n+import numpy as np\n+from numpy.testing import assert_allclose\n+\n+from keras.utils.test_utils import layer_test\n+from keras.utils.test_utils import keras_test\n+from keras.layers import recurrent\n+from keras.layers import embeddings\n+from keras.models import Sequential\n+from keras.layers.core import Masking\n+from keras import regularizers\n+from keras import backend as K\n+\n+num_samples, timesteps, embedding_dim, units = 2, 5, 4, 3\n+embedding_num = 12\n+\n+\n+@keras_test\n+def rnn_test(f):\n+    \"\"\"\n+    All the recurrent layers share the same interface,\n+    so we can run through them with a single function.\n+    \"\"\"\n+    f = keras_test(f)\n+    return pytest.mark.parametrize('layer_class', [\n+        recurrent.SimpleRNN,\n+        recurrent.GRU,\n+        recurrent.LSTM\n+    ])(f)\n+\n+\n+@rnn_test\n+def test_return_sequences(layer_class):\n+    layer_test(layer_class,\n+               kwargs={'units': units,\n+                       'return_sequences': True},\n+               input_shape=(num_samples, timesteps, embedding_dim))\n+\n+\n+@rnn_test\n+def test_dynamic_behavior(layer_class):\n+    layer = layer_class(units, input_shape=(None, embedding_dim))\n+    model = Sequential()\n+    model.add(layer)\n+    model.compile('sgd', 'mse')\n+    x = np.random.random((num_samples, timesteps, embedding_dim))\n+    y = np.random.random((num_samples, units))\n+    model.train_on_batch(x, y)\n+\n+\n+@rnn_test\n+def test_dropout(layer_class):\n+    layer_test(layer_class,\n+               kwargs={'units': units,\n+                       'dropout': 0.1,\n+                       'recurrent_dropout': 0.1},\n+               input_shape=(num_samples, timesteps, embedding_dim))\n+\n+\n+@rnn_test\n+def test_implementation_mode(layer_class):\n+    for mode in [0, 1, 2]:\n+        layer_test(layer_class,\n+                   kwargs={'units': units,\n+                           'implementation': mode},\n+                   input_shape=(num_samples, timesteps, embedding_dim))\n+\n+\n+@rnn_test\n+def test_statefulness(layer_class):\n+    model = Sequential()\n+    model.add(embeddings.Embedding(embedding_num, embedding_dim,\n+                                   mask_zero=True,\n+                                   input_length=timesteps,\n+                                   batch_input_shape=(num_samples, timesteps)))\n+    layer = layer_class(units, return_sequences=False,\n+                        stateful=True,\n+                        weights=None)\n+    model.add(layer)\n+    model.compile(optimizer='sgd', loss='mse')\n+    out1 = model.predict(np.ones((num_samples, timesteps)))\n+    assert(out1.shape == (num_samples, units))\n+\n+    # train once so that the states change\n+    model.train_on_batch(np.ones((num_samples, timesteps)),\n+                         np.ones((num_samples, units)))\n+    out2 = model.predict(np.ones((num_samples, timesteps)))\n+\n+    # if the state is not reset, output should be different\n+    assert(out1.max() != out2.max())\n+\n+    # check that output changes after states are reset\n+    # (even though the model itself didn't change)\n+    layer.reset_states()\n+    out3 = model.predict(np.ones((num_samples, timesteps)))\n+    assert(out2.max() != out3.max())\n+\n+    # check that container-level reset_states() works\n+    model.reset_states()\n+    out4 = model.predict(np.ones((num_samples, timesteps)))\n+    assert_allclose(out3, out4, atol=1e-5)\n+\n+    # check that the call to `predict` updated the states\n+    out5 = model.predict(np.ones((num_samples, timesteps)))\n+    assert(out4.max() != out5.max())\n+\n+    # Check masking\n+    layer.reset_states()\n+\n+    left_padded_input = np.ones((num_samples, timesteps))\n+    left_padded_input[0, :1] = 0\n+    left_padded_input[1, :2] = 0\n+    out6 = model.predict(left_padded_input)\n+\n+    layer.reset_states()\n+\n+    right_padded_input = np.ones((num_samples, timesteps))\n+    right_padded_input[0, -1:] = 0\n+    right_padded_input[1, -2:] = 0\n+    out7 = model.predict(right_padded_input)\n+\n+    assert_allclose(out7, out6, atol=1e-5)\n+\n+\n+@rnn_test\n+def test_regularizer(layer_class):\n+    layer = layer_class(units, return_sequences=False, weights=None,\n+                        batch_input_shape=(num_samples, timesteps, embedding_dim),\n+                        kernel_regularizer=regularizers.l1(0.01),\n+                        recurrent_regularizer=regularizers.l1(0.01),\n+                        bias_regularizer='l2')\n+    layer.build((None, None, 2))\n+    assert len(layer.losses) == 3\n+    layer(K.variable(np.ones((2, 3, 2))))\n+    assert len(layer.losses) == 3\n+\n+\n+@keras_test\n+def test_masking_layer():\n+    ''' This test based on a previously failing issue here:\n+    https://github.com/fchollet/keras/issues/1567\n+    '''\n+    inputs = np.random.random((6, 3, 4))\n+    targets = np.abs(np.random.random((6, 3, 5)))\n+    targets /= targets.sum(axis=-1, keepdims=True)\n+\n+    model = Sequential()\n+    model.add(Masking(input_shape=(3, 4)))\n+    model.add(recurrent.LSTM(units=5, return_sequences=True, unroll=False))\n+    model.compile(loss='categorical_crossentropy', optimizer='adam')\n+    model.fit(inputs, targets, epochs=1, batch_size=100, verbose=1)\n+\n+    model = Sequential()\n+    model.add(Masking(input_shape=(3, 4)))\n+    model.add(recurrent.LSTM(units=5, return_sequences=True, unroll=True))\n+    model.compile(loss='categorical_crossentropy', optimizer='adam')\n+    model.fit(inputs, targets, epochs=1, batch_size=100, verbose=1)\n+\n+\n+@rnn_test\n+def test_from_config(layer_class):\n+    for stateful in (False, True):\n+        l1 = layer_class(units=1, stateful=stateful)\n+        l2 = layer_class.from_config(l1.get_config())\n+        assert l1.get_config() == l2.get_config()\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n@@ -0,0 +1,557 @@\n+\n+@keras_test\n+def test_highway():\n+    from keras import regularizers\n+    from keras import constraints\n+\n+    layer_test(core.Highway,\n+               kwargs={},\n+               input_shape=(3, 2))\n+\n+    layer_test(core.Highway,\n+               kwargs={'W_regularizer': regularizers.l2(0.01),\n+                       'b_regularizer': regularizers.l1(0.01),\n+                       'activity_regularizer': regularizers.activity_l2(0.01),\n+                       'W_constraint': constraints.MaxNorm(1),\n+                       'b_constraint': constraints.MaxNorm(1)},\n+               input_shape=(3, 2))\n+\n+\n+@keras_test\n+def test_maxout_dense():\n+    layer_test(layers.MaxoutDense,\n+               kwargs={'output_dim': 3},\n+               input_shape=(3, 2))\n+\n+    layer_test(layers.MaxoutDense,\n+               kwargs={'output_dim': 3,\n+                       'W_regularizer': regularizers.l2(0.01),\n+                       'b_regularizer': regularizers.l1(0.01),\n+                       'activity_regularizer': regularizers.activity_l2(0.01),\n+                       'W_constraint': constraints.MaxNorm(1),\n+                       'b_constraint': constraints.MaxNorm(1)},\n+               input_shape=(3, 2))\n+\n+\n+@keras_test\n+def test_merge():\n+    # test modes: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot'.\n+    input_shapes = [(3, 2), (3, 2)]\n+    inputs = [np.random.random(shape) for shape in input_shapes]\n+\n+    # test functional API\n+    for mode in ['sum', 'mul', 'concat', 'ave', 'max']:\n+        print(mode)\n+        input_a = Input(shape=input_shapes[0][1:])\n+        input_b = Input(shape=input_shapes[1][1:])\n+        merged = merge([input_a, input_b], mode=mode)\n+        model = Model([input_a, input_b], merged)\n+        model.compile('rmsprop', 'mse')\n+\n+        expected_output_shape = model.compute_output_shape(input_shapes)\n+        actual_output_shape = model.predict(inputs).shape\n+        assert expected_output_shape == actual_output_shape\n+\n+        config = model.get_config()\n+        model = Model.from_config(config)\n+        model.compile('rmsprop', 'mse')\n+\n+        # test Merge (#2460)\n+        merged = Merge(mode=mode)([input_a, input_b])\n+        model = Model([input_a, input_b], merged)\n+        model.compile('rmsprop', 'mse')\n+\n+        expected_output_shape = model.compute_output_shape(input_shapes)\n+        actual_output_shape = model.predict(inputs).shape\n+        assert expected_output_shape == actual_output_shape\n+\n+    # test lambda with output_shape lambda\n+    input_a = Input(shape=input_shapes[0][1:])\n+    input_b = Input(shape=input_shapes[1][1:])\n+    merged = merge([input_a, input_b],\n+                   mode=lambda tup: K.concatenate([tup[0], tup[1]]),\n+                   output_shape=lambda tup: tup[0][:-1] + (tup[0][-1] + tup[1][-1],))\n+    model = Model([input_a, input_b], merged)\n+    expected_output_shape = model.compute_output_shape(input_shapes)\n+    actual_output_shape = model.predict(inputs).shape\n+    assert expected_output_shape == actual_output_shape\n+\n+    config = model.get_config()\n+    model = Model.from_config(config)\n+    model.compile('rmsprop', 'mse')\n+\n+    # test function with output_shape function\n+    def fn_mode(tup):\n+        x, y = tup\n+        return K.concatenate([x, y], axis=1)\n+\n+    def fn_output_shape(tup):\n+        s1, s2 = tup\n+        return (s1[0], s1[1] + s2[1]) + s1[2:]\n+\n+    input_a = Input(shape=input_shapes[0][1:])\n+    input_b = Input(shape=input_shapes[1][1:])\n+    merged = merge([input_a, input_b],\n+                   mode=fn_mode,\n+                   output_shape=fn_output_shape)\n+    model = Model([input_a, input_b], merged)\n+    expected_output_shape = model.compute_output_shape(input_shapes)\n+    actual_output_shape = model.predict(inputs).shape\n+    assert expected_output_shape == actual_output_shape\n+\n+    config = model.get_config()\n+    model = Model.from_config(config)\n+    model.compile('rmsprop', 'mse')\n+\n+    # test function with output_mask function\n+    # time dimension is required for masking\n+    input_shapes = [(4, 3, 2), (4, 3, 2)]\n+    inputs = [np.random.random(shape) for shape in input_shapes]\n+\n+    def fn_output_mask(tup):\n+        x_mask, y_mask = tup\n+        return K.concatenate([x_mask, y_mask])\n+\n+    input_a = Input(shape=input_shapes[0][1:])\n+    input_b = Input(shape=input_shapes[1][1:])\n+    a = Masking()(input_a)\n+    b = Masking()(input_b)\n+    merged = merge([a, b], mode=fn_mode, output_shape=fn_output_shape, output_mask=fn_output_mask)\n+    model = Model([input_a, input_b], merged)\n+    expected_output_shape = model.compute_output_shape(input_shapes)\n+    actual_output_shape = model.predict(inputs).shape\n+    assert expected_output_shape == actual_output_shape\n+\n+    config = model.get_config()\n+    model = Model.from_config(config)\n+    model.compile('rmsprop', 'mse')\n+\n+    mask_inputs = (np.zeros(input_shapes[0][:-1]), np.ones(input_shapes[1][:-1]))\n+    expected_mask_output = np.concatenate(mask_inputs, axis=-1)\n+    mask_input_placeholders = [K.placeholder(shape=input_shape[:-1]) for input_shape in input_shapes]\n+    mask_output = model.layers[-1]._output_mask(mask_input_placeholders)\n+    assert np.all(K.function(mask_input_placeholders, [mask_output])(mask_inputs)[0] == expected_mask_output)\n+\n+    # test lambda with output_mask lambda\n+    input_a = Input(shape=input_shapes[0][1:])\n+    input_b = Input(shape=input_shapes[1][1:])\n+    a = Masking()(input_a)\n+    b = Masking()(input_b)\n+    merged = merge([a, b], mode=lambda tup: K.concatenate([tup[0], tup[1]], axis=1),\n+                   output_shape=lambda tup: (tup[0][0], tup[0][1] + tup[1][1]) + tup[0][2:],\n+                   output_mask=lambda tup: K.concatenate([tup[0], tup[1]]))\n+    model = Model([input_a, input_b], merged)\n+    expected_output_shape = model.compute_output_shape(input_shapes)\n+    actual_output_shape = model.predict(inputs).shape\n+    assert expected_output_shape == actual_output_shape\n+\n+    config = model.get_config()\n+    model = Model.from_config(config)\n+    model.compile('rmsprop', 'mse')\n+\n+    mask_output = model.layers[-1]._output_mask(mask_input_placeholders)\n+    assert np.all(K.function(mask_input_placeholders, [mask_output])(mask_inputs)[0] == expected_mask_output)\n+\n+    # test with arguments\n+    input_shapes = [(3, 2), (3, 2)]\n+    inputs = [np.random.random(shape) for shape in input_shapes]\n+\n+    def fn_mode(tup, a, b):\n+        x, y = tup\n+        return x * a + y * b\n+\n+    input_a = Input(shape=input_shapes[0][1:])\n+    input_b = Input(shape=input_shapes[1][1:])\n+    merged = merge([input_a, input_b], mode=fn_mode, output_shape=lambda s: s[0], arguments={'a': 0.7, 'b': 0.3})\n+    model = Model([input_a, input_b], merged)\n+    output = model.predict(inputs)\n+\n+    config = model.get_config()\n+    model = Model.from_config(config)\n+\n+    assert np.all(model.predict(inputs) == output)\n+\n+\n+@keras_test\n+def test_merge_mask_2d():\n+    from keras.layers import Input, merge, Masking\n+    from keras.models import Model\n+\n+    rand = lambda *shape: np.asarray(np.random.random(shape) > 0.5, dtype='int32')\n+\n+    # inputs\n+    input_a = Input(shape=(3,))\n+    input_b = Input(shape=(3,))\n+\n+    # masks\n+    masked_a = Masking(mask_value=0)(input_a)\n+    masked_b = Masking(mask_value=0)(input_b)\n+\n+    # three different types of merging\n+    merged_sum = merge([masked_a, masked_b], mode='sum')\n+    merged_concat = merge([masked_a, masked_b], mode='concat', concat_axis=1)\n+    merged_concat_mixed = merge([masked_a, input_b], mode='concat', concat_axis=1)\n+\n+    # test sum\n+    model_sum = Model([input_a, input_b], [merged_sum])\n+    model_sum.compile(loss='mse', optimizer='sgd')\n+    model_sum.fit([rand(2, 3), rand(2, 3)], [rand(2, 3)], epochs=1)\n+\n+    # test concatenation\n+    model_concat = Model([input_a, input_b], [merged_concat])\n+    model_concat.compile(loss='mse', optimizer='sgd')\n+    model_concat.fit([rand(2, 3), rand(2, 3)], [rand(2, 6)], epochs=1)\n+\n+    # test concatenation with masked and non-masked inputs\n+    model_concat = Model([input_a, input_b], [merged_concat_mixed])\n+    model_concat.compile(loss='mse', optimizer='sgd')\n+    model_concat.fit([rand(2, 3), rand(2, 3)], [rand(2, 6)], epochs=1)\n+\n+\n+@keras_test\n+def test_merge_mask_3d():\n+    from keras.layers import Input, merge, Embedding, SimpleRNN\n+    from keras.models import Model\n+\n+    rand = lambda *shape: np.asarray(np.random.random(shape) > 0.5, dtype='int32')\n+\n+    # embeddings\n+    input_a = Input(shape=(3,), dtype='int32')\n+    input_b = Input(shape=(3,), dtype='int32')\n+    embedding = Embedding(3, 4, mask_zero=True)\n+    embedding_a = embedding(input_a)\n+    embedding_b = embedding(input_b)\n+\n+    # rnn\n+    rnn = SimpleRNN(3, return_sequences=True)\n+    rnn_a = rnn(embedding_a)\n+    rnn_b = rnn(embedding_b)\n+\n+    # concatenation\n+    merged_concat = merge([rnn_a, rnn_b], mode='concat', concat_axis=-1)\n+    model = Model([input_a, input_b], [merged_concat])\n+    model.compile(loss='mse', optimizer='sgd')\n+    model.fit([rand(2, 3), rand(2, 3)], [rand(2, 3, 6)])\n+\n+\n+\n+    \n+\n+@keras_test\n+def test_sequential_regression():\n+    from keras.models import Sequential, Model\n+    from keras.layers import Merge, Embedding, BatchNormalization, LSTM, InputLayer, Input\n+\n+    # start with a basic example of using a Sequential model\n+    # inside the functional API\n+    seq = Sequential()\n+    seq.add(Dense(input_dim=10, output_dim=10))\n+\n+    x = Input(shape=(10,))\n+    y = seq(x)\n+    model = Model(x, y)\n+    model.compile('rmsprop', 'mse')\n+    weights = model.get_weights()\n+\n+    # test serialization\n+    config = model.get_config()\n+    model = Model.from_config(config)\n+    model.compile('rmsprop', 'mse')\n+    model.set_weights(weights)\n+\n+    # more advanced model with multiple branches\n+\n+    branch_1 = Sequential(name='branch_1')\n+    branch_1.add(Embedding(input_dim=100,\n+                           output_dim=10,\n+                           input_length=2,\n+                           name='embed_1'))\n+    branch_1.add(LSTM(32, name='lstm_1'))\n+\n+    branch_2 = Sequential(name='branch_2')\n+    branch_2.add(Dense(32, input_shape=(8,), name='dense_2'))\n+\n+    branch_3 = Sequential(name='branch_3')\n+    branch_3.add(Dense(32, input_shape=(6,), name='dense_3'))\n+\n+    branch_1_2 = Sequential([Merge([branch_1, branch_2], mode='concat')], name='branch_1_2')\n+    branch_1_2.add(Dense(16, name='dense_1_2-0'))\n+    # test whether impromtu input_shape breaks the model\n+    branch_1_2.add(Dense(16, input_shape=(16,), name='dense_1_2-1'))\n+\n+    model = Sequential([Merge([branch_1_2, branch_3], mode='concat')], name='final')\n+    model.add(Dense(16, name='dense_final'))\n+    model.compile(optimizer='rmsprop',\n+                  loss='categorical_crossentropy',\n+                  metrics=['accuracy'])\n+\n+    model.summary()\n+\n+    x = (100 * np.random.random((100, 2))).astype('int32')\n+    y = np.random.random((100, 8))\n+    z = np.random.random((100, 6))\n+    labels = np.random.random((100, 16))\n+    model.fit([x, y, z], labels, epochs=1)\n+\n+    # test if Sequential can be called in the functional API\n+\n+    a = Input(shape=(2,), dtype='int32')\n+    b = Input(shape=(8,))\n+    c = Input(shape=(6,))\n+    o = model([a, b, c])\n+\n+    outer_model = Model([a, b, c], o)\n+    outer_model.compile(optimizer='rmsprop',\n+                        loss='categorical_crossentropy',\n+                        metrics=['accuracy'])\n+    outer_model.fit([x, y, z], labels, epochs=1)\n+\n+    # test serialization\n+    config = outer_model.get_config()\n+    outer_model = Model.from_config(config)\n+    outer_model.compile(optimizer='rmsprop',\n+                        loss='categorical_crossentropy',\n+                        metrics=['accuracy'])\n+    outer_model.fit([x, y, z], labels, epochs=1)\n+\n+\n+\n+\n+@keras_test\n+def test_merge_sum():\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n+    left = Sequential()\n+    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='sum'))\n+    model.add(Dense(num_class))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test], y_test))\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n+\n+    loss = model.evaluate([x_test, x_test], y_test, verbose=0)\n+\n+    model.predict([x_test, x_test], verbose=0)\n+    model.predict_classes([x_test, x_test], verbose=0)\n+    model.predict_proba([x_test, x_test], verbose=0)\n+\n+    # test weight saving\n+    fname = 'test_merge_sum_temp.h5'\n+    model.save_weights(fname, overwrite=True)\n+    left = Sequential()\n+    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+    right = Sequential()\n+    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='sum'))\n+    model.add(Dense(num_class))\n+    model.add(Activation('softmax'))\n+    model.load_weights(fname)\n+    os.remove(fname)\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    nloss = model.evaluate([x_test, x_test], y_test, verbose=0)\n+    assert(loss == nloss)\n+\n+    # test serialization\n+    config = model.get_config()\n+    Sequential.from_config(config)\n+\n+    model.summary()\n+    json_str = model.to_json()\n+    model_from_json(json_str)\n+\n+    yaml_str = model.to_yaml()\n+    model_from_yaml(yaml_str)\n+\n+\n+@keras_test\n+def test_merge_dot():\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n+\n+    left = Sequential()\n+    left.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n+    right.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='dot', dot_axes=1))\n+    model.add(Dense(num_class))\n+    model.add(Activation('softmax'))\n+\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    left = Sequential()\n+    left.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n+    right.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='dot', dot_axes=[1, 1]))\n+    model.add(Dense(num_class))\n+    model.add(Activation('softmax'))\n+\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+\n+@keras_test\n+def test_merge_concat():\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n+\n+    left = Sequential(name='branch_1')\n+    left.add(Dense(num_hidden, input_shape=(input_dim,), name='dense_1'))\n+    left.add(Activation('relu', name='relu_1'))\n+\n+    right = Sequential(name='branch_2')\n+    right.add(Dense(num_hidden, input_shape=(input_dim,), name='dense_2'))\n+    right.add(Activation('relu', name='relu_2'))\n+\n+    model = Sequential(name='merged_branches')\n+    model.add(Merge([left, right], mode='concat', name='merge'))\n+    model.add(Dense(num_class, name='final_dense'))\n+    model.add(Activation('softmax', name='softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test], y_test))\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n+\n+    loss = model.evaluate([x_test, x_test], y_test, verbose=0)\n+\n+    model.predict([x_test, x_test], verbose=0)\n+    model.predict_classes([x_test, x_test], verbose=0)\n+    model.predict_proba([x_test, x_test], verbose=0)\n+    model.get_config()\n+\n+    fname = 'test_merge_concat_temp.h5'\n+    model.save_weights(fname, overwrite=True)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.load_weights(fname)\n+    os.remove(fname)\n+\n+    nloss = model.evaluate([x_test, x_test], y_test, verbose=0)\n+    assert(loss == nloss)\n+\n+\n+@keras_test\n+def test_merge_recursivity():\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n+    left = Sequential()\n+    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+\n+    righter = Sequential()\n+    righter.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    righter.add(Activation('relu'))\n+\n+    intermediate = Sequential()\n+    intermediate.add(Merge([left, right], mode='sum'))\n+    intermediate.add(Dense(num_hidden))\n+    intermediate.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([intermediate, righter], mode='sum'))\n+    model.add(Dense(num_class))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test, x_test], y_test))\n+    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n+    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n+\n+    loss = model.evaluate([x_test, x_test, x_test], y_test, verbose=0)\n+\n+    model.predict([x_test, x_test, x_test], verbose=0)\n+    model.predict_classes([x_test, x_test, x_test], verbose=0)\n+    model.predict_proba([x_test, x_test, x_test], verbose=0)\n+\n+    fname = 'test_merge_recursivity_temp.h5'\n+    model.save_weights(fname, overwrite=True)\n+    model.load_weights(fname)\n+    os.remove(fname)\n+\n+    nloss = model.evaluate([x_test, x_test, x_test], y_test, verbose=0)\n+    assert(loss == nloss)\n+\n+    # test serialization\n+    config = model.get_config()\n+    Sequential.from_config(config)\n+\n+    model.summary()\n+    json_str = model.to_json()\n+    model_from_json(json_str)\n+\n+    yaml_str = model.to_yaml()\n+    model_from_yaml(yaml_str)\n+\n+\n+@keras_test\n+def test_merge_overlap():\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n+    left = Sequential()\n+    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([left, left], mode='sum'))\n+    model.add(Dense(num_class))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_split=0.1)\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=False)\n+\n+    model.train_on_batch(x_train[:32], y_train[:32])\n+\n+    loss = model.evaluate(x_test, y_test, verbose=0)\n+    model.predict(x_test, verbose=0)\n+    model.predict_classes(x_test, verbose=0)\n+    model.predict_proba(x_test, verbose=0)\n+\n+    fname = 'test_merge_overlap_temp.h5'\n+    print(model.layers)\n+    model.save_weights(fname, overwrite=True)\n+    print(model.trainable_weights)\n+\n+    model.load_weights(fname)\n+    os.remove(fname)\n+\n+    nloss = model.evaluate(x_test, y_test, verbose=0)\n+    assert(loss == nloss)\n+\n+    # test serialization\n+    config = model.get_config()\n+    Sequential.from_config(config)\n+\n+    model.summary()\n+    json_str = model.to_json()\n+    model_from_json(json_str)\n+\n+    yaml_str = model.to_yaml()\n+    model_from_yaml(yaml_str)\n\\ No newline at end of file\n\n@@ -0,0 +1,279 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+import pytest\n+import os\n+import numpy as np\n+\n+from keras.models import Sequential\n+from keras.layers import Dense, Activation\n+from keras.legacy.layers import Merge\n+from keras.utils import np_utils\n+from keras.utils.test_utils import get_test_data, keras_test\n+from keras.models import model_from_json, model_from_yaml\n+\n+\n+input_dim = 16\n+num_hidden = 8\n+num_class = 4\n+batch_size = 32\n+epochs = 1\n+\n+\n+def _get_test_data():\n+    np.random.seed(1234)\n+\n+    train_samples = 100\n+    test_samples = 50\n+\n+    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=train_samples,\n+                                                         num_test=test_samples,\n+                                                         input_shape=(input_dim,),\n+                                                         classification=True,\n+                                                         num_classes=4)\n+    y_test = np_utils.to_categorical(y_test)\n+    y_train = np_utils.to_categorical(y_train)\n+    return (x_train, y_train), (x_test, y_test)\n+\n+\n+@keras_test\n+def test_merge_sum():\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n+    left = Sequential()\n+    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='sum'))\n+    model.add(Dense(num_class))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test], y_test))\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n+\n+    loss = model.evaluate([x_test, x_test], y_test, verbose=0)\n+\n+    model.predict([x_test, x_test], verbose=0)\n+    model.predict_classes([x_test, x_test], verbose=0)\n+    model.predict_proba([x_test, x_test], verbose=0)\n+\n+    # test weight saving\n+    fname = 'test_merge_sum_temp.h5'\n+    model.save_weights(fname, overwrite=True)\n+    left = Sequential()\n+    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+    right = Sequential()\n+    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='sum'))\n+    model.add(Dense(num_class))\n+    model.add(Activation('softmax'))\n+    model.load_weights(fname)\n+    os.remove(fname)\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    nloss = model.evaluate([x_test, x_test], y_test, verbose=0)\n+    assert(loss == nloss)\n+\n+    # test serialization\n+    config = model.get_config()\n+    Sequential.from_config(config)\n+\n+    model.summary()\n+    json_str = model.to_json()\n+    model_from_json(json_str)\n+\n+    yaml_str = model.to_yaml()\n+    model_from_yaml(yaml_str)\n+\n+\n+@keras_test\n+def test_merge_dot():\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n+\n+    left = Sequential()\n+    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='dot', dot_axes=1))\n+    model.add(Dense(num_class))\n+    model.add(Activation('softmax'))\n+\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    left = Sequential()\n+    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='dot', dot_axes=[1, 1]))\n+    model.add(Dense(num_class))\n+    model.add(Activation('softmax'))\n+\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+\n+@keras_test\n+def test_merge_concat():\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n+\n+    left = Sequential(name='branch_1')\n+    left.add(Dense(num_hidden, input_shape=(input_dim,), name='dense_1'))\n+    left.add(Activation('relu', name='relu_1'))\n+\n+    right = Sequential(name='branch_2')\n+    right.add(Dense(num_hidden, input_shape=(input_dim,), name='dense_2'))\n+    right.add(Activation('relu', name='relu_2'))\n+\n+    model = Sequential(name='merged_branches')\n+    model.add(Merge([left, right], mode='concat', name='merge'))\n+    model.add(Dense(num_class, name='final_dense'))\n+    model.add(Activation('softmax', name='softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test], y_test))\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n+\n+    loss = model.evaluate([x_test, x_test], y_test, verbose=0)\n+\n+    model.predict([x_test, x_test], verbose=0)\n+    model.predict_classes([x_test, x_test], verbose=0)\n+    model.predict_proba([x_test, x_test], verbose=0)\n+    model.get_config()\n+\n+    fname = 'test_merge_concat_temp.h5'\n+    model.save_weights(fname, overwrite=True)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.load_weights(fname)\n+    os.remove(fname)\n+\n+    nloss = model.evaluate([x_test, x_test], y_test, verbose=0)\n+    assert(loss == nloss)\n+\n+\n+@keras_test\n+def test_merge_recursivity():\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n+    left = Sequential()\n+    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+\n+    righter = Sequential()\n+    righter.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    righter.add(Activation('relu'))\n+\n+    intermediate = Sequential()\n+    intermediate.add(Merge([left, right], mode='sum'))\n+    intermediate.add(Dense(num_hidden))\n+    intermediate.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([intermediate, righter], mode='sum'))\n+    model.add(Dense(num_class))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test, x_test], y_test))\n+    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n+    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n+\n+    loss = model.evaluate([x_test, x_test, x_test], y_test, verbose=0)\n+\n+    model.predict([x_test, x_test, x_test], verbose=0)\n+    model.predict_classes([x_test, x_test, x_test], verbose=0)\n+    model.predict_proba([x_test, x_test, x_test], verbose=0)\n+\n+    fname = 'test_merge_recursivity_temp.h5'\n+    model.save_weights(fname, overwrite=True)\n+    model.load_weights(fname)\n+    os.remove(fname)\n+\n+    nloss = model.evaluate([x_test, x_test, x_test], y_test, verbose=0)\n+    assert(loss == nloss)\n+\n+    # test serialization\n+    config = model.get_config()\n+    Sequential.from_config(config)\n+\n+    model.summary()\n+    json_str = model.to_json()\n+    model_from_json(json_str)\n+\n+    yaml_str = model.to_yaml()\n+    model_from_yaml(yaml_str)\n+\n+\n+@keras_test\n+def test_merge_overlap():\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n+    left = Sequential()\n+    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([left, left], mode='sum'))\n+    model.add(Dense(num_class))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_split=0.1)\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=False)\n+\n+    model.train_on_batch(x_train[:32], y_train[:32])\n+\n+    loss = model.evaluate(x_test, y_test, verbose=0)\n+    model.predict(x_test, verbose=0)\n+    model.predict_classes(x_test, verbose=0)\n+    model.predict_proba(x_test, verbose=0)\n+\n+    fname = 'test_merge_overlap_temp.h5'\n+    print(model.layers)\n+    model.save_weights(fname, overwrite=True)\n+    print(model.trainable_weights)\n+\n+    model.load_weights(fname)\n+    os.remove(fname)\n+\n+    nloss = model.evaluate(x_test, y_test, verbose=0)\n+    assert(loss == nloss)\n+\n+    # test serialization\n+    config = model.get_config()\n+    Sequential.from_config(config)\n+\n+    model.summary()\n+    json_str = model.to_json()\n+    model_from_json(json_str)\n+\n+    yaml_str = model.to_yaml()\n+    model_from_yaml(yaml_str)\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n@@ -221,246 +221,6 @@ def test_nested_sequential():\n     model_from_yaml(yaml_str)\n \n \n-@keras_test\n-def test_merge_sum():\n-    (x_train, y_train), (x_test, y_test) = _get_test_data()\n-    left = Sequential()\n-    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    left.add(Activation('relu'))\n-\n-    right = Sequential()\n-    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    right.add(Activation('relu'))\n-\n-    model = Sequential()\n-    model.add(Merge([left, right], mode='sum'))\n-    model.add(Dense(num_class))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test], y_test))\n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n-\n-    loss = model.evaluate([x_test, x_test], y_test, verbose=0)\n-\n-    model.predict([x_test, x_test], verbose=0)\n-    model.predict_classes([x_test, x_test], verbose=0)\n-    model.predict_proba([x_test, x_test], verbose=0)\n-\n-    # test weight saving\n-    fname = 'test_merge_sum_temp.h5'\n-    model.save_weights(fname, overwrite=True)\n-    left = Sequential()\n-    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    left.add(Activation('relu'))\n-    right = Sequential()\n-    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    right.add(Activation('relu'))\n-    model = Sequential()\n-    model.add(Merge([left, right], mode='sum'))\n-    model.add(Dense(num_class))\n-    model.add(Activation('softmax'))\n-    model.load_weights(fname)\n-    os.remove(fname)\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    nloss = model.evaluate([x_test, x_test], y_test, verbose=0)\n-    assert(loss == nloss)\n-\n-    # test serialization\n-    config = model.get_config()\n-    Sequential.from_config(config)\n-\n-    model.summary()\n-    json_str = model.to_json()\n-    model_from_json(json_str)\n-\n-    yaml_str = model.to_yaml()\n-    model_from_yaml(yaml_str)\n-\n-\n-@keras_test\n-def test_merge_dot():\n-    (x_train, y_train), (x_test, y_test) = _get_test_data()\n-\n-    left = Sequential()\n-    left.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n-    left.add(Activation('relu'))\n-\n-    right = Sequential()\n-    right.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n-    right.add(Activation('relu'))\n-\n-    model = Sequential()\n-    model.add(Merge([left, right], mode='dot', dot_axes=1))\n-    model.add(Dense(num_class))\n-    model.add(Activation('softmax'))\n-\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    left = Sequential()\n-    left.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n-    left.add(Activation('relu'))\n-\n-    right = Sequential()\n-    right.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n-    right.add(Activation('relu'))\n-\n-    model = Sequential()\n-    model.add(Merge([left, right], mode='dot', dot_axes=[1, 1]))\n-    model.add(Dense(num_class))\n-    model.add(Activation('softmax'))\n-\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-\n-@keras_test\n-def test_merge_concat():\n-    (x_train, y_train), (x_test, y_test) = _get_test_data()\n-\n-    left = Sequential(name='branch_1')\n-    left.add(Dense(num_hidden, input_shape=(input_dim,), name='dense_1'))\n-    left.add(Activation('relu', name='relu_1'))\n-\n-    right = Sequential(name='branch_2')\n-    right.add(Dense(num_hidden, input_shape=(input_dim,), name='dense_2'))\n-    right.add(Activation('relu', name='relu_2'))\n-\n-    model = Sequential(name='merged_branches')\n-    model.add(Merge([left, right], mode='concat', name='merge'))\n-    model.add(Dense(num_class, name='final_dense'))\n-    model.add(Activation('softmax', name='softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test], y_test))\n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n-\n-    loss = model.evaluate([x_test, x_test], y_test, verbose=0)\n-\n-    model.predict([x_test, x_test], verbose=0)\n-    model.predict_classes([x_test, x_test], verbose=0)\n-    model.predict_proba([x_test, x_test], verbose=0)\n-    model.get_config()\n-\n-    fname = 'test_merge_concat_temp.h5'\n-    model.save_weights(fname, overwrite=True)\n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.load_weights(fname)\n-    os.remove(fname)\n-\n-    nloss = model.evaluate([x_test, x_test], y_test, verbose=0)\n-    assert(loss == nloss)\n-\n-\n-@keras_test\n-def test_merge_recursivity():\n-    (x_train, y_train), (x_test, y_test) = _get_test_data()\n-    left = Sequential()\n-    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    left.add(Activation('relu'))\n-\n-    right = Sequential()\n-    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    right.add(Activation('relu'))\n-\n-    righter = Sequential()\n-    righter.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    righter.add(Activation('relu'))\n-\n-    intermediate = Sequential()\n-    intermediate.add(Merge([left, right], mode='sum'))\n-    intermediate.add(Dense(num_hidden))\n-    intermediate.add(Activation('relu'))\n-\n-    model = Sequential()\n-    model.add(Merge([intermediate, righter], mode='sum'))\n-    model.add(Dense(num_class))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test, x_test], y_test))\n-    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n-    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n-\n-    loss = model.evaluate([x_test, x_test, x_test], y_test, verbose=0)\n-\n-    model.predict([x_test, x_test, x_test], verbose=0)\n-    model.predict_classes([x_test, x_test, x_test], verbose=0)\n-    model.predict_proba([x_test, x_test, x_test], verbose=0)\n-\n-    fname = 'test_merge_recursivity_temp.h5'\n-    model.save_weights(fname, overwrite=True)\n-    model.load_weights(fname)\n-    os.remove(fname)\n-\n-    nloss = model.evaluate([x_test, x_test, x_test], y_test, verbose=0)\n-    assert(loss == nloss)\n-\n-    # test serialization\n-    config = model.get_config()\n-    Sequential.from_config(config)\n-\n-    model.summary()\n-    json_str = model.to_json()\n-    model_from_json(json_str)\n-\n-    yaml_str = model.to_yaml()\n-    model_from_yaml(yaml_str)\n-\n-\n-@keras_test\n-def test_merge_overlap():\n-    (x_train, y_train), (x_test, y_test) = _get_test_data()\n-    left = Sequential()\n-    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    left.add(Activation('relu'))\n-\n-    model = Sequential()\n-    model.add(Merge([left, left], mode='sum'))\n-    model.add(Dense(num_class))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n-    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_split=0.1)\n-    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=False)\n-\n-    model.train_on_batch(x_train[:32], y_train[:32])\n-\n-    loss = model.evaluate(x_test, y_test, verbose=0)\n-    model.predict(x_test, verbose=0)\n-    model.predict_classes(x_test, verbose=0)\n-    model.predict_proba(x_test, verbose=0)\n-\n-    fname = 'test_merge_overlap_temp.h5'\n-    print(model.layers)\n-    model.save_weights(fname, overwrite=True)\n-    print(model.trainable_weights)\n-\n-    model.load_weights(fname)\n-    os.remove(fname)\n-\n-    nloss = model.evaluate(x_test, y_test, verbose=0)\n-    assert(loss == nloss)\n-\n-    # test serialization\n-    config = model.get_config()\n-    Sequential.from_config(config)\n-\n-    model.summary()\n-    json_str = model.to_json()\n-    model_from_json(json_str)\n-\n-    yaml_str = model.to_yaml()\n-    model_from_yaml(yaml_str)\n-\n-\n @keras_test\n def test_sequential_count_params():\n     input_dim = 20\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
