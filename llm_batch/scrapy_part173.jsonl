{"custom_id": "scrapy#2177d4feec9de13e23eba89bf52cf884b0731235", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 677 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,7 +33,7 @@ from scrapy.exceptions import NotConfigured\n \n class DummyDH(object):\n \n-    def __init__(self, settings):\n+    def __init__(self, crawler):\n         pass\n \n \n@@ -61,6 +61,7 @@ class LoadTestCase(unittest.TestCase):\n         handlers = {'scheme': None}\n         dh = DownloadHandlers(get_crawler({'DOWNLOAD_HANDLERS': handlers}))\n         self.assertNotIn('scheme', dh._handlers)\n+        self.assertNotIn('scheme', dh._notconfigured)\n \n \n class FileTestCase(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fd7b40e892e36a613993a21ad5305ff9f0602491", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 212 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -39,7 +39,7 @@ class BaseRedirectMiddleware(object):\n         else:\n             log.msg(format=\"Discarding %(request)s: max redirections reached\",\n                     level=log.DEBUG, spider=spider, request=request)\n-            raise IgnoreRequest\n+            raise IgnoreRequest(\"max redirections reached\")\n \n     def _redirect_request_using_get(self, request, redirect_url):\n         redirected = request.replace(url=redirect_url, method='GET', body='')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#15454e7845a0818915e231fb39292d92e4400131", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 31 | Churn Cumulative: 194 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,17 +6,22 @@ from scrapy.contrib.spidermiddleware.httperror import HttpErrorMiddleware, HttpE\n from scrapy.settings import Settings\n \n \n+def _responses(request, status_codes):\n+    responses = []\n+    for code in status_codes:\n+        response = Response(request.url, status=code)\n+        response.request = request\n+        responses.append(response)\n+    return responses\n+\n+\n class TestHttpErrorMiddleware(TestCase):\n \n     def setUp(self):\n         self.spider = Spider('foo')\n         self.mw = HttpErrorMiddleware(Settings({}))\n         self.req = Request('http://scrapytest.org')\n-\n-        self.res200 = Response('http://scrapytest.org', status=200)\n-        self.res200.request = self.req\n-        self.res404 = Response('http://scrapytest.org', status=404)\n-        self.res404.request = self.req\n+        self.res200, self.res404 = _responses(self.req, [200, 404])\n \n     def test_process_spider_input(self):\n         self.assertEquals(None,\n@@ -50,13 +55,7 @@ class TestHttpErrorMiddlewareSettings(TestCase):\n         self.spider = Spider('foo')\n         self.mw = HttpErrorMiddleware(Settings({'HTTPERROR_ALLOWED_CODES': (402,)}))\n         self.req = Request('http://scrapytest.org')\n-\n-        self.res200 = Response('http://scrapytest.org', status=200)\n-        self.res200.request = self.req\n-        self.res404 = Response('http://scrapytest.org', status=404)\n-        self.res404.request = self.req\n-        self.res402 = Response('http://scrapytest.org', status=402)\n-        self.res402.request = self.req\n+        self.res200, self.res404, self.res402 = _responses(self.req, [200, 404, 402])\n \n     def test_process_spider_input(self):\n         self.assertEquals(None,\n@@ -92,13 +91,7 @@ class TestHttpErrorMiddlewareHandleAll(TestCase):\n         self.spider = Spider('foo')\n         self.mw = HttpErrorMiddleware(Settings({'HTTPERROR_ALLOW_ALL': True}))\n         self.req = Request('http://scrapytest.org')\n-\n-        self.res200 = Response('http://scrapytest.org', status=200)\n-        self.res200.request = self.req\n-        self.res404 = Response('http://scrapytest.org', status=404)\n-        self.res404.request = self.req\n-        self.res402 = Response('http://scrapytest.org', status=402)\n-        self.res402.request = self.req\n+        self.res200, self.res404, self.res402 = _responses(self.req, [200, 404, 402])\n \n     def test_process_spider_input(self):\n         self.assertEquals(None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#364790eb07f835e34bbd5876889b1bb3583bffe8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 20 | Files Changed: 4 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/1 | Churn Δ: 30 | Churn Cumulative: 713 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,16 +1,10 @@\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n-from scrapy.utils.test import get_crawler\n+from scrapy.utils.test import docrawl\n from scrapy.tests.spiders import FollowAllSpider, ItemSpider, ErrorSpider\n from scrapy.tests.mockserver import MockServer\n \n \n-def docrawl(spider, settings=None):\n-    crawler = get_crawler(settings)\n-    crawler.configure()\n-    crawler.crawl(spider)\n-    return crawler.start()\n-\n class TestCloseSpider(TestCase):\n \n     def setUp(self):\n\n@@ -3,19 +3,13 @@ import socket\n import mock\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n-from scrapy.utils.test import get_crawler, get_testlog\n+from scrapy.utils.test import docrawl, get_testlog\n from scrapy.tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \\\n     BrokenStartRequestsSpider, SingleRequestSpider\n from scrapy.tests.mockserver import MockServer\n from scrapy.http import Request\n \n \n-def docrawl(spider, settings=None):\n-    crawler = get_crawler(settings)\n-    crawler.crawl(spider)\n-    return crawler.start()\n-\n-\n class CrawlTestCase(TestCase):\n \n     def setUp(self):\n\n@@ -8,16 +8,11 @@ from netlib import http_auth\n \n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n-from scrapy.utils.test import get_crawler, get_testlog\n+from scrapy.utils.test import get_testlog, docrawl\n from scrapy.tests.spiders import SimpleSpider\n from scrapy.tests.mockserver import MockServer\n \n \n-def docrawl(spider, settings=None):\n-    crawler = get_crawler(settings)\n-    crawler.configure()\n-    crawler.crawl(spider)\n-    return crawler.start()\n \n \n \n\n@@ -67,3 +67,10 @@ def assert_samelines(testcase, text1, text2, msg=None):\n     line endings between platforms\n     \"\"\"\n     testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)\n+\n+def docrawl(spider, settings=None):\n+    \"\"\"Configure and start Crawler; return the result of crawler.start()\"\"\"\n+    crawler = get_crawler(settings)\n+    crawler.configure()\n+    crawler.crawl(spider)\n+    return crawler.start()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#75060d142449768d8b8b278598ee92d04d612f47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 7 | Methods Changed: 10 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 82 | Churn Cumulative: 366 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -38,11 +38,14 @@ class HttpErrorMiddleware(object):\n             allowed_statuses = getattr(spider, 'handle_httpstatus_list', self.handle_httpstatus_list)\n         if response.status in allowed_statuses:\n             return\n-        log.msg(format=\"Ignoring HTTP response code: not handled or not allowed: %(status_code)d\",\n-                            level=log.DEBUG, spider=spider,\n-                            status_code=response.status)\n         raise HttpError(response, 'Ignoring non-200 response')\n \n     def process_spider_exception(self, response, exception, spider):\n         if isinstance(exception, HttpError):\n+            log.msg(\n+                format=\"Ignoring response %(response)r: HTTP status code is not handled or not allowed\",\n+                level=log.DEBUG,\n+                spider=spider,\n+                response=response\n+            )\n             return []\n\n@@ -1,11 +1,51 @@\n from unittest import TestCase\n \n+from twisted.trial.unittest import TestCase as TrialTestCase\n+from twisted.internet import defer\n+\n+from scrapy.utils.test import docrawl, get_testlog\n+from scrapy.tests.mockserver import MockServer\n from scrapy.http import Response, Request\n from scrapy.spider import Spider\n from scrapy.contrib.spidermiddleware.httperror import HttpErrorMiddleware, HttpError\n from scrapy.settings import Settings\n \n \n+class _HttpErrorSpider(Spider):\n+    name = 'httperror'\n+    start_urls = [\n+        \"http://localhost:8998/status?n=200\",\n+        \"http://localhost:8998/status?n=404\",\n+        \"http://localhost:8998/status?n=402\",\n+        \"http://localhost:8998/status?n=500\",\n+    ]\n+    bypass_status_codes = set()\n+\n+    def __init__(self, *args, **kwargs):\n+        super(_HttpErrorSpider, self).__init__(*args, **kwargs)\n+        self.failed = set()\n+        self.skipped = set()\n+        self.parsed = set()\n+\n+    def start_requests(self):\n+        for url in self.start_urls:\n+            yield Request(url, self.parse, errback=self.on_error)\n+\n+    def parse(self, response):\n+        self.parsed.add(response.url[-3:])\n+\n+    def on_error(self, failure):\n+        if isinstance(failure.value, HttpError):\n+            response = failure.value.response\n+            if response.status in self.bypass_status_codes:\n+                self.skipped.add(response.url[-3:])\n+                return self.parse(response)\n+\n+        # it assumes there is a response attached to failure\n+        self.failed.add(failure.value.response.url[-3:])\n+        return failure\n+\n+\n def _responses(request, status_codes):\n     responses = []\n     for code in status_codes:\n@@ -48,6 +88,7 @@ class TestHttpErrorMiddleware(TestCase):\n         self.assertEquals(None,\n             self.mw.process_spider_input(self.res404, self.spider))\n \n+\n class TestHttpErrorMiddlewareSettings(TestCase):\n     \"\"\"Similar test, but with settings\"\"\"\n \n@@ -85,6 +126,7 @@ class TestHttpErrorMiddlewareSettings(TestCase):\n         self.assertRaises(HttpError,\n                 self.mw.process_spider_input, self.res402, self.spider)\n \n+\n class TestHttpErrorMiddlewareHandleAll(TestCase):\n \n     def setUp(self):\n@@ -112,3 +154,34 @@ class TestHttpErrorMiddlewareHandleAll(TestCase):\n         self.assertRaises(HttpError,\n                 self.mw.process_spider_input, res402, self.spider)\n \n+\n+class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n+    def setUp(self):\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n+\n+    def tearDown(self):\n+        self.mockserver.__exit__(None, None, None)\n+\n+    @defer.inlineCallbacks\n+    def test_middleware_works(self):\n+        spider = _HttpErrorSpider()\n+        yield docrawl(spider)\n+        assert not spider.skipped, spider.skipped\n+        self.assertEqual(spider.parsed, {'200'})\n+        self.assertEqual(spider.failed, {'404', '402', '500'})\n+\n+    @defer.inlineCallbacks\n+    def test_logging(self):\n+        spider = _HttpErrorSpider(bypass_status_codes={402})\n+        yield docrawl(spider)\n+        # print(get_testlog())\n+        self.assertEqual(spider.parsed, {'200', '402'})\n+        self.assertEqual(spider.skipped, {'402'})\n+        self.assertEqual(spider.failed, {'404', '500'})\n+\n+        log = get_testlog()\n+        self.assertIn('Ignoring response <404', log)\n+        self.assertIn('Ignoring response <500', log)\n+        self.assertNotIn('Ignoring response <200', log)\n+        self.assertNotIn('Ignoring response <402', log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#60f9dc7a54595e9a3f3c6f007e8de3d59305754e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 135 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,11 +53,13 @@ def get_testenv():\n \n def get_testlog():\n     \"\"\"Get Scrapy log of current test, ignoring the rest\"\"\"\n+    with open(\"test.log\", \"rb\") as fp:\n+        loglines = fp.readlines()\n+\n     thistest = []\n-    loglines = open(\"test.log\").readlines()\n-    for l in loglines[::-1]:\n-        thistest.append(l)\n-        if \"[-] -->\" in l:\n+    for line in loglines[::-1]:\n+        thistest.append(line)\n+        if \"[-] -->\" in line:\n             break\n     return \"\".join(thistest[::-1])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8e0b2bd343865fdb195f713151d93fc1fc103b94", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 285 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,6 +2,7 @@ from scrapy.command import ScrapyCommand\n from scrapy.utils.conf import arglist_to_dict\n from scrapy.exceptions import UsageError\n \n+\n class Command(ScrapyCommand):\n \n     requires_project = True\n@@ -14,12 +15,10 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n+        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n                           help=\"set spider argument (may be repeated)\")\n-        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", \\\n+        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\",\n                           help=\"dump scraped items into FILE (use - for stdout)\")\n-        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\", default=\"jsonlines\", \\\n-            help=\"format to use for dumping items with -o (default: %default)\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -33,6 +32,7 @@ class Command(ScrapyCommand):\n             else:\n                 self.settings.overrides['FEED_URI'] = opts.output\n             valid_output_formats = self.settings['FEED_EXPORTERS'].keys() + self.settings['FEED_EXPORTERS_BASE'].keys()\n+            opts.output_format = os.path.splitext(opts.output)[1].replace(\".\", \"\")\n             if opts.output_format not in valid_output_formats:\n                 raise UsageError('Invalid/unrecognized output format: %s, Expected %s' % (opts.output_format, valid_output_formats))\n             self.settings.overrides['FEED_FORMAT'] = opts.output_format\n\n@@ -37,12 +37,10 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n+        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n                           help=\"set spider argument (may be repeated)\")\n-        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", \\\n+        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\",\n                           help=\"dump scraped items into FILE (use - for stdout)\")\n-        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\", default=\"jsonlines\", \\\n-            help=\"format to use for dumping items with -o (default: %default)\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -56,6 +54,7 @@ class Command(ScrapyCommand):\n             else:\n                 self.settings.overrides['FEED_URI'] = opts.output\n             valid_output_formats = self.settings['FEED_EXPORTERS'].keys() + self.settings['FEED_EXPORTERS_BASE'].keys()\n+            opts.output_format = os.path.splitext(opts.output)[1].replace(\".\", \"\")\n             if opts.output_format not in valid_output_formats:\n                 raise UsageError('Invalid/unrecognized output format: %s, Expected %s' % (opts.output_format, valid_output_formats))\n             self.settings.overrides['FEED_FORMAT'] = opts.output_format\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9822eb78cc52c641416d0c0b8d5ef94d2e364de0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 6 | Churn Cumulative: 291 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -19,6 +19,8 @@ class Command(ScrapyCommand):\n                           help=\"set spider argument (may be repeated)\")\n         parser.add_option(\"-o\", \"--output\", metavar=\"FILE\",\n                           help=\"dump scraped items into FILE (use - for stdout)\")\n+        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\", default=\"jsonlines\",\n+                          help=\"format to use for dumping items with -o (default: %default)\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -32,6 +34,7 @@ class Command(ScrapyCommand):\n             else:\n                 self.settings.overrides['FEED_URI'] = opts.output\n             valid_output_formats = self.settings['FEED_EXPORTERS'].keys() + self.settings['FEED_EXPORTERS_BASE'].keys()\n+            if not opts.output_format:\n                 opts.output_format = os.path.splitext(opts.output)[1].replace(\".\", \"\")\n             if opts.output_format not in valid_output_formats:\n                 raise UsageError('Invalid/unrecognized output format: %s, Expected %s' % (opts.output_format, valid_output_formats))\n\n@@ -41,6 +41,8 @@ class Command(ScrapyCommand):\n                           help=\"set spider argument (may be repeated)\")\n         parser.add_option(\"-o\", \"--output\", metavar=\"FILE\",\n                           help=\"dump scraped items into FILE (use - for stdout)\")\n+        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\", default=\"jsonlines\",\n+                          help=\"format to use for dumping items with -o (default: %default)\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -54,6 +56,7 @@ class Command(ScrapyCommand):\n             else:\n                 self.settings.overrides['FEED_URI'] = opts.output\n             valid_output_formats = self.settings['FEED_EXPORTERS'].keys() + self.settings['FEED_EXPORTERS_BASE'].keys()\n+            if not opts.output_format:\n                 opts.output_format = os.path.splitext(opts.output)[1].replace(\".\", \"\")\n             if opts.output_format not in valid_output_formats:\n                 raise UsageError('Invalid/unrecognized output format: %s, Expected %s' % (opts.output_format, valid_output_formats))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e64d6e3f3cf263abc9d84701113d42397aeba11b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 299 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -19,8 +19,8 @@ class Command(ScrapyCommand):\n                           help=\"set spider argument (may be repeated)\")\n         parser.add_option(\"-o\", \"--output\", metavar=\"FILE\",\n                           help=\"dump scraped items into FILE (use - for stdout)\")\n-        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\", default=\"jsonlines\",\n-                          help=\"format to use for dumping items with -o (default: %default)\")\n+        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n+                          help=\"format to use for dumping items with -o\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n\n@@ -41,8 +41,8 @@ class Command(ScrapyCommand):\n                           help=\"set spider argument (may be repeated)\")\n         parser.add_option(\"-o\", \"--output\", metavar=\"FILE\",\n                           help=\"dump scraped items into FILE (use - for stdout)\")\n-        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\", default=\"jsonlines\",\n-                          help=\"format to use for dumping items with -o (default: %default)\")\n+        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n+                          help=\"format to use for dumping items with -o\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aa79abc5bd71fc684e9ad096ec0ff69292e3caf9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 170 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import os\n from scrapy.command import ScrapyCommand\n from scrapy.utils.conf import arglist_to_dict\n from scrapy.exceptions import UsageError\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#92780b70da60bbb328b534c1b43e45314713e39a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 42 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,6 @@\n import zlib\n \n-from scrapy.utils.gz import gunzip\n+from scrapy.utils.gz import gunzip, is_gzipped\n from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n from scrapy.exceptions import NotConfigured\n@@ -22,7 +22,7 @@ class HttpCompressionMiddleware(object):\n     def process_response(self, request, response, spider):\n         if isinstance(response, Response):\n             content_encoding = response.headers.getlist('Content-Encoding')\n-            if content_encoding:\n+            if content_encoding and not is_gzipped(response):\n                 encoding = content_encoding.pop()\n                 decoded_body = self._decode(response.body, encoding.lower())\n                 respcls = responsetypes.from_args(headers=response.headers, \\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#54434095fc03d5ad12694779f9e476ded28764ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 75 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -135,3 +135,12 @@ class HttpCompressionTest(TestCase):\n         self.assertEqual(newresponse.body, plainbody)\n         self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))\n \n+    def test_process_response_gzipped_contenttype(self):\n+        response = self._getresponse('gzip')\n+        response.headers['Content-Type'] = 'application/gzip'\n+        request = response.request\n+\n+        newresponse = self.mw.process_response(request, response, self.spider)\n+        self.assertIs(newresponse, response)\n+        self.assertEqual(response.headers['Content-Encoding'], 'gzip')\n+        self.assertEqual(response.headers['Content-Type'], 'application/gzip')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e353761a33261794f28f6cb1c8a15fe374388b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 312 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -38,7 +38,10 @@ class Command(ScrapyCommand):\n             if not opts.output_format:\n                 opts.output_format = os.path.splitext(opts.output)[1].replace(\".\", \"\")\n             if opts.output_format not in valid_output_formats:\n-                raise UsageError('Invalid/unrecognized output format: %s, Expected %s' % (opts.output_format, valid_output_formats))\n+                raise UsageError(\"Unrecognized output format '%s', set one\"\n+                                 \" using the '-t' switch or as a file extension\"\n+                                 \" from the supported list %s\" % (opts.output_format,\n+                                                                  tuple(valid_output_formats)))\n             self.settings.overrides['FEED_FORMAT'] = opts.output_format\n \n     def run(self, args, opts):\n\n@@ -7,6 +7,7 @@ from scrapy.command import ScrapyCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.conf import arglist_to_dict\n \n+\n def _import_file(filepath):\n     abspath = os.path.abspath(filepath)\n     dirname, file = os.path.split(abspath)\n@@ -22,6 +23,7 @@ def _import_file(filepath):\n             sys.path.pop(0)\n     return module\n \n+\n class Command(ScrapyCommand):\n \n     requires_project = False\n@@ -59,7 +61,10 @@ class Command(ScrapyCommand):\n             if not opts.output_format:\n                 opts.output_format = os.path.splitext(opts.output)[1].replace(\".\", \"\")\n             if opts.output_format not in valid_output_formats:\n-                raise UsageError('Invalid/unrecognized output format: %s, Expected %s' % (opts.output_format, valid_output_formats))\n+                raise UsageError(\"Unrecognized output format '%s', set one\"\n+                                 \" using the '-t' switch or as a file extension\"\n+                                 \" from the supported list %s\" % (opts.output_format,\n+                                                                  tuple(valid_output_formats)))\n             self.settings.overrides['FEED_FORMAT'] = opts.output_format\n \n     def run(self, args, opts):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#73109bf950ad9ab1c6c39a71c8c17fc6e83ed41d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 70 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 74 | Churn Cumulative: 777 | Contributors (this commit): 11 | Commits (past 90d): 9 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -95,7 +95,7 @@ _is_valid_url = lambda url: url.split('://', 1)[0] in set(['http', 'https', 'fil\n class SgmlLinkExtractor(BaseSgmlLinkExtractor):\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n-                 tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True, process_value=None,\n+                 tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True, process_value=None,\n                  deny_extensions=None):\n         self.allow_res = [x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(allow)]\n         self.deny_res = [x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(deny)]\n@@ -105,9 +105,9 @@ class SgmlLinkExtractor(BaseSgmlLinkExtractor):\n         self.canonicalize = canonicalize\n         if deny_extensions is None:\n             deny_extensions = IGNORED_EXTENSIONS\n-        self.deny_extensions = set(['.' + e for e in deny_extensions])\n-        tag_func = lambda x: x in tags\n-        attr_func = lambda x: x in attrs\n+        self.deny_extensions = {'.' + e for e in arg_to_iter(deny_extensions)}\n+        tag_func = lambda x: x in arg_to_iter(tags)\n+        attr_func = lambda x: x in arg_to_iter(attrs)\n         BaseSgmlLinkExtractor.__init__(self,\n                                        tag=tag_func,\n                                        attr=attr_func,\n\n@@ -277,6 +277,11 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n             Link(url='http://example.org/page.html', text=u'asd'),\n         ])\n \n+        lx = SgmlLinkExtractor(deny_extensions=\"jpg\")\n+        self.assertEqual(lx.extract_links(response), [\n+            Link(url='http://example.org/page.html', text=u'asd'),\n+        ])\n+\n     def test_process_value(self):\n         \"\"\"Test restrict_xpaths with encodings\"\"\"\n         html = \"\"\"\n@@ -304,6 +309,67 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n                          [Link(url='http://otherdomain.com/base/item/12.html', text='Item 12')])\n \n \n+    def test_attrs(self):\n+        lx = SgmlLinkExtractor(attrs=\"href\")\n+        self.assertEqual(lx.extract_links(self.response), [\n+            Link(url='http://example.com/sample1.html', text=u''),\n+            Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+            Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+            Link(url='http://www.google.com/something', text=u''),\n+            Link(url='http://example.com/innertag.html', text=u'inner tag'),\n+        ])\n+\n+        lx = SgmlLinkExtractor(attrs=(\"href\",\"src\"), tags=(\"a\",\"area\",\"img\"), deny_extensions=())\n+        self.assertEqual(lx.extract_links(self.response), [\n+            Link(url='http://example.com/sample1.html', text=u''),\n+            Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+            Link(url='http://example.com/sample2.jpg', text=u''),\n+            Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+            Link(url='http://www.google.com/something', text=u''),\n+            Link(url='http://example.com/innertag.html', text=u'inner tag'),\n+        ])\n+\n+        lx = SgmlLinkExtractor(attrs=None)\n+        self.assertEqual(lx.extract_links(self.response), [])\n+\n+        html = \"\"\"<html><area href=\"sample1.html\"></area><a ref=\"sample2.html\">sample text 2</a></html>\"\"\"\n+        response = HtmlResponse(\"http://example.com/index.html\", body=html)\n+        lx = SgmlLinkExtractor(attrs=(\"href\"))\n+        self.assertEqual(lx.extract_links(response), [\n+            Link(url='http://example.com/sample1.html', text=u''),\n+        ])\n+\n+\n+    def test_tags(self):\n+        html = \"\"\"<html><area href=\"sample1.html\"></area><a href=\"sample2.html\">sample 2</a><img src=\"sample2.jpg\"/></html>\"\"\"\n+        response = HtmlResponse(\"http://example.com/index.html\", body=html)\n+\n+        lx = SgmlLinkExtractor(tags=None)\n+        self.assertEqual(lx.extract_links(response), [])\n+\n+        lx = SgmlLinkExtractor()\n+        self.assertEqual(lx.extract_links(response), [\n+            Link(url='http://example.com/sample1.html', text=u''),\n+            Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+        ])\n+\n+        lx = SgmlLinkExtractor(tags=\"area\")\n+        self.assertEqual(lx.extract_links(response), [\n+            Link(url='http://example.com/sample1.html', text=u''),\n+        ])\n+\n+        lx = SgmlLinkExtractor(tags=\"a\")\n+        self.assertEqual(lx.extract_links(response), [\n+            Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+        ])\n+\n+        lx = SgmlLinkExtractor(tags=(\"a\",\"img\"), attrs=(\"href\", \"src\"), deny_extensions=())\n+        self.assertEqual(lx.extract_links(response), [\n+            Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+            Link(url='http://example.com/sample2.jpg', text=u''),\n+        ])\n+\n+\n class HtmlParserLinkExtractorTestCase(unittest.TestCase):\n \n     def setUp(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#43217fd698135b4795d191f8a935f3ba0b869c54", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 65 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -12,7 +12,7 @@ class Sitemap(object):\n     (type=sitemapindex) files\"\"\"\n \n     def __init__(self, xmltext):\n-        xmlp = lxml.etree.XMLParser(recover=True, remove_comments=True)\n+        xmlp = lxml.etree.XMLParser(recover=True, remove_comments=True, resolve_entities=False)\n         self._root = lxml.etree.fromstring(xmltext, parser=xmlp)\n         rt = self._root.tag\n         self.type = self._root.tag.split('}', 1)[1] if '}' in rt else rt\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c2a424daaeca851c9d4a6b930eabf2a0422fdfe3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 17 | Churn Cumulative: 1336 | Contributors (this commit): 6 | Commits (past 90d): 12 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,11 +15,16 @@ from .csstranslator import ScrapyHTMLTranslator, ScrapyGenericTranslator\n \n __all__ = ['Selector', 'SelectorList']\n \n+\n+class SafeXMLParser(etree.XMLParser):\n+    def __init__(self, *args, **kwargs):\n+        super(SafeXMLParser, self).__init__(*args, resolve_entities=False, **kwargs)\n+\n _ctgroup = {\n     'html': {'_parser': etree.HTMLParser,\n              '_csstranslator': ScrapyHTMLTranslator(),\n              '_tostring_method': 'html'},\n-    'xml': {'_parser': etree.XMLParser,\n+    'xml': {'_parser': SafeXMLParser,\n             '_csstranslator': ScrapyGenericTranslator(),\n             '_tostring_method': 'xml'},\n }\n\n@@ -332,6 +332,16 @@ class SelectorTestCase(unittest.TestCase):\n         div_class = x.xpath('//div/@class')\n         self.assertTrue(all(map(lambda e: hasattr(e._root, 'getparent'), div_class)))\n \n+    def test_xml_entity_expansion(self):\n+        malicious_xml = '<?xml version=\"1.0\" encoding=\"ISO-8859-1\"?>'\\\n+            '<!DOCTYPE foo [ <!ELEMENT foo ANY > <!ENTITY xxe SYSTEM '\\\n+            '\"file:///etc/passwd\" >]><foo>&xxe;</foo>'\n+\n+        response = XmlResponse('http://example.com', body=malicious_xml)\n+        sel = self.sscls(response=response)\n+\n+        self.assertEqual(sel.extract(), '<foo>&xxe;</foo>')\n+\n \n class DeprecatedXpathSelectorTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d034df36e8cf97e6e978ec6d75e66f76ef589263", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 219 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -195,6 +195,21 @@ Disallow: /forum/active/\n             }\n         ])\n \n+    def test_xml_entity_expansion(self):\n+        s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+          <!DOCTYPE foo [\n+          <!ELEMENT foo ANY >\n+          <!ENTITY xxe SYSTEM \"file:///etc/passwd\" >\n+          ]>\n+          <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n+            <url>\n+              <loc>http://127.0.0.1:8000/&xxe;</loc>\n+            </url>\n+          </urlset>\n+        \"\"\")\n+\n+        self.assertEqual(list(s), [{'loc': 'http://127.0.0.1:8000/'}])\n+\n \n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#554102fd70b14ee83109003cf77ab3a4f91f4f58", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 315 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,7 +18,8 @@ __all__ = ['Selector', 'SelectorList']\n \n class SafeXMLParser(etree.XMLParser):\n     def __init__(self, *args, **kwargs):\n-        super(SafeXMLParser, self).__init__(*args, resolve_entities=False, **kwargs)\n+        kwargs.setdefault('resolve_entities', False)\n+        super(SafeXMLParser, self).__init__(*args, **kwargs)\n \n _ctgroup = {\n     'html': {'_parser': etree.HTMLParser,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#713aec39194131d72f4a43cede52f5133dcf040f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 207 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ import pkgutil\n __version__ = pkgutil.get_data(__package__, 'VERSION').strip()\n if not isinstance(__version__, str):\n     __version__ = __version__.decode('ascii')\n-version_info = tuple(__version__.split('.')[:3])\n+version_info = tuple(int(v) for v in __version__.split('.')[:3])\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#20b4c8f3eac1ed9e5b48c6b19c65b405d3646bb7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 39 | Churn Cumulative: 39 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,39 @@\n+\n+import pytest\n+from twisted.python import log\n+\n+\n+class LogObservers:\n+    \"\"\"Class for keeping track of log observers across test modules\"\"\"\n+\n+    def __init__(self):\n+        self.observers = []\n+\n+    def add(self, logfile='test.log'):\n+        fileobj = open(logfile, 'wb')\n+        observer = log.FileLogObserver(fileobj)\n+        log.startLoggingWithObserver(observer.emit, 0)\n+        self.observers.append((fileobj, observer))\n+\n+    def remove(self):\n+        fileobj, observer = self.observers.pop()\n+        log.removeObserver(observer.emit)\n+        fileobj.close()\n+\n+\n+@pytest.fixture(scope='module')\n+def log_observers():\n+    return LogObservers()\n+\n+\n+@pytest.fixture()\n+def setlog(request, log_observers):\n+    \"\"\"Attach test.log file observer to twisted log, for trial compatibility\"\"\"\n+    log_observers.add()\n+    request.addfinalizer(log_observers.remove)\n+\n+\n+@pytest.fixture()\n+def chdir(tmpdir):\n+    \"\"\"Change to pytest-provided temporary directory\"\"\"\n+    tmpdir.chdir()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#81187feaac0647e5ec40781f03326ce5d73378b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 45 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -2,6 +2,12 @@\n import pytest\n from twisted.python import log\n \n+from scrapy import optional_features\n+\n+collect_ignore = [\"stats.py\"]\n+if 'django' not in optional_features:\n+    collect_ignore.append(\"tests/test_djangoitem/models.py\")\n+\n \n class LogObservers:\n     \"\"\"Class for keeping track of log observers across test modules\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#118696971538fa2bf2388f8ace525ca5158ee545", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 200 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -7,12 +7,12 @@ from scrapy.contrib.loader import ItemLoader\n class TestItem(Item):\n     name = Field()\n \n-def test_processor(x):\n+def _test_procesor(x):\n     return x + x\n \n class TestLoader(ItemLoader):\n     default_item_class = TestItem\n-    name_out = staticmethod(test_processor)\n+    name_out = staticmethod(_test_procesor)\n \n class MarshalFifoDiskQueueTest(t.FifoDiskQueueTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c50f088b631d259fd401f6a100acfa8bba9bf68d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 709 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -290,10 +290,11 @@ def parse_cachecontrol(header):\n \n     http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n \n-    >>> parse_cachecontrol('public, max-age=3600')\n-    {'public': None, 'max-age': '3600'}\n-    >>> parse_cachecontrol('')\n-    {}\n+    >>> parse_cachecontrol('public, max-age=3600') == {'public': None,\n+    ...                                                'max-age': '3600'}\n+    True\n+    >>> parse_cachecontrol('') == {}\n+    True\n \n     \"\"\"\n     directives = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dd9d3af9b3a478fb17e8fa721ee901ea68a3e833", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 28 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,6 +27,7 @@ class SpiderManagerTest(unittest.TestCase):\n \n     def tearDown(self):\n         del self.spiderman\n+        del sys.modules['test_spiders_xxx']\n         sys.path.remove(self.tmpdir)\n \n     def test_interface(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f0a0d0bc22f2038c65837308f33698d3b787d3da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 55 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -192,3 +192,14 @@ latex_documents = [\n \n # If false, no module index is generated.\n #latex_use_modindex = True\n+\n+\n+# Options for the linkcheck builder\n+# ---------------------------------\n+\n+# A list of regular expressions that match URIs that should not be checked when\n+# doing a linkcheck build.\n+linkcheck_ignore = [\n+    'http://localhost:\\d+', 'http://hg.scrapy.org',\n+    'http://directory.google.com/'\n+]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7814cbf3197605a1b125f061a78cde015a934819", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1025 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,4 @@\n import re\n-import inspect\n import warnings\n import weakref\n from twisted.trial import unittest\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9a643d689c9659c03de3630d2f379f441d2c5791", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 14 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 45 | Churn Cumulative: 252 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,40 +1,45 @@\n \"\"\"\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n-from __future__ import print_function\n+\n+__all__ = ['__version__', 'version_info', 'optional_features', 'twisted_version']\n+\n+# Scrapy version\n import pkgutil\n-__version__ = pkgutil.get_data(__package__, 'VERSION').strip()\n-if not isinstance(__version__, str):\n-    __version__ = __version__.decode('ascii')\n+__version__ = pkgutil.get_data(__package__, 'VERSION').decode('ascii').strip()\n version_info = tuple(int(v) for v in __version__.split('.')[:3])\n+del pkgutil\n \n-import sys, os, warnings\n-\n+# Check minimum required Python version\n+import sys\n if sys.version_info < (2, 7):\n     print(\"Scrapy %s requires Python 2.7\" % __version__)\n     sys.exit(1)\n+del sys\n \n-# ignore noisy twisted deprecation warnings\n+# Ignore noisy twisted deprecation warnings\n+import warnings\n warnings.filterwarnings('ignore', category=DeprecationWarning, module='twisted')\n+del warnings\n \n-# monkey patches to fix external library issues\n-from scrapy.xlib import urlparse_monkeypatches\n+# Apply monkey patches to fix issues in external libraries\n+import _monkeypatches\n+del _monkeypatches\n \n # WARNING: optional_features set is deprecated and will be removed soon. Do not use.\n optional_features = set()\n-\n # TODO: backwards compatibility, remove for Scrapy 0.20\n optional_features.add('ssl')\n-\n try:\n     import boto\n+    del boto\n except ImportError:\n     pass\n else:\n     optional_features.add('boto')\n-\n try:\n     import django\n+    del django\n except ImportError:\n     pass\n else:\n\n\n@@ -0,0 +1,16 @@\n+from unittest import TestCase\n+import six\n+import scrapy\n+\n+\n+class ToplevelTestCase(TestCase):\n+\n+    def test_version(self):\n+        self.assertIs(type(scrapy.__version__), six.text_type)\n+\n+    def test_version_info(self):\n+        self.assertIs(type(scrapy.version_info), tuple)\n+\n+    def test_optional_features(self):\n+        self.assertIs(type(scrapy.optional_features), set)\n+        self.assertIn('ssl', scrapy.optional_features)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
