{"custom_id": "keras#c269e3cd8fed713fb54d2971319df0bfe6e1bf10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 704 | Lines Deleted: 663 | Files Changed: 36 | Hunks: 118 | Methods Changed: 91 | Complexity Δ (Sum/Max): 113/80 | Churn Δ: 1367 | Churn Cumulative: 58353 | Contributors (this commit): 222 | Commits (past 90d): 151 | Contributors (cumulative): 498 | DMM Complexity: None\n\nDIFF:\n@@ -20,6 +20,7 @@ import tensorflow.compat.v2 as tf\n \n import keras.layers.activation as activation_layers\n from keras import backend\n+from keras.saving.legacy import serialization\n from keras.utils import generic_utils\n \n # isort: off\n@@ -514,7 +515,7 @@ def serialize(activation):\n         and activation.__name__ in _TF_ACTIVATIONS_V2\n     ):\n         return _TF_ACTIVATIONS_V2[activation.__name__]\n-    return generic_utils.serialize_keras_object(activation)\n+    return serialization.serialize_keras_object(activation)\n \n \n # Add additional globals so that deserialize can find these common activation\n@@ -564,7 +565,7 @@ def deserialize(name, custom_objects=None):\n         obj_filter=callable,\n     )\n \n-    return generic_utils.deserialize_keras_object(\n+    return serialization.deserialize_keras_object(\n         name,\n         module_objects=activation_functions,\n         custom_objects=custom_objects,\n\n@@ -19,8 +19,8 @@\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n-from keras.utils.generic_utils import deserialize_keras_object\n-from keras.utils.generic_utils import serialize_keras_object\n+from keras.saving.legacy.serialization import deserialize_keras_object\n+from keras.saving.legacy.serialization import serialize_keras_object\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n\n@@ -33,6 +33,7 @@ from keras.engine import input_spec\n from keras.engine import node as node_module\n from keras.engine import training as training_lib\n from keras.engine import training_utils\n+from keras.saving.legacy import serialization\n from keras.saving.legacy.saved_model import json_utils\n from keras.saving.legacy.saved_model import network_serialization\n from keras.utils import generic_utils\n@@ -1533,7 +1534,7 @@ def get_network_config(network, serialize_layer_fn=None, config=None):\n       Config dictionary.\n     \"\"\"\n     serialize_layer_fn = (\n-        serialize_layer_fn or generic_utils.serialize_keras_object\n+        serialize_layer_fn or serialization.serialize_keras_object\n     )\n     config = config or {}\n     config[\"name\"] = network.name\n@@ -1547,7 +1548,7 @@ def get_network_config(network, serialize_layer_fn=None, config=None):\n                 kept_nodes += 1\n     layer_configs = []\n \n-    with generic_utils.SharedObjectSavingScope():\n+    with serialization.SharedObjectSavingScope():\n         for layer in network.layers:  # From the earliest layers on.\n             filtered_inbound_nodes = []\n             for original_node_index, node in enumerate(layer._inbound_nodes):\n\n@@ -26,6 +26,7 @@ from keras.engine import input_layer\n from keras.engine import training\n from keras.engine import training_utils\n from keras.saving.experimental import saving_lib\n+from keras.saving.legacy import serialization\n from keras.saving.legacy.saved_model import model_serialization\n from keras.utils import generic_utils\n from keras.utils import layer_utils\n@@ -453,7 +454,7 @@ class Sequential(functional.Functional):\n             # filtered out of `self.layers`). Note that\n             # `self._self_tracked_trackables` is managed by the tracking\n             # infrastructure and should not be used.\n-            layer_configs.append(generic_utils.serialize_keras_object(layer))\n+            layer_configs.append(serialization.serialize_keras_object(layer))\n         config = training.Model.get_config(self)\n         config[\"name\"] = self.name\n         config[\"layers\"] = copy.deepcopy(layer_configs)\n\n@@ -44,6 +44,7 @@ from keras.saving.experimental import saving_lib\n from keras.saving.legacy import hdf5_format\n from keras.saving.legacy import save\n from keras.saving.legacy import saving_utils\n+from keras.saving.legacy import serialization\n from keras.saving.legacy.saved_model import json_utils\n from keras.saving.legacy.saved_model import model_serialization\n from keras.utils import generic_utils\n@@ -3102,7 +3103,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         # have to call `cls(...)` instead of `Functional.from_config`.\n         from keras.engine import functional\n \n-        with generic_utils.SharedObjectLoadingScope():\n+        with serialization.SharedObjectLoadingScope():\n             functional_model_keys = [\n                 \"name\",\n                 \"layers\",\n\n@@ -27,7 +27,7 @@ import re\n import tensorflow.compat.v2 as tf\n \n from keras.engine.base_layer import Layer\n-from keras.utils import generic_utils\n+from keras.saving.legacy import serialization\n \n \n class _BaseFeaturesLayer(Layer):\n@@ -130,7 +130,7 @@ class _BaseFeaturesLayer(Layer):\n             for fc in self._feature_columns\n         ]\n         config = {\"feature_columns\": column_configs}\n-        config[\"partitioner\"] = generic_utils.serialize_keras_object(\n+        config[\"partitioner\"] = serialization.serialize_keras_object(\n             self._partitioner\n         )\n \n@@ -147,7 +147,7 @@ class _BaseFeaturesLayer(Layer):\n             )\n             for c in config[\"feature_columns\"]\n         ]\n-        config_cp[\"partitioner\"] = generic_utils.deserialize_keras_object(\n+        config_cp[\"partitioner\"] = serialization.deserialize_keras_object(\n             config[\"partitioner\"], custom_objects\n         )\n \n\n@@ -20,6 +20,7 @@ import tensorflow.compat.v2 as tf\n \n from keras.initializers import initializers_v1\n from keras.initializers import initializers_v2\n+from keras.saving.legacy import serialization\n from keras.utils import generic_utils\n from keras.utils import tf_inspect as inspect\n \n@@ -134,14 +135,14 @@ globals().update(LOCAL.ALL_OBJECTS)\n \n @keras_export(\"keras.initializers.serialize\")\n def serialize(initializer):\n-    return generic_utils.serialize_keras_object(initializer)\n+    return serialization.serialize_keras_object(initializer)\n \n \n @keras_export(\"keras.initializers.deserialize\")\n def deserialize(config, custom_objects=None):\n     \"\"\"Return an `Initializer` object from its config.\"\"\"\n     populate_deserializable_objects()\n-    return generic_utils.deserialize_keras_object(\n+    return serialization.deserialize_keras_object(\n         config,\n         module_objects=LOCAL.ALL_OBJECTS,\n         custom_objects=custom_objects,\n\n@@ -23,6 +23,7 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n \n from keras.engine.base_layer import Layer\n+from keras.saving.legacy import serialization\n from keras.utils import generic_utils\n from keras.utils import tf_inspect\n from keras.utils import tf_utils\n@@ -380,7 +381,7 @@ class Lambda(Layer):\n         function_type = config.pop(func_type_attr_name)\n         if function_type == \"function\":\n             # Simple lookup in custom objects\n-            function = generic_utils.deserialize_keras_object(\n+            function = serialization.deserialize_keras_object(\n                 config[func_attr_name],\n                 custom_objects=custom_objects,\n                 printable_module_name=\"function in Lambda layer\",\n\n@@ -26,6 +26,7 @@ from keras.engine.input_spec import InputSpec\n from keras.layers.rnn import rnn_utils\n from keras.layers.rnn.dropout_rnn_cell_mixin import DropoutRNNCellMixin\n from keras.layers.rnn.stacked_rnn_cells import StackedRNNCells\n+from keras.saving.legacy import serialization\n from keras.saving.legacy.saved_model import layer_serialization\n from keras.utils import generic_utils\n \n@@ -957,7 +958,7 @@ class RNN(base_layer.Layer):\n         if self.zero_output_for_mask:\n             config[\"zero_output_for_mask\"] = self.zero_output_for_mask\n \n-        config[\"cell\"] = generic_utils.serialize_keras_object(self.cell)\n+        config[\"cell\"] = serialization.serialize_keras_object(self.cell)\n         base_config = super().get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n\n@@ -21,7 +21,7 @@ Wrappers are layers that augment the functionality of another layer.\n import copy\n \n from keras.engine.base_layer import Layer\n-from keras.utils import generic_utils\n+from keras.saving.legacy import serialization\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n@@ -58,7 +58,7 @@ class Wrapper(Layer):\n             return None\n \n     def get_config(self):\n-        config = {\"layer\": generic_utils.serialize_keras_object(self.layer)}\n+        config = {\"layer\": serialization.serialize_keras_object(self.layer)}\n         base_config = super().get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n\n@@ -24,6 +24,7 @@ from keras.engine.base_layer import Layer\n from keras.engine.input_spec import InputSpec\n from keras.layers.rnn import rnn_utils\n from keras.layers.rnn.base_wrapper import Wrapper\n+from keras.saving.legacy import serialization\n from keras.utils import generic_utils\n from keras.utils import tf_inspect\n from keras.utils import tf_utils\n@@ -148,7 +149,7 @@ class Bidirectional(Wrapper):\n             # Keep the custom backward layer config, so that we can save it\n             # later. The layer's name might be updated below with prefix\n             # 'backward_', and we want to preserve the original config.\n-            self._backward_layer_config = generic_utils.serialize_keras_object(\n+            self._backward_layer_config = serialization.serialize_keras_object(\n                 backward_layer\n             )\n \n\n@@ -31,6 +31,7 @@ import tensorflow.compat.v2 as tf\n \n from keras.layers.rnn import lstm\n from keras.layers.rnn.abstract_rnn_cell import AbstractRNNCell\n+from keras.saving.legacy import serialization\n from keras.utils import generic_utils\n from keras.utils import tf_inspect\n \n@@ -657,7 +658,7 @@ def _parse_config_to_function(\n     function_type = config.pop(func_type_attr_name)\n     if function_type == \"function\":\n         # Simple lookup in custom objects\n-        function = generic_utils.deserialize_keras_object(\n+        function = serialization.deserialize_keras_object(\n             config[func_attr_name],\n             custom_objects=custom_objects,\n             printable_module_name=\"function in wrapper\",\n\n@@ -22,6 +22,7 @@ import tensorflow.compat.v2 as tf\n from keras import backend\n from keras.engine import base_layer\n from keras.layers.rnn import rnn_utils\n+from keras.saving.legacy import serialization\n from keras.utils import generic_utils\n from keras.utils import tf_utils\n \n@@ -199,7 +200,7 @@ class StackedRNNCells(base_layer.Layer):\n     def get_config(self):\n         cells = []\n         for cell in self.cells:\n-            cells.append(generic_utils.serialize_keras_object(cell))\n+            cells.append(serialization.serialize_keras_object(cell))\n         config = {\"cells\": cells}\n         base_config = super().get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n\n@@ -50,6 +50,7 @@ from keras.layers.preprocessing import text_vectorization\n from keras.layers.rnn import cell_wrappers\n from keras.layers.rnn import gru\n from keras.layers.rnn import lstm\n+from keras.saving.legacy import serialization\n from keras.saving.legacy.saved_model import json_utils\n from keras.utils import generic_utils\n from keras.utils import tf_inspect as inspect\n@@ -206,7 +207,7 @@ def serialize(layer):\n     pprint(tf.keras.layers.serialize(model))\n     # prints the configuration of the model, as a dict.\n     \"\"\"\n-    return generic_utils.serialize_keras_object(layer)\n+    return serialization.serialize_keras_object(layer)\n \n \n @keras_export(\"keras.layers.deserialize\")\n@@ -248,7 +249,7 @@ def deserialize(config, custom_objects=None):\n     ```\n     \"\"\"\n     populate_deserializable_objects()\n-    return generic_utils.deserialize_keras_object(\n+    return serialization.deserialize_keras_object(\n         config,\n         module_objects=LOCAL.ALL_OBJECTS,\n         custom_objects=custom_objects,\n\n@@ -23,10 +23,10 @@ import tensorflow.compat.v2 as tf\n \n from keras import backend\n from keras.saving.experimental import saving_lib\n+from keras.saving.legacy.serialization import deserialize_keras_object\n+from keras.saving.legacy.serialization import serialize_keras_object\n from keras.utils import losses_utils\n from keras.utils import tf_utils\n-from keras.utils.generic_utils import deserialize_keras_object\n-from keras.utils.generic_utils import serialize_keras_object\n \n # isort: off\n from tensorflow.python.ops.ragged import ragged_map_ops\n\n@@ -91,8 +91,8 @@ from keras.metrics.metrics import sparse_categorical_crossentropy\n from keras.metrics.metrics import sparse_top_k_categorical_accuracy\n from keras.metrics.metrics import squared_hinge\n from keras.metrics.metrics import top_k_categorical_accuracy\n-from keras.utils.generic_utils import deserialize_keras_object\n-from keras.utils.generic_utils import serialize_keras_object\n+from keras.saving.legacy.serialization import deserialize_keras_object\n+from keras.saving.legacy.serialization import serialize_keras_object\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n\n@@ -23,7 +23,7 @@ from keras.optimizers.optimizer_experimental import (\n )\n from keras.optimizers.optimizer_v2 import optimizer_v2\n from keras.optimizers.optimizer_v2 import utils as optimizer_utils\n-from keras.utils import generic_utils\n+from keras.saving.legacy import serialization\n \n # isort: off\n from tensorflow.python.keras.optimizer_v2 import (\n@@ -895,7 +895,7 @@ class LossScaleOptimizer(\n             # If loss_scale is in config, we assume we are deserializing a\n             # LossScaleOptimizer from TF 2.3 or below. We convert the config so\n             # it can be deserialized in the current LossScaleOptimizer.\n-            loss_scale = generic_utils.deserialize_keras_object(\n+            loss_scale = serialization.deserialize_keras_object(\n                 config.pop(\"loss_scale\"),\n                 module_objects={\n                     \"FixedLossScale\": tf.compat.v1.mixed_precision.FixedLossScale,  # noqa: E501\n\n@@ -21,7 +21,7 @@ import tensorflow.compat.v2 as tf\n from keras import backend\n from keras.engine import base_layer_utils\n from keras.mixed_precision import device_compatibility_check\n-from keras.utils import generic_utils\n+from keras.saving.legacy import serialization\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n@@ -501,7 +501,7 @@ def serialize(policy):\n         # versions of Keras. If the policy name is returned, it is a dtype\n         # string such as 'float32'.\n         return None if policy.name == \"_infer\" else policy.name\n-    return generic_utils.serialize_keras_object(policy)\n+    return serialization.serialize_keras_object(policy)\n \n \n def deserialize(config, custom_objects=None):\n@@ -512,7 +512,7 @@ def deserialize(config, custom_objects=None):\n     # PolicyV1 was an old version of Policy that was removed. Deserializing it\n     # turns it into a (non-V1) Policy.\n     module_objects = {\"Policy\": Policy, \"PolicyV1\": Policy}\n-    return generic_utils.deserialize_keras_object(\n+    return serialization.deserialize_keras_object(\n         config,\n         module_objects=module_objects,\n         custom_objects=custom_objects,\n\n@@ -28,6 +28,7 @@ from keras.engine.base_layer import Layer\n from keras.engine.input_layer import Input\n from keras.engine.input_layer import InputLayer\n from keras.optimizers import optimizer_v1\n+from keras.saving.legacy import serialization\n from keras.saving.object_registration import CustomObjectScope\n from keras.utils import generic_utils\n from keras.utils import version_utils\n@@ -493,7 +494,7 @@ def clone_model(model, input_tensors=None, clone_function=None):\n     new_model = model.__class__.from_config(model.get_config())\n     ```\n     \"\"\"\n-    with generic_utils.DisableSharedObjectScope():\n+    with serialization.DisableSharedObjectScope():\n         if clone_function is None:\n             clone_function = _clone_layer\n \n\n@@ -21,8 +21,8 @@ import tensorflow.compat.v2 as tf\n from keras.engine import data_adapter\n from keras.layers import deserialize as deserialize_layer\n from keras.models import Model\n+from keras.saving.legacy.serialization import serialize_keras_object\n from keras.saving.object_registration import register_keras_serializable\n-from keras.utils.generic_utils import serialize_keras_object\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n\n@@ -78,8 +78,8 @@ from keras.optimizers.optimizer_v2.gradient_descent import SGD\n from keras.optimizers.optimizer_v2.nadam import Nadam\n from keras.optimizers.optimizer_v2.rmsprop import RMSprop\n from keras.optimizers.schedules import learning_rate_schedule\n-from keras.utils.generic_utils import deserialize_keras_object\n-from keras.utils.generic_utils import serialize_keras_object\n+from keras.saving.legacy.serialization import deserialize_keras_object\n+from keras.saving.legacy.serialization import serialize_keras_object\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n\n@@ -20,7 +20,7 @@ import math\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n-from keras.utils import generic_utils\n+from keras.saving.legacy import serialization\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n@@ -1106,7 +1106,7 @@ def serialize(learning_rate_schedule):\n     >>> tf.keras.optimizers.schedules.serialize(lr_schedule)\n     {'class_name': 'ExponentialDecay', 'config': {...}}\n     \"\"\"\n-    return generic_utils.serialize_keras_object(learning_rate_schedule)\n+    return serialization.serialize_keras_object(learning_rate_schedule)\n \n \n @keras_export(\"keras.optimizers.schedules.deserialize\")\n@@ -1137,7 +1137,7 @@ def deserialize(config, custom_objects=None):\n     lr_schedule = tf.keras.optimizers.schedules.deserialize(config)\n     ```\n     \"\"\"\n-    return generic_utils.deserialize_keras_object(\n+    return serialization.deserialize_keras_object(\n         config,\n         module_objects=globals(),\n         custom_objects=custom_objects,\n\n@@ -22,7 +22,7 @@ from keras import layers as layer_module\n from keras.engine import base_layer\n from keras.engine import data_adapter\n from keras.engine import training as keras_training\n-from keras.utils import generic_utils\n+from keras.saving.legacy import serialization\n \n # isort: off\n from tensorflow.python.util import deprecation\n@@ -211,8 +211,8 @@ class WideDeepModel(keras_training.Model):\n             self._set_trainable_state(current_trainable_state)\n \n     def get_config(self):\n-        linear_config = generic_utils.serialize_keras_object(self.linear_model)\n-        dnn_config = generic_utils.serialize_keras_object(self.dnn_model)\n+        linear_config = serialization.serialize_keras_object(self.linear_model)\n+        dnn_config = serialization.serialize_keras_object(self.dnn_model)\n         config = {\n             \"linear_model\": linear_config,\n             \"dnn_model\": dnn_config,\n\n@@ -20,8 +20,8 @@ import math\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n-from keras.utils.generic_utils import deserialize_keras_object\n-from keras.utils.generic_utils import serialize_keras_object\n+from keras.saving.legacy.serialization import deserialize_keras_object\n+from keras.saving.legacy.serialization import serialize_keras_object\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n\n@@ -22,8 +22,8 @@ from absl.testing import parameterized\n \n import keras\n from keras.saving.experimental import serialization_lib\n+from keras.saving.legacy import serialization\n from keras.testing_infra import test_utils\n-from keras.utils import generic_utils\n \n \n def custom_fn(x):\n@@ -186,7 +186,7 @@ class SerializationLibTest(tf.test.TestCase, parameterized.TestCase):\n @test_utils.run_v2_only\n class BackwardsCompatibilityTest(tf.test.TestCase, parameterized.TestCase):\n     def assert_old_format_can_be_deserialized(self, obj, custom_objects=None):\n-        old_config = generic_utils.serialize_keras_object(obj)\n+        old_config = serialization.serialize_keras_object(obj)\n         revived = serialization_lib.deserialize_keras_object(\n             old_config, custom_objects=custom_objects\n         )\n\n@@ -19,10 +19,10 @@ import tensorflow.compat.v2 as tf\n from keras.saving import object_registration\n from keras.saving.legacy import hdf5_format\n from keras.saving.legacy import saving_utils\n+from keras.saving.legacy import serialization\n from keras.saving.legacy.saved_model import load as saved_model_load\n from keras.saving.legacy.saved_model import load_context\n from keras.saving.legacy.saved_model import save as saved_model_save\n-from keras.utils import generic_utils\n from keras.utils import traceback_utils\n from keras.utils.io_utils import path_to_string\n \n@@ -163,7 +163,7 @@ def save_model(\n             model, filepath, overwrite, include_optimizer\n         )\n     else:\n-        with generic_utils.SharedObjectSavingScope():\n+        with serialization.SharedObjectSavingScope():\n             saved_model_save.save(\n                 model,\n                 filepath,\n@@ -218,7 +218,7 @@ def load_model(filepath, custom_objects=None, compile=True, options=None):\n         ImportError: if loading from an hdf5 file and h5py is not available.\n         IOError: In case of an invalid savefile.\n     \"\"\"\n-    with generic_utils.SharedObjectLoadingScope():\n+    with serialization.SharedObjectLoadingScope():\n         with object_registration.CustomObjectScope(custom_objects or {}):\n             with load_context.load_context(options):\n                 filepath_str = path_to_string(filepath)\n\n@@ -38,9 +38,9 @@ from keras.premade_models.linear import LinearModel\n from keras.saving import object_registration\n from keras.saving.legacy import model_config\n from keras.saving.legacy import save\n+from keras.saving.legacy import serialization\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n-from keras.utils import generic_utils\n \n try:\n     import h5py\n@@ -1150,7 +1150,7 @@ class TestWholeModelSaving(test_combinations.TestCase):\n \n             def get_config(self):\n                 return {\n-                    \"inner_layer\": generic_utils.serialize_keras_object(\n+                    \"inner_layer\": serialization.serialize_keras_object(\n                         self.inner_layer\n                     )\n                 }\n@@ -1158,7 +1158,7 @@ class TestWholeModelSaving(test_combinations.TestCase):\n             @classmethod\n             def from_config(cls, config):\n                 return cls(\n-                    generic_utils.deserialize_keras_object(\n+                    serialization.deserialize_keras_object(\n                         config[\"inner_layer\"]\n                     )\n                 )\n@@ -1239,7 +1239,7 @@ class TestWholeModelSaving(test_combinations.TestCase):\n             # Test recreating directly from config\n             config = model.get_config()\n             key_count = collections.Counter(_get_all_keys_recursive(config))\n-            self.assertEqual(key_count[generic_utils.SHARED_OBJECT_KEY], 2)\n+            self.assertEqual(key_count[serialization.SHARED_OBJECT_KEY], 2)\n             loaded = keras.Model.from_config(config)\n             _do_assertions(loaded)\n \n@@ -1344,7 +1344,7 @@ class TestWholeModelSaving(test_combinations.TestCase):\n         with warnings.catch_warnings(record=True) as w:\n             model.save(self._save_model_dir(), test_utils.get_save_format())\n         self.assertIn(\n-            generic_utils.CustomMaskWarning, {warning.category for warning in w}\n+            serialization.CustomMaskWarning, {warning.category for warning in w}\n         )\n \n         # Test that setting up a custom mask correctly does not issue a warning.\n@@ -1368,7 +1368,7 @@ class TestWholeModelSaving(test_combinations.TestCase):\n         with warnings.catch_warnings(record=True) as w:\n             model.save(self._save_model_dir(), test_utils.get_save_format())\n         self.assertNotIn(\n-            generic_utils.CustomMaskWarning, {warning.category for warning in w}\n+            serialization.CustomMaskWarning, {warning.category for warning in w}\n         )\n \n     # Test only in eager mode because ragged tensor inputs\n\n@@ -30,7 +30,7 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n import wrapt\n \n-from keras.utils import generic_utils\n+from keras.saving.legacy import serialization\n \n # isort: off\n from tensorflow.python.framework import type_spec\n@@ -129,7 +129,7 @@ def _decode_helper(\n             # __passive_serialization__ is added by the JSON encoder when\n             # encoding an object that has a `get_config()` method.\n             try:\n-                return generic_utils.deserialize_keras_object(\n+                return serialization.deserialize_keras_object(\n                     obj,\n                     module_objects=module_objects,\n                     custom_objects=custom_objects,\n@@ -156,7 +156,7 @@ def get_json_type(obj):\n     # if obj is a serializable Keras class instance\n     # e.g. optimizer, layer\n     if hasattr(obj, \"get_config\"):\n-        serialized = generic_utils.serialize_keras_object(obj)\n+        serialized = serialization.serialize_keras_object(obj)\n         serialized[\"__passive_serialization__\"] = True\n         return serialized\n \n\n@@ -17,11 +17,11 @@\n import tensorflow.compat.v2 as tf\n \n from keras.mixed_precision import policy\n+from keras.saving.legacy import serialization\n from keras.saving.legacy.saved_model import base_serialization\n from keras.saving.legacy.saved_model import constants\n from keras.saving.legacy.saved_model import save_impl\n from keras.saving.legacy.saved_model import serialized_attributes\n-from keras.utils import generic_utils\n \n \n class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n@@ -58,7 +58,7 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n             # Layer's input_spec has already been type-checked in the property\n             # setter.\n             metadata[\"input_spec\"] = tf.nest.map_structure(\n-                lambda x: generic_utils.serialize_keras_object(x)\n+                lambda x: serialization.serialize_keras_object(x)\n                 if x\n                 else None,\n                 self.obj.input_spec,\n@@ -68,7 +68,7 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n         ):\n             metadata[\n                 \"activity_regularizer\"\n-            ] = generic_utils.serialize_keras_object(\n+            ] = serialization.serialize_keras_object(\n                 self.obj.activity_regularizer\n             )\n         if self.obj._build_input_shape is not None:\n@@ -126,12 +126,12 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n # TODO(kathywu): Move serialization utils (and related utils from\n # generic_utils.py) to a separate file.\n def get_serialized(obj):\n-    with generic_utils.skip_failed_serialization():\n+    with serialization.skip_failed_serialization():\n         # Store the config dictionary, which may be used when reviving the\n         # object.  When loading, the program will attempt to revive the object\n         # from config, and if that fails, the object will be revived from the\n         # SavedModel.\n-        return generic_utils.serialize_keras_object(obj)\n+        return serialization.serialize_keras_object(obj)\n \n \n class InputLayerSavedModelSaver(base_serialization.SavedModelSaver):\n\n@@ -30,13 +30,13 @@ from keras.protobuf import saved_metadata_pb2\n from keras.protobuf import versions_pb2\n from keras.saving import object_registration\n from keras.saving.legacy import saving_utils\n+from keras.saving.legacy import serialization\n from keras.saving.legacy.saved_model import constants\n from keras.saving.legacy.saved_model import json_utils\n from keras.saving.legacy.saved_model import utils\n from keras.saving.legacy.saved_model.serialized_attributes import (\n     CommonEndpoints,\n )\n-from keras.utils import generic_utils\n from keras.utils import layer_utils\n from keras.utils import metrics_utils\n from keras.utils import tf_inspect\n@@ -487,7 +487,7 @@ class KerasObjectLoader:\n             _maybe_add_serialized_attributes(node, metadata)\n \n             config = metadata.get(\"config\")\n-            if _is_graph_network(node) and generic_utils.validate_config(\n+            if _is_graph_network(node) and serialization.validate_config(\n                 config\n             ):\n                 child_nodes = self._get_child_layer_node_ids(node_id)\n@@ -532,7 +532,7 @@ class KerasObjectLoader:\n         # Determine whether the metadata contains information for reviving a\n         # functional or Sequential model.\n         config = metadata.get(\"config\")\n-        if not generic_utils.validate_config(config):\n+        if not serialization.validate_config(config):\n             return None\n \n         class_name = tf.compat.as_str(metadata[\"class_name\"])\n@@ -581,12 +581,12 @@ class KerasObjectLoader:\n         config = metadata.get(\"config\")\n         shared_object_id = metadata.get(\"shared_object_id\")\n         must_restore_from_config = metadata.get(\"must_restore_from_config\")\n-        if not generic_utils.validate_config(config):\n+        if not serialization.validate_config(config):\n             return None\n \n         try:\n             obj = layers_module.deserialize(\n-                generic_utils.serialize_keras_class_and_config(\n+                serialization.serialize_keras_class_and_config(\n                     class_name, config, shared_object_id=shared_object_id\n                 )\n             )\n@@ -649,12 +649,12 @@ class KerasObjectLoader:\n         class_name = tf.compat.as_str(metadata[\"class_name\"])\n         config = metadata.get(\"config\")\n \n-        if not generic_utils.validate_config(config):\n+        if not serialization.validate_config(config):\n             return None\n \n         try:\n             obj = metrics.deserialize(\n-                generic_utils.serialize_keras_class_and_config(\n+                serialization.serialize_keras_class_and_config(\n                     class_name, config\n                 )\n             )\n@@ -1175,7 +1175,7 @@ class RevivedLayer:\n                 \"expects_training_arg\"\n             ]\n             config = metadata.get(\"config\")\n-            if generic_utils.validate_config(config):\n+            if serialization.validate_config(config):\n                 revived_obj._config = config\n             if metadata.get(\"input_spec\") is not None:\n                 revived_obj.input_spec = recursively_deserialize_keras_object(\n@@ -1269,7 +1269,7 @@ def recursively_deserialize_keras_object(config, module_objects=None):\n     \"\"\"Deserialize Keras object from a nested structure.\"\"\"\n     if isinstance(config, dict):\n         if \"class_name\" in config:\n-            return generic_utils.deserialize_keras_object(\n+            return serialization.deserialize_keras_object(\n                 config, module_objects=module_objects\n             )\n         else:\n@@ -1341,7 +1341,7 @@ class RevivedNetwork(RevivedLayer):\n                 \"expects_training_arg\"\n             ]\n             config = metadata.get(\"config\")\n-            if generic_utils.validate_config(config):\n+            if serialization.validate_config(config):\n                 revived_obj._config = config\n \n             if metadata.get(\"activity_regularizer\") is not None:\n\n@@ -25,7 +25,7 @@ from keras import losses\n from keras import optimizers\n from keras.engine import base_layer_utils\n from keras.optimizers import optimizer_v1\n-from keras.utils import generic_utils\n+from keras.saving.legacy import serialization\n from keras.utils import version_utils\n from keras.utils.io_utils import ask_to_proceed_with_overwrite\n \n@@ -305,7 +305,7 @@ def _serialize_nested_config(config):\n \n     def _serialize_fn(obj):\n         if callable(obj):\n-            return generic_utils.serialize_keras_object(obj)\n+            return serialization.serialize_keras_object(obj)\n         return obj\n \n     return tf.nest.map_structure(_serialize_fn, config)\n\n@@ -0,0 +1,590 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Legacy serialization logic for Keras models.\"\"\"\n+\n+import threading\n+import warnings\n+import weakref\n+\n+import tensorflow.compat.v2 as tf\n+\n+from keras.utils import tf_contextlib\n+from keras.utils import tf_inspect\n+\n+# isort: off\n+from tensorflow.python.util.tf_export import keras_export\n+\n+# Flag that determines whether to skip the NotImplementedError when calling\n+# get_config in custom models and layers. This is only enabled when saving to\n+# SavedModel, when the config isn't required.\n+_SKIP_FAILED_SERIALIZATION = False\n+# If a layer does not have a defined config, then the returned config will be a\n+# dictionary with the below key.\n+_LAYER_UNDEFINED_CONFIG_KEY = \"layer was saved without config\"\n+\n+# Store a unique, per-object ID for shared objects.\n+#\n+# We store a unique ID for each object so that we may, at loading time,\n+# re-create the network properly.  Without this ID, we would have no way of\n+# determining whether a config is a description of a new object that\n+# should be created or is merely a reference to an already-created object.\n+SHARED_OBJECT_KEY = \"shared_object_id\"\n+\n+SHARED_OBJECT_DISABLED = threading.local()\n+SHARED_OBJECT_LOADING = threading.local()\n+SHARED_OBJECT_SAVING = threading.local()\n+\n+\n+# Attributes on the threadlocal variable must be set per-thread, thus we\n+# cannot initialize these globally. Instead, we have accessor functions with\n+# default values.\n+def _shared_object_disabled():\n+    \"\"\"Get whether shared object handling is disabled in a threadsafe manner.\"\"\"\n+    return getattr(SHARED_OBJECT_DISABLED, \"disabled\", False)\n+\n+\n+def _shared_object_loading_scope():\n+    \"\"\"Get the current shared object saving scope in a threadsafe manner.\"\"\"\n+    return getattr(SHARED_OBJECT_LOADING, \"scope\", NoopLoadingScope())\n+\n+\n+def _shared_object_saving_scope():\n+    \"\"\"Get the current shared object saving scope in a threadsafe manner.\"\"\"\n+    return getattr(SHARED_OBJECT_SAVING, \"scope\", None)\n+\n+\n+class DisableSharedObjectScope:\n+    \"\"\"A context manager for disabling handling of shared objects.\n+\n+    Disables shared object handling for both saving and loading.\n+\n+    Created primarily for use with `clone_model`, which does extra surgery that\n+    is incompatible with shared objects.\n+    \"\"\"\n+\n+    def __enter__(self):\n+        SHARED_OBJECT_DISABLED.disabled = True\n+        self._orig_loading_scope = _shared_object_loading_scope()\n+        self._orig_saving_scope = _shared_object_saving_scope()\n+\n+    def __exit__(self, *args, **kwargs):\n+        SHARED_OBJECT_DISABLED.disabled = False\n+        SHARED_OBJECT_LOADING.scope = self._orig_loading_scope\n+        SHARED_OBJECT_SAVING.scope = self._orig_saving_scope\n+\n+\n+class NoopLoadingScope:\n+    \"\"\"The default shared object loading scope. It does nothing.\n+\n+    Created to simplify serialization code that doesn't care about shared\n+    objects (e.g. when serializing a single object).\n+    \"\"\"\n+\n+    def get(self, unused_object_id):\n+        return None\n+\n+    def set(self, object_id, obj):\n+        pass\n+\n+\n+class SharedObjectLoadingScope:\n+    \"\"\"A context manager for keeping track of loaded objects.\n+\n+    During the deserialization process, we may come across objects that are\n+    shared across multiple layers. In order to accurately restore the network\n+    structure to its original state, `SharedObjectLoadingScope` allows us to\n+    re-use shared objects rather than cloning them.\n+    \"\"\"\n+\n+    def __enter__(self):\n+        if _shared_object_disabled():\n+            return NoopLoadingScope()\n+\n+        global SHARED_OBJECT_LOADING\n+        SHARED_OBJECT_LOADING.scope = self\n+        self._obj_ids_to_obj = {}\n+        return self\n+\n+    def get(self, object_id):\n+        \"\"\"Given a shared object ID, returns a previously instantiated object.\n+\n+        Args:\n+          object_id: shared object ID to use when attempting to find\n+            already-loaded object.\n+\n+        Returns:\n+          The object, if we've seen this ID before. Else, `None`.\n+        \"\"\"\n+        # Explicitly check for `None` internally to make external calling code a\n+        # bit cleaner.\n+        if object_id is None:\n+            return\n+        return self._obj_ids_to_obj.get(object_id)\n+\n+    def set(self, object_id, obj):\n+        \"\"\"Stores an instantiated object for future lookup and sharing.\"\"\"\n+        if object_id is None:\n+            return\n+        self._obj_ids_to_obj[object_id] = obj\n+\n+    def __exit__(self, *args, **kwargs):\n+        global SHARED_OBJECT_LOADING\n+        SHARED_OBJECT_LOADING.scope = NoopLoadingScope()\n+\n+\n+class SharedObjectConfig(dict):\n+    \"\"\"A configuration container that keeps track of references.\n+\n+    `SharedObjectConfig` will automatically attach a shared object ID to any\n+    configs which are referenced more than once, allowing for proper shared\n+    object reconstruction at load time.\n+\n+    In most cases, it would be more proper to subclass something like\n+    `collections.UserDict` or `collections.Mapping` rather than `dict` directly.\n+    Unfortunately, python's json encoder does not support `Mapping`s. This is\n+    important functionality to retain, since we are dealing with serialization.\n+\n+    We should be safe to subclass `dict` here, since we aren't actually\n+    overriding any core methods, only augmenting with a new one for reference\n+    counting.\n+    \"\"\"\n+\n+    def __init__(self, base_config, object_id, **kwargs):\n+        self.ref_count = 1\n+        self.object_id = object_id\n+        super().__init__(base_config, **kwargs)\n+\n+    def increment_ref_count(self):\n+        # As soon as we've seen the object more than once, we want to attach the\n+        # shared object ID. This allows us to only attach the shared object ID\n+        # when it's strictly necessary, making backwards compatibility breakage\n+        # less likely.\n+        if self.ref_count == 1:\n+            self[SHARED_OBJECT_KEY] = self.object_id\n+        self.ref_count += 1\n+\n+\n+class SharedObjectSavingScope:\n+    \"\"\"Keeps track of shared object configs when serializing.\"\"\"\n+\n+    def __enter__(self):\n+        if _shared_object_disabled():\n+            return None\n+\n+        global SHARED_OBJECT_SAVING\n+\n+        # Serialization can happen at a number of layers for a number of\n+        # reasons.  We may end up with a case where we're opening a saving scope\n+        # within another saving scope. In that case, we'd like to use the\n+        # outermost scope available and ignore inner scopes, since there is not\n+        # (yet) a reasonable use case for having these nested and distinct.\n+        if _shared_object_saving_scope() is not None:\n+            self._passthrough = True\n+            return _shared_object_saving_scope()\n+        else:\n+            self._passthrough = False\n+\n+        SHARED_OBJECT_SAVING.scope = self\n+        self._shared_objects_config = weakref.WeakKeyDictionary()\n+        self._next_id = 0\n+        return self\n+\n+    def get_config(self, obj):\n+        \"\"\"Gets a `SharedObjectConfig` if one has already been seen for `obj`.\n+\n+        Args:\n+          obj: The object for which to retrieve the `SharedObjectConfig`.\n+\n+        Returns:\n+          The SharedObjectConfig for a given object, if already seen. Else,\n+            `None`.\n+        \"\"\"\n+        try:\n+            shared_object_config = self._shared_objects_config[obj]\n+        except (TypeError, KeyError):\n+            # If the object is unhashable (e.g. a subclass of\n+            # `AbstractBaseClass` that has not overridden `__hash__`), a\n+            # `TypeError` will be thrown.  We'll just continue on without shared\n+            # object support.\n+            return None\n+        shared_object_config.increment_ref_count()\n+        return shared_object_config\n+\n+    def create_config(self, base_config, obj):\n+        \"\"\"Create a new SharedObjectConfig for a given object.\"\"\"\n+        shared_object_config = SharedObjectConfig(base_config, self._next_id)\n+        self._next_id += 1\n+        try:\n+            self._shared_objects_config[obj] = shared_object_config\n+        except TypeError:\n+            # If the object is unhashable (e.g. a subclass of\n+            # `AbstractBaseClass` that has not overridden `__hash__`), a\n+            # `TypeError` will be thrown.  We'll just continue on without shared\n+            # object support.\n+            pass\n+        return shared_object_config\n+\n+    def __exit__(self, *args, **kwargs):\n+        if not getattr(self, \"_passthrough\", False):\n+            global SHARED_OBJECT_SAVING\n+            SHARED_OBJECT_SAVING.scope = None\n+\n+\n+def serialize_keras_class_and_config(\n+    cls_name, cls_config, obj=None, shared_object_id=None\n+):\n+    \"\"\"Returns the serialization of the class with the given config.\"\"\"\n+    base_config = {\"class_name\": cls_name, \"config\": cls_config}\n+\n+    # We call `serialize_keras_class_and_config` for some branches of the load\n+    # path. In that case, we may already have a shared object ID we'd like to\n+    # retain.\n+    if shared_object_id is not None:\n+        base_config[SHARED_OBJECT_KEY] = shared_object_id\n+\n+    # If we have an active `SharedObjectSavingScope`, check whether we've\n+    # already serialized this config. If so, just use that config. This will\n+    # store an extra ID field in the config, allowing us to re-create the shared\n+    # object relationship at load time.\n+    if _shared_object_saving_scope() is not None and obj is not None:\n+        shared_object_config = _shared_object_saving_scope().get_config(obj)\n+        if shared_object_config is None:\n+            return _shared_object_saving_scope().create_config(base_config, obj)\n+        return shared_object_config\n+\n+    return base_config\n+\n+\n+@tf_contextlib.contextmanager\n+def skip_failed_serialization():\n+    global _SKIP_FAILED_SERIALIZATION\n+    prev = _SKIP_FAILED_SERIALIZATION\n+    try:\n+        _SKIP_FAILED_SERIALIZATION = True\n+        yield\n+    finally:\n+        _SKIP_FAILED_SERIALIZATION = prev\n+\n+\n+class CustomMaskWarning(Warning):\n+    pass\n+\n+\n+@keras_export(\"keras.utils.serialize_keras_object\")\n+def serialize_keras_object(instance):\n+    \"\"\"Serialize a Keras object into a JSON-compatible representation.\n+\n+    Calls to `serialize_keras_object` while underneath the\n+    `SharedObjectSavingScope` context manager will cause any objects re-used\n+    across multiple layers to be saved with a special shared object ID. This\n+    allows the network to be re-created properly during deserialization.\n+\n+    Args:\n+      instance: The object to serialize.\n+\n+    Returns:\n+      A dict-like, JSON-compatible representation of the object's config.\n+    \"\"\"\n+    from keras.saving import object_registration\n+\n+    _, instance = tf.__internal__.decorator.unwrap(instance)\n+    if instance is None:\n+        return None\n+\n+    # For v1 layers, checking supports_masking is not enough. We have to also\n+    # check whether compute_mask has been overridden.\n+    supports_masking = getattr(instance, \"supports_masking\", False) or (\n+        hasattr(instance, \"compute_mask\")\n+        and not is_default(instance.compute_mask)\n+    )\n+    if supports_masking and is_default(instance.get_config):\n+        warnings.warn(\n+            \"Custom mask layers require a config and must override \"\n+            \"get_config. When loading, the custom mask layer must be \"\n+            \"passed to the custom_objects argument.\",\n+            category=CustomMaskWarning,\n+            stacklevel=2,\n+        )\n+\n+    if hasattr(instance, \"get_config\"):\n+        name = object_registration.get_registered_name(instance.__class__)\n+        try:\n+            config = instance.get_config()\n+        except NotImplementedError as e:\n+            if _SKIP_FAILED_SERIALIZATION:\n+                return serialize_keras_class_and_config(\n+                    name, {_LAYER_UNDEFINED_CONFIG_KEY: True}\n+                )\n+            raise e\n+        serialization_config = {}\n+        for key, item in config.items():\n+            if isinstance(item, str):\n+                serialization_config[key] = item\n+                continue\n+\n+            # Any object of a different type needs to be converted to string or\n+            # dict for serialization (e.g. custom functions, custom classes)\n+            try:\n+                serialized_item = serialize_keras_object(item)\n+                if isinstance(serialized_item, dict) and not isinstance(\n+                    item, dict\n+                ):\n+                    serialized_item[\"__passive_serialization__\"] = True\n+                serialization_config[key] = serialized_item\n+            except ValueError:\n+                serialization_config[key] = item\n+\n+        name = object_registration.get_registered_name(instance.__class__)\n+        return serialize_keras_class_and_config(\n+            name, serialization_config, instance\n+        )\n+    if hasattr(instance, \"__name__\"):\n+        return object_registration.get_registered_name(instance)\n+    raise ValueError(\n+        f\"Cannot serialize {instance} since it doesn't implement \"\n+        \"`get_config()`, and also doesn\\t have `__name__`\"\n+    )\n+\n+\n+def class_and_config_for_serialized_keras_object(\n+    config,\n+    module_objects=None,\n+    custom_objects=None,\n+    printable_module_name=\"object\",\n+):\n+    \"\"\"Returns the class name and config for a serialized keras object.\"\"\"\n+    from keras.saving import object_registration\n+\n+    if (\n+        not isinstance(config, dict)\n+        or \"class_name\" not in config\n+        or \"config\" not in config\n+    ):\n+        raise ValueError(\n+            f\"Improper config format for {config}. \"\n+            \"Expecting python dict contains `class_name` and `config` as keys\"\n+        )\n+\n+    class_name = config[\"class_name\"]\n+    cls = object_registration.get_registered_object(\n+        class_name, custom_objects, module_objects\n+    )\n+    if cls is None:\n+        raise ValueError(\n+            f\"Unknown {printable_module_name}: '{class_name}'. \"\n+            \"Please ensure you are using a `keras.utils.custom_object_scope` \"\n+            \"and that this object is included in the scope. See \"\n+            \"https://www.tensorflow.org/guide/keras/save_and_serialize\"\n+            \"#registering_the_custom_object for details.\"\n+        )\n+\n+    cls_config = config[\"config\"]\n+    # Check if `cls_config` is a list. If it is a list, return the class and the\n+    # associated class configs for recursively deserialization. This case will\n+    # happen on the old version of sequential model (e.g. `keras_version` ==\n+    # \"2.0.6\"), which is serialized in a different structure, for example\n+    # \"{'class_name': 'Sequential',\n+    #   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}\".\n+    if isinstance(cls_config, list):\n+        return (cls, cls_config)\n+\n+    deserialized_objects = {}\n+    for key, item in cls_config.items():\n+        if key == \"name\":\n+            # Assume that the value of 'name' is a string that should not be\n+            # deserialized as a function. This avoids the corner case where\n+            # cls_config['name'] has an identical name to a custom function and\n+            # gets converted into that function.\n+            deserialized_objects[key] = item\n+        elif isinstance(item, dict) and \"__passive_serialization__\" in item:\n+            deserialized_objects[key] = deserialize_keras_object(\n+                item,\n+                module_objects=module_objects,\n+                custom_objects=custom_objects,\n+                printable_module_name=\"config_item\",\n+            )\n+        # TODO(momernick): Should this also have 'module_objects'?\n+        elif isinstance(item, str) and tf_inspect.isfunction(\n+            object_registration.get_registered_object(item, custom_objects)\n+        ):\n+            # Handle custom functions here. When saving functions, we only save\n+            # the function's name as a string. If we find a matching string in\n+            # the custom objects during deserialization, we convert the string\n+            # back to the original function.\n+            # Note that a potential issue is that a string field could have a\n+            # naming conflict with a custom function name, but this should be a\n+            # rare case.  This issue does not occur if a string field has a\n+            # naming conflict with a custom object, since the config of an\n+            # object will always be a dict.\n+            deserialized_objects[\n+                key\n+            ] = object_registration.get_registered_object(item, custom_objects)\n+    for key, item in deserialized_objects.items():\n+        cls_config[key] = deserialized_objects[key]\n+\n+    return (cls, cls_config)\n+\n+\n+@keras_export(\"keras.utils.deserialize_keras_object\")\n+def deserialize_keras_object(\n+    identifier,\n+    module_objects=None,\n+    custom_objects=None,\n+    printable_module_name=\"object\",\n+):\n+    \"\"\"Turns the serialized form of a Keras object back into an actual object.\n+\n+    This function is for mid-level library implementers rather than end users.\n+\n+    Importantly, this utility requires you to provide the dict of\n+    `module_objects` to use for looking up the object config; this is not\n+    populated by default. If you need a deserialization utility that has\n+    preexisting knowledge of built-in Keras objects, use e.g.\n+    `keras.layers.deserialize(config)`, `keras.metrics.deserialize(config)`,\n+    etc.\n+\n+    Calling `deserialize_keras_object` while underneath the\n+    `SharedObjectLoadingScope` context manager will cause any already-seen\n+    shared objects to be returned as-is rather than creating a new object.\n+\n+    Args:\n+      identifier: the serialized form of the object.\n+      module_objects: A dictionary of built-in objects to look the name up in.\n+        Generally, `module_objects` is provided by midlevel library\n+        implementers.\n+      custom_objects: A dictionary of custom objects to look the name up in.\n+        Generally, `custom_objects` is provided by the end user.\n+      printable_module_name: A human-readable string representing the type of\n+        the object. Printed in case of exception.\n+\n+    Returns:\n+      The deserialized object.\n+\n+    Example:\n+\n+    A mid-level library implementer might want to implement a utility for\n+    retrieving an object from its config, as such:\n+\n+    ```python\n+    def deserialize(config, custom_objects=None):\n+       return deserialize_keras_object(\n+         identifier,\n+         module_objects=globals(),\n+         custom_objects=custom_objects,\n+         name=\"MyObjectType\",\n+       )\n+    ```\n+\n+    This is how e.g. `keras.layers.deserialize()` is implemented.\n+    \"\"\"\n+    from keras.saving import object_registration\n+\n+    if identifier is None:\n+        return None\n+\n+    if isinstance(identifier, dict):\n+        # In this case we are dealing with a Keras config dictionary.\n+        config = identifier\n+        (cls, cls_config) = class_and_config_for_serialized_keras_object(\n+            config, module_objects, custom_objects, printable_module_name\n+        )\n+\n+        # If this object has already been loaded (i.e. it's shared between\n+        # multiple objects), return the already-loaded object.\n+        shared_object_id = config.get(SHARED_OBJECT_KEY)\n+        shared_object = _shared_object_loading_scope().get(shared_object_id)\n+        if shared_object is not None:\n+            return shared_object\n+\n+        if hasattr(cls, \"from_config\"):\n+            arg_spec = tf_inspect.getfullargspec(cls.from_config)\n+            custom_objects = custom_objects or {}\n+\n+            if \"custom_objects\" in arg_spec.args:\n+                tlco = object_registration._THREAD_LOCAL_CUSTOM_OBJECTS.__dict__\n+                deserialized_obj = cls.from_config(\n+                    cls_config,\n+                    custom_objects={\n+                        **object_registration._GLOBAL_CUSTOM_OBJECTS,\n+                        **tlco,\n+                        **custom_objects,\n+                    },\n+                )\n+            else:\n+                with object_registration.CustomObjectScope(custom_objects):\n+                    deserialized_obj = cls.from_config(cls_config)\n+        else:\n+            # Then `cls` may be a function returning a class.\n+            # in this case by convention `config` holds\n+            # the kwargs of the function.\n+            custom_objects = custom_objects or {}\n+            with object_registration.CustomObjectScope(custom_objects):\n+                deserialized_obj = cls(**cls_config)\n+\n+        # Add object to shared objects, in case we find it referenced again.\n+        _shared_object_loading_scope().set(shared_object_id, deserialized_obj)\n+\n+        return deserialized_obj\n+\n+    elif isinstance(identifier, str):\n+        object_name = identifier\n+        if custom_objects and object_name in custom_objects:\n+            obj = custom_objects.get(object_name)\n+        elif (\n+            object_name\n+            in object_registration._THREAD_LOCAL_CUSTOM_OBJECTS.__dict__\n+        ):\n+            obj = object_registration._THREAD_LOCAL_CUSTOM_OBJECTS.__dict__[\n+                object_name\n+            ]\n+        elif object_name in object_registration._GLOBAL_CUSTOM_OBJECTS:\n+            obj = object_registration._GLOBAL_CUSTOM_OBJECTS[object_name]\n+        else:\n+            obj = module_objects.get(object_name)\n+            if obj is None:\n+                raise ValueError(\n+                    f\"Unknown {printable_module_name}: '{object_name}'. \"\n+                    \"Please ensure you are using a \"\n+                    \"`keras.utils.custom_object_scope` \"\n+                    \"and that this object is included in the scope. See \"\n+                    \"https://www.tensorflow.org/guide/keras/save_and_serialize\"\n+                    \"#registering_the_custom_object for details.\"\n+                )\n+\n+        # Classes passed by name are instantiated with no args, functions are\n+        # returned as-is.\n+        if tf_inspect.isclass(obj):\n+            return obj()\n+        return obj\n+    elif tf_inspect.isfunction(identifier):\n+        # If a function has already been deserialized, return as is.\n+        return identifier\n+    else:\n+        raise ValueError(\n+            \"Could not interpret serialized \"\n+            f\"{printable_module_name}: {identifier}\"\n+        )\n+\n+\n+def validate_config(config):\n+    \"\"\"Determines whether config appears to be a valid layer config.\"\"\"\n+    return (\n+        isinstance(config, dict) and _LAYER_UNDEFINED_CONFIG_KEY not in config\n+    )\n+\n+\n+def is_default(method):\n+    \"\"\"Check if a method is decorated with the `default` wrapper.\"\"\"\n+    return getattr(method, \"_is_default\", False)\n\n@@ -18,6 +18,7 @@ import tensorflow.compat.v2 as tf\n \n import keras\n from keras.saving import object_registration\n+from keras.saving.legacy import serialization\n \n \n class TestObjectRegistration(tf.test.TestCase):\n@@ -61,9 +62,9 @@ class TestObjectRegistration(tf.test.TestCase):\n         inst = TestClass(value=10)\n         class_name = object_registration._GLOBAL_CUSTOM_NAMES[TestClass]\n         self.assertEqual(serialized_name, class_name)\n-        config = keras.utils.generic_utils.serialize_keras_object(inst)\n+        config = serialization.serialize_keras_object(inst)\n         self.assertEqual(class_name, config[\"class_name\"])\n-        new_inst = keras.utils.generic_utils.deserialize_keras_object(config)\n+        new_inst = serialization.deserialize_keras_object(config)\n         self.assertIsNot(inst, new_inst)\n         self.assertIsInstance(new_inst, TestClass)\n         self.assertEqual(10, new_inst._value)\n@@ -102,9 +103,9 @@ class TestObjectRegistration(tf.test.TestCase):\n         cls = object_registration.get_registered_object(fn_class_name)\n         self.assertEqual(OtherTestClass, cls)\n \n-        config = keras.utils.generic_utils.serialize_keras_object(inst)\n+        config = keras.utils.serialization.serialize_keras_object(inst)\n         self.assertEqual(class_name, config[\"class_name\"])\n-        new_inst = keras.utils.generic_utils.deserialize_keras_object(config)\n+        new_inst = keras.utils.serialization.deserialize_keras_object(config)\n         self.assertIsNot(inst, new_inst)\n         self.assertIsInstance(new_inst, OtherTestClass)\n         self.assertEqual(5, new_inst._val)\n@@ -120,9 +121,9 @@ class TestObjectRegistration(tf.test.TestCase):\n         fn_class_name = object_registration.get_registered_name(my_fn)\n         self.assertEqual(fn_class_name, class_name)\n \n-        config = keras.utils.generic_utils.serialize_keras_object(my_fn)\n+        config = keras.utils.serialization.serialize_keras_object(my_fn)\n         self.assertEqual(class_name, config)\n-        fn = keras.utils.generic_utils.deserialize_keras_object(config)\n+        fn = keras.utils.serialization.deserialize_keras_object(config)\n         self.assertEqual(42, fn())\n \n         fn_2 = object_registration.get_registered_object(fn_class_name)\n\n@@ -14,6 +14,9 @@\n # ==============================================================================\n \"\"\"Public Keras utilities.\"\"\"\n \n+from keras.saving.legacy.serialization import deserialize_keras_object\n+from keras.saving.legacy.serialization import serialize_keras_object\n+\n # Serialization related\n from keras.saving.object_registration import CustomObjectScope\n from keras.saving.object_registration import custom_object_scope\n@@ -33,8 +36,6 @@ from keras.utils.data_utils import get_file\n from keras.utils.data_utils import pad_sequences\n from keras.utils.dataset_utils import split_dataset\n from keras.utils.generic_utils import Progbar\n-from keras.utils.generic_utils import deserialize_keras_object\n-from keras.utils.generic_utils import serialize_keras_object\n from keras.utils.image_dataset import image_dataset_from_directory\n \n # Image related\n\n@@ -21,573 +21,18 @@ import marshal\n import os\n import re\n import sys\n-import threading\n import time\n import types as python_types\n-import warnings\n-import weakref\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n \n from keras.utils import io_utils\n-from keras.utils import tf_contextlib\n from keras.utils import tf_inspect\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n-# Flag that determines whether to skip the NotImplementedError when calling\n-# get_config in custom models and layers. This is only enabled when saving to\n-# SavedModel, when the config isn't required.\n-_SKIP_FAILED_SERIALIZATION = False\n-# If a layer does not have a defined config, then the returned config will be a\n-# dictionary with the below key.\n-_LAYER_UNDEFINED_CONFIG_KEY = \"layer was saved without config\"\n-\n-# Store a unique, per-object ID for shared objects.\n-#\n-# We store a unique ID for each object so that we may, at loading time,\n-# re-create the network properly.  Without this ID, we would have no way of\n-# determining whether a config is a description of a new object that\n-# should be created or is merely a reference to an already-created object.\n-SHARED_OBJECT_KEY = \"shared_object_id\"\n-\n-SHARED_OBJECT_DISABLED = threading.local()\n-SHARED_OBJECT_LOADING = threading.local()\n-SHARED_OBJECT_SAVING = threading.local()\n-\n-\n-# Attributes on the threadlocal variable must be set per-thread, thus we\n-# cannot initialize these globally. Instead, we have accessor functions with\n-# default values.\n-def _shared_object_disabled():\n-    \"\"\"Get whether shared object handling is disabled in a threadsafe manner.\"\"\"\n-    return getattr(SHARED_OBJECT_DISABLED, \"disabled\", False)\n-\n-\n-def _shared_object_loading_scope():\n-    \"\"\"Get the current shared object saving scope in a threadsafe manner.\"\"\"\n-    return getattr(SHARED_OBJECT_LOADING, \"scope\", NoopLoadingScope())\n-\n-\n-def _shared_object_saving_scope():\n-    \"\"\"Get the current shared object saving scope in a threadsafe manner.\"\"\"\n-    return getattr(SHARED_OBJECT_SAVING, \"scope\", None)\n-\n-\n-class DisableSharedObjectScope:\n-    \"\"\"A context manager for disabling handling of shared objects.\n-\n-    Disables shared object handling for both saving and loading.\n-\n-    Created primarily for use with `clone_model`, which does extra surgery that\n-    is incompatible with shared objects.\n-    \"\"\"\n-\n-    def __enter__(self):\n-        SHARED_OBJECT_DISABLED.disabled = True\n-        self._orig_loading_scope = _shared_object_loading_scope()\n-        self._orig_saving_scope = _shared_object_saving_scope()\n-\n-    def __exit__(self, *args, **kwargs):\n-        SHARED_OBJECT_DISABLED.disabled = False\n-        SHARED_OBJECT_LOADING.scope = self._orig_loading_scope\n-        SHARED_OBJECT_SAVING.scope = self._orig_saving_scope\n-\n-\n-class NoopLoadingScope:\n-    \"\"\"The default shared object loading scope. It does nothing.\n-\n-    Created to simplify serialization code that doesn't care about shared\n-    objects (e.g. when serializing a single object).\n-    \"\"\"\n-\n-    def get(self, unused_object_id):\n-        return None\n-\n-    def set(self, object_id, obj):\n-        pass\n-\n-\n-class SharedObjectLoadingScope:\n-    \"\"\"A context manager for keeping track of loaded objects.\n-\n-    During the deserialization process, we may come across objects that are\n-    shared across multiple layers. In order to accurately restore the network\n-    structure to its original state, `SharedObjectLoadingScope` allows us to\n-    re-use shared objects rather than cloning them.\n-    \"\"\"\n-\n-    def __enter__(self):\n-        if _shared_object_disabled():\n-            return NoopLoadingScope()\n-\n-        global SHARED_OBJECT_LOADING\n-        SHARED_OBJECT_LOADING.scope = self\n-        self._obj_ids_to_obj = {}\n-        return self\n-\n-    def get(self, object_id):\n-        \"\"\"Given a shared object ID, returns a previously instantiated object.\n-\n-        Args:\n-          object_id: shared object ID to use when attempting to find\n-            already-loaded object.\n-\n-        Returns:\n-          The object, if we've seen this ID before. Else, `None`.\n-        \"\"\"\n-        # Explicitly check for `None` internally to make external calling code a\n-        # bit cleaner.\n-        if object_id is None:\n-            return\n-        return self._obj_ids_to_obj.get(object_id)\n-\n-    def set(self, object_id, obj):\n-        \"\"\"Stores an instantiated object for future lookup and sharing.\"\"\"\n-        if object_id is None:\n-            return\n-        self._obj_ids_to_obj[object_id] = obj\n-\n-    def __exit__(self, *args, **kwargs):\n-        global SHARED_OBJECT_LOADING\n-        SHARED_OBJECT_LOADING.scope = NoopLoadingScope()\n-\n-\n-class SharedObjectConfig(dict):\n-    \"\"\"A configuration container that keeps track of references.\n-\n-    `SharedObjectConfig` will automatically attach a shared object ID to any\n-    configs which are referenced more than once, allowing for proper shared\n-    object reconstruction at load time.\n-\n-    In most cases, it would be more proper to subclass something like\n-    `collections.UserDict` or `collections.Mapping` rather than `dict` directly.\n-    Unfortunately, python's json encoder does not support `Mapping`s. This is\n-    important functionality to retain, since we are dealing with serialization.\n-\n-    We should be safe to subclass `dict` here, since we aren't actually\n-    overriding any core methods, only augmenting with a new one for reference\n-    counting.\n-    \"\"\"\n-\n-    def __init__(self, base_config, object_id, **kwargs):\n-        self.ref_count = 1\n-        self.object_id = object_id\n-        super().__init__(base_config, **kwargs)\n-\n-    def increment_ref_count(self):\n-        # As soon as we've seen the object more than once, we want to attach the\n-        # shared object ID. This allows us to only attach the shared object ID\n-        # when it's strictly necessary, making backwards compatibility breakage\n-        # less likely.\n-        if self.ref_count == 1:\n-            self[SHARED_OBJECT_KEY] = self.object_id\n-        self.ref_count += 1\n-\n-\n-class SharedObjectSavingScope:\n-    \"\"\"Keeps track of shared object configs when serializing.\"\"\"\n-\n-    def __enter__(self):\n-        if _shared_object_disabled():\n-            return None\n-\n-        global SHARED_OBJECT_SAVING\n-\n-        # Serialization can happen at a number of layers for a number of\n-        # reasons.  We may end up with a case where we're opening a saving scope\n-        # within another saving scope. In that case, we'd like to use the\n-        # outermost scope available and ignore inner scopes, since there is not\n-        # (yet) a reasonable use case for having these nested and distinct.\n-        if _shared_object_saving_scope() is not None:\n-            self._passthrough = True\n-            return _shared_object_saving_scope()\n-        else:\n-            self._passthrough = False\n-\n-        SHARED_OBJECT_SAVING.scope = self\n-        self._shared_objects_config = weakref.WeakKeyDictionary()\n-        self._next_id = 0\n-        return self\n-\n-    def get_config(self, obj):\n-        \"\"\"Gets a `SharedObjectConfig` if one has already been seen for `obj`.\n-\n-        Args:\n-          obj: The object for which to retrieve the `SharedObjectConfig`.\n-\n-        Returns:\n-          The SharedObjectConfig for a given object, if already seen. Else,\n-            `None`.\n-        \"\"\"\n-        try:\n-            shared_object_config = self._shared_objects_config[obj]\n-        except (TypeError, KeyError):\n-            # If the object is unhashable (e.g. a subclass of\n-            # `AbstractBaseClass` that has not overridden `__hash__`), a\n-            # `TypeError` will be thrown.  We'll just continue on without shared\n-            # object support.\n-            return None\n-        shared_object_config.increment_ref_count()\n-        return shared_object_config\n-\n-    def create_config(self, base_config, obj):\n-        \"\"\"Create a new SharedObjectConfig for a given object.\"\"\"\n-        shared_object_config = SharedObjectConfig(base_config, self._next_id)\n-        self._next_id += 1\n-        try:\n-            self._shared_objects_config[obj] = shared_object_config\n-        except TypeError:\n-            # If the object is unhashable (e.g. a subclass of\n-            # `AbstractBaseClass` that has not overridden `__hash__`), a\n-            # `TypeError` will be thrown.  We'll just continue on without shared\n-            # object support.\n-            pass\n-        return shared_object_config\n-\n-    def __exit__(self, *args, **kwargs):\n-        if not getattr(self, \"_passthrough\", False):\n-            global SHARED_OBJECT_SAVING\n-            SHARED_OBJECT_SAVING.scope = None\n-\n-\n-def serialize_keras_class_and_config(\n-    cls_name, cls_config, obj=None, shared_object_id=None\n-):\n-    \"\"\"Returns the serialization of the class with the given config.\"\"\"\n-    base_config = {\"class_name\": cls_name, \"config\": cls_config}\n-\n-    # We call `serialize_keras_class_and_config` for some branches of the load\n-    # path. In that case, we may already have a shared object ID we'd like to\n-    # retain.\n-    if shared_object_id is not None:\n-        base_config[SHARED_OBJECT_KEY] = shared_object_id\n-\n-    # If we have an active `SharedObjectSavingScope`, check whether we've\n-    # already serialized this config. If so, just use that config. This will\n-    # store an extra ID field in the config, allowing us to re-create the shared\n-    # object relationship at load time.\n-    if _shared_object_saving_scope() is not None and obj is not None:\n-        shared_object_config = _shared_object_saving_scope().get_config(obj)\n-        if shared_object_config is None:\n-            return _shared_object_saving_scope().create_config(base_config, obj)\n-        return shared_object_config\n-\n-    return base_config\n-\n-\n-@tf_contextlib.contextmanager\n-def skip_failed_serialization():\n-    global _SKIP_FAILED_SERIALIZATION\n-    prev = _SKIP_FAILED_SERIALIZATION\n-    try:\n-        _SKIP_FAILED_SERIALIZATION = True\n-        yield\n-    finally:\n-        _SKIP_FAILED_SERIALIZATION = prev\n-\n-\n-class CustomMaskWarning(Warning):\n-    pass\n-\n-\n-@keras_export(\"keras.utils.serialize_keras_object\")\n-def serialize_keras_object(instance):\n-    \"\"\"Serialize a Keras object into a JSON-compatible representation.\n-\n-    Calls to `serialize_keras_object` while underneath the\n-    `SharedObjectSavingScope` context manager will cause any objects re-used\n-    across multiple layers to be saved with a special shared object ID. This\n-    allows the network to be re-created properly during deserialization.\n-\n-    Args:\n-      instance: The object to serialize.\n-\n-    Returns:\n-      A dict-like, JSON-compatible representation of the object's config.\n-    \"\"\"\n-    from keras.saving import object_registration\n-\n-    _, instance = tf.__internal__.decorator.unwrap(instance)\n-    if instance is None:\n-        return None\n-\n-    # For v1 layers, checking supports_masking is not enough. We have to also\n-    # check whether compute_mask has been overridden.\n-    supports_masking = getattr(instance, \"supports_masking\", False) or (\n-        hasattr(instance, \"compute_mask\")\n-        and not is_default(instance.compute_mask)\n-    )\n-    if supports_masking and is_default(instance.get_config):\n-        warnings.warn(\n-            \"Custom mask layers require a config and must override \"\n-            \"get_config. When loading, the custom mask layer must be \"\n-            \"passed to the custom_objects argument.\",\n-            category=CustomMaskWarning,\n-            stacklevel=2,\n-        )\n-\n-    if hasattr(instance, \"get_config\"):\n-        name = object_registration.get_registered_name(instance.__class__)\n-        try:\n-            config = instance.get_config()\n-        except NotImplementedError as e:\n-            if _SKIP_FAILED_SERIALIZATION:\n-                return serialize_keras_class_and_config(\n-                    name, {_LAYER_UNDEFINED_CONFIG_KEY: True}\n-                )\n-            raise e\n-        serialization_config = {}\n-        for key, item in config.items():\n-            if isinstance(item, str):\n-                serialization_config[key] = item\n-                continue\n-\n-            # Any object of a different type needs to be converted to string or\n-            # dict for serialization (e.g. custom functions, custom classes)\n-            try:\n-                serialized_item = serialize_keras_object(item)\n-                if isinstance(serialized_item, dict) and not isinstance(\n-                    item, dict\n-                ):\n-                    serialized_item[\"__passive_serialization__\"] = True\n-                serialization_config[key] = serialized_item\n-            except ValueError:\n-                serialization_config[key] = item\n-\n-        name = object_registration.get_registered_name(instance.__class__)\n-        return serialize_keras_class_and_config(\n-            name, serialization_config, instance\n-        )\n-    if hasattr(instance, \"__name__\"):\n-        return object_registration.get_registered_name(instance)\n-    raise ValueError(\n-        f\"Cannot serialize {instance} since it doesn't implement \"\n-        \"`get_config()`, and also doesn\\t have `__name__`\"\n-    )\n-\n-\n-def class_and_config_for_serialized_keras_object(\n-    config,\n-    module_objects=None,\n-    custom_objects=None,\n-    printable_module_name=\"object\",\n-):\n-    \"\"\"Returns the class name and config for a serialized keras object.\"\"\"\n-    from keras.saving import object_registration\n-\n-    if (\n-        not isinstance(config, dict)\n-        or \"class_name\" not in config\n-        or \"config\" not in config\n-    ):\n-        raise ValueError(\n-            f\"Improper config format for {config}. \"\n-            \"Expecting python dict contains `class_name` and `config` as keys\"\n-        )\n-\n-    class_name = config[\"class_name\"]\n-    cls = object_registration.get_registered_object(\n-        class_name, custom_objects, module_objects\n-    )\n-    if cls is None:\n-        raise ValueError(\n-            f\"Unknown {printable_module_name}: '{class_name}'. \"\n-            \"Please ensure you are using a `keras.utils.custom_object_scope` \"\n-            \"and that this object is included in the scope. See \"\n-            \"https://www.tensorflow.org/guide/keras/save_and_serialize\"\n-            \"#registering_the_custom_object for details.\"\n-        )\n-\n-    cls_config = config[\"config\"]\n-    # Check if `cls_config` is a list. If it is a list, return the class and the\n-    # associated class configs for recursively deserialization. This case will\n-    # happen on the old version of sequential model (e.g. `keras_version` ==\n-    # \"2.0.6\"), which is serialized in a different structure, for example\n-    # \"{'class_name': 'Sequential',\n-    #   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}\".\n-    if isinstance(cls_config, list):\n-        return (cls, cls_config)\n-\n-    deserialized_objects = {}\n-    for key, item in cls_config.items():\n-        if key == \"name\":\n-            # Assume that the value of 'name' is a string that should not be\n-            # deserialized as a function. This avoids the corner case where\n-            # cls_config['name'] has an identical name to a custom function and\n-            # gets converted into that function.\n-            deserialized_objects[key] = item\n-        elif isinstance(item, dict) and \"__passive_serialization__\" in item:\n-            deserialized_objects[key] = deserialize_keras_object(\n-                item,\n-                module_objects=module_objects,\n-                custom_objects=custom_objects,\n-                printable_module_name=\"config_item\",\n-            )\n-        # TODO(momernick): Should this also have 'module_objects'?\n-        elif isinstance(item, str) and tf_inspect.isfunction(\n-            object_registration.get_registered_object(item, custom_objects)\n-        ):\n-            # Handle custom functions here. When saving functions, we only save\n-            # the function's name as a string. If we find a matching string in\n-            # the custom objects during deserialization, we convert the string\n-            # back to the original function.\n-            # Note that a potential issue is that a string field could have a\n-            # naming conflict with a custom function name, but this should be a\n-            # rare case.  This issue does not occur if a string field has a\n-            # naming conflict with a custom object, since the config of an\n-            # object will always be a dict.\n-            deserialized_objects[\n-                key\n-            ] = object_registration.get_registered_object(item, custom_objects)\n-    for key, item in deserialized_objects.items():\n-        cls_config[key] = deserialized_objects[key]\n-\n-    return (cls, cls_config)\n-\n-\n-@keras_export(\"keras.utils.deserialize_keras_object\")\n-def deserialize_keras_object(\n-    identifier,\n-    module_objects=None,\n-    custom_objects=None,\n-    printable_module_name=\"object\",\n-):\n-    \"\"\"Turns the serialized form of a Keras object back into an actual object.\n-\n-    This function is for mid-level library implementers rather than end users.\n-\n-    Importantly, this utility requires you to provide the dict of\n-    `module_objects` to use for looking up the object config; this is not\n-    populated by default. If you need a deserialization utility that has\n-    preexisting knowledge of built-in Keras objects, use e.g.\n-    `keras.layers.deserialize(config)`, `keras.metrics.deserialize(config)`,\n-    etc.\n-\n-    Calling `deserialize_keras_object` while underneath the\n-    `SharedObjectLoadingScope` context manager will cause any already-seen\n-    shared objects to be returned as-is rather than creating a new object.\n-\n-    Args:\n-      identifier: the serialized form of the object.\n-      module_objects: A dictionary of built-in objects to look the name up in.\n-        Generally, `module_objects` is provided by midlevel library\n-        implementers.\n-      custom_objects: A dictionary of custom objects to look the name up in.\n-        Generally, `custom_objects` is provided by the end user.\n-      printable_module_name: A human-readable string representing the type of\n-        the object. Printed in case of exception.\n-\n-    Returns:\n-      The deserialized object.\n-\n-    Example:\n-\n-    A mid-level library implementer might want to implement a utility for\n-    retrieving an object from its config, as such:\n-\n-    ```python\n-    def deserialize(config, custom_objects=None):\n-       return deserialize_keras_object(\n-         identifier,\n-         module_objects=globals(),\n-         custom_objects=custom_objects,\n-         name=\"MyObjectType\",\n-       )\n-    ```\n-\n-    This is how e.g. `keras.layers.deserialize()` is implemented.\n-    \"\"\"\n-    from keras.saving import object_registration\n-\n-    if identifier is None:\n-        return None\n-\n-    if isinstance(identifier, dict):\n-        # In this case we are dealing with a Keras config dictionary.\n-        config = identifier\n-        (cls, cls_config) = class_and_config_for_serialized_keras_object(\n-            config, module_objects, custom_objects, printable_module_name\n-        )\n-\n-        # If this object has already been loaded (i.e. it's shared between\n-        # multiple objects), return the already-loaded object.\n-        shared_object_id = config.get(SHARED_OBJECT_KEY)\n-        shared_object = _shared_object_loading_scope().get(shared_object_id)\n-        if shared_object is not None:\n-            return shared_object\n-\n-        if hasattr(cls, \"from_config\"):\n-            arg_spec = tf_inspect.getfullargspec(cls.from_config)\n-            custom_objects = custom_objects or {}\n-\n-            if \"custom_objects\" in arg_spec.args:\n-                tlco = object_registration._THREAD_LOCAL_CUSTOM_OBJECTS.__dict__\n-                deserialized_obj = cls.from_config(\n-                    cls_config,\n-                    custom_objects={\n-                        **object_registration._GLOBAL_CUSTOM_OBJECTS,\n-                        **tlco,\n-                        **custom_objects,\n-                    },\n-                )\n-            else:\n-                with object_registration.CustomObjectScope(custom_objects):\n-                    deserialized_obj = cls.from_config(cls_config)\n-        else:\n-            # Then `cls` may be a function returning a class.\n-            # in this case by convention `config` holds\n-            # the kwargs of the function.\n-            custom_objects = custom_objects or {}\n-            with object_registration.CustomObjectScope(custom_objects):\n-                deserialized_obj = cls(**cls_config)\n-\n-        # Add object to shared objects, in case we find it referenced again.\n-        _shared_object_loading_scope().set(shared_object_id, deserialized_obj)\n-\n-        return deserialized_obj\n-\n-    elif isinstance(identifier, str):\n-        object_name = identifier\n-        if custom_objects and object_name in custom_objects:\n-            obj = custom_objects.get(object_name)\n-        elif (\n-            object_name\n-            in object_registration._THREAD_LOCAL_CUSTOM_OBJECTS.__dict__\n-        ):\n-            obj = object_registration._THREAD_LOCAL_CUSTOM_OBJECTS.__dict__[\n-                object_name\n-            ]\n-        elif object_name in object_registration._GLOBAL_CUSTOM_OBJECTS:\n-            obj = object_registration._GLOBAL_CUSTOM_OBJECTS[object_name]\n-        else:\n-            obj = module_objects.get(object_name)\n-            if obj is None:\n-                raise ValueError(\n-                    f\"Unknown {printable_module_name}: '{object_name}'. \"\n-                    \"Please ensure you are using a \"\n-                    \"`keras.utils.custom_object_scope` \"\n-                    \"and that this object is included in the scope. See \"\n-                    \"https://www.tensorflow.org/guide/keras/save_and_serialize\"\n-                    \"#registering_the_custom_object for details.\"\n-                )\n-\n-        # Classes passed by name are instantiated with no args, functions are\n-        # returned as-is.\n-        if tf_inspect.isclass(obj):\n-            return obj()\n-        return obj\n-    elif tf_inspect.isfunction(identifier):\n-        # If a function has already been deserialized, return as is.\n-        return identifier\n-    else:\n-        raise ValueError(\n-            \"Could not interpret serialized \"\n-            f\"{printable_module_name}: {identifier}\"\n-        )\n-\n \n def func_dump(func):\n     \"\"\"Serializes a user defined function.\n@@ -1069,13 +514,6 @@ def validate_kwargs(\n             raise TypeError(error_message, kwarg)\n \n \n-def validate_config(config):\n-    \"\"\"Determines whether config appears to be a valid layer config.\"\"\"\n-    return (\n-        isinstance(config, dict) and _LAYER_UNDEFINED_CONFIG_KEY not in config\n-    )\n-\n-\n def default(method):\n     \"\"\"Decorates a method to detect overrides in subclasses.\"\"\"\n     method._is_default = True\n\n@@ -23,6 +23,7 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n \n import keras\n+from keras.saving.legacy import serialization\n from keras.utils import generic_utils\n from keras.utils import io_utils\n \n@@ -82,11 +83,9 @@ class HasArgTest(tf.test.TestCase):\n \n class SerializeKerasObjectTest(tf.test.TestCase):\n     def test_serialize_none(self):\n-        serialized = keras.utils.generic_utils.serialize_keras_object(None)\n+        serialized = serialization.serialize_keras_object(None)\n         self.assertEqual(serialized, None)\n-        deserialized = keras.utils.generic_utils.deserialize_keras_object(\n-            serialized\n-        )\n+        deserialized = serialization.deserialize_keras_object(serialized)\n         self.assertEqual(deserialized, None)\n \n     def test_serializable_object(self):\n@@ -263,7 +262,7 @@ class SerializeKerasObjectTest(tf.test.TestCase):\n                 }\n             ],\n         }\n-        old_model = keras.utils.generic_utils.deserialize_keras_object(\n+        old_model = serialization.deserialize_keras_object(\n             old_model_config, module_objects={\"Sequential\": keras.Sequential}\n         )\n         new_model = keras.Sequential(\n@@ -283,12 +282,12 @@ class SerializeKerasObjectTest(tf.test.TestCase):\n             pass\n \n         layer = CustomLayer()\n-        config = keras.utils.generic_utils.serialize_keras_object(layer)\n+        config = serialization.serialize_keras_object(layer)\n         with self.assertRaisesRegexp(\n             ValueError, \"using a `keras.utils.custom_object_scope`\"\n         ):\n-            keras.utils.generic_utils.deserialize_keras_object(config)\n-        restored = keras.utils.generic_utils.deserialize_keras_object(\n+            serialization.deserialize_keras_object(config)\n+        restored = serialization.deserialize_keras_object(\n             config, custom_objects={\"CustomLayer\": CustomLayer}\n         )\n         self.assertIsInstance(restored, CustomLayer)\n@@ -319,24 +318,24 @@ class MaybeSharedObject:\n \n class SharedObjectScopeTest(tf.test.TestCase):\n     def test_shared_object_saving_scope_single_object_doesnt_export_id(self):\n-        with generic_utils.SharedObjectSavingScope() as scope:\n+        with serialization.SharedObjectSavingScope() as scope:\n             single_object = MaybeSharedObject()\n             self.assertIsNone(scope.get_config(single_object))\n             single_object_config = scope.create_config({}, single_object)\n             self.assertIsNotNone(single_object_config)\n             self.assertNotIn(\n-                generic_utils.SHARED_OBJECT_KEY, single_object_config\n+                serialization.SHARED_OBJECT_KEY, single_object_config\n             )\n \n     def test_shared_object_saving_scope_shared_object_exports_id(self):\n-        with generic_utils.SharedObjectSavingScope() as scope:\n+        with serialization.SharedObjectSavingScope() as scope:\n             shared_object = MaybeSharedObject()\n             self.assertIsNone(scope.get_config(shared_object))\n             scope.create_config({}, shared_object)\n             first_object_config = scope.get_config(shared_object)\n             second_object_config = scope.get_config(shared_object)\n-            self.assertIn(generic_utils.SHARED_OBJECT_KEY, first_object_config)\n-            self.assertIn(generic_utils.SHARED_OBJECT_KEY, second_object_config)\n+            self.assertIn(serialization.SHARED_OBJECT_KEY, first_object_config)\n+            self.assertIn(serialization.SHARED_OBJECT_KEY, second_object_config)\n             self.assertIs(first_object_config, second_object_config)\n \n     def test_shared_object_loading_scope_noop(self):\n@@ -344,29 +343,29 @@ class SharedObjectScopeTest(tf.test.TestCase):\n         # nothing.\n         obj_id = 1\n         obj = MaybeSharedObject()\n-        generic_utils._shared_object_loading_scope().set(obj_id, obj)\n+        serialization._shared_object_loading_scope().set(obj_id, obj)\n         self.assertIsNone(\n-            generic_utils._shared_object_loading_scope().get(obj_id)\n+            serialization._shared_object_loading_scope().get(obj_id)\n         )\n \n     def test_shared_object_loading_scope_returns_shared_obj(self):\n         obj_id = 1\n         obj = MaybeSharedObject()\n-        with generic_utils.SharedObjectLoadingScope() as scope:\n+        with serialization.SharedObjectLoadingScope() as scope:\n             scope.set(obj_id, obj)\n             self.assertIs(scope.get(obj_id), obj)\n \n     def test_nested_shared_object_saving_scopes(self):\n         my_obj = MaybeSharedObject()\n-        with generic_utils.SharedObjectSavingScope() as scope_1:\n+        with serialization.SharedObjectSavingScope() as scope_1:\n             scope_1.create_config({}, my_obj)\n-            with generic_utils.SharedObjectSavingScope() as scope_2:\n+            with serialization.SharedObjectSavingScope() as scope_2:\n                 # Nesting saving scopes should return the original scope and\n                 # should not clear any objects we're tracking.\n                 self.assertIs(scope_1, scope_2)\n                 self.assertIsNotNone(scope_2.get_config(my_obj))\n             self.assertIsNotNone(scope_1.get_config(my_obj))\n-        self.assertIsNone(generic_utils._shared_object_saving_scope())\n+        self.assertIsNone(serialization._shared_object_saving_scope())\n \n     def test_custom_object_scope_correct_class(self):\n         train_step_message = \"This is my training step\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
