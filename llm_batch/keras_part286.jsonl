{"custom_id": "keras#4b9d169989cad88775e958beaa5c2836aaff7a4e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 42 | Files Changed: 5 | Hunks: 25 | Methods Changed: 10 | Complexity Δ (Sum/Max): 1/3 | Churn Δ: 102 | Churn Cumulative: 26100 | Contributors (this commit): 160 | Commits (past 90d): 44 | Contributors (cumulative): 227 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,8 +10,6 @@ from tensorflow.python.ops import math_ops as tf_math_ops\n from tensorflow.python.ops import state_ops as tf_state_ops\n from tensorflow.python.keras import backend as tf_keras_backend\n from tensorflow.python.keras.utils import tf_utils\n-from tensorflow.python.ops import tensor_array_ops\n-from tensorflow.python.ops import control_flow_ops\n from tensorflow.python.ops import functional_ops\n from tensorflow.python.ops import ctc_ops as ctc\n from .common import floatx, epsilon, image_data_format\n@@ -22,7 +20,6 @@ import threading\n import numpy as np\n from distutils.version import StrictVersion\n \n-from .common import normalize_data_format\n from ..utils.generic_utils import transpose_shape\n \n py_all = all\n@@ -45,10 +42,12 @@ _LEARNING_PHASE_CACHE = {}\n def _is_tf_1():\n     return tf.__version__.startswith('1.')\n \n+# Set initial config\n tf_keras_backend.set_floatx(floatx())\n tf_keras_backend.set_epsilon(epsilon())\n tf_keras_backend.set_image_data_format(image_data_format())\n \n+# Redefine config access/setting methods\n floatx = tf_keras_backend.floatx\n epsilon = tf_keras_backend.epsilon\n image_data_format = tf_keras_backend.image_data_format\n@@ -56,13 +55,12 @@ set_floatx = tf_keras_backend.set_floatx\n set_epsilon = tf_keras_backend.set_floatx\n set_image_data_format = tf_keras_backend.set_image_data_format\n \n+# Private TF Keras utils\n get_graph = tf_keras_backend.get_graph\n get_uid = tf_keras_backend.get_uid\n reset_uids = tf_keras_backend.reset_uids\n manual_variable_initialization = tf_keras_backend.manual_variable_initialization\n-\n-# TODO\n-learning_phase_scope = tf_keras_backend.learning_phase_scope\n+learning_phase_scope = tf_keras_backend.learning_phase_scope  # TODO\n name_scope = tf.name_scope\n \n \n@@ -141,6 +139,37 @@ def set_learning_phase(value):\n     tf_keras_backend.set_learning_phase(value)\n \n \n+def normalize_data_format(value):\n+    \"\"\"Checks that the value correspond to a valid data format.\n+\n+    # Arguments\n+        value: String or None. `'channels_first'` or `'channels_last'`.\n+\n+    # Returns\n+        A string, either `'channels_first'` or `'channels_last'`\n+\n+    # Example\n+    ```python\n+        >>> from keras import backend as K\n+        >>> K.normalize_data_format(None)\n+        'channels_first'\n+        >>> K.normalize_data_format('channels_last')\n+        'channels_last'\n+    ```\n+\n+    # Raises\n+        ValueError: if `value` or the global `data_format` invalid.\n+    \"\"\"\n+    if value is None:\n+        value = image_data_format()\n+    data_format = value.lower()\n+    if data_format not in {'channels_first', 'channels_last'}:\n+        raise ValueError('The `data_format` argument must be one of '\n+                         '\"channels_first\", \"channels_last\". Received: ' +\n+                         str(value))\n+    return data_format\n+\n+\n def get_session():\n     \"\"\"Returns the TF session to be used by the backend.\n \n@@ -2626,7 +2655,6 @@ def batch_get_value(ops):\n     # Returns\n         A list of Numpy arrays.\n     \"\"\"\n-    # TODO\n     return tf_keras_backend.batch_get_value(ops)\n \n \n@@ -2790,9 +2818,6 @@ def rnn(step_function, inputs, initial_states,\n         input_length=input_length)\n     reachable = tf_utils.get_reachable_from_inputs([learning_phase()],\n                                                    targets=[last_output])\n-    print('learning_phase:', learning_phase())\n-    print('reachable:', reachable)\n-    print('last_output:', last_output)\n     if last_output in reachable:\n         last_output._uses_learning_phase = True\n     return last_output, outputs, new_states\n@@ -3210,7 +3235,9 @@ def in_top_k(predictions, targets, k):\n         `output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k`\n         values of `predictions[i]`.\n     \"\"\"\n-    return tf.nn.in_top_k(predictions, targets, k)\n+    return tf_math_ops.in_top_k(tf.cast(predictions, 'float32'),\n+                                tf.cast(targets, 'int32'),\n+                                k)\n \n \n # CONVOLUTIONS\n@@ -3401,7 +3428,6 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',\n         padding=padding,\n         data_format=tf_data_format,\n         **kwargs)\n-\n     if data_format == 'channels_first' and tf_data_format == 'NHWC':\n         x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n     return x\n\n@@ -39,8 +39,9 @@ if K.backend() == 'cntk':\n     supports_sparse = False\n elif K.backend() == 'theano' and not KTH.th_sparse_module:\n     supports_sparse = False\n-else:\n-    supports_sparse = True\n+elif K.backend() == 'tensorflow':\n+    # Must wait for tf.keras to support sparse ops.\n+    supports_sparse = False\n \n \n def check_dtype(var, dtype):\n\n@@ -468,27 +468,24 @@ def test_recursion():\n         Model([j, k], [m, n, 0])\n \n     ####################################################\n-    # test calling layers/models on TF tensors\n-\n-    if K.backend() == 'tensorflow':\n-        import tensorflow as tf\n+    # test calling layers/models on placeholders\n     j = Input(shape=(32,), name='input_j')\n     k = Input(shape=(32,), name='input_k')\n     m, n = model([j, k])\n-        tf_model = Model([j, k], [m, n])\n+    outer_model = Model([j, k], [m, n])\n \n-        j_tf = tf.placeholder(dtype=K.floatx())\n-        k_tf = tf.placeholder(dtype=K.floatx())\n-        m_tf, n_tf = tf_model([j_tf, k_tf])\n-        assert m_tf.get_shape().as_list() == [None, 64]\n-        assert n_tf.get_shape().as_list() == [None, 5]\n+    j_tf = K.placeholder(shape=(None, 32), dtype=K.floatx())\n+    k_tf = K.placeholder(shape=(None, 32), dtype=K.floatx())\n+    m_tf, n_tf = outer_model([j_tf, k_tf])\n+    assert K.int_shape(m_tf) == (None, 64)\n+    assert K.int_shape(n_tf) == (None, 5)\n \n     # test merge\n     layers.concatenate([j_tf, k_tf], axis=1)\n     layers.add([j_tf, k_tf])\n \n     # test tensor input\n-        x = tf.placeholder(shape=(None, 2), dtype=K.floatx())\n+    x = K.placeholder(shape=(None, 2), dtype=K.floatx())\n     InputLayer(input_tensor=x)\n \n     x = Input(tensor=x)\n\n@@ -642,6 +642,8 @@ def test_warnings():\n         'A warning was raised for Sequence.')\n \n \n+@pytest.mark.skipif(K.backend() == 'tensorflow',\n+                    reason='Must for for tf.keras to support sparse ops.')\n def test_sparse_inputs_targets():\n     test_inputs = [sparse.random(6, 3, density=0.25).tocsr() for _ in range(2)]\n     test_outputs = [sparse.random(6, i, density=0.25).tocsr() for i in range(3, 5)]\n@@ -659,7 +661,8 @@ def test_sparse_inputs_targets():\n \n @pytest.mark.skipif(K.backend() != 'tensorflow',\n                     reason='sparse operations supported only by TensorFlow')\n-def test_sparse_placeholder_fit():\n+def DISABLED_test_sparse_placeholder_fit():\n+    \"\"\"Must wait for tf.keras to support sparse operations.\"\"\"\n     test_inputs = [sparse.random(6, 3, density=0.25).tocsr() for _ in range(2)]\n     test_outputs = [sparse.random(6, i, density=0.25).tocsr() for i in range(3, 5)]\n     in1 = Input(shape=(3,))\n@@ -1224,10 +1227,8 @@ def test_model_custom_target_tensors():\n                                {y: np.random.random((10, 4)),\n                                 y1: np.random.random((10, 3))})\n \n-    if K.backend() == 'tensorflow':\n-        import tensorflow as tf\n-        # test with custom TF placeholder as target\n-        pl_target_a = tf.placeholder('float32', shape=(None, 4))\n+    # test with custom placeholder as target\n+    pl_target_a = K.placeholder(shape=(None, 4))\n     model.compile(optimizer='rmsprop', loss='mse',\n                   target_tensors={'dense_1': pl_target_a})\n     model.train_on_batch([input_a_np, input_b_np],\n@@ -1358,10 +1359,6 @@ def test_pandas_dataframe():\n \n \n @pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TensorFlow')\n-@pytest.mark.skipif((K.backend() == 'tensorflow' and\n-                     not hasattr(K.get_session(),\n-                                 '_make_callable_from_options')),\n-                    reason='Requires TF 1.8 or higher')\n def test_training_and_eval_methods_on_symbolic_tensors_single_io():\n     x = keras.layers.Input(shape=(3,), name='input')\n     y = keras.layers.Dense(4, name='dense')(x)\n@@ -1386,10 +1383,6 @@ def test_training_and_eval_methods_on_symbolic_tensors_single_io():\n \n \n @pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TensorFlow')\n-@pytest.mark.skipif((K.backend() == 'tensorflow' and\n-                     not hasattr(K.get_session(),\n-                                 '_make_callable_from_options')),\n-                    reason='Requires TF 1.8 or higher')\n def test_training_and_eval_methods_on_symbolic_tensors_multi_io():\n     a = keras.layers.Input(shape=(3,), name='input_a')\n     b = keras.layers.Input(shape=(3,), name='input_b')\n@@ -1549,6 +1542,7 @@ def test_model_with_crossentropy_losses_channels_first():\n     # Evaluate the same network with channels first, with all three loss\n     # functions:\n     K.set_image_data_format('channels_first')\n+    assert K.image_data_format() == 'channels_first'\n     data = data_channels_first\n     for index, loss_function in enumerate(losses_to_test):\n         labels = labels_channels_first[index]\n\n@@ -327,9 +327,9 @@ def test_Bidirectional_merged_value(merge_mode):\n     layer = wrappers.Bidirectional(rnn(units, return_sequences=True),\n                                    merge_mode=merge_mode)\n     f_merged = K.function([inputs], to_list(layer(inputs)))\n-    f_forward = K.function([inputs], [layer.forward_layer.call(inputs)])\n+    f_forward = K.function([inputs], [layer.forward_layer(inputs)])\n     f_backward = K.function([inputs],\n-                            [K.reverse(layer.backward_layer.call(inputs), 1)])\n+                            [K.reverse(layer.backward_layer(inputs), 1)])\n \n     y_merged = f_merged(X)\n     y_expected = to_list(merge_func(f_forward(X)[0], f_backward(X)[0]))\n@@ -342,8 +342,8 @@ def test_Bidirectional_merged_value(merge_mode):\n     layer = wrappers.Bidirectional(rnn(units, return_state=True),\n                                    merge_mode=merge_mode)\n     f_merged = K.function([inputs], layer(inputs))\n-    f_forward = K.function([inputs], layer.forward_layer.call(inputs))\n-    f_backward = K.function([inputs], layer.backward_layer.call(inputs))\n+    f_forward = K.function([inputs], layer.forward_layer(inputs))\n+    f_backward = K.function([inputs], layer.backward_layer(inputs))\n     n_states = len(layer.layer.states)\n \n     y_merged = f_merged(X)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ed387f1243893936de2d118657676549b6864eb6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 6852 | Contributors (this commit): 39 | Commits (past 90d): 16 | Contributors (cumulative): 46 | DMM Complexity: 0.0\n\nDIFF:\n@@ -42,6 +42,8 @@ elif K.backend() == 'theano' and not KTH.th_sparse_module:\n elif K.backend() == 'tensorflow':\n     # Must wait for tf.keras to support sparse ops.\n     supports_sparse = False\n+else:\n+    supports_sparse = True\n \n \n def check_dtype(var, dtype):\n\n@@ -21,11 +21,9 @@ sequence_len = 2\n @pytest.mark.parametrize('data_format', ['channels_first', 'channels_last'])\n @pytest.mark.parametrize('return_sequences', [True, False])\n @pytest.mark.parametrize('use_mask', [True, False])\n-@pytest.mark.parametrize('use_bias', [True, False])\n def test_convolutional_recurrent(data_format='channels_last',\n                                  return_sequences=False,\n-                                 use_mask=False,\n-                                 use_bias=True):\n+                                 use_mask=False):\n \n     class Masking5D(Masking):\n         \"\"\"Regular masking layer returns wrong shape of mask for RNN\"\"\"\n@@ -49,8 +47,7 @@ def test_convolutional_recurrent(data_format='channels_last',\n               'stateful': True,\n               'filters': filters,\n               'kernel_size': (num_row, num_col),\n-              'padding': 'valid',\n-              'use_bias': use_bias}\n+              'padding': 'valid'}\n     layer = convolutional_recurrent.ConvLSTM2D(**kwargs)\n     layer.build(inputs.shape)\n     if use_mask:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#91765e700b1d5e465fcb56bd2c2f56384bc2772d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 12 | Churn Cumulative: 2373 | Contributors (this commit): 35 | Commits (past 90d): 6 | Contributors (cumulative): 35 | DMM Complexity: None\n\nDIFF:\n@@ -532,8 +532,8 @@ class Adamax(Optimizer):\n \n     # Arguments\n         lr: float >= 0. Learning rate.\n-        beta_1: floats, 0 < beta < 1. Generally close to 1.\n-        beta_2: floats, 0 < beta < 1. Generally close to 1.\n+        beta_1: float, 0 < beta < 1. Generally close to 1.\n+        beta_2: float, 0 < beta < 1. Generally close to 1.\n         epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n         decay: float >= 0. Learning rate decay over each update.\n \n@@ -607,7 +607,7 @@ class Nadam(Optimizer):\n     \"\"\"Nesterov Adam optimizer.\n \n     Much like Adam is essentially RMSprop with momentum,\n-    Nadam is Adam RMSprop with Nesterov momentum.\n+    Nadam is RMSprop with Nesterov momentum.\n \n     Default parameters follow those provided in the paper.\n     It is recommended to leave the parameters of this optimizer\n@@ -615,10 +615,10 @@ class Nadam(Optimizer):\n \n     # Arguments\n         lr: float >= 0. Learning rate.\n-        beta_1: floats, 0 < beta < 1. Generally close to 1.\n-        beta_2: floats, 0 < beta < 1. Generally close to 1.\n+        beta_1: float, 0 < beta < 1. Generally close to 1.\n+        beta_2: float, 0 < beta < 1. Generally close to 1.\n         epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n-        schedule_decay: floats, 0 < schedule_decay < 1.\n+        schedule_decay: float, 0 < schedule_decay < 1.\n \n     # References\n         - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#820251a00d2c612066f393de652b9223f85271e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 16704 | Contributors (this commit): 147 | Commits (past 90d): 35 | Contributors (cumulative): 162 | DMM Complexity: None\n\nDIFF:\n@@ -33,9 +33,9 @@ keras_dir = pathlib.Path(__file__).resolve().parents[1]\n def get_function_signature(function, method=True):\n     wrapped = getattr(function, '_original_function', None)\n     if wrapped is None:\n-        signature = inspect.getargspec(function)\n+        signature = inspect.getfullargspec(function)\n     else:\n-        signature = inspect.getargspec(wrapped)\n+        signature = inspect.getfullargspec(wrapped)\n     defaults = signature.defaults\n     if method:\n         args = signature.args[1:]\n\n@@ -3235,7 +3235,7 @@ def in_top_k(predictions, targets, k):\n         `output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k`\n         values of `predictions[i]`.\n     \"\"\"\n-    return tf_math_ops.in_top_k(tf.cast(predictions, 'float32'),\n+    return tf.nn.in_top_k(tf.cast(predictions, 'float32'),\n                           tf.cast(targets, 'int32'),\n                           k)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c8b0e33c3edf7e1f70b2fbf60a250909c917d55e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 14 | Files Changed: 3 | Hunks: 16 | Methods Changed: 4 | Complexity Δ (Sum/Max): 10/6 | Churn Δ: 85 | Churn Cumulative: 4583 | Contributors (this commit): 49 | Commits (past 90d): 12 | Contributors (cumulative): 60 | DMM Complexity: 0.5476190476190477\n\nDIFF:\n@@ -34,6 +34,25 @@ except ImportError:\n     tf_file_io = None\n \n \n+def _uniquify(names):\n+    \"\"\"\n+    Custom layers and optimizers written by users\n+    for TF 1.x might produce weights with same variable\n+    names in TF 2. This method \"uniquifies\" a given list\n+    of names. e.g: ['a', 'b', 'b', 'c'] -> ['a', 'b', 'b_2', 'c']\n+    \"\"\"\n+    counts = {}\n+    unique_names = []\n+    for name in names:\n+        if name in counts:\n+            counts[name] += 1\n+            name = name + '_' + str(counts[name])\n+        else:\n+            counts[name] = 1\n+        unique_names.append(name)\n+    return unique_names\n+\n+\n def _serialize_model(model, h5dict, include_optimizer=True):\n     \"\"\"Model serialization logic.\n \n@@ -119,6 +138,7 @@ def _serialize_model(model, h5dict, include_optimizer=True):\n                     idx += 1\n                 name = unique_name\n             weight_names.append(name.encode('utf8'))\n+        weight_names = _uniquify(weight_names)\n         layer_group['weight_names'] = weight_names\n         for name, val in zip(weight_names, weight_values):\n             layer_group[name] = val\n@@ -177,6 +197,7 @@ def _serialize_model(model, h5dict, include_optimizer=True):\n                             idx += 1\n                         name = unique_name\n                     weight_names.append(name.encode('utf8'))\n+                weight_names = _uniquify(weight_names)\n                 optimizer_weights_group['weight_names'] = weight_names\n                 for name, val in zip(weight_names, weight_values):\n                     optimizer_weights_group[name] = val\n\n@@ -192,7 +192,8 @@ class SGD(Optimizer):\n                                                       K.dtype(self.decay))))\n         # momentum\n         shapes = [K.int_shape(p) for p in params]\n-        moments = [K.zeros(shape) for shape in shapes]\n+        moments = [K.zeros(shape, name='moment_' + str(i))\n+                   for (i, shape) in enumerate(shapes)]\n         self.weights = [self.iterations] + moments\n         for p, g, m in zip(params, grads, moments):\n             v = self.momentum * m - lr * g  # velocity\n@@ -257,7 +258,10 @@ class RMSprop(Optimizer):\n     @K.symbolic\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n-        accumulators = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n+        accumulators = [K.zeros(K.int_shape(p),\n+                        dtype=K.dtype(p),\n+                        name='accumulator_' + str(i))\n+                        for (i, p) in enumerate(params)]\n         self.weights = accumulators\n         self.updates = [K.update_add(self.iterations, 1)]\n \n@@ -325,7 +329,8 @@ class Adagrad(Optimizer):\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n         shapes = [K.int_shape(p) for p in params]\n-        accumulators = [K.zeros(shape) for shape in shapes]\n+        accumulators = [K.zeros(shape, name='accumulator_' + str(i))\n+                        for (i, shape) in enumerate(shapes)]\n         self.weights = accumulators\n         self.updates = [K.update_add(self.iterations, 1)]\n \n@@ -399,8 +404,10 @@ class Adadelta(Optimizer):\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n         shapes = [K.int_shape(p) for p in params]\n-        accumulators = [K.zeros(shape) for shape in shapes]\n-        delta_accumulators = [K.zeros(shape) for shape in shapes]\n+        accumulators = [K.zeros(shape, name='accumulator_' + str(i))\n+                        for (i, shape) in enumerate(shapes)]\n+        delta_accumulators = [K.zeros(shape, name='delta_accumulator_' + str(i))\n+                        for (i, shape) in enumerate(shapes)]\n         self.weights = accumulators + delta_accumulators\n         self.updates = [K.update_add(self.iterations, 1)]\n \n@@ -490,12 +497,22 @@ class Adam(Optimizer):\n         lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n                      (1. - K.pow(self.beta_1, t)))\n \n-        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n-        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n+        ms = [K.zeros(K.int_shape(p),\n+              dtype=K.dtype(p),\n+              name='m_' + str(i))\n+              for (i, p) in enumerate(params)]\n+        vs = [K.zeros(K.int_shape(p),\n+              dtype=K.dtype(p),\n+              name='v_' + str(i))\n+              for (i, p) in enumerate(params)]\n+\n         if self.amsgrad:\n-            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n+            vhats = [K.zeros(K.int_shape(p),\n+                dtype=K.dtype(p),\n+                name='vhat_' + str(i))\n+                for (i, p) in enumerate(params)]\n         else:\n-            vhats = [K.zeros(1) for _ in params]\n+            vhats = [K.zeros(1, name='vhat_' + str(i)) for i in range(len(params))]\n         self.weights = [self.iterations] + ms + vs + vhats\n \n         for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n@@ -578,9 +595,11 @@ class Adamax(Optimizer):\n \n         shapes = [K.int_shape(p) for p in params]\n         # zero init of 1st moment\n-        ms = [K.zeros(shape) for shape in shapes]\n+        ms = [K.zeros(shape, name='m_' + str(i))\n+              for (i, shape) in enumerate(shapes)]\n         # zero init of exponentially weighted infinity norm\n-        us = [K.zeros(shape) for shape in shapes]\n+        us = [K.zeros(shape, name='u_' + str(i))\n+              for (i, shape) in enumerate(shapes)]\n         self.weights = [self.iterations] + ms + us\n \n         for p, g, m, u in zip(params, grads, ms, us):\n@@ -665,8 +684,10 @@ class Nadam(Optimizer):\n         self.updates.append((self.m_schedule, m_schedule_new))\n \n         shapes = [K.int_shape(p) for p in params]\n-        ms = [K.zeros(shape) for shape in shapes]\n-        vs = [K.zeros(shape) for shape in shapes]\n+        ms = [K.zeros(shape, name='m_' + str(i))\n+              for (i, shape) in enumerate(shapes)]\n+        vs = [K.zeros(shape, name='v_' + str(i))\n+              for (i, shape) in enumerate(shapes)]\n \n         self.weights = [self.iterations] + ms + vs\n \n\n@@ -5,10 +5,12 @@ from numpy.testing import assert_allclose\n \n from keras.utils import test_utils\n from keras import optimizers, Input\n-from keras.models import Sequential, Model\n+from keras.models import Sequential, Model, load_model\n from keras.layers.core import Dense, Activation, Lambda\n from keras.utils.np_utils import to_categorical\n from keras import backend as K\n+import tempfile\n+\n \n num_classes = 2\n \n@@ -62,6 +64,19 @@ def _test_optimizer(optimizer, target=0.75):\n     assert_allclose(kernel, 1.)\n     assert_allclose(bias, 2.)\n \n+    # Test saving.\n+    model = Sequential()\n+    model.add(Dense(1, input_dim=1))\n+    model.compile(loss='mse', optimizer=optimizer)\n+    model.fit(np.zeros((1, 1)), np.zeros((1, 1)))\n+\n+    _, fname = tempfile.mkstemp('.h5')\n+    model.save(fname)\n+    model2 = load_model(fname)\n+\n+    for w1, w2 in zip(model.get_weights(), model2.get_weights()):\n+        assert_allclose(w1, w2)\n+\n \n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n                     reason=\"Only Tensorflow raises a \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2fbcf34459bab0a126600d26309570497d0f44c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 1851 | Contributors (this commit): 14 | Commits (past 90d): 3 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -35,11 +35,18 @@ except ImportError:\n \n \n def _uniquify(names):\n-    \"\"\"\n+    \"\"\"Uniquify list of strings.\n+\n     Custom layers and optimizers written by users\n     for TF 1.x might produce weights with same variable\n     names in TF 2. This method \"uniquifies\" a given list\n     of names. e.g: ['a', 'b', 'b', 'c'] -> ['a', 'b', 'b_2', 'c']\n+\n+    # Arguments:\n+        names: List of strings.\n+\n+    # Returns:\n+        List of unique strings.\n     \"\"\"\n     counts = {}\n     unique_names = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a41ccecfe593589b3793ce8d633e797875fa80e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1859 | Contributors (this commit): 14 | Commits (past 90d): 4 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -40,12 +40,14 @@ def _uniquify(names):\n     Custom layers and optimizers written by users\n     for TF 1.x might produce weights with same variable\n     names in TF 2. This method \"uniquifies\" a given list\n-    of names. e.g: ['a', 'b', 'b', 'c'] -> ['a', 'b', 'b_2', 'c']\n+    of names.\n \n-    # Arguments:\n+    e.g: `['a', 'b', 'b', 'c'] -> ['a', 'b', 'b_2', 'c']`\n+\n+    # Arguments\n         names: List of strings.\n \n-    # Returns:\n+    # Returns\n         List of unique strings.\n     \"\"\"\n     counts = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#408a344e50ef809812febc6e8dcabd34cfc11c60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 4 | Churn Cumulative: 123 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,7 @@ from ..utils.generic_utils import to_list\n \n \n def _get_available_devices():\n-    return [x.name for x in K.get_session().list_devices()]\n+    return K.tensorflow_backend._get_available_gpus()\n \n \n def _normalize_device_name(name):\n@@ -153,7 +153,7 @@ def multi_gpu_model(model, gpus=None, cpu_merge=True, cpu_relocation=False):\n     if not gpus:\n         # Using all visible GPUs when not specifying `gpus`\n         # e.g. CUDA_VISIBLE_DEVICES=0,2 python keras_mgpu.py\n-        gpus = len([x for x in available_devices if 'gpu' in x])\n+        gpus = len(available_devices)\n \n     if isinstance(gpus, (list, tuple)):\n         if len(gpus) <= 1:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c6428ee021c3e4dbff71ea0fc7ecc677c5584eab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 14272 | Contributors (this commit): 129 | Commits (past 90d): 21 | Contributors (cumulative): 129 | DMM Complexity: None\n\nDIFF:\n@@ -3235,9 +3235,11 @@ def in_top_k(predictions, targets, k):\n         `output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k`\n         values of `predictions[i]`.\n     \"\"\"\n-    return tf.nn.in_top_k(tf.cast(predictions, 'float32'),\n-                          tf.cast(targets, 'int32'),\n-                          k)\n+    # Note that the order of the 2 first positional arguments\n+    # has been inverted in TF 2.\n+    return tf.nn.in_top_k(predictions=predictions,\n+                          targets=targets,\n+                          k=k)\n \n \n # CONVOLUTIONS\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7c5057b42c63337446dc4eb79c433a216520864d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 772 | Contributors (this commit): 14 | Commits (past 90d): 2 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -260,15 +260,6 @@ def test_Bidirectional():\n         model.compile(loss='mse', optimizer='sgd')\n         model.fit(x, y, epochs=1, batch_size=1)\n \n-        # test with functional API\n-        inputs = Input((timesteps, dim))\n-        outputs = wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n-                                             recurrent_dropout=dropout_rate),\n-                                         merge_mode=mode)(inputs)\n-        model = Model(inputs, outputs)\n-        model.compile(loss='mse', optimizer='sgd')\n-        model.fit(x, y, epochs=1, batch_size=1)\n-\n         # Bidirectional and stateful\n         inputs = Input(batch_shape=(1, timesteps, dim))\n         outputs = wrappers.Bidirectional(rnn(output_dim, stateful=True),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#995f1e708fc189fd2a6786be96edf82abab2c599", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 908 | Lines Deleted: 662 | Files Changed: 6 | Hunks: 23 | Methods Changed: 50 | Complexity Δ (Sum/Max): 400/217 | Churn Δ: 1570 | Churn Cumulative: 1570 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.6526315789473685\n\nDIFF:\n@@ -0,0 +1,22 @@\n+from __future__ import absolute_import\n+\n+from .callbacks import Callback\n+from .callbacks import CallbackList\n+from .callbacks import BaseLogger\n+from .callbacks import TerminateOnNaN\n+from .callbacks import ProgbarLogger\n+from .callbacks import History\n+from .callbacks import ModelCheckpoint\n+from .callbacks import EarlyStopping\n+from .callbacks import RemoteMonitor\n+from .callbacks import LearningRateScheduler\n+from .callbacks import ReduceLROnPlateau\n+from .callbacks import CSVLogger\n+from .callbacks import LambdaCallback\n+\n+from .. import backend as K\n+\n+if K.backend() == 'tensorflow' and not K.tensorflow_backend._is_tf_1():\n+    from .tensorboard_v2 import TensorBoard\n+else:\n+    from .tensorboard_v1 import TensorBoard\n\n@@ -18,9 +18,9 @@ from collections import deque\n from collections import OrderedDict\n from collections import Iterable\n from collections import defaultdict\n-from .utils.generic_utils import Progbar\n-from . import backend as K\n-from .engine.training_utils import standardize_input_data\n+from ..utils.generic_utils import Progbar\n+from .. import backend as K\n+from ..engine.training_utils import standardize_input_data\n \n try:\n     import requests\n@@ -939,350 +939,6 @@ class LearningRateScheduler(Callback):\n         logs['lr'] = K.get_value(self.model.optimizer.lr)\n \n \n-class TensorBoard(Callback):\n-    \"\"\"TensorBoard basic visualizations.\n-\n-    [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard)\n-    is a visualization tool provided with TensorFlow.\n-\n-    This callback writes a log for TensorBoard, which allows\n-    you to visualize dynamic graphs of your training and test\n-    metrics, as well as activation histograms for the different\n-    layers in your model.\n-\n-    If you have installed TensorFlow with pip, you should be able\n-    to launch TensorBoard from the command line:\n-    ```sh\n-    tensorboard --logdir=/full_path_to_your_logs\n-    ```\n-\n-    When using a backend other than TensorFlow, TensorBoard will still work\n-    (if you have TensorFlow installed), but the only feature available will\n-    be the display of the losses and metrics plots.\n-\n-    # Arguments\n-        log_dir: the path of the directory where to save the log\n-            files to be parsed by TensorBoard.\n-        histogram_freq: frequency (in epochs) at which to compute activation\n-            and weight histograms for the layers of the model. If set to 0,\n-            histograms won't be computed. Validation data (or split) must be\n-            specified for histogram visualizations.\n-        batch_size: size of batch of inputs to feed to the network\n-            for histograms computation.\n-        write_graph: whether to visualize the graph in TensorBoard.\n-            The log file can become quite large when\n-            write_graph is set to True.\n-        write_grads: whether to visualize gradient histograms in TensorBoard.\n-            `histogram_freq` must be greater than 0.\n-        write_images: whether to write model weights to visualize as\n-            image in TensorBoard.\n-        embeddings_freq: frequency (in epochs) at which selected embedding\n-            layers will be saved. If set to 0, embeddings won't be computed.\n-            Data to be visualized in TensorBoard's Embedding tab must be passed\n-            as `embeddings_data`.\n-        embeddings_layer_names: a list of names of layers to keep eye on. If\n-            None or empty list all the embedding layer will be watched.\n-        embeddings_metadata: a dictionary which maps layer name to a file name\n-            in which metadata for this embedding layer is saved. See the\n-            [details](https://www.tensorflow.org/guide/embedding#metadata)\n-            about metadata files format. In case if the same metadata file is\n-            used for all embedding layers, string can be passed.\n-        embeddings_data: data to be embedded at layers specified in\n-            `embeddings_layer_names`. Numpy array (if the model has a single\n-            input) or list of Numpy arrays (if the model has multiple inputs).\n-            Learn [more about embeddings](\n-            https://www.tensorflow.org/guide/embedding).\n-        update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`, writes\n-            the losses and metrics to TensorBoard after each batch. The same\n-            applies for `'epoch'`. If using an integer, let's say `10000`,\n-            the callback will write the metrics and losses to TensorBoard every\n-            10000 samples. Note that writing too frequently to TensorBoard\n-            can slow down your training.\n-    \"\"\"\n-\n-    def __init__(self, log_dir='./logs',\n-                 histogram_freq=0,\n-                 batch_size=32,\n-                 write_graph=True,\n-                 write_grads=False,\n-                 write_images=False,\n-                 embeddings_freq=0,\n-                 embeddings_layer_names=None,\n-                 embeddings_metadata=None,\n-                 embeddings_data=None,\n-                 update_freq='epoch'):\n-        super(TensorBoard, self).__init__()\n-        global tf, projector\n-        try:\n-            import tensorflow as tf\n-            from tensorflow.contrib.tensorboard.plugins import projector\n-        except ImportError:\n-            raise ImportError('You need the TensorFlow module installed to '\n-                              'use TensorBoard.')\n-\n-        if K.backend() != 'tensorflow':\n-            if histogram_freq != 0:\n-                warnings.warn('You are not using the TensorFlow backend. '\n-                              'histogram_freq was set to 0')\n-                histogram_freq = 0\n-            if write_graph:\n-                warnings.warn('You are not using the TensorFlow backend. '\n-                              'write_graph was set to False')\n-                write_graph = False\n-            if write_images:\n-                warnings.warn('You are not using the TensorFlow backend. '\n-                              'write_images was set to False')\n-                write_images = False\n-            if embeddings_freq != 0:\n-                warnings.warn('You are not using the TensorFlow backend. '\n-                              'embeddings_freq was set to 0')\n-                embeddings_freq = 0\n-\n-        self.log_dir = log_dir\n-        self.histogram_freq = histogram_freq\n-        self.merged = None\n-        self.write_graph = write_graph\n-        self.write_grads = write_grads\n-        self.write_images = write_images\n-        self.embeddings_freq = embeddings_freq\n-        self.embeddings_layer_names = embeddings_layer_names\n-        self.embeddings_metadata = embeddings_metadata or {}\n-        self.batch_size = batch_size\n-        self.embeddings_data = embeddings_data\n-        if update_freq == 'batch':\n-            # It is the same as writing as frequently as possible.\n-            self.update_freq = 1\n-        else:\n-            self.update_freq = update_freq\n-        self.samples_seen = 0\n-        self.samples_seen_at_last_write = 0\n-\n-    def set_model(self, model):\n-        self.model = model\n-        if K.backend() == 'tensorflow':\n-            self.sess = K.get_session()\n-        if self.histogram_freq and self.merged is None:\n-            for layer in self.model.layers:\n-                for weight in layer.weights:\n-                    mapped_weight_name = weight.name.replace(':', '_')\n-                    tf.summary.histogram(mapped_weight_name, weight)\n-                    if self.write_grads and weight in layer.trainable_weights:\n-                        grads = model.optimizer.get_gradients(model.total_loss,\n-                                                              weight)\n-\n-                        def is_indexed_slices(grad):\n-                            return type(grad).__name__ == 'IndexedSlices'\n-                        grads = [\n-                            grad.values if is_indexed_slices(grad) else grad\n-                            for grad in grads]\n-                        tf.summary.histogram('{}_grad'.format(mapped_weight_name),\n-                                             grads)\n-                    if self.write_images:\n-                        w_img = tf.squeeze(weight)\n-                        shape = K.int_shape(w_img)\n-                        if len(shape) == 2:  # dense layer kernel case\n-                            if shape[0] > shape[1]:\n-                                w_img = tf.transpose(w_img)\n-                                shape = K.int_shape(w_img)\n-                            w_img = tf.reshape(w_img, [1,\n-                                                       shape[0],\n-                                                       shape[1],\n-                                                       1])\n-                        elif len(shape) == 3:  # convnet case\n-                            if K.image_data_format() == 'channels_last':\n-                                # switch to channels_first to display\n-                                # every kernel as a separate image\n-                                w_img = tf.transpose(w_img, perm=[2, 0, 1])\n-                                shape = K.int_shape(w_img)\n-                            w_img = tf.reshape(w_img, [shape[0],\n-                                                       shape[1],\n-                                                       shape[2],\n-                                                       1])\n-                        elif len(shape) == 1:  # bias case\n-                            w_img = tf.reshape(w_img, [1,\n-                                                       shape[0],\n-                                                       1,\n-                                                       1])\n-                        else:\n-                            # not possible to handle 3D convnets etc.\n-                            continue\n-\n-                        shape = K.int_shape(w_img)\n-                        assert len(shape) == 4 and shape[-1] in [1, 3, 4]\n-                        tf.summary.image(mapped_weight_name, w_img)\n-\n-                if hasattr(layer, 'output'):\n-                    if isinstance(layer.output, list):\n-                        for i, output in enumerate(layer.output):\n-                            tf.summary.histogram('{}_out_{}'.format(layer.name, i),\n-                                                 output)\n-                    else:\n-                        tf.summary.histogram('{}_out'.format(layer.name),\n-                                             layer.output)\n-        self.merged = tf.summary.merge_all()\n-\n-        if self.write_graph:\n-            self.writer = tf.summary.FileWriter(self.log_dir,\n-                                                self.sess.graph)\n-        else:\n-            self.writer = tf.summary.FileWriter(self.log_dir)\n-\n-        if self.embeddings_freq and self.embeddings_data is not None:\n-            self.embeddings_data = standardize_input_data(self.embeddings_data,\n-                                                          model.input_names)\n-\n-            embeddings_layer_names = self.embeddings_layer_names\n-\n-            if not embeddings_layer_names:\n-                embeddings_layer_names = [layer.name for layer in self.model.layers\n-                                          if type(layer).__name__ == 'Embedding']\n-            self.assign_embeddings = []\n-            embeddings_vars = {}\n-\n-            self.batch_id = batch_id = tf.placeholder(tf.int32)\n-            self.step = step = tf.placeholder(tf.int32)\n-\n-            for layer in self.model.layers:\n-                if layer.name in embeddings_layer_names:\n-                    embedding_input = self.model.get_layer(layer.name).output\n-                    embedding_size = np.prod(embedding_input.shape[1:])\n-                    embedding_input = tf.reshape(embedding_input,\n-                                                 (step, int(embedding_size)))\n-                    shape = (self.embeddings_data[0].shape[0], int(embedding_size))\n-                    embedding = K.variable(K.zeros(shape),\n-                                           name=layer.name + '_embedding')\n-                    embeddings_vars[layer.name] = embedding\n-                    batch = tf.assign(embedding[batch_id:batch_id + step],\n-                                      embedding_input)\n-                    self.assign_embeddings.append(batch)\n-\n-            self.saver = tf.train.Saver(list(embeddings_vars.values()))\n-\n-            if not isinstance(self.embeddings_metadata, str):\n-                embeddings_metadata = self.embeddings_metadata\n-            else:\n-                embeddings_metadata = {layer_name: self.embeddings_metadata\n-                                       for layer_name in embeddings_vars.keys()}\n-\n-            config = projector.ProjectorConfig()\n-\n-            for layer_name, tensor in embeddings_vars.items():\n-                embedding = config.embeddings.add()\n-                embedding.tensor_name = tensor.name\n-\n-                if layer_name in embeddings_metadata:\n-                    embedding.metadata_path = embeddings_metadata[layer_name]\n-\n-            projector.visualize_embeddings(self.writer, config)\n-\n-    def on_epoch_end(self, epoch, logs=None):\n-        logs = logs or {}\n-\n-        if not self.validation_data and self.histogram_freq:\n-            raise ValueError(\"If printing histograms, validation_data must be \"\n-                             \"provided, and cannot be a generator.\")\n-        if self.embeddings_data is None and self.embeddings_freq:\n-            raise ValueError(\"To visualize embeddings, embeddings_data must \"\n-                             \"be provided.\")\n-        if self.validation_data and self.histogram_freq:\n-            if epoch % self.histogram_freq == 0:\n-\n-                val_data = self.validation_data\n-                tensors = (self.model.inputs +\n-                           self.model.targets +\n-                           self.model.sample_weights)\n-\n-                if self.model.uses_learning_phase:\n-                    tensors += [K.learning_phase()]\n-\n-                assert len(val_data) == len(tensors)\n-                val_size = val_data[0].shape[0]\n-                i = 0\n-                while i < val_size:\n-                    step = min(self.batch_size, val_size - i)\n-                    if self.model.uses_learning_phase:\n-                        # do not slice the learning phase\n-                        batch_val = [x[i:i + step] for x in val_data[:-1]]\n-                        batch_val.append(val_data[-1])\n-                    else:\n-                        batch_val = [x[i:i + step] for x in val_data]\n-                    assert len(batch_val) == len(tensors)\n-                    feed_dict = dict(zip(tensors, batch_val))\n-                    result = self.sess.run([self.merged], feed_dict=feed_dict)\n-                    summary_str = result[0]\n-                    self.writer.add_summary(summary_str, epoch)\n-                    i += self.batch_size\n-\n-        if self.embeddings_freq and self.embeddings_data is not None:\n-            if epoch % self.embeddings_freq == 0:\n-                # We need a second forward-pass here because we're passing\n-                # the `embeddings_data` explicitly. This design allows to pass\n-                # arbitrary data as `embeddings_data` and results from the fact\n-                # that we need to know the size of the `tf.Variable`s which\n-                # hold the embeddings in `set_model`. At this point, however,\n-                # the `validation_data` is not yet set.\n-\n-                # More details in this discussion:\n-                # https://github.com/keras-team/keras/pull/7766#issuecomment-329195622\n-\n-                embeddings_data = self.embeddings_data\n-                n_samples = embeddings_data[0].shape[0]\n-\n-                i = 0\n-                while i < n_samples:\n-                    step = min(self.batch_size, n_samples - i)\n-                    batch = slice(i, i + step)\n-\n-                    if type(self.model.input) == list:\n-                        feed_dict = {_input: embeddings_data[idx][batch]\n-                                     for idx, _input in enumerate(self.model.input)}\n-                    else:\n-                        feed_dict = {self.model.input: embeddings_data[0][batch]}\n-\n-                    feed_dict.update({self.batch_id: i, self.step: step})\n-\n-                    if self.model.uses_learning_phase:\n-                        feed_dict[K.learning_phase()] = False\n-\n-                    self.sess.run(self.assign_embeddings, feed_dict=feed_dict)\n-                    self.saver.save(self.sess,\n-                                    os.path.join(self.log_dir,\n-                                                 'keras_embedding.ckpt'),\n-                                    epoch)\n-\n-                    i += self.batch_size\n-\n-        if self.update_freq == 'epoch':\n-            index = epoch\n-        else:\n-            index = self.samples_seen\n-        self._write_logs(logs, index)\n-\n-    def _write_logs(self, logs, index):\n-        for name, value in logs.items():\n-            if name in ['batch', 'size']:\n-                continue\n-            summary = tf.Summary()\n-            summary_value = summary.value.add()\n-            if isinstance(value, np.ndarray):\n-                summary_value.simple_value = value.item()\n-            else:\n-                summary_value.simple_value = value\n-            summary_value.tag = name\n-            self.writer.add_summary(summary, index)\n-        self.writer.flush()\n-\n-    def on_train_end(self, _):\n-        self.writer.close()\n-\n-    def on_batch_end(self, batch, logs=None):\n-        if self.update_freq != 'epoch':\n-            self.samples_seen += logs['size']\n-            samples_seen_since = self.samples_seen - self.samples_seen_at_last_write\n-            if samples_seen_since >= self.update_freq:\n-                self._write_logs(logs, self.samples_seen)\n-                self.samples_seen_at_last_write = self.samples_seen\n-\n \n class ReduceLROnPlateau(Callback):\n     \"\"\"Reduce learning rate when a metric has stopped improving.\n\n@@ -0,0 +1,362 @@\n+\"\"\"TensorBoard callback for training visualization.\n+\n+This is the TF v1 version. A subset of the functionality\n+also works with other backends.\n+\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import os\n+\n+import numpy as np\n+import warnings\n+\n+from .. import backend as K\n+from ..engine.training_utils import standardize_input_data\n+from . import Callback\n+\n+\n+class TensorBoard(Callback):\n+    \"\"\"TensorBoard basic visualizations.\n+\n+    [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard)\n+    is a visualization tool provided with TensorFlow.\n+\n+    This callback writes a log for TensorBoard, which allows\n+    you to visualize dynamic graphs of your training and test\n+    metrics, as well as activation histograms for the different\n+    layers in your model.\n+\n+    If you have installed TensorFlow with pip, you should be able\n+    to launch TensorBoard from the command line:\n+    ```sh\n+    tensorboard --logdir=/full_path_to_your_logs\n+    ```\n+\n+    When using a backend other than TensorFlow, TensorBoard will still work\n+    (if you have TensorFlow installed), but the only feature available will\n+    be the display of the losses and metrics plots.\n+\n+    # Arguments\n+        log_dir: the path of the directory where to save the log\n+            files to be parsed by TensorBoard.\n+        histogram_freq: frequency (in epochs) at which to compute activation\n+            and weight histograms for the layers of the model. If set to 0,\n+            histograms won't be computed. Validation data (or split) must be\n+            specified for histogram visualizations.\n+        batch_size: size of batch of inputs to feed to the network\n+            for histograms computation.\n+        write_graph: whether to visualize the graph in TensorBoard.\n+            The log file can become quite large when\n+            write_graph is set to True.\n+        write_grads: whether to visualize gradient histograms in TensorBoard.\n+            `histogram_freq` must be greater than 0.\n+        write_images: whether to write model weights to visualize as\n+            image in TensorBoard.\n+        embeddings_freq: frequency (in epochs) at which selected embedding\n+            layers will be saved. If set to 0, embeddings won't be computed.\n+            Data to be visualized in TensorBoard's Embedding tab must be passed\n+            as `embeddings_data`.\n+        embeddings_layer_names: a list of names of layers to keep eye on. If\n+            None or empty list all the embedding layer will be watched.\n+        embeddings_metadata: a dictionary which maps layer name to a file name\n+            in which metadata for this embedding layer is saved. See the\n+            [details](https://www.tensorflow.org/guide/embedding#metadata)\n+            about metadata files format. In case if the same metadata file is\n+            used for all embedding layers, string can be passed.\n+        embeddings_data: data to be embedded at layers specified in\n+            `embeddings_layer_names`. Numpy array (if the model has a single\n+            input) or list of Numpy arrays (if the model has multiple inputs).\n+            Learn [more about embeddings](\n+            https://www.tensorflow.org/guide/embedding).\n+        update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`, writes\n+            the losses and metrics to TensorBoard after each batch. The same\n+            applies for `'epoch'`. If using an integer, let's say `10000`,\n+            the callback will write the metrics and losses to TensorBoard every\n+            10000 samples. Note that writing too frequently to TensorBoard\n+            can slow down your training.\n+    \"\"\"\n+\n+    def __init__(self, log_dir='./logs',\n+                 histogram_freq=0,\n+                 batch_size=32,\n+                 write_graph=True,\n+                 write_grads=False,\n+                 write_images=False,\n+                 embeddings_freq=0,\n+                 embeddings_layer_names=None,\n+                 embeddings_metadata=None,\n+                 embeddings_data=None,\n+                 update_freq='epoch'):\n+        super(TensorBoard, self).__init__()\n+        global tf, projector\n+        try:\n+            import tensorflow as tf\n+            from tensorflow.contrib.tensorboard.plugins import projector\n+        except ImportError:\n+            raise ImportError('You need the TensorFlow module installed to '\n+                              'use TensorBoard.')\n+\n+        if K.backend() != 'tensorflow':\n+            if histogram_freq != 0:\n+                warnings.warn('You are not using the TensorFlow backend. '\n+                              'histogram_freq was set to 0')\n+                histogram_freq = 0\n+            if write_graph:\n+                warnings.warn('You are not using the TensorFlow backend. '\n+                              'write_graph was set to False')\n+                write_graph = False\n+            if write_images:\n+                warnings.warn('You are not using the TensorFlow backend. '\n+                              'write_images was set to False')\n+                write_images = False\n+            if embeddings_freq != 0:\n+                warnings.warn('You are not using the TensorFlow backend. '\n+                              'embeddings_freq was set to 0')\n+                embeddings_freq = 0\n+\n+        self.log_dir = log_dir\n+        self.histogram_freq = histogram_freq\n+        self.merged = None\n+        self.write_graph = write_graph\n+        self.write_grads = write_grads\n+        self.write_images = write_images\n+        self.embeddings_freq = embeddings_freq\n+        self.embeddings_layer_names = embeddings_layer_names\n+        self.embeddings_metadata = embeddings_metadata or {}\n+        self.batch_size = batch_size\n+        self.embeddings_data = embeddings_data\n+        if update_freq == 'batch':\n+            # It is the same as writing as frequently as possible.\n+            self.update_freq = 1\n+        else:\n+            self.update_freq = update_freq\n+        self.samples_seen = 0\n+        self.samples_seen_at_last_write = 0\n+\n+    def set_model(self, model):\n+        self.model = model\n+        if K.backend() == 'tensorflow':\n+            self.sess = K.get_session()\n+        if self.histogram_freq and self.merged is None:\n+            for layer in self.model.layers:\n+                for weight in layer.weights:\n+                    mapped_weight_name = weight.name.replace(':', '_')\n+                    tf.summary.histogram(mapped_weight_name, weight)\n+                    if self.write_grads and weight in layer.trainable_weights:\n+                        grads = model.optimizer.get_gradients(model.total_loss,\n+                                                              weight)\n+\n+                        def is_indexed_slices(grad):\n+                            return type(grad).__name__ == 'IndexedSlices'\n+                        grads = [\n+                            grad.values if is_indexed_slices(grad) else grad\n+                            for grad in grads]\n+                        tf.summary.histogram('{}_grad'.format(mapped_weight_name),\n+                                             grads)\n+                    if self.write_images:\n+                        w_img = tf.squeeze(weight)\n+                        shape = K.int_shape(w_img)\n+                        if len(shape) == 2:  # dense layer kernel case\n+                            if shape[0] > shape[1]:\n+                                w_img = tf.transpose(w_img)\n+                                shape = K.int_shape(w_img)\n+                            w_img = tf.reshape(w_img, [1,\n+                                                       shape[0],\n+                                                       shape[1],\n+                                                       1])\n+                        elif len(shape) == 3:  # convnet case\n+                            if K.image_data_format() == 'channels_last':\n+                                # switch to channels_first to display\n+                                # every kernel as a separate image\n+                                w_img = tf.transpose(w_img, perm=[2, 0, 1])\n+                                shape = K.int_shape(w_img)\n+                            w_img = tf.reshape(w_img, [shape[0],\n+                                                       shape[1],\n+                                                       shape[2],\n+                                                       1])\n+                        elif len(shape) == 1:  # bias case\n+                            w_img = tf.reshape(w_img, [1,\n+                                                       shape[0],\n+                                                       1,\n+                                                       1])\n+                        else:\n+                            # not possible to handle 3D convnets etc.\n+                            continue\n+\n+                        shape = K.int_shape(w_img)\n+                        assert len(shape) == 4 and shape[-1] in [1, 3, 4]\n+                        tf.summary.image(mapped_weight_name, w_img)\n+\n+                if hasattr(layer, 'output'):\n+                    if isinstance(layer.output, list):\n+                        for i, output in enumerate(layer.output):\n+                            tf.summary.histogram('{}_out_{}'.format(layer.name, i),\n+                                                 output)\n+                    else:\n+                        tf.summary.histogram('{}_out'.format(layer.name),\n+                                             layer.output)\n+        self.merged = tf.summary.merge_all()\n+\n+        if self.write_graph:\n+            self.writer = tf.summary.FileWriter(self.log_dir,\n+                                                self.sess.graph)\n+        else:\n+            self.writer = tf.summary.FileWriter(self.log_dir)\n+\n+        if self.embeddings_freq and self.embeddings_data is not None:\n+            self.embeddings_data = standardize_input_data(self.embeddings_data,\n+                                                          model.input_names)\n+\n+            embeddings_layer_names = self.embeddings_layer_names\n+\n+            if not embeddings_layer_names:\n+                embeddings_layer_names = [layer.name for layer in self.model.layers\n+                                          if type(layer).__name__ == 'Embedding']\n+            self.assign_embeddings = []\n+            embeddings_vars = {}\n+\n+            self.batch_id = batch_id = tf.placeholder(tf.int32)\n+            self.step = step = tf.placeholder(tf.int32)\n+\n+            for layer in self.model.layers:\n+                if layer.name in embeddings_layer_names:\n+                    embedding_input = self.model.get_layer(layer.name).output\n+                    embedding_size = np.prod(embedding_input.shape[1:])\n+                    embedding_input = tf.reshape(embedding_input,\n+                                                 (step, int(embedding_size)))\n+                    shape = (self.embeddings_data[0].shape[0], int(embedding_size))\n+                    embedding = K.variable(K.zeros(shape),\n+                                           name=layer.name + '_embedding')\n+                    embeddings_vars[layer.name] = embedding\n+                    batch = tf.assign(embedding[batch_id:batch_id + step],\n+                                      embedding_input)\n+                    self.assign_embeddings.append(batch)\n+\n+            self.saver = tf.train.Saver(list(embeddings_vars.values()))\n+\n+            if not isinstance(self.embeddings_metadata, str):\n+                embeddings_metadata = self.embeddings_metadata\n+            else:\n+                embeddings_metadata = {layer_name: self.embeddings_metadata\n+                                       for layer_name in embeddings_vars.keys()}\n+\n+            config = projector.ProjectorConfig()\n+\n+            for layer_name, tensor in embeddings_vars.items():\n+                embedding = config.embeddings.add()\n+                embedding.tensor_name = tensor.name\n+\n+                if layer_name in embeddings_metadata:\n+                    embedding.metadata_path = embeddings_metadata[layer_name]\n+\n+            projector.visualize_embeddings(self.writer, config)\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        logs = logs or {}\n+\n+        if not self.validation_data and self.histogram_freq:\n+            raise ValueError(\"If printing histograms, validation_data must be \"\n+                             \"provided, and cannot be a generator.\")\n+        if self.embeddings_data is None and self.embeddings_freq:\n+            raise ValueError(\"To visualize embeddings, embeddings_data must \"\n+                             \"be provided.\")\n+        if self.validation_data and self.histogram_freq:\n+            if epoch % self.histogram_freq == 0:\n+\n+                val_data = self.validation_data\n+                tensors = (self.model.inputs +\n+                           self.model.targets +\n+                           self.model.sample_weights)\n+\n+                if self.model.uses_learning_phase:\n+                    tensors += [K.learning_phase()]\n+\n+                assert len(val_data) == len(tensors)\n+                val_size = val_data[0].shape[0]\n+                i = 0\n+                while i < val_size:\n+                    step = min(self.batch_size, val_size - i)\n+                    if self.model.uses_learning_phase:\n+                        # do not slice the learning phase\n+                        batch_val = [x[i:i + step] for x in val_data[:-1]]\n+                        batch_val.append(val_data[-1])\n+                    else:\n+                        batch_val = [x[i:i + step] for x in val_data]\n+                    assert len(batch_val) == len(tensors)\n+                    feed_dict = dict(zip(tensors, batch_val))\n+                    result = self.sess.run([self.merged], feed_dict=feed_dict)\n+                    summary_str = result[0]\n+                    self.writer.add_summary(summary_str, epoch)\n+                    i += self.batch_size\n+\n+        if self.embeddings_freq and self.embeddings_data is not None:\n+            if epoch % self.embeddings_freq == 0:\n+                # We need a second forward-pass here because we're passing\n+                # the `embeddings_data` explicitly. This design allows to pass\n+                # arbitrary data as `embeddings_data` and results from the fact\n+                # that we need to know the size of the `tf.Variable`s which\n+                # hold the embeddings in `set_model`. At this point, however,\n+                # the `validation_data` is not yet set.\n+\n+                # More details in this discussion:\n+                # https://github.com/keras-team/keras/pull/7766#issuecomment-329195622\n+\n+                embeddings_data = self.embeddings_data\n+                n_samples = embeddings_data[0].shape[0]\n+\n+                i = 0\n+                while i < n_samples:\n+                    step = min(self.batch_size, n_samples - i)\n+                    batch = slice(i, i + step)\n+\n+                    if type(self.model.input) == list:\n+                        feed_dict = {_input: embeddings_data[idx][batch]\n+                                     for idx, _input in enumerate(self.model.input)}\n+                    else:\n+                        feed_dict = {self.model.input: embeddings_data[0][batch]}\n+\n+                    feed_dict.update({self.batch_id: i, self.step: step})\n+\n+                    if self.model.uses_learning_phase:\n+                        feed_dict[K.learning_phase()] = False\n+\n+                    self.sess.run(self.assign_embeddings, feed_dict=feed_dict)\n+                    self.saver.save(self.sess,\n+                                    os.path.join(self.log_dir,\n+                                                 'keras_embedding.ckpt'),\n+                                    epoch)\n+\n+                    i += self.batch_size\n+\n+        if self.update_freq == 'epoch':\n+            index = epoch\n+        else:\n+            index = self.samples_seen\n+        self._write_logs(logs, index)\n+\n+    def _write_logs(self, logs, index):\n+        for name, value in logs.items():\n+            if name in ['batch', 'size']:\n+                continue\n+            summary = tf.Summary()\n+            summary_value = summary.value.add()\n+            if isinstance(value, np.ndarray):\n+                summary_value.simple_value = value.item()\n+            else:\n+                summary_value.simple_value = value\n+            summary_value.tag = name\n+            self.writer.add_summary(summary, index)\n+        self.writer.flush()\n+\n+    def on_train_end(self, _):\n+        self.writer.close()\n+\n+    def on_batch_end(self, batch, logs=None):\n+        if self.update_freq != 'epoch':\n+            self.samples_seen += logs['size']\n+            samples_seen_since = self.samples_seen - self.samples_seen_at_last_write\n+            if samples_seen_since >= self.update_freq:\n+                self._write_logs(logs, self.samples_seen)\n+                self.samples_seen_at_last_write = self.samples_seen\n\\ No newline at end of file\n\n@@ -0,0 +1,115 @@\n+\"\"\"TensorBoard callback for training visualization.\n+\n+This is the TF v2 version. A lot of the functionality\n+from the v1 version isn't currently supported (but will\n+likely be added back later).\n+\n+The docstring is left unchanged\n+to avoid creating confusion on the docs website.\n+\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import tensorflow as tf\n+\n+\n+class TensorBoard(tf.keras.callbacks.TensorBoard):\n+    \"\"\"TensorBoard basic visualizations.\n+\n+    [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard)\n+    is a visualization tool provided with TensorFlow.\n+\n+    This callback writes a log for TensorBoard, which allows\n+    you to visualize dynamic graphs of your training and test\n+    metrics, as well as activation histograms for the different\n+    layers in your model.\n+\n+    If you have installed TensorFlow with pip, you should be able\n+    to launch TensorBoard from the command line:\n+    ```sh\n+    tensorboard --logdir=/full_path_to_your_logs\n+    ```\n+\n+    When using a backend other than TensorFlow, TensorBoard will still work\n+    (if you have TensorFlow installed), but the only feature available will\n+    be the display of the losses and metrics plots.\n+\n+    # Arguments\n+        log_dir: the path of the directory where to save the log\n+            files to be parsed by TensorBoard.\n+        histogram_freq: frequency (in epochs) at which to compute activation\n+            and weight histograms for the layers of the model. If set to 0,\n+            histograms won't be computed. Validation data (or split) must be\n+            specified for histogram visualizations.\n+        batch_size: size of batch of inputs to feed to the network\n+            for histograms computation.\n+        write_graph: whether to visualize the graph in TensorBoard.\n+            The log file can become quite large when\n+            write_graph is set to True.\n+        write_grads: whether to visualize gradient histograms in TensorBoard.\n+            `histogram_freq` must be greater than 0.\n+        write_images: whether to write model weights to visualize as\n+            image in TensorBoard.\n+        embeddings_freq: frequency (in epochs) at which selected embedding\n+            layers will be saved. If set to 0, embeddings won't be computed.\n+            Data to be visualized in TensorBoard's Embedding tab must be passed\n+            as `embeddings_data`.\n+        embeddings_layer_names: a list of names of layers to keep eye on. If\n+            None or empty list all the embedding layer will be watched.\n+        embeddings_metadata: a dictionary which maps layer name to a file name\n+            in which metadata for this embedding layer is saved. See the\n+            [details](https://www.tensorflow.org/guide/embedding#metadata)\n+            about metadata files format. In case if the same metadata file is\n+            used for all embedding layers, string can be passed.\n+        embeddings_data: data to be embedded at layers specified in\n+            `embeddings_layer_names`. Numpy array (if the model has a single\n+            input) or list of Numpy arrays (if the model has multiple inputs).\n+            Learn [more about embeddings](\n+            https://www.tensorflow.org/guide/embedding).\n+        update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`, writes\n+            the losses and metrics to TensorBoard after each batch. The same\n+            applies for `'epoch'`. If using an integer, let's say `10000`,\n+            the callback will write the metrics and losses to TensorBoard every\n+            10000 samples. Note that writing too frequently to TensorBoard\n+            can slow down your training.\n+    \"\"\"\n+\n+    def __init__(self, log_dir='./logs',\n+                 histogram_freq=0,\n+                 batch_size=None,\n+                 write_graph=True,\n+                 write_grads=False,\n+                 write_images=False,\n+                 embeddings_freq=0,\n+                 embeddings_layer_names=None,\n+                 embeddings_metadata=None,\n+                 embeddings_data=None,\n+                 update_freq='epoch',\n+                 **kwargs):\n+        if batch_size is not None:\n+            warnings.warn('The TensorBoard callback `batch_size` argument '\n+                          '(for histogram computation) '\n+                          'is deprecated with TensorFlow 2.0. '\n+                          'It will be ignored.')\n+        if write_grads:\n+            warnings.warn('The TensorBoard callback does not support '\n+                          'gradients display when using TensorFlow 2.0. '\n+                          'The `write_grads` argument is ignored.')\n+        if (embeddings_freq or embeddings_layer_names or\n+                embeddings_metadata or embeddings_data):\n+            warnings.warn('The TensorBoard callback does not support '\n+                          'embeddings display when using TensorFlow 2.0. '\n+                          'Embeddings-related arguments are ignored.')\n+        super(TensorBoard, self).__init__(\n+                log_dir=log_dir,\n+                histogram_freq=histogram_freq,\n+                write_graph=write_graph,\n+                write_images=write_images,\n+                update_freq=update_freq,\n+                **kwargs)\n+\n+    def set_model(self, model):\n+        \"\"\"Sets Keras model and writes graph if specified.\"\"\"\n+        model.run_eagerly = False\n+        super(TensorBoard, self).set_model(model)\n\n@@ -23,9 +23,10 @@ from keras.utils.generic_utils import to_list\n from keras.utils.generic_utils import unpack_singleton\n from keras import backend as K\n from keras.utils import np_utils\n+\n try:\n     from unittest.mock import patch\n-except:\n+except ImportError:\n     from mock import patch\n \n \n@@ -70,8 +71,8 @@ class Counter(callbacks.Callback):\n     \"\"\"Counts the number of times each callback method was run.\n \n     # Arguments\n-        method_counts: dict, contains the counts of time each callback method was\n-            run.\n+        method_counts: dict, contains the counts of time\n+            each callback method was run.\n     \"\"\"\n \n     def __init__(self):\n@@ -81,12 +82,14 @@ class Counter(callbacks.Callback):\n             'on_train_batch_begin', 'on_train_batch_end',\n             'on_test_batch_begin', 'on_test_batch_end',\n             'on_predict_batch_begin', 'on_predict_batch_end',\n-            'on_train_begin', 'on_train_end', 'on_predict_begin', 'on_predict_end',\n+            'on_train_begin', 'on_train_end',\n+            'on_predict_begin', 'on_predict_end',\n             'on_test_begin', 'on_test_end',\n         ]\n         for method_name in methods_to_count:\n             setattr(self, method_name,\n-                    self.wrap_with_counts(method_name, getattr(self, method_name)))\n+                    self.wrap_with_counts(\n+                        method_name, getattr(self, method_name)))\n \n     def wrap_with_counts(self, method_name, method):\n \n@@ -100,7 +103,7 @@ class Counter(callbacks.Callback):\n class TestCallbackCounts(object):\n \n     def _check_counts(self, counter, expected_counts):\n-        \"\"\"Checks that the counts registered by `counter` are those expected.\"\"\"\n+        \"\"\"Checks that counts registered by `counter` are those expected.\"\"\"\n         for method_name, expected_count in expected_counts.items():\n             count = counter.method_counts[method_name]\n             assert count == expected_count, \\\n@@ -215,9 +218,12 @@ class TestCallbackCounts(object):\n \n         model = self._get_model()\n         counter = Counter()\n-        model.fit_generator(train_generator, steps_per_epoch=len(X_train) // 2,\n-                            epochs=5, validation_data=validation_generator,\n-                            validation_steps=len(X_test) // 2, callbacks=[counter])\n+        model.fit_generator(train_generator,\n+                            steps_per_epoch=len(X_train) // 2,\n+                            epochs=5,\n+                            validation_data=validation_generator,\n+                            validation_steps=len(X_test) // 2,\n+                            callbacks=[counter])\n \n         self._check_counts(\n             counter, {\n@@ -357,8 +363,11 @@ def test_TerminateOnNaN():\n                   optimizer='rmsprop')\n \n     # case 1 fit\n-    history = model.fit(X_train, y_train, batch_size=batch_size,\n-                        validation_data=(X_test, y_test), callbacks=cbks, epochs=20)\n+    history = model.fit(X_train, y_train,\n+                        batch_size=batch_size,\n+                        validation_data=(X_test, y_test),\n+                        callbacks=cbks,\n+                        epochs=20)\n     loss = history.history['loss']\n     assert len(loss) == 1\n     assert loss[0] == np.inf\n@@ -458,8 +467,10 @@ def test_ModelCheckpoint(tmpdir):\n     # case 3\n     mode = 'max'\n     monitor = 'val_acc'\n-    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n-                                      save_best_only=save_best_only, mode=mode)]\n+    cbks = [callbacks.ModelCheckpoint(filepath,\n+                                      monitor=monitor,\n+                                      save_best_only=save_best_only,\n+                                      mode=mode)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n               validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n     assert os.path.isfile(filepath)\n@@ -467,8 +478,10 @@ def test_ModelCheckpoint(tmpdir):\n \n     # case 4\n     save_best_only = True\n-    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n-                                      save_best_only=save_best_only, mode=mode)]\n+    cbks = [callbacks.ModelCheckpoint(filepath,\n+                                      monitor=monitor,\n+                                      save_best_only=save_best_only,\n+                                      mode=mode)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n               validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n     assert os.path.isfile(filepath)\n@@ -479,8 +492,10 @@ def test_ModelCheckpoint(tmpdir):\n     period = 2\n     mode = 'auto'\n     filepath = 'checkpoint.{epoch:02d}.h5'\n-    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n-                                      save_best_only=save_best_only, mode=mode,\n+    cbks = [callbacks.ModelCheckpoint(filepath,\n+                                      monitor=monitor,\n+                                      save_best_only=save_best_only,\n+                                      mode=mode,\n                                       period=period)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n               validation_data=(X_test, y_test), callbacks=cbks, epochs=4)\n@@ -507,16 +522,25 @@ def test_EarlyStopping():\n     mode = 'max'\n     monitor = 'val_acc'\n     patience = 0\n-    cbks = [callbacks.EarlyStopping(patience=patience, monitor=monitor, mode=mode)]\n-    history = model.fit(X_train, y_train, batch_size=batch_size,\n-                        validation_data=(X_test, y_test), callbacks=cbks, epochs=20)\n-\n+    cbks = [callbacks.EarlyStopping(patience=patience,\n+                                    monitor=monitor,\n+                                    mode=mode)]\n+    history = model.fit(X_train, y_train,\n+                        batch_size=batch_size,\n+                        validation_data=(X_test, y_test),\n+                        callbacks=cbks,\n+                        epochs=20)\n     mode = 'auto'\n     monitor = 'val_acc'\n     patience = 2\n-    cbks = [callbacks.EarlyStopping(patience=patience, monitor=monitor, mode=mode)]\n-    history = model.fit(X_train, y_train, batch_size=batch_size,\n-                        validation_data=(X_test, y_test), callbacks=cbks, epochs=20)\n+    cbks = [callbacks.EarlyStopping(patience=patience,\n+                                    monitor=monitor,\n+                                    mode=mode)]\n+    history = model.fit(X_train, y_train,\n+                        batch_size=batch_size,\n+                        validation_data=(X_test, y_test),\n+                        callbacks=cbks,\n+                        epochs=20)\n \n \n def test_EarlyStopping_reuse():\n@@ -528,7 +552,9 @@ def test_EarlyStopping_reuse():\n         Dense(1, input_dim=1, activation='relu'),\n         Dense(1, activation='sigmoid'),\n     ))\n-    model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n+    model.compile(optimizer='sgd',\n+                  loss='binary_crossentropy',\n+                  metrics=['accuracy'])\n     stopper = callbacks.EarlyStopping(monitor='acc', patience=patience)\n     weights = model.get_weights()\n \n@@ -721,18 +747,31 @@ def test_ReduceLROnPlateau():\n     model = make_model()\n \n     # This should reduce the LR after the first epoch (due to high epsilon).\n-    cbks = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n-                                        min_delta=10, patience=1, cooldown=5)]\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, epochs=5, verbose=2)\n-    assert_allclose(float(K.get_value(model.optimizer.lr)), 0.01, atol=K.epsilon())\n+    cbks = [callbacks.ReduceLROnPlateau(monitor='val_loss',\n+                                        factor=0.1,\n+                                        min_delta=10,\n+                                        patience=1,\n+                                        cooldown=5)]\n+    model.fit(X_train, y_train,\n+              batch_size=batch_size,\n+              validation_data=(X_test, y_test),\n+              callbacks=cbks,\n+              epochs=5,\n+              verbose=2)\n+    assert_allclose(\n+        float(K.get_value(model.optimizer.lr)), 0.01, atol=K.epsilon())\n \n     model = make_model()\n     cbks = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                                         min_delta=0, patience=1, cooldown=5)]\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, epochs=5, verbose=2)\n-    assert_allclose(float(K.get_value(model.optimizer.lr)), 0.1, atol=K.epsilon())\n+    model.fit(X_train, y_train,\n+              batch_size=batch_size,\n+              validation_data=(X_test, y_test),\n+              callbacks=cbks,\n+              epochs=5,\n+              verbose=2)\n+    assert_allclose(\n+        float(K.get_value(model.optimizer.lr)), 0.1, atol=K.epsilon())\n \n \n def test_ReduceLROnPlateau_patience():\n@@ -826,288 +865,6 @@ def test_CSVLogger(tmpdir):\n     assert not tmpdir.listdir()\n \n \n-@pytest.mark.parametrize('update_freq', ['batch', 'epoch', 9])\n-def test_TensorBoard(tmpdir, update_freq):\n-    np.random.seed(np.random.randint(1, 1e7))\n-    filepath = str(tmpdir / 'logs')\n-\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-\n-    class DummyStatefulMetric(Layer):\n-\n-        def __init__(self, name='dummy_stateful_metric', **kwargs):\n-            super(DummyStatefulMetric, self).__init__(name=name, **kwargs)\n-            self.stateful = True\n-            self.state = K.variable(value=0, dtype='int32')\n-\n-        def reset_states(self):\n-            pass\n-\n-        def __call__(self, y_true, y_pred):\n-            return self.state\n-\n-    inp = Input((input_dim,))\n-    hidden = Dense(num_hidden, activation='relu')(inp)\n-    hidden = Dropout(0.1)(hidden)\n-    hidden = BatchNormalization()(hidden)\n-    output = Dense(num_classes, activation='softmax')(hidden)\n-    model = Model(inputs=inp, outputs=output)\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='sgd',\n-                  metrics=['accuracy', DummyStatefulMetric()])\n-\n-    # we must generate new callbacks for each test, as they aren't stateless\n-    def callbacks_factory(histogram_freq, embeddings_freq=1, write_images=True,\n-                          write_grads=True):\n-        return [callbacks.TensorBoard(log_dir=filepath,\n-                                      histogram_freq=histogram_freq,\n-                                      write_images=write_images,\n-                                      write_grads=write_grads,\n-                                      embeddings_freq=embeddings_freq,\n-                                      embeddings_layer_names=['dense_1'],\n-                                      embeddings_data=X_test,\n-                                      batch_size=5,\n-                                      update_freq=update_freq)]\n-\n-    # fit without validation data\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              callbacks=callbacks_factory(histogram_freq=0, embeddings_freq=0),\n-              epochs=2)\n-\n-    # fit with validation data and accuracy\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test),\n-              callbacks=callbacks_factory(histogram_freq=0, write_images=False,\n-                                          write_grads=False),\n-              epochs=2)\n-\n-    # fit generator without validation data\n-    train_generator = data_generator(X_train, y_train, batch_size)\n-    model.fit_generator(train_generator, len(X_train), epochs=2,\n-                        callbacks=callbacks_factory(histogram_freq=0,\n-                                                    write_images=False,\n-                                                    write_grads=False,\n-                                                    embeddings_freq=0))\n-\n-    # fit generator with validation data and accuracy\n-    train_generator = data_generator(X_train, y_train, batch_size)\n-    model.fit_generator(train_generator, len(X_train), epochs=2,\n-                        validation_data=(X_test, y_test),\n-                        callbacks=callbacks_factory(histogram_freq=1,\n-                                                    write_images=False,\n-                                                    write_grads=False))\n-\n-    assert os.path.isdir(filepath)\n-    shutil.rmtree(filepath)\n-    assert not tmpdir.listdir()\n-\n-\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-def test_TensorBoard_histogram_freq_must_have_validation_data(tmpdir):\n-    np.random.seed(np.random.randint(1, 1e7))\n-    filepath = str(tmpdir / 'logs')\n-\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-\n-    inp = Input((input_dim,))\n-    hidden = Dense(num_hidden, activation='relu')(inp)\n-    hidden = Dropout(0.1)(hidden)\n-    output = Dense(num_classes, activation='softmax')(hidden)\n-    model = Model(inputs=inp, outputs=output)\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='sgd',\n-                  metrics=['accuracy'])\n-\n-    # we must generate new callbacks for each test, as they aren't stateless\n-    def callbacks_factory(histogram_freq, embeddings_freq=1, write_images=True,\n-                          write_grads=True):\n-        return [callbacks.TensorBoard(log_dir=filepath,\n-                                      histogram_freq=histogram_freq,\n-                                      write_images=write_images,\n-                                      write_grads=write_grads,\n-                                      embeddings_freq=embeddings_freq,\n-                                      embeddings_layer_names=['dense_1'],\n-                                      embeddings_data=X_test,\n-                                      batch_size=5)]\n-\n-    # fit without validation data should raise ValueError if histogram_freq > 0\n-    with pytest.raises(ValueError) as raised_exception:\n-        model.fit(X_train, y_train, batch_size=batch_size,\n-                  callbacks=callbacks_factory(histogram_freq=1), epochs=3)\n-    assert 'validation_data must be provided' in str(raised_exception.value)\n-\n-    train_generator = data_generator(X_train, y_train, batch_size)\n-    validation_generator = data_generator(X_test, y_test, batch_size)\n-\n-    # fit generator without validation data should raise ValueError if\n-    # histogram_freq > 0\n-    with pytest.raises(ValueError) as raised_exception:\n-        model.fit_generator(train_generator,\n-                            len(X_train), epochs=2,\n-                            callbacks=callbacks_factory(histogram_freq=1,\n-                                                        write_images=False,\n-                                                        write_grads=False))\n-    assert 'validation_data must be provided' in str(raised_exception.value)\n-\n-    # fit generator with validation data generator should raise ValueError if\n-    # histogram_freq > 0\n-    with pytest.raises(ValueError) as raised_exception:\n-        model.fit_generator(train_generator, len(X_train), epochs=2,\n-                            validation_data=validation_generator,\n-                            validation_steps=1,\n-                            callbacks=callbacks_factory(histogram_freq=1,\n-                                                        write_images=False,\n-                                                        write_grads=False))\n-    assert 'validation_data must be provided' in str(raised_exception.value)\n-\n-\n-def test_TensorBoard_multi_input_output(tmpdir):\n-    np.random.seed(np.random.randint(1, 1e7))\n-    filepath = str(tmpdir / 'logs')\n-\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks(\n-        input_shape=(input_dim, input_dim))\n-\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-\n-    inp1 = Input((input_dim, input_dim))\n-    inp2 = Input((input_dim, input_dim))\n-    inp_3d = add([inp1, inp2])\n-    inp_2d = GlobalAveragePooling1D()(inp_3d)\n-    # test a layer with a list of output tensors\n-    inp_pair = Lambda(lambda x: x)([inp_3d, inp_2d])\n-    hidden = dot(inp_pair, axes=-1)\n-    hidden = Dense(num_hidden, activation='relu')(hidden)\n-    hidden = Dropout(0.1)(hidden)\n-    output1 = Dense(num_classes, activation='softmax')(hidden)\n-    output2 = Dense(num_classes, activation='softmax')(hidden)\n-    model = Model(inputs=[inp1, inp2], outputs=[output1, output2])\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='sgd',\n-                  metrics=['accuracy'])\n-\n-    # we must generate new callbacks for each test, as they aren't stateless\n-    def callbacks_factory(histogram_freq, embeddings_freq=1, write_images=True,\n-                          write_grads=True):\n-        return [callbacks.TensorBoard(log_dir=filepath,\n-                                      histogram_freq=histogram_freq,\n-                                      write_images=write_images,\n-                                      write_grads=write_grads,\n-                                      embeddings_freq=embeddings_freq,\n-                                      embeddings_layer_names=['dense_1'],\n-                                      embeddings_data=[X_test] * 2,\n-                                      batch_size=5)]\n-\n-    # fit without validation data\n-    model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n-              callbacks=callbacks_factory(histogram_freq=0, embeddings_freq=0),\n-              epochs=3)\n-\n-    # fit with validation data and accuracy\n-    model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n-              validation_data=([X_test] * 2, [y_test] * 2),\n-              callbacks=callbacks_factory(histogram_freq=1, write_images=False,\n-                                          write_grads=False),\n-              epochs=2)\n-\n-    train_generator = data_generator([X_train] * 2, [y_train] * 2, batch_size)\n-\n-    # fit generator without validation data\n-    model.fit_generator(train_generator, len(X_train), epochs=2,\n-                        callbacks=callbacks_factory(histogram_freq=0,\n-                                                    embeddings_freq=0,\n-                                                    write_images=False,\n-                                                    write_grads=False))\n-\n-    # fit generator with validation data and accuracy\n-    model.fit_generator(train_generator, len(X_train), epochs=2,\n-                        validation_data=([X_test] * 2, [y_test] * 2),\n-                        callbacks=callbacks_factory(histogram_freq=1,\n-                                                    write_images=False,\n-                                                    write_grads=False))\n-\n-    assert os.path.isdir(filepath)\n-    shutil.rmtree(filepath)\n-    assert not tmpdir.listdir()\n-\n-\n-def test_TensorBoard_convnet(tmpdir):\n-    np.random.seed(np.random.randint(1, 1e7))\n-    filepath = str(tmpdir / 'logs')\n-\n-    input_shape = (16, 16, 3)\n-    (x_train, y_train), (x_test, y_test) = get_data_callbacks(\n-        num_train=500,\n-        num_test=200,\n-        input_shape=input_shape)\n-    y_train = np_utils.to_categorical(y_train)\n-    y_test = np_utils.to_categorical(y_test)\n-\n-    model = Sequential([\n-        Conv2D(filters=8, kernel_size=3,\n-               activation='relu',\n-               input_shape=input_shape),\n-        MaxPooling2D(pool_size=2),\n-        Conv2D(filters=4, kernel_size=(3, 3),\n-               activation='relu', padding='same'),\n-        BatchNormalization(),\n-        GlobalAveragePooling2D(),\n-        Dense(num_classes, activation='softmax')\n-    ])\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='rmsprop',\n-                  metrics=['accuracy'])\n-    tsb = callbacks.TensorBoard(log_dir=filepath, histogram_freq=1,\n-                                write_images=True, write_grads=True,\n-                                batch_size=16)\n-    cbks = [tsb]\n-    model.summary()\n-    history = model.fit(x_train, y_train, epochs=2, batch_size=16,\n-                        validation_data=(x_test, y_test),\n-                        callbacks=cbks,\n-                        verbose=0)\n-    assert os.path.isdir(filepath)\n-    shutil.rmtree(filepath)\n-    assert not tmpdir.listdir()\n-\n-\n-def test_TensorBoard_display_float_from_logs(tmpdir):\n-    filepath = str(tmpdir / 'logs')\n-\n-    input_shape = (3,)\n-    (x_train, y_train), _ = get_data_callbacks(num_train=10,\n-                                               num_test=0,\n-                                               input_shape=input_shape)\n-    y_train = np_utils.to_categorical(y_train)\n-\n-    model = Sequential([\n-        Dense(num_classes, activation='softmax')\n-    ])\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='rmsprop')\n-\n-    class CustomCallback(callbacks.Callback):\n-\n-        def on_epoch_end(self, epoch, logs=None):\n-            logs['test'] = 0.\n-\n-    tsb = callbacks.TensorBoard(log_dir=filepath,\n-                                batch_size=16)\n-    cbks = [CustomCallback(), tsb]\n-    model.fit(x_train, y_train, epochs=2, batch_size=16,\n-              callbacks=cbks,\n-              verbose=0)\n-    assert os.path.isdir(filepath)\n-    shutil.rmtree(filepath)\n-    assert not tmpdir.listdir()\n-\n-\n def test_CallbackValData():\n     np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n\n@@ -0,0 +1,334 @@\n+import os\n+import numpy as np\n+import pytest\n+import shutil\n+\n+from keras import callbacks\n+from keras.models import Sequential, Model\n+from keras import layers\n+from keras import backend as K\n+from keras.utils import np_utils\n+from keras.utils.test_utils import get_test_data\n+from keras.utils.generic_utils import to_list\n+from keras.utils.generic_utils import unpack_singleton\n+\n+\n+input_dim = 2\n+num_hidden = 4\n+num_classes = 2\n+batch_size = 5\n+train_samples = 20\n+test_samples = 20\n+\n+\n+def data_generator(x, y, batch_size):\n+    x = to_list(x)\n+    y = to_list(y)\n+    max_batch_index = len(x[0]) // batch_size\n+    i = 0\n+    while 1:\n+        x_batch = [array[i * batch_size: (i + 1) * batch_size] for array in x]\n+        x_batch = unpack_singleton(x_batch)\n+\n+        y_batch = [array[i * batch_size: (i + 1) * batch_size] for array in y]\n+        y_batch = unpack_singleton(y_batch)\n+        yield x_batch, y_batch\n+        i += 1\n+        i = i % max_batch_index\n+\n+\n+# Changing the default arguments of get_test_data.\n+def get_data_callbacks(num_train=train_samples,\n+                       num_test=test_samples,\n+                       input_shape=(input_dim,),\n+                       classification=True,\n+                       num_classes=num_classes):\n+    return get_test_data(num_train=num_train,\n+                         num_test=num_test,\n+                         input_shape=input_shape,\n+                         classification=classification,\n+                         num_classes=num_classes)\n+\n+\n+@pytest.mark.parametrize('update_freq', ['batch', 'epoch', 9])\n+def test_TensorBoard(tmpdir, update_freq):\n+    np.random.seed(np.random.randint(1, 1e7))\n+    filepath = str(tmpdir / 'logs')\n+\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n+    y_test = np_utils.to_categorical(y_test)\n+    y_train = np_utils.to_categorical(y_train)\n+\n+    class DummyStatefulMetric(layers.Layer):\n+\n+        def __init__(self, name='dummy_stateful_metric', **kwargs):\n+            super(DummyStatefulMetric, self).__init__(name=name, **kwargs)\n+            self.stateful = True\n+            self.state = K.variable(value=0, dtype='int32')\n+\n+        def reset_states(self):\n+            pass\n+\n+        def __call__(self, y_true, y_pred):\n+            return self.state\n+\n+    inp = layers.Input((input_dim,))\n+    hidden = layers.Dense(num_hidden, activation='relu')(inp)\n+    hidden = layers.Dropout(0.1)(hidden)\n+    hidden = layers.BatchNormalization()(hidden)\n+    output = layers.Dense(num_classes, activation='softmax')(hidden)\n+    model = Model(inputs=inp, outputs=output)\n+    model.compile(loss='categorical_crossentropy',\n+                  optimizer='sgd',\n+                  metrics=['accuracy', DummyStatefulMetric()])\n+\n+    # we must generate new callbacks for each test, as they aren't stateless\n+    def callbacks_factory(histogram_freq=0,\n+                          embeddings_freq=0,\n+                          write_images=False,\n+                          write_grads=False):\n+        if embeddings_freq:\n+            embeddings_layer_names = ['dense_1']\n+            embeddings_data = X_test\n+        else:\n+            embeddings_layer_names = None\n+            embeddings_data = None\n+        return [callbacks.TensorBoard(log_dir=filepath,\n+                                      histogram_freq=histogram_freq,\n+                                      write_images=write_images,\n+                                      write_grads=write_grads,\n+                                      embeddings_freq=embeddings_freq,\n+                                      embeddings_layer_names=embeddings_layer_names,\n+                                      embeddings_data=embeddings_data,\n+                                      update_freq=update_freq)]\n+\n+    # fit without validation data\n+    model.fit(X_train, y_train, batch_size=batch_size,\n+              callbacks=callbacks_factory(),\n+              epochs=2)\n+\n+    # fit with validation data and accuracy\n+    model.fit(X_train, y_train, batch_size=batch_size,\n+              validation_data=(X_test, y_test),\n+              callbacks=callbacks_factory(),\n+              epochs=2)\n+\n+    # fit generator without validation data\n+    train_generator = data_generator(X_train, y_train, batch_size)\n+    model.fit_generator(train_generator, len(X_train), epochs=2,\n+                        callbacks=callbacks_factory())\n+\n+    # fit generator with validation data and accuracy\n+    train_generator = data_generator(X_train, y_train, batch_size)\n+    model.fit_generator(train_generator, len(X_train), epochs=2,\n+                        validation_data=(X_test, y_test),\n+                        callbacks=callbacks_factory(histogram_freq=1))\n+\n+    assert os.path.isdir(filepath)\n+    shutil.rmtree(filepath)\n+    assert not tmpdir.listdir()\n+\n+\n+@pytest.mark.skipif((K.backend() != 'tensorflow'),\n+                    reason='Requires TensorFlow backend')\n+def test_TensorBoard_histogram_freq_must_have_validation_data(tmpdir):\n+    np.random.seed(np.random.randint(1, 1e7))\n+    filepath = str(tmpdir / 'logs')\n+\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n+    y_test = np_utils.to_categorical(y_test)\n+    y_train = np_utils.to_categorical(y_train)\n+\n+    inp = layers.Input((input_dim,))\n+    hidden = layers.Dense(num_hidden, activation='relu')(inp)\n+    hidden = layers.Dropout(0.1)(hidden)\n+    output = layers.Dense(num_classes, activation='softmax')(hidden)\n+    model = Model(inputs=inp, outputs=output)\n+    model.compile(loss='categorical_crossentropy',\n+                  optimizer='sgd',\n+                  metrics=['accuracy'])\n+\n+    # we must generate new callbacks for each test, as they aren't stateless\n+    def callbacks_factory(histogram_freq=0,\n+                          embeddings_freq=0,\n+                          write_images=False,\n+                          write_grads=False):\n+        if embeddings_freq:\n+            embeddings_layer_names = ['dense_1']\n+            embeddings_data = X_test\n+        else:\n+            embeddings_layer_names = None\n+            embeddings_data = None\n+        return [callbacks.TensorBoard(log_dir=filepath,\n+                                      histogram_freq=histogram_freq,\n+                                      write_images=write_images,\n+                                      write_grads=write_grads,\n+                                      embeddings_freq=embeddings_freq,\n+                                      embeddings_layer_names=embeddings_layer_names,\n+                                      embeddings_data=embeddings_data)]\n+\n+    # fit without validation data should raise ValueError if histogram_freq > 0\n+    with pytest.raises(ValueError) as raised_exception:\n+        model.fit(X_train, y_train, batch_size=batch_size,\n+                  callbacks=callbacks_factory(histogram_freq=1), epochs=3)\n+    assert 'validation_data must be provided' in str(raised_exception.value)\n+\n+    train_generator = data_generator(X_train, y_train, batch_size)\n+    validation_generator = data_generator(X_test, y_test, batch_size)\n+\n+    # fit generator without validation data should raise ValueError if\n+    # histogram_freq > 0\n+    with pytest.raises(ValueError) as raised_exception:\n+        model.fit_generator(train_generator,\n+                            len(X_train), epochs=2,\n+                            callbacks=callbacks_factory(histogram_freq=1))\n+    assert 'validation_data must be provided' in str(raised_exception.value)\n+\n+    # fit generator with validation data generator should raise ValueError if\n+    # histogram_freq > 0\n+    with pytest.raises(ValueError) as raised_exception:\n+        model.fit_generator(train_generator, len(X_train), epochs=2,\n+                            validation_data=validation_generator,\n+                            validation_steps=1,\n+                            callbacks=callbacks_factory(histogram_freq=1))\n+    assert 'validation_data must be provided' in str(raised_exception.value)\n+\n+\n+def test_TensorBoard_multi_input_output(tmpdir):\n+    np.random.seed(np.random.randint(1, 1e7))\n+    filepath = str(tmpdir / 'logs')\n+\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks(\n+        input_shape=(input_dim, input_dim))\n+\n+    y_test = np_utils.to_categorical(y_test)\n+    y_train = np_utils.to_categorical(y_train)\n+\n+    inp1 = layers.Input((input_dim, input_dim))\n+    inp2 = layers.Input((input_dim, input_dim))\n+    inp_3d = layers.add([inp1, inp2])\n+    inp_2d = layers.GlobalAveragePooling1D()(inp_3d)\n+    # test a layer with a list of output tensors\n+    inp_pair = layers.Lambda(lambda x: x)([inp_3d, inp_2d])\n+    hidden = layers.dot(inp_pair, axes=-1)\n+    hidden = layers.Dense(num_hidden, activation='relu')(hidden)\n+    hidden = layers.Dropout(0.1)(hidden)\n+    output1 = layers.Dense(num_classes, activation='softmax')(hidden)\n+    output2 = layers.Dense(num_classes, activation='softmax')(hidden)\n+    model = Model(inputs=[inp1, inp2], outputs=[output1, output2])\n+    model.compile(loss='categorical_crossentropy',\n+                  optimizer='sgd',\n+                  metrics=['accuracy'])\n+\n+    # we must generate new callbacks for each test, as they aren't stateless\n+    def callbacks_factory(histogram_freq=0,\n+                          embeddings_freq=0,\n+                          write_images=False,\n+                          write_grads=False):\n+        if embeddings_freq:\n+            embeddings_layer_names = ['dense_1']\n+            embeddings_data = [X_test] * 2\n+        else:\n+            embeddings_layer_names = None\n+            embeddings_data = None\n+        return [callbacks.TensorBoard(log_dir=filepath,\n+                                      histogram_freq=histogram_freq,\n+                                      write_images=write_images,\n+                                      write_grads=write_grads,\n+                                      embeddings_freq=embeddings_freq,\n+                                      embeddings_layer_names=embeddings_layer_names,\n+                                      embeddings_data=embeddings_data)]\n+\n+    # fit without validation data\n+    model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n+              callbacks=callbacks_factory(),\n+              epochs=3)\n+\n+    # fit with validation data and accuracy\n+    model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n+              validation_data=([X_test] * 2, [y_test] * 2),\n+              callbacks=callbacks_factory(histogram_freq=1),\n+              epochs=2)\n+\n+    train_generator = data_generator([X_train] * 2, [y_train] * 2, batch_size)\n+\n+    # fit generator without validation data\n+    model.fit_generator(train_generator, len(X_train), epochs=2,\n+                        callbacks=callbacks_factory())\n+\n+    # fit generator with validation data and accuracy\n+    model.fit_generator(train_generator, len(X_train), epochs=2,\n+                        validation_data=([X_test] * 2, [y_test] * 2),\n+                        callbacks=callbacks_factory())\n+\n+    assert os.path.isdir(filepath)\n+    shutil.rmtree(filepath)\n+    assert not tmpdir.listdir()\n+\n+\n+def test_TensorBoard_convnet(tmpdir):\n+    np.random.seed(np.random.randint(1, 1e7))\n+    filepath = str(tmpdir / 'logs')\n+\n+    input_shape = (16, 16, 3)\n+    (x_train, y_train), (x_test, y_test) = get_data_callbacks(\n+        num_train=500,\n+        num_test=200,\n+        input_shape=input_shape)\n+    y_train = np_utils.to_categorical(y_train)\n+    y_test = np_utils.to_categorical(y_test)\n+\n+    model = Sequential([\n+        layers.Conv2D(filters=8, kernel_size=3,\n+                      activation='relu',\n+                      input_shape=input_shape),\n+        layers.MaxPooling2D(pool_size=2),\n+        layers.Conv2D(filters=4, kernel_size=(3, 3),\n+                      activation='relu', padding='same'),\n+        layers.BatchNormalization(),\n+        layers.GlobalAveragePooling2D(),\n+        layers.Dense(num_classes, activation='softmax')\n+    ])\n+    model.compile(loss='categorical_crossentropy',\n+                  optimizer='rmsprop',\n+                  metrics=['accuracy'])\n+    tsb = callbacks.TensorBoard(histogram_freq=1)\n+    cbks = [tsb]\n+    model.summary()\n+    history = model.fit(x_train, y_train, epochs=2, batch_size=16,\n+                        validation_data=(x_test, y_test),\n+                        callbacks=cbks,\n+                        verbose=0)\n+    assert os.path.isdir(filepath)\n+    shutil.rmtree(filepath)\n+    assert not tmpdir.listdir()\n+\n+\n+def test_TensorBoard_display_float_from_logs(tmpdir):\n+    filepath = str(tmpdir / 'logs')\n+\n+    input_shape = (3,)\n+    (x_train, y_train), _ = get_data_callbacks(num_train=10,\n+                                               num_test=0,\n+                                               input_shape=input_shape)\n+    y_train = np_utils.to_categorical(y_train)\n+\n+    model = Sequential([\n+        layers.Dense(num_classes, activation='softmax')\n+    ])\n+    model.compile(loss='categorical_crossentropy',\n+                  optimizer='rmsprop')\n+\n+    class CustomCallback(callbacks.Callback):\n+\n+        def on_epoch_end(self, epoch, logs=None):\n+            logs['test'] = 0.\n+\n+    tsb = callbacks.TensorBoard(log_dir=filepath)\n+    cbks = [CustomCallback(), tsb]\n+    model.fit(x_train, y_train, epochs=2, batch_size=16,\n+              callbacks=cbks,\n+              verbose=0)\n+    assert os.path.isdir(filepath)\n+    shutil.rmtree(filepath)\n+    assert not tmpdir.listdir()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
