{"custom_id": "keras#fcb6ae8eed5058d7759d2db8bdfbf59e1033b1d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 477 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -46,7 +46,7 @@ class ActivityRegularizer(Regularizer):\n \n     def __call__(self, loss):\n         output = self.layer.output\n-        regularized_loss = self.l1 * K.sum(K.mean(K.abs(output), axis=0))\n+        regularized_loss = loss + self.l1 * K.sum(K.mean(K.abs(output), axis=0))\n         regularized_loss += self.l2 * K.sum(K.mean(K.square(output), axis=0))\n         return K.in_train_phase(regularized_loss, loss)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e754581ecb8ad61182c06b70b6b4c329c76bd1b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 46 | Files Changed: 2 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 66 | Churn Cumulative: 265 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -26,20 +26,17 @@ class GaussianNoise(Layer):\n     def __init__(self, sigma, **kwargs):\n         self.supports_masking = True\n         self.sigma = sigma\n+        self.uses_learning_phase = True\n         super(GaussianNoise, self).__init__(**kwargs)\n \n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        if not train or self.sigma == 0:\n-            return X\n-        else:\n-            return X + K.random_normal(shape=K.shape(X),\n+    def call(self, x, mask=None):\n+        noise_x = x + K.random_normal(shape=K.shape(x),\n                                       mean=0.,\n                                       std=self.sigma)\n+        return K.in_train_phase(noise_x, x)\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"sigma\": self.sigma}\n+        config = {'sigma': self.sigma}\n         base_config = super(GaussianNoise, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -59,19 +56,18 @@ class GaussianDropout(Layer):\n     def __init__(self, p, **kwargs):\n         self.supports_masking = True\n         self.p = p\n+        if 0 < p < 1:\n+            self.uses_learning_phase = True\n         super(GaussianDropout, self).__init__(**kwargs)\n \n-    def get_output(self, train):\n-        X = self.get_input(train)\n-        if train:\n-            # self.p refers to drop probability rather than\n-            # retain probability (as in paper), for consistency\n-            X *= K.random_normal(shape=K.shape(X), mean=1.0,\n+    def call(self, x, mask=None):\n+        if 0 < self.p < 1:\n+            noise_x = x * K.random_normal(shape=K.shape(x), mean=1.0,\n                                           std=K.sqrt(self.p / (1.0 - self.p)))\n-        return X\n+            return K.in_train_phase(noise_x, x)\n+        return x\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"p\": self.p}\n+        config = {'p': self.p}\n         base_config = super(GaussianDropout, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n\n@@ -1,40 +1,18 @@\n import pytest\n-import numpy as np\n-from keras import backend as K\n-from keras.layers import core\n+from keras.utils.test_utils import layer_test\n from keras.layers import noise\n \n-input_shape = (10, 10)\n-batch_input_shape = (10, 10, 10)\n-\n \n def test_GaussianNoise():\n-    layer = noise.GaussianNoise(sigma=1., input_shape=input_shape)\n-    _runner(layer)\n+    layer_test(noise.GaussianNoise,\n+               kwargs={'sigma': 1.},\n+               input_shape=(3, 2, 3))\n \n \n def test_GaussianDropout():\n-    layer = noise.GaussianDropout(p=0.2, input_shape=input_shape)\n-    _runner(layer)\n-\n-\n-def _runner(layer):\n-    assert isinstance(layer, core.Layer)\n-    layer.build()\n-    conf = layer.get_config()\n-    assert (type(conf) == dict)\n-\n-    param = layer.get_params()\n-    # Typically a list or a tuple, but may be any iterable\n-    assert hasattr(param, '__iter__')\n-    layer.input = K.variable(np.random.random(batch_input_shape))\n-    output = layer.get_output(train=False)\n-    output_np = K.eval(output)\n-    assert output_np.shape == batch_input_shape\n-\n-    output = layer.get_output(train=True)\n-    output_np = K.eval(output)\n-    assert output_np.shape == batch_input_shape\n+    layer_test(noise.GaussianDropout,\n+               kwargs={'p': 0.5},\n+               input_shape=(3, 2, 3))\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b0ea92bc127d724f514d25d64d079b2d31e980ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 19 | Churn Cumulative: 3281 | Contributors (this commit): 22 | Commits (past 90d): 92 | Contributors (cumulative): 40 | DMM Complexity: 1.0\n\nDIFF:\n@@ -107,6 +107,16 @@ def dot(x, y):\n     return tf.matmul(x, y)\n \n \n+def batch_dot(x, y, axes=None):\n+    if axes:\n+        adj_x = None if axes[0][0] == x.ndim-1 else True\n+        adj_y = True if axes[1][0] == y.ndim-1 else None\n+    else:\n+        adj_x = None\n+        adj_y = None\n+    return tf.batch_matmul(x, y, adj_x=adj_x, adj_y=adj_y)\n+\n+\n def transpose(x):\n     return tf.transpose(x)\n \n\n@@ -126,6 +126,13 @@ def dot(x, y):\n     return T.dot(x, y)\n \n \n+def batch_dot(x, y, axes=None):\n+    if axes is None:\n+        # behaves like tf.batch_matmul as default\n+        axes = [(x.ndim-1,), (y.ndim-2,)]\n+    return T.batched_tensordot(x, y, axes=axes)\n+\n+\n def transpose(x):\n     return T.transpose(x)\n \n\n@@ -40,6 +40,8 @@ class TestBackend(object):\n \n     def test_linear_operations(self):\n         check_two_tensor_operation('dot', (4, 2), (2, 4))\n+        check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3),\n+                                   axes=((2,), (2,)))\n         check_single_tensor_operation('transpose', (4, 2))\n \n     def test_shape_operations(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#133699c2f33302e124d31811844a6f24decadcd6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 19 | Churn Cumulative: 2354 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1104,9 +1104,6 @@ class Merge(Layer):\n                                 'be merged using ' + mode + ' mode. ' +\n                                 'Layer shapes: %s' % input_shapes)\n         if mode in {'cos', 'dot'}:\n-            if K._BACKEND != 'theano':\n-                raise Exception('\"' + mode + '\" merge mode will only work with Theano.')\n-\n             if len(layers) > 2:\n                 raise Exception(mode + ' merge takes exactly 2 layers')\n             shape1 = input_shapes[0]\n@@ -1170,27 +1167,21 @@ class Merge(Layer):\n             return s\n \n         elif self.mode == 'dot':\n-            if K._BACKEND != 'theano':\n-                raise Exception('\"dot\" merge mode will only work with Theano.')\n-            from theano import tensor as T\n             l1 = inputs[0]\n             l2 = inputs[1]\n-            output = T.batched_tensordot(l1, l2, self.dot_axes)\n+            output = K.batch_dot(l1, l2, self.dot_axes)\n             # output_shape = list(self.get_output_shape_for(inputs[0]._keras_shape))\n             # output_shape[0] = l1.shape[0]\n             # output = output.reshape(tuple(output_shape))\n             return output\n \n         elif self.mode == 'cos':\n-            if K._BACKEND != 'theano':\n-                raise Exception('\"cos\" merge mode will only work with Theano.')\n-            from theano import tensor as T\n             l1 = inputs[0]\n             l2 = inputs[1]\n-            denominator = T.sqrt(T.batched_tensordot(l1, l1, self.dot_axes) *\n-                                 T.batched_tensordot(l2, l2, self.dot_axes))\n-            output = T.batched_tensordot(l1, l2, self.dot_axes) / denominator\n-            output = output.dimshuffle((0, 'x'))\n+            denominator = K.sqrt(K.batch_dot(l1, l1, self.dot_axes) *\n+                                 K.batch_dot(l2, l2, self.dot_axes))\n+            output = K.batch_dot(l1, l2, self.dot_axes) / denominator\n+            output = K.expand_dims(output, 1)\n             return output\n         else:\n             raise Exception('Unknown merge mode.')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8740791d5cef5e88a588bdda4b5d3ef6006808f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 22 | Files Changed: 4 | Hunks: 13 | Methods Changed: 8 | Complexity Δ (Sum/Max): 10/8 | Churn Δ: 62 | Churn Cumulative: 12055 | Contributors (this commit): 44 | Commits (past 90d): 52 | Contributors (cumulative): 50 | DMM Complexity: 0.0\n\nDIFF:\n@@ -931,12 +931,13 @@ class Model(Container):\n                                    self.internal_input_shapes,\n                                    check_batch_dim=False)\n         if self.stateful:\n-            if x[0].shape[0] % batch_size != 0:\n+            if x[0].shape[0] > batch_size and x[0].shape[0] % batch_size != 0:\n                 raise Exception('In a stateful network, '\n                                 'you should only pass inputs with '\n                                 'a number of samples that can be '\n                                 'divided by the batch size. Found: ' +\n-                                str(x[0].shape[0]) + ' samples')\n+                                str(x[0].shape[0]) + ' samples. '\n+                                'Batch size: ' + str(batch_size) + '.')\n \n         # prepare inputs, delegate logic to _predict_loop\n         if self.uses_learning_phase:\n\n@@ -500,7 +500,6 @@ class Dense(Layer):\n \n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n-        self.constraints = [self.W_constraint, self.b_constraint]\n \n         self.initial_weights = weights\n         self.input_spec = [InputSpec(ndim=2)]\n@@ -534,6 +533,12 @@ class Dense(Layer):\n             self.activity_regularizer.set_layer(self)\n             self.regularizers.append(self.activity_regularizer)\n \n+        self.constraints = {}\n+        if self.W_constraint:\n+            self.constraints[self.W] = self.W_constraint\n+        if self.b_constraint:\n+            self.constraints[self.b] = self.b_constraint\n+\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n@@ -658,7 +663,6 @@ class MaxoutDense(Layer):\n \n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n-        self.constraints = [self.W_constraint, self.b_constraint]\n \n         self.initial_weights = weights\n         self.input_spec = [InputSpec(ndim=2)]\n@@ -693,6 +697,12 @@ class MaxoutDense(Layer):\n             self.activity_regularizer.set_layer(self)\n             self.regularizers.append(self.activity_regularizer)\n \n+        self.constraints = {}\n+        if self.W_constraint:\n+            self.constraints[self.W] = self.W_constraint\n+        if self.b_constraint:\n+            self.constraints[self.b] = self.b_constraint\n+\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n@@ -776,7 +786,6 @@ class Highway(Layer):\n \n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n-        self.constraints = [self.W_constraint, self.b_constraint]\n \n         self.initial_weights = weights\n         self.input_spec = [InputSpec(ndim=2)]\n@@ -816,6 +825,12 @@ class Highway(Layer):\n             self.activity_regularizer.set_layer(self)\n             self.regularizers.append(self.activity_regularizer)\n \n+        self.constraints = {}\n+        if self.W_constraint:\n+            self.constraints[self.W] = self.W_constraint\n+        if self.b_constraint:\n+            self.constraints[self.b] = self.b_constraint\n+\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n@@ -895,7 +910,6 @@ class TimeDistributedDense(Layer):\n \n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n-        self.constraints = [self.W_constraint, self.b_constraint]\n \n         self.initial_weights = weights\n         self.input_spec = [InputSpec(ndim=3)]\n@@ -932,6 +946,12 @@ class TimeDistributedDense(Layer):\n             self.activity_regularizer.set_layer(self)\n             self.regularizers.append(self.activity_regularizer)\n \n+        self.constraints = {}\n+        if self.W_constraint:\n+            self.constraints[self.W] = self.W_constraint\n+        if self.b_constraint:\n+            self.constraints[self.b] = self.b_constraint\n+\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n\n@@ -25,6 +25,15 @@ class WeightRegularizer(Regularizer):\n         self.p = p\n \n     def __call__(self, loss):\n+        if not hasattr(self, 'p'):\n+            raise Exception('Need to call `set_param` on '\n+                            'WeightRegularizer instance '\n+                            'before calling the instance. '\n+                            'Check that you are not passing '\n+                            'a WeightRegularizer instead of an '\n+                            'ActivityRegularizer '\n+                            '(i.e. activity_regularizer=\"l2\" instead '\n+                            'of activity_regularizer=\"activity_l2\".')\n         loss += K.sum(K.abs(self.p)) * self.l1\n         loss += K.sum(K.square(self.p)) * self.l2\n         return loss\n@@ -45,6 +54,10 @@ class ActivityRegularizer(Regularizer):\n         layer.uses_learning_phase = True\n \n     def __call__(self, loss):\n+        if not hasattr(self, 'layer'):\n+            raise Exception('Need to call `set_layer` on '\n+                            'ActivityRegularizer instance '\n+                            'before calling the instance.')\n         output = self.layer.output\n         regularized_loss = loss + self.l1 * K.sum(K.mean(K.abs(output), axis=0))\n         regularized_loss += self.l2 * K.sum(K.mean(K.square(output), axis=0))\n\n@@ -77,22 +77,6 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n     model = Model.from_config(model_config)\n     model.compile('rmsprop', 'mse')\n \n-    # test whether container recursion works\n-    x_outer = Input(shape=input_shape[1:], dtype=input_dtype)\n-    y_outer = model(x_outer)\n-    outer_model = Model(x_outer, y_outer)\n-    outer_model.compile('rmsprop', 'mse')\n-\n-    actual_output = outer_model.predict(input_data)\n-    actual_output_shape = actual_output.shape\n-    assert expected_output_shape == actual_output_shape\n-    if expected_output is not None:\n-        assert_allclose(actual_output, expected_output, rtol=1e-3)\n-\n-    outer_model_config = outer_model.get_config()\n-    outer_model = Model.from_config(outer_model_config)\n-    outer_model.compile('rmsprop', 'mse')\n-\n     # test as first layer in Sequential API\n     layer_config = layer.get_config()\n     layer_config['batch_input_shape'] = input_shape\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f57128bd3d347194d77bfe43ff6a794054679d69", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 26 | Files Changed: 2 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 50 | Churn Cumulative: 295 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -7,3 +7,4 @@ from .normalization import *\n from .embeddings import *\n from .noise import *\n from .advanced_activations import *\n+from .wrappers import *\n\n@@ -1,4 +1,4 @@\n-from ..engine import Layer\n+from ..engine import Layer, InputSpec\n from .. import backend as K\n \n \n@@ -8,21 +8,16 @@ class Wrapper(Layer):\n         self.layer = layer\n         super(Wrapper, self).__init__(**kwargs)\n \n-    @property\n-    def trainable_weights(self):\n-        return self.layer.trainable_weights\n-\n-    @property\n-    def non_trainable_weights(self):\n-        return self.layer.non_trainable_weights\n-\n-    @property\n-    def regularizers(self):\n-        return self.layer.regularizers\n-\n-    @property\n-    def updates(self):\n-        return self.layer.updates\n+    def build(self, input_shape=None):\n+        '''Assumes that self.layer is already set.\n+        Should be called at the end of .build() in the\n+        children classes.\n+        '''\n+        self.trainable_weights = getattr(self.layer, 'trainable_weights', [])\n+        self.non_trainable_weights = getattr(self.layer, 'non_trainable_weights', [])\n+        self.updates = getattr(self.layer, 'updates', [])\n+        self.regularizers = getattr(self.layer, 'regularizers', [])\n+        self.constraints = getattr(self.layer, 'constraints', {})\n \n     def get_weights(self):\n         weights = self.layer.get_weights()\n@@ -32,10 +27,9 @@ class Wrapper(Layer):\n         self.layer.set_weights(weights)\n \n     def get_config(self):\n-        config = {'name': self.__class__.__name__,\n-                  'layer': {'class_name': self.layer.__class__.__name__,\n+        config = {'layer': {'class_name': self.layer.__class__.__name__,\n                             'config': self.layer.get_config()}}\n-        base_config = super(TimeDistributed, self).get_config()\n+        base_config = super(Wrapper, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n     @classmethod\n@@ -81,8 +75,9 @@ class TimeDistributed(Wrapper):\n \n     def build(self, input_shape):\n         assert len(input_shape) >= 3\n+        self.input_spec = [InputSpec(shape=input_shape)]\n         if K._BACKEND == 'tensorflow':\n-            if not self.input_shape[1]:\n+            if not input_shape[1]:\n                 raise Exception('When using TensorFlow, you should define '\n                                 'explicitly the number of timesteps of '\n                                 'your sequences.\\n'\n@@ -94,15 +89,17 @@ class TimeDistributed(Wrapper):\n                                 'argument, including the time axis.')\n         child_input_shape = (input_shape[0],) + input_shape[2:]\n         self.layer.build(child_input_shape)\n+        super(TimeDistributed, self).build()\n \n     def get_output_shape_for(self, input_shape):\n-        child_input_shape = (input_shape[0], input_shape[2:])\n+        child_input_shape = (input_shape[0],) + input_shape[2:]\n         child_output_shape = self.layer.get_output_shape_for(child_input_shape)\n         timesteps = input_shape[1]\n         return (child_output_shape[0], timesteps) + child_output_shape[1:]\n \n     def call(self, X, mask=None):\n-        if self.input_shape[0]:\n+        input_shape = self.input_spec[0].shape\n+        if input_shape[0]:\n             # batch size matters, use rnn-based implementation\n             def step(x, states):\n                 output = self.layer.call(x)\n@@ -115,12 +112,12 @@ class TimeDistributed(Wrapper):\n             # no batch size specified, therefore the layer will be able\n             # to process batches of any size\n             # we can go with reshape-based implementation for performance\n-            input_shape = self.input_shape\n-            x = K.reshape(X, (-1, ) + input_shape[2:])  # (nb_samples * timesteps, ...)\n-            y = self.layer(x, train=False)  # (nb_samples * timesteps, ...)\n+            X = K.reshape(X, (-1, ) + input_shape[2:])  # (nb_samples * timesteps, ...)\n+            y = self.layer.call(X)  # (nb_samples * timesteps, ...)\n             input_length = input_shape[1]\n             if not input_length:\n                 input_length = K.shape(X)[1]\n             # (nb_samples, timesteps, ...)\n-            y = K.reshape(y, (-1, input_length) + self.layer.output_shape[1:])\n+            output_shape = self.get_output_shape_for(input_shape)\n+            y = K.reshape(y, (-1, input_length) + output_shape[2:])\n         return y\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#64449c196e3ad7f5a9e85b9c01e8f0ac83335232", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 71 | Files Changed: 3 | Hunks: 12 | Methods Changed: 6 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 95 | Churn Cumulative: 4320 | Contributors (this commit): 22 | Commits (past 90d): 40 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -224,7 +224,7 @@ class Recurrent(Layer):\n                   'unroll': self.unroll,\n                   'consume_less': self.consume_less}\n         if self.stateful:\n-            config['batch_input_shape'] = self.input_shape\n+            config['batch_input_shape'] = self.input_spec[0].shape\n         else:\n             config['input_dim'] = self.input_dim\n             config['input_length'] = self.input_length\n@@ -479,7 +479,7 @@ class GRU(Recurrent):\n \n     def reset_states(self):\n         assert self.stateful, 'Layer must be stateful.'\n-        input_shape = self.input_shape\n+        input_shape = self.input_spec[0].shape\n         if not input_shape[0]:\n             raise Exception('If a RNN is stateful, a complete ' +\n                             'input_shape must be provided (including batch size).')\n\n@@ -1,30 +1,13 @@\n import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Activation, Flatten\n+from keras.utils.test_utils import layer_test\n from keras.layers.embeddings import Embedding\n-from keras.constraints import unitnorm\n-from keras import backend as K\n \n \n-X1 = np.array([[1], [2]], dtype='int32')\n-W1 = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]], dtype='float32')\n-\n-\n-def test_unitnorm_constraint():\n-    lookup = Sequential()\n-    lookup.add(Embedding(3, 2, weights=[W1],\n-                         W_constraint=unitnorm(),\n-                         input_length=1))\n-    lookup.add(Flatten())\n-    lookup.add(Dense(1))\n-    lookup.add(Activation('sigmoid'))\n-    lookup.compile(loss='binary_crossentropy', optimizer='sgd',\n-                   class_mode='binary')\n-    lookup.train_on_batch(X1, np.array([[1], [0]], dtype='int32'))\n-    norm = np.linalg.norm(K.get_value(lookup.trainable_weights[0]), axis=0)\n-    assert_allclose(norm, np.ones_like(norm).astype('float32'), rtol=1e-05)\n+def test_embedding():\n+    layer_test(Embedding,\n+               kwargs={'output_dim': 4., 'input_dim': 10, 'input_length': 2},\n+               input_shape=(3, 2),\n+               input_dtype='int32')\n \n \n if __name__ == '__main__':\n\n@@ -2,13 +2,13 @@ import pytest\n import numpy as np\n from numpy.testing import assert_allclose\n \n+from keras.utils.test_utils import layer_test\n from keras.layers import recurrent, embeddings\n from keras.models import Sequential\n from keras.layers.core import Masking\n from keras import regularizers\n \n from keras import backend as K\n-from keras.models import Sequential, model_from_json\n \n nb_samples, timesteps, embedding_dim, output_dim = 3, 5, 10, 5\n embedding_num = 12\n@@ -19,39 +19,18 @@ def _runner(layer_class):\n     All the recurrent layers share the same interface,\n     so we can run through them with a single function.\n     \"\"\"\n-    for ret_seq in [True, False]:\n-        layer = layer_class(output_dim, return_sequences=ret_seq,\n-                            weights=None, input_shape=(timesteps, embedding_dim))\n-        layer.input = K.variable(np.ones((nb_samples, timesteps, embedding_dim)))\n-        layer.get_config()\n-\n-        for train in [True, False]:\n-            out = K.eval(layer.get_output(train))\n-            # Make sure the output has the desired shape\n-            if ret_seq:\n-                assert(out.shape == (nb_samples, timesteps, output_dim))\n-            else:\n-                assert(out.shape == (nb_samples, output_dim))\n-\n-            mask = layer.get_output_mask(train)\n+    # check return_sequences\n+    layer_test(layer_class,\n+               kwargs={'output_dim': output_dim,\n+                       'return_sequences': True},\n+               input_shape=(3, 2, 3))\n \n     # check dropout\n-    for ret_seq in [True, False]:\n-        layer = layer_class(output_dim, return_sequences=ret_seq, weights=None, \n-                            batch_input_shape=(nb_samples, timesteps, embedding_dim),\n-                            dropout_W=0.5, dropout_U=0.5)\n-        layer.input = K.variable(np.ones((nb_samples, timesteps, embedding_dim)))\n-        layer.get_config()\n-\n-        for train in [True, False]:\n-            out = K.eval(layer.get_output(train))\n-            # Make sure the output has the desired shape\n-            if ret_seq:\n-                assert(out.shape == (nb_samples, timesteps, output_dim))\n-            else:\n-                assert(out.shape == (nb_samples, output_dim))\n-\n-            mask = layer.get_output_mask(train)\n+    layer_test(layer_class,\n+               kwargs={'output_dim': output_dim,\n+                       'dropout_U': 0.1,\n+                       'dropout_W': 0.1},\n+               input_shape=(3, 2, 3))\n \n     # check statefulness\n     model = Sequential()\n@@ -110,13 +89,15 @@ def _runner(layer_class):\n     assert_allclose(out7, out6, atol=1e-5)\n \n     # check regularizers\n-    layer = layer_class(output_dim, return_sequences=ret_seq, weights=None,\n+    layer = layer_class(output_dim, return_sequences=False, weights=None,\n                         batch_input_shape=(nb_samples, timesteps, embedding_dim),\n                         W_regularizer=regularizers.WeightRegularizer(l1=0.01),\n                         U_regularizer=regularizers.WeightRegularizer(l1=0.01),\n                         b_regularizer='l2')\n-    layer.input = K.variable(np.ones((nb_samples, timesteps, embedding_dim)))\n-    out = K.eval(layer.get_output(train=True))\n+    shape = (nb_samples, timesteps, embedding_dim)\n+    layer.set_input(K.variable(np.ones(shape)),\n+                    shape=shape)\n+    K.eval(layer.output)\n \n \n def test_SimpleRNN():\n@@ -131,17 +112,6 @@ def test_LSTM():\n     _runner(recurrent.LSTM)\n \n \n-def test_batch_input_shape_serialization():\n-    model = Sequential()\n-    model.add(embeddings.Embedding(2, 2,\n-                                   mask_zero=True,\n-                                   input_length=2,\n-                                   batch_input_shape=(2, 2)))\n-    json_data = model.to_json()\n-    reconstructed_model = model_from_json(json_data)\n-    assert(reconstructed_model.input_shape == (2, 2))\n-\n-\n def test_masking_layer():\n     ''' This test based on a previously failing issue here:\n     https://github.com/fchollet/keras/issues/1567\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#efe5916109e220a429a2cff110edb952d747466f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 147 | Lines Deleted: 253 | Files Changed: 2 | Hunks: 54 | Methods Changed: 21 | Complexity Δ (Sum/Max): -1/6 | Churn Δ: 400 | Churn Cumulative: 4019 | Contributors (this commit): 19 | Commits (past 90d): 17 | Contributors (cumulative): 23 | DMM Complexity: 0.9207920792079208\n\nDIFF:\n@@ -91,7 +91,6 @@ class Convolution1D(Layer):\n \n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n-        self.constraints = [self.W_constraint, self.b_constraint]\n \n         self.input_spec = [InputSpec(ndim=3)]\n         self.initial_weights = weights\n@@ -121,6 +120,12 @@ class Convolution1D(Layer):\n             self.activity_regularizer.set_layer(self)\n             self.regularizers.append(self.activity_regularizer)\n \n+        self.constraints = {}\n+        if self.W_constraint:\n+            self.constraints[self.W] = self.W_constraint\n+        if self.b_constraint:\n+            self.constraints[self.b] = self.b_constraint\n+\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n@@ -240,7 +245,6 @@ class Convolution2D(Layer):\n \n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n-        self.constraints = [self.W_constraint, self.b_constraint]\n \n         self.input_spec = [InputSpec(ndim=4)]\n         self.initial_weights = weights\n@@ -272,6 +276,12 @@ class Convolution2D(Layer):\n             self.activity_regularizer.set_layer(self)\n             self.regularizers.append(self.activity_regularizer)\n \n+        self.constraints = {}\n+        if self.W_constraint:\n+            self.constraints[self.W] = self.W_constraint\n+        if self.b_constraint:\n+            self.constraints[self.b] = self.b_constraint\n+\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n@@ -414,7 +424,6 @@ class Convolution3D(Layer):\n \n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n-        self.constraints = [self.W_constraint, self.b_constraint]\n \n         self.input_spec = [InputSpec(ndim=5)]\n         self.initial_weights = weights\n@@ -449,6 +458,12 @@ class Convolution3D(Layer):\n             self.activity_regularizer.set_layer(self)\n             self.regularizers.append(self.activity_regularizer)\n \n+        self.constraints = {}\n+        if self.W_constraint:\n+            self.constraints[self.W] = self.W_constraint\n+        if self.b_constraint:\n+            self.constraints[self.b] = self.b_constraint\n+\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n@@ -911,9 +926,9 @@ class UpSampling1D(Layer):\n     '''\n \n     def __init__(self, length=2, **kwargs):\n-        super(UpSampling1D, self).__init__(**kwargs)\n         self.length = length\n         self.input_spec = [InputSpec(ndim=3)]\n+        super(UpSampling1D, self).__init__(**kwargs)\n \n     def get_output_shape_for(self, input_shape):\n         return (input_shape[0], self.length * input_shape[1], input_shape[2])\n@@ -952,11 +967,11 @@ class UpSampling2D(Layer):\n     '''\n \n     def __init__(self, size=(2, 2), dim_ordering='th', **kwargs):\n-        super(UpSampling2D, self).__init__(**kwargs)\n         self.size = tuple(size)\n         assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n         self.dim_ordering = dim_ordering\n         self.input_spec = [InputSpec(ndim=4)]\n+        super(UpSampling2D, self).__init__(**kwargs)\n \n     def get_output_shape_for(self, input_shape):\n         if self.dim_ordering == 'th':\n@@ -1011,11 +1026,11 @@ class UpSampling3D(Layer):\n         if K._BACKEND != 'theano':\n             raise Exception(self.__class__.__name__ +\n                             ' is currently only working with Theano backend.') \n-        super(UpSampling3D, self).__init__(**kwargs)\n         self.size = tuple(size)\n         assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n         self.dim_ordering = dim_ordering\n         self.input_spec = [InputSpec(ndim=5)]\n+        super(UpSampling3D, self).__init__(**kwargs)\n \n     def get_output_shape_for(self, input_shape):\n         if self.dim_ordering == 'th':\n\n@@ -2,295 +2,175 @@ import pytest\n import numpy as np\n from numpy.testing import assert_allclose\n \n+from keras.utils.test_utils import layer_test\n from keras import backend as K\n from keras.layers import convolutional\n \n \n def test_convolution_1d():\n-    nb_samples = 9\n-    nb_steps = 7\n-    input_dim = 10\n-    filter_length = 6\n-    nb_filter = 5\n+    nb_samples = 2\n+    nb_steps = 8\n+    input_dim = 5\n+    filter_length = 3\n+    nb_filter = 4\n \n-    weights_in = [np.ones((nb_filter, input_dim, filter_length, 1)),\n-                  np.ones(nb_filter)]\n-\n-    input = np.ones((nb_samples, nb_steps, input_dim))\n-    for weight in [None, weights_in]:\n     for border_mode in ['valid', 'same']:\n         for subsample_length in [1]:\n             if border_mode == 'same' and subsample_length != 1:\n                 continue\n-                for W_regularizer in [None, 'l2']:\n-                    for b_regularizer in [None, 'l2']:\n-                        for act_regularizer in [None, 'l2']:\n-                            layer = convolutional.Convolution1D(\n-                                nb_filter, filter_length,\n-                                weights=weight,\n-                                border_mode=border_mode,\n-                                W_regularizer=W_regularizer,\n-                                b_regularizer=b_regularizer,\n-                                activity_regularizer=act_regularizer,\n-                                subsample_length=subsample_length,\n-                                input_shape=(None, input_dim))\n+            layer_test(convolutional.Convolution1D,\n+                       kwargs={'nb_filter': nb_filter,\n+                               'filter_length': filter_length,\n+                               'border_mode': border_mode,\n+                               'subsample_length': subsample_length},\n+                       input_shape=(nb_samples, nb_steps, input_dim))\n \n-                        layer.input = K.variable(input)\n-                        for train in [True, False]:\n-                            out = K.eval(layer.get_output(train))\n-                            assert input.shape[0] == out.shape[0]\n-                            if border_mode == 'same' and subsample_length == 1:\n-                                assert input.shape[1] == out.shape[1]\n-                        layer.get_config()\n+            layer_test(convolutional.Convolution1D,\n+                       kwargs={'nb_filter': nb_filter,\n+                               'filter_length': filter_length,\n+                               'border_mode': border_mode,\n+                               'W_regularizer': 'l2',\n+                               'b_regularizer': 'l2',\n+                               'activity_regularizer': 'activity_l2',\n+                               'subsample_length': subsample_length},\n+                       input_shape=(nb_samples, nb_steps, input_dim))\n \n \n def test_maxpooling_1d():\n-    nb_samples = 9\n-    nb_steps = 7\n-    input_dim = 10\n-\n-    input = np.ones((nb_samples, nb_steps, input_dim))\n     for stride in [1, 2]:\n-        layer = convolutional.MaxPooling1D(stride=stride,\n-                                           border_mode='valid')\n-        layer.input = K.variable(input)\n-        for train in [True, False]:\n-            K.eval(layer.get_output(train))\n-        layer.get_config()\n+        layer_test(convolutional.MaxPooling1D,\n+                   kwargs={'stride': stride,\n+                           'border_mode': 'valid'},\n+                   input_shape=(3, 5, 4))\n \n \n def test_averagepooling_1d():\n-    nb_samples = 9\n-    nb_steps = 7\n-    input_dim = 10\n-\n-    input = np.ones((nb_samples, nb_steps, input_dim))\n     for stride in [1, 2]:\n-        layer = convolutional.AveragePooling1D(stride=stride,\n-                                               border_mode='valid')\n-        layer.input = K.variable(input)\n-        for train in [True, False]:\n-            K.eval(layer.get_output(train))\n-        layer.get_config()\n+        layer_test(convolutional.AveragePooling1D,\n+                   kwargs={'stride': stride,\n+                           'border_mode': 'valid'},\n+                   input_shape=(3, 5, 4))\n \n \n def test_convolution_2d():\n     nb_samples = 8\n-    nb_filter = 9\n-    stack_size = 7\n+    nb_filter = 3\n+    stack_size = 4\n     nb_row = 10\n     nb_col = 6\n \n-    input_nb_row = 11\n-    input_nb_col = 12\n-\n-    weights_in = [np.ones((nb_filter, stack_size, nb_row, nb_col)), np.ones(nb_filter)]\n-\n-    input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n-    for weight in [None, weights_in]:\n     for border_mode in ['valid', 'same']:\n         for subsample in [(1, 1), (2, 2)]:\n             if border_mode == 'same' and subsample != (1, 1):\n                 continue\n-                for W_regularizer in [None, 'l2']:\n-                    for b_regularizer in [None, 'l2']:\n-                        for act_regularizer in [None, 'l2']:\n-                            layer = convolutional.Convolution2D(\n-                                nb_filter, nb_row, nb_col,\n-                                weights=weight,\n-                                border_mode=border_mode,\n-                                W_regularizer=W_regularizer,\n-                                b_regularizer=b_regularizer,\n-                                activity_regularizer=act_regularizer,\n-                                subsample=subsample,\n-                                input_shape=(stack_size, None, None))\n \n-                            layer.input = K.variable(input)\n-                            for train in [True, False]:\n-                                out = K.eval(layer.get_output(train))\n-                                if border_mode == 'same' and subsample == (1, 1):\n-                                    assert out.shape[2:] == input.shape[2:]\n-                            layer.get_config()\n+            layer_test(convolutional.Convolution2D,\n+                       kwargs={'nb_filter': nb_filter,\n+                               'nb_row': 3,\n+                               'nb_col': 3,\n+                               'border_mode': border_mode,\n+                               'subsample': subsample},\n+                       input_shape=(nb_samples, stack_size, nb_row, nb_col))\n \n-\n-def test_convolution_2d_dim_ordering():\n-    nb_filter = 4\n-    nb_row = 3\n-    nb_col = 2\n-    stack_size = 3\n-\n-    np.random.seed(1337)\n-    weights = [np.random.random((nb_filter, stack_size, nb_row, nb_col)),\n-               np.random.random(nb_filter)]\n-    input = np.random.random((1, stack_size, 10, 10))\n-\n-    layer = convolutional.Convolution2D(\n-        nb_filter, nb_row, nb_col,\n-        weights=weights,\n-        input_shape=input.shape[1:],\n-        dim_ordering='th')\n-    layer.input = K.variable(input)\n-    out_th = K.eval(layer.get_output(False))\n-\n-    input = np.transpose(input, (0, 2, 3, 1))\n-    weights[0] = np.transpose(weights[0], (2, 3, 1, 0))\n-    layer = convolutional.Convolution2D(\n-        nb_filter, nb_row, nb_col,\n-        weights=weights,\n-        input_shape=input.shape[1:],\n-        dim_ordering='tf')\n-    layer.input = K.variable(input)\n-    out_tf = K.eval(layer.get_output(False))\n-\n-    assert_allclose(out_tf, np.transpose(out_th, (0, 2, 3, 1)), atol=1e-05)\n+            layer_test(convolutional.Convolution2D,\n+                       kwargs={'nb_filter': nb_filter,\n+                               'nb_row': 3,\n+                               'nb_col': 3,\n+                               'border_mode': border_mode,\n+                               'W_regularizer': 'l2',\n+                               'b_regularizer': 'l2',\n+                               'activity_regularizer': 'activity_l2',\n+                               'subsample': subsample},\n+                       input_shape=(nb_samples, stack_size, nb_row, nb_col))\n \n \n def test_maxpooling_2d():\n-    nb_samples = 9\n-    stack_size = 7\n-    input_nb_row = 11\n-    input_nb_col = 12\n     pool_size = (3, 3)\n \n-    input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n     for strides in [(1, 1), (2, 2)]:\n-        layer = convolutional.MaxPooling2D(strides=strides,\n-                                           border_mode='valid',\n-                                           pool_size=pool_size)\n-        layer.input = K.variable(input)\n-        for train in [True, False]:\n-            K.eval(layer.get_output(train))\n-        layer.get_config()\n-\n-\n-def test_maxpooling_2d_dim_ordering():\n-    stack_size = 3\n-\n-    input = np.random.random((1, stack_size, 10, 10))\n-\n-    layer = convolutional.MaxPooling2D(\n-        (2, 2),\n-        input_shape=input.shape[1:],\n-        dim_ordering='th')\n-    layer.input = K.variable(input)\n-    out_th = K.eval(layer.get_output(False))\n-\n-    input = np.transpose(input, (0, 2, 3, 1))\n-    layer = convolutional.MaxPooling2D(\n-        (2, 2),\n-        input_shape=input.shape[1:],\n-        dim_ordering='tf')\n-    layer.input = K.variable(input)\n-    out_tf = K.eval(layer.get_output(False))\n-\n-    assert_allclose(out_tf, np.transpose(out_th, (0, 2, 3, 1)), atol=1e-05)\n+        layer_test(convolutional.MaxPooling2D,\n+                   kwargs={'strides': strides,\n+                           'border_mode': 'valid',\n+                           'pool_size': pool_size},\n+                   input_shape=(3, 4, 11, 12))\n \n \n def test_averagepooling_2d():\n-    nb_samples = 9\n-    stack_size = 7\n-    input_nb_row = 11\n-    input_nb_col = 12\n+    pool_size = (3, 3)\n \n-    input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n     for border_mode in ['valid', 'same']:\n         for pool_size in [(2, 2), (3, 3), (4, 4), (5, 5)]:\n             for strides in [(1, 1), (2, 2)]:\n-                layer = convolutional.AveragePooling2D(strides=strides,\n-                                                       border_mode=border_mode,\n-                                                       pool_size=pool_size)\n-                layer.input = K.variable(input)\n-                for train in [True, False]:\n-                    out = K.eval(layer.get_output(train))\n-                    if border_mode == 'same' and strides == (1, 1):\n-                        assert input.shape == out.shape\n-                layer.get_config()\n+                layer_test(convolutional.MaxPooling2D,\n+                           kwargs={'strides': strides,\n+                                   'border_mode': border_mode,\n+                                   'pool_size': pool_size},\n+                           input_shape=(3, 4, 11, 12))\n \n \n @pytest.mark.skipif(K._BACKEND != 'theano', reason=\"Requires Theano backend\")\n def test_convolution_3d():\n-    nb_samples = 8\n-    nb_filter = 9\n-    stack_size = 7\n-    len_conv_dim1 = 2\n-    len_conv_dim2 = 10\n-    len_conv_dim3 = 6\n+    nb_samples = 2\n+    nb_filter = 5\n+    stack_size = 4\n+    kernel_dim1 = 2\n+    kernel_dim2 = 3\n+    kernel_dim3 = 1\n \n     input_len_dim1 = 10\n     input_len_dim2 = 11\n     input_len_dim3 = 12\n \n-    weights_in = [np.ones((nb_filter, stack_size, len_conv_dim1, len_conv_dim2, len_conv_dim3)),\n-                  np.ones(nb_filter)]\n-\n-    input = np.ones((nb_samples, stack_size, input_len_dim1,\n-                     input_len_dim2, input_len_dim3))\n-    for weight in [None, weights_in]:\n     for border_mode in ['same', 'valid']:\n         for subsample in [(1, 1, 1), (2, 2, 2)]:\n             if border_mode == 'same' and subsample != (1, 1, 1):\n                 continue\n-                for W_regularizer in [None, 'l2']:\n-                    for b_regularizer in [None, 'l2']:\n-                        for act_regularizer in [None, 'l2']:\n-                            layer = convolutional.Convolution3D(\n-                                nb_filter, len_conv_dim1, len_conv_dim2, len_conv_dim3,\n-                                weights=weight,\n-                                border_mode=border_mode,\n-                                W_regularizer=W_regularizer,\n-                                b_regularizer=b_regularizer,\n-                                activity_regularizer=act_regularizer,\n-                                subsample=subsample,\n-                                input_shape=(stack_size, None, None, None))\n \n-                            layer.input = K.variable(input)\n-                            for train in [True, False]:\n-                                out = K.eval(layer.get_output(train))\n-                                if border_mode == 'same' and subsample == (1, 1, 1):\n-                                    assert out.shape[2:] == input.shape[2:]\n-                            layer.get_config()\n+            layer_test(convolutional.Convolution3D,\n+                       kwargs={'nb_filter': nb_filter,\n+                               'kernel_dim1': kernel_dim1,\n+                               'kernel_dim2': kernel_dim2,\n+                               'kernel_dim3': kernel_dim3,\n+                               'border_mode': border_mode,\n+                               'subsample': subsample},\n+                       input_shape=(nb_samples, stack_size, nb_row, nb_col))\n+\n+            layer_test(convolutional.Convolution3D,\n+                       kwargs={'nb_filter': nb_filter,\n+                               'kernel_dim1': kernel_dim1,\n+                               'kernel_dim2': kernel_dim2,\n+                               'kernel_dim3': kernel_dim3,\n+                               'border_mode': border_mode,\n+                               'W_regularizer': 'l2',\n+                               'b_regularizer': 'l2',\n+                               'activity_regularizer': 'activity_l2',\n+                               'subsample': subsample},\n+                       input_shape=(nb_samples, stack_size,\n+                                    input_len_dim1, input_len_dim2, input_len_dim3))\n \n \n @pytest.mark.skipif(K._BACKEND != 'theano', reason=\"Requires Theano backend\")\n def test_maxpooling_3d():\n-    nb_samples = 9\n-    stack_size = 7\n-    input_len_dim1 = 10\n-    input_len_dim2 = 11\n-    input_len_dim3 = 12\n     pool_size = (3, 3, 3)\n \n-    input = np.ones((nb_samples, stack_size, input_len_dim1,\n-                     input_len_dim2, input_len_dim3))\n     for strides in [(1, 1, 1), (2, 2, 2)]:\n-        layer = convolutional.MaxPooling3D(strides=strides,\n-                                           border_mode='valid',\n-                                           pool_size=pool_size)\n-        layer.input = K.variable(input)\n-        for train in [True, False]:\n-            K.eval(layer.get_output(train))\n-        layer.get_config()\n+        layer_test(convolutional.MaxPooling3D,\n+                   kwargs={'strides': strides,\n+                           'border_mode': 'valid',\n+                           'pool_size': pool_size},\n+                   input_shape=(3, 4, 11, 12, 10))\n \n \n @pytest.mark.skipif(K._BACKEND != 'theano', reason=\"Requires Theano backend\")\n def test_averagepooling_3d():\n-    nb_samples = 9\n-    stack_size = 7\n-    input_len_dim1 = 10\n-    input_len_dim2 = 11\n-    input_len_dim3 = 12\n     pool_size = (3, 3, 3)\n \n-    input = np.ones((nb_samples, stack_size, input_len_dim1,\n-                     input_len_dim2, input_len_dim3))\n     for strides in [(1, 1, 1), (2, 2, 2)]:\n-        layer = convolutional.AveragePooling3D(strides=strides,\n-                                               border_mode='valid',\n-                                               pool_size=pool_size)\n-        layer.input = K.variable(input)\n-        for train in [True, False]:\n-            K.eval(layer.get_output(train))\n-        layer.get_config()\n+        layer_test(convolutional.AveragePooling3D,\n+                   kwargs={'strides': strides,\n+                           'border_mode': 'valid',\n+                           'pool_size': pool_size},\n+                   input_shape=(3, 4, 11, 12, 10))\n \n \n def test_zero_padding_2d():\n@@ -300,10 +180,17 @@ def test_zero_padding_2d():\n     input_nb_col = 12\n \n     input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n+\n+    # basic test\n+    layer_test(convolutional.ZeroPadding2D,\n+               kwargs={'padding': (2, 2)},\n+               input_shape=input.shape)\n+\n+    # correctness test\n     layer = convolutional.ZeroPadding2D(padding=(2, 2))\n-    layer.input = K.variable(input)\n-    for train in [True, False]:\n-        out = K.eval(layer.get_output(train))\n+    layer.set_input(K.variable(input), shape=input.shape)\n+\n+    out = K.eval(layer.output)\n     for offset in [0, 1, -1, -2]:\n         assert_allclose(out[:, :, offset, :], 0.)\n         assert_allclose(out[:, :, :, offset], 0.)\n@@ -321,10 +208,16 @@ def test_zero_padding_3d():\n \n     input = np.ones((nb_samples, stack_size, input_len_dim1,\n                      input_len_dim2, input_len_dim3))\n+\n+    # basic test\n+    layer_test(convolutional.ZeroPadding3D,\n+               kwargs={'padding': (2, 2, 2)},\n+               input_shape=input.shape)\n+\n+    # correctness test\n     layer = convolutional.ZeroPadding3D(padding=(2, 2, 2))\n-    layer.input = K.variable(input)\n-    for train in [True, False]:\n-        out = K.eval(layer.get_output(train))\n+    layer.set_input(K.variable(input), shape=input.shape)\n+    out = K.eval(layer.output)\n     for offset in [0, 1, -1, -2]:\n         assert_allclose(out[:, :, offset, :, :], 0.)\n         assert_allclose(out[:, :, :, offset, :], 0.)\n@@ -334,18 +227,9 @@ def test_zero_padding_3d():\n \n \n def test_upsampling_1d():\n-    nb_samples = 9\n-    nb_steps = 7\n-    input_dim = 10\n-\n-    input = np.ones((nb_samples, nb_steps, input_dim))\n-    for length in [2, 3, 9]:\n-        layer = convolutional.UpSampling1D(length=length)\n-        layer.input = K.variable(input)\n-        for train in [True, False]:\n-            out = K.eval(layer.get_output(train))\n-            assert out.shape[1] == length * nb_steps\n-        layer.get_config()\n+    layer_test(convolutional.UpSampling1D,\n+               kwargs={'length': 2},\n+               input_shape=(3, 5, 4))\n \n \n def test_upsampling_2d():\n@@ -366,11 +250,10 @@ def test_upsampling_2d():\n             for length_col in [2, 3, 9]:\n                 layer = convolutional.UpSampling2D(\n                     size=(length_row, length_col),\n-                    input_shape=input.shape[1:],\n                     dim_ordering=dim_ordering)\n-                layer.input = K.variable(input)\n-                for train in [True, False]:\n-                    out = K.eval(layer.get_output(train))\n+                layer.set_input(K.variable(input), shape=input.shape)\n+\n+                out = K.eval(layer.output)\n                 if dim_ordering == 'th':\n                     assert out.shape[2] == length_row * input_nb_row\n                     assert out.shape[3] == length_col * input_nb_col\n@@ -388,8 +271,6 @@ def test_upsampling_2d():\n \n                 assert_allclose(out, expected_out)\n \n-                layer.get_config()\n-\n \n @pytest.mark.skipif(K._BACKEND != 'theano', reason=\"Requires Theano backend\")\n def test_upsampling_3d():\n@@ -411,11 +292,10 @@ def test_upsampling_3d():\n                 for length_dim3 in [2, 3, 9]:\n                     layer = convolutional.UpSampling3D(\n                         size=(length_dim1, length_dim2, length_dim3),\n-                        input_shape=input.shape[1:],\n                         dim_ordering=dim_ordering)\n-                    layer.input = K.variable(input)\n-                    for train in [True, False]:\n-                        out = K.eval(layer.get_output(train))\n+                    layer.set_input(K.variable(input), shape=input.shape)\n+\n+                    out = K.eval(layer.output)\n                     if dim_ordering == 'th':\n                         assert out.shape[2] == length_dim1 * input_len_dim1\n                         assert out.shape[3] == length_dim2 * input_len_dim2\n@@ -437,8 +317,7 @@ def test_upsampling_3d():\n \n                     assert_allclose(out, expected_out)\n \n-                    layer.get_config()\n-\n \n if __name__ == '__main__':\n-    pytest.main([__file__])\n+    # pytest.main([__file__])\n+    test_convolution_1d()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
