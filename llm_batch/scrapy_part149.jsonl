{"custom_id": "scrapy#93a1102189b5cc53b5816ba74568a0ab7e6f5e6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 54 | Churn Cumulative: 1199 | Contributors (this commit): 9 | Commits (past 90d): 9 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,6 +27,8 @@ class HttpCacheMiddleware(object):\n         self.ignore_http_codes = map(int, settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES'))\n         self.use_dummy_cache = settings.getbool('HTTPCACHE_USE_DUMMY')\n         self.stats = stats\n+        self.policy_request = settings.get('HTTPCACHE_POLICY_REQUEST')\n+        self.policy_response = settings.get('HTTPCACHE_POLICY_RESPONSE')\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -86,12 +88,14 @@ class HttpCacheMiddleware(object):\n         retval = response.status not in self.ignore_http_codes\n         if not self.use_dummy_cache and response.headers.has_key('cache-control'):\n             retval = retval and (response.headers['cache-control'].lower().find('no-store') == -1)\n+        retval = retval and self.policy_response(response)\n         return retval\n \n     def is_cacheable(self, request):\n         retval = urlparse_cached(request).scheme not in self.ignore_schemes\n         if not self.use_dummy_cache and request.headers.has_key('cache-control'):\n             retval = retval and (request.headers['cache-control'].lower().find('no-store') == -1)\n+        retval = retval and self.policy_request(request)\n         return retval\n \n class FilesystemCacheStorage(object):\n\n@@ -142,6 +142,8 @@ HTTPCACHE_EXPIRATION_SECS = 0\n HTTPCACHE_IGNORE_HTTP_CODES = []\n HTTPCACHE_IGNORE_SCHEMES = ['file']\n HTTPCACHE_DBM_MODULE = 'anydbm'\n+HTTPCACHE_POLICY_REQUEST = lambda request : True\n+HTTPCACHE_POLICY_RESPONSE = lambda response : True\n \n ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'\n \n\n@@ -45,6 +45,8 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n             'HTTPCACHE_DIR': self.tmpdir,\n             'HTTPCACHE_EXPIRATION_SECS': 1,\n             'HTTPCACHE_IGNORE_HTTP_CODES': [],\n+            'HTTPCACHE_POLICY_REQUEST': lambda request : True,\n+            'HTTPCACHE_POLICY_RESPONSE': lambda response : True,\n         }\n         settings.update(new_settings)\n         return Settings(settings)\n@@ -304,6 +306,52 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n             assert isinstance(response, Request)\n             self.assertEqualRequest(request, response)\n     \n+    def test_http_cache_request_policy(self):\n+        callback = lambda r: (r.url == 'http://www.example.com/')\n+        \n+        # Should be cached\n+        req, res = Request('http://www.example.com/'), Response('http://www.example.com/')\n+        with self._middleware(HTTPCACHE_POLICY_REQUEST=callback) as mw:\n+            assert mw.process_request(req, self.spider) is None\n+            mw.process_response(req, res, self.spider)\n+\n+            cached = mw.process_request(req, self.spider)\n+            assert isinstance(cached, Response), type(cached)\n+            self.assertEqualResponse(res, cached)\n+            assert 'cached' in cached.flags\n+        \n+        # Should not be cached\n+        req, res = Request('http://www.test.com/'), Response('http://www.test.com/')\n+        with self._middleware(HTTPCACHE_POLICY_REQUEST=callback) as mw:\n+            assert mw.process_request(req, self.spider) is None\n+            mw.process_response(req, res, self.spider)\n+\n+            assert mw.storage.retrieve_response(self.spider, req) is None\n+            assert mw.process_request(req, self.spider) is None\n+            \n+    def test_http_cache_response_policy(self):\n+        callback = lambda r: (r.url == 'http://www.example.com/')\n+        \n+        # Should be cached\n+        req, res = Request('http://www.example.com/'), Response('http://www.example.com/')\n+        with self._middleware(HTTPCACHE_POLICY_RESPONSE=callback) as mw:\n+            assert mw.process_request(req, self.spider) is None\n+            mw.process_response(req, res, self.spider)\n+\n+            cached = mw.process_request(req, self.spider)\n+            assert isinstance(cached, Response), type(cached)\n+            self.assertEqualResponse(res, cached)\n+            assert 'cached' in cached.flags\n+        \n+        # Should not be cached\n+        req, res = Request('http://www.test.com/'), Response('http://www.test.com/')\n+        with self._middleware(HTTPCACHE_POLICY_RESPONSE=callback) as mw:\n+            assert mw.process_request(req, self.spider) is None\n+            mw.process_response(req, res, self.spider)\n+\n+            assert mw.storage.retrieve_response(self.spider, req) is None\n+            assert mw.process_request(req, self.spider) is None\n+\n     def assertEqualResponse(self, response1, response2):\n         self.assertEqual(response1.url, response2.url)\n         self.assertEqual(response1.status, response2.status)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#21de68b766088ac05e863fdb30676188fad1674b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 261 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,6 @@ from twisted.python import log\n from scrapy.utils.misc import load_object\n \n from .interfaces import IEggStorage, IPoller, ISpiderScheduler, IEnvironment\n-from .launcher import Launcher\n from .eggstorage import FilesystemEggStorage\n from .scheduler import SpiderScheduler\n from .poller import QueuePoller\n\n@@ -7,8 +7,6 @@ from scrapy.utils.misc import load_object\n \n from .interfaces import IPoller, IEggStorage, ISpiderScheduler\n \n-from . import webservice\n-\n class Root(resource.Resource):\n \n     def __init__(self, config, app):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3d397de0e934233e70148c01afb78e63e2a03592", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 8 | Churn Cumulative: 804 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -201,14 +201,18 @@ class Scraper(object):\n             ex = output.value\n             if isinstance(ex, DropItem):\n                 logkws = self.logformatter.dropped(item, ex, response, spider)\n-                log.msg(level=log.WARNING, spider=spider, **logkws)\n+                if 'level' not in logkws:\n+                    logkws['level'] = log.WARNING\n+                log.msg(spider=spider, **logkws)\n                 return self.signals.send_catch_log_deferred(signal=signals.item_dropped, \\\n                     item=item, spider=spider, exception=output.value)\n             else:\n                 log.err(output, 'Error processing %(item)s', item=item, spider=spider)\n         else:\n             logkws = self.logformatter.scraped(output, response, spider)\n-            log.msg(level=log.DEBUG, spider=spider, **logkws)\n+            if 'level' not in logkws:\n+                logkws['level'] = log.DEBUG\n+            log.msg(spider=spider, **logkws)\n             return self.signals.send_catch_log_deferred(signal=signals.item_scraped, \\\n                 item=output, response=response, spider=spider)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#63d0b9f8c8c09b3e9825df02d6333b8238871b78", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 574 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -81,7 +81,7 @@ class HttpCacheMiddleware(object):\n                     self.stats.inc_value('httpcache/store', spider=spider)\n                 else:\n                     response.flags.append('cached')\n-                    self.stats.inc_value('httpcache/hits', spider=spider)\n+                    self.stats.inc_value('httpcache/hit', spider=spider)\n         return response\n \n     def is_cacheable_response(self, response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3e31d068725ad7b85f2ccdfa63c96f0b248fe5a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 41 | Files Changed: 3 | Hunks: 23 | Methods Changed: 21 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 86 | Churn Cumulative: 1287 | Contributors (this commit): 9 | Commits (past 90d): 13 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -16,23 +16,44 @@ from scrapy.utils.misc import load_object\n from scrapy.utils.project import data_path\n \n \n-class HttpCacheMiddleware(object):\n+class HttpCachePolicy(object):\n+    def __init__(self, settings):\n+        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n+        self.ignore_http_codes = map(int, settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES'))\n+        self.policy = settings.get('HTTPCACHE_POLICY')\n+\n+        if self.policy == 'dummy':\n+            self.use_dummy_cache = True\n+        else:\n+            self.use_dummy_cache = False\n+\n+    def should_cache_response(self, response):\n+        retval = response.status not in self.ignore_http_codes\n+        if not self.use_dummy_cache and response.headers.has_key('cache-control'):\n+            retval = retval and (response.headers['cache-control'].lower().find('no-store') == -1)\n+        #retval = retval and self.policy_response(response)\n+        return retval\n+\n+    def should_cache_request(self, request):\n+        retval = urlparse_cached(request).scheme not in self.ignore_schemes\n+        if not self.use_dummy_cache and request.headers.has_key('cache-control'):\n+            retval = retval and (request.headers['cache-control'].lower().find('no-store') == -1)\n+        #retval = retval and self.policy_request(request)\n+        return retval\n+\n+class HttpCacheMiddleware(HttpCachePolicy):\n \n     def __init__(self, settings, stats):\n         if not settings.getbool('HTTPCACHE_ENABLED'):\n             raise NotConfigured\n         self.storage = load_object(settings['HTTPCACHE_STORAGE'])(settings)\n         self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')\n-        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n-        self.ignore_http_codes = map(int, settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES'))\n-        self.use_dummy_cache = settings.getbool('HTTPCACHE_USE_DUMMY')\n         self.stats = stats\n-        self.policy_request = settings.get('HTTPCACHE_POLICY_REQUEST')\n-        self.policy_response = settings.get('HTTPCACHE_POLICY_RESPONSE')\n+        super(HttpCacheMiddleware, self).__init__(settings)\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        o = cls(crawler.settings, crawler.stats)\n+        o = cls.from_settings(crawler.settings, crawler.stats)\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n         return o\n@@ -44,7 +65,7 @@ class HttpCacheMiddleware(object):\n         self.storage.close_spider(spider)\n \n     def process_request(self, request, spider):\n-        if not self.is_cacheable(request):\n+        if not self.should_cache_request(request):\n             return\n         response = self.storage.retrieve_response(spider, request)\n \n@@ -54,7 +75,7 @@ class HttpCacheMiddleware(object):\n             self.stats.inc_value('httpcache/revalidation', spider=spider)\n             return\n \n-        if response and self.is_cacheable_response(response):\n+        if response and self.should_cache_response(response):\n             self.stats.inc_value('httpcache/hit', spider=spider)\n             if self.use_dummy_cache:\n                 response.flags.append('cached')\n@@ -69,8 +90,8 @@ class HttpCacheMiddleware(object):\n             raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n \n     def process_response(self, request, response, spider):\n-        if (self.is_cacheable(request)\n-            and self.is_cacheable_response(response)):\n+        if (self.should_cache_request(request)\n+            and self.should_cache_response(response)):\n             if self.use_dummy_cache:\n                 if 'cached' not in response.flags:\n                     self.storage.store_response(spider, request, response)\n@@ -84,19 +105,6 @@ class HttpCacheMiddleware(object):\n                     self.stats.inc_value('httpcache/hit', spider=spider)\n         return response\n \n-    def is_cacheable_response(self, response):\n-        retval = response.status not in self.ignore_http_codes\n-        if not self.use_dummy_cache and response.headers.has_key('cache-control'):\n-            retval = retval and (response.headers['cache-control'].lower().find('no-store') == -1)\n-        retval = retval and self.policy_response(response)\n-        return retval\n-\n-    def is_cacheable(self, request):\n-        retval = urlparse_cached(request).scheme not in self.ignore_schemes\n-        if not self.use_dummy_cache and request.headers.has_key('cache-control'):\n-            retval = retval and (request.headers['cache-control'].lower().find('no-store') == -1)\n-        retval = retval and self.policy_request(request)\n-        return retval\n \n class FilesystemCacheStorage(object):\n \n\n@@ -134,7 +134,6 @@ FEED_EXPORTERS_BASE = {\n }\n \n HTTPCACHE_ENABLED = False\n-HTTPCACHE_USE_DUMMY = True\n HTTPCACHE_DIR = 'httpcache'\n HTTPCACHE_IGNORE_MISSING = False\n HTTPCACHE_STORAGE = 'scrapy.contrib.httpcache.DbmCacheStorage'\n@@ -142,8 +141,7 @@ HTTPCACHE_EXPIRATION_SECS = 0\n HTTPCACHE_IGNORE_HTTP_CODES = []\n HTTPCACHE_IGNORE_SCHEMES = ['file']\n HTTPCACHE_DBM_MODULE = 'anydbm'\n-HTTPCACHE_POLICY_REQUEST = lambda request : True\n-HTTPCACHE_POLICY_RESPONSE = lambda response : True\n+HTTPCACHE_POLICY = 'dummy'\n \n ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'\n \n\n@@ -41,12 +41,10 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n     def _get_settings(self, **new_settings):\n         settings = {\n             'HTTPCACHE_ENABLED': True,\n-            'HTTPCACHE_USE_DUMMY': True,\n             'HTTPCACHE_DIR': self.tmpdir,\n             'HTTPCACHE_EXPIRATION_SECS': 1,\n             'HTTPCACHE_IGNORE_HTTP_CODES': [],\n-            'HTTPCACHE_POLICY_REQUEST': lambda request : True,\n-            'HTTPCACHE_POLICY_RESPONSE': lambda response : True,\n+            'HTTPCACHE_POLICY': 'dummy'\n         }\n         settings.update(new_settings)\n         return Settings(settings)\n@@ -54,7 +52,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n     @contextmanager\n     def _storage(self, **new_settings):\n         settings = self._get_settings(**new_settings)\n-        if settings.getbool('HTTPCACHE_USE_DUMMY'):\n+        if settings.get('HTTPCACHE_POLICY') == 'dummy':\n             storage = self.storage_class(settings)\n         else:\n             storage = self.realcache_storage_class(settings)\n@@ -187,7 +185,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n     def test_real_http_cache_middleware_response304_not_cached(self):\n         # test response is not cached because the status is 304 Not Modified\n         # (so it should be cached already)\n-        with self._middleware(HTTPCACHE_USE_DUMMY=False) as mw:\n+        with self._middleware(HTTPCACHE_POLICY='rfc2616') as mw:\n             assert mw.process_request(self.request, self.spider) is None\n             response = Response('http://www.example.com', status=304)\n             mw.process_response(self.request, response, self.spider)\n@@ -199,7 +197,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n     def test_real_http_cache_middleware_response_nostore_not_cached(self):\n         # test response is not cached because of the Cache-Control 'no-store' directive\n         # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.2\n-        with self._middleware(HTTPCACHE_USE_DUMMY=False) as mw:\n+        with self._middleware(HTTPCACHE_POLICY='rfc2616') as mw:\n             assert mw.process_request(self.request, self.spider) is None\n             response = Response('http://www.example.com', headers=\n                 {'Content-Type': 'text/html', 'Cache-Control': 'no-store'},\n@@ -212,7 +210,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n     def test_real_http_cache_middleware_request_nostore_not_cached(self):\n         # test response is not cached because of the request's Cache-Control 'no-store' directive\n         # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.2\n-        with self._middleware(HTTPCACHE_USE_DUMMY=False) as mw:\n+        with self._middleware(HTTPCACHE_POLICY='rfc2616') as mw:\n             request = Request('http://www.example.com',\n                 headers={'User-Agent': 'test', 'Cache-Control': 'no-store'})\n             assert mw.process_request(request, self.spider) is None\n@@ -223,14 +221,14 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n \n     def test_real_http_cache_middleware_response_cached_and_fresh(self):\n         # test response cached and fresh\n-        with self._middleware(HTTPCACHE_USE_DUMMY=False) as mw:\n+        with self._middleware(HTTPCACHE_POLICY='rfc2616') as mw:\n             response = mw.process_response(self.request, self.response, self.spider)\n             self.assertRaises(IgnoreRequest, mw.process_request, self.request, self.spider)\n             assert 'cached' not in response.flags\n \n     def test_real_http_cache_middleware_response_cached_and_stale(self):\n         # test response cached but stale\n-        with self._middleware(HTTPCACHE_USE_DUMMY=False,\n+        with self._middleware(HTTPCACHE_POLICY='rfc2616',\n             HTTPCACHE_STORAGE = 'scrapy.contrib.httpcache.DbmRealCacheStorage') as mw:\n             response = Response('http://www.example.com', headers=\n                 {'Content-Type': 'text/html', 'Cache-Control': 'no-cache'},\n@@ -244,7 +242,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n     def test_real_http_cache_storage_response_cached_and_fresh(self):\n         # test response is cached and is fresh\n         # (response requested should be same as response received)\n-        with self._storage(HTTPCACHE_USE_DUMMY=False) as storage:\n+        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:\n             assert storage.retrieve_response(self.spider, self.request) is None\n \n             response = Response('http://www.example.com', headers=\n@@ -257,7 +255,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n     def test_real_http_cache_storage_response403_cached_and_further_requests_ignored(self):\n         # test response is cached but further requests are ignored\n         # because response status is 403 (as per the RFC)\n-        with self._storage(HTTPCACHE_USE_DUMMY=False) as storage:\n+        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:\n             assert storage.retrieve_response(self.spider, self.request) is None\n \n             response = Response('http://www.example.com', headers=\n@@ -270,7 +268,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n     def test_real_http_cache_storage_response_cached_and_stale(self):\n         # test response is cached and is stale (no cache validators inserted)\n         # (request should be same as response received)\n-        with self._storage(HTTPCACHE_USE_DUMMY=False) as storage:\n+        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:\n             assert storage.retrieve_response(self.spider, self.request) is None\n \n             response = Response('http://www.example.com', headers=\n@@ -283,7 +281,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n \n     def test_real_http_cache_storage_response_cached_and_stale_with_cache_validators(self):\n         # test response is cached and is stale and cache validators are inserted\n-        with self._storage(HTTPCACHE_USE_DUMMY=False) as storage:\n+        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:\n             assert storage.retrieve_response(self.spider, self.request) is None\n \n             response = Response('http://www.example.com', headers=\n@@ -297,7 +295,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n     def test_real_http_cache_storage_response_cached_and_transparent(self):\n         # test response is not cached because of the request's Cache-Control 'no-cache' directive\n         # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.2\n-        with self._storage(HTTPCACHE_USE_DUMMY=False) as storage:\n+        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:\n             request = Request('http://www.example.com',\n                 headers={'User-Agent': 'test', 'Cache-Control': 'no-cache'})\n             assert storage.retrieve_response(self.spider, request) is None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cf5f0203b782425aec470e603853885e2aec641c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 50 | Files Changed: 3 | Hunks: 38 | Methods Changed: 29 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 110 | Churn Cumulative: 1397 | Contributors (this commit): 9 | Commits (past 90d): 16 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,44 +16,51 @@ from scrapy.utils.misc import load_object\n from scrapy.utils.project import data_path\n \n \n-class HttpCachePolicy(object):\n+class DummyPolicy(object):\n     def __init__(self, settings):\n         self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n         self.ignore_http_codes = map(int, settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES'))\n-        self.policy = settings.get('HTTPCACHE_POLICY')\n-\n-        if self.policy == 'dummy':\n-            self.use_dummy_cache = True\n-        else:\n-            self.use_dummy_cache = False\n         \n     def should_cache_response(self, response):\n-        retval = response.status not in self.ignore_http_codes\n-        if not self.use_dummy_cache and response.headers.has_key('cache-control'):\n+        return response.status not in self.ignore_http_codes\n+\n+    def should_cache_request(self, request):\n+        return urlparse_cached(request).scheme not in self.ignore_schemes\n+\n+\n+class RFC2616Policy(DummyPolicy):\n+    def __init__(self, settings):\n+        super(RFC2616Policy, self).__init__(settings)\n+\n+    def should_cache_response(self, response):\n+        retval = super(RFC2616Policy, self).should_cache_response(response)\n+\n+        if response.headers.has_key('cache-control'):\n             retval = retval and (response.headers['cache-control'].lower().find('no-store') == -1)\n         #retval = retval and self.policy_response(response)\n         return retval\n \n     def should_cache_request(self, request):\n-        retval = urlparse_cached(request).scheme not in self.ignore_schemes\n-        if not self.use_dummy_cache and request.headers.has_key('cache-control'):\n+        retval = super(RFC2616Policy, self).should_cache_request(request)\n+        \n+        if request.headers.has_key('cache-control'):\n             retval = retval and (request.headers['cache-control'].lower().find('no-store') == -1)\n         #retval = retval and self.policy_request(request)\n         return retval\n \n-class HttpCacheMiddleware(HttpCachePolicy):\n+class HttpCacheMiddleware(object):\n \n     def __init__(self, settings, stats):\n         if not settings.getbool('HTTPCACHE_ENABLED'):\n             raise NotConfigured\n         self.storage = load_object(settings['HTTPCACHE_STORAGE'])(settings)\n         self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')\n+        self.policy = load_object(settings['HTTPCACHE_POLICY'])(settings)\n         self.stats = stats\n-        super(HttpCacheMiddleware, self).__init__(settings)\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        o = cls.from_settings(crawler.settings, crawler.stats)\n+        o = cls(crawler.settings, crawler.stats)\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n         return o\n@@ -65,7 +72,7 @@ class HttpCacheMiddleware(HttpCachePolicy):\n         self.storage.close_spider(spider)\n \n     def process_request(self, request, spider):\n-        if not self.should_cache_request(request):\n+        if not self.policy.should_cache_request(request):\n             return\n         response = self.storage.retrieve_response(spider, request)\n \n@@ -75,14 +82,14 @@ class HttpCacheMiddleware(HttpCachePolicy):\n             self.stats.inc_value('httpcache/revalidation', spider=spider)\n             return\n \n-        if response and self.should_cache_response(response):\n+        if response and self.policy.should_cache_response(response):\n             self.stats.inc_value('httpcache/hit', spider=spider)\n-            if self.use_dummy_cache:\n-                response.flags.append('cached')\n-                return response\n-            else:\n+            if isinstance(self.policy, RFC2616Policy):\n                 # Response cached and fresh\n                 raise IgnoreRequest(\"Ignored request already in cache: %s\" % request)\n+            else:\n+                response.flags.append('cached')\n+                return response\n \n         # Response not cached\n         self.stats.inc_value('httpcache/miss', spider=spider)\n@@ -90,19 +97,19 @@ class HttpCacheMiddleware(HttpCachePolicy):\n             raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n \n     def process_response(self, request, response, spider):\n-        if (self.should_cache_request(request)\n-            and self.should_cache_response(response)):\n-            if self.use_dummy_cache:\n-                if 'cached' not in response.flags:\n-                    self.storage.store_response(spider, request, response)\n-                    self.stats.inc_value('httpcache/store', spider=spider)\n-            else:\n+        if (self.policy.should_cache_request(request)\n+            and self.policy.should_cache_response(response)):\n+            if isinstance(self.policy, RFC2616Policy):\n                 if response.status != 304:\n                     self.storage.store_response(spider, request, response)\n                     self.stats.inc_value('httpcache/store', spider=spider)\n                 else:\n                     response.flags.append('cached')\n                     self.stats.inc_value('httpcache/hit', spider=spider)\n+            else:\n+                if 'cached' not in response.flags:\n+                    self.storage.store_response(spider, request, response)\n+                    self.stats.inc_value('httpcache/store', spider=spider)\n         return response\n \n \n\n@@ -141,7 +141,7 @@ HTTPCACHE_EXPIRATION_SECS = 0\n HTTPCACHE_IGNORE_HTTP_CODES = []\n HTTPCACHE_IGNORE_SCHEMES = ['file']\n HTTPCACHE_DBM_MODULE = 'anydbm'\n-HTTPCACHE_POLICY = 'dummy'\n+HTTPCACHE_POLICY = 'scrapy.contrib.downloadermiddleware.httpcache.DummyPolicy'\n \n ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'\n \n\n@@ -20,6 +20,9 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n     storage_class = FilesystemCacheStorage\n     realcache_storage_class = DbmRealCacheStorage\n \n+    dummy_policy = 'scrapy.contrib.downloadermiddleware.httpcache.DummyPolicy'\n+    rfc2616_policy = 'scrapy.contrib.downloadermiddleware.httpcache.RFC2616Policy'\n+\n     yesterday = email.utils.formatdate(time.time() - 1 * 24 * 60 * 60)\n     now = email.utils.formatdate()\n     tomorrow = email.utils.formatdate(time.time() + 1 * 24 * 60 * 60)\n@@ -44,7 +47,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n             'HTTPCACHE_DIR': self.tmpdir,\n             'HTTPCACHE_EXPIRATION_SECS': 1,\n             'HTTPCACHE_IGNORE_HTTP_CODES': [],\n-            'HTTPCACHE_POLICY': 'dummy'\n+            'HTTPCACHE_POLICY': self.dummy_policy\n         }\n         settings.update(new_settings)\n         return Settings(settings)\n@@ -52,7 +55,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n     @contextmanager\n     def _storage(self, **new_settings):\n         settings = self._get_settings(**new_settings)\n-        if settings.get('HTTPCACHE_POLICY') == 'dummy':\n+        if settings.get('HTTPCACHE_POLICY') == self.dummy_policy:\n             storage = self.storage_class(settings)\n         else:\n             storage = self.realcache_storage_class(settings)\n@@ -182,10 +185,10 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n             self.assertEqualResponse(self.response, response)\n             assert 'cached' in response.flags\n \n-    def test_real_http_cache_middleware_response304_not_cached(self):\n+    def test_middleware_rfc2616policy_response304_not_cached(self):\n         # test response is not cached because the status is 304 Not Modified\n         # (so it should be cached already)\n-        with self._middleware(HTTPCACHE_POLICY='rfc2616') as mw:\n+        with self._middleware(HTTPCACHE_POLICY=self.rfc2616_policy) as mw:\n             assert mw.process_request(self.request, self.spider) is None\n             response = Response('http://www.example.com', status=304)\n             mw.process_response(self.request, response, self.spider)\n@@ -194,10 +197,10 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n             assert mw.storage.retrieve_response(self.spider, self.request) is None\n             assert mw.process_request(self.request, self.spider) is None\n \n-    def test_real_http_cache_middleware_response_nostore_not_cached(self):\n+    def test_middleware_rfc2616policy_response_nostore_not_cached(self):\n         # test response is not cached because of the Cache-Control 'no-store' directive\n         # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.2\n-        with self._middleware(HTTPCACHE_POLICY='rfc2616') as mw:\n+        with self._middleware(HTTPCACHE_POLICY=self.rfc2616_policy) as mw:\n             assert mw.process_request(self.request, self.spider) is None\n             response = Response('http://www.example.com', headers=\n                 {'Content-Type': 'text/html', 'Cache-Control': 'no-store'},\n@@ -207,10 +210,10 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n             assert mw.storage.retrieve_response(self.spider, self.request) is None\n             assert mw.process_request(self.request, self.spider) is None\n \n-    def test_real_http_cache_middleware_request_nostore_not_cached(self):\n+    def test_middleware_rfc2616policy_request_nostore_not_cached(self):\n         # test response is not cached because of the request's Cache-Control 'no-store' directive\n         # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.2\n-        with self._middleware(HTTPCACHE_POLICY='rfc2616') as mw:\n+        with self._middleware(HTTPCACHE_POLICY=self.rfc2616_policy) as mw:\n             request = Request('http://www.example.com',\n                 headers={'User-Agent': 'test', 'Cache-Control': 'no-store'})\n             assert mw.process_request(request, self.spider) is None\n@@ -219,16 +222,16 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n             assert mw.storage.retrieve_response(self.spider, request) is None\n             assert mw.process_request(request, self.spider) is None\n \n-    def test_real_http_cache_middleware_response_cached_and_fresh(self):\n+    def test_middleware_rfc2616policy_response_cached_and_fresh(self):\n         # test response cached and fresh\n-        with self._middleware(HTTPCACHE_POLICY='rfc2616') as mw:\n+        with self._middleware(HTTPCACHE_POLICY=self.rfc2616_policy) as mw:\n             response = mw.process_response(self.request, self.response, self.spider)\n             self.assertRaises(IgnoreRequest, mw.process_request, self.request, self.spider)\n             assert 'cached' not in response.flags\n \n-    def test_real_http_cache_middleware_response_cached_and_stale(self):\n+    def test_middleware_rfc2616policy_response_cached_and_stale(self):\n         # test response cached but stale\n-        with self._middleware(HTTPCACHE_POLICY='rfc2616',\n+        with self._middleware(HTTPCACHE_POLICY=self.rfc2616_policy,\n             HTTPCACHE_STORAGE = 'scrapy.contrib.httpcache.DbmRealCacheStorage') as mw:\n             response = Response('http://www.example.com', headers=\n                 {'Content-Type': 'text/html', 'Cache-Control': 'no-cache'},\n@@ -239,10 +242,10 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n             response = mw.storage.retrieve_response(self.spider, self.request)\n             assert isinstance(response, Request)\n \n-    def test_real_http_cache_storage_response_cached_and_fresh(self):\n+    def test_storage_rfc2616policy_response_cached_and_fresh(self):\n         # test response is cached and is fresh\n         # (response requested should be same as response received)\n-        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:\n+        with self._storage(HTTPCACHE_POLICY=self.rfc2616_policy) as storage:\n             assert storage.retrieve_response(self.spider, self.request) is None\n \n             response = Response('http://www.example.com', headers=\n@@ -252,10 +255,10 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n             response2 = storage.retrieve_response(self.spider, self.request)\n             self.assertEqualResponse(response, response2)\n \n-    def test_real_http_cache_storage_response403_cached_and_further_requests_ignored(self):\n+    def test_storage_rfc2616policy_response403_cached_and_further_requests_ignored(self):\n         # test response is cached but further requests are ignored\n         # because response status is 403 (as per the RFC)\n-        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:\n+        with self._storage(HTTPCACHE_POLICY=self.rfc2616_policy) as storage:\n             assert storage.retrieve_response(self.spider, self.request) is None\n \n             response = Response('http://www.example.com', headers=\n@@ -265,10 +268,10 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n             self.assertRaises(IgnoreRequest, storage.retrieve_response,\n                 self.spider, self.request)\n \n-    def test_real_http_cache_storage_response_cached_and_stale(self):\n+    def test_storage_rfc2616policy_response_cached_and_stale(self):\n         # test response is cached and is stale (no cache validators inserted)\n         # (request should be same as response received)\n-        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:\n+        with self._storage(HTTPCACHE_POLICY=self.rfc2616_policy) as storage:\n             assert storage.retrieve_response(self.spider, self.request) is None\n \n             response = Response('http://www.example.com', headers=\n@@ -279,9 +282,9 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n             assert isinstance(response2, Request)\n             self.assertEqualRequest(self.request, response2)\n \n-    def test_real_http_cache_storage_response_cached_and_stale_with_cache_validators(self):\n+    def test_storage_rfc2616policy_response_cached_and_stale_with_cache_validators(self):\n         # test response is cached and is stale and cache validators are inserted\n-        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:\n+        with self._storage(HTTPCACHE_POLICY=self.rfc2616_policy) as storage:\n             assert storage.retrieve_response(self.spider, self.request) is None\n \n             response = Response('http://www.example.com', headers=\n@@ -292,10 +295,10 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n             assert isinstance(response2, Request)\n             self.assertEqualRequestButWithCacheValidators(self.request, response2)\n \n-    def test_real_http_cache_storage_response_cached_and_transparent(self):\n+    def test_storage_rfc2616policy_response_cached_and_transparent(self):\n         # test response is not cached because of the request's Cache-Control 'no-cache' directive\n         # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.2\n-        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:\n+        with self._storage(HTTPCACHE_POLICY=self.rfc2616_policy) as storage:\n             request = Request('http://www.example.com',\n                 headers={'User-Agent': 'test', 'Cache-Control': 'no-cache'})\n             assert storage.retrieve_response(self.spider, request) is None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#71b071ffebc82dd0c6e1196f3d5f1e73389b8879", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 11 | Churn Cumulative: 2884 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -195,7 +195,7 @@ class ExecutionEngine(object):\n             if isinstance(response, Response):\n                 response.request = request # tie request to response received\n                 logkws = self.logformatter.crawled(request, response, spider)\n-                log.msg(level=log.DEBUG, spider=spider, **logkws)\n+                log.msg(spider=spider, **logkws)\n                 self.signals.send_catch_log(signal=signals.response_received, \\\n                     response=response, request=request, spider=spider)\n             return response\n\n@@ -201,8 +201,6 @@ class Scraper(object):\n             ex = output.value\n             if isinstance(ex, DropItem):\n                 logkws = self.logformatter.dropped(item, ex, response, spider)\n-                if 'level' not in logkws:\n-                    logkws['level'] = log.WARNING\n                 log.msg(spider=spider, **logkws)\n                 return self.signals.send_catch_log_deferred(signal=signals.item_dropped, \\\n                     item=item, spider=spider, exception=output.value)\n@@ -210,8 +208,6 @@ class Scraper(object):\n                 log.err(output, 'Error processing %(item)s', item=item, spider=spider)\n         else:\n             logkws = self.logformatter.scraped(output, response, spider)\n-            if 'level' not in logkws:\n-                logkws['level'] = log.DEBUG\n             log.msg(spider=spider, **logkws)\n             return self.signals.send_catch_log_deferred(signal=signals.item_scraped, \\\n                 item=output, response=response, spider=spider)\n\n@@ -2,6 +2,8 @@ import os\n \n from twisted.python.failure import Failure\n \n+from scrapy import log\n+\n \n SCRAPEDFMT = u\"Scraped from %(src)s\" + os.linesep + \"%(item)s\"\n DROPPEDFMT = u\"Dropped: %(exception)s\" + os.linesep + \"%(item)s\"\n@@ -16,6 +18,7 @@ class LogFormatter(object):\n     def crawled(self, request, response, spider):\n         flags = ' %s' % str(response.flags) if response.flags else ''\n         return {\n+            'level': log.DEBUG,\n             'format': CRAWLEDFMT,\n             'status': response.status,\n             'request': request,\n@@ -26,6 +29,7 @@ class LogFormatter(object):\n     def scraped(self, item, response, spider):\n         src = response.getErrorMessage() if isinstance(response, Failure) else response\n         return {\n+            'level': log.DEBUG,\n             'format': SCRAPEDFMT,\n             'src': src,\n             'item': item,\n@@ -33,6 +37,7 @@ class LogFormatter(object):\n \n     def dropped(self, item, exception, response, spider):\n         return {\n+            'level': log.WARNING,\n             'format': DROPPEDFMT,\n             'exception': exception,\n             'item': item,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ea68250d77e2469cadec5950f3790c2b7d16660d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 78 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -28,6 +28,8 @@ class Command(ScrapyCommand):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_option(\"-c\", dest=\"code\",\n             help=\"evaluate the code in the shell, print the result and exit\")\n+        parser.add_option(\"--spider\", dest=\"spider\",\n+            help=\"use this spider\")\n \n     def update_vars(self, vars):\n         \"\"\"You can use this function to update the Scrapy objects that will be\n@@ -37,9 +39,12 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         url = args[0] if args else None\n+        spider = None\n+        if opts.spider:\n+            spider = self.crawler.spiders.create(opts.spider)\n         shell = Shell(self.crawler, update_vars=self.update_vars, code=opts.code)\n         self._start_crawler_thread()\n-        shell.start(url=url)\n+        shell.start(url=url, spider=spider)\n \n     def _start_crawler_thread(self):\n         t = Thread(target=self.crawler.start)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a6ef76ed8833b2d15d9b667ed26dad760d853467", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 29 | Churn Cumulative: 722 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,6 +25,7 @@ from scrapy.contrib.pipeline.media import MediaPipeline\n class NoimagesDrop(DropItem):\n     \"\"\"Product with no images exception\"\"\"\n \n+\n class ImageException(Exception):\n     \"\"\"General image error exception\"\"\"\n \n@@ -113,7 +114,7 @@ class S3ImagesStore(object):\n         k.set_metadata('width', str(width))\n         k.set_metadata('height', str(height))\n         buf.seek(0)\n-        return threads.deferToThread(k.set_contents_from_file, buf, \\\n+        return threads.deferToThread(k.set_contents_from_file, buf,\n                                      headers=self.HEADERS, policy=self.POLICY)\n \n \n@@ -180,13 +181,13 @@ class ImagesPipeline(MediaPipeline):\n             log.msg(format='Image (code: %(status)s): Error downloading image from %(request)s referred in <%(referer)s>',\n                     level=log.WARNING, spider=info.spider,\n                     status=response.status, request=request, referer=referer)\n-            raise ImageException\n+            raise ImageException('download-error')\n \n         if not response.body:\n             log.msg(format='Image (empty-content): Empty image from %(request)s referred in <%(referer)s>: no-content',\n                     level=log.WARNING, spider=info.spider,\n                     request=request, referer=referer)\n-            raise ImageException\n+            raise ImageException('empty-content')\n \n         status = 'cached' if 'cached' in response.flags else 'downloaded'\n         log.msg(format='Image (%(status)s): Downloaded image from %(request)s referred in <%(referer)s>',\n@@ -197,13 +198,15 @@ class ImagesPipeline(MediaPipeline):\n         try:\n             key = self.image_key(request.url)\n             checksum = self.image_downloaded(response, request, info)\n-        except ImageException, ex:\n-            log.err('image_downloaded hook failed: %s' % ex,\n-                    level=log.WARNING, spider=info.spider)\n+        except ImageException as exc:\n+            whyfmt = 'Image (error): Error processing image from %(request)s referred in <%(referer)s>: %(errormsg)s'\n+            log.msg(format=whyfmt, level=log.WARNING, spider=info.spider,\n+                    request=request, referer=referer, errormsg=str(exc))\n             raise\n-        except Exception:\n-            log.err(spider=info.spider)\n-            raise ImageException\n+        except Exception as exc:\n+            whyfmt = 'Image (unknown-error): Error processing image from %(request)s referred in <%(referer)s>'\n+            log.err(None, whyfmt % {'request': request, 'referer': referer}, spider=info.spider)\n+            raise ImageException(str(exc))\n \n         return {'url': request.url, 'path': key, 'checksum': checksum}\n \n@@ -262,8 +265,8 @@ class ImagesPipeline(MediaPipeline):\n \n         width, height = orig_image.size\n         if width < self.MIN_WIDTH or height < self.MIN_HEIGHT:\n-            raise ImageException(\"Image too small (%dx%d < %dx%d): %s\" % \\\n-                    (width, height, self.MIN_WIDTH, self.MIN_HEIGHT, response.url))\n+            raise ImageException(\"Image too small (%dx%d < %dx%d)\" %\n+                                 (width, height, self.MIN_WIDTH, self.MIN_HEIGHT))\n \n         image, buf = self.convert_image(orig_image)\n         yield key, image, buf\n@@ -290,11 +293,7 @@ class ImagesPipeline(MediaPipeline):\n             image.thumbnail(size, Image.ANTIALIAS)\n \n         buf = StringIO()\n-        try:\n         image.save(buf, 'JPEG')\n-        except Exception, ex:\n-            raise ImageException(\"Cannot process image. Error: %s\" % ex)\n-\n         return image, buf\n \n     def image_key(self, url):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8ea89b277c458e4a3f2cee5e7c96c40dc63e4396", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 810 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -205,7 +205,7 @@ class Scraper(object):\n                 return self.signals.send_catch_log_deferred(signal=signals.item_dropped, \\\n                     item=item, spider=spider, exception=output.value)\n             else:\n-                log.err(output, 'Error processing %(item)s', item=item, spider=spider)\n+                log.err(output, 'Error processing %s' % item, spider=spider)\n         else:\n             logkws = self.logformatter.scraped(output, response, spider)\n             log.msg(spider=spider, **logkws)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#035d1e99a9cca495b110c4ed426cb877fabac5b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 8 | Complexity Δ (Sum/Max): 11/9 | Churn Δ: 83 | Churn Cumulative: 222 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+from django.core.exceptions import ValidationError\n+\n from scrapy.item import Field, Item, ItemMeta\n \n \n@@ -24,10 +26,49 @@ class DjangoItem(Item):\n \n     django_model = None\n \n+    def __init__(self, *args, **kwargs):\n+        super(DjangoItem, self).__init__(*args, **kwargs)\n+        self._instance = None\n+        self._errors = None\n+\n     def save(self, commit=True):\n+        if commit:\n+            self.instance.save()\n+        return self.instance\n+\n+    def is_valid(self, exclude=None):\n+        self._get_errors(exclude)\n+        return not bool(self._errors)\n+\n+    def _get_errors(self, exclude=None):\n+        if self._errors is not None:\n+            return self._errors\n+\n+        self._errors = {}\n+        if exclude is None:\n+            exclude = []\n+\n+        try:\n+            self.instance.clean_fields(exclude=exclude)\n+        except ValidationError, e:\n+            self._errors = e.update_error_dict(self._errors)\n+\n+        try:\n+            self.instance.clean()\n+        except ValidationError, e:\n+            self._errors = e.update_error_dict(self._errors)\n+\n+        # uniqueness is not checked, because it is faster to check it when\n+        # saving object to database. Just beware, that failed save()\n+        # raises IntegrityError instead of ValidationError.\n+\n+        return self._errors\n+    errors = property(_get_errors)\n+\n+    @property\n+    def instance(self):\n+        if self._instance is None:\n             modelargs = dict((k, self.get(k)) for k in self._values\n                              if k in self._model_fields)\n-        model = self.django_model(**modelargs)\n-        if commit:\n-            model.save()\n-        return model\n+            self._instance = self.django_model(**modelargs)\n+        return self._instance\n\n@@ -74,9 +74,43 @@ class DjangoItemTest(unittest.TestCase):\n         i = OverrideFieldPersonItem()\n \n         i['name'] = 'John'\n+        # it is not obvious that \"age\" should be saved also, since it was\n+        # redefined in child class\n+        i['age'] = '22'\n         person = i.save(commit=False)\n \n         self.assertEqual(person.name, 'John')\n+        self.assertEqual(person.age, '22')\n+\n+    def test_validation(self):\n+        long_name = 'z' * 300\n+        i = BasePersonItem(name=long_name)\n+        self.assertFalse(i.is_valid())\n+        self.assertDictEqual(\n+            {\n+                'age': [u'This field cannot be null.'],\n+                'name': [u'Ensure this value has at most 255 characters (it has 300).']\n+            },\n+            i.errors)\n+\n+        i = BasePersonItem(name='John', age='22')\n+        self.assertDictEqual({}, i.errors)\n+        self.assertTrue(i.is_valid())\n+\n+        # once the item is validated, it does not validate again\n+        i['name'] = long_name\n+        self.assertTrue(i.is_valid())\n+\n+    def test_override_validation(self):\n+        i = OverrideFieldPersonItem()\n+        i['name'] = 'John'\n+        self.assertFalse(i.is_valid())\n+\n+        i = i = OverrideFieldPersonItem()\n+        i['name'] = 'John'\n+        i['age'] = '22'\n+        self.assertTrue(i.is_valid())\n+\n \n     def test_default_field_values(self):\n         i = BasePersonItem()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
