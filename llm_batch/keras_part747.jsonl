{"custom_id": "keras#c8a5a8969a8712a9a1939937ce34158e04cfc09d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 533 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -217,7 +217,7 @@ def train_step(state, data):\n         trainable_variables, non_trainable_variables, x, y\n     )\n     trainable_variables, optimizer_variables = optimizer.stateless_apply(\n-        grads, trainable_variables, optimizer_variables\n+        optimizer_variables, grads, trainable_variables\n     )\n     # Return updated state\n     return loss, (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bc9bffb7fc55ebac5cd8fbc5aaa4982ee90823d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 740 | Lines Deleted: 65 | Files Changed: 3 | Hunks: 25 | Methods Changed: 119 | Complexity Δ (Sum/Max): 92/80 | Churn Δ: 805 | Churn Cumulative: 1080 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -184,7 +184,7 @@ class KerasVariable:\n         return self.value.__neg__()\n \n     def __pos__(self):\n-        return self.value.__pos__()\n+        return self.value\n \n     def __abs__(self):\n         return self.value.__abs__()\n@@ -240,14 +240,6 @@ class KerasVariable:\n         value = self.value\n         return value.__rmul__(self._convert_to_tensor(other, dtype=value.dtype))\n \n-    def __div__(self, other):\n-        value = self.value\n-        return value.__div__(self._convert_to_tensor(other, dtype=value.dtype))\n-\n-    def __rdiv__(self, other):\n-        value = self.value\n-        return value.__rdiv__(self._convert_to_tensor(other, dtype=value.dtype))\n-\n     def __truediv__(self, other):\n         value = self.value\n         return value.__truediv__(\n@@ -272,18 +264,6 @@ class KerasVariable:\n             self._convert_to_tensor(other, dtype=value.dtype)\n         )\n \n-    def __divmod__(self, other):\n-        value = self.value\n-        return value.__divmod__(\n-            self._convert_to_tensor(other, dtype=value.dtype)\n-        )\n-\n-    def __rdivmod__(self, other):\n-        value = self.value\n-        return value.__rdivmod__(\n-            self._convert_to_tensor(other, dtype=value.dtype)\n-        )\n-\n     def __mod__(self, other):\n         value = self.value\n         return value.__mod__(self._convert_to_tensor(other, dtype=value.dtype))\n@@ -336,34 +316,6 @@ class KerasVariable:\n         value = self.value\n         return value.__rxor__(self._convert_to_tensor(other, dtype=value.dtype))\n \n-    def __lshift__(self, other):\n-        value = self.value\n-        return value.__lshift__(\n-            self._convert_to_tensor(other, dtype=value.dtype)\n-        )\n-\n-    def __rlshift__(self, other):\n-        value = self.value\n-        return value.__rlshift__(\n-            self._convert_to_tensor(other, dtype=self.dtype)\n-        )\n-\n-    def __rshift__(self, other):\n-        value = self.value\n-        return value.__rshift__(\n-            self._convert_to_tensor(other, dtype=value.dtype)\n-        )\n-\n-    def __rrshift__(self, other):\n-        value = self.value\n-        return value.__rrshift__(\n-            self._convert_to_tensor(other, dtype=self.dtype)\n-        )\n-\n-    def __round__(self, ndigits=None):\n-        value = self.value\n-        return value.__round__(ndigits)\n-\n \n def register_uninitialized_variable(variable):\n     uninitialized_variables = global_state.get_global_attribute(\n\n@@ -1,15 +1,23 @@\n+from unittest.mock import patch\n+\n import numpy as np\n+import pytest\n \n from keras import backend\n from keras import initializers\n from keras.backend.common.variables import AutocastScope\n from keras.backend.common.variables import KerasVariable\n+from keras.backend.common.variables import shape_equal\n+from keras.backend.common.variables import standardize_dtype\n from keras.backend.common.variables import standardize_shape\n from keras.testing import test_case\n \n \n-class VariablesTest(test_case.TestCase):\n+class VariableInitializationTest(test_case.TestCase):\n+    \"\"\"Tests for KerasVariable.__init__()\"\"\"\n+\n     def test_deferred_initialization(self):\n+        \"\"\"Tests deferred initialization of variables.\"\"\"\n         with backend.StatelessScope():\n             v = backend.Variable(\n                 initializer=initializers.RandomNormal(), shape=(2, 2)\n@@ -23,7 +31,54 @@ class VariablesTest(test_case.TestCase):\n             with backend.StatelessScope():\n                 v = backend.Variable(initializer=0)\n \n+    def test_variable_initialization_with_non_callable(self):\n+        \"\"\"Test variable init with non-callable initializer.\"\"\"\n+        v = backend.Variable(initializer=np.ones((2, 2)))\n+        self.assertAllClose(v.value, np.ones((2, 2)))\n+\n+    def test_variable_initialization_with_non_trainable(self):\n+        \"\"\"Test variable initialization with non-trainable flag.\"\"\"\n+        v = backend.Variable(initializer=np.ones((2, 2)), trainable=False)\n+        self.assertFalse(v.trainable)\n+\n+    def test_variable_initialization_without_shape(self):\n+        \"\"\"Test variable init without a shape.\"\"\"\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"When creating a Variable from an initializer, the `shape` \",\n+        ):\n+            backend.Variable(initializer=initializers.RandomNormal())\n+\n+    def test_deferred_initialize_already_initialized(self):\n+        \"\"\"Test deferred init on an already initialized variable.\"\"\"\n+        v = backend.Variable(initializer=np.ones((2, 2)))\n+        with self.assertRaisesRegex(\n+            ValueError, f\"Variable {v.path} is already initialized.\"\n+        ):\n+            v._deferred_initialize()\n+\n+    def test_variable_initialize(self):\n+        \"\"\"Test initializing a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        init_value = np.array([4, 5, 6])\n+        v._initialize(value=init_value)\n+        self.assertAllClose(v.value, init_value)\n+\n+    def test_variable_without_shape_from_callable_initializer(self):\n+        \"\"\"Test that KerasVariable raises error\n+        if shape is not provided for callable initializer.\"\"\"\n+        with self.assertRaisesRegex(\n+            ValueError, \"When creating a Variable from an initializer\"\n+        ):\n+            KerasVariable(initializer=lambda: np.ones((2, 2)))\n+\n+\n+class VariablePropertiesTest(test_case.TestCase):\n+    \"\"\"Tests for KerasVariable._deferred_initialize\n+    KerasVariable._maybe_autocast\"\"\"\n+\n     def test_deferred_assignment(self):\n+        \"\"\"Tests deferred assignment to variables.\"\"\"\n         with backend.StatelessScope() as scope:\n             v = backend.Variable(\n                 initializer=initializers.RandomNormal(), shape=(2, 2)\n@@ -36,6 +91,7 @@ class VariablesTest(test_case.TestCase):\n         self.assertAllClose(out, np.ones((2, 2)))\n \n     def test_trainable_setter(self):\n+        \"\"\"Tests the trainable setter.\"\"\"\n         v = backend.Variable(\n             initializer=initializers.RandomNormal(),\n             shape=(2, 2),\n@@ -51,6 +107,7 @@ class VariablesTest(test_case.TestCase):\n             self.assertFalse(v._value.requires_grad)\n \n     def test_autocasting(self):\n+        \"\"\"Tests autocasting of float variables.\"\"\"\n         v = backend.Variable(\n             initializer=initializers.RandomNormal(),\n             shape=(2, 2),\n@@ -58,7 +115,6 @@ class VariablesTest(test_case.TestCase):\n         )\n         self.assertEqual(v.dtype, \"float32\")\n         self.assertEqual(backend.standardize_dtype(v.value.dtype), \"float32\")\n-\n         print(\"open scope\")\n         with AutocastScope(\"float16\"):\n             self.assertEqual(\n@@ -80,54 +136,46 @@ class VariablesTest(test_case.TestCase):\n             self.assertEqual(backend.standardize_dtype(v.value.dtype), \"int32\")\n \n     def test_standardize_dtype_with_torch_dtype(self):\n+        \"\"\"Tests dtype standardization with PyTorch dtypes.\"\"\"\n         import torch\n \n         x = torch.randn(4, 4)\n         backend.standardize_dtype(x.dtype)\n \n     def test_name_validation(self):\n-        # Test when name is not a string\n+        \"\"\"Tests validation of variable names.\"\"\"\n         with self.assertRaisesRegex(\n             ValueError, \"Argument `name` must be a string\"\n         ):\n             KerasVariable(initializer=initializers.RandomNormal(), name=12345)\n \n-        # Test when name contains a '/'\n         with self.assertRaisesRegex(ValueError, \"cannot contain character `/`\"):\n             KerasVariable(\n                 initializer=initializers.RandomNormal(), name=\"invalid/name\"\n             )\n \n     def test_standardize_shape_with_none(self):\n+        \"\"\"Tests standardizing shape with None.\"\"\"\n         with self.assertRaisesRegex(\n             ValueError, \"Undefined shapes are not supported.\"\n         ):\n             standardize_shape(None)\n \n     def test_standardize_shape_with_non_iterable(self):\n+        \"\"\"Tests shape standardization with non-iterables.\"\"\"\n         with self.assertRaisesRegex(\n             ValueError, \"Cannot convert '42' to a shape.\"\n         ):\n             standardize_shape(42)\n \n     def test_standardize_shape_with_valid_input(self):\n+        \"\"\"Tests standardizing shape with valid input.\"\"\"\n         shape = [3, 4, 5]\n         standardized_shape = standardize_shape(shape)\n         self.assertEqual(standardized_shape, (3, 4, 5))\n \n-    # TODO\n-    # (3.9,torch) FAILED keras/backend/common/variables_test.py\n-    # ::VariablesTest::test_standardize_shape_with_non_integer_entry:\n-    #  - AssertionError \"Cannot convert '\\(3, 4, 'a'\\)' to a shape.\n-    # \" does not match \"invalid literal for int() with base 10: 'a'\"\n-    # def test_standardize_shape_with_non_integer_entry(self):\n-    #     with self.assertRaisesRegex(\n-    #         ValueError,\n-    #         \"Cannot convert '\\\\(3, 4, 'a'\\\\)' to a shape. Found invalid\",\n-    #     ):\n-    #         standardize_shape([3, 4, \"a\"])\n-\n     def test_standardize_shape_with_negative_entry(self):\n+        \"\"\"Tests standardizing shape with negative entries.\"\"\"\n         with self.assertRaisesRegex(\n             ValueError,\n             \"Cannot convert '\\\\(3, 4, -5\\\\)' to a shape. Negative dimensions\",\n@@ -135,8 +183,551 @@ class VariablesTest(test_case.TestCase):\n             standardize_shape([3, 4, -5])\n \n     def test_autocast_scope_with_non_float_dtype(self):\n+        \"\"\"Tests autocast scope with non-float dtype.\"\"\"\n         with self.assertRaisesRegex(\n             ValueError,\n             \"`AutocastScope` can only be used with a floating-point\",\n         ):\n             _ = AutocastScope(\"int32\")\n+\n+    def test_variable_path_creation(self):\n+        \"\"\"Test path creation for a variable.\"\"\"\n+        v = backend.Variable(initializer=np.ones((2, 2)), name=\"test_var\")\n+        self.assertEqual(v.path, \"test_var\")\n+\n+\n+class VariableNumpyValueAndAssignmentTest(test_case.TestCase):\n+    \"\"\"tests for KerasVariable.numpy(), KerasVariable.value()\n+    and KerasVariable.assign()\"\"\"\n+\n+    def test_variable_numpy(self):\n+        \"\"\"Test retrieving the value of a variable as a numpy array.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        self.assertIsInstance(v.numpy(), np.ndarray)\n+        self.assertAllClose(v.numpy(), np.array([1, 2, 3]))\n+\n+    def test_variable_value(self):\n+        \"\"\"Test retrieving the value of a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        self.assertAllClose(v.value, np.array([1, 2, 3]))\n+\n+    def test_variable_assign(self):\n+        \"\"\"Test assigning a new value to a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v.assign(np.array([4, 5, 6]))\n+        self.assertAllClose(v.value, np.array([4, 5, 6]))\n+\n+    def test_variable_assign_add(self):\n+        \"\"\"Test the assign_add method on a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v.assign_add(np.array([1, 1, 1]))\n+        self.assertAllClose(v.value, np.array([2, 3, 4]))\n+\n+    def test_variable_assign_sub(self):\n+        \"\"\"Test the assign_sub method on a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([2, 3, 4]))\n+        v.assign_sub(np.array([1, 1, 1]))\n+        self.assertAllClose(v.value, np.array([1, 2, 3]))\n+\n+    def test_deferred_initialize_within_stateless_scope(self):\n+        \"\"\"Test deferred init within a stateless scope.\"\"\"\n+        with backend.StatelessScope():\n+            v = backend.Variable(\n+                initializer=initializers.RandomNormal(), shape=(2, 2)\n+            )\n+            with self.assertRaisesRegex(\n+                ValueError,\n+                \"You are attempting to initialize a variable \"\n+                \"while in a stateless scope. This is disallowed.\",\n+            ):\n+                v._deferred_initialize()\n+\n+\n+class VariableDtypeShapeNdimRepr(test_case.TestCase):\n+    \"\"\"tests for dtype, shape, ndim, __repr__\"\"\"\n+\n+    def test_variable_dtype(self):\n+        \"\"\"Test retrieving the dtype of a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        self.assertEqual(v.dtype, \"float32\")\n+\n+    def test_variable_shape(self):\n+        \"\"\"Test retrieving the shape of a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([[1, 2], [3, 4]]))\n+        self.assertEqual(v.shape, (2, 2))\n+\n+    def test_variable_ndim(self):\n+        \"\"\"Test retrieving the number of dimensions of a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([[1, 2], [3, 4]]))\n+        self.assertEqual(v.ndim, 2)\n+\n+    def test_variable_repr(self):\n+        \"\"\"Test the string representation of a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2, 3]), name=\"test_var\")\n+        expected_repr = (\n+            \"<KerasVariable shape=(3,), dtype=float32, path=test_var>\"\n+        )\n+        self.assertEqual(repr(v), expected_repr)\n+\n+    def test_variable_getitem(self):\n+        \"\"\"Test getting an item from a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        self.assertEqual(v[0], 1)\n+\n+    def test_variable_initialize(self):\n+        \"\"\"Test initializing a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        init_value = np.array([4, 5, 6])\n+        v._initialize(value=init_value)\n+        self.assertAllClose(v.value, init_value)\n+\n+    def test_variable_convert_to_tensor(self):\n+        \"\"\"Test converting a variable to a tensor.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        self.assertAllClose(v._convert_to_tensor(v.value), np.array([1, 2, 3]))\n+\n+    def test_variable_convert_to_tensor_with_dtype(self):\n+        \"\"\"Test converting a variable to a tensor with a dtype.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        self.assertAllClose(\n+            v._convert_to_tensor(v.value, dtype=\"float32\"), np.array([1, 2, 3])\n+        )\n+\n+    def test_variable_array(self):\n+        \"\"\"Test converting a variable to an array.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        self.assertAllClose(v.__array__(), np.array([1, 2, 3]))\n+\n+\n+class VariableOperationsTest(test_case.TestCase):\n+    \"\"\"Tests for operations on KerasVariable.\"\"\"\n+\n+    def test_variable_as_boolean(self):\n+        \"\"\"Test converting a variable to boolean.\"\"\"\n+        v = backend.Variable(initializer=np.ones((2, 2)))\n+        with self.assertRaisesRegex(\n+            TypeError, \"A Keras Variable cannot be used as a boolean.\"\n+        ):\n+            bool(v)\n+\n+    def test__neg__(self):\n+        \"\"\"Test negating a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([-1, 2]), trainable=False)\n+        self.assertAllClose(v.__neg__(), np.array([1, -2]))\n+\n+    def test__abs__(self):\n+        \"\"\"Test absolute value on a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([-1, 2]), trainable=False)\n+        self.assertAllClose(v.__abs__(), np.array([1, 2]))\n+\n+    def test__invert__(self):\n+        \"\"\"Test bitwise not on a variable.\"\"\"\n+        v = backend.Variable(\n+            initializer=np.array([True, False]), trainable=False, dtype=\"bool\"\n+        )\n+        self.assertAllClose(v.__invert__(), np.array([False, True]))\n+\n+    def test__eq__(self):\n+        \"\"\"Test equality comparison on a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2]), trainable=False)\n+        self.assertAllClose(v.__eq__(np.array([1, 2])), np.array([True, True]))\n+\n+    def test__ne__(self):\n+        \"\"\"Test inequality comparison on a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2]), trainable=False)\n+        self.assertAllClose(\n+            v.__ne__(np.array([1, 2])), np.array([False, False])\n+        )\n+\n+    def test__lt__(self):\n+        \"\"\"Test less than comparison on a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2]), trainable=False)\n+        self.assertAllClose(\n+            v.__lt__(np.array([1, 2])), np.array([False, False])\n+        )\n+\n+    def test__le__(self):\n+        \"\"\"Test less than or equal to comparison on a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2]), trainable=False)\n+        self.assertAllClose(v.__le__(np.array([1, 2])), np.array([True, True]))\n+\n+    def test__gt__(self):\n+        \"\"\"Test greater than comparison on a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2]), trainable=False)\n+        self.assertAllClose(\n+            v.__gt__(np.array([1, 2])), np.array([False, False])\n+        )\n+\n+    def test__ge__(self):\n+        \"\"\"Test greater than or equal to comparison on a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2]), trainable=False)\n+        self.assertAllClose(v.__ge__(np.array([1, 2])), np.array([True, True]))\n+\n+    def test__add__(self):\n+        \"\"\"Test addition operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v2 = backend.Variable(initializer=np.array([4, 5, 6]))\n+        self.assertAllClose(v1.__add__(v2), np.array([5, 7, 9]))\n+\n+    def test__radd__(self):\n+        \"\"\"Test reverse addition operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v2 = backend.Variable(initializer=np.array([4, 5, 6]))\n+        self.assertAllClose(v1.__radd__(v2), np.array([5, 7, 9]))\n+\n+    def test__sub__(self):\n+        \"\"\"Test subtraction operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v2 = backend.Variable(initializer=np.array([4, 5, 6]))\n+        self.assertAllClose(v1.__sub__(v2), np.array([-3, -3, -3]))\n+\n+    def test__rsub__(self):\n+        \"\"\"Test reverse subtraction operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([4, 5, 6]))\n+        v2 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        self.assertAllClose(v1.__rsub__(v2), np.array([-3, -3, -3]))\n+\n+    def test__mul__(self):\n+        \"\"\"Test multiplication operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v2 = backend.Variable(initializer=np.array([4, 5, 6]))\n+        self.assertAllClose(v1.__mul__(v2), np.array([4, 10, 18]))\n+\n+    def test__rmul__(self):\n+        \"\"\"Test reverse multiplication operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v2 = backend.Variable(initializer=np.array([4, 5, 6]))\n+        self.assertAllClose(v1.__rmul__(v2), np.array([4, 10, 18]))\n+\n+    def test__truediv__(self):\n+        \"\"\"Test true division operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v2 = backend.Variable(initializer=np.array([4, 5, 6]))\n+        self.assertAllClose(v1.__truediv__(v2), np.array([0.25, 0.4, 0.5]))\n+\n+    def test__rtruediv__(self):\n+        \"\"\"Test reverse true division operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([4, 5, 6]))\n+        v2 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        self.assertAllClose(v1.__rtruediv__(v2), np.array([0.25, 0.4, 0.5]))\n+\n+    def test__floordiv__(self):\n+        \"\"\"Test floordiv operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v2 = backend.Variable(initializer=np.array([-4, 5, 6]))\n+        self.assertAllClose(v1.__floordiv__(v2), np.array([-1, 0, 0]))\n+\n+    def test__rfloordiv__(self):\n+        \"\"\"Test reverse floordiv operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([-4, 5, 6]))\n+        v2 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        self.assertAllClose(v1.__rfloordiv__(v2), np.array([-1, 0, 0]))\n+\n+    def test__mod__(self):\n+        \"\"\"Test mod operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v2 = backend.Variable(initializer=np.array([-4, 5, 6]))\n+        self.assertAllClose(v1.__mod__(v2), np.array([-3, 2, 3]))\n+\n+    def test__rmod__(self):\n+        \"\"\"Test reverse mod operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v2 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        self.assertAllClose(v1.__rmod__(v2), np.array([0, 0, 0]))\n+\n+    def test__pow__(self):\n+        \"\"\"Test pow operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v2 = backend.Variable(initializer=np.array([-4, 5, 6]))\n+        self.assertAllClose(v1.__pow__(v2), np.array([1, 32, 729]))\n+\n+    def test__rpow__(self):\n+        \"\"\"Test reverse power operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v2 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        self.assertAllClose(v1.__rpow__(v2), np.array([1, 4, 27]))\n+\n+    def test__matmul__(self):\n+        \"\"\"Test matmul operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([[1, 2], [3, 4]]))\n+        v2 = backend.Variable(initializer=np.array([[5, 6], [7, 8]]))\n+        self.assertAllClose(v1.__matmul__(v2), np.array([[19, 22], [43, 50]]))\n+\n+    def test__rmatmul__(self):\n+        \"\"\"Test reverse matmul operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([[1, 2], [3, 4]]))\n+        v2 = backend.Variable(initializer=np.array([[5, 6], [7, 8]]))\n+        self.assertAllClose(v1.__rmatmul__(v2), np.array([[23, 34], [31, 46]]))\n+\n+    def test__and__(self):\n+        \"\"\"Test bitwise and operation on a variable.\"\"\"\n+        v1 = backend.Variable(\n+            initializer=np.array([True, False]), dtype=\"bool\", trainable=False\n+        )\n+        v2 = backend.Variable(\n+            initializer=np.array([True, True]), dtype=\"bool\", trainable=False\n+        )\n+        self.assertAllClose(v1.__and__(v2), np.array([True, False]))\n+\n+    def test__rand__(self):\n+        \"\"\"Test reverse bitwise and operation on a variable.\"\"\"\n+        v1 = backend.Variable(\n+            initializer=np.array([True, False]), dtype=\"bool\", trainable=False\n+        )\n+        v2 = backend.Variable(\n+            initializer=np.array([True, True]), dtype=\"bool\", trainable=False\n+        )\n+        self.assertAllClose(v1.__rand__(v2), np.array([True, False]))\n+\n+    def test__or__(self):\n+        \"\"\"Test bitwise or operation on a variable.\"\"\"\n+        v1 = backend.Variable(\n+            initializer=np.array([True, False]), dtype=\"bool\", trainable=False\n+        )\n+        v2 = backend.Variable(\n+            initializer=np.array([True, True]), dtype=\"bool\", trainable=False\n+        )\n+        self.assertAllClose(v1.__or__(v2), np.array([True, True]))\n+\n+    def test__ror__(self):\n+        \"\"\"Test reverse bitwise or operation on a variable.\"\"\"\n+        v1 = backend.Variable(\n+            initializer=np.array([True, False]), dtype=\"bool\", trainable=False\n+        )\n+        v2 = backend.Variable(\n+            initializer=np.array([True, True]), dtype=\"bool\", trainable=False\n+        )\n+        self.assertAllClose(v1.__ror__(v2), np.array([True, True]))\n+\n+    def test__xor__(self):\n+        \"\"\"Test bitwise xor operation on a variable.\"\"\"\n+        v1 = backend.Variable(\n+            initializer=np.array([True, False]), dtype=\"bool\", trainable=False\n+        )\n+        v2 = backend.Variable(\n+            initializer=np.array([True, True]), dtype=\"bool\", trainable=False\n+        )\n+        self.assertAllClose(v1.__xor__(v2), np.array([False, True]))\n+\n+    def test__rxor__(self):\n+        \"\"\"Test reverse bitwise xor operation on a variable.\"\"\"\n+        v1 = backend.Variable(\n+            initializer=np.array([True, False]), dtype=\"bool\", trainable=False\n+        )\n+        v2 = backend.Variable(\n+            initializer=np.array([True, True]), dtype=\"bool\", trainable=False\n+        )\n+        self.assertAllClose(v1.__rxor__(v2), np.array([False, True]))\n+\n+    def test__pos__(self):\n+        \"\"\"Test unary plus on a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([-1, 2]), trainable=False)\n+        self.assertAllClose(v.__pos__(), np.array([-1, 2]))\n+\n+    def test_variable_pow(self):\n+        \"\"\"Test pow operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v2 = backend.Variable(initializer=np.array([4, 5, 6]))\n+        result = v1**v2\n+        self.assertAllClose(result, np.array([1, 32, 729]))\n+\n+    def test_variable_rpow(self):\n+        \"\"\"Test reverse power operation on a variable.\"\"\"\n+        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v2 = backend.Variable(initializer=np.array([4, 5, 6]))\n+        result = v2**v1\n+        self.assertAllClose(result, np.array([4, 25, 216]))\n+\n+\n+class VariableBinaryOperationsTest(test_case.TestCase):\n+    \"\"\"Tests for binary operations on KerasVariable.\"\"\"\n+\n+    def test_variable_bool(self):\n+        \"\"\"Test converting a variable to boolean.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        with self.assertRaises(TypeError):\n+            bool(v)\n+\n+    def test_variable_neg(self):\n+        \"\"\"Test negating a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([-1, 2]))\n+        neg_v = -v\n+        self.assertAllClose(neg_v, np.array([1, -2]))\n+\n+    def test_variable_abs(self):\n+        \"\"\"Test absolute value of a variable.\"\"\"\n+        v = backend.Variable(initializer=np.array([-1, 2]))\n+        abs_v = abs(v)\n+        self.assertAllClose(abs_v, np.array([1, 2]))\n+\n+    def test_invalid_dtype(self):\n+        \"\"\"Test invalid dtype standardization.\"\"\"\n+        invalid_dtype = \"invalid_dtype\"\n+        with self.assertRaisesRegex(\n+            ValueError, f\"Invalid dtype: {invalid_dtype}\"\n+        ):\n+            standardize_dtype(invalid_dtype)\n+\n+    @patch(\"keras.backend.config.backend\", return_value=\"jax\")\n+    def test_jax_backend_b_dimension(self, mock_backend):\n+        \"\"\"Test 'b' dimension handling with JAX backend.\"\"\"\n+        shape = (3, \"b\", 5)\n+        standardized_shape = standardize_shape(shape)\n+        self.assertEqual(standardized_shape, shape)\n+\n+    def test_negative_shape_entry(self):\n+        \"\"\"Test negative shape entry.\"\"\"\n+        shape = (3, -1, 5)\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Negative dimensions are not allowed\",\n+        ):\n+            standardize_shape(shape)\n+\n+    def test_shape_equal_length_mismatch(self):\n+        \"\"\"Test mismatch in lengths of shapes.\"\"\"\n+        self.assertFalse(shape_equal((3, 2), (3, 2, 4)))\n+        self.assertFalse(shape_equal((), (3,)))\n+        self.assertFalse(shape_equal((3, 2, 4, 5), (3, 2, 4)))\n+\n+\n+@pytest.mark.skipif(\n+    backend.backend() != \"torch\",\n+    reason=\"Tests for standardize_shape with Torch backend\",\n+)\n+class TestStandardizeShapeWithTorch(test_case.TestCase):\n+    \"\"\"Tests for standardize_shape with Torch backend.\"\"\"\n+\n+    def test_standardize_shape_with_torch_size_containing_negative_value(self):\n+        \"\"\"Tests shape with a negative value.\"\"\"\n+        shape_with_negative_value = (3, 4, -5)\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Cannot convert '\\\\(3, 4, -5\\\\)' to a shape. Negative dimensions\",\n+        ):\n+            _ = standardize_shape(shape_with_negative_value)\n+\n+    def test_standardize_shape_with_torch_size_valid(self):\n+        \"\"\"Tests a valid shape.\"\"\"\n+        shape_valid = (3, 4, 5)\n+        standardized_shape = standardize_shape(shape_valid)\n+        self.assertEqual(standardized_shape, (3, 4, 5))\n+\n+    def test_standardize_shape_with_torch_size_multidimensional(self):\n+        \"\"\"Tests shape of a multi-dimensional tensor.\"\"\"\n+        import torch\n+\n+        tensor = torch.randn(3, 4, 5)\n+        shape = tensor.size()\n+        standardized_shape = standardize_shape(shape)\n+        self.assertEqual(standardized_shape, (3, 4, 5))\n+\n+    def test_standardize_shape_with_torch_size_single_dimension(self):\n+        \"\"\"Tests shape of a single-dimensional tensor.\"\"\"\n+        import torch\n+\n+        tensor = torch.randn(10)\n+        shape = tensor.size()\n+        standardized_shape = standardize_shape(shape)\n+        self.assertEqual(standardized_shape, (10,))\n+\n+    def test_standardize_shape_with_torch_size_with_valid_1_dimension(self):\n+        \"\"\"Tests a valid shape.\"\"\"\n+        shape_valid = [3]\n+        standardized_shape = standardize_shape(shape_valid)\n+        self.assertEqual(standardized_shape, (3,))\n+\n+    def test_standardize_shape_with_torch_size_with_valid_2_dimension(self):\n+        \"\"\"Tests a valid shape.\"\"\"\n+        shape_valid = [3, 4]\n+        standardized_shape = standardize_shape(shape_valid)\n+        self.assertEqual(standardized_shape, (3, 4))\n+\n+    def test_standardize_shape_with_torch_size_with_valid_3_dimension(self):\n+        \"\"\"Tests a valid shape.\"\"\"\n+        shape_valid = [3, 4, 5]\n+        standardized_shape = standardize_shape(shape_valid)\n+        self.assertEqual(standardized_shape, (3, 4, 5))\n+\n+    def test_standardize_shape_with_torch_size_with_negative_value(self):\n+        \"\"\"Tests shape with a negative value appended.\"\"\"\n+        import torch\n+\n+        tensor = torch.randn(3, 4, 5)\n+        shape = tuple(tensor.size())\n+        shape_with_negative = shape + (-1,)\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Cannot convert .* to a shape. Negative dimensions are not\",\n+        ):\n+            _ = standardize_shape(shape_with_negative)\n+\n+    def test_standardize_shape_with_non_integer_entry(self):\n+        \"\"\"Tests shape with a non-integer value.\"\"\"\n+        with self.assertRaisesRegex(\n+            # different error message for torch\n+            ValueError,\n+            r\"invalid literal for int\\(\\) with base 10: 'a'\",\n+        ):\n+            standardize_shape([3, 4, \"a\"])\n+\n+    def test_standardize_shape_with_negative_entry(self):\n+        \"\"\"Tests shape with a negative value.\"\"\"\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Cannot convert '\\\\(3, 4, -5\\\\)' to a shape. Negative dimensions\",\n+        ):\n+            standardize_shape([3, 4, -5])\n+\n+    def test_standardize_shape_with_valid_not_tuple(self):\n+        \"\"\"Tests a valid shape.\"\"\"\n+        shape_valid = [3, 4, 5]\n+        standardized_shape = standardize_shape(shape_valid)\n+        self.assertEqual(standardized_shape, (3, 4, 5))\n+\n+\n+@pytest.mark.skipif(\n+    backend.backend() == \"torch\",\n+    reason=\"Tests for standardize_shape with others backend\",\n+)\n+class TestStandardizeShapeWithOutTorch(test_case.TestCase):\n+    \"\"\"Tests for standardize_shape with others backend.\"\"\"\n+\n+    def test_standardize_shape_with_out_torch_negative_value(self):\n+        \"\"\"Tests shape with a negative value.\"\"\"\n+        shape_with_negative_value = (3, 4, -5)\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Cannot convert '\\\\(3, 4, -5\\\\)' to a shape. Negative dimensions\",\n+        ):\n+            _ = standardize_shape(shape_with_negative_value)\n+\n+    def test_standardize_shape_with_out_torch_string(self):\n+        \"\"\"Tests shape with a string value.\"\"\"\n+        shape_with_string = (3, 4, \"5\")\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Cannot convert .* to a shape. Found invalid entry '5'.\",\n+        ):\n+            _ = standardize_shape(shape_with_string)\n+\n+    def test_standardize_shape_with_out_torch_float(self):\n+        \"\"\"Tests shape with a float value.\"\"\"\n+        shape_with_float = (3, 4, 5.0)\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Cannot convert .* to a shape. Found invalid entry '5.0'.\",\n+        ):\n+            _ = standardize_shape(shape_with_float)\n+\n+    def test_standardize_shape_with_out_torch_valid(self):\n+        \"\"\"Tests a valid shape.\"\"\"\n+        shape_valid = (3, 4, 5)\n+        standardized_shape = standardize_shape(shape_valid)\n+        self.assertEqual(standardized_shape, (3, 4, 5))\n+\n+    def test_standardize_shape_with_out_torch_valid_not_tuple(self):\n+        \"\"\"Tests a valid shape.\"\"\"\n+        shape_valid = [3, 4, 5]\n+        standardized_shape = standardize_shape(shape_valid)\n+        self.assertEqual(standardized_shape, (3, 4, 5))\n\n@@ -0,0 +1,132 @@\n+from keras.mixed_precision import DTypePolicy\n+from keras.mixed_precision import dtype_policy\n+from keras.mixed_precision import set_dtype_policy\n+from keras.testing import test_case\n+\n+\n+class DTypePolicyTest(test_case.TestCase):\n+    def test_initialization_valid_name(self):\n+        \"\"\"Test initialization with a valid name.\"\"\"\n+        policy = DTypePolicy(\"mixed_float16\")\n+        self.assertEqual(policy.compute_dtype, \"float16\")\n+        self.assertEqual(policy.variable_dtype, \"float32\")\n+\n+    def test_initialization_invalid_name(self):\n+        \"\"\"Test initialization with an invalid name.\"\"\"\n+        with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n+            DTypePolicy(\"invalid_name\")\n+\n+    def test_initialization_non_string_name(self):\n+        \"\"\"Test initialization with a non-string name.\"\"\"\n+        with self.assertRaisesRegex(TypeError, \"'name' must be a string\"):\n+            DTypePolicy(123)\n+\n+    def test_properties_mixed_float16(self):\n+        \"\"\"Test properties for 'mixed_float16'.\"\"\"\n+        policy = DTypePolicy(\"mixed_float16\")\n+        self.assertEqual(policy.compute_dtype, \"float16\")\n+        self.assertEqual(policy.variable_dtype, \"float32\")\n+\n+    def test_properties_mixed_bfloat16(self):\n+        \"\"\"Test properties for 'mixed_bfloat16'.\"\"\"\n+        policy = DTypePolicy(\"mixed_bfloat16\")\n+        self.assertEqual(policy.compute_dtype, \"bfloat16\")\n+        self.assertEqual(policy.variable_dtype, \"float32\")\n+\n+    def test_initialization_with_invalid_name_behaviour(self):\n+        \"\"\"Test initialization behavior with an invalid name.\"\"\"\n+        with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n+            DTypePolicy(\"invalid_name\")\n+\n+    def test_properties(self):\n+        \"\"\"Test variable_dtype, compute_dtype, and name properties.\"\"\"\n+        policy = DTypePolicy(\"mixed_float16\")\n+        self.assertEqual(policy.variable_dtype, \"float32\")\n+        self.assertEqual(policy.compute_dtype, \"float16\")\n+        self.assertEqual(policy.name, \"mixed_float16\")\n+\n+    def test_repr(self):\n+        \"\"\"Test __repr__ method.\"\"\"\n+        policy = DTypePolicy(\"mixed_float16\")\n+        self.assertEqual(repr(policy), '<DTypePolicy \"mixed_float16\">')\n+\n+    def test_get_config_from_config(self):\n+        \"\"\"Test get_config and from_config methods.\"\"\"\n+        policy = DTypePolicy(\"mixed_float16\")\n+        config = policy.get_config()\n+        self.assertEqual(config, {\"name\": \"mixed_float16\"})\n+\n+        new_policy = DTypePolicy.from_config(config)\n+        self.assertEqual(new_policy.name, \"mixed_float16\")\n+\n+\n+class DTypePolicyGlobalFunctionsTest(test_case.TestCase):\n+    def setUp(self):\n+        \"\"\"Reset the global dtype policy before each test.\"\"\"\n+        set_dtype_policy(\"float32\")\n+\n+    def test_set_dtype_policy_valid_string(self):\n+        \"\"\"Test set_dtype_policy with a valid string.\"\"\"\n+        set_dtype_policy(\"mixed_float16\")\n+        policy = dtype_policy()\n+        self.assertEqual(policy.name, \"mixed_float16\")\n+\n+    def test_set_dtype_policy_valid_policy(self):\n+        \"\"\"Test set_dtype_policy with a valid DTypePolicy object.\"\"\"\n+        policy_obj = DTypePolicy(\"mixed_float16\")\n+        set_dtype_policy(policy_obj)\n+        policy = dtype_policy()\n+        self.assertEqual(policy.name, \"mixed_float16\")\n+\n+    def test_set_dtype_policy_invalid(self):\n+        \"\"\"Test set_dtype_policy with an invalid input.\"\"\"\n+        with self.assertRaisesRegex(ValueError, \"Invalid `policy` argument\"):\n+            set_dtype_policy(12345)\n+\n+    def test_dtype_policy_default(self):\n+        \"\"\"Test dtype_policy default value.\"\"\"\n+        policy = dtype_policy()\n+        self.assertEqual(policy.name, \"float32\")\n+\n+\n+class DTypePolicyEdgeCasesTest(test_case.TestCase):\n+    def test_empty_name(self):\n+        \"\"\"Test initialization with an empty name.\"\"\"\n+        with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n+            DTypePolicy(\"\")\n+\n+    def test_special_character_name(self):\n+        \"\"\"Test initialization with special characters in the name.\"\"\"\n+        with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n+            DTypePolicy(\"@mixed_float16!\")\n+\n+    def test_very_long_name(self):\n+        \"\"\"Test initialization with a very long name.\"\"\"\n+        with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n+            DTypePolicy(\"mixed_float16\" * 100)\n+\n+    def test_almost_valid_name(self):\n+        \"\"\"Test initialization with a name close to a valid one.\"\"\"\n+        with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n+            DTypePolicy(\"mixed_float15\")\n+\n+\n+class DTypePolicyGlobalFunctionsEdgeCasesTest(test_case.TestCase):\n+    def setUp(self):\n+        \"\"\"Reset the global dtype policy before each test.\"\"\"\n+        set_dtype_policy(\"float32\")\n+\n+    def test_set_policy_multiple_times(self):\n+        \"\"\"Test setting the policy multiple times in a row.\"\"\"\n+        set_dtype_policy(\"mixed_float16\")\n+        policy = dtype_policy()\n+        self.assertEqual(policy.name, \"mixed_float16\")\n+\n+        set_dtype_policy(\"float32\")\n+        policy = dtype_policy()\n+        self.assertEqual(policy.name, \"float32\")\n+\n+    def test_set_policy_none(self):\n+        \"\"\"Test setting the policy to None.\"\"\"\n+        with self.assertRaisesRegex(ValueError, \"Invalid `policy` argument\"):\n+            set_dtype_policy(None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e82c68ecdb4eabbf3332af87d12bb2d9153811c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 136 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -108,9 +108,10 @@ def raise_unsupported_arg(arg_name, arg_description, input_type):\n def is_tf_dataset(x):\n     if hasattr(x, \"__class__\"):\n         for parent in x.__class__.__mro__:\n-            if parent.__name__ == \"DatasetV2\" and str(\n-                parent.__module__\n-            ).startswith(\"tensorflow.python.types.data\"):\n+            if (\n+                parent.__name__ == \"DatasetV2\"\n+                and \"tensorflow.python.types.data\" in str(parent.__module__)\n+            ):\n                 return True\n     return False\n \n@@ -118,8 +119,8 @@ def is_tf_dataset(x):\n def is_torch_dataloader(x):\n     if hasattr(x, \"__class__\"):\n         for parent in x.__class__.__mro__:\n-            if parent.__name__ == \"DataLoader\" and str(\n+            if parent.__name__ == \"DataLoader\" and \"torch.utils.data\" in str(\n                 parent.__module__\n-            ).startswith(\"torch.utils.data\"):\n+            ):\n                 return True\n     return False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1fb14e178c881b328045e74fae9d5b00bcc58714", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 33 | Methods Changed: 11 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 72 | Churn Cumulative: 278 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -592,7 +592,7 @@ class MaxPool(Operation):\n         super().__init__()\n         self.pool_size = pool_size\n         self.strides = strides\n-        self.padding = padding\n+        self.padding = padding.lower()\n         self.data_format = data_format\n \n     def call(self, inputs):\n@@ -656,6 +656,7 @@ def max_pool(\n         A tensor of rank N+2, the result of the max pooling operation.\n     \"\"\"\n     data_format = standardize_data_format(data_format)\n+    padding = padding.lower()\n     if any_symbolic_tensors((inputs,)):\n         return MaxPool(\n             pool_size,\n@@ -677,7 +678,7 @@ class AveragePool(Operation):\n         super().__init__()\n         self.pool_size = pool_size\n         self.strides = strides\n-        self.padding = padding\n+        self.padding = padding.lower()\n         self.data_format = data_format\n \n     def call(self, inputs):\n@@ -746,6 +747,7 @@ def average_pool(\n         A tensor of rank N+2, the result of the average pooling operation.\n     \"\"\"\n     data_format = standardize_data_format(data_format)\n+    padding = padding.lower()\n     if any_symbolic_tensors((inputs,)):\n         return AveragePool(\n             pool_size,\n@@ -768,7 +770,7 @@ class Conv(Operation):\n     ):\n         super().__init__()\n         self.strides = strides\n-        self.padding = padding\n+        self.padding = padding.lower()\n         self.data_format = data_format\n         self.dilation_rate = dilation_rate\n \n@@ -841,6 +843,7 @@ def conv(\n         A tensor of rank N+2, the result of the conv operation.\n     \"\"\"\n     data_format = standardize_data_format(data_format)\n+    padding = padding.lower()\n     if any_symbolic_tensors((inputs,)):\n         return Conv(strides, padding, data_format, dilation_rate).symbolic_call(\n             inputs, kernel\n@@ -860,7 +863,7 @@ class DepthwiseConv(Operation):\n     ):\n         super().__init__()\n         self.strides = strides\n-        self.padding = padding\n+        self.padding = padding.lower()\n         self.data_format = data_format\n         self.dilation_rate = dilation_rate\n \n@@ -938,6 +941,7 @@ def depthwise_conv(\n         A tensor of rank N+2, the result of the depthwise conv operation.\n     \"\"\"\n     data_format = standardize_data_format(data_format)\n+    padding = padding.lower()\n     if any_symbolic_tensors((inputs,)):\n         return DepthwiseConv(\n             strides, padding, data_format, dilation_rate\n@@ -962,7 +966,7 @@ class SeparableConv(Operation):\n     ):\n         super().__init__()\n         self.strides = strides\n-        self.padding = padding\n+        self.padding = padding.lower()\n         self.data_format = data_format\n         self.dilation_rate = dilation_rate\n \n@@ -1051,6 +1055,7 @@ def separable_conv(\n         A tensor of rank N+2, the result of the depthwise conv operation.\n     \"\"\"\n     data_format = standardize_data_format(data_format)\n+    padding = padding.lower()\n     if any_symbolic_tensors((inputs,)):\n         return SeparableConv(\n             strides,\n@@ -1081,7 +1086,7 @@ class ConvTranspose(Operation):\n         super().__init__()\n         self.strides = strides\n         self.output_padding = output_padding\n-        self.padding = padding\n+        self.padding = padding.lower()\n         self.data_format = data_format\n         self.dilation_rate = dilation_rate\n \n@@ -1175,6 +1180,7 @@ def conv_transpose(\n         A tensor of rank N+2, the result of the conv operation.\n     \"\"\"\n     data_format = standardize_data_format(data_format)\n+    padding = padding.lower()\n     if any_symbolic_tensors((inputs,)):\n         return ConvTranspose(\n             strides, padding, output_padding, data_format, dilation_rate\n\n@@ -121,11 +121,15 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         # Test 1D conv.\n         inputs_1d = KerasTensor([None, 20, 3])\n         kernel = KerasTensor([4, 3, 2])\n+        for padding in [\"valid\", \"VALID\"]:\n             self.assertEqual(\n-            knn.conv(inputs_1d, kernel, 1, padding=\"valid\").shape, (None, 17, 2)\n+                knn.conv(inputs_1d, kernel, 1, padding=padding).shape,\n+                (None, 17, 2),\n             )\n+        for padding in [\"same\", \"SAME\"]:\n             self.assertEqual(\n-            knn.conv(inputs_1d, kernel, 1, padding=\"same\").shape, (None, 20, 2)\n+                knn.conv(inputs_1d, kernel, 1, padding=padding).shape,\n+                (None, 20, 2),\n             )\n         self.assertEqual(\n             knn.conv(inputs_1d, kernel, (2,), dilation_rate=2).shape,\n@@ -135,12 +139,14 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         # Test 2D conv.\n         inputs_2d = KerasTensor([None, 10, None, 3])\n         kernel = KerasTensor([2, 2, 3, 2])\n+        for padding in [\"valid\", \"VALID\"]:\n             self.assertEqual(\n-            knn.conv(inputs_2d, kernel, 1, padding=\"valid\").shape,\n+                knn.conv(inputs_2d, kernel, 1, padding=padding).shape,\n                 (None, 9, None, 2),\n             )\n+        for padding in [\"same\", \"SAME\"]:\n             self.assertEqual(\n-            knn.conv(inputs_2d, kernel, 1, padding=\"same\").shape,\n+                knn.conv(inputs_2d, kernel, 1, padding=padding).shape,\n                 (None, 10, None, 2),\n             )\n         self.assertEqual(\n@@ -148,15 +154,35 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n             (None, 4, None, 2),\n         )\n \n+        # Test 2D conv - H, W specified\n+        inputs_2d = KerasTensor([None, 10, 10, 3])\n+        kernel = KerasTensor([2, 2, 3, 2])\n+        for padding in [\"valid\", \"VALID\"]:\n+            self.assertEqual(\n+                knn.conv(inputs_2d, kernel, 1, padding=padding).shape,\n+                (None, 9, 9, 2),\n+            )\n+        for padding in [\"same\", \"SAME\"]:\n+            self.assertEqual(\n+                knn.conv(inputs_2d, kernel, 1, padding=padding).shape,\n+                (None, 10, 10, 2),\n+            )\n+        self.assertEqual(\n+            knn.conv(inputs_2d, kernel, (2, 1), dilation_rate=(2, 1)).shape,\n+            (None, 4, 9, 2),\n+        )\n+\n         # Test 3D conv.\n         inputs_3d = KerasTensor([None, 8, None, 8, 3])\n         kernel = KerasTensor([3, 3, 3, 3, 2])\n+        for padding in [\"valid\", \"VALID\"]:\n             self.assertEqual(\n-            knn.conv(inputs_3d, kernel, 1, padding=\"valid\").shape,\n+                knn.conv(inputs_3d, kernel, 1, padding=padding).shape,\n                 (None, 6, None, 6, 2),\n             )\n+        for padding in [\"same\", \"SAME\"]:\n             self.assertEqual(\n-            knn.conv(inputs_3d, kernel, (2, 1, 2), padding=\"same\").shape,\n+                knn.conv(inputs_3d, kernel, (2, 1, 2), padding=padding).shape,\n                 (None, 4, None, 4, 2),\n             )\n         self.assertEqual(\n@@ -170,12 +196,16 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         # Test 1D depthwise conv.\n         inputs_1d = KerasTensor([None, 20, 3])\n         kernel = KerasTensor([4, 3, 1])\n+        for padding in [\"valid\", \"VALID\"]:\n             self.assertEqual(\n-            knn.depthwise_conv(inputs_1d, kernel, 1, padding=\"valid\").shape,\n+                knn.depthwise_conv(inputs_1d, kernel, 1, padding=padding).shape,\n                 (None, 17, 3),\n             )\n+        for padding in [\"same\", \"SAME\"]:\n             self.assertEqual(\n-            knn.depthwise_conv(inputs_1d, kernel, (1,), padding=\"same\").shape,\n+                knn.depthwise_conv(\n+                    inputs_1d, kernel, (1,), padding=padding\n+                ).shape,\n                 (None, 20, 3),\n             )\n         self.assertEqual(\n@@ -186,12 +216,16 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         # Test 2D depthwise conv.\n         inputs_2d = KerasTensor([None, 10, 10, 3])\n         kernel = KerasTensor([2, 2, 3, 1])\n+        for padding in [\"valid\", \"VALID\"]:\n             self.assertEqual(\n-            knn.depthwise_conv(inputs_2d, kernel, 1, padding=\"valid\").shape,\n+                knn.depthwise_conv(inputs_2d, kernel, 1, padding=padding).shape,\n                 (None, 9, 9, 3),\n             )\n+        for padding in [\"same\", \"SAME\"]:\n             self.assertEqual(\n-            knn.depthwise_conv(inputs_2d, kernel, (1, 2), padding=\"same\").shape,\n+                knn.depthwise_conv(\n+                    inputs_2d, kernel, (1, 2), padding=padding\n+                ).shape,\n                 (None, 10, 5, 3),\n             )\n         self.assertEqual(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
