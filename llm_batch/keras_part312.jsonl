{"custom_id": "keras#e94a6641dccda6dd7f8ad86fad82e62388548777", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 68 | Churn Cumulative: 1076 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -113,7 +113,7 @@ class Initializer(object):\n \n \n @keras_export('keras.initializers.Zeros', 'keras.initializers.zeros', v1=[])\n-class Zeros(tf.zeros_initializer, Initializer):\n+class Zeros(Initializer):\n   \"\"\"Initializer that generates tensors initialized to 0.\n \n   Also available via the shortcut function `tf.keras.initializers.zeros`.\n@@ -140,11 +140,17 @@ class Zeros(tf.zeros_initializer, Initializer):\n        (via `tf.keras.backend.set_floatx(float_dtype)`).\n       **kwargs: Additional keyword arguments.\n     \"\"\"\n-    return super(Zeros, self).__call__(shape, dtype=_get_dtype(dtype), **kwargs)\n+    _validate_kwargs(self.__class__.__name__, kwargs)\n+    dtype = _get_dtype(dtype)\n+    if not dtype.is_numpy_compatible or dtype == tf.string:\n+      raise ValueError('Expected numeric or boolean dtype, got %s.' % dtype)\n+    if _PARTITION_SHAPE in kwargs:\n+      shape = kwargs[_PARTITION_SHAPE]\n+    return tf.zeros(shape, dtype)\n \n \n @keras_export('keras.initializers.Ones', 'keras.initializers.ones', v1=[])\n-class Ones(tf.ones_initializer, Initializer):\n+class Ones(Initializer):\n   \"\"\"Initializer that generates tensors initialized to 1.\n \n   Also available via the shortcut function `tf.keras.initializers.ones`.\n@@ -171,7 +177,13 @@ class Ones(tf.ones_initializer, Initializer):\n        (via `tf.keras.backend.set_floatx(float_dtype)`).\n       **kwargs: Additional keyword arguments.\n     \"\"\"\n-    return super(Ones, self).__call__(shape, dtype=_get_dtype(dtype), **kwargs)\n+    _validate_kwargs(self.__class__.__name__, kwargs)\n+    dtype = _get_dtype(dtype)\n+    if not dtype.is_numpy_compatible or dtype == tf.string:\n+      raise ValueError('Expected numeric or boolean dtype, got %s.' % dtype)\n+    if _PARTITION_SHAPE in kwargs:\n+      shape = kwargs[_PARTITION_SHAPE]\n+    return tf.ones(shape, dtype)\n \n \n @keras_export('keras.initializers.Constant',\n@@ -225,7 +237,7 @@ class Constant(Initializer):\n @keras_export('keras.initializers.RandomUniform',\n               'keras.initializers.random_uniform',\n               v1=[])\n-class RandomUniform(tf.random_uniform_initializer, Initializer):\n+class RandomUniform(Initializer):\n   \"\"\"Initializer that generates tensors with a uniform distribution.\n \n   Also available via the shortcut function\n@@ -250,6 +262,12 @@ class RandomUniform(tf.random_uniform_initializer, Initializer):\n       always produce the same random tensor for a given shape and dtype.\n   \"\"\"\n \n+  def __init__(self, minval=-0.05, maxval=0.05, seed=None):\n+    self.minval = minval\n+    self.maxval = maxval\n+    self.seed = seed\n+    self._random_generator = _RandomGenerator(seed)\n+\n   def __call__(self, shape, dtype=None, **kwargs):\n     \"\"\"Returns a tensor object initialized as specified by the initializer.\n \n@@ -262,14 +280,27 @@ class RandomUniform(tf.random_uniform_initializer, Initializer):\n        (via `tf.keras.backend.set_floatx(float_dtype)`).\n       **kwargs: Additional keyword arguments.\n     \"\"\"\n-    return super(RandomUniform, self).__call__(\n-        shape, dtype=_get_dtype(dtype), **kwargs)\n+    _validate_kwargs(self.__class__.__name__, kwargs)\n+    dtype = _get_dtype(dtype)\n+    if not dtype.is_floating and not dtype.is_integer:\n+      raise ValueError('Expected float or integer dtype, got %s.' % dtype)\n+    if _PARTITION_SHAPE in kwargs:\n+      shape = kwargs[_PARTITION_SHAPE]\n+    return self._random_generator.random_uniform(shape, self.minval,\n+                                                 self.maxval, dtype)\n+\n+  def get_config(self):\n+    return {\n+        'minval': self.minval,\n+        'maxval': self.maxval,\n+        'seed': self.seed\n+    }\n \n \n @keras_export('keras.initializers.RandomNormal',\n               'keras.initializers.random_normal',\n               v1=[])\n-class RandomNormal(tf.random_normal_initializer, Initializer):\n+class RandomNormal(Initializer):\n   \"\"\"Initializer that generates tensors with a normal distribution.\n \n   Also available via the shortcut function\n@@ -294,6 +325,12 @@ class RandomNormal(tf.random_normal_initializer, Initializer):\n       always produce the same random tensor for a given shape and dtype.\n   \"\"\"\n \n+  def __init__(self, mean=0.0, stddev=0.05, seed=None):\n+    self.mean = mean\n+    self.stddev = stddev\n+    self.seed = seed\n+    self._random_generator = _RandomGenerator(seed)\n+\n   def __call__(self, shape, dtype=None, **kwargs):\n     \"\"\"Returns a tensor object initialized to random normal values.\n \n@@ -305,8 +342,19 @@ class RandomNormal(tf.random_normal_initializer, Initializer):\n         `tf.keras.backend.set_floatx(float_dtype)`)\n       **kwargs: Additional keyword arguments.\n     \"\"\"\n-    return super(RandomNormal, self).__call__(\n-        shape, dtype=_get_dtype(dtype), **kwargs)\n+    _validate_kwargs(self.__class__.__name__, kwargs)\n+    dtype = _assert_float_dtype(_get_dtype(dtype))\n+    if _PARTITION_SHAPE in kwargs:\n+      shape = kwargs[_PARTITION_SHAPE]\n+    return self._random_generator.random_normal(shape, self.mean, self.stddev,\n+                                                dtype)\n+\n+  def get_config(self):\n+    return {\n+        'mean': self.mean,\n+        'stddev': self.stddev,\n+        'seed': self.seed\n+    }\n \n \n @keras_export('keras.initializers.TruncatedNormal',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bf464d7ed9283988fea1f548a11a0171fd2ff364", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 18 | Churn Cumulative: 143 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -92,21 +92,27 @@ def should_use_v2():\n         graph.name.startswith(\"wrapped_function\")):\n       return False\n     return True\n+  else:\n+    return False\n \n \n def swap_class(cls, v2_cls, v1_cls, use_v2):\n   \"\"\"Swaps in v2_cls or v1_cls depending on graph mode.\"\"\"\n   if cls == object:\n     return cls\n-\n   if cls in (v2_cls, v1_cls):\n-    if use_v2:\n-      return v2_cls\n-    return v1_cls\n+    return v2_cls if use_v2 else v1_cls\n \n   # Recursively search superclasses to swap in the right Keras class.\n-  cls.__bases__ = tuple(\n-      swap_class(base, v2_cls, v1_cls, use_v2) for base in cls.__bases__)\n+  new_bases = []\n+  for base in cls.__bases__:\n+    if ((use_v2 and issubclass(base, v1_cls)\n+         or (not use_v2 and issubclass(base, v2_cls)))):\n+      new_base = swap_class(base, v2_cls, v1_cls, use_v2)\n+    else:\n+      new_base = base\n+    new_bases.append(new_base)\n+  cls.__bases__ = tuple(new_bases)\n   return cls\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#36916af601ebd387f6ebee007b6f8c99a7605a37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 528 | Lines Deleted: 60 | Files Changed: 21 | Hunks: 79 | Methods Changed: 52 | Complexity Δ (Sum/Max): 67/24 | Churn Δ: 588 | Churn Cumulative: 33209 | Contributors (this commit): 107 | Commits (past 90d): 63 | Contributors (cumulative): 159 | DMM Complexity: 0.8283828382838284\n\nDIFF:\n@@ -1304,7 +1304,6 @@ def placeholder(shape=None,\n     if sparse:\n       spec = tf.SparseTensorSpec(\n           shape=shape, dtype=dtype)\n-      x = keras_tensor.SparseKerasTensor(spec, name=name)\n     elif ragged:\n       ragged_rank = 0\n       for i in range(1, len(shape)):\n@@ -1316,12 +1315,10 @@ def placeholder(shape=None,\n           ragged_rank = i\n       spec = tf.RaggedTensorSpec(\n           shape=shape, dtype=dtype, ragged_rank=ragged_rank)\n-\n-      x = keras_tensor.RaggedKerasTensor(spec, name=name)\n     else:\n       spec = tf.TensorSpec(\n           shape=shape, dtype=dtype, name=name)\n-      x = keras_tensor.KerasTensor(spec, name=name)\n+    x = keras_tensor.keras_tensor_from_type_spec(spec, name=name)\n   else:\n     with get_graph().as_default():\n       if sparse:\n\n@@ -2350,7 +2350,8 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     self._pop_writer()\n \n   def _implements_train_batch_hooks(self):\n-    return self._should_trace  # Only call batch hooks when tracing is enabled\n+    # Only call batch hooks when tracing or write_steps_per_second are enabled\n+    return self._should_trace or self.write_steps_per_second\n \n   def on_train_batch_begin(self, batch, logs=None):\n     self._global_train_batch += 1\n\n@@ -2026,9 +2026,13 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n         y,\n         batch_size=2,\n         epochs=2,\n+        verbose=0,\n         callbacks=[\n             keras.callbacks.TensorBoard(\n-                self.logdir, update_freq=1, write_steps_per_second=True)\n+                self.logdir,\n+                update_freq=1,\n+                profile_batch=0,\n+                write_steps_per_second=True)\n         ])\n \n     summary_file = list_summaries(self.logdir)\n\n@@ -146,14 +146,20 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     depend on the shape(s) of the input(s), using `add_weight()`. `__call__()`\n     will automatically build the layer (if it has not been built yet) by\n     calling `build()`.\n-  * `call(self, *args, **kwargs)`: Called in `__call__` after making sure\n-    `build()` has been called. `call()` performs the logic of applying the\n+  * `call(self, inputs, *args, **kwargs)`: Called in `__call__` after making\n+    sure `build()` has been called. `call()` performs the logic of applying the\n     layer to the input tensors (which should be passed in as argument).\n     Two reserved keyword arguments you can optionally use in `call()` are:\n-      - `training` (boolean, whether the call is in\n-        inference mode or training mode)\n+      - `training` (boolean, whether the call is in inference mode or training\n+        mode). See more details in [the layer/model subclassing guide](\n+        https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_training_argument_in_the_call_method)\n       - `mask` (boolean tensor encoding masked timesteps in the input, used\n-        in RNN layers)\n+        in RNN layers). See more details in [the layer/model subclassing guide](\n+        https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_mask_argument_in_the_call_method)\n+    A typical signature for this method is `call(self, inputs)`, and user could\n+    optionally add `training` and `mask` if the layer need them. `*args` and\n+    `**kwargs` is only useful for future extension when more input parameters\n+    are planned to be added.\n   * `get_config(self)`: Returns a dictionary containing the configuration used\n     to initialize this layer. If the keys differ from the arguments\n     in `__init__`, then override `from_config(self)` as well.\n@@ -449,7 +455,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     self.built = True\n \n   @doc_controls.for_subclass_implementers\n-  def call(self, inputs, **kwargs):  # pylint: disable=unused-argument\n+  def call(self, inputs, *args, **kwargs):  # pylint: disable=unused-argument\n     \"\"\"This is where the layer's logic lives.\n \n     Note here that `call()` method in `tf.keras` is little bit different\n@@ -459,6 +465,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n     Args:\n         inputs: Input tensor, or list/tuple of input tensors.\n+        *args: Additional positional arguments. Currently unused.\n         **kwargs: Additional keyword arguments. Currently unused.\n \n     Returns:\n\n@@ -26,6 +26,7 @@ from keras import losses as losses_mod\n from keras import metrics as metrics_mod\n from keras.utils import generic_utils\n from keras.utils import losses_utils\n+from keras.utils import tf_utils\n \n \n class Container(object):\n@@ -206,7 +207,11 @@ class LossesContainer(Container):\n         loss_metric_value *= tf.distribute.get_strategy().num_replicas_in_sync\n \n       if batch_dim is None:\n+        if tf_utils.is_ragged(y_t):\n+          batch_dim = y_t.nrows()\n+        else:\n           batch_dim = tf.compat.v1.shape(y_t)[0]\n+\n       if metric_obj is not None:\n         metric_obj.update_state(loss_metric_value, sample_weight=batch_dim)\n \n\n@@ -350,6 +350,35 @@ class LossesContainerTest(keras_parameterized.TestCase):\n     self.assertEqual(loss_container._losses[0].name, 'custom_loss_fn')\n     self.assertEqual(loss_container._losses[1].name, 'custom_loss_class')\n \n+  def test_ragged_tensor_output(self):\n+    \"\"\" Ensure that ragged tensors can be passed as targets and predictions.\"\"\"\n+\n+    def custom_loss_fn(y_true, y_pred):\n+      losses = tf.ragged.map_flat_values(losses_mod.mse, y_true,\n+                                                     y_pred)\n+      return tf.reduce_mean(losses)\n+\n+    class CustomLossClass(object):\n+\n+      def __call__(self, y_true, y_pred):\n+        losses = tf.ragged.map_flat_values(losses_mod.mse, y_true,\n+                                                       y_pred)\n+        return tf.reduce_mean(losses)\n+\n+    loss_container = compile_utils.LossesContainer(\n+        [custom_loss_fn, CustomLossClass()])\n+\n+    v_t = tf.constant([[3., 4.], [1., 2.], [3., 5.]])\n+    v_p = tf.constant([[3.1, 4.], [1., 2.], [3., 5.]])\n+\n+    y_t = tf.compat.v1.expand_dims(\n+        tf.RaggedTensor.from_row_splits(v_t, [0, 2, 3]), 0)\n+    y_p = tf.compat.v1.expand_dims(\n+        tf.RaggedTensor.from_row_splits(v_p, [0, 2, 3]), 0)\n+    loss_container(y_t, y_p)\n+\n+    self.assertEqual(loss_container._losses[0].name, 'custom_loss_fn')\n+\n \n class MetricsContainerTest(keras_parameterized.TestCase):\n \n\n@@ -639,8 +639,11 @@ class Functional(training_lib.Model):\n       # Dtype casting.\n       tensor = tf.cast(tensor, dtype=ref_input.dtype)\n     elif tf_utils.is_extension_type(tensor):\n-      # Dtype casting.\n-      tensor = tf.cast(tensor, dtype=ref_input.dtype)\n+      # Dtype casting (If the extension type has a non-variant dtype and\n+      # supports being cast)\n+      ref_input_dtype = getattr(ref_input, 'dtype', None)\n+      if ref_input_dtype is not None and ref_input_dtype != tf.variant:\n+        tensor = tf.cast(tensor, dtype=ref_input_dtype)\n \n     return tensor\n \n@@ -1085,10 +1088,13 @@ def _should_skip_first_node(layer):\n   # Networks that are constructed with an Input layer/shape start with a\n   # pre-existing node linking their input to output. This node is excluded from\n   # the network config.\n+  if layer._self_tracked_trackables:\n     return (isinstance(layer, Functional) and\n             # Filter out Sequential models without an input shape.\n             isinstance(layer._self_tracked_trackables[0],\n                        input_layer_module.InputLayer))\n+  else:\n+    return isinstance(layer, Functional)\n \n \n def connect_ancillary_layers(model, created_layers):\n\n@@ -30,6 +30,13 @@ from keras.utils import tf_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n+def _assert_other_arg_none(arg_name, arg):\n+  if arg is not None:\n+    raise ValueError('When `type_spec` is not None, all other args '\n+                     'except `name` must be None, '\n+                     'but %s is not None.' % arg_name)\n+\n+\n @keras_export('keras.layers.InputLayer')\n class InputLayer(base_layer.Layer):\n   \"\"\"Layer to be used as an entry point into a Network (a graph of layers).\n@@ -83,6 +90,9 @@ class InputLayer(base_layer.Layer):\n           ragged dimensions. For more information about RaggedTensors, see\n           [this guide](https://www.tensorflow.org/guide/ragged_tensors).\n           Default to False.\n+      type_spec: A `tf.TypeSpec` object to create Input from. This `tf.TypeSpec`\n+          represents the entire batch. When provided, all other args except\n+          name must be None.\n       name: Optional name of the layer (string).\n   \"\"\"\n \n@@ -91,10 +101,18 @@ class InputLayer(base_layer.Layer):\n                batch_size=None,\n                dtype=None,\n                input_tensor=None,\n-               sparse=False,\n+               sparse=None,\n                name=None,\n-               ragged=False,\n+               ragged=None,\n+               type_spec=None,\n                **kwargs):\n+    self._init_input_shape = input_shape\n+    self._init_batch_size = batch_size\n+    self._init_dtype = dtype\n+    self._init_sparse = sparse\n+    self._init_ragged = ragged\n+    self._init_type_spec = type_spec\n+\n     strategy = tf.distribute.get_strategy()\n     if strategy and batch_size is not None and \\\n         distributed_training_utils.global_batch_size_supported(strategy):\n@@ -133,8 +151,8 @@ class InputLayer(base_layer.Layer):\n                        (input_tensor.dtype, dtype))\n     super(InputLayer, self).__init__(dtype=dtype, name=name)\n     self.built = True\n-    self.sparse = sparse\n-    self.ragged = ragged\n+    self.sparse = True if sparse else False\n+    self.ragged = True if ragged else False\n     self.batch_size = batch_size\n     self.supports_masking = True\n \n@@ -143,7 +161,32 @@ class InputLayer(base_layer.Layer):\n     elif isinstance(input_shape, int):\n       input_shape = (input_shape,)\n \n-    if input_tensor is None:\n+    if type_spec is not None:\n+      args_that_must_be_none = [\n+          ('(input_)shape', self._init_input_shape),\n+          ('batch_size', self._init_batch_size),\n+          ('dtype', self._init_dtype),\n+          ('input_tensor', input_tensor),\n+          ('sparse', self._init_sparse),\n+          ('ragged', self._init_ragged),\n+      ]\n+      for arg_name, arg in args_that_must_be_none:\n+        _assert_other_arg_none(arg_name, arg)\n+      if not keras_tensor.keras_tensors_enabled():\n+        raise ValueError('Creating Keras inputs from a type_spec is only '\n+                         'supported when eager execution is enabled.')\n+      input_tensor = keras_tensor.keras_tensor_from_type_spec(type_spec)\n+      if isinstance(input_tensor, keras_tensor.SparseKerasTensor):\n+        self.sparse = True\n+      if isinstance(input_tensor, keras_tensor.RaggedKerasTensor):\n+        self.ragged = True\n+      self.is_placeholder = True\n+      try:\n+        self._batch_input_shape = tuple(input_tensor.shape.as_list())\n+      except ValueError:\n+        # If the shape cannot be represented as a tuple (e.g. unknown rank)\n+        self._batch_input_shape = None\n+    elif input_tensor is None:\n       if input_shape is not None:\n         batch_input_shape = (batch_size,) + tuple(input_shape)\n       else:\n@@ -188,12 +231,18 @@ class InputLayer(base_layer.Layer):\n           shape=input_tensor.shape, dtype=input_tensor.dtype, name=self.name)\n \n   def get_config(self):\n+    if self._init_type_spec is not None:\n+      config = {\n+          'name': self.name,\n+          'type_spec': self._init_type_spec\n+      }\n+    else:\n       config = {\n           'batch_input_shape': self._batch_input_shape,\n           'dtype': self.dtype,\n           'sparse': self.sparse,\n           'ragged': self.ragged,\n-        'name': self.name\n+          'name': self.name,\n       }\n     return config\n \n@@ -208,13 +257,14 @@ def Input(  # pylint: disable=invalid-name\n     batch_size=None,\n     name=None,\n     dtype=None,\n-    sparse=False,\n+    sparse=None,\n     tensor=None,\n-    ragged=False,\n+    ragged=None,\n+    type_spec=None,\n     **kwargs):\n   \"\"\"`Input()` is used to instantiate a Keras tensor.\n \n-  A Keras tensor is a TensorFlow symbolic tensor object,\n+  A Keras tensor is a symbolic tensor-like object,\n   which we augment with certain attributes that allow us to build a Keras model\n   just by knowing the inputs and outputs of the model.\n \n@@ -246,6 +296,8 @@ def Input(  # pylint: disable=invalid-name\n           values of 'None' in the 'shape' argument represent ragged dimensions.\n           For more information about RaggedTensors, see\n           [this guide](https://www.tensorflow.org/guide/ragged_tensors).\n+      type_spec: A `tf.TypeSpec` object to create the input placeholder from.\n+          When provided, all other args except name must be None.\n       **kwargs: deprecated arguments support. Supports `batch_shape` and\n           `batch_input_shape`.\n \n@@ -262,8 +314,8 @@ def Input(  # pylint: disable=invalid-name\n   ```\n \n   Note that even if eager execution is enabled,\n-  `Input` produces a symbolic tensor (i.e. a placeholder).\n-  This symbolic tensor can be used with other\n+  `Input` produces a symbolic tensor-like object (i.e. a placeholder).\n+  This symbolic tensor-like object can be used with other\n   TensorFlow ops, as such:\n \n   ```python\n@@ -271,11 +323,29 @@ def Input(  # pylint: disable=invalid-name\n   y = tf.square(x)\n   ```\n \n+  However, the resulting model will not track any variables that were\n+  used as inputs to TensorFlow ops. All variable usages must happen within\n+  Keras layers to make sure they will be tracked by the model's weights.\n+\n+  The Keras Input can also create a placeholder from an arbitrary `tf.TypeSpec`,\n+  e.g:\n+\n+  ```python\n+  x = Input(type_spec=tf.RaggedTensorSpec(shape=[None, None],\n+                                          dtype=tf.float32, ragged_rank=1))\n+  y = x.values\n+  model = Model(x, y)\n+  ```\n+  When passing an arbitrary `tf.TypeSpec`, it must represent the signature of an\n+  entire batch instead of just one example.\n+\n   Raises:\n     ValueError: If both `sparse` and `ragged` are provided.\n     ValueError: If both `shape` and (`batch_input_shape` or `batch_shape`) are\n       provided.\n-    ValueError: If both `shape` and `tensor` are None.\n+    ValueError: If `shape`, `tensor` and `type_spec` are None.\n+    ValueError: If arguments besides `type_spec` are non-None while `type_spec`\n+                is passed.\n     ValueError: if any unrecognized parameters are provided.\n   \"\"\"\n   if sparse and ragged:\n@@ -283,16 +353,18 @@ def Input(  # pylint: disable=invalid-name\n         'Cannot set both sparse and ragged to True in a Keras input.')\n \n   input_layer_config = {'name': name, 'dtype': dtype, 'sparse': sparse,\n-                        'ragged': ragged, 'input_tensor': tensor}\n+                        'ragged': ragged, 'input_tensor': tensor,\n+                        'type_spec': type_spec}\n \n   batch_input_shape = kwargs.pop('batch_input_shape',\n                                  kwargs.pop('batch_shape', None))\n   if shape is not None and batch_input_shape is not None:\n     raise ValueError('Only provide the `shape` OR `batch_input_shape` argument '\n                      'to Input, not both at the same time.')\n-  if batch_input_shape is None and shape is None and tensor is None:\n-    raise ValueError('Please provide to Input either a `shape`'\n-                     ' or a `tensor` argument. Note that '\n+  if (batch_input_shape is None and shape is None and tensor is None\n+      and type_spec is None):\n+    raise ValueError('Please provide to Input a `shape`'\n+                     ' or a `tensor` or a `type_spec` argument. Note that '\n                      '`shape` does not include the batch '\n                      'dimension.')\n   if kwargs:\n\n@@ -19,11 +19,94 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow as tf\n+from tensorflow.python.framework import type_spec\n from keras import combinations\n from keras import keras_parameterized\n from keras import testing_utils\n from keras.engine import functional\n from keras.engine import input_layer as input_layer_lib\n+from keras.layers import core\n+from keras.saving import model_config\n+\n+\n+class TwoTensors(tf.__internal__.CompositeTensor):\n+  \"\"\"A simple value type to test TypeSpec.\n+\n+  Contains two tensors (x, y) and a string (color).  The color value is a\n+  stand-in for any extra type metadata we might need to store.\n+\n+  This value type contains no single dtype.\n+  \"\"\"\n+\n+  def __init__(self, x, y, color='red', assign_variant_dtype=False):\n+    assert isinstance(color, str)\n+    self.x = tf.convert_to_tensor(x)\n+    self.y = tf.convert_to_tensor(y)\n+    self.color = color\n+    self.shape = tf.TensorShape(None)\n+    self._shape = tf.TensorShape(None)\n+    if assign_variant_dtype:\n+      self.dtype = tf.variant\n+    self._assign_variant_dtype = assign_variant_dtype\n+\n+  def _type_spec(self):\n+    return TwoTensorsSpecNoOneDtype(\n+        self.x.shape, self.x.dtype, self.y.shape,\n+        self.y.dtype, color=self.color,\n+        assign_variant_dtype=self._assign_variant_dtype)\n+\n+\n+def as_shape(shape):\n+  \"\"\"Converts the given object to a TensorShape.\"\"\"\n+  if isinstance(shape, tf.TensorShape):\n+    return shape\n+  else:\n+    return tf.TensorShape(shape)\n+\n+\n+@type_spec.register('tf.TwoTensorsSpec')\n+class TwoTensorsSpecNoOneDtype(tf.TypeSpec):\n+  \"\"\"A TypeSpec for the TwoTensors value type.\"\"\"\n+\n+  def __init__(\n+      self, x_shape, x_dtype, y_shape, y_dtype, color='red',\n+      assign_variant_dtype=False):\n+    self.x_shape = as_shape(x_shape)\n+    self.x_dtype = tf.as_dtype(x_dtype)\n+    self.y_shape = as_shape(y_shape)\n+    self.y_dtype = tf.as_dtype(y_dtype)\n+    self.color = color\n+    self.shape = tf.TensorShape(None)\n+    self._shape = tf.TensorShape(None)\n+    if assign_variant_dtype:\n+      self.dtype = tf.variant\n+    self._assign_variant_dtype = assign_variant_dtype\n+\n+  value_type = property(lambda self: TwoTensors)\n+\n+  @property\n+  def _component_specs(self):\n+    return (tf.TensorSpec(self.x_shape, self.x_dtype),\n+            tf.TensorSpec(self.y_shape, self.y_dtype))\n+\n+  def _to_components(self, value):\n+    return (value.x, value.y)\n+\n+  def _from_components(self, components):\n+    x, y = components\n+    return TwoTensors(x, y, self.color)\n+\n+  def _serialize(self):\n+    return (self.x_shape, self.x_dtype, self.y_shape, self.y_dtype, self.color)\n+\n+  @classmethod\n+  def from_value(cls, value):\n+    return cls(value.x.shape, value.x.dtype, value.y.shape, value.y.dtype,\n+               value.color)\n+\n+\n+type_spec.register_type_spec_from_value_converter(\n+    TwoTensors, TwoTensorsSpecNoOneDtype.from_value)\n \n \n class InputLayerTest(keras_parameterized.TestCase):\n@@ -141,5 +224,152 @@ class InputLayerTest(keras_parameterized.TestCase):\n           values=[3, 21, 4, 1, 53, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\n       self.assertAllEqual(run_model(rt), rt * 3)\n \n+  @combinations.generate(combinations.combine(mode=['eager']))\n+  def testNoMixingArgsWithTypeSpecArg(self):\n+    with testing_utils.use_keras_tensors_scope(True):\n+      with self.assertRaisesRegexp(\n+          ValueError, 'all other args except `name` must be None'):\n+        input_layer_lib.Input(\n+            shape=(4, 7),\n+            type_spec=tf.TensorSpec((2, 7, 32), tf.float32))\n+      with self.assertRaisesRegexp(\n+          ValueError, 'all other args except `name` must be None'):\n+        input_layer_lib.Input(\n+            batch_size=4,\n+            type_spec=tf.TensorSpec((7, 32), tf.float32))\n+      with self.assertRaisesRegexp(\n+          ValueError, 'all other args except `name` must be None'):\n+        input_layer_lib.Input(\n+            dtype=tf.int64,\n+            type_spec=tf.TensorSpec((7, 32), tf.float32))\n+      with self.assertRaisesRegexp(\n+          ValueError, 'all other args except `name` must be None'):\n+        input_layer_lib.Input(\n+            sparse=True,\n+            type_spec=tf.TensorSpec((7, 32), tf.float32))\n+      with self.assertRaisesRegexp(\n+          ValueError, 'all other args except `name` must be None'):\n+        input_layer_lib.Input(\n+            ragged=True,\n+            type_spec=tf.TensorSpec((7, 32), tf.float32))\n+\n+  @combinations.generate(combinations.combine(mode=['eager']))\n+  def testTypeSpecArg(self):\n+    with testing_utils.use_keras_tensors_scope(True):\n+      # Create a Keras Input\n+      x = input_layer_lib.Input(\n+          type_spec=tf.TensorSpec((7, 32), tf.float32))\n+      self.assertAllEqual(x.shape.as_list(), [7, 32])\n+\n+      # Verify you can construct and use a model w/ this input\n+      model = functional.Functional(x, x * 2.0)\n+      self.assertAllEqual(model(tf.ones(x.shape)),\n+                          tf.ones(x.shape) * 2.0)\n+\n+      # Test serialization / deserialization\n+      model = functional.Functional.from_config(model.get_config())\n+      self.assertAllEqual(model(tf.ones(x.shape)),\n+                          tf.ones(x.shape) * 2.0)\n+\n+      model = model_config.model_from_json(model.to_json())\n+      self.assertAllEqual(model(tf.ones(x.shape)),\n+                          tf.ones(x.shape) * 2.0)\n+\n+  @combinations.generate(combinations.combine(mode=['eager']))\n+  def testTypeSpecArgInTFFunction(self):\n+    with testing_utils.use_keras_tensors_scope(True):\n+      # We use a mutable model container instead of a model python variable,\n+      # because python 2.7 does not have `nonlocal`\n+      model_container = {}\n+\n+      @tf.function\n+      def run_model(inp):\n+        if not model_container:\n+          # Create a Keras Input\n+          x = input_layer_lib.Input(\n+              type_spec=tf.TensorSpec((10, 16), tf.float32))\n+          self.assertAllEqual(x.shape.as_list(), [10, 16])\n+\n+          # Verify you can construct and use a model w/ this input\n+          model_container['model'] = functional.Functional(x, x * 3.0)\n+        return model_container['model'](inp)\n+\n+      self.assertAllEqual(run_model(tf.ones((10, 16))),\n+                          tf.ones((10, 16)) * 3.0)\n+\n+  @combinations.generate(combinations.combine(mode=['eager']))\n+  def testCompositeTypeSpecArg(self):\n+    with testing_utils.use_keras_tensors_scope(True):\n+      # Create a Keras Input\n+      rt = tf.RaggedTensor.from_row_splits(\n+          values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\n+      x = input_layer_lib.Input(type_spec=rt._type_spec)\n+\n+      # Verify you can construct and use a model w/ this input\n+      model = functional.Functional(x, x * 2)\n+\n+      # And that the model works\n+      rt = tf.RaggedTensor.from_row_splits(\n+          values=[3, 21, 4, 1, 53, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\n+      self.assertAllEqual(model(rt), rt * 2)\n+\n+      # Test serialization / deserialization\n+      model = functional.Functional.from_config(model.get_config())\n+      self.assertAllEqual(model(rt), rt * 2)\n+      model = model_config.model_from_json(model.to_json())\n+      self.assertAllEqual(model(rt), rt * 2)\n+\n+  @combinations.generate(combinations.combine(mode=['eager']))\n+  def testCompositeTypeSpecArgInTFFunction(self):\n+    with testing_utils.use_keras_tensors_scope(True):\n+      # We use a mutable model container instead of a model pysthon variable,\n+      # because python 2.7 does not have `nonlocal`\n+      model_container = {}\n+\n+      @tf.function\n+      def run_model(inp):\n+        if not model_container:\n+          # Create a Keras Input\n+          rt = tf.RaggedTensor.from_row_splits(\n+              values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\n+          x = input_layer_lib.Input(type_spec=rt._type_spec)\n+\n+          # Verify you can construct and use a model w/ this input\n+          model_container['model'] = functional.Functional(x, x * 3)\n+        return model_container['model'](inp)\n+\n+      # And verify the model works\n+      rt = tf.RaggedTensor.from_row_splits(\n+          values=[3, 21, 4, 1, 53, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\n+      self.assertAllEqual(run_model(rt), rt * 3)\n+\n+  @combinations.generate(combinations.combine(mode=['eager']))\n+  def testCompositeTypeSpecArgWithoutDtype(self):\n+    with testing_utils.use_keras_tensors_scope(True):\n+      for assign_variant_dtype in [False, True]:\n+        # Create a Keras Input\n+        spec = TwoTensorsSpecNoOneDtype(\n+            (1, 2, 3), tf.float32, (1, 2, 3), tf.int64,\n+            assign_variant_dtype=assign_variant_dtype)\n+        x = input_layer_lib.Input(type_spec=spec)\n+\n+        def lambda_fn(tensors):\n+          return (tf.cast(tensors.x, tf.float64)\n+                  + tf.cast(tensors.y, tf.float64))\n+        # Verify you can construct and use a model w/ this input\n+        model = functional.Functional(x, core.Lambda(lambda_fn)(x))\n+\n+        # And that the model works\n+        two_tensors = TwoTensors(tf.ones((1, 2, 3)) * 2.0,\n+                                 tf.ones(1, 2, 3))\n+        self.assertAllEqual(model(two_tensors), lambda_fn(two_tensors))\n+\n+        # Test serialization / deserialization\n+        model = functional.Functional.from_config(model.get_config())\n+        self.assertAllEqual(model(two_tensors), lambda_fn(two_tensors))\n+        model = model_config.model_from_json(model.to_json())\n+        self.assertAllEqual(model(two_tensors), lambda_fn(two_tensors))\n+\n+\n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -195,6 +195,10 @@ class KerasTensor(object):\n       type_spec = tf.type_spec_from_value(tensor)\n       return cls(type_spec, name=name)\n \n+  @classmethod\n+  def from_type_spec(cls, type_spec, name=None):\n+    return cls(type_spec=type_spec, name=name)\n+\n   def _to_placeholder(self):\n     \"\"\"Convert this KerasTensor to a placeholder in a graph.\"\"\"\n     # If there is an inferred value for this tensor, inject the inferred value\n@@ -528,6 +532,11 @@ class UserRegisteredTypeKerasTensor(KerasTensor):\n   def from_tensor(cls, tensor):\n     return cls(tensor)\n \n+  @classmethod\n+  def from_type_spec(cls, type_spec, name=None):\n+    raise NotImplementedError('You cannot instantiate a KerasTensor '\n+                              'directly from TypeSpec: %s' % type_spec)\n+\n   def _to_placeholder(self):\n     return self._user_registered_symbolic_object\n \n@@ -598,3 +607,17 @@ def keras_tensor_from_tensor(tensor):\n   if hasattr(tensor, '_keras_mask'):\n     out._keras_mask = keras_tensor_from_tensor(tensor._keras_mask)  # pylint: disable=protected-access\n   return out\n+\n+\n+def keras_tensor_from_type_spec(type_spec, name=None):\n+  \"\"\"Convert a TypeSpec to a representative KerasTensor.\"\"\"\n+  # Create a specialized KerasTensor that supports instance methods,\n+  # operators, and additional value inference if possible\n+  keras_tensor_cls = None\n+  value_type = type_spec.value_type\n+  for tensor_type, cls in keras_tensor_classes:\n+    if issubclass(value_type, tensor_type):\n+      keras_tensor_cls = cls\n+      break\n+\n+  return keras_tensor_cls.from_type_spec(type_spec, name=name)\n\n@@ -47,11 +47,11 @@ _RUNTIME_UNKNOWN = 0\n _RUNTIME_CPU = 1\n _RUNTIME_GPU = 2\n \n-_CUDNN_AVAILABLE_MSG = 'Layer %s will use cuDNN kernel when run on GPU.'\n-_CUDNN_NOT_AVAILABLE_MSG = ('Layer %s will not use cuDNN kernel since it '\n-                            'doesn\\'t meet the cuDNN kernel criteria. It will '\n-                            'use generic GPU kernel as fallback when running '\n-                            'on GPU')\n+_CUDNN_AVAILABLE_MSG = 'Layer %s will use cuDNN kernels when running on GPU.'\n+_CUDNN_NOT_AVAILABLE_MSG = ('Layer %s will not use cuDNN kernels since it '\n+                            'doesn\\'t meet the criteria. It will '\n+                            'use a generic GPU kernel as fallback when running '\n+                            'on GPU.')\n \n \n def _use_new_code():\n\n@@ -23,7 +23,6 @@ import threading\n \n from tensorflow.python.distribute import ps_values as ps_distribute_values\n from tensorflow.python.distribute import values as distribute_values\n-from tensorflow.python.framework import ops\n from tensorflow.python.ops import resource_variable_ops\n from tensorflow.python.types import core\n \n@@ -136,10 +135,14 @@ class AutoCastVariable(tf.Variable, core.Tensor):\n \n   def _dense_var_to_tensor(self, dtype=None, name=None, as_ref=False):\n     \"\"\"Converts this variable to a tensor.\"\"\"\n+    if as_ref:\n+      # This ValueError should not occur in practice since it is impossible to\n+      # pass as_ref=True using public APIs.\n+      raise ValueError('Cannot convert AutoCastVariable to a tensor if '\n+                       'as_ref=True is passed to convert_to_tensor')\n     if not self._should_cast():\n-      return ops.convert_to_tensor(self._variable, dtype, name, as_ref)\n-    # TODO(reedwm): Support as_ref?\n-    assert not as_ref\n+      return tf.convert_to_tensor(self._variable, dtype=dtype,\n+                                                    name=name)\n     if dtype is not None and not dtype.is_compatible_with(self._cast_dtype):\n       raise ValueError(\n           'Incompatible type conversion requested to type {!r} for '\n\n@@ -39,6 +39,7 @@ def index_directory(directory,\n     directory: The target directory (string).\n     labels: Either \"inferred\"\n         (labels are generated from the directory structure),\n+        None (no labels),\n         or a list/tuple of integer labels of the same size as the number of\n         valid files found in the directory. Labels should be sorted according\n         to the alphanumeric order of the image file paths\n@@ -59,19 +60,24 @@ def index_directory(directory,\n       labels: list of matching integer labels (same length as file_paths)\n       class_names: names of the classes corresponding to these labels, in order.\n   \"\"\"\n-  inferred_class_names = []\n+  if labels is None:\n+    # in the no-label case, index from the parent directory down.\n+    subdirs = ['']\n+    class_names = subdirs\n+  else:\n+    subdirs = []\n     for subdir in sorted(os.listdir(directory)):\n       if os.path.isdir(os.path.join(directory, subdir)):\n-      inferred_class_names.append(subdir)\n+        subdirs.append(subdir)\n     if not class_names:\n-    class_names = inferred_class_names\n+      class_names = subdirs\n     else:\n-    if set(class_names) != set(inferred_class_names):\n+      if set(class_names) != set(subdirs):\n         raise ValueError(\n             'The `class_names` passed did not match the '\n             'names of the subdirectories of the target directory. '\n             'Expected: %s, but received: %s' %\n-          (inferred_class_names, class_names))\n+            (subdirs, class_names))\n   class_indices = dict(zip(class_names, range(len(class_names))))\n \n   # Build an index of the files\n@@ -79,7 +85,8 @@ def index_directory(directory,\n   pool = multiprocessing.pool.ThreadPool()\n   results = []\n   filenames = []\n-  for dirpath in (os.path.join(directory, subdir) for subdir in class_names):\n+\n+  for dirpath in (os.path.join(directory, subdir) for subdir in subdirs):\n     results.append(\n         pool.apply_async(index_subdirectory,\n                          (dirpath, class_indices, follow_links, formats)))\n@@ -88,7 +95,7 @@ def index_directory(directory,\n     partial_filenames, partial_labels = res.get()\n     labels_list.append(partial_labels)\n     filenames += partial_filenames\n-  if labels != 'inferred':\n+  if labels not in ('inferred', None):\n     if len(labels) != len(filenames):\n       raise ValueError('Expected the lengths of `labels` to match the number '\n                        'of files in the target directory. len(labels) is %s '\n@@ -101,6 +108,9 @@ def index_directory(directory,\n       labels[i:i + len(partial_labels)] = partial_labels\n       i += len(partial_labels)\n \n+  if labels is None:\n+    print('Found %d files.' % (len(filenames),))\n+  else:\n     print('Found %d files belonging to %d classes.' %\n           (len(filenames), len(class_names)))\n   pool.close()\n\n@@ -72,6 +72,7 @@ def image_dataset_from_directory(directory,\n         Otherwise, the directory structure is ignored.\n     labels: Either \"inferred\"\n         (labels are generated from the directory structure),\n+        None (no labels),\n         or a list/tuple of integer labels of the same size as the number of\n         image files found in the directory. Labels should be sorted according\n         to the alphanumeric order of the image file paths\n@@ -137,7 +138,7 @@ def image_dataset_from_directory(directory,\n     - if `color_mode` is `rgba`,\n       there are 4 channel in the image tensors.\n   \"\"\"\n-  if labels != 'inferred':\n+  if labels not in ('inferred', None):\n     if not isinstance(labels, (list, tuple)):\n       raise ValueError(\n           '`labels` argument should be a list/tuple of integer labels, of '\n@@ -154,6 +155,9 @@ def image_dataset_from_directory(directory,\n     raise ValueError(\n         '`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", '\n         'or None. Received: %s' % (label_mode,))\n+  if labels is None or label_mode is None:\n+    labels = None\n+    label_mode = None\n   if color_mode == 'rgb':\n     num_channels = 3\n   elif color_mode == 'rgba':\n@@ -186,6 +190,8 @@ def image_dataset_from_directory(directory,\n \n   image_paths, labels = dataset_utils.get_training_or_validation_split(\n       image_paths, labels, validation_split, subset)\n+  if not image_paths:\n+    raise ValueError('No images found.')\n \n   dataset = paths_and_labels_to_dataset(\n       image_paths=image_paths,\n\n@@ -80,7 +80,7 @@ class ImageDatasetFromDirectoryTest(keras_parameterized.TestCase):\n     # Save images to the paths\n     i = 0\n     for img in self._get_images(color_mode=color_mode, count=count):\n-      path = paths[count % len(paths)]\n+      path = paths[i % len(paths)]\n       if color_mode == 'rgb':\n         ext = 'jpg'\n       else:\n@@ -90,6 +90,32 @@ class ImageDatasetFromDirectoryTest(keras_parameterized.TestCase):\n       i += 1\n     return temp_dir\n \n+  def test_image_dataset_from_directory_standalone(self):\n+    # Test retrieving images without labels from a directory and its subdirs.\n+    if PIL is None:\n+      return  # Skip test if PIL is not available.\n+\n+    # Save a few extra images in the parent directory.\n+    directory = self._prepare_directory(count=7, num_classes=2)\n+    for i, img in enumerate(self._get_images(3)):\n+      filename = 'image_%s.jpg' % (i,)\n+      img.save(os.path.join(directory, filename))\n+\n+    dataset = image_dataset.image_dataset_from_directory(\n+        directory, batch_size=5, image_size=(18, 18), labels=None)\n+    batch = next(iter(dataset))\n+    # We return plain images\n+    self.assertEqual(batch.shape, (5, 18, 18, 3))\n+    self.assertEqual(batch.dtype.name, 'float32')\n+    # Count samples\n+    batch_count = 0\n+    sample_count = 0\n+    for batch in dataset:\n+      batch_count += 1\n+      sample_count += batch.shape[0]\n+    self.assertEqual(batch_count, 2)\n+    self.assertEqual(sample_count, 10)\n+\n   def test_image_dataset_from_directory_binary(self):\n     if PIL is None:\n       return  # Skip test if PIL is not available.\n@@ -251,6 +277,11 @@ class ImageDatasetFromDirectoryTest(keras_parameterized.TestCase):\n       sample_count += batch.shape[0]\n     self.assertEqual(sample_count, 25)\n \n+  def test_image_dataset_from_directory_no_images(self):\n+    directory = self._prepare_directory(num_classes=2, count=0)\n+    with self.assertRaisesRegex(ValueError, 'No images found.'):\n+      _ = image_dataset.image_dataset_from_directory(directory)\n+\n   def test_image_dataset_from_directory_errors(self):\n     if PIL is None:\n       return  # Skip test if PIL is not available.\n@@ -259,7 +290,7 @@ class ImageDatasetFromDirectoryTest(keras_parameterized.TestCase):\n \n     with self.assertRaisesRegex(ValueError, '`labels` argument should be'):\n       _ = image_dataset.image_dataset_from_directory(\n-          directory, labels=None)\n+          directory, labels='other')\n \n     with self.assertRaisesRegex(ValueError, '`label_mode` argument must be'):\n       _ = image_dataset.image_dataset_from_directory(\n\n@@ -64,6 +64,7 @@ def text_dataset_from_directory(directory,\n         Otherwise, the directory structure is ignored.\n     labels: Either \"inferred\"\n         (labels are generated from the directory structure),\n+        None (no labels),\n         or a list/tuple of integer labels of the same size as the number of\n         text files found in the directory. Labels should be sorted according\n         to the alphanumeric order of the text file paths\n@@ -112,7 +113,7 @@ def text_dataset_from_directory(directory,\n       of shape `(batch_size, num_classes)`, representing a one-hot\n       encoding of the class index.\n   \"\"\"\n-  if labels != 'inferred':\n+  if labels not in ('inferred', None):\n     if not isinstance(labels, (list, tuple)):\n       raise ValueError(\n           '`labels` argument should be a list/tuple of integer labels, of '\n@@ -129,6 +130,9 @@ def text_dataset_from_directory(directory,\n     raise ValueError(\n         '`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", '\n         'or None. Received: %s' % (label_mode,))\n+  if labels is None or label_mode is None:\n+    labels = None\n+    label_mode = None\n   dataset_utils.check_validation_split_arg(\n       validation_split, subset, shuffle, seed)\n \n@@ -150,6 +154,8 @@ def text_dataset_from_directory(directory,\n \n   file_paths, labels = dataset_utils.get_training_or_validation_split(\n       file_paths, labels, validation_split, subset)\n+  if not file_paths:\n+    raise ValueError('No text files found.')\n \n   dataset = paths_and_labels_to_dataset(\n       file_paths=file_paths,\n\n@@ -57,7 +57,7 @@ class TextDatasetFromDirectoryTest(keras_parameterized.TestCase):\n       paths += class_paths\n \n     for i in range(count):\n-      path = paths[count % len(paths)]\n+      path = paths[i % len(paths)]\n       filename = os.path.join(path, 'text_%s.txt' % (i,))\n       f = open(os.path.join(temp_dir, filename), 'w')\n       text = ''.join([random.choice(string.printable) for _ in range(length)])\n@@ -65,6 +65,32 @@ class TextDatasetFromDirectoryTest(keras_parameterized.TestCase):\n       f.close()\n     return temp_dir\n \n+  def test_text_dataset_from_directory_standalone(self):\n+    # Test retrieving txt files without labels from a directory and its subdirs.\n+    # Save a few extra files in the parent directory.\n+    directory = self._prepare_directory(count=7, num_classes=2)\n+    for i in range(3):\n+      filename = 'text_%s.txt' % (i,)\n+      f = open(os.path.join(directory, filename), 'w')\n+      text = ''.join([random.choice(string.printable) for _ in range(20)])\n+      f.write(text)\n+      f.close()\n+\n+    dataset = text_dataset.text_dataset_from_directory(\n+        directory, batch_size=5, label_mode=None, max_length=10)\n+    batch = next(iter(dataset))\n+    # We just return the texts, no labels\n+    self.assertEqual(batch.shape, (5,))\n+    self.assertEqual(batch.dtype.name, 'string')\n+    # Count samples\n+    batch_count = 0\n+    sample_count = 0\n+    for batch in dataset:\n+      batch_count += 1\n+      sample_count += batch.shape[0]\n+    self.assertEqual(batch_count, 2)\n+    self.assertEqual(sample_count, 10)\n+\n   def test_text_dataset_from_directory_binary(self):\n     directory = self._prepare_directory(num_classes=2)\n     dataset = text_dataset.text_dataset_from_directory(\n@@ -171,12 +197,17 @@ class TextDatasetFromDirectoryTest(keras_parameterized.TestCase):\n       sample_count += batch.shape[0]\n     self.assertEqual(sample_count, 25)\n \n+  def test_text_dataset_from_directory_no_files(self):\n+    directory = self._prepare_directory(num_classes=2, count=0)\n+    with self.assertRaisesRegex(ValueError, 'No text files found.'):\n+      _ = text_dataset.text_dataset_from_directory(directory)\n+\n   def test_text_dataset_from_directory_errors(self):\n     directory = self._prepare_directory(num_classes=3, count=5)\n \n     with self.assertRaisesRegex(ValueError, '`labels` argument should be'):\n       _ = text_dataset.text_dataset_from_directory(\n-          directory, labels=None)\n+          directory, labels='other')\n \n     with self.assertRaisesRegex(ValueError, '`label_mode` argument must be'):\n       _ = text_dataset.text_dataset_from_directory(\n\n@@ -19,8 +19,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import json\n-\n+from keras.saving.saved_model import json_utils\n from tensorflow.python.util.tf_export import keras_export\n \n # pylint: disable=g-import-not-at-top\n@@ -126,6 +125,6 @@ def model_from_json(json_string, custom_objects=None):\n   Returns:\n       A Keras model instance (uncompiled).\n   \"\"\"\n-  config = json.loads(json_string)\n+  config = json_utils.decode(json_string)\n   from keras.layers import deserialize  # pylint: disable=g-import-not-at-top\n   return deserialize(config, custom_objects=custom_objects)\n\n@@ -58,11 +58,13 @@ def remove_squeezable_dimensions(\n     Tuple of `labels` and `predictions`, possibly with last dim squeezed.\n   \"\"\"\n   with K.name_scope(name or 'remove_squeezable_dimensions'):\n+    if not isinstance(predictions, tf.RaggedTensor):\n       predictions = tf.convert_to_tensor(predictions)\n+    if not isinstance(labels, tf.RaggedTensor):\n       labels = tf.convert_to_tensor(labels)\n-    predictions_shape = predictions.get_shape()\n+    predictions_shape = predictions.shape\n     predictions_rank = predictions_shape.ndims\n-    labels_shape = labels.get_shape()\n+    labels_shape = labels.shape\n     labels_rank = labels_shape.ndims\n     if (labels_rank is not None) and (predictions_rank is not None):\n       # Use static rank.\n\n@@ -107,6 +107,11 @@ def swap_class(cls, v2_cls, v1_cls, use_v2):\n   new_bases = []\n   for base in cls.__bases__:\n     if ((use_v2 and issubclass(base, v1_cls)\n+         # `v1_cls` often extends `v2_cls`, so it may still call `swap_class`\n+         # even if it doesn't need to. That being said, it may be the safest\n+         # not to over optimize this logic for the sake of correctness,\n+         # especially if we swap v1 & v2 classes that don't extend each other,\n+         # or when the inheritance order is different.\n          or (not use_v2 and issubclass(base, v2_cls)))):\n       new_base = swap_class(base, v2_cls, v1_cls, use_v2)\n     else:\n\n@@ -105,9 +105,10 @@ def model_to_dot(model,\n \n   if not check_pydot():\n     message = (\n-        'Failed to import pydot. You must `pip install pydot` '\n-        'and install graphviz (https://graphviz.gitlab.io/download/), ',\n-        'for `pydotprint` to work.')\n+        'You must install pydot (`pip install pydot`) '\n+        'and install graphviz '\n+        '(see instructions at https://graphviz.gitlab.io/download/) ',\n+        'for plot_model/model_to_dot to work.')\n     if 'IPython.core.magics.namespace' in sys.modules:\n       # We don't raise an exception here in order to avoid crashing notebook\n       # tests where graphviz is not available.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
