{"custom_id": "scrapy#88bec496f2b1a1e83b2a55dc1155988ebc7c08f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 1032 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,10 @@\n \"\"\"Download handlers for http and https schemes\"\"\"\n \n+import re\n+\n from time import time\n from cStringIO import StringIO\n from urlparse import urldefrag\n-from re import match\n \n from zope.interface import implements\n from twisted.internet import defer, reactor, protocol\n@@ -51,6 +52,8 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     for it.\n     \"\"\"\n \n+    _responseMatcher = re.compile('HTTP/1\\.. 200')\n+\n     def __init__(self, reactor, host, port, proxyConf, contextFactory,\n                  timeout=30, bindAddress=None):\n         proxyHost, proxyPort, self._proxyAuthHeader = proxyConf\n@@ -81,7 +84,7 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n         raises a TunnelError.\n         \"\"\"\n         self._protocol.dataReceived = self._protocolDataReceived\n-        if match('HTTP/1\\.. 200', bytes):\n+        if  TunnelingTCP4ClientEndpoint._responseMatcher.match(bytes):\n             self._protocol.transport.startTLS(self._contextFactory,\n                                               self._protocolFactory)\n             self._tunnelReadyDeferred.callback(self._protocol)\n@@ -140,8 +143,8 @@ class ScrapyAgent(object):\n         if proxy:\n             _, _, proxyHost, proxyPort, proxyParams = _parse(proxy)\n             scheme = _parse(request.url)[0]\n-            skipConnectTunnel = proxyParams.find('noconnect') >= 0\n-            if  scheme == 'https' and not skipConnectTunnel:\n+            omitConnectTunnel = proxyParams.find('noconnect') >= 0\n+            if  scheme == 'https' and not omitConnectTunnel:\n                 proxyConf = (proxyHost, proxyPort,\n                              request.headers.get('Proxy-Authorization', None))\n                 return self._TunnelingAgent(reactor, proxyConf,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d69ba7c1aede3b8f5e683695957777f2567d5067", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 11 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 96 | Churn Cumulative: 96 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,96 @@\n+import os\n+import subprocess\n+import time\n+\n+from threading import Thread\n+from libmproxy import controller, proxy\n+from netlib import http_auth\n+\n+from twisted.internet import defer\n+from twisted.trial.unittest import TestCase\n+from scrapy.utils.test import get_crawler, get_testlog\n+from scrapy.tests.spiders import SimpleSpider\n+from scrapy.tests.mockserver import MockServer\n+\n+\n+def docrawl(spider, settings=None):\n+    crawler = get_crawler(settings)\n+    crawler.configure()\n+    crawler.crawl(spider)\n+    return crawler.start()\n+\n+\n+\n+class HTTPSProxy(controller.Master, Thread):\n+\n+    def __init__(self, port):\n+        password_manager = http_auth.PassManSingleUser('scrapy', 'scrapy')\n+        authenticator = http_auth.BasicProxyAuth(password_manager, \"mitmproxy\")\n+        server = proxy.ProxyServer(proxy.ProxyConfig(\n+            authenticator = authenticator,\n+            cacert = os.path.expanduser(\"~/.mitmproxy/mitmproxy-ca.pem\")),\n+            port)\n+        Thread.__init__(self)\n+        controller.Master.__init__(self, server)\n+\n+\n+class ProxyConnectTestCase(TestCase):\n+\n+    def setUp(self):\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n+        self._oldenv = os.environ.copy()\n+        self._proxy = HTTPSProxy(8888)\n+        self._proxy.start()\n+        os.environ['http_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n+        os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n+\n+    def tearDown(self):\n+        self.mockserver.__exit__(None, None, None)\n+        self._proxy.shutdown()\n+        os.environ = self._oldenv\n+\n+    @defer.inlineCallbacks\n+    def test_https_connect_tunnel(self):\n+        spider = SimpleSpider(\"https://localhost:8999/status?n=200\")\n+        yield docrawl(spider)\n+        self._assert_got_response_code(200)\n+\n+    @defer.inlineCallbacks\n+    def test_https_noconnect(self):\n+        os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888?noconnect'\n+        spider = SimpleSpider(\"https://localhost:8999/status?n=200\")\n+        yield docrawl(spider)\n+        self._assert_got_response_code(200)\n+        os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n+\n+    @defer.inlineCallbacks\n+    def test_https_connect_tunnel_error(self):\n+        spider = SimpleSpider(\"https://localhost:99999/status?n=200\")\n+        yield docrawl(spider)\n+        self._assert_got_tunnel_error()\n+\n+    @defer.inlineCallbacks\n+    def test_https_tunnel_auth_error(self):\n+        os.environ['https_proxy'] = 'http://wrong:wronger@localhost:8888'\n+        spider = SimpleSpider(\"https://localhost:8999/status?n=200\")\n+        yield docrawl(spider)\n+        # The proxy returns a 407 error code but it does not reach the client;\n+        # he just sees a TunnelError.\n+        self._assert_got_tunnel_error()\n+        os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n+\n+    @defer.inlineCallbacks\n+    def test_https_noconnect_auth_error(self):\n+        os.environ['https_proxy'] = 'http://wrong:wronger@localhost:8888?noconnect'\n+        spider = SimpleSpider(\"https://localhost:8999/status?n=200\")\n+        yield docrawl(spider)\n+        self._assert_got_response_code(407)\n+\n+    def _assert_got_response_code(self, code):\n+        log = get_testlog()\n+        self.assertEqual(log.count('Crawled (%d)' % code), 1)\n+\n+    def _assert_got_tunnel_error(self):\n+        log = get_testlog()\n+        self.assertEqual(log.count('TunnelError'), 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f8dea74948698d26fcd7e4eafe67f556b11f0b06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 98 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -42,6 +42,8 @@ class ProxyConnectTestCase(TestCase):\n         self._oldenv = os.environ.copy()\n         self._proxy = HTTPSProxy(8888)\n         self._proxy.start()\n+        # Wait for the proxy to start.\n+        time.sleep(1.0)\n         os.environ['http_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n         os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#500490ee73c85897723b3060c9f1fecf8b516b99", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 597 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -207,10 +207,10 @@ class HttpProxyTestCase(unittest.TestCase):\n         def _test(response):\n             self.assertEquals(response.status, 200)\n             self.assertEquals(response.url, request.url)\n-            self.assertEquals(response.body, 'https://example.com')\n+            self.assertEquals(response.body, 'http://example.com')\n \n         http_proxy = self.getURL('')\n-        request = Request('https://example.com', meta={'proxy': http_proxy})\n+        request = Request('http://example.com', meta={'proxy': http_proxy})\n         return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n \n     def test_download_without_proxy(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6427d60fd8967bc430c0e50415d120080663a65b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 102 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,9 +26,11 @@ class HTTPSProxy(controller.Master, Thread):\n     def __init__(self, port):\n         password_manager = http_auth.PassManSingleUser('scrapy', 'scrapy')\n         authenticator = http_auth.BasicProxyAuth(password_manager, \"mitmproxy\")\n+        cert_path = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n+            'keys', 'mitmproxy-ca.pem')\n         server = proxy.ProxyServer(proxy.ProxyConfig(\n             authenticator = authenticator,\n-            cacert = os.path.expanduser(\"~/.mitmproxy/mitmproxy-ca.pem\")),\n+            cacert = cert_path),\n             port)\n         Thread.__init__(self)\n         controller.Master.__init__(self, server)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8ada8f5f36b3d2b4dd92dc7bb4822c909e2b6d03", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 607 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -213,6 +213,16 @@ class HttpProxyTestCase(unittest.TestCase):\n         request = Request('http://example.com', meta={'proxy': http_proxy})\n         return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n \n+    def test_download_with_proxy_https_noconnect(self):\n+        def _test(response):\n+            self.assertEquals(response.status, 200)\n+            self.assertEquals(response.url, request.url)\n+            self.assertEquals(response.body, 'https://example.com')\n+\n+        http_proxy = '%s?noconnect' % self.getURL('')\n+        request = Request('https://example.com', meta={'proxy': http_proxy})\n+        return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n+\n     def test_download_without_proxy(self):\n         def _test(response):\n             self.assertEquals(response.status, 200)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#62f7171942512cedb576e944e15bb1bf5e19c859", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 1 | Churn Cumulative: 275 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -74,6 +74,7 @@ class BaseSgmlLinkExtractor(FixedSGMLParser):\n                         self.current_link = link\n \n     def unknown_endtag(self, tag):\n+        if self.scan_tag(tag):\n             self.current_link = None\n \n     def handle_data(self, data):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#79179f59b24d7ec7334d65138d4d11577aa5928c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 350 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -121,6 +121,7 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n             Link(url='http://example.com/sample2.html', text=u'sample 2'),\n             Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n             Link(url='http://www.google.com/something', text=u''),\n+            Link(url='http://example.com/innertag.html', text=u'inner tag'),\n         ])\n \n         lx = SgmlLinkExtractor(allow=('sample', ))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#022538d3b0d14d78fb64c904ce06250a083da966", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 279 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -37,7 +37,7 @@ class BaseSgmlLinkExtractor(FixedSGMLParser):\n                 link.url = link.url.encode(response_encoding)\n             link.url = urljoin(base_url, link.url)\n             link.url = safe_url_string(link.url, response_encoding)\n-            link.text = str_to_unicode(link.text, response_encoding, errors='replace')\n+            link.text = str_to_unicode(link.text, response_encoding, errors='replace').strip()\n             ret.append(link)\n \n         return ret\n@@ -79,7 +79,7 @@ class BaseSgmlLinkExtractor(FixedSGMLParser):\n \n     def handle_data(self, data):\n         if self.current_link:\n-            self.current_link.text = self.current_link.text + data.strip()\n+            self.current_link.text = self.current_link.text + data\n \n     def matches(self, url):\n         \"\"\"This extractor matches with any url, since\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#270e9190ae52805c953fdbafb3ae51bda28e26d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 42 | Files Changed: 2 | Hunks: 27 | Methods Changed: 21 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 96 | Churn Cumulative: 1386 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,14 +33,14 @@ class FSFilesStore(object):\n         self._mkdir(self.basedir)\n         self.created_directories = defaultdict(set)\n \n-    def persist_file(self, key, buf, info, meta=None, headers=None):\n-        absolute_path = self._get_filesystem_path(key)\n+    def persist_file(self, path, buf, info, meta=None, headers=None):\n+        absolute_path = self._get_filesystem_path(path)\n         self._mkdir(os.path.dirname(absolute_path), info)\n         with open(absolute_path, 'wb') as f:\n             f.write(buf.getvalue())\n \n-    def stat_file(self, key, info):\n-        absolute_path = self._get_filesystem_path(key)\n+    def stat_file(self, path, info):\n+        absolute_path = self._get_filesystem_path(path)\n         try:\n             last_modified = os.path.getmtime(absolute_path)\n         except:  # FIXME: catching everything!\n@@ -51,8 +51,8 @@ class FSFilesStore(object):\n \n         return {'last_modified': last_modified, 'checksum': checksum}\n \n-    def _get_filesystem_path(self, key):\n-        path_comps = key.split('/')\n+    def _get_filesystem_path(self, path):\n+        path_comps = path.split('/')\n         return os.path.join(self.basedir, *path_comps)\n \n     def _mkdir(self, dirname, domain=None):\n@@ -77,7 +77,7 @@ class S3FilesStore(object):\n         assert uri.startswith('s3://')\n         self.bucket, self.prefix = uri[5:].split('/', 1)\n \n-    def stat_file(self, key, info):\n+    def stat_file(self, path, info):\n         def _onsuccess(boto_key):\n             checksum = boto_key.etag.strip('\"')\n             last_modified = boto_key.last_modified\n@@ -85,7 +85,7 @@ class S3FilesStore(object):\n             modified_stamp = int(rfc822.mktime_tz(modified_tuple))\n             return {'checksum': checksum, 'last_modified': modified_stamp}\n \n-        return self._get_boto_key(key).addCallback(_onsuccess)\n+        return self._get_boto_key(path).addCallback(_onsuccess)\n \n     def _get_boto_bucket(self):\n         from boto.s3.connection import S3Connection\n@@ -94,15 +94,15 @@ class S3FilesStore(object):\n         c = S3Connection(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, is_secure=False)\n         return c.get_bucket(self.bucket, validate=False)\n \n-    def _get_boto_key(self, key):\n+    def _get_boto_key(self, path):\n         b = self._get_boto_bucket()\n-        key_name = '%s%s' % (self.prefix, key)\n+        key_name = '%s%s' % (self.prefix, path)\n         return threads.deferToThread(b.get_key, key_name)\n \n-    def persist_file(self, key, buf, info, meta=None, headers=None):\n+    def persist_file(self, path, buf, info, meta=None, headers=None):\n         \"\"\"Upload file to S3 storage\"\"\"\n         b = self._get_boto_bucket()\n-        key_name = '%s%s' % (self.prefix, key)\n+        key_name = '%s%s' % (self.prefix, path)\n         k = b.new_key(key_name)\n         if meta:\n             for metakey, metavalue in meta.iteritems():\n@@ -191,10 +191,10 @@ class FilesPipeline(MediaPipeline):\n             self.inc_stats(info.spider, 'uptodate')\n \n             checksum = result.get('checksum', None)\n-            return {'url': request.url, 'path': key, 'checksum': checksum}\n+            return {'url': request.url, 'path': path, 'checksum': checksum}\n \n-        key = self.file_key(request.url)\n-        dfd = defer.maybeDeferred(self.store.stat_file, key, info)\n+        path = self.file_path(request, info=info)\n+        dfd = defer.maybeDeferred(self.store.stat_file, path, info)\n         dfd.addCallbacks(_onsuccess, lambda _: None)\n         dfd.addErrback(log.err, self.__class__.__name__ + '.store.stat_file')\n         return dfd\n@@ -232,7 +232,7 @@ class FilesPipeline(MediaPipeline):\n         self.inc_stats(info.spider, status)\n \n         try:\n-            key = self.file_key(request.url)\n+            path = self.file_path(request, response=response, info=info)\n             checksum = self.file_downloaded(response, request, info)\n         except FileException as exc:\n             whyfmt = 'File (error): Error processing image from %(request)s referred in <%(referer)s>: %(errormsg)s'\n@@ -244,7 +244,7 @@ class FilesPipeline(MediaPipeline):\n             log.err(None, whyfmt % {'request': request, 'referer': referer}, spider=info.spider)\n             raise FileException(str(exc))\n \n-        return {'url': request.url, 'path': key, 'checksum': checksum}\n+        return {'url': request.url, 'path': path, 'checksum': checksum}\n \n     def inc_stats(self, spider, status):\n         spider.crawler.stats.inc_value('file_count', spider=spider)\n@@ -254,15 +254,10 @@ class FilesPipeline(MediaPipeline):\n     def get_media_requests(self, item, info):\n         return [Request(x) for x in item.get(self.FILES_URLS_FIELD, [])]\n \n-    def file_key(self, url):\n-        media_guid = hashlib.sha1(url).hexdigest()\n-        media_ext = os.path.splitext(url)[1]\n-        return 'full/%s%s' % (media_guid, media_ext)\n-\n     def file_downloaded(self, response, request, info):\n-        key = self.file_key(request.url)\n+        path = self.file_path(request)\n         buf = StringIO(response.body)\n-        self.store.persist_file(key, buf, info)\n+        self.store.persist_file(path, buf, info)\n         checksum = md5sum(buf)\n         return checksum\n \n@@ -270,3 +265,12 @@ class FilesPipeline(MediaPipeline):\n         if self.FILES_RESULT_FIELD in item.fields:\n             item[self.FILES_RESULT_FIELD] = [x for ok, x in results if ok]\n         return item\n+\n+    def file_path(self, request, response=None, info=None):\n+        return self.file_key(request.url)\n+\n+    # deprecated\n+    def file_key(self, url):\n+        media_guid = hashlib.sha1(url).hexdigest()\n+        media_ext = os.path.splitext(url)[1]\n+        return 'full/%s%s' % (media_guid, media_ext)\n\n@@ -51,27 +51,24 @@ class ImagesPipeline(FilesPipeline):\n         store_uri = settings['IMAGES_STORE']\n         return cls(store_uri)\n \n-    def file_key(self, url):\n-        return self.image_key(url)\n-\n     def file_downloaded(self, response, request, info):\n         return self.image_downloaded(response, request, info)\n \n     def image_downloaded(self, response, request, info):\n         checksum = None\n-        for key, image, buf in self.get_images(response, request, info):\n+        for path, image, buf in self.get_images(response, request, info):\n             if checksum is None:\n                 buf.seek(0)\n                 checksum = md5sum(buf)\n             width, height = image.size\n             self.store.persist_file(\n-                key, buf, info,\n+                path, buf, info,\n                 meta={'width': width, 'height': height},\n                 headers={'Content-Type': 'image/jpeg'})\n         return checksum\n \n     def get_images(self, response, request, info):\n-        key = self.file_key(request.url)\n+        path = self.file_path(request, response=response, info=info)\n         orig_image = Image.open(StringIO(response.body))\n \n         width, height = orig_image.size\n@@ -80,12 +77,12 @@ class ImagesPipeline(FilesPipeline):\n                                  (width, height, self.MIN_WIDTH, self.MIN_HEIGHT))\n \n         image, buf = self.convert_image(orig_image)\n-        yield key, image, buf\n+        yield path, image, buf\n \n         for thumb_id, size in self.THUMBS.iteritems():\n-            thumb_key = self.thumb_key(request.url, thumb_id)\n+            thumb_path = self.thumb_path(request, thumb_id, response=response, info=info)\n             thumb_image, thumb_buf = self.convert_image(image, size)\n-            yield thumb_key, thumb_image, thumb_buf\n+            yield thumb_path, thumb_image, thumb_buf\n \n     def convert_image(self, image, size=None):\n         if image.format == 'PNG' and image.mode == 'RGBA':\n@@ -103,19 +100,30 @@ class ImagesPipeline(FilesPipeline):\n         image.save(buf, 'JPEG')\n         return image, buf\n \n-    def thumb_key(self, url, thumb_id):\n-        image_guid = hashlib.sha1(url).hexdigest()\n-        return 'thumbs/%s/%s.jpg' % (thumb_id, image_guid)\n-\n     def get_media_requests(self, item, info):\n         return [Request(x) for x in item.get(self.IMAGES_URLS_FIELD, [])]\n \n-    # backwards compatibility\n-    def image_key(self, url):\n-        media_guid = hashlib.sha1(url).hexdigest()\n-        return 'full/%s.jpg' % (media_guid)\n-\n     def item_completed(self, results, item, info):\n         if self.IMAGES_RESULT_FIELD in item.fields:\n             item[self.IMAGES_RESULT_FIELD] = [x for ok, x in results if ok]\n         return item\n+\n+    def file_path(self, request, response=None, info=None):\n+        return self.file_key(request.url)\n+\n+    def thumb_path(self, request, thumb_id, response=None, info=None):\n+        return self.thumb_key(request.url)\n+\n+    # deprecated\n+    def file_key(self, url):\n+        return self.image_key(url)\n+\n+    # deprecated\n+    def image_key(self, url):\n+        image_guid = hashlib.sha1(url).hexdigest()\n+        return 'full/%s.jpg' % (image_guid)\n+\n+    # deprecated\n+    def thumb_key(self, url, thumb_id):\n+        thumb_guid = hashlib.sha1(url).hexdigest()\n+        return 'thumbs/%s/%s.jpg' % (thumb_id, thumb_guid)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c29939862a0b11af07b355073e261ba68865e678", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 7 | Methods Changed: 9 | Complexity Δ (Sum/Max): 13/9 | Churn Δ: 90 | Churn Cumulative: 1476 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -267,10 +267,32 @@ class FilesPipeline(MediaPipeline):\n         return item\n \n     def file_path(self, request, response=None, info=None):\n-        return self.file_key(request.url)\n+        ## start of deprecation warning block (can be removed in the future)\n+        def _warn():\n+            from scrapy.exceptions import ScrapyDeprecationWarning\n+            import warnings\n+            warnings.warn('FilesPipeline.file_key(url) method is deprecated, please use '\n+                          'file_path(request, response=None, info=None) instead',\n+                          category=ScrapyDeprecationWarning, stacklevel=1)\n+\n+        # check if called from file_key with url as first argument\n+        if isinstance(request, unicode) or isinstance(request, str):\n+            _warn()\n+            url = request\n+        else:\n+            url = request.url\n+\n+        # detect if file_key() method has been overridden\n+        if not hasattr(self.file_key, '_overridden'):\n+            _warn()\n+            return self.file_key(url)\n+        ## end of deprecation warning block\n+\n+        media_guid = hashlib.sha1(url).hexdigest()  # change to request.url after deprecation\n+        media_ext = os.path.splitext(url)[1]  # change to request.url after deprecation\n+        return 'full/%s%s' % (media_guid, media_ext)\n \n     # deprecated\n     def file_key(self, url):\n-        media_guid = hashlib.sha1(url).hexdigest()\n-        media_ext = os.path.splitext(url)[1]\n-        return 'full/%s%s' % (media_guid, media_ext)\n+        return self.file_path(url)\n+    file_key._overridden = False\n\n@@ -109,21 +109,69 @@ class ImagesPipeline(FilesPipeline):\n         return item\n \n     def file_path(self, request, response=None, info=None):\n-        return self.file_key(request.url)\n+        ## start of deprecation warning block (can be removed in the future)\n+        def _warn():\n+            from scrapy.exceptions import ScrapyDeprecationWarning\n+            import warnings\n+            warnings.warn('ImagesPipeline.image_key(url) and file_key(url) methods are deprecated, '\n+                          'please use file_path(request, response=None, info=None) instead',\n+                          category=ScrapyDeprecationWarning, stacklevel=1)\n+\n+        # check if called from image_key or file_key with url as first argument\n+        if isinstance(request, unicode) or isinstance(request, str):\n+            _warn()\n+            url = request\n+        else:\n+            url = request.url\n+\n+        # detect if file_key() or image_key() methods have been overridden\n+        if not hasattr(self.file_key, '_overridden'):\n+            _warn()\n+            return self.file_key(url)\n+        elif not hasattr(self.image_key, '_overridden'):\n+            _warn()\n+            return self.image_key(url)\n+        ## end of deprecation warning block\n+\n+        image_guid = hashlib.sha1(url).hexdigest()  # change to request.url after deprecation\n+        return 'full/%s.jpg' % (image_guid)\n \n     def thumb_path(self, request, thumb_id, response=None, info=None):\n-        return self.thumb_key(request.url)\n+        ## start of deprecation warning block (can be removed in the future)\n+        def _warn():\n+            from scrapy.exceptions import ScrapyDeprecationWarning\n+            import warnings\n+            warnings.warn('ImagesPipeline.thumb_key(url) method is deprecated, please use '\n+                          'thumb_path(request, thumb_id, response=None, info=None) instead',\n+                          category=ScrapyDeprecationWarning, stacklevel=1)\n+\n+        # check if called from thumb_key with url as first argument\n+        if isinstance(request, unicode) or isinstance(request, str):\n+            _warn()\n+            url = request\n+        else:\n+            url = request.url\n+\n+        # detect if thumb_key() method has been overridden\n+        if not hasattr(self.thumb_key, '_overridden'):\n+            _warn()\n+            return self.thumb_key(url, thumb_id)\n+        ## end of deprecation warning block\n+\n+        thumb_guid = hashlib.sha1(url).hexdigest()  # change to request.url after deprecation\n+        return 'thumbs/%s/%s.jpg' % (thumb_id, thumb_guid)\n \n     # deprecated\n     def file_key(self, url):\n         return self.image_key(url)\n+    file_key._overridden = False\n \n     # deprecated\n     def image_key(self, url):\n-        image_guid = hashlib.sha1(url).hexdigest()\n-        return 'full/%s.jpg' % (image_guid)\n+        return self.file_path(url)\n+    image_key._overridden = False\n \n     # deprecated\n     def thumb_key(self, url, thumb_id):\n-        thumb_guid = hashlib.sha1(url).hexdigest()\n-        return 'thumbs/%s/%s.jpg' % (thumb_id, thumb_guid)\n+        return self.thumb_path(url, thumb_id)\n+    thumb_key._overridden = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#86a6d6898b3b4edcbdaa449a258309cda6760c23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 1492 | Contributors (this commit): 8 | Commits (past 90d): 8 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -283,7 +283,7 @@ class FilesPipeline(MediaPipeline):\n             url = request.url\n \n         # detect if file_key() method has been overridden\n-        if not hasattr(self.file_key, '_overridden'):\n+        if not hasattr(self.file_key, '_base'):\n             _warn()\n             return self.file_key(url)\n         ## end of deprecation warning block\n@@ -295,4 +295,4 @@ class FilesPipeline(MediaPipeline):\n     # deprecated\n     def file_key(self, url):\n         return self.file_path(url)\n-    file_key._overridden = False\n+    file_key._base = True\n\n@@ -125,10 +125,10 @@ class ImagesPipeline(FilesPipeline):\n             url = request.url\n \n         # detect if file_key() or image_key() methods have been overridden\n-        if not hasattr(self.file_key, '_overridden'):\n+        if not hasattr(self.file_key, '_base'):\n             _warn()\n             return self.file_key(url)\n-        elif not hasattr(self.image_key, '_overridden'):\n+        elif not hasattr(self.image_key, '_base'):\n             _warn()\n             return self.image_key(url)\n         ## end of deprecation warning block\n@@ -153,7 +153,7 @@ class ImagesPipeline(FilesPipeline):\n             url = request.url\n \n         # detect if thumb_key() method has been overridden\n-        if not hasattr(self.thumb_key, '_overridden'):\n+        if not hasattr(self.thumb_key, '_base'):\n             _warn()\n             return self.thumb_key(url, thumb_id)\n         ## end of deprecation warning block\n@@ -164,14 +164,14 @@ class ImagesPipeline(FilesPipeline):\n     # deprecated\n     def file_key(self, url):\n         return self.image_key(url)\n-    file_key._overridden = False\n+    file_key._base = True\n \n     # deprecated\n     def image_key(self, url):\n         return self.file_path(url)\n-    image_key._overridden = False\n+    image_key._base = True\n \n     # deprecated\n     def thumb_key(self, url, thumb_id):\n         return self.thumb_path(url, thumb_id)\n-    thumb_key._overridden = False\n+    thumb_key._base = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#220ced163c2013368311931c00ef7f72e4116371", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 6 | Churn Cumulative: 1498 | Contributors (this commit): 8 | Commits (past 90d): 10 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -276,7 +276,7 @@ class FilesPipeline(MediaPipeline):\n                           category=ScrapyDeprecationWarning, stacklevel=1)\n \n         # check if called from file_key with url as first argument\n-        if isinstance(request, unicode) or isinstance(request, str):\n+        if not isinstance(request, Request):\n             _warn()\n             url = request\n         else:\n\n@@ -118,7 +118,7 @@ class ImagesPipeline(FilesPipeline):\n                           category=ScrapyDeprecationWarning, stacklevel=1)\n \n         # check if called from image_key or file_key with url as first argument\n-        if isinstance(request, unicode) or isinstance(request, str):\n+        if not isinstance(request, Request):\n             _warn()\n             url = request\n         else:\n@@ -146,7 +146,7 @@ class ImagesPipeline(FilesPipeline):\n                           category=ScrapyDeprecationWarning, stacklevel=1)\n \n         # check if called from thumb_key with url as first argument\n-        if isinstance(request, unicode) or isinstance(request, str):\n+        if not isinstance(request, Request):\n             _warn()\n             url = request\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d58a4639cd8f1c0cc659f2268a560a4ff72bb2fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 23 | Files Changed: 2 | Hunks: 19 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 51 | Churn Cumulative: 391 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -30,18 +30,18 @@ class FilesPipelineTestCase(unittest.TestCase):\n         rmtree(self.tempdir)\n \n     def test_file_path(self):\n-        image_path = self.pipeline.file_key\n-        self.assertEqual(image_path(\"https://dev.mydeco.com/mydeco.pdf\"),\n+        image_path = self.pipeline.file_path\n+        self.assertEqual(image_path(Request(\"https://dev.mydeco.com/mydeco.pdf\")),\n                          'full/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf')\n-        self.assertEqual(image_path(\"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt\"),\n+        self.assertEqual(image_path(Request(\"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt\")),\n                          'full/4ce274dd83db0368bafd7e406f382ae088e39219.txt')\n-        self.assertEqual(image_path(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc\"),\n+        self.assertEqual(image_path(Request(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc\")),\n                          'full/94ccc495a17b9ac5d40e3eabf3afcb8c2c9b9e1a.doc')\n-        self.assertEqual(image_path(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\"),\n+        self.assertEqual(image_path(Request(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\")),\n                          'full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg')\n-        self.assertEqual(image_path(\"http://www.dorma.co.uk/images/product_details/2532/\"),\n+        self.assertEqual(image_path(Request(\"http://www.dorma.co.uk/images/product_details/2532/\")),\n                          'full/97ee6f8a46cbbb418ea91502fd24176865cf39b2')\n-        self.assertEqual(image_path(\"http://www.dorma.co.uk/images/product_details/2532\"),\n+        self.assertEqual(image_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\")),\n                          'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1')\n \n     def test_fs_store(self):\n@@ -93,6 +93,7 @@ class FilesPipelineTestCase(unittest.TestCase):\n         for p in patchers:\n             p.stop()\n \n+\n class FilesPipelineTestCaseFields(unittest.TestCase):\n \n     def test_item_fields_default(self):\n@@ -126,6 +127,7 @@ class FilesPipelineTestCaseFields(unittest.TestCase):\n         pipeline.item_completed(results, item, None)\n         self.assertEqual(item['stored_file'], [results[0][1]])\n \n+\n class ItemWithFiles(Item):\n     file_urls = Field()\n     files = Field()\n\n@@ -6,6 +6,7 @@ from shutil import rmtree\n from twisted.trial import unittest\n \n from scrapy.item import Item, Field\n+from scrapy.http import Request\n from scrapy.settings import Settings\n \n skip = False\n@@ -37,31 +38,31 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         rmtree(self.tempdir)\n \n     def test_image_path(self):\n-        image_path = self.pipeline.file_key\n-        self.assertEqual(image_path(\"https://dev.mydeco.com/mydeco.gif\"),\n+        image_path = self.pipeline.file_path\n+        self.assertEqual(image_path(Request(\"https://dev.mydeco.com/mydeco.gif\")),\n                          'full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n-        self.assertEqual(image_path(\"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.jpg\"),\n+        self.assertEqual(image_path(Request(\"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.jpg\")),\n                          'full/0ffcd85d563bca45e2f90becd0ca737bc58a00b2.jpg')\n-        self.assertEqual(image_path(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.gif\"),\n+        self.assertEqual(image_path(Request(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.gif\")),\n                          'full/b250e3a74fff2e4703e310048a5b13eba79379d2.jpg')\n-        self.assertEqual(image_path(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\"),\n+        self.assertEqual(image_path(Request(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\")),\n                          'full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg')\n-        self.assertEqual(image_path(\"http://www.dorma.co.uk/images/product_details/2532/\"),\n+        self.assertEqual(image_path(Request(\"http://www.dorma.co.uk/images/product_details/2532/\")),\n                          'full/97ee6f8a46cbbb418ea91502fd24176865cf39b2.jpg')\n-        self.assertEqual(image_path(\"http://www.dorma.co.uk/images/product_details/2532\"),\n+        self.assertEqual(image_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\")),\n                          'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1.jpg')\n \n     def test_thumbnail_name(self):\n-        thumbnail_name = self.pipeline.thumb_key\n+        thumb_path = self.pipeline.thumb_path\n         name = '50'\n-        self.assertEqual(thumbnail_name(\"/tmp/foo.jpg\", name),\n-                         'thumbs/50/271f172bb4727281011c80fe763e93a47bb6b3fe.jpg')\n-        self.assertEqual(thumbnail_name(\"foo.png\", name),\n-                         'thumbs/50/0945c699b5580b99e4f40dffc009699b2b6830a7.jpg')\n-        self.assertEqual(thumbnail_name(\"/tmp/foo\", name),\n-                         'thumbs/50/469150566bd728fc90b4adf6495202fd70ec3537.jpg')\n-        self.assertEqual(thumbnail_name(\"/tmp/some.name/foo\", name),\n-                         'thumbs/50/92dac2a6a2072c5695a5dff1f865b3cb70c657bb.jpg')\n+        self.assertEqual(thumb_path(Request(\"file:///tmp/foo.jpg\"), name),\n+                         'thumbs/50/38a86208c36e59d4404db9e37ce04be863ef0335.jpg')\n+        self.assertEqual(thumb_path(Request(\"file://foo.png\"), name),\n+                         'thumbs/50/e55b765eba0ec7348e50a1df496040449071b96a.jpg')\n+        self.assertEqual(thumb_path(Request(\"file:///tmp/foo\"), name),\n+                         'thumbs/50/0329ad83ebb8e93ea7c7906d46e9ed55f7349a50.jpg')\n+        self.assertEqual(thumb_path(Request(\"file:///tmp/some.name/foo\"), name),\n+                         'thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg')\n \n     def test_convert_image(self):\n         SIZE = (100, 100)\n@@ -84,6 +85,7 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         self.assertEquals(converted.mode, 'RGB')\n         self.assertEquals(converted.getcolors(), [(10000, (205, 230, 255))])\n \n+\n class ImagesPipelineTestCaseFields(unittest.TestCase):\n \n     def test_item_fields_default(self):\n@@ -117,6 +119,7 @@ class ImagesPipelineTestCaseFields(unittest.TestCase):\n         pipeline.item_completed(results, item, None)\n         self.assertEqual(item['stored_image'], [results[0][1]])\n \n+\n def _create_image(format, *a, **kw):\n     buf = StringIO()\n     Image.new(*a, **kw).save(buf, format)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#23c50bd9738aaf9e1244e3d9584855843ae165c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 182 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -48,9 +48,9 @@ class FilesPipelineTestCase(unittest.TestCase):\n         assert isinstance(self.pipeline.store, FSFilesStore)\n         self.assertEqual(self.pipeline.store.basedir, self.tempdir)\n \n-        key = 'some/image/key.jpg'\n-        path = os.path.join(self.tempdir, 'some', 'image', 'key.jpg')\n-        self.assertEqual(self.pipeline.store._get_filesystem_path(key), path)\n+        path = 'some/image/key.jpg'\n+        fullpath = os.path.join(self.tempdir, 'some', 'image', 'key.jpg')\n+        self.assertEqual(self.pipeline.store._get_filesystem_path(path), fullpath)\n \n     @defer.inlineCallbacks\n     def test_file_not_expired(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#14b68b47d67573cfcadf895033aeb6198d19bc55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 196 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -30,18 +30,18 @@ class FilesPipelineTestCase(unittest.TestCase):\n         rmtree(self.tempdir)\n \n     def test_file_path(self):\n-        image_path = self.pipeline.file_path\n-        self.assertEqual(image_path(Request(\"https://dev.mydeco.com/mydeco.pdf\")),\n+        file_path = self.pipeline.file_path\n+        self.assertEqual(file_path(Request(\"https://dev.mydeco.com/mydeco.pdf\")),\n                          'full/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf')\n-        self.assertEqual(image_path(Request(\"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt\")),\n+        self.assertEqual(file_path(Request(\"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt\")),\n                          'full/4ce274dd83db0368bafd7e406f382ae088e39219.txt')\n-        self.assertEqual(image_path(Request(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc\")),\n+        self.assertEqual(file_path(Request(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc\")),\n                          'full/94ccc495a17b9ac5d40e3eabf3afcb8c2c9b9e1a.doc')\n-        self.assertEqual(image_path(Request(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\")),\n+        self.assertEqual(file_path(Request(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\")),\n                          'full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg')\n-        self.assertEqual(image_path(Request(\"http://www.dorma.co.uk/images/product_details/2532/\")),\n+        self.assertEqual(file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532/\")),\n                          'full/97ee6f8a46cbbb418ea91502fd24176865cf39b2')\n-        self.assertEqual(image_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\")),\n+        self.assertEqual(file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\")),\n                          'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1')\n \n     def test_fs_store(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#65c017c5fe5fafe06900508f6d5a5f627dca619d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 41 | Churn Cumulative: 237 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,8 @@\n import mock\n import os\n import time\n+import hashlib\n+import warnings\n from tempfile import mkdtemp\n from shutil import rmtree\n \n@@ -94,6 +96,44 @@ class FilesPipelineTestCase(unittest.TestCase):\n             p.stop()\n \n \n+class DeprecatedFilesPipeline(FilesPipeline):\n+    def file_key(self, url):\n+        media_guid = hashlib.sha1(url).hexdigest()\n+        media_ext = os.path.splitext(url)[1]\n+        return 'empty/%s%s' % (media_guid, media_ext)\n+\n+\n+class DeprecatedFilesPipelineTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.tempdir = mkdtemp()\n+\n+    def init_pipeline(self, pipeline_class):\n+        self.pipeline = pipeline_class.from_settings(Settings({'FILES_STORE': self.tempdir}))\n+        self.pipeline.download_func = _mocked_download_func\n+        self.pipeline.open_spider(None)\n+\n+    def test_default_file_key_method(self):\n+        self.init_pipeline(FilesPipeline)\n+        with warnings.catch_warnings(record=True) as w:\n+            warnings.simplefilter('always')\n+            self.assertEqual(self.pipeline.file_key(\"https://dev.mydeco.com/mydeco.pdf\"),\n+                             'full/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf')\n+            self.assertEqual(len(w), 1)\n+            self.assertTrue('file_key(url) method is deprecated' in str(w[-1].message))\n+\n+    def test_overridden_file_key_method(self):\n+        self.init_pipeline(DeprecatedFilesPipeline)\n+        with warnings.catch_warnings(record=True) as w:\n+            warnings.simplefilter('always')\n+            self.assertEqual(self.pipeline.file_path(Request(\"https://dev.mydeco.com/mydeco.pdf\")),\n+                             'empty/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf')\n+            self.assertEqual(len(w), 1)\n+            self.assertTrue('file_key(url) method is deprecated' in str(w[-1].message))\n+\n+    def tearDown(self):\n+        rmtree(self.tempdir)\n+\n+\n class FilesPipelineTestCaseFields(unittest.TestCase):\n \n     def test_item_fields_default(self):\n@@ -144,5 +184,6 @@ def _prepare_request_object(item_url):\n         item_url,\n         meta={'response': Response(item_url, status=200, body='data')})\n \n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#69595233383f11d2c98a76c6e25f19d3899a3ad1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 11 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 74 | Churn Cumulative: 289 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,6 @@\n import os\n+import hashlib\n+import warnings\n from cStringIO import StringIO\n from tempfile import mkdtemp\n from shutil import rmtree\n@@ -8,6 +10,7 @@ from twisted.trial import unittest\n from scrapy.item import Item, Field\n from scrapy.http import Request\n from scrapy.settings import Settings\n+from scrapy.contrib.pipeline.images import ImagesPipeline\n \n skip = False\n try:\n@@ -30,7 +33,6 @@ class ImagesPipelineTestCase(unittest.TestCase):\n     skip = skip\n \n     def setUp(self):\n-        from scrapy.contrib.pipeline.images import ImagesPipeline\n         self.tempdir = mkdtemp()\n         self.pipeline = ImagesPipeline(self.tempdir, download_func=_mocked_download_func)\n \n@@ -86,6 +88,76 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         self.assertEquals(converted.getcolors(), [(10000, (205, 230, 255))])\n \n \n+class DeprecatedImagesPipeline(ImagesPipeline):\n+    def file_key(self, url):\n+        return self.image_key(url)\n+\n+    def image_key(self, url):\n+        image_guid = hashlib.sha1(url).hexdigest()\n+        return 'empty/%s.jpg' % (image_guid)\n+\n+    def thumb_key(self, url, thumb_id):\n+        thumb_guid = hashlib.sha1(url).hexdigest()\n+        return 'thumbsup/%s/%s.jpg' % (thumb_id, thumb_guid)\n+\n+\n+class DeprecatedImagesPipelineTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.tempdir = mkdtemp()\n+\n+    def init_pipeline(self, pipeline_class):\n+        self.pipeline = pipeline_class(self.tempdir, download_func=_mocked_download_func)\n+        self.pipeline.open_spider(None)\n+\n+    def test_default_file_key_method(self):\n+        self.init_pipeline(ImagesPipeline)\n+        with warnings.catch_warnings(record=True) as w:\n+            warnings.simplefilter('always')\n+            self.assertEqual(self.pipeline.file_key(\"https://dev.mydeco.com/mydeco.gif\"),\n+                             'full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n+            self.assertEqual(len(w), 1)\n+            self.assertTrue('image_key(url) and file_key(url) methods are deprecated' in str(w[-1].message))\n+\n+    def test_default_image_key_method(self):\n+        self.init_pipeline(ImagesPipeline)\n+        with warnings.catch_warnings(record=True) as w:\n+            warnings.simplefilter('always')\n+            self.assertEqual(self.pipeline.image_key(\"https://dev.mydeco.com/mydeco.gif\"),\n+                             'full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n+            self.assertEqual(len(w), 1)\n+            self.assertTrue('image_key(url) and file_key(url) methods are deprecated' in str(w[-1].message))\n+\n+    def test_overridden_file_key_method(self):\n+        self.init_pipeline(DeprecatedImagesPipeline)\n+        with warnings.catch_warnings(record=True) as w:\n+            warnings.simplefilter('always')\n+            self.assertEqual(self.pipeline.file_path(Request(\"https://dev.mydeco.com/mydeco.gif\")),\n+                             'empty/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n+            self.assertEqual(len(w), 1)\n+            self.assertTrue('image_key(url) and file_key(url) methods are deprecated' in str(w[-1].message))\n+\n+    def test_default_thumb_key_method(self):\n+        self.init_pipeline(ImagesPipeline)\n+        with warnings.catch_warnings(record=True) as w:\n+            warnings.simplefilter('always')\n+            self.assertEqual(self.pipeline.thumb_key(\"file:///tmp/foo.jpg\", 50),\n+                             'thumbs/50/38a86208c36e59d4404db9e37ce04be863ef0335.jpg')\n+            self.assertEqual(len(w), 1)\n+            self.assertTrue('thumb_key(url) method is deprecated' in str(w[-1].message))\n+\n+    def test_overridden_thumb_key_method(self):\n+        self.init_pipeline(DeprecatedImagesPipeline)\n+        with warnings.catch_warnings(record=True) as w:\n+            warnings.simplefilter('always')\n+            self.assertEqual(self.pipeline.thumb_path(Request(\"file:///tmp/foo.jpg\"), 50),\n+                             'thumbsup/50/38a86208c36e59d4404db9e37ce04be863ef0335.jpg')\n+            self.assertEqual(len(w), 1)\n+            self.assertTrue('thumb_key(url) method is deprecated' in str(w[-1].message))\n+\n+    def tearDown(self):\n+        rmtree(self.tempdir)\n+\n+\n class ImagesPipelineTestCaseFields(unittest.TestCase):\n \n     def test_item_fields_default(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
