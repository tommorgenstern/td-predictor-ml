{"custom_id": "scrapy#ca320feb2afa5eae3990ed21df6a8df930e8cf9f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 33 | Churn Cumulative: 1961 | Contributors (this commit): 60 | Commits (past 90d): 5 | Contributors (cumulative): 88 | DMM Complexity: 1.0\n\nDIFF:\n@@ -207,6 +207,7 @@ LOG_DATEFORMAT = '%Y-%m-%d %H:%M:%S'\n LOG_STDOUT = False\n LOG_LEVEL = 'DEBUG'\n LOG_FILE = None\n+LOG_FILE_APPEND = True\n LOG_SHORT_NAMES = False\n \n SCHEDULER_DEBUG = False\n\n@@ -124,8 +124,9 @@ def _get_handler(settings):\n     \"\"\" Return a log handler object according to settings \"\"\"\n     filename = settings.get('LOG_FILE')\n     if filename:\n+        mode = 'a' if settings.getbool('LOG_FILE_APPEND') else 'w'\n         encoding = settings.get('LOG_ENCODING')\n-        handler = logging.FileHandler(filename, encoding=encoding)\n+        handler = logging.FileHandler(filename, mode=mode, encoding=encoding)\n     elif settings.getbool('LOG_ENABLED'):\n         handler = logging.StreamHandler()\n     else:\n\n@@ -98,6 +98,8 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n \n     def test_spider_custom_settings_log_level(self):\n         log_file = self.mktemp()\n+        with open(log_file, 'wb') as fo:\n+            fo.write('previous message\\n'.encode('utf-8'))\n \n         class MySpider(scrapy.Spider):\n             name = 'spider'\n@@ -119,8 +121,9 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n         logging.error('error message')\n \n         with open(log_file, 'rb') as fo:\n-            logged = fo.read().decode('utf8')\n+            logged = fo.read().decode('utf-8')\n \n+        self.assertIn('previous message', logged)\n         self.assertNotIn('debug message', logged)\n         self.assertIn('info message', logged)\n         self.assertIn('warning message', logged)\n@@ -131,6 +134,30 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n             crawler.stats.get_value('log_count/INFO') - info_count, 1)\n         self.assertEqual(crawler.stats.get_value('log_count/DEBUG', 0), 0)\n \n+    def test_spider_custom_settings_log_append(self):\n+        log_file = self.mktemp()\n+        with open(log_file, 'wb') as fo:\n+            fo.write('previous message\\n'.encode('utf-8'))\n+\n+        class MySpider(scrapy.Spider):\n+            name = 'spider'\n+            custom_settings = {\n+                'LOG_FILE': log_file,\n+                'LOG_FILE_APPEND': False,\n+                # disable telnet if not available to avoid an extra warning\n+                'TELNETCONSOLE_ENABLED': telnet.TWISTED_CONCH_AVAILABLE,\n+            }\n+\n+        configure_logging()\n+        crawler = Crawler(MySpider, {})\n+        logging.debug('debug message')\n+\n+        with open(log_file, 'rb') as fo:\n+            logged = fo.read().decode('utf-8')\n+\n+        self.assertNotIn('previous message', logged)\n+        self.assertIn('debug message', logged)\n+\n \n class SpiderLoaderWithWrongInterface:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d774d6a9c46cb9697d8fe64a55da9ae321be7539", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 864 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -149,7 +149,7 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n             }\n \n         configure_logging()\n-        crawler = Crawler(MySpider, {})\n+        Crawler(MySpider, {})\n         logging.debug('debug message')\n \n         with open(log_file, 'rb') as fo:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cfff79cee6a97528185b7d24e2b660b99c07945f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 609 | Contributors (this commit): 29 | Commits (past 90d): 1 | Contributors (cumulative): 29 | DMM Complexity: None\n\nDIFF:\n@@ -87,6 +87,7 @@ setup(\n         'Programming Language :: Python :: 3.7',\n         'Programming Language :: Python :: 3.8',\n         'Programming Language :: Python :: 3.9',\n+        'Programming Language :: Python :: 3.10',\n         'Programming Language :: Python :: Implementation :: CPython',\n         'Programming Language :: Python :: Implementation :: PyPy',\n         'Topic :: Internet :: WWW/HTTP',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#144d1eb8341c427fa1fae109db3e9487f255d3fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/3 | Churn Δ: 99 | Churn Cumulative: 1080 | Contributors (this commit): 25 | Commits (past 90d): 4 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -75,6 +75,12 @@ def only_asyncio(request, reactor_pytest):\n         pytest.skip('This test is only run with --reactor=asyncio')\n \n \n+@pytest.fixture(autouse=True)\n+def only_not_asyncio(request, reactor_pytest):\n+    if request.node.get_closest_marker('only_not_asyncio') and reactor_pytest == 'asyncio':\n+        pytest.skip('This test is only run without --reactor=asyncio')\n+\n+\n def pytest_configure(config):\n     if config.getoption(\"--reactor\") == \"asyncio\":\n         install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n\n@@ -3,9 +3,16 @@ Helper functions for dealing with Twisted deferreds\n \"\"\"\n import asyncio\n import inspect\n-from collections.abc import Coroutine\n+from asyncio import Future\n from functools import wraps\n-from typing import Any, Callable, Generator, Iterable\n+from typing import (\n+    Any,\n+    Callable,\n+    Coroutine,\n+    Generator,\n+    Iterable,\n+    Union\n+)\n \n from twisted.internet import defer\n from twisted.internet.defer import Deferred, DeferredList, ensureDeferred\n@@ -171,3 +178,55 @@ def maybeDeferred_coro(f: Callable, *args, **kw) -> Deferred:\n         return defer.fail(result)\n     else:\n         return defer.succeed(result)\n+\n+\n+def deferred_to_future(d: Deferred) -> Future:\n+    \"\"\"\n+    .. versionadded:: VERSION\n+\n+    Return an :class:`asyncio.Future` object that wraps *d*.\n+\n+    When :ref:`using the asyncio reactor <install-asyncio>`, you cannot await\n+    on :class:`~twisted.internet.defer.Deferred` objects from :ref:`Scrapy\n+    callables defined as coroutines <coroutine-support>`, you can only await on\n+    ``Future`` objects. Wrapping ``Deferred`` objects into ``Future`` objects\n+    allows you to wait on them::\n+\n+        class MySpider(Spider):\n+            ...\n+            async def parse(self, response):\n+                d = treq.get('https://example.com/additional')\n+                additional_response = await deferred_to_future(d)\n+    \"\"\"\n+    return d.asFuture(asyncio.get_event_loop())\n+\n+\n+def maybe_deferred_to_future(d: Deferred) -> Union[Deferred, Future]:\n+    \"\"\"\n+    .. versionadded:: VERSION\n+\n+    Return *d* as an object that can be awaited from a :ref:`Scrapy callable\n+    defined as a coroutine <coroutine-support>`.\n+\n+    What you can await in Scrapy callables defined as coroutines depends on the\n+    value of :setting:`TWISTED_REACTOR`:\n+\n+    -   When not using the asyncio reactor, you can only await on\n+        :class:`~twisted.internet.defer.Deferred` objects.\n+\n+    -   When :ref:`using the asyncio reactor <install-asyncio>`, you can only\n+        await on :class:`asyncio.Future` objects.\n+\n+    If you want to write code that uses ``Deferred`` objects but works with any\n+    reactor, use this function on all ``Deferred`` objects::\n+\n+        class MySpider(Spider):\n+            ...\n+            async def parse(self, response):\n+                d = treq.get('https://example.com/additional')\n+                extra_response = await maybe_deferred_to_future(d)\n+    \"\"\"\n+    if not is_asyncio_reactor_installed():\n+        return d\n+    else:\n+        return deferred_to_future(d)\n\n@@ -6,6 +6,7 @@ from twisted.internet.defer import Deferred\n from twisted.trial import unittest\n \n from scrapy import Spider, signals, Request\n+from scrapy.utils.defer import maybe_deferred_to_future, deferred_to_future\n from scrapy.utils.test import get_crawler, get_from_asyncio_queue\n \n from tests.mockserver import MockServer\n@@ -31,18 +32,38 @@ class DeferredPipeline:\n \n class AsyncDefPipeline:\n     async def process_item(self, item, spider):\n-        await defer.succeed(42)\n+        d = Deferred()\n+        from twisted.internet import reactor\n+        reactor.callLater(0, d.callback, None)\n+        await maybe_deferred_to_future(d)\n         item['pipeline_passed'] = True\n         return item\n \n \n class AsyncDefAsyncioPipeline:\n     async def process_item(self, item, spider):\n+        d = Deferred()\n+        from twisted.internet import reactor\n+        reactor.callLater(0, d.callback, None)\n+        await deferred_to_future(d)\n         await asyncio.sleep(0.2)\n         item['pipeline_passed'] = await get_from_asyncio_queue(True)\n         return item\n \n \n+class AsyncDefNotAsyncioPipeline:\n+    async def process_item(self, item, spider):\n+        d1 = Deferred()\n+        from twisted.internet import reactor\n+        reactor.callLater(0, d1.callback, None)\n+        await d1\n+        d2 = Deferred()\n+        reactor.callLater(0, d2.callback, None)\n+        await maybe_deferred_to_future(d2)\n+        item['pipeline_passed'] = True\n+        return item\n+\n+\n class ItemSpider(Spider):\n     name = 'itemspider'\n \n@@ -99,3 +120,10 @@ class PipelineTestCase(unittest.TestCase):\n         crawler = self._create_crawler(AsyncDefAsyncioPipeline)\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertEqual(len(self.items), 1)\n+\n+    @mark.only_not_asyncio()\n+    @defer.inlineCallbacks\n+    def test_asyncdef_not_asyncio_pipeline(self):\n+        crawler = self._create_crawler(AsyncDefNotAsyncioPipeline)\n+        yield crawler.crawl(mockserver=self.mockserver)\n+        self.assertEqual(len(self.items), 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#67994d1dddcba4c1fe53dd7bdf7b978d1733ae1b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 41 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,11 @@ from doctest import ELLIPSIS, NORMALIZE_WHITESPACE\n \n from scrapy.http.response.html import HtmlResponse\n from sybil import Sybil\n-from sybil.parsers.codeblock import CodeBlockParser\n+try:\n+    # >2.0.1\n+    from sybil.parsers.codeblock import PythonCodeBlockParser\n+except ImportError:\n+    from sybil.parsers.codeblock import CodeBlockParser as PythonCodeBlockParser\n from sybil.parsers.doctest import DocTestParser\n from sybil.parsers.skip import skip\n \n@@ -21,7 +25,7 @@ def setup(namespace):\n pytest_collect_file = Sybil(\n     parsers=[\n         DocTestParser(optionflags=ELLIPSIS | NORMALIZE_WHITESPACE),\n-        CodeBlockParser(future_imports=['print_function']),\n+        PythonCodeBlockParser(future_imports=['print_function']),\n         skip,\n     ],\n     pattern='*.rst',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#55cce25a799ab0ed9d5edd60ff0f35988318e9b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 22 | Churn Cumulative: 1038 | Contributors (this commit): 17 | Commits (past 90d): 1 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -274,6 +274,28 @@ with multiples lines\n         self.assertEqual(s['engine.spider.name'], crawler.spider.name)\n         self.assertEqual(s['len(engine.scraper.slot.active)'], 1)\n \n+    @defer.inlineCallbacks\n+    def test_format_engine_status(self):\n+        from scrapy.utils.engine import format_engine_status\n+        est = []\n+\n+        def cb(response):\n+            est.append(format_engine_status(crawler.engine))\n+\n+        crawler = self.runner.create_crawler(SingleRequestSpider)\n+        yield crawler.crawl(seed=self.mockserver.url('/'), callback_func=cb, mockserver=self.mockserver)\n+        self.assertEqual(len(est), 1, est)\n+        est = est[0].split(\"\\n\")[2:-2]  # remove header & footer\n+        # convert to dict\n+        est = [x.split(\":\") for x in est]\n+        est = [x for sublist in est for x in sublist]  # flatten\n+        est = [x.lstrip().rstrip() for x in est]\n+        it = iter(est)\n+        s = dict(zip(it, it))\n+\n+        self.assertEqual(s['engine.spider.name'], crawler.spider.name)\n+        self.assertEqual(s['len(engine.scraper.slot.active)'], '1')\n+\n     @defer.inlineCallbacks\n     def test_graceful_crawl_error_handling(self):\n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f2c800c5c9f88b4b583181e9cf49eb3cd8d538f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 404 | Contributors (this commit): 12 | Commits (past 90d): 1 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,8 +3,9 @@ This module provides some useful functions for working with\n scrapy.http.Response objects\n \"\"\"\n import os\n-import webbrowser\n+import re\n import tempfile\n+import webbrowser\n from typing import Any, Callable, Iterable, Optional, Tuple, Union\n from weakref import WeakKeyDictionary\n \n@@ -80,8 +81,9 @@ def open_in_browser(\n     body = response.body\n     if isinstance(response, HtmlResponse):\n         if b'<base' not in body:\n-            repl = f'<head><base href=\"{response.url}\">'\n-            body = body.replace(b'<head>', to_bytes(repl))\n+            repl = fr'\\1<base href=\"{response.url}\">'\n+            body = re.sub(b\"<!--.*?-->\", b\"\", body, flags=re.DOTALL)\n+            body = re.sub(rb\"(<head(?:>|\\s.*?>))\", to_bytes(repl), body)\n         ext = '.html'\n     elif isinstance(response, TextResponse):\n         ext = '.txt'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#75ed765476a2ac66ea8f52e7b29186864f65535c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 53 | Churn Cumulative: 115 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -83,3 +83,56 @@ class ResponseUtilsTest(unittest.TestCase):\n         self.assertEqual(response_status_message(200), '200 OK')\n         self.assertEqual(response_status_message(404), '404 Not Found')\n         self.assertEqual(response_status_message(573), \"573 Unknown Status\")\n+\n+    def test_inject_base_url(self):\n+        url = \"http://www.example.com\"\n+\n+        def check_base_url(burl):\n+            path = urlparse(burl).path\n+            if not os.path.exists(path):\n+                path = burl.replace('file://', '')\n+            with open(path, \"rb\") as f:\n+                bbody = f.read()\n+            self.assertEqual(bbody.count(b'<base href=\"' + to_bytes(url) + b'\">'), 1)\n+            return True\n+\n+        r1 = HtmlResponse(url, body=b\"\"\"\n+        <html>\n+            <head><title>Dummy</title></head>\n+            <body><p>Hello world.</p></body>\n+        </html>\"\"\")\n+        r2 = HtmlResponse(url, body=b\"\"\"\n+        <html>\n+            <head id=\"foo\"><title>Dummy</title></head>\n+            <body>Hello world.</body>\n+        </html>\"\"\")\n+        r3 = HtmlResponse(url, body=b\"\"\"\n+        <html>\n+            <head><title>Dummy</title></head>\n+            <body>\n+                <header>Hello header</header>\n+                <p>Hello world.</p>\n+            </body>\n+        </html>\"\"\")\n+        r4 = HtmlResponse(url, body=b\"\"\"\n+        <html>\n+            <!-- <head>Dummy comment</head> -->\n+            <head><title>Dummy</title></head>\n+            <body><p>Hello world.</p></body>\n+        </html>\"\"\")\n+        r5 = HtmlResponse(url, body=b\"\"\"\n+        <html>\n+            <!--[if IE]>\n+            <head><title>IE head</title></head>\n+            <![endif]-->\n+            <!--[if !IE]>-->\n+            <head><title>Standard head</title></head>\n+            <!--<![endif]-->\n+            <body><p>Hello world.</p></body>\n+        </html>\"\"\")\n+\n+        assert open_in_browser(r1, _openfunc=check_base_url), \"Inject base url\"\n+        assert open_in_browser(r2, _openfunc=check_base_url), \"Inject base url with argumented head\"\n+        assert open_in_browser(r3, _openfunc=check_base_url), \"Inject unique base url with misleading tag\"\n+        assert open_in_browser(r4, _openfunc=check_base_url), \"Inject unique base url with misleading comment\"\n+        assert open_in_browser(r5, _openfunc=check_base_url), \"Inject unique base url with conditional comment\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c316ca45a5b1b19622c96049c9378d8c45adba60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 8 | Churn Cumulative: 4011 | Contributors (this commit): 35 | Commits (past 90d): 3 | Contributors (cumulative): 42 | DMM Complexity: None\n\nDIFF:\n@@ -213,7 +213,7 @@ class TunnelingAgent(Agent):\n         # proxy host and port are required for HTTP pool `key`\n         # otherwise, same remote host connection request could reuse\n         # a cached tunneled connection to a different proxy\n-        key = key + self._proxyConf\n+        key += self._proxyConf\n         return super()._requestWithEndpoint(\n             key=key,\n             endpoint=endpoint,\n\n@@ -285,8 +285,8 @@ class Stream:\n \n             self._protocol.conn.send_data(self.stream_id, data_chunk, end_stream=False)\n \n-            bytes_to_send_size = bytes_to_send_size - chunk_size\n-            self.metadata['remaining_content_length'] = self.metadata['remaining_content_length'] - chunk_size\n+            bytes_to_send_size -= chunk_size\n+            self.metadata['remaining_content_length'] -= chunk_size\n \n         self.metadata['remaining_content_length'] = max(0, self.metadata['remaining_content_length'])\n \n\n@@ -22,7 +22,7 @@ class SignalCatcherSpider(Spider):\n         return spider\n \n     def on_request_left(self, request, spider):\n-        self.caught_times = self.caught_times + 1\n+        self.caught_times += 1\n \n \n class TestCatching(TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eea56c4912ebabf94d366b4ead0d0d0a40a1d682", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 824 | Contributors (this commit): 13 | Commits (past 90d): 1 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -83,6 +83,7 @@ class Downloader:\n         self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n         self._slot_gc_loop = task.LoopingCall(self._slot_gc)\n         self._slot_gc_loop.start(60)\n+        self.per_slot_settings = self.settings.getdict('PER_SLOT_SETTINGS', {})\n \n     def fetch(self, request, spider):\n         def _deactivate(response):\n@@ -99,9 +100,13 @@ class Downloader:\n     def _get_slot(self, request, spider):\n         key = self._get_slot_key(request, spider)\n         if key not in self.slots:\n-            conc = self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n+            conc = self.per_slot_settings.get(key,{}).get('concurrency', self.ip_concurrency if self.ip_concurrency else self.domain_concurrency)\n             conc, delay = _get_concurrency_delay(conc, spider, self.settings)\n-            self.slots[key] = Slot(conc, delay, self.randomize_delay)\n+            delay = self.per_slot_settings.get(key,{}).get('delay', delay)\n+            randomize_delay = self.per_slot_settings.get(key, {}).get('randomize_delay', self.randomize_delay)\n+            new_slot = Slot(conc, delay, randomize_delay)\n+            self.slots[key] = new_slot\n+\n \n         return key, self.slots[key]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d5acf88ca55988a6c92046da5ef2bf065b0b3e72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 828 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import logging\n import random\n from time import time\n from datetime import datetime\n@@ -12,6 +13,7 @@ from scrapy import signals\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.core.downloader.handlers import DownloadHandlers\n \n+logger = logging.getLogger(__name__)\n \n class Slot:\n     \"\"\"Downloader slot\"\"\"\n@@ -106,7 +108,7 @@ class Downloader:\n             randomize_delay = self.per_slot_settings.get(key, {}).get('randomize_delay', self.randomize_delay)\n             new_slot = Slot(conc, delay, randomize_delay)\n             self.slots[key] = new_slot\n-\n+            logger.debug(f\"Downloader slot created {'from per slot settings' if key in self.per_slot_settings.keys() else ''}: {new_slot}\")\n \n         return key, self.slots[key]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1b7d7ecfcdb6067f3dccd69fc56e25dc1dbf498f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 836 | Contributors (this commit): 13 | Commits (past 90d): 3 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -108,7 +108,8 @@ class Downloader:\n             randomize_delay = self.per_slot_settings.get(key, {}).get('randomize_delay', self.randomize_delay)\n             new_slot = Slot(conc, delay, randomize_delay)\n             self.slots[key] = new_slot\n-            logger.debug(f\"Downloader slot created {'from per slot settings' if key in self.per_slot_settings.keys() else ''}: {new_slot}\")\n+            logger.debug(\n+                f\"Downloader slot '{key}' - created {'(from per slot settings)' if key in self.per_slot_settings.keys() else ''}: {new_slot}\")\n \n         return key, self.slots[key]\n \n@@ -205,4 +206,7 @@ class Downloader:\n         mintime = time() - age\n         for key, slot in list(self.slots.items()):\n             if not slot.active and slot.lastseen + slot.delay < mintime:\n-                self.slots.pop(key).close()\n+                inactive_slot = self.slots.pop(key)\n+                inactive_slot.close()\n+                logger.debug(\n+                    f\"Downloader slot '{key}' - closed {'(from per slot settings)' if key in self.per_slot_settings.keys() else ''} : {inactive_slot}\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8185aa5265c663326474897c24aa0691a2ee0e5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 837 | Contributors (this commit): 13 | Commits (past 90d): 4 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -15,6 +15,7 @@ from scrapy.core.downloader.handlers import DownloadHandlers\n \n logger = logging.getLogger(__name__)\n \n+\n class Slot:\n     \"\"\"Downloader slot\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4cc039628eb4861f98f7997e90125745e30f8687", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 11 | Methods Changed: 9 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 24 | Churn Cumulative: 1329 | Contributors (this commit): 19 | Commits (past 90d): 3 | Contributors (cumulative): 31 | DMM Complexity: 0.8571428571428571\n\nDIFF:\n@@ -3,7 +3,7 @@ Downloader Middleware manager\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n-from typing import Callable, Union\n+from typing import Callable, Union, cast\n \n from twisted.internet import defer\n from twisted.python.failure import Failure\n@@ -37,6 +37,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n         @defer.inlineCallbacks\n         def process_request(request: Request):\n             for method in self.methods['process_request']:\n+                method = cast(Callable, method)\n                 response = yield deferred_from_coro(method(request=request, spider=spider))\n                 if response is not None and not isinstance(response, (Response, Request)):\n                     raise _InvalidOutput(\n@@ -55,6 +56,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                 return response\n \n             for method in self.methods['process_response']:\n+                method = cast(Callable, method)\n                 response = yield deferred_from_coro(method(request=request, response=response, spider=spider))\n                 if not isinstance(response, (Response, Request)):\n                     raise _InvalidOutput(\n@@ -69,6 +71,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n         def process_exception(failure: Failure):\n             exception = failure.value\n             for method in self.methods['process_exception']:\n+                method = cast(Callable, method)\n                 response = yield deferred_from_coro(method(request=request, exception=exception, spider=spider))\n                 if response is not None and not isinstance(response, (Response, Request)):\n                     raise _InvalidOutput(\n\n@@ -4,7 +4,7 @@ Spider Middleware manager\n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n from itertools import islice\n-from typing import Any, Callable, Generator, Iterable, Union\n+from typing import Any, Callable, Generator, Iterable, Union, cast\n \n from twisted.internet.defer import Deferred\n from twisted.python.failure import Failure\n@@ -47,6 +47,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n     def _process_spider_input(self, scrape_func: ScrapeFunc, response: Response, request: Request,\n                               spider: Spider) -> Any:\n         for method in self.methods['process_spider_input']:\n+            method = cast(Callable, method)\n             try:\n                 result = method(response=response, spider=spider)\n                 if result is not None:\n\n@@ -1,7 +1,7 @@\n import logging\n import pprint\n from collections import defaultdict, deque\n-from typing import Callable, Deque, Dict\n+from typing import Callable, Deque, Dict, Optional, cast, Iterable\n \n from twisted.internet.defer import Deferred\n \n@@ -21,7 +21,8 @@ class MiddlewareManager:\n \n     def __init__(self, *middlewares):\n         self.middlewares = middlewares\n-        self.methods: Dict[str, Deque[Callable]] = defaultdict(deque)\n+        # Optional because process_spider_output and process_spider_exception can be None\n+        self.methods: Dict[str, Deque[Optional[Callable]]] = defaultdict(deque)\n         for mw in middlewares:\n             self._add_middleware(mw)\n \n@@ -64,14 +65,17 @@ class MiddlewareManager:\n             self.methods['close_spider'].appendleft(mw.close_spider)\n \n     def _process_parallel(self, methodname: str, obj, *args) -> Deferred:\n-        return process_parallel(self.methods[methodname], obj, *args)\n+        methods = cast(Iterable[Callable], self.methods[methodname])\n+        return process_parallel(methods, obj, *args)\n \n     def _process_chain(self, methodname: str, obj, *args) -> Deferred:\n-        return process_chain(self.methods[methodname], obj, *args)\n+        methods = cast(Iterable[Callable], self.methods[methodname])\n+        return process_chain(methods, obj, *args)\n \n     def _process_chain_both(self, cb_methodname: str, eb_methodname: str, obj, *args) -> Deferred:\n-        return process_chain_both(self.methods[cb_methodname],\n-                                  self.methods[eb_methodname], obj, *args)\n+        cb_methods = cast(Iterable[Callable], self.methods[cb_methodname])\n+        eb_methods = cast(Iterable[Callable], self.methods[eb_methodname])\n+        return process_chain_both(cb_methods, eb_methods, obj, *args)\n \n     def open_spider(self, spider: Spider) -> Deferred:\n         return self._process_parallel('open_spider', spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb62906c3e4c1e1f8e3e6c7965a04d5e65c61907", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 479 | Contributors (this commit): 17 | Commits (past 90d): 2 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -143,7 +143,7 @@ def _get_handler(settings):\n     return handler\n \n \n-def log_scrapy_info(settings):\n+def log_scrapy_info(settings: Settings) -> None:\n     logger.info(\"Scrapy %(version)s started (bot: %(bot)s)\",\n                 {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})\n     versions = [\n@@ -152,6 +152,10 @@ def log_scrapy_info(settings):\n         if name != \"Scrapy\"\n     ]\n     logger.info(\"Versions: %(versions)s\", {'versions': \", \".join(versions)})\n+    log_reactor_info()\n+\n+\n+def log_reactor_info() -> None:\n     from twisted.internet import reactor\n     logger.debug(\"Using reactor: %s.%s\", reactor.__module__, reactor.__class__.__name__)\n     from twisted.internet import asyncioreactor\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6483dfdbe17cd66c409435b95a05850a3c94b5ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1790 | Contributors (this commit): 28 | Commits (past 90d): 1 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -278,7 +278,6 @@ class CrawlerProcess(CrawlerRunner):\n \n     def __init__(self, settings=None, install_root_handler=True):\n         super().__init__(settings)\n-        install_shutdown_handlers(self._signal_shutdown)\n         configure_logging(self.settings, install_root_handler)\n         log_scrapy_info(self.settings)\n \n@@ -318,6 +317,7 @@ class CrawlerProcess(CrawlerRunner):\n                 return\n             d.addBoth(self._stop_reactor)\n \n+        install_shutdown_handlers(self._signal_shutdown)\n         resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n         resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)\n         resolver.install_on_reactor()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d6a384b3cfdb36cd19b32942479487a9e47c244b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 105 | Contributors (this commit): 12 | Commits (past 90d): 1 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -43,14 +43,14 @@ class ScrapyCommand:\n \n     def long_desc(self):\n         \"\"\"A long description of the command. Return short description when not\n-        available. It cannot contain newlines, since contents will be formatted\n+        available. It cannot contain newlines since contents will be formatted\n         by optparser which removes newlines and wraps text.\n         \"\"\"\n         return self.short_desc()\n \n     def help(self):\n         \"\"\"An extensive help for the command. It will be shown when using the\n-        \"help\" command. It can contain newlines, since no post-formatting will\n+        \"help\" command. It can contain newlines since no post-formatting will\n         be applied to its contents.\n         \"\"\"\n         return self.long_desc()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#46ef9cf771789f1db513bbf2f65243d3320ce695", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 2091 | Contributors (this commit): 33 | Commits (past 90d): 3 | Contributors (cumulative): 41 | DMM Complexity: 1.0\n\nDIFF:\n@@ -75,6 +75,6 @@ class Command(ScrapyCommand):\n \n     def _start_crawler_thread(self):\n         t = Thread(target=self.crawler_process.start,\n-                   kwargs={'stop_after_crawl': False})\n+                   kwargs={'stop_after_crawl': False, 'install_signal_handlers': False})\n         t.daemon = True\n         t.start()\n\n@@ -297,7 +297,7 @@ class CrawlerProcess(CrawlerRunner):\n                     {'signame': signame})\n         reactor.callFromThread(self._stop_reactor)\n \n-    def start(self, stop_after_crawl=True):\n+    def start(self, stop_after_crawl=True, install_signal_handlers=True):\n         \"\"\"\n         This method starts a :mod:`~twisted.internet.reactor`, adjusts its pool\n         size to :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache\n@@ -308,6 +308,9 @@ class CrawlerProcess(CrawlerRunner):\n \n         :param bool stop_after_crawl: stop or not the reactor when all\n             crawlers have finished\n+\n+        :param bool install_signal_handlers: whether to install the shutdown\n+            handlers (default: True)\n         \"\"\"\n         from twisted.internet import reactor\n         if stop_after_crawl:\n@@ -317,6 +320,7 @@ class CrawlerProcess(CrawlerRunner):\n                 return\n             d.addBoth(self._stop_reactor)\n \n+        if install_signal_handlers:\n             install_shutdown_handlers(self._signal_shutdown)\n         resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n         resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#60c8838554a79e70c22a7c6a57baedfcaf521444", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 13 | Files Changed: 3 | Hunks: 12 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 37 | Churn Cumulative: 3176 | Contributors (this commit): 33 | Commits (past 90d): 9 | Contributors (cumulative): 60 | DMM Complexity: 0.0\n\nDIFF:\n@@ -25,6 +25,7 @@ from scrapy.utils.log import (\n     configure_logging,\n     get_scrapy_root_handler,\n     install_scrapy_root_handler,\n+    log_reactor_info,\n     log_scrapy_info,\n     LogCounterHandler,\n )\n@@ -38,7 +39,7 @@ logger = logging.getLogger(__name__)\n \n class Crawler:\n \n-    def __init__(self, spidercls, settings=None):\n+    def __init__(self, spidercls, settings=None, init_reactor: bool = False):\n         if isinstance(spidercls, Spider):\n             raise ValueError('The spidercls argument must be a class, not an object')\n \n@@ -69,6 +70,19 @@ class Crawler:\n \n         lf_cls = load_object(self.settings['LOG_FORMATTER'])\n         self.logformatter = lf_cls.from_crawler(self)\n+\n+        if init_reactor:\n+            # this needs to be done after the spider settings are merged,\n+            # but before something imports twisted.internet.reactor\n+            if self.settings.get(\"TWISTED_REACTOR\"):\n+                install_reactor(self.settings[\"TWISTED_REACTOR\"], self.settings[\"ASYNCIO_EVENT_LOOP\"])\n+            else:\n+                from twisted.internet import default\n+                default.install()\n+            log_reactor_info()\n+        if self.settings.get(\"TWISTED_REACTOR\"):\n+            verify_installed_reactor(self.settings[\"TWISTED_REACTOR\"])\n+\n         self.extensions = ExtensionManager.from_crawler(self)\n \n         self.settings.freeze()\n@@ -153,7 +167,6 @@ class CrawlerRunner:\n         self._crawlers = set()\n         self._active = set()\n         self.bootstrap_failed = False\n-        self._handle_twisted_reactor()\n \n     @property\n     def spiders(self):\n@@ -247,10 +260,6 @@ class CrawlerRunner:\n         while self._active:\n             yield defer.DeferredList(self._active)\n \n-    def _handle_twisted_reactor(self):\n-        if self.settings.get(\"TWISTED_REACTOR\"):\n-            verify_installed_reactor(self.settings[\"TWISTED_REACTOR\"])\n-\n \n class CrawlerProcess(CrawlerRunner):\n     \"\"\"\n@@ -297,6 +306,11 @@ class CrawlerProcess(CrawlerRunner):\n                     {'signame': signame})\n         reactor.callFromThread(self._stop_reactor)\n \n+    def _create_crawler(self, spidercls):\n+        if isinstance(spidercls, str):\n+            spidercls = self.spider_loader.load(spidercls)\n+        return Crawler(spidercls, self.settings, init_reactor=True)\n+\n     def start(self, stop_after_crawl=True, install_signal_handlers=True):\n         \"\"\"\n         This method starts a :mod:`~twisted.internet.reactor`, adjusts its pool\n@@ -341,8 +355,3 @@ class CrawlerProcess(CrawlerRunner):\n             reactor.stop()\n         except RuntimeError:  # raised if already stopped or in shutdown stage\n             pass\n-\n-    def _handle_twisted_reactor(self):\n-        if self.settings.get(\"TWISTED_REACTOR\"):\n-            install_reactor(self.settings[\"TWISTED_REACTOR\"], self.settings[\"ASYNCIO_EVENT_LOOP\"])\n-        super()._handle_twisted_reactor()\n\n@@ -152,7 +152,6 @@ def log_scrapy_info(settings: Settings) -> None:\n         if name != \"Scrapy\"\n     ]\n     logger.info(\"Versions: %(versions)s\", {'versions': \", \".join(versions)})\n-    log_reactor_info()\n \n \n def log_reactor_info() -> None:\n\n@@ -271,6 +271,7 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n \n         self.assertEqual(runner.bootstrap_failed, True)\n \n+    @defer.inlineCallbacks\n     def test_crawler_runner_asyncio_enabled_true(self):\n         if self.reactor_pytest == 'asyncio':\n             CrawlerRunner(settings={\n@@ -279,9 +280,10 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n         else:\n             msg = r\"The installed reactor \\(.*?\\) does not match the requested one \\(.*?\\)\"\n             with self.assertRaisesRegex(Exception, msg):\n-                CrawlerRunner(settings={\n+                runner = CrawlerRunner(settings={\n                     \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n                 })\n+                yield runner.crawl(NoRequestsSpider)\n \n     @defer.inlineCallbacks\n     # https://twistedmatrix.com/trac/ticket/9766\n@@ -301,6 +303,7 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n                     runner = CrawlerProcess(settings={\n                         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n                     })\n+                    yield runner.crawl(NoRequestsSpider)\n \n     @defer.inlineCallbacks\n     def test_crawler_process_asyncio_enabled_false(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#041699b54cfa6cde9f886a98ff300e3276e2eaad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 27 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 27 | Churn Cumulative: 896 | Contributors (this commit): 15 | Commits (past 90d): 4 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -285,33 +285,6 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n                 })\n                 yield runner.crawl(NoRequestsSpider)\n \n-    @defer.inlineCallbacks\n-    # https://twistedmatrix.com/trac/ticket/9766\n-    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n-            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n-    def test_crawler_process_asyncio_enabled_true(self):\n-        with LogCapture(level=logging.DEBUG) as log:\n-            if self.reactor_pytest == 'asyncio':\n-                runner = CrawlerProcess(settings={\n-                    \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n-                })\n-                yield runner.crawl(NoRequestsSpider)\n-                self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", str(log))\n-            else:\n-                msg = r\"The installed reactor \\(.*?\\) does not match the requested one \\(.*?\\)\"\n-                with self.assertRaisesRegex(Exception, msg):\n-                    runner = CrawlerProcess(settings={\n-                        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n-                    })\n-                    yield runner.crawl(NoRequestsSpider)\n-\n-    @defer.inlineCallbacks\n-    def test_crawler_process_asyncio_enabled_false(self):\n-        runner = CrawlerProcess(settings={\"TWISTED_REACTOR\": None})\n-        with LogCapture(level=logging.DEBUG) as log:\n-            yield runner.crawl(NoRequestsSpider)\n-            self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", str(log))\n-\n \n class ScriptRunnerMixin:\n     def run_script(self, script_name, *script_args):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ebcafdf4a9e0692bf301546b6d60465b3b2c4b06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 81 | Churn Cumulative: 977 | Contributors (this commit): 15 | Commits (past 90d): 8 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,14 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n+\n+class AsyncioReactorSpider(scrapy.Spider):\n+    name = 'asyncio_reactor'\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+    }\n+\n+\n+process = CrawlerProcess()\n+process.crawl(AsyncioReactorSpider)\n+process.start()\n\n@@ -0,0 +1,22 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n+\n+class PollReactorSpider(scrapy.Spider):\n+    name = 'poll_reactor'\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.pollreactor.PollReactor\",\n+    }\n+\n+\n+class AsyncioReactorSpider(scrapy.Spider):\n+    name = 'asyncio_reactor'\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+    }\n+\n+\n+process = CrawlerProcess()\n+process.crawl(PollReactorSpider)\n+process.crawl(AsyncioReactorSpider)\n+process.start()\n\n@@ -0,0 +1,21 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n+\n+class AsyncioReactorSpider1(scrapy.Spider):\n+    name = 'asyncio_reactor1'\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+    }\n+\n+class AsyncioReactorSpider2(scrapy.Spider):\n+    name = 'asyncio_reactor2'\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+    }\n+\n+\n+process = CrawlerProcess()\n+process.crawl(AsyncioReactorSpider1)\n+process.crawl(AsyncioReactorSpider2)\n+process.start()\n\n@@ -361,6 +361,30 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn(\"Spider closed (finished)\", log)\n         self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n+    # https://twistedmatrix.com/trac/ticket/9766\n+    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n+            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n+    def test_reactor_asyncio_custom_settings(self):\n+        log = self.run_script(\"twisted_reactor_custom_settings.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+\n+    # https://twistedmatrix.com/trac/ticket/9766\n+    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n+            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n+    def test_reactor_asyncio_custom_settings_same(self):\n+        log = self.run_script(\"twisted_reactor_custom_settings_same.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+\n+    # https://twistedmatrix.com/trac/ticket/9766\n+    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n+            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n+    def test_reactor_asyncio_custom_settings_conflict(self):\n+        log = self.run_script(\"twisted_reactor_custom_settings_conflict.py\")\n+        self.assertIn(\"Using reactor: twisted.internet.pollreactor.PollReactor\", log)\n+        self.assertIn(\"(twisted.internet.pollreactor.PollReactor) does not match the requested one\", log)\n+\n     @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')\n     @mark.skipif(platform.system() == 'Windows', reason='uvloop does not support Windows')\n     @mark.skipif(twisted_version == Version('twisted', 21, 2, 0), reason='https://twistedmatrix.com/trac/ticket/10106')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
