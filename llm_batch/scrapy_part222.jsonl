{"custom_id": "scrapy#960b1bc8f0335f34db9595532a93d2bef32df9a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 23 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -43,7 +43,7 @@ class HttpProxyMiddleware(object):\n         return creds, proxy_url\n \n     def process_request(self, request, spider):\n-        # ignore if proxy is already seted\n+        # ignore if proxy is already set\n         if 'proxy' in request.meta:\n             return\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#743a0aa422ae2f515cbea69153e60a02c39da98d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 602 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -121,8 +121,8 @@ class Command(ScrapyCommand):\n     def get_callback_from_rules(self, spider, response):\n         if getattr(spider, 'rules', None):\n             for rule in spider.rules:\n-                if rule.link_extractor.matches(response.url) and rule.callback:\n-                    return rule.callback\n+                if rule.link_extractor.matches(response.url):\n+                    return rule.callback or \"parse\"\n         else:\n             logger.error('No CrawlSpider rules found in spider %(spider)r, '\n                          'please specify a callback to use for parsing',\n@@ -166,6 +166,11 @@ class Command(ScrapyCommand):\n             if not cb:\n                 if opts.rules and self.first_response == response:\n                     cb = self.get_callback_from_rules(spider, response)\n+\n+                    if not cb:\n+                        logger.error('Cannot find a rule that matches %(url)r in spider: %(spider)s',\n+                                 {'url': response.url, 'spider': spider.name})\n+                        return\n                 else:\n                     cb = 'parse'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#80260824c65512edc3f60aa7eb8fead9818b1a90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 151 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,9 @@ def _embed_ipython_shell(namespace={}, banner=''):\n     @wraps(_embed_ipython_shell)\n     def wrapper(namespace=namespace, banner=''):\n         config = load_default_config()\n-        shell = InteractiveShellEmbed(\n+        # Always use .instace() to ensure _instance propagation to all parents\n+        # this is needed for <TAB> completion works well for new imports\n+        shell = InteractiveShellEmbed.instance(\n             banner1=banner, user_ns=namespace, config=config)\n         shell()\n     return wrapper\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fbb555929977f91eabfa5dc28f3ae3d68972371a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 8 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 82 | Churn Cumulative: 325 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -257,6 +257,9 @@ class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n         with open(fname, 'w') as f:\n             f.write(\"\"\"\n import scrapy\n+from scrapy.linkextractors import LinkExtractor\n+from scrapy.spiders import CrawlSpider, Rule\n+\n \n class MySpider(scrapy.Spider):\n     name = '{0}'\n@@ -265,6 +268,33 @@ class MySpider(scrapy.Spider):\n         if getattr(self, 'test_arg', None):\n             self.logger.debug('It Works!')\n         return [scrapy.Item(), dict(foo='bar')]\n+\n+\n+class MyGoodCrawlSpider(CrawlSpider):\n+    name = 'goodcrawl{0}'\n+\n+    rules = (\n+        Rule(LinkExtractor(allow=r'/html'), callback='parse_item', follow=True),\n+        Rule(LinkExtractor(allow=r'/text'), follow=True),\n+    )\n+\n+    def parse_item(self, response):\n+        return [scrapy.Item(), dict(foo='bar')]\n+\n+    def parse(self, response):\n+        return [scrapy.Item(), dict(nomatch='default')]\n+\n+\n+class MyBadCrawlSpider(CrawlSpider):\n+    '''Spider which doesn't define a parse_item callback while using it in a rule.'''\n+    name = 'badcrawl{0}'\n+\n+    rules = (\n+        Rule(LinkExtractor(allow=r'/html'), callback='parse_item', follow=True),\n+    )\n+\n+    def parse(self, response):\n+        return [scrapy.Item(), dict(foo='bar')]\n \"\"\".format(self.spider_name))\n \n         fname = abspath(join(self.proj_mod_path, 'pipelines.py'))\n@@ -309,6 +339,58 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n         )\n         self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", to_native_str(out))\n \n+    @defer.inlineCallbacks\n+    def test_parse_items_no_callback_passed(self):\n+        status, out, stderr = yield self.execute(\n+            ['--spider', self.spider_name, self.url('/html')]\n+        )\n+        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", to_native_str(out))\n+\n+    @defer.inlineCallbacks\n+    def test_wrong_callback_passed(self):\n+        status, out, stderr = yield self.execute(\n+            ['--spider', self.spider_name, '-c', 'dummy', self.url('/html')]\n+        )\n+        self.assertRegexpMatches(to_native_str(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n+\n+    @defer.inlineCallbacks\n+    def test_crawlspider_matching_rule_callback_set(self):\n+        \"\"\"If a rule matches the URL, use it's defined callback.\"\"\"\n+        status, out, stderr = yield self.execute(\n+            ['--spider', 'goodcrawl'+self.spider_name, '-r', self.url('/html')]\n+        )\n+        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", to_native_str(out))\n+\n+    @defer.inlineCallbacks\n+    def test_crawlspider_matching_rule_default_callback(self):\n+        \"\"\"If a rule match but it has no callback set, use the 'parse' callback.\"\"\"\n+        status, out, stderr = yield self.execute(\n+            ['--spider', 'goodcrawl'+self.spider_name, '-r', self.url('/text')]\n+        )\n+        self.assertIn(\"\"\"[{}, {'nomatch': 'default'}]\"\"\", to_native_str(out))\n+\n+    @defer.inlineCallbacks\n+    def test_spider_with_no_rules_attribute(self):\n+        \"\"\"Using -r with a spider with no rule should not produce items.\"\"\"\n+        status, out, stderr = yield self.execute(\n+            ['--spider', self.spider_name, '-r', self.url('/html')]\n+        )\n+        self.assertRegexpMatches(to_native_str(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n+\n+    @defer.inlineCallbacks\n+    def test_crawlspider_missing_callback(self):\n+        status, out, stderr = yield self.execute(\n+            ['--spider', 'badcrawl'+self.spider_name, '-r', self.url('/html')]\n+        )\n+        self.assertRegexpMatches(to_native_str(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n+\n+    @defer.inlineCallbacks\n+    def test_crawlspider_no_matching_rule(self):\n+        \"\"\"The requested URL has no matching rule, so no items should be scraped\"\"\"\n+        status, out, stderr = yield self.execute(\n+            ['--spider', 'badcrawl'+self.spider_name, '-r', self.url('/enc-gb18030')]\n+        )\n+        self.assertRegexpMatches(to_native_str(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n \n \n class BenchCommandTest(CommandTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#498a3725d18c5f8f5fd63a69ceffe9de9986aa28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 269 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -145,6 +145,9 @@ class StreamLogger(object):\n         for line in buf.rstrip().splitlines():\n             self.logger.log(self.log_level, line.rstrip())\n \n+    def flush(self):\n+        pass\n+\n \n class LogCounterHandler(logging.Handler):\n     \"\"\"Record log levels count into a crawler stats\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7d882095437a82085ee62cf55b9a2e1003a92145", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 13 | Files Changed: 5 | Hunks: 14 | Methods Changed: 9 | Complexity Δ (Sum/Max): 6/2 | Churn Δ: 67 | Churn Cumulative: 1237 | Contributors (this commit): 9 | Commits (past 90d): 17 | Contributors (cumulative): 28 | DMM Complexity: 0.96\n\nDIFF:\n@@ -233,7 +233,8 @@ class FilesPipeline(MediaPipeline):\n         cls_name = \"FilesPipeline\"\n         self.store = self._get_store(store_uri)\n         resolve = functools.partial(self._key_for_pipe,\n-                                    base_class_name=cls_name)\n+                                    base_class_name=cls_name,\n+                                    settings=settings)\n         self.expires = settings.getint(\n             resolve('FILES_EXPIRES'), self.EXPIRES\n         )\n\n@@ -55,7 +55,8 @@ class ImagesPipeline(FilesPipeline):\n             settings = Settings(settings)\n \n         resolve = functools.partial(self._key_for_pipe,\n-                                    base_class_name=\"ImagesPipeline\")\n+                                    base_class_name=\"ImagesPipeline\",\n+                                    settings=settings)\n         self.expires = settings.getint(\n             resolve(\"IMAGES_EXPIRES\"), self.EXPIRES\n         )\n\n@@ -28,7 +28,8 @@ class MediaPipeline(object):\n         self.download_func = download_func\n \n \n-    def _key_for_pipe(self, key, base_class_name=None):\n+    def _key_for_pipe(self, key, base_class_name=None,\n+                      settings=None):\n         \"\"\"\n         >>> MediaPipeline()._key_for_pipe(\"IMAGES\")\n         'IMAGES'\n@@ -38,9 +39,11 @@ class MediaPipeline(object):\n         'MYPIPE_IMAGES'\n         \"\"\"\n         class_name = self.__class__.__name__\n-        if class_name == base_class_name or not base_class_name:\n+        formatted_key = \"{}_{}\".format(class_name.upper(), key)\n+        if class_name == base_class_name or not base_class_name \\\n+            or (settings and not settings.get(formatted_key)):\n             return key\n-        return \"{}_{}\".format(class_name.upper(), key)\n+        return formatted_key\n \n     @classmethod\n     def from_crawler(cls, crawler):\n\n@@ -255,16 +255,17 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n \n     def test_subclass_attrs_preserved_custom_settings(self):\n         \"\"\"\n-        If file settings are defined but they are not defined for subclass class attributes\n-        should be preserved.\n+        If file settings are defined but they are not defined for subclass\n+        settings should be preserved.\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline()\n         settings = self._generate_fake_settings()\n         pipeline = pipeline_cls.from_settings(Settings(settings))\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             value = getattr(pipeline, pipe_ins_attr)\n+            setting_value = settings.get(settings_attr)\n             self.assertNotEqual(value, self.default_cls_settings[pipe_attr])\n-            self.assertEqual(value, getattr(pipeline, pipe_attr))\n+            self.assertEqual(value, setting_value)\n \n     def test_no_custom_settings_for_subclasses(self):\n         \"\"\"\n@@ -321,6 +322,24 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n         self.assertEqual(pipeline.files_urls_field, \"that\")\n \n \n+    def test_user_defined_subclass_default_key_names(self):\n+        \"\"\"Test situation when user defines subclass of FilesPipeline,\n+        but uses attribute names for default pipeline (without prefixing\n+        them with pipeline class name).\n+        \"\"\"\n+        settings = self._generate_fake_settings()\n+\n+        class UserPipe(FilesPipeline):\n+            pass\n+\n+        pipeline_cls = UserPipe.from_settings(Settings(settings))\n+\n+        for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n+            expected_value = settings.get(settings_attr)\n+            self.assertEqual(getattr(pipeline_cls, pipe_inst_attr),\n+                             expected_value)\n+\n+\n class TestS3FilesStore(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_persist(self):\n\n@@ -309,17 +309,19 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n \n     def test_subclass_attrs_preserved_custom_settings(self):\n         \"\"\"\n-        If image settings are defined but they are not defined for subclass class attributes\n-        should be preserved.\n+        If image settings are defined but they are not defined for subclass default\n+        values taken from settings should be preserved.\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline_subclass()\n         settings = self._generate_fake_settings()\n         pipeline = pipeline_cls.from_settings(Settings(settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n-            # Instance attribute (lowercase) must be equal to class attribute (uppercase).\n+            # Instance attribute (lowercase) must be equal to\n+            # value defined in settings.\n             value = getattr(pipeline, pipe_attr.lower())\n             self.assertNotEqual(value, self.default_pipeline_settings[pipe_attr])\n-            self.assertEqual(value, getattr(pipeline, pipe_attr))\n+            setings_value = settings.get(settings_attr)\n+            self.assertEqual(value, setings_value)\n \n     def test_no_custom_settings_for_subclasses(self):\n         \"\"\"\n@@ -370,11 +372,26 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n         class UserDefinedImagePipeline(ImagesPipeline):\n             DEFAULT_IMAGES_URLS_FIELD = \"something\"\n             DEFAULT_IMAGES_RESULT_FIELD = \"something_else\"\n-\n         pipeline = UserDefinedImagePipeline.from_settings(Settings({\"IMAGES_STORE\": self.tempdir}))\n         self.assertEqual(pipeline.images_result_field, \"something_else\")\n         self.assertEqual(pipeline.images_urls_field, \"something\")\n \n+    def test_user_defined_subclass_default_key_names(self):\n+        \"\"\"Test situation when user defines subclass of ImagePipeline,\n+        but uses attribute names for default pipeline (without prefixing\n+        them with pipeline class name).\n+        \"\"\"\n+        settings = self._generate_fake_settings()\n+\n+        class UserPipe(ImagesPipeline):\n+            pass\n+\n+        pipeline_cls = UserPipe.from_settings(Settings(settings))\n+\n+        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n+            expected_value = settings.get(settings_attr)\n+            self.assertEqual(getattr(pipeline_cls, pipe_attr.lower()),\n+                             expected_value)\n \n def _create_image(format, *a, **kw):\n     buf = TemporaryFile()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b828facff499e97f8d2dfec0c2fd5a76a5152237", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 517 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -56,6 +56,13 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n         errcode, out, _ = yield self.execute(['-c', code.format(url)])\n         self.assertEqual(errcode, 0, out)\n \n+    @defer.inlineCallbacks\n+    def test_scrapy_import(self):\n+        url = self.url('/text')\n+        code = \"fetch(scrapy.Request('{0}'))\"\n+        errcode, out, _ = yield self.execute(['-c', code.format(url)])\n+        self.assertEqual(errcode, 0, out)\n+\n     @defer.inlineCallbacks\n     def test_local_file(self):\n         filepath = join(tests_datadir, 'test_site/index.html')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#105163fece502c7afd93d253398cfa6061ef9609", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 764 | Contributors (this commit): 13 | Commits (past 90d): 1 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -115,6 +115,9 @@ class Shell(object):\n         self.populate_vars(response, request, spider)\n \n     def populate_vars(self, response=None, request=None, spider=None):\n+        import scrapy\n+\n+        self.vars['scrapy'] = scrapy\n         self.vars['crawler'] = self.crawler\n         self.vars['item'] = self.item_class()\n         self.vars['settings'] = self.crawler.settings\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#effaab867e45d441b5ca99ea6b24fd423e256a33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 765 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -139,6 +139,7 @@ class Shell(object):\n     def get_help(self):\n         b = []\n         b.append(\"Available Scrapy objects:\")\n+        b.append(\"  scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\")\n         for k, v in sorted(self.vars.items()):\n             if self._is_relevant(v):\n                 b.append(\"  %-10s %s\" % (k, v))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#03ab0772491c61cb7f2198b2c019c7eb7672a331", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 8 | Methods Changed: 8 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 49 | Churn Cumulative: 368 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: 0.972972972972973\n\nDIFF:\n@@ -170,6 +170,7 @@ class FeedExporter(object):\n         if not self._exporter_supported(self.format):\n             raise NotConfigured\n         self.store_empty = settings.getbool('FEED_STORE_EMPTY')\n+        self._exporting = False\n         self.export_fields = settings.getlist('FEED_EXPORT_FIELDS') or None\n         uripar = settings['FEED_URI_PARAMS']\n         self._uripar = load_object(uripar) if uripar else lambda x, y: None\n@@ -188,14 +189,18 @@ class FeedExporter(object):\n         file = storage.open(spider)\n         exporter = self._get_exporter(file, fields_to_export=self.export_fields,\n             encoding=self.export_encoding)\n+        if self.store_empty:\n             exporter.start_exporting()\n+            self._exporting = True\n         self.slot = SpiderSlot(file, exporter, storage, uri)\n \n     def close_spider(self, spider):\n         slot = self.slot\n         if not slot.itemcount and not self.store_empty:\n             return\n+        if self._exporting:\n             slot.exporter.finish_exporting()\n+            self._exporting = False\n         logfmt = \"%s %%(format)s feed (%%(itemcount)d items) in: %%(uri)s\"\n         log_args = {'format': self.format,\n                     'itemcount': slot.itemcount,\n@@ -210,6 +215,9 @@ class FeedExporter(object):\n \n     def item_scraped(self, item, spider):\n         slot = self.slot\n+        if not self._exporting:\n+            slot.exporter.start_exporting()\n+            self._exporting = True\n         slot.exporter.export_item(item)\n         slot.itemcount += 1\n         return item\n\n@@ -197,6 +197,21 @@ class FeedExportTest(unittest.TestCase):\n         data = yield self.run_and_export(TestSpider, settings)\n         defer.returnValue(data)\n \n+    @defer.inlineCallbacks\n+    def exported_no_data(self, settings):\n+        \"\"\"\n+        Return exported data which a spider yielding no ``items`` would return.\n+        \"\"\"\n+        class TestSpider(scrapy.Spider):\n+            name = 'testspider'\n+            start_urls = ['http://localhost:8998/']\n+\n+            def parse(self, response):\n+                pass\n+\n+        data = yield self.run_and_export(TestSpider, settings)\n+        defer.returnValue(data)\n+\n     @defer.inlineCallbacks\n     def assertExportedCsv(self, items, header, rows, settings=None, ordered=True):\n         settings = settings or {}\n@@ -283,6 +298,32 @@ class FeedExportTest(unittest.TestCase):\n         header = self.MyItem.fields.keys()\n         yield self.assertExported(items, header, rows, ordered=False)\n \n+    @defer.inlineCallbacks\n+    def test_export_no_items_not_store_empty(self):\n+        formats = ('json',\n+                   'jsonlines',\n+                   'xml',\n+                   'csv',)\n+\n+        for fmt in formats:\n+            settings = {'FEED_FORMAT': fmt}\n+            data = yield self.exported_no_data(settings)\n+            self.assertEqual(data, b'')\n+\n+    @defer.inlineCallbacks\n+    def test_export_no_items_store_empty(self):\n+        formats = (\n+            ('json', b'[\\n\\n]'),\n+            ('jsonlines', b''),\n+            ('xml', b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items></items>'),\n+            ('csv', b''),\n+        )\n+\n+        for fmt, expctd in formats:\n+            settings = {'FEED_FORMAT': fmt, 'FEED_STORE_EMPTY': True}\n+            data = yield self.exported_no_data(settings)\n+            self.assertEqual(data, expctd)\n+\n     @defer.inlineCallbacks\n     def test_export_multiple_item_classes(self):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#48f6a065b8d1ed1f9e45a78643b52874f09e2c30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 272 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -146,7 +146,8 @@ class StreamLogger(object):\n             self.logger.log(self.log_level, line.rstrip())\n \n     def flush(self):\n-        pass\n+        for h in self.logger.handlers:\n+            h.flush()\n \n \n class LogCounterHandler(logging.Handler):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8c38dde4e8e60371a2599523aa793a4786dc904e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 156 | Lines Deleted: 147 | Files Changed: 2 | Hunks: 2 | Methods Changed: 22 | Complexity Δ (Sum/Max): 0/11 | Churn Δ: 303 | Churn Cumulative: 628 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,156 @@\n+from os.path import join, abspath\n+from twisted.trial import unittest\n+from twisted.internet import defer\n+from scrapy.utils.testsite import SiteTest\n+from scrapy.utils.testproc import ProcessTest\n+from scrapy.utils.python import to_native_str\n+from tests.test_commands import CommandTest\n+\n+\n+class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n+    command = 'parse'\n+\n+    def setUp(self):\n+        super(ParseCommandTest, self).setUp()\n+        self.spider_name = 'parse_spider'\n+        fname = abspath(join(self.proj_mod_path, 'spiders', 'myspider.py'))\n+        with open(fname, 'w') as f:\n+            f.write(\"\"\"\n+import scrapy\n+from scrapy.linkextractors import LinkExtractor\n+from scrapy.spiders import CrawlSpider, Rule\n+\n+\n+class MySpider(scrapy.Spider):\n+    name = '{0}'\n+\n+    def parse(self, response):\n+        if getattr(self, 'test_arg', None):\n+            self.logger.debug('It Works!')\n+        return [scrapy.Item(), dict(foo='bar')]\n+\n+\n+class MyGoodCrawlSpider(CrawlSpider):\n+    name = 'goodcrawl{0}'\n+\n+    rules = (\n+        Rule(LinkExtractor(allow=r'/html'), callback='parse_item', follow=True),\n+        Rule(LinkExtractor(allow=r'/text'), follow=True),\n+    )\n+\n+    def parse_item(self, response):\n+        return [scrapy.Item(), dict(foo='bar')]\n+\n+    def parse(self, response):\n+        return [scrapy.Item(), dict(nomatch='default')]\n+\n+\n+class MyBadCrawlSpider(CrawlSpider):\n+    '''Spider which doesn't define a parse_item callback while using it in a rule.'''\n+    name = 'badcrawl{0}'\n+\n+    rules = (\n+        Rule(LinkExtractor(allow=r'/html'), callback='parse_item', follow=True),\n+    )\n+\n+    def parse(self, response):\n+        return [scrapy.Item(), dict(foo='bar')]\n+\"\"\".format(self.spider_name))\n+\n+        fname = abspath(join(self.proj_mod_path, 'pipelines.py'))\n+        with open(fname, 'w') as f:\n+            f.write(\"\"\"\n+import logging\n+\n+class MyPipeline(object):\n+    component_name = 'my_pipeline'\n+\n+    def process_item(self, item, spider):\n+        logging.info('It Works!')\n+        return item\n+\"\"\")\n+\n+        fname = abspath(join(self.proj_mod_path, 'settings.py'))\n+        with open(fname, 'a') as f:\n+            f.write(\"\"\"\n+ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n+\"\"\" % self.project_name)\n+\n+    @defer.inlineCallbacks\n+    def test_spider_arguments(self):\n+        _, _, stderr = yield self.execute(['--spider', self.spider_name,\n+                                           '-a', 'test_arg=1',\n+                                           '-c', 'parse',\n+                                           self.url('/html')])\n+        self.assertIn(\"DEBUG: It Works!\", to_native_str(stderr))\n+\n+    @defer.inlineCallbacks\n+    def test_pipelines(self):\n+        _, _, stderr = yield self.execute(['--spider', self.spider_name,\n+                                           '--pipelines',\n+                                           '-c', 'parse',\n+                                           self.url('/html')])\n+        self.assertIn(\"INFO: It Works!\", to_native_str(stderr))\n+\n+    @defer.inlineCallbacks\n+    def test_parse_items(self):\n+        status, out, stderr = yield self.execute(\n+            ['--spider', self.spider_name, '-c', 'parse', self.url('/html')]\n+        )\n+        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", to_native_str(out))\n+\n+    @defer.inlineCallbacks\n+    def test_parse_items_no_callback_passed(self):\n+        status, out, stderr = yield self.execute(\n+            ['--spider', self.spider_name, self.url('/html')]\n+        )\n+        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", to_native_str(out))\n+\n+    @defer.inlineCallbacks\n+    def test_wrong_callback_passed(self):\n+        status, out, stderr = yield self.execute(\n+            ['--spider', self.spider_name, '-c', 'dummy', self.url('/html')]\n+        )\n+        self.assertRegexpMatches(to_native_str(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n+        self.assertIn(\"\"\"Cannot find callback\"\"\", to_native_str(stderr))\n+\n+    @defer.inlineCallbacks\n+    def test_crawlspider_matching_rule_callback_set(self):\n+        \"\"\"If a rule matches the URL, use it's defined callback.\"\"\"\n+        status, out, stderr = yield self.execute(\n+            ['--spider', 'goodcrawl'+self.spider_name, '-r', self.url('/html')]\n+        )\n+        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", to_native_str(out))\n+\n+    @defer.inlineCallbacks\n+    def test_crawlspider_matching_rule_default_callback(self):\n+        \"\"\"If a rule match but it has no callback set, use the 'parse' callback.\"\"\"\n+        status, out, stderr = yield self.execute(\n+            ['--spider', 'goodcrawl'+self.spider_name, '-r', self.url('/text')]\n+        )\n+        self.assertIn(\"\"\"[{}, {'nomatch': 'default'}]\"\"\", to_native_str(out))\n+\n+    @defer.inlineCallbacks\n+    def test_spider_with_no_rules_attribute(self):\n+        \"\"\"Using -r with a spider with no rule should not produce items.\"\"\"\n+        status, out, stderr = yield self.execute(\n+            ['--spider', self.spider_name, '-r', self.url('/html')]\n+        )\n+        self.assertRegexpMatches(to_native_str(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n+        self.assertIn(\"\"\"No CrawlSpider rules found\"\"\", to_native_str(stderr))\n+\n+    @defer.inlineCallbacks\n+    def test_crawlspider_missing_callback(self):\n+        status, out, stderr = yield self.execute(\n+            ['--spider', 'badcrawl'+self.spider_name, '-r', self.url('/html')]\n+        )\n+        self.assertRegexpMatches(to_native_str(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n+\n+    @defer.inlineCallbacks\n+    def test_crawlspider_no_matching_rule(self):\n+        \"\"\"The requested URL has no matching rule, so no items should be scraped\"\"\"\n+        status, out, stderr = yield self.execute(\n+            ['--spider', 'badcrawl'+self.spider_name, '-r', self.url('/enc-gb18030')]\n+        )\n+        self.assertRegexpMatches(to_native_str(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n+        self.assertIn(\"\"\"Cannot find a rule that matches\"\"\", to_native_str(stderr))\n\n@@ -246,153 +246,6 @@ class BadSpider(scrapy.Spider):\n         self.assertIn(\"start_requests\", log)\n         self.assertIn(\"badspider.py\", log)\n \n-\n-class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n-    command = 'parse'\n-\n-    def setUp(self):\n-        super(ParseCommandTest, self).setUp()\n-        self.spider_name = 'parse_spider'\n-        fname = abspath(join(self.proj_mod_path, 'spiders', 'myspider.py'))\n-        with open(fname, 'w') as f:\n-            f.write(\"\"\"\n-import scrapy\n-from scrapy.linkextractors import LinkExtractor\n-from scrapy.spiders import CrawlSpider, Rule\n-\n-\n-class MySpider(scrapy.Spider):\n-    name = '{0}'\n-\n-    def parse(self, response):\n-        if getattr(self, 'test_arg', None):\n-            self.logger.debug('It Works!')\n-        return [scrapy.Item(), dict(foo='bar')]\n-\n-\n-class MyGoodCrawlSpider(CrawlSpider):\n-    name = 'goodcrawl{0}'\n-\n-    rules = (\n-        Rule(LinkExtractor(allow=r'/html'), callback='parse_item', follow=True),\n-        Rule(LinkExtractor(allow=r'/text'), follow=True),\n-    )\n-\n-    def parse_item(self, response):\n-        return [scrapy.Item(), dict(foo='bar')]\n-\n-    def parse(self, response):\n-        return [scrapy.Item(), dict(nomatch='default')]\n-\n-\n-class MyBadCrawlSpider(CrawlSpider):\n-    '''Spider which doesn't define a parse_item callback while using it in a rule.'''\n-    name = 'badcrawl{0}'\n-\n-    rules = (\n-        Rule(LinkExtractor(allow=r'/html'), callback='parse_item', follow=True),\n-    )\n-\n-    def parse(self, response):\n-        return [scrapy.Item(), dict(foo='bar')]\n-\"\"\".format(self.spider_name))\n-\n-        fname = abspath(join(self.proj_mod_path, 'pipelines.py'))\n-        with open(fname, 'w') as f:\n-            f.write(\"\"\"\n-import logging\n-\n-class MyPipeline(object):\n-    component_name = 'my_pipeline'\n-\n-    def process_item(self, item, spider):\n-        logging.info('It Works!')\n-        return item\n-\"\"\")\n-\n-        fname = abspath(join(self.proj_mod_path, 'settings.py'))\n-        with open(fname, 'a') as f:\n-            f.write(\"\"\"\n-ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n-\"\"\" % self.project_name)\n-\n-    @defer.inlineCallbacks\n-    def test_spider_arguments(self):\n-        _, _, stderr = yield self.execute(['--spider', self.spider_name,\n-                                           '-a', 'test_arg=1',\n-                                           '-c', 'parse',\n-                                           self.url('/html')])\n-        self.assertIn(\"DEBUG: It Works!\", to_native_str(stderr))\n-\n-    @defer.inlineCallbacks\n-    def test_pipelines(self):\n-        _, _, stderr = yield self.execute(['--spider', self.spider_name,\n-                                           '--pipelines',\n-                                           '-c', 'parse',\n-                                           self.url('/html')])\n-        self.assertIn(\"INFO: It Works!\", to_native_str(stderr))\n-\n-    @defer.inlineCallbacks\n-    def test_parse_items(self):\n-        status, out, stderr = yield self.execute(\n-            ['--spider', self.spider_name, '-c', 'parse', self.url('/html')]\n-        )\n-        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", to_native_str(out))\n-\n-    @defer.inlineCallbacks\n-    def test_parse_items_no_callback_passed(self):\n-        status, out, stderr = yield self.execute(\n-            ['--spider', self.spider_name, self.url('/html')]\n-        )\n-        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", to_native_str(out))\n-\n-    @defer.inlineCallbacks\n-    def test_wrong_callback_passed(self):\n-        status, out, stderr = yield self.execute(\n-            ['--spider', self.spider_name, '-c', 'dummy', self.url('/html')]\n-        )\n-        self.assertRegexpMatches(to_native_str(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n-\n-    @defer.inlineCallbacks\n-    def test_crawlspider_matching_rule_callback_set(self):\n-        \"\"\"If a rule matches the URL, use it's defined callback.\"\"\"\n-        status, out, stderr = yield self.execute(\n-            ['--spider', 'goodcrawl'+self.spider_name, '-r', self.url('/html')]\n-        )\n-        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", to_native_str(out))\n-\n-    @defer.inlineCallbacks\n-    def test_crawlspider_matching_rule_default_callback(self):\n-        \"\"\"If a rule match but it has no callback set, use the 'parse' callback.\"\"\"\n-        status, out, stderr = yield self.execute(\n-            ['--spider', 'goodcrawl'+self.spider_name, '-r', self.url('/text')]\n-        )\n-        self.assertIn(\"\"\"[{}, {'nomatch': 'default'}]\"\"\", to_native_str(out))\n-\n-    @defer.inlineCallbacks\n-    def test_spider_with_no_rules_attribute(self):\n-        \"\"\"Using -r with a spider with no rule should not produce items.\"\"\"\n-        status, out, stderr = yield self.execute(\n-            ['--spider', self.spider_name, '-r', self.url('/html')]\n-        )\n-        self.assertRegexpMatches(to_native_str(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n-\n-    @defer.inlineCallbacks\n-    def test_crawlspider_missing_callback(self):\n-        status, out, stderr = yield self.execute(\n-            ['--spider', 'badcrawl'+self.spider_name, '-r', self.url('/html')]\n-        )\n-        self.assertRegexpMatches(to_native_str(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n-\n-    @defer.inlineCallbacks\n-    def test_crawlspider_no_matching_rule(self):\n-        \"\"\"The requested URL has no matching rule, so no items should be scraped\"\"\"\n-        status, out, stderr = yield self.execute(\n-            ['--spider', 'badcrawl'+self.spider_name, '-r', self.url('/enc-gb18030')]\n-        )\n-        self.assertRegexpMatches(to_native_str(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n-\n-\n class BenchCommandTest(CommandTest):\n \n     def test_run(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a135dbaf19f45b9cf59b4e89954f93c74bdf7771", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 399 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -89,7 +89,7 @@ class Scheduler(object):\n                 msg = (\"Unable to serialize request: %(request)s - reason:\"\n                        \" %(reason)s - no more unserializable requests will be\"\n                        \" logged (stats being collected)\")\n-                logger.error(msg, {'request': request, 'reason': e},\n+                logger.warning(msg, {'request': request, 'reason': e},\n                                exc_info=True, extra={'spider': self.spider})\n                 self.logunser = False\n             self.stats.inc_value('scheduler/unserializable',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#81a0e3cd93954ecd0a728ef5fcf5dc92a281fdd5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 12/11 | Churn Δ: 29 | Churn Cumulative: 71 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -46,7 +46,7 @@ class HttpErrorMiddleware(object):\n \n     def process_spider_exception(self, response, exception, spider):\n         if isinstance(exception, HttpError):\n-            logger.debug(\n+            logger.info(\n                 \"Ignoring response %(response)r: HTTP status code is not handled or not allowed\",\n                 {'response': response}, extra={'spider': spider},\n             )\n\n@@ -1,3 +1,4 @@\n+import logging\n from unittest import TestCase\n \n from testfixtures import LogCapture\n@@ -185,3 +186,29 @@ class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n         self.assertIn('Ignoring response <500', str(log))\n         self.assertNotIn('Ignoring response <200', str(log))\n         self.assertNotIn('Ignoring response <402', str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_logging_level(self):\n+        # HttpError logs ignored responses with level INFO\n+        crawler = get_crawler(_HttpErrorSpider)\n+        with LogCapture(level=logging.INFO) as log:\n+            yield crawler.crawl()\n+        self.assertEqual(crawler.spider.parsed, {'200'})\n+        self.assertEqual(crawler.spider.failed, {'404', '402', '500'})\n+\n+        self.assertIn('Ignoring response <402', str(log))\n+        self.assertIn('Ignoring response <404', str(log))\n+        self.assertIn('Ignoring response <500', str(log))\n+        self.assertNotIn('Ignoring response <200', str(log))\n+\n+        # with level WARNING, we shouldn't capture anything from HttpError\n+        crawler = get_crawler(_HttpErrorSpider)\n+        with LogCapture(level=logging.WARNING) as log:\n+            yield crawler.crawl()\n+        self.assertEqual(crawler.spider.parsed, {'200'})\n+        self.assertEqual(crawler.spider.failed, {'404', '402', '500'})\n+\n+        self.assertNotIn('Ignoring response <402', str(log))\n+        self.assertNotIn('Ignoring response <404', str(log))\n+        self.assertNotIn('Ignoring response <500', str(log))\n+        self.assertNotIn('Ignoring response <200', str(log))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c9690c76cf95df53c1eb2d3ad0360209eb2a50a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 34 | Churn Cumulative: 432 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,6 +9,8 @@ from six.moves.urllib.parse import urljoin\n from scrapy.http.headers import Headers\n from scrapy.utils.trackref import object_ref\n from scrapy.http.common import obsolete_setter\n+from scrapy.exceptions import NotSupported\n+\n \n class Response(object_ref):\n \n@@ -80,3 +82,22 @@ class Response(object_ref):\n         \"\"\"Join this Response's url with a possible relative url to form an\n         absolute interpretation of the latter.\"\"\"\n         return urljoin(self.url, url)\n+\n+    @property\n+    def text(self):\n+        \"\"\"For subclasses of TextResponse, this will return the body\n+        as text (unicode object in Python 2 and str in Python 3)\n+        \"\"\"\n+        raise AttributeError(\"Response content isn't text\")\n+\n+    def css(self, *a, **kw):\n+        \"\"\"Shortcut method implemented only by responses whose content\n+        is text (subclasses of TextResponse).\n+        \"\"\"\n+        raise NotSupported(\"Response content isn't text\")\n+\n+    def xpath(self, *a, **kw):\n+        \"\"\"Shortcut method implemented only by responses whose content\n+        is text (subclasses of TextResponse).\n+        \"\"\"\n+        raise NotSupported(\"Response content isn't text\")\n\n@@ -7,6 +7,7 @@ from scrapy.http import (Request, Response, TextResponse, HtmlResponse,\n                          XmlResponse, Headers)\n from scrapy.selector import Selector\n from scrapy.utils.python import to_native_str\n+from scrapy.exceptions import NotSupported\n \n \n class BaseResponseTest(unittest.TestCase):\n@@ -127,6 +128,18 @@ class BaseResponseTest(unittest.TestCase):\n         absolute = 'http://www.example.com/test'\n         self.assertEqual(joined, absolute)\n \n+    def test_shortcut_attributes(self):\n+        r = self.response_class(\"http://example.com\", body=b'hello')\n+        if self.response_class == Response:\n+            msg = \"Response content isn't text\"\n+            self.assertRaisesRegexp(AttributeError, msg, getattr, r, 'text')\n+            self.assertRaisesRegexp(NotSupported, msg, r.css, 'body')\n+            self.assertRaisesRegexp(NotSupported, msg, r.xpath, '//body')\n+        else:\n+            r.text\n+            r.css('body')\n+            r.xpath('//body')\n+\n \n class TextResponseTest(BaseResponseTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8e4947ef0dd4bad368784f0d56e6b0f675ea2a18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 221 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -40,10 +40,10 @@ def project_data_dir(project='default'):\n     return d\n \n def data_path(path, createdir=False):\n-    \"\"\"If path is relative, return the given path inside the project data dir,\n-    otherwise return the path unmodified\n+    \"\"\"If inside the project and path is relative, return the given path\n+    as relative the project data dir, otherwise return it unmodified\n     \"\"\"\n-    if not isabs(path):\n+    if inside_project() and not isabs(path):\n         path = join(project_data_dir(), path)\n     if createdir and not exists(path):\n         os.makedirs(path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#25bd3b3fea444f9f20b089436f94c4c91d37c05e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 45 | Churn Cumulative: 320 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,6 +12,7 @@ from scrapy.exceptions import NotConfigured\n ENVVAR = 'SCRAPY_SETTINGS_MODULE'\n DATADIR_CFG_SECTION = 'datadir'\n \n+\n def inside_project():\n     scrapy_module = os.environ.get('SCRAPY_SETTINGS_MODULE')\n     if scrapy_module is not None:\n@@ -23,6 +24,7 @@ def inside_project():\n             return True\n     return bool(closest_scrapy_cfg())\n \n+\n def project_data_dir(project='default'):\n     \"\"\"Return the current project data dir, creating it if it doesn't exist\"\"\"\n     if not inside_project():\n@@ -39,16 +41,21 @@ def project_data_dir(project='default'):\n         os.makedirs(d)\n     return d\n \n+\n def data_path(path, createdir=False):\n     \"\"\"If inside the project and path is relative, return the given path\n-    as relative the project data dir, otherwise return it unmodified\n+    as relative to the project data dir, otherwise return it unmodified\n     \"\"\"\n-    if inside_project() and not isabs(path):\n+    if not isabs(path):\n+        if inside_project():\n             path = join(project_data_dir(), path)\n+        else:\n+            path = join('.scrapy', path)\n     if createdir and not exists(path):\n         os.makedirs(path)\n     return path\n \n+\n def get_project_settings():\n     if ENVVAR not in os.environ:\n         project = os.environ.get('SCRAPY_PROJECT', 'default')\n\n@@ -0,0 +1,34 @@\n+import unittest\n+import os\n+import tempfile\n+import shutil\n+import contextlib\n+from scrapy.utils.project import data_path\n+\n+\n+@contextlib.contextmanager\n+def inside_a_project():\n+    prev_dir = os.getcwd()\n+    project_dir = tempfile.mkdtemp()\n+\n+    try:\n+        os.chdir(project_dir)\n+        with open('scrapy.cfg', 'w') as f:\n+            # create an empty scrapy.cfg\n+            f.close()\n+\n+        yield project_dir\n+    finally:\n+        os.chdir(prev_dir)\n+        shutil.rmtree(project_dir)\n+\n+\n+class ProjectUtilsTest(unittest.TestCase):\n+    def test_data_path_outside_project(self):\n+        self.assertEquals('.scrapy/somepath', data_path('somepath'))\n+\n+    def test_data_path_inside_project(self):\n+        with inside_a_project() as proj_path:\n+            expected = os.path.join(proj_path, '.scrapy', 'somepath')\n+            self.assertEquals(expected, data_path('somepath'))\n+            self.assertEquals('/absolute/path', data_path('/absolute/path'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c90d9515a50ede29415b8b5d6ba11229f333b49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 237 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -43,8 +43,9 @@ def project_data_dir(project='default'):\n \n \n def data_path(path, createdir=False):\n-    \"\"\"If inside the project and path is relative, return the given path\n-    as relative to the project data dir, otherwise return it unmodified\n+    \"\"\"\n+    Return the given path joined with the .scrapy data directory.\n+    If given an absolute path, return it unmodified.\n     \"\"\"\n     if not isabs(path):\n         if inside_project():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
