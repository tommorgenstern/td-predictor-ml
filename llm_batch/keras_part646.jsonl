{"custom_id": "keras#02076132b8f84c1be7897de5104339aee66276dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 287 | Files Changed: 5 | Hunks: 21 | Methods Changed: 23 | Complexity Δ (Sum/Max): -24/0 | Churn Δ: 322 | Churn Cumulative: 4402 | Contributors (this commit): 10 | Commits (past 90d): 112 | Contributors (cumulative): 22 | DMM Complexity: 0.02564102564102564\n\nDIFF:\n@@ -29,11 +29,6 @@ history = model.fit(\n model.evaluate(x, y, verbose=0)\n model.predict(x, verbose=0)\n \n-# Test on batch functions\n-model.train_on_batch(x, y)\n-model.test_on_batch(x, y)\n-model.predict_on_batch(x)\n-\n # Test functional model.\n inputs = keras_core.Input(shape=(32, 32, 3))\n outputs = layers.Conv2D(filters=10, kernel_size=3)(inputs)\n\n@@ -18,16 +18,9 @@ class TorchTrainer(base_trainer.Trainer):\n         self.test_function = None\n         self.predict_function = None\n \n-    def make_train_function(self, force=False):\n-        if self.train_function is not None and not force:\n-            return self.train_function\n-\n-        def one_step_on_data(data):\n-            \"\"\"Runs a single training step on a batch of data.\"\"\"\n+    def train_step(self, data):\n         data = data[0]\n-            x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(\n-                data\n-            )\n+        x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n \n         # Compute prediction error\n         if self._call_has_training_arg():\n@@ -35,8 +28,8 @@ class TorchTrainer(base_trainer.Trainer):\n         else:\n             y_pred = self(x)\n \n-            # Call torch.nn.Module.zero_grad() to clear the leftover gradients\n-            # for the weights from the previous train step.\n+        # Call torch.nn.Module.zero_grad() to clear the leftover gradients for\n+        # the weights from the previous train step.\n         self.zero_grad()\n \n         loss = self.compute_loss(\n@@ -49,8 +42,8 @@ class TorchTrainer(base_trainer.Trainer):\n             # Backpropagation\n             trainable_weights = [v for v in self.trainable_weights]\n \n-                # Call torch.Tensor.backward() on the loss to compute gradients\n-                # for the weights.\n+            # Call torch.Tensor.backward() on the loss to compute gradients for\n+            # the weights.\n             loss.backward()\n \n             gradients = [v.value.grad for v in trainable_weights]\n@@ -63,55 +56,7 @@ class TorchTrainer(base_trainer.Trainer):\n         else:\n             warnings.warn(\"The model does not have any trainable weights.\")\n \n-            return self.compute_metrics(\n-                x, y, y_pred, sample_weight=sample_weight\n-            )\n-\n-        self.train_function = one_step_on_data\n-\n-    def make_test_function(self, force=False):\n-        if self.test_function is not None and not force:\n-            return self.test_function\n-\n-        def one_step_on_data(data):\n-            \"\"\"Runs a single test step on a batch of data.\"\"\"\n-            with torch.no_grad():\n-                data = data[0]\n-                (\n-                    x,\n-                    y,\n-                    sample_weight,\n-                ) = data_adapter_utils.unpack_x_y_sample_weight(data)\n-                if self._call_has_training_arg():\n-                    y_pred = self(x, training=False)\n-                else:\n-                    y_pred = self(x)\n-                loss = self.compute_loss(\n-                    x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n-                )\n-                self._loss_tracker.update_state(loss)\n-                return self.compute_metrics(\n-                    x, y, y_pred, sample_weight=sample_weight\n-                )\n-\n-        self.test_function = one_step_on_data\n-\n-    def make_predict_function(self, force=False):\n-        if self.predict_function is not None and not force:\n-            return self.predict_function\n-\n-        def one_step_on_data(data):\n-            \"\"\"Runs a predict test step on a batch of data.\"\"\"\n-            with torch.no_grad():\n-                data = data[0]\n-                x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n-                if self._call_has_training_arg():\n-                    y_pred = self(x, training=False)\n-                else:\n-                    y_pred = self(x)\n-                return y_pred\n-\n-        self.predict_function = one_step_on_data\n+        return self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n \n     def fit(\n         self,\n@@ -182,7 +127,6 @@ class TorchTrainer(base_trainer.Trainer):\n             )\n \n         self.stop_training = False\n-        self.make_train_function()\n         callbacks.on_train_begin()\n \n         for epoch in range(initial_epoch, epochs):\n@@ -197,7 +141,7 @@ class TorchTrainer(base_trainer.Trainer):\n                 # Callbacks\n                 callbacks.on_train_batch_begin(step)\n \n-                logs = self.train_function(data)\n+                logs = self.train_step(data)\n \n                 # Callbacks\n                 callbacks.on_train_batch_end(step, self._pythonify_logs(logs))\n@@ -253,6 +197,19 @@ class TorchTrainer(base_trainer.Trainer):\n         callbacks.on_train_end(logs=training_logs)\n         return self.history\n \n+    def test_step(self, data):\n+        data = data[0]\n+        x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n+        if self._call_has_training_arg():\n+            y_pred = self(x, training=False)\n+        else:\n+            y_pred = self(x)\n+        loss = self.compute_loss(\n+            x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n+        )\n+        self._loss_tracker.update_state(loss)\n+        return self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n+\n     def evaluate(\n         self,\n         x=None,\n@@ -299,13 +256,13 @@ class TorchTrainer(base_trainer.Trainer):\n         # Switch the torch Module back to testing mode.\n         self.eval()\n \n-        self.make_test_function()\n         callbacks.on_test_begin()\n         logs = None\n         self.reset_metrics()\n         for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n             callbacks.on_test_batch_begin(step)\n-            logs = self.test_function(data)\n+            with torch.no_grad():\n+                logs = self.test_step(data)\n             callbacks.on_test_batch_end(step, self._pythonify_logs(logs))\n         logs = self.get_metrics_result()\n         callbacks.on_test_end(logs)\n@@ -314,6 +271,15 @@ class TorchTrainer(base_trainer.Trainer):\n             return logs\n         return self._flatten_metrics_in_order(logs)\n \n+    def predict_step(self, data):\n+        data = data[0]\n+        x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n+        if self._call_has_training_arg():\n+            y_pred = self(x, training=False)\n+        else:\n+            y_pred = self(x)\n+        return y_pred\n+\n     def predict(\n         self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n     ):\n@@ -356,126 +322,15 @@ class TorchTrainer(base_trainer.Trainer):\n         # Switch the torch Module back to testing mode.\n         self.eval()\n \n-        self.make_predict_function()\n         callbacks.on_predict_begin()\n         outputs = None\n         for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n             callbacks.on_predict_batch_begin(step)\n-            batch_outputs = self.predict_function(data)\n+            with torch.no_grad():\n+                batch_outputs = self.predict_step(data)\n             outputs = append_to_outputs(batch_outputs, outputs)\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n         callbacks.on_predict_end()\n         return tf.__internal__.nest.map_structure_up_to(\n             batch_outputs, np.concatenate, outputs\n         )\n-\n-    def train_on_batch(\n-        self,\n-        x,\n-        y=None,\n-        sample_weight=None,\n-        class_weight=None,\n-        return_dict=False,\n-    ):\n-        \"\"\"Runs a single gradient update on a single batch of data.\n-\n-        Args:\n-            x: Input data. Must be array-like.\n-            y: Target data. Must be array-like.\n-            sample_weight: Optional array of the same length as x, containing\n-                weights to apply to the model's loss for each sample.\n-                In the case of temporal data, you can pass a 2D array\n-                with shape `(samples, sequence_length)`, to apply a different\n-                weight to every timestep of every sample.\n-            class_weight: Optional dictionary mapping class indices (integers)\n-                to a weight (float) to apply to the model's loss for the samples\n-                from this class during training. This can be useful to tell the\n-                model to \"pay more attention\" to samples from an\n-                under-represented class. When `class_weight` is specified\n-                and targets have a rank of 2 or greater, either `y` must\n-                be one-hot encoded, or an explicit final dimension of 1\n-                must be included for sparse class labels.\n-            return_dict: If `True`, loss and metric results are returned as a\n-                dict, with each key being the name of the metric. If `False`,\n-                they are returned as a list.\n-\n-        Returns:\n-            A scalar loss value (when no metrics and `return_dict=False`),\n-            a list of loss and metric values\n-            (if there are metrics and `return_dict=False`), or a dict of\n-            metric and loss values (if `return_dict=True`).\n-        \"\"\"\n-        self._assert_compile_called(\"train_on_batch\")\n-        self.make_train_function()\n-        if class_weight is not None:\n-            if sample_weight is not None:\n-                raise ValueError(\n-                    \"Arguments `sample_weight` and `class_weight` \"\n-                    \"cannot be specified at the same time. \"\n-                    f\"Received: sample_weight={sample_weight}, \"\n-                    f\"class_weight={class_weight}\"\n-                )\n-            sample_weight = data_adapter_utils.class_weight_to_sample_weights(\n-                y, class_weight\n-            )\n-\n-        data = (x, y, sample_weight)\n-        logs = self.train_function([data])\n-        logs = tf.nest.map_structure(lambda x: np.array(x), logs)\n-        if return_dict:\n-            return logs\n-        return self._flatten_metrics_in_order(logs)\n-\n-    def test_on_batch(\n-        self,\n-        x,\n-        y=None,\n-        sample_weight=None,\n-        return_dict=False,\n-    ):\n-        \"\"\"Test the model on a single batch of samples.\n-\n-        Args:\n-            x: Input data. Must be array-like.\n-            y: Target data. Must be array-like.\n-            sample_weight: Optional array of the same length as x, containing\n-                weights to apply to the model's loss for each sample.\n-                In the case of temporal data, you can pass a 2D array\n-                with shape `(samples, sequence_length)`, to apply a different\n-                weight to every timestep of every sample.\n-            return_dict: If `True`, loss and metric results are returned as a\n-                dict, with each key being the name of the metric. If `False`,\n-                they are returned as a list.\n-\n-        Returns:\n-            A scalar loss value (when no metrics and `return_dict=False`),\n-            a list of loss and metric values\n-            (if there are metrics and `return_dict=False`), or a dict of\n-            metric and loss values (if `return_dict=True`).\n-        \"\"\"\n-        self._assert_compile_called(\"test_on_batch\")\n-        self.make_test_function()\n-\n-        data = (x, y, sample_weight)\n-\n-        logs = self.test_function([data])\n-        logs = tf.nest.map_structure(lambda x: np.array(x), logs)\n-        if return_dict:\n-            return logs\n-        return self._flatten_metrics_in_order(logs)\n-\n-    def predict_on_batch(self, x):\n-        \"\"\"Returns predictions for a single batch of samples.\n-\n-        Args:\n-            x: Input data. It must be array-like.\n-\n-        Returns:\n-            NumPy array(s) of predictions.\n-        \"\"\"\n-        self.make_predict_function()\n-        batch_outputs = self.predict_function((x,))\n-        batch_outputs = tf.nest.map_structure(\n-            lambda x: np.array(x), batch_outputs\n-        )\n-        return batch_outputs\n\n@@ -152,9 +152,6 @@ class Functional(Function, Model):\n \n         self._layers = self.layers\n         self.built = True\n-        # We will convert directly (to the correct dtype per input).\n-        self._convert_input_args = False\n-        self._allow_non_tensor_positional_args = True\n         self._post_build()\n \n     @property\n@@ -235,16 +232,6 @@ class Functional(Function, Model):\n         # Otherwise both ref inputs and inputs will already be in same order.\n         return nest.flatten(inputs)\n \n-    def _convert_inputs_to_tensors(self, flat_inputs):\n-        flat_dtypes = [x.dtype for x in self._inputs]\n-        converted = []\n-        for x, dtype in zip(flat_inputs, flat_dtypes):\n-            if backend.is_tensor(x):\n-                converted.append(backend.cast(x, dtype=dtype))\n-            else:\n-                converted.append(backend.convert_to_tensor(x, dtype=dtype))\n-        return converted\n-\n     def _adjust_input_rank(self, flat_inputs):\n         flat_ref_shapes = [x.shape for x in self._inputs]\n         adjusted = []\n@@ -276,7 +263,6 @@ class Functional(Function, Model):\n \n     def _standardize_inputs(self, inputs):\n         flat_inputs = self._flatten_to_reference_inputs(inputs)\n-        flat_inputs = self._convert_inputs_to_tensors(flat_inputs)\n         return self._adjust_input_rank(flat_inputs)\n \n     @property\n\n@@ -171,16 +171,6 @@ class FunctionalTest(testing.TestCase):\n         out_val = model(np.random.random((2, 3)))\n         self.assertEqual(out_val.shape, (2, 3, 3))\n \n-    def test_dtype_standardization(self):\n-        float_input = Input(shape=(2,), dtype=\"float16\")\n-        int_input = Input(shape=(2,), dtype=\"int32\")\n-        float_output = float_input + 2\n-        int_output = int_input + 2\n-        model = Functional((float_input, int_input), (float_output, int_output))\n-        float_data, int_data = model((np.ones((2, 2)), np.ones((2, 2))))\n-        self.assertEqual(backend.standardize_dtype(float_data.dtype), \"float16\")\n-        self.assertEqual(backend.standardize_dtype(int_data.dtype), \"int32\")\n-\n     def test_serialization(self):\n         # Test basic model\n         inputs = Input(shape=(3,), batch_size=2)\n\n@@ -277,84 +277,6 @@ class Trainer:\n     ):\n         raise NotImplementedError\n \n-    def train_on_batch(\n-        self,\n-        x,\n-        y=None,\n-        sample_weight=None,\n-        class_weight=None,\n-        return_dict=False,\n-    ):\n-        \"\"\"Runs a single gradient update on a single batch of data.\n-\n-        Args:\n-            x: Input data. Must be array-like.\n-            y: Target data. Must be array-like.\n-            sample_weight: Optional array of the same length as x, containing\n-                weights to apply to the model's loss for each sample.\n-                In the case of temporal data, you can pass a 2D array\n-                with shape `(samples, sequence_length)`, to apply a different\n-                weight to every timestep of every sample.\n-            class_weight: Optional dictionary mapping class indices (integers)\n-                to a weight (float) to apply to the model's loss for the samples\n-                from this class during training. This can be useful to tell the\n-                model to \"pay more attention\" to samples from an\n-                under-represented class. When `class_weight` is specified\n-                and targets have a rank of 2 or greater, either `y` must\n-                be one-hot encoded, or an explicit final dimension of 1\n-                must be included for sparse class labels.\n-            return_dict: If `True`, loss and metric results are returned as a\n-                dict, with each key being the name of the metric. If `False`,\n-                they are returned as a list.\n-\n-        Returns:\n-            A scalar loss value (when no metrics and `return_dict=False`),\n-            a list of loss and metric values\n-            (if there are metrics and `return_dict=False`), or a dict of\n-            metric and loss values (if `return_dict=True`).\n-        \"\"\"\n-        raise NotImplementedError\n-\n-    def test_on_batch(\n-        self,\n-        x,\n-        y=None,\n-        sample_weight=None,\n-        return_dict=False,\n-    ):\n-        \"\"\"Test the model on a single batch of samples.\n-\n-        Args:\n-            x: Input data. Must be array-like.\n-            y: Target data. Must be array-like.\n-            sample_weight: Optional array of the same length as x, containing\n-                weights to apply to the model's loss for each sample.\n-                In the case of temporal data, you can pass a 2D array\n-                with shape `(samples, sequence_length)`, to apply a different\n-                weight to every timestep of every sample.\n-            return_dict: If `True`, loss and metric results are returned as a\n-                dict, with each key being the name of the metric. If `False`,\n-                they are returned as a list.\n-\n-        Returns:\n-            A scalar loss value (when no metrics and `return_dict=False`),\n-            a list of loss and metric values\n-            (if there are metrics and `return_dict=False`), or a dict of\n-            metric and loss values (if `return_dict=True`).\n-        \"\"\"\n-        raise NotImplementedError\n-\n-    def predict_on_batch(self, x):\n-        \"\"\"Returns predictions for a single batch of samples.\n-\n-        Args:\n-            x: Input data. It must be array-like.\n-\n-        Returns:\n-            NumPy array(s) of predictions.\n-        \"\"\"\n-        raise NotImplementedError\n-\n     def get_compile_config(self):\n         \"\"\"Returns a serialized config with information for compiling the model.\n \n@@ -410,7 +332,7 @@ class Trainer:\n \n     def _pythonify_logs(self, logs):\n         result = {}\n-        for key, value in logs.items():\n+        for key, value in sorted(logs.items()):\n             try:\n                 value = float(value)\n             except:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#638d9a002c2febc428a08de591fb343e5251f429", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 286 | Lines Deleted: 34 | Files Changed: 5 | Hunks: 20 | Methods Changed: 22 | Complexity Δ (Sum/Max): 24/16 | Churn Δ: 320 | Churn Cumulative: 4722 | Contributors (this commit): 10 | Commits (past 90d): 117 | Contributors (cumulative): 22 | DMM Complexity: 0.9743589743589743\n\nDIFF:\n@@ -29,6 +29,11 @@ history = model.fit(\n model.evaluate(x, y, verbose=0)\n model.predict(x, verbose=0)\n \n+# Test on batch functions\n+model.train_on_batch(x, y)\n+model.test_on_batch(x, y)\n+model.predict_on_batch(x)\n+\n # Test functional model.\n inputs = keras_core.Input(shape=(32, 32, 3))\n outputs = layers.Conv2D(filters=10, kernel_size=3)(inputs)\n\n@@ -18,9 +18,16 @@ class TorchTrainer(base_trainer.Trainer):\n         self.test_function = None\n         self.predict_function = None\n \n-    def train_step(self, data):\n+    def make_train_function(self, force=False):\n+        if self.train_function is not None and not force:\n+            return self.train_function\n+\n+        def one_step_on_data(data):\n+            \"\"\"Runs a single training step on a batch of data.\"\"\"\n             data = data[0]\n-        x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n+            x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(\n+                data\n+            )\n \n             # Compute prediction error\n             if self._call_has_training_arg():\n@@ -28,8 +35,8 @@ class TorchTrainer(base_trainer.Trainer):\n             else:\n                 y_pred = self(x)\n \n-        # Call torch.nn.Module.zero_grad() to clear the leftover gradients for\n-        # the weights from the previous train step.\n+            # Call torch.nn.Module.zero_grad() to clear the leftover gradients\n+            # for the weights from the previous train step.\n             self.zero_grad()\n \n             loss = self.compute_loss(\n@@ -42,8 +49,8 @@ class TorchTrainer(base_trainer.Trainer):\n                 # Backpropagation\n                 trainable_weights = [v for v in self.trainable_weights]\n \n-            # Call torch.Tensor.backward() on the loss to compute gradients for\n-            # the weights.\n+                # Call torch.Tensor.backward() on the loss to compute gradients\n+                # for the weights.\n                 loss.backward()\n \n                 gradients = [v.value.grad for v in trainable_weights]\n@@ -56,7 +63,55 @@ class TorchTrainer(base_trainer.Trainer):\n             else:\n                 warnings.warn(\"The model does not have any trainable weights.\")\n \n-        return self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n+            return self.compute_metrics(\n+                x, y, y_pred, sample_weight=sample_weight\n+            )\n+\n+        self.train_function = one_step_on_data\n+\n+    def make_test_function(self, force=False):\n+        if self.test_function is not None and not force:\n+            return self.test_function\n+\n+        def one_step_on_data(data):\n+            \"\"\"Runs a single test step on a batch of data.\"\"\"\n+            with torch.no_grad():\n+                data = data[0]\n+                (\n+                    x,\n+                    y,\n+                    sample_weight,\n+                ) = data_adapter_utils.unpack_x_y_sample_weight(data)\n+                if self._call_has_training_arg():\n+                    y_pred = self(x, training=False)\n+                else:\n+                    y_pred = self(x)\n+                loss = self.compute_loss(\n+                    x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n+                )\n+                self._loss_tracker.update_state(loss)\n+                return self.compute_metrics(\n+                    x, y, y_pred, sample_weight=sample_weight\n+                )\n+\n+        self.test_function = one_step_on_data\n+\n+    def make_predict_function(self, force=False):\n+        if self.predict_function is not None and not force:\n+            return self.predict_function\n+\n+        def one_step_on_data(data):\n+            \"\"\"Runs a predict test step on a batch of data.\"\"\"\n+            with torch.no_grad():\n+                data = data[0]\n+                x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n+                if self._call_has_training_arg():\n+                    y_pred = self(x, training=False)\n+                else:\n+                    y_pred = self(x)\n+                return y_pred\n+\n+        self.predict_function = one_step_on_data\n \n     def fit(\n         self,\n@@ -127,6 +182,7 @@ class TorchTrainer(base_trainer.Trainer):\n             )\n \n         self.stop_training = False\n+        self.make_train_function()\n         callbacks.on_train_begin()\n \n         for epoch in range(initial_epoch, epochs):\n@@ -141,7 +197,7 @@ class TorchTrainer(base_trainer.Trainer):\n                 # Callbacks\n                 callbacks.on_train_batch_begin(step)\n \n-                logs = self.train_step(data)\n+                logs = self.train_function(data)\n \n                 # Callbacks\n                 callbacks.on_train_batch_end(step, self._pythonify_logs(logs))\n@@ -197,19 +253,6 @@ class TorchTrainer(base_trainer.Trainer):\n         callbacks.on_train_end(logs=training_logs)\n         return self.history\n \n-    def test_step(self, data):\n-        data = data[0]\n-        x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        if self._call_has_training_arg():\n-            y_pred = self(x, training=False)\n-        else:\n-            y_pred = self(x)\n-        loss = self.compute_loss(\n-            x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n-        )\n-        self._loss_tracker.update_state(loss)\n-        return self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n-\n     def evaluate(\n         self,\n         x=None,\n@@ -256,13 +299,13 @@ class TorchTrainer(base_trainer.Trainer):\n         # Switch the torch Module back to testing mode.\n         self.eval()\n \n+        self.make_test_function()\n         callbacks.on_test_begin()\n         logs = None\n         self.reset_metrics()\n         for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n             callbacks.on_test_batch_begin(step)\n-            with torch.no_grad():\n-                logs = self.test_step(data)\n+            logs = self.test_function(data)\n             callbacks.on_test_batch_end(step, self._pythonify_logs(logs))\n         logs = self.get_metrics_result()\n         callbacks.on_test_end(logs)\n@@ -271,15 +314,6 @@ class TorchTrainer(base_trainer.Trainer):\n             return logs\n         return self._flatten_metrics_in_order(logs)\n \n-    def predict_step(self, data):\n-        data = data[0]\n-        x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        if self._call_has_training_arg():\n-            y_pred = self(x, training=False)\n-        else:\n-            y_pred = self(x)\n-        return y_pred\n-\n     def predict(\n         self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n     ):\n@@ -322,15 +356,126 @@ class TorchTrainer(base_trainer.Trainer):\n         # Switch the torch Module back to testing mode.\n         self.eval()\n \n+        self.make_predict_function()\n         callbacks.on_predict_begin()\n         outputs = None\n         for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n             callbacks.on_predict_batch_begin(step)\n-            with torch.no_grad():\n-                batch_outputs = self.predict_step(data)\n+            batch_outputs = self.predict_function(data)\n             outputs = append_to_outputs(batch_outputs, outputs)\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n         callbacks.on_predict_end()\n         return tf.__internal__.nest.map_structure_up_to(\n             batch_outputs, np.concatenate, outputs\n         )\n+\n+    def train_on_batch(\n+        self,\n+        x,\n+        y=None,\n+        sample_weight=None,\n+        class_weight=None,\n+        return_dict=False,\n+    ):\n+        \"\"\"Runs a single gradient update on a single batch of data.\n+\n+        Args:\n+            x: Input data. Must be array-like.\n+            y: Target data. Must be array-like.\n+            sample_weight: Optional array of the same length as x, containing\n+                weights to apply to the model's loss for each sample.\n+                In the case of temporal data, you can pass a 2D array\n+                with shape `(samples, sequence_length)`, to apply a different\n+                weight to every timestep of every sample.\n+            class_weight: Optional dictionary mapping class indices (integers)\n+                to a weight (float) to apply to the model's loss for the samples\n+                from this class during training. This can be useful to tell the\n+                model to \"pay more attention\" to samples from an\n+                under-represented class. When `class_weight` is specified\n+                and targets have a rank of 2 or greater, either `y` must\n+                be one-hot encoded, or an explicit final dimension of 1\n+                must be included for sparse class labels.\n+            return_dict: If `True`, loss and metric results are returned as a\n+                dict, with each key being the name of the metric. If `False`,\n+                they are returned as a list.\n+\n+        Returns:\n+            A scalar loss value (when no metrics and `return_dict=False`),\n+            a list of loss and metric values\n+            (if there are metrics and `return_dict=False`), or a dict of\n+            metric and loss values (if `return_dict=True`).\n+        \"\"\"\n+        self._assert_compile_called(\"train_on_batch\")\n+        self.make_train_function()\n+        if class_weight is not None:\n+            if sample_weight is not None:\n+                raise ValueError(\n+                    \"Arguments `sample_weight` and `class_weight` \"\n+                    \"cannot be specified at the same time. \"\n+                    f\"Received: sample_weight={sample_weight}, \"\n+                    f\"class_weight={class_weight}\"\n+                )\n+            sample_weight = data_adapter_utils.class_weight_to_sample_weights(\n+                y, class_weight\n+            )\n+\n+        data = (x, y, sample_weight)\n+        logs = self.train_function([data])\n+        logs = tf.nest.map_structure(lambda x: np.array(x), logs)\n+        if return_dict:\n+            return logs\n+        return self._flatten_metrics_in_order(logs)\n+\n+    def test_on_batch(\n+        self,\n+        x,\n+        y=None,\n+        sample_weight=None,\n+        return_dict=False,\n+    ):\n+        \"\"\"Test the model on a single batch of samples.\n+\n+        Args:\n+            x: Input data. Must be array-like.\n+            y: Target data. Must be array-like.\n+            sample_weight: Optional array of the same length as x, containing\n+                weights to apply to the model's loss for each sample.\n+                In the case of temporal data, you can pass a 2D array\n+                with shape `(samples, sequence_length)`, to apply a different\n+                weight to every timestep of every sample.\n+            return_dict: If `True`, loss and metric results are returned as a\n+                dict, with each key being the name of the metric. If `False`,\n+                they are returned as a list.\n+\n+        Returns:\n+            A scalar loss value (when no metrics and `return_dict=False`),\n+            a list of loss and metric values\n+            (if there are metrics and `return_dict=False`), or a dict of\n+            metric and loss values (if `return_dict=True`).\n+        \"\"\"\n+        self._assert_compile_called(\"test_on_batch\")\n+        self.make_test_function()\n+\n+        data = (x, y, sample_weight)\n+\n+        logs = self.test_function([data])\n+        logs = tf.nest.map_structure(lambda x: np.array(x), logs)\n+        if return_dict:\n+            return logs\n+        return self._flatten_metrics_in_order(logs)\n+\n+    def predict_on_batch(self, x):\n+        \"\"\"Returns predictions for a single batch of samples.\n+\n+        Args:\n+            x: Input data. It must be array-like.\n+\n+        Returns:\n+            NumPy array(s) of predictions.\n+        \"\"\"\n+        self.make_predict_function()\n+        batch_outputs = self.predict_function((x,))\n+        batch_outputs = tf.nest.map_structure(\n+            lambda x: np.array(x), batch_outputs\n+        )\n+        return batch_outputs\n\n@@ -152,6 +152,9 @@ class Functional(Function, Model):\n \n         self._layers = self.layers\n         self.built = True\n+        # We will convert directly (to the correct dtype per input).\n+        self._convert_input_args = False\n+        self._allow_non_tensor_positional_args = True\n         self._post_build()\n \n     @property\n@@ -232,6 +235,16 @@ class Functional(Function, Model):\n         # Otherwise both ref inputs and inputs will already be in same order.\n         return nest.flatten(inputs)\n \n+    def _convert_inputs_to_tensors(self, flat_inputs):\n+        flat_dtypes = [x.dtype for x in self._inputs]\n+        converted = []\n+        for x, dtype in zip(flat_inputs, flat_dtypes):\n+            if backend.is_tensor(x):\n+                converted.append(backend.cast(x, dtype=dtype))\n+            else:\n+                converted.append(backend.convert_to_tensor(x, dtype=dtype))\n+        return converted\n+\n     def _adjust_input_rank(self, flat_inputs):\n         flat_ref_shapes = [x.shape for x in self._inputs]\n         adjusted = []\n@@ -263,6 +276,7 @@ class Functional(Function, Model):\n \n     def _standardize_inputs(self, inputs):\n         flat_inputs = self._flatten_to_reference_inputs(inputs)\n+        flat_inputs = self._convert_inputs_to_tensors(flat_inputs)\n         return self._adjust_input_rank(flat_inputs)\n \n     @property\n\n@@ -171,6 +171,16 @@ class FunctionalTest(testing.TestCase):\n         out_val = model(np.random.random((2, 3)))\n         self.assertEqual(out_val.shape, (2, 3, 3))\n \n+    def test_dtype_standardization(self):\n+        float_input = Input(shape=(2,), dtype=\"float16\")\n+        int_input = Input(shape=(2,), dtype=\"int32\")\n+        float_output = float_input + 2\n+        int_output = int_input + 2\n+        model = Functional((float_input, int_input), (float_output, int_output))\n+        float_data, int_data = model((np.ones((2, 2)), np.ones((2, 2))))\n+        self.assertEqual(backend.standardize_dtype(float_data.dtype), \"float16\")\n+        self.assertEqual(backend.standardize_dtype(int_data.dtype), \"int32\")\n+\n     def test_serialization(self):\n         # Test basic model\n         inputs = Input(shape=(3,), batch_size=2)\n\n@@ -277,6 +277,84 @@ class Trainer:\n     ):\n         raise NotImplementedError\n \n+    def train_on_batch(\n+        self,\n+        x,\n+        y=None,\n+        sample_weight=None,\n+        class_weight=None,\n+        return_dict=False,\n+    ):\n+        \"\"\"Runs a single gradient update on a single batch of data.\n+\n+        Args:\n+            x: Input data. Must be array-like.\n+            y: Target data. Must be array-like.\n+            sample_weight: Optional array of the same length as x, containing\n+                weights to apply to the model's loss for each sample.\n+                In the case of temporal data, you can pass a 2D array\n+                with shape `(samples, sequence_length)`, to apply a different\n+                weight to every timestep of every sample.\n+            class_weight: Optional dictionary mapping class indices (integers)\n+                to a weight (float) to apply to the model's loss for the samples\n+                from this class during training. This can be useful to tell the\n+                model to \"pay more attention\" to samples from an\n+                under-represented class. When `class_weight` is specified\n+                and targets have a rank of 2 or greater, either `y` must\n+                be one-hot encoded, or an explicit final dimension of 1\n+                must be included for sparse class labels.\n+            return_dict: If `True`, loss and metric results are returned as a\n+                dict, with each key being the name of the metric. If `False`,\n+                they are returned as a list.\n+\n+        Returns:\n+            A scalar loss value (when no metrics and `return_dict=False`),\n+            a list of loss and metric values\n+            (if there are metrics and `return_dict=False`), or a dict of\n+            metric and loss values (if `return_dict=True`).\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def test_on_batch(\n+        self,\n+        x,\n+        y=None,\n+        sample_weight=None,\n+        return_dict=False,\n+    ):\n+        \"\"\"Test the model on a single batch of samples.\n+\n+        Args:\n+            x: Input data. Must be array-like.\n+            y: Target data. Must be array-like.\n+            sample_weight: Optional array of the same length as x, containing\n+                weights to apply to the model's loss for each sample.\n+                In the case of temporal data, you can pass a 2D array\n+                with shape `(samples, sequence_length)`, to apply a different\n+                weight to every timestep of every sample.\n+            return_dict: If `True`, loss and metric results are returned as a\n+                dict, with each key being the name of the metric. If `False`,\n+                they are returned as a list.\n+\n+        Returns:\n+            A scalar loss value (when no metrics and `return_dict=False`),\n+            a list of loss and metric values\n+            (if there are metrics and `return_dict=False`), or a dict of\n+            metric and loss values (if `return_dict=True`).\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def predict_on_batch(self, x):\n+        \"\"\"Returns predictions for a single batch of samples.\n+\n+        Args:\n+            x: Input data. It must be array-like.\n+\n+        Returns:\n+            NumPy array(s) of predictions.\n+        \"\"\"\n+        raise NotImplementedError\n+\n     def get_compile_config(self):\n         \"\"\"Returns a serialized config with information for compiling the model.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1f1ea1436c571841079297403db6ae633c10f959", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 12 | Files Changed: 5 | Hunks: 12 | Methods Changed: 13 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 37 | Churn Cumulative: 12659 | Contributors (this commit): 13 | Commits (past 90d): 117 | Contributors (cumulative): 38 | DMM Complexity: 1.0\n\nDIFF:\n@@ -387,8 +387,8 @@ def conv_transpose(\n     )\n \n \n-def one_hot(x, num_classes, axis=-1):\n-    return jnn.one_hot(x, num_classes, axis=axis)\n+def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+    return jnn.one_hot(x, num_classes, axis=axis, dtype=dtype)\n \n \n def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n\n@@ -416,8 +416,8 @@ def conv_transpose(\n     )\n \n \n-def one_hot(x, num_classes, axis=-1):\n-    return tf.one_hot(x, num_classes, axis=axis)\n+def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+    return tf.one_hot(x, num_classes, axis=axis, dtype=dtype)\n \n \n def _get_logits(output, from_logits, op_type, fn_name):\n\n@@ -470,11 +470,12 @@ def conv_transpose(\n     return outputs\n \n \n-def one_hot(x, num_classes, axis=-1):\n+def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n     # Axis is the output axis. By default, PyTorch, outputs to last axis.\n     # If axis is not last, change output to axis and shift remaining elements.\n     x = convert_to_tensor(x, dtype=torch.long)\n     output = tnn.one_hot(x, num_classes)\n+    output = convert_to_tensor(output, dtype=dtype)\n     dims = output.dim()\n     if axis != -1 and axis != dims:\n         new_axes_order = list(range(dims))\n\n@@ -960,13 +960,16 @@ def conv_transpose(\n \n \n class OneHot(Operation):\n-    def __init__(self, num_classes, axis=-1):\n+    def __init__(self, num_classes, axis=-1, dtype=None):\n         super().__init__()\n         self.num_classes = num_classes\n         self.axis = axis\n+        self.dtype = dtype or backend.floatx()\n \n     def call(self, x):\n-        return backend.nn.one_hot(x, self.num_classes, axis=self.axis)\n+        return backend.nn.one_hot(\n+            x, self.num_classes, axis=self.axis, dtype=self.dtype\n+        )\n \n     def compute_output_spec(self, x):\n         x_shape = list(getattr(x, \"shape\", []))\n@@ -979,16 +982,18 @@ class OneHot(Operation):\n                 f\"axis must be -1 or between [0, {len(x.shape)}), but \"\n                 f\"received {self.axis}.\"\n             )\n-        return KerasTensor(x_shape)\n+        return KerasTensor(x_shape, dtype=self.dtype)\n \n \n @keras_core_export(\n     [\"keras_core.operations.one_hot\", \"keras_core.operations.nn.one_hot\"]\n )\n-def one_hot(x, num_classes, axis=-1):\n+def one_hot(x, num_classes, axis=-1, dtype=None):\n     if any_symbolic_tensors((x,)):\n-        return OneHot(num_classes, axis=axis).symbolic_call(x)\n-    return backend.nn.one_hot(x, num_classes, axis=axis)\n+        return OneHot(num_classes, axis=axis, dtype=dtype).symbolic_call(x)\n+    return backend.nn.one_hot(\n+        x, num_classes, axis=axis, dtype=dtype or backend.floatx()\n+    )\n \n \n class BinaryCrossentropy(Operation):\n\n@@ -8,7 +8,7 @@ from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.operations import nn as knn\n \n \n-class NNOpsDynamicShapeTest(testing.TestCase):\n+class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n     def test_relu(self):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.relu(x).shape, (None, 2, 3))\n@@ -281,6 +281,13 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(knn.one_hot(x, 5, 1).shape, (None, 5, 3, 1))\n         self.assertEqual(knn.one_hot(x, 5, 2).shape, (None, 3, 5, 1))\n \n+    @parameterized.product(dtype=[\"float32\", \"int32\"])\n+    def test_one_hot_dtype(self, dtype):\n+        # dtype tests\n+        x = np.arange(5)\n+        out = knn.one_hot(x, 5, axis=0, dtype=dtype)\n+        self.assertEqual(out.dtype.name, dtype)\n+\n \n class NNOpsStaticShapeTest(testing.TestCase):\n     def test_relu(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b2fe70618f3c46f4d386386385e37b7970b321cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 571 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 10 | Complexity Δ (Sum/Max): 26/26 | Churn Δ: 571 | Churn Cumulative: 571 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.8341968911917098\n\nDIFF:\n@@ -0,0 +1,571 @@\n+\"\"\"\n+Title: Classification using Attention-based Deep Multiple Instance Learning (MIL).\n+Author: [Mohamad Jaber](https://www.linkedin.com/in/mohamadjaber1/)\n+Date created: 2021/08/16\n+Last modified: 2021/11/25\n+Description: MIL approach to classify bags of instances and get their individual instance score.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+### What is Multiple Instance Learning (MIL)?\n+\n+Usually, with supervised learning algorithms, the learner receives labels for a set of\n+instances. In the case of MIL, the learner receives labels for a set of bags, each of which\n+contains a set of instances. The bag is labeled positive if it contains at least\n+one positive instance, and negative if it does not contain any.\n+\n+### Motivation\n+\n+It is often assumed in image classification tasks that each image clearly represents a\n+class label. In medical imaging (e.g. computational pathology, etc.) an *entire image*\n+is represented by a single class label (cancerous/non-cancerous) or a region of interest\n+could be given. However, one will be interested in knowing which patterns in the image\n+is actually causing it to belong to that class. In this context, the image(s) will be\n+divided and the subimages will form the bag of instances.\n+\n+Therefore, the goals are to:\n+\n+1. Learn a model to predict a class label for a bag of instances.\n+2. Find out which instances within the bag caused a position class label\n+prediction.\n+\n+### Implementation\n+\n+The following steps describe how the model works:\n+\n+1. The feature extractor layers extract feature embeddings.\n+2. The embeddings are fed into the MIL attention layer to get\n+the attention scores. The layer is designed as permutation-invariant.\n+3. Input features and their corresponding attention scores are multiplied together.\n+4. The resulting output is passed to a softmax function for classification.\n+\n+### References\n+\n+- [Attention-based Deep Multiple Instance Learning](https://arxiv.org/abs/1802.04712).\n+- Some of the attention operator code implementation was inspired from https://github.com/utayao/Atten_Deep_MIL.\n+- Imbalanced data [tutorial](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data)\n+by TensorFlow.\n+\n+\"\"\"\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import numpy as np\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core import operations as ops\n+from tqdm import tqdm\n+from matplotlib import pyplot as plt\n+\n+plt.style.use(\"ggplot\")\n+\n+\"\"\"\n+## Create dataset\n+\n+We will create a set of bags and assign their labels according to their contents.\n+If at least one positive instance\n+is available in a bag, the bag is considered as a positive bag. If it does not contain any\n+positive instance, the bag will be considered as negative.\n+\n+### Configuration parameters\n+\n+- `POSITIVE_CLASS`: The desired class to be kept in the positive bag.\n+- `BAG_COUNT`: The number of training bags.\n+- `VAL_BAG_COUNT`: The number of validation bags.\n+- `BAG_SIZE`: The number of instances in a bag.\n+- `PLOT_SIZE`: The number of bags to plot.\n+- `ENSEMBLE_AVG_COUNT`: The number of models to create and average together. (Optional:\n+often results in better performance - set to 1 for single model)\n+\"\"\"\n+\n+POSITIVE_CLASS = 1\n+BAG_COUNT = 1000\n+VAL_BAG_COUNT = 300\n+BAG_SIZE = 3\n+PLOT_SIZE = 3\n+ENSEMBLE_AVG_COUNT = 1\n+\n+\"\"\"\n+### Prepare bags\n+\n+Since the attention operator is a permutation-invariant operator, an instance with a\n+positive class label is randomly placed among the instances in the positive bag.\n+\"\"\"\n+\n+\n+def create_bags(input_data, input_labels, positive_class, bag_count, instance_count):\n+    # Set up bags.\n+    bags = []\n+    bag_labels = []\n+\n+    # Normalize input data.\n+    input_data = np.divide(input_data, 255.0)\n+\n+    # Count positive samples.\n+    count = 0\n+\n+    for _ in range(bag_count):\n+        # Pick a fixed size random subset of samples.\n+        index = np.random.choice(input_data.shape[0], instance_count, replace=False)\n+        instances_data = input_data[index]\n+        instances_labels = input_labels[index]\n+\n+        # By default, all bags are labeled as 0.\n+        bag_label = 0\n+\n+        # Check if there is at least a positive class in the bag.\n+        if positive_class in instances_labels:\n+            # Positive bag will be labeled as 1.\n+            bag_label = 1\n+            count += 1\n+\n+        bags.append(instances_data)\n+        bag_labels.append(np.array([bag_label]))\n+\n+    print(f\"Positive bags: {count}\")\n+    print(f\"Negative bags: {bag_count - count}\")\n+\n+    return (list(np.swapaxes(bags, 0, 1)), np.array(bag_labels))\n+\n+\n+# Load the MNIST dataset.\n+(x_train, y_train), (x_val, y_val) = keras.datasets.mnist.load_data()\n+\n+# Create training data.\n+train_data, train_labels = create_bags(\n+    x_train, y_train, POSITIVE_CLASS, BAG_COUNT, BAG_SIZE\n+)\n+\n+# Create validation data.\n+val_data, val_labels = create_bags(\n+    x_val, y_val, POSITIVE_CLASS, VAL_BAG_COUNT, BAG_SIZE\n+)\n+\n+\"\"\"\n+## Create the model\n+\n+We will now build the attention layer, prepare some utilities, then build and train the\n+entire model.\n+\n+### Attention operator implementation\n+\n+The output size of this layer is decided by the size of a single bag.\n+\n+The attention mechanism uses a weighted average of instances in a bag, in which the sum\n+of the weights must equal to 1 (invariant of the bag size).\n+\n+The weight matrices (parameters) are **w** and **v**. To include positive and negative\n+values, hyperbolic tangent element-wise non-linearity is utilized.\n+\n+A **Gated attention mechanism** can be used to deal with complex relations. Another weight\n+matrix, **u**, is added to the computation.\n+A sigmoid non-linearity is used to overcome approximately linear behavior for *x* ∈ [−1, 1]\n+by hyperbolic tangent non-linearity.\n+\"\"\"\n+\n+\n+class MILAttentionLayer(layers.Layer):\n+    \"\"\"Implementation of the attention-based Deep MIL layer.\n+\n+    Args:\n+      weight_params_dim: Positive Integer. Dimension of the weight matrix.\n+      kernel_initializer: Initializer for the `kernel` matrix.\n+      kernel_regularizer: Regularizer function applied to the `kernel` matrix.\n+      use_gated: Boolean, whether or not to use the gated mechanism.\n+\n+    Returns:\n+      List of 2D tensors with BAG_SIZE length.\n+      The tensors are the attention scores after softmax with shape `(batch_size, 1)`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        weight_params_dim,\n+        kernel_initializer=\"glorot_uniform\",\n+        kernel_regularizer=None,\n+        use_gated=False,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.weight_params_dim = weight_params_dim\n+        self.use_gated = use_gated\n+\n+        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n+        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n+\n+        self.v_init = self.kernel_initializer\n+        self.w_init = self.kernel_initializer\n+        self.u_init = self.kernel_initializer\n+\n+        self.v_regularizer = self.kernel_regularizer\n+        self.w_regularizer = self.kernel_regularizer\n+        self.u_regularizer = self.kernel_regularizer\n+\n+    def build(self, input_shape):\n+        # Input shape.\n+        # List of 2D tensors with shape: (batch_size, input_dim).\n+        input_dim = input_shape[0][1]\n+\n+        self.v_weight_params = self.add_weight(\n+            shape=(input_dim, self.weight_params_dim),\n+            initializer=self.v_init,\n+            name=\"v\",\n+            regularizer=self.v_regularizer,\n+            trainable=True,\n+        )\n+\n+        self.w_weight_params = self.add_weight(\n+            shape=(self.weight_params_dim, 1),\n+            initializer=self.w_init,\n+            name=\"w\",\n+            regularizer=self.w_regularizer,\n+            trainable=True,\n+        )\n+\n+        if self.use_gated:\n+            self.u_weight_params = self.add_weight(\n+                shape=(input_dim, self.weight_params_dim),\n+                initializer=self.u_init,\n+                name=\"u\",\n+                regularizer=self.u_regularizer,\n+                trainable=True,\n+            )\n+        else:\n+            self.u_weight_params = None\n+\n+        self.input_built = True\n+\n+    def call(self, inputs):\n+        # Assigning variables from the number of inputs.\n+        instances = [self.compute_attention_scores(instance) for instance in inputs]\n+\n+        # Stack instances into a single tensor.\n+        instances = ops.stack(instances)\n+\n+        # Apply softmax over instances such that the output summation is equal to 1.\n+        alpha = ops.softmax(instances, axis=0)\n+\n+        # Split to recreate the same array of tensors we had as inputs.\n+        return [alpha[i] for i in range(alpha.shape[0])]\n+\n+    def compute_attention_scores(self, instance):\n+        # Reserve in-case \"gated mechanism\" used.\n+        original_instance = instance\n+\n+        # tanh(v*h_k^T)\n+        instance = ops.tanh(ops.tensordot(instance, self.v_weight_params, axes=1))\n+\n+        # for learning non-linear relations efficiently.\n+        if self.use_gated:\n+            instance = instance * ops.sigmoid(\n+                ops.tensordot(original_instance, self.u_weight_params, axes=1)\n+            )\n+\n+        # w^T*(tanh(v*h_k^T)) / w^T*(tanh(v*h_k^T)*sigmoid(u*h_k^T))\n+        return ops.tensordot(instance, self.w_weight_params, axes=1)\n+\n+\n+\"\"\"\n+## Visualizer tool\n+\n+Plot the number of bags (given by `PLOT_SIZE`) with respect to the class.\n+\n+Moreover, if activated, the class label prediction with its associated instance score\n+for each bag (after the model has been trained) can be seen.\n+\"\"\"\n+\n+\n+def plot(data, labels, bag_class, predictions=None, attention_weights=None):\n+    \"\"\" \"Utility for plotting bags and attention weights.\n+\n+    Args:\n+      data: Input data that contains the bags of instances.\n+      labels: The associated bag labels of the input data.\n+      bag_class: String name of the desired bag class.\n+        The options are: \"positive\" or \"negative\".\n+      predictions: Class labels model predictions.\n+      If you don't specify anything, ground truth labels will be used.\n+      attention_weights: Attention weights for each instance within the input data.\n+      If you don't specify anything, the values won't be displayed.\n+    \"\"\"\n+    return ## TODO\n+    labels = np.array(labels).reshape(-1)\n+\n+    if bag_class == \"positive\":\n+        if predictions is not None:\n+            labels = np.where(predictions.argmax(1) == 1)[0]\n+            bags = np.array(data)[:, labels[0:PLOT_SIZE]]\n+\n+        else:\n+            labels = np.where(labels == 1)[0]\n+            bags = np.array(data)[:, labels[0:PLOT_SIZE]]\n+\n+    elif bag_class == \"negative\":\n+        if predictions is not None:\n+            labels = np.where(predictions.argmax(1) == 0)[0]\n+            bags = np.array(data)[:, labels[0:PLOT_SIZE]]\n+        else:\n+            labels = np.where(labels == 0)[0]\n+            bags = np.array(data)[:, labels[0:PLOT_SIZE]]\n+\n+    else:\n+        print(f\"There is no class {bag_class}\")\n+        return\n+\n+    print(f\"The bag class label is {bag_class}\")\n+    for i in range(PLOT_SIZE):\n+        figure = plt.figure(figsize=(8, 8))\n+        print(f\"Bag number: {labels[i]}\")\n+        for j in range(BAG_SIZE):\n+            image = bags[j][i]\n+            figure.add_subplot(1, BAG_SIZE, j + 1)\n+            plt.grid(False)\n+            if attention_weights is not None:\n+                plt.title(np.around(attention_weights[labels[i]][j], 2))\n+            plt.imshow(image)\n+        plt.show()\n+\n+\n+# Plot some of validation data bags per class.\n+plot(val_data, val_labels, \"positive\")\n+plot(val_data, val_labels, \"negative\")\n+\n+\"\"\"\n+## Create model\n+\n+First we will create some embeddings per instance, invoke the attention operator and then\n+use the softmax function to output the class probabilities.\n+\"\"\"\n+\n+\n+def create_model(instance_shape):\n+    # Extract features from inputs.\n+    inputs, embeddings = [], []\n+    shared_dense_layer_1 = layers.Dense(128, activation=\"relu\")\n+    shared_dense_layer_2 = layers.Dense(64, activation=\"relu\")\n+    for _ in range(BAG_SIZE):\n+        inp = layers.Input(instance_shape)\n+        flatten = layers.Flatten()(inp)\n+        dense_1 = shared_dense_layer_1(flatten)\n+        dense_2 = shared_dense_layer_2(dense_1)\n+        inputs.append(inp)\n+        embeddings.append(dense_2)\n+\n+    # Invoke the attention layer.\n+    alpha = MILAttentionLayer(\n+        weight_params_dim=256,\n+        kernel_regularizer=keras.regularizers.L2(0.01),\n+        use_gated=True,\n+        name=\"alpha\",\n+    )(embeddings)\n+\n+    # Multiply attention weights with the input layers.\n+    multiply_layers = [\n+        layers.multiply([alpha[i], embeddings[i]]) for i in range(len(alpha))\n+    ]\n+\n+    # Concatenate layers.\n+    concat = layers.concatenate(multiply_layers, axis=1)\n+\n+    # Classification output node.\n+    output = layers.Dense(2, activation=\"softmax\")(concat)\n+\n+    return keras.Model(inputs, output)\n+\n+\n+\"\"\"\n+## Class weights\n+\n+Since this kind of problem could simply turn into imbalanced data classification problem,\n+class weighting should be considered.\n+\n+Let's say there are 1000 bags. There often could be cases were ~90 % of the bags do not\n+contain any positive label and ~10 % do.\n+Such data can be referred to as **Imbalanced data**.\n+\n+Using class weights, the model will tend to give a higher weight to the rare class.\n+\"\"\"\n+\n+\n+def compute_class_weights(labels):\n+    # Count number of postive and negative bags.\n+    negative_count = len(np.where(labels == 0)[0])\n+    positive_count = len(np.where(labels == 1)[0])\n+    total_count = negative_count + positive_count\n+\n+    # Build class weight dictionary.\n+    return {\n+        0: (1 / negative_count) * (total_count / 2),\n+        1: (1 / positive_count) * (total_count / 2),\n+    }\n+\n+\n+\"\"\"\n+## Build and train model\n+\n+The model is built and trained in this section.\n+\"\"\"\n+\n+\n+def train(train_data, train_labels, val_data, val_labels, model):\n+    # Train model.\n+    # Prepare callbacks.\n+    # Path where to save best weights.\n+\n+    # Take the file name from the wrapper.\n+    file_path = \"/tmp/best_model.weights.h5\"\n+\n+    # Initialize model checkpoint callback.\n+    model_checkpoint = keras.callbacks.ModelCheckpoint(\n+        file_path,\n+        monitor=\"val_loss\",\n+        verbose=0,\n+        mode=\"min\",\n+        save_best_only=True,\n+        save_weights_only=True,\n+    )\n+\n+    # Initialize early stopping callback.\n+    # The model performance is monitored across the validation data and stops training\n+    # when the generalization error cease to decrease.\n+    early_stopping = keras.callbacks.EarlyStopping(\n+        monitor=\"val_loss\", patience=10, mode=\"min\"\n+    )\n+\n+    # Compile model.\n+    model.compile(\n+        optimizer=\"adam\",\n+        loss=\"sparse_categorical_crossentropy\",\n+        metrics=[\"accuracy\"],\n+    )\n+\n+    # Fit model.\n+    model.fit(\n+        train_data,\n+        train_labels,\n+        validation_data=(val_data, val_labels),\n+        epochs=20,\n+        class_weight=compute_class_weights(train_labels),\n+        batch_size=1,\n+        callbacks=[early_stopping, model_checkpoint],\n+        verbose=0,\n+    )\n+\n+    # Load best weights.\n+    model.load_weights(file_path)\n+\n+    return model\n+\n+\n+# Building model(s).\n+instance_shape = train_data[0][0].shape\n+models = [create_model(instance_shape) for _ in range(ENSEMBLE_AVG_COUNT)]\n+\n+# Show single model architecture.\n+print(models[0].summary())\n+\n+# Training model(s).\n+trained_models = [\n+    train(train_data, train_labels, val_data, val_labels, model)\n+    for model in tqdm(models)\n+]\n+\n+\"\"\"\n+## Model evaluation\n+\n+The models are now ready for evaluation.\n+With each model we also create an associated intermediate model to get the\n+weights from the attention layer.\n+\n+We will compute a prediction for each of our `ENSEMBLE_AVG_COUNT` models, and\n+average them together for our final prediction.\n+\"\"\"\n+\n+\n+def predict(data, labels, trained_models):\n+    # Collect info per model.\n+    models_predictions = []\n+    models_attention_weights = []\n+    models_losses = []\n+    models_accuracies = []\n+\n+    for model in trained_models:\n+        # Predict output classes on data.\n+        predictions = model.predict(data)\n+        models_predictions.append(predictions)\n+\n+        # Create intermediate model to get MIL attention layer weights.\n+        intermediate_model = keras.Model(model.input, model.get_layer(\"alpha\").output)\n+\n+        # Predict MIL attention layer weights.\n+        intermediate_predictions = intermediate_model.predict(data)\n+\n+        attention_weights = np.squeeze(np.swapaxes(intermediate_predictions, 1, 0))\n+        models_attention_weights.append(attention_weights)\n+\n+        loss, accuracy = model.evaluate(data, labels, verbose=0)\n+        models_losses.append(loss)\n+        models_accuracies.append(accuracy)\n+\n+    print(\n+        f\"The average loss and accuracy are {np.sum(models_losses, axis=0) / ENSEMBLE_AVG_COUNT:.2f}\"\n+        f\" and {100 * np.sum(models_accuracies, axis=0) / ENSEMBLE_AVG_COUNT:.2f} % resp.\"\n+    )\n+\n+    return (\n+        np.sum(models_predictions, axis=0) / ENSEMBLE_AVG_COUNT,\n+        np.sum(models_attention_weights, axis=0) / ENSEMBLE_AVG_COUNT,\n+    )\n+\n+\n+# Evaluate and predict classes and attention scores on validation data.\n+class_predictions, attention_params = predict(val_data, val_labels, trained_models)\n+\n+# Plot some results from our validation data.\n+plot(\n+    val_data,\n+    val_labels,\n+    \"positive\",\n+    predictions=class_predictions,\n+    attention_weights=attention_params,\n+)\n+plot(\n+    val_data,\n+    val_labels,\n+    \"negative\",\n+    predictions=class_predictions,\n+    attention_weights=attention_params,\n+)\n+\n+\"\"\"\n+## Conclusion\n+\n+From the above plot, you can notice that the weights always sum to 1. In a\n+positively predict bag, the instance which resulted in the positive labeling will have\n+a substantially higher attention score than the rest of the bag. However, in a negatively\n+predicted bag, there are two cases:\n+\n+* All instances will have approximately similar scores.\n+* An instance will have relatively higher score (but not as high as of a positive instance).\n+This is because the feature space of this instance is close to that of the positive instance.\n+\n+## Remarks\n+\n+- If the model is overfit, the weights will be equally distributed for all bags. Hence,\n+the regularization techniques are necessary.\n+- In the paper, the bag sizes can differ from one bag to another. For simplicity, the\n+bag sizes are fixed here.\n+- In order not to rely on the random initial weights of a single model, averaging ensemble\n+methods should be considered.\n+\n+Example available on HuggingFace.\n+\n+| Trained Model | Demo |\n+| :--: | :--: |\n+| [![Generic badge](https://img.shields.io/badge/🤗%20Model-Attention%20MIL-black.svg)](https://huggingface.co/keras-io/attention_mil) | [![Generic badge](https://img.shields.io/badge/🤗%20Spaces-Attention%20MIL-black.svg)](https://huggingface.co/spaces/keras-io/Attention_based_Deep_Multiple_Instance_Learning) |\n+\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
