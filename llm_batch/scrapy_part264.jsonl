{"custom_id": "scrapy#5f168cd459cfc026de1e4d0b43c45ea740fe1dd5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 179 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 9 | Methods Changed: 14 | Complexity Δ (Sum/Max): 23/15 | Churn Δ: 183 | Churn Cumulative: 1578 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -123,7 +123,9 @@ class Response(object_ref):\n         elif url is None:\n             raise ValueError(\"url can't be None\")\n         url = self.urljoin(url)\n-        return Request(url, callback,\n+        return Request(\n+            url=url,\n+            callback=callback,\n             method=method,\n             headers=headers,\n             body=body,\n@@ -133,4 +135,39 @@ class Response(object_ref):\n             priority=priority,\n             dont_filter=dont_filter,\n             errback=errback,\n-                       cb_kwargs=cb_kwargs)\n+            cb_kwargs=cb_kwargs,\n+        )\n+\n+    def follow_all(self, urls, callback=None, method='GET', headers=None, body=None,\n+                   cookies=None, meta=None, encoding='utf-8', priority=0,\n+                   dont_filter=False, errback=None, cb_kwargs=None):\n+        # type: (...) -> Generator[Request, None, None]\n+        \"\"\"\n+        Return an iterable of :class:`~.Request` instance to follow all links\n+        in ``urls``. It accepts the same arguments as ``Request.__init__`` method,\n+        but elements of ``urls`` can be relative URLs or ``scrapy.link.Link`` objects\n+        not only absolute URLs.\n+\n+        :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow_all`\n+        method which supports selectors in addition to absolute/relative URLs\n+        and Link objects.\n+        \"\"\"\n+        if not hasattr(urls, '__iter__'):\n+            raise TypeError(\"'urls' argument must be an iterable\")\n+        return (\n+            self.follow(\n+                url=url,\n+                callback=callback,\n+                method=method,\n+                headers=headers,\n+                body=body,\n+                cookies=cookies,\n+                meta=meta,\n+                encoding=encoding,\n+                priority=priority,\n+                dont_filter=dont_filter,\n+                errback=errback,\n+                cb_kwargs=cb_kwargs,\n+            )\n+            for url in urls\n+        )\n\n@@ -13,7 +13,6 @@ from w3lib.encoding import html_to_unicode, resolve_encoding, \\\n     html_body_declared_encoding, http_content_type_encoding\n from w3lib.html import strip_html5_whitespace\n \n-from scrapy.http.request import Request\n from scrapy.http.response import Response\n from scrapy.utils.response import get_base_url\n from scrapy.utils.python import memoizemethod_noargs, to_native_str\n@@ -145,7 +144,9 @@ class TextResponse(Response):\n         elif isinstance(url, parsel.SelectorList):\n             raise ValueError(\"SelectorList is not supported\")\n         encoding = self.encoding if encoding is None else encoding\n-        return super(TextResponse, self).follow(url, callback,\n+        return super(TextResponse, self).follow(\n+            url=url,\n+            callback=callback,\n             method=method,\n             headers=headers,\n             body=body,\n@@ -158,6 +159,52 @@ class TextResponse(Response):\n             cb_kwargs=cb_kwargs,\n         )\n \n+    def follow_all(self, urls=None, callback=None, method='GET', headers=None, body=None,\n+                   cookies=None, meta=None, encoding=None, priority=0,\n+                   dont_filter=False, errback=None, cb_kwargs=None,\n+                   css=None, xpath=None):\n+        # type: (...) -> Generator[Request, None, None]\n+        \"\"\"\n+        A generator that produces :class:`~.Request` instances to follow all\n+        links in ``urls``. It accepts the same arguments as the :class:`~.Request`\n+        initializer, except that each ``urls`` element does not need to be an absolute\n+        URL, it can be any of the following:\n+\n+        * a relative URL;\n+        * a scrapy.link.Link object (e.g. a link extractor result);\n+        * an attribute Selector (not SelectorList) - e.g.\n+          ``response.css('a::attr(href)')[0]`` or\n+          ``response.xpath('//img/@src')[0]``.\n+        * a Selector for ``<a>`` or ``<link>`` element, e.g.\n+          ``response.css('a.my_link')[0]``.\n+\n+        In addition, ``css`` and ``xpath`` arguments are accepted to perform the link extraction\n+        within the ``follow_all`` method (only one of ``urls``, ``css`` and ``xpath`` are accepted).\n+        \"\"\"\n+        if len(list(filter(None, (urls, css, xpath)))) > 1:\n+            raise ValueError('Please supply only one of the following arguments: {urls, css, xpath}')\n+        if css:\n+            urls = self.css(css)\n+        elif xpath:\n+            urls = self.xpath(xpath)\n+        return (\n+            self.follow(\n+                url=url,\n+                callback=callback,\n+                method=method,\n+                headers=headers,\n+                body=body,\n+                cookies=cookies,\n+                meta=meta,\n+                encoding=encoding,\n+                priority=priority,\n+                dont_filter=dont_filter,\n+                errback=errback,\n+                cb_kwargs=cb_kwargs,\n+            )\n+            for url in urls\n+        )\n+\n \n def _url_from_selector(sel):\n     # type: (parsel.Selector) -> str\n\n@@ -143,6 +143,8 @@ class BaseResponseTest(unittest.TestCase):\n             r.css('body')\n             r.xpath('//body')\n \n+    # Response.follow\n+\n     def test_follow_url_absolute(self):\n         self._assert_followed_url('http://foo.example.com',\n                                   'http://foo.example.com')\n@@ -166,6 +168,64 @@ class BaseResponseTest(unittest.TestCase):\n     def test_follow_whitespace_link(self):\n         self._assert_followed_url(Link('http://example.com/foo '),\n                                   'http://example.com/foo%20')\n+\n+    # Response.follow_all\n+\n+    def test_follow_all_absolute(self):\n+        url_list = ['http://example.org', 'http://www.example.org',\n+                    'http://example.com', 'http://www.example.com']\n+        self._assert_followed_all_urls(url_list, url_list)\n+\n+    def test_follow_all_relative(self):\n+        relative = ['foo', 'bar', 'foo/bar', 'bar/foo']\n+        absolute = [\n+            'http://example.com/foo',\n+            'http://example.com/bar',\n+            'http://example.com/foo/bar',\n+            'http://example.com/bar/foo',\n+        ]\n+        self._assert_followed_all_urls(relative, absolute)\n+\n+    def test_follow_all_links(self):\n+        absolute = [\n+            'http://example.com/foo',\n+            'http://example.com/bar',\n+            'http://example.com/foo/bar',\n+            'http://example.com/bar/foo',\n+        ]\n+        links = map(Link, absolute)\n+        self._assert_followed_all_urls(links, absolute)\n+\n+    def test_follow_all_invalid(self):\n+        r = self.response_class(\"http://example.com\")\n+        with self.assertRaises(TypeError):\n+            list(r.follow_all(None))\n+        with self.assertRaises(TypeError):\n+            list(r.follow_all(12345))\n+        with self.assertRaises(ValueError):\n+            list(r.follow_all([None]))\n+\n+    def test_follow_all_whitespace(self):\n+        relative = ['foo ', 'bar ', 'foo/bar ', 'bar/foo ']\n+        absolute = [\n+            'http://example.com/foo%20',\n+            'http://example.com/bar%20',\n+            'http://example.com/foo/bar%20',\n+            'http://example.com/bar/foo%20',\n+        ]\n+        self._assert_followed_all_urls(relative, absolute)\n+\n+    def test_follow_all_whitespace_links(self):\n+        absolute = [\n+            'http://example.com/foo ',\n+            'http://example.com/bar ',\n+            'http://example.com/foo/bar ',\n+            'http://example.com/bar/foo ',\n+        ]\n+        links = map(Link, absolute)\n+        expected = [u.replace(' ', '%20') for u in absolute]\n+        self._assert_followed_all_urls(links, expected)\n+\n     def _assert_followed_url(self, follow_obj, target_url, response=None):\n         if response is None:\n             response = self._links_response()\n@@ -173,6 +233,14 @@ class BaseResponseTest(unittest.TestCase):\n         self.assertEqual(req.url, target_url)\n         return req\n \n+    def _assert_followed_all_urls(self, follow_obj, target_urls, response=None):\n+        if response is None:\n+            response = self._links_response()\n+        followed = response.follow_all(follow_obj)\n+        for req, target in zip(followed, target_urls):\n+            self.assertEqual(req.url, target)\n+            yield req\n+\n     def _links_response(self):\n         body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n         resp = self.response_class('http://example.com/index', body=body)\n@@ -483,6 +551,29 @@ class TextResponseTest(BaseResponseTest):\n         )\n         self.assertEqual(req.encoding, 'cp1251')\n \n+    def test_follow_all_css(self):\n+        expected = [\n+            'http://example.com/sample3.html',\n+            'http://example.com/innertag.html',\n+        ]\n+        response = self._links_response()\n+        extracted = [r.url for r in response.follow_all(css='a[href*=\"example.com\"]')]\n+        self.assertEqual(expected, extracted)\n+\n+    def test_follow_all_xpath(self):\n+        expected = [\n+            'http://example.com/sample3.html',\n+            'http://example.com/innertag.html',\n+        ]\n+        response = self._links_response()\n+        extracted = response.follow_all(xpath='//a[contains(@href, \"example.com\")]')\n+        self.assertEqual(expected, [r.url for r in extracted])\n+\n+    def test_follow_all_exception(self):\n+        response = self._links_response()\n+        with self.assertRaises(ValueError):\n+            response.follow_all(css='a[href*=\"example.com\"]', xpath='//a[contains(@href, \"example.com\")]')\n+\n \n class HtmlResponseTest(TextResponseTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#877ef4269e5683bf87832e816cf20a3cac352440", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 85 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ IGNORED_EXTENSIONS = [\n \n     # video\n     '3gp', 'asf', 'asx', 'avi', 'mov', 'mp4', 'mpg', 'qt', 'rm', 'swf', 'wmv',\n-    'm4a', 'm4v', 'flv',\n+    'm4a', 'm4v', 'flv', 'webm',\n \n     # office suites\n     'xls', 'xlsx', 'ppt', 'pptx', 'pps', 'doc', 'docx', 'odt', 'ods', 'odg',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a25a2d5ee4755046d736efa0a03898d9d18671f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 87 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -35,7 +35,7 @@ IGNORED_EXTENSIONS = [\n     'odp',\n \n     # other\n-    'css', 'pdf', 'exe', 'bin', 'rss', 'zip', 'rar', 'dmg', 'iso', 'apk'\n+    'css', 'pdf', 'exe', 'bin', 'rss', 'zip', 'rar', 'dmg', 'iso', 'apk', 'tar.xz'\n ]\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#532770df5226757498a941746cf94ae47043e101", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 89 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -35,7 +35,7 @@ IGNORED_EXTENSIONS = [\n     'odp',\n \n     # other\n-    'css', 'pdf', 'exe', 'bin', 'rss', 'zip', 'rar', 'dmg', 'iso', 'apk', 'tar.xz'\n+    'css', 'pdf', 'exe', 'bin', 'rss', 'zip', 'rar', 'dmg', 'iso', 'apk', 'xz'\n ]\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e1fa1fd8ad62cb8958d21d3a33648ed4de6af757", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 11 | Methods Changed: 7 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 63 | Churn Cumulative: 1303 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 16 | DMM Complexity: 0.075\n\nDIFF:\n@@ -180,13 +180,27 @@ class TextResponse(Response):\n \n         In addition, ``css`` and ``xpath`` arguments are accepted to perform the link extraction\n         within the ``follow_all`` method (only one of ``urls``, ``css`` and ``xpath`` are accepted).\n+\n+        Note that when using the ``css`` or ``xpath`` parameters, this method will not produce\n+        requests for selectors from which links cannot be obtained (for instance, anchor tags\n+        without ``href`` attribute)\n         \"\"\"\n-        if len(list(filter(None, (urls, css, xpath)))) > 1:\n-            raise ValueError('Please supply only one of the following arguments: {urls, css, xpath}')\n+        arg_count = len(list(filter(None, (urls, css, xpath))))\n+        if arg_count != 1:\n+            raise ValueError('Please supply exactly one of the following arguments: {urls, css, xpath}')\n+        if not urls:\n+            urls = []\n             if css:\n-            urls = self.css(css)\n+                selector_method = getattr(self, 'css')\n+                expression = css\n             elif xpath:\n-            urls = self.xpath(xpath)\n+                selector_method = getattr(self, 'xpath')\n+                expression = xpath\n+            for selector in selector_method(expression):\n+                try:\n+                    urls.append(_url_from_selector(selector))\n+                except ValueError:\n+                    pass\n         return (\n             self.follow(\n                 url=url,\n\n@@ -198,12 +198,20 @@ class BaseResponseTest(unittest.TestCase):\n \n     def test_follow_all_invalid(self):\n         r = self.response_class(\"http://example.com\")\n+        if self.response_class == Response:\n             with self.assertRaises(TypeError):\n-            list(r.follow_all(None))\n+                list(r.follow_all(urls=None))\n             with self.assertRaises(TypeError):\n-            list(r.follow_all(12345))\n+                list(r.follow_all(urls=12345))\n             with self.assertRaises(ValueError):\n-            list(r.follow_all([None]))\n+                list(r.follow_all(urls=[None]))\n+        else:\n+            with self.assertRaises(ValueError):\n+                list(r.follow_all(urls=None))\n+            with self.assertRaises(TypeError):\n+                list(r.follow_all(urls=12345))\n+            with self.assertRaises(ValueError):\n+                list(r.follow_all(urls=[None]))\n \n     def test_follow_all_whitespace(self):\n         relative = ['foo ', 'bar ', 'foo/bar ', 'bar/foo ']\n@@ -246,6 +254,11 @@ class BaseResponseTest(unittest.TestCase):\n         resp = self.response_class('http://example.com/index', body=body)\n         return resp\n \n+    def _links_response_no_href(self):\n+        body = get_testdata('link_extractor', 'sgml_linkextractor_no_href.html')\n+        resp = self.response_class('http://example.com/index', body=body)\n+        return resp\n+\n \n class TextResponseTest(BaseResponseTest):\n \n@@ -560,6 +573,16 @@ class TextResponseTest(BaseResponseTest):\n         extracted = [r.url for r in response.follow_all(css='a[href*=\"example.com\"]')]\n         self.assertEqual(expected, extracted)\n \n+    def test_follow_all_css_skip_invalid(self):\n+        expected = [\n+            'http://example.com/page/1/',\n+            'http://example.com/page/3/',\n+            'http://example.com/page/4/',\n+        ]\n+        response = self._links_response_no_href()\n+        extracted = [r.url for r in response.follow_all(css='.pagination a')]\n+        self.assertEqual(expected, extracted)\n+\n     def test_follow_all_xpath(self):\n         expected = [\n             'http://example.com/sample3.html',\n@@ -569,7 +592,17 @@ class TextResponseTest(BaseResponseTest):\n         extracted = response.follow_all(xpath='//a[contains(@href, \"example.com\")]')\n         self.assertEqual(expected, [r.url for r in extracted])\n \n-    def test_follow_all_exception(self):\n+    def test_follow_all_xpath_skip_invalid(self):\n+        expected = [\n+            'http://example.com/page/1/',\n+            'http://example.com/page/3/',\n+            'http://example.com/page/4/',\n+        ]\n+        response = self._links_response_no_href()\n+        extracted = [r.url for r in response.follow_all(xpath='//div[@id=\"pagination\"]/a')]\n+        self.assertEqual(expected, extracted)\n+\n+    def test_follow_all_too_many_arguments(self):\n         response = self._links_response()\n         with self.assertRaises(ValueError):\n             response.follow_all(css='a[href*=\"example.com\"]', xpath='//a[contains(@href, \"example.com\")]')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#12f1e468e9f071e8f5c00921d0fbada460e41c57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 281 | Contributors (this commit): 18 | Commits (past 90d): 4 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -199,9 +199,9 @@ class FeedExporter(object):\n \n     def __init__(self, settings):\n         self.settings = settings\n-        self.urifmt = settings['FEED_URI']\n-        if not self.urifmt:\n+        if not settings['FEED_URI']:\n             raise NotConfigured\n+        self.urifmt=str(settings['FEED_URI'])\n         self.format = settings['FEED_FORMAT'].lower()\n         self.export_encoding = settings['FEED_EXPORT_ENCODING']\n         self.storages = self._load_components('FEED_STORAGES')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b970851299bf561af7ff0bb21caca9b6c3f296af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 340 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -143,7 +143,7 @@ class Response(object_ref):\n                    dont_filter=False, errback=None, cb_kwargs=None):\n         # type: (...) -> Generator[Request, None, None]\n         \"\"\"\n-        Return an iterable of :class:`~.Request` instance to follow all links\n+        Return an iterable of :class:`~.Request` instances to follow all links\n         in ``urls``. It accepts the same arguments as ``Request.__init__`` method,\n         but elements of ``urls`` can be relative URLs or ``scrapy.link.Link`` objects\n         not only absolute URLs.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ba840c5a6bea95333b3b78fb767ac6f092d02177", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 342 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -145,7 +145,7 @@ class Response(object_ref):\n         \"\"\"\n         Return an iterable of :class:`~.Request` instances to follow all links\n         in ``urls``. It accepts the same arguments as ``Request.__init__`` method,\n-        but elements of ``urls`` can be relative URLs or ``scrapy.link.Link`` objects\n+        but elements of ``urls`` can be relative URLs or :class:`~scrapy.link.Link` objects,\n         not only absolute URLs.\n \n         :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow_all`\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#498d33aac37d5d7a0955e9b409fc3d5ff8627dc0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 561 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -183,7 +183,7 @@ class TextResponse(Response):\n \n         Note that when using the ``css`` or ``xpath`` parameters, this method will not produce\n         requests for selectors from which links cannot be obtained (for instance, anchor tags\n-        without ``href`` attribute)\n+        without an ``href`` attribute)\n         \"\"\"\n         arg_count = len(list(filter(None, (urls, css, xpath))))\n         if arg_count != 1:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c7c54f5453eaa16f2b6d6441be68b0a272606ec9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 563 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -179,7 +179,7 @@ class TextResponse(Response):\n           ``response.css('a.my_link')[0]``.\n \n         In addition, ``css`` and ``xpath`` arguments are accepted to perform the link extraction\n-        within the ``follow_all`` method (only one of ``urls``, ``css`` and ``xpath`` are accepted).\n+        within the ``follow_all`` method (only one of ``urls``, ``css`` and ``xpath`` is accepted).\n \n         Note that when using the ``css`` or ``xpath`` parameters, this method will not produce\n         requests for selectors from which links cannot be obtained (for instance, anchor tags\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d5398e7f2834940cbf9c2efa312bf5d96ffba98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 591 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -129,13 +129,14 @@ class TextResponse(Response):\n         It accepts the same arguments as ``Request.__init__`` method,\n         but ``url`` can be not only an absolute URL, but also\n \n-        * a relative URL;\n-        * a scrapy.link.Link object (e.g. a link extractor result);\n-        * an attribute Selector (not SelectorList) - e.g.\n+        * a relative URL\n+        * a :class:`~scrapy.link.Link` object, e.g. the result of\n+          :ref:`topics-link-extractors`\n+        * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.\n+          ``response.css('a.my_link')[0]``\n+        * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.\n           ``response.css('a::attr(href)')[0]`` or\n-          ``response.xpath('//img/@src')[0]``.\n-        * a Selector for ``<a>`` or ``<link>`` element, e.g.\n-          ``response.css('a.my_link')[0]``.\n+          ``response.xpath('//img/@src')[0]``\n \n         See :ref:`response-follow-example` for usage examples.\n         \"\"\"\n@@ -170,13 +171,14 @@ class TextResponse(Response):\n         initializer, except that each ``urls`` element does not need to be an absolute\n         URL, it can be any of the following:\n \n-        * a relative URL;\n-        * a scrapy.link.Link object (e.g. a link extractor result);\n-        * an attribute Selector (not SelectorList) - e.g.\n+        * a relative URL\n+        * a :class:`~scrapy.link.Link` object, e.g. the result of\n+          :ref:`topics-link-extractors`\n+        * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.\n+          ``response.css('a.my_link')[0]``\n+        * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.\n           ``response.css('a::attr(href)')[0]`` or\n-          ``response.xpath('//img/@src')[0]``.\n-        * a Selector for ``<a>`` or ``<link>`` element, e.g.\n-          ``response.css('a.my_link')[0]``.\n+          ``response.xpath('//img/@src')[0]``\n \n         In addition, ``css`` and ``xpath`` arguments are accepted to perform the link extraction\n         within the ``follow_all`` method (only one of ``urls``, ``css`` and ``xpath`` is accepted).\n@@ -187,7 +189,7 @@ class TextResponse(Response):\n         \"\"\"\n         arg_count = len(list(filter(None, (urls, css, xpath))))\n         if arg_count != 1:\n-            raise ValueError('Please supply exactly one of the following arguments: {urls, css, xpath}')\n+            raise ValueError('Please supply exactly one of the following arguments: urls, css, xpath')\n         if not urls:\n             urls = []\n             if css:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2a4d4a466aa6cede4829b62c7287332a627877ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 603 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -191,14 +191,12 @@ class TextResponse(Response):\n         if arg_count != 1:\n             raise ValueError('Please supply exactly one of the following arguments: urls, css, xpath')\n         if not urls:\n-            urls = []\n             if css:\n-                selector_method = getattr(self, 'css')\n-                expression = css\n-            elif xpath:\n-                selector_method = getattr(self, 'xpath')\n-                expression = xpath\n-            for selector in selector_method(expression):\n+                selector_list = self.css(css)\n+            if xpath:\n+                selector_list = self.xpath(xpath)\n+            urls = []\n+            for selector in selector_list:\n                 try:\n                     urls.append(_url_from_selector(selector))\n                 except ValueError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2c6f7fee6456168f4870293c442258a2470b1b48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 610 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -201,9 +201,8 @@ class TextResponse(Response):\n                     urls.append(_url_from_selector(selector))\n                 except ValueError:\n                     pass\n-        return (\n-            self.follow(\n-                url=url,\n+        return super(TextResponse, self).follow_all(\n+            urls=urls,\n             callback=callback,\n             method=method,\n             headers=headers,\n@@ -216,8 +215,6 @@ class TextResponse(Response):\n             errback=errback,\n             cb_kwargs=cb_kwargs,\n         )\n-            for url in urls\n-        )\n \n \n def _url_from_selector(sel):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6df6b6dd6a5afd9172c03f6b9e8ce8e180867339", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 616 | Contributors (this commit): 8 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -167,9 +167,9 @@ class TextResponse(Response):\n         # type: (...) -> Generator[Request, None, None]\n         \"\"\"\n         A generator that produces :class:`~.Request` instances to follow all\n-        links in ``urls``. It accepts the same arguments as the :class:`~.Request`\n-        initializer, except that each ``urls`` element does not need to be an absolute\n-        URL, it can be any of the following:\n+        links in ``urls``. It accepts the same arguments as the :class:`~.Request`'s\n+        ``__init__`` method, except that each ``urls`` element does not need to be\n+        an absolute URL, it can be any of the following:\n \n         * a relative URL\n         * a :class:`~scrapy.link.Link` object, e.g. the result of\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0fbd1ff4a91c68e552a3f919824c60437fc9a141", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 104 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -120,8 +120,8 @@ class LxmlLinkExtractor(FilteringLinkExtractor):\n         \"\"\"Returns a list of :class:`~scrapy.link.Link` objects from the\n         specified :class:`response <scrapy.http.Response>`.\n \n-        Only links that match the settings passed to the link extractor\n-        constructor are returned.\n+        Only links that match the settings passed to the ``__init__`` method of\n+        the link extractor are returned.\n \n         Duplicate links are omitted.\n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#68a7d05ed86331ae70778c83a131e6dcc8b4ffc8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 23 | Files Changed: 11 | Hunks: 19 | Methods Changed: 21 | Complexity Δ (Sum/Max): -21/0 | Churn Δ: 46 | Churn Cumulative: 5148 | Contributors (this commit): 70 | Commits (past 90d): 25 | Contributors (cumulative): 150 | DMM Complexity: None\n\nDIFF:\n@@ -237,7 +237,7 @@ coverage_ignore_pyobjects = [\n     r'\\bContractsManager\\b$',\n \n     # For default contracts we only want to document their general purpose in\n-    # their constructor, the methods they reimplement to achieve that purpose\n+    # their __init__ method, the methods they reimplement to achieve that purpose\n     # should be irrelevant to developers using those contracts.\n     r'\\w+Contract\\.(adjust_request_args|(pre|post)_process)$',\n \n\n@@ -31,7 +31,7 @@ class BaseItemExporter(object):\n     def _configure(self, options, dont_fail=False):\n         \"\"\"Configure the exporter by poping options from the ``options`` dict.\n         If dont_fail is set, it won't raise an exception on unexpected options\n-        (useful for using with keyword arguments in subclasses constructors)\n+        (useful for using with keyword arguments in subclasses __init__ methods)\n         \"\"\"\n         self.encoding = options.pop('encoding', None)\n         self.fields_to_export = options.pop('fields_to_export', None)\n\n@@ -106,7 +106,7 @@ class S3FeedStorage(BlockingFeedStorage):\n                 warnings.warn(\n                     \"Initialising `scrapy.extensions.feedexport.S3FeedStorage` \"\n                     \"without AWS keys is deprecated. Please supply credentials or \"\n-                    \"use the `from_crawler()` constructor.\",\n+                    \"use the `from_crawler()` __init__ method.\",\n                     category=ScrapyDeprecationWarning,\n                     stacklevel=2\n                 )\n\n@@ -246,7 +246,7 @@ class CaselessDict(dict):\n class MergeDict(object):\n     \"\"\"\n     A simple class for creating new \"virtual\" dictionaries that actually look\n-    up values in more than one dictionary, passed in the constructor.\n+    up values in more than one dictionary, passed in the __init__ method.\n \n     If a key appears in more than one of the given dictionaries, only the\n     first occurrence will be used.\n\n@@ -123,14 +123,14 @@ def rel_has_nofollow(rel):\n \n def create_instance(objcls, settings, crawler, *args, **kwargs):\n     \"\"\"Construct a class instance using its ``from_crawler`` or\n-    ``from_settings`` constructors, if available.\n+    ``from_settings`` __init__ method, if available.\n \n     At least one of ``settings`` and ``crawler`` needs to be different from\n     ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.\n-    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be\n+    If ``crawler`` is ``None``, only the ``from_settings`` __init__ method will be\n     tried.\n \n-    ``*args`` and ``**kwargs`` are forwarded to the constructors.\n+    ``*args`` and ``**kwargs`` are forwarded to the __init__ methods.\n \n     Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.\n     \"\"\"\n\n@@ -303,7 +303,7 @@ class WeakKeyCache(object):\n def stringify_dict(dct_or_tuples, encoding='utf-8', keys_only=True):\n     \"\"\"Return a (new) dict with unicode keys (and values when \"keys_only\" is\n     False) of the given dict converted to strings. ``dct_or_tuples`` can be a\n-    dict or a list of tuples, like any dict constructor supports.\n+    dict or a list of tuples, like any dict __init__ method supports.\n     \"\"\"\n     d = {}\n     for k, v in six.iteritems(dict(dct_or_tuples)):\n\n@@ -25,7 +25,7 @@ class RequestTest(unittest.TestCase):\n     default_meta = {}\n \n     def test_init(self):\n-        # Request requires url in the constructor\n+        # Request requires url in the __init__ method\n         self.assertRaises(Exception, self.request_class)\n \n         # url argument must be basestring\n\n@@ -534,7 +534,7 @@ class XmlResponseTest(TextResponseTest):\n         r2 = self.response_class(\"http://www.example.com\", body=body)\n         self._assert_response_values(r2, 'iso-8859-1', body)\n \n-        # make sure replace() preserves the explicit encoding passed in the constructor\n+        # make sure replace() preserves the explicit encoding passed in the __init__ method\n         body = b\"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n         r3 = self.response_class(\"http://www.example.com\", body=body, encoding='utf-8')\n         body2 = b\"New body\"\n\n@@ -548,11 +548,11 @@ class SelectortemLoaderTest(unittest.TestCase):\n     </html>\n     \"\"\")\n \n-    def test_constructor(self):\n+    def test_init_method(self):\n         l = TestItemLoader()\n         self.assertEqual(l.selector, None)\n \n-    def test_constructor_errors(self):\n+    def test_init_method_errors(self):\n         l = TestItemLoader()\n         self.assertRaises(RuntimeError, l.add_xpath, 'url', '//a/@href')\n         self.assertRaises(RuntimeError, l.replace_xpath, 'url', '//a/@href')\n@@ -561,7 +561,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n         self.assertRaises(RuntimeError, l.replace_css, 'name', '#name::text')\n         self.assertRaises(RuntimeError, l.get_css, '#name::text')\n \n-    def test_constructor_with_selector(self):\n+    def test_init_method_with_selector(self):\n         sel = Selector(text=u\"<html><body><div>marta</div></body></html>\")\n         l = TestItemLoader(selector=sel)\n         self.assertIs(l.selector, sel)\n@@ -569,7 +569,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n         l.add_xpath('name', '//div/text()')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n \n-    def test_constructor_with_selector_css(self):\n+    def test_init_method_with_selector_css(self):\n         sel = Selector(text=u\"<html><body><div>marta</div></body></html>\")\n         l = TestItemLoader(selector=sel)\n         self.assertIs(l.selector, sel)\n@@ -577,14 +577,14 @@ class SelectortemLoaderTest(unittest.TestCase):\n         l.add_css('name', 'div::text')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n \n-    def test_constructor_with_response(self):\n+    def test_init_method_with_response(self):\n         l = TestItemLoader(response=self.response)\n         self.assertTrue(l.selector)\n \n         l.add_xpath('name', '//div/text()')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n \n-    def test_constructor_with_response_css(self):\n+    def test_init_method_with_response_css(self):\n         l = TestItemLoader(response=self.response)\n         self.assertTrue(l.selector)\n \n\n@@ -42,12 +42,12 @@ class SpiderTest(unittest.TestCase):\n         self.assertEqual(list(start_requests), [])\n \n     def test_spider_args(self):\n-        \"\"\"Constructor arguments are assigned to spider attributes\"\"\"\n+        \"\"\"__init__ method arguments are assigned to spider attributes\"\"\"\n         spider = self.spider_class('example.com', foo='bar')\n         self.assertEqual(spider.foo, 'bar')\n \n     def test_spider_without_name(self):\n-        \"\"\"Constructor arguments are assigned to spider attributes\"\"\"\n+        \"\"\"__init__ method arguments are assigned to spider attributes\"\"\"\n         self.assertRaises(ValueError, self.spider_class)\n         self.assertRaises(ValueError, self.spider_class, somearg='foo')\n \n\n@@ -109,11 +109,11 @@ class UtilsMiscTestCase(unittest.TestCase):\n             else:\n                 mock.assert_called_once_with(*args, **kwargs)\n \n-        # Check usage of correct constructor using four mocks:\n-        #   1. with no alternative constructors\n-        #   2. with from_settings() constructor\n-        #   3. with from_crawler() constructor\n-        #   4. with from_settings() and from_crawler() constructor\n+        # Check usage of correct __init__ method using four mocks:\n+        #   1. with no alternative __init__ methods\n+        #   2. with from_settings() __init__ method\n+        #   3. with from_crawler() __init__ method\n+        #   4. with from_settings() and from_crawler() __init__ method\n         spec_sets = ([], ['from_settings'], ['from_crawler'],\n                      ['from_settings', 'from_crawler'])\n         for specs in spec_sets:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ad6075440c0285d903ec7238354d093ea300b8f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 24 | Files Changed: 4 | Hunks: 24 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 41 | Churn Cumulative: 2757 | Contributors (this commit): 22 | Commits (past 90d): 6 | Contributors (cumulative): 36 | DMM Complexity: None\n\nDIFF:\n@@ -292,7 +292,7 @@ class CrawlerProcess(CrawlerRunner):\n         If ``stop_after_crawl`` is True, the reactor will be stopped after all\n         crawlers have finished, using :meth:`join`.\n \n-        :param boolean stop_after_crawl: stop or not the reactor when all\n+        :param bool stop_after_crawl: stop or not the reactor when all\n             crawlers have finished\n         \"\"\"\n         if stop_after_crawl:\n\n@@ -43,10 +43,10 @@ class RobotParser(with_metaclass(ABCMeta)):\n         \"\"\"Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``.\n \n         :param url: Absolute URL\n-        :type url: string\n+        :type url: str\n \n         :param user_agent: User agent\n-        :type user_agent: string\n+        :type user_agent: str\n         \"\"\"\n         pass\n \n\n@@ -105,10 +105,9 @@ class BaseSettings(MutableMapping):\n         Get a setting value without affecting its original type.\n \n         :param name: the setting name\n-        :type name: string\n+        :type name: str\n \n         :param default: the value to return if no setting is found\n-        :type default: any\n         \"\"\"\n         return self[name] if self[name] is not None else default\n \n@@ -123,10 +122,9 @@ class BaseSettings(MutableMapping):\n         ``'0'`` will return ``False`` when using this method.\n \n         :param name: the setting name\n-        :type name: string\n+        :type name: str\n \n         :param default: the value to return if no setting is found\n-        :type default: any\n         \"\"\"\n         got = self.get(name, default)\n         try:\n@@ -145,10 +143,9 @@ class BaseSettings(MutableMapping):\n         Get a setting value as an int.\n \n         :param name: the setting name\n-        :type name: string\n+        :type name: str\n \n         :param default: the value to return if no setting is found\n-        :type default: any\n         \"\"\"\n         return int(self.get(name, default))\n \n@@ -157,10 +154,9 @@ class BaseSettings(MutableMapping):\n         Get a setting value as a float.\n \n         :param name: the setting name\n-        :type name: string\n+        :type name: str\n \n         :param default: the value to return if no setting is found\n-        :type default: any\n         \"\"\"\n         return float(self.get(name, default))\n \n@@ -173,10 +169,9 @@ class BaseSettings(MutableMapping):\n         ``'one,two'`` will return a list ['one', 'two'] when using this method.\n \n         :param name: the setting name\n-        :type name: string\n+        :type name: str\n \n         :param default: the value to return if no setting is found\n-        :type default: any\n         \"\"\"\n         value = self.get(name, default or [])\n         if isinstance(value, six.string_types):\n@@ -194,10 +189,9 @@ class BaseSettings(MutableMapping):\n         and losing all information about priority and mutability.\n \n         :param name: the setting name\n-        :type name: string\n+        :type name: str\n \n         :param default: the value to return if no setting is found\n-        :type default: any\n         \"\"\"\n         value = self.get(name, default or {})\n         if isinstance(value, six.string_types):\n@@ -209,7 +203,7 @@ class BaseSettings(MutableMapping):\n         counterpart.\n \n         :param name: name of the dictionary-like setting\n-        :type name: string\n+        :type name: str\n         \"\"\"\n         compbs = BaseSettings()\n         compbs.update(self[name + '_BASE'])\n@@ -222,7 +216,7 @@ class BaseSettings(MutableMapping):\n         the given ``name`` does not exist.\n \n         :param name: the setting name\n-        :type name: string\n+        :type name: str\n         \"\"\"\n         if name not in self:\n             return None\n@@ -252,14 +246,13 @@ class BaseSettings(MutableMapping):\n         otherwise they won't have any effect.\n \n         :param name: the setting name\n-        :type name: string\n+        :type name: str\n \n         :param value: the value to associate with the setting\n-        :type value: any\n \n         :param priority: the priority of the setting. Should be a key of\n             :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n-        :type priority: string or int\n+        :type priority: str or int\n         \"\"\"\n         self._assert_mutability()\n         priority = get_settings_priority(priority)\n@@ -283,11 +276,11 @@ class BaseSettings(MutableMapping):\n         uppercase variable of ``module`` with the provided ``priority``.\n \n         :param module: the module or the path of the module\n-        :type module: module object or string\n+        :type module: types.ModuleType or str\n \n         :param priority: the priority of the settings. Should be a key of\n             :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n-        :type priority: string or int\n+        :type priority: str or int\n         \"\"\"\n         self._assert_mutability()\n         if isinstance(module, six.string_types):\n@@ -316,7 +309,7 @@ class BaseSettings(MutableMapping):\n \n         :param priority: the priority of the settings. Should be a key of\n             :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n-        :type priority: string or int\n+        :type priority: str or int\n         \"\"\"\n         self._assert_mutability()\n         if isinstance(values, six.string_types):\n\n@@ -17,7 +17,7 @@ class SignalManager(object):\n         section.\n \n         :param receiver: the function to be connected\n-        :type receiver: callable\n+        :type receiver: collections.abc.Callable\n \n         :param signal: the signal to connect to\n         :type signal: object\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4e939ca75d14daf5bc658fdbe5a97af6f4c3c498", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 495 | Contributors (this commit): 21 | Commits (past 90d): 4 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -56,6 +56,7 @@ setup(\n         'Programming Language :: Python :: 3.5',\n         'Programming Language :: Python :: 3.6',\n         'Programming Language :: Python :: 3.7',\n+        'Programming Language :: Python :: 3.8',\n         'Programming Language :: Python :: Implementation :: CPython',\n         'Programming Language :: Python :: Implementation :: PyPy',\n         'Topic :: Internet :: WWW/HTTP',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#da8cd9448de28bdeac7343bcd367e95237423789", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 6 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 2585 | Contributors (this commit): 49 | Commits (past 90d): 18 | Contributors (cumulative): 92 | DMM Complexity: None\n\nDIFF:\n@@ -31,7 +31,7 @@ class BaseItemExporter(object):\n     def _configure(self, options, dont_fail=False):\n         \"\"\"Configure the exporter by poping options from the ``options`` dict.\n         If dont_fail is set, it won't raise an exception on unexpected options\n-        (useful for using with keyword arguments in subclasses __init__ methods)\n+        (useful for using with keyword arguments in subclasses ``__init__`` methods)\n         \"\"\"\n         self.encoding = options.pop('encoding', None)\n         self.fields_to_export = options.pop('fields_to_export', None)\n\n@@ -106,7 +106,7 @@ class S3FeedStorage(BlockingFeedStorage):\n                 warnings.warn(\n                     \"Initialising `scrapy.extensions.feedexport.S3FeedStorage` \"\n                     \"without AWS keys is deprecated. Please supply credentials or \"\n-                    \"use the `from_crawler()` __init__ method.\",\n+                    \"use the `from_crawler()` ``__init__`` method.\",\n                     category=ScrapyDeprecationWarning,\n                     stacklevel=2\n                 )\n\n@@ -246,7 +246,7 @@ class CaselessDict(dict):\n class MergeDict(object):\n     \"\"\"\n     A simple class for creating new \"virtual\" dictionaries that actually look\n-    up values in more than one dictionary, passed in the __init__ method.\n+    up values in more than one dictionary, passed in the ``__init__`` method.\n \n     If a key appears in more than one of the given dictionaries, only the\n     first occurrence will be used.\n\n@@ -123,14 +123,14 @@ def rel_has_nofollow(rel):\n \n def create_instance(objcls, settings, crawler, *args, **kwargs):\n     \"\"\"Construct a class instance using its ``from_crawler`` or\n-    ``from_settings`` __init__ method, if available.\n+    ``from_settings`` ``__init__`` method, if available.\n \n     At least one of ``settings`` and ``crawler`` needs to be different from\n     ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.\n-    If ``crawler`` is ``None``, only the ``from_settings`` __init__ method will be\n+    If ``crawler`` is ``None``, only the ``from_settings`` ``__init__`` method will be\n     tried.\n \n-    ``*args`` and ``**kwargs`` are forwarded to the __init__ methods.\n+    ``*args`` and ``**kwargs`` are forwarded to the ``__init__`` methods.\n \n     Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.\n     \"\"\"\n\n@@ -303,7 +303,7 @@ class WeakKeyCache(object):\n def stringify_dict(dct_or_tuples, encoding='utf-8', keys_only=True):\n     \"\"\"Return a (new) dict with unicode keys (and values when \"keys_only\" is\n     False) of the given dict converted to strings. ``dct_or_tuples`` can be a\n-    dict or a list of tuples, like any dict __init__ method supports.\n+    dict or a list of tuples, like any dict ``__init__`` method supports.\n     \"\"\"\n     d = {}\n     for k, v in six.iteritems(dict(dct_or_tuples)):\n\n@@ -42,12 +42,12 @@ class SpiderTest(unittest.TestCase):\n         self.assertEqual(list(start_requests), [])\n \n     def test_spider_args(self):\n-        \"\"\"__init__ method arguments are assigned to spider attributes\"\"\"\n+        \"\"\"``__init__`` method arguments are assigned to spider attributes\"\"\"\n         spider = self.spider_class('example.com', foo='bar')\n         self.assertEqual(spider.foo, 'bar')\n \n     def test_spider_without_name(self):\n-        \"\"\"__init__ method arguments are assigned to spider attributes\"\"\"\n+        \"\"\"``__init__`` method arguments are assigned to spider attributes\"\"\"\n         self.assertRaises(ValueError, self.spider_class)\n         self.assertRaises(ValueError, self.spider_class, somearg='foo')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
