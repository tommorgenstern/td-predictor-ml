{"custom_id": "keras#08014eea360fd8d66b7baab19cdb9335f52c167b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 222 | Lines Deleted: 118 | Files Changed: 5 | Hunks: 41 | Methods Changed: 8 | Complexity Δ (Sum/Max): 18/20 | Churn Δ: 340 | Churn Cumulative: 9081 | Contributors (this commit): 50 | Commits (past 90d): 75 | Contributors (cumulative): 78 | DMM Complexity: 0.6153846153846154\n\nDIFF:\n@@ -1,8 +1,6 @@\n '''Trains a LSTM on the IMDB sentiment classification task.\n-\n The dataset is actually too small for LSTM to be of any advantage\n compared to simpler, much faster methods such as TF-IDF + LogReg.\n-\n Notes:\n \n - RNNs are tricky. Choice of batch size is important,\n\n@@ -764,13 +764,13 @@ def repeat(x, n):\n     the output will have shape (samples, 2, dim)\n     '''\n     assert ndim(x) == 2\n-    tensors = [x] * n\n-    stacked = tf.pack(tensors)\n-    return tf.transpose(stacked, (1, 0, 2))\n+    x = tf.expand_dims(x, 1)\n+    pattern = tf.pack([1, n, 1])\n+    return tf.tile(x, pattern)\n \n \n def tile(x, n):\n-    if not hasattr(n, 'shape') and not hasattr(n, '__len__'):\n+    if not hasattr(n, 'shape') and not hasattr(n, '__len__') and not hasattr(n, '_shape'):\n         n = [n]\n     return tf.tile(x, n)\n \n@@ -1020,10 +1020,11 @@ def rnn(step_function, inputs, initial_states,\n                     time step.\n                 states: list of tensors.\n             Returns:\n-                output: tensor with shape (samples, ...) (no time dimension),\n+                output: tensor with shape (samples, output_dim) (no time dimension),\n                 new_states: list of tensors, same length and shapes\n-                    as 'states'.\n-        initial_states: tensor with shape (samples, ...) (no time dimension),\n+                    as 'states'. The first state in the list must be the\n+                    output tensor at the previous timestep.\n+        initial_states: tensor with shape (samples, output_dim) (no time dimension),\n             containing the initial values for the states used in\n             the step function.\n         go_backwards: boolean. If True, do the iteration over\n@@ -1047,16 +1048,22 @@ def rnn(step_function, inputs, initial_states,\n             the step function, of shape (samples, ...).\n     '''\n     ndim = len(inputs.get_shape())\n-    assert ndim >= 3, \"Input should be at least 3D.\"\n+    assert ndim >= 3, 'Input should be at least 3D.'\n     axes = [1, 0] + list(range(2, ndim))\n     inputs = tf.transpose(inputs, (axes))\n-    input_list = tf.unpack(inputs)\n+\n     if constants is None:\n         constants = []\n \n+    if unroll:\n+        if not inputs.get_shape()[0]:\n+            raise Exception('Unrolling requires a fixed number of timesteps.')\n+\n         states = initial_states\n         successive_states = []\n         successive_outputs = []\n+\n+        input_list = tf.unpack(inputs)\n         if go_backwards:\n             input_list.reverse()\n \n@@ -1098,15 +1105,107 @@ def rnn(step_function, inputs, initial_states,\n                 states = return_states\n                 successive_outputs.append(output)\n                 successive_states.append(states)\n+                last_output = successive_outputs[-1]\n+                new_states = successive_states[-1]\n+                outputs = tf.pack(successive_outputs)\n         else:\n             for input in input_list:\n                 output, states = step_function(input, states + constants)\n                 successive_outputs.append(output)\n                 successive_states.append(states)\n-\n             last_output = successive_outputs[-1]\n-    outputs = tf.pack(successive_outputs)\n             new_states = successive_states[-1]\n+            outputs = tf.pack(successive_outputs)\n+\n+    else:\n+        from tensorflow.python.ops.rnn import _dynamic_rnn_loop\n+\n+        if go_backwards:\n+            inputs = tf.reverse(inputs, [True, False, False])\n+\n+        states = initial_states\n+        nb_states = len(states)\n+        if nb_states == 0:\n+            raise Exception('No initial states provided.')\n+        elif nb_states == 1:\n+            state = states[0]\n+        else:\n+            state = tf.concat(1, states)\n+\n+        state_size = int(states[0].get_shape()[-1])\n+\n+        if mask is not None:\n+            if go_backwards:\n+                mask = tf.reverse(mask, [True, False, False])\n+\n+            # Transpose not supported by bool tensor types, hence round-trip to uint8.\n+            mask = tf.cast(mask, tf.uint8)\n+            if len(mask.get_shape()) == ndim - 1:\n+                mask = expand_dims(mask)\n+            mask = tf.transpose(mask, axes)\n+            inputs = tf.concat(2, [tf.cast(mask, inputs.dtype), inputs])\n+\n+            def _step(input, state):\n+                if nb_states > 1:\n+                    states = []\n+                    for i in range(nb_states):\n+                        states.append(state[:, i * state_size: (i + 1) * state_size])\n+                else:\n+                    states = [state]\n+                mask_t = tf.cast(input[:, 0], tf.bool)\n+                input = input[:, 1:]\n+                output, new_states = step_function(input, states + constants)\n+\n+                output = tf.select(mask_t, output, states[0])\n+                new_states = [tf.select(mask_t, new_states[i], states[i]) for i in range(len(states))]\n+\n+                if len(new_states) == 1:\n+                    new_state = new_states[0]\n+                else:\n+                    new_state = tf.concat(1, new_states)\n+\n+                return output, new_state\n+        else:\n+            def _step(input, state):\n+                if nb_states > 1:\n+                    states = []\n+                    for i in range(nb_states):\n+                        states.append(state[:, i * state_size: (i + 1) * state_size])\n+                else:\n+                    states = [state]\n+                output, new_states = step_function(input, states + constants)\n+\n+                if len(new_states) == 1:\n+                    new_state = new_states[0]\n+                else:\n+                    new_state = tf.concat(1, new_states)\n+                return output, new_state\n+\n+        # state size is assumed to be the same as output size\n+        # (always the case)\n+        _step.state_size = state_size * nb_states\n+        _step.output_size = state_size\n+\n+        (outputs, final_state) = _dynamic_rnn_loop(\n+            _step,\n+            inputs,\n+            state,\n+            parallel_iterations=32,\n+            swap_memory=True,\n+            sequence_length=None)\n+\n+        if nb_states > 1:\n+            new_states = []\n+            for i in range(nb_states):\n+                new_states.append(final_state[:, i * state_size: (i + 1) * state_size])\n+        else:\n+            new_states = [final_state]\n+\n+        # all this circus is to recover the last vector in the sequence.\n+        begin = tf.pack([tf.shape(outputs)[0] - 1, 0, 0])\n+        size = tf.pack([1, -1, -1])\n+        last_output = tf.slice(outputs, begin, size)\n+        last_output = tf.squeeze(last_output, [0])\n \n     axes = [1, 0] + list(range(2, len(outputs.get_shape())))\n     outputs = tf.transpose(outputs, axes)\n\n@@ -12,13 +12,10 @@ def time_distributed_dense(x, w, b=None, dropout=None,\n     '''Apply y.w + b for every temporal slice y of x.\n     '''\n     if not input_dim:\n-        # won't work with TensorFlow\n         input_dim = K.shape(x)[2]\n     if not timesteps:\n-        # won't work with TensorFlow\n         timesteps = K.shape(x)[1]\n     if not output_dim:\n-        # won't work with TensorFlow\n         output_dim = K.shape(w)[1]\n \n     if dropout is not None and 0. < dropout < 1.:\n@@ -30,12 +27,13 @@ def time_distributed_dense(x, w, b=None, dropout=None,\n \n     # collapse time dimension and batch dimension together\n     x = K.reshape(x, (-1, input_dim))\n-\n     x = K.dot(x, w)\n     if b:\n         x = x + b\n     # reshape to 3D tensor\n-    x = K.reshape(x, (-1, timesteps, output_dim))\n+    x = K.reshape(x, K.pack([-1, timesteps, output_dim]))\n+    if K.backend() == 'tensorflow':\n+        x.set_shape([None, None, output_dim])\n     return x\n \n \n@@ -120,14 +118,10 @@ class Recurrent(Layer):\n         use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n         set to `True`.\n \n-    # TensorFlow warning\n-        For the time being, when using the TensorFlow backend,\n-        the number of timesteps used must be specified in your model.\n-        Make sure to pass an `input_length` int argument to your\n-        recurrent layer (if it comes first in your model),\n-        or to pass a complete `input_shape` argument to the first layer\n-        in your model otherwise.\n-\n+    # Note on performance\n+        You will see much better performance with RNNs in Theano compared to\n+        TensorFlow. Additionally, when using TensorFlow, it is preferable\n+        to set `unroll=True` for better performance.\n \n     # Note on using statefulness in RNNs\n         You can set RNN layers to be 'stateful', which means that the states\n@@ -148,10 +142,6 @@ class Recurrent(Layer):\n \n         To reset the states of your model, call `.reset_states()` on either\n         a specific layer, or on your entire model.\n-\n-    # Note on using dropout with TensorFlow\n-        When using the TensorFlow backend, specify a fixed batch size for your model\n-        following the notes on statefulness RNNs.\n     '''\n     def __init__(self, weights=None,\n                  return_sequences=False, go_backwards=False, stateful=False,\n@@ -207,19 +197,6 @@ class Recurrent(Layer):\n         # note that the .build() method of subclasses MUST define\n         # self.input_spec with a complete input shape.\n         input_shape = self.input_spec[0].shape\n-        if K._BACKEND == 'tensorflow':\n-            if not input_shape[1]:\n-                raise Exception('When using TensorFlow, you should define '\n-                                'explicitly the number of timesteps of '\n-                                'your sequences.\\n'\n-                                'If your first layer is an Embedding, '\n-                                'make sure to pass it an \"input_length\" '\n-                                'argument. Otherwise, make sure '\n-                                'the first layer has '\n-                                'an \"input_shape\" or \"batch_input_shape\" '\n-                                'argument, including the time axis. '\n-                                'Found input shape at layer ' + self.name +\n-                                ': ' + str(input_shape))\n         if self.stateful:\n             initial_states = self.states\n         else:\n\n@@ -297,6 +297,7 @@ class TestBackend(object):\n                 return output, [output]\n             return step_function\n \n+        # test default setup\n         th_rnn_step_fn = rnn_step_fn(input_dim, output_dim, KTH)\n         th_inputs = KTH.variable(input_val)\n         th_initial_states = [KTH.variable(init_state_val)]\n@@ -342,6 +343,35 @@ class TestBackend(object):\n         assert_allclose(th_outputs, unrolled_th_outputs, atol=1e-04)\n         assert_allclose(th_state, unrolled_th_state, atol=1e-04)\n \n+        # test go_backwards\n+        th_rnn_step_fn = rnn_step_fn(input_dim, output_dim, KTH)\n+        th_inputs = KTH.variable(input_val)\n+        th_initial_states = [KTH.variable(init_state_val)]\n+        last_output, outputs, new_states = KTH.rnn(th_rnn_step_fn, th_inputs,\n+                                                   th_initial_states,\n+                                                   go_backwards=True,\n+                                                   mask=None)\n+        th_last_output = KTH.eval(last_output)\n+        th_outputs = KTH.eval(outputs)\n+        assert len(new_states) == 1\n+        th_state = KTH.eval(new_states[0])\n+\n+        tf_rnn_step_fn = rnn_step_fn(input_dim, output_dim, KTF)\n+        tf_inputs = KTF.variable(input_val)\n+        tf_initial_states = [KTF.variable(init_state_val)]\n+        last_output, outputs, new_states = KTF.rnn(tf_rnn_step_fn, tf_inputs,\n+                                                   tf_initial_states,\n+                                                   go_backwards=True,\n+                                                   mask=None)\n+        tf_last_output = KTF.eval(last_output)\n+        tf_outputs = KTF.eval(outputs)\n+        assert len(new_states) == 1\n+        tf_state = KTF.eval(new_states[0])\n+\n+        assert_allclose(tf_last_output, th_last_output, atol=1e-04)\n+        assert_allclose(tf_outputs, th_outputs, atol=1e-04)\n+        assert_allclose(tf_state, th_state, atol=1e-04)\n+\n         # test unroll with backwards = True\n         bwd_last_output, bwd_outputs, bwd_new_states = KTH.rnn(\n             th_rnn_step_fn, th_inputs,\n\n@@ -448,97 +448,97 @@ def test_recursion():\n         y = Dense(2)(x)\n \n \n-@keras_test\n-def test_functional_guide():\n-    # MNIST\n-    from keras.layers import Input, Dense, LSTM\n-    from keras.models import Model\n-    from keras.utils import np_utils\n+# @keras_test\n+# def test_functional_guide():\n+#     # MNIST\n+#     from keras.layers import Input, Dense, LSTM\n+#     from keras.models import Model\n+#     from keras.utils import np_utils\n \n-    # this returns a tensor\n-    inputs = Input(shape=(784,))\n+#     # this returns a tensor\n+#     inputs = Input(shape=(784,))\n \n-    # a layer instance is callable on a tensor, and returns a tensor\n-    x = Dense(64, activation='relu')(inputs)\n-    x = Dense(64, activation='relu')(x)\n-    predictions = Dense(10, activation='softmax')(x)\n+#     # a layer instance is callable on a tensor, and returns a tensor\n+#     x = Dense(64, activation='relu')(inputs)\n+#     x = Dense(64, activation='relu')(x)\n+#     predictions = Dense(10, activation='softmax')(x)\n \n-    # this creates a model that includes\n-    # the Input layer and three Dense layers\n-    model = Model(input=inputs, output=predictions)\n-    model.compile(optimizer='rmsprop',\n-                  loss='categorical_crossentropy',\n-                  metrics=['accuracy'])\n+#     # this creates a model that includes\n+#     # the Input layer and three Dense layers\n+#     model = Model(input=inputs, output=predictions)\n+#     model.compile(optimizer='rmsprop',\n+#                   loss='categorical_crossentropy',\n+#                   metrics=['accuracy'])\n \n-    # the data, shuffled and split between tran and test sets\n-    X_train = np.random.random((100, 784))\n-    Y_train = np.random.random((100, 10))\n+#     # the data, shuffled and split between tran and test sets\n+#     X_train = np.random.random((100, 784))\n+#     Y_train = np.random.random((100, 10))\n \n-    model.fit(X_train, Y_train, nb_epoch=2, batch_size=128)\n+#     model.fit(X_train, Y_train, nb_epoch=2, batch_size=128)\n \n-    assert model.inputs == [inputs]\n-    assert model.outputs == [predictions]\n-    assert model.input == inputs\n-    assert model.output == predictions\n-    assert model.input_shape == (None, 784)\n-    assert model.output_shape == (None, 10)\n+#     assert model.inputs == [inputs]\n+#     assert model.outputs == [predictions]\n+#     assert model.input == inputs\n+#     assert model.output == predictions\n+#     assert model.input_shape == (None, 784)\n+#     assert model.output_shape == (None, 10)\n \n-    # try calling the sequential model\n-    inputs = Input(shape=(784,))\n-    new_outputs = model(inputs)\n-    new_model = Model(input=inputs, output=new_outputs)\n-    new_model.compile(optimizer='rmsprop',\n-                      loss='categorical_crossentropy',\n-                      metrics=['accuracy'])\n+#     # try calling the sequential model\n+#     inputs = Input(shape=(784,))\n+#     new_outputs = model(inputs)\n+#     new_model = Model(input=inputs, output=new_outputs)\n+#     new_model.compile(optimizer='rmsprop',\n+#                       loss='categorical_crossentropy',\n+#                       metrics=['accuracy'])\n \n-    ##################################################\n-    # multi-io\n-    ##################################################\n-    tweet_a = Input(shape=(4, 25))\n-    tweet_b = Input(shape=(4, 25))\n-    # this layer can take as input a matrix\n-    # and will return a vector of size 64\n-    shared_lstm = LSTM(64)\n+#     ##################################################\n+#     # multi-io\n+#     ##################################################\n+#     tweet_a = Input(shape=(4, 25))\n+#     tweet_b = Input(shape=(4, 25))\n+#     # this layer can take as input a matrix\n+#     # and will return a vector of size 64\n+#     shared_lstm = LSTM(64)\n \n-    # when we reuse the same layer instance\n-    # multiple times, the weights of the layer\n-    # are also being reused\n-    # (it is effectively *the same* layer)\n-    encoded_a = shared_lstm(tweet_a)\n-    encoded_b = shared_lstm(tweet_b)\n+#     # when we reuse the same layer instance\n+#     # multiple times, the weights of the layer\n+#     # are also being reused\n+#     # (it is effectively *the same* layer)\n+#     encoded_a = shared_lstm(tweet_a)\n+#     encoded_b = shared_lstm(tweet_b)\n \n-    # we can then concatenate the two vectors:\n-    merged_vector = merge([encoded_a, encoded_b],\n-                          mode='concat', concat_axis=-1)\n+#     # we can then concatenate the two vectors:\n+#     merged_vector = merge([encoded_a, encoded_b],\n+#                           mode='concat', concat_axis=-1)\n \n-    # and add a logistic regression on top\n-    predictions = Dense(1, activation='sigmoid')(merged_vector)\n+#     # and add a logistic regression on top\n+#     predictions = Dense(1, activation='sigmoid')(merged_vector)\n \n-    # we define a trainable model linking the\n-    # tweet inputs to the predictions\n-    model = Model(input=[tweet_a, tweet_b], output=predictions)\n+#     # we define a trainable model linking the\n+#     # tweet inputs to the predictions\n+#     model = Model(input=[tweet_a, tweet_b], output=predictions)\n \n-    model.compile(optimizer='rmsprop',\n-                  loss='binary_crossentropy',\n-                  metrics=['accuracy'])\n-    data_a = np.random.random((1000, 4, 25))\n-    data_b = np.random.random((1000, 4, 25))\n-    labels = np.random.random((1000,))\n-    model.fit([data_a, data_b], labels, nb_epoch=1)\n+#     model.compile(optimizer='rmsprop',\n+#                   loss='binary_crossentropy',\n+#                   metrics=['accuracy'])\n+#     data_a = np.random.random((1000, 4, 25))\n+#     data_b = np.random.random((1000, 4, 25))\n+#     labels = np.random.random((1000,))\n+#     model.fit([data_a, data_b], labels, nb_epoch=1)\n \n-    model.summary()\n-    assert model.inputs == [tweet_a, tweet_b]\n-    assert model.outputs == [predictions]\n-    assert model.input == [tweet_a, tweet_b]\n-    assert model.output == predictions\n+#     model.summary()\n+#     assert model.inputs == [tweet_a, tweet_b]\n+#     assert model.outputs == [predictions]\n+#     assert model.input == [tweet_a, tweet_b]\n+#     assert model.output == predictions\n \n-    assert model.output == predictions\n-    assert model.input_shape == [(None, 4, 25), (None, 4, 25)]\n-    assert model.output_shape == (None, 1)\n+#     assert model.output == predictions\n+#     assert model.input_shape == [(None, 4, 25), (None, 4, 25)]\n+#     assert model.output_shape == (None, 1)\n \n-    assert shared_lstm.get_output_at(0) == encoded_a\n-    assert shared_lstm.get_output_at(1) == encoded_b\n-    assert shared_lstm.input_shape == (None, 4, 25)\n+#     assert shared_lstm.get_output_at(0) == encoded_a\n+#     assert shared_lstm.get_output_at(1) == encoded_b\n+#     assert shared_lstm.input_shape == (None, 4, 25)\n \n \n @keras_test\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8459d0403c308a86076001ac63a1294547417bdb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 4815 | Contributors (this commit): 24 | Commits (past 90d): 15 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -1451,7 +1451,7 @@ class Cropping1D(Layer):\n         super(Cropping1D, self).__init__(**kwargs)\n         self.cropping = tuple(cropping)\n         assert len(self.cropping) == 2, 'cropping must be a tuple length of 2'\n-        self.input_spec = [InputSpec(ndim=3)] # redundant due to build()?       \n+        self.input_spec = [InputSpec(ndim=3)]\n \n     def build(self, input_shape):\n         self.input_spec = [InputSpec(shape=input_shape)]\n@@ -1591,7 +1591,7 @@ class Cropping3D(Layer):\n         assert len(self.cropping[2]) == 2, 'cropping[2] must be a tuple length of 2'\n         assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n         self.dim_ordering = dim_ordering\n-        self.input_spec = [InputSpec(ndim=4)]        \n+        self.input_spec = [InputSpec(ndim=5)]\n \n     def build(self, input_shape):\n         self.input_spec = [InputSpec(shape=input_shape)]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ad3db301f29b2e56cd24ebb9898009046cc09873", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 4363 | Contributors (this commit): 28 | Commits (past 90d): 9 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -119,9 +119,9 @@ class Recurrent(Layer):\n         set to `True`.\n \n     # Note on performance\n-        You will see much better performance with RNNs in Theano compared to\n-        TensorFlow. Additionally, when using TensorFlow, it is preferable\n-        to set `unroll=True` for better performance.\n+        You are likely to see better performance with RNNs in Theano compared\n+        to TensorFlow. Additionally, when using TensorFlow, it is often\n+        preferable to set `unroll=True` for better performance.\n \n     # Note on using statefulness in RNNs\n         You can set RNN layers to be 'stateful', which means that the states\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1cf04b7a10b1970c8fe15678b9ffd7f6b3abcfe4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 14 | Files Changed: 3 | Hunks: 10 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 50 | Churn Cumulative: 6159 | Contributors (this commit): 26 | Commits (past 90d): 22 | Contributors (cumulative): 30 | DMM Complexity: None\n\nDIFF:\n@@ -65,6 +65,7 @@ if sys.version[0] == '2':\n     sys.setdefaultencoding('utf8')\n \n from keras.layers import convolutional\n+from keras.layers import local\n from keras.layers import recurrent\n from keras.layers import core\n from keras.layers import noise\n@@ -106,6 +107,7 @@ PAGES = [\n             models.Sequential.predict_on_batch,\n             models.Sequential.fit_generator,\n             models.Sequential.evaluate_generator,\n+            models.Sequential.predict_generator,\n         ],\n     },\n     {\n@@ -120,6 +122,7 @@ PAGES = [\n             models.Model.predict_on_batch,\n             models.Model.fit_generator,\n             models.Model.evaluate_generator,\n+            models.Model.predict_generator,\n             models.Model.get_layer,\n         ]\n     },\n@@ -147,7 +150,9 @@ PAGES = [\n         'classes': [\n             convolutional.Convolution1D,\n             convolutional.Convolution2D,\n-            convolutional.AtrousConv2D,\n+            convolutional.AtrousConvolution2D,\n+            convolutional.SeparableConvolution2D,\n+            convolutional.Deconvolution2D,\n             convolutional.Convolution3D,\n             convolutional.UpSampling1D,\n             convolutional.UpSampling2D,\n@@ -168,6 +173,13 @@ PAGES = [\n             convolutional.AveragePooling3D,\n         ],\n     },\n+    {\n+        'page': 'layers/local.md',\n+        'classes': [\n+            local.LocallyConnected1D,\n+            local.LocallyConnected2D,\n+        ],\n+    },\n     {\n         'page': 'layers/recurrent.md',\n         'classes': [\n\n@@ -696,6 +696,11 @@ class SeparableConvolution2D(Layer):\n     (tuple of integers, does not include the sample axis),\n     e.g. `input_shape=(3, 128, 128)` for 128x128 RGB pictures.\n \n+    # Theano warning\n+\n+    This layer is only available with the\n+    TensorFlow backend for the time being.\n+\n     # Arguments\n         nb_filter: Number of convolution filters to use.\n         nb_row: Number of rows in the convolution kernel.\n\n@@ -8,14 +8,17 @@ from ..utils.np_utils import conv_output_length\n \n \n class LocallyConnected1D(Layer):\n-    '''LocallyConnected1D layer works almost the same as Convolution1D layer,\n-    except that weights are unshared, that is, a different set of filters is\n-    applied at each different patch of the input. When using this layer as the\n-    first layer in a model, either provide the keyword argument `input_dim`\n+    '''The `LocallyConnected1D` layer works similarly to\n+    the `Convolution1D` layer, except that weights are unshared,\n+    that is, a different set of filters is applied at each different patch\n+    of the input.\n+    When using this layer as the first layer in a model,\n+    either provide the keyword argument `input_dim`\n     (int, e.g. 128 for sequences of 128-dimensional vectors), or `input_shape`\n-    (tuple of integers, e.g. (10, 128) for sequences of 10 vectors of\n-    128-dimensional vectors). Also, you will need to fix shape of the previous\n-    layer, since the weights can only be defined with determined output shape.\n+    (tuple of integers, e.g. `input_shape=(10, 128)`\n+    for sequences of 10 vectors of 128-dimensional vectors).\n+    Also, note that this layer can only be used with\n+    a fully-specified input shape (`None` dimensions not allowed).\n \n     # Example\n     ```python\n@@ -180,14 +183,16 @@ class LocallyConnected1D(Layer):\n \n \n class LocallyConnected2D(Layer):\n-    '''LocallyConnected2D layer works almost the same as Convolution2D layer,\n-    except that weights are unshared, that is, a different set of filters is\n-    applied at each different patch of the input. When using this layer as the\n+    '''The `LocallyConnected2D` layer works similarly\n+    to the `Convolution2D` layer, except that weights are unshared,\n+    that is, a different set of filters is applied at each\n+    different patch of the input.\n+    When using this layer as the\n     first layer in a model, provide the keyword argument `input_shape` (tuple\n     of integers, does not include the sample axis), e.g.\n-    `input_shape=(3, 128, 128)` for 128x128 RGB pictures. Also, you will need\n-    to fix shape of the previous layer, since the weights can only be defined\n-    with determined output shape.\n+    `input_shape=(3, 128, 128)` for 128x128 RGB pictures.\n+    Also, note that this layer can only be used with\n+    a fully-specified input shape (`None` dimensions not allowed).\n \n     # Examples\n     ```python\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4cefd6136b48b431d69215901a9356a0db68c8a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 146 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -13,5 +13,6 @@ setup(name='Keras',\n       install_requires=['theano', 'pyyaml', 'six'],\n       extras_require={\n           'h5py': ['h5py'],\n+          'visualize': ['pydot-ng'],\n       },\n       packages=find_packages())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e582f9dcac06e1b942596fd67917ca0012951cad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 2092 | Contributors (this commit): 21 | Commits (past 90d): 23 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1471,6 +1471,7 @@ class Model(Container):\n                         # no need for try/except because\n                         # data has already been validated\n                         val_outs = self.evaluate(val_x, val_y,\n+                                                 batch_size=batch_size,\n                                                  sample_weight=val_sample_weights,\n                                                  verbose=0)\n                     if type(val_outs) is not list:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#79a2bcd05fa23912e569e4150214c22de1cdfeae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 2686 | Contributors (this commit): 29 | Commits (past 90d): 47 | Contributors (cumulative): 29 | DMM Complexity: None\n\nDIFF:\n@@ -1121,7 +1121,7 @@ def rnn(step_function, inputs, initial_states,\n         from tensorflow.python.ops.rnn import _dynamic_rnn_loop\n \n         if go_backwards:\n-            inputs = tf.reverse(inputs, [True, False, False])\n+            inputs = tf.reverse(inputs, [True] + [False] * (ndim - 1))\n \n         states = initial_states\n         nb_states = len(states)\n@@ -1136,7 +1136,7 @@ def rnn(step_function, inputs, initial_states,\n \n         if mask is not None:\n             if go_backwards:\n-                mask = tf.reverse(mask, [True, False, False])\n+                mask = tf.reverse(mask, [True] + [False] * (ndim - 1))\n \n             # Transpose not supported by bool tensor types, hence round-trip to uint8.\n             mask = tf.cast(mask, tf.uint8)\n@@ -1202,8 +1202,8 @@ def rnn(step_function, inputs, initial_states,\n             new_states = [final_state]\n \n         # all this circus is to recover the last vector in the sequence.\n-        begin = tf.pack([tf.shape(outputs)[0] - 1, 0, 0])\n-        size = tf.pack([1, -1, -1])\n+        begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1))\n+        size = tf.pack([1] + [-1] * (ndim - 1))\n         last_output = tf.slice(outputs, begin, size)\n         last_output = tf.squeeze(last_output, [0])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#88b301f182d904e71478e8985155500e3ce38d1a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 110 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -52,7 +52,7 @@ class HDF5Matrix():\n \n     @property\n     def shape(self):\n-        return tuple([self.end - self.start, self.data.shape[1]])\n+        return (self.end - self.start,) + self.data.shape[1:]\n \n \n def save_array(array, name):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d0659327bd2d70e47d8608ea9a48a9b9f4cd4a3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 228 | Contributors (this commit): 9 | Commits (past 90d): 11 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -15,4 +15,4 @@ from . import objectives\n from . import optimizers\r\n from . import regularizers\r\n \r\n-__version__ = '1.0.7'\r\n+__version__ = '1.0.8'\r\n\n@@ -3,12 +3,12 @@ from setuptools import find_packages\n \n \n setup(name='Keras',\n-      version='1.0.7',\n+      version='1.0.8',\n       description='Deep Learning for Python',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n-      download_url='https://github.com/fchollet/keras/tarball/1.0.7',\n+      download_url='https://github.com/fchollet/keras/tarball/1.0.8',\n       license='MIT',\n       install_requires=['theano', 'pyyaml', 'six'],\n       extras_require={\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f23f2ff2c96537274beb99aca8417f054f817501", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 947 | Lines Deleted: 154 | Files Changed: 8 | Hunks: 40 | Methods Changed: 15 | Complexity Δ (Sum/Max): 78/20 | Churn Δ: 1101 | Churn Cumulative: 1717 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 11 | DMM Complexity: 0.18984962406015038\n\nDIFF:\n@@ -3,32 +3,21 @@\n This script can run on CPU in a few minutes (with the TensorFlow backend).\n \n Results example: http://i.imgur.com/4nj4KjN.jpg\n-\n-Before running this script, download the weights for the VGG16 model at:\n-https://drive.google.com/file/d/0Bz7KyqmuGsilT0J5dmRCM0ROVHc/view?usp=sharing\n-(source: https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3)\n-and make sure the variable `weights_path` in this script matches the location of the file.\n '''\n from __future__ import print_function\n from scipy.misc import imsave\n import numpy as np\n import time\n-import os\n-import h5py\n-\n-from keras.models import Sequential\n-from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D\n+from keras.applications import vgg16\n from keras import backend as K\n \n # dimensions of the generated pictures for each filter.\n img_width = 128\n img_height = 128\n \n-# path to the model weights file.\n-weights_path = 'vgg16_weights.h5'\n-\n-# the name of the layer we want to visualize (see model definition below)\n-layer_name = 'conv5_1'\n+# the name of the layer we want to visualize\n+# (see model definition at keras/applications/vgg16.py)\n+layer_name = 'block5_conv1'\n \n # util function to convert a tensor into a valid image\n def deprocess_image(x):\n@@ -43,70 +32,22 @@ def deprocess_image(x):\n \n     # convert to RGB array\n     x *= 255\n+    if K.image_dim_ordering() == 'th':\n         x = x.transpose((1, 2, 0))\n     x = np.clip(x, 0, 255).astype('uint8')\n     return x\n \n-# build the VGG16 network\n-model = Sequential()\n-model.add(ZeroPadding2D((1, 1), batch_input_shape=(1, 3, img_width, img_height)))\n-first_layer = model.layers[-1]\n-# this is a placeholder tensor that will contain our generated images\n-input_img = first_layer.input\n-\n-model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n-model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n-\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n-model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n-\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n-model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n-\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n-model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n-\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n-model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n-\n-# load the weights of the VGG16 networks\n-# (trained on ImageNet, won the ILSVRC competition in 2014)\n-# note: when there is a complete match between your model definition\n-# and your weight savefile, you can simply call model.load_weights(filename)\n-assert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\n-f = h5py.File(weights_path)\n-for k in range(f.attrs['nb_layers']):\n-    if k >= len(model.layers):\n-        # we don't look at the last (fully-connected) layers in the savefile\n-        break\n-    g = f['layer_{}'.format(k)]\n-    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n-    model.layers[k].set_weights(weights)\n-f.close()\n+# build the VGG16 network with ImageNet weights\n+model = vgg16.VGG16(weights='imagenet', include_top=False)\n print('Model loaded.')\n \n+model.summary()\n+\n+# this is the placeholder for the input images\n+input_img = model.input\n+\n # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n-layer_dict = dict([(layer.name, layer) for layer in model.layers])\n+layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n \n \n def normalize(x):\n@@ -124,7 +65,10 @@ for filter_index in range(0, 200):\n     # we build a loss function that maximizes the activation\n     # of the nth filter of the layer considered\n     layer_output = layer_dict[layer_name].output\n+    if K.image_dim_ordering() == 'th':\n         loss = K.mean(layer_output[:, filter_index, :, :])\n+    else:\n+        loss = K.mean(layer_output[:, :, :, filter_index])\n \n     # we compute the gradient of the input picture wrt this loss\n     grads = K.gradients(loss, input_img)[0]\n@@ -139,7 +83,11 @@ for filter_index in range(0, 200):\n     step = 1.\n \n     # we start from a gray image with some random noise\n-    input_img_data = np.random.random((1, 3, img_width, img_height)) * 20 + 128.\n+    if K.image_dim_ordering() == 'th':\n+        input_img_data = np.random.random((1, 3, img_width, img_height))\n+    else:\n+        input_img_data = np.random.random((1, img_width, img_height, 3))\n+    input_img_data = (input_img_data - 0.5) * 20 + 128\n \n     # we run gradient ascent for 20 steps\n     for i in range(20):\n\n@@ -1,10 +1,5 @@\n '''Neural style transfer with Keras.\n \n-Before running this script, download the weights for the VGG16 model at:\n-https://drive.google.com/file/d/0Bz7KyqmuGsilT0J5dmRCM0ROVHc/view?usp=sharing\n-(source: https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3)\n-and make sure the variable `weights_path` in this script matches the location of the file.\n-\n Run the script with:\n ```\n python neural_style_transfer.py path_to_your_base_image.jpg path_to_your_reference.jpg prefix_for_results\n@@ -15,7 +10,6 @@ python neural_style_transfer.py img/tuebingen.jpg img/starry_night.jpg results/m\n ```\n \n It is preferable to run this script on GPU, for speed.\n-If running on CPU, prefer the TensorFlow backend (much faster).\n \n Example result: https://twitter.com/fchollet/status/686631033085677568\n \n@@ -49,16 +43,14 @@ keeping the generated image close enough to the original one.\n '''\n \n from __future__ import print_function\n-from scipy.misc import imread, imresize, imsave\n+from keras.preprocessing.image import load_img, img_to_array\n+from scipy.misc import imsave\n import numpy as np\n from scipy.optimize import fmin_l_bfgs_b\n import time\n-import os\n import argparse\n-import h5py\n \n-from keras.models import Sequential\n-from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D\n+from keras.applications import vgg16\n from keras import backend as K\n \n parser = argparse.ArgumentParser(description='Neural style transfer with Keras.')\n@@ -73,14 +65,12 @@ args = parser.parse_args()\n base_image_path = args.base_image_path\n style_reference_image_path = args.style_reference_image_path\n result_prefix = args.result_prefix\n-weights_path = 'vgg16_weights.h5'\n \n # these are the weights of the different loss components\n total_variation_weight = 1.\n style_weight = 1.\n content_weight = 0.025\n \n-\n # dimensions of the generated picture.\n img_width = 400\n img_height = 400\n@@ -88,22 +78,23 @@ assert img_height == img_width, 'Due to the use of the Gram matrix, width and he\n \n # util function to open, resize and format pictures into appropriate tensors\n def preprocess_image(image_path):\n-    img = imresize(imread(image_path), (img_width, img_height))\n-    img = img[:, :, ::-1].astype('float64')\n-    img[:, :, 0] -= 103.939\n-    img[:, :, 1] -= 116.779\n-    img[:, :, 2] -= 123.68\n-    img = img.transpose((2, 0, 1))\n+    img = load_img(image_path, target_size=(img_width, img_height))\n+    img = img_to_array(img)\n     img = np.expand_dims(img, axis=0)\n+    img = vgg16.preprocess_input(img)\n     return img\n \n # util function to convert a tensor into a valid image\n def deprocess_image(x):\n+    if K.image_dim_ordering() == 'th':\n+        x = x.reshape((3, img_width, img_height))\n         x = x.transpose((1, 2, 0))\n+    else:\n+        x = x.reshape((img_width, img_height, 3))\n+    x = x[:, :, ::-1]\n     x[:, :, 0] += 103.939\n     x[:, :, 1] += 116.779\n     x[:, :, 2] += 123.68\n-    x = x[:, :, ::-1]\n     x = np.clip(x, 0, 255).astype('uint8')\n     return x\n \n@@ -112,7 +103,10 @@ base_image = K.variable(preprocess_image(base_image_path))\n style_reference_image = K.variable(preprocess_image(style_reference_image_path))\n \n # this will contain our generated image\n+if K.image_dim_ordering() == 'th':\n     combination_image = K.placeholder((1, 3, img_width, img_height))\n+else:\n+    combination_image = K.placeholder((1, img_width, img_height, 3))\n \n # combine the 3 images into a single Keras tensor\n input_tensor = K.concatenate([base_image,\n@@ -120,60 +114,9 @@ input_tensor = K.concatenate([base_image,\n                               combination_image], axis=0)\n \n # build the VGG16 network with our 3 images as input\n-first_layer = ZeroPadding2D((1, 1))\n-first_layer.set_input(input_tensor, shape=(3, 3, img_width, img_height))\n-\n-model = Sequential()\n-model.add(first_layer)\n-model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(64, 3, 3, activation='relu'))\n-model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n-\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(128, 3, 3, activation='relu'))\n-model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n-\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(256, 3, 3, activation='relu'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(256, 3, 3, activation='relu'))\n-model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n-\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu'))\n-model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n-\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu'))\n-model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n-\n-# load the weights of the VGG16 networks\n-# (trained on ImageNet, won the ILSVRC competition in 2014)\n-# note: when there is a complete match between your model definition\n-# and your weight savefile, you can simply call model.load_weights(filename)\n-assert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\n-f = h5py.File(weights_path)\n-for k in range(f.attrs['nb_layers']):\n-    if k >= len(model.layers):\n-        # we don't look at the last (fully-connected) layers in the savefile\n-        break\n-    g = f['layer_{}'.format(k)]\n-    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n-    model.layers[k].set_weights(weights)\n-f.close()\n+# the model will be loaded with pre-trained ImageNet weights\n+model = vgg16.VGG16(input_tensor=input_tensor,\n+                    weights='imagenet', include_top=False)\n print('Model loaded.')\n \n # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n@@ -213,19 +156,25 @@ def content_loss(base, combination):\n # designed to keep the generated image locally coherent\n def total_variation_loss(x):\n     assert K.ndim(x) == 4\n+    if K.image_dim_ordering() == 'th':\n         a = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, 1:, :img_height-1])\n         b = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, :img_width-1, 1:])\n+    else:\n+        a = K.square(x[:, :img_width-1, :img_height-1, :] - x[:, 1:, :img_height-1, :])\n+        b = K.square(x[:, :img_width-1, :img_height-1, :] - x[:, :img_width-1, 1:, :])\n     return K.sum(K.pow(a + b, 1.25))\n \n # combine these loss functions into a single scalar\n loss = K.variable(0.)\n-layer_features = outputs_dict['conv4_2']\n+layer_features = outputs_dict['block4_conv2']\n base_image_features = layer_features[0, :, :, :]\n combination_features = layer_features[2, :, :, :]\n loss += content_weight * content_loss(base_image_features,\n                                       combination_features)\n \n-feature_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n+feature_layers = ['block1_conv1', 'block2_conv1',\n+                  'block3_conv1', 'block4_conv1',\n+                  'block5_conv1']\n for layer_name in feature_layers:\n     layer_features = outputs_dict[layer_name]\n     style_reference_features = layer_features[1, :, :, :]\n@@ -244,8 +193,12 @@ else:\n     outputs.append(grads)\n \n f_outputs = K.function([combination_image], outputs)\n+\n def eval_loss_and_grads(x):\n+    if K.image_dim_ordering() == 'th':\n         x = x.reshape((1, 3, img_width, img_height))\n+    else:\n+        x = x.reshape((1, img_width, img_height, 3))\n     outs = f_outputs([x])\n     loss_value = outs[0]\n     if len(outs[1:]) == 1:\n@@ -283,10 +236,11 @@ evaluator = Evaluator()\n \n # run scipy-based optimization (L-BFGS) over the pixels of the generated image\n # so as to minimize the neural style loss\n-x = np.random.uniform(0, 255, (1, 3, img_width, img_height))\n-x[0, 0, :, :] -= 103.939\n-x[0, 1, :, :] -= 116.779\n-x[0, 2, :, :] -= 123.68\n+if K.image_dim_ordering() == 'th':\n+    x = np.random.uniform(0, 255, (1, 3, img_width, img_height)) - 128.\n+else:\n+    x = np.random.uniform(0, 255, (1, img_width, img_height, 3)) - 128.\n+\n for i in range(10):\n     print('Start of iteration', i)\n     start_time = time.time()\n@@ -294,7 +248,7 @@ for i in range(10):\n                                      fprime=evaluator.grads, maxfun=20)\n     print('Current loss value:', min_val)\n     # save current generated image\n-    img = deprocess_image(x.copy().reshape((3, img_width, img_height)))\n+    img = deprocess_image(x.copy())\n     fname = result_prefix + '_at_iteration_%d.png' % i\n     imsave(fname, img)\n     end_time = time.time()\n\n\n@@ -0,0 +1,43 @@\n+import numpy as np\n+import json\n+\n+from ..utils.data_utils import get_file\n+from .. import backend as K\n+\n+CLASS_INDEX = None\n+CLASS_INDEX_PATH = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n+\n+\n+def preprocess_input(x, dim_ordering='default'):\n+    if dim_ordering == 'default':\n+        dim_ordering = K.image_dim_ordering()\n+    assert dim_ordering in {'tf', 'th'}\n+\n+    if dim_ordering == 'th':\n+        x[:, 0, :, :] -= 103.939\n+        x[:, 1, :, :] -= 116.779\n+        x[:, 2, :, :] -= 123.68\n+        # 'RGB'->'BGR'\n+        x = x[:, ::-1, :, :]\n+    else:\n+        x[:, :, :, 0] -= 103.939\n+        x[:, :, :, 1] -= 116.779\n+        x[:, :, :, 2] -= 123.68\n+        # 'RGB'->'BGR'\n+        x = x[:, :, :, ::-1]\n+    return x\n+\n+\n+def decode_predictions(preds):\n+    global CLASS_INDEX\n+    assert len(preds.shape) == 2 and preds.shape[1] == 1000\n+    if CLASS_INDEX is None:\n+        fpath = get_file('imagenet_class_index.json',\n+                         CLASS_INDEX_PATH,\n+                         cache_subdir='models')\n+        CLASS_INDEX = json.load(open(fpath))\n+    indices = np.argmax(preds, axis=-1)\n+    results = []\n+    for i in indices:\n+        results.append(CLASS_INDEX[str(i)])\n+    return results\n\n@@ -0,0 +1,312 @@\n+# -*- coding: utf-8 -*-\n+'''Inception V3 model for Keras.\n+\n+Note that the ImageNet weights provided are from a model that had not fully converged.\n+Inception v3 should be able to reach 6.9% top-5 error, but our model\n+only gets to 7.8% (same as a fully-converged ResNet 50).\n+For comparison, VGG16 only gets to 9.9%, quite a bit worse.\n+\n+Also, do note that the input image format for this model is different than for\n+other models (299x299 instead of 224x224), and that the input preprocessing function\n+is also different.\n+\n+# Reference:\n+\n+- [Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567)\n+\n+'''\n+from __future__ import print_function\n+from __future__ import absolute_import\n+\n+import warnings\n+\n+from ..models import Model\n+from ..layers import Flatten, Dense, Input, BatchNormalization, merge\n+from ..layers import Convolution2D, MaxPooling2D, AveragePooling2D\n+from ..utils.layer_utils import convert_all_kernels_in_model\n+from ..utils.data_utils import get_file\n+from .. import backend as K\n+from .imagenet_utils import decode_predictions\n+\n+\n+TH_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/inception_v3_weights_th_dim_ordering_th_kernels.h5'\n+TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/inception_v3_weights_tf_dim_ordering_tf_kernels.h5'\n+TH_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/inception_v3_weights_th_dim_ordering_th_kernels_notop.h5'\n+TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n+\n+\n+def conv2d_bn(x, nb_filter, nb_row, nb_col,\n+              border_mode='same', subsample=(1, 1),\n+              name=None):\n+    '''Utility function to apply conv + BN.\n+    '''\n+    if name is not None:\n+        bn_name = name + '_bn'\n+        conv_name = name + '_conv'\n+    else:\n+        bn_name = None\n+        conv_name = None\n+    if K.image_dim_ordering() == 'th':\n+        bn_axis = 1\n+    else:\n+        bn_axis = 3\n+    x = Convolution2D(nb_filter, nb_row, nb_col,\n+                      subsample=subsample,\n+                      activation='relu',\n+                      border_mode=border_mode,\n+                      name=conv_name)(x)\n+    x = BatchNormalization(axis=bn_axis, name=bn_name)(x)\n+    return x\n+\n+\n+def InceptionV3(include_top=True, weights='imagenet',\n+                input_tensor=None):\n+    '''Instantiate the Inception v3 architecture,\n+    optionally loading weights pre-trained\n+    on ImageNet. Note that when using TensorFlow,\n+    for best performance you should set\n+    `image_dim_ordering=\"tf\"` in your Keras config\n+    at ~/.keras/keras.json.\n+\n+    The model and the weights are compatible with both\n+    TensorFlow and Theano. The dimension ordering\n+    convention used by the model is the one\n+    specified in your Keras config file.\n+\n+    Note that the default input image size for this model is 299x299.\n+\n+    # Arguments\n+        include_top: whether to include the 3 fully-connected\n+            layers at the top of the network.\n+        weights: one of `None` (random initialization)\n+            or \"imagenet\" (pre-training on ImageNet).\n+        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n+            to use as image input for the model.\n+\n+    # Returns\n+        A Keras model instance.\n+    '''\n+    if weights not in {'imagenet', None}:\n+        raise ValueError('The `weights` argument should be either '\n+                         '`None` (random initialization) or `imagenet` '\n+                         '(pre-training on ImageNet).')\n+    # Determine proper input shape\n+    if K.image_dim_ordering() == 'th':\n+        if include_top:\n+            input_shape = (3, 299, 299)\n+        else:\n+            input_shape = (3, None, None)\n+    else:\n+        if include_top:\n+            input_shape = (299, 299, 3)\n+        else:\n+            input_shape = (None, None, 3)\n+\n+    if input_tensor is None:\n+        img_input = Input(shape=input_shape)\n+    else:\n+        if not K.is_keras_tensor(input_tensor):\n+            img_input = Input(tensor=input_tensor)\n+        else:\n+            img_input = input_tensor\n+\n+    if K.image_dim_ordering() == 'th':\n+        channel_axis = 1\n+    else:\n+        channel_axis = 3\n+\n+    x = conv2d_bn(img_input, 32, 3, 3, subsample=(2, 2), border_mode='valid')\n+    x = conv2d_bn(x, 32, 3, 3, border_mode='valid')\n+    x = conv2d_bn(x, 64, 3, 3)\n+    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n+\n+    x = conv2d_bn(x, 80, 1, 1, border_mode='valid')\n+    x = conv2d_bn(x, 192, 3, 3, border_mode='valid')\n+    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n+\n+    # mixed 0, 1, 2: 35 x 35 x 256\n+    for i in range(3):\n+        branch1x1 = conv2d_bn(x, 64, 1, 1)\n+\n+        branch5x5 = conv2d_bn(x, 48, 1, 1)\n+        branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n+\n+        branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n+        branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n+        branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n+\n+        branch_pool = AveragePooling2D(\n+            (3, 3), strides=(1, 1), border_mode='same')(x)\n+        branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n+        x = merge([branch1x1, branch5x5, branch3x3dbl, branch_pool],\n+                  mode='concat', concat_axis=channel_axis,\n+                  name='mixed' + str(i))\n+\n+    # mixed 3: 17 x 17 x 768\n+    branch3x3 = conv2d_bn(x, 384, 3, 3, subsample=(2, 2), border_mode='valid')\n+\n+    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n+    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n+    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3,\n+                             subsample=(2, 2), border_mode='valid')\n+\n+    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n+    x = merge([branch3x3, branch3x3dbl, branch_pool],\n+              mode='concat', concat_axis=channel_axis,\n+              name='mixed3')\n+\n+    # mixed 4: 17 x 17 x 768\n+    branch1x1 = conv2d_bn(x, 192, 1, 1)\n+\n+    branch7x7 = conv2d_bn(x, 128, 1, 1)\n+    branch7x7 = conv2d_bn(branch7x7, 128, 1, 7)\n+    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n+\n+    branch7x7dbl = conv2d_bn(x, 128, 1, 1)\n+    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n+    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7)\n+    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n+    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n+\n+    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), border_mode='same')(x)\n+    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n+    x = merge([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n+              mode='concat', concat_axis=channel_axis,\n+              name='mixed4')\n+\n+    # mixed 5, 6: 17 x 17 x 768\n+    for i in range(2):\n+        branch1x1 = conv2d_bn(x, 192, 1, 1)\n+\n+        branch7x7 = conv2d_bn(x, 160, 1, 1)\n+        branch7x7 = conv2d_bn(branch7x7, 160, 1, 7)\n+        branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n+\n+        branch7x7dbl = conv2d_bn(x, 160, 1, 1)\n+        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n+        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 1, 7)\n+        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n+        branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n+\n+        branch_pool = AveragePooling2D(\n+            (3, 3), strides=(1, 1), border_mode='same')(x)\n+        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n+        x = merge([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n+                  mode='concat', concat_axis=channel_axis,\n+                  name='mixed' + str(5 + i))\n+\n+    # mixed 7: 17 x 17 x 768\n+    branch1x1 = conv2d_bn(x, 192, 1, 1)\n+\n+    branch7x7 = conv2d_bn(x, 192, 1, 1)\n+    branch7x7 = conv2d_bn(branch7x7, 192, 1, 7)\n+    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n+\n+    branch7x7dbl = conv2d_bn(x, 160, 1, 1)\n+    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n+    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n+    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n+    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n+\n+    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), border_mode='same')(x)\n+    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n+    x = merge([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n+              mode='concat', concat_axis=channel_axis,\n+              name='mixed7')\n+\n+    # mixed 8: 8 x 8 x 1280\n+    branch3x3 = conv2d_bn(x, 192, 1, 1)\n+    branch3x3 = conv2d_bn(branch3x3, 320, 3, 3,\n+                          subsample=(2, 2), border_mode='valid')\n+\n+    branch7x7x3 = conv2d_bn(x, 192, 1, 1)\n+    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)\n+    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)\n+    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 3, 3,\n+                            subsample=(2, 2), border_mode='valid')\n+\n+    branch_pool = AveragePooling2D((3, 3), strides=(2, 2))(x)\n+    x = merge([branch3x3, branch7x7x3, branch_pool],\n+              mode='concat', concat_axis=channel_axis,\n+              name='mixed8')\n+\n+    # mixed 9: 8 x 8 x 2048\n+    for i in range(2):\n+        branch1x1 = conv2d_bn(x, 320, 1, 1)\n+\n+        branch3x3 = conv2d_bn(x, 384, 1, 1)\n+        branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)\n+        branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)\n+        branch3x3 = merge([branch3x3_1, branch3x3_2],\n+                          mode='concat', concat_axis=channel_axis,\n+                          name='mixed9_' + str(i))\n+\n+        branch3x3dbl = conv2d_bn(x, 448, 1, 1)\n+        branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)\n+        branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)\n+        branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)\n+        branch3x3dbl = merge([branch3x3dbl_1, branch3x3dbl_2],\n+                             mode='concat', concat_axis=channel_axis)\n+\n+        branch_pool = AveragePooling2D(\n+            (3, 3), strides=(1, 1), border_mode='same')(x)\n+        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n+        x = merge([branch1x1, branch3x3, branch3x3dbl, branch_pool],\n+                  mode='concat', concat_axis=channel_axis,\n+                  name='mixed' + str(9 + i))\n+\n+    if include_top:\n+        # Classification block\n+        x = AveragePooling2D((8, 8), strides=(8, 8), name='avg_pool')(x)\n+        x = Flatten(name='flatten')(x)\n+        x = Dense(1000, activation='softmax', name='predictions')(x)\n+\n+    # Create model\n+    model = Model(img_input, x)\n+\n+    # load weights\n+    if weights == 'imagenet':\n+        if K.image_dim_ordering() == 'th':\n+            if include_top:\n+                weights_path = get_file('inception_v3_weights_th_dim_ordering_th_kernels.h5',\n+                                        TH_WEIGHTS_PATH,\n+                                        cache_subdir='models',\n+                                        md5_hash='b3baf3070cc4bf476d43a2ea61b0ca5f')\n+            else:\n+                weights_path = get_file('inception_v3_weights_th_dim_ordering_th_kernels_notop.h5',\n+                                        TH_WEIGHTS_PATH_NO_TOP,\n+                                        cache_subdir='models',\n+                                        md5_hash='79aaa90ab4372b4593ba3df64e142f05')\n+            model.load_weights(weights_path)\n+            if K.backend() == 'tensorflow':\n+                warnings.warn('You are using the TensorFlow backend, yet you '\n+                              'are using the Theano '\n+                              'image dimension ordering convention '\n+                              '(`image_dim_ordering=\"th\"`). '\n+                              'For best performance, set '\n+                              '`image_dim_ordering=\"tf\"` in '\n+                              'your Keras config '\n+                              'at ~/.keras/keras.json.')\n+                convert_all_kernels_in_model(model)\n+        else:\n+            if include_top:\n+                weights_path = get_file('inception_v3_weights_tf_dim_ordering_tf_kernels.h5',\n+                                        TF_WEIGHTS_PATH,\n+                                        cache_subdir='models',\n+                                        md5_hash='fe114b3ff2ea4bf891e9353d1bbfb32f')\n+            else:\n+                weights_path = get_file('inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n+                                        TF_WEIGHTS_PATH_NO_TOP,\n+                                        cache_subdir='models',\n+                                        md5_hash='2f3609166de1d967d1a481094754f691')\n+            model.load_weights(weights_path)\n+            if K.backend() == 'theano':\n+                convert_all_kernels_in_model(model)\n+    return model\n+\n+\n+def preprocess_input(x):\n+    x /= 255.\n+    x -= 0.5\n+    x *= 2.\n+    return x\n\n@@ -0,0 +1,235 @@\n+# -*- coding: utf-8 -*-\n+'''ResNet50 model for Keras.\n+\n+# Reference:\n+\n+- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n+\n+Adapted from code contributed by BigMoyan.\n+'''\n+from __future__ import print_function\n+from __future__ import absolute_import\n+\n+import warnings\n+\n+from ..layers import merge, Input\n+from ..layers import Dense, Activation, Flatten\n+from ..layers import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n+from ..layers import BatchNormalization\n+from ..models import Model\n+from .. import backend as K\n+from ..utils.layer_utils import convert_all_kernels_in_model\n+from ..utils.data_utils import get_file\n+from .imagenet_utils import decode_predictions, preprocess_input\n+\n+\n+TH_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_th_dim_ordering_th_kernels.h5'\n+TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n+TH_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_th_dim_ordering_th_kernels_notop.h5'\n+TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n+\n+\n+def identity_block(input_tensor, kernel_size, filters, stage, block):\n+    '''The identity_block is the block that has no conv layer at shortcut\n+\n+    # Arguments\n+        input_tensor: input tensor\n+        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n+        filters: list of integers, the nb_filters of 3 conv layer at main path\n+        stage: integer, current stage label, used for generating layer names\n+        block: 'a','b'..., current block label, used for generating layer names\n+    '''\n+    nb_filter1, nb_filter2, nb_filter3 = filters\n+    if K.image_dim_ordering() == 'tf':\n+        bn_axis = 3\n+    else:\n+        bn_axis = 1\n+    conv_name_base = 'res' + str(stage) + block + '_branch'\n+    bn_name_base = 'bn' + str(stage) + block + '_branch'\n+\n+    x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\n+    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n+    x = Activation('relu')(x)\n+\n+    x = Convolution2D(nb_filter2, kernel_size, kernel_size,\n+                      border_mode='same', name=conv_name_base + '2b')(x)\n+    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n+    x = Activation('relu')(x)\n+\n+    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\n+    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n+\n+    x = merge([x, input_tensor], mode='sum')\n+    x = Activation('relu')(x)\n+    return x\n+\n+\n+def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n+    '''conv_block is the block that has a conv layer at shortcut\n+\n+    # Arguments\n+        input_tensor: input tensor\n+        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n+        filters: list of integers, the nb_filters of 3 conv layer at main path\n+        stage: integer, current stage label, used for generating layer names\n+        block: 'a','b'..., current block label, used for generating layer names\n+\n+    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n+    And the shortcut should have subsample=(2,2) as well\n+    '''\n+    nb_filter1, nb_filter2, nb_filter3 = filters\n+    if K.image_dim_ordering() == 'tf':\n+        bn_axis = 3\n+    else:\n+        bn_axis = 1\n+    conv_name_base = 'res' + str(stage) + block + '_branch'\n+    bn_name_base = 'bn' + str(stage) + block + '_branch'\n+\n+    x = Convolution2D(nb_filter1, 1, 1, subsample=strides,\n+                      name=conv_name_base + '2a')(input_tensor)\n+    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n+    x = Activation('relu')(x)\n+\n+    x = Convolution2D(nb_filter2, kernel_size, kernel_size, border_mode='same',\n+                      name=conv_name_base + '2b')(x)\n+    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n+    x = Activation('relu')(x)\n+\n+    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\n+    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n+\n+    shortcut = Convolution2D(nb_filter3, 1, 1, subsample=strides,\n+                             name=conv_name_base + '1')(input_tensor)\n+    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n+\n+    x = merge([x, shortcut], mode='sum')\n+    x = Activation('relu')(x)\n+    return x\n+\n+\n+def ResNet50(include_top=True, weights='imagenet',\n+             input_tensor=None):\n+    '''Instantiate the ResNet50 architecture,\n+    optionally loading weights pre-trained\n+    on ImageNet. Note that when using TensorFlow,\n+    for best performance you should set\n+    `image_dim_ordering=\"tf\"` in your Keras config\n+    at ~/.keras/keras.json.\n+\n+    The model and the weights are compatible with both\n+    TensorFlow and Theano. The dimension ordering\n+    convention used by the model is the one\n+    specified in your Keras config file.\n+\n+    # Arguments\n+        include_top: whether to include the 3 fully-connected\n+            layers at the top of the network.\n+        weights: one of `None` (random initialization)\n+            or \"imagenet\" (pre-training on ImageNet).\n+        input_tensor: optional Keras tensor (i.e. xput of `layers.Input()`)\n+            to use as image input for the model.\n+\n+    # Returns\n+        A Keras model instance.\n+    '''\n+    if weights not in {'imagenet', None}:\n+        raise ValueError('The `weights` argument should be either '\n+                         '`None` (random initialization) or `imagenet` '\n+                         '(pre-training on ImageNet).')\n+    # Determine proper input shape\n+    if K.image_dim_ordering() == 'th':\n+        if include_top:\n+            input_shape = (3, 224, 224)\n+        else:\n+            input_shape = (3, None, None)\n+    else:\n+        if include_top:\n+            input_shape = (224, 224, 3)\n+        else:\n+            input_shape = (None, None, 3)\n+\n+    if input_tensor is None:\n+        img_input = Input(shape=input_shape)\n+    else:\n+        if not K.is_keras_tensor(input_tensor):\n+            img_input = Input(tensor=input_tensor)\n+        else:\n+            img_input = input_tensor\n+    if K.image_dim_ordering() == 'tf':\n+        bn_axis = 3\n+    else:\n+        bn_axis = 1\n+\n+    x = ZeroPadding2D((3, 3))(img_input)\n+    x = Convolution2D(64, 7, 7, subsample=(2, 2), name='conv1')(x)\n+    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n+    x = Activation('relu')(x)\n+    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n+\n+    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n+    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n+    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n+\n+    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n+    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n+    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n+    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n+\n+    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n+    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n+    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n+    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n+    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n+    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n+\n+    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n+    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n+    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n+\n+    x = AveragePooling2D((7, 7), name='avg_pool')(x)\n+\n+    if include_top:\n+        x = Flatten()(x)\n+        x = Dense(1000, activation='softmax', name='fc1000')(x)\n+\n+    model = Model(img_input, x)\n+\n+    # load weights\n+    if weights == 'imagenet':\n+        if K.image_dim_ordering() == 'th':\n+            if include_top:\n+                weights_path = get_file('resnet50_weights_th_dim_ordering_th_kernels.h5',\n+                                        TH_WEIGHTS_PATH,\n+                                        cache_subdir='models',\n+                                        md5_hash='1c1f8f5b0c8ee28fe9d950625a230e1c')\n+            else:\n+                weights_path = get_file('resnet50_weights_th_dim_ordering_th_kernels_notop.h5',\n+                                        TH_WEIGHTS_PATH_NO_TOP,\n+                                        cache_subdir='models',\n+                                        md5_hash='f64f049c92468c9affcd44b0976cdafe')\n+            model.load_weights(weights_path)\n+            if K.backend() == 'tensorflow':\n+                warnings.warn('You are using the TensorFlow backend, yet you '\n+                              'are using the Theano '\n+                              'image dimension ordering convention '\n+                              '(`image_dim_ordering=\"th\"`). '\n+                              'For best performance, set '\n+                              '`image_dim_ordering=\"tf\"` in '\n+                              'your Keras config '\n+                              'at ~/.keras/keras.json.')\n+                convert_all_kernels_in_model(model)\n+        else:\n+            if include_top:\n+                weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels.h5',\n+                                        TF_WEIGHTS_PATH,\n+                                        cache_subdir='models',\n+                                        md5_hash='a7b3fe01876f51b976af0dea6bc144eb')\n+            else:\n+                weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n+                                        TF_WEIGHTS_PATH_NO_TOP,\n+                                        cache_subdir='models',\n+                                        md5_hash='a268eb855778b3df3c7506639542a6af')\n+            model.load_weights(weights_path)\n+            if K.backend() == 'theano':\n+                convert_all_kernels_in_model(model)\n+    return model\n\n@@ -0,0 +1,149 @@\n+# -*- coding: utf-8 -*-\n+'''VGG16 model for Keras.\n+\n+# Reference:\n+\n+- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n+\n+'''\n+from __future__ import print_function\n+from __future__ import absolute_import\n+\n+import warnings\n+\n+from ..models import Model\n+from ..layers import Flatten, Dense, Input\n+from ..layers import Convolution2D, MaxPooling2D\n+from ..utils.layer_utils import convert_all_kernels_in_model\n+from ..utils.data_utils import get_file\n+from .. import backend as K\n+from .imagenet_utils import decode_predictions, preprocess_input\n+\n+\n+TH_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels.h5'\n+TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n+TH_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels_notop.h5'\n+TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n+\n+\n+def VGG16(include_top=True, weights='imagenet',\n+          input_tensor=None):\n+    '''Instantiate the VGG16 architecture,\n+    optionally loading weights pre-trained\n+    on ImageNet. Note that when using TensorFlow,\n+    for best performance you should set\n+    `image_dim_ordering=\"tf\"` in your Keras config\n+    at ~/.keras/keras.json.\n+\n+    The model and the weights are compatible with both\n+    TensorFlow and Theano. The dimension ordering\n+    convention used by the model is the one\n+    specified in your Keras config file.\n+\n+    # Arguments\n+        include_top: whether to include the 3 fully-connected\n+            layers at the top of the network.\n+        weights: one of `None` (random initialization)\n+            or \"imagenet\" (pre-training on ImageNet).\n+        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n+            to use as image input for the model.\n+\n+    # Returns\n+        A Keras model instance.\n+    '''\n+    if weights not in {'imagenet', None}:\n+        raise ValueError('The `weights` argument should be either '\n+                         '`None` (random initialization) or `imagenet` '\n+                         '(pre-training on ImageNet).')\n+    # Determine proper input shape\n+    if K.image_dim_ordering() == 'th':\n+        if include_top:\n+            input_shape = (3, 224, 224)\n+        else:\n+            input_shape = (3, None, None)\n+    else:\n+        if include_top:\n+            input_shape = (224, 224, 3)\n+        else:\n+            input_shape = (None, None, 3)\n+\n+    if input_tensor is None:\n+        img_input = Input(shape=input_shape)\n+    else:\n+        if not K.is_keras_tensor(input_tensor):\n+            img_input = Input(tensor=input_tensor)\n+        else:\n+            img_input = input_tensor\n+    # Block 1\n+    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv1')(img_input)\n+    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv2')(x)\n+    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n+\n+    # Block 2\n+    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv1')(x)\n+    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv2')(x)\n+    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n+\n+    # Block 3\n+    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv1')(x)\n+    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv2')(x)\n+    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv3')(x)\n+    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n+\n+    # Block 4\n+    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv1')(x)\n+    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv2')(x)\n+    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv3')(x)\n+    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n+\n+    # Block 5\n+    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv1')(x)\n+    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv2')(x)\n+    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv3')(x)\n+    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n+\n+    if include_top:\n+        # Classification block\n+        x = Flatten(name='flatten')(x)\n+        x = Dense(4096, activation='relu', name='fc1')(x)\n+        x = Dense(4096, activation='relu', name='fc2')(x)\n+        x = Dense(1000, activation='softmax', name='predictions')(x)\n+\n+    # Create model\n+    model = Model(img_input, x)\n+\n+    # load weights\n+    if weights == 'imagenet':\n+        if K.image_dim_ordering() == 'th':\n+            if include_top:\n+                weights_path = get_file('vgg16_weights_th_dim_ordering_th_kernels.h5',\n+                                        TH_WEIGHTS_PATH,\n+                                        cache_subdir='models')\n+            else:\n+                weights_path = get_file('vgg16_weights_th_dim_ordering_th_kernels_notop.h5',\n+                                        TH_WEIGHTS_PATH_NO_TOP,\n+                                        cache_subdir='models')\n+            model.load_weights(weights_path)\n+            if K.backend() == 'tensorflow':\n+                warnings.warn('You are using the TensorFlow backend, yet you '\n+                              'are using the Theano '\n+                              'image dimension ordering convention '\n+                              '(`image_dim_ordering=\"th\"`). '\n+                              'For best performance, set '\n+                              '`image_dim_ordering=\"tf\"` in '\n+                              'your Keras config '\n+                              'at ~/.keras/keras.json.')\n+                convert_all_kernels_in_model(model)\n+        else:\n+            if include_top:\n+                weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n+                                        TF_WEIGHTS_PATH,\n+                                        cache_subdir='models')\n+            else:\n+                weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n+                                        TF_WEIGHTS_PATH_NO_TOP,\n+                                        cache_subdir='models')\n+            model.load_weights(weights_path)\n+            if K.backend() == 'theano':\n+                convert_all_kernels_in_model(model)\n+    return model\n\n@@ -0,0 +1,152 @@\n+# -*- coding: utf-8 -*-\n+'''VGG19 model for Keras.\n+\n+# Reference:\n+\n+- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n+\n+'''\n+from __future__ import print_function\n+from __future__ import absolute_import\n+\n+import warnings\n+\n+from ..models import Model\n+from ..layers import Flatten, Dense, Input\n+from ..layers import Convolution2D, MaxPooling2D\n+from ..utils.layer_utils import convert_all_kernels_in_model\n+from ..utils.data_utils import get_file\n+from .. import backend as K\n+from .imagenet_utils import decode_predictions, preprocess_input\n+\n+\n+TH_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_th_dim_ordering_th_kernels.h5'\n+TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5'\n+TH_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_th_dim_ordering_th_kernels_notop.h5'\n+TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n+\n+\n+def VGG19(include_top=True, weights='imagenet',\n+          input_tensor=None):\n+    '''Instantiate the VGG19 architecture,\n+    optionally loading weights pre-trained\n+    on ImageNet. Note that when using TensorFlow,\n+    for best performance you should set\n+    `image_dim_ordering=\"tf\"` in your Keras config\n+    at ~/.keras/keras.json.\n+\n+    The model and the weights are compatible with both\n+    TensorFlow and Theano. The dimension ordering\n+    convention used by the model is the one\n+    specified in your Keras config file.\n+\n+    # Arguments\n+        include_top: whether to include the 3 fully-connected\n+            layers at the top of the network.\n+        weights: one of `None` (random initialization)\n+            or \"imagenet\" (pre-training on ImageNet).\n+        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n+            to use as image input for the model.\n+\n+    # Returns\n+        A Keras model instance.\n+    '''\n+    if weights not in {'imagenet', None}:\n+        raise ValueError('The `weights` argument should be either '\n+                         '`None` (random initialization) or `imagenet` '\n+                         '(pre-training on ImageNet).')\n+    # Determine proper input shape\n+    if K.image_dim_ordering() == 'th':\n+        if include_top:\n+            input_shape = (3, 224, 224)\n+        else:\n+            input_shape = (3, None, None)\n+    else:\n+        if include_top:\n+            input_shape = (224, 224, 3)\n+        else:\n+            input_shape = (None, None, 3)\n+\n+    if input_tensor is None:\n+        img_input = Input(shape=input_shape)\n+    else:\n+        if not K.is_keras_tensor(input_tensor):\n+            img_input = Input(tensor=input_tensor)\n+        else:\n+            img_input = input_tensor\n+    # Block 1\n+    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv1')(img_input)\n+    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv2')(x)\n+    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n+\n+    # Block 2\n+    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv1')(x)\n+    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv2')(x)\n+    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n+\n+    # Block 3\n+    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv1')(x)\n+    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv2')(x)\n+    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv3')(x)\n+    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv4')(x)\n+    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n+\n+    # Block 4\n+    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv1')(x)\n+    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv2')(x)\n+    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv3')(x)\n+    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv4')(x)\n+    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n+\n+    # Block 5\n+    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv1')(x)\n+    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv2')(x)\n+    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv3')(x)\n+    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv4')(x)\n+    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n+\n+    if include_top:\n+        # Classification block\n+        x = Flatten(name='flatten')(x)\n+        x = Dense(4096, activation='relu', name='fc1')(x)\n+        x = Dense(4096, activation='relu', name='fc2')(x)\n+        x = Dense(1000, activation='softmax', name='predictions')(x)\n+\n+    # Create model\n+    model = Model(img_input, x)\n+\n+    # load weights\n+    if weights == 'imagenet':\n+        if K.image_dim_ordering() == 'th':\n+            if include_top:\n+                weights_path = get_file('vgg19_weights_th_dim_ordering_th_kernels.h5',\n+                                        TH_WEIGHTS_PATH,\n+                                        cache_subdir='models')\n+            else:\n+                weights_path = get_file('vgg19_weights_th_dim_ordering_th_kernels_notop.h5',\n+                                        TH_WEIGHTS_PATH_NO_TOP,\n+                                        cache_subdir='models')\n+            model.load_weights(weights_path)\n+            if K.backend() == 'tensorflow':\n+                warnings.warn('You are using the TensorFlow backend, yet you '\n+                              'are using the Theano '\n+                              'image dimension ordering convention '\n+                              '(`image_dim_ordering=\"th\"`). '\n+                              'For best performance, set '\n+                              '`image_dim_ordering=\"tf\"` in '\n+                              'your Keras config '\n+                              'at ~/.keras/keras.json.')\n+                convert_all_kernels_in_model(model)\n+        else:\n+            if include_top:\n+                weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels.h5',\n+                                        TF_WEIGHTS_PATH,\n+                                        cache_subdir='models')\n+            else:\n+                weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n+                                        TF_WEIGHTS_PATH_NO_TOP,\n+                                        cache_subdir='models')\n+            model.load_weights(weights_path)\n+            if K.backend() == 'theano':\n+                convert_all_kernels_in_model(model)\n+    return model\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
