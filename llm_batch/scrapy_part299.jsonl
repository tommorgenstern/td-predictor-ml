{"custom_id": "scrapy#7b1ad995a4996babf9a019815dd7256a1cbfa044", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 106 | Lines Deleted: 57 | Files Changed: 5 | Hunks: 33 | Methods Changed: 31 | Complexity Δ (Sum/Max): 12/10 | Churn Δ: 163 | Churn Cumulative: 3252 | Contributors (this commit): 24 | Commits (past 90d): 48 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -122,6 +122,9 @@ class H2ClientProtocol(Protocol):\n         self.transport.write(data)\n \n     def request(self, request: Request):\n+        if not isinstance(request, Request):\n+            raise TypeError(f'Expected type scrapy.http.Request but received {request.__class__.__name__}')\n+\n         stream = self._new_stream(request)\n         d = stream.get_response()\n \n@@ -134,9 +137,7 @@ class H2ClientProtocol(Protocol):\n         sending some data now: we should open with the connection preamble.\n         \"\"\"\n         self.destination = self.transport.getPeer()\n-        logger.info('Connection made to {}'.format(self.destination))\n-\n-        self._metadata['certificate'] = Certificate(self.transport.getPeerCertificate())\n+        logger.info(f'Connection made to {self.destination}')\n         self._metadata['ip_address'] = ipaddress.ip_address(self.destination.host)\n \n         self.conn.initiate_connection()\n@@ -200,7 +201,7 @@ class H2ClientProtocol(Protocol):\n             elif isinstance(event, SettingsAcknowledged):\n                 self.settings_acknowledged(event)\n             else:\n-                logger.debug(\"Received unhandled event {}\".format(event))\n+                logger.debug(f'Received unhandled event {event}')\n \n     # Event handler functions starts here\n     def data_received(self, event: DataReceived):\n@@ -214,6 +215,9 @@ class H2ClientProtocol(Protocol):\n         # established a proper HTTP/2 connection\n         self._send_pending_requests()\n \n+        # Update certificate when our HTTP/2 connection is established\n+        self._metadata['certificate'] = Certificate(self.transport.getPeerCertificate())\n+\n     def stream_ended(self, event: StreamEnded):\n         self.streams[event.stream_id].close(StreamCloseReason.ENDED)\n \n\n@@ -131,7 +131,7 @@ class Stream:\n         self._deferred_response = Deferred(_cancel)\n \n     def __str__(self):\n-        return \"Stream(id={})\".format(repr(self.stream_id))\n+        return f'Stream(id={self.stream_id})'\n \n     __repr__ = __str__\n \n@@ -167,13 +167,15 @@ class Stream:\n         url = urlparse(self._request.url)\n \n         # Make sure pseudo-headers comes before all the other headers\n+        path = url.path\n+        if url.query:\n+            path += '?' + url.query\n+\n         headers = [\n             (':method', self._request.method),\n             (':authority', url.netloc),\n-\n-            # TODO: Check if scheme can be 'http' for HTTP/2 ?\n             (':scheme', 'https'),\n-            (':path', url.path),\n+            (':path', path),\n         ]\n \n         for name, value in self._request.headers.items():\n@@ -253,14 +255,9 @@ class Stream:\n \n         if self._log_warnsize:\n             self._reached_warnsize = True\n-            warning_msg = 'Received more ({bytes}) bytes than download ' \\\n-                          + 'warn size ({warnsize}) in request {request}'\n-            warning_args = {\n-                'bytes': self._response['flow_controlled_size'],\n-                'warnsize': self._download_warnsize,\n-                'request': self._request\n-            }\n-            logger.warning(warning_msg.format(**warning_args))\n+            warning_msg = f\"Received more ({self._response['flow_controlled_size']}) bytes than download \" \\\n+                          + f'warn size ({self._download_warnsize}) in request {self._request}'\n+            logger.warning(warning_msg)\n \n         # Acknowledge the data received\n         self._conn.acknowledge_received_data(\n@@ -280,14 +277,9 @@ class Stream:\n \n         if self._log_warnsize:\n             self._reached_warnsize = True\n-            warning_msg = 'Expected response size ({size}) larger than ' \\\n-                          + 'download warn size ({warnsize}) in request {request}'\n-            warning_args = {\n-                'size': expected_size,\n-                'warnsize': self._download_warnsize,\n-                'request': self._request\n-            }\n-            logger.warning(warning_msg.format(**warning_args))\n+            warning_msg = f'Expected response size ({expected_size}) larger than ' \\\n+                          + f'download warn size ({self._download_warnsize}) in request {self._request}'\n+            logger.warning(warning_msg)\n \n     def reset_stream(self, reason=StreamCloseReason.RESET):\n         \"\"\"Close this stream by sending a RST_FRAME to the remote peer\"\"\"\n@@ -332,16 +324,10 @@ class Stream:\n         # having Content-Length)\n         if reason is StreamCloseReason.MAXSIZE_EXCEEDED:\n             expected_size = int(self._response['headers'].get(b'Content-Length', -1))\n-            error_msg = (\"Cancelling download of {url}: expected response \"\n-                         \"size ({size}) larger than download max size ({maxsize}).\")\n-            error_args = {\n-                'url': self._request.url,\n-                'size': expected_size,\n-                'maxsize': self._download_maxsize\n-            }\n-\n-            logger.error(error_msg, error_args)\n-            self._deferred_response.errback(CancelledError(error_msg.format(**error_args)))\n+            error_msg = f'Cancelling download of {self._request.url}: expected response ' \\\n+                        f'size ({expected_size}) larger than download max size ({self._download_maxsize}).'\n+            logger.error(error_msg)\n+            self._deferred_response.errback(CancelledError(error_msg))\n \n         elif reason is StreamCloseReason.ENDED:\n             self._fire_response_deferred(flags)\n\n@@ -1,6 +1,6 @@\n from io import BytesIO\n from ipaddress import IPv4Address, IPv6Address\n-from typing import Union\n+from typing import Union, Optional\n \n from twisted.internet.ssl import Certificate\n # for python < 3.8 -- typing.TypedDict is undefined\n@@ -13,8 +13,8 @@ class H2ConnectionMetadataDict(TypedDict):\n     \"\"\"Some meta data of this connection\n     initialized when connection is successfully made\n     \"\"\"\n-    certificate: Union[None, Certificate]\n-    ip_address: Union[None, IPv4Address, IPv6Address]\n+    certificate: Optional[Certificate]\n+    ip_address: Optional[Union[IPv4Address, IPv6Address]]\n \n \n class H2ResponseDict(TypedDict):\n\n@@ -1,8 +1,8 @@\n from os.path import dirname, join\n-\n from pkg_resources import parse_version\n from setuptools import setup, find_packages, __version__ as setuptools_version\n \n+\n with open(join(dirname(__file__), 'scrapy/VERSION'), 'rb') as f:\n     version = f.read().decode('ascii').strip()\n \n@@ -25,6 +25,7 @@ if has_environment_marker_platform_impl_support():\n         'PyPyDispatcher>=2.1.0',\n     ]\n \n+\n setup(\n     name='Scrapy',\n     version=version,\n\n@@ -4,13 +4,15 @@ import random\n import re\n import shutil\n import string\n+from ipaddress import IPv4Address\n+from urllib.parse import urlencode\n \n from h2.exceptions import InvalidBodyLengthError\n from twisted.internet import reactor\n from twisted.internet.defer import inlineCallbacks, DeferredList, CancelledError\n from twisted.internet.endpoints import SSL4ClientEndpoint, SSL4ServerEndpoint, TCP4ServerEndpoint\n from twisted.internet.protocol import Factory\n-from twisted.internet.ssl import optionsForClientTLS, PrivateCertificate\n+from twisted.internet.ssl import optionsForClientTLS, PrivateCertificate, Certificate\n from twisted.python.failure import Failure\n from twisted.trial.unittest import TestCase\n from twisted.web.http import Request as TxRequest\n@@ -21,7 +23,7 @@ from scrapy.core.http2.protocol import H2ClientProtocol\n from scrapy.core.http2.stream import InactiveStreamClosed\n from scrapy.http import Request, Response, JsonRequest\n from scrapy.utils.python import to_bytes, to_unicode\n-from tests.mockserver import ssl_context_factory, LeafResource\n+from tests.mockserver import ssl_context_factory, LeafResource, Status\n \n \n def generate_random_string(size):\n@@ -32,10 +34,10 @@ def generate_random_string(size):\n \n \n def make_html_body(val):\n-    response = '''<html>\n+    response = f'''<html>\n <h1>Hello from HTTP2<h1>\n-<p>{}</p>\n-</html>'''.format(val)\n+<p>{val}</p>\n+</html>'''\n     return to_bytes(response)\n \n \n@@ -122,6 +124,17 @@ class NoContentLengthHeader(LeafResource):\n         request.finish()\n \n \n+class QueryParams(LeafResource):\n+    def render_GET(self, request: TxRequest):\n+        request.setHeader('Content-Type', 'application/json')\n+\n+        query_params = {}\n+        for k, v in request.args.items():\n+            query_params[to_unicode(k)] = to_unicode(v[0])\n+\n+        return to_bytes(json.dumps(query_params))\n+\n+\n def get_client_certificate(key_file, certificate_file):\n     with open(key_file, 'r') as key, open(certificate_file, 'r') as certificate:\n         pem = ''.join(key.readlines()) + ''.join(certificate.readlines())\n@@ -146,6 +159,8 @@ class Https2ClientProtocolTestCase(TestCase):\n \n         r.putChild(b'dataloss', Dataloss())\n         r.putChild(b'no-content-length-header', NoContentLengthHeader())\n+        r.putChild(b'status', Status())\n+        r.putChild(b'query-params', QueryParams())\n         return r\n \n     @inlineCallbacks\n@@ -189,7 +204,7 @@ class Https2ClientProtocolTestCase(TestCase):\n         :return: Complete url\n         \"\"\"\n         assert len(path) > 0 and (path[0] == '/' or path[0] == '&')\n-        return \"{}://{}:{}{}\".format(self.scheme, self.hostname, self.port_number, path)\n+        return f'{self.scheme}://{self.hostname}:{self.port_number}{path}'\n \n     @staticmethod\n     def _check_repeat(get_deferred, count):\n@@ -210,7 +225,6 @@ class Https2ClientProtocolTestCase(TestCase):\n             self.assertEqual(response.status, expected_status)\n             self.assertEqual(response.body, expected_body)\n             self.assertEqual(response.request, request)\n-            self.assertEqual(response.url, request.url)\n \n             content_length = int(response.headers.get('Content-Length'))\n             self.assertEqual(len(response.body), content_length)\n@@ -260,7 +274,6 @@ class Https2ClientProtocolTestCase(TestCase):\n         def assert_response(response: Response):\n             self.assertEqual(response.status, expected_status)\n             self.assertEqual(response.request, request)\n-            self.assertEqual(response.url, request.url)\n \n             content_length = int(response.headers.get('Content-Length'))\n             self.assertEqual(len(response.body), content_length)\n@@ -336,7 +349,6 @@ class Https2ClientProtocolTestCase(TestCase):\n         def assert_response(response: Response):\n             self.assertEqual(response.status, 499)\n             self.assertEqual(response.request, request)\n-            self.assertEqual(response.url, request.url)\n \n         d = self.client.request(request)\n         d.addCallback(assert_response)\n@@ -356,10 +368,6 @@ class Https2ClientProtocolTestCase(TestCase):\n         d.addErrback(assert_cancelled_error)\n         return d\n \n-    # TODO: Test in multiple requests if one request fails due to dataloss\n-    #  remaining request do not fail (change expected behaviour)\n-    #  Can be done only when hyper-h2 don't terminate connection over\n-    #  InvalidBodyLengthError check\n     def test_received_dataloss_response(self):\n         \"\"\"In case when value of Header Content-Length != len(Received Data)\n         ProtocolError is raised\"\"\"\n@@ -384,7 +392,6 @@ class Https2ClientProtocolTestCase(TestCase):\n             self.assertEqual(response.status, 200)\n             self.assertEqual(response.body, Data.NO_CONTENT_LENGTH)\n             self.assertEqual(response.request, request)\n-            self.assertEqual(response.url, request.url)\n             self.assertIn('partial', response.flags)\n             self.assertNotIn('Content-Length', response.headers)\n \n@@ -404,7 +411,6 @@ class Https2ClientProtocolTestCase(TestCase):\n             response = yield self.client.request(request)\n             self.assertEqual(response.status, 200)\n             self.assertEqual(response.request, request)\n-            self.assertEqual(response.url, request.url)\n             self.assertEqual(response.body, expected_body)\n \n             # Check the warning is raised only once for this request\n@@ -417,8 +423,8 @@ class Https2ClientProtocolTestCase(TestCase):\n     def test_log_expected_warnsize(self):\n         request = Request(url=self.get_url('/get-data-html-large'), meta={'download_warnsize': 1000})\n         warn_pattern = re.compile(\n-            r'Expected response size \\(\\d*\\) larger than '\n-            r'download warn size \\(1000\\) in request {}'.format(request)\n+            rf'Expected response size \\(\\d*\\) larger than '\n+            rf'download warn size \\(1000\\) in request {request}'\n         )\n \n         yield self._check_log_warnsize(request, warn_pattern, Data.HTML_LARGE)\n@@ -427,8 +433,8 @@ class Https2ClientProtocolTestCase(TestCase):\n     def test_log_received_warnsize(self):\n         request = Request(url=self.get_url('/no-content-length-header'), meta={'download_warnsize': 10})\n         warn_pattern = re.compile(\n-            r'Received more \\(\\d*\\) bytes than download '\n-            r'warn size \\(10\\) in request {}'.format(request)\n+            rf'Received more \\(\\d*\\) bytes than download '\n+            rf'warn size \\(10\\) in request {request}'\n         )\n \n         yield self._check_log_warnsize(request, warn_pattern, Data.NO_CONTENT_LENGTH)\n@@ -474,3 +480,55 @@ class Https2ClientProtocolTestCase(TestCase):\n         self.client.transport.abortConnection()\n \n         return DeferredList(d_list, consumeErrors=True, fireOnOneErrback=True)\n+\n+    def test_invalid_request_type(self):\n+        with self.assertRaises(TypeError):\n+            self.client.request('https://InvalidDataTypePassed.com')\n+\n+    def test_query_parameters(self):\n+        params = {\n+            'a': generate_random_string(20),\n+            'b': generate_random_string(20),\n+            'c': generate_random_string(20),\n+            'd': generate_random_string(20)\n+        }\n+        request = Request(self.get_url(f'/query-params?{urlencode(params)}'))\n+\n+        def assert_query_params(response: Response):\n+            data = json.loads(to_unicode(response.body))\n+            self.assertEqual(data, params)\n+\n+        d = self.client.request(request)\n+        d.addCallback(assert_query_params)\n+        d.addErrback(self.fail)\n+\n+        return d\n+\n+    def test_status_codes(self):\n+        def assert_response_status(response: Response, expected_status: int):\n+            self.assertEqual(response.status, expected_status)\n+\n+        d_list = []\n+        for status in [200, 404]:\n+            request = Request(self.get_url(f'/status?n={status}'))\n+            d = self.client.request(request)\n+            d.addCallback(assert_response_status, status)\n+            d.addErrback(self.fail)\n+            d_list.append(d)\n+\n+        return DeferredList(d_list, fireOnOneErrback=True)\n+\n+    def test_response_has_correct_certificate_ip_address(self):\n+        request = Request(self.get_url('/status?n=200'))\n+\n+        def assert_metadata(response: Response):\n+            self.assertEqual(response.request, request)\n+            self.assertIsInstance(response.certificate, Certificate)\n+            self.assertIsInstance(response.ip_address, IPv4Address)\n+            self.assertEqual(str(response.ip_address), '127.0.0.1')\n+\n+        d = self.client.request(request)\n+        d.addCallback(assert_metadata)\n+        d.addErrback(self.fail)\n+\n+        return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7fc80671a84144ad36c3d5332aa822ef3be00781", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 15 | Churn Cumulative: 2592 | Contributors (this commit): 29 | Commits (past 90d): 10 | Contributors (cumulative): 36 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -7,7 +7,7 @@ import warnings\n from contextlib import suppress\n from io import BytesIO\n from time import time\n-from urllib.parse import urldefrag, urlparse\n+from urllib.parse import urldefrag, urlunparse\n \n from twisted.internet import defer, protocol, ssl\n from twisted.internet.endpoints import TCP4ClientEndpoint\n@@ -255,7 +255,7 @@ class ScrapyProxyAgent(Agent):\n             bindAddress=bindAddress,\n             pool=pool,\n         )\n-        self._proxyURI = URI.fromBytes(urlparse(proxyURI)._replace(scheme=b'http').geturl())\n+        self._proxyURI = URI.fromBytes(proxyURI)\n \n     def request(self, method, uri, headers=None, bodyProducer=None):\n         \"\"\"\n@@ -297,7 +297,7 @@ class ScrapyAgent:\n         bindaddress = request.meta.get('bindaddress') or self._bindAddress\n         proxy = request.meta.get('proxy')\n         if proxy:\n-            _, _, proxyHost, proxyPort, proxyParams = _parse(proxy)\n+            proxyScheme, proxyNetloc, proxyHost, proxyPort, proxyParams = _parse(proxy)\n             scheme = _parse(request.url)[0]\n             proxyHost = to_unicode(proxyHost)\n             omitConnectTunnel = b'noconnect' in proxyParams\n@@ -319,9 +319,13 @@ class ScrapyAgent:\n                     pool=self._pool,\n                 )\n             else:\n+                proxyScheme = b'http' if not proxyScheme else proxyScheme\n+                proxyHost = to_bytes(proxyHost, encoding='ascii')\n+                proxyPort = to_bytes(str(proxyPort), encoding='ascii')\n+                proxyURI = urlunparse((proxyScheme, proxyNetloc, proxyParams, '', '', ''))\n                 return self._ProxyAgent(\n                     reactor=reactor,\n-                    proxyURI=to_bytes(proxy, encoding='ascii'),\n+                    proxyURI=to_bytes(proxyURI, encoding='ascii'),\n                     connectTimeout=timeout,\n                     bindAddress=bindaddress,\n                     pool=self._pool,\n\n@@ -1,5 +1,6 @@\n from time import time\n from urllib.parse import urlparse, urlunparse, urldefrag\n+from re import match\n \n from twisted.web.client import HTTPClientFactory\n from twisted.web.http import HTTPClient\n@@ -32,6 +33,8 @@ def _parse(url):\n     and is ascii-only.\n     \"\"\"\n     url = url.strip()\n+    if not match(r'^\\w+://', url):\n+        url = '//' + url\n     parsed = urlparse(url)\n     return _parsed_url_args(parsed)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#006a945214422da22f645b4416263124377adc5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1566 | Contributors (this commit): 21 | Commits (past 90d): 5 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -741,7 +741,7 @@ class Http11ProxyTestCase(HttpProxyTestCase):\n             self.assertEqual(response.url, request.url)\n             self.assertEqual(response.body, b'http://example.com')\n \n-        http_proxy = self.getURL('').replace('http:', '')\n+        http_proxy = self.getURL('').replace('http://', '')\n         request = Request('http://example.com', meta={'proxy': http_proxy})\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7b1d3c35ea3bfde2ac7fc69a2a26bbcb94aec1bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 30 | Files Changed: 4 | Hunks: 23 | Methods Changed: 13 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 59 | Churn Cumulative: 3918 | Contributors (this commit): 30 | Commits (past 90d): 36 | Contributors (cumulative): 63 | DMM Complexity: 1.0\n\nDIFF:\n@@ -242,7 +242,6 @@ class FeedExporter:\n \n         self.storages = self._load_components('FEED_STORAGES')\n         self.exporters = self._load_components('FEED_EXPORTERS')\n-        self.storage_batch_item_count = self.settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n         for uri, feed in self.feeds.items():\n             if not self._storage_supported(uri):\n                 raise NotConfigured\n@@ -253,7 +252,7 @@ class FeedExporter:\n \n     def open_spider(self, spider):\n         for uri, feed in self.feeds.items():\n-            uri_params = self._get_uri_params(spider, feed['uri_params'], None)\n+            uri_params = self._get_uri_params(spider, feed['uri_params'])\n             self.slots.append(self._start_new_batch(\n                 batch_id=1,\n                 uri=uri % uri_params,\n@@ -299,7 +298,7 @@ class FeedExporter:\n     def _start_new_batch(self, batch_id, uri, feed, spider, uri_template):\n         \"\"\"\n         Redirect the output data stream to a new file.\n-        Execute multiple times if 'FEED_STORAGE_BATCH_ITEM_COUNT' setting is specified.\n+        Execute multiple times if FEED_STORAGE_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified\n         :param batch_id: sequence number of current batch\n         :param uri: uri of the new batch to start\n         :param feed: dict with parameters of feed\n@@ -331,14 +330,15 @@ class FeedExporter:\n \n     def item_scraped(self, item, spider):\n         slots = []\n-        for idx, slot in enumerate(self.slots):\n+        for slot in self.slots:\n             slot.start_exporting()\n             slot.exporter.export_item(item)\n             slot.itemcount += 1\n             # create new slot for each slot with itemcount == FEED_STORAGE_BATCH_ITEM_COUNT and close the old one\n-            if self.feeds[slot.uri_template].get('batch_item_count', self.storage_batch_item_count) \\\n-                    and slot.itemcount == self.feeds[slot.uri_template].get('batch_item_count',\n-                                                                            self.storage_batch_item_count):\n+            if (\n+                    self.feeds[slot.uri_template]['batch_item_count']\n+                    and slot.itemcount >= self.feeds[slot.uri_template]['batch_item_count']\n+            ):\n                 uri_params = self._get_uri_params(spider, self.feeds[slot.uri_template]['uri_params'], slot)\n                 self._close_slot(slot, spider)\n                 slots.append(self._start_new_batch(\n@@ -369,15 +369,17 @@ class FeedExporter:\n \n     def _settings_are_valid(self, uri):\n         \"\"\"\n-        If FEED_STORAGE_BATCH_ITEM_COUNT setting is specified uri has to contain %(batch_time)s or %(batch_id)s\n-        to distinguish different files of partial output\n+        If FEED_STORAGE_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified uri has to contain\n+        %(batch_time)s or %(batch_id)s to distinguish different files of partial output\n         \"\"\"\n-        if not self.storage_batch_item_count or '%(batch_time)s' in uri or '%(batch_id)s' in uri:\n-            return True\n+        for uri_template, values in self.feeds.items():\n+            if values['batch_item_count'] and not any(s in uri_template for s in ['%(batch_time)s', '%(batch_id)s']):\n                 logger.error(\n-            '%(batch_time)s or %(batch_id)s must be in uri if FEED_STORAGE_BATCH_ITEM_COUNT setting is specified'\n+                    '%(batch_time)s or %(batch_id)s must be in uri({}) if FEED_STORAGE_BATCH_ITEM_COUNT setting '\n+                    'or FEEDS.batch_item_count is specified and greater than 0.'.format(uri_template)\n                 )\n                 return False\n+        return True\n \n     def _storage_supported(self, uri):\n         scheme = urlparse(uri).scheme\n@@ -404,7 +406,7 @@ class FeedExporter:\n     def _get_storage(self, uri):\n         return self._get_instance(self.storages[urlparse(uri).scheme], uri)\n \n-    def _get_uri_params(self, spider, uri_params, slot):\n+    def _get_uri_params(self, spider, uri_params, slot=None):\n         params = {}\n         for k in dir(spider):\n             params[k] = getattr(spider, k)\n\n@@ -115,6 +115,10 @@ def feed_complete_default_values_from_settings(feed, settings):\n     out = feed.copy()\n     out.setdefault(\"encoding\", settings[\"FEED_EXPORT_ENCODING\"])\n     out.setdefault(\"fields\", settings.getlist(\"FEED_EXPORT_FIELDS\") or None)\n+    out.setdefault(\n+        \"batch_item_count\",\n+        out.get('batch_item_count', settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT'))\n+    )\n     out.setdefault(\"store_empty\", settings.getbool(\"FEED_STORE_EMPTY\"))\n     out.setdefault(\"uri_params\", settings[\"FEED_URI_PARAMS\"])\n     if settings[\"FEED_EXPORT_INDENT\"] is None:\n\n@@ -1265,7 +1265,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         yield self.assertExported(items, header, rows, settings=Settings(settings))\n \n     def test_wrong_path(self):\n-        \"\"\" If path is without %(batch_time)s or %(batch_id)s an exception must be raised \"\"\"\n+        \"\"\" If path is without %(batch_time)s and %(batch_id)s an exception must be raised \"\"\"\n         settings = {\n             'FEEDS': {\n                 self._random_temp_filename(): {'format': 'xml'},\n@@ -1329,7 +1329,6 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             ],\n             'csv': ['foo,bar\\r\\nFOO,BAR\\r\\n'.encode('utf-8'),\n                     'foo,bar\\r\\nFOO1,BAR1\\r\\n'.encode('utf-8')],\n-            'jsonlines': ['{\"foo\": \"FOO\", \"bar\": \"BAR\"}\\n{\"foo\": \"FOO1\", \"bar\": \"BAR1\"}\\n'.encode('utf-8')],\n         }\n \n         settings = {\n@@ -1352,13 +1351,6 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                     'fields': ['foo', 'bar'],\n                     'encoding': 'utf-8',\n                 },\n-                os.path.join(self._random_temp_filename(), 'jsonlines', self._file_mark): {\n-                    'format': 'jsonlines',\n-                    'indent': None,\n-                    'fields': ['foo', 'bar'],\n-                    'encoding': 'utf-8',\n-                    'batch_item_count': 0,\n-                },\n             },\n             'FEED_STORAGE_BATCH_ITEM_COUNT': 1,\n         }\n@@ -1369,19 +1361,16 @@ class BatchDeliveriesTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_batch_item_count_feeds_setting(self):\n-        items = [dict({'foo': u'FOO', 'bar': u'BAR'}), dict({'foo': u'FOO1', 'bar': u'BAR1'})]\n-\n+        items = [dict({'foo': u'FOO'}), dict({'foo': u'FOO1'})]\n         formats = {\n-            'jsonlines': ['{\"foo\": \"FOO\", \"bar\": \"BAR\"}\\n'.encode('utf-8'),\n-                          '{\"foo\": \"FOO1\", \"bar\": \"BAR1\"}\\n'.encode('utf-8')],\n+            'json': ['[{\"foo\": \"FOO\"}]'.encode('utf-8'),\n+                     '[{\"foo\": \"FOO1\"}]'.encode('utf-8')],\n         }\n-\n         settings = {\n             'FEEDS': {\n-                os.path.join(self._random_temp_filename(), 'jsonlines', self._file_mark): {\n-                    'format': 'jsonlines',\n+                os.path.join(self._random_temp_filename(), 'json', self._file_mark): {\n+                    'format': 'json',\n                     'indent': None,\n-                    'fields': ['foo', 'bar'],\n                     'encoding': 'utf-8',\n                     'batch_item_count': 1,\n                 },\n\n@@ -149,6 +149,7 @@ class FeedExportConfigTestCase(unittest.TestCase):\n             \"FEED_EXPORT_INDENT\": 42,\n             \"FEED_STORE_EMPTY\": True,\n             \"FEED_URI_PARAMS\": (1, 2, 3, 4),\n+            \"FEED_STORAGE_BATCH_ITEM_COUNT\": 2,\n         })\n         new_feed = feed_complete_default_values_from_settings(feed, settings)\n         self.assertEqual(new_feed, {\n@@ -157,6 +158,7 @@ class FeedExportConfigTestCase(unittest.TestCase):\n             \"indent\": 42,\n             \"store_empty\": True,\n             \"uri_params\": (1, 2, 3, 4),\n+            \"batch_item_count\": 2,\n         })\n \n     def test_feed_complete_default_values_from_settings_non_empty(self):\n@@ -169,6 +171,7 @@ class FeedExportConfigTestCase(unittest.TestCase):\n             \"FEED_EXPORT_FIELDS\": [\"f1\", \"f2\", \"f3\"],\n             \"FEED_EXPORT_INDENT\": 42,\n             \"FEED_STORE_EMPTY\": True,\n+            \"FEED_STORAGE_BATCH_ITEM_COUNT\": 2,\n         })\n         new_feed = feed_complete_default_values_from_settings(feed, settings)\n         self.assertEqual(new_feed, {\n@@ -177,6 +180,7 @@ class FeedExportConfigTestCase(unittest.TestCase):\n             \"indent\": 42,\n             \"store_empty\": True,\n             \"uri_params\": None,\n+            \"batch_item_count\": 2,\n         })\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#065b9b1170fe249fbfce51c87631e5572127df21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 454 | Contributors (this commit): 12 | Commits (past 90d): 4 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,6 @@\n+import re\n from time import time\n from urllib.parse import urlparse, urlunparse, urldefrag\n-from re import match\n \n from twisted.web.client import HTTPClientFactory\n from twisted.web.http import HTTPClient\n@@ -33,7 +33,7 @@ def _parse(url):\n     and is ascii-only.\n     \"\"\"\n     url = url.strip()\n-    if not match(r'^\\w+://', url):\n+    if not re.match(r'^\\w+://', url):\n         url = '//' + url\n     parsed = urlparse(url)\n     return _parsed_url_args(parsed)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c361fe0d3b80ff8a9f88adc05e730d5e469db225", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 112 | Lines Deleted: 27 | Files Changed: 4 | Hunks: 30 | Methods Changed: 21 | Complexity Δ (Sum/Max): 12/8 | Churn Δ: 139 | Churn Cumulative: 2860 | Contributors (this commit): 1 | Commits (past 90d): 45 | Contributors (cumulative): 4 | DMM Complexity: 0.8596491228070176\n\nDIFF:\n@@ -2,7 +2,7 @@ import ipaddress\n import itertools\n import logging\n from collections import deque\n-from typing import Union, Dict\n+from typing import Dict, Optional\n \n from h2.config import H2Configuration\n from h2.connection import H2Connection\n@@ -26,10 +26,6 @@ class H2ClientProtocol(Protocol):\n         config = H2Configuration(client_side=True, header_encoding='utf-8')\n         self.conn = H2Connection(config=config)\n \n-        # Address of the server we are connected to\n-        # these are updated when connection is successfully made\n-        self.destination = None\n-\n         # ID of the next request stream\n         # Following the convention made by hyper-h2 each client ID\n         # will be odd.\n@@ -50,11 +46,13 @@ class H2ClientProtocol(Protocol):\n \n         # Save an instance of ProtocolError raised by hyper-h2\n         # We pass this instance to the streams ResponseFailed() failure\n-        self._protocol_error: Union[None, ProtocolError] = None\n+        self._protocol_error: Optional[ProtocolError] = None\n \n         self._metadata: H2ConnectionMetadataDict = {\n             'certificate': None,\n-            'ip_address': None\n+            'ip_address': None,\n+            'hostname': None,\n+            'port': None\n         }\n \n     @property\n@@ -123,7 +121,7 @@ class H2ClientProtocol(Protocol):\n \n     def request(self, request: Request):\n         if not isinstance(request, Request):\n-            raise TypeError(f'Expected type scrapy.http.Request but received {request.__class__.__name__}')\n+            raise TypeError(f'Expected scrapy.http.Request, received {request.__class__.__name__}')\n \n         stream = self._new_stream(request)\n         d = stream.get_response()\n@@ -136,9 +134,11 @@ class H2ClientProtocol(Protocol):\n         \"\"\"Called by Twisted when the connection is established. We can start\n         sending some data now: we should open with the connection preamble.\n         \"\"\"\n-        self.destination = self.transport.getPeer()\n-        logger.info(f'Connection made to {self.destination}')\n-        self._metadata['ip_address'] = ipaddress.ip_address(self.destination.host)\n+        destination = self.transport.getPeer()\n+        logger.debug('Connection made to {}'.format(destination))\n+        self._metadata['ip_address'] = ipaddress.ip_address(destination.host)\n+        self._metadata['port'] = destination.port\n+        self._metadata['hostname'] = self.transport.transport.addr[0]\n \n         self.conn.initiate_connection()\n         self._write_to_transport()\n@@ -148,8 +148,6 @@ class H2ClientProtocol(Protocol):\n             events = self.conn.receive_data(data)\n             self._handle_events(events)\n         except ProtocolError as e:\n-            # TODO: In case of InvalidBodyLengthError -- terminate only one stream\n-\n             # Save this error as ultimately the connection will be dropped\n             # internally by hyper-h2. Saved error will be passed to all the streams\n             # closed with the connection.\n@@ -201,7 +199,7 @@ class H2ClientProtocol(Protocol):\n             elif isinstance(event, SettingsAcknowledged):\n                 self.settings_acknowledged(event)\n             else:\n-                logger.debug(f'Received unhandled event {event}')\n+                logger.debug('Received unhandled event {}'.format(event))\n \n     # Event handler functions starts here\n     def data_received(self, event: DataReceived):\n\n@@ -29,6 +29,17 @@ class InactiveStreamClosed(ConnectionClosed):\n         self.request = request\n \n \n+class InvalidHostname(Exception):\n+\n+    def __init__(self, request: Request, expected_hostname, expected_netloc):\n+        self.request = request\n+        self.expected_hostname = expected_hostname\n+        self.expected_netloc = expected_netloc\n+\n+    def __str__(self):\n+        return f'InvalidHostname: Expected {self.expected_hostname} or {self.expected_netloc} in {self.request}'\n+\n+\n class StreamCloseReason(Enum):\n     # Received a StreamEnded event\n     ENDED = 1\n@@ -49,6 +60,10 @@ class StreamCloseReason(Enum):\n     # Connection lost and the stream was not initiated\n     INACTIVE = 6\n \n+    # The hostname of the request is not same as of connected peer hostname\n+    # As a result sending this request will the end the connection\n+    INVALID_HOSTNAME = 7\n+\n \n class Stream:\n     \"\"\"Represents a single HTTP/2 Stream.\n@@ -163,6 +178,15 @@ class Stream:\n         \"\"\"\n         return self._deferred_response\n \n+    def check_request_url(self) -> bool:\n+        # Make sure that we are sending the request to the correct URL\n+        url = urlparse(self._request.url)\n+        return (\n+            url.netloc == self._conn_metadata['hostname']\n+            or url.netloc == f'{self._conn_metadata[\"hostname\"]}:{self._conn_metadata[\"port\"]}'\n+            or url.netloc == f'{self._conn_metadata[\"ip_address\"]}:{self._conn_metadata[\"port\"]}'\n+        )\n+\n     def _get_request_headers(self):\n         url = urlparse(self._request.url)\n \n@@ -184,10 +208,15 @@ class Stream:\n         return headers\n \n     def initiate_request(self):\n+        if self.check_request_url():\n             headers = self._get_request_headers()\n             self._conn.send_headers(self.stream_id, headers, end_stream=False)\n             self.request_sent = True\n             self.send_data()\n+        else:\n+            # Close this stream calling the response errback\n+            # Note that we have not sent any headers\n+            self.close(StreamCloseReason.INVALID_HOSTNAME)\n \n     def send_data(self):\n         \"\"\"Called immediately after the headers are sent. Here we send all the\n@@ -310,6 +339,9 @@ class Stream:\n         if self.stream_closed_server:\n             raise StreamClosedError(self.stream_id)\n \n+        if not isinstance(reason, StreamCloseReason):\n+            raise TypeError(f'Expected StreamCloseReason, received {reason.__class__.__name__}')\n+\n         self._cb_close(self.stream_id)\n         self.stream_closed_server = True\n \n@@ -353,6 +385,13 @@ class Stream:\n         elif reason is StreamCloseReason.INACTIVE:\n             self._deferred_response.errback(InactiveStreamClosed(self._request))\n \n+        elif reason is StreamCloseReason.INVALID_HOSTNAME:\n+            self._deferred_response.errback(InvalidHostname(\n+                self._request,\n+                self._conn_metadata['hostname'],\n+                f'{self._conn_metadata[\"ip_address\"]}:{self._conn_metadata[\"port\"]}'\n+            ))\n+\n     def _fire_response_deferred(self, flags: List[str] = None):\n         \"\"\"Builds response from the self._response dict\n         and fires the response deferred callback with the\n@@ -365,10 +404,9 @@ class Stream:\n             body=body\n         )\n \n-        status = self._response['headers'][':status']\n         response = response_cls(\n             url=self._request.url,\n-            status=status,\n+            status=self._response['headers'][':status'],\n             headers=self._response['headers'],\n             body=body,\n             request=self._request,\n\n@@ -14,8 +14,19 @@ class H2ConnectionMetadataDict(TypedDict):\n     initialized when connection is successfully made\n     \"\"\"\n     certificate: Optional[Certificate]\n+\n+    # Address of the server we are connected to which\n+    # is updated when HTTP/2 connection is  made successfully\n     ip_address: Optional[Union[IPv4Address, IPv6Address]]\n \n+    # Name of the peer HTTP/2 connection is established\n+    hostname: Optional[str]\n+\n+    port: Optional[int]\n+\n+    # Both ip_address and hostname are used by the Stream before\n+    # initiating the request to verify that the base address\n+\n \n class H2ResponseDict(TypedDict):\n     # Data received frame by frame from the server is appended\n\n@@ -10,7 +10,7 @@ from urllib.parse import urlencode\n from h2.exceptions import InvalidBodyLengthError\n from twisted.internet import reactor\n from twisted.internet.defer import inlineCallbacks, DeferredList, CancelledError\n-from twisted.internet.endpoints import SSL4ClientEndpoint, SSL4ServerEndpoint, TCP4ServerEndpoint\n+from twisted.internet.endpoints import SSL4ClientEndpoint, SSL4ServerEndpoint\n from twisted.internet.protocol import Factory\n from twisted.internet.ssl import optionsForClientTLS, PrivateCertificate, Certificate\n from twisted.python.failure import Failure\n@@ -20,7 +20,7 @@ from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.static import File\n \n from scrapy.core.http2.protocol import H2ClientProtocol\n-from scrapy.core.http2.stream import InactiveStreamClosed\n+from scrapy.core.http2.stream import InactiveStreamClosed, InvalidHostname\n from scrapy.http import Request, Response, JsonRequest\n from scrapy.utils.python import to_bytes, to_unicode\n from tests.mockserver import ssl_context_factory, LeafResource, Status\n@@ -135,7 +135,7 @@ class QueryParams(LeafResource):\n         return to_bytes(json.dumps(query_params))\n \n \n-def get_client_certificate(key_file, certificate_file):\n+def get_client_certificate(key_file, certificate_file) -> PrivateCertificate:\n     with open(key_file, 'r') as key, open(certificate_file, 'r') as certificate:\n         pem = ''.join(key.readlines()) + ''.join(certificate.readlines())\n \n@@ -171,19 +171,16 @@ class Https2ClientProtocolTestCase(TestCase):\n \n         # Start server for testing\n         self.hostname = u'localhost'\n-        if self.scheme == 'https':\n         context_factory = ssl_context_factory(self.key_file, self.certificate_file)\n         server_endpoint = SSL4ServerEndpoint(reactor, 0, context_factory, interface=self.hostname)\n-        else:\n-            server_endpoint = TCP4ServerEndpoint(reactor, 0, interface=self.hostname)\n         self.server = yield server_endpoint.listen(self.site)\n         self.port_number = self.server.getHost().port\n \n         # Connect H2 client with server\n-        client_certificate = get_client_certificate(self.key_file, self.certificate_file)\n+        self.client_certificate = get_client_certificate(self.key_file, self.certificate_file)\n         client_options = optionsForClientTLS(\n             hostname=self.hostname,\n-            trustRoot=client_certificate,\n+            trustRoot=self.client_certificate,\n             acceptableProtocols=[b'h2']\n         )\n         h2_client_factory = Factory.forProtocol(H2ClientProtocol)\n@@ -440,8 +437,8 @@ class Https2ClientProtocolTestCase(TestCase):\n         yield self._check_log_warnsize(request, warn_pattern, Data.NO_CONTENT_LENGTH)\n \n     def test_max_concurrent_streams(self):\n-        \"\"\"Send 1000 requests to check if we can handle\n-        very large number of request\n+        \"\"\"Send 500 requests at one to check if we can handle\n+        very large number of request.\n         \"\"\"\n \n         def get_deferred():\n@@ -451,7 +448,7 @@ class Https2ClientProtocolTestCase(TestCase):\n                 200\n             )\n \n-        return self._check_repeat(get_deferred, 1000)\n+        return self._check_repeat(get_deferred, 500)\n \n     def test_inactive_stream(self):\n         \"\"\"Here we send 110 requests considering the MAX_CONCURRENT_STREAMS\n@@ -524,6 +521,10 @@ class Https2ClientProtocolTestCase(TestCase):\n         def assert_metadata(response: Response):\n             self.assertEqual(response.request, request)\n             self.assertIsInstance(response.certificate, Certificate)\n+            self.assertIsNotNone(response.certificate.original)\n+            self.assertEqual(response.certificate.getIssuer(), self.client_certificate.getIssuer())\n+            self.assertTrue(response.certificate.getPublicKey().matches(self.client_certificate.getPublicKey()))\n+\n             self.assertIsInstance(response.ip_address, IPv4Address)\n             self.assertEqual(str(response.ip_address), '127.0.0.1')\n \n@@ -532,3 +533,40 @@ class Https2ClientProtocolTestCase(TestCase):\n         d.addErrback(self.fail)\n \n         return d\n+\n+    def _check_invalid_netloc(self, url):\n+        request = Request(url)\n+\n+        def assert_invalid_hostname(failure: Failure):\n+            self.assertIsNotNone(failure.check(InvalidHostname))\n+            error_msg = str(failure.value)\n+            self.assertIn('localhost', error_msg)\n+            self.assertIn('127.0.0.1', error_msg)\n+            self.assertIn(str(request), error_msg)\n+\n+        d = self.client.request(request)\n+        d.addCallback(self.fail)\n+        d.addErrback(assert_invalid_hostname)\n+        return d\n+\n+    def test_invalid_hostname(self):\n+        return self._check_invalid_netloc('https://notlocalhost.notlocalhostdomain')\n+\n+    def test_invalid_host_port(self):\n+        port = self.port_number + 1\n+        return self._check_invalid_netloc(f'https://127.0.0.1:{port}')\n+\n+    def test_connection_stays_with_invalid_requests(self):\n+        d_list = [\n+            self.test_invalid_hostname(),\n+            self.test_invalid_host_port(),\n+            self._check_GET(Request(self.get_url('/get-data-html-small')), Data.HTML_SMALL, 200),\n+            self._check_POST_json(\n+                JsonRequest(url=self.get_url('/post-data-json-small'), method='POST', data=Data.JSON_SMALL),\n+                Data.JSON_SMALL,\n+                Data.EXTRA_SMALL,\n+                200\n+            )\n+        ]\n+\n+        return DeferredList(d_list, fireOnOneErrback=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4acdc2e5d623c6330235647aaad817e77eaad800", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 16 | Files Changed: 3 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 2846 | Contributors (this commit): 1 | Commits (past 90d): 45 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -121,7 +121,7 @@ class H2ClientProtocol(Protocol):\n \n     def request(self, request: Request):\n         if not isinstance(request, Request):\n-            raise TypeError(f'Expected scrapy.http.Request, received {request.__class__.__name__}')\n+            raise TypeError(f'Expected scrapy.http.Request, received {request.__class__.__qualname__}')\n \n         stream = self._new_stream(request)\n         d = stream.get_response()\n\n@@ -146,7 +146,7 @@ class Stream:\n         self._deferred_response = Deferred(_cancel)\n \n     def __str__(self):\n-        return f'Stream(id={self.stream_id})'\n+        return f'Stream(id={self.stream_id!r})'\n \n     __repr__ = __str__\n \n@@ -190,11 +190,11 @@ class Stream:\n     def _get_request_headers(self):\n         url = urlparse(self._request.url)\n \n-        # Make sure pseudo-headers comes before all the other headers\n         path = url.path\n         if url.query:\n             path += '?' + url.query\n \n+        # Make sure pseudo-headers comes before all the other headers\n         headers = [\n             (':method', self._request.method),\n             (':authority', url.netloc),\n@@ -284,8 +284,10 @@ class Stream:\n \n         if self._log_warnsize:\n             self._reached_warnsize = True\n-            warning_msg = f\"Received more ({self._response['flow_controlled_size']}) bytes than download \" \\\n-                          + f'warn size ({self._download_warnsize}) in request {self._request}'\n+            warning_msg = (\n+                f'Received more ({self._response[\"flow_controlled_size\"]}) bytes than download '\n+                f'warn size ({self._download_warnsize}) in request {self._request}'\n+            )\n             logger.warning(warning_msg)\n \n         # Acknowledge the data received\n@@ -306,8 +308,10 @@ class Stream:\n \n         if self._log_warnsize:\n             self._reached_warnsize = True\n-            warning_msg = f'Expected response size ({expected_size}) larger than ' \\\n-                          + f'download warn size ({self._download_warnsize}) in request {self._request}'\n+            warning_msg = (\n+                f'Expected response size ({expected_size}) larger than '\n+                f'download warn size ({self._download_warnsize}) in request {self._request}'\n+            )\n             logger.warning(warning_msg)\n \n     def reset_stream(self, reason=StreamCloseReason.RESET):\n@@ -340,7 +344,7 @@ class Stream:\n             raise StreamClosedError(self.stream_id)\n \n         if not isinstance(reason, StreamCloseReason):\n-            raise TypeError(f'Expected StreamCloseReason, received {reason.__class__.__name__}')\n+            raise TypeError(f'Expected StreamCloseReason, received {reason.__class__.__qualname__}')\n \n         self._cb_close(self.stream_id)\n         self.stream_closed_server = True\n@@ -356,8 +360,10 @@ class Stream:\n         # having Content-Length)\n         if reason is StreamCloseReason.MAXSIZE_EXCEEDED:\n             expected_size = int(self._response['headers'].get(b'Content-Length', -1))\n-            error_msg = f'Cancelling download of {self._request.url}: expected response ' \\\n+            error_msg = (\n+                f'Cancelling download of {self._request.url}: expected response '\n                 f'size ({expected_size}) larger than download max size ({self._download_maxsize}).'\n+            )\n             logger.error(error_msg)\n             self._deferred_response.errback(CancelledError(error_msg))\n \n\n@@ -560,13 +560,8 @@ class Https2ClientProtocolTestCase(TestCase):\n         d_list = [\n             self.test_invalid_hostname(),\n             self.test_invalid_host_port(),\n-            self._check_GET(Request(self.get_url('/get-data-html-small')), Data.HTML_SMALL, 200),\n-            self._check_POST_json(\n-                JsonRequest(url=self.get_url('/post-data-json-small'), method='POST', data=Data.JSON_SMALL),\n-                Data.JSON_SMALL,\n-                Data.EXTRA_SMALL,\n-                200\n-            )\n+            self.test_GET_small_body(),\n+            self.test_POST_small_json()\n         ]\n \n         return DeferredList(d_list, fireOnOneErrback=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
