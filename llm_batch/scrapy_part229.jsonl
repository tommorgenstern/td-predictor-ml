{"custom_id": "scrapy#df446d167f6b539c4a79fb64d91cbc1607f30acb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 20 | Churn Cumulative: 317 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,7 +6,7 @@ from six.moves.urllib.parse import urljoin\n import warnings\n from sgmllib import SGMLParser\n \n-from w3lib.url import safe_url_string\n+from w3lib.url import safe_url_string, canonicalize_url\n from w3lib.html import strip_html5_whitespace\n \n from scrapy.link import Link\n@@ -20,7 +20,7 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n class BaseSgmlLinkExtractor(SGMLParser):\n \n     def __init__(self, tag=\"a\", attr=\"href\", unique=False, process_value=None,\n-                 strip=True):\n+                 strip=True, canonicalized=False):\n         warnings.warn(\n             \"BaseSgmlLinkExtractor is deprecated and will be removed in future releases. \"\n             \"Please use scrapy.linkextractors.LinkExtractor\",\n@@ -33,6 +33,11 @@ class BaseSgmlLinkExtractor(SGMLParser):\n         self.current_link = None\n         self.unique = unique\n         self.strip = strip\n+        if canonicalized:\n+            self.link_key = lambda link: link.url\n+        else:\n+            self.link_key = lambda link: canonicalize_url(link.url,\n+                                                          keep_fragments=True)\n \n     def _extract_links(self, response_text, response_url, response_encoding, base_url=None):\n         \"\"\" Do the real extraction work \"\"\"\n@@ -61,8 +66,7 @@ class BaseSgmlLinkExtractor(SGMLParser):\n \n         The subclass should override it if necessary\n         \"\"\"\n-        links = unique_list(links, key=lambda link: link.url) if self.unique else links\n-        return links\n+        return unique_list(links, key=self.link_key) if self.unique else links\n \n     def extract_links(self, response):\n         # wrapper needed to allow to work directly with text\n@@ -107,10 +111,9 @@ class BaseSgmlLinkExtractor(SGMLParser):\n class SgmlLinkExtractor(FilteringLinkExtractor):\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n-                 tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True,\n+                 tags=('a', 'area'), attrs=('href',), canonicalize=False, unique=True,\n                  process_value=None, deny_extensions=None, restrict_css=(),\n                  strip=True):\n-\n         warnings.warn(\n             \"SgmlLinkExtractor is deprecated and will be removed in future releases. \"\n             \"Please use scrapy.linkextractors.LinkExtractor\",\n@@ -124,7 +127,8 @@ class SgmlLinkExtractor(FilteringLinkExtractor):\n         with warnings.catch_warnings():\n             warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n             lx = BaseSgmlLinkExtractor(tag=tag_func, attr=attr_func,\n-                unique=unique, process_value=process_value, strip=strip)\n+                unique=unique, process_value=process_value, strip=strip,\n+                canonicalized=canonicalize)\n \n         super(SgmlLinkExtractor, self).__init__(lx, allow=allow, deny=deny,\n             allow_domains=allow_domains, deny_domains=deny_domains,\n\n@@ -121,6 +121,7 @@ class HtmlParserLinkExtractorTestCase(unittest.TestCase):\n             Link(url='http://example.com/sample2.html', text=u'sample 2'),\n             Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n             Link(url='http://example.com/sample3.html', text=u'sample 3 repetition'),\n+            Link(url='http://example.com/sample3.html#foo', text=u'sample 3 repetition with fragment'),\n             Link(url='http://www.google.com/something', text=u''),\n             Link(url='http://example.com/innertag.html', text=u'inner tag'),\n             Link(url='http://example.com/page%204.html', text=u'href with whitespaces'),\n@@ -190,6 +191,7 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.extract_links(self.response),\n                          [Link(url='http://example.com/sample2.html', text=u'sample 2'),\n                           Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+                          Link(url='http://example.com/sample3.html#foo', text=u'sample 3 repetition with fragment'),\n                           Link(url='http://www.google.com/something', text=u''),\n                           Link(url='http://example.com/innertag.html', text=u'inner tag'),])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2b4d46315f2ce7ce20e7355fe4ad184bb041d6f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 19 | Churn Cumulative: 1235 | Contributors (this commit): 10 | Commits (past 90d): 12 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -392,6 +392,7 @@ class TextResponseTest(BaseResponseTest):\n             'http://example.com/sample2.html',\n             'http://example.com/sample3.html',\n             'http://example.com/sample3.html',\n+            'http://example.com/sample3.html#foo',\n             'http://www.google.com/something',\n             'http://example.com/innertag.html'\n         ]\n\n@@ -13,6 +13,7 @@ from tests import get_testdata\n class Base:\n     class LinkExtractorTestCase(unittest.TestCase):\n         extractor_cls = None\n+        escapes_whitespace = False\n \n         def setUp(self):\n             body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n@@ -26,6 +27,11 @@ class Base:\n \n         def test_extract_all_links(self):\n             lx = self.extractor_cls()\n+            if self.escapes_whitespace:\n+                page4_url = 'http://example.com/page%204.html'\n+            else:\n+                page4_url = 'http://example.com/page 4.html'\n+\n             self.assertEqual([link for link in lx.extract_links(self.response)], [\n                 Link(url='http://example.com/sample1.html', text=u''),\n                 Link(url='http://example.com/sample2.html', text=u'sample 2'),\n@@ -33,7 +39,7 @@ class Base:\n                 Link(url='http://example.com/sample3.html#foo', text='sample 3 repetition with fragment'),\n                 Link(url='http://www.google.com/something', text=u''),\n                 Link(url='http://example.com/innertag.html', text=u'inner tag'),\n-                Link(url='http://example.com/page%204.html', text=u'href with whitespaces'),\n+                Link(url=page4_url, text=u'href with whitespaces'),\n             ])\n \n         def test_extract_filter_allow(self):\n@@ -301,6 +307,11 @@ class Base:\n \n         def test_attrs(self):\n             lx = self.extractor_cls(attrs=\"href\")\n+            if self.escapes_whitespace:\n+                page4_url = 'http://example.com/page%204.html'\n+            else:\n+                page4_url = 'http://example.com/page 4.html'\n+\n             self.assertEqual(lx.extract_links(self.response), [\n                 Link(url='http://example.com/sample1.html', text=u''),\n                 Link(url='http://example.com/sample2.html', text=u'sample 2'),\n@@ -308,7 +319,7 @@ class Base:\n                 Link(url='http://example.com/sample3.html#foo', text='sample 3 repetition with fragment'),\n                 Link(url='http://www.google.com/something', text=u''),\n                 Link(url='http://example.com/innertag.html', text=u'inner tag'),\n-                Link(url='http://example.com/page%204.html', text=u'href with whitespaces'),\n+                Link(url=page4_url, text=u'href with whitespaces'),\n             ])\n \n             lx = self.extractor_cls(attrs=(\"href\",\"src\"), tags=(\"a\",\"area\",\"img\"), deny_extensions=())\n@@ -320,7 +331,7 @@ class Base:\n                 Link(url='http://example.com/sample3.html#foo', text='sample 3 repetition with fragment'),\n                 Link(url='http://www.google.com/something', text=u''),\n                 Link(url='http://example.com/innertag.html', text=u'inner tag'),\n-                Link(url='http://example.com/page%204.html', text=u'href with whitespaces'),\n+                Link(url=page4_url, text=u'href with whitespaces'),\n             ])\n \n             lx = self.extractor_cls(attrs=None)\n\n@@ -143,6 +143,7 @@ class HtmlParserLinkExtractorTestCase(unittest.TestCase):\n \n class SgmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n     extractor_cls = SgmlLinkExtractor\n+    escapes_whitespace = True\n \n     def test_deny_extensions(self):\n         html = \"\"\"<a href=\"page.html\">asd</a> and <a href=\"photo.jpg\">\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#26c1256f95e21f474783338bd1f1dda6ea3afecd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 88 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -46,6 +46,10 @@ class HttpErrorMiddleware(object):\n \n     def process_spider_exception(self, response, exception, spider):\n         if isinstance(exception, HttpError):\n+            spider.crawler.stats.inc_value('httperror/response_ignored_count')\n+            spider.crawler.stats.inc_value(\n+                'httperror/response_ignored_status_count/%s' % response.status\n+            )\n             logger.info(\n                 \"Ignoring response %(response)r: HTTP status code is not handled or not allowed\",\n                 {'response': response}, extra={'spider': spider},\n\n@@ -60,7 +60,8 @@ def _responses(request, status_codes):\n class TestHttpErrorMiddleware(TestCase):\n \n     def setUp(self):\n-        self.spider = Spider('foo')\n+        crawler = get_crawler(Spider)\n+        self.spider = Spider.from_crawler(crawler, name='foo')\n         self.mw = HttpErrorMiddleware(Settings({}))\n         self.req = Request('http://scrapytest.org')\n         self.res200, self.res404 = _responses(self.req, [200, 404])\n@@ -73,10 +74,10 @@ class TestHttpErrorMiddleware(TestCase):\n \n     def test_process_spider_exception(self):\n         self.assertEquals([],\n-                self.mw.process_spider_exception(self.res404, \\\n+                self.mw.process_spider_exception(self.res404,\n                         HttpError(self.res404), self.spider))\n         self.assertEquals(None,\n-                self.mw.process_spider_exception(self.res404, \\\n+                self.mw.process_spider_exception(self.res404,\n                         Exception(), self.spider))\n \n     def test_handle_httpstatus_list(self):\n@@ -173,6 +174,12 @@ class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n         self.assertEqual(crawler.spider.parsed, {'200'})\n         self.assertEqual(crawler.spider.failed, {'404', '402', '500'})\n \n+        get_value = crawler.stats.get_value\n+        self.assertEqual(get_value('httperror/response_ignored_count'), 3)\n+        self.assertEqual(get_value('httperror/response_ignored_status_count/404'), 1)\n+        self.assertEqual(get_value('httperror/response_ignored_status_count/402'), 1)\n+        self.assertEqual(get_value('httperror/response_ignored_status_count/500'), 1)\n+\n     @defer.inlineCallbacks\n     def test_logging(self):\n         crawler = get_crawler(_HttpErrorSpider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d60642e1755b004e46b87e2011c1873f25895bb8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 150 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 13 | Complexity Δ (Sum/Max): 24/12 | Churn Δ: 151 | Churn Cumulative: 866 | Contributors (this commit): 11 | Commits (past 90d): 10 | Contributors (cumulative): 12 | DMM Complexity: 0.5714285714285714\n\nDIFF:\n@@ -0,0 +1,93 @@\n+import base64\n+import re\n+from six.moves.urllib.parse import unquote\n+\n+from scrapy.http import TextResponse\n+from scrapy.responsetypes import responsetypes\n+from scrapy.utils.datatypes import CaselessDict\n+from scrapy.utils.decorators import defers\n+\n+\n+# ASCII characters.\n+_char = set(map(chr, range(127)))\n+\n+# RFC 2045 token.\n+_token = r'[{}]+'.format(re.escape(''.join(_char -\n+                                           # Control characters.\n+                                           set(map(chr, range(0, 32))) -\n+                                           # tspecials and space.\n+                                           set('()<>@,;:\\\\\"/[]?= '))))\n+\n+# RFC 822 quoted-string, without surrounding quotation marks.\n+_quoted_string = r'(?:[{}]|(?:\\\\[{}]))*'.format(\n+    re.escape(''.join(_char - {'\"', '\\\\', '\\r'})),\n+    re.escape(''.join(_char))\n+)\n+\n+# RFC 2397 mediatype.\n+_mediatype_pattern = re.compile(r'{token}/{token}'.format(token=_token))\n+\n+_mediatype_parameter_pattern = re.compile(\n+    r';({token})=(?:({token})|\"({quoted})\")'.format(token=_token,\n+                                                    quoted=_quoted_string)\n+)\n+\n+\n+class DataURIDownloadHandler(object):\n+    def __init__(self, settings):\n+        super(DataURIDownloadHandler, self).__init__()\n+\n+    @defers\n+    def download_request(self, request, spider):\n+        url = request.url\n+\n+        scheme, url = url.split(':', 1)\n+        if scheme != 'data':\n+            raise ValueError(\"not a data URI\")\n+\n+        # RFC 3986 section 2.1 allows percent encoding to escape characters\n+        # that would be interpreted as delimiters, implying that actual\n+        # delimiters should not be percent-encoded.\n+        # Decoding before parsing will allow malformed URIs with\n+        # percent-encoded delimiters, but it makes parsing easier and should\n+        # not affect well-formed URIs, as the delimiters used in this URI\n+        # scheme are not allowed, percent-encoded or not, in tokens.\n+        url = unquote(url)\n+\n+        media_type = \"text/plain\"\n+        media_type_params = CaselessDict()\n+\n+        m = _mediatype_pattern.match(url)\n+        if m:\n+            media_type = m.group()\n+            url = url[m.end():]\n+        else:\n+            media_type_params['charset'] = \"US-ASCII\"\n+\n+        while True:\n+            m = _mediatype_parameter_pattern.match(url)\n+            if m:\n+                attribute, value, value_quoted = m.groups()\n+                if value_quoted:\n+                    value = re.sub(r'\\\\(.)', '\\1', value_quoted)\n+                media_type_params[attribute] = value\n+                url = url[m.end():]\n+            else:\n+                break\n+\n+        is_base64, data = url.split(',', 1)\n+        if is_base64:\n+            if is_base64 != \";base64\":\n+                raise ValueError(\"invalid data URI\")\n+            data = base64.b64decode(data)\n+\n+        respcls = responsetypes.from_mimetype(media_type)\n+\n+        resp_kwargs = {}\n+\n+        if media_type:\n+            media_type = media_type.split('/')\n+            if issubclass(respcls, TextResponse) and media_type[0] == 'text':\n+                resp_kwargs['encoding'] = media_type_params.get('charset')\n+\n+        return respcls(url=request.url, body=data, **resp_kwargs)\n\n@@ -6,7 +6,6 @@ try:\n     from unittest import mock\n except ImportError:\n     import mock\n-import shutil\n \n from twisted.trial import unittest\n from twisted.protocols.policies import WrappingFactory\n@@ -20,6 +19,7 @@ from twisted.cred import portal, checkers, credentials\n from w3lib.url import path_to_file_uri\n \n from scrapy.core.downloader.handlers import DownloadHandlers\n+from scrapy.core.downloader.handlers.data import DataURIDownloadHandler\n from scrapy.core.downloader.handlers.file import FileDownloadHandler\n from scrapy.core.downloader.handlers.http import HTTPDownloadHandler, HttpDownloadHandler\n from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n@@ -29,6 +29,7 @@ from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n from scrapy.spiders import Spider\n from scrapy.http import Request\n from scrapy.http.response.text import TextResponse\n+from scrapy.responsetypes import responsetypes\n from scrapy.settings import Settings\n from scrapy.utils.test import get_crawler, skip_if_no_boto\n from scrapy.utils.python import to_bytes\n@@ -828,3 +829,58 @@ class AnonymousFTPTestCase(BaseFTPTestCase):\n \n     def tearDown(self):\n         shutil.rmtree(self.directory)\n+\n+\n+class DataURITestCase(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.download_handler = DataURIDownloadHandler(Settings())\n+        self.download_request = self.download_handler.download_request\n+        self.spider = Spider('foo')\n+\n+    def test_default_mediatype_encoding(self):\n+        def _test(response):\n+            self.assertEquals(response.text, 'A brief note')\n+            self.assertEquals(type(response),\n+                              responsetypes.from_mimetype(\"text/plain\"))\n+            self.assertEquals(response.encoding, \"US-ASCII\")\n+\n+        request = Request(\"data:,A%20brief%20note\")\n+        return self.download_request(request, self.spider).addCallback(_test)\n+\n+    def test_default_mediatype(self):\n+        def _test(response):\n+            self.assertEquals(response.text, u'\\u038e\\u03a3\\u038e')\n+            self.assertEquals(type(response),\n+                              responsetypes.from_mimetype(\"text/plain\"))\n+            self.assertEquals(response.encoding, \"iso-8859-7\")\n+\n+        request = Request(\"data:;charset=iso-8859-7,%be%d3%be\")\n+        return self.download_request(request, self.spider).addCallback(_test)\n+\n+    def test_text_charset(self):\n+        def _test(response):\n+            self.assertEquals(response.text, u'\\u038e\\u03a3\\u038e')\n+            self.assertEquals(response.encoding, \"iso-8859-7\")\n+\n+        request = Request(\"data:text/plain;charset=iso-8859-7,%be%d3%be\")\n+        return self.download_request(request, self.spider).addCallback(_test)\n+\n+    def test_mediatype_parameters(self):\n+        def _test(response):\n+            self.assertEquals(response.text, u'\\u038e\\u03a3\\u038e')\n+            self.assertEquals(type(response),\n+                              responsetypes.from_mimetype(\"text/plain\"))\n+            self.assertEquals(response.encoding, \"utf-8\")\n+\n+        request = Request('data:text/plain;foo=%22foo;bar%5C%22%22;'\n+                          'charset=utf-8;bar=%22foo;%5C%22 foo ;/,%22'\n+                          ',%CE%8E%CE%A3%CE%8E')\n+        return self.download_request(request, self.spider).addCallback(_test)\n+\n+    def test_base64(self):\n+        def _test(response):\n+            self.assertEquals(response.text, 'Hello, world.')\n+\n+        request = Request('data:text/plain;base64,SGVsbG8sIHdvcmxkLg%3D%3D')\n+        return self.download_request(request, self.spider).addCallback(_test)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e9f2c31d78dcc798a281e5ba6ddf91714dc9cc7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 8 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 122 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,12 @@\n import base64\n import re\n-from six.moves.urllib.parse import unquote\n+\n+import six\n+\n+if six.PY2:\n+    from urllib import unquote\n+else:\n+    from urllib.parse import unquote_to_bytes as unquote\n \n from scrapy.http import TextResponse\n from scrapy.responsetypes import responsetypes\n@@ -24,12 +30,19 @@ _quoted_string = r'(?:[{}]|(?:\\\\[{}]))*'.format(\n     re.escape(''.join(_char))\n )\n \n+# Encode the regular expression strings to make them into bytes, as Python 3\n+# bytes have no format() method, but bytes must be passed to re.compile() in\n+# order to make a pattern object that can be used to match on bytes.\n+\n # RFC 2397 mediatype.\n-_mediatype_pattern = re.compile(r'{token}/{token}'.format(token=_token))\n+_mediatype_pattern = re.compile(\n+    r'{token}/{token}'.format(token=_token).encode()\n+)\n \n _mediatype_parameter_pattern = re.compile(\n     r';({token})=(?:({token})|\"({quoted})\")'.format(token=_token,\n-                                                    quoted=_quoted_string)\n+                                                    quoted=_quoted_string\n+                                                    ).encode()\n )\n \n \n@@ -59,7 +72,7 @@ class DataURIDownloadHandler(object):\n \n         m = _mediatype_pattern.match(url)\n         if m:\n-            media_type = m.group()\n+            media_type = m.group().decode()\n             url = url[m.end():]\n         else:\n             media_type_params['charset'] = \"US-ASCII\"\n@@ -69,15 +82,15 @@ class DataURIDownloadHandler(object):\n             if m:\n                 attribute, value, value_quoted = m.groups()\n                 if value_quoted:\n-                    value = re.sub(r'\\\\(.)', '\\1', value_quoted)\n-                media_type_params[attribute] = value\n+                    value = re.sub(br'\\\\(.)', r'\\1', value_quoted)\n+                media_type_params[attribute.decode()] = value.decode()\n                 url = url[m.end():]\n             else:\n                 break\n \n-        is_base64, data = url.split(',', 1)\n+        is_base64, data = url.split(b',', 1)\n         if is_base64:\n-            if is_base64 != \";base64\":\n+            if is_base64 != b\";base64\":\n                 raise ValueError(\"invalid data URI\")\n             data = base64.b64decode(data)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3397d27574d03a222b6a790004fcaa1a03711d75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 774 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -861,6 +861,7 @@ class DataURITestCase(unittest.TestCase):\n     def test_text_charset(self):\n         def _test(response):\n             self.assertEquals(response.text, u'\\u038e\\u03a3\\u038e')\n+            self.assertEquals(response.body, b'\\xbe\\xd3\\xbe')\n             self.assertEquals(response.encoding, \"iso-8859-7\")\n \n         request = Request(\"data:text/plain;charset=iso-8859-7,%be%d3%be\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c847e7d4d00647a953a3c5458d198439c30b87f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 91 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 99 | Churn Cumulative: 221 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,106 +1,23 @@\n-import base64\n-import re\n-\n-import six\n-\n-if six.PY2:\n-    from urllib import unquote\n-else:\n-    from urllib.parse import unquote_to_bytes as unquote\n+from w3lib.url import parse_data_uri\n \n from scrapy.http import TextResponse\n from scrapy.responsetypes import responsetypes\n-from scrapy.utils.datatypes import CaselessDict\n from scrapy.utils.decorators import defers\n \n \n-# ASCII characters.\n-_char = set(map(chr, range(127)))\n-\n-# RFC 2045 token.\n-_token = r'[{}]+'.format(re.escape(''.join(_char -\n-                                           # Control characters.\n-                                           set(map(chr, range(0, 32))) -\n-                                           # tspecials and space.\n-                                           set('()<>@,;:\\\\\"/[]?= '))))\n-\n-# RFC 822 quoted-string, without surrounding quotation marks.\n-_quoted_string = r'(?:[{}]|(?:\\\\[{}]))*'.format(\n-    re.escape(''.join(_char - {'\"', '\\\\', '\\r'})),\n-    re.escape(''.join(_char))\n-)\n-\n-# Encode the regular expression strings to make them into bytes, as Python 3\n-# bytes have no format() method, but bytes must be passed to re.compile() in\n-# order to make a pattern object that can be used to match on bytes.\n-\n-# RFC 2397 mediatype.\n-_mediatype_pattern = re.compile(\n-    r'{token}/{token}'.format(token=_token).encode()\n-)\n-\n-_mediatype_parameter_pattern = re.compile(\n-    r';({token})=(?:({token})|\"({quoted})\")'.format(token=_token,\n-                                                    quoted=_quoted_string\n-                                                    ).encode()\n-)\n-\n-\n class DataURIDownloadHandler(object):\n     def __init__(self, settings):\n         super(DataURIDownloadHandler, self).__init__()\n \n     @defers\n     def download_request(self, request, spider):\n-        url = request.url\n-\n-        scheme, url = url.split(':', 1)\n-        if scheme != 'data':\n-            raise ValueError(\"not a data URI\")\n-\n-        # RFC 3986 section 2.1 allows percent encoding to escape characters\n-        # that would be interpreted as delimiters, implying that actual\n-        # delimiters should not be percent-encoded.\n-        # Decoding before parsing will allow malformed URIs with\n-        # percent-encoded delimiters, but it makes parsing easier and should\n-        # not affect well-formed URIs, as the delimiters used in this URI\n-        # scheme are not allowed, percent-encoded or not, in tokens.\n-        url = unquote(url)\n-\n-        media_type = \"text/plain\"\n-        media_type_params = CaselessDict()\n-\n-        m = _mediatype_pattern.match(url)\n-        if m:\n-            media_type = m.group().decode()\n-            url = url[m.end():]\n-        else:\n-            media_type_params['charset'] = \"US-ASCII\"\n-\n-        while True:\n-            m = _mediatype_parameter_pattern.match(url)\n-            if m:\n-                attribute, value, value_quoted = m.groups()\n-                if value_quoted:\n-                    value = re.sub(br'\\\\(.)', r'\\1', value_quoted)\n-                media_type_params[attribute.decode()] = value.decode()\n-                url = url[m.end():]\n-            else:\n-                break\n-\n-        is_base64, data = url.split(b',', 1)\n-        if is_base64:\n-            if is_base64 != b\";base64\":\n-                raise ValueError(\"invalid data URI\")\n-            data = base64.b64decode(data)\n-\n-        respcls = responsetypes.from_mimetype(media_type)\n+        uri = parse_data_uri(request.url)\n+        respcls = responsetypes.from_mimetype(uri.media_type)\n \n         resp_kwargs = {}\n+        if (issubclass(respcls, TextResponse) and\n+                uri.media_type.split('/')[0] == 'text'):\n+            charset = uri.media_type_parameters.get('charset')\n+            resp_kwargs['encoding'] = charset\n \n-        if media_type:\n-            media_type = media_type.split('/')\n-            if issubclass(respcls, TextResponse) and media_type[0] == 'text':\n-                resp_kwargs['encoding'] = media_type_params.get('charset')\n-\n-        return respcls(url=request.url, body=data, **resp_kwargs)\n+        return respcls(url=request.url, body=uri.data, **resp_kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#121a668a479c3b681581a36c7da090518fbc2120", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 776 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n\n@@ -19,7 +19,7 @@ from twisted.cred import portal, checkers, credentials\n from w3lib.url import path_to_file_uri\n \n from scrapy.core.downloader.handlers import DownloadHandlers\n-from scrapy.core.downloader.handlers.data import DataURIDownloadHandler\n+from scrapy.core.downloader.handlers.datauri import DataURIDownloadHandler\n from scrapy.core.downloader.handlers.file import FileDownloadHandler\n from scrapy.core.downloader.handlers.http import HTTPDownloadHandler, HttpDownloadHandler\n from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3139f4a5f700a100b6ab6890677dac808b01835f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 502 | Contributors (this commit): 34 | Commits (past 90d): 5 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -67,6 +67,7 @@ DOWNLOAD_DELAY = 0\n \n DOWNLOAD_HANDLERS = {}\n DOWNLOAD_HANDLERS_BASE = {\n+    'data': 'scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler',\n     'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n     'http': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',\n     'https': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0b90c3b43c4eedc891056fa433e0608afbe6cd32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): -11/0 | Churn Δ: 34 | Churn Cumulative: 963 | Contributors (this commit): 13 | Commits (past 90d): 7 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -39,12 +39,13 @@ from twisted.internet.protocol import Protocol, ClientCreator\n from scrapy.http import Response\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.utils.python import to_bytes\n \n \n class ReceivedDataProtocol(Protocol):\n     def __init__(self, filename=None):\n         self.__filename = filename\n-        self.body = open(filename, \"w\") if filename else BytesIO()\n+        self.body = open(filename, \"wb\") if filename else BytesIO()\n         self.size = 0\n \n     def dataReceived(self, data):\n@@ -97,7 +98,7 @@ class FTPDownloadHandler(object):\n         protocol.close()\n         body = protocol.filename or protocol.body.read()\n         headers = {\"local filename\": protocol.filename or '', \"size\": protocol.size}\n-        return respcls(url=request.url, status=200, body=body, headers=headers)\n+        return respcls(url=request.url, status=200, body=to_bytes(body), headers=headers)\n \n     def _failed(self, result, request):\n         message = result.getErrorMessage()\n@@ -106,6 +107,6 @@ class FTPDownloadHandler(object):\n             if m:\n                 ftpcode = m.group()\n                 httpcode = self.CODE_MAPPING.get(ftpcode, self.CODE_MAPPING[\"default\"])\n-                return Response(url=request.url, status=httpcode, body=message)\n+                return Response(url=request.url, status=httpcode, body=to_bytes(message))\n         raise result.type(result.value)\n \n\n@@ -687,9 +687,6 @@ class BaseFTPTestCase(unittest.TestCase):\n     password = \"passwd\"\n     req_meta = {\"ftp_user\": username, \"ftp_password\": password}\n \n-    if six.PY3:\n-        skip = \"Twisted missing ftp support for PY3\"\n-\n     def setUp(self):\n         from twisted.protocols.ftp import FTPRealm, FTPFactory\n         from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n@@ -700,8 +697,8 @@ class BaseFTPTestCase(unittest.TestCase):\n         userdir = os.path.join(self.directory, self.username)\n         os.mkdir(userdir)\n         fp = FilePath(userdir)\n-        fp.child('file.txt').setContent(\"I have the power!\")\n-        fp.child('file with spaces.txt').setContent(\"Moooooooooo power!\")\n+        fp.child('file.txt').setContent(b\"I have the power!\")\n+        fp.child('file with spaces.txt').setContent(b\"Moooooooooo power!\")\n \n         # setup server\n         realm = FTPRealm(anonymousRoot=self.directory, userHome=self.directory)\n@@ -736,8 +733,8 @@ class BaseFTPTestCase(unittest.TestCase):\n \n         def _test(r):\n             self.assertEqual(r.status, 200)\n-            self.assertEqual(r.body, 'I have the power!')\n-            self.assertEqual(r.headers, {'Local Filename': [''], 'Size': ['17']})\n+            self.assertEqual(r.body, b'I have the power!')\n+            self.assertEqual(r.headers, {b'Local Filename': [b''], b'Size': [b'17']})\n         return self._add_test_callbacks(d, _test)\n \n     def test_ftp_download_path_with_spaces(self):\n@@ -749,8 +746,8 @@ class BaseFTPTestCase(unittest.TestCase):\n \n         def _test(r):\n             self.assertEqual(r.status, 200)\n-            self.assertEqual(r.body, 'Moooooooooo power!')\n-            self.assertEqual(r.headers, {'Local Filename': [''], 'Size': ['18']})\n+            self.assertEqual(r.body, b'Moooooooooo power!')\n+            self.assertEqual(r.headers, {b'Local Filename': [b''], b'Size': [b'18']})\n         return self._add_test_callbacks(d, _test)\n \n     def test_ftp_download_notexist(self):\n@@ -763,7 +760,7 @@ class BaseFTPTestCase(unittest.TestCase):\n         return self._add_test_callbacks(d, _test)\n \n     def test_ftp_local_filename(self):\n-        local_fname = \"/tmp/file.txt\"\n+        local_fname = b\"/tmp/file.txt\"\n         meta = {\"ftp_local_filename\": local_fname}\n         meta.update(self.req_meta)\n         request = Request(url=\"ftp://127.0.0.1:%s/file.txt\" % self.portNum,\n@@ -772,10 +769,10 @@ class BaseFTPTestCase(unittest.TestCase):\n \n         def _test(r):\n             self.assertEqual(r.body, local_fname)\n-            self.assertEqual(r.headers, {'Local Filename': ['/tmp/file.txt'], 'Size': ['17']})\n+            self.assertEqual(r.headers, {b'Local Filename': [b'/tmp/file.txt'], b'Size': [b'17']})\n             self.assertTrue(os.path.exists(local_fname))\n-            with open(local_fname) as f:\n-                self.assertEqual(f.read(), \"I have the power!\")\n+            with open(local_fname, \"rb\") as f:\n+                self.assertEqual(f.read(), b\"I have the power!\")\n             os.remove(local_fname)\n         return self._add_test_callbacks(d, _test)\n \n@@ -810,8 +807,8 @@ class AnonymousFTPTestCase(BaseFTPTestCase):\n         os.mkdir(self.directory)\n \n         fp = FilePath(self.directory)\n-        fp.child('file.txt').setContent(\"I have the power!\")\n-        fp.child('file with spaces.txt').setContent(\"Moooooooooo power!\")\n+        fp.child('file.txt').setContent(b\"I have the power!\")\n+        fp.child('file with spaces.txt').setContent(b\"Moooooooooo power!\")\n \n         # setup server for anonymous access\n         realm = FTPRealm(anonymousRoot=self.directory)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f01ae6ffcd431b73f5358f9f876f8e9ee9be0113", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 131 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 18 | Methods Changed: 20 | Complexity Δ (Sum/Max): 24/19 | Churn Δ: 140 | Churn Cumulative: 2890 | Contributors (this commit): 42 | Commits (past 90d): 17 | Contributors (cumulative): 63 | DMM Complexity: 0.7472527472527473\n\nDIFF:\n@@ -12,9 +12,9 @@ from twisted.internet import defer, reactor, protocol\n from twisted.web.http_headers import Headers as TxHeaders\n from twisted.web.iweb import IBodyProducer, UNKNOWN_LENGTH\n from twisted.internet.error import TimeoutError\n-from twisted.web.http import PotentialDataLoss\n+from twisted.web.http import _DataLoss, PotentialDataLoss\n from twisted.web.client import Agent, ProxyAgent, ResponseDone, \\\n-    HTTPConnectionPool\n+    HTTPConnectionPool, ResponseFailed\n from twisted.internet.endpoints import TCP4ClientEndpoint\n \n from scrapy.http import Headers\n@@ -51,13 +51,15 @@ class HTTP11DownloadHandler(object):\n             warnings.warn(msg)\n         self._default_maxsize = settings.getint('DOWNLOAD_MAXSIZE')\n         self._default_warnsize = settings.getint('DOWNLOAD_WARNSIZE')\n+        self._fail_on_dataloss = settings.getbool('DOWNLOAD_FAIL_ON_DATALOSS')\n         self._disconnect_timeout = 1\n \n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n         agent = ScrapyAgent(contextFactory=self._contextFactory, pool=self._pool,\n             maxsize=getattr(spider, 'download_maxsize', self._default_maxsize),\n-            warnsize=getattr(spider, 'download_warnsize', self._default_warnsize))\n+            warnsize=getattr(spider, 'download_warnsize', self._default_warnsize),\n+            fail_on_dataloss=self._fail_on_dataloss)\n         return agent.download_request(request)\n \n     def close(self):\n@@ -233,13 +235,14 @@ class ScrapyAgent(object):\n     _TunnelingAgent = TunnelingAgent\n \n     def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None,\n-                 maxsize=0, warnsize=0):\n+                 maxsize=0, warnsize=0, fail_on_dataloss=True):\n         self._contextFactory = contextFactory\n         self._connectTimeout = connectTimeout\n         self._bindAddress = bindAddress\n         self._pool = pool\n         self._maxsize = maxsize\n         self._warnsize = warnsize\n+        self._fail_on_dataloss = fail_on_dataloss\n         self._txresponse = None\n \n     def _get_agent(self, request, timeout):\n@@ -326,6 +329,7 @@ class ScrapyAgent(object):\n         maxsize = request.meta.get('download_maxsize', self._maxsize)\n         warnsize = request.meta.get('download_warnsize', self._warnsize)\n         expected_size = txresponse.length if txresponse.length != UNKNOWN_LENGTH else -1\n+        fail_on_dataloss = request.meta.get('download_fail_on_dataloss', self._fail_on_dataloss)\n \n         if maxsize and expected_size > maxsize:\n             error_msg = (\"Cancelling download of %(url)s: expected response \"\n@@ -345,7 +349,8 @@ class ScrapyAgent(object):\n             txresponse._transport._producer.loseConnection()\n \n         d = defer.Deferred(_cancel)\n-        txresponse.deliverBody(_ResponseReader(d, txresponse, request, maxsize, warnsize))\n+        txresponse.deliverBody(_ResponseReader(\n+            d, txresponse, request, maxsize, warnsize, fail_on_dataloss))\n \n         # save response for timeouts\n         self._txresponse = txresponse\n@@ -380,13 +385,16 @@ class _RequestBodyProducer(object):\n \n class _ResponseReader(protocol.Protocol):\n \n-    def __init__(self, finished, txresponse, request, maxsize, warnsize):\n+    def __init__(self, finished, txresponse, request, maxsize, warnsize,\n+                 fail_on_dataloss):\n         self._finished = finished\n         self._txresponse = txresponse\n         self._request = request\n         self._bodybuf = BytesIO()\n         self._maxsize  = maxsize\n         self._warnsize  = warnsize\n+        self._fail_on_dataloss = fail_on_dataloss\n+        self._fail_on_dataloss_warned = False\n         self._reached_warnsize = False\n         self._bytes_received = 0\n \n@@ -415,7 +423,22 @@ class _ResponseReader(protocol.Protocol):\n         body = self._bodybuf.getvalue()\n         if reason.check(ResponseDone):\n             self._finished.callback((self._txresponse, body, None))\n-        elif reason.check(PotentialDataLoss):\n+            return\n+\n+        if reason.check(PotentialDataLoss):\n             self._finished.callback((self._txresponse, body, ['partial']))\n-        else:\n+            return\n+\n+        if reason.check(ResponseFailed) and any(r.check(_DataLoss) for r in reason.value.reasons):\n+            if not self._fail_on_dataloss:\n+                self._finished.callback((self._txresponse, body, ['dataloss']))\n+                return\n+\n+            elif not self._fail_on_dataloss_warned:\n+                logger.warn(\"Got data loss in %s. If you want to process broken \"\n+                            \"responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False\"\n+                            \" -- This message won't be shown in further requests\",\n+                            self._txresponse.request.absoluteURI.decode())\n+                self._fail_on_dataloss_warned = True\n+\n         self._finished.errback(reason)\n\n@@ -79,6 +79,8 @@ DOWNLOAD_TIMEOUT = 180      # 3mins\n DOWNLOAD_MAXSIZE = 1024*1024*1024   # 1024m\n DOWNLOAD_WARNSIZE = 32*1024*1024    # 32m\n \n+DOWNLOAD_FAIL_ON_DATALOSS = True\n+\n DOWNLOADER = 'scrapy.core.downloader.Downloader'\n \n DOWNLOADER_HTTPCLIENTFACTORY = 'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'\n\n@@ -13,9 +13,11 @@ from twisted.protocols.policies import WrappingFactory\n from twisted.python.filepath import FilePath\n from twisted.internet import reactor, defer, error\n from twisted.web import server, static, util, resource\n+from twisted.web._newclient import ResponseFailed\n+from twisted.web.http import _DataLoss\n from twisted.web.test.test_webclient import ForeverTakingResource, \\\n         NoLengthResource, HostHeaderResource, \\\n-        PayloadResource, BrokenDownloadResource\n+        PayloadResource\n from twisted.cred import portal, checkers, credentials\n from w3lib.url import path_to_file_uri\n \n@@ -118,6 +120,52 @@ class ContentLengthHeaderResource(resource.Resource):\n         return request.requestHeaders.getRawHeaders(b\"content-length\")[0]\n \n \n+class ChunkedResource(resource.Resource):\n+\n+    def render(self, request):\n+        def response():\n+            request.write(b\"chunked \")\n+            request.write(b\"content\\n\")\n+            request.finish()\n+        reactor.callLater(0, response)\n+        return server.NOT_DONE_YET\n+\n+\n+class BrokenChunkedResource(resource.Resource):\n+\n+    def render(self, request):\n+        def response():\n+            request.write(b\"chunked \")\n+            request.write(b\"content\\n\")\n+            # Disable terminating chunk on finish.\n+            request.chunked = False\n+            closeConnection(request)\n+        reactor.callLater(0, response)\n+        return server.NOT_DONE_YET\n+\n+\n+class BrokenDownloadResource(resource.Resource):\n+\n+    def render(self, request):\n+        def response():\n+            request.setHeader(b\"Content-Length\", b\"20\")\n+            request.write(b\"partial\")\n+            closeConnection(request)\n+\n+        reactor.callLater(0, response)\n+        return server.NOT_DONE_YET\n+\n+\n+def closeConnection(request):\n+    # We have to force a disconnection for HTTP/1.1 clients. Otherwise\n+    # client keeps the connection open waiting for more data.\n+    if hasattr(request.channel, 'loseConnection'):  # twisted >=16.3.0\n+        request.channel.loseConnection()\n+    else:\n+        request.channel.transport.loseConnection()\n+    request.finish()\n+\n+\n class EmptyContentTypeHeaderResource(resource.Resource):\n     \"\"\"\n     A testing resource which renders itself as the value of request body\n@@ -149,6 +197,8 @@ class HttpTestCase(unittest.TestCase):\n         r.putChild(b\"host\", HostHeaderResource())\n         r.putChild(b\"payload\", PayloadResource())\n         r.putChild(b\"broken\", BrokenDownloadResource())\n+        r.putChild(b\"chunked\", ChunkedResource())\n+        r.putChild(b\"broken-chunked\", BrokenChunkedResource())\n         r.putChild(b\"contentlength\", ContentLengthHeaderResource())\n         r.putChild(b\"nocontenttype\", EmptyContentTypeHeaderResource())\n         self.site = server.Site(r, timeout=None)\n@@ -341,6 +391,53 @@ class Http11TestCase(HttpTestCase):\n         d.addCallback(self.assertEquals, b\"0123456789\")\n         return d\n \n+    def test_download_chunked_content(self):\n+        request = Request(self.getURL('chunked'))\n+        d = self.download_request(request, Spider('foo'))\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEquals, b\"chunked content\\n\")\n+        return d\n+\n+    def test_download_broken_content_cause_data_loss(self, url='broken'):\n+        request = Request(self.getURL(url))\n+        d = self.download_request(request, Spider('foo'))\n+\n+        def checkDataLoss(failure):\n+            if failure.check(ResponseFailed):\n+                if any(r.check(_DataLoss) for r in failure.value.reasons):\n+                    return None\n+            return failure\n+\n+        d.addCallback(lambda _: self.fail(\"No DataLoss exception\"))\n+        d.addErrback(checkDataLoss)\n+        return d\n+\n+    def test_download_broken_chunked_content_cause_data_loss(self):\n+        return self.test_download_broken_content_cause_data_loss('broken-chunked')\n+\n+    def test_download_broken_content_allow_data_loss(self, url='broken'):\n+        request = Request(self.getURL(url), meta={'download_fail_on_dataloss': False})\n+        d = self.download_request(request, Spider('foo'))\n+        d.addCallback(lambda r: r.flags)\n+        d.addCallback(self.assertEqual, ['dataloss'])\n+        return d\n+\n+    def test_download_broken_chunked_content_allow_data_loss(self):\n+        return self.test_download_broken_content_allow_data_loss('broken-chunked')\n+\n+    def test_download_broken_content_allow_data_loss_via_setting(self, url='broken'):\n+        download_handler = self.download_handler_cls(Settings({\n+            'DOWNLOAD_FAIL_ON_DATALOSS': False,\n+        }))\n+        request = Request(self.getURL(url))\n+        d = download_handler.download_request(request, Spider('foo'))\n+        d.addCallback(lambda r: r.flags)\n+        d.addCallback(self.assertEqual, ['dataloss'])\n+        return d\n+\n+    def test_download_broken_chunked_content_allow_data_loss_via_setting(self):\n+        return self.test_download_broken_content_allow_data_loss_via_setting('broken-chunked')\n+\n \n class Https11TestCase(Http11TestCase):\n     scheme = 'https'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#706ed0e049fe008cac12f243371b67ee0230a08a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 372 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 8 | Methods Changed: 15 | Complexity Δ (Sum/Max): 50/40 | Churn Δ: 382 | Churn Cumulative: 555 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 0.8394160583941606\n\nDIFF:\n@@ -5,6 +5,7 @@ See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n import six\n from twisted.python.failure import Failure\n+from scrapy.exceptions import InvalidOutput\n from scrapy.middleware import MiddlewareManager\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.conf import build_component_list\n@@ -40,31 +41,47 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             for method in self.methods['process_spider_input']:\n                 try:\n                     result = method(response=response, spider=spider)\n-                    assert result is None, \\\n-                            'Middleware %s must returns None or ' \\\n-                            'raise an exception, got %s ' \\\n-                            % (fname(method), type(result))\n+                    if result is not None:\n+                        raise InvalidOutput('Middleware {} must return None or raise ' \\\n+                            'an exception, got {}'.format(fname(method), type(result)))\n                 except:\n                     return scrape_func(Failure(), request, spider)\n             return scrape_func(response, request, spider)\n \n         def process_spider_exception(_failure):\n             exception = _failure.value\n+            # don't handle InvalidOutput exception\n+            if isinstance(exception, InvalidOutput):\n+                return _failure\n             for method in self.methods['process_spider_exception']:\n                 result = method(response=response, exception=exception, spider=spider)\n-                assert result is None or _isiterable(result), \\\n-                    'Middleware %s must returns None, or an iterable object, got %s ' % \\\n-                    (fname(method), type(result))\n+                if result is not None and not _isiterable(result):\n+                    raise InvalidOutput('Middleware {} must return None or an iterable ' \\\n+                        'object, got {}'.format(fname(method), type(result)))\n+                # stop exception handling by handing control over to the\n+                # process_spider_output chain if an iterable has been returned\n                 if result is not None:\n                     return result\n             return _failure\n \n         def process_spider_output(result):\n+            def wrapper(result_iterable):\n+                try:\n+                    for r in result_iterable:\n+                        yield r\n+                except Exception as ex:\n+                    exception_result = process_spider_exception(Failure(ex))\n+                    if exception_result is None or isinstance(exception_result, Failure):\n+                        raise\n+                    for output in exception_result:\n+                        yield output\n             for method in self.methods['process_spider_output']:\n                 result = method(response=response, result=result, spider=spider)\n-                assert _isiterable(result), \\\n-                    'Middleware %s must returns an iterable object, got %s ' % \\\n-                    (fname(method), type(result))\n+                if _isiterable(result):\n+                    result = wrapper(result)\n+                else:\n+                    raise InvalidOutput('Middleware {} must return an iterable object, ' \\\n+                        'got {}'.format(fname(method), type(result)))\n             return result\n \n         dfd = mustbe_deferred(process_spider_input, response)\n\n@@ -11,6 +11,11 @@ class NotConfigured(Exception):\n     \"\"\"Indicates a missing configuration situation\"\"\"\n     pass\n \n+class InvalidOutput(TypeError):\n+    \"\"\"Indicates an invalid value has been returned\n+    by a middleware's processing method\"\"\"\n+    pass\n+\n # HTTP and crawling\n \n class IgnoreRequest(Exception):\n\n@@ -0,0 +1,340 @@\n+\n+import logging\n+\n+from testfixtures import LogCapture\n+from twisted.trial.unittest import TestCase\n+from twisted.internet import defer\n+\n+from scrapy.spiders import Spider\n+from scrapy.item import Item, Field\n+from scrapy.http import Request\n+from scrapy.utils.test import get_crawler\n+\n+\n+class TestItem(Item):\n+    value = Field()\n+\n+\n+# ================================================================================\n+# exceptions from a spider's parse method\n+class BaseExceptionFromParseMethodSpider(Spider):\n+    start_urls = [\"http://example.com/\"]\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {'tests.test_spidermiddleware.CatchExceptionMiddleware': 540}\n+    }\n+\n+\n+class NotAGeneratorSpider(BaseExceptionFromParseMethodSpider):\n+    \"\"\" return value is NOT a generator \"\"\"\n+    name = 'not_a_generator'\n+\n+    def parse(self, response):\n+        raise AssertionError\n+\n+\n+class GeneratorErrorBeforeItemsSpider(BaseExceptionFromParseMethodSpider):\n+    \"\"\" return value is a generator; the exception is raised\n+    before the items are yielded: no items should be scraped \"\"\"\n+    name = 'generator_error_before_items'\n+\n+    def parse(self, response):\n+        raise ValueError\n+        for i in range(3):\n+            yield {'value': i}\n+\n+\n+class GeneratorErrorAfterItemsSpider(BaseExceptionFromParseMethodSpider):\n+    \"\"\" return value is a generator; the exception is raised\n+    after the items are yielded: 3 items should be scraped \"\"\"\n+    name = 'generator_error_after_items'\n+\n+    def parse(self, response):\n+        for i in range(3):\n+            yield {'value': i}\n+        raise FloatingPointError\n+\n+\n+class CatchExceptionMiddleware(object):\n+    def process_spider_exception(self, response, exception, spider):\n+        \"\"\" catch an exception and log it \"\"\"\n+        logging.warn('{} exception caught'.format(exception.__class__.__name__))\n+        return None\n+\n+\n+# ================================================================================\n+# exception from a previous middleware's process_spider_input method\n+# process_spider_input is not expected to return an iterable, so there are no\n+# separate tests for generator/non-generator implementations\n+class FromPreviousMiddlewareInputSpider(Spider):\n+    start_urls = [\"http://example.com/\"]\n+    name = 'not_a_generator_from_previous_middleware_input'\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            # engine side\n+            'tests.test_spidermiddleware.CatchExceptionMiddleware': 540,\n+            'tests.test_spidermiddleware.RaiseExceptionOnInputMiddleware': 545,\n+            # spider side\n+        }\n+    }\n+\n+    def parse(self, response):\n+        return None\n+\n+\n+class RaiseExceptionOnInputMiddleware(object):\n+    def process_spider_input(self, response, spider):\n+        raise LookupError\n+\n+\n+# ================================================================================\n+# exception from a previous middleware's process_spider_output method (not a generator)\n+class NotAGeneratorFromPreviousMiddlewareOutputSpider(Spider):\n+    start_urls = [\"http://example.com/\"]\n+    name = 'not_a_generator_from_previous_middleware_output'\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            # engine side\n+            'tests.test_spidermiddleware.CatchExceptionMiddleware': 540,\n+            'tests.test_spidermiddleware.RaiseExceptionOnOutputNotAGeneratorMiddleware': 545,\n+            # spider side\n+        }\n+    }\n+\n+    def parse(self, response):\n+        return [{'value': i} for i in range(3)]\n+\n+\n+class RaiseExceptionOnOutputNotAGeneratorMiddleware(object):\n+    def process_spider_output(self, response, result, spider):\n+        raise UnicodeError\n+\n+\n+# ================================================================================\n+# exception from a previous middleware's process_spider_output method (generator)\n+class GeneratorFromPreviousMiddlewareOutputSpider(Spider):\n+    start_urls = [\"http://example.com/\"]\n+    name = 'generator_from_previous_middleware_output'\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            # engine side\n+            'tests.test_spidermiddleware.CatchExceptionMiddleware': 540,\n+            'tests.test_spidermiddleware.RaiseExceptionOnOutputGeneratorMiddleware': 545,\n+            # spider side\n+        }\n+    }\n+\n+    def parse(self, response):\n+        return [{'value': i} for i in range(10, 13)]\n+\n+\n+class RaiseExceptionOnOutputGeneratorMiddleware(object):\n+    def process_spider_output(self, response, result, spider):\n+        for r in result:\n+            yield r\n+        raise NameError\n+\n+\n+# ================================================================================\n+# do something useful from the exception handler\n+class DoSomethingSpider(Spider):\n+    start_urls = [\"http://example.com\"]\n+    name = 'do_something'\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            # engine side\n+            'tests.test_spidermiddleware.DoSomethingMiddleware': 540,\n+            'tests.test_spidermiddleware.CatchExceptionMiddleware': 545,\n+            # spider side\n+        }\n+    }\n+\n+    def parse(self, response):\n+        yield {'value': response.url}\n+        raise ImportError\n+\n+\n+class DoSomethingMiddleware(object):\n+    def process_spider_exception(self, response, exception, spider):\n+        return [Request('http://example.org'), {'value': 10}, TestItem(value='asdf')]\n+\n+\n+# ================================================================================\n+# don't catch InvalidOutput from scrapy's spider middleware manager\n+class InvalidReturnValueFromPreviousMiddlewareInputSpider(Spider):\n+    start_urls = [\"http://example.com/\"]\n+    name = 'invalid_return_value_from_previous_middleware_input'\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            # engine side\n+            'tests.test_spidermiddleware.InvalidReturnValueInputMiddleware': 540,\n+            'tests.test_spidermiddleware.CatchExceptionMiddleware': 545,\n+            # spider side\n+        }\n+    }\n+\n+    def parse(self, response):\n+        return None\n+\n+\n+class InvalidReturnValueInputMiddleware(object):\n+    def process_spider_input(self, response, spider):\n+        return 1.0  # <type 'float'>, not None\n+\n+\n+class InvalidReturnValueFromPreviousMiddlewareOutputSpider(Spider):\n+    start_urls = [\"http://example.com/\"]\n+    name = 'invalid_return_value_from_previous_middleware_output'\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            # engine side\n+            'tests.test_spidermiddleware.CatchExceptionMiddleware': 540,\n+            'tests.test_spidermiddleware.InvalidReturnValueOutputMiddleware': 545,\n+            # spider side\n+        }\n+    }\n+\n+    def parse(self, response):\n+        return None\n+\n+\n+class InvalidReturnValueOutputMiddleware(object):\n+    def process_spider_output(self, response, result, spider):\n+        return 1  # <type 'int'>, not an iterable\n+\n+\n+# ================================================================================\n+# make sure only non already called process_spider_output methods\n+# are called if process_spider_exception returns an iterable\n+class ExecutionChainSpider(Spider):\n+    start_urls = [\"http://example.com\"]\n+    name = 'execution_chain'\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            # engine side\n+            'tests.test_spidermiddleware.ThirdMiddleware': 540,\n+            'tests.test_spidermiddleware.SecondMiddleware': 541,\n+            'tests.test_spidermiddleware.FirstMiddleware': 542\n+            # spider side\n+        },\n+    }\n+\n+    def parse(self, response):\n+        return None\n+\n+\n+class FirstMiddleware(object):\n+    def process_spider_output(self, response, result, spider):\n+        for r in result:\n+            if isinstance(r, dict):\n+                r['handled_by_first_middleware'] = True\n+            yield r\n+\n+    def process_spider_exception(self, response, exception, spider):\n+        # log exception, handle control to the next middleware's process_spider_exception\n+        logging.warn('{} exception caught'.format(exception.__class__.__name__))\n+        return None\n+\n+\n+class SecondMiddleware(object):\n+    def process_spider_output(self, response, result, spider):\n+        for r in result:\n+            if isinstance(r, dict):\n+                r['handled_by_second_middleware'] = True\n+            yield r\n+        raise MemoryError\n+\n+\n+class ThirdMiddleware(object):\n+    def process_spider_output(self, response, result, spider):\n+        for r in result:\n+            if isinstance(r, dict):\n+                r['handled_by_third_middleware'] = True\n+            yield r\n+\n+    def process_spider_exception(self, response, exception, spider):\n+        # handle control to the next middleware's process_spider_output\n+        return [{'item': i} for i in range(3)]\n+\n+\n+class TestSpiderMiddleware(TestCase):\n+\n+    @defer.inlineCallbacks\n+    def test_process_spider_exception_from_parse_method(self):\n+        # non-generator return value\n+        crawler = get_crawler(NotAGeneratorSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl()\n+        self.assertIn(\"AssertionError exception caught\", str(log))\n+        self.assertIn(\"spider_exceptions/AssertionError\", str(log))\n+        # generator return value, no items before the error\n+        crawler = get_crawler(GeneratorErrorBeforeItemsSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl()\n+        self.assertIn(\"ValueError exception caught\", str(log))\n+        self.assertIn(\"spider_exceptions/ValueError\", str(log))\n+        # generator return value, 3 items before the error\n+        crawler = get_crawler(GeneratorErrorAfterItemsSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl()\n+        self.assertIn(\"'item_scraped_count': 3\", str(log))\n+        self.assertIn(\"FloatingPointError exception caught\", str(log))\n+        self.assertIn(\"spider_exceptions/FloatingPointError\", str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_process_spider_exception_from_previous_middleware_input(self):\n+        crawler = get_crawler(FromPreviousMiddlewareInputSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl()\n+        self.assertIn(\"LookupError exception caught\", str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_process_spider_exception_from_previous_middleware_output(self):\n+        # non-generator output value\n+        crawler = get_crawler(NotAGeneratorFromPreviousMiddlewareOutputSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl()\n+        self.assertNotIn(\"UnicodeError exception caught\", str(log))\n+        # generator output value\n+        crawler = get_crawler(GeneratorFromPreviousMiddlewareOutputSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl()\n+        self.assertIn(\"'item_scraped_count': 3\", str(log))\n+        self.assertIn(\"NameError exception caught\", str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_process_spider_exception_do_something(self):\n+        crawler = get_crawler(DoSomethingSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl()\n+        self.assertIn(\"ImportError exception caught\", str(log))\n+        self.assertIn(\"{'value': 10}\", str(log))\n+        self.assertIn(\"{'value': 'asdf'}\", str(log))\n+        self.assertIn(\"{'value': 'http://example.com'}\", str(log))\n+        self.assertIn(\"{'value': 'http://example.org'}\", str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_process_spider_exception_invalid_return_value_previous_middleware(self):\n+        \"\"\" don't catch InvalidOutput from middleware \"\"\"\n+        # on middleware's input\n+        crawler1 = get_crawler(InvalidReturnValueFromPreviousMiddlewareInputSpider)\n+        with LogCapture() as log1:\n+            yield crawler1.crawl()\n+        self.assertNotIn(\"InvalidOutput exception caught\", str(log1))\n+        self.assertIn(\"'spider_exceptions/InvalidOutput'\", str(log1))\n+        # on middleware's output\n+        crawler2 = get_crawler(InvalidReturnValueFromPreviousMiddlewareOutputSpider)\n+        with LogCapture() as log2:\n+            yield crawler2.crawl()\n+        self.assertNotIn(\"InvalidOutput exception caught\", str(log2))\n+        self.assertIn(\"'spider_exceptions/InvalidOutput'\", str(log2))\n+\n+    @defer.inlineCallbacks\n+    def test_process_spider_exception_execution_chain(self):\n+        # on middleware's input\n+        crawler1 = get_crawler(ExecutionChainSpider)\n+        with LogCapture() as log1:\n+            yield crawler1.crawl()\n+        self.assertNotIn(\"handled_by_first_middleware\", str(log1))\n+        self.assertNotIn(\"handled_by_second_middleware\", str(log1))\n+        self.assertIn(\"MemoryError exception caught\", str(log1))\n+        self.assertIn(\"handled_by_third_middleware\", str(log1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
