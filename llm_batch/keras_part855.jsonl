{"custom_id": "keras#fcb3c276e0e1af94303013cbe809cac5905b1327", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 290 | Lines Deleted: 102 | Files Changed: 4 | Hunks: 42 | Methods Changed: 45 | Complexity Δ (Sum/Max): 29/23 | Churn Δ: 392 | Churn Cumulative: 1882 | Contributors (this commit): 10 | Commits (past 90d): 26 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,5 @@\n import collections\n import itertools\n-from functools import partial\n \n import jax\n import numpy as np\n@@ -224,108 +223,109 @@ class JAXTrainer(base_trainer.Trainer):\n         )\n         return outputs, non_trainable_variables\n \n+    def _make_function(self, step_function, concatenate_outputs=False):\n+        if self.steps_per_execution > 1:\n+            if concatenate_outputs:\n+\n+                def concatenate(outputs):\n+                    output = outputs[0]\n+                    for next_output in outputs[1:]:\n+                        output = tree.map_structure(\n+                            lambda t1, t2: jax.numpy.concatenate([t1, t2]),\n+                            output,\n+                            next_output,\n+                        )\n+                    return output\n+\n+                if not self.run_eagerly and self.jit_compile:\n+                    concatenate = jax.jit(concatenate)\n+\n+                def iterator_step(state, iterator):\n+                    data = next(iterator)\n+                    outputs, state = step_function(state, data)\n+                    outputs = [outputs]\n+                    try:\n+                        for _ in range(self.steps_per_execution - 1):\n+                            data = next(iterator)\n+                            _outputs, state = step_function(state, data)\n+                            outputs.append(_outputs)\n+                    except StopIteration:\n+                        pass\n+                    outputs = concatenate(outputs)\n+                    return outputs, state\n+\n+            else:\n+\n+                def iterator_step(state, iterator):\n+                    data = next(iterator)\n+                    outputs, state = step_function(state, data)\n+                    try:\n+                        for _ in range(self.steps_per_execution - 1):\n+                            data = next(iterator)\n+                            outputs, state = step_function(state, data)\n+                    except StopIteration:\n+                        pass\n+                    return outputs, state\n+\n+        else:\n+\n+            def iterator_step(state, iterator):\n+                return step_function(state, next(iterator))\n+\n+        return iterator_step\n+\n     def make_train_function(self, force=False):\n         if self.train_function is not None and not force:\n             return\n-\n-        def one_train_step(state, data):\n-            data = data[0]\n-            return self.train_step(state, data)\n-\n-        def multi_train_steps(state, data):\n-            for single_step_data in data:\n-                logs, state = one_train_step(state, [single_step_data])\n-            return logs, state\n-\n-        if self.steps_per_execution > 1:\n-            train_step = multi_train_steps\n-        else:\n-            train_step = one_train_step\n-\n         if not self.run_eagerly and self.jit_compile:\n-            # Note that we mark the state and data to be donated to jax,\n+            # Note that we mark the state to be donated to jax,\n             # so that jax will reuse the memory buffer for outputs.\n             # This will reduce the memory usage of the training function by\n             # half.\n-            @partial(jax.jit, donate_argnames=\"state\")\n-            def compiled_train_step(state, data):\n-                return train_step(state, data)\n-\n-            self.train_function = compiled_train_step\n-\n+            train_step = jax.jit(self.train_step, donate_argnums=0)\n         else:\n-            self.train_function = train_step\n+            train_step = self.train_step\n+\n+        step_function = self._make_function(train_step)\n+\n+        self.train_function = step_function\n \n     def make_test_function(self, force=False):\n         if self.test_function is not None and not force:\n             return\n-\n-        def one_test_step(state, data):\n-            data = data[0]\n-            return self.test_step(state, data)\n-\n-        def multi_test_steps(state, data):\n-            for single_step_data in data:\n-                logs, state = one_test_step(state, [single_step_data])\n-            return logs, state\n-\n-        if self.steps_per_execution > 1:\n-            test_step = multi_test_steps\n-        else:\n-            test_step = one_test_step\n-\n         if not self.run_eagerly and self.jit_compile:\n-            # Note that we mark the state and data to be donated to jax,\n+            # Note that we mark the state to be donated to jax,\n             # so that jax will reuse the memory buffer for outputs.\n             # This will reduce the memory usage of the training function by\n             # half.\n-            @partial(jax.jit, donate_argnames=\"state\")\n-            def compiled_test_step(state, data):\n-                return test_step(state, data)\n-\n-            self.test_function = compiled_test_step\n-\n+            test_step = jax.jit(self.test_step, donate_argnums=0)\n         else:\n-            self.test_function = test_step\n+            test_step = self.test_step\n+\n+        step_function = self._make_function(test_step)\n+\n+        self.test_function = step_function\n \n     def make_predict_function(self, force=False):\n         if self.predict_function is not None and not force:\n             return self.predict_function\n \n-        def one_predict_step(state, data):\n-            data = data[0]\n-            return self.predict_step(state, data)\n-\n-        def multi_predict_steps(state, data):\n-            outputs, trainable_variables = one_predict_step(state, data[:1])\n-\n-            for single_step_data in data[1:]:\n-                step_outputs, trainable_variables = one_predict_step(\n-                    state,\n-                    [single_step_data],\n-                )\n-                outputs = tree.map_structure(\n-                    lambda t1, t2: jax.numpy.concatenate([t1, t2]),\n-                    outputs,\n-                    step_outputs,\n-                )\n-            return outputs, trainable_variables\n-\n-        if self.steps_per_execution > 1:\n-            predict_step = multi_predict_steps\n-        else:\n-            predict_step = one_predict_step\n+        def predict_step(state, data):\n+            outputs, non_trainable_variables = self.predict_step(state, data)\n+            return outputs, (state[0], non_trainable_variables)\n \n         if not self.run_eagerly and self.jit_compile:\n+            predict_step = jax.jit(predict_step)\n \n-            @jax.jit\n-            def compiled_predict_step(state, data):\n-                return predict_step(state, data)\n+        _step_function = self._make_function(\n+            predict_step, concatenate_outputs=True\n+        )\n \n-            self.predict_function = compiled_predict_step\n+        def step_function(state, iterator):\n+            outputs, state = _step_function(state, iterator)\n+            return outputs, state[1]\n \n-        else:\n-            self.predict_function = predict_step\n+        self.predict_function = step_function\n \n     @traceback_utils.filter_traceback\n     def fit(\n@@ -406,7 +406,8 @@ class JAXTrainer(base_trainer.Trainer):\n             callbacks.on_epoch_begin(epoch)\n \n             self._jax_state_synced = True\n-            for step, data in epoch_iterator:\n+            with epoch_iterator.catch_stop_iteration():\n+                for step, iterator in epoch_iterator:\n                     # Callbacks\n                     callbacks.on_train_batch_begin(step)\n \n@@ -422,7 +423,7 @@ class JAXTrainer(base_trainer.Trainer):\n                         )\n                         self._jax_state_synced = False\n \n-                logs, state = self.train_function(state, data)\n+                    logs, state = self.train_function(state, iterator)\n                     (\n                         trainable_variables,\n                         non_trainable_variables,\n@@ -562,7 +563,8 @@ class JAXTrainer(base_trainer.Trainer):\n         self.reset_metrics()\n \n         self._jax_state_synced = True\n-        for step, data in epoch_iterator:\n+        with epoch_iterator.catch_stop_iteration():\n+            for step, iterator in epoch_iterator:\n                 callbacks.on_test_batch_begin(step)\n \n                 if self._jax_state_synced:\n@@ -575,7 +577,7 @@ class JAXTrainer(base_trainer.Trainer):\n                     )\n                     self._jax_state_synced = False\n \n-            logs, state = self.test_function(state, data)\n+                logs, state = self.test_function(state, iterator)\n                 (\n                     trainable_variables,\n                     non_trainable_variables,\n@@ -627,13 +629,15 @@ class JAXTrainer(base_trainer.Trainer):\n \n         if not all(layer.built for layer in self._flatten_layers()):\n             # Build the model on one batch of data.\n-            for _, data in epoch_iterator:\n+            for _, iterator in epoch_iterator:\n                 # Build model\n-                x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data[0])\n+                x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(\n+                    next(iterator)\n+                )\n                 with backend.StatelessScope():\n                     self(x)\n                 break\n-\n+            epoch_iterator.reset()\n         # Container that configures and calls callbacks.\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n             callbacks = callbacks_module.CallbackList(\n@@ -669,7 +673,8 @@ class JAXTrainer(base_trainer.Trainer):\n         self._jax_state_synced = True\n         outputs = None\n         non_trainable_variables = None\n-        for step, x in epoch_iterator:\n+        with epoch_iterator.catch_stop_iteration():\n+            for step, iterator in epoch_iterator:\n                 callbacks.on_predict_batch_begin(step)\n                 if self._jax_state_synced:\n                     # The state may have been synced by a callback.\n@@ -682,7 +687,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 else:\n                     state = (state[0], non_trainable_variables)\n                 batch_outputs, non_trainable_variables = self.predict_function(\n-                state, x\n+                    state, iterator\n                 )\n                 outputs = append_to_outputs(batch_outputs, outputs)\n \n@@ -722,11 +727,12 @@ class JAXTrainer(base_trainer.Trainer):\n             sample_weight = data_adapter_utils.class_weight_to_sample_weights(\n                 y, class_weight\n             )\n-        data = (x, y, sample_weight)\n-        data = _distribute_data(data)\n+\n+        def data():\n+            yield _distribute_data((x, y, sample_weight))\n \n         # Maybe build model\n-        self._symbolic_build(data_batch=data)\n+        self._symbolic_build(data_batch=next(data()))\n         self._record_training_state_sharding_spec()\n         self.make_train_function()\n \n@@ -739,7 +745,7 @@ class JAXTrainer(base_trainer.Trainer):\n             purge_model_variables=False,\n         )\n         self._jax_state_synced = False\n-        logs, state = self.train_function(state, [data])\n+        logs, state = self.train_function(state, data())\n \n         # State sync\n         (\n@@ -771,10 +777,11 @@ class JAXTrainer(base_trainer.Trainer):\n     ):\n         self._assert_compile_called(\"test_on_batch\")\n \n-        data = (x, y, sample_weight)\n-        data = _distribute_data(data)\n+        def data():\n+            yield _distribute_data((x, y, sample_weight))\n+\n         # Maybe build model\n-        self._symbolic_build(data_batch=data)\n+        self._symbolic_build(data_batch=next(data()))\n         self._record_training_state_sharding_spec()\n         self.make_test_function()\n \n@@ -786,7 +793,7 @@ class JAXTrainer(base_trainer.Trainer):\n             purge_model_variables=False,\n         )\n         self._jax_state_synced = False\n-        logs, state = self.test_function(state, [data])\n+        logs, state = self.test_function(state, data())\n \n         # State sync\n         trainable_variables, non_trainable_variables, metrics_variables = state\n@@ -818,8 +825,12 @@ class JAXTrainer(base_trainer.Trainer):\n             purge_model_variables=False,\n         )\n         self._jax_state_synced = False\n+\n+        def data():\n+            yield (x,)\n+\n         batch_outputs, non_trainable_variables = self.predict_function(\n-            state, [(x,)]\n+            state, data()\n         )\n         self._jax_state = {\n             \"non_trainable_variables\": non_trainable_variables,\n@@ -992,6 +1003,9 @@ def _distribute_data(data, layouts=None):\n \n \n class JAXEpochIterator(EpochIterator):\n+    def __next__(self):\n+        return next(self._epoch_iterator)\n+\n     def _get_iterator(self):\n         distribution = distribution_lib.distribution()\n         if distribution is not None:\n\n@@ -23,10 +23,10 @@ class GeneratorDataAdapter(DataAdapter):\n             )\n \n     def get_numpy_iterator(self):\n-        return data_adapter_utils.get_numpy_iterator(self.generator)\n+        return data_adapter_utils.get_numpy_iterator(self.generator())\n \n     def get_jax_iterator(self):\n-        return data_adapter_utils.get_jax_iterator(self.generator)\n+        return data_adapter_utils.get_jax_iterator(self.generator())\n \n     def get_tf_dataset(self):\n         from keras.src.utils.module_utils import tensorflow as tf\n@@ -49,7 +49,7 @@ class GeneratorDataAdapter(DataAdapter):\n             return x\n \n         def get_tf_iterator():\n-            for batch in self.generator:\n+            for batch in self.generator():\n                 batch = tree.map_structure(\n                     convert_to_tf, batch, self._output_signature\n                 )\n@@ -67,7 +67,7 @@ class GeneratorDataAdapter(DataAdapter):\n         return ds\n \n     def get_torch_dataloader(self):\n-        return data_adapter_utils.get_torch_dataloader(self.generator)\n+        return data_adapter_utils.get_torch_dataloader(self.generator())\n \n     @property\n     def num_batches(self):\n@@ -84,4 +84,4 @@ def peek_and_restore(generator):\n             generator, data_adapter_utils.NUM_BATCHES_FOR_TENSOR_SPEC\n         )\n     )\n-    return batches, itertools.chain(batches, generator)\n+    return batches, lambda: itertools.chain(batches, generator)\n\n@@ -1050,8 +1050,11 @@ class Trainer:\n                 )\n \n             if data_batch is None:\n-                for _, data in iterator:\n-                    data_batch = data[0]\n+                for _, data_or_iterator in iterator:\n+                    if isinstance(data_or_iterator, (list, tuple)):\n+                        data_batch = data_or_iterator[0]\n+                    else:\n+                        data_batch = next(data_or_iterator)\n                     break\n             data_batch = tree.map_structure(to_symbolic_input, data_batch)\n             (\n\n@@ -655,7 +655,11 @@ class TestTrainer(testing.TestCase):\n             jit_compile=False,\n         )\n         dataset = sparse_generator(generator_type)\n-        model.predict(dataset)\n+        dataset_size = sum(\n+            [batch[1].shape[0] for batch in sparse_generator(generator_type)]\n+        )\n+        y = model.predict(dataset)\n+        self.assertEqual(len(y), dataset_size)\n \n     @pytest.mark.skipif(\n         backend.backend() != \"jax\",\n@@ -782,6 +786,100 @@ class TestTrainer(testing.TestCase):\n         )\n         self.assertAllClose(model.evaluate(x, y), model_2.evaluate(x, y))\n \n+    @parameterized.named_parameters(\n+        named_product(\n+            steps_per_execution=[1, 50], mode=[\"eager\", \"non_jit\", \"jit\"]\n+        )\n+    )\n+    def test_predict_preserve_order(self, steps_per_execution, mode):\n+        if steps_per_execution > 1 and backend.backend() == \"torch\":\n+            self.skipTest(\"`steps_per_execution` not implemented for torch yet\")\n+\n+        def generate_uneven_batches():\n+            batch_sizes = [2, 3, 4]\n+\n+            def gen_i():\n+                for i in range(100):\n+                    yield i\n+\n+            iterator = iter(gen_i())\n+            j = 0\n+            while True:\n+                batch_size = batch_sizes[j % len(batch_sizes)]\n+                try:\n+                    batch = np.array(\n+                        [next(iterator) for _ in range(batch_size)]\n+                    )\n+                except StopIteration:\n+                    break\n+                j += 1\n+                yield batch\n+\n+        from keras.src.utils.module_utils import tensorflow as tf\n+\n+        dataset = tf.data.Dataset.from_generator(\n+            generate_uneven_batches,\n+            output_signature=tf.TensorSpec((None,), dtype=tf.int32),\n+        )\n+        x = keras.layers.Input(shape=())\n+        y = keras.layers.Identity()(x)\n+        model = keras.Model(x, y)\n+        model.compile(\n+            loss=\"mse\",\n+            optimizer=\"sgd\",\n+            steps_per_execution=steps_per_execution,\n+            run_eagerly=(mode == \"eager\"),\n+            jit_compile=(mode == \"jit\"),\n+        )\n+\n+        preds = model.predict(x=dataset, verbose=0)\n+\n+        self.assertAllEqual(preds, np.arange(len(preds), dtype=np.float32))\n+\n+    @parameterized.named_parameters(\n+        named_product(\n+            steps_per_execution=[1, 50], mode=[\"eager\", \"non_jit\", \"jit\"]\n+        )\n+    )\n+    def test_predict_generator(self, steps_per_execution, mode):\n+        if steps_per_execution > 1 and backend.backend() == \"torch\":\n+            self.skipTest(\"`steps_per_execution` not implemented for torch yet\")\n+\n+        batch_size = 2\n+\n+        def generate_batches():\n+            def gen_i():\n+                for i in range(10):\n+                    yield i\n+\n+            iterator = iter(gen_i())\n+            j = 0\n+            while True:\n+                try:\n+                    batch = np.array(\n+                        [next(iterator) for _ in range(batch_size)]\n+                    )\n+                except StopIteration:\n+                    break\n+                j += 1\n+                yield (batch,)\n+\n+        model = keras.Sequential(\n+            [keras.layers.InputLayer(shape=()), keras.layers.Identity()]\n+        )\n+        model.compile(\n+            loss=\"mse\",\n+            optimizer=\"sgd\",\n+            steps_per_execution=steps_per_execution,\n+            run_eagerly=(mode == \"eager\"),\n+            jit_compile=(mode == \"jit\"),\n+        )\n+\n+        preds = model.predict(x=generate_batches(), verbose=0)\n+        self.assertAllEqual(\n+            preds, np.concatenate(list(generate_batches()), axis=1)[0]\n+        )\n+\n     @parameterized.named_parameters(\n         named_product(\n             steps_per_execution=[3, 101], mode=[\"eager\", \"non_jit\", \"jit\"]\n@@ -2277,6 +2375,79 @@ class TestTrainer(testing.TestCase):\n         self.assertFalse(model.jit_compile)\n         disable_op_determinism()\n \n+    @pytest.mark.requires_trainable_backend\n+    @pytest.mark.skipif(\n+        backend.backend() == \"torch\",\n+        reason=\"`steps_per_execution` not implemented for torch yet\",\n+    )\n+    def test_retracing(self):\n+        x = np.ones((100, 4))\n+        y = np.ones((100, 1))\n+\n+        input = keras.Input(shape=[4])\n+        output = keras.layers.Dense(1, activation=\"relu\")(input)\n+\n+        tracing_count = [0]\n+\n+        class TracingCounterModel(keras.Model):\n+            def train_step(self, *args):\n+                tracing_count[0] = tracing_count[0] + 1\n+                return super().train_step(*args)\n+\n+        model = TracingCounterModel(inputs=input, outputs=output)\n+        model.compile(\n+            loss=\"mse\",\n+            optimizer=\"adam\",\n+            steps_per_execution=20,\n+        )\n+\n+        epochs = 1\n+        model.fit(\n+            x=x,\n+            y=y,\n+            batch_size=1,\n+            epochs=epochs,\n+            verbose=0,\n+        )\n+        self.assertLessEqual(tracing_count[0], 2)\n+\n+    @pytest.mark.requires_trainable_backend\n+    @pytest.mark.skipif(\n+        backend.backend() == \"torch\",\n+        reason=\"`steps_per_execution` not implemented for torch yet\",\n+    )\n+    @pytest.mark.skipif(\n+        backend.backend() == \"tensorflow\",\n+        reason=\"`predict_function` with `steps_per_execution` is not \"\n+        \"optimized for tensorflow yet\",\n+    )\n+    def test_retracing_predict(self):\n+        x = np.ones((100, 4))\n+\n+        input = keras.Input(shape=[4])\n+        output = keras.layers.Dense(1, activation=\"relu\")(input)\n+\n+        tracing_count = [0]\n+\n+        class TracingCounterModel(keras.Model):\n+            def predict_step(self, *args):\n+                tracing_count[0] = tracing_count[0] + 1\n+                return super().predict_step(*args)\n+\n+        model = TracingCounterModel(inputs=input, outputs=output)\n+        model.compile(\n+            loss=\"mse\",\n+            optimizer=\"adam\",\n+            steps_per_execution=20,\n+        )\n+\n+        model.predict(\n+            x=x,\n+            batch_size=1,\n+            verbose=0,\n+        )\n+        self.assertLessEqual(tracing_count[0], 2)\n+\n \n class TrainerDistributeTest(testing.TestCase):\n     @pytest.mark.skipif(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5e37be81cca83253fd20789384f5c4a9c0efbc35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 60 | Churn Cumulative: 285 | Contributors (this commit): 4 | Commits (past 90d): 22 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,6 +18,7 @@ from keras.src.activations.activations import hard_silu as hard_swish\n from keras.src.activations.activations import hard_tanh\n from keras.src.activations.activations import leaky_relu\n from keras.src.activations.activations import linear\n+from keras.src.activations.activations import log_sigmoid\n from keras.src.activations.activations import log_softmax\n from keras.src.activations.activations import mish\n from keras.src.activations.activations import relu\n\n@@ -18,6 +18,7 @@ from keras.src.activations.activations import hard_silu as hard_swish\n from keras.src.activations.activations import hard_tanh\n from keras.src.activations.activations import leaky_relu\n from keras.src.activations.activations import linear\n+from keras.src.activations.activations import log_sigmoid\n from keras.src.activations.activations import log_softmax\n from keras.src.activations.activations import mish\n from keras.src.activations.activations import relu\n\n@@ -10,6 +10,7 @@ from keras.src.activations.activations import hard_silu\n from keras.src.activations.activations import hard_tanh\n from keras.src.activations.activations import leaky_relu\n from keras.src.activations.activations import linear\n+from keras.src.activations.activations import log_sigmoid\n from keras.src.activations.activations import log_softmax\n from keras.src.activations.activations import mish\n from keras.src.activations.activations import relu\n@@ -47,6 +48,7 @@ ALL_OBJECTS = {\n     linear,\n     mish,\n     log_softmax,\n+    log_sigmoid,\n }\n \n ALL_OBJECTS_DICT = {fn.__name__: fn for fn in ALL_OBJECTS}\n\n@@ -433,6 +433,19 @@ def hard_sigmoid(x):\n     return ops.hard_sigmoid(x)\n \n \n+@keras_export(\"keras.activations.log_sigmoid\")\n+def log_sigmoid(x):\n+    \"\"\"Logarithm of the sigmoid activation function.\n+\n+    It is defined as `f(x) = log(1 / (1 + exp(-x)))`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    \"\"\"\n+    return ops.log_sigmoid(x)\n+\n+\n @keras_export([\"keras.activations.hard_silu\", \"keras.activations.hard_swish\"])\n def hard_silu(x):\n     \"\"\"Hard SiLU activation function, also known as Hard Swish.\n\n@@ -40,6 +40,10 @@ def _ref_hard_sigmoid(x):\n     return z\n \n \n+def _ref_log_sigmoid(x):\n+    return -1 * _ref_softplus(-x)\n+\n+\n def _ref_hard_silu(x):\n     return x * np.minimum(np.maximum(0.0, x + 3.0), 6.0) * (1.0 / 6.0)\n \n@@ -337,6 +341,45 @@ class ActivationsTest(testing.TestCase):\n             result_positive_above_1, expected_positive_above_1, rtol=1e-05\n         )\n \n+    def test_log_sigmoid(self):\n+        # Basic test for random values between 0 and 1\n+        x = np.random.uniform(0, 1, (2, 5))\n+        result = activations.log_sigmoid(x[np.newaxis, :])[0]\n+        expected = np.vectorize(_ref_log_sigmoid)(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+        # Test with 1D array\n+        x_1d = np.random.uniform(-10, 10, 5)\n+        result_1d = activations.log_sigmoid(x_1d)\n+        expected_1d = np.vectorize(_ref_log_sigmoid)(x_1d)\n+        self.assertAllClose(result_1d, expected_1d, rtol=1e-05)\n+\n+        # Test with 3D array\n+        x_3d = np.random.uniform(-10, 10, (3, 3, 3))\n+        result_3d = activations.log_sigmoid(x_3d)\n+        expected_3d = np.vectorize(_ref_log_sigmoid)(x_3d)\n+        self.assertAllClose(result_3d, expected_3d, rtol=1e-05)\n+\n+        # Test large positive values\n+        x_large_positive = np.random.uniform(10, 100, (2, 5))\n+        result_large_positive = activations.log_sigmoid(x_large_positive)\n+        expected_large_positive = np.vectorize(_ref_log_sigmoid)(\n+            x_large_positive\n+        )\n+        self.assertAllClose(\n+            result_large_positive, expected_large_positive, rtol=1e-05\n+        )\n+\n+        # Test large negative values\n+        x_large_negative = np.random.uniform(-100, -10, (2, 5))\n+        result_large_negative = activations.log_sigmoid(x_large_negative)\n+        expected_large_negative = np.vectorize(_ref_log_sigmoid)(\n+            x_large_negative\n+        )\n+        self.assertAllClose(\n+            result_large_negative, expected_large_negative, rtol=1e-05\n+        )\n+\n     def test_hard_silu(self):\n         # Basic test for random values between -3 and 3\n         x = np.random.uniform(-3, 3, (2, 5)).astype(\"float32\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#533ea632abcd074ac4ddd5cadc07798c4062b192", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 707 | Contributors (this commit): 9 | Commits (past 90d): 13 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -638,7 +638,7 @@ class ActivationsTest(testing.TestCase):\n \n         x = np.random.random((2, 5))\n         result = activations.celu(x[np.newaxis, :], alpha=0.5)[0]\n-        expected = celu(x, True)\n+        expected = celu(x, alpha=0.5)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n     def test_glu(self):\n\n@@ -607,7 +607,7 @@ def hard_tanh(x):\n \n     Example:\n \n-    >>> x = x = np.array([-2., -1., 0., 1., 2.])\n+    >>> x = np.array([-2., -1., 0., 1., 2.])\n     >>> x_hard_tanh = keras.ops.hard_tanh(x)\n     >>> print(x_hard_tanh)\n     array([-1. -1.  0.  1.  1.], shape=(5,), dtype=float64)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4ba7d6f0b70e90c0270f9e86fccbb2f30d8a0647", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 909 | Contributors (this commit): 9 | Commits (past 90d): 9 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -39,7 +39,7 @@ def add(x1, x2):\n     # Special case of `tf.add`: `tf.nn.bias_add`\n     # `BiasAdd` can be fused with `MatMul` and `Conv*` kernels\n     # Expecting `x1` to be `inputs` and `x2` to be `bias` (no swapping)\n-    x2_squeeze_shape = [d for d in x2.shape if d is None or d > 1]\n+    x2_squeeze_shape = [d for d in x2.shape.as_list() if d is None or d > 1]\n     if (\n         # `x2` looks like bias (can be squeezed to vector)\n         1 == len(x2_squeeze_shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#00c904f9f009918614e160ccf66c6b463c432fb7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 419 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -290,6 +290,8 @@ class PyDatasetAdapter(DataAdapter):\n                 self._standardize_batch(self.py_dataset[i])\n                 for i in range(num_samples)\n             ]\n+            if len(batches) == 0:\n+                raise ValueError(\"The PyDataset has length 0\")\n             self._output_signature = data_adapter_utils.get_tensor_spec(batches)\n \n         ds = tf.data.Dataset.from_generator(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d8c74fa789542476bfd1354a14c89fb72da0c653", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 168 | Lines Deleted: 1 | Files Changed: 7 | Hunks: 15 | Methods Changed: 9 | Complexity Δ (Sum/Max): 17/7 | Churn Δ: 169 | Churn Cumulative: 721 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 18 | DMM Complexity: 0.875\n\nDIFF:\n@@ -4,8 +4,10 @@ This file was autogenerated. Do not edit it by hand,\n since your modifications would be overwritten.\n \"\"\"\n \n+from keras.src.tree.tree_api import assert_same_paths\n from keras.src.tree.tree_api import assert_same_structure\n from keras.src.tree.tree_api import flatten\n+from keras.src.tree.tree_api import flatten_with_path\n from keras.src.tree.tree_api import is_nested\n from keras.src.tree.tree_api import lists_to_tuples\n from keras.src.tree.tree_api import map_shape_structure\n\n@@ -4,8 +4,10 @@ This file was autogenerated. Do not edit it by hand,\n since your modifications would be overwritten.\n \"\"\"\n \n+from keras.src.tree.tree_api import assert_same_paths\n from keras.src.tree.tree_api import assert_same_structure\n from keras.src.tree.tree_api import flatten\n+from keras.src.tree.tree_api import flatten_with_path\n from keras.src.tree.tree_api import is_nested\n from keras.src.tree.tree_api import lists_to_tuples\n from keras.src.tree.tree_api import map_shape_structure\n\n@@ -1,5 +1,7 @@\n+from keras.src.tree.tree_api import assert_same_paths\n from keras.src.tree.tree_api import assert_same_structure\n from keras.src.tree.tree_api import flatten\n+from keras.src.tree.tree_api import flatten_with_path\n from keras.src.tree.tree_api import is_nested\n from keras.src.tree.tree_api import lists_to_tuples\n from keras.src.tree.tree_api import map_shape_structure\n\n@@ -17,6 +17,10 @@ def flatten(structure):\n     return dmtree.flatten(structure)\n \n \n+def flatten_with_path(structure):\n+    return dmtree.flatten_with_path(structure)\n+\n+\n def map_structure(func, *structures):\n     return dmtree.map_structure(func, *structures)\n \n@@ -29,6 +33,21 @@ def assert_same_structure(a, b, check_types=True):\n     return dmtree.assert_same_structure(a, b, check_types=check_types)\n \n \n+def assert_same_paths(a, b):\n+    a_paths = set([path for path, leaf in dmtree.flatten_with_path(a)])\n+    b_paths = set([path for path, leaf in dmtree.flatten_with_path(b)])\n+\n+    if a_paths != b_paths:\n+        msg = \"`a` and `b` don't have the same paths.\"\n+        a_diff = a_paths.difference(b_paths)\n+        if a_diff:\n+            msg += f\"\\nPaths in `a` missing in `b`:\\n{a_diff}\"\n+        b_diff = b_paths.difference(a_paths)\n+        if b_diff:\n+            msg += f\"\\nPaths in `b` missing in `a`:\\n{b_diff}\"\n+        raise ValueError(msg)\n+\n+\n def pack_sequence_as(structure, flat_sequence, sequence_fn=None):\n     is_nested_fn = dmtree.is_nested\n     sequence_fn = sequence_fn or dmtree._sequence_like\n\n@@ -80,6 +80,14 @@ def flatten(structure):\n     return leaves\n \n \n+def flatten_with_path(structure):\n+    paths, leaves, _ = optree.tree_flatten_with_path(\n+        structure, none_is_leaf=True, namespace=\"keras\"\n+    )\n+    leaves_with_path = list(zip(paths, leaves))\n+    return leaves_with_path\n+\n+\n def map_structure(func, *structures):\n     if not callable(func):\n         raise TypeError(f\"`func` must be callable. Received: func={func}\")\n@@ -125,6 +133,21 @@ def assert_same_structure(a, b, check_types=True):\n             )\n \n \n+def assert_same_paths(a, b):\n+    a_paths = set(optree.tree_paths(a, none_is_leaf=True, namespace=\"keras\"))\n+    b_paths = set(optree.tree_paths(b, none_is_leaf=True, namespace=\"keras\"))\n+\n+    if a_paths != b_paths:\n+        msg = \"`a` and `b` don't have the same paths.\"\n+        a_diff = a_paths.difference(b_paths)\n+        if a_diff:\n+            msg += f\"\\nPaths in `a` missing in `b`:\\n{a_diff}\"\n+        b_diff = b_paths.difference(a_paths)\n+        if b_diff:\n+            msg += f\"\\nPaths in `b` missing in `a`:\\n{b_diff}\"\n+        raise ValueError(msg)\n+\n+\n def pack_sequence_as(structure, flat_sequence, sequence_fn=None):\n     sequence_fn = sequence_fn or _sequence_like\n \n\n@@ -121,6 +121,32 @@ def flatten(structure):\n     return tree_impl.flatten(structure)\n \n \n+@keras_export(\"keras.tree.flatten_with_path\")\n+def flatten_with_path(structure):\n+    \"\"\"Flattens a possibly nested structure into a list.\n+\n+    This is a variant of flattens() which produces a\n+    list of pairs: `(path, item)`. A path is a tuple of indices and/or keys\n+    which uniquely identifies the position of the corresponding item.\n+\n+    Dictionaries with non-sortable keys cannot be flattened.\n+\n+    Examples:\n+\n+    >>> keras.flatten_with_path([{\"foo\": 42}])\n+    [((0, 'foo'), 42)]\n+\n+\n+    Args:\n+        structure: An arbitrarily nested structure.\n+\n+    Returns:\n+        A list of `(path, item)` pairs corresponding to the flattened\n+        version of the input `structure`.\n+    \"\"\"\n+    return tree_impl.flatten_with_path(structure)\n+\n+\n @keras_export(\"keras.tree.map_structure\")\n def map_structure(func, *structures):\n     \"\"\"Maps `func` through given structures.\n@@ -205,6 +231,32 @@ def assert_same_structure(a, b, check_types=True):\n     return tree_impl.assert_same_structure(a, b, check_types=check_types)\n \n \n+@keras_export(\"keras.tree.assert_same_paths\")\n+def assert_same_paths(a, b):\n+    \"\"\"Asserts that two structures have identical paths in their tree structure.\n+\n+    This function verifies that two nested structures have the same paths.\n+    Unlike `assert_same_structure`, this function only checks the paths\n+    and ignores the nodes' types.\n+\n+    Examples:\n+    >>> keras.tree.assert_same_paths([0, 1], (2, 3))\n+    >>> Point1 = collections.namedtuple('Point1', ['x', 'y'])\n+    >>> Point2 = collections.namedtuple('Point2', ['x', 'y'])\n+    >>> keras.tree.assert_same_paths(Point1(0, 1), Point2(2, 3))\n+\n+    Args:\n+        a: an arbitrarily nested structure.\n+        b: an arbitrarily nested structure.\n+\n+    Raises:\n+        `ValueError`: If the paths in structure `a` don't match the paths\n+        in structure `b`. The error message will include the specific\n+        paths that differ.\n+    \"\"\"\n+    return tree_impl.assert_same_paths(a, b)\n+\n+\n @keras_export(\"keras.tree.pack_sequence_as\")\n def pack_sequence_as(structure, flat_sequence, sequence_fn=None):\n     \"\"\"Returns a given flattened sequence packed into a given structure.\n\n@@ -55,7 +55,6 @@ class TreeTest(testing.TestCase):\n \n     def test_flatten(self, tree_impl, is_optree):\n         structure = ((3, 4), 5, (6, 7, (9, 10), 8))\n-        flat = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n \n         self.assertEqual(\n             tree_impl.flatten(structure), [3, 4, 5, 6, 7, 9, 10, 8]\n@@ -68,6 +67,48 @@ class TreeTest(testing.TestCase):\n         self.assertEqual([5], tree_impl.flatten(5))\n         self.assertEqual([np.array([5])], tree_impl.flatten(np.array([5])))\n \n+    def test_flatten_with_path(self, tree_impl, is_optree):\n+        structure = {\"b\": (0, 1), \"a\": [2, 3]}\n+        flat_with_path = tree_impl.flatten_with_path(structure)\n+\n+        self.assertEqual(\n+            tree_impl.flatten(flat_with_path),\n+            tree_impl.flatten(\n+                [((\"a\", 0), 2), ((\"a\", 1), 3), ((\"b\", 0), 0), ((\"b\", 1), 1)]\n+            ),\n+        )\n+        point = collections.namedtuple(\"Point\", [\"x\", \"y\", \"z\"])\n+        structure = point(x=(0, 1), y=[2, 3], z={\"a\": 4})\n+        flat_with_path = tree_impl.flatten_with_path(structure)\n+\n+        if is_optree:\n+            # optree doesn't return namedtuple's field name, but the index\n+            self.assertEqual(\n+                tree_impl.flatten(flat_with_path),\n+                tree_impl.flatten(\n+                    [\n+                        ((0, 0), 0),\n+                        ((0, 1), 1),\n+                        ((1, 0), 2),\n+                        ((1, 1), 3),\n+                        ((2, \"a\"), 4),\n+                    ]\n+                ),\n+            )\n+        else:\n+            self.assertEqual(\n+                tree_impl.flatten(flat_with_path),\n+                tree_impl.flatten(\n+                    [\n+                        ((\"x\", 0), 0),\n+                        ((\"x\", 1), 1),\n+                        ((\"y\", 0), 2),\n+                        ((\"y\", 1), 3),\n+                        ((\"z\", \"a\"), 4),\n+                    ]\n+                ),\n+            )\n+\n     def test_flatten_dict_order(self, tree_impl, is_optree):\n         ordered = collections.OrderedDict(\n             [(\"d\", 3), (\"b\", 1), (\"a\", 0), (\"c\", 2)]\n@@ -225,6 +266,32 @@ class TreeTest(testing.TestCase):\n                 STRUCTURE1, structure1_list, check_types=False\n             )\n \n+    def test_assert_same_paths(self, tree_impl, is_optree):\n+        assertion_message = \"don't have the same paths\"\n+\n+        tree_impl.assert_same_paths([0, 1], (0, 1))\n+        Point1 = collections.namedtuple(\"Point1\", [\"x\", \"y\"])\n+        Point2 = collections.namedtuple(\"Point2\", [\"x\", \"y\"])\n+        tree_impl.assert_same_paths(Point1(0, 1), Point2(0, 1))\n+\n+        with self.assertRaisesRegex(ValueError, assertion_message):\n+            tree_impl.assert_same_paths(\n+                STRUCTURE1, STRUCTURE_DIFFERENT_NUM_ELEMENTS\n+            )\n+        with self.assertRaisesRegex(ValueError, assertion_message):\n+            tree_impl.assert_same_paths([0, 1], np.array([0, 1]))\n+        with self.assertRaisesRegex(ValueError, assertion_message):\n+            tree_impl.assert_same_paths(0, [0, 1])\n+        with self.assertRaisesRegex(ValueError, assertion_message):\n+            tree_impl.assert_same_paths(STRUCTURE1, STRUCTURE_DIFFERENT_NESTING)\n+        with self.assertRaisesRegex(ValueError, assertion_message):\n+            tree_impl.assert_same_paths([[3], 4], [3, [4]])\n+        with self.assertRaisesRegex(ValueError, assertion_message):\n+            tree_impl.assert_same_paths({\"a\": 1}, {\"b\": 1})\n+        structure1_list = [[[1, 2], 3], 4, [5, 6]]\n+        tree_impl.assert_same_paths(STRUCTURE1, structure1_list)\n+        tree_impl.assert_same_paths(STRUCTURE1, STRUCTURE2)\n+\n     def test_pack_sequence_as(self, tree_impl, is_optree):\n         structure = {\"key3\": \"\", \"key1\": \"\", \"key2\": \"\"}\n         flat_sequence = [\"value1\", \"value2\", \"value3\"]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2be521f150583e792cc2739d1bd729243385a915", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 60 | Churn Cumulative: 1703 | Contributors (this commit): 10 | Commits (past 90d): 20 | Contributors (cumulative): 17 | DMM Complexity: 0.8372093023255814\n\nDIFF:\n@@ -727,7 +727,8 @@ class ModelTest(testing.TestCase):\n         # Fit the model to make sure compile_metrics are built\n         with self.assertRaisesRegex(\n             KeyError,\n-            \"in the `loss` argument, can't be found in the model's output\",\n+            \"in the `loss` argument, can't be found \"\n+            \"in either the model's output\",\n         ):\n             model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n \n\n@@ -534,7 +534,11 @@ class CompileLoss(losses_module.Loss):\n             iterator = enumerate(loss)\n \n             def key_check_fn(key, objs):\n-                return all([key < len(obj) for obj in objs])\n+                try:\n+                    [obj[key] for obj in objs]\n+                except:\n+                    return False\n+                return True\n \n         else:\n             raise TypeError(\n@@ -547,7 +551,8 @@ class CompileLoss(losses_module.Loss):\n                 raise KeyError(\n                     f\"The path: {current_path + (key,)} in \"\n                     \"the `loss` argument, can't be found in \"\n-                    \"the model's output (`y_pred`).\"\n+                    \"either the model's output (`y_pred`) or in the \"\n+                    \"labels (`y_true`).\"\n                 )\n \n             self._build_nested(\n@@ -664,7 +669,8 @@ class CompileLoss(losses_module.Loss):\n         try:\n             tree.assert_same_structure(y_pred, y_true, check_types=False)\n         except ValueError:\n-            # y_true is either flat or leaf\n+            try:\n+                # Check case where y_true is either flat or leaf\n                 if (\n                     not tree.is_nested(y_true)\n                     and hasattr(y_pred, \"__len__\")\n@@ -674,10 +680,18 @@ class CompileLoss(losses_module.Loss):\n                 try:\n                     y_true = tree.pack_sequence_as(y_pred, y_true)\n                 except:\n+                    # Check case where y_true has the same structure but uses\n+                    # different (but reconcilable) container types,\n+                    # e.g `list` vs `tuple`.\n+                    tree.assert_same_paths(y_true, y_pred)\n+                    y_true = tree.pack_sequence_as(y_pred, tree.flatten(y_true))\n+            except:\n+                y_true_struct = tree.map_structure(lambda _: \"*\", y_true)\n+                y_pred_struct = tree.map_structure(lambda _: \"*\", y_pred)\n                 raise ValueError(\n                     \"y_true and y_pred have different structures.\\n\"\n-                    f\"y_true: {y_true}\\n\"\n-                    f\"y_pred: {y_pred}\\n\"\n+                    f\"y_true: {y_true_struct}\\n\"\n+                    f\"y_pred: {y_pred_struct}\\n\"\n                 )\n \n         if not self.built:\n\n@@ -490,3 +490,36 @@ class TestCompileLoss(testing.TestCase):\n             KeyError, \"can't be found in the model's output\"\n         ):\n             compile_loss.build(y_true_symb, y_pred_symb)\n+\n+    def test_different_container_types(self):\n+        y1, y2, y3 = np.array([[1]]), np.array([[2]]), np.array([[3]])\n+        y_true = ([{\"a\": y1}, {\"b\": ([y2], y3)}],)\n+        y_pred = [({\"a\": y1}, {\"b\": [(y2,), y3]})]\n+        loss = \"mse\"\n+        compile_loss = CompileLoss(loss=loss, output_names=[\"a\", \"b\", \"c\"])\n+        y_true_symb = tree.map_structure(\n+            lambda _: backend.KerasTensor((1, 1)), y_true\n+        )\n+        y_pred_symb = tree.map_structure(\n+            lambda _: backend.KerasTensor((1, 1)), y_pred\n+        )\n+        compile_loss.build(y_true_symb, y_pred_symb)\n+        compile_loss(y_true, y_pred)\n+\n+    def test_structure_mismatch(self):\n+        y_true = [np.array([[1]]), np.array([[1]])]\n+        y_pred = [np.array([[1]]), np.array([[1]])]\n+        loss = [\"mse\", \"mse\"]\n+        compile_loss = CompileLoss(loss=loss, output_names=[\"a\", \"b\"])\n+        y_true_symb = tree.map_structure(\n+            lambda _: backend.KerasTensor((1, 1)), y_true\n+        )\n+        y_pred_symb = tree.map_structure(\n+            lambda _: backend.KerasTensor((1, 1)), y_pred\n+        )\n+        compile_loss.build(y_true_symb, y_pred_symb)\n+        with self.assertRaisesRegex(\n+            ValueError, \"y_true and y_pred have different structures.\"\n+        ):\n+            wrong_struc_y_true = [np.array([[1]])]\n+            compile_loss(wrong_struc_y_true, y_pred)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#965c4a2d8c482b0cd09b9a5d71f4851c9651cfc0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 122 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 13 | Methods Changed: 9 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 129 | Churn Cumulative: 247 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: 0.896551724137931\n\nDIFF:\n@@ -380,10 +380,32 @@ class TopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n \n \n @keras_export(\"keras.metrics.sparse_top_k_categorical_accuracy\")\n-def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n+def sparse_top_k_categorical_accuracy(\n+    y_true, y_pred, k=5, from_sorted_ids=False\n+):\n+    \"\"\"Computes how often integer targets are in the top `K` predictions.\n+\n+    Args:\n+        y_true: A tensor of shape `(batch_size)` representing indices or IDs of\n+            true categories.\n+        y_pred: If `from_sorted_ids=False`, a tensor of shape\n+            `(batch_size, num_categories)` containing the scores for each sample\n+            for all possible categories. If `from_sorted_ids=True`, a tensor of\n+            shape `(batch_size, N)` containing indices or IDs of the top `N`\n+            categories in order from highest score to lowest score.\n+        k: (Optional) Number of top elements to look at for computing accuracy.\n+            Defaults to `5`.\n+        from_sorted_ids: (Optional) Whether `y_pred` is sorted category IDs or\n+            scores for all categories (the default).\n+\n+    Returns:\n+        A tensor with the same shape as `y_true` containing ones where `y_true`\n+        is in the top `k` and zeros elsewhere.\n+    \"\"\"\n     reshape_matches = False\n     y_pred = ops.convert_to_tensor(y_pred)\n-    y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n+    y_true_dtype = y_pred.dtype if from_sorted_ids else \"int32\"\n+    y_true = ops.convert_to_tensor(y_true, dtype=y_true_dtype)\n     y_true_rank = len(y_true.shape)\n     y_pred_rank = len(y_pred.shape)\n     y_true_org_shape = ops.shape(y_true)\n@@ -396,10 +418,16 @@ def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n             reshape_matches = True\n             y_true = ops.reshape(y_true, [-1])\n \n-    matches = ops.cast(\n-        ops.in_top_k(ops.cast(y_true, \"int32\"), y_pred, k=k),\n-        dtype=backend.floatx(),\n+    if from_sorted_ids:\n+        # By slicing the first k items, we assume they are sorted by score.\n+        # Reduce with `any` to count multiple matches only once.\n+        matches = ops.any(\n+            ops.equal(ops.expand_dims(y_true, axis=1), y_pred[:, :k]), axis=1\n         )\n+    else:\n+        matches = ops.in_top_k(y_true, y_pred, k=k)\n+\n+    matches = ops.cast(matches, dtype=backend.floatx())\n \n     # returned matches is expected to have same shape as y_true input\n     if reshape_matches:\n@@ -412,11 +440,33 @@ def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n class SparseTopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes how often integer targets are in the top `K` predictions.\n \n+    By default, the arguments expected by `update_state()` are:\n+    - `y_true`: a tensor of shape `(batch_size)` representing indices of true\n+        categories.\n+    - `y_pred`: a tensor of shape `(batch_size, num_categories)` containing the\n+        scores for each sample for all possible categories.\n+\n+    With `from_sorted_ids=True`, the arguments expected by `update_state` are:\n+    - `y_true`: a tensor of shape `(batch_size)` representing indices or IDs of\n+        true categories.\n+    - `y_pred`: a tensor of shape `(batch_size, N)` containing the indices or\n+        IDs of the top `N` categories sorted in order from highest score to\n+        lowest score. `N` must be greater or equal to `k`.\n+\n+    The `from_sorted_ids=True` option can be more efficient when the set of\n+    categories is very large and the model has an optimized way to retrieve the\n+    top ones either without scoring or without maintaining the scores for all\n+    the possible categories.\n+\n     Args:\n         k: (Optional) Number of top elements to look at for computing accuracy.\n             Defaults to `5`.\n         name: (Optional) string name of the metric instance.\n         dtype: (Optional) data type of the metric result.\n+        from_sorted_ids: (Optional) When `False`, the default, the tensor passed\n+            in `y_pred` contains the unsorted scores of all possible categories.\n+            When `True`, `y_pred` contains a the indices or IDs for the top\n+            categories.\n \n     Example:\n \n@@ -431,6 +481,12 @@ class SparseTopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n     >>> m.result()\n     0.3\n \n+    >>> m = keras.metrics.SparseTopKCategoricalAccuracy(k=1,\n+    ...                                                from_sorted_ids=True)\n+    >>> m.update_state([2, 1], [[1, 0, 3], [1, 2, 3]])\n+    >>> m.result()\n+    0.5\n+\n     Usage with `compile()` API:\n \n     ```python\n@@ -441,17 +497,26 @@ class SparseTopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n     \"\"\"\n \n     def __init__(\n-        self, k=5, name=\"sparse_top_k_categorical_accuracy\", dtype=None\n+        self,\n+        k=5,\n+        name=\"sparse_top_k_categorical_accuracy\",\n+        dtype=None,\n+        from_sorted_ids=False,\n     ):\n         super().__init__(\n             fn=sparse_top_k_categorical_accuracy,\n             name=name,\n             dtype=dtype,\n             k=k,\n+            from_sorted_ids=from_sorted_ids,\n         )\n         self.k = k\n+        self.from_sorted_ids = from_sorted_ids\n         # Metric should be maximized during optimization.\n         self._direction = \"up\"\n \n     def get_config(self):\n-        return {\"name\": self.name, \"dtype\": self.dtype, \"k\": self.k}\n+        config = {\"name\": self.name, \"dtype\": self.dtype, \"k\": self.k}\n+        if self.from_sorted_ids:\n+            config[\"from_sorted_ids\"] = True\n+        return config\n\n@@ -440,6 +440,27 @@ class SparseTopKCategoricalAccuracyTest(testing.TestCase):\n         self.assertEqual(len(sp_top_k_cat_acc_obj2.variables), 2)\n         self.assertEqual(sp_top_k_cat_acc_obj2._dtype, \"float32\")\n         self.assertEqual(sp_top_k_cat_acc_obj2.k, 1)\n+        self.assertFalse(sp_top_k_cat_acc_obj2.from_sorted_ids)\n+\n+    def test_config_from_sorted_ids(self):\n+        sp_top_k_cat_acc_obj = accuracy_metrics.SparseTopKCategoricalAccuracy(\n+            k=1,\n+            name=\"sparse_top_k_categorical_accuracy\",\n+            dtype=\"float32\",\n+            from_sorted_ids=True,\n+        )\n+\n+        # Test get_config\n+        sp_top_k_cat_acc_obj_config = sp_top_k_cat_acc_obj.get_config()\n+        self.assertTrue(sp_top_k_cat_acc_obj_config[\"from_sorted_ids\"])\n+\n+        # Check save and restore config\n+        sp_top_k_cat_acc_obj2 = (\n+            accuracy_metrics.SparseTopKCategoricalAccuracy.from_config(\n+                sp_top_k_cat_acc_obj_config\n+            )\n+        )\n+        self.assertTrue(sp_top_k_cat_acc_obj2.from_sorted_ids)\n \n     def test_unweighted(self):\n         sp_top_k_cat_acc_obj = accuracy_metrics.SparseTopKCategoricalAccuracy(\n@@ -463,3 +484,32 @@ class SparseTopKCategoricalAccuracyTest(testing.TestCase):\n         )\n         result = sp_top_k_cat_acc_obj.result()\n         self.assertAllClose(result, 0.3, atol=1e-3)\n+\n+    def test_from_sorted_ids_unweighted(self):\n+        sp_top_k_cat_acc_obj = accuracy_metrics.SparseTopKCategoricalAccuracy(\n+            k=1,\n+            name=\"sparse_top_k_categorical_accuracy\",\n+            dtype=\"float32\",\n+            from_sorted_ids=True,\n+        )\n+        y_true = np.array([2, 1])\n+        y_pred = np.array([[1, 0, 3], [1, 2, 3]])\n+        sp_top_k_cat_acc_obj.update_state(y_true, y_pred)\n+        result = sp_top_k_cat_acc_obj.result()\n+        self.assertAllClose(result, 0.5, atol=1e-3)\n+\n+    def test_from_sorted_ids_weighted(self):\n+        sp_top_k_cat_acc_obj = accuracy_metrics.SparseTopKCategoricalAccuracy(\n+            k=1,\n+            name=\"sparse_top_k_categorical_accuracy\",\n+            dtype=\"float32\",\n+            from_sorted_ids=True,\n+        )\n+        y_true = np.array([2, 1])\n+        y_pred = np.array([[1, 0, 3], [1, 2, 3]])\n+        sample_weight = np.array([0.7, 0.3])\n+        sp_top_k_cat_acc_obj.update_state(\n+            y_true, y_pred, sample_weight=sample_weight\n+        )\n+        result = sp_top_k_cat_acc_obj.result()\n+        self.assertAllClose(result, 0.3, atol=1e-3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
