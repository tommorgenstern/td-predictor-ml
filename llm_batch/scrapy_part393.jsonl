{"custom_id": "scrapy#897e124a27772b9a710f501954f50e3c66e27c79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 44 | Files Changed: 18 | Hunks: 28 | Methods Changed: 21 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 67 | Churn Cumulative: 22314 | Contributors (this commit): 104 | Commits (past 90d): 58 | Contributors (cumulative): 313 | DMM Complexity: 0.0\n\nDIFF:\n@@ -225,8 +225,9 @@ class Command(BaseRunSpiderCommand):\n         cb_kwargs: dict[str, Any] | None = None,\n     ) -> Deferred[Any]:\n         cb_kwargs = cb_kwargs or {}\n-        d = maybeDeferred(self.iterate_spider_output, callback(response, **cb_kwargs))\n-        return d\n+        return maybeDeferred(\n+            self.iterate_spider_output, callback(response, **cb_kwargs)\n+        )\n \n     def get_callback_from_rules(\n         self, spider: Spider, response: Response\n\n@@ -41,7 +41,7 @@ class OffsiteMiddleware:\n \n     def process_request(self, request: Request, spider: Spider) -> None:\n         if request.dont_filter or self.should_follow(request, spider):\n-            return None\n+            return\n         domain = urlparse_cached(request).hostname\n         if domain and domain not in self.domains_seen:\n             self.domains_seen.add(domain)\n\n@@ -586,7 +586,7 @@ class FeedExporter:\n         :param uri_template: template of uri which contains %(batch_time)s or %(batch_id)d to create new uri\n         \"\"\"\n         storage = self._get_storage(uri, feed_options)\n-        slot = FeedSlot(\n+        return FeedSlot(\n             storage=storage,\n             uri=uri,\n             format=feed_options[\"format\"],\n@@ -600,7 +600,6 @@ class FeedExporter:\n             settings=self.settings,\n             crawler=self.crawler,\n         )\n-        return slot\n \n     def item_scraped(self, item: Any, spider: Spider) -> None:\n         slots = []\n\n@@ -282,8 +282,7 @@ class DbmCacheStorage:\n         headers = Headers(data[\"headers\"])\n         body = data[\"body\"]\n         respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n-        response = respcls(url=url, headers=headers, status=status, body=body)\n-        return response\n+        return respcls(url=url, headers=headers, status=status, body=body)\n \n     def store_response(\n         self, spider: Spider, request: Request, response: Response\n@@ -349,8 +348,7 @@ class FilesystemCacheStorage:\n         status = metadata[\"status\"]\n         headers = Headers(headers_raw_to_dict(rawheaders))\n         respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n-        response = respcls(url=url, headers=headers, status=status, body=body)\n-        return response\n+        return respcls(url=url, headers=headers, status=status, body=body)\n \n     def store_response(\n         self, spider: Spider, request: Request, response: Response\n\n@@ -157,8 +157,7 @@ class PostProcessingManager(IOBase):\n         return True\n \n     def _load_plugins(self, plugins: list[Any]) -> list[Any]:\n-        plugins = [load_object(plugin) for plugin in plugins]\n-        return plugins\n+        return [load_object(plugin) for plugin in plugins]\n \n     def _get_head_plugin(self) -> Any:\n         prev = self.file\n\n@@ -253,8 +253,7 @@ class LxmlLinkExtractor:\n         if self.canonicalize:\n             for link in links:\n                 link.url = canonicalize_url(link.url)\n-        links = self.link_extractor._process_links(links)\n-        return links\n+        return self.link_extractor._process_links(links)\n \n     def _extract_links(self, *args: Any, **kwargs: Any) -> list[Link]:\n         return self.link_extractor._extract_links(*args, **kwargs)\n\n@@ -79,8 +79,7 @@ class PythonRobotParser(RobotParser):\n     @classmethod\n     def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n         spider = None if not crawler else crawler.spider\n-        o = cls(robotstxt_body, spider)\n-        return o\n+        return cls(robotstxt_body, spider)\n \n     def allowed(self, url: str | bytes, user_agent: str | bytes) -> bool:\n         user_agent = to_unicode(user_agent)\n@@ -100,8 +99,7 @@ class RerpRobotParser(RobotParser):\n     @classmethod\n     def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n         spider = None if not crawler else crawler.spider\n-        o = cls(robotstxt_body, spider)\n-        return o\n+        return cls(robotstxt_body, spider)\n \n     def allowed(self, url: str | bytes, user_agent: str | bytes) -> bool:\n         user_agent = to_unicode(user_agent)\n@@ -120,8 +118,7 @@ class ProtegoRobotParser(RobotParser):\n     @classmethod\n     def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n         spider = None if not crawler else crawler.spider\n-        o = cls(robotstxt_body, spider)\n-        return o\n+        return cls(robotstxt_body, spider)\n \n     def allowed(self, url: str | bytes, user_agent: str | bytes) -> bool:\n         user_agent = to_unicode(user_agent)\n\n@@ -36,7 +36,7 @@ def listen_tcp(portrange: list[int], host: str, factory: ServerFactory) -> Port:\n         return reactor.listenTCP(0, factory, interface=host)\n     if len(portrange) == 1:\n         return reactor.listenTCP(portrange[0], factory, interface=host)\n-    for x in range(portrange[0], portrange[1] + 1):\n+    for x in range(portrange[0], portrange[1] + 1):  # noqa: RET503\n         try:\n             return reactor.listenTCP(x, factory, interface=host)\n         except error.CannotListenError:\n\n@@ -175,7 +175,7 @@ class AsyncDefAsyncioReqsReturnSpider(SimpleSpider):\n         status = await get_from_asyncio_queue(response.status)\n         self.logger.info(f\"Got response {status}, req_id {req_id}\")\n         if req_id > 0:\n-            return\n+            return None\n         reqs = []\n         for i in range(1, 3):\n             req = Request(self.start_urls[0], dont_filter=True, meta={\"req_id\": i})\n\n@@ -250,8 +250,7 @@ class MiddlewareUsingCoro(ManagerTestCase):\n         class CoroMiddleware:\n             async def process_request(self, request, spider):\n                 await asyncio.sleep(0.1)\n-                result = await get_from_asyncio_queue(resp)\n-                return result\n+                return await get_from_asyncio_queue(resp)\n \n         self.mwman._add_middleware(CoroMiddleware())\n         req = Request(\"http://example.com/index.html\")\n\n@@ -116,7 +116,7 @@ Disallow: /some/randome/page.html\n     def test_robotstxt_garbage(self):\n         # garbage response should be discarded, equal 'allow all'\n         middleware = RobotsTxtMiddleware(self._get_garbage_crawler())\n-        deferred = DeferredList(\n+        return DeferredList(\n             [\n                 self.assertNotIgnored(Request(\"http://site.local\"), middleware),\n                 self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware),\n@@ -127,7 +127,6 @@ Disallow: /some/randome/page.html\n             ],\n             fireOnOneErrback=True,\n         )\n-        return deferred\n \n     def _get_emptybody_crawler(self):\n         crawler = self.crawler\n\n@@ -134,8 +134,7 @@ class FTPFeedStorageTest(unittest.TestCase):\n             name = \"test_spider\"\n \n         crawler = get_crawler(settings_dict=settings)\n-        spider = TestSpider.from_crawler(crawler)\n-        return spider\n+        return TestSpider.from_crawler(crawler)\n \n     def _store(self, uri, content, feed_options=None, settings=None):\n         crawler = get_crawler(settings_dict=settings or {})\n@@ -210,8 +209,7 @@ class BlockingFeedStorageTest(unittest.TestCase):\n             name = \"test_spider\"\n \n         crawler = get_crawler(settings_dict=settings)\n-        spider = TestSpider.from_crawler(crawler)\n-        return spider\n+        return TestSpider.from_crawler(crawler)\n \n     def test_default_temp_dir(self):\n         b = BlockingFeedStorage()\n\n@@ -342,13 +342,11 @@ class BaseResponseTest(unittest.TestCase):\n \n     def _links_response(self):\n         body = get_testdata(\"link_extractor\", \"linkextractor.html\")\n-        resp = self.response_class(\"http://example.com/index\", body=body)\n-        return resp\n+        return self.response_class(\"http://example.com/index\", body=body)\n \n     def _links_response_no_href(self):\n         body = get_testdata(\"link_extractor\", \"linkextractor_no_href.html\")\n-        resp = self.response_class(\"http://example.com/index\", body=body)\n-        return resp\n+        return self.response_class(\"http://example.com/index\", body=body)\n \n \n class TextResponseTest(BaseResponseTest):\n\n@@ -48,8 +48,7 @@ sys.exit(mitmdump())\n         )\n         line = self.proc.stdout.readline().decode(\"utf-8\")\n         host_port = re.search(r\"listening at (?:http://)?([^:]+:\\d+)\", line).group(1)\n-        address = f\"http://{self.auth_user}:{self.auth_pass}@{host_port}\"\n-        return address\n+        return f\"http://{self.auth_user}:{self.auth_pass}@{host_port}\"\n \n     def stop(self):\n         self.proc.kill()\n\n@@ -16,7 +16,6 @@ class InjectArgumentsDownloaderMiddleware:\n     def process_request(self, request, spider):\n         if request.callback.__name__ == \"parse_downloader_mw\":\n             request.cb_kwargs[\"from_process_request\"] = True\n-        return None\n \n     def process_response(self, request, response, spider):\n         if request.callback.__name__ == \"parse_downloader_mw\":\n@@ -39,7 +38,6 @@ class InjectArgumentsSpiderMiddleware:\n         request = response.request\n         if request.callback.__name__ == \"parse_spider_mw\":\n             request.cb_kwargs[\"from_process_spider_input\"] = True\n-        return None\n \n     def process_spider_output(self, response, result, spider):\n         for element in result:\n\n@@ -18,8 +18,7 @@ class SignalCatcherSpider(Spider):\n \n     @classmethod\n     def from_crawler(cls, crawler, *args, **kwargs):\n-        spider = cls(crawler, *args, **kwargs)\n-        return spider\n+        return cls(crawler, *args, **kwargs)\n \n     def on_request_left(self, request, spider):\n         self.caught_times += 1\n\n@@ -37,8 +37,7 @@ class SpiderMiddlewareTestCase(TestCase):\n         results = []\n         dfd.addBoth(results.append)\n         self._wait(dfd)\n-        ret = results[0]\n-        return ret\n+        return results[0]\n \n \n class ProcessSpiderInputInvalidOutput(SpiderMiddlewareTestCase):\n\n@@ -12,7 +12,6 @@ class LogExceptionMiddleware:\n         spider.logger.info(\n             \"Middleware: %s exception caught\", exception.__class__.__name__\n         )\n-        return None\n \n \n # ================================================================================\n@@ -170,7 +169,6 @@ class _GeneratorDoNothingMiddleware:\n     def process_spider_exception(self, response, exception, spider):\n         method = f\"{self.__class__.__name__}.process_spider_exception\"\n         spider.logger.info(\"%s: %s caught\", method, exception.__class__.__name__)\n-        return None\n \n \n class GeneratorFailMiddleware:\n@@ -240,7 +238,6 @@ class _NotGeneratorDoNothingMiddleware:\n     def process_spider_exception(self, response, exception, spider):\n         method = f\"{self.__class__.__name__}.process_spider_exception\"\n         spider.logger.info(\"%s: %s caught\", method, exception.__class__.__name__)\n-        return None\n \n \n class NotGeneratorFailMiddleware:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e7595837a60b896eb0efed4b452b0bdd62e8039f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 30 | Files Changed: 19 | Hunks: 30 | Methods Changed: 30 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 60 | Churn Cumulative: 15490 | Contributors (this commit): 91 | Commits (past 90d): 69 | Contributors (cumulative): 303 | DMM Complexity: None\n\nDIFF:\n@@ -22,7 +22,7 @@ class Command(BaseRunSpiderCommand):\n \n     def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         if len(args) < 1:\n-            raise UsageError()\n+            raise UsageError\n         if len(args) > 1:\n             raise UsageError(\n                 \"running 'scrapy crawl' with more than one spider is not supported\"\n\n@@ -28,7 +28,7 @@ class Command(ScrapyCommand):\n \n     def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         if len(args) != 1:\n-            raise UsageError()\n+            raise UsageError\n \n         editor = self.settings[\"EDITOR\"]\n         assert self.crawler_process\n\n@@ -68,7 +68,7 @@ class Command(ScrapyCommand):\n \n     def run(self, args: list[str], opts: Namespace) -> None:\n         if len(args) != 1 or not is_url(args[0]):\n-            raise UsageError()\n+            raise UsageError\n         request = Request(\n             args[0],\n             callback=self._print_response,\n\n@@ -101,7 +101,7 @@ class Command(ScrapyCommand):\n                 print(template_file.read_text(encoding=\"utf-8\"))\n             return\n         if len(args) != 2:\n-            raise UsageError()\n+            raise UsageError\n \n         name, url = args[0:2]\n         url = verify_url_scheme(url)\n\n@@ -399,7 +399,7 @@ class Command(BaseRunSpiderCommand):\n     def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         # parse arguments\n         if not len(args) == 1 or not is_url(args[0]):\n-            raise UsageError()\n+            raise UsageError\n         url = args[0]\n \n         # prepare spidercls\n\n@@ -43,7 +43,7 @@ class Command(BaseRunSpiderCommand):\n \n     def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         if len(args) != 1:\n-            raise UsageError()\n+            raise UsageError\n         filename = Path(args[0])\n         if not filename.exists():\n             raise UsageError(f\"File not found: {filename}\\n\")\n\n@@ -92,7 +92,7 @@ class Command(ScrapyCommand):\n \n     def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         if len(args) not in (1, 2):\n-            raise UsageError()\n+            raise UsageError\n \n         project_name = args[0]\n \n\n@@ -98,7 +98,7 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):\n         \"\"\"\n         ``True`` if the scheduler has enqueued requests, ``False`` otherwise\n         \"\"\"\n-        raise NotImplementedError()\n+        raise NotImplementedError\n \n     @abstractmethod\n     def enqueue_request(self, request: Request) -> bool:\n@@ -112,7 +112,7 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):\n         For reference, the default Scrapy scheduler returns ``False`` when the\n         request is rejected by the dupefilter.\n         \"\"\"\n-        raise NotImplementedError()\n+        raise NotImplementedError\n \n     @abstractmethod\n     def next_request(self) -> Request | None:\n@@ -124,7 +124,7 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):\n         to the downloader in the current reactor cycle. The engine will continue\n         calling ``next_request`` until ``has_pending_requests`` is ``False``.\n         \"\"\"\n-        raise NotImplementedError()\n+        raise NotImplementedError\n \n \n class Scheduler(BaseScheduler):\n\n@@ -293,12 +293,12 @@ class MediaPipeline(ABC):\n         self, request: Request, info: SpiderInfo, *, item: Any = None\n     ) -> Deferred[FileInfo | None]:\n         \"\"\"Check request before starting download\"\"\"\n-        raise NotImplementedError()\n+        raise NotImplementedError\n \n     @abstractmethod\n     def get_media_requests(self, item: Any, info: SpiderInfo) -> list[Request]:\n         \"\"\"Returns the media requests to download\"\"\"\n-        raise NotImplementedError()\n+        raise NotImplementedError\n \n     @abstractmethod\n     def media_downloaded(\n@@ -310,14 +310,14 @@ class MediaPipeline(ABC):\n         item: Any = None,\n     ) -> FileInfo:\n         \"\"\"Handler for success downloads\"\"\"\n-        raise NotImplementedError()\n+        raise NotImplementedError\n \n     @abstractmethod\n     def media_failed(\n         self, failure: Failure, request: Request, info: SpiderInfo\n     ) -> NoReturn:\n         \"\"\"Handler for failed downloads\"\"\"\n-        raise NotImplementedError()\n+        raise NotImplementedError\n \n     def item_completed(\n         self, results: list[FileInfoOrError], item: Any, info: SpiderInfo\n@@ -345,4 +345,4 @@ class MediaPipeline(ABC):\n         item: Any = None,\n     ) -> str:\n         \"\"\"Returns the path where downloaded media should be stored\"\"\"\n-        raise NotImplementedError()\n+        raise NotImplementedError\n\n@@ -76,7 +76,7 @@ class HostResolution:\n         self.name: str = name\n \n     def cancel(self) -> None:\n-        raise NotImplementedError()\n+        raise NotImplementedError\n \n \n @provider(IResolutionReceiver)\n\n@@ -51,7 +51,7 @@ class ReferrerPolicy:\n     name: str\n \n     def referrer(self, response_url: str, request_url: str) -> str | None:\n-        raise NotImplementedError()\n+        raise NotImplementedError\n \n     def stripped_referrer(self, url: str) -> str | None:\n         if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n\n@@ -64,7 +64,7 @@ class AddonManagerTest(unittest.TestCase):\n     def test_notconfigured(self):\n         class NotConfiguredAddon:\n             def update_settings(self, settings):\n-                raise NotConfigured()\n+                raise NotConfigured\n \n         settings_dict = {\n             \"ADDONS\": {NotConfiguredAddon: 0},\n\n@@ -894,7 +894,7 @@ class S3TestCase(unittest.TestCase):\n         except Exception as e:\n             self.assertIsInstance(e, (TypeError, NotConfigured))\n         else:\n-            raise AssertionError()\n+            raise AssertionError\n \n     def test_request_signing1(self):\n         # gets an object from the johnsmith bucket.\n\n@@ -178,7 +178,7 @@ class ProcessExceptionInvalidOutput(ManagerTestCase):\n \n         class InvalidProcessExceptionMiddleware:\n             def process_request(self, request, spider):\n-                raise Exception()\n+                raise Exception\n \n             def process_exception(self, request, exception, spider):\n                 return 1\n\n@@ -59,7 +59,7 @@ class HttpCompressionTest(TestCase):\n \n     def _getresponse(self, coding):\n         if coding not in FORMAT:\n-            raise ValueError()\n+            raise ValueError\n \n         samplefile, contentencoding = FORMAT[coding]\n \n\n@@ -78,7 +78,7 @@ class ProcessSpiderExceptionInvalidOutput(SpiderMiddlewareTestCase):\n \n         class RaiseExceptionProcessSpiderOutputMiddleware:\n             def process_spider_output(self, response, result, spider):\n-                raise Exception()\n+                raise Exception\n \n         self.mwman._add_middleware(InvalidProcessSpiderOutputExceptionMiddleware())\n         self.mwman._add_middleware(RaiseExceptionProcessSpiderOutputMiddleware())\n\n@@ -43,7 +43,7 @@ class RecoverySpider(Spider):\n         yield {\"test\": 1}\n         self.logger.info(\"DONT_FAIL: %s\", response.meta.get(\"dont_fail\"))\n         if not response.meta.get(\"dont_fail\"):\n-            raise TabError()\n+            raise TabError\n \n \n class RecoveryAsyncGenSpider(RecoverySpider):\n@@ -59,7 +59,7 @@ class RecoveryAsyncGenSpider(RecoverySpider):\n class FailProcessSpiderInputMiddleware:\n     def process_spider_input(self, response, spider):\n         spider.logger.info(\"Middleware: will raise IndexError\")\n-        raise IndexError()\n+        raise IndexError\n \n \n class ProcessSpiderInputSpiderWithoutErrback(Spider):\n@@ -109,14 +109,14 @@ class GeneratorCallbackSpider(Spider):\n     def parse(self, response):\n         yield {\"test\": 1}\n         yield {\"test\": 2}\n-        raise ImportError()\n+        raise ImportError\n \n \n class AsyncGeneratorCallbackSpider(GeneratorCallbackSpider):\n     async def parse(self, response):\n         yield {\"test\": 1}\n         yield {\"test\": 2}\n-        raise ImportError()\n+        raise ImportError\n \n \n # ================================================================================\n@@ -176,7 +176,7 @@ class GeneratorFailMiddleware:\n         for r in result:\n             r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n             yield r\n-            raise LookupError()\n+            raise LookupError\n \n     def process_spider_exception(self, response, exception, spider):\n         method = f\"{self.__class__.__name__}.process_spider_exception\"\n@@ -246,7 +246,7 @@ class NotGeneratorFailMiddleware:\n         for r in result:\n             r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n             out.append(r)\n-        raise ReferenceError()\n+        raise ReferenceError\n         return out\n \n     def process_spider_exception(self, response, exception, spider):\n\n@@ -41,7 +41,7 @@ class BaseQueueTestCase(unittest.TestCase):\n \n class RequestQueueTestMixin:\n     def queue(self):\n-        raise NotImplementedError()\n+        raise NotImplementedError\n \n     def test_one_element_with_peek(self):\n         if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n\n@@ -10,7 +10,7 @@ from scrapy.utils.misc import (\n \n \n def _indentation_error(*args, **kwargs):\n-    raise IndentationError()\n+    raise IndentationError\n \n \n def top_level_return_something():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#93644f2c30ee74a68584c9e4cdfd0827c2187d34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 38 | Files Changed: 11 | Hunks: 26 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 50 | Churn Cumulative: 8618 | Contributors (this commit): 68 | Commits (past 90d): 41 | Contributors (cumulative): 179 | DMM Complexity: 0.0\n\nDIFF:\n@@ -61,7 +61,6 @@ class Command(ScrapyCommand):\n         \"\"\"You can use this function to update the Scrapy objects that will be\n         available in the shell\n         \"\"\"\n-        pass\n \n     def run(self, args: list[str], opts: Namespace) -> None:\n         url = args[0] if args else None\n\n@@ -81,7 +81,6 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):\n         :param spider: the spider object for the current crawl\n         :type spider: :class:`~scrapy.spiders.Spider`\n         \"\"\"\n-        pass\n \n     def close(self, reason: str) -> Deferred[None] | None:\n         \"\"\"\n@@ -91,7 +90,6 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):\n         :param reason: a string which describes the reason why the spider was closed\n         :type reason: :class:`str`\n         \"\"\"\n-        pass\n \n     @abstractmethod\n     def has_pending_requests(self) -> bool:\n\n@@ -50,7 +50,6 @@ class BaseDupeFilter:\n \n     def log(self, request: Request, spider: Spider) -> None:\n         \"\"\"Log that a request has been filtered\"\"\"\n-        pass\n \n \n class RFPDupeFilter(BaseDupeFilter):\n\n@@ -13,8 +13,6 @@ from typing import Any\n class NotConfigured(Exception):\n     \"\"\"Indicates a missing configuration situation\"\"\"\n \n-    pass\n-\n \n class _InvalidOutput(TypeError):\n     \"\"\"\n@@ -22,8 +20,6 @@ class _InvalidOutput(TypeError):\n     Internal and undocumented, it should not be raised or caught by user code.\n     \"\"\"\n \n-    pass\n-\n \n # HTTP and crawling\n \n@@ -35,8 +31,6 @@ class IgnoreRequest(Exception):\n class DontCloseSpider(Exception):\n     \"\"\"Request the spider not to be closed yet\"\"\"\n \n-    pass\n-\n \n class CloseSpider(Exception):\n     \"\"\"Raise this from callbacks to request the spider to be closed\"\"\"\n@@ -64,14 +58,10 @@ class StopDownload(Exception):\n class DropItem(Exception):\n     \"\"\"Drop item from the item pipeline\"\"\"\n \n-    pass\n-\n \n class NotSupported(Exception):\n     \"\"\"Indicates a feature or method is not supported\"\"\"\n \n-    pass\n-\n \n # Commands\n \n@@ -89,10 +79,6 @@ class ScrapyDeprecationWarning(Warning):\n     DeprecationWarning is silenced on Python 2.7+\n     \"\"\"\n \n-    pass\n-\n \n class ContractFail(AssertionError):\n     \"\"\"Error raised in case of a failing contract\"\"\"\n-\n-    pass\n\n@@ -52,7 +52,6 @@ class RobotParser(metaclass=ABCMeta):\n         :param robotstxt_body: content of a robots.txt_ file.\n         :type robotstxt_body: bytes\n         \"\"\"\n-        pass\n \n     @abstractmethod\n     def allowed(self, url: str | bytes, user_agent: str | bytes) -> bool:\n@@ -64,7 +63,6 @@ class RobotParser(metaclass=ABCMeta):\n         :param user_agent: User agent\n         :type user_agent: str or bytes\n         \"\"\"\n-        pass\n \n \n class PythonRobotParser(RobotParser):\n\n@@ -393,8 +393,8 @@ class DuplicateStartRequestsSpider(MockServerSpider):\n     dupe_factor = 3\n \n     def start_requests(self):\n-        for i in range(0, self.distinct_urls):\n-            for j in range(0, self.dupe_factor):\n+        for i in range(self.distinct_urls):\n+            for j in range(self.dupe_factor):\n                 url = self.mockserver.url(f\"/echo?headers=1&body=test{i}\")\n                 yield Request(url, dont_filter=self.dont_filter)\n \n\n@@ -178,27 +178,23 @@ class TestSpider(Spider):\n         \"\"\"method with no url\n         @returns items 1 1\n         \"\"\"\n-        pass\n \n     def custom_form(self, response):\n         \"\"\"\n         @url http://scrapy.org\n         @custom_form\n         \"\"\"\n-        pass\n \n     def invalid_regex(self, response):\n         \"\"\"method with invalid regex\n         @ Scrapy is awsome\n         \"\"\"\n-        pass\n \n     def invalid_regex_with_valid_contract(self, response):\n         \"\"\"method with invalid regex\n         @ scrapy is awsome\n         @url http://scrapy.org\n         \"\"\"\n-        pass\n \n     def returns_request_meta(self, response):\n         \"\"\"method which returns request\n@@ -235,7 +231,6 @@ class CustomContractSuccessSpider(Spider):\n         \"\"\"\n         @custom_success_contract\n         \"\"\"\n-        pass\n \n \n class CustomContractFailSpider(Spider):\n@@ -245,7 +240,6 @@ class CustomContractFailSpider(Spider):\n         \"\"\"\n         @custom_fail_contract\n         \"\"\"\n-        pass\n \n \n class InheritsTestSpider(TestSpider):\n\n@@ -265,7 +265,7 @@ class MaxRetryTimesTest(unittest.TestCase):\n         spider = spider or self.spider\n         middleware = middleware or self.mw\n \n-        for i in range(0, max_retry_times):\n+        for i in range(max_retry_times):\n             req = middleware.process_exception(req, exception, spider)\n             assert isinstance(req, Request)\n \n\n@@ -13,7 +13,7 @@ class TelnetExtensionTest(unittest.TestCase):\n         console = TelnetConsole(crawler)\n \n         # This function has some side effects we don't need for this test\n-        console._get_telnet_vars = lambda: {}\n+        console._get_telnet_vars = dict\n \n         console.start_listening()\n         protocol = console.protocol()\n\n@@ -311,11 +311,11 @@ class FilesPipelineTestCaseFieldsDataClass(\n class FilesPipelineTestAttrsItem:\n     name = attr.ib(default=\"\")\n     # default fields\n-    file_urls: list[str] = attr.ib(default=lambda: [])\n-    files: list[dict[str, str]] = attr.ib(default=lambda: [])\n+    file_urls: list[str] = attr.ib(default=list)\n+    files: list[dict[str, str]] = attr.ib(default=list)\n     # overridden fields\n-    custom_file_urls: list[str] = attr.ib(default=lambda: [])\n-    custom_files: list[dict[str, str]] = attr.ib(default=lambda: [])\n+    custom_file_urls: list[str] = attr.ib(default=list)\n+    custom_files: list[dict[str, str]] = attr.ib(default=list)\n \n \n class FilesPipelineTestCaseFieldsAttrsItem(\n\n@@ -295,11 +295,11 @@ class ImagesPipelineTestCaseFieldsDataClass(\n class ImagesPipelineTestAttrsItem:\n     name = attr.ib(default=\"\")\n     # default fields\n-    image_urls: list[str] = attr.ib(default=lambda: [])\n-    images: list[dict[str, str]] = attr.ib(default=lambda: [])\n+    image_urls: list[str] = attr.ib(default=list)\n+    images: list[dict[str, str]] = attr.ib(default=list)\n     # overridden fields\n-    custom_image_urls: list[str] = attr.ib(default=lambda: [])\n-    custom_images: list[dict[str, str]] = attr.ib(default=lambda: [])\n+    custom_image_urls: list[str] = attr.ib(default=list)\n+    custom_images: list[dict[str, str]] = attr.ib(default=list)\n \n \n class ImagesPipelineTestCaseFieldsAttrsItem(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c2832ed1316b25813870a3ef8ebc15473a35d82f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 22 | Files Changed: 5 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 2975 | Contributors (this commit): 40 | Commits (past 90d): 22 | Contributors (cumulative): 81 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,7 @@ enable this middleware and enable the ROBOTSTXT_OBEY setting.\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, TypeVar\n+from typing import TYPE_CHECKING\n \n from twisted.internet.defer import Deferred, maybeDeferred\n \n@@ -31,8 +31,6 @@ if TYPE_CHECKING:\n \n logger = logging.getLogger(__name__)\n \n-_T = TypeVar(\"_T\")\n-\n \n class RobotsTxtMiddleware:\n     DOWNLOAD_PRIORITY: int = 1000\n\n@@ -5,8 +5,6 @@ For actual link extractors implementation see scrapy.linkextractors, or\n its documentation in: docs/topics/link-extractors.rst\n \"\"\"\n \n-from typing import Any\n-\n \n class Link:\n     \"\"\"Link objects represent an extracted link by the LinkExtractor.\n@@ -39,7 +37,7 @@ class Link:\n         self.fragment: str = fragment\n         self.nofollow: bool = nofollow\n \n-    def __eq__(self, other: Any) -> bool:\n+    def __eq__(self, other: object) -> bool:\n         if not isinstance(other, Link):\n             raise NotImplementedError\n         return (\n\n@@ -5,16 +5,7 @@ import logging\n import warnings\n from abc import ABC, abstractmethod\n from collections import defaultdict\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Literal,\n-    NoReturn,\n-    TypedDict,\n-    TypeVar,\n-    Union,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, Literal, NoReturn, TypedDict, Union, cast\n \n from twisted import version as twisted_version\n from twisted.internet.defer import Deferred, DeferredList\n@@ -41,8 +32,6 @@ if TYPE_CHECKING:\n     from scrapy.http import Response\n     from scrapy.utils.request import RequestFingerprinter\n \n-_T = TypeVar(\"_T\")\n-\n \n class FileInfo(TypedDict):\n     url: str\n\n@@ -53,7 +53,7 @@ def get_meta_refresh(\n     return _metaref_cache[response]\n \n \n-def response_status_message(status: bytes | float | int | str) -> str:\n+def response_status_message(status: bytes | float | str) -> str:\n     \"\"\"Return status code plus status text descriptive message\"\"\"\n     status_int = int(status)\n     message = http.RESPONSES.get(status_int, \"Unknown Status\")\n\n@@ -1,9 +1,9 @@\n from __future__ import annotations\n \n-import collections\n import shutil\n import tempfile\n import unittest\n+from typing import Any, NamedTuple\n \n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n@@ -18,8 +18,13 @@ from scrapy.utils.misc import load_object\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n \n-MockEngine = collections.namedtuple(\"MockEngine\", [\"downloader\"])\n-MockSlot = collections.namedtuple(\"MockSlot\", [\"active\"])\n+\n+class MockEngine(NamedTuple):\n+    downloader: MockDownloader\n+\n+\n+class MockSlot(NamedTuple):\n+    active: list[Any]\n \n \n class MockDownloader:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1e4c81e9dce7dfc06c8ab1b89d85da32f0cb54de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 30 | Files Changed: 7 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 42 | Churn Cumulative: 13081 | Contributors (this commit): 75 | Commits (past 90d): 25 | Contributors (cumulative): 136 | DMM Complexity: 0.0625\n\nDIFF:\n@@ -7,10 +7,7 @@ _T = TypeVar(\"_T\")\n \n \n async def collect_asyncgen(result: AsyncIterable[_T]) -> list[_T]:\n-    results = []\n-    async for x in result:\n-        results.append(x)\n-    return results\n+    return [x async for x in result]\n \n \n async def as_async_generator(\n\n@@ -235,8 +235,7 @@ def get_func_args(func: Callable[..., Any], stripself: bool = False) -> list[str\n                 continue\n             args.append(name)\n     else:\n-        for name in sig.parameters.keys():\n-            args.append(name)\n+        args = list(sig.parameters)\n \n     if stripself and args and args[0] == \"self\":\n         args = args[1:]\n\n@@ -1757,13 +1757,13 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n                 crawler = get_crawler(spider_cls, settings)\n                 yield crawler.crawl()\n \n-            for file_path, feed_options in FEEDS.items():\n+            for file_path in FEEDS:\n                 content[str(file_path)] = (\n                     Path(file_path).read_bytes() if Path(file_path).exists() else None\n                 )\n \n         finally:\n-            for file_path in FEEDS.keys():\n+            for file_path in FEEDS:\n                 if not Path(file_path).exists():\n                     continue\n \n\n@@ -289,10 +289,7 @@ class ProcessSpiderOutputNonIterableMiddleware:\n \n class ProcessSpiderOutputCoroutineMiddleware:\n     async def process_spider_output(self, response, result, spider):\n-        results = []\n-        for r in result:\n-            results.append(r)\n-        return results\n+        return result\n \n \n class ProcessSpiderOutputInvalidResult(BaseAsyncSpiderMiddlewareTestCase):\n\n@@ -8,9 +8,7 @@ class AsyncgenUtilsTest(unittest.TestCase):\n     @deferred_f_from_coro_f\n     async def test_as_async_generator(self):\n         ag = as_async_generator(range(42))\n-        results = []\n-        async for i in ag:\n-            results.append(i)\n+        results = [i async for i in ag]\n         self.assertEqual(results, list(range(42)))\n \n     @deferred_f_from_coro_f\n\n@@ -26,15 +26,14 @@ class XmliterBaseTestCase:\n         \"\"\"\n \n         response = XmlResponse(url=\"http://example.com\", body=body)\n-        attrs = []\n-        for x in self.xmliter(response, \"product\"):\n-            attrs.append(\n+        attrs = [\n             (\n                 x.attrib[\"id\"],\n                 x.xpath(\"name/text()\").getall(),\n                 x.xpath(\"./type/text()\").getall(),\n             )\n-            )\n+            for x in self.xmliter(response, \"product\")\n+        ]\n \n         self.assertEqual(\n             attrs, [(\"001\", [\"Name 1\"], [\"Type 1\"]), (\"002\", [\"Name 2\"], [\"Type 2\"])]\n@@ -99,15 +98,14 @@ class XmliterBaseTestCase:\n             # Unicode body needs encoding information\n             XmlResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\"),\n         ):\n-            attrs = []\n-            for x in self.xmliter(r, \"þingflokkur\"):\n-                attrs.append(\n+            attrs = [\n                 (\n                     x.attrib[\"id\"],\n                     x.xpath(\"./skammstafanir/stuttskammstöfun/text()\").getall(),\n                     x.xpath(\"./tímabil/fyrstaþing/text()\").getall(),\n                 )\n-                )\n+                for x in self.xmliter(r, \"þingflokkur\")\n+            ]\n \n             self.assertEqual(\n                 attrs,\n\n@@ -58,13 +58,6 @@ class MutableAsyncChainTest(unittest.TestCase):\n         for i in range(5, 7):\n             yield i\n \n-    @staticmethod\n-    async def collect_asyncgen_exc(asyncgen):\n-        results = []\n-        async for x in asyncgen:\n-            results.append(x)\n-        return results\n-\n     @deferred_f_from_coro_f\n     async def test_mutableasyncchain(self):\n         m = MutableAsyncChain(self.g1(), as_async_generator(range(3, 7)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7dd92e6e4341dc6d5ba70a20d7e89edb3a87fba9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 41 | Files Changed: 22 | Hunks: 32 | Methods Changed: 34 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 62 | Churn Cumulative: 25172 | Contributors (this commit): 121 | Commits (past 90d): 74 | Contributors (cumulative): 368 | DMM Complexity: 0.13333333333333333\n\nDIFF:\n@@ -31,7 +31,7 @@ version_info = tuple(int(v) if v.isdigit() else v for v in __version__.split(\".\"\n \n def __getattr__(name: str):\n     if name == \"twisted_version\":\n-        import warnings\n+        import warnings  # pylint: disable=reimported\n \n         from twisted import version as _txv\n \n\n@@ -74,7 +74,7 @@ def _get_commands_from_entry_points(\n         if inspect.isclass(obj):\n             cmds[entry_point.name] = obj()\n         else:\n-            raise Exception(f\"Invalid entry point {entry_point.name}\")\n+            raise ValueError(f\"Invalid entry point {entry_point.name}\")\n     return cmds\n \n \n\n@@ -92,8 +92,7 @@ class BaseItemExporter:\n                 field_iter = (\n                     (x, y) for x, y in self.fields_to_export.items() if x in item\n                 )\n-        else:\n-            if include_empty:\n+        elif include_empty:\n             field_iter = self.fields_to_export\n         else:\n             field_iter = (x for x in self.fields_to_export if x in item)\n\n@@ -398,7 +398,7 @@ def maybeDeferred_coro(\n     \"\"\"Copy of defer.maybeDeferred that also converts coroutines to Deferreds.\"\"\"\n     try:\n         result = f(*args, **kw)\n-    except:  # noqa: E722,B001\n+    except:  # noqa: E722  # pylint: disable=bare-except\n         return defer.fail(failure.Failure(captureVars=Deferred.debug))\n \n     if isinstance(result, Deferred):\n\n@@ -60,7 +60,7 @@ def create_deprecated_class(\n         deprecated_class: type | None = None\n         warned_on_subclass: bool = False\n \n-        def __new__(\n+        def __new__(  # pylint: disable=bad-classmethod-argument\n             metacls, name: str, bases: tuple[type, ...], clsdict_: dict[str, Any]\n         ) -> type:\n             cls = super().__new__(metacls, name, bases, clsdict_)\n\n@@ -130,7 +130,7 @@ _scrapy_root_handler: logging.Handler | None = None\n \n \n def install_scrapy_root_handler(settings: Settings) -> None:\n-    global _scrapy_root_handler\n+    global _scrapy_root_handler  # noqa: PLW0603  # pylint: disable=global-statement\n \n     if (\n         _scrapy_root_handler is not None\n\n@@ -149,12 +149,11 @@ def verify_installed_reactor(reactor_path: str) -> None:\n \n     reactor_class = load_object(reactor_path)\n     if not reactor.__class__ == reactor_class:\n-        msg = (\n+        raise RuntimeError(\n             \"The installed reactor \"\n             f\"({reactor.__module__}.{reactor.__class__.__name__}) does not \"\n             f\"match the requested one ({reactor_path})\"\n         )\n-        raise Exception(msg)\n \n \n def verify_installed_asyncio_event_loop(loop_path: str) -> None:\n@@ -168,7 +167,7 @@ def verify_installed_asyncio_event_loop(loop_path: str) -> None:\n         f\".{reactor._asyncioEventloop.__class__.__qualname__}\"\n     )\n     specified = f\"{loop_class.__module__}.{loop_class.__qualname__}\"\n-    raise Exception(\n+    raise RuntimeError(\n         \"Scrapy found an asyncio Twisted reactor already \"\n         f\"installed, and its event loop class ({installed}) does \"\n         \"not match the one specified in the ASYNCIO_EVENT_LOOP \"\n\n@@ -52,10 +52,6 @@ def iter_spider_classes(module: ModuleType) -> Iterable[type[Spider]]:\n     \"\"\"Return an iterator over all spider classes defined in the given module\n     that can be instantiated (i.e. which have name)\n     \"\"\"\n-    # this needs to be imported here until get rid of the spider manager\n-    # singleton in scrapy.spider.spiders\n-    from scrapy.spiders import Spider\n-\n     for obj in vars(module).values():\n         if (\n             inspect.isclass(obj)\n\n@@ -14,7 +14,7 @@ from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n \n # scrapy.utils.url was moved to w3lib.url and import * ensures this\n # move doesn't break old code\n-from w3lib.url import *  # pylint: disable=unused-wildcard-import\n+from w3lib.url import *  # pylint: disable=unused-wildcard-import,wildcard-import\n from w3lib.url import _safe_chars, _unquotepath  # noqa: F401\n \n from scrapy.utils.python import to_unicode\n@@ -50,7 +50,9 @@ def url_has_any_extension(url: UrlT, extensions: Iterable[str]) -> bool:\n     return any(lowercase_path.endswith(ext) for ext in extensions)\n \n \n-def parse_url(url: UrlT, encoding: str | None = None) -> ParseResult:\n+def parse_url(  # pylint: disable=function-redefined\n+    url: UrlT, encoding: str | None = None\n+) -> ParseResult:\n     \"\"\"Return urlparsed url from the given argument (which could be an already\n     parsed url)\n     \"\"\"\n\n@@ -8,7 +8,7 @@ class TestSpiderPipeline:\n \n class TestSpiderExceptionPipeline:\n     def open_spider(self, spider):\n-        raise Exception(\"exception\")\n+        raise RuntimeError(\"exception\")\n \n     def process_item(self, item, spider):\n         return item\n\n@@ -349,11 +349,6 @@ class HttpTestCase(unittest.TestCase):\n         request = Request(self.getURL(\"host\"), headers={\"Host\": host})\n         return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n \n-        d = self.download_request(request, Spider(\"foo\"))\n-        d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEqual, b\"localhost\")\n-        return d\n-\n     def test_content_length_zero_bodyless_post_request_headers(self):\n         \"\"\"Tests if \"Content-Length: 0\" is sent for bodyless POST requests.\n \n\n@@ -178,7 +178,7 @@ class ProcessExceptionInvalidOutput(ManagerTestCase):\n \n         class InvalidProcessExceptionMiddleware:\n             def process_request(self, request, spider):\n-                raise Exception\n+                raise RuntimeError\n \n             def process_exception(self, request, exception, spider):\n                 return 1\n\n@@ -192,4 +192,3 @@ class TestPeriodicLog(unittest.TestCase):\n             {\"PERIODIC_LOG_STATS\": {\"include\": [\"downloader/\"], \"exclude\": [\"bytes\"]}},\n             lambda k, v: \"downloader/\" in k and \"bytes\" not in k,\n         )\n-        #\n\n@@ -727,7 +727,7 @@ class ExceptionJsonItemExporter(JsonItemExporter):\n     \"\"\"JsonItemExporter that throws an exception every time export_item is called.\"\"\"\n \n     def export_item(self, _):\n-        raise Exception(\"foo\")\n+        raise RuntimeError(\"foo\")\n \n \n class FeedExportTest(FeedExportTestBase):\n\n@@ -330,10 +330,10 @@ class BasicItemLoaderTest(unittest.TestCase):\n         il.add_value(\"name\", [\"mar\", \"ta\"])\n         self.assertEqual(il.get_output_value(\"name\"), \"Mar Ta\")\n \n-        class TakeFirstItemLoader(TestItemLoader):\n+        class TakeFirstItemLoader2(TestItemLoader):\n             name_out = Join(\"<br>\")\n \n-        il = TakeFirstItemLoader()\n+        il = TakeFirstItemLoader2()\n         il.add_value(\"name\", [\"mar\", \"ta\"])\n         self.assertEqual(il.get_output_value(\"name\"), \"Mar<br>Ta\")\n \n\n@@ -78,7 +78,7 @@ class ProcessSpiderExceptionInvalidOutput(SpiderMiddlewareTestCase):\n \n         class RaiseExceptionProcessSpiderOutputMiddleware:\n             def process_spider_output(self, response, result, spider):\n-                raise Exception\n+                raise RuntimeError\n \n         self.mwman._add_middleware(InvalidProcessSpiderOutputExceptionMiddleware())\n         self.mwman._add_middleware(RaiseExceptionProcessSpiderOutputMiddleware())\n\n@@ -247,7 +247,6 @@ class NotGeneratorFailMiddleware:\n             r[\"processed\"].append(f\"{self.__class__.__name__}.process_spider_output\")\n             out.append(r)\n         raise ReferenceError\n-        return out\n \n     def process_spider_exception(self, response, exception, spider):\n         method = f\"{self.__class__.__name__}.process_spider_exception\"\n\n@@ -26,7 +26,7 @@ class AsyncioTest(TestCase):\n         with warnings.catch_warnings(record=True) as w:\n             install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n             self.assertEqual(len(w), 0)\n-        from twisted.internet import reactor\n+        from twisted.internet import reactor  # pylint: disable=reimported\n \n         assert original_reactor == reactor\n \n\n@@ -153,7 +153,7 @@ class AsyncDefTestsuiteTest(unittest.TestCase):\n     @mark.xfail(reason=\"Checks that the test is actually executed\", strict=True)\n     @deferred_f_from_coro_f\n     async def test_deferred_f_from_coro_f_xfail(self):\n-        raise Exception(\"This is expected to be raised\")\n+        raise RuntimeError(\"This is expected to be raised\")\n \n \n class AsyncCooperatorTest(unittest.TestCase):\n\n@@ -32,7 +32,6 @@ def top_level_return_none():\n https://example.org\n \"\"\"\n     yield url\n-    return\n \n \n def generator_that_returns_stuff():\n@@ -103,11 +102,9 @@ https://example.org\n     def test_generators_return_none(self):\n         def f2():\n             yield 1\n-            return None\n \n         def g2():\n             yield 1\n-            return\n \n         def h2():\n             yield 1\n@@ -132,7 +129,6 @@ https://example.org\n https://example.org\n         \"\"\"\n             yield url\n-            return\n \n         def l2():\n             return\n@@ -181,12 +177,10 @@ https://example.org\n         @decorator\n         def f3():\n             yield 1\n-            return None\n \n         @decorator\n         def g3():\n             yield 1\n-            return\n \n         @decorator\n         def h3():\n@@ -215,7 +209,6 @@ https://example.org\n https://example.org\n         \"\"\"\n             yield url\n-            return\n \n         @decorator\n         def l3():\n\n@@ -26,7 +26,7 @@ class UtilsSpidersTestCase(unittest.TestCase):\n         self.assertEqual(list(iterate_spider_output([r, i, o])), [r, i, o])\n \n     def test_iter_spider_classes(self):\n-        import tests.test_utils_spider  # pylint: disable=import-self\n+        import tests.test_utils_spider  # noqa: PLW0406  # pylint: disable=import-self\n \n         it = iter_spider_classes(tests.test_utils_spider)\n         self.assertEqual(set(it), {MySpider1, MySpider2})\n\n@@ -327,8 +327,6 @@ def create_guess_scheme_t(args):\n def create_skipped_scheme_t(args):\n     def do_expected(self):\n         raise unittest.SkipTest(args[2])\n-        url = guess_scheme(args[0])\n-        assert url.startswith(args[1])\n \n     return do_expected\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#21b9ba717c1687a889879232f226c75fb4dbe0bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 84 | Lines Deleted: 31 | Files Changed: 5 | Hunks: 15 | Methods Changed: 6 | Complexity Δ (Sum/Max): 10/8 | Churn Δ: 115 | Churn Cumulative: 4271 | Contributors (this commit): 82 | Commits (past 90d): 18 | Contributors (cumulative): 133 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,7 @@ import argparse\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n-from scrapy.utils.versions import scrapy_components_versions\n+from scrapy.utils.versions import get_versions\n \n \n class Command(ScrapyCommand):\n@@ -26,7 +26,7 @@ class Command(ScrapyCommand):\n \n     def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         if opts.verbose:\n-            versions = scrapy_components_versions()\n+            versions = get_versions()\n             width = max(len(n) for (n, _) in versions)\n             for name, version in versions:\n                 print(f\"{name:<{width}} : {version}\")\n\n@@ -219,6 +219,18 @@ LOG_LEVEL = \"DEBUG\"\n LOG_FILE = None\n LOG_FILE_APPEND = True\n LOG_SHORT_NAMES = False\n+LOG_VERSIONS = [\n+    \"lxml\",\n+    \"libxml2\",\n+    \"cssselect\",\n+    \"parsel\",\n+    \"w3lib\",\n+    \"Twisted\",\n+    \"Python\",\n+    \"pyOpenSSL\",\n+    \"cryptography\",\n+    \"Platform\",\n+]\n \n SCHEDULER_DEBUG = False\n \n\n@@ -1,6 +1,7 @@\n from __future__ import annotations\n \n import logging\n+import pprint\n import sys\n from collections.abc import MutableMapping\n from logging.config import dictConfig\n@@ -12,7 +13,7 @@ from twisted.python.failure import Failure\n \n import scrapy\n from scrapy.settings import Settings, _SettingsKeyT\n-from scrapy.utils.versions import scrapy_components_versions\n+from scrapy.utils.versions import get_versions\n \n if TYPE_CHECKING:\n \n@@ -174,12 +175,11 @@ def log_scrapy_info(settings: Settings) -> None:\n         \"Scrapy %(version)s started (bot: %(bot)s)\",\n         {\"version\": scrapy.__version__, \"bot\": settings[\"BOT_NAME\"]},\n     )\n-    versions = [\n-        f\"{name} {version}\"\n-        for name, version in scrapy_components_versions()\n-        if name != \"Scrapy\"\n-    ]\n-    logger.info(\"Versions: %(versions)s\", {\"versions\": \", \".join(versions)})\n+    software = settings.getlist(\"LOG_VERSIONS\")\n+    if not software:\n+        return\n+    versions = pprint.pformat(dict(get_versions(software)), sort_dicts=False)\n+    logger.info(f\"Versions:\\n{versions}\")\n \n \n def log_reactor_info() -> None:\n\n@@ -1,31 +1,46 @@\n+from __future__ import annotations\n+\n import platform\n import sys\n+from importlib.metadata import version\n+from warnings import warn\n \n-import cryptography\n-import cssselect\n import lxml.etree\n-import parsel\n-import twisted\n-import w3lib\n \n-import scrapy\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.settings.default_settings import LOG_VERSIONS\n from scrapy.utils.ssl import get_openssl_version\n \n+_DEFAULT_SOFTWARE = [\"Scrapy\"] + LOG_VERSIONS\n+\n+\n+def _version(item):\n+    lowercase_item = item.lower()\n+    if lowercase_item == \"libxml2\":\n+        return \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n+    if lowercase_item == \"platform\":\n+        return platform.platform()\n+    if lowercase_item == \"pyopenssl\":\n+        return get_openssl_version()\n+    if lowercase_item == \"python\":\n+        return sys.version.replace(\"\\n\", \"- \")\n+    return version(item)\n+\n+\n+def get_versions(\n+    software: list | None = None,\n+) -> list[tuple[str, str]]:\n+    software = software or _DEFAULT_SOFTWARE\n+    return [(item, _version(item)) for item in software]\n+\n \n def scrapy_components_versions() -> list[tuple[str, str]]:\n-    lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\n-    libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n-\n-    return [\n-        (\"Scrapy\", scrapy.__version__),\n-        (\"lxml\", lxml_version),\n-        (\"libxml2\", libxml2_version),\n-        (\"cssselect\", cssselect.__version__),\n-        (\"parsel\", parsel.__version__),\n-        (\"w3lib\", w3lib.__version__),\n-        (\"Twisted\", twisted.version.short()),\n-        (\"Python\", sys.version.replace(\"\\n\", \"- \")),\n-        (\"pyOpenSSL\", get_openssl_version()),\n-        (\"cryptography\", cryptography.__version__),\n-        (\"Platform\", platform.platform()),\n-    ]\n+    warn(\n+        (\n+            \"scrapy.utils.versions.scrapy_components_versions() is deprecated, \"\n+            \"use scrapy.utils.versions.get_versions() instead.\"\n+        ),\n+        ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n+    return get_versions()\n\n@@ -1,6 +1,7 @@\n import logging\n import os\n import platform\n+import re\n import signal\n import subprocess\n import sys\n@@ -923,3 +924,28 @@ class CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):\n             log,\n         )\n         self.assertIn(\"DEBUG: Using asyncio event loop\", log)\n+\n+\n+@mark.parametrize(\n+    [\"settings\", \"items\"],\n+    (\n+        ({}, default_settings.LOG_VERSIONS),\n+        ({\"LOG_VERSIONS\": [\"itemadapter\"]}, [\"itemadapter\"]),\n+        ({\"LOG_VERSIONS\": []}, None),\n+    ),\n+)\n+def test_log_scrapy_info(settings, items, caplog):\n+    with caplog.at_level(\"INFO\"):\n+        CrawlerProcess(settings)\n+    assert (\n+        caplog.records[0].getMessage()\n+        == f\"Scrapy {scrapy.__version__} started (bot: scrapybot)\"\n+    ), repr(caplog.records[0].msg)\n+    if not items:\n+        assert len(caplog.records) == 1\n+        return\n+    version_string = caplog.records[1].getMessage()\n+    expected_items_pattern = \"',\\n '\".join(\n+        f\"{item}': '[^']+('\\n +'[^']+)*\" for item in items\n+    )\n+    assert re.search(r\"^Versions:\\n{'\" + expected_items_pattern + \"'}$\", version_string)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
