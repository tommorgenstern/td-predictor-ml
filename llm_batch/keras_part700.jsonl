{"custom_id": "keras#f9e6cce2e46fea782d7992580dd6ceabc5a77875", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 264 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -34,7 +34,7 @@ def enable_traceback_filtering():\n     `keras_core.config.disable_traceback_filtering()`, you can re-enable it via\n     `keras_core.config.enable_traceback_filtering()`.\n     \"\"\"\n-    global_state.set_global_setting(\"traceback_filtering\", True)\n+    global_state.set_global_attribute(\"traceback_filtering\", True)\n \n \n @keras_core_export(\"keras_core.config.disable_traceback_filtering\")\n@@ -55,7 +55,7 @@ def disable_traceback_filtering():\n     `keras_core.config.disable_traceback_filtering()`, you can re-enable it via\n     `keras_core.config.enable_traceback_filtering()`.\n     \"\"\"\n-    global_state.set_global_setting(\"traceback_filtering\", False)\n+    global_state.set_global_attribute(\"traceback_filtering\", False)\n \n \n @keras_core_export(\"keras_core.config.is_traceback_filtering_enabled\")\n@@ -80,7 +80,7 @@ def is_traceback_filtering_enabled():\n         Boolean, `True` if traceback filtering is enabled,\n         and `False` otherwise.\n     \"\"\"\n-    return global_state.get_global_setting(\"traceback_filtering\", True)\n+    return global_state.get_global_attribute(\"traceback_filtering\", True)\n \n \n def include_frame(fname):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#76184ee917dd713f661815279795bc762eda7d00", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 364 | Lines Deleted: 101 | Files Changed: 4 | Hunks: 56 | Methods Changed: 25 | Complexity Δ (Sum/Max): 49/20 | Churn Δ: 465 | Churn Cumulative: 3632 | Contributors (this commit): 12 | Commits (past 90d): 41 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n from absl.testing import parameterized\n+from numpy.lib.stride_tricks import as_strided\n \n from keras_core import backend\n from keras_core import layers\n@@ -109,9 +109,133 @@ class AveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n \n \n class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n+    def _same_padding(self, input_size, pool_size, stride):\n+        if input_size % stride == 0:\n+            return max(pool_size - stride, 0)\n+        else:\n+            return max(pool_size - (input_size % stride), 0)\n+\n+    def _np_avgpool1d(self, x, pool_size, strides, padding, data_format):\n+        if data_format == \"channels_first\":\n+            x = x.swapaxes(1, 2)\n+        if isinstance(pool_size, (tuple, list)):\n+            pool_size = pool_size[0]\n+        if isinstance(strides, (tuple, list)):\n+            h_stride = strides[0]\n+        else:\n+            h_stride = strides\n+\n+        if padding == \"same\":\n+            n_batch, h_x, ch_x = x.shape\n+            pad_value = self._same_padding(h_x, pool_size, h_stride)\n+            npad = [(0, 0)] * x.ndim\n+            npad[1] = (0, pad_value)\n+            x = np.pad(x, pad_width=npad, mode=\"edge\")\n+\n+        n_batch, h_x, ch_x = x.shape\n+        out_h = int((h_x - pool_size) / h_stride) + 1\n+\n+        stride_shape = (n_batch, out_h, ch_x, pool_size)\n+        strides = (\n+            x.strides[0],\n+            h_stride * x.strides[1],\n+            x.strides[2],\n+            x.strides[1],\n+        )\n+        windows = as_strided(x, shape=stride_shape, strides=strides)\n+        out = np.mean(windows, axis=(3,))\n+        if data_format == \"channels_first\":\n+            out = out.swapaxes(1, 2)\n+        return out\n+\n+    def _np_avgpool2d(self, x, pool_size, strides, padding, data_format):\n+        if data_format == \"channels_first\":\n+            x = x.transpose((0, 2, 3, 1))\n+        if isinstance(pool_size, int):\n+            pool_size = (pool_size, pool_size)\n+        if isinstance(strides, int):\n+            strides = (strides, strides)\n+\n+        h_pool_size, w_pool_size = pool_size\n+        h_stride, w_stride = strides\n+        if padding == \"same\":\n+            n_batch, h_x, w_x, ch_x = x.shape\n+            h_padding = self._same_padding(h_x, h_pool_size, h_stride)\n+            w_padding = self._same_padding(w_x, w_pool_size, w_stride)\n+            npad = [(0, 0)] * x.ndim\n+            npad[1] = (0, h_padding)\n+            npad[2] = (0, w_padding)\n+            x = np.pad(x, pad_width=npad, mode=\"edge\")\n+\n+        n_batch, h_x, w_x, ch_x = x.shape\n+        out_h = int((h_x - h_pool_size) / h_stride) + 1\n+        out_w = int((w_x - w_pool_size) / w_stride) + 1\n+\n+        stride_shape = (n_batch, out_h, out_w, ch_x, *pool_size)\n+        strides = (\n+            x.strides[0],\n+            h_stride * x.strides[1],\n+            w_stride * x.strides[2],\n+            x.strides[3],\n+            x.strides[1],\n+            x.strides[2],\n+        )\n+        windows = as_strided(x, shape=stride_shape, strides=strides)\n+        out = np.mean(windows, axis=(4, 5))\n+        if data_format == \"channels_first\":\n+            out = out.transpose((0, 3, 1, 2))\n+        return out\n+\n+    def _np_avgpool3d(self, x, pool_size, strides, padding, data_format):\n+        if data_format == \"channels_first\":\n+            x = x.transpose((0, 2, 3, 4, 1))\n+\n+        if isinstance(pool_size, int):\n+            pool_size = (pool_size, pool_size, pool_size)\n+        if isinstance(strides, int):\n+            strides = (strides, strides, strides)\n+\n+        h_pool_size, w_pool_size, d_pool_size = pool_size\n+        h_stride, w_stride, d_stride = strides\n+\n+        if padding == \"same\":\n+            n_batch, h_x, w_x, d_x, ch_x = x.shape\n+            h_padding = self._same_padding(h_x, h_pool_size, h_stride)\n+            w_padding = self._same_padding(w_x, w_pool_size, w_stride)\n+            d_padding = self._same_padding(d_x, d_pool_size, d_stride)\n+            npad = [(0, 0)] * x.ndim\n+            npad[1] = (0, h_padding)\n+            npad[2] = (0, w_padding)\n+            npad[3] = (0, d_padding)\n+            x = np.pad(x, pad_width=npad, mode=\"symmetric\")\n+\n+        n_batch, h_x, w_x, d_x, ch_x = x.shape\n+        out_h = int((h_x - h_pool_size) / h_stride) + 1\n+        out_w = int((w_x - w_pool_size) / w_stride) + 1\n+        out_d = int((d_x - d_pool_size) / d_stride) + 1\n+\n+        stride_shape = (n_batch, out_h, out_w, out_d, ch_x, *pool_size)\n+        strides = (\n+            x.strides[0],\n+            h_stride * x.strides[1],\n+            w_stride * x.strides[2],\n+            d_stride * x.strides[3],\n+            x.strides[4],\n+            x.strides[1],\n+            x.strides[2],\n+            x.strides[3],\n+        )\n+        windows = as_strided(x, shape=stride_shape, strides=strides)\n+        out = np.mean(windows, axis=(5, 6, 7))\n+        if data_format == \"channels_first\":\n+            out = out.transpose((0, 4, 1, 2, 3))\n+        return out\n+\n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n+        (2, 1, \"valid\", \"channels_first\"),\n         ((2,), (2,), \"valid\", \"channels_last\"),\n+        ((2,), (2,), \"valid\", \"channels_first\"),\n     )\n     def test_average_pooling1d(self, pool_size, strides, padding, data_format):\n         inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n@@ -122,20 +246,17 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.AveragePooling1D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_avgpool1d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n+        (2, 1, \"same\", \"channels_last\"),\n         (2, 1, \"same\", \"channels_first\"),\n         ((2,), (2,), \"same\", \"channels_last\"),\n+        ((2,), (2,), \"same\", \"channels_first\"),\n     )\n     @pytest.mark.skipif(\n         backend.backend() == \"torch\",\n@@ -152,15 +273,10 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.AveragePooling1D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_avgpool1d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n@@ -175,20 +291,17 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.AveragePooling2D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_avgpool2d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (2, (2, 1), \"same\", \"channels_last\"),\n-        ((2, 3), (2, 2), \"same\", \"channels_last\"),\n+        (2, (2, 1), \"same\", \"channels_first\"),\n+        ((2, 2), (2, 2), \"same\", \"channels_last\"),\n+        ((2, 2), (2, 2), \"same\", \"channels_first\"),\n     )\n     @pytest.mark.skipif(\n         backend.backend() == \"torch\",\n@@ -204,20 +317,17 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.AveragePooling2D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_avgpool2d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n+        (2, 1, \"valid\", \"channels_first\"),\n         ((2, 3, 2), (2, 2, 1), \"valid\", \"channels_last\"),\n+        ((2, 3, 2), (2, 2, 1), \"valid\", \"channels_first\"),\n     )\n     def test_average_pooling3d(self, pool_size, strides, padding, data_format):\n         inputs = np.arange(240, dtype=\"float32\").reshape((2, 3, 4, 5, 2))\n@@ -228,20 +338,17 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.AveragePooling3D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_avgpool3d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n+        (2, 1, \"same\", \"channels_last\"),\n         (2, 1, \"same\", \"channels_first\"),\n-        ((2, 3, 2), (2, 2, 1), \"same\", \"channels_last\"),\n+        ((2, 2, 2), (2, 2, 1), \"same\", \"channels_last\"),\n+        ((2, 2, 2), (2, 2, 1), \"same\", \"channels_first\"),\n     )\n     @pytest.mark.skipif(\n         backend.backend() == \"torch\",\n@@ -258,13 +365,8 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.AveragePooling3D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_avgpool3d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n\n@@ -1,6 +1,5 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n from absl.testing import parameterized\n \n from keras_core import layers\n@@ -93,61 +92,69 @@ class GlobalMaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         (\"channels_last\", False),\n         (\"channels_last\", True),\n         (\"channels_first\", False),\n+        (\"channels_first\", True),\n     )\n     def test_global_max_pooling1d(self, data_format, keepdims):\n-        inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n+        def np_global_max_pool1d(x, data_format, keepdims):\n+            steps_axis = [1] if data_format == \"channels_last\" else [2]\n+            res = np.apply_over_axes(np.max, x, steps_axis)\n+            if not keepdims:\n+                res = res.squeeze()\n+            return res\n \n+        inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n         layer = layers.GlobalMaxPooling1D(\n             data_format=data_format,\n             keepdims=keepdims,\n         )\n-        tf_keras_layer = tf.keras.layers.GlobalMaxPooling1D(\n-            data_format=data_format,\n-            keepdims=keepdims,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = np_global_max_pool1d(inputs, data_format, keepdims)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (\"channels_last\", False),\n         (\"channels_last\", True),\n         (\"channels_first\", False),\n+        (\"channels_first\", True),\n     )\n     def test_global_max_pooling2d(self, data_format, keepdims):\n-        inputs = np.arange(96, dtype=\"float32\").reshape((2, 3, 4, 4))\n+        def np_global_max_pool2d(x, data_format, keepdims):\n+            steps_axis = [1, 2] if data_format == \"channels_last\" else [2, 3]\n+            res = np.apply_over_axes(np.max, x, steps_axis)\n+            if not keepdims:\n+                res = res.squeeze()\n+            return res\n \n+        inputs = np.arange(96, dtype=\"float32\").reshape((2, 3, 4, 4))\n         layer = layers.GlobalMaxPooling2D(\n             data_format=data_format,\n             keepdims=keepdims,\n         )\n-        tf_keras_layer = tf.keras.layers.GlobalMaxPooling2D(\n-            data_format=data_format,\n-            keepdims=keepdims,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = np_global_max_pool2d(inputs, data_format, keepdims)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (\"channels_last\", False),\n         (\"channels_last\", True),\n         (\"channels_first\", False),\n+        (\"channels_first\", True),\n     )\n     def test_global_max_pooling3d(self, data_format, keepdims):\n-        inputs = np.arange(360, dtype=\"float32\").reshape((2, 3, 3, 5, 4))\n+        def np_global_max_pool3d(x, data_format, keepdims):\n+            steps_axis = (\n+                [1, 2, 3] if data_format == \"channels_last\" else [2, 3, 4]\n+            )\n+            res = np.apply_over_axes(np.max, x, steps_axis)\n+            if not keepdims:\n+                res = res.squeeze()\n+            return res\n \n+        inputs = np.arange(360, dtype=\"float32\").reshape((2, 3, 3, 5, 4))\n         layer = layers.GlobalMaxPooling3D(\n             data_format=data_format,\n             keepdims=keepdims,\n         )\n-        tf_keras_layer = tf.keras.layers.GlobalMaxPooling3D(\n-            data_format=data_format,\n-            keepdims=keepdims,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = np_global_max_pool3d(inputs, data_format, keepdims)\n         self.assertAllClose(outputs, expected)\n\n@@ -1,7 +1,7 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n from absl.testing import parameterized\n+from numpy.lib.stride_tricks import as_strided\n \n from keras_core import layers\n from keras_core import testing\n@@ -108,10 +108,141 @@ class MaxPoolingBasicTest(testing.TestCase, parameterized.TestCase):\n \n \n class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n+    def _same_padding(self, input_size, pool_size, stride):\n+        if input_size % stride == 0:\n+            return max(pool_size - stride, 0)\n+        else:\n+            return max(pool_size - (input_size % stride), 0)\n+\n+    def _np_maxpool1d(self, x, pool_size, strides, padding, data_format):\n+        if data_format == \"channels_first\":\n+            x = x.swapaxes(1, 2)\n+        if isinstance(pool_size, (tuple, list)):\n+            pool_size = pool_size[0]\n+        if isinstance(strides, (tuple, list)):\n+            h_stride = strides[0]\n+        else:\n+            h_stride = strides\n+\n+        if padding == \"same\":\n+            n_batch, h_x, ch_x = x.shape\n+            pad_value = self._same_padding(h_x, pool_size, h_stride)\n+            npad = [(0, 0)] * x.ndim\n+            npad[1] = (0, pad_value)\n+            x = np.pad(\n+                x, pad_width=npad, mode=\"constant\", constant_values=-np.inf\n+            )\n+\n+        n_batch, h_x, ch_x = x.shape\n+        out_h = int((h_x - pool_size) / h_stride) + 1\n+\n+        stride_shape = (n_batch, out_h, ch_x, pool_size)\n+        strides = (\n+            x.strides[0],\n+            h_stride * x.strides[1],\n+            x.strides[2],\n+            x.strides[1],\n+        )\n+        windows = as_strided(x, shape=stride_shape, strides=strides)\n+        out = np.max(windows, axis=(3,))\n+        if data_format == \"channels_first\":\n+            out = out.swapaxes(1, 2)\n+        return out\n+\n+    def _np_maxpool2d(self, x, pool_size, strides, padding, data_format):\n+        if data_format == \"channels_first\":\n+            x = x.transpose((0, 2, 3, 1))\n+        if isinstance(pool_size, int):\n+            pool_size = (pool_size, pool_size)\n+        if isinstance(strides, int):\n+            strides = (strides, strides)\n+\n+        h_pool_size, w_pool_size = pool_size\n+        h_stride, w_stride = strides\n+        if padding == \"same\":\n+            n_batch, h_x, w_x, ch_x = x.shape\n+            h_padding = self._same_padding(h_x, h_pool_size, h_stride)\n+            w_padding = self._same_padding(w_x, w_pool_size, w_stride)\n+            npad = [(0, 0)] * x.ndim\n+            npad[1] = (0, h_padding)\n+            npad[2] = (0, w_padding)\n+            x = np.pad(\n+                x, pad_width=npad, mode=\"constant\", constant_values=-np.inf\n+            )\n+\n+        n_batch, h_x, w_x, ch_x = x.shape\n+        out_h = int((h_x - h_pool_size) / h_stride) + 1\n+        out_w = int((w_x - w_pool_size) / w_stride) + 1\n+\n+        stride_shape = (n_batch, out_h, out_w, ch_x, *pool_size)\n+        strides = (\n+            x.strides[0],\n+            h_stride * x.strides[1],\n+            w_stride * x.strides[2],\n+            x.strides[3],\n+            x.strides[1],\n+            x.strides[2],\n+        )\n+        windows = as_strided(x, shape=stride_shape, strides=strides)\n+        out = np.max(windows, axis=(4, 5))\n+        if data_format == \"channels_first\":\n+            out = out.transpose((0, 3, 1, 2))\n+        return out\n+\n+    def _np_maxpool3d(self, x, pool_size, strides, padding, data_format):\n+        if data_format == \"channels_first\":\n+            x = x.transpose((0, 2, 3, 4, 1))\n+\n+        if isinstance(pool_size, int):\n+            pool_size = (pool_size, pool_size, pool_size)\n+        if isinstance(strides, int):\n+            strides = (strides, strides, strides)\n+\n+        h_pool_size, w_pool_size, d_pool_size = pool_size\n+        h_stride, w_stride, d_stride = strides\n+\n+        if padding == \"same\":\n+            n_batch, h_x, w_x, d_x, ch_x = x.shape\n+            h_padding = self._same_padding(h_x, h_pool_size, h_stride)\n+            w_padding = self._same_padding(w_x, w_pool_size, w_stride)\n+            d_padding = self._same_padding(d_x, d_pool_size, d_stride)\n+            npad = [(0, 0)] * x.ndim\n+            npad[1] = (0, h_padding)\n+            npad[2] = (0, w_padding)\n+            npad[3] = (0, d_padding)\n+            x = np.pad(\n+                x, pad_width=npad, mode=\"constant\", constant_values=-np.inf\n+            )\n+\n+        n_batch, h_x, w_x, d_x, ch_x = x.shape\n+        out_h = int((h_x - h_pool_size) / h_stride) + 1\n+        out_w = int((w_x - w_pool_size) / w_stride) + 1\n+        out_d = int((d_x - d_pool_size) / d_stride) + 1\n+\n+        stride_shape = (n_batch, out_h, out_w, out_d, ch_x, *pool_size)\n+        strides = (\n+            x.strides[0],\n+            h_stride * x.strides[1],\n+            w_stride * x.strides[2],\n+            d_stride * x.strides[3],\n+            x.strides[4],\n+            x.strides[1],\n+            x.strides[2],\n+            x.strides[3],\n+        )\n+        windows = as_strided(x, shape=stride_shape, strides=strides)\n+        out = np.max(windows, axis=(5, 6, 7))\n+        if data_format == \"channels_first\":\n+            out = out.transpose((0, 4, 1, 2, 3))\n+        return out\n+\n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n+        (2, 1, \"valid\", \"channels_first\"),\n+        (2, 1, \"same\", \"channels_last\"),\n         (2, 1, \"same\", \"channels_first\"),\n         ((2,), (2,), \"valid\", \"channels_last\"),\n+        ((2,), (2,), \"valid\", \"channels_first\"),\n     )\n     def test_max_pooling1d(self, pool_size, strides, padding, data_format):\n         inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n@@ -122,23 +253,21 @@ class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.MaxPool1D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_maxpool1d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n-        ((2, 3), (2, 2), \"same\", \"channels_last\"),\n+        (2, 1, \"valid\", \"channels_first\"),\n+        ((2, 2), (2, 2), \"same\", \"channels_last\"),\n+        ((2, 2), (2, 2), \"same\", \"channels_first\"),\n+        ((2, 3), (3, 3), \"same\", \"channels_last\"),\n     )\n     def test_max_pooling2d(self, pool_size, strides, padding, data_format):\n-        inputs = np.arange(300, dtype=\"float32\").reshape((3, 5, 5, 4))\n+        inputs = np.arange(100, dtype=\"float32\").reshape((1, 5, 5, 4))\n \n         layer = layers.MaxPooling2D(\n             pool_size=pool_size,\n@@ -146,21 +275,17 @@ class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.MaxPool2D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_maxpool2d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n         (2, 1, \"same\", \"channels_first\"),\n         ((2, 3, 2), (2, 2, 1), \"valid\", \"channels_last\"),\n+        ((2, 3, 2), (2, 2, 1), \"valid\", \"channels_first\"),\n     )\n     def test_max_pooling3d(self, pool_size, strides, padding, data_format):\n         inputs = np.arange(240, dtype=\"float32\").reshape((2, 3, 4, 5, 2))\n@@ -171,13 +296,8 @@ class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.MaxPool3D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_maxpool3d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n\n@@ -1129,7 +1129,7 @@ def average(x, axis=None, weights=None):\n     Args:\n         x: Input tensor.\n         axis: Integer along which to average `x`. The default, `axis=None`,\n-            will average over all of the elements of the input tendor. If axis\n+            will average over all of the elements of the input tensor. If axis\n             is negative it counts from the last to the first axis.\n         weights: Tensor of wieghts associated with the values in `x`. Each\n             value in `x` contributes to the average according to its\n@@ -1198,6 +1198,40 @@ class Bincount(Operation):\n \n @keras_core_export([\"keras_core.ops.bincount\", \"keras_core.ops.numpy.bincount\"])\n def bincount(x, weights=None, minlength=0):\n+    \"\"\"Count the number of occurrences of each value in a tensor of integers.\n+\n+    Args:\n+        x: Input tensor.\n+            It must be of dimension 1, and it must only contain non-negative\n+            integer(s).\n+        weights: Weight tensor.\n+            It must have the same length as `x`. The default value is `None`.\n+            If specified, `x` is weighted by it, i.e. if `n = x[i]`,\n+            `out[n] += weight[i]` instead of the default behavior `out[n] += 1`.\n+        minlength: An integer.\n+            The default value is 0. If specified, there will be at least\n+            this number of bins in the output tensor. If greater than\n+            `max(x) + 1`, each value of the output at an index higher than\n+            `max(x)` is set to 0.\n+\n+    Returns:\n+        1D tensor where each element gives the number of occurrence(s) of its\n+        index value in x. Its length is the maximum between `max(x) + 1` and\n+        minlength.\n+\n+    Examples:\n+    >>> x = keras_core.ops.array([1, 2, 2, 3], dtype=\"uint8\")\n+    >>> keras_core.ops.bincount(x)\n+    array([0, 1, 2, 1], dtype=int32)\n+    >>> weights = x / 2\n+    >>> weights\n+    array([0.5, 1., 1., 1.5], dtype=float64)\n+    >>> keras_core.ops.bincount(x, weights=weights)\n+    array([0., 0.5, 2., 1.5], dtype=float64)\n+    >>> minlength = (keras_core.ops.max(x).numpy() + 1) + 2 # 6\n+    >>> keras_core.ops.bincount(x, minlength=minlength)\n+    array([0, 1, 2, 1, 0, 0], dtype=int32)\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Bincount(weights=weights, minlength=minlength).symbolic_call(x)\n     return backend.numpy.bincount(x, weights=weights, minlength=minlength)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#af117ddfc3532bfcc1843a165b7fecbc2d50a193", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 212 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 8 | Methods Changed: 10 | Complexity Δ (Sum/Max): 20/5 | Churn Δ: 212 | Churn Cumulative: 1218 | Contributors (this commit): 10 | Commits (past 90d): 34 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,6 +13,17 @@ def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n     )\n \n \n+def segment_max(data, segment_ids, num_segments=None, sorted=False):\n+    if num_segments is None:\n+        raise ValueError(\n+            \"Argument `num_segments` must be set when using the JAX backend. \"\n+            \"Received: num_segments=None\"\n+        )\n+    return jax.ops.segment_max(\n+        data, segment_ids, num_segments, indices_are_sorted=sorted\n+    )\n+\n+\n def top_k(x, k, sorted=True):\n     # Jax does not supported `sorted`, but in the case where `sorted=False`,\n     # order is not guaranteed, so OK to return sorted output.\n\n@@ -31,6 +31,33 @@ def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n     return result\n \n \n+def segment_max(data, segment_ids, num_segments=None, sorted=False):\n+    if num_segments is None:\n+        num_segments = np.amax(segment_ids) + 1\n+\n+    valid_indices = segment_ids >= 0  # Ignore segment_ids that are -1\n+    valid_data = data[valid_indices]\n+    valid_segment_ids = segment_ids[valid_indices]\n+\n+    data_shape = list(valid_data.shape)\n+    data_shape[\n+        0\n+    ] = num_segments  # Replace first dimension (which corresponds to segments)\n+\n+    if sorted:\n+        result = np.zeros(data_shape, dtype=valid_data.dtype)\n+        np.maximum.at(result, valid_segment_ids, valid_data)\n+    else:\n+        sort_indices = np.argsort(valid_segment_ids)\n+        sorted_segment_ids = valid_segment_ids[sort_indices]\n+        sorted_data = valid_data[sort_indices]\n+\n+        result = np.zeros(data_shape, dtype=valid_data.dtype)\n+        np.maximum.at(result, sorted_segment_ids, sorted_data)\n+\n+    return result\n+\n+\n def top_k(x, k, sorted=False):\n     sorted_indices = np.argsort(x, axis=-1)[..., ::-1]\n     sorted_values = np.sort(x, axis=-1)[..., ::-1]\n\n@@ -13,6 +13,16 @@ def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n         return tf.math.unsorted_segment_sum(data, segment_ids, num_segments)\n \n \n+def segment_max(data, segment_ids, num_segments=None, sorted=False):\n+    if sorted:\n+        return tf.math.segment_max(data, segment_ids)\n+    else:\n+        if num_segments is None:\n+            unique_segment_ids, _ = tf.unique(segment_ids)\n+            num_segments = tf.shape(unique_segment_ids)[0]\n+        return tf.math.unsorted_segment_max(data, segment_ids, num_segments)\n+\n+\n def top_k(x, k, sorted=True):\n     return tf.math.top_k(x, k, sorted=sorted)\n \n\n@@ -42,6 +42,44 @@ def segment_sum(data, segment_ids, num_segments=None, **kwargs):\n     return result.type(data.dtype)\n \n \n+def segment_max(data, segment_ids, num_segments=None, **kwargs):\n+    data = convert_to_tensor(data)\n+    segment_ids = convert_to_tensor(segment_ids)\n+    num_repeats = torch.prod(\n+        torch.tensor(data.shape[1:], device=get_device())\n+    ).long()\n+    # To use `scatter_reduce` in torch, we need to replicate `segment_ids` into\n+    # the shape of `data`.\n+    segment_ids = (\n+        segment_ids.repeat_interleave(num_repeats)\n+        .view(*data.shape)\n+        .type(torch.int64)\n+    )\n+    num_segments = num_segments or len(torch.unique(segment_ids))\n+\n+    # .scatter_reduce does not support -1 in the indices.\n+    # Add all out-of-bound indices value to an extra dimension after\n+    # num_segments, which is removed before returning the result.\n+\n+    # Replacing the out-of-bound indices.\n+    segment_ids = torch.where(segment_ids >= 0, segment_ids, num_segments)\n+    segment_ids = torch.where(\n+        segment_ids < num_segments, segment_ids, num_segments\n+    )\n+\n+    # Add one more dimension to the result shape with the \"+1\".\n+    shape = (num_segments + 1,) + tuple(data.shape[1:])\n+\n+    result = torch.zeros(*shape, device=get_device()).scatter_reduce(\n+        0, segment_ids, data.float(), \"amax\"\n+    )\n+\n+    # Removing the extra dimension.\n+    result = result[:-1, ...]\n+\n+    return result.type(data.dtype)\n+\n+\n def top_k(x, k, sorted=True):\n     x = convert_to_tensor(x)\n     return torch.topk(x, k, sorted=sorted)\n\n@@ -60,6 +60,58 @@ def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n     )\n \n \n+class SegmentMax(Operation):\n+    def __init__(self, num_segments=None, sorted=False):\n+        super().__init__()\n+        self.num_segments = num_segments\n+        self.sorted = sorted\n+\n+    def compute_output_spec(self, data, segment_ids):\n+        num_segments = self.num_segments\n+        output_shape = (num_segments,) + tuple(data.shape[1:])\n+        return KerasTensor(shape=output_shape, dtype=data.dtype)\n+\n+    def call(self, data, segment_ids):\n+        return backend.math.segment_max(\n+            data,\n+            segment_ids,\n+            num_segments=self.num_segments,\n+            sorted=self.sorted,\n+        )\n+\n+\n+@keras_core_export(\"keras_core.ops.segment_max\")\n+def segment_max(data, segment_ids, num_segments=None, sorted=False):\n+    \"\"\"Computes the max of segments in a tensor.\n+\n+    Args:\n+        data: Input tensor.\n+        segment_ids: A 1-D tensor containing segment indices for each\n+            element in `data`.\n+        num_segments: An integer representing the total number of\n+            segments. If not specified, it is inferred from the maximum\n+            value in `segment_ids`.\n+        sorted: A boolean indicating whether `segment_ids` is sorted.\n+            Default is `False`.\n+\n+    Returns:\n+        A tensor containing the max of segments, where each element\n+        represents the max of the corresponding segment in `data`.\n+\n+    Example:\n+\n+    >>> data = keras_core.ops.convert_to_tensor([1, 2, 3, 4, 5, 6])\n+    >>> segment_ids = keras_core.ops.convert_to_tensor([0, 1, 0, 1, 0, 1])\n+    >>> segment_max(data, segment_ids)\n+    array([9 12], shape=(2,), dtype=int32)\n+    \"\"\"\n+    if any_symbolic_tensors((data,)):\n+        return SegmentSum(num_segments, sorted).symbolic_call(data, segment_ids)\n+    return backend.math.segment_max(\n+        data, segment_ids, num_segments=num_segments, sorted=sorted\n+    )\n+\n+\n class TopK(Operation):\n     def __init__(self, k, sorted=False):\n         super().__init__()\n\n@@ -20,6 +20,17 @@ class MathOpsDynamicShapeTest(testing.TestCase):\n         outputs = kmath.segment_sum(data, segment_ids, num_segments=5)\n         self.assertEqual(outputs.shape, (5, 4))\n \n+    def test_segment_max(self):\n+        data = KerasTensor((None, 4), dtype=\"float32\")\n+        segment_ids = KerasTensor((10,), dtype=\"int32\")\n+        outputs = kmath.segment_max(data, segment_ids)\n+        self.assertEqual(outputs.shape, (None, 4))\n+\n+        data = KerasTensor((None, 4), dtype=\"float32\")\n+        segment_ids = KerasTensor((10,), dtype=\"int32\")\n+        outputs = kmath.segment_max(data, segment_ids, num_segments=5)\n+        self.assertEqual(outputs.shape, (5, 4))\n+\n     def test_top_k(self):\n         x = KerasTensor((None, 2, 3))\n         values, indices = kmath.top_k(x, k=1)\n@@ -90,6 +101,22 @@ class MathOpsStaticShapeTest(testing.TestCase):\n         outputs = kmath.segment_sum(data, segment_ids, num_segments=5)\n         self.assertEqual(outputs.shape, (5, 4))\n \n+    @pytest.mark.skipif(\n+        backend.backend() == \"jax\",\n+        reason=\"JAX does not support `num_segments=None`.\",\n+    )\n+    def test_segment_max(self):\n+        data = KerasTensor((10, 4), dtype=\"float32\")\n+        segment_ids = KerasTensor((10,), dtype=\"int32\")\n+        outputs = kmath.segment_max(data, segment_ids)\n+        self.assertEqual(outputs.shape, (None, 4))\n+\n+    def test_segment_max_explicit_num_segments(self):\n+        data = KerasTensor((10, 4), dtype=\"float32\")\n+        segment_ids = KerasTensor((10,), dtype=\"int32\")\n+        outputs = kmath.segment_max(data, segment_ids, num_segments=5)\n+        self.assertEqual(outputs.shape, (5, 4))\n+\n     def test_topk(self):\n         x = KerasTensor((1, 2, 3))\n         values, indices = kmath.top_k(x, k=1)\n@@ -183,6 +210,53 @@ class MathOpsCorrectnessTest(testing.TestCase):\n         )\n         self.assertAllClose(outputs, expected)\n \n+    @pytest.mark.skipif(\n+        backend.backend() == \"jax\",\n+        reason=\"JAX does not support `num_segments=None`.\",\n+    )\n+    def test_segment_max(self):\n+        # Test 1D case.\n+        data = np.array([1, 2, 3, 4, 5, 6, 7, 8], dtype=np.float32)\n+        segment_ids = np.array([0, 0, 1, 1, 1, 2, 2, 2], dtype=np.int32)\n+        outputs = kmath.segment_max(data, segment_ids)\n+        expected = tf.math.segment_max(data, segment_ids)\n+        self.assertAllClose(outputs, expected)\n+\n+        # Test N-D case.\n+        data = np.random.rand(9, 3, 3)\n+        segment_ids = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2], dtype=np.int32)\n+        outputs = kmath.segment_max(data, segment_ids)\n+        expected = tf.math.segment_max(data, segment_ids)\n+        self.assertAllClose(outputs, expected)\n+\n+    def test_segment_max_explicit_num_segments(self):\n+        # Test 1D case.\n+        data = np.array([1, 2, 3, 4, 5, 6, 7, 8], dtype=np.float32)\n+        segment_ids = np.array([0, 0, 1, 1, 1, 2, 2, 2], dtype=np.int32)\n+        outputs = kmath.segment_max(data, segment_ids, num_segments=3)\n+        expected = tf.math.unsorted_segment_max(\n+            data, segment_ids, num_segments=3\n+        )\n+        self.assertAllClose(outputs, expected)\n+\n+        # Test 1D with -1 case.\n+        data = np.array([1, 2, 3, 4, 5, 6, 7, 8], dtype=np.float32)\n+        segment_ids = np.array([0, 0, 1, 1, -1, 2, 2, -1], dtype=np.int32)\n+        outputs = kmath.segment_max(data, segment_ids, num_segments=3)\n+        expected = tf.math.unsorted_segment_max(\n+            data, segment_ids, num_segments=3\n+        )\n+        self.assertAllClose(outputs, expected)\n+\n+        # Test N-D case.\n+        data = np.random.rand(9, 3, 3)\n+        segment_ids = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2], dtype=np.int32)\n+        outputs = kmath.segment_max(data, segment_ids, num_segments=3)\n+        expected = tf.math.unsorted_segment_max(\n+            data, segment_ids, num_segments=3\n+        )\n+        self.assertAllClose(outputs, expected)\n+\n     def test_top_k(self):\n         x = np.array([0, 4, 2, 1, 3, -1], dtype=np.float32)\n         values, indices = kmath.top_k(x, k=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5534880cb85f7a2445c3c8eb6499ff784a04f5c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 200 | Contributors (this commit): 5 | Commits (past 90d): 15 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -109,7 +109,7 @@ def global_rng_state():\n \n     In JAX, if you're using unseeded random ops, be mindful that\n     their outputs will be unchanged across different calls of\n-    traced function (e.g. a `jax.jit`-transformed function) since\n+    a traced function (e.g. a `jax.jit`-transformed function) since\n     traced functions in JAX are fully stateless. To get\n     different outputs across different calls, you will need to pass the\n     global RNG state in and out of the function boundary, like this:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fbee24c0458fc7d87a5fb33c774427f2ed435892", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 629 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -28,6 +28,6 @@ from keras.engine.training import Model\n from tensorflow.python import tf2\n from tensorflow.python.util.tf_export import keras_export\n \n-__version__ = \"2.14.0\"\n+__version__ = \"2.15.0\"\n \n keras_export(\"keras.__version__\").export_constant(__name__, \"__version__\")\n\n@@ -31,7 +31,7 @@ DOCLINES = __doc__.split(\"\\n\")\n # This version string is semver compatible, but incompatible with pip.\n # For pip, we will remove all '-' characters from this string, and use the\n # result for pip.\n-_VERSION = \"2.14.0\"\n+_VERSION = \"2.15.0\"\n \n REQUIRED_PACKAGES = [\n     # We depend on TensorFlow's declared pip dependencies.\n@@ -58,7 +58,7 @@ setuptools.setup(\n     packages=setuptools.find_packages(),\n     install_requires=REQUIRED_PACKAGES,\n     # Supported Python versions\n-    python_requires=\">=3.8\",\n+    python_requires=\">=3.9\",\n     # PyPI package information.\n     classifiers=[\n         \"Development Status :: 5 - Production/Stable\",\n@@ -67,7 +67,6 @@ setuptools.setup(\n         \"Intended Audience :: Science/Research\",\n         \"License :: OSI Approved :: Apache Software License\",\n         \"Programming Language :: Python :: 3\",\n-        \"Programming Language :: Python :: 3.8\",\n         \"Programming Language :: Python :: 3.9\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2692983c33e3ddd91947c93bad178dd7de8e06bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 420 | Contributors (this commit): 7 | Commits (past 90d): 20 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -197,3 +197,5 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         self.assertGreater(np.abs(y1 - y2), 1e-4)\n         self.assertGreater(np.abs(y1 - y3), 1e-4)\n         self.assertGreater(np.abs(y2 - y3), 1e-4)\n+\n+        seed_generator.global_rng_state().assign(seed)\n\n@@ -57,7 +57,7 @@ class TestNumericalUtils(testing.TestCase, parameterized.TestCase):\n     @parameterized.parameters([1, 2, 3])\n     def test_normalize(self, order):\n         xnp = np.random.random((3, 3))\n-        xb = backend.random.uniform((3, 3))\n+        xb = backend.random.uniform((3, 3), seed=1337)\n \n         # Test NumPy\n         out = numerical_utils.normalize(xnp, axis=-1, order=order)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9fc393686305059d527fb184bb45ced3eadf2941", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 40 | Files Changed: 1 | Hunks: 20 | Methods Changed: 15 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 122 | Churn Cumulative: 647 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,9 @@\n \"\"\"\n Title: Image classification with modern MLP models\n Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n+Converted to Keras Core by: [Guillaume Baquiast](https://www.linkedin.com/in/guillaume-baquiast-478965ba/), [divyasreepat](https://github.com/divyashreepathihalli)\n Date created: 2021/05/30\n-Last modified: 2021/05/30\n+Last modified: 2023/08/03\n Description: Implementing the MLP-Mixer, FNet, and gMLP models for CIFAR-100 image classification.\n Accelerator: GPU\n \"\"\"\n@@ -28,10 +29,8 @@ main building blocks.\n \"\"\"\n \n import numpy as np\n-import tensorflow as tf\n import keras_core as keras\n from keras_core import layers\n-from keras_core.layers import Lambda\n \n \"\"\"\n ## Prepare the data\n@@ -76,15 +75,11 @@ def build_classifier(blocks, positional_encoding=False):\n     # Augment data.\n     augmented = data_augmentation(inputs)\n     # Create patches.\n-    patches = Patches(patch_size, num_patches)(augmented)\n+    patches = Patches(patch_size)(augmented)\n     # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n     x = layers.Dense(units=embedding_dim)(patches)\n     if positional_encoding:\n-        positions = tf.range(start=0, limit=num_patches, delta=1)\n-        position_embedding = layers.Embedding(\n-            input_dim=num_patches, output_dim=embedding_dim\n-        )(positions)\n-        x = x + position_embedding\n+        x = x + PositionEmbedding(sequence_length=num_patches)(x)\n     # Process x using the module blocks.\n     x = blocks(x)\n     # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.\n@@ -168,26 +163,74 @@ data_augmentation.layers[0].adapt(x_train)\n \n \n class Patches(layers.Layer):\n-    def __init__(self, patch_size, num_patches):\n-        super().__init__()\n+    def __init__(self, patch_size, **kwargs):\n+        super().__init__(**kwargs)\n         self.patch_size = patch_size\n-        self.num_patches = num_patches\n \n-    def call(self, images):\n-        batch_size = tf.shape(images)[0]\n-        patches = tf.image.extract_patches(\n-            images=images,\n-            sizes=[1, self.patch_size, self.patch_size, 1],\n-            strides=[1, self.patch_size, self.patch_size, 1],\n-            rates=[1, 1, 1, 1],\n-            padding=\"VALID\",\n-        )\n-        patch_dims = patches.shape[-1]\n-        patches = tf.reshape(\n-            patches, [batch_size, self.num_patches, patch_dims]\n-        )\n-        return patches\n+    def call(self, x):\n+        patches = keras.ops.image.extract_patches(x, self.patch_size)\n+        batch_size = keras.ops.shape(patches)[0]\n+        num_patches = keras.ops.shape(patches)[1] * keras.ops.shape(patches)[2]\n+        patch_dim = keras.ops.shape(patches)[3]\n+        out = keras.ops.reshape(patches, (batch_size, num_patches, patch_dim))\n+        return out\n \n+\"\"\"\n+## Implement position embedding as a layer\n+\"\"\"\n+\n+class PositionEmbedding(keras.layers.Layer):\n+    def __init__(\n+        self,\n+        sequence_length,\n+        initializer=\"glorot_uniform\",\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        if sequence_length is None:\n+            raise ValueError(\n+                \"`sequence_length` must be an Integer, received `None`.\"\n+            )\n+        self.sequence_length = int(sequence_length)\n+        self.initializer = keras.initializers.get(initializer)\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"sequence_length\": self.sequence_length,\n+                \"initializer\": keras.initializers.serialize(self.initializer),\n+            }\n+        )\n+        return config\n+\n+    def build(self, input_shape):\n+        feature_size = input_shape[-1]\n+        self.position_embeddings = self.add_weight(\n+            name=\"embeddings\",\n+            shape=[self.sequence_length, feature_size],\n+            initializer=self.initializer,\n+            trainable=True,\n+        )\n+\n+        super().build(input_shape)\n+\n+    def call(self, inputs, start_index=0):\n+        shape = keras.ops.shape(inputs)\n+        feature_length = shape[-1]\n+        sequence_length = shape[-2]\n+        # trim to match the length of the input sequence, which might be less\n+        # than the sequence_length of the layer.\n+        position_embeddings = keras.ops.convert_to_tensor(self.position_embeddings)\n+        position_embeddings = keras.ops.slice(\n+            position_embeddings,\n+            (start_index, 0),\n+            (sequence_length, feature_length),\n+        )\n+        return keras.ops.broadcast_to(position_embeddings, shape)\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n \n \"\"\"\n ## The MLP-Mixer model\n@@ -224,7 +267,7 @@ class MLPMixerLayer(layers.Layer):\n         self.mlp2 = keras.Sequential(\n             [\n                 layers.Dense(units=num_patches, activation=\"gelu\"),\n-                layers.Dense(units=embedding_dim),\n+                layers.Dense(units=hidden_units),\n                 layers.Dropout(rate=dropout_rate),\n             ]\n         )\n@@ -237,11 +280,11 @@ class MLPMixerLayer(layers.Layer):\n         # Apply layer normalization.\n         x = self.normalize(inputs)\n         # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n-        x_channels = tf.linalg.matrix_transpose(x)\n+        x_channels = keras.ops.transpose(x, axes=(0, 2, 1))\n         # Apply mlp1 on each channel independently.\n         mlp1_outputs = self.mlp1(x_channels)\n         # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n-        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n+        mlp1_outputs = keras.ops.transpose(mlp1_outputs, axes=(0, 2, 1))\n         # Add skip connection.\n         x = mlp1_outputs + inputs\n         # Apply layer normalization.\n@@ -279,7 +322,7 @@ As mentioned in the [MLP-Mixer](https://arxiv.org/abs/2105.01601) paper,\n when pre-trained on large datasets, or with modern regularization schemes,\n the MLP-Mixer attains competitive scores to state-of-the-art models.\n You can obtain better results by increasing the embedding dimensions,\n-increasing, increasing the number of mixer blocks, and training the model for longer.\n+increasing the number of mixer blocks, and training the model for longer.\n You may also try to increase the size of the input images and use different patch sizes.\n \"\"\"\n \n@@ -300,7 +343,7 @@ in the Transformer block with a parameter-free 2D Fourier transformation layer:\n \n class FNetLayer(layers.Layer):\n     def __init__(\n-        self, num_patches, embedding_dim, dropout_rate, *args, **kwargs\n+        self, embedding_dim, dropout_rate, *args, **kwargs\n     ):\n         super().__init__(*args, **kwargs)\n \n@@ -317,10 +360,9 @@ class FNetLayer(layers.Layer):\n \n     def call(self, inputs):\n         # Apply fourier transformations.\n-        x = tf.cast(\n-            tf.signal.fft2d(tf.cast(inputs, dtype=tf.dtypes.complex64)),\n-            dtype=tf.dtypes.float32,\n-        )\n+        real_part = inputs\n+        im_part = keras.ops.zeros_like(inputs)\n+        x = keras.ops.fft2((real_part, im_part))[0]\n         # Add skip connection.\n         x = x + inputs\n         # Apply layer normalization.\n@@ -342,7 +384,7 @@ takes around 8 seconds per epoch.\n \n fnet_blocks = keras.Sequential(\n     [\n-        FNetLayer(num_patches, embedding_dim, dropout_rate)\n+        FNetLayer(embedding_dim, dropout_rate)\n         for _ in range(num_blocks)\n     ]\n )\n@@ -398,14 +440,14 @@ class gMLPLayer(layers.Layer):\n \n     def spatial_gating_unit(self, x):\n         # Split x along the channel dimensions.\n-        # Tensors u and v will in th shape of [batch_size, num_patchs, embedding_dim].\n-        u, v = tf.split(x, num_or_size_splits=2, axis=2)\n+        # Tensors u and v will in the shape of [batch_size, num_patchs, embedding_dim].\n+        u, v = keras.ops.split(x, indices_or_sections=2, axis=2)\n         # Apply layer normalization.\n         v = self.normalize2(v)\n         # Apply spatial projection.\n-        v_channels = tf.linalg.matrix_transpose(v)\n+        v_channels = keras.ops.transpose(v, axes=(0, 2, 1))\n         v_projected = self.spatial_projection(v_channels)\n-        v_projected = tf.linalg.matrix_transpose(v_projected)\n+        v_projected = keras.ops.transpose(v_projected, axes=(0, 2, 1))\n         # Apply element-wise multiplication.\n         return u * v_projected\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
