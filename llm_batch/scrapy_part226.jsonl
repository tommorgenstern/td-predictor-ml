{"custom_id": "scrapy#fc07711614549ce8f0464b3fca5b0f4acd746681", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 30 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -12,8 +12,7 @@ class Command(fetch.Command):\n \n     def add_options(self, parser):\n         super(Command, self).add_options(parser)\n-        parser.add_option(\"--spider\", dest=\"spider\",\n-            help=\"use this spider\")\n+        parser.remove_option(\"--headers\")\n \n     def _print_response(self, response, opts):\n         open_in_browser(response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ae6d8d728e12e8efd704ba529de1db2eacfb494f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 28 | Churn Cumulative: 1171 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -114,8 +114,8 @@ class BaseSettings(MutableMapping):\n         \"\"\"\n         Get a setting value as a boolean.\n \n-        ``1``, ``'1'``, and ``True`` return ``True``, while ``0``, ``'0'``,\n-        ``False`` and ``None`` return ``False``.\n+        ``1``, ``'1'``, `True`` and ``'True'`` return ``True``,\n+        while ``0``, ``'0'``, ``False``, ``'False'`` and ``None`` return ``False``.\n \n         For example, settings populated through environment variables set to\n         ``'0'`` will return ``False`` when using this method.\n@@ -126,7 +126,17 @@ class BaseSettings(MutableMapping):\n         :param default: the value to return if no setting is found\n         :type default: any\n         \"\"\"\n-        return bool(int(self.get(name, default)))\n+        got = self.get(name, default)\n+        try:\n+            return bool(int(got))\n+        except ValueError:\n+            if got in (\"True\", \"true\"):\n+                return True\n+            if got in (\"False\", \"false\"):\n+                return False\n+            raise ValueError(\"Supported values for boolean settings \"\n+                             \"are 0/1, True/False, '0'/'1', \"\n+                             \"'True'/'False' and 'true'/'false'\")\n \n     def getint(self, name, default=0):\n         \"\"\"\n\n@@ -211,9 +211,15 @@ class BaseSettingsTest(unittest.TestCase):\n             'TEST_ENABLED1': '1',\n             'TEST_ENABLED2': True,\n             'TEST_ENABLED3': 1,\n+            'TEST_ENABLED4': 'True',\n+            'TEST_ENABLED5': 'true',\n+            'TEST_ENABLED_WRONG': 'on',\n             'TEST_DISABLED1': '0',\n             'TEST_DISABLED2': False,\n             'TEST_DISABLED3': 0,\n+            'TEST_DISABLED4': 'False',\n+            'TEST_DISABLED5': 'false',\n+            'TEST_DISABLED_WRONG': 'off',\n             'TEST_INT1': 123,\n             'TEST_INT2': '123',\n             'TEST_FLOAT1': 123.45,\n@@ -231,11 +237,15 @@ class BaseSettingsTest(unittest.TestCase):\n         self.assertTrue(settings.getbool('TEST_ENABLED1'))\n         self.assertTrue(settings.getbool('TEST_ENABLED2'))\n         self.assertTrue(settings.getbool('TEST_ENABLED3'))\n+        self.assertTrue(settings.getbool('TEST_ENABLED4'))\n+        self.assertTrue(settings.getbool('TEST_ENABLED5'))\n         self.assertFalse(settings.getbool('TEST_ENABLEDx'))\n         self.assertTrue(settings.getbool('TEST_ENABLEDx', True))\n         self.assertFalse(settings.getbool('TEST_DISABLED1'))\n         self.assertFalse(settings.getbool('TEST_DISABLED2'))\n         self.assertFalse(settings.getbool('TEST_DISABLED3'))\n+        self.assertFalse(settings.getbool('TEST_DISABLED4'))\n+        self.assertFalse(settings.getbool('TEST_DISABLED5'))\n         self.assertEqual(settings.getint('TEST_INT1'), 123)\n         self.assertEqual(settings.getint('TEST_INT2'), 123)\n         self.assertEqual(settings.getint('TEST_INTx'), 0)\n@@ -258,6 +268,8 @@ class BaseSettingsTest(unittest.TestCase):\n         self.assertEqual(settings.getdict('TEST_DICT3'), {})\n         self.assertEqual(settings.getdict('TEST_DICT3', {'key1': 5}), {'key1': 5})\n         self.assertRaises(ValueError, settings.getdict, 'TEST_LIST1')\n+        self.assertRaises(ValueError, settings.getbool, 'TEST_ENABLED_WRONG')\n+        self.assertRaises(ValueError, settings.getbool, 'TEST_DISABLED_WRONG')\n \n     def test_getpriority(self):\n         settings = BaseSettings({'key': 'value'}, priority=99)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d2e9ea0c88b7578c5fc8d4d37e5df9d078e9b884", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 56 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -16,8 +16,11 @@ class CachingThreadedResolver(ThreadedResolver):\n     def getHostByName(self, name, timeout=None):\n         if name in dnscache:\n             return defer.succeed(dnscache[name])\n-        if not timeout:\n-            timeout = self.timeout\n+        # in Twisted<=16.6, getHostByName() is always called with\n+        # a default timeout of 60s (actually passed as (1, 3, 11, 45) tuple),\n+        # so the input argument above is simply overridden\n+        # to enforce Scrapy's DNS_TIMEOUT setting's value\n+        timeout = (self.timeout,)\n         d = super(CachingThreadedResolver, self).getHostByName(name, timeout)\n         d.addCallback(self._cache_result, name)\n         return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a58624375824d821f0bdadb7bc04fad53171ca70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 1443 | Contributors (this commit): 16 | Commits (past 90d): 3 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -105,6 +105,7 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n         self._tunneledHost = host\n         self._tunneledPort = port\n         self._contextFactory = contextFactory\n+        self._connectBuffer = b''\n \n     def requestTunnel(self, protocol):\n         \"\"\"Asks the proxy to open a tunnel.\"\"\"\n@@ -121,8 +122,16 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n         created, notifies the client that we are ready to send requests. If not\n         raises a TunnelError.\n         \"\"\"\n+        self._connectBuffer += rcvd_bytes\n+        # make sure that enough (all) bytes are consumed\n+        # and that we've got all HTTP headers (ending with a blank line)\n+        # from the proxy so that we don't send those bytes to the TLS layer\n+        #\n+        # see https://github.com/scrapy/scrapy/issues/2491\n+        if b'\\r\\n\\r\\n' not in self._connectBuffer:\n+            return\n         self._protocol.dataReceived = self._protocolDataReceived\n-        respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(rcvd_bytes)\n+        respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(self._connectBuffer)\n         if respm and int(respm.group('status')) == 200:\n             try:\n                 # this sets proper Server Name Indication extension\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0cf6344cc22a193079ea1fd2fbe28187cc212c7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 370 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -111,8 +111,8 @@ class TextResponse(Response):\n             self._cached_selector = Selector(self)\n         return self._cached_selector\n \n-    def xpath(self, query):\n-        return self.selector.xpath(query)\n+    def xpath(self, query, **kwargs):\n+        return self.selector.xpath(query, **kwargs)\n \n     def css(self, query):\n         return self.selector.css(query)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#803d8c4b5709d792f1c314bbde124e69903ff23f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 32 | Churn Cumulative: 220 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -320,6 +320,20 @@ class TextResponseTest(BaseResponseTest):\n             response.selector.css(\"title::text\").extract(),\n         )\n \n+    def test_selector_shortcuts_kwargs(self):\n+        body = b\"<html><head><title>Some page</title><body><p class=\\\"content\\\">A nice paragraph.</p></body></html>\"\n+        response = self.response_class(\"http://www.example.com\", body=body)\n+\n+        self.assertEqual(\n+            response.xpath(\"normalize-space(//p[@class=$pclass])\", pclass=\"content\").extract(),\n+            response.xpath(\"normalize-space(//p[@class=\\\"content\\\"])\").extract(),\n+        )\n+        self.assertEqual(\n+            response.xpath(\"//title[count(following::p[@class=$pclass])=$pcount]/text()\",\n+                pclass=\"content\", pcount=1).extract(),\n+            response.xpath(\"//title[count(following::p[@class=\\\"content\\\"])=1]/text()\").extract(),\n+        )\n+\n     def test_urljoin_with_base_url(self):\n         \"\"\"Test urljoin shortcut which also evaluates base-url through get_base_url().\"\"\"\n         body = b'<html><body><base href=\"https://example.net\"></body></html>'\n@@ -428,3 +442,21 @@ class XmlResponseTest(TextResponseTest):\n             response.xpath(\"//elem/text()\").extract(),\n             response.selector.xpath(\"//elem/text()\").extract(),\n         )\n+\n+    def test_selector_shortcuts_kwargs(self):\n+        body = b'''<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+        <xml xmlns:somens=\"http://scrapy.org\">\n+        <somens:elem>value</somens:elem>\n+        </xml>'''\n+        response = self.response_class(\"http://www.example.com\", body=body)\n+\n+        self.assertEqual(\n+            response.xpath(\"//s:elem/text()\", namespaces={'s': 'http://scrapy.org'}).extract(),\n+            response.selector.xpath(\"//s:elem/text()\", namespaces={'s': 'http://scrapy.org'}).extract(),\n+        )\n+\n+        response.selector.register_namespace('s2', 'http://scrapy.org')\n+        self.assertEqual(\n+            response.xpath(\"//s1:elem/text()\", namespaces={'s1': 'http://scrapy.org'}).extract(),\n+            response.selector.xpath(\"//s2:elem/text()\").extract(),\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1295c17a26544f451261f4c6558b344162f3c4d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 425 | Contributors (this commit): 12 | Commits (past 90d): 4 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -49,7 +49,7 @@ setup(\n         'pyOpenSSL',\n         'cssselect>=0.9',\n         'six>=1.5.2',\n-        'parsel>=0.9.5',\n+        'parsel>=1.1',\n         'PyDispatcher>=2.0.5',\n         'service_identity',\n     ],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3358254c5c78a1b8de1d02cb3b1db85fe824c798", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 356 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -91,11 +91,10 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_retry_dns_error(self):\n-        with mock.patch('socket.gethostbyname',\n-                        side_effect=socket.gaierror(-5, 'No address associated with hostname')):\n         crawler = self.runner.create_crawler(SimpleSpider)\n         with LogCapture() as l:\n-                yield crawler.crawl(\"http://example.com/\")\n+            # try to fetch the homepage of a non-existent domain\n+            yield crawler.crawl(\"http://dns.resolution.invalid/\")\n         self._assert_retried(l)\n \n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e604c0f3abeafdd896606960170ab5bad699d79d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 358 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,4 @@\n import json\n-import socket\n import logging\n \n from testfixtures import LogCapture\n@@ -9,7 +8,6 @@ from twisted.trial.unittest import TestCase\n from scrapy.http import Request\n from scrapy.crawler import CrawlerRunner\n from scrapy.utils.python import to_unicode\n-from tests import mock\n from tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \\\n     BrokenStartRequestsSpider, SingleRequestSpider, DuplicateStartRequestsSpider\n from tests.mockserver import MockServer\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#02e1d2b1fd0257a926f438a7758a9e65a0114325", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 360 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -92,7 +92,7 @@ class CrawlTestCase(TestCase):\n         crawler = self.runner.create_crawler(SimpleSpider)\n         with LogCapture() as l:\n             # try to fetch the homepage of a non-existent domain\n-            yield crawler.crawl(\"http://dns.resolution.invalid/\")\n+            yield crawler.crawl(\"http://dns.resolution.invalid./\")\n         self._assert_retried(l)\n \n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#16c4b4e184c1b213eafacecca0097ff44effa696", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 19 | Churn Cumulative: 88 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,7 @@\n import zlib\n \n+import brotli\n+\n from scrapy.utils.gz import gunzip, is_gzipped\n from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n@@ -17,7 +19,7 @@ class HttpCompressionMiddleware(object):\n         return cls()\n \n     def process_request(self, request, spider):\n-        request.headers.setdefault('Accept-Encoding', 'gzip,deflate')\n+        request.headers.setdefault('Accept-Encoding', 'gzip,deflate,br')\n \n     def process_response(self, request, response, spider):\n \n@@ -55,5 +57,7 @@ class HttpCompressionMiddleware(object):\n                 # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\n                 # http://www.gzip.org/zlib/zlib_faq.html#faq38\n                 body = zlib.decompress(body, -15)\n+        if encoding == b\"br\":\n+            body = brotli.decompress(body)\n         return body\n \n\n@@ -17,6 +17,7 @@ FORMAT = {\n         'x-gzip': ('html-gzip.bin', 'gzip'),\n         'rawdeflate': ('html-rawdeflate.bin', 'deflate'),\n         'zlibdeflate': ('html-zlibdeflate.bin', 'deflate'),\n+        'br': ('html-br.bin', 'br')\n         }\n \n class HttpCompressionTest(TestCase):\n@@ -50,7 +51,8 @@ class HttpCompressionTest(TestCase):\n         request = Request('http://scrapytest.org')\n         assert 'Accept-Encoding' not in request.headers\n         self.mw.process_request(request, self.spider)\n-        self.assertEqual(request.headers.get('Accept-Encoding'), b'gzip,deflate')\n+        self.assertEqual(request.headers.get('Accept-Encoding'),\n+                         b'gzip,deflate,br')\n \n     def test_process_response_gzip(self):\n         response = self._getresponse('gzip')\n@@ -62,6 +64,15 @@ class HttpCompressionTest(TestCase):\n         assert newresponse.body.startswith(b'<!DOCTYPE')\n         assert 'Content-Encoding' not in newresponse.headers\n \n+    def test_process_response_br(self):\n+        response = self._getresponse('br')\n+        request = response.request\n+        self.assertEqual(response.headers['Content-Encoding'], b'br')\n+        newresponse = self.mw.process_response(request, response, self.spider)\n+        assert newresponse is not response\n+        assert newresponse.body.startswith(b\"<!DOCTYPE\")\n+        assert 'Content-Encoding' not in newresponse.headers\n+\n     def test_process_response_rawdeflate(self):\n         response = self._getresponse('rawdeflate')\n         request = response.request\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f8f8bbe080163ba39db3bb78efbc36f8e5cd5883", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 20 | Churn Cumulative: 108 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,5 @@\n import zlib\n \n-import brotli\n \n from scrapy.utils.gz import gunzip, is_gzipped\n from scrapy.http import Response, TextResponse\n@@ -8,10 +7,18 @@ from scrapy.responsetypes import responsetypes\n from scrapy.exceptions import NotConfigured\n \n \n+ACCEPTED_ENCODINGS = [b'gzip', b'deflate']\n+\n+try:\n+    import brotli\n+    ACCEPTED_ENCODINGS.append(b'br')\n+except ImportError:\n+    pass\n+\n+\n class HttpCompressionMiddleware(object):\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n     sent/received from web sites\"\"\"\n-\n     @classmethod\n     def from_crawler(cls, crawler):\n         if not crawler.settings.getbool('COMPRESSION_ENABLED'):\n@@ -19,7 +26,8 @@ class HttpCompressionMiddleware(object):\n         return cls()\n \n     def process_request(self, request, spider):\n-        request.headers.setdefault('Accept-Encoding', 'gzip,deflate,br')\n+        request.headers.setdefault('Accept-Encoding',\n+                                   \",\".join(ACCEPTED_ENCODINGS))\n \n     def process_response(self, request, response, spider):\n \n@@ -57,7 +65,6 @@ class HttpCompressionMiddleware(object):\n                 # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\n                 # http://www.gzip.org/zlib/zlib_faq.html#faq38\n                 body = zlib.decompress(body, -15)\n-        if encoding == b\"br\":\n+        if encoding == b'br' and b'br' in ACCEPTED_ENCODINGS:\n             body = brotli.decompress(body)\n         return body\n-\n\n@@ -1,6 +1,6 @@\n from io import BytesIO\n from unittest import TestCase\n-from os.path import join, abspath, dirname\n+from os.path import join\n from gzip import GzipFile\n \n from scrapy.spiders import Spider\n@@ -20,6 +20,7 @@ FORMAT = {\n         'br': ('html-br.bin', 'br')\n         }\n \n+\n class HttpCompressionTest(TestCase):\n \n     def setUp(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3daf473686aab89aa03ebd0ebc59a73b4b6e00f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 121 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,7 +27,7 @@ class HttpCompressionMiddleware(object):\n \n     def process_request(self, request, spider):\n         request.headers.setdefault('Accept-Encoding',\n-                                   \",\".join(ACCEPTED_ENCODINGS))\n+                                   b\",\".join(ACCEPTED_ENCODINGS))\n \n     def process_response(self, request, response, spider):\n \n\n@@ -1,11 +1,12 @@\n from io import BytesIO\n-from unittest import TestCase\n+from unittest import TestCase, SkipTest\n from os.path import join\n from gzip import GzipFile\n \n from scrapy.spiders import Spider\n from scrapy.http import Response, Request, HtmlResponse\n-from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware\n+from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware, \\\n+    ACCEPTED_ENCODINGS\n from tests import tests_datadir\n from w3lib.encoding import resolve_encoding\n \n@@ -53,7 +54,7 @@ class HttpCompressionTest(TestCase):\n         assert 'Accept-Encoding' not in request.headers\n         self.mw.process_request(request, self.spider)\n         self.assertEqual(request.headers.get('Accept-Encoding'),\n-                         b'gzip,deflate,br')\n+                         b','.join(ACCEPTED_ENCODINGS))\n \n     def test_process_response_gzip(self):\n         response = self._getresponse('gzip')\n@@ -66,6 +67,10 @@ class HttpCompressionTest(TestCase):\n         assert 'Content-Encoding' not in newresponse.headers\n \n     def test_process_response_br(self):\n+        try:\n+            import brotli\n+        except ImportError:\n+            raise SkipTest(\"no brotli\")\n         response = self._getresponse('br')\n         request = response.request\n         self.assertEqual(response.headers['Content-Encoding'], b'br')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f73eb715ac4374b1eafb1da3c1311c5a7c153b5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 55 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -40,7 +40,7 @@ IGNORED_EXTENSIONS = [\n \n \n _re_type = type(re.compile(\"\", 0))\n-_matches = lambda url, regexs: any((r.search(url) for r in regexs))\n+_matches = lambda url, regexs: any(r.search(url) for r in regexs)\n _is_valid_url = lambda url: url.split('://', 1)[0] in {'http', 'https', 'file'}\n \n \n@@ -93,8 +93,8 @@ class FilteringLinkExtractor(object):\n         if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):\n             return False\n \n-        allowed = [regex.search(url) for regex in self.allow_res] if self.allow_res else [True]\n-        denied = [regex.search(url) for regex in self.deny_res] if self.deny_res else []\n+        allowed = (regex.search(url) for regex in self.allow_res) if self.allow_res else [True]\n+        denied = (regex.search(url) for regex in self.deny_res) if self.deny_res else []\n         return any(allowed) and not any(denied)\n \n     def _process_links(self, links):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#85a124970ad406c2680402256853dd3ff86d191f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 490 | Contributors (this commit): 32 | Commits (past 90d): 2 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -207,7 +207,7 @@ MEMDEBUG_ENABLED = False        # enable memory debugging\n MEMDEBUG_NOTIFY = []            # send memory debugging report by mail at engine shutdown\n \n MEMUSAGE_CHECK_INTERVAL_SECONDS = 60.0\n-MEMUSAGE_ENABLED = False\n+MEMUSAGE_ENABLED = True\n MEMUSAGE_LIMIT_MB = 0\n MEMUSAGE_NOTIFY_MAIL = []\n MEMUSAGE_REPORT = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fb4ef21a1dcdc0c70fa80443e7379150639c42f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 33 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,5 @@\n import zlib\n \n-\n from scrapy.utils.gz import gunzip, is_gzipped\n from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#24e82bfe75ad1a7b5ceb7a2840ecbae82d05268d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 30 | Churn Cumulative: 363 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n import os\n import sys\n+import numbers\n from operator import itemgetter\n \n import six\n@@ -34,6 +35,13 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n             _check_components(compdict)\n             return {convert(k): v for k, v in six.iteritems(compdict)}\n \n+    def _validate_values(compdict):\n+        \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n+        for name, value in six.iteritems(compdict):\n+            if value is not None and not isinstance(value, numbers.Real):\n+                raise ValueError('Invalid value {} for component {}, please provide ' \\\n+                                 'a real number or None instead'.format(value, name))\n+\n     # BEGIN Backwards compatibility for old (base, custom) call signature\n     if isinstance(custom, (list, tuple)):\n         _check_components(custom)\n@@ -43,6 +51,7 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n         compdict.update(custom)\n     # END Backwards compatibility\n \n+    _validate_values(compdict)\n     compdict = without_none_values(_map_keys(compdict))\n     return [k for k, v in sorted(six.iteritems(compdict), key=itemgetter(1))]\n \n\n@@ -62,6 +62,27 @@ class BuildComponentListTest(unittest.TestCase):\n         self.assertRaises(ValueError, build_component_list, duplicate_bs,\n                           convert=lambda x: x.lower())\n \n+    def test_valid_numbers(self):\n+        # work well with None and numeric values\n+        d = {'a': 10, 'b': None, 'c': 15, 'd': 5.0}\n+        self.assertEqual(build_component_list(d, convert=lambda x: x),\n+                         ['d', 'a', 'c'])\n+        d = {'a': 33333333333333333333, 'b': 11111111111111111111, 'c': 22222222222222222222}\n+        self.assertEqual(build_component_list(d, convert=lambda x: x),\n+                         ['b', 'c', 'a'])\n+        # raise exception for invalid values\n+        d = {'one': '5'}\n+        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n+        d = {'one': '1.0'}\n+        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n+        d = {'one': [1, 2, 3]}\n+        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n+        d = {'one': {'a': 'a', 'b': 2}}\n+        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n+        d = {'one': 'lorem ipsum',}\n+        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n+\n+\n \n class UtilsConfTestCase(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eaf62ab69c23459fa36dafe298bcfabc5952b7f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 59 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,8 +53,10 @@ class BaseRedirectMiddleware(object):\n \n \n class RedirectMiddleware(BaseRedirectMiddleware):\n-    \"\"\"Handle redirection of requests based on response status and meta-refresh html tag\"\"\"\n-\n+    \"\"\"\n+    Handle redirection of requests based on response status\n+    and meta-refresh html tag.\n+    \"\"\"\n     def process_response(self, request, response, spider):\n         if (request.meta.get('dont_redirect', False) or\n                 response.status in getattr(spider, 'handle_httpstatus_list', []) or\n@@ -92,7 +94,6 @@ class MetaRefreshMiddleware(BaseRedirectMiddleware):\n                 not isinstance(response, HtmlResponse):\n             return response\n \n-        if isinstance(response, HtmlResponse):\n         interval, url = get_meta_refresh(response)\n         if url and interval < self._maxdelay:\n             redirected = self._redirect_request_using_get(request, url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#04b2f79e7a1316d29b2cb94c8fb2623e040b64ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 52 | Files Changed: 4 | Hunks: 17 | Methods Changed: 6 | Complexity Δ (Sum/Max): -3/3 | Churn Δ: 63 | Churn Cumulative: 1042 | Contributors (this commit): 13 | Commits (past 90d): 5 | Contributors (cumulative): 26 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,10 +1,6 @@\n-from scrapy import twisted_version\n+from __future__ import absolute_import\n from .http10 import HTTP10DownloadHandler\n-\n-if twisted_version >= (11, 1, 0):\n from .http11 import HTTP11DownloadHandler as HTTPDownloadHandler\n-else:\n-    HTTPDownloadHandler = HTTP10DownloadHandler\n \n \n # backwards compatibility\n\n@@ -2,33 +2,17 @@ from __future__ import print_function\n import sys, time, random, os, json\n from six.moves.urllib.parse import urlencode\n from subprocess import Popen, PIPE\n+\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n-from twisted.internet import reactor, defer, ssl\n-from scrapy import twisted_version\n-from scrapy.utils.python import to_bytes, to_unicode\n-\n-\n-if twisted_version < (11, 0, 0):\n-    def deferLater(clock, delay, func, *args, **kw):\n-        def _cancel_method():\n-            _cancel_cb(None)\n-            d.errback(Exception())\n-\n-        def _cancel_cb(result):\n-            if cl.active():\n-                cl.cancel()\n-            return result\n-\n-        d = defer.Deferred()\n-        d.cancel = _cancel_method\n-        d.addCallback(lambda ignored: func(*args, **kw))\n-        d.addBoth(_cancel_cb)\n-        cl = clock.callLater(delay, d.callback, None)\n-        return d\n-else:\n+from twisted.web.test.test_webclient import PayloadResource\n+from twisted.web.server import GzipEncoderFactory\n+from twisted.web.resource import EncodingResourceWrapper\n+from twisted.internet import reactor, ssl\n from twisted.internet.task import deferLater\n \n+from scrapy.utils.python import to_bytes, to_unicode\n+\n \n def getarg(request, name, default=None, type=None):\n     if name in request.args:\n@@ -174,11 +158,6 @@ class Root(Resource):\n         self.putChild(b\"drop\", Drop())\n         self.putChild(b\"raw\", Raw())\n         self.putChild(b\"echo\", Echo())\n-\n-        if twisted_version > (12, 3, 0):\n-            from twisted.web.test.test_webclient import PayloadResource\n-            from twisted.web.server import GzipEncoderFactory\n-            from twisted.web.resource import EncodingResourceWrapper\n         self.putChild(b\"payload\", PayloadResource())\n         self.putChild(b\"xpayload\", EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]))\n \n\n@@ -17,7 +17,6 @@ from twisted.web.test.test_webclient import ForeverTakingResource, \\\n from twisted.cred import portal, checkers, credentials\n from w3lib.url import path_to_file_uri\n \n-from scrapy import twisted_version\n from scrapy.core.downloader.handlers import DownloadHandlers\n from scrapy.core.downloader.handlers.file import FileDownloadHandler\n from scrapy.core.downloader.handlers.http import HTTPDownloadHandler, HttpDownloadHandler\n@@ -281,8 +280,6 @@ class Https10TestCase(Http10TestCase):\n class Http11TestCase(HttpTestCase):\n     \"\"\"HTTP 1.1 test case\"\"\"\n     download_handler_cls = HTTP11DownloadHandler\n-    if twisted_version < (11, 1, 0):\n-        skip = 'HTTP1.1 not supported in twisted < 11.1.0'\n \n     def test_download_without_maxsize_limit(self):\n         request = Request(self.getURL('file'))\n@@ -366,8 +363,6 @@ class Https11InvalidDNSId(Https11TestCase):\n \n class Http11MockServerTestCase(unittest.TestCase):\n     \"\"\"HTTP 1.1 test case with MockServer\"\"\"\n-    if twisted_version < (11, 1, 0):\n-        skip = 'HTTP1.1 not supported in twisted < 11.1.0'\n \n     def setUp(self):\n         self.mockserver = MockServer()\n@@ -396,12 +391,10 @@ class Http11MockServerTestCase(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_download_gzip_response(self):\n-\n-        if twisted_version > (12, 3, 0):\n-\n         crawler = get_crawler(SingleRequestSpider)\n         body = b'1' * 100  # PayloadResource requires body length to be 100\n-            request = Request('http://localhost:8998/payload', method='POST', body=body, meta={'download_maxsize': 50})\n+        request = Request('http://localhost:8998/payload', method='POST',\n+                          body=body, meta={'download_maxsize': 50})\n         yield crawler.crawl(seed=request)\n         failure = crawler.spider.meta['failure']\n         # download_maxsize < 100, hence the CancelledError\n@@ -419,8 +412,6 @@ class Http11MockServerTestCase(unittest.TestCase):\n         else:\n             # See issue https://twistedmatrix.com/trac/ticket/8175\n             raise unittest.SkipTest(\"xpayload only enabled for PY2\")\n-        else:\n-            raise unittest.SkipTest(\"xpayload and payload endpoint only enabled for twisted > 12.3.0\")\n \n \n class UriResource(resource.Resource):\n@@ -500,8 +491,6 @@ class Http10ProxyTestCase(HttpProxyTestCase):\n \n class Http11ProxyTestCase(HttpProxyTestCase):\n     download_handler_cls = HTTP11DownloadHandler\n-    if twisted_version < (11, 1, 0):\n-        skip = 'HTTP1.1 not supported in twisted < 11.1.0'\n \n     @defer.inlineCallbacks\n     def test_download_with_proxy_https_timeout(self):\n@@ -692,8 +681,6 @@ class FTPTestCase(unittest.TestCase):\n     username = \"scrapy\"\n     password = \"passwd\"\n \n-    if twisted_version < (10, 2, 0):\n-        skip = \"Twisted pre 10.2.0 doesn't allow to set home path other than /home\"\n     if six.PY3:\n         skip = \"Twisted missing ftp support for PY3\"\n \n\n@@ -5,7 +5,6 @@ from twisted.internet.error import TimeoutError, DNSLookupError, \\\n         ConnectionLost, TCPTimedOutError\n from twisted.web.client import ResponseFailed\n \n-from scrapy import twisted_version\n from scrapy.downloadermiddlewares.retry import RetryMiddleware\n from scrapy.spiders import Spider\n from scrapy.http import Request, Response\n@@ -74,9 +73,7 @@ class RetryTest(unittest.TestCase):\n     def test_twistederrors(self):\n         exceptions = [defer.TimeoutError, TCPTimedOutError, TimeoutError,\n                 DNSLookupError, ConnectionRefusedError, ConnectionDone,\n-                ConnectError, ConnectionLost]\n-        if twisted_version >= (11, 1, 0): # http11 available\n-            exceptions.append(ResponseFailed)\n+                ConnectError, ConnectionLost, ResponseFailed]\n \n         for exc in exceptions:\n             req = Request('http://www.scrapytest.org/%s' % exc.__name__)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4e765acaed7a914630ee5320fa6f6523890a2b9d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 55 | Churn Cumulative: 384 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 0.875\n\nDIFF:\n@@ -25,6 +25,7 @@ class Field(dict):\n class ItemMeta(ABCMeta):\n \n     def __new__(mcs, class_name, bases, attrs):\n+        classcell = attrs.pop('__classcell__', None)\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n \n@@ -39,6 +40,8 @@ class ItemMeta(ABCMeta):\n \n         new_attrs['fields'] = fields\n         new_attrs['_class'] = _class\n+        if classcell is not None:\n+            new_attrs['__classcell__'] = classcell\n         return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)\n \n \n\n@@ -1,8 +1,14 @@\n+import sys\n import unittest\n \n-from scrapy.item import Item, Field\n import six\n \n+from scrapy.item import ABCMeta, Item, ItemMeta, Field\n+from tests import mock\n+\n+\n+PY36_PLUS = (sys.version_info.major >= 3) and (sys.version_info.minor >= 6)\n+\n \n class ItemTest(unittest.TestCase):\n \n@@ -244,5 +250,49 @@ class ItemTest(unittest.TestCase):\n         self.assertNotEqual(item['name'], copied_item['name'])\n \n \n+class ItemMetaTest(unittest.TestCase):\n+\n+    def test_new_method_propagates_classcell(self):\n+        new_mock = mock.Mock(side_effect=ABCMeta.__new__)\n+        base = ItemMeta.__bases__[0]\n+\n+        with mock.patch.object(base, '__new__', new_mock):\n+\n+            class MyItem(Item):\n+                if not PY36_PLUS:\n+                    # This attribute is an internal attribute in Python 3.6+\n+                    # and must be propagated properly. See\n+                    # https://docs.python.org/3.6/reference/datamodel.html#creating-the-class-object\n+                    # In <3.6, we add a dummy attribute just to ensure the\n+                    # __new__ method propagates it correctly.\n+                    __classcell__ = object()\n+\n+                def f(self):\n+                    # For rationale of this see:\n+                    # https://github.com/python/cpython/blob/ee1a81b77444c6715cbe610e951c655b6adab88b/Lib/test/test_super.py#L222\n+                    return __class__\n+\n+            MyItem()\n+\n+        (first_call, second_call) = new_mock.call_args_list[-2:]\n+\n+        mcs, class_name, bases, attrs = first_call[0]\n+        assert '__classcell__' not in attrs\n+        mcs, class_name, bases, attrs = second_call[0]\n+        assert '__classcell__' in attrs\n+\n+\n+class ItemMetaClassCellRegression(unittest.TestCase):\n+\n+    def test_item_meta_classcell_regression(self):\n+        class MyItem(six.with_metaclass(ItemMeta, Item)):\n+            def __init__(self, *args, **kwargs):\n+                # This call to super() trigger the __classcell__ propagation\n+                # requirement. When not done properly raises an error:\n+                # TypeError: __class__ set to <class '__main__.MyItem'>\n+                # defining 'MyItem' as <class '__main__.MyItem'>\n+                super(MyItem, self).__init__(*args, **kwargs)\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c0aae724ed821fd954a14db83902a86f7fe7731", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 14 | Methods Changed: 11 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 67 | Churn Cumulative: 125 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 0.45161290322580644\n\nDIFF:\n@@ -8,7 +8,6 @@ except ImportError:\n from six.moves.urllib.parse import urlunparse\n \n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.exceptions import NotConfigured\n from scrapy.utils.python import to_bytes\n \n \n@@ -20,23 +19,23 @@ class HttpProxyMiddleware(object):\n         for type, url in getproxies().items():\n             self.proxies[type] = self._get_proxy(url, type)\n \n-        if not self.proxies:\n-            raise NotConfigured\n-\n     @classmethod\n     def from_crawler(cls, crawler):\n         auth_encoding = crawler.settings.get('HTTPPROXY_AUTH_ENCODING')\n         return cls(auth_encoding)\n \n+    def _basic_auth_header(self, username, password):\n+        user_pass = to_bytes(\n+            '%s:%s' % (unquote(username), unquote(password)),\n+            encoding=self.auth_encoding)\n+        return base64.b64encode(user_pass).strip()\n+\n     def _get_proxy(self, url, orig_type):\n         proxy_type, user, password, hostport = _parse_proxy(url)\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n \n         if user:\n-            user_pass = to_bytes(\n-                '%s:%s' % (unquote(user), unquote(password)),\n-                encoding=self.auth_encoding)\n-            creds = base64.b64encode(user_pass).strip()\n+            creds = self._basic_auth_header(user, password)\n         else:\n             creds = None\n \n@@ -45,6 +44,15 @@ class HttpProxyMiddleware(object):\n     def process_request(self, request, spider):\n         # ignore if proxy is already set\n         if 'proxy' in request.meta:\n+            if request.meta['proxy'] is None:\n+                return\n+            # extract credentials if present\n+            creds, proxy_url = self._get_proxy(request.meta['proxy'], '')\n+            request.meta['proxy'] = proxy_url\n+            if creds and not request.headers.get('Proxy-Authorization'):\n+                request.headers['Proxy-Authorization'] = b'Basic ' + creds\n+            return\n+        elif not self.proxies:\n             return\n \n         parsed = urlparse_cached(request)\n\n@@ -20,10 +20,6 @@ class TestDefaultHeadersMiddleware(TestCase):\n     def tearDown(self):\n         os.environ = self._oldenv\n \n-    def test_no_proxies(self):\n-        os.environ = {}\n-        self.assertRaises(NotConfigured, HttpProxyMiddleware)\n-\n     def test_no_enviroment_proxies(self):\n         os.environ = {'dummy_proxy': 'reset_env_and_do_not_raise'}\n         mw = HttpProxyMiddleware()\n@@ -47,6 +43,13 @@ class TestDefaultHeadersMiddleware(TestCase):\n             self.assertEquals(req.url, url)\n             self.assertEquals(req.meta.get('proxy'), proxy)\n \n+    def test_proxy_precedence_meta(self):\n+        os.environ['http_proxy'] = 'https://proxy.com'\n+        mw = HttpProxyMiddleware()\n+        req = Request('http://scrapytest.org', meta={'proxy': 'https://new.proxy:3128'})\n+        assert mw.process_request(req, spider) is None\n+        self.assertEquals(req.meta, {'proxy': 'https://new.proxy:3128'})\n+\n     def test_proxy_auth(self):\n         os.environ['http_proxy'] = 'https://user:pass@proxy:3128'\n         mw = HttpProxyMiddleware()\n@@ -54,6 +57,11 @@ class TestDefaultHeadersMiddleware(TestCase):\n         assert mw.process_request(req, spider) is None\n         self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n         self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjpwYXNz')\n+        # proxy from request.meta\n+        req = Request('http://scrapytest.org', meta={'proxy': 'https://username:password@proxy:3128'})\n+        assert mw.process_request(req, spider) is None\n+        self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcm5hbWU6cGFzc3dvcmQ=')\n \n     def test_proxy_auth_empty_passwd(self):\n         os.environ['http_proxy'] = 'https://user:@proxy:3128'\n@@ -62,6 +70,11 @@ class TestDefaultHeadersMiddleware(TestCase):\n         assert mw.process_request(req, spider) is None\n         self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n         self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjo=')\n+        # proxy from request.meta\n+        req = Request('http://scrapytest.org', meta={'proxy': 'https://username:@proxy:3128'})\n+        assert mw.process_request(req, spider) is None\n+        self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcm5hbWU6')\n \n     def test_proxy_auth_encoding(self):\n         # utf-8 encoding\n@@ -72,6 +85,12 @@ class TestDefaultHeadersMiddleware(TestCase):\n         self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n         self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic bcOhbjpwYXNz')\n \n+        # proxy from request.meta\n+        req = Request('http://scrapytest.org', meta={'proxy': u'https://\\u00FCser:pass@proxy:3128'})\n+        assert mw.process_request(req, spider) is None\n+        self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic w7xzZXI6cGFzcw==')\n+\n         # default latin-1 encoding\n         mw = HttpProxyMiddleware(auth_encoding='latin-1')\n         req = Request('http://scrapytest.org')\n@@ -79,15 +98,21 @@ class TestDefaultHeadersMiddleware(TestCase):\n         self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n         self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic beFuOnBhc3M=')\n \n+        # proxy from request.meta, latin-1 encoding\n+        req = Request('http://scrapytest.org', meta={'proxy': u'https://\\u00FCser:pass@proxy:3128'})\n+        assert mw.process_request(req, spider) is None\n+        self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic /HNlcjpwYXNz')\n+\n     def test_proxy_already_seted(self):\n-        os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'\n+        os.environ['http_proxy'] = 'https://proxy.for.http:3128'\n         mw = HttpProxyMiddleware()\n         req = Request('http://noproxy.com', meta={'proxy': None})\n         assert mw.process_request(req, spider) is None\n         assert 'proxy' in req.meta and req.meta['proxy'] is None\n \n     def test_no_proxy(self):\n-        os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'\n+        os.environ['http_proxy'] = 'https://proxy.for.http:3128'\n         mw = HttpProxyMiddleware()\n \n         os.environ['no_proxy'] = '*'\n@@ -104,3 +129,9 @@ class TestDefaultHeadersMiddleware(TestCase):\n         req = Request('http://noproxy.com')\n         assert mw.process_request(req, spider) is None\n         assert 'proxy' not in req.meta\n+\n+        # proxy from meta['proxy'] takes precedence\n+        os.environ['no_proxy'] = '*'\n+        req = Request('http://noproxy.com', meta={'proxy': 'http://proxy.com'})\n+        assert mw.process_request(req, spider) is None\n+        self.assertEquals(req.meta, {'proxy': 'http://proxy.com'})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
