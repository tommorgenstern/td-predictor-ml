{"custom_id": "keras#f46f04d54564cb2fd7d0559b8bcb5757afc1371e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 271 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -112,7 +112,7 @@ def test_MaxPooling1D():\n                     check_layer_output_shape(layer, input_data)\n \n \n-def test_AveragePooling1D(self):\n+def test_AveragePooling1D():\n     for ignore_border in [True, False]:\n         for pool_length in [1, 2]:\n             for stride in [1]:\n@@ -145,7 +145,7 @@ def test_MaxPooling2D():\n                     check_layer_output_shape(layer, input_data)\n \n \n-def test_AveragePooling2D(self):\n+def test_AveragePooling2D():\n     for ignore_border in [True, False]:\n         for strides in [(1, 1), (2, 2)]:\n             for pool_size in [(2, 2), (3, 3), (4, 4)]:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#de2884af79ecb8242226466b1f5c82409c8e72cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 5660 | Contributors (this commit): 32 | Commits (past 90d): 90 | Contributors (cumulative): 32 | DMM Complexity: 0.5\n\nDIFF:\n@@ -393,7 +393,7 @@ class Merge(Layer):\n                 shape = tensordot_output.shape\n             return (shape1[0],) + shape\n         elif self.mode == 'cos':\n-            return tuple(input_shapes[0][0], 1)\n+            return (input_shapes[0][0], 1)\n \n     def get_params(self):\n         return self.params, self.regularizers, self.constraints, self.updates\n@@ -440,9 +440,8 @@ class Merge(Layer):\n             import theano\n             l1 = self.layers[0].get_output(train)\n             l2 = self.layers[1].get_output(train)\n-            output, _ = theano.scan(lambda v1, v2: K.dot(v1, v2) / K.sqrt(K.dot(v1, v1) * K.dot(v2, v2)),\n-                                    sequences=[l1, l2],\n-                                    outputs_info=None)\n+            output = T.batched_tensordot(l1, l2, self.dot_axes) / T.sqrt(T.batched_tensordot(l1, l1, self.dot_axes) * T.batched_tensordot(l2, l2, self.dot_axes))\n+            output = output.dimshuffle((0, 'x'))\n             return output\n         else:\n             raise Exception('Unknown merge mode')\n@@ -1249,7 +1248,7 @@ class Siamese(Layer):\n             return tuple(shape)\n \n         elif self.merge_mode == 'cos':\n-            return tuple(input_shapes[0][0], 1)\n+            return (input_shapes[0][0], 1)\n \n     def get_params(self):\n         return self.params, self.regularizers, self.constraints, self.updates\n@@ -1318,8 +1317,7 @@ class Siamese(Layer):\n         from theano import tensor as T\n         l1 = self.get_output_at(0, train)\n         l2 = self.get_output_at(1, train)\n-        cos = lambda v1, v2: T.dot(v1, v2) / T.sqrt(T.dot(v1, v1) * T.dot(v2, v2))\n-        output, _ = theano.scan(cos, sequences=[l1, l2], outputs_info=None)\n+        output = T.batched_tensordot(l1, l2, self.dot_axes) / T.sqrt(T.batched_tensordot(l1, l1, self.dot_axes) * T.batched_tensordot(l2, l2, self.dot_axes))\n         output = output.dimshuffle((0, 'x'))\n         return output\n \n@@ -1338,7 +1336,7 @@ class Siamese(Layer):\n         elif mode == 'dot':\n             return self.get_output_dot(train)\n         elif mode == 'cos':\n-            return self.get_output_dot(train)\n+            return self.get_output_cos(train)\n \n     def get_input(self, train=False):\n         res = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fe6c554e7a9ac970a8f7881019d8bd535bead304", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#57d5a7ca78a057dffee1c162359453b387b7ced1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d5b4d04b5bdf429cce7f81104b7dd7674c42a836", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 161 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,11 +9,12 @@ def get_standard_values():\n     These are just a set of floats used for testing the activation\n     functions, and are useful in multiple tests.\n \n-    The values should all be non-negative becasue they and their negative\n+    The values should all be non-negative because they and their negatives\n     are used to test the ReLU derivates in this file.\n     '''\n     return np.array([[0, 0.1, 0.5, 0.9, 1.0, 10, 1e2, 0.01]], dtype=K.floatx())\n \n+\n class TestAdvancedActivations(unittest.TestCase):\n     def test_leaky_relu(self):\n         np.random.seed(1337)\n@@ -39,7 +40,6 @@ class TestAdvancedActivations(unittest.TestCase):\n         np.random.seed(1337)\n         inp = get_standard_values()\n \n-\n         for train in [True, False]:\n             # test with custom weights\n             alphas = np.random.random(inp.shape)\n@@ -96,12 +96,15 @@ class TestAdvancedActivations(unittest.TestCase):\n         inp = inp[:-2]\n         for alpha in [.5, -1., 1., 5]:\n             for beta in [.5, -1., 1., 2]:\n-                layer = ParametricSoftplus(alpha_init=alpha, beta_init=beta, input_shape=inp.shape)\n+                layer = ParametricSoftplus(alpha_init=alpha,\n+                                           beta_init=beta,\n+                                           input_shape=inp.shape)\n                 layer.input = K.variable(inp)\n                 layer.build()\n                 for train in [True, False]:\n                     outp = K.eval(layer.get_output(train))\n-                    assert_allclose(outp,alpha*np.log(1.+np.exp(beta*inp)),atol=1e-3)\n+                    assert_allclose(outp, alpha*np.log(1.+np.exp(beta*inp)),\n+                                    atol=1e-3)\n \n                 config = layer.get_config()\n                 assert config['alpha_init'] == alpha\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b864fa974b00e63a200568667ce151eba8204ebd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 352 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -9,11 +9,6 @@ from keras.layers import containers\n from keras.layers.core import Dense, Activation\n from keras.utils.test_utils import get_test_data\n \n-X = np.random.random((100, 32))\n-X2 = np.random.random((100, 32))\n-y = np.random.random((100, 4))\n-y2 = np.random.random((100,))\n-\n (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n                                                      nb_test=200,\n                                                      input_shape=(32,),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8b0676cb9491f4971d105f674e4e9dfaf719e224", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 665 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 19 | Complexity Δ (Sum/Max): 21/21 | Churn Δ: 665 | Churn Cumulative: 665 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,665 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+import pytest\n+import numpy as np\n+np.random.seed(1337)\n+\n+from keras import backend as K\n+from keras.models import Graph, Sequential, model_from_json, model_from_yaml\n+from keras.layers.core import Dense, Activation, Merge, Lambda, LambdaMerge\n+from keras.layers import containers\n+from keras.utils import np_utils\n+from keras.utils.test_utils import get_test_data\n+\n+import os\n+\n+input_dim = 32\n+nb_hidden = 16\n+nb_class = 4\n+batch_size = 32\n+nb_epoch = 1\n+\n+train_samples = 2000\n+test_samples = 500\n+\n+(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n+                                                     nb_test=test_samples,\n+                                                     input_shape=(input_dim,),\n+                                                     classification=True,\n+                                                     nb_class=4)\n+y_test = np_utils.to_categorical(y_test)\n+y_train = np_utils.to_categorical(y_train)\n+\n+\n+####################\n+# SEQUENTIAL TEST  #\n+####################\n+\n+\n+def test_sequential():\n+    model = Sequential()\n+    model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    model.add(Activation('relu'))\n+    model.add(Dense(nb_class))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1, validation_data=(X_test, y_test))\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2, validation_data=(X_test, y_test))\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_split=0.1)\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=1, validation_split=0.1)\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, shuffle=False)\n+\n+    model.train_on_batch(X_train[:32], y_train[:32])\n+\n+    loss = model.evaluate(X_train, y_train, verbose=0)\n+    assert(loss < 0.7)\n+\n+    model.predict(X_test, verbose=0)\n+    model.predict_classes(X_test, verbose=0)\n+    model.predict_proba(X_test, verbose=0)\n+    model.get_config(verbose=0)\n+\n+    fname = 'test_sequential_temp.h5'\n+    model.save_weights(fname, overwrite=True)\n+    model = Sequential()\n+    model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    model.add(Activation('relu'))\n+    model.add(Dense(nb_class))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+    model.load_weights(fname)\n+    os.remove(fname)\n+\n+    nloss = model.evaluate(X_train, y_train, verbose=0)\n+    assert(loss == nloss)\n+\n+    # test json serialization\n+    json_data = model.to_json()\n+    model = model_from_json(json_data)\n+\n+    # test yaml serialization\n+    yaml_data = model.to_yaml()\n+    model = model_from_yaml(yaml_data)\n+\n+\n+def test_merge_sum():\n+    left = Sequential()\n+    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='sum'))\n+    model.add(Dense(nb_class))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+\n+    loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+    assert(loss < 0.7)\n+\n+    model.predict([X_test, X_test], verbose=0)\n+    model.predict_classes([X_test, X_test], verbose=0)\n+    model.predict_proba([X_test, X_test], verbose=0)\n+    model.get_config(verbose=0)\n+\n+    # test weight saving\n+    fname = 'test_merge_sum_temp.h5'\n+    model.save_weights(fname, overwrite=True)\n+    left = Sequential()\n+    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+    right = Sequential()\n+    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='sum'))\n+    model.add(Dense(nb_class))\n+    model.add(Activation('softmax'))\n+    model.load_weights(fname)\n+    os.remove(fname)\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+    assert(loss == nloss)\n+\n+\n+def test_merge_dot():\n+    if K._BACKEND == 'tensorflow':\n+        return\n+\n+    left = Sequential()\n+    left.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n+    right.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='dot', dot_axes=1))\n+    model.add(Dense(nb_class))\n+    model.add(Activation('softmax'))\n+\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    left = Sequential()\n+    left.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n+    right.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='dot', dot_axes=([1], [1])))\n+    model.add(Dense(nb_class))\n+    model.add(Activation('softmax'))\n+\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+\n+def test_merge_concat():\n+    left = Sequential()\n+    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='concat'))\n+    model.add(Dense(nb_class))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+\n+    loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+    assert(loss < 0.7)\n+\n+    model.predict([X_test, X_test], verbose=0)\n+    model.predict_classes([X_test, X_test], verbose=0)\n+    model.predict_proba([X_test, X_test], verbose=0)\n+    model.get_config(verbose=0)\n+\n+    fname = 'test_merge_concat_temp.h5'\n+    model.save_weights(fname, overwrite=True)\n+    left = Sequential()\n+    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='concat'))\n+\n+    model.add(Dense(nb_class))\n+    model.add(Activation('softmax'))\n+\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+    model.load_weights(fname)\n+    os.remove(fname)\n+\n+    nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+    assert(loss == nloss)\n+\n+\n+def test_merge_recursivity():\n+    left = Sequential()\n+    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+\n+    righter = Sequential()\n+    righter.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    righter.add(Activation('relu'))\n+\n+    intermediate = Sequential()\n+    intermediate.add(Merge([left, right], mode='sum'))\n+    intermediate.add(Dense(nb_hidden))\n+    intermediate.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([intermediate, righter], mode='sum'))\n+    model.add(Dense(nb_class))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test, X_test], y_test))\n+    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test, X_test], y_test))\n+    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n+    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n+    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+\n+    loss = model.evaluate([X_train, X_train, X_train], y_train, verbose=0)\n+    assert(loss < 0.7)\n+\n+    model.predict([X_test, X_test, X_test], verbose=0)\n+    model.predict_classes([X_test, X_test, X_test], verbose=0)\n+    model.predict_proba([X_test, X_test, X_test], verbose=0)\n+    model.get_config(verbose=0)\n+\n+    fname = 'test_merge_recursivity_temp.h5'\n+    model.save_weights(fname, overwrite=True)\n+    model.load_weights(fname)\n+    os.remove(fname)\n+\n+    nloss = model.evaluate([X_train, X_train, X_train], y_train, verbose=0)\n+    assert(loss == nloss)\n+\n+\n+def test_merge_overlap():\n+    left = Sequential()\n+    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([left, left], mode='sum'))\n+    model.add(Dense(nb_class))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1, validation_data=(X_test, y_test))\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2, validation_data=(X_test, y_test))\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_split=0.1)\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=1, validation_split=0.1)\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, shuffle=False)\n+\n+    model.train_on_batch(X_train[:32], y_train[:32])\n+\n+    loss = model.evaluate(X_train, y_train, verbose=0)\n+    assert(loss < 0.7)\n+    model.predict(X_test, verbose=0)\n+    model.predict_classes(X_test, verbose=0)\n+    model.predict_proba(X_test, verbose=0)\n+    model.get_config(verbose=0)\n+\n+    fname = 'test_merge_overlap_temp.h5'\n+    model.save_weights(fname, overwrite=True)\n+    model.load_weights(fname)\n+    os.remove(fname)\n+\n+    nloss = model.evaluate(X_train, y_train, verbose=0)\n+    assert(loss == nloss)\n+\n+\n+def test_lambda():\n+    def func(X):\n+        s = X[0]\n+        for i in range(1, len(X)):\n+            s += X[i]\n+        return s\n+\n+    def activation(X):\n+        return K.softmax(X)\n+\n+    def output_shape(input_shapes):\n+        return input_shapes[0]\n+\n+    left = Sequential()\n+    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(LambdaMerge([left, right], function=func,\n+                          output_shape=output_shape))\n+    model.add(Dense(nb_class))\n+    model.add(Lambda(activation))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+\n+    loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+    assert(loss < 0.7)\n+\n+    model.predict([X_test, X_test], verbose=0)\n+    model.predict_classes([X_test, X_test], verbose=0)\n+    model.predict_proba([X_test, X_test], verbose=0)\n+    model.get_config(verbose=0)\n+\n+    # test weight saving\n+    fname = 'test_lambda_temp.h5'\n+    model.save_weights(fname, overwrite=True)\n+    left = Sequential()\n+    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+    right = Sequential()\n+    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+    model = Sequential()\n+    model.add(LambdaMerge([left, right], function=func,\n+                          output_shape=output_shape))\n+    model.add(Dense(nb_class))\n+    model.add(Lambda(activation))\n+    model.load_weights(fname)\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+    os.remove(fname)\n+\n+    nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+    assert(loss == nloss)\n+\n+\n+def test_sequential_count_params():\n+    input_dim = 20\n+    nb_units = 10\n+    nb_classes = 2\n+\n+    n = input_dim * nb_units + nb_units\n+    n += nb_units * nb_units + nb_units\n+    n += nb_units * nb_classes + nb_classes\n+\n+    model = Sequential()\n+    model.add(Dense(nb_units, input_shape=(input_dim,)))\n+    model.add(Dense(nb_units))\n+    model.add(Dense(nb_classes))\n+    model.add(Activation('softmax'))\n+\n+    assert(n == model.count_params())\n+\n+    model.compile('sgd', 'binary_crossentropy')\n+\n+    assert(n == model.count_params())\n+\n+\n+###############\n+# GRAPH TEST  #\n+###############\n+\n+(X_train_graph, y_train_graph), (X_test_graph, y_test_graph) = get_test_data(nb_train=1000,\n+                                                     nb_test=200,\n+                                                     input_shape=(32,),\n+                                                     classification=False,\n+                                                     output_shape=(4,))\n+(X2_train_graph, y2_train_graph), (X2_test_graph, y2_test_graph) = get_test_data(nb_train=1000,\n+                                                         nb_test=200,\n+                                                         input_shape=(32,),\n+                                                         classification=False,\n+                                                         output_shape=(1,))\n+\n+\n+def test_1o_1i():\n+    # test a non-sequential graph with 1 input and 1 output\n+    np.random.seed(1337)\n+\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2', input='input1')\n+    graph.add_node(Dense(4), name='dense3', input='dense1')\n+\n+    graph.add_output(name='output1',\n+                     inputs=['dense2', 'dense3'],\n+                     merge_mode='sum')\n+    graph.compile('rmsprop', {'output1': 'mse'})\n+\n+    history = graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n+                        nb_epoch=10)\n+    out = graph.predict({'input1': X_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 1)\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph})\n+    assert(loss < 2.5)\n+\n+    # test validation split\n+    history = graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n+                        validation_split=0.2, nb_epoch=1)\n+    # test validation data\n+    history = graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n+                        validation_data={'input1': X_train_graph, 'output1': y_train_graph},\n+                        nb_epoch=1)\n+\n+\n+def test_1o_1i_2():\n+    # test a more complex non-sequential graph with 1 input and 1 output\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2-0', input='input1')\n+    graph.add_node(Activation('relu'), name='dense2', input='dense2-0')\n+\n+    graph.add_node(Dense(16), name='dense3', input='dense2')\n+    graph.add_node(Dense(4), name='dense4', inputs=['dense1', 'dense3'],\n+                   merge_mode='sum')\n+\n+    graph.add_output(name='output1', inputs=['dense2', 'dense4'],\n+                     merge_mode='sum')\n+    graph.compile('rmsprop', {'output1': 'mse'})\n+\n+    history = graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n+                        nb_epoch=10)\n+    out = graph.predict({'input1': X_train_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 1)\n+\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph})\n+    assert(loss < 2.5)\n+\n+    graph.get_config(verbose=1)\n+\n+\n+def test_1o_2i():\n+    # test a non-sequential graph with 2 inputs and 1 output\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+    graph.add_input(name='input2', input_shape=(32,))\n+\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2', input='input2')\n+    graph.add_node(Dense(4), name='dense3', input='dense1')\n+\n+    graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n+                     merge_mode='sum')\n+    graph.compile('rmsprop', {'output1': 'mse'})\n+\n+    history = graph.fit({'input1': X_train_graph, 'input2': X2_train_graph, 'output1': y_train_graph},\n+                        nb_epoch=10)\n+    out = graph.predict({'input1': X_test_graph, 'input2': X2_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 1)\n+\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    assert(loss < 3.0)\n+\n+    graph.get_config(verbose=1)\n+\n+\n+def test_2o_1i_weights():\n+    # test a non-sequential graph with 1 input and 2 outputs\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2', input='input1')\n+    graph.add_node(Dense(1), name='dense3', input='dense1')\n+\n+    graph.add_output(name='output1', input='dense2')\n+    graph.add_output(name='output2', input='dense3')\n+    graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n+\n+    history = graph.fit({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n+                        nb_epoch=10)\n+    out = graph.predict({'input1': X_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 2)\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph})\n+    assert(loss < 4.)\n+\n+    # test weight saving\n+    fname = 'test_2o_1i_weights_temp.h5'\n+    graph.save_weights(fname, overwrite=True)\n+\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2', input='input1')\n+    graph.add_node(Dense(1), name='dense3', input='dense1')\n+    graph.add_output(name='output1', input='dense2')\n+    graph.add_output(name='output2', input='dense3')\n+    graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n+    graph.load_weights('test_2o_1i_weights_temp.h5')\n+    os.remove(fname)\n+\n+    nloss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph})\n+    assert(loss == nloss)\n+\n+\n+def test_2o_1i_sample_weights():\n+    # test a non-sequential graph with 1 input and 2 outputs with sample weights\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2', input='input1')\n+    graph.add_node(Dense(1), name='dense3', input='dense1')\n+\n+    graph.add_output(name='output1', input='dense2')\n+    graph.add_output(name='output2', input='dense3')\n+\n+    weights1 = np.random.uniform(size=y_train_graph.shape[0])\n+    weights2 = np.random.uniform(size=y2_train_graph.shape[0])\n+    weights1_test = np.random.uniform(size=y_test_graph.shape[0])\n+    weights2_test = np.random.uniform(size=y2_test_graph.shape[0])\n+\n+    graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n+\n+    history = graph.fit({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n+                        nb_epoch=10,\n+                        sample_weight={'output1': weights1, 'output2': weights2})\n+    out = graph.predict({'input1': X_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 2)\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph},\n+                               sample_weight={'output1': weights1_test, 'output2': weights2_test})\n+    loss = graph.train_on_batch({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n+                                sample_weight={'output1': weights1, 'output2': weights2})\n+    loss = graph.evaluate({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n+                          sample_weight={'output1': weights1, 'output2': weights2})\n+\n+\n+def test_recursive():\n+    # test layer-like API\n+\n+    graph = containers.Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2', input='input1')\n+    graph.add_node(Dense(4), name='dense3', input='dense1')\n+    graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n+                     merge_mode='sum')\n+\n+    seq = Sequential()\n+    seq.add(Dense(32, input_shape=(32,)))\n+    seq.add(graph)\n+    seq.add(Dense(4))\n+\n+    seq.compile('rmsprop', 'mse')\n+\n+    history = seq.fit(X_train_graph, y_train_graph, batch_size=10, nb_epoch=10)\n+    loss = seq.evaluate(X_test_graph, y_test_graph)\n+    assert(loss < 2.5)\n+\n+    loss = seq.evaluate(X_test_graph, y_test_graph, show_accuracy=True)\n+    pred = seq.predict(X_test_graph)\n+    seq.get_config(verbose=1)\n+\n+\n+def test_create_output():\n+    # test create_output argument\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+\n+    graph.add_node(Dense(16), name='dense1', input='input1')\n+    graph.add_node(Dense(4), name='dense2', input='input1')\n+    graph.add_node(Dense(4), name='dense3', input='dense1')\n+    graph.add_node(Dense(4), name='output1', inputs=['dense2', 'dense3'],\n+                   merge_mode='sum', create_output=True)\n+    graph.compile('rmsprop', {'output1': 'mse'})\n+\n+    history = graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n+                        nb_epoch=10)\n+    out = graph.predict({'input1': X_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 1)\n+\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph})\n+    assert(loss < 2.5)\n+\n+\n+def test_count_params():\n+    # test count params\n+\n+    nb_units = 100\n+    nb_classes = 2\n+\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+    graph.add_input(name='input2', input_shape=(32,))\n+    graph.add_node(Dense(nb_units),\n+                   name='dense1', input='input1')\n+    graph.add_node(Dense(nb_classes),\n+                   name='dense2', input='input2')\n+    graph.add_node(Dense(nb_classes),\n+                   name='dense3', input='dense1')\n+    graph.add_output(name='output', inputs=['dense2', 'dense3'],\n+                     merge_mode='sum')\n+\n+    n = 32 * nb_units + nb_units\n+    n += 32 * nb_classes + nb_classes\n+    n += nb_units * nb_classes + nb_classes\n+\n+    assert(n == graph.count_params())\n+\n+    graph.compile('rmsprop', {'output': 'binary_crossentropy'})\n+\n+    assert(n == graph.count_params())\n+\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#36a41720af85ec282054bd3db1d160d7e151e73c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 666 | Files Changed: 2 | Hunks: 2 | Methods Changed: 19 | Complexity Δ (Sum/Max): -21/0 | Churn Δ: 666 | Churn Cumulative: 1576 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,274 +0,0 @@\n-from __future__ import print_function\n-import pytest\n-import numpy as np\n-import os\n-np.random.seed(1337)\n-\n-from keras.models import Graph, Sequential\n-from keras.layers import containers\n-from keras.layers.core import Dense, Activation\n-from keras.utils.test_utils import get_test_data\n-\n-(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n-                                                     nb_test=200,\n-                                                     input_shape=(32,),\n-                                                     classification=False,\n-                                                     output_shape=(4,))\n-(X2_train, y2_train), (X2_test, y2_test) = get_test_data(nb_train=1000,\n-                                                         nb_test=200,\n-                                                         input_shape=(32,),\n-                                                         classification=False,\n-                                                         output_shape=(1,))\n-\n-\n-def test_1o_1i():\n-    # test a non-sequential graph with 1 input and 1 output\n-    np.random.seed(1337)\n-\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(4), name='dense3', input='dense1')\n-\n-    graph.add_output(name='output1',\n-                     inputs=['dense2', 'dense3'],\n-                     merge_mode='sum')\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    history = graph.fit({'input1': X_train, 'output1': y_train},\n-                        nb_epoch=10)\n-    out = graph.predict({'input1': X_test})\n-    assert(type(out == dict))\n-    assert(len(out) == 1)\n-    loss = graph.test_on_batch({'input1': X_test, 'output1': y_test})\n-    loss = graph.train_on_batch({'input1': X_test, 'output1': y_test})\n-    loss = graph.evaluate({'input1': X_test, 'output1': y_test})\n-    assert(loss < 2.5)\n-\n-    # test validation split\n-    history = graph.fit({'input1': X_train, 'output1': y_train},\n-                        validation_split=0.2, nb_epoch=1)\n-    # test validation data\n-    history = graph.fit({'input1': X_train, 'output1': y_train},\n-                        validation_data={'input1': X_train, 'output1': y_train},\n-                        nb_epoch=1)\n-\n-\n-def test_1o_1i_2():\n-    # test a more complex non-sequential graph with 1 input and 1 output\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2-0', input='input1')\n-    graph.add_node(Activation('relu'), name='dense2', input='dense2-0')\n-\n-    graph.add_node(Dense(16), name='dense3', input='dense2')\n-    graph.add_node(Dense(4), name='dense4', inputs=['dense1', 'dense3'],\n-                   merge_mode='sum')\n-\n-    graph.add_output(name='output1', inputs=['dense2', 'dense4'],\n-                     merge_mode='sum')\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    history = graph.fit({'input1': X_train, 'output1': y_train},\n-                        nb_epoch=10)\n-    out = graph.predict({'input1': X_train})\n-    assert(type(out == dict))\n-    assert(len(out) == 1)\n-\n-    loss = graph.test_on_batch({'input1': X_test, 'output1': y_test})\n-    loss = graph.train_on_batch({'input1': X_test, 'output1': y_test})\n-    loss = graph.evaluate({'input1': X_test, 'output1': y_test})\n-    assert(loss < 2.5)\n-\n-    graph.get_config(verbose=1)\n-\n-\n-def test_1o_2i():\n-    # test a non-sequential graph with 2 inputs and 1 output\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-    graph.add_input(name='input2', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input2')\n-    graph.add_node(Dense(4), name='dense3', input='dense1')\n-\n-    graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n-                     merge_mode='sum')\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    history = graph.fit({'input1': X_train, 'input2': X2_train, 'output1': y_train},\n-                        nb_epoch=10)\n-    out = graph.predict({'input1': X_test, 'input2': X2_test})\n-    assert(type(out == dict))\n-    assert(len(out) == 1)\n-\n-    loss = graph.test_on_batch({'input1': X_test, 'input2': X2_test, 'output1': y_test})\n-    loss = graph.train_on_batch({'input1': X_test, 'input2': X2_test, 'output1': y_test})\n-    loss = graph.evaluate({'input1': X_test, 'input2': X2_test, 'output1': y_test})\n-    assert(loss < 3.0)\n-\n-    graph.get_config(verbose=1)\n-\n-\n-def test_2o_1i_weights():\n-    # test a non-sequential graph with 1 input and 2 outputs\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(1), name='dense3', input='dense1')\n-\n-    graph.add_output(name='output1', input='dense2')\n-    graph.add_output(name='output2', input='dense3')\n-    graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n-\n-    history = graph.fit({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n-                        nb_epoch=10)\n-    out = graph.predict({'input1': X_test})\n-    assert(type(out == dict))\n-    assert(len(out) == 2)\n-    loss = graph.test_on_batch({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n-    loss = graph.train_on_batch({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n-    loss = graph.evaluate({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n-    assert(loss < 4.)\n-\n-    # test weight saving\n-    fname = 'test_2o_1i_weights_temp.h5'\n-    graph.save_weights(fname, overwrite=True)\n-\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(1), name='dense3', input='dense1')\n-    graph.add_output(name='output1', input='dense2')\n-    graph.add_output(name='output2', input='dense3')\n-    graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n-    graph.load_weights('test_2o_1i_weights_temp.h5')\n-    os.remove(fname)\n-\n-    nloss = graph.evaluate({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n-    assert(loss == nloss)\n-\n-\n-def test_2o_1i_sample_weights():\n-    # test a non-sequential graph with 1 input and 2 outputs with sample weights\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(1), name='dense3', input='dense1')\n-\n-    graph.add_output(name='output1', input='dense2')\n-    graph.add_output(name='output2', input='dense3')\n-\n-    weights1 = np.random.uniform(size=y_train.shape[0])\n-    weights2 = np.random.uniform(size=y2_train.shape[0])\n-    weights1_test = np.random.uniform(size=y_test.shape[0])\n-    weights2_test = np.random.uniform(size=y2_test.shape[0])\n-\n-    graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n-\n-    history = graph.fit({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n-                        nb_epoch=10,\n-                        sample_weight={'output1': weights1, 'output2': weights2})\n-    out = graph.predict({'input1': X_test})\n-    assert(type(out == dict))\n-    assert(len(out) == 2)\n-    loss = graph.test_on_batch({'input1': X_test, 'output1': y_test, 'output2': y2_test},\n-                               sample_weight={'output1': weights1_test, 'output2': weights2_test})\n-    loss = graph.train_on_batch({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n-                                sample_weight={'output1': weights1, 'output2': weights2})\n-    loss = graph.evaluate({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n-                          sample_weight={'output1': weights1, 'output2': weights2})\n-\n-\n-def test_recursive():\n-    # test layer-like API\n-\n-    graph = containers.Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(4), name='dense3', input='dense1')\n-    graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n-                     merge_mode='sum')\n-\n-    seq = Sequential()\n-    seq.add(Dense(32, input_shape=(32,)))\n-    seq.add(graph)\n-    seq.add(Dense(4))\n-\n-    seq.compile('rmsprop', 'mse')\n-\n-    history = seq.fit(X_train, y_train, batch_size=10, nb_epoch=10)\n-    loss = seq.evaluate(X_test, y_test)\n-    assert(loss < 2.5)\n-\n-    loss = seq.evaluate(X_test, y_test, show_accuracy=True)\n-    pred = seq.predict(X_test)\n-    seq.get_config(verbose=1)\n-\n-\n-def test_create_output():\n-    # test create_output argument\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(4), name='dense3', input='dense1')\n-    graph.add_node(Dense(4), name='output1', inputs=['dense2', 'dense3'],\n-                   merge_mode='sum', create_output=True)\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    history = graph.fit({'input1': X_train, 'output1': y_train},\n-                        nb_epoch=10)\n-    out = graph.predict({'input1': X_test})\n-    assert(type(out == dict))\n-    assert(len(out) == 1)\n-\n-    loss = graph.test_on_batch({'input1': X_test, 'output1': y_test})\n-    loss = graph.train_on_batch({'input1': X_test, 'output1': y_test})\n-    loss = graph.evaluate({'input1': X_test, 'output1': y_test})\n-    assert(loss < 2.5)\n-\n-\n-def test_count_params():\n-    # test count params\n-\n-    nb_units = 100\n-    nb_classes = 2\n-\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-    graph.add_input(name='input2', input_shape=(32,))\n-    graph.add_node(Dense(nb_units),\n-                   name='dense1', input='input1')\n-    graph.add_node(Dense(nb_classes),\n-                   name='dense2', input='input2')\n-    graph.add_node(Dense(nb_classes),\n-                   name='dense3', input='dense1')\n-    graph.add_output(name='output', inputs=['dense2', 'dense3'],\n-                     merge_mode='sum')\n-\n-    n = 32 * nb_units + nb_units\n-    n += 32 * nb_classes + nb_classes\n-    n += nb_units * nb_classes + nb_classes\n-\n-    assert(n == graph.count_params())\n-\n-    graph.compile('rmsprop', {'output': 'binary_crossentropy'})\n-\n-    assert(n == graph.count_params())\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,392 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-import pytest\n-import numpy as np\n-np.random.seed(1337)\n-\n-from keras import backend as K\n-from keras.models import Sequential, model_from_json, model_from_yaml\n-from keras.layers.core import Dense, Activation, Merge, Lambda, LambdaMerge\n-from keras.utils import np_utils\n-from keras.utils.test_utils import get_test_data\n-import os\n-\n-input_dim = 32\n-nb_hidden = 16\n-nb_class = 4\n-batch_size = 32\n-nb_epoch = 1\n-\n-train_samples = 2000\n-test_samples = 500\n-\n-(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n-                                                     nb_test=test_samples,\n-                                                     input_shape=(input_dim,),\n-                                                     classification=True,\n-                                                     nb_class=4)\n-y_test = np_utils.to_categorical(y_test)\n-y_train = np_utils.to_categorical(y_train)\n-\n-\n-def test_sequential():\n-    model = Sequential()\n-    model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    model.add(Activation('relu'))\n-    model.add(Dense(nb_class))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1, validation_data=(X_test, y_test))\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2, validation_data=(X_test, y_test))\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_split=0.1)\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=1, validation_split=0.1)\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, shuffle=False)\n-\n-    model.train_on_batch(X_train[:32], y_train[:32])\n-\n-    loss = model.evaluate(X_train, y_train, verbose=0)\n-    assert(loss < 0.7)\n-\n-    model.predict(X_test, verbose=0)\n-    model.predict_classes(X_test, verbose=0)\n-    model.predict_proba(X_test, verbose=0)\n-    model.get_config(verbose=0)\n-\n-    fname = 'test_sequential_temp.h5'\n-    model.save_weights(fname, overwrite=True)\n-    model = Sequential()\n-    model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    model.add(Activation('relu'))\n-    model.add(Dense(nb_class))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-    model.load_weights(fname)\n-    os.remove(fname)\n-\n-    nloss = model.evaluate(X_train, y_train, verbose=0)\n-    assert(loss == nloss)\n-\n-    # test json serialization\n-    json_data = model.to_json()\n-    model = model_from_json(json_data)\n-\n-    # test yaml serialization\n-    yaml_data = model.to_yaml()\n-    model = model_from_yaml(yaml_data)\n-\n-\n-def test_merge_sum():\n-    left = Sequential()\n-    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    left.add(Activation('relu'))\n-\n-    right = Sequential()\n-    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    right.add(Activation('relu'))\n-\n-    model = Sequential()\n-    model.add(Merge([left, right], mode='sum'))\n-    model.add(Dense(nb_class))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n-\n-    loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-    assert(loss < 0.7)\n-\n-    model.predict([X_test, X_test], verbose=0)\n-    model.predict_classes([X_test, X_test], verbose=0)\n-    model.predict_proba([X_test, X_test], verbose=0)\n-    model.get_config(verbose=0)\n-\n-    # test weight saving\n-    fname = 'test_merge_sum_temp.h5'\n-    model.save_weights(fname, overwrite=True)\n-    left = Sequential()\n-    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    left.add(Activation('relu'))\n-    right = Sequential()\n-    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    right.add(Activation('relu'))\n-    model = Sequential()\n-    model.add(Merge([left, right], mode='sum'))\n-    model.add(Dense(nb_class))\n-    model.add(Activation('softmax'))\n-    model.load_weights(fname)\n-    os.remove(fname)\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-    assert(loss == nloss)\n-\n-\n-def test_merge_dot():\n-    if K._BACKEND == 'tensorflow':\n-        return\n-\n-    left = Sequential()\n-    left.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n-    left.add(Activation('relu'))\n-\n-    right = Sequential()\n-    right.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n-    right.add(Activation('relu'))\n-\n-    model = Sequential()\n-    model.add(Merge([left, right], mode='dot', dot_axes=1))\n-    model.add(Dense(nb_class))\n-    model.add(Activation('softmax'))\n-\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    left = Sequential()\n-    left.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n-    left.add(Activation('relu'))\n-\n-    right = Sequential()\n-    right.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n-    right.add(Activation('relu'))\n-\n-    model = Sequential()\n-    model.add(Merge([left, right], mode='dot', dot_axes=([1], [1])))\n-    model.add(Dense(nb_class))\n-    model.add(Activation('softmax'))\n-\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-\n-def test_merge_concat():\n-    left = Sequential()\n-    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    left.add(Activation('relu'))\n-\n-    right = Sequential()\n-    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    right.add(Activation('relu'))\n-\n-    model = Sequential()\n-    model.add(Merge([left, right], mode='concat'))\n-    model.add(Dense(nb_class))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n-\n-    loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-    assert(loss < 0.7)\n-\n-    model.predict([X_test, X_test], verbose=0)\n-    model.predict_classes([X_test, X_test], verbose=0)\n-    model.predict_proba([X_test, X_test], verbose=0)\n-    model.get_config(verbose=0)\n-\n-    fname = 'test_merge_concat_temp.h5'\n-    model.save_weights(fname, overwrite=True)\n-    left = Sequential()\n-    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    left.add(Activation('relu'))\n-\n-    right = Sequential()\n-    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    right.add(Activation('relu'))\n-\n-    model = Sequential()\n-    model.add(Merge([left, right], mode='concat'))\n-\n-    model.add(Dense(nb_class))\n-    model.add(Activation('softmax'))\n-\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-    model.load_weights(fname)\n-    os.remove(fname)\n-\n-    nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-    assert(loss == nloss)\n-\n-\n-def test_merge_recursivity():\n-    left = Sequential()\n-    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    left.add(Activation('relu'))\n-\n-    right = Sequential()\n-    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    right.add(Activation('relu'))\n-\n-    righter = Sequential()\n-    righter.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    righter.add(Activation('relu'))\n-\n-    intermediate = Sequential()\n-    intermediate.add(Merge([left, right], mode='sum'))\n-    intermediate.add(Dense(nb_hidden))\n-    intermediate.add(Activation('relu'))\n-\n-    model = Sequential()\n-    model.add(Merge([intermediate, righter], mode='sum'))\n-    model.add(Dense(nb_class))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test, X_test], y_test))\n-    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test, X_test], y_test))\n-    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n-    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n-    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n-\n-    loss = model.evaluate([X_train, X_train, X_train], y_train, verbose=0)\n-    assert(loss < 0.7)\n-\n-    model.predict([X_test, X_test, X_test], verbose=0)\n-    model.predict_classes([X_test, X_test, X_test], verbose=0)\n-    model.predict_proba([X_test, X_test, X_test], verbose=0)\n-    model.get_config(verbose=0)\n-\n-    fname = 'test_merge_recursivity_temp.h5'\n-    model.save_weights(fname, overwrite=True)\n-    model.load_weights(fname)\n-    os.remove(fname)\n-\n-    nloss = model.evaluate([X_train, X_train, X_train], y_train, verbose=0)\n-    assert(loss == nloss)\n-\n-\n-def test_merge_overlap():\n-    left = Sequential()\n-    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    left.add(Activation('relu'))\n-\n-    model = Sequential()\n-    model.add(Merge([left, left], mode='sum'))\n-    model.add(Dense(nb_class))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1, validation_data=(X_test, y_test))\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2, validation_data=(X_test, y_test))\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_split=0.1)\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=1, validation_split=0.1)\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, shuffle=False)\n-\n-    model.train_on_batch(X_train[:32], y_train[:32])\n-\n-    loss = model.evaluate(X_train, y_train, verbose=0)\n-    assert(loss < 0.7)\n-    model.predict(X_test, verbose=0)\n-    model.predict_classes(X_test, verbose=0)\n-    model.predict_proba(X_test, verbose=0)\n-    model.get_config(verbose=0)\n-\n-    fname = 'test_merge_overlap_temp.h5'\n-    model.save_weights(fname, overwrite=True)\n-    model.load_weights(fname)\n-    os.remove(fname)\n-\n-    nloss = model.evaluate(X_train, y_train, verbose=0)\n-    assert(loss == nloss)\n-\n-\n-def test_lambda():\n-    def func(X):\n-        s = X[0]\n-        for i in range(1, len(X)):\n-            s += X[i]\n-        return s\n-\n-    def activation(X):\n-        return K.softmax(X)\n-\n-    def output_shape(input_shapes):\n-        return input_shapes[0]\n-\n-    left = Sequential()\n-    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    left.add(Activation('relu'))\n-\n-    right = Sequential()\n-    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    right.add(Activation('relu'))\n-\n-    model = Sequential()\n-    model.add(LambdaMerge([left, right], function=func,\n-                          output_shape=output_shape))\n-    model.add(Dense(nb_class))\n-    model.add(Lambda(activation))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n-\n-    loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-    assert(loss < 0.7)\n-\n-    model.predict([X_test, X_test], verbose=0)\n-    model.predict_classes([X_test, X_test], verbose=0)\n-    model.predict_proba([X_test, X_test], verbose=0)\n-    model.get_config(verbose=0)\n-\n-    # test weight saving\n-    fname = 'test_lambda_temp.h5'\n-    model.save_weights(fname, overwrite=True)\n-    left = Sequential()\n-    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    left.add(Activation('relu'))\n-    right = Sequential()\n-    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    right.add(Activation('relu'))\n-    model = Sequential()\n-    model.add(LambdaMerge([left, right], function=func,\n-                          output_shape=output_shape))\n-    model.add(Dense(nb_class))\n-    model.add(Lambda(activation))\n-    model.load_weights(fname)\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-    os.remove(fname)\n-\n-    nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-    assert(loss == nloss)\n-\n-\n-def test_count_params():\n-    input_dim = 20\n-    nb_units = 10\n-    nb_classes = 2\n-\n-    n = input_dim * nb_units + nb_units\n-    n += nb_units * nb_units + nb_units\n-    n += nb_units * nb_classes + nb_classes\n-\n-    model = Sequential()\n-    model.add(Dense(nb_units, input_shape=(input_dim,)))\n-    model.add(Dense(nb_units))\n-    model.add(Dense(nb_classes))\n-    model.add(Activation('softmax'))\n-\n-    assert(n == model.count_params())\n-\n-    model.compile('sgd', 'binary_crossentropy')\n-\n-    assert(n == model.count_params())\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
