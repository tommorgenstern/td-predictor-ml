{"custom_id": "keras#12080ba701965f6eaa840704b78c981f33ecf671", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 18 | Methods Changed: 17 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 82 | Churn Cumulative: 1966 | Contributors (this commit): 33 | Commits (past 90d): 3 | Contributors (cumulative): 39 | DMM Complexity: 1.0\n\nDIFF:\n@@ -142,7 +142,7 @@ def elu(x, alpha=1.0):\n \n \n     Reference:\n-        [Fast and Accurate Deep Network Learning by Exponential Linear Units\n+        - [Fast and Accurate Deep Network Learning by Exponential Linear Units\n         (ELUs) (Clevert et al, 2016)](https://arxiv.org/abs/1511.07289)\n     \"\"\"\n     return backend.elu(x, alpha)\n@@ -288,7 +288,7 @@ def relu(x, alpha=0.0, max_value=None, threshold=0.0):\n     change the max value of the activation,\n     and to use a non-zero multiple of the input for values below the threshold.\n \n-    For example:\n+    Example:\n \n     >>> foo = tf.constant([-10, -5, 0.0, 5, 10], dtype = tf.float32)\n     >>> tf.keras.activations.relu(foo).numpy()\n@@ -329,7 +329,7 @@ def gelu(x, approximate=False):\n     The (GELU) nonlinearity weights inputs by their value, rather than gates\n     inputs by their sign as in ReLU.\n \n-    For example:\n+    Example:\n \n     >>> x = tf.constant([-3.0, -1.0, 0.0, 1.0, 3.0], dtype=tf.float32)\n     >>> y = tf.keras.activations.gelu(x)\n@@ -364,7 +364,7 @@ def gelu(x, approximate=False):\n def tanh(x):\n     \"\"\"Hyperbolic tangent activation function.\n \n-    For example:\n+    Example:\n \n     >>> a = tf.constant([-3.0, -1.0, 0.0, 1.0, 3.0], dtype = tf.float32)\n     >>> b = tf.keras.activations.tanh(a)\n@@ -394,7 +394,7 @@ def sigmoid(x):\n     assumed to be zero. The sigmoid function always returns a value between\n     0 and 1.\n \n-    For example:\n+    Example:\n \n     >>> a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)\n     >>> b = tf.keras.activations.sigmoid(a)\n@@ -419,7 +419,7 @@ def sigmoid(x):\n def exponential(x):\n     \"\"\"Exponential activation function.\n \n-    For example:\n+    Example:\n \n     >>> a = tf.constant([-3.0, -1.0, 0.0, 1.0, 3.0], dtype = tf.float32)\n     >>> b = tf.keras.activations.exponential(a)\n@@ -444,7 +444,7 @@ def hard_sigmoid(x):\n     Piecewise linear approximation of the sigmoid function.\n     Ref: 'https://en.wikipedia.org/wiki/Hard_sigmoid'\n \n-    For example:\n+    Example:\n \n     >>> a = tf.constant([-3.0, -1.0, 0.0, 1.0, 3.0], dtype = tf.float32)\n     >>> b = tf.keras.activations.hard_sigmoid(a)\n@@ -469,7 +469,7 @@ def hard_sigmoid(x):\n def linear(x):\n     \"\"\"Linear activation function (pass-through).\n \n-    For example:\n+    Example:\n \n     >>> a = tf.constant([-3.0, -1.0, 0.0, 1.0, 3.0], dtype = tf.float32)\n     >>> b = tf.keras.activations.linear(a)\n@@ -485,6 +485,45 @@ def linear(x):\n     return x\n \n \n+@keras_export(\"keras.activations.mish\")\n+@tf.__internal__.dispatch.add_dispatch_support\n+def mish(x):\n+    \"\"\"Mish activation function.\n+\n+    It is defined as:\n+\n+    ```python\n+    def mish(x):\n+        return x * tanh(softplus(x))\n+    ```\n+\n+    where `softplus` is defined as:\n+\n+    ```python\n+    def softplus(x):\n+        return log(exp(x) + 1)\n+    ```\n+\n+    Example:\n+\n+    >>> a = tf.constant([-3.0, -1.0, 0.0, 1.0], dtype = tf.float32)\n+    >>> b = tf.keras.activations.mish(a)\n+    >>> b.numpy()\n+    array([-0.14564745, -0.30340144,  0.,  0.86509836], dtype=float32)\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        The mish activation.\n+\n+    Reference:\n+        - [Mish: A Self Regularized Non-Monotonic\n+        Activation Function](https://arxiv.org/abs/1908.08681)\n+    \"\"\"\n+    return x * tf.math.tanh(tf.math.softplus(x))\n+\n+\n @keras_export(\"keras.activations.serialize\")\n @tf.__internal__.dispatch.add_dispatch_support\n def serialize(activation, use_legacy_format=False):\n@@ -496,7 +535,7 @@ def serialize(activation, use_legacy_format=False):\n     Returns:\n         String denoting the name attribute of the input function\n \n-    For example:\n+    Example:\n \n     >>> tf.keras.activations.serialize(tf.keras.activations.tanh)\n     'tanh'\n@@ -523,7 +562,7 @@ def serialize(activation, use_legacy_format=False):\n     return legacy_serialization.serialize_keras_object(activation)\n \n \n-# Add additional globals so that deserialize can find these common activation\n+# Add additional globals so that deserialize() can find these common activation\n # functions\n leaky_relu = tf.nn.leaky_relu\n log_softmax = tf.nn.log_softmax\n@@ -544,7 +583,7 @@ def deserialize(name, custom_objects=None, use_legacy_format=False):\n     Returns:\n         Corresponding activation function.\n \n-    For example:\n+    Example:\n \n     >>> tf.keras.activations.deserialize('linear')\n      <function linear at 0x1239596a8>\n@@ -598,7 +637,7 @@ def get(identifier):\n     Returns:\n         Function corresponding to the input string or input function.\n \n-    For example:\n+    Example:\n \n     >>> tf.keras.activations.get('softmax')\n      <function softmax at 0x1222a3d90>\n\n@@ -32,6 +32,10 @@ def _ref_softmax(values):\n     return e / np.sum(e)\n \n \n+def _ref_softplus(x):\n+    return np.log(np.ones_like(x) + np.exp(x))\n+\n+\n @test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n class KerasActivationsTest(tf.test.TestCase, parameterized.TestCase):\n     def test_serialization(self):\n@@ -48,6 +52,7 @@ class KerasActivationsTest(tf.test.TestCase, parameterized.TestCase):\n             \"selu\",\n             \"gelu\",\n             \"relu6\",\n+            \"mish\",\n         ]\n         for name in all_activations:\n             fn = activations.get(name)\n@@ -147,14 +152,11 @@ class KerasActivationsTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertAllClose(result, true_result)\n \n     def test_softplus(self):\n-        def softplus(x):\n-            return np.log(np.ones_like(x) + np.exp(x))\n-\n         x = backend.placeholder(ndim=2)\n         f = backend.function([x], [activations.softplus(x)])\n         test_values = np.random.random((2, 5))\n         result = f([test_values])[0]\n-        expected = softplus(test_values)\n+        expected = _ref_softplus(test_values)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n     def test_softsign(self):\n@@ -272,6 +274,15 @@ class KerasActivationsTest(tf.test.TestCase, parameterized.TestCase):\n         expected = np.exp(test_values)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+    def test_mish(self):\n+        test_values = np.random.random((2, 5))\n+        x = backend.placeholder(ndim=2)\n+        output = activations.mish(x)\n+        f = backend.function([x], [output])\n+        result = f([test_values])[0]\n+        expected = test_values * np.tanh(_ref_softplus(test_values))\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n     def test_linear(self):\n         x = np.random.random((10, 5))\n         self.assertAllClose(x, activations.linear(x))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d6654c8157554981b35bfce89031e0211aa21f52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2751 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1423,12 +1423,13 @@ class SyncBatchNormalization(BatchNormalizationBase):\n         gamma_constraint=None,\n         **kwargs,\n     ):\n-        logging.warning(\n+        warning = (\n             \"`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is \"\n             \"deprecated and will be removed in a future release. Please use \"\n             \"`tf.keras.layers.BatchNormalization` with parameter \"\n             \"`synchronized` set to True.\"\n         )\n+        logging.log_first_n(logging.WARN, warning, 1)\n         super().__init__(\n             axis=axis,\n             momentum=momentum,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ce3738a7c35df6527130338e034705be1d72355b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 303 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 7 | Methods Changed: 11 | Complexity Δ (Sum/Max): 26/21 | Churn Δ: 303 | Churn Cumulative: 1628 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 7 | DMM Complexity: 0.8284313725490197\n\nDIFF:\n@@ -72,6 +72,7 @@ from keras.metrics.regression_metrics import MeanRelativeError\n from keras.metrics.regression_metrics import MeanSquaredError\n from keras.metrics.regression_metrics import MeanSquaredLogarithmicError\n from keras.metrics.regression_metrics import RootMeanSquaredError\n+from keras.metrics.regression_metrics import R2Score\n \n from keras.metrics.regression_metrics import cosine_similarity\n from keras.metrics.regression_metrics import logcosh\n\n@@ -14,6 +14,8 @@\n # ==============================================================================\n \"\"\"Regression metrics, e.g. MAE/MSE/etc.\"\"\"\n \n+import warnings\n+\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n@@ -412,6 +414,200 @@ class LogCoshError(base_metric.MeanMetricWrapper):\n         super().__init__(logcosh, name, dtype=dtype)\n \n \n+# Adapted from TF-Addons implementation (RSquare class).\n+@keras_export(\"keras.metrics.R2Score\")\n+class R2Score(base_metric.Metric):\n+    \"\"\"Computes R2 score.\n+\n+    This is also called the\n+    [coefficient of\n+    determination](https://en.wikipedia.org/wiki/Coefficient_of_determination).\n+\n+    It indicates how close the fitted regression line\n+    is to ground-truth data.\n+\n+    - The highest score possible is 1.0. It indicates that the predictors\n+        perfectly accounts for variation in the target.\n+    - A score of 0.0 indicates that the predictors do not\n+        account for variation in the target.\n+    - It can also be negative if the model is worse than random.\n+\n+    This metric can also compute the \"Adjusted R2\" score.\n+\n+    Args:\n+        class_aggregation: Specifies how to aggregate scores corresponding to\n+            different output classes (or target dimensions),\n+            i.e. different dimensions on the last axis of the predictions.\n+            Equivalent to `multioutput` argument in Scikit-Learn.\n+            Should be one of\n+            `None` (no aggregation), `\"uniform_average\"`,\n+            `\"variance_weighted_average\"`.\n+        num_regressors: Number of independent regressors used\n+            (\"Adjusted R2\" score). Defaults to 0 (standard R2 score).\n+        name: Optional. string name of the metric instance.\n+        dtype: Optional. data type of the metric result.\n+\n+    Example:\n+\n+    >>> y_true = np.array([[1], [4], [3]], dtype=np.float32)\n+    >>> y_pred = np.array([[2], [4], [4]], dtype=np.float32)\n+    >>> metric = tf.keras.metrics.R2Score()\n+    >>> metric.update_state(y_true, y_pred)\n+    >>> result = metric.result()\n+    >>> result.numpy()\n+    0.57142854\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self,\n+        class_aggregation=\"uniform_average\",\n+        num_regressors=0,\n+        name=\"r2_score\",\n+        dtype=None,\n+    ):\n+        super().__init__(name=name, dtype=dtype)\n+\n+        valid_class_aggregation_values = (\n+            None,\n+            \"uniform_average\",\n+            \"variance_weighted_average\",\n+        )\n+        if class_aggregation not in valid_class_aggregation_values:\n+            raise ValueError(\n+                \"Invalid value for argument `class_aggregation`. Expected \"\n+                f\"one of {valid_class_aggregation_values}. \"\n+                f\"Received: class_aggregation={class_aggregation}\"\n+            )\n+        if num_regressors < 0:\n+            raise ValueError(\n+                \"Invalid value for argument `num_regressors`. \"\n+                \"Expected a value >= 0. \"\n+                f\"Received: num_regressors={num_regressors}\"\n+            )\n+        self.class_aggregation = class_aggregation\n+        self.num_regressors = num_regressors\n+        self.num_samples = self.add_weight(name=\"num_samples\", dtype=\"int32\")\n+        self.built = False\n+\n+    def build(self, y_true_shape, y_pred_shape):\n+        if len(y_pred_shape) != 2 or len(y_true_shape) != 2:\n+            raise ValueError(\n+                \"R2Score expects 2D inputs with shape \"\n+                \"(batch_size, output_dim). Received input \"\n+                f\"shapes: y_pred.shape={y_pred_shape} and \"\n+                f\"y_true.shape={y_true_shape}.\"\n+            )\n+        if y_pred_shape[-1] is None or y_true_shape[-1] is None:\n+            raise ValueError(\n+                \"R2Score expects 2D inputs with shape \"\n+                \"(batch_size, output_dim), with output_dim fully \"\n+                \"defined (not None). Received input \"\n+                f\"shapes: y_pred.shape={y_pred_shape} and \"\n+                f\"y_true.shape={y_true_shape}.\"\n+            )\n+        num_classes = y_pred_shape[-1]\n+        self.squared_sum = self.add_weight(\n+            name=\"squared_sum\",\n+            shape=[num_classes],\n+            initializer=\"zeros\",\n+        )\n+        self.sum = self.add_weight(\n+            name=\"sum\",\n+            shape=[num_classes],\n+            initializer=\"zeros\",\n+        )\n+        self.total_mse = self.add_weight(\n+            name=\"residual\",\n+            shape=[num_classes],\n+            initializer=\"zeros\",\n+        )\n+        self.count = self.add_weight(\n+            name=\"count\",\n+            shape=[num_classes],\n+            initializer=\"zeros\",\n+        )\n+        self.built = True\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        y_true = tf.convert_to_tensor(y_true, dtype=self.dtype)\n+        y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n+        if not self.built:\n+            self.build(y_true.shape, y_pred.shape)\n+\n+        if sample_weight is None:\n+            sample_weight = 1\n+\n+        sample_weight = tf.convert_to_tensor(sample_weight, dtype=self.dtype)\n+        if sample_weight.shape.rank == 1:\n+            # Make sure there's a features dimension\n+            sample_weight = tf.expand_dims(sample_weight, axis=1)\n+        sample_weight = tf.__internal__.ops.broadcast_weights(\n+            weights=sample_weight, values=y_true\n+        )\n+\n+        weighted_y_true = y_true * sample_weight\n+        self.sum.assign_add(tf.reduce_sum(weighted_y_true, axis=0))\n+        self.squared_sum.assign_add(\n+            tf.reduce_sum(y_true * weighted_y_true, axis=0)\n+        )\n+        self.total_mse.assign_add(\n+            tf.reduce_sum((y_true - y_pred) ** 2 * sample_weight, axis=0)\n+        )\n+        self.count.assign_add(tf.reduce_sum(sample_weight, axis=0))\n+        self.num_samples.assign_add(tf.size(y_true))\n+\n+    def result(self):\n+        mean = self.sum / self.count\n+        total = self.squared_sum - self.sum * mean\n+        raw_scores = 1 - (self.total_mse / total)\n+        raw_scores = tf.where(tf.math.is_inf(raw_scores), 0.0, raw_scores)\n+\n+        if self.class_aggregation == \"uniform_average\":\n+            r2_score = tf.reduce_mean(raw_scores)\n+        elif self.class_aggregation == \"variance_weighted_average\":\n+            weighted_sum = tf.reduce_sum(total * raw_scores)\n+            sum_of_weights = tf.reduce_sum(total)\n+            r2_score = weighted_sum / sum_of_weights\n+        else:\n+            r2_score = raw_scores\n+\n+        if self.num_regressors != 0:\n+            if self.num_regressors > self.num_samples - 1:\n+                warnings.warn(\n+                    \"More independent predictors than datapoints \"\n+                    \"in adjusted R2 score. Falling back to standard R2 score.\",\n+                    stacklevel=2,\n+                )\n+            elif self.num_regressors == self.num_samples - 1:\n+                warnings.warn(\n+                    \"Division by zero in Adjusted R2 score. \"\n+                    \"Falling back to standard R2 score.\",\n+                    stacklevel=2,\n+                )\n+            else:\n+                n = tf.cast(self.num_samples, dtype=tf.float32)\n+                p = tf.cast(self.num_regressors, dtype=tf.float32)\n+                num = tf.multiply(\n+                    tf.subtract(1.0, r2_score), tf.subtract(n, 1.0)\n+                )\n+                den = tf.subtract(tf.subtract(n, p), 1.0)\n+                r2_score = tf.subtract(1.0, tf.divide(num, den))\n+        return r2_score\n+\n+    def reset_state(self):\n+        for v in self.variables:\n+            v.assign(tf.zeros(v.shape))\n+\n+    def get_config(self):\n+        config = {\n+            \"class_aggregation\": self.class_aggregation,\n+            \"num_regressors\": self.num_regressors,\n+        }\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n+\n+\n def cosine_similarity(y_true, y_pred, axis=-1):\n     \"\"\"Computes the cosine similarity between labels and predictions.\n \n\n@@ -18,9 +18,12 @@ import math\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n+from absl.testing import parameterized\n \n+from keras import Input\n from keras import metrics\n from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n \n \n @test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n@@ -396,5 +399,108 @@ class MeanRelativeErrorTest(tf.test.TestCase):\n         self.assertEqual(self.evaluate(result), 0)\n \n \n+@test_utils.run_v2_only\n+class R2ScoreTest(parameterized.TestCase, tf.test.TestCase):\n+    def _run_test(\n+        self,\n+        y_true,\n+        y_pred,\n+        sample_weights,\n+        class_aggregation,\n+        num_regressors,\n+        reference_result,\n+    ):\n+        y_true = tf.constant(y_true, dtype=\"float32\")\n+        y_pred = tf.constant(y_pred, dtype=\"float32\")\n+        r2 = metrics.R2Score(class_aggregation, num_regressors)\n+        r2.update_state(y_true, y_pred, sample_weights)\n+        result = r2.result().numpy()\n+        self.assertAllClose(result, reference_result, atol=1e-6)\n+\n+    def test_config(self):\n+        r2_obj = metrics.R2Score(\n+            class_aggregation=None,\n+            num_regressors=2,\n+        )\n+        self.assertEqual(r2_obj.class_aggregation, None)\n+        self.assertEqual(r2_obj.num_regressors, 2)\n+        self.assertEqual(r2_obj.dtype, tf.float32)\n+\n+        # Check save and restore config\n+        r2_obj2 = metrics.R2Score.from_config(r2_obj.get_config())\n+        self.assertEqual(r2_obj2.class_aggregation, None)\n+        self.assertEqual(r2_obj2.num_regressors, 2)\n+        self.assertEqual(r2_obj2.dtype, tf.float32)\n+\n+    @parameterized.parameters(\n+        # class_aggregation, num_regressors, result\n+        (None, 0, [0.37, -1.295, 0.565]),\n+        (\"uniform_average\", 0, -0.12),\n+        (\"variance_weighted_average\", 0, -0.12),\n+    )\n+    def test_r2_sklearn_comparison(\n+        self, class_aggregation, num_regressors, result\n+    ):\n+        y_true = [[0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0]]\n+        y_pred = [[0.4, 0.5, 0.6], [0.1, 0.2, 0.3], [0.5, 0.8, 0.2]]\n+        self._run_test(\n+            y_true,\n+            y_pred,\n+            None,\n+            class_aggregation=class_aggregation,\n+            num_regressors=num_regressors,\n+            reference_result=result,\n+        )\n+\n+    @parameterized.parameters(\n+        # class_aggregation, num_regressors, result\n+        (None, 0, [0.17305559, -8.836666, -0.521]),\n+        (None, 1, [0.054920673, -10.241904, -0.7382858]),\n+        (None, 2, [-0.10259259, -12.115555, -1.0280001]),\n+        (\"uniform_average\", 0, -3.0615367889404297),\n+        (\"uniform_average\", 1, -3.641756534576416),\n+        (\"uniform_average\", 2, -4.415382385253906),\n+        (\"variance_weighted_average\", 0, -1.3710224628448486),\n+        (\"variance_weighted_average\", 1, -1.7097399234771729),\n+        (\"variance_weighted_average\", 2, -2.161363363265991),\n+    )\n+    def test_r2_tfa_comparison(self, class_aggregation, num_regressors, result):\n+        y_true = [[0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0]]\n+        y_pred = [[0.4, 0.9, 1.6], [0.1, 1.2, 0.6], [1.5, 0.8, 0.6]]\n+        sample_weights = [0.8, 0.1, 0.4]\n+        self._run_test(\n+            y_true,\n+            y_pred,\n+            sample_weights,\n+            class_aggregation=class_aggregation,\n+            num_regressors=num_regressors,\n+            reference_result=result,\n+        )\n+\n+    def test_errors(self):\n+        # Bad class_aggregation value\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid value for argument `class_aggregation`\"\n+        ):\n+            metrics.R2Score(class_aggregation=\"wrong\")\n+\n+        # Bad num_regressors value\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid value for argument `num_regressors`\"\n+        ):\n+            metrics.R2Score(num_regressors=-1)\n+\n+        # Bad input shape\n+        with self.assertRaisesRegex(ValueError, \"expects 2D inputs with shape\"):\n+            r2 = metrics.R2Score()\n+            r2.update_state(tf.constant([0.0, 1.0]), tf.constant([0.0, 1.0]))\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"with output_dim fully defined\"\n+        ):\n+            r2 = metrics.R2Score()\n+            r2.update_state(Input(shape=(None,)), tf.constant([[0.0], [1.0]]))\n+\n+\n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a32070b2146e8b0442505fb32ab93d8123931364", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 32 | Churn Cumulative: 105 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1097,6 +1097,13 @@ class Optimizer(_BaseOptimizer):\n             **kwargs,\n         )\n         self._distribution_strategy = tf.distribute.get_strategy()\n+        # `tf.CriticalSection()` is used to resolve race condition under\n+        # PSS training. See b/261724919 for more context.\n+        if isinstance(\n+            self._distribution_strategy,\n+            tf.distribute.ParameterServerStrategy,\n+        ):\n+            self._critical_section = tf.CriticalSection()\n \n     def add_variable_from_reference(\n         self, model_variable, variable_name, shape=None, initial_value=None\n@@ -1240,16 +1247,35 @@ class Optimizer(_BaseOptimizer):\n     ):\n         \"\"\"`apply_gradients` using a `DistributionStrategy`.\"\"\"\n \n-        def apply_grad_to_update_var(var, grad):\n+        def apply_grad_to_update_var_step(var, grad):\n             if self.jit_compile:\n                 return self._update_step_xla(grad, var, id(self._var_key(var)))\n             else:\n                 return self._update_step(grad, var)\n \n-        for grad, var in grads_and_vars:\n+        def apply_grad_to_update_var():\n             distribution.extended.update(\n-                var, apply_grad_to_update_var, args=(grad,), group=False\n+                var, apply_grad_to_update_var_step, args=(grad,), group=False\n             )\n+            # Functions executed inside `tf.CriticalSection` needs to return\n+            # a tensor. Return a dummy tensor since we have nothing to return.\n+            return tf.constant(0)\n+\n+        for grad, var in grads_and_vars:\n+            if isinstance(\n+                self._distribution_strategy,\n+                tf.distribute.ParameterServerStrategy,\n+            ):\n+                # Use `tf.CriticalSection` to avoid race condition, it's the\n+                # same effect as acquiring a mutex. PSS training hit race\n+                # condition without mutex, see b/261724919 for context.\n+                self._critical_section.execute(\n+                    apply_grad_to_update_var,\n+                    exclusive_resource_access=True,\n+                    name=None,\n+                )\n+            else:\n+                apply_grad_to_update_var()\n \n         if self.use_ema:\n             _, var_list = zip(*grads_and_vars)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0046e0dcd5b7cac5bd7cc9cb5276f667b80229a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 240 | Contributors (this commit): 18 | Commits (past 90d): 4 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -54,6 +54,7 @@ from keras.utils.feature_space import FeatureSpace\n \n # Internal\n from keras.utils.layer_utils import get_source_inputs\n+from keras.utils.layer_utils import warmstart_embedding_matrix\n \n # Deprecated\n from keras.utils.np_utils import normalize\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e49136ce8b942a6db3824b42fd5eb17dfd4697ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1160 | Contributors (this commit): 13 | Commits (past 90d): 1 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -35,7 +35,7 @@ class StringLookup(index_lookup.IndexLookup):\n     This layer translates a set of arbitrary strings into integer output via a\n     table-based vocabulary lookup. This layer will perform no splitting or\n     transformation of input strings. For a layer than can split and tokenize\n-    natural language, see the `TextVectorization` layer.\n+    natural language, see the [TextVectorization](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/#textvectorization-class) layer.\n \n     The vocabulary for the layer must be either supplied on construction or\n     learned via `adapt()`. During `adapt()`, the layer will analyze a data set,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eedabb62ad1ad288b380b1822687b7e67265a6ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 3143 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: 0.5\n\nDIFF:\n@@ -324,7 +324,7 @@ def PreStem(name=None):\n     return apply\n \n \n-def Head(num_classes=1000, name=None):\n+def Head(num_classes=1000, classifier_activation=None, name=None):\n     \"\"\"Implementation of classification head of RegNet.\n \n     Args:\n@@ -342,7 +342,9 @@ def Head(num_classes=1000, name=None):\n         x = layers.LayerNormalization(\n             epsilon=1e-6, name=name + \"_head_layernorm\"\n         )(x)\n-        x = layers.Dense(num_classes, name=name + \"_head_dense\")(x)\n+        x = layers.Dense(num_classes,\n+                         activation=classifier_activation,\n+                         name=name + \"_head_dense\")(x)\n         return x\n \n     return apply\n@@ -522,7 +524,9 @@ def ConvNeXt(\n         cur += depths[i]\n \n     if include_top:\n-        x = Head(num_classes=classes, name=model_name)(x)\n+        x = Head(num_classes=classes,\n+                 classifier_activation=classifier_activation,\n+                 name=model_name)(x)\n         imagenet_utils.validate_activation(classifier_activation, weights)\n \n     else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c1a43df9bbddca535a3847b382b38b2048c864bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 1594 | Contributors (this commit): 34 | Commits (past 90d): 3 | Contributors (cumulative): 34 | DMM Complexity: 0.0\n\nDIFF:\n@@ -80,18 +80,18 @@ def softmax(x, axis=-1):\n     >>> layer = tf.keras.layers.Dense(32,\n     ...                               activation=tf.keras.activations.softmax)\n     \"\"\"\n-    if x.shape.rank > 1:\n+    if x.shape.rank <= 1:\n+        raise ValueError(\n+            f\"Cannot apply softmax to a tensor that is 1D. Received input: {x}\"\n+        )\n+\n     if isinstance(axis, int):\n         output = tf.nn.softmax(x, axis=axis)\n     else:\n         # nn.softmax does not support tuple axis.\n-            e = tf.exp(x - tf.reduce_max(x, axis=axis, keepdims=True))\n-            s = tf.reduce_sum(e, axis=axis, keepdims=True)\n-            output = e / s\n-    else:\n-        raise ValueError(\n-            f\"Cannot apply softmax to a tensor that is 1D. Received input: {x}\"\n-        )\n+        numerator = tf.exp(x - tf.reduce_max(x, axis=axis, keepdims=True))\n+        denominator = tf.reduce_sum(e, axis=axis, keepdims=True)\n+        output = numerator / denominator\n \n     # Cache the logits to use for crossentropy loss.\n     output._keras_logits = x\n@@ -667,7 +667,6 @@ def get(identifier):\n         return deserialize(identifier, use_legacy_format=use_legacy_format)\n     elif callable(identifier):\n         return identifier\n-    else:\n     raise TypeError(\n         f\"Could not interpret activation function identifier: {identifier}\"\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0d6e2f2a424fccf3ef98ea8a03b5b5e708c03d84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1596 | Contributors (this commit): 34 | Commits (past 90d): 4 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -90,7 +90,7 @@ def softmax(x, axis=-1):\n     else:\n         # nn.softmax does not support tuple axis.\n         numerator = tf.exp(x - tf.reduce_max(x, axis=axis, keepdims=True))\n-        denominator = tf.reduce_sum(e, axis=axis, keepdims=True)\n+        denominator = tf.reduce_sum(numerator, axis=axis, keepdims=True)\n         output = numerator / denominator\n \n     # Cache the logits to use for crossentropy loss.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1ed03d6e2502a6ce523514de02c28e59c5996ca7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 410 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 26 | Complexity Δ (Sum/Max): 36/19 | Churn Δ: 410 | Churn Cumulative: 907 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,72 @@\n+\"\"\"Test Model.fit with a PyMetric.\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+from absl.testing import parameterized\n+\n+from keras import Sequential\n+from keras import layers\n+from keras import losses\n+from keras import metrics\n+from keras.testing_infra import test_combinations\n+\n+\n+def get_dataset(num_batches=5, batch_size=2):\n+    x = tf.random.uniform((num_batches * batch_size, 100))\n+    y = tf.random.uniform((num_batches * batch_size, 2))\n+    dataset = (\n+        tf.data.Dataset.from_tensor_slices((x, y))\n+        .prefetch(batch_size * 2)\n+        .batch(batch_size)\n+    )\n+    return dataset\n+\n+\n+class CountingPyMetric(metrics.PyMetric):\n+    \"\"\"A test-only PyMetric which simply counts how many results it's seen.\"\"\"\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        self.y_pred.append(y_pred)\n+\n+    def reset_state(self):\n+        self.y_pred = []\n+\n+    def result(self):\n+        return len(self.y_pred)\n+\n+\n+class PyMetricTest(test_combinations.TestCase):\n+    @parameterized.named_parameters((\"eager\", True), (\"graph\", False))\n+    def test_fit(self, run_eagerly):\n+        num_batches = 5\n+        dataset = get_dataset(num_batches=num_batches)\n+\n+        counting_metric = CountingPyMetric()\n+\n+        model = Sequential(layers.Dense(2))\n+        model.compile(\n+            loss=losses.BinaryCrossentropy(),\n+            metrics=[counting_metric],\n+            run_eagerly=run_eagerly,\n+        )\n+        model.fit(dataset, epochs=1)\n+\n+        self.assertEqual(counting_metric.result(), num_batches)\n+\n+    @parameterized.named_parameters((\"eager\", True), (\"graph\", False))\n+    def test_evaluate(self, run_eagerly):\n+        num_batches = 5\n+        dataset = get_dataset(num_batches=num_batches)\n+\n+        model = Sequential(layers.Dense(2))\n+        model.compile(\n+            loss=losses.BinaryCrossentropy(),\n+            metrics=[CountingPyMetric()],\n+            run_eagerly=run_eagerly,\n+        )\n+        loss, count = model.evaluate(dataset)\n+\n+        self.assertEqual(count, num_batches)\n+\n+\n+if __name__ == \"__main__\":\n+    tf.test.main()\n\n@@ -33,6 +33,8 @@ from keras.saving.legacy import serialization as legacy_serialization\n from keras.saving.legacy.serialization import deserialize_keras_object\n from keras.saving.legacy.serialization import serialize_keras_object\n \n+from keras.metrics.py_metric import PyMetric\n+\n # Individual metric classes\n \n # Accuracy metrics\n\n@@ -0,0 +1,191 @@\n+# Copyright 2023 The Keras Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Base class for Python-based metrics\"\"\"\n+\n+import types\n+\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.util.tf_export import keras_export\n+\n+from keras.metrics import base_metric\n+\n+\n+@keras_export(\"keras.metrics.experimental.PyMetric\", v1=[])\n+class PyMetric(base_metric.Metric):\n+    \"\"\"Metric which runs in Python, compiled outside of the TensorFlow graph.\n+\n+    Args:\n+      name: (Optional) string name of the PyMetric instance.\n+      dtype: (Optional) data type of the PyMetric result.\n+      **kwargs: Additional layer keywords arguments.\n+\n+    Usage of `PyMetric` is generally identical to `keras.metrics.Metric`.\n+    It can be used in isolation, or in tandem with the `compile()` API. For more\n+    information about the usage of `PyMetric`, see `keras.metrics.Metric`.\n+\n+    Unlike regular metrics, `PyMetric` instances are outside-compiled\n+    with respect to the TensorFlow graph during training or evaluation.\n+    They have access to the same\n+    inputs of a standard in-graph metric, but they run in a Python interpreter\n+    on the host CPU. Any data stored in a `PyMetric` is located on the main\n+    memory of the host CPU, and any TensorFlow ops used in a PyMetric are\n+    run eagerly on the host CPU.\n+\n+    As a result, `PyMetric` instances are generally not as performant\n+    as in-graph metrics, and should only be used in cases where computing\n+    the metric inside of the TensorFlow graph is either impossible\n+    or prohibitively expensive.\n+\n+    **Note:** Due to the use of `tf.py_function`, PyMetrics\n+    are incompatible with XLA and therefore TPUs.\n+\n+    Methods to be implemented by subclasses:\n+\n+    * `update_state()`: Handles updates to internal state variables\n+    * `result()`: Computes and returns a scalar value or a dict of scalar values\n+      for the metric from the state variables.\n+    * `reset_state()`: Computes and returns a scalar value for the metric from\n+      the state variables.\n+\n+    This subclass implementation is similar to that of `keras.metrics.Metric`,\n+    with two notable differences:\n+\n+    * Inputs to `update_state()` in a `PyMetric` are eager tensors, and both\n+    `update_state()` and `result()` run outside of the TensorFlow graph,\n+    executing any TensorFlow ops eagerly.\n+    * `reset_state()` is also called at initialization time to initialize the\n+    Python state of the metric.\n+    * `result()` can only return a single scalar. It does not support returning\n+    a dictionary of results like `keras.metrics.Metric`.\n+\n+    Example subclass implementation using sklearn's Jaccard Score:\n+\n+    ```python\n+    from sklearn.metrics import jaccard_score\n+    import tensorflow as tf\n+\n+    class JaccardScore(tf.keras.metrics.experimental.PyMetric):\n+\n+      def __init__(self, name='jaccard_score', **kwargs):\n+        super().__init__(name=name, **kwargs)\n+\n+      def update_state(self, y_true, y_pred, sample_weight=None):\n+        self.jaccard_sum += jaccard_score(y_pred, y_true, average=\"macro\")\n+        self.count += 1\n+\n+      def reset_state(self):\n+        self.jaccard_sum = 0.\n+        self.count = 0.\n+\n+      def result(self):\n+        return self.jaccard_sum / self.count\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, name=None, dtype=None, **kwargs):\n+        super().__init__(name=name, dtype=dtype, **kwargs)\n+        self.reset_state()\n+\n+    def __new__(cls, *args, **kwargs):\n+        obj = super(base_metric.Metric, cls).__new__(cls)\n+\n+        # Wrap the update_state function in a py_function and scope it to /cpu:0\n+        obj_update_state = obj.update_state\n+\n+        def update_state_on_cpu(y_true, y_pred, sample_weight=None):\n+            with tf.device(\"/cpu:0\"):\n+                return obj_update_state(y_true, y_pred, sample_weight)\n+\n+        obj.update_state_on_cpu = update_state_on_cpu\n+\n+        def update_state_fn(self, y_true, y_pred, sample_weight=None):\n+            eager_inputs = [y_true, y_pred]\n+            if sample_weight is not None:\n+                eager_inputs.append(sample_weight)\n+            return tf.py_function(\n+                func=self.update_state_on_cpu, inp=eager_inputs, Tout=[]\n+            )\n+\n+        obj.update_state = types.MethodType(update_state_fn, obj)\n+\n+        # Wrap the result function in a py_function and scope it to /cpu:0\n+        obj_result = obj.result\n+\n+        def result_on_host_cpu():\n+            with tf.device(\"/cpu:0\"):\n+                return obj_result()\n+\n+        obj.result_on_host_cpu = result_on_host_cpu\n+\n+        def result_fn(self):\n+            return tf.py_function(\n+                self.result_on_host_cpu, inp=[], Tout=obj.dtype\n+            )\n+\n+        obj.result = types.MethodType(result_fn, obj)\n+\n+        return obj\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates statistics for the metric.\n+\n+        **Note:** This function is executed outside of the TensorFlow graph\n+        on the CPU host.\n+\n+        This means:\n+\n+        a) Inputs are eager tensors.\n+        b) Any TensorFlow ops run in this method are run eagerly.\n+        c) Any Tensors created are allocated to the CPU's main memory.\n+\n+        Args:\n+          y_true: Target output\n+          y_pred: Predicted output\n+          sample_weight: (Optional) weights for the individual samples in\n+            `y_true` and `y_pred`\n+        \"\"\"\n+        raise NotImplementedError(\"Subclasses should implement `update_state`\")\n+\n+    def merge_state(self, metrics):\n+        \"\"\"Merges the state from one or more metrics.\n+\n+        `PyMetric` instances that intend to support merging state must override\n+         this method, as the default implementation\n+        in `keras.metrics.Metric` does not apply to `PyMetric`.\n+        \"\"\"\n+        raise NotImplementedError(\"Subclasses should implement `merge_state`\")\n+\n+    def reset_state(self):\n+        \"\"\"Resets all of the metric state variables.\n+\n+        This function is called between epochs when a metric is evaluated during\n+        training. It's also called when the metric is initialized.\n+        \"\"\"\n+        raise NotImplementedError(\"Subclasses should implement `reset_state`\")\n+\n+    def result(self):\n+        \"\"\"Computes and returns the scalar metric value.\n+\n+        **Note:** This function is executed outside of the TensorFlow graph\n+         on the CPU host. This means any TensorFlow ops run in this method\n+         are run eagerly.\n+\n+        Result computation is an idempotent operation that simply calculates the\n+        metric value using the state variables.\n+\n+        Returns:\n+            A Python scalar.\n+        \"\"\"\n+        raise NotImplementedError(\"Subclasses should implement `result`\")\n\n@@ -0,0 +1,145 @@\n+# Copyright 2023 The Keras Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for Keras PyMetric classes.\"\"\"\n+\n+\n+import tensorflow.compat.v2 as tf\n+\n+from keras import metrics\n+from keras.testing_infra import test_combinations\n+\n+\n+class KTrimmedMean(metrics.PyMetric):\n+    \"\"\"An example PyMetric which computes the trimmed mean of `y_pred`.\"\"\"\n+\n+    def __init__(self, k=0.1, name=\"k_trimmed_mean\", **kwargs):\n+        super().__init__(name=name, **kwargs)\n+        self.k = k\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        y_true = y_true.numpy()\n+\n+        if sample_weight is not None:\n+            y_true *= sample_weight.numpy()\n+\n+        # Insert y_pred into our values list (keeping the list sorted)\n+        index = 0\n+        for i, element in enumerate(self.values):\n+            if y_true > element:\n+                index = i\n+                break\n+        self.values = self.values[:index] + [y_true] + self.values[index:]\n+\n+    def reset_state(self):\n+        self.values = []\n+\n+    def result(self):\n+        k = int(self.k * len(self.values))\n+        return tf.reduce_mean(self.values[k:-k])\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"k\": self.k})\n+        return config\n+\n+\n+class Mean(metrics.PyMetric):\n+    \"\"\"An example PyMetric which computes the mean of `y_pred`.\"\"\"\n+\n+    def __init__(self, name=\"mean\", **kwargs):\n+        super().__init__(name=name, **kwargs)\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        self.values.append(y_true)\n+\n+    def reset_state(self):\n+        self.values = []\n+\n+    def result(self):\n+        return tf.reduce_mean(tf.concat(self.values, axis=0))\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class PyMetricsTest(tf.test.TestCase):\n+    def test_config(self):\n+        ktm_object = KTrimmedMean(name=\"ktm\", k=0.2, dtype=tf.float16)\n+        self.assertEqual(ktm_object.name, \"ktm\")\n+        self.assertEqual(ktm_object.k, 0.2)\n+        self.assertEqual(ktm_object.dtype, tf.float16)\n+\n+        # Check save and restore config\n+        ktm_object2 = KTrimmedMean.from_config(ktm_object.get_config())\n+        self.assertEqual(ktm_object2.name, \"ktm\")\n+        self.assertEqual(ktm_object.k, 0.2)\n+        self.assertEqual(ktm_object2.dtype, tf.float16)\n+\n+    def test_unweighted(self):\n+        ktm_object = KTrimmedMean(k=0.2)\n+\n+        for y_true in [-100, -10, 1, 2, 3, 4, 5, 6, 14, 9001]:\n+            self.evaluate(\n+                ktm_object.update_state(\n+                    tf.constant(y_true, dtype=tf.float32),\n+                    y_pred=tf.constant(0, dtype=tf.float32),\n+                )\n+            )\n+\n+        result = ktm_object.result()\n+        self.assertEqual(3.5, self.evaluate(result))\n+\n+    def test_weighted(self):\n+        ktm_object = KTrimmedMean(k=0.2)\n+\n+        for y_true in [-100, -10, 1, 2, 3, 4, 5, 6, 14, 9001]:\n+            self.evaluate(\n+                ktm_object.update_state(\n+                    tf.constant(y_true, dtype=tf.float32),\n+                    y_pred=tf.constant(0, dtype=tf.float32),\n+                    sample_weight=tf.constant(2, dtype=tf.float32),\n+                )\n+            )\n+\n+        result = ktm_object.result()\n+        self.assertEqual(7, self.evaluate(result))\n+\n+    def test_state_stored_on_cpu_host(self):\n+        with tf.device(\"/device:GPU:0\"):\n+            mean_obj = Mean()\n+\n+            y_true_0 = tf.constant([0, 1, 2], dtype=tf.float32)\n+            y_true_1 = tf.constant([3, 4], dtype=tf.float32)\n+            self.evaluate(\n+                mean_obj.update_state(\n+                    y_true=y_true_0, y_pred=tf.constant(0, dtype=tf.float32)\n+                )\n+            )\n+            self.evaluate(\n+                mean_obj.update_state(\n+                    y_true=y_true_1, y_pred=tf.constant(0, dtype=tf.float32)\n+                )\n+            )\n+\n+        self.assertEqual(2, self.evaluate(mean_obj.result()))\n+\n+        if not tf.executing_eagerly():\n+            self.assertEndsWith(y_true_0.device, \"/device:GPU:0\")\n+            self.assertEndsWith(y_true_1.device, \"/device:GPU:0\")\n+\n+        self.assertEndsWith(mean_obj.values[0].device, \"/device:CPU:0\")\n+        self.assertEndsWith(mean_obj.values[1].device, \"/device:CPU:0\")\n+\n+\n+if __name__ == \"__main__\":\n+    tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e226091eeb8f268954c0b98b66b0a4da36d8f3e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 3144 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -329,6 +329,7 @@ def Head(num_classes=1000, classifier_activation=None, name=None):\n \n     Args:\n       num_classes: number of classes for Dense layer\n+      classifier_activation: activation function for the Dense layer\n       name: name prefix\n \n     Returns:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3856fed93f43f7c009dd2ffb143ad52d42a36ce2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 429 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -191,6 +191,13 @@ class ApplicationsTest(tf.test.TestCase, parameterized.TestCase):\n         )\n         self.assertShapeEqual(output_shape, (None, last_dim))\n \n+    @parameterized.parameters(MODEL_LIST)\n+    def test_application_classifier_activation(self, app):\n+        last_activation = _get_last_layer_activation(\n+            lambda: app(weights=None, include_top=True, classifier_activation=\"softmax\")\n+        )\n+        self.assertEqual(last_activation, \"softmax\")\n+\n     @parameterized.parameters(*MODEL_LIST_NO_NASNET)\n     def test_application_variable_input_channels(self, app, last_dim):\n         if backend.image_data_format() == \"channels_first\":\n@@ -235,5 +242,11 @@ def _get_output_shape(model_fn):\n     return model.output_shape\n \n \n+def _get_last_layer_activation(model_fn):\n+    model = model_fn()\n+    return model.layers[-1].activation.__name__\n+\n+\n if __name__ == \"__main__\":\n     tf.test.main()\n+    \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
