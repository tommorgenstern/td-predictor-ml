{"custom_id": "scrapy#48bfd3fe4b11eee1cd65bfd011d2af9984da283e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 60 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -34,7 +34,7 @@ CLUSTER_MASTER_PORT = 8790\n CLUSTER_MASTER_ENABLED = 0\n CLUSTER_MASTER_POLL_INTERVAL = 60\n CLUSTER_MASTER_NODES = {}\n-CLUSTER_MASTER_CACHEFILE = \"\"\n+CLUSTER_MASTER_STATEFILE = \"\"\n \n CLUSTER_WORKER_ENABLED = 0\n CLUSTER_WORKER_MAXPROC = 4\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1ffe64dab30b20361a59faabde30539da61dd92d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 6 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 79 | Churn Cumulative: 79 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import datetime\n+import decimal\n import unittest\n \n from scrapy.contrib_exp.newitem import *\n@@ -22,3 +24,80 @@ class NewItemTest(unittest.TestCase):\n         i.name = 'name'\n         i.names = ['name1', 'name2']\n         assert i.names == ['name1', 'name2']\n+\n+\n+class NewItemFieldsTest(unittest.TestCase):\n+    def test_boolean_field(self):\n+        class TestItem(Item):\n+            field = BooleanField()\n+\n+        i = TestItem()\n+\n+        i.field = True\n+        assert i.field == True\n+        i.field = 1\n+        assert i.field == True\n+\n+        i.field = False\n+        assert i.field == False\n+        i.field = 0\n+        assert i.field == False\n+\n+    def test_date_field(self):\n+        class TestItem(Item):\n+            field = DateField()\n+\n+        i = TestItem()\n+\n+        today = datetime.date.today()\n+        i.field = today\n+        assert i.field == today\n+ \n+        i.field = '2009-05-21'\n+        assert i.field == datetime.date(2009, 5, 21)\n+\n+    def test_decimal_field(self):\n+        class TestItem(Item):\n+            field = DecimalField()\n+\n+        i = TestItem()\n+\n+        i.field = decimal.Decimal('3.14')\n+        assert i.field == decimal.Decimal('3.14')\n+\n+        i.field = '3.14'\n+        assert i.field == decimal.Decimal('3.14')\n+\n+    def test_float_field(self):\n+        class TestItem(Item):\n+            field = FloatField()\n+\n+        i = TestItem()\n+\n+        i.field = 3.14\n+        assert i.field == 3.14\n+\n+        i.field = '3.14'\n+        assert i.field == 3.14\n+\n+    def test_integer_field(self):\n+        class TestItem(Item):\n+            field = IntegerField()\n+\n+        i = TestItem()\n+\n+        i.field = 3\n+        assert i.field == 3\n+\n+        i.field = '3'\n+        assert i.field == 3\n+\n+    def test_sting_field(self):\n+        class TestItem(Item):\n+            field = StringField()\n+\n+        i = TestItem()\n+\n+        i.field = 'hello'\n+        assert i.field == 'hello'\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#55d922a4b0a0b3ca3424326345ab8be244df16d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 34/34 | Churn Δ: 1 | Churn Cumulative: 1 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -51,6 +51,7 @@ class MultiValuedField(Field):\n \n class BooleanField(Field):\n     def to_python(self, value):\n+        if value is None: return False\n         if value in (True, False): return value\n         if value in ('t', 'True', '1'): return True\n         if value in ('f', 'False', '0'): return False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0cc289ac84cdbe2bbf375f663b2671d8507d5cbc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 6 | Churn Cumulative: 7 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -51,11 +51,7 @@ class MultiValuedField(Field):\n \n class BooleanField(Field):\n     def to_python(self, value):\n-        if value is None: return False\n-        if value in (True, False): return value\n-        if value in ('t', 'True', '1'): return True\n-        if value in ('f', 'False', '0'): return False\n-        raise FieldValueError(\"This value must be either True or False.\")\n+        return bool(value)\n \n \n ansi_date_re = re.compile(r'^\\d{4}-\\d{1,2}-\\d{1,2}$')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d5f0cae77630fbe70d978e4e0d739856f1dcf090", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 22 | Churn Cumulative: 29 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -11,15 +11,12 @@ class FieldValueError(Exception):\n     pass\n \n \n-class Field(object):\n+class BaseField(object):\n     def __init__(self, required=False, default=None):\n         self.required = required\n         self.default = default or self.to_python(None)\n \n     def assign(self, value):\n-        if hasattr(value, '__iter__'):\n-            return self.to_python(self.deiter(value))\n-        else:\n         return self.to_python(value)\n \n     def to_python(self, value):\n@@ -29,12 +26,20 @@ class Field(object):\n         \"\"\"\n         return value\n \n+\n+class Field(BaseField):\n+    def assign(self, value):\n+        if hasattr(value, '__iter__'):\n+            return self.to_python(self.deiter(value))\n+        else:\n+            return self.to_python(value)\n+\n     def deiter(self, value):\n         \"Converts the input iterable into a single value.\"\n         return ' '.join(value)\n \n \n-class MultiValuedField(Field):\n+class MultiValuedField(BaseField):\n     def __init__(self, field_type, required=False, default=None):\n         self._field = field_type()\n         super(MultiValuedField, self).__init__(required, default)\n@@ -45,9 +50,6 @@ class MultiValuedField(Field):\n         else:\n             return [self._field.to_python(v) for v in value]\n \n-    def deiter(self, value):\n-        return value\n-\n \n class BooleanField(Field):\n     def to_python(self, value):\n\n@@ -1,5 +1,5 @@\n from scrapy.item import ScrapedItem\n-from scrapy.contrib_exp.newitem.fields import Field\n+from scrapy.contrib_exp.newitem.fields import BaseField\n \n \n class ItemMeta(type):\n@@ -8,7 +8,7 @@ class ItemMeta(type):\n         cls = type.__new__(meta, class_name, bases, attrs)\n         cls.fields = cls.fields.copy()\n         for n, v in attrs.items():\n-            if isinstance(v, Field):\n+            if isinstance(v, BaseField):\n                 cls.fields[n] = v\n \n         return cls\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c03e2460020ad698ab9aa44944a224264a98aa96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 10/9 | Churn Δ: 66 | Churn Cumulative: 170 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 0.3617021276595745\n\nDIFF:\n@@ -1,10 +1,11 @@\n import datetime\n import decimal\n import re\n+import time\n \n \n-__all__ = ['MultiValuedField', 'BooleanField', 'DateField', 'DecimalField',\n-           'FloatField', 'IntegerField', 'StringField']\n+__all__ = ['MultiValuedField', 'BooleanField', 'DateField', 'DateTimeField',\n+           'DecimalField', 'FloatField', 'IntegerField', 'StringField']\n \n \n class FieldValueError(Exception):\n@@ -79,6 +80,43 @@ class DateField(Field):\n             raise FieldValueError(\"Invalid date: %s\" % str(e))\n \n \n+class DateTimeField(Field):\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        if isinstance(value, datetime.datetime):\n+            return value\n+        if isinstance(value, datetime.date):\n+            return datetime.datetime(value.year, value.month, value.day)\n+\n+        # Attempt to parse a datetime:\n+        value = str(value)\n+        # split usecs, because they are not recognized by strptime.\n+        if '.' in value:\n+            try:\n+                value, usecs = value.split('.')\n+                usecs = int(usecs)\n+            except ValueError:\n+                raise FieldValueError('Enter a valid date/time in YYYY-MM-DD HH:MM[:ss[.uuuuuu]] format.')\n+        else:\n+            usecs = 0\n+        kwargs = {'microsecond': usecs}\n+        try: # Seconds are optional, so try converting seconds first.\n+            return datetime.datetime(*time.strptime(value, '%Y-%m-%d %H:%M:%S')[:6],\n+                                     **kwargs)\n+\n+        except ValueError:\n+            try: # Try without seconds.\n+                return datetime.datetime(*time.strptime(value, '%Y-%m-%d %H:%M')[:5],\n+                                         **kwargs)\n+            except ValueError: # Try without hour/minutes/seconds.\n+                try:\n+                    return datetime.datetime(*time.strptime(value, '%Y-%m-%d')[:3],\n+                                             **kwargs)\n+                except ValueError:\n+                    raise FieldValueError('Enter a valid date/time in YYYY-MM-DD HH:MM[:ss[.uuuuuu]] format.')\n+\n+\n class DecimalField(Field):\n     def to_python(self, value):\n         if value is None:\n\n@@ -56,6 +56,30 @@ class NewItemFieldsTest(unittest.TestCase):\n         i.field = '2009-05-21'\n         assert i.field == datetime.date(2009, 5, 21)\n \n+    def test_datetime_field(self):\n+        class TestItem(Item):\n+            field = DateTimeField()\n+\n+        i = TestItem()\n+\n+        dt_today = datetime.datetime.today()\n+        i.field = dt_today\n+        assert i.field == dt_today\n+\n+        d_today = datetime.date.today()\n+        i.field = d_today\n+        assert i.field == datetime.datetime(d_today.year, d_today.month,\n+                                            d_today.day)\n+ \n+        i.field = '2009-05-21 11:08:10'\n+        assert i.field == datetime.datetime(2009, 5, 21, 11, 8, 10)\n+\n+        i.field = '2009-05-21 11:08'\n+        assert i.field == datetime.datetime(2009, 5, 21, 11, 8)\n+\n+        i.field = '2009-05-21'\n+        assert i.field == datetime.datetime(2009, 5, 21)\n+\n     def test_decimal_field(self):\n         class TestItem(Item):\n             field = DecimalField()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#39558441155d2d1bec834881ea6ff9fad3c89285", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 9 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 88 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,10 +8,6 @@ __all__ = ['MultiValuedField', 'BooleanField', 'DateField', 'DateTimeField',\n            'DecimalField', 'FloatField', 'IntegerField', 'StringField']\n \n \n-class FieldValueError(Exception):\n-    pass\n-\n-\n class BaseField(object):\n     def __init__(self, required=False, default=None):\n         self.required = required\n@@ -70,14 +66,13 @@ class DateField(Field):\n             return value\n \n         if not ansi_date_re.search(value):\n-            raise FieldValueError(\n-                \"Enter a valid date in YYYY-MM-DD format.\")\n+            raise ValueError(\"Enter a valid date in YYYY-MM-DD format.\")\n \n         year, month, day = map(int, value.split('-'))\n         try:\n             return datetime.date(year, month, day)\n         except ValueError, e:\n-            raise FieldValueError(\"Invalid date: %s\" % str(e))\n+            raise ValueError(\"Invalid date: %s\" % str(e))\n \n \n class DateTimeField(Field):\n@@ -97,7 +92,7 @@ class DateTimeField(Field):\n                 value, usecs = value.split('.')\n                 usecs = int(usecs)\n             except ValueError:\n-                raise FieldValueError('Enter a valid date/time in YYYY-MM-DD HH:MM[:ss[.uuuuuu]] format.')\n+                raise ValueError('Enter a valid date/time in YYYY-MM-DD HH:MM[:ss[.uuuuuu]] format.')\n         else:\n             usecs = 0\n         kwargs = {'microsecond': usecs}\n@@ -114,7 +109,7 @@ class DateTimeField(Field):\n                     return datetime.datetime(*time.strptime(value, '%Y-%m-%d')[:3],\n                                              **kwargs)\n                 except ValueError:\n-                    raise FieldValueError('Enter a valid date/time in YYYY-MM-DD HH:MM[:ss[.uuuuuu]] format.')\n+                    raise ValueError('Enter a valid date/time in YYYY-MM-DD HH:MM[:ss[.uuuuuu]] format.')\n \n \n class DecimalField(Field):\n@@ -124,7 +119,7 @@ class DecimalField(Field):\n         try:\n             return decimal.Decimal(value)\n         except decimal.InvalidOperation:\n-            raise FieldValueError(\"This value must be a decimal number.\")\n+            raise ValueError(\"This value must be a decimal number.\")\n \n \n class FloatField(Field):\n@@ -134,7 +129,7 @@ class FloatField(Field):\n         try:\n             return float(value)\n         except (TypeError, ValueError):\n-            raise FieldValueError(\"This value must be a float.\")\n+            raise ValueError(\"This value must be a float.\")\n \n \n class IntegerField(Field):\n@@ -144,7 +139,7 @@ class IntegerField(Field):\n         try:\n             return int(value)\n         except (TypeError, ValueError):\n-            raise FieldValueError(\"This value must be an integer.\")\n+            raise ValueError(\"This value must be an integer.\")\n \n \n class StringField(Field):\n@@ -153,5 +148,5 @@ class StringField(Field):\n             return value\n         if value is None:\n             return value\n-        raise FieldValueError(\"This field must be a string.\")\n+        raise ValueError(\"This field must be a string.\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4efcf78a4a7c48774c29e7917712b030eccc5536", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 22/22 | Churn Δ: 4 | Churn Cumulative: 4 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -77,8 +77,8 @@ def sign_request(req, accesskey, secretkey):\n \n class AWSMiddleware(object):\n     def __init__(self):\n-        self.access_key = settings['AWS_ACCESS_KEY_ID']\n-        self.secret_key = settings['AWS_SECRET_ACCESS_KEY']\n+        self.access_key = settings['AWS_ACCESS_KEY_ID'] or os.environ.get('AWS_ACCESS_KEY_ID')\n+        self.secret_key = settings['AWS_SECRET_ACCESS_KEY'] or os.environ.get('AWS_SECRET_ACCESS_KEY')\n \n     def process_request(self, request, spider):\n         if spider.domain_name == 's3.amazonaws.com' \\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dae0b1973b6881edb98da77224e324f03c73ae8a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 5 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import os\n import re\n import time\n import hmac\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#19f2992b2604203c0c5abc02fb7525ec4e0e8384", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 20 | Churn Cumulative: 20 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,8 +10,7 @@ from scrapy.store.db import DomainDataHistory\n from scrapy.utils.db import mysql_connect, parse_uri, URIValidationError\n from scrapy.conf import settings\n \n-class ConnectionTestCase(unittest.TestCase):\n-    \"\"\" Test connection wrapper \"\"\"\n+class MySqlTestCase(unittest.TestCase):\n     test_db = settings.get('TEST_SCRAPING_DB')\n \n     def setUp(self):\n@@ -25,6 +24,8 @@ class ConnectionTestCase(unittest.TestCase):\n         except MySQLdb.OperationalError:\n             raise unittest.SkipTest(\"Test database not available at: %s\" % self.test_db)\n \n+class ConnectionTestCase(MySqlTestCase):\n+    \"\"\" Test connection wrapper \"\"\"\n     def test_parse_uri(self):\n         self.assertRaises(URIValidationError, parse_uri, [self.test_db])\n         self.assertRaises(URIValidationError, parse_uri, self.test_db.replace('mysql:', 'gopher:'))\n@@ -41,21 +42,8 @@ class ConnectionTestCase(unittest.TestCase):\n         self.assertTrue(isinstance(mysql_connect(self.test_db), MySQLdb.connection))\n         self.assertTrue(isinstance(mysql_connect(parse_uri(self.test_db)), MySQLdb.connection))\n \n-class ProductComparisonTestCase(unittest.TestCase):\n+class ProductComparisonTestCase(MySqlTestCase):\n     \"\"\" Test product comparison functions \"\"\"\n-    test_db = settings.get('TEST_SCRAPING_DB')\n-\n-    def setUp(self):\n-        if not self.test_db:\n-            raise unittest.SkipTest(\"Missing TEST_SCRAPING_DB setting\")\n-\n-        try:\n-            mysql_connect(self.test_db)\n-        except ImportError:\n-            raise unittest.SkipTest(\"MySQLdb module not available\")\n-        except MySQLdb.OperationalError:\n-            raise unittest.SkipTest(\"Test database not available at: %s\" % self.test_db)\n-\n     def test_domaindatahistory(self):\n         ddh = DomainDataHistory(self.test_db, 'domain_data_history')\n         c = ddh.mysql_conn.cursor()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b1dad251ae9602fa7c36e87ab1940f7b7508248a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 21 | Files Changed: 3 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 49 | Churn Cumulative: 121 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -44,6 +44,11 @@ COOKIES_DEBUG = False\n \n DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n \n+DEFAULT_REQUEST_HEADERS = {\n+    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n+    'Accept-Language': 'en',\n+}\n+\n DEFAULT_SPIDER = None\n \n DEPTH_LIMIT = 0\n@@ -63,7 +68,7 @@ DOWNLOADER_MIDDLEWARES_BASE = {\n     'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n     'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n     'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n-    'scrapy.contrib.downloadermiddleware.common.CommonMiddleware': 550,\n+    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n     'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n     'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n     'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 800,\n@@ -138,9 +143,6 @@ PRIORITIZER = 'scrapy.core.prioritizers.RandomPrioritizer'\n \n REDIRECTMIDDLEWARE_MAX_TIMES = 20 # uses Firefox default setting\n \n-REQUEST_HEADER_ACCEPT = 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\n-REQUEST_HEADER_ACCEPT_LANGUAGE = 'en'\n-\n REQUESTS_QUEUE_SIZE = 0\n REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n \n\n@@ -1,22 +1,10 @@\n-\"\"\"\n-Common downloader middleware\n \n-See documentation in docs/ref/downloader-middleware.rst\n-\"\"\"\n+import warnings\n \n-from scrapy.conf import settings\n+from scrapy.contrib.downloadermiddleware.defaultheaders import DefaultHeadersMiddleware\n \n-class CommonMiddleware(object):\n+class CommonMiddleware(DefaultHeadersMiddleware):\n \n     def __init__(self):\n-        self.header_accept = settings.get('REQUEST_HEADER_ACCEPT')\n-        self.header_accept_language = settings.get('REQUEST_HEADER_ACCEPT_LANGUAGE')\n-\n-    def process_request(self, request, spider):\n-        request.headers.setdefault('Accept', self.header_accept)\n-        request.headers.setdefault('Accept-Language', self.header_accept_language)\n-        if request.method == 'POST':\n-            request.headers.setdefault('Content-Type', 'application/x-www-form-urlencoded')\n-            if request.body:\n-                request.headers.setdefault('Content-Length', '%d' % len(request.body))\n-\n+        warnings.warn(\"scrapy.contrib.downloadermiddleware.common.CommonMiddleware has been replaced by scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware\")\n+        DefaultHeadersMiddleware.__init__(self)\n\n@@ -0,0 +1,17 @@\n+\"\"\"\n+DefaultHeaders downloader middleware\n+\n+See documentation in docs/ref/downloader-middleware.rst\n+\"\"\"\n+\n+from scrapy.conf import settings\n+\n+class DefaultHeadersMiddleware(object):\n+\n+    def __init__(self):\n+        self.default_headers = settings.get('DEFAULT_REQUEST_HEADERS')\n+\n+    def process_request(self, request, spider):\n+        for k, v in self.default_headers.iteritems():\n+            if v:\n+                request.headers.setdefault(k, v)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#89950af834e6e19b3b95cc0b788616fc3eb6d638", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 29/29 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -88,7 +88,7 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n             er = \", error=%s\" % str(status.value)\n         log.msg(\"ClusterWorker: finished domain=%s status=%s pid=%d log=%s%s\" % (self.domain, st, self.pid, self.logfile, er))\n         del self.worker.running[self.domain]\n-        del self.worker.crawlers[self.pid]\n+        self.worker.crawlers.pop(self.pid, None)\n         self.worker.update_master(self.domain, st)\n \n class ClusterWorker(pb.Root):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7eaa3ed24dbd5ef3062017233ced2256f4d31e53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 94 | Lines Deleted: 117 | Files Changed: 12 | Hunks: 62 | Methods Changed: 27 | Complexity Δ (Sum/Max): 63/16 | Churn Δ: 211 | Churn Cumulative: 496 | Contributors (this commit): 2 | Commits (past 90d): 31 | Contributors (cumulative): 14 | DMM Complexity: 0.5\n\nDIFF:\n@@ -136,7 +136,8 @@ NEWSPIDER_MODULE = ''\n \n PRIORITIZER = 'scrapy.core.prioritizers.RandomPrioritizer'\n \n-REDIRECTMIDDLEWARE_MAX_TIMES = 20 # uses Firefox default setting\n+REDIRECT_MAX_METAREFRESH_DELAY = 100\n+REDIRECT_MAX_TIMES = 20 # uses Firefox default setting\n \n REQUEST_HEADER_ACCEPT = 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\n REQUEST_HEADER_ACCEPT_LANGUAGE = 'en'\n@@ -146,7 +147,7 @@ REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n \n # contrib.middleware.retry.RetryMiddleware default settings\n RETRY_TIMES = 2 # initial response + 2 retries = 3 requests\n-RETRY_HTTP_CODES = ['500', '503', '504', '400', '408', '200']\n+RETRY_HTTP_CODES = ['500', '503', '504', '400', '408']\n \n \n ROBOTSTXT_OBEY = False\n\n@@ -6,7 +6,6 @@ from pydispatch import dispatcher\n from scrapy.core import signals\n from scrapy.http import Response\n from scrapy.http.cookies import CookieJar\n-from scrapy.core.exceptions import HttpException\n from scrapy.conf import settings\n from scrapy import log\n \n@@ -44,11 +43,6 @@ class CookiesMiddleware(object):\n \n         return response\n \n-    # cookies should be set on non-200 responses too\n-    def process_exception(self, request, exception, spider):\n-        if isinstance(exception, HttpException):\n-            self.process_response(request, exception.response, spider)\n-\n     def domain_closed(self, domain):\n         self.jars.pop(domain, None)\n \n\n@@ -1,13 +1,17 @@\n from scrapy.core.exceptions import HttpException\n+from scrapy.utils.response import response_status_message\n \n class ErrorPagesMiddleware(object):\n-    \"\"\"This middleware allows the spiders to receive error (non 200) responses,\n-    the same way the receive normal responses\"\"\"\n+    \"\"\"This middleware filters out responses with status code others than 2XX\n+    or defined in spider handle_httpstatus_list attribute.\n \n-    def process_exception(self, request, exception, spider):\n-        if isinstance(exception, HttpException):\n-            statuses = getattr(spider, 'handle_httpstatus_list', None)\n-            httpstatus = exception.response.status\n-            if statuses and httpstatus in statuses:\n-                return exception.response\n+    TODO: move this mw to spidermiddleware and remove me\n+    \"\"\"\n+\n+    def process_response(self, request, response, spider):\n+        status = response.status\n+        if 200 <= status < 300 or status in getattr(spider, 'handle_httpstatus_list', []):\n+            return response\n+        else:\n+            raise HttpException(status, None, response)\n \n\n@@ -10,12 +10,13 @@ from pydispatch import dispatcher\n from scrapy.core import signals\n from scrapy import log\n from scrapy.http import Response, Headers\n-from scrapy.core.exceptions import NotConfigured, HttpException, IgnoreRequest\n+from scrapy.core.exceptions import NotConfigured, IgnoreRequest\n from scrapy.core.downloader.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.http import headers_dict_to_raw, headers_raw_to_dict\n from scrapy.conf import settings\n \n+\n class HttpCacheMiddleware(object):\n     def __init__(self):\n         if not settings['HTTPCACHE_DIR']:\n@@ -33,37 +34,25 @@ class HttpCacheMiddleware(object):\n \n         key = request_fingerprint(request)\n         domain = spider.domain_name\n+\n         try:\n             response = self.cache.retrieve_response(domain, key)\n         except:\n             log.msg(\"Corrupt cache for %s\" % request.url, log.WARNING)\n             response = False\n+\n         if response:\n-            if not 200 <= int(response.status) < 300:\n-                raise HttpException(response.status, None, response)\n             return response\n         elif self.ignore_missing:\n             raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n \n     def process_response(self, request, response, spider):\n-        if not is_cacheable(request):\n-            return response\n-\n-        if isinstance(response, Response) and not response.meta.get('cached'):\n+        if is_cacheable(request):\n             key = request_fingerprint(request)\n-            domain = spider.domain_name\n-            self.cache.store(domain, key, request, response)\n+            self.cache.store(spider.domain_name, key, request, response)\n \n         return response\n \n-    def process_exception(self, request, exception, spider):\n-        if not is_cacheable(request):\n-            return\n-\n-        if isinstance(exception, HttpException) and isinstance(exception.response, Response):\n-            key = request_fingerprint(request)\n-            domain = spider.domain_name\n-            self.cache.store(domain, key, request, exception.response)\n \n def is_cacheable(request):\n     return request.url.scheme in ['http', 'https']\n\n@@ -1,41 +1,33 @@\n from scrapy import log\n-from scrapy.core.exceptions import HttpException\n from scrapy.utils.url import urljoin_rfc as urljoin\n from scrapy.utils.response import get_meta_refresh\n from scrapy.conf import settings\n \n-# some sites use meta-refresh for redirecting to a session expired page, so we\n-# restrict automatic redirection to a maximum delay (in number of seconds)\n-META_REFRESH_MAXSEC = 100\n-MAX_REDIRECT_LOOP = 10\n \n class RedirectMiddleware(object):\n+    \"\"\"Handle redirection of requests based on response status and meta-refresh html tag\"\"\"\n+\n     def __init__(self):\n-        self.max_redirect_times = settings.getint('REDIRECTMIDDLEWARE_MAX_TIMES')\n-\n-    def process_exception(self, request, exception, spider):\n-        if not isinstance(exception, HttpException):\n-            return\n+        self.max_metarefresh_delay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY')\n+        self.max_redirect_times = settings.getint('REDIRECT_MAX_TIMES')\n \n+    def process_response(self, request, response, spider):\n         domain = spider.domain_name\n-        status = exception.status\n-        response = exception.response\n \n-        if status in [302, 303] and 'Location' in response.headers:\n+        if response.status in [302, 303] and 'Location' in response.headers:\n             redirected_url = urljoin(request.url, response.headers['location'])\n             redirected = request.replace(url=redirected_url, method='GET', body='')\n             redirected.headers.pop('Content-Type', None)\n             redirected.headers.pop('Content-Length', None)\n-            return self._redirect(redirected, request, spider, status)\n+            return self._redirect(redirected, request, spider, response.status)\n \n-        if status in [301, 307] and 'Location' in response.headers:\n+        if response.status in [301, 307] and 'Location' in response.headers:\n             redirected_url = urljoin(request.url, response.headers['location'])\n             redirected = request.replace(url=redirected_url)\n-            return self._redirect(redirected, request, spider, status)\n+            return self._redirect(redirected, request, spider, response.status)\n \n-    def process_response(self, request, response, spider):\n         interval, url = get_meta_refresh(response)\n-        if url and int(interval) < META_REFRESH_MAXSEC:\n+        if url and int(interval) < self.max_metarefresh_delay:\n             redirected = request.replace(url=urljoin(request.url, url))\n             return self._redirect(redirected, request, spider, 'meta refresh')\n \n\n@@ -19,46 +19,50 @@ About HTTP errors to consider:\n - You may want to remove 400 from RETRY_HTTP_CODES, if you stick to the HTTP\n   protocol. It's included by default because it's a common code used to\n   indicate server overload, which would be something we want to retry\n-- 200 is included by default (and shoudln't be removed) to check for partial\n-  downloads errors, which means the TCP connection has broken in the middle of\n-  a HTTP download\n \"\"\"\n \n from twisted.internet.error import TimeoutError as ServerTimeoutError, DNSLookupError, \\\n                                    ConnectionRefusedError, ConnectionDone, ConnectError, \\\n                                    ConnectionLost\n from twisted.internet.defer import TimeoutError as UserTimeoutError\n+from twisted.web.client import PartialDownloadError\n \n from scrapy import log\n-from scrapy.core.exceptions import HttpException\n from scrapy.utils.request import request_fingerprint\n+from scrapy.utils.response import response_status_message\n from scrapy.conf import settings\n \n class RetryMiddleware(object):\n \n     EXCEPTIONS_TO_RETRY = (ServerTimeoutError, UserTimeoutError, DNSLookupError,\n                            ConnectionRefusedError, ConnectionDone, ConnectError,\n-                           ConnectionLost)\n+                           ConnectionLost, PartialDownloadError)\n \n     def __init__(self):\n         self.max_retry_times = settings.getint('RETRY_TIMES')\n         self.retry_http_codes = map(int, settings.getlist('RETRY_HTTP_CODES'))\n \n-    def process_exception(self, request, exception, spider):\n-        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) \\\n-                or (isinstance(exception, HttpException) \\\n-                        and (int(exception.status) in self.retry_http_codes)):\n+    def process_response(self, request, response, spider):\n+        if response.status in self.retry_http_codes:\n+            reason = response_status_message(response.status)\n+            return self._retry(request, reason, spider) or response\n+        return response\n \n+    def process_exception(self, request, exception, spider):\n+        if isinstance(exception, self.EXCEPTIONS_TO_RETRY):\n+            return self._retry(request, exception, spider)\n+\n+    def _retry(self, request, reason, spider):\n         retries = request.meta.get('retry_times', 0) + 1\n \n         if retries <= self.max_retry_times:\n-                log.msg(\"Retrying %s (failed %d times): %s\" % (request, retries, exception),\n+            log.msg(\"Retrying %s (failed %d times): %s\" % (request, retries, reason),\n                     domain=spider.domain_name, level=log.DEBUG)\n             retryreq = request.copy()\n             retryreq.meta['retry_times'] = retries\n             retryreq.dont_filter = True\n             return retryreq\n         else:\n-                log.msg(\"Discarding %s (failed %d times): %s\" % (request, retries, exception),\n+            log.msg(\"Discarding %s (failed %d times): %s\" % (request, retries, reason),\n                     domain=spider.domain_name, level=log.DEBUG)\n \n\n@@ -1,4 +1,4 @@\n-from scrapy.core.exceptions import HttpException, NotConfigured\n+from scrapy.core.exceptions import NotConfigured\n from scrapy.stats import stats\n from scrapy.conf import settings\n \n@@ -36,8 +36,6 @@ class DownloaderStats(object):\n         stats.incpath('_global/downloader/exception_count')\n         stats.incpath('%s/downloader/exception_count' % spider.domain_name)\n         stats.incpath('%s/downloader/exception_type_count/%s' % (spider.domain_name, ex_class))\n-        if isinstance(exception, HttpException):\n-            self._inc_response_count(exception.response, spider.domain_name)\n \n     def _inc_response_count(self, response, domain):\n         stats.incpath('_global/downloader/response_count')\n\n@@ -16,7 +16,7 @@ except ImportError:\n from scrapy import optional_features\n from scrapy.core import signals\n from scrapy.http import Headers\n-from scrapy.core.exceptions import HttpException, NotSupported\n+from scrapy.core.exceptions import NotSupported\n from scrapy.utils.defer import defer_succeed\n from scrapy.conf import settings\n \n@@ -65,13 +65,7 @@ def create_factory(request, spider):\n         signals.send_catch_log(signal=signals.response_downloaded, sender='download_http', response=r, spider=spider)\n         return r\n \n-    def _on_success(body):\n-        response = _create_response(body)\n-        if response.status not in (200, 201, 202):\n-            raise HttpException(response.status, None, response)\n-        return response\n-\n-    factory.deferred.addCallbacks(_on_success)\n+    factory.deferred.addCallbacks(_create_response)\n     return factory\n \n def download_http(request, spider):\n\n@@ -3,7 +3,6 @@ from __future__ import with_statement\n from unittest import TestCase\n \n from scrapy.spider import spiders\n-from scrapy.core.exceptions import HttpException\n from scrapy.http import Response, Request\n from scrapy.contrib.downloadermiddleware.cookies import CookiesMiddleware\n \n@@ -35,21 +34,6 @@ class CookiesMiddlewareTest(TestCase):\n         assert self.mw.process_request(req2, self.spider) is None\n         self.assertEquals(req2.headers.get('Cookie'), \"C1=value1\")\n \n-    def test_http_exception(self):\n-        req = Request('http://scrapytest.org/')\n-        assert self.mw.process_request(req, self.spider) is None\n-        assert 'Cookie' not in req.headers\n-\n-        headers = {'Set-Cookie': 'C1=value1; path=/'}\n-        res = Response('http://scrapytest.org/', headers=headers)\n-        exc = HttpException(302, 'Redirect', res)\n-        assert self.mw.process_exception(req, exc, self.spider) is None\n-        #assert exc.response.cookies\n-\n-        req2 = Request('http://scrapytest.org/sub1/')\n-        assert self.mw.process_request(req2, self.spider) is None\n-        self.assertEquals(req2.headers.get('Cookie'), \"C1=value1\")\n-\n     def test_dont_merge_cookies(self):\n         # merge some cookies into jar\n         headers = {'Set-Cookie': 'C1=value1; path=/'}\n\n@@ -1,7 +1,6 @@\n import unittest\n \n from scrapy.contrib.downloadermiddleware.redirect import RedirectMiddleware\n-from scrapy.core.exceptions import HttpException\n from scrapy.dupefilter import dupefilter\n from scrapy.spider import spiders\n from scrapy.http import Request, Response, Headers\n@@ -22,28 +21,24 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         url = 'http://www.example.com/301'\n         url2 = 'http://www.example.com/redirected'\n         req = Request(url)\n-        hdr = Headers({'Location': [url2]})\n-        rsp = Response(url, headers=hdr)\n-        exc = HttpException('301', None, rsp)\n+        rsp = Response(url, headers={'Location': url2}, status=301)\n \n-        req2 = self.mw.process_exception(req, exc, self.spider)\n+        req2 = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req2, Request)\n         self.assertEqual(req2.url, url2)\n \n         # response without Location header but with status code is 3XX should be ignored\n         del rsp.headers['Location']\n-        assert self.mw.process_exception(req, exc, self.spider) is None\n+        assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n     def test_redirect_302(self):\n         url = 'http://www.example.com/302'\n         url2 = 'http://www.example.com/redirected2'\n         req = Request(url, method='POST', body='test', \n             headers={'Content-Type': 'text/plain', 'Content-length': '4'})\n-        hdr = Headers({'Location': [url2]})\n-        rsp = Response(url, headers=hdr)\n-        exc = HttpException('302', None, rsp)\n+        rsp = Response(url, headers={'Location': url2}, status=302)\n \n-        req2 = self.mw.process_exception(req, exc, self.spider)\n+        req2 = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req2, Request)\n         self.assertEqual(req2.url, url2)\n         self.assertEqual(req2.method, 'GET')\n@@ -56,7 +51,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n \n         # response without Location header but with status code is 3XX should be ignored\n         del rsp.headers['Location']\n-        assert self.mw.process_exception(req, exc, self.spider) is None\n+        assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n     def test_meta_refresh(self):\n         body = \"\"\"<html>\n@@ -82,24 +77,24 @@ class RedirectMiddlewareTest(unittest.TestCase):\n     def test_max_redirect_times(self):\n         self.mw.max_redirect_times = 1\n         req = Request('http://scrapytest.org/302')\n-        exc = HttpException('302', None, Response('http://www.scrapytest.org/302', headers={'Location': '/redirected'}))\n+        rsp = Response('http://scrapytest.org/302', headers={'Location': '/redirected'}, status=302)\n \n-        req = self.mw.process_exception(req, exc, self.spider)\n+        req = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req, Request)\n         assert 'redirect_times' in req.meta\n         self.assertEqual(req.meta['redirect_times'], 1)\n \n-        req = self.mw.process_exception(req, exc, self.spider)\n+        req = self.mw.process_response(req, rsp, self.spider)\n         self.assertEqual(req, None)\n \n     def test_ttl(self):\n         self.mw.max_redirect_times = 100\n         req = Request('http://scrapytest.org/302', meta={'redirect_ttl': 1})\n-        exc = HttpException('302', None, Response('http://www.scrapytest.org/302', headers={'Location': '/redirected'}))\n+        rsp = Response('http://www.scrapytest.org/302', headers={'Location': '/redirected'}, status=302)\n \n-        req = self.mw.process_exception(req, exc, self.spider)\n+        req = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req, Request)\n-        req = self.mw.process_exception(req, exc, self.spider)\n+        req = self.mw.process_response(req, rsp, self.spider)\n         self.assertEqual(req, None)\n \n if __name__ == \"__main__\":\n\n@@ -5,7 +5,6 @@ from twisted.internet.error import TimeoutError as ServerTimeoutError, DNSLookup\n                                    ConnectionLost\n \n from scrapy.contrib.downloadermiddleware.retry import RetryMiddleware\n-from scrapy.core.exceptions import HttpException\n from scrapy.spider import spiders\n from scrapy.http import Request, Response\n \n@@ -17,20 +16,31 @@ class RetryTest(unittest.TestCase):\n         self.mw = RetryMiddleware()\n         self.mw.max_retry_times = 2\n \n-    def test_process_exception_404(self):\n-        req404 = Request('http://www.scrapytest.org/404')\n-        exc404 = HttpException('404', None, Response('http://www.scrapytest.org/404', body=''))\n+    def test_404(self):\n+        req = Request('http://www.scrapytest.org/404')\n+        rsp = Response('http://www.scrapytest.org/404', body='', status=404)\n \n         # dont retry 404s\n-        req = self.mw.process_exception(req404, exc404, self.spider)\n-        self.assertTrue(req is None)\n+        assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n-    def test_process_exception_503(self):\n-        req503 = Request('http://www.scrapytest.org/503')\n-        exc503 = HttpException('503', None, Response('http://www.scrapytest.org/503', body=''))\n-        self._test_retry_exception(req503, exc503)\n+    def test_503(self):\n+        req = Request('http://www.scrapytest.org/503')\n+        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n \n-    def test_process_exception_twistederrors(self):\n+        # first retry\n+        req = self.mw.process_response(req, rsp, self.spider)\n+        assert isinstance(req, Request)\n+        self.assertEqual(req.meta['retry_times'], 1)\n+\n+        # second retry\n+        req = self.mw.process_response(req, rsp, self.spider)\n+        assert isinstance(req, Request)\n+        self.assertEqual(req.meta['retry_times'], 2)\n+\n+        # discard it\n+        assert self.mw.process_response(req, rsp, self.spider) is rsp\n+\n+    def test_twistederrors(self):\n         for exc in (ServerTimeoutError, DNSLookupError, ConnectionRefusedError, ConnectionDone, ConnectError, ConnectionLost):\n             req = Request('http://www.scrapytest.org/%s' % exc.__name__)\n             self._test_retry_exception(req, exc())\n\n@@ -4,6 +4,7 @@ scrapy.http.Response objects\n \"\"\"\n \n import re\n+from twisted.web import http\n from scrapy.http.response import Response\n \n def body_or_str(obj, unicode=True):\n@@ -36,3 +37,14 @@ def get_meta_refresh(response):\n         match = META_REFRESH_RE.search(response.body[0:4096])\n         response.cache['meta_refresh_url'] = match.groups() if match else (None, None)\n     return response.cache['meta_refresh_url']\n+\n+def response_status_message(status):\n+    \"\"\"Return status code plus status text descriptive message\n+\n+    >>> response_status_message(200)\n+    200 OK\n+\n+    >>> response_status_message(404)\n+    404 Not Found\n+    \"\"\"\n+    return '%s %s' % (status, http.responses.get(int(status)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e5bea67fd41b970602cab5c97f69ce2728a8759", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 50/41 | Churn Δ: 27 | Churn Cumulative: 27 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -62,20 +62,25 @@ class BaseImagesPipeline(MediaPipeline):\n #             ('270', (270, 270))\n      )\n \n-\n-\n     def media_downloaded(self, response, request, info):\n         mtype = self.MEDIA_NAME\n         referer = request.headers.get('Referer')\n \n-        if not response or not response.body:\n-            msg = 'Image (empty): Empty %s (no content) in %s referred in <%s>: no-content' \\\n+        if response.status != 200:\n+            msg = 'Image (http-error): Error downloading %s from %s referred in <%s>: %s' \\\n+                    % (mtype, request, referer, errmsg)\n+            log.msg(msg, level=log.WARNING, domain=info.domain)\n+            raise ImageException(msg)\n+\n+        if not response.body:\n+            msg = 'Image (empty-content): Empty %s from %s referred in <%s>: no-content' \\\n                     % (mtype, request, referer)\n             log.msg(msg, level=log.WARNING, domain=info.domain)\n             raise ImageException(msg)\n \n-        status = 'cached' if getattr(response, 'cached', False) else 'downloaded'\n-        msg = 'Image (%s): Downloaded %s from %s referred in <%s>' % (status, mtype, request, referer)\n+        status = 'cached' if 'cached' in response.flags else 'downloaded'\n+        msg = 'Image (%s): Downloaded %s from %s referred in <%s>' % \\\n+                (status, mtype, request, referer)\n         log.msg(msg, level=log.DEBUG, domain=info.domain)\n         self.inc_stats(info.domain, status)\n \n@@ -93,13 +98,8 @@ class BaseImagesPipeline(MediaPipeline):\n \n     def media_failed(self, failure, request, info):\n         referer = request.headers.get('Referer')\n-        if isinstance(failure.value, (HttpException, IgnoreRequest)):\n-            errmsg = str(failure.value)\n-        else:\n-            errmsg = str(failure)\n-\n-        msg = 'Image (http-error): Error downloading %s from %s referred in <%s>: %s' \\\n-                % (self.MEDIA_NAME, request, referer, errmsg)\n+        msg = 'Image (unknow-error): Error downloading %s from %s referred in <%s>: %s' \\\n+                % (self.MEDIA_NAME, request, referer, str(failure))\n         log.msg(msg, level=log.WARNING, domain=info.domain)\n         raise ImageException(msg)\n \n\n@@ -59,6 +59,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n \n     def stat_key(self, key, info):\n         def _onsuccess(response):\n+            if response.status == 200:\n                 checksum = response.headers['Etag'].strip('\"')\n                 last_modified = response.headers['Last-Modified']\n                 modified_tuple = rfc822.parsedate_tz(last_modified)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#85dbdf5789cdc3faf1e93359a31003b20818aab6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 25 | Files Changed: 6 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 48 | Churn Cumulative: 183 | Contributors (this commit): 2 | Commits (past 90d): 20 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -59,7 +59,6 @@ DOWNLOADER_MIDDLEWARES = {}\n DOWNLOADER_MIDDLEWARES_BASE = {\n     # Engine side\n     'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n-    'scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware': 200,\n     'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n     'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n     'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n@@ -170,6 +169,7 @@ SPIDER_MIDDLEWARES = {}\n \n SPIDER_MIDDLEWARES_BASE = {\n     # Engine side\n+    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n     'scrapy.contrib.itemsampler.ItemSamplerMiddleware': 100,\n     'scrapy.contrib.spidermiddleware.limit.RequestLimitMiddleware': 200,\n     'scrapy.contrib.spidermiddleware.restrict.RestrictMiddleware': 300,\n\n@@ -1,4 +1,4 @@\n-from scrapy.core.exceptions import HttpException\n+from scrapy.core.exceptions import IgnoreRequest\n from scrapy.utils.response import response_status_message\n \n class ErrorPagesMiddleware(object):\n@@ -13,5 +13,5 @@ class ErrorPagesMiddleware(object):\n         if 200 <= status < 300 or status in getattr(spider, 'handle_httpstatus_list', []):\n             return response\n         else:\n-            raise HttpException(status, None, response)\n+            raise IgnoreRequest(response_status_message(status))\n \n\n@@ -11,7 +11,6 @@ from scrapy import log\n from scrapy.stats import stats\n from scrapy.utils.misc import md5sum\n from scrapy.core.exceptions import DropItem, NotConfigured\n-from scrapy.core.exceptions import HttpException, IgnoreRequest\n from scrapy.conf import settings\n from scrapy.contrib.pipeline.media import MediaPipeline\n \n@@ -105,6 +104,9 @@ class BaseImagesPipeline(MediaPipeline):\n \n     def media_to_download(self, request, info):\n         def _onsuccess(result):\n+            if not result:\n+                return # returning None force download\n+\n             last_modified = result.get('last_modified', None)\n             if not last_modified:\n                 return # returning None force download\n\n@@ -0,0 +1,13 @@\n+\n+\n+class HttpErrorMiddleware(object):\n+    \"\"\"Filter out response outside of a range of valid status codes\n+\n+    This middleware filters out every response with status outside of the range 200<=status<300\n+    Spiders can add more exceptions using `handle_httpstatus_list` spider attribute.\n+    \"\"\"\n+\n+    def process_spider_input(self, response, spider):\n+        if not (200 <= response.status < 300 or \\\n+                response.status in getattr(spider, 'handle_httpstatus_list', [])):\n+            return [] # skip response\n\n@@ -16,7 +16,7 @@ from scrapy.conf import settings\n from scrapy.core import signals\n from scrapy.core.scheduler import Scheduler, SchedulerMiddlewareManager\n from scrapy.core.downloader import Downloader\n-from scrapy.core.exceptions import IgnoreRequest, HttpException, DontCloseDomain\n+from scrapy.core.exceptions import IgnoreRequest, DontCloseDomain\n from scrapy.http import Response, Request\n from scrapy.item import ScrapedItem\n from scrapy.item.pipeline import ItemPipelineManager\n@@ -330,6 +330,7 @@ class ExecutionEngine(object):\n         if self.debug_mode:\n             log.msg('Downloading %s' % request_info(request), log.DEBUG)\n         domain = spider.domain_name\n+        referer = request.headers.get('Referer', None)\n \n         def _on_success(response):\n             \"\"\"handle the result of a page download\"\"\"\n@@ -338,7 +339,7 @@ class ExecutionEngine(object):\n                 log.msg(\"Requested %s\" % request_info(request), level=log.DEBUG, domain=domain)\n             if isinstance(response, Response):\n                 response.request = request # tie request to obtained response\n-                log.msg(\"Crawled %s from <%s>\" % (response, request.headers.get('referer')), level=log.DEBUG, domain=domain)\n+                log.msg(\"Crawled %s from <%s>\" % (response, referer), level=log.DEBUG, domain=domain)\n                 return response\n             elif isinstance(response, Request):\n                 redirected = response # proper alias\n@@ -349,11 +350,7 @@ class ExecutionEngine(object):\n         def _on_error(_failure):\n             \"\"\"handle an error processing a page\"\"\"\n             ex = _failure.value\n-            if isinstance(ex, IgnoreRequest):\n-                log.msg(_failure.getErrorMessage(), level=log.DEBUG, domain=domain)\n-                return _failure\n-            referer = request.headers.get('Referer', None)\n-            errmsg = str(ex) if isinstance(ex, HttpException) else str(_failure)\n+            errmsg = str(_failure) if not isinstance(ex, IgnoreRequest) else _failure.getErrorMessage()\n             log.msg(\"Downloading <%s> from <%s>: %s\" % (request.url, referer, errmsg), log.ERROR, domain=domain)\n             return Failure(IgnoreRequest(str(ex)))\n \n\n@@ -21,20 +21,6 @@ class DontCloseDomain(Exception):\n     \"\"\"Request the domain not to be closed yet\"\"\"\n     pass\n \n-class HttpException(Exception):\n-    def __init__(self, status, message, response):\n-        if not message:\n-            from twisted.web import http\n-            message = http.responses.get(int(status))\n-\n-        self.status = int(status)\n-        self.message = message\n-        self.response = response\n-        Exception.__init__(self, status, message, response)\n-\n-    def __str__(self):\n-        return '%s %s' % (self.status, self.message)\n-\n # Items\n \n class DropItem(Exception):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
