{"custom_id": "scrapy#bc6eee71e325fb2707c5ddad007d94f158d73b2c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 408 | Lines Deleted: 191 | Files Changed: 8 | Hunks: 102 | Methods Changed: 58 | Complexity Δ (Sum/Max): 34/24 | Churn Δ: 599 | Churn Cumulative: 4033 | Contributors (this commit): 5 | Commits (past 90d): 17 | Contributors (cumulative): 26 | DMM Complexity: 0.4342105263157895\n\nDIFF:\n@@ -3,25 +3,22 @@ Extended types for IBL extraction\n \"\"\"\n from itertools import chain \n \n-from scrapy.contrib.ibl.extractors import text\n+from scrapy.contrib.ibl.extractors import text, html\n \n class FieldDescriptor(object):\n     \"\"\"description of a scraped attribute\"\"\"\n-    __slots__ = ('name', 'description', 'extractor', 'required', 'allow_markup')\n+    __slots__ = ('name', 'description', 'extractor', 'required')\n \n-    def __init__(self, name, description, extractor=text, required=False,\n-            allow_markup=False):\n+    def __init__(self, name, description, extractor=text, required=False):\n         self.name = name\n         self.description = description\n         self.extractor = extractor\n         self.required = required\n-        self.allow_markup = allow_markup\n     \n     @classmethod\n     def from_field(cls, name, field):\n         return cls(name, field.get('description'), \\\n-            field.get('ibl_extractor', text), field.get('required', False), \\\n-            field.get('allow_markup', False))\n+            field.get('ibl_extractor', text), field.get('required', False))\n \n     def __str__(self):\n         return \"FieldDescriptor(%s)\" % self.name\n\n@@ -4,16 +4,14 @@ Page objects\n This module contains objects representing pages and parts of pages (e.g. tokens\n and annotations) used in the instance based learning algorithm.\n \"\"\"\n+from itertools import chain\n from numpy import array, ndarray\n \n-from scrapy.contrib.ibl.htmlpage import HtmlTagType\n+from scrapy.contrib.ibl.htmlpage import HtmlTagType, HtmlPageRegion\n \n-class TokenType(object):\n+class TokenType(HtmlTagType):\n     \"\"\"constants for token types\"\"\"\n     WORD = 0\n-    OPEN_TAG = HtmlTagType.OPEN_TAG\n-    CLOSE_TAG = HtmlTagType.CLOSE_TAG\n-    NON_PAIRED_TAG = HtmlTagType.UNPAIRED_TAG\n \n class TokenDict(object):\n     \"\"\"Mapping from parse tokens to integers\n@@ -68,6 +66,36 @@ class TokenDict(object):\n         templates = [\"%s\", \"<%s>\", \"</%s>\", \"<%s/>\"]\n         return templates[tid >> 24] % self.find_token(tid)\n \n+class PageRegion(object):\n+    \"\"\"A region in a page, defined by a start and end index\"\"\"\n+\n+    __slots__ = ('start_index', 'end_index')\n+    \n+    def __init__(self, start, end):\n+        self.start_index = start\n+        self.end_index = end\n+        \n+    def __str__(self):\n+        return \"%s(%s, %s)\" % (self.__class__.__name__, self.start_index,\n+                self.end_index)\n+    \n+    def __repr__(self):\n+        return str(self)\n+\n+class FragmentedHtmlPageRegion(HtmlPageRegion):\n+    \"\"\"An HtmlPageRegion consisting of possibly non-contiguous sub-regions\"\"\"\n+    def __new__(cls, htmlpage, regions):\n+        text = u''.join(regions)\n+        return HtmlPageRegion.__new__(cls, htmlpage, text)\n+\n+    def __init__(self, htmlpage, regions):\n+        self.htmlpage = htmlpage\n+        self.regions = regions\n+    \n+    @property\n+    def parsed_fragments(self):\n+        return chain(*(r.parsed_fragments for r in self.regions))\n+\n class Page(object):\n     \"\"\"Basic representation of a page. This consists of a reference to a\n     dictionary of tokens and an array of raw token ids\n@@ -92,7 +120,8 @@ class TemplatePage(Page):\n         annotations = sorted(annotations, key=lambda x: x.end_index, reverse=True)\n         self.annotations = sorted(annotations, key=lambda x: x.start_index)\n         self.id = template_id\n-        self.ignored_regions = ignored_regions or []\n+        self.ignored_regions = [i if isinstance(i, PageRegion) else PageRegion(*i) \\\n+            for i in (ignored_regions or [])]\n         self.extra_required_attrs = set(extra_required or [])\n \n     def __str__(self):\n@@ -107,66 +136,53 @@ class ExtractionPage(Page):\n     \"\"\"Parsed data belonging to a web page upon which we wish to perform\n     extraction.\n     \"\"\"\n-    __slots__ = ('text', \n-            'token_start_indexes', # index in text of the start of a token\n-            'token_follow_indexes', # index in text of data following token\n-            'tag_attributes'        # a map from token index to tag attributes\n-        )\n+    __slots__ = ('htmlpage', 'token_page_indexes')\n \n-    def __init__(self, text, token_dict, page_tokens, token_start_indexes, \n-            token_follow_indexes, tag_attributes):\n+    def __init__(self, htmlpage, token_dict, page_tokens, token_page_indexes):\n+        \"\"\"Construct a new ExtractionPage\n+\n+        Arguments:\n+            `htmlpage`: The source HtmlPage\n+            `token_dict`: Token Dictionary used for tokenization\n+            `page_tokens': array of page tokens for matching\n+            `token_page_indexes`: indexes of each token in the parsed htmlpage\n+        \"\"\"\n         Page.__init__(self, token_dict, page_tokens)\n-        self.text = text\n-        self.token_start_indexes = token_start_indexes\n-        self.token_follow_indexes = token_follow_indexes\n-        self.tag_attributes = tag_attributes\n+        self.htmlpage = htmlpage\n+        self.token_page_indexes = token_page_indexes\n     \n-    def token_html(self, token_index):\n-        \"\"\"The raw html for a page token at the given index in the page_tokens\n-        list\n+    def htmlpage_region(self, start_token_index, end_token_index):\n+        \"\"\"The region in the HtmlPage corresonding to the area defined by\n+        the start_token_index and the end_token_index\n+\n+        This includes the tokens at the specified indexes\n         \"\"\"\n-        text_start = self.token_start_indexes[token_index]\n-        text_end = self.token_follow_indexes[token_index]\n-        return self.text[text_start:text_end]\n+        start = self.token_page_indexes[start_token_index]\n+        end = self.token_page_indexes[end_token_index]\n+        return self.htmlpage.subregion(start, end)\n \n-    def html_between_tokens(self, start_token_index, end_token_index):\n-        \"\"\"The raw html between the tokens at the specified indexes in the \n-        page_tokens list\n+    def htmlpage_region_inside(self, start_token_index, end_token_index):\n+        \"\"\"The region in the HtmlPage corresonding to the area between \n+        the start_token_index and the end_token_index. \n \n-        This assumes start_token_index <= end_token_index\n+        This excludes the tokens at the specified indexes\n         \"\"\"\n-        text_start = self.token_follow_indexes[start_token_index]\n-        text_end = self.token_start_indexes[end_token_index]\n-        return self.text[text_start:text_end]\n+        start = self.token_page_indexes[start_token_index] + 1\n+        end = self.token_page_indexes[end_token_index] - 1\n+        return self.htmlpage.subregion(start, end)\n     \n-    def text_between_tokens(self, start_token_index, end_token_index, \n-            tag_replacement=u' '):\n-        \"\"\"The text between the  the tokens at the specified indexes in the \n-        page_tokens list. Tags are replaced by tag_replacement (default one space \n-        character)\n-        \"\"\"\n-        return tag_replacement.join([self.text[\n-            self.token_follow_indexes[i]:self.token_start_indexes[i+1]] \\\n-            for i in xrange(start_token_index, end_token_index)])\n-\n-    def tag_attribute(self, token_index, attribute):\n-        \"\"\"The value of a tag attribute. The tag is identified by its \n-        corresponding token index.\n-\n-        If the tag or attribute is not present, None is returned\n-        \"\"\"\n-        return self.tag_attributes.get(token_index, {}).get(attribute) \n+    def htmlpage_tag(self, token_index):\n+        \"\"\"The HtmlPage tag at corresponding to the token at token_index\"\"\"\n+        return self.htmlpage.parsed_body[self.token_page_indexes[token_index]]\n \n     def __str__(self):\n         summary = []\n-        for (token, start, follow) in zip(self.page_tokens, self.token_start_indexes, \n-                self.token_follow_indexes):\n-            text = \"%s %s-%s (%s)\" % (self.token_dict.find_token(token), start, follow, \n-                    self.text[start:follow])\n+        for token, tindex in zip(self.page_tokens, self.token_page_indexes):\n+            text = \"%s page[%s]: %s\" % (self.token_dict.find_token(token), \n+                tindex, self.htmlpage.parsed_body[tindex])\n             summary.append(text)\n         return \"ExtractionPage\\n==============\\nTokens: %s\\n\\nRaw text: %s\\n\\n\" \\\n-                \"Tag attributes: %s\\n\" % ('\\n'.join(summary), self.text, \n-                        self.tag_attributes)\n+                % ('\\n'.join(summary), self.htmlpage.body)\n \n class AnnotationText(object):\n     __slots__ = ('start_text', 'follow_text')\n@@ -179,7 +195,8 @@ class AnnotationText(object):\n         return \"AnnotationText(%s..%s)\" % \\\n                 (repr(self.start_text), repr(self.follow_text))\n \n-class AnnotationTag(object):\n+\n+class AnnotationTag(PageRegion):\n     \"\"\"A tag that annotates part of the document\n \n     It has the following properties:\n@@ -197,8 +214,7 @@ class AnnotationTag(object):\n     \n     def __init__(self, start_index, end_index, surrounds_attribute=None, \n             annotation_text=None, tag_attributes=None, variant_id=None):\n-        self.start_index = start_index\n-        self.end_index = end_index\n+        PageRegion.__init__(self, start_index, end_index)\n         self.surrounds_attribute = surrounds_attribute\n         self.annotation_text = annotation_text\n         self.tag_attributes = tag_attributes or []\n@@ -213,16 +229,3 @@ class AnnotationTag(object):\n     def __repr__(self):\n         return str(self)\n \n-class LabelledRegion(object):\n-    __slots__ = ('start_index', 'end_index')\n-    \n-    def __init__(self, start, end):\n-        self.start_index = start\n-        self.end_index = end\n-        \n-    def __str__(self):\n-        return \"LabelledRegion (%s, %s)\" % (self.start_index, self.end_index)\n-    \n-    def __repr__(self):\n-        return str(self)\n-\n\n@@ -10,7 +10,7 @@ from scrapy.utils.py26 import json\n \n from scrapy.contrib.ibl.htmlpage import HtmlTagType, HtmlTag, HtmlPage\n from scrapy.contrib.ibl.extraction.pageobjects import (AnnotationTag,\n-        TemplatePage, ExtractionPage, AnnotationText, TokenDict)\n+    TemplatePage, ExtractionPage, AnnotationText, TokenDict, FragmentedHtmlPageRegion)\n \n def parse_strings(template_html, extraction_html):\n     \"\"\"Create a template and extraction page from raw strings\n@@ -52,18 +52,18 @@ class InstanceLearningParser(object):\n     def feed(self, html_page):\n         self.html_page = html_page\n         self.previous_element_class = None\n-        for data in html_page.parsed_body:\n+        for index, data in enumerate(html_page.parsed_body):\n             if isinstance(data, HtmlTag):\n                 self._add_token(data.tag, data.tag_type, data.start, data.end)\n-                self.handle_tag(data)\n+                self.handle_tag(data, index)\n             else:\n-                self.handle_data(data)\n+                self.handle_data(data, index)\n             self.previous_element_class = data.__class__\n \n-    def handle_data(self, html_data_fragment):\n+    def handle_data(self, html_data_fragment, index):\n         pass\n \n-    def handle_tag(self, html_tag):\n+    def handle_tag(self, html_tag, index):\n         pass\n \n _END_UNPAIREDTAG_TAGS = [\"form\", \"div\", \"p\", \"table\", \"tr\", \"td\"]\n@@ -86,7 +86,7 @@ class TemplatePageParser(InstanceLearningParser):\n         self.last_text_region = None\n         self.next_tag_index = 0\n \n-    def handle_tag(self, html_tag):\n+    def handle_tag(self, html_tag, index):\n         if self.last_text_region:\n             self._process_text('')\n         \n@@ -275,7 +275,7 @@ class TemplatePageParser(InstanceLearningParser):\n                 if prev != annotation.variant_id:\n                     raise ValueError(\"unbalanced variant annotation tags\")\n                     \n-    def handle_data(self, html_data_fragment):\n+    def handle_data(self, html_data_fragment, index):\n         fragment_text = self.html_page.fragment_data(html_data_fragment)\n         self._process_text(fragment_text)\n \n@@ -300,20 +300,11 @@ class ExtractionPageParser(InstanceLearningParser):\n     \"\"\"\n     def __init__(self, token_dict):\n         InstanceLearningParser.__init__(self, token_dict)\n-        self.page_data = []\n-        self.token_start_index = []\n-        self.token_follow_index = []\n-        self.tag_attrs = {}\n+        self._page_token_indexes = []\n \n-    def _add_token(self, token, token_type, start, end):\n-        InstanceLearningParser._add_token(self, token, token_type, start, end)\n-        self.token_start_index.append(start)\n-        self.token_follow_index.append(end)\n-\n-    def handle_tag(self, html_tag):\n-        if html_tag.attributes:\n-            self.tag_attrs[len(self.token_list) - 1] = html_tag.attributes\n+    def handle_tag(self, html_tag, index):\n+        self._page_token_indexes.append(index)\n     \n     def to_extraction_page(self):\n-        return ExtractionPage(self.html_page.body, self.token_dict, array(self.token_list), \n-                self.token_start_index, self.token_follow_index, self.tag_attrs)\n+        return ExtractionPage(self.html_page, self.token_dict, array(self.token_list), \n+                self._page_token_indexes)\n\n@@ -8,14 +8,16 @@ import operator\n import copy\n import pprint\n import cStringIO\n-from itertools import groupby\n+from itertools import groupby, izip, starmap\n \n from numpy import array\n \n from scrapy.contrib.ibl.descriptor import FieldDescriptor\n+from scrapy.contrib.ibl.htmlpage import HtmlPageRegion\n from scrapy.contrib.ibl.extraction.similarity import (similar_region,\n     longest_unique_subsequence, common_prefix)\n-from scrapy.contrib.ibl.extraction.pageobjects import AnnotationTag, LabelledRegion\n+from scrapy.contrib.ibl.extraction.pageobjects import (AnnotationTag,\n+    PageRegion, FragmentedHtmlPageRegion)\n \n def build_extraction_tree(template, type_descriptor, trace=True):\n     \"\"\"Build a tree of region extractors corresponding to the \n@@ -33,16 +35,14 @@ def build_extraction_tree(template, type_descriptor, trace=True):\n \n     return TemplatePageExtractor(template, extractors)\n \n-_ID = lambda x: x\n+_EXTRACT_HTML = lambda x: x\n _DEFAULT_DESCRIPTOR = FieldDescriptor('none', None)\n \n def _labelled(obj):\n     \"\"\"\n     Returns labelled element of the object (extractor or labelled region)\n     \"\"\"\n-    if hasattr(obj, \"annotation\"):\n-        return obj.annotation\n-    return obj\n+    return getattr(obj, 'annotation', obj)\n \n def _compose(f, g):\n     \"\"\"given unary functions f and g, return a function that computes f(g(x))\n@@ -75,7 +75,7 @@ class BasicTypeExtractor(object):\n         u'<div data-scrapy-annotate=\"{&quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">x<b> xx</b></div>',\\\n         u'<div>a name<b> id-9</b></div>')\n     >>> ex = BasicTypeExtractor(template.annotations[0])\n-    >>> ex.extract(page, 0, 3, [LabelledRegion(1, 2)])\n+    >>> ex.extract(page, 0, 3, [PageRegion(1, 2)])\n     [(u'name', u'a name')]\n     \"\"\"\n \n@@ -88,17 +88,15 @@ class BasicTypeExtractor(object):\n             descriptor = attribute_descriptors.get(annotation.surrounds_attribute)\n             if descriptor:\n                 self.content_validate = descriptor.extractor\n-                self.allow_markup = descriptor.allow_markup\n             else:\n-                self.content_validate = _ID\n-                self.allow_markup = False\n+                self.content_validate = _EXTRACT_HTML\n             self.extract = self._extract_content\n \n         if annotation.tag_attributes:\n             self.tag_data = []\n             for (tag_attr, extraction_attr) in annotation.tag_attributes:\n                 descriptor = attribute_descriptors.get(extraction_attr)\n-                extractf = descriptor.extractor if descriptor else _ID\n+                extractf = descriptor.extractor if descriptor else _EXTRACT_HTML\n                 self.tag_data.append((extractf, tag_attr, extraction_attr))\n \n             self.extract = self._extract_both if \\\n@@ -109,33 +107,32 @@ class BasicTypeExtractor(object):\n             self._extract_attribute(page, start_index, end_index, ignored_regions)\n \n     def _extract_content(self, extraction_page, start_index, end_index, ignored_regions=None):\n-        # we might want to add opening/closing ul/ol/table if we have the\n-        # middle of a region. This would require support in the scrapy\n-        # cleansing.\n-        complete_data = \"\"\n-        start = start_index\n-        end = ignored_regions[0].start_index if ignored_regions else end_index\n-        while start is not None:\n-            if self.allow_markup:\n-                data = extraction_page.html_between_tokens(start, end)\n+        # extract content between annotation indexes\n+        if not ignored_regions:\n+            region = extraction_page.htmlpage_region_inside(start_index, end_index)\n         else:\n-                data = extraction_page.text_between_tokens(start, end)\n-            complete_data += data\n-            if ignored_regions:\n-                start = ignored_regions[0].end_index\n-                ignored_regions.pop(0)\n-                end = ignored_regions[0].start_index if ignored_regions else end_index\n-            else:\n-                start = None\n-        complete_data = self.content_validate(complete_data)\n-        return [(self.annotation.surrounds_attribute, complete_data)] if complete_data else []\n+            # assumes ignored_regions are completely contained within start and end index\n+            assert (start_index <= ignored_regions[0].start_index and \n+                end_index >= ignored_regions[-1].end_index)\n+            starts = [start_index] + [i.end_index for i in ignored_regions]\n+            ends = [i.start_index for i in ignored_regions]\n+            if starts[-1] is not None:\n+                ends.append(end_index)\n+            included_regions = izip(starts, ends)\n+            if ends[0] is None:\n+                included_regions.next()\n+            regions = starmap(extraction_page.htmlpage_region_inside, included_regions)\n+            region = FragmentedHtmlPageRegion(extraction_page.htmlpage, list(regions))\n+        validated = self.content_validate(region)\n+        return [(self.annotation.surrounds_attribute, validated)] if validated else []\n     \n     def _extract_attribute(self, extraction_page, start_index, end_index, ignored_regions=None):\n         data = []\n         for (f, ta, ea) in self.tag_data:\n-            tag_value = extraction_page.tag_attribute(start_index, ta)\n+            tag_value = extraction_page.htmlpage_tag(start_index).attributes.get(ta)\n             if tag_value:\n-                extracted = f(tag_value)\n+                region = HtmlPageRegion(extraction_page.htmlpage, tag_value)\n+                extracted = f(region)\n                 if extracted is not None:\n                     data.append((ea, extracted))\n         return data\n@@ -178,10 +175,8 @@ class BasicTypeExtractor(object):\n     def __str__(self):\n         messages = ['BasicTypeExtractor(']\n         if self.annotation.surrounds_attribute:\n-            messages += [self.annotation.surrounds_attribute, ': ', \n-                    'html content' if self.allow_markup else 'text content',\n-            ]\n-            if self.content_validate != _ID:\n+            messages.append(self.annotation.surrounds_attribute)\n+            if self.content_validate != _EXTRACT_HTML:\n                 messages += [', extracted with \\'', \n                         self.content_validate.__name__, '\\'']\n         \n@@ -190,7 +185,7 @@ class BasicTypeExtractor(object):\n                 messages.append(';')\n             for (f, ta, ea) in self.tag_data:\n                 messages += [ea, ': tag attribute \"', ta, '\"']\n-                if f != _ID:\n+                if f != _EXTRACT_HTML:\n                     messages += [', validated by ', str(f)]\n         messages.append(\", template[%s:%s])\" % \\\n                 (self.annotation.start_index, self.annotation.end_index))\n@@ -340,7 +335,8 @@ class RecordExtractor(object):\n         The region in the page to be extracted from may be specified using\n         start_index and end_index\n         \"\"\"\n-        ignored_regions = [i if isinstance(i, LabelledRegion) else LabelledRegion(*i) for i in (ignored_regions or [])]\n+        if ignored_regions is None:\n+            ignored_regions = []\n         region_elements = sorted(self.extractors + ignored_regions, key=lambda x: _labelled(x).start_index)\n         _, _, attributes = self._doextract(page, region_elements, start_index, \n                 end_index)\n@@ -395,7 +391,7 @@ class RecordExtractor(object):\n                     s, p, e = similar_region(page.page_tokens, self.template_tokens, \\\n                               i, start, sindex)\n                     if s > 0:\n-                        similar_ignored_regions.append(LabelledRegion(p, e))\n+                        similar_ignored_regions.append(PageRegion(p, e))\n                         start = e or start\n                 extracted_data = first_region.extract(page, pindex, sindex, similar_ignored_regions)\n                 if extracted_data:\n@@ -493,12 +489,12 @@ class TraceExtractor(object):\n             for t in template.page_tokens[tend:tend+5]])\n     \n     def summarize_trace(self, page, start, end, ret):\n-        text_start = page.token_follow_indexes[start]\n-        text_end = page.token_start_indexes[end or -1]\n+        text_start = page.htmlpage.parsed_body[page.token_page_indexes[start]].start\n+        text_end = page.htmlpage.parsed_body[page.token_page_indexes[end or -1]].end\n         page_snippet = \"(...%s)%s(%s...)\" % (\n-                page.text[text_start-50:text_start].replace('\\n', ' '), \n-                page.text[text_start:text_end], \n-                page.text[text_end:text_end+50].replace('\\n', ' '))\n+                page.htmlpage.body[text_start-50:text_start].replace('\\n', ' '), \n+                page.htmlpage.body[text_start:text_end], \n+                page.htmlpage.body[text_end:text_end+50].replace('\\n', ' '))\n         pre_summary = \"\\nstart %s page[%s:%s]\\n\" % (self.traced.__class__.__name__, start, end)\n         post_summary = \"\"\"\n %s page[%s:%s] \n@@ -572,25 +568,26 @@ class TemplatePageExtractor(object):\n _tokenize = re.compile(r'\\w+|[^\\w\\s]+', re.UNICODE | re.MULTILINE | re.DOTALL).findall\n \n class TextRegionDataExtractor(object):\n-    \"\"\"Data Extractor for extracting text fragments from within a larger\n-    body of text. It extracts based on the longest unique prefix and suffix.\n+    \"\"\"Data Extractor for extracting text fragments from an annotation page\n+    fragment or string. It extracts based on the longest unique prefix and\n+    suffix.\n \n     for example:\n     >>> extractor = TextRegionDataExtractor('designed by ', '.')\n-    >>> extractor.extract(\"by Marc Newson.\")\n+    >>> extractor.extract_text(\"by Marc Newson.\")\n     'Marc Newson'\n \n     Both prefix and suffix are optional:\n     >>> extractor = TextRegionDataExtractor('designed by ')\n-    >>> extractor.extract(\"by Marc Newson.\")\n+    >>> extractor.extract_text(\"by Marc Newson.\")\n     'Marc Newson.'\n     >>> extractor = TextRegionDataExtractor(suffix='.')\n-    >>> extractor.extract(\"by Marc Newson.\")\n+    >>> extractor.extract_text(\"by Marc Newson.\")\n     'by Marc Newson'\n \n     It requires a minimum match of at least one word or punctuation character:\n     >>> extractor = TextRegionDataExtractor('designed by')\n-    >>> extractor.extract(\"y Marc Newson.\") is None\n+    >>> extractor.extract_text(\"y Marc Newson.\") is None\n     True\n     \"\"\"\n     def __init__(self, prefix=None, suffix=None):\n@@ -609,8 +606,13 @@ class TextRegionDataExtractor(object):\n         tokens = _tokenize(matchstring or '')\n         return len(tokens[0]) if tokens else 0\n \n-    def extract(self, text):\n-        \"\"\"attempt to extract a substring from the text\"\"\"\n+    def extract(self, region):\n+        \"\"\"Extract a region from the region passed\"\"\"\n+        text = self.extract_text(region)\n+        return HtmlPageRegion(region.htmlpage, text) if text else None\n+\n+    def extract_text(self, text):\n+        \"\"\"Extract a substring from the text\"\"\"\n         pref_index = 0\n         if self.minprefix > 0:\n             rev_idx, plen = longest_unique_subsequence(text[::-1], self.prefix)\n@@ -623,5 +625,3 @@ class TextRegionDataExtractor(object):\n         if slen < self.minsuffix:\n             return None\n         return text[pref_index:pref_index + sidx]\n-\n-   \n\n@@ -4,8 +4,9 @@ Extractors for attributes\n import re\n import urlparse\n \n-from scrapy.utils.markup import remove_entities\n+from scrapy.utils.markup import remove_entities, remove_comments\n from scrapy.utils.url import safe_url_string\n+from scrapy.contrib.ibl.htmlpage import HtmlTag, HtmlTagType\n \n #FIXME: the use of \".\" needs to be localized\n _NUMERIC_ENTITIES = re.compile(\"&#([0-9]+)(?:;|\\s)\", re.U)\n@@ -22,11 +23,175 @@ _CSS_IMAGERE = re.compile(\"background(?:-image)?\\s*:\\s*url\\((.*?)\\)\", re.I)\n _BASE_PATH_RE = \"/?(?:[^/]+/)*(?:.+%s)\"\n _IMAGE_PATH_RE = re.compile(_BASE_PATH_RE % '\\.(?:%s)' % _IMAGES_TYPES, re.I)\n _GENERIC_PATH_RE = re.compile(_BASE_PATH_RE % '', re.I)\n+_WS = re.compile(\"\\s+\", re.U)\n \n-def text(txt):\n-    stripped = txt.strip() if txt else None\n-    if stripped:\n-        return stripped\n+# tags to keep (only for attributes with markup)\n+_TAGS_TO_KEEP = frozenset(['br', 'p', 'big', 'em', 'small', 'strong', 'sub', \n+    'sup', 'ins', 'del', 'code', 'kbd', 'samp', 'tt', 'var', 'pre', 'listing',\n+    'plaintext', 'abbr', 'acronym', 'address', 'bdo', 'blockquote', 'q', \n+    'cite', 'dfn', 'table', 'tr', 'th', 'td', 'tbody', 'ul', 'ol', 'li', 'dl',\n+    'dd', 'dt'])\n+\n+# tag names to be replaced by other tag names (overrides tags_to_keep)\n+_TAGS_TO_REPLACE = {\n+    'h1': 'strong',\n+    'h2': 'strong',\n+    'h3': 'strong',\n+    'h4': 'strong',\n+    'h5': 'strong',\n+    'h6': 'strong',\n+    'b' : 'strong',\n+    'i' : 'em',\n+}\n+\n+# tags whoose content will be completely removed (recursively)\n+# (overrides tags_to_keep and tags_to_replace)\n+_TAGS_TO_PURGE = ('script', 'img', 'input')\n+\n+def htmlregion(text):\n+    \"\"\"convenience function to make an html region from text.\n+    This is useful for testing\n+    \"\"\"\n+    from scrapy.contrib.ibl.htmlpage import HtmlPage\n+    return HtmlPage(body=text).subregion()\n+\n+def notags(region, tag_replace=u' '):\n+    \"\"\"Removes all html tags\"\"\"\n+    fragments = getattr(region, 'parsed_fragments', None)\n+    if fragments is None:\n+        return region\n+    page = region.htmlpage\n+    data = [page.fragment_data(f) for f in fragments if not isinstance(f, HtmlTag)]\n+    return tag_replace.join(data)\n+\n+def text(region):\n+    \"\"\"Converts HTML to text. There is no attempt at formatting other than\n+    removing excessive whitespace,\n+    \n+    For example:\n+    >>> t = lambda s: text(htmlregion(s))\n+    >>> t(u'<h1>test</h1>')\n+    u'test'\n+    \n+    Leading and trailing whitespace are removed\n+    >>> t(u'<h1> test</h1> ')\n+    u'test'\n+    \n+    Comments are removed\n+    >>> t(u'test <!-- this is a comment --> me')\n+    u'test me'\n+    \n+    Text between script tags is ignored\n+    >>> t(u\"scripts are<script>n't</script> ignored\")\n+    u'scripts are ignored'\n+    \n+    HTML entities are converted to text\n+    >>> t(u\"only &pound;42\")\n+    u'only \\\\xa342'\n+    \"\"\"\n+    chunks = _process_markup(region, \n+        lambda text: remove_entities(text, encoding=region.htmlpage.encoding),\n+        lambda tag: u' '\n+    )\n+    text = u''.join(chunks)\n+    return _WS.sub(u' ', text).strip()\n+\n+def safehtml(region, allowed_tags=_TAGS_TO_KEEP, replace_tags=_TAGS_TO_REPLACE):\n+    \"\"\"Creates an HTML subset, using a whitelist of HTML tags.\n+\n+    The HTML generated is safe for display on a website,without escaping and\n+    should not cause formatting problems.\n+    \n+    Allowed_tags is a set of tags that are allowed and replace_tags is a mapping of \n+    tags to alternative tags to substitute.\n+\n+    For example:\n+    >>> t = lambda s: safehtml(htmlregion(s))\n+    >>> t(u'<strong>test <blink>test</blink></strong>')\n+    u'<strong>test test</strong>'\n+    \n+    Some tags, like script, are completely removed\n+    >>> t(u'<script>test </script>test')\n+    u'test'\n+\n+    replace_tags define tags that are converted. By default all headers, bold and indenting\n+    are converted to strong and em.\n+    >>> t(u'<h2>header</h2> test <b>bold</b> <i>indent</i>')\n+    u'<strong>header</strong> test <strong>bold</strong> <em>indent</em>'\n+\n+    Comments are stripped, but entities are not converted\n+    >>> t(u'<!-- comment --> only &pound;42')\n+    u'only &pound;42'\n+    \n+    Paired tags are closed\n+    >>> t(u'<p>test')\n+    u'<p>test</p>'\n+\n+    >>> t(u'<p>test <i><br/><b>test</p>')\n+    u'<p>test <em><br/><strong>test</strong></em></p>'\n+\n+    \"\"\"\n+    tagstack = []\n+    def _process_tag(tag):\n+        tagstr = replace_tags.get(tag.tag, tag.tag)\n+        if tagstr not in allowed_tags:\n+            return\n+        if tag.tag_type == HtmlTagType.OPEN_TAG:\n+            tagstack.append(tagstr)\n+            return u\"<%s>\" % tagstr\n+        elif tag.tag_type == HtmlTagType.CLOSE_TAG:\n+            try:\n+                last = tagstack.pop()\n+                # common case of matching tag\n+                if last == tagstr:\n+                    return u\"</%s>\" % last\n+                # output all preceeding tags (if present)\n+                revtags = tagstack[::-1]\n+                tindex = revtags.index(tagstr)\n+                del tagstack[-tindex-1:]\n+                return u\"</%s></%s>\" % (last, u\"></\".join(revtags[:tindex+1]))\n+            except (ValueError, IndexError):\n+                # popped from empty stack or failed to find the tag\n+                pass \n+        else:\n+            assert tag.tag_type == HtmlTagType.UNPAIRED_TAG, \"unrecognised tag type\"\n+            return u\"<%s/>\" % tag.tag\n+    chunks = list(_process_markup(region, lambda text: text, _process_tag)) + \\\n+        [\"</%s>\" % t for t in reversed(tagstack)]\n+    return u''.join(chunks).strip()\n+\n+def _process_markup(region, textf, tagf):\n+    fragments = getattr(region, 'parsed_fragments', None)\n+    if fragments is None:\n+        textf(region)\n+        return\n+    fiter = iter(fragments)\n+    for fragment in fiter:\n+        if isinstance(fragment, HtmlTag):\n+            # skip forward to closing script tags\n+            tag = fragment.tag\n+            if tag in _TAGS_TO_PURGE:\n+                # if opening, keep going until closed\n+                if fragment.tag_type == HtmlTagType.OPEN_TAG:\n+                    for probe in fiter:\n+                        if isinstance(probe, HtmlTag) and \\\n+                            probe.tag == tag and \\\n+                            probe.tag_type == HtmlTagType.CLOSE_TAG:\n+                            break\n+            else:\n+                output = tagf(fragment)\n+                if output:\n+                    yield output\n+        else:\n+            text = region.htmlpage.fragment_data(fragment)\n+            text = remove_comments(text)\n+            text = textf(text)\n+            if text:\n+                yield text\n+\n+def html(pageregion):\n+    \"\"\"A page region is already html, so this is the identity function\"\"\"\n+    return pageregion\n \n def contains_any_numbers(txt):\n     \"\"\"text that must contain at least one number\n\n@@ -1,8 +1,9 @@\n \"\"\"\n htmlpage\n \n-Container object for representing html pages in the IBL system. This\n-encapsulates page related information and prevents parsing multiple times.\n+Container objects for representing html pages and their parts in the IBL\n+system. This encapsulates page related information and prevents parsing\n+multiple times.\n \"\"\"\n import re\n import hashlib\n@@ -26,26 +27,82 @@ def create_page_from_jsonpage(jsonpage, body_key):\n     return HtmlPage(url, headers, body, page_id)\n \n class HtmlPage(object):\n-    def __init__(self, url=None, headers=None, body=None, page_id=None):\n+    \"\"\"HtmlPage\n+\n+    This is a parsed HTML page. It contains the page headers, url, raw body and parsed \n+    body.\n+\n+    The parsed body is a list of HtmlDataFragment objects.\n+    \"\"\"\n+    def __init__(self, url=None, headers=None, body=None, page_id=None, encoding='utf-8'):\n         assert isinstance(body, unicode), \"unicode expected, got: %s\" % type(body).__name__\n         self.headers = headers or {}\n         self.body = body\n         self.url = url or u''\n+        self.encoding = encoding\n         if page_id is None and url:\n             self.page_id = hashlib.sha1(url).hexdigest()\n         else:\n             self.page_id = page_id \n     \n+    @classmethod\n+    def from_response(cls, response, page_id=None):\n+        \"\"\"Create an HtmlPage from a scrapy response\"\"\"\n+        return HtmlPage(response.url, response.headers,\n+            response.body_as_unicode(), page_id, response.encoding)\n \n     def _set_body(self, body):\n         self._body = body\n         self.parsed_body = list(parse_html(body))\n         \n-    body = property(lambda x: x._body, _set_body)\n+    body = property(lambda x: x._body, _set_body, doc=\"raw html for the page\")\n+    \n+    def subregion(self, start=0, end=None):\n+        \"\"\"HtmlPageRegion constructed from the start and end index (inclusive)\n+        into the parsed page\n+        \"\"\"\n+        return HtmlPageParsedRegion(self, start, end)\n \n     def fragment_data(self, data_fragment):\n+        \"\"\"portion of the body corresponding to the HtmlDataFragment\"\"\"\n         return self.body[data_fragment.start:data_fragment.end]\n \n+class HtmlPageRegion(unicode):\n+    \"\"\"A Region of an HtmlPage that has been extracted\n+    \"\"\"\n+    def __new__(cls, htmlpage, data):\n+        return unicode.__new__(cls, data)\n+\n+    def __init__(self, htmlpage, data):\n+        \"\"\"Construct a new HtmlPageRegion object.\n+\n+        htmlpage is the original page and data is the raw html\n+        \"\"\"\n+        self.htmlpage = htmlpage\n+    \n+class HtmlPageParsedRegion(HtmlPageRegion):\n+    \"\"\"A region of an HtmlPage that has been extracted\n+\n+    This has a parsed_fragments property that contains the parsed html \n+    fragments contained within this region\n+    \"\"\"\n+    def __new__(cls, htmlpage, start_index, end_index):\n+        text_start = htmlpage.parsed_body[start_index].start\n+        text_end = htmlpage.parsed_body[end_index or -1].end\n+        text = htmlpage.body[text_start:text_end]\n+        return HtmlPageRegion.__new__(cls, htmlpage, text)\n+\n+    def __init__(self, htmlpage, start_index, end_index):\n+        self.htmlpage = htmlpage\n+        self.start_index = start_index\n+        self.end_index = end_index\n+\n+    @property\n+    def parsed_fragments(self):\n+        \"\"\"HtmlDataFragment or HtmlTag objects for this parsed region\"\"\"\n+        end = self.end_index + 1 if self.end_index is not None else None\n+        return self.htmlpage.parsed_body[self.start_index:end]\n+\n class HtmlTagType(object):\n     OPEN_TAG = 1\n     CLOSE_TAG = 2 \n\n@@ -9,7 +9,7 @@ from scrapy.contrib.ibl.htmlpage import HtmlPage\n from scrapy.contrib.ibl.descriptor import (FieldDescriptor as A, \n         ItemDescriptor)\n from scrapy.contrib.ibl.extractors import (contains_any_numbers,\n-        image_url)\n+        image_url, html, notags)\n \n try:\n     import numpy\n@@ -719,19 +719,23 @@ EXTRACT_PAGE23 = u\"\"\"\n </body></html>\n \"\"\"\n \n+DEFAULT_DESCRIPTOR = ItemDescriptor('test', \n+    'item test, removes tags from description attribute',\n+    [A('description', 'description field without tags', notags)])\n+\n SAMPLE_DESCRIPTOR1 = ItemDescriptor('test', 'product test', [\n     A('name', \"Product name\", required=True),\n     A('price', \"Product price, including any discounts and tax or vat\", \n         contains_any_numbers, True),    \n     A('image_urls', \"URLs for one or more images\", image_url, True),\n-    A('description', \"The full description of the product\", allow_markup=True),\n+    A('description', \"The full description of the product\", html),\n     ]\n )\n \n # A list of (test name, [templates], page, extractors, expected_result)\n TEST_DATA = [\n     # extract from a similar page\n-    ('similar page extraction', [ANNOTATED_PAGE1], EXTRACT_PAGE1, None,\n+    ('similar page extraction', [ANNOTATED_PAGE1], EXTRACT_PAGE1, DEFAULT_DESCRIPTOR,\n         {u'title': [u'Nice Product'], u'description': [u'wonderful product'], \n             u'image_url': [u'nice_product.jpg']}\n     ),\n@@ -743,20 +747,20 @@ TEST_DATA = [\n             u'image_url': [u'nice_product.jpg']}\n     ),\n     # compilicated tag (multiple attributes and annotation)\n-    ('multiple attributes and annotation', [ANNOTATED_PAGE2], EXTRACT_PAGE2, None,\n+    ('multiple attributes and annotation', [ANNOTATED_PAGE2], EXTRACT_PAGE2, DEFAULT_DESCRIPTOR,\n         {'name': [u'product 1'], 'image_url': [u'http://example.com/product1.jpg'],\n             'description': [u'product 1 is great']}\n     ),\n     # can only work out correct placement by matching the second attribute first\n-    ('ambiguous description', [ANNOTATED_PAGE3], EXTRACT_PAGE3, None,\n+    ('ambiguous description', [ANNOTATED_PAGE3], EXTRACT_PAGE3, DEFAULT_DESCRIPTOR,\n         {'description': [u'description'], 'delivery': [u'delivery']}\n     ),\n     # infer a repeated structure\n-    ('repeated elements', [ANNOTATED_PAGE4], EXTRACT_PAGE4, None,\n+    ('repeated elements', [ANNOTATED_PAGE4], EXTRACT_PAGE4, DEFAULT_DESCRIPTOR,\n         {'features': [u'feature1', u'feature2', u'feature3']}\n     ),\n     # identical variants with a repeated structure\n-    ('repeated identical variants', [ANNOTATED_PAGE5], EXTRACT_PAGE5, None,\n+    ('repeated identical variants', [ANNOTATED_PAGE5], EXTRACT_PAGE5, DEFAULT_DESCRIPTOR,\n          {\n              'description': [u'description'],\n              'variants': [\n@@ -767,7 +771,7 @@ TEST_DATA = [\n          }\n     ),\n     # variants with an irregular structure\n-    ('irregular variants', [ANNOTATED_PAGE6], EXTRACT_PAGE6, None,\n+    ('irregular variants', [ANNOTATED_PAGE6], EXTRACT_PAGE6, DEFAULT_DESCRIPTOR,\n          {\n              'description': [u'description'],\n              'variants': [\n@@ -779,7 +783,7 @@ TEST_DATA = [\n     ),\n \n     # discovering repeated variants in table columns\n-#    ('variants in table columns', [ANNOTATED_PAGE7], EXTRACT_PAGE7, None,\n+#    ('variants in table columns', [ANNOTATED_PAGE7], EXTRACT_PAGE7, DEFAULT_DESCRIPTOR,\n #         {'variants': [\n #             {u'colour': [u'colour 1'], u'price': [u'price 1']}, \n #             {u'colour': [u'colour 2'], u'price': [u'price 2']}, \n@@ -790,7 +794,7 @@ TEST_DATA = [\n     \n     # ignored regions\n     (\n-    'ignored_regions', [ANNOTATED_PAGE8], EXTRACT_PAGE8, None,\n+    'ignored_regions', [ANNOTATED_PAGE8], EXTRACT_PAGE8, DEFAULT_DESCRIPTOR,\n           {\n              'description': [u'\\n A very nice product for all intelligent people \\n \\n'],\n              'price': [u'\\n12.00\\n(VAT exc.)'],\n@@ -798,58 +802,58 @@ TEST_DATA = [\n     ),\n     # shifted ignored regions (detected by region similarity)\n     (\n-    'shifted_ignored_regions', [ANNOTATED_PAGE9], EXTRACT_PAGE9, None,\n+    'shifted_ignored_regions', [ANNOTATED_PAGE9], EXTRACT_PAGE9, DEFAULT_DESCRIPTOR,\n           {\n              'description': [u'\\n A very nice product for all intelligent people \\n \\n'],\n              'price': [u'\\n12.00\\n(VAT exc.)'],\n           }\n     ),\n     (# special case with partial annotations\n-    'special_partial_annotation', [ANNOTATED_PAGE11], EXTRACT_PAGE11, None,\n+    'special_partial_annotation', [ANNOTATED_PAGE11], EXTRACT_PAGE11, DEFAULT_DESCRIPTOR,\n           {\n             'name': [u'SL342'],\n-            'description': [u'\\nSL342\\n \\nNice product for ladies\\n \\n&pounds;85.00\\n'],\n+            'description': ['\\nSL342\\n \\nNice product for ladies\\n \\n&pounds;85.00\\n'],\n             'price': [u'&pounds;85.00'],\n             'price_before_discount': [u'&pounds;100.00'],\n           }\n     ),\n     (# with ignore-beneath feature\n-    'ignore-beneath', [ANNOTATED_PAGE12], EXTRACT_PAGE12a, None,\n+    'ignore-beneath', [ANNOTATED_PAGE12], EXTRACT_PAGE12a, DEFAULT_DESCRIPTOR,\n           {\n             'description': [u'\\n A very nice product for all intelligent people \\n'],\n           }\n     ),\n     (# ignore-beneath with extra tags\n-    'ignore-beneath with extra tags', [ANNOTATED_PAGE12], EXTRACT_PAGE12b, None,\n+    'ignore-beneath with extra tags', [ANNOTATED_PAGE12], EXTRACT_PAGE12b, DEFAULT_DESCRIPTOR,\n           {\n             'description': [u'\\n A very nice product for all intelligent people \\n'],\n           }\n     ),\n-    ('nested annotation with replica outside', [ANNOTATED_PAGE13a], EXTRACT_PAGE13a, None,\n+    ('nested annotation with replica outside', [ANNOTATED_PAGE13a], EXTRACT_PAGE13a, DEFAULT_DESCRIPTOR,\n           {'description': [u'\\n A product \\n $50.00 \\nThis product is excelent. Buy it!\\n \\n'],\n            'price': [\"$50.00\"],\n            'name': [u'A product']}\n     ),\n-    ('outside annotation with nested replica', [ANNOTATED_PAGE13b], EXTRACT_PAGE13b, None,\n+    ('outside annotation with nested replica', [ANNOTATED_PAGE13b], EXTRACT_PAGE13b, DEFAULT_DESCRIPTOR,\n           {'description': [u'\\n A product \\n $50.00 \\nThis product is excelent. Buy it!\\n'],\n            'price': [\"$45.00\"],\n            'name': [u'A product']}\n     ),\n-    ('consistency check', [ANNOTATED_PAGE14], EXTRACT_PAGE14, None,\n+    ('consistency check', [ANNOTATED_PAGE14], EXTRACT_PAGE14, DEFAULT_DESCRIPTOR,\n           {},\n     ),\n-    ('consecutive nesting', [ANNOTATED_PAGE15], EXTRACT_PAGE15, None,\n+    ('consecutive nesting', [ANNOTATED_PAGE15], EXTRACT_PAGE15, DEFAULT_DESCRIPTOR,\n           {'description': [u'Description\\n \\n'],\n            'price': [u'80.00']},\n     ),\n-    ('nested inside not found', [ANNOTATED_PAGE16], EXTRACT_PAGE16, None,\n+    ('nested inside not found', [ANNOTATED_PAGE16], EXTRACT_PAGE16, DEFAULT_DESCRIPTOR,\n           {'price': [u'90.00'],\n            'name': [u'product name']},\n     ),\n-    ('ignored region helps to find attributes', [ANNOTATED_PAGE17], EXTRACT_PAGE17, None,\n+    ('ignored region helps to find attributes', [ANNOTATED_PAGE17], EXTRACT_PAGE17, DEFAULT_DESCRIPTOR,\n           {'description': [u'\\nThis product is excelent. Buy it!\\n']},\n     ),\n-    ('ignored region in partial annotation', [ANNOTATED_PAGE18], EXTRACT_PAGE18, None,\n+    ('ignored region in partial annotation', [ANNOTATED_PAGE18], EXTRACT_PAGE18, DEFAULT_DESCRIPTOR,\n           {u'site_id': [u'Item Id'],\n            u'description': [u'\\nDescription\\n']},\n     ),\n@@ -864,13 +868,13 @@ TEST_DATA = [\n          SAMPLE_DESCRIPTOR1,\n          None,\n     ),\n-    ('repeated partial annotations with variants', [ANNOTATED_PAGE20], EXTRACT_PAGE20, None,\n+    ('repeated partial annotations with variants', [ANNOTATED_PAGE20], EXTRACT_PAGE20, DEFAULT_DESCRIPTOR,\n             {u'variants': [\n                 {'price': ['270'], 'name': ['Twin']},\n                 {'price': ['330'], 'name': ['Queen']},\n             ]},\n     ),\n-    ('variants with swatches', [ANNOTATED_PAGE21], EXTRACT_PAGE21, None,\n+    ('variants with swatches', [ANNOTATED_PAGE21], EXTRACT_PAGE21, DEFAULT_DESCRIPTOR,\n             {u'category': [u'chairs'],\n              u'image_urls': [u'image.jpg'],\n              u'variants': [\n@@ -881,7 +885,7 @@ TEST_DATA = [\n              ]\n             },\n     ),\n-    ('variants with swatches complete', [ANNOTATED_PAGE22], EXTRACT_PAGE22, None,\n+    ('variants with swatches complete', [ANNOTATED_PAGE22], EXTRACT_PAGE22, DEFAULT_DESCRIPTOR,\n             {u'category': [u'chairs'],\n              u'variants': [\n                  {u'swatches': [u'swatch1.jpg'],\n@@ -899,7 +903,7 @@ TEST_DATA = [\n              ],\n              u'image_urls': [u'image.jpg']},\n     ),\n-    ('repeated (variants) with ignore annotations', [ANNOTATED_PAGE23], EXTRACT_PAGE23, None,\n+    ('repeated (variants) with ignore annotations', [ANNOTATED_PAGE23], EXTRACT_PAGE23, DEFAULT_DESCRIPTOR,\n         {'variants': [\n             {u'price': [u'300'], u'name': [u'Variant 1']},\n             {u'price': [u'320'], u'name': [u'Variant 2']},\n\n@@ -228,11 +228,11 @@ class TestPageParsing(TestCase):\n         epp = _parse_page(ExtractionPageParser, SIMPLE_PAGE)\n         ep = epp.to_extraction_page()\n         assert len(ep.page_tokens) == 4\n-        assert ep.token_html(0) == '<html>'\n-        assert ep.token_html(1) == '<p some-attr=\"foo\">'\n+        assert ep.htmlpage.fragment_data(ep.htmlpage_tag(0)) == '<html>'\n+        assert ep.htmlpage.fragment_data(ep.htmlpage_tag(1)) == '<p some-attr=\"foo\">'\n         \n-        assert ep.html_between_tokens(1, 2) == 'this is a test'\n-        assert ep.html_between_tokens(1, 3) == 'this is a test</p> '\n+        assert ep.htmlpage_region_inside(1, 2) == 'this is a test'\n+        assert ep.htmlpage_region_inside(1, 3) == 'this is a test</p> '\n \n     def test_invalid_html(self):\n         p = _parse_page(InstanceLearningParser, BROKEN_PAGE)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#407f7f2d65a99c1ae911a2faef8c0a16e46d5d93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 1574 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -163,7 +163,7 @@ def safehtml(region, allowed_tags=_TAGS_TO_KEEP, replace_tags=_TAGS_TO_REPLACE):\n def _process_markup(region, textf, tagf):\n     fragments = getattr(region, 'parsed_fragments', None)\n     if fragments is None:\n-        textf(region)\n+        yield textf(region)\n         return\n     fiter = iter(fragments)\n     for fragment in fiter:\n\n@@ -249,12 +249,12 @@ SL342\n Nice product for ladies\n <br/><ins data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;generated&quot;: true,\n      &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">\n-&pounds;85.00\n+&pound;85.00\n </ins>\n </p>\n <ins data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;generated&quot;: true,\n      &quot;annotations&quot;: {&quot;content&quot;: &quot;price_before_discount&quot;}}\">\n-&pounds;100.00\n+&pound;100.00\n </ins>\n </body></html>\n \"\"\"\n@@ -266,9 +266,9 @@ SL342\n <br/>\n Nice product for ladies\n <br/>\n-&pounds;85.00\n+&pound;85.00\n </p>\n-&pounds;100.00\n+&pound;100.00\n </body></html>\n \"\"\"\n \n@@ -812,9 +812,9 @@ TEST_DATA = [\n     'special_partial_annotation', [ANNOTATED_PAGE11], EXTRACT_PAGE11, DEFAULT_DESCRIPTOR,\n           {\n             'name': [u'SL342'],\n-            'description': ['\\nSL342\\n \\nNice product for ladies\\n \\n&pounds;85.00\\n'],\n-            'price': [u'&pounds;85.00'],\n-            'price_before_discount': [u'&pounds;100.00'],\n+            'description': ['\\nSL342\\n \\nNice product for ladies\\n \\n&pound;85.00\\n'],\n+            'price': [u'\\xa385.00'],\n+            'price_before_discount': [u'\\xa3100.00'],\n           }\n     ),\n     (# with ignore-beneath feature\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3954e600ca39fd8b02e881758b1bbc0029279d1e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 65 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 7 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 65 | Churn Cumulative: 76 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,64 @@\n+from __future__ import with_statement\n+\n+import os\n+from time import time\n+import cPickle as pickle\n+\n+from scrapy.http import Headers\n+from scrapy.core.downloader.responsetypes import responsetypes\n+from scrapy.utils.request import request_fingerprint\n+from scrapy.utils.project import data_path\n+from scrapy import conf\n+\n+\n+class DbmCacheStorage(object):\n+\n+    def __init__(self, settings=conf.settings):\n+        self.cachedir = data_path(settings['HTTPCACHE_DIR'])\n+        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n+        self.dbmodule = __import__(settings['HTTPCACHE_DBM_MODULE'])\n+        self.dbs = {}\n+\n+    def open_spider(self, spider):\n+        dbpath = os.path.join(self.cachedir, '%s.db' % spider.name)\n+        self.dbs[spider] = self.dbmodule.open(dbpath, 'c')\n+\n+    def close_spider(self, spider):\n+        self.dbs[spider].close()\n+\n+    def retrieve_response(self, spider, request):\n+        data = self._read_data(spider, request)\n+        if data is None:\n+            return # not cached\n+        url = data['url']\n+        status = data['status']\n+        headers = Headers(data['headers'])\n+        body = data['body']\n+        respcls = responsetypes.from_args(headers=headers, url=url)\n+        response = respcls(url=url, headers=headers, status=status, body=body)\n+        return response\n+\n+    def store_response(self, spider, request, response):\n+        key = self._request_key(request)\n+        data = {\n+            'status': response.status,\n+            'url': response.url,\n+            'headers': dict(response.headers),\n+            'body': response.body,\n+        }\n+        self.dbs[spider]['%s_data' % key] = pickle.dumps(data, protocol=2)\n+        self.dbs[spider]['%s_time' % key] = str(time())\n+\n+    def _read_data(self, spider, request):\n+        key = self._request_key(request)\n+        db = self.dbs[spider]\n+        tkey = '%s_time' % key\n+        if not db.has_key(tkey):\n+            return # not found\n+        ts = db[tkey]\n+        if 0 < self.expiration_secs < time() - float(ts):\n+            return # expired\n+        return pickle.loads(db['%s_data' % key])\n+\n+    def _request_key(self, request):\n+        return request_fingerprint(request)\n\n@@ -155,6 +155,7 @@ HTTPCACHE_STORAGE = 'scrapy.contrib.downloadermiddleware.httpcache.FilesystemCac\n HTTPCACHE_EXPIRATION_SECS = 0\n HTTPCACHE_IGNORE_HTTP_CODES = []\n HTTPCACHE_IGNORE_SCHEMES = ['file']\n+HTTPCACHE_DBM_MODULE = 'anydbm'\n \n ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
