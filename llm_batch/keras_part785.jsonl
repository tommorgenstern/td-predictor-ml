{"custom_id": "keras#418649817525e95e66fe73420504b4133c1b3a2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 8 | Churn Cumulative: 450 | Contributors (this commit): 6 | Commits (past 90d): 11 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -47,8 +47,10 @@ class JAXTrainer(base_trainer.Trainer):\n             **kwargs,\n         )\n \n-        trainable_mapping = zip(self.trainable_variables, trainable_variables)\n-        with backend.StatelessScope(state_mapping=trainable_mapping):\n+        var_mapping = list(zip(self.trainable_variables, trainable_variables))\n+        var_mapping.extend(\n+                zip(self.non_trainable_variables, non_trainable_variables))\n+        with backend.StatelessScope(state_mapping=var_mapping):\n             # Note that this is needed for the regularization loss, which need\n             # the latest value of train/non-trainable variables.\n             loss = self.compute_loss(\n\n@@ -166,7 +166,7 @@ class Metric:\n \n     def add_variable(self, shape, initializer, dtype=None, name=None):\n         self._check_super_called()\n-        with backend.name_scope(self.name, caller=self):\n+        with backend.name_scope(self.name.replace(\"/\", \">\"), caller=self):\n             initializer = initializers.get(initializer)\n             variable = backend.Variable(\n                 initializer=initializer,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#00b9d94f0c9476a6d03fc8fad58d72c474e3e5f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 18 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -175,14 +175,14 @@ def print_summary(\n                     break\n \n     if sequential_like:\n-        default_line_length = 84\n-        positions = positions or [0.45, 0.84, 1.0]\n+        default_line_length = 88\n+        positions = positions or [0.45, 0.80, 1.0]\n         # header names for the different log elements\n         header = [\"Layer (type)\", \"Output Shape\", \"Param #\"]\n         alignment = [\"left\", \"left\", \"right\"]\n     else:\n         default_line_length = 108\n-        positions = positions or [0.3, 0.56, 0.70, 1.0]\n+        positions = positions or [0.3, 0.56, 0.74, 1.0]\n         # header names for the different log elements\n         header = [\"Layer (type)\", \"Output Shape\", \"Param #\", \"Connected to\"]\n         alignment = [\"left\", \"left\", \"right\", \"left\"]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8d58b790e4deda87240b73d23b5c19e1d24921da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 78 | Lines Deleted: 26 | Files Changed: 10 | Hunks: 31 | Methods Changed: 24 | Complexity Δ (Sum/Max): -3/4 | Churn Δ: 104 | Churn Cumulative: 3547 | Contributors (this commit): 13 | Commits (past 90d): 32 | Contributors (cumulative): 27 | DMM Complexity: 0.8181818181818182\n\nDIFF:\n@@ -50,7 +50,8 @@ class JAXTrainer(base_trainer.Trainer):\n \n         var_mapping = list(zip(self.trainable_variables, trainable_variables))\n         var_mapping.extend(\n-                zip(self.non_trainable_variables, non_trainable_variables))\n+            zip(self.non_trainable_variables, non_trainable_variables)\n+        )\n         with backend.StatelessScope(state_mapping=var_mapping):\n             # Note that this is needed for the regularization loss, which need\n             # the latest value of train/non-trainable variables.\n\n@@ -82,17 +82,23 @@ def standardize_reduction(reduction):\n     return reduction\n \n \n-def squeeze_to_same_rank(x1, x2):\n-    \"\"\"Squeeze last dim if ranks differ from expected by exactly 1.\"\"\"\n+def squeeze_or_expand_to_same_rank(x1, x2, expand_rank_1=True):\n+    \"\"\"Squeeze/expand last dim if ranks differ from expected by exactly 1.\"\"\"\n     x1_rank = len(x1.shape)\n     x2_rank = len(x2.shape)\n     if x1_rank == x2_rank:\n         return x1, x2\n     if x1_rank == x2_rank + 1:\n         if x1.shape[-1] == 1:\n+            if x2_rank == 1 and expand_rank_1:\n+                x2 = ops.expand_dims(x2, axis=-1)\n+            else:\n                 x1 = ops.squeeze(x1, axis=-1)\n     if x2_rank == x1_rank + 1:\n         if x2.shape[-1] == 1:\n+            if x1_rank == 1 and expand_rank_1:\n+                x1 = ops.expand_dims(x1, axis=-1)\n+            else:\n                 x2 = ops.squeeze(x2, axis=-1)\n     return x1, x2\n \n@@ -137,7 +143,9 @@ def reduce_weighted_values(\n     if sample_weight is not None:\n         sample_weight = ops.cast(sample_weight, values.dtype)\n         # Update dimensions of `sample_weight` to match `losses`.\n-        values, sample_weight = squeeze_to_same_rank(values, sample_weight)\n+        values, sample_weight = squeeze_or_expand_to_same_rank(\n+            values, sample_weight\n+        )\n         values = values * sample_weight\n \n     # Apply reduction function to the individual weighted losses.\n@@ -166,7 +174,9 @@ def apply_mask(sample_weight, mask, dtype, reduction):\n \n         if sample_weight is not None:\n             sample_weight = ops.cast(sample_weight, dtype=dtype)\n-            mask, sample_weight = squeeze_to_same_rank(mask, sample_weight)\n+            mask, sample_weight = squeeze_or_expand_to_same_rank(\n+                mask, sample_weight\n+            )\n             sample_weight *= mask\n         else:\n             sample_weight = mask\n\n@@ -6,6 +6,7 @@ from keras import losses as losses_module\n from keras import ops\n from keras import testing\n from keras.losses.loss import Loss\n+from keras.losses.loss import squeeze_or_expand_to_same_rank\n \n \n class ExampleLoss(Loss):\n@@ -14,6 +15,31 @@ class ExampleLoss(Loss):\n \n \n class LossTest(testing.TestCase):\n+    def test_squeeze_or_expand(self):\n+        x1 = ops.ones((3,))\n+        x2 = ops.ones((3, 1))\n+        x1, x2 = squeeze_or_expand_to_same_rank(x1, x2)\n+        self.assertEqual(ops.shape(x1), (3, 1))\n+        self.assertEqual(ops.shape(x2), (3, 1))\n+\n+        x1 = ops.ones((3, 2))\n+        x2 = ops.ones((3, 2, 1))\n+        x1, x2 = squeeze_or_expand_to_same_rank(x1, x2)\n+        self.assertEqual(ops.shape(x1), (3, 2))\n+        self.assertEqual(ops.shape(x2), (3, 2))\n+\n+        x1 = ops.ones((3,))\n+        x2 = ops.ones((3, 1))\n+        x2, x1 = squeeze_or_expand_to_same_rank(x2, x1)\n+        self.assertEqual(ops.shape(x1), (3, 1))\n+        self.assertEqual(ops.shape(x2), (3, 1))\n+\n+        x1 = ops.ones((3, 2))\n+        x2 = ops.ones((3, 2, 1))\n+        x2, x1 = squeeze_or_expand_to_same_rank(x2, x1)\n+        self.assertEqual(ops.shape(x1), (3, 2))\n+        self.assertEqual(ops.shape(x2), (3, 2))\n+\n     def test_reduction(self):\n         y_true = np.array([1.0, 0.0, 1.0, 0.0])\n         y_pred = np.array([0.1, 0.2, 0.3, 0.4])\n\n@@ -4,7 +4,7 @@ from keras import backend\n from keras import ops\n from keras.api_export import keras_export\n from keras.losses.loss import Loss\n-from keras.losses.loss import squeeze_to_same_rank\n+from keras.losses.loss import squeeze_or_expand_to_same_rank\n from keras.saving import serialization_lib\n from keras.utils.numerical_utils import normalize\n \n@@ -18,7 +18,7 @@ class LossFunctionWrapper(Loss):\n         self._fn_kwargs = kwargs\n \n     def call(self, y_true, y_pred):\n-        y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n+        y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n         return self.fn(y_true, y_pred, **self._fn_kwargs)\n \n     def get_config(self):\n@@ -1150,7 +1150,7 @@ def mean_squared_error(y_true, y_pred):\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n-    y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n+    y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n     return ops.mean(ops.square(y_true - y_pred), axis=-1)\n \n \n@@ -1187,7 +1187,7 @@ def mean_absolute_error(y_true, y_pred):\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n-    y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n+    y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n     return ops.mean(ops.abs(y_true - y_pred), axis=-1)\n \n \n@@ -1232,7 +1232,7 @@ def mean_absolute_percentage_error(y_true, y_pred):\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n     epsilon = ops.convert_to_tensor(backend.epsilon())\n-    y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n+    y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n     diff = ops.abs((y_true - y_pred) / ops.maximum(ops.abs(y_true), epsilon))\n     return 100.0 * ops.mean(diff, axis=-1)\n \n@@ -1278,7 +1278,7 @@ def mean_squared_logarithmic_error(y_true, y_pred):\n     epsilon = ops.convert_to_tensor(backend.epsilon())\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n-    y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n+    y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n     first_log = ops.log(ops.maximum(y_pred, epsilon) + 1.0)\n     second_log = ops.log(ops.maximum(y_true, epsilon) + 1.0)\n     return ops.mean(ops.square(first_log - second_log), axis=-1)\n@@ -1318,7 +1318,7 @@ def cosine_similarity(y_true, y_pred, axis=-1):\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n-    y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n+    y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n     y_pred = normalize(y_pred, axis=axis)\n     y_true = normalize(y_true, axis=axis)\n     return -ops.sum(y_true * y_pred, axis=axis)\n@@ -1359,7 +1359,7 @@ def huber(y_true, y_pred, delta=1.0):\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n-    y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n+    y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n     delta = ops.convert_to_tensor(delta)\n     error = ops.subtract(y_pred, y_true)\n     abs_error = ops.abs(error)\n@@ -1412,7 +1412,7 @@ def log_cosh(y_true, y_pred):\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n-    y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n+    y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n     log2 = ops.convert_to_tensor(ops.log(2.0), dtype=y_pred.dtype)\n \n     def _logcosh(x):\n\n@@ -10,6 +10,13 @@ class MeanSquaredErrorTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(losses.MeanSquaredError(name=\"mymse\"))\n \n+    def test_base_function_reduction(self):\n+        mse_fn = losses.mean_squared_error\n+        y_true = np.array([4, 8, 12])\n+        y_pred = np.array([[3], [0], [1]])\n+        loss = mse_fn(y_true, y_pred)\n+        self.assertEqual(backend.shape(loss), (3,))\n+\n     def test_all_correct_unweighted(self):\n         mse_obj = losses.MeanSquaredError()\n         y_true = np.array([[4, 8, 12], [8, 1, 3]])\n\n@@ -1,14 +1,14 @@\n from keras import backend\n from keras import ops\n from keras.api_export import keras_export\n-from keras.losses.loss import squeeze_to_same_rank\n+from keras.losses.loss import squeeze_or_expand_to_same_rank\n from keras.metrics import reduction_metrics\n \n \n def accuracy(y_true, y_pred):\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n-    y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n+    y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n     return ops.mean(\n         ops.cast(ops.equal(y_true, y_pred), dtype=backend.floatx()),\n         axis=-1,\n@@ -66,7 +66,7 @@ class Accuracy(reduction_metrics.MeanMetricWrapper):\n def binary_accuracy(y_true, y_pred, threshold=0.5):\n     y_true = ops.convert_to_tensor(y_true)\n     y_pred = ops.convert_to_tensor(y_pred)\n-    y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n+    y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n     threshold = ops.cast(threshold, y_pred.dtype)\n     y_pred = ops.cast(y_pred > threshold, y_true.dtype)\n     return ops.mean(\n\n@@ -217,3 +217,9 @@ class Metric:\n                 \"You forgot to call `super().__init__()` \"\n                 \"in the `__init__()` method. Go add it!\"\n             )\n+\n+    def __repr__(self):\n+        return f\"<{self.__class__.__name__} \" f\"name={self.name}>\"\n+\n+    def __str__(self):\n+        return f\"<{self.__class__.__name__} \" f\"name={self.name}>\"\n\n@@ -4,7 +4,7 @@ import numpy as np\n \n from keras import backend\n from keras import ops\n-from keras.losses.loss import squeeze_to_same_rank\n+from keras.losses.loss import squeeze_or_expand_to_same_rank\n from keras.utils.python_utils import to_list\n \n NEG_INF = -1e10\n@@ -453,12 +453,14 @@ def update_confusion_matrix_variables(\n             f'Valid variable key options are: \"{list(ConfusionMatrix)}\"'\n         )\n \n-    y_pred, y_true = squeeze_to_same_rank(y_pred, y_true)\n+    y_pred, y_true = squeeze_or_expand_to_same_rank(y_pred, y_true)\n     if sample_weight is not None:\n         sample_weight = ops.expand_dims(\n             ops.cast(sample_weight, dtype=variable_dtype), axis=-1\n         )\n-        _, sample_weight = squeeze_to_same_rank(y_true, sample_weight)\n+        _, sample_weight = squeeze_or_expand_to_same_rank(\n+            y_true, sample_weight, expand_rank_1=False\n+        )\n \n     if top_k is not None:\n         y_pred = _filter_top_k(y_pred, top_k)\n@@ -664,7 +666,7 @@ def confusion_matrix(\n     \"\"\"\n     labels = ops.convert_to_tensor(labels, dtype)\n     predictions = ops.convert_to_tensor(predictions, dtype)\n-    labels, predictions = squeeze_to_same_rank(labels, predictions)\n+    labels, predictions = squeeze_or_expand_to_same_rank(labels, predictions)\n \n     predictions = ops.cast(predictions, dtype)\n     labels = ops.cast(labels, dtype)\n\n@@ -17,7 +17,7 @@ def reduce_to_samplewise_values(values, sample_weight, reduce_fn, dtype):\n                 sample_weight, mask, dtype=dtype, reduction=\"sum\"\n             )\n         # Update dimensions of weights to match with values if possible.\n-        values, sample_weight = losses.loss.squeeze_to_same_rank(\n+        values, sample_weight = losses.loss.squeeze_or_expand_to_same_rank(\n             values, sample_weight\n         )\n         # Reduce values to same ndim as weight array\n\n@@ -3,7 +3,7 @@ import warnings\n from keras import initializers\n from keras import ops\n from keras.api_export import keras_export\n-from keras.losses.loss import squeeze_to_same_rank\n+from keras.losses.loss import squeeze_or_expand_to_same_rank\n from keras.losses.losses import log_cosh\n from keras.losses.losses import mean_absolute_error\n from keras.losses.losses import mean_absolute_percentage_error\n@@ -245,7 +245,7 @@ class RootMeanSquaredError(reduction_metrics.Mean):\n         \"\"\"\n         y_true = ops.convert_to_tensor(y_true, self._dtype)\n         y_pred = ops.convert_to_tensor(y_pred, self._dtype)\n-        y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n+        y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n         error_sq = ops.square(y_pred - y_true)\n         return super().update_state(error_sq, sample_weight=sample_weight)\n \n@@ -502,7 +502,7 @@ class R2Score(reduction_metrics.Metric):\n         \"\"\"\n         y_true = ops.convert_to_tensor(y_true, dtype=self._dtype)\n         y_pred = ops.convert_to_tensor(y_pred, dtype=self._dtype)\n-        y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n+        y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n         if not self._built:\n             self._build(y_true.shape, y_pred.shape)\n \n@@ -611,7 +611,7 @@ def cosine_similarity(y_true, y_pred, axis=-1):\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n-    y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n+    y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n     y_pred = normalize(y_pred, axis=axis)\n     y_true = normalize(y_true, axis=axis)\n     return ops.sum(y_true * y_pred, axis=axis)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3cd7be13cbe6c60c883ae1ec7244129f9e4712af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 93 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -150,10 +150,11 @@ class Mean(Metric):\n         self.count.assign(0)\n \n     def result(self):\n-        return self.total / (\n-            ops.maximum(\n-                ops.cast(self.count, dtype=self.dtype), backend.epsilon()\n-            )\n+        count = ops.cast(self.count, dtype=self.dtype)\n+        return (\n+            ops.sign(count)\n+            * self.total\n+            / ops.maximum(ops.abs(count), backend.epsilon())\n         )\n \n \n\n@@ -62,6 +62,12 @@ class MeanTest(testing.TestCase):\n         result = mean_obj.result()\n         self.assertAllClose(result, 2.0, atol=1e-3)\n \n+    def test_weighted_negative_weigts(self):\n+        mean_obj = reduction_metrics.Mean(name=\"mean\", dtype=\"float32\")\n+        mean_obj.update_state([1, 3, 5, 7], sample_weight=[-1, -1, 0, 0])\n+        result = mean_obj.result()\n+        self.assertAllClose(result, 2.0, atol=1e-3)\n+\n     def test_weighted_nd(self):\n         mean_obj = reduction_metrics.Mean(name=\"mean\", dtype=\"float32\")\n         mean_obj.update_state([[1, 3], [5, 7]], sample_weight=[[1, 1], [1, 0]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b246242a25d29af3561a1be8796596d67638c0ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 71 | Churn Cumulative: 1646 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 0.7222222222222222\n\nDIFF:\n@@ -142,6 +142,21 @@ class ExportArchive:\n         if isinstance(resource, Layer):\n             # Variables in the lists below are actually part of the trackables\n             # that get saved, because the lists are created in __init__.\n+            if backend.backend() == \"jax\":\n+                self._tf_trackable.variables += tf.nest.flatten(\n+                    tf.nest.map_structure(tf.Variable, resource.variables)\n+                )\n+                self._tf_trackable.trainable_variables += tf.nest.flatten(\n+                    tf.nest.map_structure(\n+                        tf.Variable, resource.trainable_variables\n+                    )\n+                )\n+                self._tf_trackable.non_trainable_variables += tf.nest.flatten(\n+                    tf.nest.map_structure(\n+                        tf.Variable, resource.non_trainable_variables\n+                    )\n+                )\n+            else:\n                 self._tf_trackable.variables += resource.variables\n                 self._tf_trackable.trainable_variables += (\n                     resource.trainable_variables\n\n@@ -114,6 +114,62 @@ class ExportArchiveTest(testing.TestCase):\n         self.assertLen(revived_model.trainable_variables, 6)\n         self.assertLen(revived_model.non_trainable_variables, 2)\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"jax\",\n+        reason=\"This test is native to the JAX backend.\",\n+    )\n+    def test_jax_endpoint_registration_tf_function(self):\n+        model = get_model()\n+        ref_input = np.random.normal(size=(3, 10))\n+        model(ref_input)\n+\n+        # build a JAX function\n+        def model_call(x):\n+            return model(x)\n+\n+        from jax.experimental import jax2tf\n+\n+        # now, convert JAX function\n+        converted_model_call = jax2tf.convert(\n+            model_call,\n+            native_serialization=True,\n+            polymorphic_shapes=[\"(b, 10)\"],\n+        )\n+\n+        # you can now build a TF inference function\n+        @tf.function(\n+            input_signature=[tf.TensorSpec(shape=(None, 10), dtype=tf.float32)],\n+            autograph=False,\n+        )\n+        def infer_fn(x):\n+            return converted_model_call(x)\n+\n+        ref_output = infer_fn(ref_input)\n+\n+        # Export with TF inference function as endpoint\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"my_model\")\n+        export_archive = export_lib.ExportArchive()\n+        export_archive.track(model)\n+        export_archive.add_endpoint(\"serve\", infer_fn)\n+        export_archive.write_out(temp_filepath)\n+\n+        # Reload and verify outputs\n+        revived_model = tf.saved_model.load(temp_filepath)\n+        self.assertFalse(hasattr(revived_model, \"_tracked\"))\n+        self.assertAllClose(\n+            ref_output, revived_model.serve(ref_input), atol=1e-6\n+        )\n+        self.assertLen(revived_model.variables, 8)\n+        self.assertLen(revived_model.trainable_variables, 6)\n+        self.assertLen(revived_model.non_trainable_variables, 2)\n+\n+        # Assert all variables wrapped as `tf.Variable`\n+        assert isinstance(export_archive.variables[0], tf.Variable)\n+        assert isinstance(export_archive.trainable_variables[0], tf.Variable)\n+        assert isinstance(\n+            export_archive.non_trainable_variables[0], tf.Variable\n+        )\n+\n     def test_layer_export(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_layer\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f33c02c6e47e9291d127792b13412b9ac94ae7f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 842 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -127,12 +127,17 @@ class ExportArchiveTest(testing.TestCase):\n         def model_call(x):\n             return model(x)\n \n+        from jax import default_backend as jax_device\n         from jax.experimental import jax2tf\n \n+        native_jax_compatible = not (\n+            jax_device() == \"gpu\"\n+            and len(tf.config.list_physical_devices(\"GPU\")) == 0\n+        )\n         # now, convert JAX function\n         converted_model_call = jax2tf.convert(\n             model_call,\n-            native_serialization=True,\n+            native_serialization=native_jax_compatible,\n             polymorphic_shapes=[\"(b, 10)\"],\n         )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06ac1c225a891ed8b23133f0cc72cd6d2493d898", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1628 | Contributors (this commit): 7 | Commits (past 90d): 19 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -665,7 +665,7 @@ def arange(start, stop=None, step=1, dtype=None):\n         stop: Integer or real, representing the end of the interval. The\n             interval does not include this value, except in some cases where\n             `step` is not an integer and floating point round-off affects the\n-            lenght of `out`. Defaults to `None`.\n+            length of `out`. Defaults to `None`.\n         step: Integer or real, represent the spacing between values. For any\n             output `out`, this is the distance between two adjacent values,\n             `out[i+1] - out[i]`. The default step size is 1. If `step` is\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#08c4b0414ccad56c286bb30fff6d7a97c5721e0a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 47 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.2\n\nDIFF:\n@@ -248,7 +248,6 @@ class TensorBoard(Callback):\n     def _write_keras_model_train_graph(self):\n         \"\"\"Writes Keras model train_function graph to TensorBoard.\"\"\"\n         with self._train_writer.as_default():\n-            with self.summary.record_if(True):\n             train_fn = self.model.train_function\n             # If the train_function is a `tf.function`, we can write out a\n             # graph\n@@ -264,7 +263,6 @@ class TensorBoard(Callback):\n     def _write_keras_model_summary(self):\n         \"\"\"Writes Keras graph network summary to TensorBoard.\"\"\"\n         with self._train_writer.as_default():\n-            with self.summary.record_if(True):\n             if (\n                 self.model.__class__.__name__ == \"Functional\"\n                 or self.model.__class__.__name__ == \"Sequential\"\n@@ -419,7 +417,7 @@ class TensorBoard(Callback):\n \n     def on_test_end(self, logs=None):\n         if self.model.optimizer and hasattr(self.model.optimizer, \"iterations\"):\n-            with self.summary.record_if(True), self._val_writer.as_default():\n+            with self._val_writer.as_default():\n                 for name, value in logs.items():\n                     self.summary.scalar(\n                         \"evaluation_\" + name + \"_vs_iterations\",\n@@ -494,7 +492,6 @@ class TensorBoard(Callback):\n         if batch is None:\n             batch = self._stop_batch\n         with self._train_writer.as_default():\n-            with self.summary.record_if(True):\n             # TODO(b/126388999): Remove step info in the summary name.\n             self.summary.trace_export(name=\"batch_%d\" % batch, step=batch)\n         self._stop_profiler()\n@@ -539,7 +536,6 @@ class TensorBoard(Callback):\n         if self.write_steps_per_second:\n             train_logs[\"steps_per_second\"] = self._compute_steps_per_second()\n \n-        with self.summary.record_if(True):\n         if train_logs:\n             with self._train_writer.as_default():\n                 for name, value in train_logs.items():\n@@ -553,7 +549,6 @@ class TensorBoard(Callback):\n     def _log_weights(self, epoch):\n         \"\"\"Logs the weights of the Model to TensorBoard.\"\"\"\n         with self._train_writer.as_default():\n-            with self.summary.record_if(True):\n             for layer in self.model.layers:\n                 for weight in layer.weights:\n                     weight_name = weight.name.replace(\":\", \"_\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e3858739d178fe16a0c77ce7fab88b0be6dbbdc7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 187 | Lines Deleted: 103 | Files Changed: 10 | Hunks: 60 | Methods Changed: 20 | Complexity Δ (Sum/Max): 18/5 | Churn Δ: 290 | Churn Cumulative: 18721 | Contributors (this commit): 20 | Commits (past 90d): 123 | Contributors (cumulative): 59 | DMM Complexity: 1.0\n\nDIFF:\n@@ -90,7 +90,7 @@ def multiply(x1, x2):\n         if isinstance(x2, jax_sparse.BCOO):\n             # x1 is sparse, x2 is sparse.\n             if x1.indices is x2.indices:\n-                # `bcoo_multiply_sparse`` will not detect that the indices are\n+                # `bcoo_multiply_sparse` will not detect that the indices are\n                 # the same, optimize this case here.\n                 if not x1.unique_indices:\n                     x1 = jax_sparse.bcoo_sum_duplicates(x1)\n@@ -957,7 +957,12 @@ def divide(x1, x2):\n     return jnp.divide(x1, x2)\n \n \n-@sparse.elementwise_division\n+def divide_no_nan(x1, x2):\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n+    return jnp.where(x2 == 0, 0, jnp.divide(x1, x2))\n+\n+\n def true_divide(x1, x2):\n     return divide(x1, x2)\n \n\n@@ -967,6 +967,21 @@ def divide(x1, x2):\n     return np.divide(x1, x2)\n \n \n+def divide_no_nan(x1, x2):\n+    if not isinstance(x1, (int, float)):\n+        x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n+        x2 = convert_to_tensor(x2)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+        float,\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n+    return np.where(x2 == 0, 0, np.divide(x1, x2))\n+\n+\n def true_divide(x1, x2):\n     return divide(x1, x2)\n \n\n@@ -1681,7 +1681,21 @@ def divide(x1, x2):\n     return tf.divide(x1, x2)\n \n \n-@sparse.elementwise_division\n+def divide_no_nan(x1, x2):\n+    if not isinstance(x1, (int, float)):\n+        x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n+        x2 = convert_to_tensor(x2)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+        float,\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n+    return tf.math.divide_no_nan(x1, x2)\n+\n+\n def true_divide(x1, x2):\n     return divide(x1, x2)\n \n\n@@ -1366,6 +1366,14 @@ def divide(x1, x2):\n     return torch.divide(x1, x2)\n \n \n+def divide_no_nan(x1, x2):\n+    if not isinstance(x1, (int, float)):\n+        x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n+        x2 = convert_to_tensor(x2)\n+    return torch.where(x2 == 0, 0, torch.divide(x1, x2))\n+\n+\n def true_divide(x1, x2):\n     return divide(x1, x2)\n \n\n@@ -390,9 +390,9 @@ class Precision(Metric):\n         )\n \n     def result(self):\n-        result = ops.divide(\n+        result = ops.divide_no_nan(\n             self.true_positives,\n-            self.true_positives + self.false_positives + backend.epsilon(),\n+            ops.add(self.true_positives, self.false_positives),\n         )\n         return result[0] if len(self.thresholds) == 1 else result\n \n@@ -534,9 +534,9 @@ class Recall(Metric):\n         )\n \n     def result(self):\n-        result = ops.divide(\n+        result = ops.divide_no_nan(\n             self.true_positives,\n-            self.true_positives + self.false_negatives + backend.epsilon(),\n+            ops.add(self.true_positives, self.false_negatives),\n         )\n         return result[0] if len(self.thresholds) == 1 else result\n \n@@ -757,13 +757,13 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n         )\n \n     def result(self):\n-        sensitivities = ops.divide(\n+        sensitivities = ops.divide_no_nan(\n             self.true_positives,\n-            self.true_positives + self.false_negatives + backend.epsilon(),\n+            ops.add(self.true_positives, self.false_negatives),\n         )\n-        specificities = ops.divide(\n+        specificities = ops.divide_no_nan(\n             self.true_negatives,\n-            self.true_negatives + self.false_positives + backend.epsilon(),\n+            ops.add(self.true_negatives, self.false_positives),\n         )\n         return self._find_max_under_constraint(\n             specificities, sensitivities, ops.greater_equal\n@@ -861,13 +861,13 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n         )\n \n     def result(self):\n-        sensitivities = ops.divide(\n+        sensitivities = ops.divide_no_nan(\n             self.true_positives,\n-            self.true_positives + self.false_negatives + backend.epsilon(),\n+            ops.add(self.true_positives, self.false_negatives),\n         )\n-        specificities = ops.divide(\n+        specificities = ops.divide_no_nan(\n             self.true_negatives,\n-            self.true_negatives + self.false_positives + backend.epsilon(),\n+            ops.add(self.true_negatives, self.false_positives),\n         )\n         return self._find_max_under_constraint(\n             sensitivities, specificities, ops.greater_equal\n@@ -951,13 +951,13 @@ class PrecisionAtRecall(SensitivitySpecificityBase):\n         )\n \n     def result(self):\n-        recalls = ops.divide(\n+        recalls = ops.divide_no_nan(\n             self.true_positives,\n-            self.true_positives + self.false_negatives + backend.epsilon(),\n+            ops.add(self.true_positives, self.false_negatives),\n         )\n-        precisions = ops.divide(\n+        precisions = ops.divide_no_nan(\n             self.true_positives,\n-            self.true_positives + self.false_positives + backend.epsilon(),\n+            ops.add(self.true_positives, self.false_positives),\n         )\n         return self._find_max_under_constraint(\n             recalls, precisions, ops.greater_equal\n@@ -1046,13 +1046,13 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n         )\n \n     def result(self):\n-        recalls = ops.divide(\n+        recalls = ops.divide_no_nan(\n             self.true_positives,\n-            self.true_positives + self.false_negatives + backend.epsilon(),\n+            ops.add(self.true_positives, self.false_negatives),\n         )\n-        precisions = ops.divide(\n+        precisions = ops.divide_no_nan(\n             self.true_positives,\n-            self.true_positives + self.false_positives + backend.epsilon(),\n+            ops.add(self.true_positives, self.false_positives),\n         )\n         return self._find_max_under_constraint(\n             precisions, recalls, ops.greater_equal\n@@ -1439,29 +1439,32 @@ class AUC(Metric):\n             pr_auc: an approximation of the area under the P-R curve.\n         \"\"\"\n \n-        dtp = (\n-            self.true_positives[: self.num_thresholds - 1]\n-            - self.true_positives[1:]\n+        dtp = ops.subtract(\n+            self.true_positives[: self.num_thresholds - 1],\n+            self.true_positives[1:],\n         )\n         p = ops.add(self.true_positives, self.false_positives)\n-        dp = p[: self.num_thresholds - 1] - p[1:]\n-        prec_slope = ops.divide(dtp, ops.maximum(dp, backend.epsilon()))\n-        intercept = self.true_positives[1:] - ops.multiply(prec_slope, p[1:])\n+        dp = ops.subtract(p[: self.num_thresholds - 1], p[1:])\n+        prec_slope = ops.divide_no_nan(dtp, ops.maximum(dp, 0))\n+        intercept = ops.subtract(\n+            self.true_positives[1:], ops.multiply(prec_slope, p[1:])\n+        )\n \n         safe_p_ratio = ops.where(\n             ops.logical_and(p[: self.num_thresholds - 1] > 0, p[1:] > 0),\n-            ops.divide(\n-                p[: self.num_thresholds - 1],\n-                ops.maximum(p[1:], backend.epsilon()),\n+            ops.divide_no_nan(\n+                p[: self.num_thresholds - 1], ops.maximum(p[1:], 0)\n             ),\n             ops.ones_like(p[1:]),\n         )\n \n-        pr_auc_increment = ops.divide(\n-            prec_slope * (dtp + intercept * ops.log(safe_p_ratio)),\n+        pr_auc_increment = ops.divide_no_nan(\n+            ops.multiply(\n+                prec_slope,\n+                (ops.add(dtp, ops.multiply(intercept, ops.log(safe_p_ratio)))),\n+            ),\n             ops.maximum(\n-                self.true_positives[1:] + self.false_negatives[1:],\n-                backend.epsilon(),\n+                ops.add(self.true_positives[1:], self.false_negatives[1:]), 0\n             ),\n         )\n \n@@ -1472,9 +1475,9 @@ class AUC(Metric):\n                 return ops.mean(by_label_auc)\n             else:\n                 # Weighted average of the label AUCs.\n-                return ops.divide(\n+                return ops.divide_no_nan(\n                     ops.sum(ops.multiply(by_label_auc, self.label_weights)),\n-                    ops.maximum(ops.sum(self.label_weights), backend.epsilon()),\n+                    ops.sum(self.label_weights),\n                 )\n         else:\n             return ops.sum(pr_auc_increment)\n@@ -1489,30 +1492,21 @@ class AUC(Metric):\n             return self.interpolate_pr_auc()\n \n         # Set `x` and `y` values for the curves based on `curve` config.\n-        recall = ops.divide(\n+        recall = ops.divide_no_nan(\n             self.true_positives,\n-            ops.maximum(\n             ops.add(self.true_positives, self.false_negatives),\n-                backend.epsilon(),\n-            ),\n         )\n         if self.curve == metrics_utils.AUCCurve.ROC:\n-            fp_rate = ops.divide(\n+            fp_rate = ops.divide_no_nan(\n                 self.false_positives,\n-                ops.maximum(\n                 ops.add(self.false_positives, self.true_negatives),\n-                    backend.epsilon(),\n-                ),\n             )\n             x = fp_rate\n             y = recall\n         else:  # curve == 'PR'.\n-            precision = ops.divide(\n+            precision = ops.divide_no_nan(\n                 self.true_positives,\n-                ops.maximum(\n                 ops.add(self.true_positives, self.false_positives),\n-                    backend.epsilon(),\n-                ),\n             )\n             x = recall\n             y = precision\n@@ -1523,7 +1517,9 @@ class AUC(Metric):\n             == metrics_utils.AUCSummationMethod.INTERPOLATION\n         ):\n             # Note: the case ('PR', 'interpolation') has been handled above.\n-            heights = (y[: self.num_thresholds - 1] + y[1:]) / 2.0\n+            heights = ops.divide(\n+                ops.add(y[: self.num_thresholds - 1], y[1:]), 2.0\n+            )\n         elif self.summation_method == metrics_utils.AUCSummationMethod.MINORING:\n             heights = ops.minimum(y[: self.num_thresholds - 1], y[1:])\n         # self.summation_method = metrics_utils.AUCSummationMethod.MAJORING:\n@@ -1531,10 +1527,10 @@ class AUC(Metric):\n             heights = ops.maximum(y[: self.num_thresholds - 1], y[1:])\n \n         # Sum up the areas of all the rectangles.\n-        if self.multi_label:\n         riemann_terms = ops.multiply(\n-                x[: self.num_thresholds - 1] - x[1:], heights\n+            ops.subtract(x[: self.num_thresholds - 1], x[1:]), heights\n         )\n+        if self.multi_label:\n             by_label_auc = ops.sum(riemann_terms, axis=0)\n \n             if self.label_weights is None:\n@@ -1542,14 +1538,12 @@ class AUC(Metric):\n                 return ops.mean(by_label_auc)\n             else:\n                 # Weighted average of the label AUCs.\n-                return ops.divide(\n+                return ops.divide_no_nan(\n                     ops.sum(ops.multiply(by_label_auc, self.label_weights)),\n-                    ops.maximum(ops.sum(self.label_weights), backend.epsilon()),\n+                    ops.sum(self.label_weights),\n                 )\n         else:\n-            return ops.sum(\n-                ops.multiply(x[: self.num_thresholds - 1] - x[1:], heights)\n-            )\n+            return ops.sum(riemann_terms)\n \n     def reset_state(self):\n         if self._built:\n\n@@ -1386,6 +1386,16 @@ class AUCTest(testing.TestCase):\n         expected_result = 2.416 / 7 + 4 / 7\n         self.assertAllClose(result, expected_result, 1e-3)\n \n+    def test_weighted_pr_interpolation_negative_weights(self):\n+        auc_obj = metrics.AUC(num_thresholds=self.num_thresholds, curve=\"PR\")\n+        sample_weight = [-1, -2, -3, -4]\n+        result = auc_obj(self.y_true, self.y_pred, sample_weight=sample_weight)\n+\n+        # Divisor in auc formula is max(tp[1:]+fn[1:], 0), which is all zeros\n+        # because the all values in tp and fn are negative, divide_no_nan will\n+        # produce all zeros.\n+        self.assertAllClose(result, 0.0, 1e-3)\n+\n     def test_invalid_num_thresholds(self):\n         with self.assertRaisesRegex(\n             ValueError, \"Argument `num_thresholds` must be an integer > 1\"\n\n@@ -1,4 +1,3 @@\n-from keras import backend\n from keras import initializers\n from keras import losses\n from keras import ops\n@@ -150,11 +149,8 @@ class Mean(Metric):\n         self.count.assign(0)\n \n     def result(self):\n-        count = ops.cast(self.count, dtype=self.dtype)\n-        return (\n-            ops.sign(count)\n-            * self.total\n-            / ops.maximum(ops.abs(count), backend.epsilon())\n+        return ops.divide_no_nan(\n+            self.total, ops.cast(self.count, dtype=self.dtype)\n         )\n \n \n\n@@ -62,7 +62,7 @@ class MeanTest(testing.TestCase):\n         result = mean_obj.result()\n         self.assertAllClose(result, 2.0, atol=1e-3)\n \n-    def test_weighted_negative_weigts(self):\n+    def test_weighted_negative_weights(self):\n         mean_obj = reduction_metrics.Mean(name=\"mean\", dtype=\"float32\")\n         mean_obj.update_state([1, 3, 5, 7], sample_weight=[-1, -1, 0, 0])\n         result = mean_obj.result()\n\n@@ -5540,6 +5540,43 @@ def divide(x1, x2):\n     return backend.numpy.divide(x1, x2)\n \n \n+class DivideNoNan(Operation):\n+    def call(self, x1, x2):\n+        return backend.numpy.divide_no_nan(x1, x2)\n+\n+    def compute_output_spec(self, x1, x2):\n+        x1_shape = getattr(x1, \"shape\", [])\n+        x2_shape = getattr(x2, \"shape\", [])\n+        output_shape = broadcast_shapes(x1_shape, x2_shape)\n+        output_dtype = dtypes.result_type(\n+            getattr(x1, \"dtype\", type(x1)),\n+            getattr(x2, \"dtype\", type(x2)),\n+            float,\n+        )\n+        x1_sparse = getattr(x1, \"sparse\", False)\n+        x2_sparse = getattr(x2, \"sparse\", False)\n+        output_sparse = x1_sparse and not x2_sparse\n+        return KerasTensor(\n+            output_shape, dtype=output_dtype, sparse=output_sparse\n+        )\n+\n+\n+@keras_export([\"keras.ops.divide_no_nan\", \"keras.ops.numpy.divide_no_nan\"])\n+def divide_no_nan(x1, x2):\n+    \"\"\"Safe element-wise division which returns 0 where the denominator is 0.\n+\n+    Args:\n+        x1: First input tensor.\n+        x2: Second input tensor.\n+\n+    Returns:\n+        The quotient `x1/x2`, element-wise, with zero where x2 is zero.\n+    \"\"\"\n+    if any_symbolic_tensors((x1, x2)):\n+        return DivideNoNan().symbolic_call(x1, x2)\n+    return backend.numpy.divide_no_nan(x1, x2)\n+\n+\n class TrueDivide(Operation):\n     def call(self, x1, x2):\n         return backend.numpy.true_divide(x1, x2)\n\n@@ -48,6 +48,11 @@ class NumpyTwoInputOpsDynamicShapeTest(testing.TestCase):\n         y = KerasTensor((2, None))\n         self.assertEqual(knp.divide(x, y).shape, (2, 3))\n \n+    def test_divide_no_nan(self):\n+        x = KerasTensor((None, 3))\n+        y = KerasTensor((2, None))\n+        self.assertEqual(knp.divide_no_nan(x, y).shape, (2, 3))\n+\n     def test_true_divide(self):\n         x = KerasTensor((None, 3))\n         y = KerasTensor((2, None))\n@@ -470,6 +475,19 @@ class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n             y = KerasTensor((2, 3, 4))\n             knp.divide(x, y)\n \n+    def test_divide_no_nan(self):\n+        x = KerasTensor((2, 3))\n+        y = KerasTensor((2, 3))\n+        self.assertEqual(knp.divide_no_nan(x, y).shape, (2, 3))\n+\n+        x = KerasTensor((2, 3))\n+        self.assertEqual(knp.divide_no_nan(x, 2).shape, (2, 3))\n+\n+        with self.assertRaises(ValueError):\n+            x = KerasTensor((2, 3))\n+            y = KerasTensor((2, 3, 4))\n+            knp.divide_no_nan(x, y)\n+\n     def test_true_divide(self):\n         x = KerasTensor((2, 3))\n         y = KerasTensor((2, 3))\n@@ -2025,6 +2043,17 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.Divide()(x, y), np.divide(x, y))\n         self.assertAllClose(knp.Divide()(x, z), np.divide(x, z))\n \n+    def test_divide_no_nan(self):\n+        x = np.array(\n+            [[2, 1, 0], [np.inf, -np.inf, np.nan], [np.inf, -np.inf, np.nan]]\n+        )\n+        y = np.array([[2, 0, 0], [0, 0, 0], [3, 2, 1]])\n+        expected_result = np.array(\n+            [[1, 0, 0], [0, 0, 0], [np.inf, -np.inf, np.nan]]\n+        )\n+        self.assertAllClose(knp.divide_no_nan(x, y), expected_result)\n+        self.assertAllClose(knp.DivideNoNan()(x, y), expected_result)\n+\n     def test_true_divide(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         y = np.array([[4, 5, 6], [3, 2, 1]])\n@@ -4320,47 +4349,23 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(op_class()(x, y), expected_result)\n         self.assertSparseness(op_class()(x, y), op_sparseness(x, y))\n \n-    def test_divide_with_zeros_in_int_sparse_tensor(self):\n-        x = backend.convert_to_tensor([[0, 2, 3], [3, 2, 1]], dtype=\"int32\")\n-        x = create_sparse_tensor(x, start=0, delta=2)\n-        y = backend.convert_to_tensor([[0, 0, 0], [0, 0, 0]], dtype=\"int32\")\n-        expected_result = np.divide(\n-            backend.convert_to_numpy(x), backend.convert_to_numpy(y)\n+    @parameterized.named_parameters(\n+        named_product(\n+            sparse_type=[\"sparse_tensor\", \"indexed_slices\"],\n+            dtype=[\"int32\", \"float32\"],\n         )\n-\n-        self.assertAllClose(knp.divide(x, y), expected_result)\n-        self.assertAllClose(knp.Divide()(x, y), expected_result)\n-\n-    def test_divide_with_zeros_nans_in_float_sparse_tensor(self):\n-        x = backend.convert_to_tensor([[0, 2, 3], [3, 2, 1]], dtype=\"float32\")\n-        x = create_sparse_tensor(x, start=0, delta=2)\n-        y = backend.convert_to_tensor(\n-            [[np.nan, np.nan, 3], [0, 0, 1]], dtype=\"float32\"\n     )\n-        expected_result = np.divide(\n-            backend.convert_to_numpy(x), backend.convert_to_numpy(y)\n-        )\n-\n-        self.assertAllClose(knp.divide(x, y), expected_result)\n-        self.assertAllClose(knp.Divide()(x, y), expected_result)\n-\n-    def test_divide_with_zeros_in_int_indexed_slices(self):\n-        x = backend.convert_to_tensor([[0, 2, 3], [3, 2, 1]], dtype=\"int32\")\n+    def test_divide_with_zeros_nans(self, sparse_type, dtype):\n+        x = backend.convert_to_tensor([[0, 2, 3], [3, 2, 1]], dtype=dtype)\n+        if sparse_type == \"indexed_slices\":\n             x = create_indexed_slices(x, start=0, delta=2)\n-        y = backend.convert_to_tensor([[0, 0, 3], [0, 0, 1]], dtype=\"int32\")\n-        expected_result = np.divide(\n-            backend.convert_to_numpy(x), backend.convert_to_numpy(y)\n-        )\n-\n-        self.assertAllClose(knp.divide(x, y), expected_result)\n-        self.assertAllClose(knp.Divide()(x, y), expected_result)\n-\n-    def test_divide_with_zeros_nans_in_float_indexed_slices(self):\n-        x = backend.convert_to_tensor([[0, 2, 3], [3, 2, 1]], dtype=\"float32\")\n-        x = create_indexed_slices(x, start=0, delta=2)\n-        y = backend.convert_to_tensor(\n-            [[np.nan, 0, 3], [np.nan, 0, 1]], dtype=\"float32\"\n-        )\n+        else:\n+            x = create_sparse_tensor(x, start=0, delta=2)\n+        if dtype.startswith(\"int\"):\n+            y = [[0, 0, 3], [0, 0, 1]]\n+        else:\n+            y = [[np.nan, np.nan, 3], [0, 0, 1]]\n+        y = backend.convert_to_tensor(y, dtype=dtype)\n         expected_result = np.divide(\n             backend.convert_to_numpy(x), backend.convert_to_numpy(y)\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4917d97d0aa238943ba5ab743c8ec874c701ba96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 113 | Lines Deleted: 13 | Files Changed: 6 | Hunks: 12 | Methods Changed: 11 | Complexity Δ (Sum/Max): 19/17 | Churn Δ: 126 | Churn Cumulative: 1034 | Contributors (this commit): 9 | Commits (past 90d): 31 | Contributors (cumulative): 27 | DMM Complexity: 0.16470588235294117\n\nDIFF:\n@@ -334,7 +334,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 (x, y, sample_weight), validation_split=validation_split\n             )\n \n-        if validation_data:\n+        if validation_data is not None:\n             (\n                 val_x,\n                 val_y,\n@@ -428,7 +428,9 @@ class JAXTrainer(base_trainer.Trainer):\n                 epoch_logs = self.get_metrics_result()\n \n             # Run validation.\n-            if validation_data and self._should_eval(epoch, validation_freq):\n+            if validation_data is not None and self._should_eval(\n+                epoch, validation_freq\n+            ):\n                 # Create JAXEpochIterator for evaluation and cache it.\n                 if getattr(self, \"_eval_epoch_iterator\", None) is None:\n                     self._eval_epoch_iterator = JAXEpochIterator(\n\n@@ -277,7 +277,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 (x, y, sample_weight), validation_split=validation_split\n             )\n \n-        if validation_data:\n+        if validation_data is not None:\n             (\n                 val_x,\n                 val_y,\n@@ -331,7 +331,9 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             epoch_logs = self.get_metrics_result()\n \n             # Run validation.\n-            if validation_data and self._should_eval(epoch, validation_freq):\n+            if validation_data is not None and self._should_eval(\n+                epoch, validation_freq\n+            ):\n                 # Create EpochIterator for evaluation and cache it.\n                 if getattr(self, \"_eval_epoch_iterator\", None) is None:\n                     self._eval_epoch_iterator = TFEpochIterator(\n\n@@ -197,7 +197,7 @@ class TorchTrainer(base_trainer.Trainer):\n                 (x, y, sample_weight), validation_split=validation_split\n             )\n \n-        if validation_data:\n+        if validation_data is not None:\n             (\n                 val_x,\n                 val_y,\n@@ -260,7 +260,9 @@ class TorchTrainer(base_trainer.Trainer):\n             self.eval()\n \n             # Run validation.\n-            if validation_data and self._should_eval(epoch, validation_freq):\n+            if validation_data is not None and self._should_eval(\n+                epoch, validation_freq\n+            ):\n                 # Create TorchEpochIterator for evaluation and cache it.\n                 if getattr(self, \"_eval_epoch_iterator\", None) is None:\n                     self._eval_epoch_iterator = TorchEpochIterator(\n\n@@ -75,7 +75,7 @@ class TFDatasetAdapter(DataAdapter):\n         else:\n             # However, in the case of `DistributedDataset`, it's a np.int64.\n             cardinality = int(cardinality)\n-        # Return None for Unknown and Infiite cardinality datasets\n+        # Return None for Unknown and Infinite cardinality datasets\n         if cardinality < 0:\n             return None\n         return cardinality\n\n@@ -20,8 +20,14 @@ class TorchDataLoaderAdapter(DataAdapter):\n \n         self._dataloader = dataloader\n         self._batch_size = dataloader.batch_size\n-        self._size = len(dataloader)\n-        self._partial_batch_size = len(dataloader.dataset) % self._batch_size\n+        self._num_batches = None\n+        self._partial_batch_size = None\n+        if hasattr(dataloader.dataset, \"__len__\"):\n+            self._num_batches = len(dataloader)\n+            if self._batch_size is not None:\n+                self._partial_batch_size = (\n+                    len(dataloader.dataset) % self._batch_size\n+                )\n \n     def get_numpy_iterator(self):\n         for batch in self._dataloader:\n@@ -70,7 +76,7 @@ class TorchDataLoaderAdapter(DataAdapter):\n \n     @property\n     def num_batches(self):\n-        return self._size\n+        return self._num_batches\n \n     @property\n     def batch_size(self):\n\n@@ -1,3 +1,5 @@\n+import math\n+\n import jax\n import numpy as np\n import tensorflow as tf\n@@ -18,9 +20,9 @@ class TestTorchDataLoaderAdapter(testing.TestCase, parameterized.TestCase):\n     def test_basic_dataloader(self, iterator_type):\n         x = torch.normal(2, 3, size=(34, 4))\n         y = torch.normal(1, 3, size=(34, 2))\n-        base_ds = torch.utils.data.TensorDataset(x, y)\n-        base_dataloader = torch.utils.data.DataLoader(base_ds, batch_size=16)\n-        adapter = TorchDataLoaderAdapter(base_dataloader)\n+        ds = torch.utils.data.TensorDataset(x, y)\n+        dataloader = torch.utils.data.DataLoader(ds, batch_size=16)\n+        adapter = TorchDataLoaderAdapter(dataloader)\n \n         self.assertEqual(adapter.num_batches, 3)\n         self.assertEqual(adapter.batch_size, 16)\n@@ -53,3 +55,89 @@ class TestTorchDataLoaderAdapter(testing.TestCase, parameterized.TestCase):\n             else:\n                 self.assertEqual(bx.shape, (2, 4))\n                 self.assertEqual(by.shape, (2, 2))\n+\n+    @parameterized.named_parameters(\n+        named_product(\n+            batch_size=[None, 3],\n+            implements_len=[True, False],\n+            iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"],\n+        )\n+    )\n+    def test_dataloader_iterable_dataset(\n+        self, batch_size, implements_len, iterator_type\n+    ):\n+\n+        class TestIterableDataset(torch.utils.data.IterableDataset):\n+            def __init__(self):\n+                self.x = torch.normal(2, 3, size=(16, 4))\n+                self.y = torch.normal(1, 3, size=(16, 2))\n+\n+            def __iter__(self):\n+                for _ in range(10):\n+                    yield (self.x, self.y)\n+\n+        class TestIterableDatasetWithLen(TestIterableDataset):\n+            def __len__(self):\n+                return 10\n+\n+        ds = (\n+            TestIterableDatasetWithLen()\n+            if implements_len\n+            else TestIterableDataset()\n+        )\n+        dataloader = torch.utils.data.DataLoader(ds, batch_size=batch_size)\n+        adapter = TorchDataLoaderAdapter(dataloader)\n+\n+        if implements_len and batch_size:\n+            self.assertEqual(adapter.num_batches, math.ceil(10 / batch_size))\n+            self.assertEqual(adapter.batch_size, batch_size)\n+            self.assertEqual(adapter.has_partial_batch, True)\n+            self.assertEqual(adapter.partial_batch_size, 10 % batch_size)\n+        elif implements_len:\n+            self.assertEqual(adapter.num_batches, 10)\n+            self.assertEqual(adapter.batch_size, None)\n+            self.assertEqual(adapter.has_partial_batch, None)\n+            self.assertEqual(adapter.partial_batch_size, None)\n+        else:\n+            self.assertIsNone(adapter.num_batches)\n+            self.assertEqual(adapter.batch_size, batch_size)\n+            self.assertIsNone(adapter.has_partial_batch)\n+            self.assertIsNone(adapter.partial_batch_size)\n+\n+        if iterator_type == \"np\":\n+            it = adapter.get_numpy_iterator()\n+            expected_class = np.ndarray\n+        elif iterator_type == \"tf\":\n+            it = adapter.get_tf_dataset()\n+            expected_class = tf.Tensor\n+        elif iterator_type == \"jax\":\n+            it = adapter.get_jax_iterator()\n+            expected_class = jax.Array\n+        elif iterator_type == \"torch\":\n+            it = adapter.get_torch_dataloader()\n+            expected_class = torch.Tensor\n+\n+        batch_count = 0\n+        for i, batch in enumerate(it):\n+            batch_count += 1\n+            self.assertEqual(len(batch), 2)\n+            bx, by = batch\n+            self.assertIsInstance(bx, expected_class)\n+            self.assertIsInstance(by, expected_class)\n+            self.assertEqual(bx.dtype, by.dtype)\n+            self.assertContainsExactSubsequence(str(bx.dtype), \"float32\")\n+            if batch_size:\n+                if i < 3:\n+                    self.assertEqual(bx.shape, (batch_size, 16, 4))\n+                    self.assertEqual(by.shape, (batch_size, 16, 2))\n+                else:\n+                    self.assertEqual(bx.shape, (10 % batch_size, 16, 4))\n+                    self.assertEqual(by.shape, (10 % batch_size, 16, 2))\n+            else:\n+                self.assertEqual(bx.shape, (16, 4))\n+                self.assertEqual(by.shape, (16, 2))\n+\n+        if batch_size:\n+            self.assertEqual(batch_count, math.ceil(10 / batch_size))\n+        else:\n+            self.assertEqual(batch_count, 10)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
