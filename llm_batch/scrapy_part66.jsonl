{"custom_id": "scrapy#467dbef20e912c6efbcf861d8a53178d017d0613", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 167 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -61,6 +61,8 @@ def usage(prog):\n     return s\n \n def update_default_settings(module, cmdname):\n+    if not module:\n+        return\n     try:\n         mod = __import__('%s.%s' % (module, cmdname), {}, {}, [''])\n     except ImportError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b8eb08f444eb5e455c398796ab7f7685fd1b5abb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 24 | Files Changed: 4 | Hunks: 16 | Methods Changed: 9 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 50 | Churn Cumulative: 1125 | Contributors (this commit): 5 | Commits (past 90d): 55 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -236,7 +236,8 @@ class ExecutionEngine(object):\n             assert isinstance(response, (Response, Request))\n             if isinstance(response, Response):\n                 response.request = request # tie request to response received\n-                log.msg(\"Crawled %s from <%s>\" % (response, referer), level=log.DEBUG, domain=domain)\n+                log.msg(\"Crawled %s from <%s>\" % (response, referer), level=log.DEBUG, \\\n+                    domain=domain)\n                 return response\n             elif isinstance(response, Request):\n                 newrequest = response\n\n@@ -127,7 +127,8 @@ class Scraper(object):\n \n     def handle_spider_error(self, _failure, request, spider, propagated_failure=None):\n         referer = request.headers.get('Referer', None)\n-        msg = \"SPIDER BUG processing <%s> from <%s>: %s\" % (request.url, referer, _failure)\n+        msg = \"Spider exception caught while processing <%s> (referer: <%s>): %s\" % \\\n+            (request.url, referer, _failure)\n         log.msg(msg, log.ERROR, domain=spider.domain_name)\n         stats.inc_value(\"spider_exceptions/%s\" % _failure.value.__class__.__name__, \\\n             domain=spider.domain_name)\n\n@@ -14,8 +14,10 @@ html_parser_options = libxml2.HTML_PARSE_RECOVER + \\\n                       libxml2.HTML_PARSE_NOERROR + \\\n                       libxml2.HTML_PARSE_NOWARNING\n \n+utf8_encodings = set(('utf-8', 'UTF-8', 'utf8', 'UTF8'))\n+\n def body_as_utf8(response):\n-    if response.encoding in ('utf-8', 'utf8'):\n+    if response.encoding in utf8_encodings:\n         return response.body\n     else:\n         return response.body_as_unicode().encode('utf-8')\n@@ -24,16 +26,20 @@ def xmlDoc_from_html(response):\n     \"\"\"Return libxml2 doc for HTMLs\"\"\"\n     utf8body = body_as_utf8(response)\n     try:\n-        lxdoc = libxml2.htmlReadDoc(utf8body, response.url, 'utf-8', html_parser_options)\n+        lxdoc = libxml2.htmlReadDoc(utf8body, response.url, 'utf-8', \\\n+            html_parser_options)\n     except TypeError:  # libxml2 doesn't parse text with null bytes\n-        lxdoc = libxml2.htmlReadDoc(utf8body.replace(\"\\x00\", \"\"), response.url, 'utf-8', html_parser_options)\n+        lxdoc = libxml2.htmlReadDoc(utf8body.replace(\"\\x00\", \"\"), response.url, \\\n+            'utf-8', html_parser_options)\n     return lxdoc\n \n def xmlDoc_from_xml(response):\n     \"\"\"Return libxml2 doc for XMLs\"\"\"\n     utf8body = body_as_utf8(response)\n     try:\n-        lxdoc = libxml2.readDoc(utf8body, response.url, 'utf-8', xml_parser_options)\n+        lxdoc = libxml2.readDoc(utf8body, response.url, 'utf-8', \\\n+            xml_parser_options)\n     except TypeError:  # libxml2 doesn't parse text with null bytes\n-        lxdoc = libxml2.readDoc(utf8body.replace(\"\\x00\", \"\"), response.url, 'utf-8', xml_parser_options)\n+        lxdoc = libxml2.readDoc(utf8body.replace(\"\\x00\", \"\"), response.url, \\\n+            'utf-8', xml_parser_options)\n     return lxdoc\n\n@@ -13,17 +13,6 @@ from scrapy.utils.python import flatten, unicode_to_str\n from scrapy.utils.misc import extract_regex\n \n class XPathSelector(object):\n-    \"\"\"The XPathSelector class provides a convenient way for selecting document\n-    parts using XPaths and regexs, with support for nested queries.\n-\n-    Although this is not an abstract class, you usually instantiate one of its\n-    children:\n-    \n-    - XmlXPathSelector (for XML content)\n-    - HtmlXPathSelector (for HTML content)\n-    \"\"\"\n-\n-    xmlDoc_factory = staticmethod(xmlDoc_from_html)\n \n     def __init__(self, response=None, text=None, node=None, parent=None, expr=None):\n         if parent:\n@@ -32,14 +21,14 @@ class XPathSelector(object):\n         elif response:\n             try:\n                 # try with cached version first\n-                self.doc = response.getlibxml2doc(factory=self.xmlDoc_factory)\n+                self.doc = response.getlibxml2doc(factory=self._get_libxml2_doc)\n             except AttributeError:\n-                self.doc = Libxml2Document(response, factory=self.xmlDoc_factory)\n+                self.doc = Libxml2Document(response, factory=self._get_libxml2_doc)\n             self.xmlNode = self.doc.xmlDoc\n         elif text:\n             response = TextResponse(url=None, body=unicode_to_str(text), \\\n                 encoding='utf-8')\n-            self.doc = Libxml2Document(response, factory=self.xmlDoc_factory)\n+            self.doc = Libxml2Document(response, factory=self._get_libxml2_doc)\n             self.xmlNode = self.doc.xmlDoc\n         self.expr = expr\n         self.response = response\n@@ -101,11 +90,16 @@ class XPathSelector(object):\n         \"\"\"Register namespace so that it can be used in XPath queries\"\"\"\n         self.doc.xpathContext.xpathRegisterNs(prefix, uri)\n \n+    def _get_libxml2_doc(self, response):\n+        \"\"\"Return libxml2 document (xmlDoc) from response\"\"\"\n+        return xmlDoc_from_html(response)\n+\n     def __nonzero__(self):\n         return bool(self.extract())\n \n     def __str__(self):\n-        return \"<%s (%s) xpath=%s>\" % (type(self).__name__, getattr(self.xmlNode, 'name', type(self.xmlNode).__name__), self.expr)\n+        return \"<%s (%s) xpath=%s>\" % (type(self).__name__, getattr(self.xmlNode, \\\n+            'name', type(self.xmlNode).__name__), self.expr)\n \n     __repr__ = __str__\n \n@@ -137,9 +131,9 @@ class XPathSelectorList(list):\n \n class XmlXPathSelector(XPathSelector):\n     \"\"\"XPathSelector for XML content\"\"\"\n-    xmlDoc_factory = staticmethod(xmlDoc_from_xml)\n+    _get_libxml2_doc = staticmethod(xmlDoc_from_xml)\n \n \n class HtmlXPathSelector(XPathSelector):\n     \"\"\"XPathSelector for HTML content\"\"\"\n-    xmlDoc_factory = staticmethod(xmlDoc_from_html)\n+    _get_libxml2_doc = staticmethod(xmlDoc_from_html)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#80b96fc1951656fba19e4194389b3e7ad331956f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 700 | Contributors (this commit): 4 | Commits (past 90d): 40 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -229,14 +229,14 @@ class ExecutionEngine(object):\n \n     def download(self, request, spider):\n         domain = spider.domain_name\n-        referer = request.headers.get('Referer', None)\n+        referer = request.headers.get('Referer')\n \n         def _on_success(response):\n             \"\"\"handle the result of a page download\"\"\"\n             assert isinstance(response, (Response, Request))\n             if isinstance(response, Response):\n                 response.request = request # tie request to response received\n-                log.msg(\"Crawled %s from <%s>\" % (response, referer), level=log.DEBUG, \\\n+                log.msg(\"Crawled %s (referer: <%s>)\" % (response, referer), level=log.DEBUG, \\\n                     domain=domain)\n                 return response\n             elif isinstance(response, Request):\n@@ -248,8 +248,10 @@ class ExecutionEngine(object):\n         def _on_error(_failure):\n             \"\"\"handle an error processing a page\"\"\"\n             ex = _failure.value\n-            errmsg = str(_failure) if not isinstance(ex, IgnoreRequest) else _failure.getErrorMessage()\n-            log.msg(\"Downloading <%s> from <%s>: %s\" % (request.url, referer, errmsg), log.ERROR, domain=domain)\n+            errmsg = str(_failure) if not isinstance(ex, IgnoreRequest) \\\n+                else _failure.getErrorMessage()\n+            log.msg(\"Downloading <%s> (referer: <%s>): %s\" % (request.url, referer, errmsg), \\\n+                log.ERROR, domain=domain)\n             return Failure(IgnoreRequest(str(ex)))\n \n         def _on_complete(_):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5be0cee57740a74e9306d3b782b551b6227895a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 10 | Churn Cumulative: 821 | Contributors (this commit): 1 | Commits (past 90d): 14 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,7 +26,7 @@ class TreeExpander(object):\n     def wrap_with_args(self, f):\n         return lambda x, y: f(x, loader_args=y)\n \n-    def __call__(self, value, loader_args):\n+    def __call__(self, value, loader_args=None):\n         values = arg_to_iter(value)\n         largs = self.default_loader_args\n         if loader_args:\n\n@@ -70,6 +70,14 @@ class LoaderTest(unittest.TestCase):\n         self.assertEqual(il.get_expanded_value('name'), [u'Pepe'])\n         self.assertEqual(il.get_reduced_value('name'), u'Pepe')\n \n+    def test_tree_expander_filter(self):\n+        def filter_world(x):\n+            return None if x == 'world' else x\n+\n+        expander = TreeExpander(filter_world, str.upper)\n+        self.assertEqual(expander(['hello', 'world', 'this', 'is', 'scrapy']),\n+                         ['HELLO', 'THIS', 'IS', 'SCRAPY'])\n+\n     def test_tree_expander_multiple_functions(self):\n         class TestLoader(NameLoader):\n             name_exp = TreeExpander(lambda v: v.title(), lambda v: v[:-1])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dfa4a4846ce49438bd5fcfb565fe9b67fd8ab80d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 17 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,13 +1,9 @@\n \"\"\"\n-The scrapy.xpath module provides useful classes for parsing HTML and XML\n+The scrapy.xpath module provides useful classes for selecting HTML and XML\n documents using XPath. It requires libxml2 and its python bindings.\n \n This parent module exports the classes most commonly used when building\n spiders, for convenience.\n-\n-* XPath - a simple class to represent a XPath expression\n-* XPathSelector - to extract data using XPaths (parses the entire response)\n \"\"\"\n \n-from scrapy.xpath.types import XPath\n from scrapy.xpath.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n\n@@ -1,11 +0,0 @@\n-class XPath(object):\n-    \"\"\"A XPath expression\"\"\"\n-\n-    def __init__(self, xpath_expr):\n-        self.expr = xpath_expr\n-\n-    def __repr__(self):\n-        return \"XPath(%s)\" % repr(self.expr)\n-\n-    def __str__(self):\n-        return self.expr\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#49bab4777b42937784bcf6f8494031f08d472c09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 8 | Methods Changed: 9 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 24 | Churn Cumulative: 989 | Contributors (this commit): 1 | Commits (past 90d): 21 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -72,14 +72,18 @@ class XPathLoader(Loader):\n                 \"or response\" % self.__class__.__name__)\n         if selector is None:\n             selector = self.default_selector_class(response)\n-        self._selector = selector\n+        self.selector = selector\n         loader_args.update(selector=selector, response=response)\n         super(XPathLoader, self).__init__(item, **loader_args)\n \n-    def add_xpath(self, field_name, xpath, **new_loader_args):\n-        self.add_value(field_name, self._selector.x(xpath).extract(), \\\n+    def add_xpath(self, field_name, xpath, re=None, **new_loader_args):\n+        self.add_value(field_name, self._get_values(field_name, xpath, re),\n             **new_loader_args)\n \n-    def replace_xpath(self, field_name, xpath, **new_loader_args):\n-        self.replace_value(field_name, self._selector.x(xpath).extract(), \\\n+    def replace_xpath(self, field_name, xpath, re=None, **new_loader_args):\n+        self.replace_value(field_name, self._get_values(field_name, xpath, re), \\\n             **new_loader_args)\n+\n+    def _get_values(self, field_name, xpath, re):\n+        x = self.selector.x(xpath)\n+        return x.re(re) if re else x.extract()\n\n@@ -249,14 +249,24 @@ class XPathLoaderTest(unittest.TestCase):\n     def test_constructor_with_selector(self):\n         sel = HtmlXPathSelector(text=u\"<html><body><div>marta</div></body></html>\")\n         l = TestXPathLoader(selector=sel)\n+        self.assert_(l.selector is sel)\n         l.add_xpath('name', '//div/text()')\n         self.assertEqual(l.get_reduced_value('name'), u'Marta')\n \n     def test_constructor_with_response(self):\n         response = HtmlResponse(url=\"\", body=\"<html><body><div>marta</div></body></html>\")\n         l = TestXPathLoader(response=response)\n+        self.assert_(l.selector)\n         l.add_xpath('name', '//div/text()')\n         self.assertEqual(l.get_reduced_value('name'), u'Marta')\n \n+    def test_add_xpath_re(self):\n+        response = HtmlResponse(url=\"\", body=\"<html><body><div>marta</div></body></html>\")\n+        l = TestXPathLoader(response=response)\n+        l.add_xpath('name', '//div/text()', re='ma')\n+        self.assertEqual(l.get_reduced_value('name'), u'Ma')\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7078cdc3360472de57c31d52a9b4a6bf1c770067", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 436 | Files Changed: 5 | Hunks: 4 | Methods Changed: 34 | Complexity Δ (Sum/Max): -18/0 | Churn Δ: 436 | Churn Cumulative: 448 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: 0.10096153846153846\n\nDIFF:\n\n@@ -1,115 +0,0 @@\n-\"\"\"A django alike request-response model\n-\n-most of this code is borrowed from django\n-\"\"\"\n-\n-from Cookie import SimpleCookie\n-from scrapy.utils.datatypes import MultiValueDict, CaselessDict\n-\n-\n-def build_httprequest(twistedrequest):\n-    \"\"\"Translate twisted request object to a django request approach\"\"\"\n-    request = HttpRequest()\n-    request.path = twistedrequest.path\n-    request.method = twistedrequest.method.upper()\n-    request.COOKIES = SimpleCookie(twistedrequest.received_cookies)\n-    request.HEADERS = Headers(twistedrequest.received_headers)\n-    request.ARGS = MultiValueDict(twistedrequest.args)\n-    request.FILES = {} # not yet supported\n-    request.content = twistedrequest.content\n-    request.twistedrequest = twistedrequest\n-    return request\n-\n-\n-class HttpRequest(object):\n-    def __init__(self):\n-        self.path = ''\n-        self.method = None\n-        self.COOKIES = {}\n-        self.HEADERS = {}\n-        self.ARGS = {}\n-        self.FILES = {}\n-\n-\n-class HttpResponse(object):\n-    status_code = 200\n-\n-    def __init__(self, content='', status=None, content_type=None):\n-        content_type = content_type or \"text/html; charset=utf-8\"\n-        self._headers = {'content-type': content_type}\n-        self.content = content\n-        self.cookies = SimpleCookie()\n-        self.status_code = status\n-\n-    def __str__(self):\n-        \"Full HTTP message, including headers\"\n-        return '\\n'.join(['%s: %s' % (key, value)\n-            for key, value in self._headers.items()]) \\\n-            + '\\n\\n' + self.content\n-\n-    def __setitem__(self, header, value):\n-        self._headers[header.lower()] = value\n-\n-    def __delitem__(self, header):\n-        try:\n-            del self._headers[header.lower()]\n-        except KeyError:\n-            pass\n-\n-    def __getitem__(self, header):\n-        return self._headers[header.lower()]\n-\n-    def has_header(self, header):\n-        \"Case-insensitive check for a header\"\n-        return self._headers.has_key(header.lower())\n-\n-    __contains__ = has_header\n-\n-    def items(self):\n-        return self._headers.items()\n-\n-    def get(self, header, alternate):\n-        return self._headers.get(header, alternate)\n-\n-    def set_cookie(self, key, value='', max_age=None, expires=None, path='/', domain=None, secure=None):\n-        self.cookies[key] = value\n-        for var in ('max_age', 'path', 'domain', 'secure', 'expires'):\n-            val = locals()[var]\n-            if val is not None:\n-                self.cookies[key][var.replace('_', '-')] = val\n-\n-    def delete_cookie(self, key, path='/', domain=None):\n-        self.cookies[key] = ''\n-        if path is not None:\n-            self.cookies[key]['path'] = path\n-        if domain is not None:\n-            self.cookies[key]['domain'] = domain\n-        self.cookies[key]['expires'] = 0\n-        self.cookies[key]['max-age'] = 0\n-\n-\n-class Headers(CaselessDict):\n-    def __init__(self, source=None, encoding='utf-8'):\n-        self.encoding = encoding\n-\n-        if getattr(source, 'iteritems', None):\n-            d = source.iteritems()\n-        else:\n-            d = source # best effort\n-\n-        # can't use CaselessDict.__init__(self, d) because it doesn't call __setitem__\n-        for k,v in d:\n-            self.__setitem__(k.lower(), v) \n-\n-    def normkey(self, key):\n-        return key.title() # 'Content-Type' styles headers\n-\n-    def __setitem__(self, key, value):\n-        \"\"\"Headers must not be unicode\"\"\"\n-        if isinstance(key, unicode):\n-            key = key.encode(self.encoding)\n-        if isinstance(value, unicode):\n-            value = value.encode(self.encoding)\n-        super(Headers, self).__setitem__(key, value)\n-\n-\n\n@@ -1,99 +0,0 @@\n-from functools import wraps\n-\n-from scrapy.utils.defer import mustbe_deferred\n-from scrapy.utils.serialization import serialize as _serialize, unserialize as _unserialize\n-\n-from .http import HttpResponse\n-\n-JSON_CONTENT_TYPES = ('application/json',)\n-\n-serialize = lambda x: _serialize(x, 'json')\n-unserialize = lambda x: _unserialize(x, 'json')\n-\n-class JsonException(Exception):\n-    pass\n-\n-class JsonResponse(HttpResponse):\n-    def __init__(self, content=None, callback=None, serialize=serialize, *args, **kwargs):\n-        content = serialize(content)\n-        if callback: # JSONP support\n-            status, content = 200, '%s(%s)' % (callback, content)\n-        kwargs.setdefault('content_type', 'application/x-javascript')\n-        HttpResponse.__init__(self, content=content, *args, **kwargs)\n-\n-class JsonResponseAccepted(JsonResponse):\n-    status_code = 202\n-\n-class JsonResponseNoContent(JsonResponse):\n-    status_code = 204\n-\n-class JsonResponseNotModified(JsonResponse):\n-    status_code = 304\n-\n-class JsonResponseBadRequest(JsonResponse):\n-    status_code = 400\n-\n-class JsonResponseUnauthorized(JsonResponse):\n-    status_code = 401\n-\n-class JsonResponseForbidden(JsonResponse):\n-    status_code = 403\n-\n-class JsonResponseNotFound(JsonResponse):\n-    status_code = 404\n-\n-class JsonResponseInternalServerError(JsonResponse):\n-    status_code = 500\n-\n-class JsonResponseNotImplemented(JsonResponse):\n-    status_code = 501\n-\n-\n-def json(func):\n-    \"\"\" Decorator to wrap a json prepared view and return a JsonResponse\n-\n-    if content-type is application/json, sets request.JSON to unserialized request body.\n-    in case of unserialization failure, returns JsonResponseBadRequest()\n-\n-    if returned data from func is a dictionary, serialize it and returns JsonResponse()\n-    \"\"\"\n-    if not hasattr(func, '__call__'):\n-        raise TypeError('The argument should be a callable')\n-\n-    @wraps(func)\n-    def wrapper(request, *args, **kwargs):\n-        json_callback = request.ARGS.get('callback') # JSONP support\n-        request.method = method = _x_http_method_override(request)\n-        request.content_type = ct = content_type(request)\n-        request.JSON = None\n-\n-        if method in ('POST', 'PUT'):\n-            if ct in JSON_CONTENT_TYPES:\n-                body = request.content.read()\n-                try:\n-                    request.JSON = unserialize(body)\n-                except Exception, e:\n-                    return JsonResponseBadRequest('Invalid json: %s' % e )\n-\n-        def _onsuccess(response):\n-            if not isinstance(response, HttpResponse):\n-                return JsonResponse(response, json_callback) # best effort\n-            return response\n-\n-        ret = mustbe_deferred(func, request, *args, **kwargs)\n-        ret.addCallback(_onsuccess)\n-        return ret\n-    return wrapper\n-\n-def content_type(request):\n-    ct = request.HEADERS.get('content-type','')\n-    return ct.split(';')[0].strip()\n-\n-def _x_http_method_override(request):\n-    \"\"\" support for X-Http-Method-Override hack\n-\n-    some clients does not support methods others than GET and POST, that clients\n-    has a chance to set an extra header to indicate intended method.\n-    \"\"\"\n-    return request.HEADERS.get('x-http-method-override', request.method).upper()\n-\n\n@@ -1,89 +0,0 @@\n-import re\n-import urllib\n-import hashlib\n-from twisted.internet import defer\n-\n-from scrapy.core.engine import scrapyengine\n-from scrapy.spider import spiders\n-from scrapy.http import Request\n-from scrapy.item.models import BaseItem\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.conf import settings\n-from scrapy.utils.misc import memoize, arg_to_iter\n-from scrapy.xlib.lrucache import LRUCache\n-\n-from .site import WebSite, WebResource\n-from .http import HttpResponse\n-from .json import JsonResponse\n-\n-JSONCALLBACK_RE = '^[a-zA-Z][a-zA-Z_.-]*$'\n-CACHESIZE = settings.get('WS_CACHESIZE', 20)\n-\n-\n-def _urlhash(request):\n-    h = hashlib.sha1()\n-    for a in sorted(request.ARGS):\n-        h.update(request.ARGS[a])\n-    return h.hexdigest()\n-\n-\n-@memoize(cache=LRUCache(CACHESIZE), hash=_urlhash)\n-def url_to_guid(httprequest):\n-    url = httprequest.ARGS.get('url')\n-    if not url:\n-        return HttpResponse('Bad Request', 400)\n-    url = urllib.unquote(url)\n-\n-    jsoncb = httprequest.ARGS.get('callback')\n-    if jsoncb and not re.match(JSONCALLBACK_RE, jsoncb):\n-        return HttpResponse('Bad callback argument', 400)\n-\n-    def _response(guids=(), message=None):\n-        content = {\n-            'guids': list(guids),\n-            'domain': getattr(spider, 'domain_name', None),\n-            'message': message,\n-        }\n-        return JsonResponse(content=content, callback=jsoncb)\n-\n-    spider = spiders.fromurl(url)\n-    if not spider:\n-        return _response(message='No crawler found for site')\n-\n-    if httprequest.ARGS.get('dontcrawl'):\n-        return _response()\n-\n-\n-    def _on_error(_failure):\n-        return _response(message='Error downloading url from site')\n-\n-    def _on_success(pagedata):\n-        try:\n-            items = spider.identify(pagedata)\n-        except Exception, ex:\n-            return _response(message='Error processing url')\n-\n-        guids = [i.guid for i in arg_to_iter(items) if isinstance(i, BaseItem)]\n-        return _response(guids=guids)\n-\n-    deferred = defer.Deferred().addCallbacks(_on_success, _on_error)\n-    request = Request(url=url, callback=deferred, dont_filter=True)\n-    schd = scrapyengine.download(request, spider)\n-    schd.chainDeferred(deferred)\n-    return deferred\n-\n-\n-urlmapping = (\n-    ('^ws/tools/url_to_guid/$', url_to_guid),\n-)\n-\n-\n-class UrlToGuidService(WebSite):\n-    def __init__(self):\n-        if not settings.getbool('WS_ENABLED'):\n-            raise NotConfigured\n-\n-        port = settings.getint('WS_PORT') or 8088\n-        timeout = settings.getint('WS_TIMEOUT') or 15 # seconds\n-        resource = WebResource(urlmapping, timeout=timeout)\n-        WebSite.__init__(self, port=port, resource=resource)\n\n@@ -1,133 +0,0 @@\n-\"\"\"Twisted website object as django\n-\n-################################################################################\n-## Simple Usage example:\n-\n-from twisted.internet import reactor\n-from scrapy.contrib.web.http import WebSite, HttpResponse\n-\n-def helloword(request):\n-   return HttpResponse('Hello World!')\n-\n-def hello(request, name):\n-    return HttpResponse('Hello %s' % name)\n-\n-urls = (\n-        ('^hello/(?P<name>\\w+)/$', hello),\n-        ('^$', helloword),\n-        )\n-\n-\n-resource = WebResource(urls)\n-site = WebSite(port=8081, resource=resource)\n-reactor.run()\n-\n-# now go to http://localhost:8081/\n-\n-\n-\n-################################################################################\n-## Complex usage example:\n-\n-from twisted.internet import reactor, defer\n-from scrapy.contrib.web.http import WebSite, HttpResponse\n-\n-def delayed(request):\n-    def _callback(result):\n-        return HttpResponse('Heavy task completed: %s' % result)\n-\n-    def _errback(_failure):\n-        return HttpResponse('Internal Server Error: %s' % _failure, status=500)\n-\n-    def heavytask(_):\n-        import random\n-        assert random.randint(0,1), \"Exception found processing request\"\n-        return _\n-\n-    d = defer.Deferred().addCallback(heavytask)\n-    d.addCallbacks(_callback, _errback)\n-    reactor.callLater(1, d.callback, \"Well done\")\n-    return d\n-\n-urls = (('^delayed/$', delayed),)\n-\n-resource = WebResource(urls)\n-site = WebSite(port=8081, resource=resource)\n-reactor.run()\n-\n-\"\"\"\n-\n-import re\n-\n-from twisted.web import server, resource\n-from twisted.internet import reactor\n-from scrapy.utils.defer import mustbe_deferred\n-\n-from .http import HttpResponse, build_httprequest\n-\n-\n-def urlresolver(urls, path):\n-    \"\"\"Simple path to view mapper\"\"\"\n-    path = path.lstrip('/')\n-    for pathre, view in urls:\n-        m = re.search(pathre, path)\n-        if m:\n-            kwargs = m.groupdict()\n-            args = () if kwargs else m.groups()\n-            return view, args, kwargs\n-    return None, (), {}\n-\n-\n-class WebSite(server.Site):\n-    def __init__(self, port=None, *args, **kwargs):\n-        server.Site.__init__(self, *args, **kwargs)\n-        if port:\n-            self.bind(port)\n-\n-    def bind(self, port):\n-        from scrapy.core.engine import scrapyengine\n-        scrapyengine.listenTCP(port, self)\n-\n-\n-class WebResource(resource.Resource):\n-    \"\"\"Translate twisted web approach to django alike way\"\"\"\n-    isLeaf = True\n-    debug = True\n-\n-    def __init__(self, urls, timeout=3, urlresolver=urlresolver):\n-        resource.Resource.__init__(self)\n-        self.urlresolver = urlresolver\n-        self.timeout = timeout\n-        self.urls = urls\n-\n-    def render(self, twistedrequest):\n-        httprequest = build_httprequest(twistedrequest)\n-\n-        def _send_response(response):\n-            assert isinstance(response, HttpResponse), 'view should return a HttpResponse object'\n-            twistedrequest.setResponseCode(response.status_code or 200)\n-            for key, val in response.items():\n-                twistedrequest.setHeader(key, response[key])\n-            twistedrequest.write(response.content)\n-            twistedrequest.finish()\n-\n-        def _on_error(_failure):\n-            content = _failure.getTraceback() if self.debug else 'Internal Error'\n-            response = HttpResponse(content=str(_failure), status=500)\n-            return _send_response(response)\n-\n-        view, args, kwargs = self.urlresolver(self.urls, httprequest.path)\n-        if not view:\n-            response = HttpResponse(content='Not Found', status=404)\n-            _send_response(response)\n-            return server.NOT_DONE_YET\n-\n-        deferred = mustbe_deferred(view, httprequest, *args, **kwargs)\n-        deferred.addCallback(_send_response)\n-        deferred.addErrback(_on_error)\n-        if not deferred.timeoutCall:\n-            deferred.setTimeout(self.timeout)\n-\n-        return server.NOT_DONE_YET\n-\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d334c035c071d98a437d0115f05b23df264fa212", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 93 | Files Changed: 8 | Hunks: 14 | Methods Changed: 12 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 151 | Churn Cumulative: 405 | Contributors (this commit): 3 | Commits (past 90d): 42 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -86,7 +86,6 @@ DUPEFILTER_CLASS = 'scrapy.contrib.dupefilter.RequestFingerprintDupeFilter'\n \n EXTENSIONS = [\n     'scrapy.stats.corestats.CoreStats',\n-    'scrapy.xpath.extension.ResponseLibxml2',\n     'scrapy.management.web.WebConsole',\n     'scrapy.management.telnet.TelnetConsole',\n     'scrapy.contrib.webconsole.scheduler.SchedulerQueue',\n\n@@ -1,36 +0,0 @@\n-import unittest\n-\n-import libxml2\n-\n-from scrapy.http import TextResponse\n-from scrapy.utils.test import libxml2debug\n-\n-class Libxml2Test(unittest.TestCase):\n-\n-    @libxml2debug\n-    def test_xpath(self):\n-        #this test will fail in version 2.6.27 but passes on 2.6.29+\n-        html = \"<td>1<b>2</b>3</td>\"\n-        node = libxml2.htmlParseDoc(html, 'utf-8')\n-        result = [str(r) for r in node.xpathEval('//text()')]\n-        self.assertEquals(result, ['1', '2', '3'])\n-        node.freeDoc()\n-\n-class ResponseLibxml2DocTest(unittest.TestCase):\n-\n-    @libxml2debug\n-    def test_getlibxml2doc(self):\n-        # test to simulate '\\x00' char in body of html page\n-        #this method shouldn't raise TypeError Exception\n-\n-        # make sure we load the libxml2 extension\n-        from scrapy.extension import extensions\n-        extensions.load() # \n-\n-        self.body_content = 'test problematic \\x00 body'\n-        response = TextResponse('http://example.com/catalog/product/blabla-123',\n-                            headers={'Content-Type': 'text/plain; charset=utf-8'}, body=self.body_content)\n-        response.getlibxml2doc()\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -5,9 +5,10 @@ import libxml2\n \n from scrapy.http import TextResponse, HtmlResponse, XmlResponse\n from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector\n+from scrapy.xpath.document import Libxml2Document\n from scrapy.utils.test import libxml2debug\n \n-class XPathTestCase(unittest.TestCase):\n+class XPathSelectorTestCase(unittest.TestCase):\n \n     @libxml2debug\n     def test_selector_simple(self):\n@@ -238,5 +239,44 @@ class XPathTestCase(unittest.TestCase):\n             u'\\n  ',\n             u'\\n  pff\\n'])\n \n+class Libxml2DocumentTest(unittest.TestCase):\n+\n+    @libxml2debug\n+    def test_response_libxml2_caching(self):\n+        r1 = HtmlResponse('http://www.example.com', body='<html><head></head><body></body></html>')\n+        r2 = r1.copy()\n+\n+        doc1 = Libxml2Document(r1)\n+        doc2 = Libxml2Document(r1)\n+        doc3 = Libxml2Document(r2)\n+\n+        # make sure it's cached\n+        assert doc1 is doc2\n+        assert doc1.xmlDoc is doc2.xmlDoc\n+        assert doc1 is not doc3\n+        assert doc1.xmlDoc is not doc3.xmlDoc\n+\n+        # don't leave libxml2 documents in memory to avoid wrong libxml2 leaks reports\n+        del doc1, doc2, doc3\n+\n+    @libxml2debug\n+    def test_null_char(self):\n+        # make sure bodies with null char ('\\x00') don't raise a TypeError exception\n+        self.body_content = 'test problematic \\x00 body'\n+        response = TextResponse('http://example.com/catalog/product/blabla-123',\n+                            headers={'Content-Type': 'text/plain; charset=utf-8'}, body=self.body_content)\n+        Libxml2Document(response)\n+\n+class Libxml2Test(unittest.TestCase):\n+\n+    @libxml2debug\n+    def test_libxml2_bug_2_6_27(self):\n+        # this test will fail in version 2.6.27 but passes on 2.6.29+\n+        html = \"<td>1<b>2</b>3</td>\"\n+        node = libxml2.htmlParseDoc(html, 'utf-8')\n+        result = [str(r) for r in node.xpathEval('//text()')]\n+        self.assertEquals(result, ['1', '2', '3'])\n+        node.freeDoc()\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -1,25 +0,0 @@\n-import unittest\n-\n-from scrapy.http import HtmlResponse\n-from scrapy.xpath.extension import ResponseLibxml2\n-\n-class ResponseLibxml2Test(unittest.TestCase):\n-\n-    def setUp(self):\n-        ResponseLibxml2()\n-\n-    def test_response_libxml2_caching(self):\n-        r1 = HtmlResponse('http://www.example.com', body='<html><head></head><body></body></html>')\n-        r2 = r1.copy()\n-\n-        doc1 = r1.getlibxml2doc()\n-        doc2 = r1.getlibxml2doc()\n-        doc3 = r2.getlibxml2doc()\n-\n-        # make sure it's cached\n-        assert doc1 is doc2\n-        assert doc1 is not doc3\n-\n-        # don't leave libxml2 documents in memory to avoid wrong libxml2 leaks reports\n-        del doc1, doc2, doc3\n-\n\n@@ -6,6 +6,8 @@ import os\n \n import libxml2\n \n+from scrapy.xpath.document import Libxml2Document\n+\n def libxml2debug(testfunction):\n     \"\"\"Decorator for debugging libxml2 memory leaks inside a function.\n     \n\n@@ -1,15 +1,24 @@\n \"\"\"\n-This module contains a simple class (Libxml2Document) to wrap libxml2 documents\n-(xmlDoc) for proper garbage collection.\n+This module contains a simple class (Libxml2Document) which provides cache and\n+garbage collection to libxml2 documents (xmlDoc).\n \"\"\"\n \n+import weakref\n+\n from scrapy.xpath.factories import xmlDoc_from_html\n \n class Libxml2Document(object):\n \n-    def __init__(self, response, factory=xmlDoc_from_html):\n-        self.xmlDoc = factory(response)\n-        self.xpathContext = self.xmlDoc.xpathNewContext()\n+    cache = weakref.WeakKeyDictionary()\n+\n+    def __new__(cls, response, factory=xmlDoc_from_html):\n+        cache = cls.cache.setdefault(response, {})\n+        if factory not in cache:\n+            obj = object.__new__(cls)\n+            obj.xmlDoc = factory(response)\n+            obj.xpathContext = obj.xmlDoc.xpathNewContext()\n+            cache[factory] = obj\n+        return cache[factory]\n \n     def __del__(self):\n         # we must call both cleanup functions, so we try/except all exceptions\n\n@@ -1,20 +0,0 @@\n-\"\"\"\n-The ResponseLibxml2 extension causes the Response objects to grow a new method\n-(\"getlibxml2doc\") which returns a (cached) libxml2 document of itself.\n-\"\"\"\n-\n-from scrapy.http import Response\n-from scrapy.xpath.document import Libxml2Document\n-from scrapy.xpath.factories import xmlDoc_from_html\n-\n-class ResponseLibxml2(object):\n-    def __init__(self):\n-        setattr(Response, 'getlibxml2doc', getlibxml2doc)\n-\n-def getlibxml2doc(response, factory=xmlDoc_from_html):\n-    cachekey = 'lx2doc_%s' % factory.__name__\n-    if cachekey not in response.cache:\n-        lx2doc = Libxml2Document(response, factory=factory)\n-        response.cache[cachekey] = lx2doc\n-    return response.cache[cachekey]\n-\n\n@@ -7,8 +7,8 @@ See documentation in docs/ref/selectors.rst\n import libxml2\n \n from scrapy.http import TextResponse\n-from scrapy.xpath.extension import Libxml2Document\n from scrapy.xpath.factories import xmlDoc_from_html, xmlDoc_from_xml\n+from scrapy.xpath.document import Libxml2Document\n from scrapy.utils.python import flatten, unicode_to_str\n from scrapy.utils.misc import extract_regex\n \n@@ -19,10 +19,6 @@ class XPathSelector(object):\n             self.doc = parent.doc\n             self.xmlNode = node\n         elif response:\n-            try:\n-                # try with cached version first\n-                self.doc = response.getlibxml2doc(factory=self._get_libxml2_doc)\n-            except AttributeError:\n             self.doc = Libxml2Document(response, factory=self._get_libxml2_doc)\n             self.xmlNode = self.doc.xmlDoc\n         elif text:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c0532712f90085f2afe30e20330615cfd0fc7520", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 62 | Files Changed: 6 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): -4/2 | Churn Δ: 100 | Churn Cumulative: 290 | Contributors (this commit): 3 | Commits (past 90d): 38 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -98,7 +98,6 @@ EXTENSIONS = [\n     'scrapy.contrib.memdebug.MemoryDebugger',\n     'scrapy.contrib.closedomain.CloseDomain',\n     'scrapy.contrib.debug.StackTraceDump',\n-    'scrapy.contrib.response.soup.ResponseSoup',\n ]\n \n GROUPSETTINGS_ENABLED = False\n\n\n@@ -1,22 +0,0 @@\n-\"\"\"\n-ResponseSoup extension \n-\n-See documentation in docs/ref/extensions.rst\n-\"\"\" \n-\n-from scrapy.xlib.BeautifulSoup import BeautifulSoup\n-\n-from scrapy.http import Response\n-\n-\n-class ResponseSoup(object):\n-    def __init__(self):\n-        setattr(Response, 'getsoup', getsoup)\n-        setattr(Response, 'soup', property(getsoup))\n-\n-def getsoup(response, **kwargs):\n-    # TODO: use different cache buckets depending on constructor parameters\n-    if 'soup' not in response.cache:\n-        body = response.body if response.body is not None else \"\"\n-        response.cache['soup'] = BeautifulSoup(body, **kwargs)\n-    return response.cache['soup']\n\n@@ -1,37 +0,0 @@\n-import unittest\n-\n-from scrapy.xlib.BeautifulSoup import BeautifulSoup\n-\n-from scrapy.http import Response\n-from scrapy.contrib.response.soup import ResponseSoup\n-\n-class ResponseSoupTest(unittest.TestCase):\n-\n-    def setUp(self):\n-        ResponseSoup()\n-\n-    def test_response_soup(self):\n-        r1 = Response('http://www.example.com', body='')\n-\n-        soup1 = r1.getsoup()\n-        soup2 = r1.getsoup()\n-\n-        assert isinstance(r1.soup, BeautifulSoup)\n-        assert isinstance(soup2, BeautifulSoup)\n-        # make sure it's cached\n-        assert soup1 is soup2\n-\n-        # when body is None, an empty soup should be returned\n-        r1 = Response('http://www.example.com')\n-        assert r1.body == \"\"\n-        assert isinstance(r1.getsoup(), BeautifulSoup)\n-\n-    def test_response_soup_caching(self):\n-        r1 = Response('http://www.example.com', body='')\n-        soup1 = r1.getsoup()\n-        r2 = r1.copy()\n-        soup2 = r1.getsoup()\n-        soup3 = r2.getsoup()\n-\n-        assert soup1 is soup2\n-        assert soup1 is not soup3\n\n@@ -1,6 +1,9 @@\n import unittest\n+\n+from scrapy.xlib.BeautifulSoup import BeautifulSoup\n from scrapy.http import Response, TextResponse\n-from scrapy.utils.response import body_or_str, get_base_url, get_meta_refresh, response_httprepr\n+from scrapy.utils.response import body_or_str, get_base_url, get_meta_refresh, \\\n+    response_httprepr, get_cached_beautifulsoup\n \n class ResponseUtilsTest(unittest.TestCase):\n     dummy_response = TextResponse(url='http://example.org/', body='dummy_response')\n@@ -58,6 +61,30 @@ class ResponseUtilsTest(unittest.TestCase):\n         r1 = Response(\"http://www.example.com\", status=404, headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n         self.assertEqual(response_httprepr(r1), 'HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n \n+    def test_get_cached_beautifulsoup(self):\n+        r1 = Response('http://www.example.com', body='')\n+\n+        soup1 = get_cached_beautifulsoup(r1)\n+        soup2 = get_cached_beautifulsoup(r1)\n+\n+        assert isinstance(soup1, BeautifulSoup)\n+        assert isinstance(soup2, BeautifulSoup)\n+        # make sure it's cached\n+        assert soup1 is soup2\n+\n+        # when body is None, an empty soup should be returned\n+        r1 = Response('http://www.example.com')\n+        assert r1.body == \"\"\n+        assert isinstance(get_cached_beautifulsoup(r1), BeautifulSoup)\n+\n+        r1 = Response('http://www.example.com', body='')\n+        soup1 = get_cached_beautifulsoup(r1)\n+        r2 = r1.copy()\n+        soup2 = get_cached_beautifulsoup(r1)\n+        soup3 = get_cached_beautifulsoup(r2)\n+\n+        assert soup1 is soup2\n+        assert soup1 is not soup3\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -3,11 +3,12 @@ This module provides some useful functions for working with\n scrapy.http.Response objects\n \"\"\"\n \n-import re\n+import re, weakref\n \n from twisted.web import http\n from twisted.web.http import RESPONSES\n \n+from scrapy.xlib.BeautifulSoup import BeautifulSoup\n from scrapy.http.response import Response\n \n def body_or_str(obj, unicode=True):\n@@ -41,6 +42,14 @@ def get_meta_refresh(response):\n         response.cache['meta_refresh_url'] = match.groups() if match else (None, None)\n     return response.cache['meta_refresh_url']\n \n+_beautifulsoup_cache = weakref.WeakKeyDictionary()\n+def get_cached_beautifulsoup(response):\n+    \"\"\"Return BeautifulSoup object of the given response, with caching\n+    support\"\"\"\n+    if response not in _beautifulsoup_cache:\n+        _beautifulsoup_cache[response] = BeautifulSoup(response.body)\n+    return _beautifulsoup_cache[response]\n+\n def response_status_message(status):\n     \"\"\"Return status code plus status text descriptive message\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e96622166e1ed195d55b10b68862ff5a09e8da48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 14 | Churn Cumulative: 86 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n\n@@ -1,14 +0,0 @@\n-# FIXME: code below is for backwards compatibility and should be removed before\n-# the 0.7 release\n-\n-import warnings\n-\n-from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n-\n-class RegexLinkExtractor(SgmlLinkExtractor):\n-\n-    def __init__(self, *args, **kwargs):\n-        warnings.warn(\"scrapy.link.extractors.RegexLinkExtractor is deprecated, use scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor instead\",\n-            DeprecationWarning, stacklevel=2)\n-        SgmlLinkExtractor.__init__(self, *args, **kwargs)\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a0e2086b1bab227b79d1668f9aa400ba65abe610", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 4 | Files Changed: 6 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 22/15 | Churn Δ: 19 | Churn Cumulative: 39 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -1 +1,11 @@\n+\"\"\"\n+WARNING: This module is obsolete and will be removed before the Scrapy 0.7\n+release\n+\"\"\"\n+\n+import warnings\n+\n from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline\n+\n+warnings.warn(\"scrapy.contrib.item is obsolete and will be removed soon\", \\\n+                DeprecationWarning, stacklevel=2)\n\n\n@@ -5,11 +5,12 @@ useful in some Scrapy implementations\n \n import copy\n import hashlib\n+import warnings\n \n from pprint import PrettyPrinter\n \n from scrapy.item import ScrapedItem\n-from scrapy.item.adaptors import AdaptorPipe\n+from scrapy.contrib.item.adaptors import AdaptorPipe\n from scrapy.core.exceptions import DropItem\n from scrapy.utils.python import unique\n \n\n@@ -4,7 +4,7 @@ import re\n from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n from scrapy.utils.misc import extract_regex\n from scrapy.utils.python import flatten, str_to_unicode, unicode_to_str\n-from scrapy.item.adaptors import adaptize\n+from scrapy.contrib.item.adaptors import adaptize\n \n def to_unicode(value, adaptor_args):\n     \"\"\"\n\n@@ -2,7 +2,7 @@\n import unittest\n import re\n \n-from scrapy.item.adaptors import AdaptorPipe\n+from scrapy.contrib.item.adaptors import AdaptorPipe\n from scrapy.contrib_exp import adaptors\n from scrapy.http import HtmlResponse, Headers\n from scrapy.xpath.selector import HtmlXPathSelector, XmlXPathSelector\n\n@@ -2,7 +2,7 @@\n import unittest\n \n from scrapy.contrib.item import RobustScrapedItem\n-from scrapy.item.adaptors import AdaptorPipe\n+from scrapy.contrib.item.adaptors import AdaptorPipe\n from scrapy.contrib_exp import adaptors\n \n class MyItem(RobustScrapedItem):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7ac7597950ba04feb1d2e51fb3bd53439d7732ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1041 | Files Changed: 14 | Hunks: 12 | Methods Changed: 69 | Complexity Δ (Sum/Max): -173/0 | Churn Δ: 1041 | Churn Cumulative: 1206 | Contributors (this commit): 3 | Commits (past 90d): 49 | Contributors (cumulative): 29 | DMM Complexity: 0.44493392070484583\n\nDIFF:\n@@ -23,18 +23,6 @@ BOT_VERSION = '1.0'\n CLOSEDOMAIN_TIMEOUT = 0\n CLOSEDOMAIN_ITEMPASSED = 0\n \n-CLUSTER_LOGDIR = ''\n-\n-CLUSTER_MASTER_PORT = 8790\n-CLUSTER_MASTER_ENABLED = 0\n-CLUSTER_MASTER_POLL_INTERVAL = 60\n-CLUSTER_MASTER_NODES = {}\n-CLUSTER_MASTER_STATEFILE = \"\"\n-\n-CLUSTER_WORKER_ENABLED = 0\n-CLUSTER_WORKER_MAXPROC = 4\n-CLUSTER_WORKER_PORT = 8789\n-\n COMMANDS_MODULE = ''\n COMMANDS_SETTINGS_MODULE = ''\n \n@@ -199,6 +187,3 @@ WEBCONSOLE_ENABLED = True\n WEBCONSOLE_PORT = 6080\n WEBCONSOLE_LOGFILE = None\n \n-# this setting is used by the cluster master to pass additional settings to\n-# workers at connection time\n-GLOBAL_CLUSTER_SETTINGS = [] \n\n@@ -1,3 +0,0 @@\n-from scrapy.contrib.cluster.worker.manager import ClusterWorker\n-from scrapy.contrib.cluster.master.web import ClusterMasterWeb\n-from scrapy.contrib.cluster.crawler.manager import ClusterCrawler\n\n\n@@ -1,41 +0,0 @@\n-import os\n-\n-from twisted.spread import pb\n-from twisted.internet import reactor\n-\n-from scrapy.conf import settings\n-from scrapy import log\n-from scrapy.core.manager import scrapymanager\n-from scrapy.core.exceptions import NotConfigured\n-\n-class ClusterCrawlerBroker(pb.Referenceable):\n-    \"\"\"ClusterCrawlerBroker is the class that's used for communication between\n-    the cluster worker and the crawling proces\"\"\"\n-\n-    def __init__(self, crawler, remote):\n-        self.__remote = remote\n-        self.__crawler = crawler\n-        deferred = self.__remote.callRemote(\"register_crawler\", os.getpid(), self)\n-        deferred.addCallbacks(callback=lambda x: None, errback=lambda reason: log.msg(reason, log.ERROR))\n-\n-    def remote_stop(self):\n-        scrapymanager.stop()\n-    \n-class ClusterCrawler(object):\n-    \"\"\"ClusterCrawler is an extension that instances a ClusterCrawlerBroker\n-    which is used to control a crawling process from the cluster worker. It\n-    also registers that broker to the local cluster worker\"\"\"\n-\n-    def __init__(self):\n-        if not settings.getbool('CLUSTER_CRAWLER_ENABLED'):\n-            raise NotConfigured\n-\n-        self.worker = None\n-\n-        factory = pb.PBClientFactory()\n-        reactor.connectTCP(\"localhost\", settings.getint('CLUSTER_WORKER_PORT'), factory)\n-        d = factory.getRootObject()\n-        def _set_worker(obj):\n-            self.worker = ClusterCrawlerBroker(self, obj)\n-        d.addCallbacks(callback=_set_worker, errback=lambda reason: log.msg(reason, log.ERROR))\n-        \n\n@@ -1,13 +0,0 @@\n-\"\"\"\n-This module contains pre-run hooks that can be attached to scrapy workers.\n-\n-Pre-run hooks must be callable objects (ie. functions) which implement this\n-interface:\n-\n-pre_hook(domain, spider_settings)\n-\n-domain is the domain to be scraped\n-spider_settings is the settings to use to scrape it\n-\n-Values returned from the pre-run hooks will be ignored.\n-\"\"\"\n\n@@ -1,23 +0,0 @@\n-\"\"\" \n-This module contains hooks for updating code via svn. Useful for running before\n-starting to crawl a domain, for example to update spider code \n-\"\"\"\n-\n-import pysvn\n-\n-from scrapy.conf import settings\n-from scrapy import log\n-\n-SVN_DIR = settings['SVN_DIR']\n-SVN_USER = settings['SVN_USER']\n-SVN_PASS = settings['SVN_PASS']\n-\n-def svnup(domain, spider_settings):\n-    c = pysvn.Client()\n-    c.callback_get_login = lambda x,y,z: (True, SVN_USER, SVN_PASS, False)\n-    try:\n-        r = c.update(SVN_DIR)\n-        log.msg(\"ClusterWorker: SVN code updated to revision %s (triggered by domain %s)\" % \\\n-                (r[0].number, domain), level=log.DEBUG)\n-    except pysvn.ClientError, e:\n-        log.msg(\"ClusterWorker: unable to update svn code - %s\" % e, level=log.WARNING)\n\n\n@@ -1,390 +0,0 @@\n-from __future__ import with_statement\n-\n-import datetime\n-import cPickle as pickle\n-\n-from scrapy.xlib.pydispatch import dispatcher\n-from twisted.spread import pb\n-from twisted.internet import reactor\n-\n-from scrapy.core import signals\n-from scrapy import log\n-from scrapy.core.engine import scrapyengine\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.contrib.cluster.worker.manager import ResponseCode\n-from scrapy.conf import settings\n-\n-def my_import(name):\n-    mod = __import__(name)\n-    components = name.split('.')\n-    for comp in components[1:]:\n-        mod = getattr(mod, comp)\n-    return mod\n-\n-class ClusterNodeBroker(pb.Referenceable):\n-\n-    def __init__(self, worker, name, master):\n-        self.unsafeTracebacks = True\n-        self._worker = worker\n-        self.alive = False\n-        self.name = name\n-        self.master = master\n-        self.available = True\n-        try:\n-            deferred = self._worker.callRemote(\"set_master\", self)\n-        except pb.DeadReferenceError:\n-            self._set_status(None)\n-            log.msg(\"ClusterMaster: Lost connection to node %s.\" % self.name, log.ERROR)\n-        else:\n-            def _eb(failure):\n-                self._logfailure(\"Error while setting master to worker node\", failure)\n-            deferred.addCallbacks(callback=self._set_status, errback=_eb)\n-\n-    def status_as_dict(self, verbosity=1):\n-        if verbosity == 0:\n-            return\n-        status = {\"alive\": self.alive}\n-        if self.alive:\n-            if verbosity == 1:\n-                # dont show spider settings\n-                status[\"running\"] = []\n-                for proc in self.running:\n-                    proccopy = proc.copy()\n-                    del proccopy[\"settings\"]\n-                    status[\"running\"].append(proccopy)\n-            elif verbosity == 2:\n-                status[\"running\"] = self.running\n-            status[\"maxproc\"] = self.maxproc\n-            status[\"freeslots\"] = self.maxproc - len(self.running)\n-            status[\"available\"] = self.available\n-            status[\"starttime\"] = self.starttime\n-            status[\"timestamp\"] = self.timestamp\n-            status[\"loadavg\"] = self.loadavg\n-        return status\n-\n-    def update_status(self):\n-        \"\"\"Update status from this worker. This is called periodically.\"\"\"\n-        try:\n-            deferred = self._worker.callRemote(\"status\")\n-        except pb.DeadReferenceError:\n-            self._set_status(None)\n-            log.msg(\"ClusterMaster: Lost connection to worker=%s.\" % self.name, log.ERROR)\n-        else:\n-            def _eb(failure):\n-                self._logfailure(\"Error while updating status\", failure)\n-            deferred.addCallbacks(callback=self._set_status, errback=_eb)\n-\n-    def stop(self, domain):\n-        try:\n-            deferred = self._worker.callRemote(\"stop\", domain)\n-        except pb.DeadReferenceError:\n-            self._set_status(None)\n-            log.msg(\"ClusterMaster: Lost connection to worker=%s.\" % self.name, log.ERROR)\n-        else:\n-            def _eb(failure):\n-                self._logfailure(\"Error while stopping domain=%s\" % domain, failure)\n-            deferred.addCallbacks(callback=self._set_status, errback=_eb)\n-\n-    def run(self, domain_info):\n-        \"\"\"Run the given domain.\n-\n-        domain_info is a dict of keys:\n-        domain - the domain to run\n-        settings - the settings to use\n-        priority - the priority to use\n-        \"\"\"\n-\n-        domain = domain_info['domain']\n-        priority = domain_info['priority']\n-        spider_settings  = domain_info['settings']\n-        dsettings = self.master.compute_final_spider_settings(domain, spider_settings)\n-\n-        def _run_errback(failure):\n-            self._logfailure(\"Error while running domain=%s\" % domain, failure)\n-            self.master.loading.remove(domain)\n-            newprio = priority - 1 # increase priority for reschedule\n-            self.master.reschedule([domain], spider_settings, newprio,\n-                    reason=\"error while try to run it\")\n-\n-        def _run_callback(status):\n-            if status['callresponse'][0] == ResponseCode.NO_FREE_SLOT:\n-                log.msg(\"ClusterMaster: No available slots at worker=%s when trying to run domain=%s\"\n-                        % (self.name, domain), log.WARNING)\n-                self.master.loading.remove(domain)\n-                newprio = priority - 1 # increase priority for rerunning asap\n-                self.master.reschedule([domain], spider_settings, newprio,\n-                        reason=\"no available slots at worker=%s\" % self.name)\n-            elif status['callresponse'][0] == ResponseCode.DOMAIN_ALREADY_RUNNING:\n-                log.msg(\"ClusterMaster: Already running domain=%s at worker=%s\" %\n-                        (domain, self.name), log.WARNING)\n-                self.master.loading.remove(domain)\n-                self.master.reschedule([domain], spider_settings, priority,\n-                        reason=\"domain already running at worker=%s\" % self.name)\n-\n-        try:\n-            log.msg(\"ClusterMaster: Running domain=%s at worker=%s\" % (domain, self.name), log.DEBUG)\n-            deferred = self._worker.callRemote(\"run\", domain, dsettings)\n-        except pb.DeadReferenceError:\n-            self._set_status(None)\n-            log.msg(\"ClusterMaster: Lost connection to worker=%s.\" % self.name, log.ERROR)\n-        else:\n-            deferred.addCallbacks(callback=_run_callback, errback=_run_errback)\n-\n-    def remote_update(self, worker_status, domain, domain_status):\n-        \"\"\"Called remotely form worker when domains finish to update status\"\"\"\n-        self._set_status(worker_status)\n-        stats = self.master.statistics\n-        dstats = stats[\"domains\"]\n-\n-        if domain in self.master.loading and domain_status == \"running\":\n-            self.master.loading.remove(domain)\n-            dstats[\"running\"].add(domain)\n-        elif domain_status in (\"done\", \"terminated\"):\n-            dstats[\"running\"].remove(domain)\n-            dstats[\"scraped\"][domain] = dstats[\"scraped\"].get(domain, 0) + 1\n-            stats[\"scraped_count\"] = stats.get(\"scraped_count\", 0) + 1\n-            if domain in dstats[\"lost\"]:\n-                dstats[\"lost\"].remove(domain)\n-\n-        log.msg(\"ClusterMaster: Changed status to <%s> for domain=%s at worker=%s\" %\n-                (domain_status, domain, self.name))\n-\n-    def _logfailure(self, msg, failure):\n-        log.msg(\"ClusterMaster: %s (worker=%s)\\n%s\" % (msg, self.name, failure), log.ERROR)\n-\n-    def _set_status(self, status):\n-        if not status:\n-            self.alive = False\n-        else:\n-            self.alive = True\n-            self.running = status['running']\n-            self.maxproc = status['maxproc']\n-            self.starttime = status['starttime']\n-            self.timestamp = status['timestamp']\n-            self.loadavg = status['loadavg']\n-            self.logdir = status['logdir']\n-            free_slots = self.maxproc - len(self.running)\n-\n-            # load domains by one, so to mix up better the domain loading between nodes.\n-            # The next one in the same node will be loaded\n-            # when there is no loading domain or in the next status update.\n-            # This way also we load the nodes softly\n-            if self.available and free_slots > 0 and self.master.pending:\n-                pending = self.master.pending.pop(0)\n-                # If domain already running in some node, reschedule with same\n-                # priority (so it will be run later)\n-                if pending['domain'] in self.master.running or pending['domain'] in self.master.loading:\n-                    self.master.reschedule([pending['domain']], pending['settings'],\n-                            pending['priority'], reason=\"domain already running in other worker\")\n-                else:\n-                    self.run(pending)\n-                    self.master.loading.append(pending['domain'])\n-\n-\n-class ScrapyPBClientFactory(pb.PBClientFactory):\n-\n-    noisy = False\n-\n-    def __init__(self, master, nodename):\n-        pb.PBClientFactory.__init__(self)\n-        self.unsafeTracebacks = True\n-        self.master = master\n-        self.nodename = nodename\n-\n-    def clientConnectionLost(self, *args, **kargs):\n-        pb.PBClientFactory.clientConnectionLost(self, *args, **kargs)\n-        self.master.remove_node(self.nodename)\n-        log.msg(\"ClusterMaster: Lost connection to worker=%s. Node removed\" % self.nodename)\n-\n-\n-class ClusterMaster(object):\n-\n-    def __init__(self):\n-\n-        if not settings.getbool('CLUSTER_MASTER_ENABLED'):\n-            raise NotConfigured\n-\n-        self.statefile = settings['CLUSTER_MASTER_STATEFILE']\n-        if not self.statefile:\n-            raise NotConfigured(\"ClusterMaster: Missing CLUSTER_MASTER_STATEFILE setting\")\n-\n-        # import groups settings\n-        if settings.getbool('GROUPSETTINGS_ENABLED'):\n-            self.get_spider_groupsettings = my_import(settings[\"GROUPSETTINGS_MODULE\"]).get_spider_groupsettings\n-        else:\n-            self.get_spider_groupsettings = lambda x: {}\n-\n-        # load pending domains\n-        try:\n-            statefile = open(self.statefile, \"r\")\n-            self.pending = pickle.load(statefile)\n-            log.msg(\"ClusterMaster: Loaded state from %s\" % self.statefile)\n-        except IOError:\n-            self.pending = []\n-\n-        self.loading = []\n-        self.nodes = {}\n-        self.nodesconf = settings.get('CLUSTER_MASTER_NODES', {})\n-        self.start_time = datetime.datetime.utcnow()\n-\n-        # for more info about statistics see self.update_nodes() and ClusterNodeBroker.remote_update()\n-        self.statistics = {\"domains\": {\"running\": set(), \"scraped\": {}, \"lost_count\": {}, \"lost\": set()}, \"scraped_count\": 0 }\n-        self.global_settings = {}\n-\n-        # load cluster global settings\n-        for sname in settings.getlist('GLOBAL_CLUSTER_SETTINGS'):\n-            self.global_settings[sname] = settings[sname]\n-\n-        dispatcher.connect(self._engine_started, signal=signals.engine_started)\n-        dispatcher.connect(self._engine_stopped, signal=signals.engine_stopped)\n-\n-    def load_nodes(self):\n-        \"\"\"Loads nodes listed in CLUSTER_MASTER_NODES setting\"\"\"\n-        for name, hostport in self.nodesconf.iteritems():\n-            self.load_node(name, hostport)\n-\n-    def load_node(self, name, hostport):\n-        \"\"\"Creates the remote reference for a worker node\"\"\"\n-        server, port = hostport.split(\":\")\n-        port = int(port)\n-        log.msg(\"ClusterMaster: Connecting to worker=%s (%s)...\" % (name, hostport))\n-        factory = ScrapyPBClientFactory(self, name)\n-        try:\n-            reactor.connectTCP(server, port, factory)\n-        except Exception, err:\n-            log.msg(\"ClusterMaster: Could not connect to worker=%s (%s): %s\" %\n-                    (name, hostport, err), log.ERROR)\n-        else:\n-            def _eb(failure):\n-                log.msg(\"ClusterMaster: Could not connect to worker=%s (%s): %s\" %\n-                        (name, hostport, failure.value), log.ERROR)\n-\n-            d = factory.getRootObject()\n-            d.addCallbacks(callback=lambda obj: self.add_node(obj, name), errback=_eb)\n-\n-    def update_nodes(self):\n-        \"\"\"Update worker nodes statistics\"\"\"\n-        for name, hostport in self.nodesconf.iteritems():\n-            if name in self.nodes and self.nodes[name].alive:\n-                self.nodes[name].update_status()\n-            else:\n-                self.load_node(name, hostport)\n-\n-        dstats = self.statistics[\"domains\"]\n-        real_running = set(self.running.keys())\n-        lost = dstats[\"running\"].difference(real_running)\n-        for domain in lost:\n-            dstats[\"lost_count\"][domain] = dstats[\"lost_count\"].get(domain, 0) + 1\n-        dstats[\"lost\"] = dstats[\"lost\"].union(lost)\n-\n-    def add_node(self, cworker, name):\n-        \"\"\"Add node given its node\"\"\"\n-        node = ClusterNodeBroker(cworker, name, self)\n-        self.nodes[name] = node\n-        log.msg(\"ClusterMaster: Added worker=%s\" % name)\n-\n-    def remove_node(self, nodename):\n-        del self.nodes[nodename]\n-\n-    def disable_node(self, name):\n-        self.nodes[name].available = False\n-\n-    def enable_node(self, name):\n-        self.nodes[name].available = True\n-\n-    def _schedule(self, domains, spider_settings=None, priority=20):\n-        \"\"\"Private method which performs the schedule of the given domains,\n-        with the given priority. Used for both scheduling and rescheduling.\"\"\"\n-        insert_pos = len([p for p in self.pending if p['priority'] <= priority])\n-        for domain in domains:\n-            pd = self.get_first_pending(domain)\n-            if pd: # domain already pending, so just change priority if new is higher\n-                if priority < pd['priority']:\n-                    self.pending.remove(pd)\n-                    pd['priority'] = priority\n-                    self.pending.insert(insert_pos, pd)\n-            else:\n-                self.pending.insert(insert_pos,\n-                        {'domain': domain, 'settings': spider_settings, 'priority': priority})\n-\n-    def compute_final_spider_settings(self, domain, spider_settings=None):\n-        \"\"\"Return merged dictionary with final settings to run spider\"\"\"\n-        final = dict(self.get_spider_groupsettings(domain) or {})\n-        final.update(self.global_settings)\n-        final.update(spider_settings or {})\n-        return final\n-\n-    def schedule(self, domains, spider_settings=None, priority=20):\n-        \"\"\"Schedule the given domains, with the given priority\"\"\"\n-        self._schedule(domains, spider_settings, priority)\n-        log.msg(\"clustermaster: Scheduled domains=%s with priority=%s\" %\n-                (','.join(domains), priority), log.DEBUG)\n-\n-    def reschedule(self, domains, spider_settings=None, priority=20, reason=None):\n-        \"\"\"Reschedule the given domains, with the given priority\"\"\"\n-        self._schedule(domains, spider_settings, priority)\n-        log.msg(\"clustermaster: Rescheduled domains=%s with priority=%s reason='%s'\" %\n-                (','.join(domains), priority, reason), log.DEBUG)\n-\n-\n-    def stop(self, domains):\n-        \"\"\"Stop the given domains\"\"\"\n-        to_stop = {}\n-        for domain in domains:\n-            node = self.running.get(domain, None)\n-            if node:\n-                if node.name not in to_stop:\n-                    to_stop[node.name] = []\n-                to_stop[node.name].append(domain)\n-\n-        for nodename, domains in to_stop.iteritems():\n-            for domain in domains:\n-                self.nodes[nodename].stop(domain)\n-\n-    def remove(self, domains):\n-        \"\"\"Remove all scheduled instances of the given domains (if they haven't\n-        started yet). Otherwise use stop() to stop running domains\"\"\"\n-\n-        self.pending = [p for p in self.pending if p['domain'] not in domains]\n-\n-    def discard(self, domains):\n-        \"\"\"Stop and remove all running and pending instances of the given\n-        domains\"\"\"\n-        self.remove(domains)\n-        self.stop(domains)\n-\n-    @property\n-    def running(self):\n-        \"\"\"Return dict of running domains as domain -> node\"\"\"\n-        d = {}\n-        for node in self.nodes.itervalues():\n-            for proc in node.running:\n-                d[proc['domain']] = node\n-        return d\n-\n-    def get_first_pending(self, domain):\n-        \"\"\"Return first pending instance of a given domain\"\"\"\n-        for p in self.pending:\n-            if domain == p['domain']:\n-                return p\n-\n-    def get_pending(self, verbosity=1):\n-        if verbosity == 1:\n-            pending = []\n-            for p in self.pending:\n-                pp = p.copy()\n-                del pp[\"settings\"]\n-                pending.append(pp)\n-            return pending\n-        elif verbosity == 2:\n-            return self.pending\n-        return\n-\n-    def _engine_started(self):\n-        self.load_nodes()\n-        scrapyengine.addtask(self.update_nodes, settings.getint('CLUSTER_MASTER_POLL_INTERVAL', 60))\n-\n-    def _engine_stopped(self):\n-        with open(self.statefile, \"w\") as f:\n-            pickle.dump(self.pending, f)\n-            log.msg(\"ClusterMaster: Saved state in %s\" % self.statefile)\n\n@@ -1,239 +0,0 @@\n-import datetime\n-\n-from scrapy.xlib.pydispatch import dispatcher\n-\n-from scrapy.spider import spiders \n-from scrapy.management.web import banner, webconsole_discover_module\n-from scrapy.contrib.cluster.master.manager import ClusterMaster\n-from scrapy.utils.serialization import serialize\n-\n-class ClusterMasterWeb(ClusterMaster):\n-    webconsole_id = 'cluster_master'\n-    webconsole_name = 'Cluster master'\n-\n-    def __init__(self):\n-        ClusterMaster.__init__(self)\n-\n-        dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n-\n-    def webconsole_render(self, wc_request):\n-        changes = \"\"\n-        if wc_request.path == '/cluster_master/nodes/':\n-            return self.render_nodes(wc_request)\n-        elif wc_request.path == '/cluster_master/domains/':\n-            return self.render_domains(wc_request)\n-        elif wc_request.path == '/cluster_master/ws/':\n-            return self.webconsole_control(wc_request, ws=True)\n-        elif wc_request.args:\n-            changes = self.webconsole_control(wc_request)\n-\n-        s = self.render_header()\n-\n-        s += \"<h2>Home</h2>\\n\"\n-\n-        s += \"<table border='1'>\\n\"\n-        s += \"<tr><th>&nbsp;</th><th>Name</th><th>Available</th><th>Running</th><th>Load.avg</th></tr>\\n\"\n-        for node in self.nodes.itervalues():\n-            #chkbox = \"<input type='checkbox' name='shutdown' value='%s' />\" % domain if node.status in [\"up\", \"idle\"] else \"&nbsp;\"\n-            nodelink = \"<a href='nodes/#%s'>%s</a>\" % (node.name, node.name)\n-            chkbox = \"&nbsp;\"\n-            loadavg = \"%.2f %.2f %.2f\" % node.loadavg\n-            s += \"<tr><td>%s</td><td>%s</td><td>%s</td><td>%d/%d</td><td>%s</td></tr>\\n\" % \\\n-                 (chkbox, nodelink, node.available, len(node.running), node.maxproc, loadavg)\n-        s += \"</table>\\n\"\n-\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-\n-        return str(s)\n-\n-    def webconsole_control(self, wc_request, ws=False):\n-        args = wc_request.args\n-        if \"updatenodes\" in args:\n-            self.update_nodes()\n-            if ws:\n-                return self.ws_status(wc_request)\n-\n-        if \"schedule\" in args:\n-            if ws:\n-                sep = \",\"\n-                domains = args[\"schedule\"][0].split(sep)\n-            else:\n-                sep = \"\\r\"\n-                domains = args[\"schedule\"]\n-            priority = int(args.get(\"priority\", [20])[0])\n-            \n-            # spider settings\n-            slist = args.get(\"settings\", [\"\"])[0].split(sep)\n-            spider_settings = {}\n-            for s in slist:\n-                try:\n-                    k, v = s.strip().split(\"=\")\n-                except ValueError:\n-                    pass\n-                else:\n-                    spider_settings[k] = v\n-\n-            self.schedule(domains, spider_settings, priority)\n-            if ws:\n-                return self.ws_status(wc_request, verbosity=0)\n-\n-        if \"stop\" in args:\n-            if ws:\n-                domains = args[\"stop\"][0].split(\",\")\n-            else:\n-                domains=args[\"stop\"]\n-            self.stop(domains)\n-            if ws:\n-                return self.ws_status(wc_request)\n-\n-        if \"remove\" in args:\n-            if ws:\n-                domains = args[\"remove\"][0].split(\",\")\n-            else:\n-                domains=args[\"remove\"]\n-            self.remove(domains)\n-            if ws:\n-                return self.ws_status(wc_request)\n-        if \"disable_node\" in args:\n-            self.disable_node(args[\"disable_node\"][0])\n-            if ws:\n-                return self.ws_status(wc_request)\n-        if \"enable_node\" in args:\n-            self.enable_node(args[\"enable_node\"][0])\n-            if ws:\n-                return self.ws_status(wc_request)\n-        if \"statistics\" in args:\n-            if ws:\n-                return self.ws_statistics(wc_request)\n-            \n-        if ws:\n-            return self.ws_status(wc_request)\n-        else:\n-            return \"\"\n-\n-    def render_nodes(self, wc_request):\n-        if wc_request.args:\n-            self.webconsole_control(wc_request)\n-\n-        now = datetime.datetime.utcnow()\n-\n-        s = self.render_header()\n-        for node in self.nodes.itervalues():\n-            if node.available:\n-                s += \"<h2><a name='%s'>%s</h2>\\n\" % (node.name, node.name)\n-        \n-                s += \"<h3>Running domains</h3>\\n\"\n-                if node.running:\n-                    s += \"<form method='post' action='.'>\\n\"\n-                    s += \"<table border='1'>\\n\"\n-                    s += \"<tr><th>&nbsp;</th><th>PID</th><th>Domain</th><th>Status</th><th>Running time</th><th>Log file</th></tr>\\n\"\n-                    for proc in node.running:\n-                        chkbox = \"<input type='checkbox' name='stop' value='%s' />\" % proc['domain'] if proc['status'] == \"running\" else \"&nbsp;\"\n-                        start_time = proc.get('starttime', None)\n-                        elapsed = now - start_time if start_time else None\n-                        s += \"<tr><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>\\n\" % \\\n-                                (chkbox, proc['pid'], proc['domain'], proc['status'], elapsed, proc['logfile'])\n-                    s += \"</table>\\n\"\n-                    s += \"<input type='hidden' name='node' value='%s'>\\n\" % node.name\n-                    s += \"<p><input type='submit' value='Stop selected domains on %s'></p>\\n\" % node.name\n-                    s += \"</form>\\n\"\n-                else:\n-                    s += \"<p>No running domains on %s</p>\\n\" % node.name\n-\n-        return str(s)\n-\n-    def render_domains(self, wc_request):\n-        if wc_request.args:\n-            self.webconsole_control(wc_request)\n-\n-        enabled_domains = set(spiders.asdict().keys())\n-        print \"Enabled domains: %s\" % len(enabled_domains)\n-        inactive_domains = enabled_domains - set(self.running.keys() + [p['domain'] for p in self.pending])\n-\n-        s = self.render_header()\n-\n-        s += \"<h2>Schedule domains</h2>\\n\"\n-\n-        s += \"Inactive domains (not running or pending)<br />\"\n-        s += \"<form method='post' action='.'>\\n\"\n-        s += \"<select name='schedule' multiple='multiple' size='10'>\\n\"\n-        for domain in sorted(inactive_domains):\n-            s += \"<option>%s</option>\\n\" % domain\n-        s += \"</select>\\n\"\n-        s += \"<br />\\n\"\n-        \n-        s += \"Priority:<br />\\n\"\n-        s += \"<input type='text' name='priority'>%s</input>\" % 20\n-        s += \"<br />\\n\"\n-        \n-        # spider settings\n-        s += \"Overrided spider settings:<br />\\n\"\n-        s += \"<textarea name='settings' rows='4'>\\n\"\n-        s += \"UNAVAILABLES_NOTIFY=2\\n\"\n-        s += \"</textarea>\\n\"\n-        s += \"<br />\\n\"\n-        \n-        s += \"<p><input type='submit' value='Schedule selected domains'></p>\\n\"\n-        s += \"</form>\\n\"\n-\n-        s += \"<h2>Domains</h2>\\n\"\n-\n-        s += \"<table border='1'>\\n\"\n-        s += \"<tr><th>Domain</th><th>Status</th><th>Node</th></tr>\\n\"\n-        s += self._domains_table(self.running, '<b>running</b>')\n-        s += \"</table>\\n\"\n-\n-        # pending domains\n-        s += \"<h3>Pending domains</h3>\\n\"\n-        if self.pending:\n-            s += \"<form method='post' action='.'>\\n\"\n-            s += \"<select name='remove' multiple='multiple' size='10'>\\n\"\n-            for p in self.pending:\n-                s += \"<option value='%s'>%s (P:%s)</option>\\n\" % (p['domain'], p['domain'],p['priority'])\n-            s += \"</select>\\n\"\n-            s += \"<p><input type='submit' value='Remove selected pending domains'></p>\\n\"\n-            s += \"</form>\\n\"\n-        else:\n-            s += \"<p>No pending domains</p>\\n\"\n-\n-        return str(s)\n-\n-    def render_header(self):\n-        s = banner(self)\n-        s += \"<p>Nav: \"\n-        s += \"<a href='/cluster_master/'>Home</a> | \"\n-        s += \"<a href='/cluster_master/domains/'>Domains</a> | \"\n-        s += \"<a href='/cluster_master/nodes/'>Nodes</a> (<a href='/cluster_master/nodes/?updatenodes=1'>update</a>)\"\n-        s += \"</p>\"\n-        return s\n-\n-    def _domains_table(self, dict_, status):\n-        s = \"\"\n-        for domain, node in dict_.iteritems():\n-            s += \"<tr><td>%s</td><td>%s</td><td>%s</td></tr>\\n\" % (domain, status, node.name)\n-        return s\n-\n-    def webconsole_discover_module(self):\n-        return self\n-\n-    def ws_status(self, wc_request, verbosity=1):\n-        format = wc_request.args['format'][0] if 'format' in wc_request.args else 'json'\n-        verbosity = int(wc_request.args['verbosity'][0]) if 'verbosity' in wc_request.args else verbosity\n-        wc_request.setHeader('content-type', 'text/plain')\n-        status = {}\n-        nodes_status = {}\n-        if verbosity > 0:\n-            for d, n in self.nodes.iteritems():\n-                nodes_status[d] = n.status_as_dict(verbosity)\n-            status[\"nodes\"] = nodes_status\n-            status[\"pending\"] = self.get_pending(verbosity)\n-            status[\"loading\"] = self.loading\n-            content = serialize(status, format)\n-            return content\n-        return \"\"\n-\n-    def ws_statistics(self, wc_request):\n-        format = wc_request.args['format'][0] if 'format' in wc_request.args else 'json'\n-        content = serialize(self.statistics, format)\n-        return content\n\n@@ -1,80 +0,0 @@\n-#!/usr/bin/env python\n-\"\"\"\n-Cluster control script\n-\"\"\"\n-\n-from optparse import OptionParser\n-import urllib\n-\n-def main():\n-    parser = OptionParser(usage=\"Usage: scrapy-cluster-ctl.py [domain [domain [...]]] [options]\" )\n-    parser.add_option(\"--disablenode\", dest=\"disable_node\", help=\"Disable given node (by name) so it will no accept more run requests.\")\n-    parser.add_option(\"--enablenode\", dest=\"enable_node\", help=\"Enable given node (by name) so it will accept again run requests\")\n-    parser.add_option(\"--format\", dest=\"format\", help=\"Output format. Default: pprint.\", default=\"pprint\")\n-    parser.add_option(\"--list\", metavar=\"FILE\", dest=\"list\", help=\"Specify a file from where to read domains, one per line.\")\n-    parser.add_option(\"--now\", action=\"store_true\", dest=\"now\", help=\"Schedule domains to run with priority now.\")\n-    parser.add_option(\"--output\", metavar=\"FILE\", dest=\"output\", help=\"Output file. If not given, output to stdout.\")\n-    parser.add_option(\"--port\", dest=\"port\", type=\"int\", help=\"Cluster master port. Default: 8060.\", default=8060)\n-    parser.add_option(\"--remove\", dest=\"remove\", action=\"store_true\", help=\"Remove from schedule domains given as args.\")\n-    parser.add_option(\"--schedule\", dest=\"schedule\", action=\"store_true\", help=\"Schedule domains given as args.\")\n-    parser.add_option(\"--server\", dest=\"server\", help=\"Cluster master server name. Default: localhost.\", default=\"localhost\")\n-    parser.add_option(\"--status\", dest=\"status\", action=\"store_true\", help=\"Print cluster master status and quit.\")\n-    parser.add_option(\"--statistics\", dest=\"statistics\", action=\"store_true\", help=\"Print cluster statistics\")\n-    parser.add_option(\"--stop\", dest=\"stop\", action=\"store_true\", help=\"Stops a running domain.\")\n-    parser.add_option(\"--verbosity\", dest=\"verbosity\", type=\"int\", help=\"Sets the report status verbosity.\")\n-    (opts, args) = parser.parse_args()\n-    \n-    output = \"\"\n-    domains = []\n-    urlstring = \"http://%s:%s/cluster_master/ws/\" % (opts.server, opts.port)\n-    post = {\"format\":opts.format}\n-    if isinstance(opts.verbosity, int):\n-        post[\"verbosity\"] = opts.verbosity\n-    \n-    if args:\n-        domains = \",\".join(args)\n-    elif opts.list:\n-        try:\n-            domainlist = []\n-            for d in open(opts.list, \"r\").readlines():\n-                domainlist.append(d.strip())\n-            domains = \",\".join(domainlist)\n-        except IOError:\n-            print \"Can't open file %s\" % opts.list\n-    \n-    if opts.status:\n-        pass\n-    elif opts.statistics:\n-        post[\"statistics\"] = True\n-    elif opts.schedule and domains:\n-        post[\"schedule\"] = domains\n-        if opts.now:\n-            post[\"priority\"] = \"0\"\n-            post[\"settings\"] = \"UNAVAILABLES_NOTIFY=2\"\n-    elif opts.remove and domains:\n-        post[\"remove\"] = domains\n-    elif opts.stop and domains:\n-        post[\"stop\"] = domains\n-    elif opts.disable_node:\n-        post[\"disable_node\"] = opts.disable_node\n-    elif opts.enable_node:\n-        post[\"enable_node\"] = opts.enable_node\n-    else:\n-        parser.print_help()\n-        return\n-\n-    f = urllib.urlopen(urlstring, urllib.urlencode(post))\n-    output=f.read()\n-    if not output:\n-        return\n-    if not opts.output:\n-        print output\n-    else:\n-        try:\n-            open(opts.output, \"w\").write(output)\n-        except IOError:\n-            open(\"/tmp/scrapy-cluster-schedule.tmp\", \"w\").write(output)\n-            print \"Could not open file %s for writing. Output dumped to /tmp/scrapy-cluster-schedule.tmp instead.\" % opts.output\n-\n-if __name__ == '__main__':\n-    main()\n\n@@ -1,24 +0,0 @@\n-#!/usr/bin/python2.5\n-\n-import sys\n-import pprint\n-\n-from twisted.spread import pb\n-from twisted.internet import reactor\n-\n-factory = pb.PBClientFactory()\n-reactor.connectTCP(\"localhost\", 8789, factory)\n-d = factory.getRootObject()\n-\n-sys.argv.pop(0)\n-\n-if not sys.argv or sys.argv[0] == '--status':\n-    d.addCallback(lambda object: object.callRemote(\"status\"))\n-elif sys.argv[0] == \"--stop\":\n-    d.addCallback(lambda object: object.callRemote(\"stop\", sys.argv[1]))\n-elif sys.argv[0] == \"--run\":\n-    d.addCallback(lambda object: object.callRemote(\"run\", sys.argv[1]))\n-\n-d.addCallbacks(callback=pprint.pprint, errback=lambda reason:'error: ' + str(reason.value))\n-d.addCallback(lambda _: reactor.stop())\n-reactor.run()\n\n\n@@ -1,208 +0,0 @@\n-import os\n-import sys\n-import time\n-import datetime\n-import cPickle as pickle\n-\n-from twisted.internet import protocol, reactor\n-from twisted.internet.error import ProcessDone\n-from twisted.spread import pb\n-\n-from scrapy import log\n-from scrapy.core.engine import scrapyengine\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.misc import load_object, gzip_file\n-from scrapy.conf import settings\n-\n-\n-class ScrapyProcessProtocol(protocol.ProcessProtocol):\n-\n-    def __init__(self, worker, domain, logfile=None, spider_settings=None):\n-        self.worker = worker\n-        self.domain = domain\n-        self.logfile = logfile\n-        self.start_time = datetime.datetime.utcnow()\n-        self.status = \"starting\"\n-        self.pid = -1\n-        self.env = {}\n-        # We preserve the original settings format for info purposes (avoid\n-        # lots of unnecesary \"SCRAPY_\")\n-        self.scrapy_settings = spider_settings or {}\n-        self.scrapy_settings.update({'LOGFILE': self.logfile, \n-                                     'CLUSTER_WORKER_ENABLED': 0, \n-                                     'CLUSTER_CRAWLER_ENABLED': 1, \n-                                     'WEBCONSOLE_ENABLED': 0})\n-        pickled_settings = pickle.dumps(self.scrapy_settings)\n-        self.env[\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\"] = pickled_settings\n-        # we nee to pass the worker python path to the crawling process so it\n-        # knows where to find the local_scrapy_settings\n-        self.env[\"PYTHONPATH\"] = \":\".join(sys.path)\n-\n-    def __str__(self):\n-        return \"<ScrapyProcess domain=%s, pid=%s, status=%s>\" % (self.domain, self.pid, self.status)\n-\n-    def info(self):\n-        \"\"\"Return this scrapy process info as a dict.\n-        \n-        The keys are: \n-\n-        domain:\n-          the domain being crawled\n-        pid:\n-          the pid of this process\n-        status:\n-          the status of this process (starting, running)\n-        settings:\n-          the scrapy settings overrided for this process by the worker\n-        logfile:\n-          the log file being used\n-        starttime:\n-          the start time of this process as a UTC datetime object\n-        \"\"\"\n-        return {\"domain\": self.domain, \n-                \"pid\": self.pid, \n-                \"status\": self.status, \n-                \"settings\": self.scrapy_settings, \n-                \"logfile\": self.logfile, \n-                \"starttime\": self.start_time}\n-\n-    def connectionMade(self):\n-        self.pid = self.transport.pid\n-        log.msg(\"ClusterWorker: started domain=%s pid=%d log=%s\" % (self.domain, self.pid, self.logfile))\n-        self.transport.closeStdin()\n-        self.status = \"running\"\n-        self.worker.update_master(self.domain, \"running\")\n-\n-    def processEnded(self, status):\n-        if settings.getbool('CLUSTER_WORKER_GZIP_LOGS'):\n-            try:\n-                self.logfile = gzip_file(self.logfile)\n-            except Exception, e:\n-                log.msg(\"failed to compress %s exception=%s (domain=%s, pid=%s)\" % (self.logfile, e, self.domain, self.pid))\n-\n-        if isinstance(status.value, ProcessDone):\n-            st = \"done\"\n-            er = \"\"\n-        else:\n-            st = \"terminated\"\n-            er = \", error=%s\" % str(status.value)\n-        log.msg(\"ClusterWorker: finished domain=%s status=%s pid=%d log=%s%s\" % (self.domain, st, self.pid, self.logfile, er))\n-        del self.worker.running[self.domain]\n-        self.worker.crawlers.pop(self.pid, None)\n-        self.worker.update_master(self.domain, st)\n-\n-class ClusterWorker(pb.Root):\n-\n-    def __init__(self):\n-        if not settings.getbool('CLUSTER_WORKER_ENABLED'):\n-            raise NotConfigured\n-\n-        self.maxproc = settings.getint('CLUSTER_WORKER_MAXPROC')\n-        self.logdir = settings['CLUSTER_LOGDIR']\n-        self.running = {} # dict of domain->ScrapyProcessControl \n-        self.crawlers = {} # dict of pid->scrapy process remote pb connection\n-        self.starttime = datetime.datetime.utcnow()\n-        self.prerun_hooks = [load_object(f) for f in settings.getlist('CLUSTER_WORKER_PRERUN_HOOKS', [])]\n-        port = settings.getint('CLUSTER_WORKER_PORT')\n-        factory = pb.PBServerFactory(self, unsafeTracebacks=True)\n-        scrapyengine.listenTCP(port, factory)\n-        log.msg(\"Using sys.path: %s\" % repr(sys.path), level=log.DEBUG)\n-\n-    def status(self, rcode=0, rstring=None):\n-        \"\"\"Return the status of this worker as dict.\n-        \n-        The keys of the dict are:\n-        \n-        running:\n-          list of dicts of processes running by this worker. for information\n-          about the dict see ScrapyProcessControl.status()\n-        starttime:\n-          the start time of this worker as a UTC datetime object\n-        timestamp: \n-          the current timestamp as a UTC datetime object\n-        maxproc: \n-          the maximum number of processes supported by this worker\n-        loadavg: \n-          the load average of this worker. see os.getloadavg()\n-        logdir: \n-          the log directory used by this worker\n-        callresponse: \n-          response to the request performed. only available when there was a request\n-        \"\"\"\n-\n-        status = {}\n-        status[\"running\"] = [self.running[k].info() for k in self.running.keys()]\n-        status[\"starttime\"] = self.starttime\n-        status[\"timestamp\"] = datetime.datetime.utcnow()\n-        status[\"maxproc\"] = self.maxproc\n-        status[\"loadavg\"] = os.getloadavg()\n-        status[\"logdir\"] = self.logdir\n-        status[\"callresponse\"] = (rcode, rstring) if rstring else (0, \"No request\")\n-        return status\n-\n-    def update_master(self, domain, domain_status):\n-        try:\n-            deferred = self._master.callRemote(\"update\", self.status(), domain, domain_status)\n-        except pb.DeadReferenceError:\n-            self._master = None\n-            log.msg(\"Lost connection to master\", log.ERROR)\n-        else:\n-            def _eb(failure):\n-                log.msg(\"Error received from ClusterMaster\\n%s\" % failure, level=log.ERROR)\n-            deferred.addErrback(_eb)\n-        \n-    def remote_set_master(self, master):\n-        \"\"\"Set the master for this worker\"\"\"\n-        log.msg(\"ClusterWorker: ClusterMaster connected from %s:%s\" % master.broker.transport.client)\n-        self._master = master\n-        return self.status()\n-\n-    def remote_stop(self, domain):\n-        \"\"\"Stop a running domain\"\"\"\n-        if domain in self.running:\n-            proc = self.running[domain]\n-            log.msg(\"ClusterWorker: Sending shutdown signal to domain=%s pid=%d\" % (domain, proc.pid))\n-            d = self.crawlers[proc.pid].callRemote(\"stop\")\n-            def _close():\n-                proc.status = \"closing\"\n-            d.addCallbacks(callback=_close, errback=lambda reason: log.msg(reason, log.ERROR))\n-            return self.status(ResponseCode.DOMAIN_STOPPED, \"Stopped process %s\" % proc)\n-        else:\n-            return self.status(ResponseCode.DOMAIN_NOT_RUNNING, \"%s: domain not running\" % domain)\n-\n-    def remote_status(self):\n-        \"\"\"Return worker status as a dict. For infomation about the keys see\n-        the the status() method\"\"\" \n-        return self.status()\n-\n-    def remote_run(self, domain, spider_settings=None):\n-        \"\"\"Start scraping the given domain by spawning a process\"\"\"\n-        if len(self.running) < self.maxproc:\n-            if not domain in self.running:\n-                logfile = os.path.join(self.logdir, domain, time.strftime(\"%FT%T.log\"))\n-                if not os.path.exists(os.path.dirname(logfile)):\n-                    os.makedirs(os.path.dirname(logfile))\n-\n-                for prerun_hook in self.prerun_hooks:\n-                    prerun_hook(domain, spider_settings)\n-\n-                scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings)\n-                args = [sys.executable, sys.argv[0], 'crawl', domain]\n-                self.running[domain] = scrapy_proc\n-                reactor.spawnProcess(scrapy_proc, sys.executable, args=args, env=scrapy_proc.env)\n-                return self.status(ResponseCode.DOMAIN_STARTED, \"Started process %s\" % scrapy_proc)\n-            else:\n-                return self.status(ResponseCode.DOMAIN_ALREADY_RUNNING, \"Domain %s already running\" % domain )\n-        else:\n-            return self.status(ResponseCode.NO_FREE_SLOT, \"No free slot to run another domain\")\n-\n-    def remote_register_crawler(self, pid, crawler):\n-        \"\"\"Register the crawler to the list of crawlers managed by this worker\"\"\"\n-        self.crawlers[pid] = crawler\n-\n-class ResponseCode(object):\n-    DOMAIN_STARTED = 1\n-    DOMAIN_STOPPED = 2\n-    DOMAIN_ALREADY_RUNNING = 3\n-    DOMAIN_NOT_RUNNING = 4\n-    NO_FREE_SLOT = 5\n\n@@ -105,11 +105,6 @@ class EngineTest(unittest.TestCase):\n         if not session.wasrun:\n             session.run()\n \n-            # disable extensions that cause problems with tests (probably\n-            # because they leave the reactor in an unclean state)\n-            from scrapy.conf import settings\n-            settings.overrides['CLUSTER_MANAGER_ENABLED'] = 0\n-\n     def test_spider_locator(self):\n         \"\"\"\n         Check the spider is loaded and located properly via the SpiderLocator\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
