{"custom_id": "keras#d57dae7699e5cc1974944b4205e8f83a4b4e0cb3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 1555 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -124,7 +124,7 @@ def load(path, compile=True, options=None):  # pylint: disable=redefined-builtin\n                     'with model.save() or tf.keras.models.save_model(), *NOT* '\n                     'tf.saved_model.save(). To confirm, there should be a file '\n                     'named \"keras_metadata.pb\" in the SavedModel directory.')\n-    _read_legacy_metadata(object_graph_def, metadata)\n+    _read_legacy_metadata(object_graph_def, metadata, path)\n \n   if not metadata.nodes:\n     # When there are no Keras objects, return the results from the core loader\n@@ -193,7 +193,7 @@ def _update_to_current_version(metadata):\n   return metadata\n \n \n-def _read_legacy_metadata(object_graph_def, metadata):\n+def _read_legacy_metadata(object_graph_def, metadata, path):\n   \"\"\"Builds a KerasMetadata proto from the SavedModel ObjectGraphDef.\"\"\"\n   # Older SavedModels store the metadata directly in the proto instead of the\n   # separate pb file.\n@@ -202,11 +202,12 @@ def _read_legacy_metadata(object_graph_def, metadata):\n     if (proto.WhichOneof('kind') == 'user_object' and\n         proto.user_object.identifier in constants.KERAS_OBJECT_IDENTIFIERS):\n       if not proto.user_object.metadata:\n-        raise ValueError('Unable to create a Keras model from this SavedModel. '\n-                         'This SavedModel was created with '\n-                         '`tf.saved_model.save`, and lacks the Keras metadata.'\n-                         'Please save your Keras model by calling `model.save`'\n-                         'or `tf.keras.models.save_model`.')\n+        raise ValueError(\n+            f'Unable to create a Keras model from SavedModel at {path}. '\n+            'This SavedModel was exported with `tf.saved_model.save`, and '\n+            'lacks the Keras metadata file. Please save your Keras model by '\n+            'calling `model.save`or `tf.keras.models.save_model`. Note that '\n+            'you can still load this SavedModel with `tf.saved_model.load`.')\n       metadata.nodes.add(\n           node_id=node_id,\n           node_path=node_paths[node_id],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fdf2ae643c636165e825e72807f8eafc2391cbfa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2752 | Contributors (this commit): 19 | Commits (past 90d): 2 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -220,7 +220,7 @@ class _Merge(Layer):\n         backend.concatenate(masks, axis=0), axis=0, keepdims=False)\n \n   def get_config(self):\n-    return super(_Merge, self).get_config()\n+    return super().get_config()\n \n \n @keras_export('keras.layers.Add')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6be9dcfa320cb4856d5ce0e4e9353bebab58c839", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1080 | Contributors (this commit): 24 | Commits (past 90d): 10 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -152,7 +152,7 @@ def model_to_dot(model,\n     message = (\n         'You must install pydot (`pip install pydot`) '\n         'and install graphviz '\n-        '(see instructions at https://graphviz.gitlab.io/download/) ',\n+        '(see instructions at https://graphviz.gitlab.io/download/) '\n         'for plot_model/model_to_dot to work.')\n     if 'IPython.core.magics.namespace' in sys.modules:\n       # We don't raise an exception here in order to avoid crashing notebook\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e65e814268743a5e944074856dddbbaeaa2362e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 23 | Files Changed: 1 | Hunks: 19 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 54 | Churn Cumulative: 7382 | Contributors (this commit): 100 | Commits (past 90d): 15 | Contributors (cumulative): 100 | DMM Complexity: 0.14285714285714285\n\nDIFF:\n@@ -16,8 +16,6 @@\n # pylint: disable=g-classes-have-attributes\n \"\"\"Callbacks: utilities called at certain points during model training.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n import collections\n import copy\n import csv\n@@ -27,18 +25,20 @@ import re\n import sys\n import time\n \n-import numpy as np\n from keras import backend\n from keras.distribute import distributed_file_utils\n from keras.distribute import worker_training_state\n from keras.optimizer_v2 import learning_rate_schedule\n from keras.utils import generic_utils\n+from keras.utils import io_utils\n from keras.utils import tf_utils\n from keras.utils import version_utils\n from keras.utils.data_utils import Sequence\n from keras.utils.generic_utils import Progbar\n-from keras.utils.io_utils import path_to_string\n from keras.utils.mode_keys import ModeKeys\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n@@ -943,7 +943,7 @@ class TerminateOnNaN(Callback):\n     if loss is not None:\n       loss = tf_utils.sync_to_numpy_or_python_type(loss)\n       if np.isnan(loss) or np.isinf(loss):\n-        print('Batch %d: Invalid loss, terminating training' % (batch))\n+        io_utils.print_msg(f'Batch {batch}: Invalid loss, terminating training')\n         self.model.stop_training = True\n \n \n@@ -1026,7 +1026,7 @@ class ProgbarLogger(Callback):\n     self._reset_progbar()\n     self._maybe_init_progbar()\n     if self.verbose and self.epochs > 1:\n-      print('Epoch %d/%d' % (epoch + 1, self.epochs))\n+      io_utils.print_msg(f'Epoch {epoch + 1}/{self.epochs}')\n \n   def on_train_batch_end(self, batch, logs=None):\n     self._batch_update_progbar(batch, logs)\n@@ -1271,7 +1271,7 @@ class ModelCheckpoint(Callback):\n     self._supports_tf_logs = True\n     self.monitor = monitor\n     self.verbose = verbose\n-    self.filepath = path_to_string(filepath)\n+    self.filepath = io_utils.path_to_string(filepath)\n     self.save_best_only = save_best_only\n     self.save_weights_only = save_weights_only\n     self.save_freq = save_freq\n@@ -1419,9 +1419,10 @@ class ModelCheckpoint(Callback):\n           else:\n             if self.monitor_op(current, self.best):\n               if self.verbose > 0:\n-                print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n-                      ' saving model to %s' % (epoch + 1, self.monitor,\n-                                               self.best, current, filepath))\n+                io_utils.print_msg(\n+                    f'\\nEpoch {epoch + 1}: {self.monitor} improved '\n+                    f'from {self.best:.5f} to {current:.5f}, '\n+                    f'saving model to {filepath}')\n               self.best = current\n               if self.save_weights_only:\n                 self.model.save_weights(\n@@ -1430,11 +1431,13 @@ class ModelCheckpoint(Callback):\n                 self.model.save(filepath, overwrite=True, options=self._options)\n             else:\n               if self.verbose > 0:\n-                print('\\nEpoch %05d: %s did not improve from %0.5f' %\n-                      (epoch + 1, self.monitor, self.best))\n+                io_utils.print_msg(\n+                    f'\\nEpoch {epoch + 1}: '\n+                    f'{self.monitor} did not improve from {self.best:.5f}')\n         else:\n           if self.verbose > 0:\n-            print('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\n+            io_utils.print_msg(\n+                f'\\nEpoch {epoch + 1}: saving model to {filepath}')\n           if self.save_weights_only:\n             self.model.save_weights(\n                 filepath, overwrite=True, options=self._options)\n@@ -1834,13 +1837,15 @@ class EarlyStopping(Callback):\n       self.model.stop_training = True\n       if self.restore_best_weights and self.best_weights is not None:\n         if self.verbose > 0:\n-          print('Restoring model weights from the end of the best epoch: '\n+          io_utils.print_msg(\n+              'Restoring model weights from the end of the best epoch: '\n               f'{self.best_epoch + 1}.')\n         self.model.set_weights(self.best_weights)\n \n   def on_train_end(self, logs=None):\n     if self.stopped_epoch > 0 and self.verbose > 0:\n-      print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n+      io_utils.print_msg(\n+          f'Epoch {self.stopped_epoch + 1}: early stopping')\n \n   def get_monitor_value(self, logs):\n     logs = logs or {}\n@@ -1977,8 +1982,9 @@ class LearningRateScheduler(Callback):\n           f'The dtype of `lr` Tensor should be float. Got: {lr.dtype}')\n     backend.set_value(self.model.optimizer.lr, backend.get_value(lr))\n     if self.verbose > 0:\n-      print('\\nEpoch %05d: LearningRateScheduler setting learning '\n-            'rate to %s.' % (epoch + 1, lr))\n+      io_utils.print_msg(\n+          f'\\nEpoch {epoch + 1}: LearningRateScheduler setting learning '\n+          'rate to {lr}.')\n \n   def on_epoch_end(self, epoch, logs=None):\n     logs = logs or {}\n@@ -2022,8 +2028,8 @@ def keras_model_summary(name, data, step=None):\n     logging.warning('Model failed to serialize as JSON. Ignoring... %s', exc)\n     return False\n \n-  with tf.summary.experimental.summary_scope(name, 'graph_keras_model',\n-                                    [data, step]) as (tag, _):\n+  with tf.summary.experimental.summary_scope(\n+      name, 'graph_keras_model', [data, step]) as (tag, _):\n     with tf.device('cpu:0'):\n       tensor = tf.constant(json_string, dtype=tf.string)\n     return tf.summary.write(\n@@ -2175,7 +2181,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     self._supports_tf_logs = True\n     self._validate_kwargs(kwargs)\n \n-    self.log_dir = path_to_string(log_dir)\n+    self.log_dir = io_utils.path_to_string(log_dir)\n     self.histogram_freq = histogram_freq\n     self.write_graph = write_graph\n     self.write_images = write_images\n@@ -2350,6 +2356,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n \n   def _init_profile_batch(self, profile_batch):\n     \"\"\"Validate profile_batch value and set the range of batches to profile.\n+\n     Sets values of _start_batch and _stop_batch attributes,\n     specifying the start and stop batch to profile.\n     Setting `profile_batch=0` disables profiling.\n@@ -2715,8 +2722,9 @@ class ReduceLROnPlateau(Callback):\n             new_lr = max(new_lr, self.min_lr)\n             backend.set_value(self.model.optimizer.lr, new_lr)\n             if self.verbose > 0:\n-              print('\\nEpoch %05d: ReduceLROnPlateau reducing learning '\n-                    'rate to %s.' % (epoch + 1, new_lr))\n+              io_utils.print_msg(\n+                  f'\\nEpoch {epoch +1}: '\n+                  f'ReduceLROnPlateau reducing learning rate to {new_lr}.')\n             self.cooldown_counter = self.cooldown\n             self.wait = 0\n \n@@ -2747,7 +2755,7 @@ class CSVLogger(Callback):\n \n   def __init__(self, filename, separator=',', append=False):\n     self.sep = separator\n-    self.filename = path_to_string(filename)\n+    self.filename = io_utils.path_to_string(filename)\n     self.append = append\n     self.writer = None\n     self.keys = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bd26bab4c15b346c0c44b42209a09848db50f95f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4437 | Contributors (this commit): 18 | Commits (past 90d): 4 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -374,7 +374,7 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n \n   `loss = 100 * abs((y_true - y_pred) / y_true)`\n   \n-  Note that to avoid division by zero error, a small value epsilon is added to the denominator.\n+  Note that to avoid dividing by zero, a small epsilon value is added to the denominator.\n \n   Standalone usage:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#36c086bbf5e22d40eb9edb9ee8534ddbe35a88ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 14 | Churn Cumulative: 4184 | Contributors (this commit): 50 | Commits (past 90d): 23 | Contributors (cumulative): 62 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,8 +27,7 @@ ABSL_LOGGING.enable = False\n \n def print_msg(message):\n   \"\"\"Print the message to absl logging or stdout.\"\"\"\n-  # Checking have the attribute before accessing as a temporary bug fix.\n-  # Need to drill down for the cause. (b/154647185)\n+  # Use `getattr` in case `ABSL_LOGGING` does not have the `enable` attribute.\n   if getattr(ABSL_LOGGING, 'enable', False):\n     absl.logging.info(message)\n   else:\n\n@@ -20,6 +20,7 @@ import tensorflow.compat.v2 as tf\n import functools\n import weakref\n \n+from keras.utils import io_utils\n import numpy as np\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -139,7 +140,7 @@ def print_summary(model,\n           If not provided, defaults to `False`.\n   \"\"\"\n   if print_fn is None:\n-    print_fn = print\n+    print_fn = io_utils.print_msg\n \n   if model.__class__.__name__ == 'Sequential':\n     sequential_like = True\n\n@@ -23,6 +23,7 @@ import multiprocessing.dummy\n import os\n import pickle\n import shutil\n+import sys\n import time\n import timeit\n \n@@ -77,6 +78,13 @@ class LayerUtilsTest(tf.test.TestCase):\n     except ImportError:\n       pass\n \n+  def test_print_summary_without_print_fn(self):\n+    model = keras.Sequential([\n+        keras.layers.Dense(5, input_shape=(10,), name='dense')])\n+    with self.captureWritesToStream(sys.stdout) as printed:\n+      layer_utils.print_summary(model)\n+    self.assertIn('dense (Dense)', printed.contents())\n+\n   def test_print_summary_expand_nested(self):\n     shape = (None, None, 3)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#afdfddab35c2bbfd1dd979bdb8e5a8b366bd9143", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 313 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -83,8 +83,8 @@ class DeterministicRandomTestTool(object):\n       else:\n         if op_seed in self._observed_seeds:\n           raise ValueError(\n-              \"This `DeterministicTestTool` object is trying to re-use the \" +\n-              \"already-used operation seed {}. \".format(op_seed) +\n+              \"This `DeterministicRandomTestTool` object is trying to re-use the \"\n+              + \"already-used operation seed {}. \".format(op_seed) +\n               \"It cannot guarantee random numbers will match between eager \" +\n               \"and sessions when an operation seed is reused. \" +\n               \"You most likely set \" +\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e4b31ad4c5e806163bf7475cbab89bcef5ba9ef6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 154 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 154 | Churn Cumulative: 12884 | Contributors (this commit): 27 | Commits (past 90d): 14 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3501,6 +3501,108 @@ class OneHotIoU(IoU):\n     return super().update_state(y_true, y_pred, sample_weight)\n \n \n+@keras_export('keras.metrics.OneHotMeanIoU')\n+class OneHotMeanIoU(MeanIoU):\n+  \"\"\"Computes mean Intersection-Over-Union metric for one-hot encoded labels.\n+\n+  General definition and computation:\n+\n+  Intersection-Over-Union is a common evaluation metric for semantic image\n+  segmentation.\n+\n+  For an individual class, the IoU metric is defined as follows:\n+\n+  ```\n+  iou = true_positives / (true_positives + false_positives + false_negatives)\n+  ```\n+\n+  To compute IoUs, the predictions are accumulated in a confusion matrix,\n+  weighted by `sample_weight` and the metric is then calculated from it.\n+\n+  If `sample_weight` is `None`, weights default to 1.\n+  Use `sample_weight` of 0 to mask values.\n+\n+  This class can be used to compute the mean IoU for multi-class classification\n+  tasks where the labels are one-hot encoded (the last axis should have one\n+  dimension per class). Note that the predictions should also have the same\n+  shape. To compute the mean IoU, first the labels and predictions are converted\n+  back into integer format by taking the argmax over the class axis. Then the\n+  same computation steps as for the base `MeanIoU` class apply.\n+\n+  Note, if there is only one channel in the labels and predictions, this class\n+  is the same as class `MeanIoU`. In this case, use `MeanIoU` instead.\n+\n+  Also, make sure that `num_classes` is equal to the number of classes in the\n+  data, to avoid a \"labels out of bound\" error when the confusion matrix is\n+  computed.\n+\n+  Args:\n+    num_classes: The possible number of labels the prediction task can have.\n+      A confusion matrix of shape `(num_classes, num_classes)` will be\n+      allocated to accumulate predictions from which the metric is calculated.\n+    name: (Optional) string name of the metric instance.\n+    dtype: (Optional) data type of the metric result.\n+\n+  Standalone usage:\n+\n+  >>> y_true = tf.constant([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])\n+  >>> y_pred = tf.constant([[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1],\n+  >>>                       [0.1, 0.4, 0.5]])\n+  >>> sample_weight = [0.1, 0.2, 0.3, 0.4]\n+  >>> m = metrics.OneHotMeanIoU(num_classes=3)\n+  >>> m.update_state(y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)\n+  >>> # cm = [[0, 0, 0.2+0.4],\n+  >>> #       [0.3, 0, 0],\n+  >>> #       [0, 0, 0.1]]\n+  >>> # sum_row = [0.3, 0, 0.7], sum_col = [0.6, 0.3, 0.1]\n+  >>> # true_positives = [0, 0, 0.1]\n+  >>> # single_iou = true_positives / (sum_row + sum_col - true_positives))\n+  >>> # mean_iou = (0 + 0 + 0.1 / (0.7 + 0.1 - 0.1)) / 3\n+  >>> m.result().numpy()\n+  0.048\n+\n+  Usage with `compile()` API:\n+\n+  ```python\n+  model.compile(\n+    optimizer='sgd',\n+    loss='mse',\n+    metrics=[tf.keras.metrics.OneHotMeanIoU(num_classes=3)])\n+  ```\n+  \"\"\"\n+\n+  def __init__(\n+      self,\n+      num_classes: int,\n+      name=None,\n+      dtype=None,\n+  ):\n+    super(OneHotMeanIoU, self).__init__(\n+        num_classes=num_classes,\n+        name=name,\n+        dtype=dtype,\n+    )\n+\n+  def update_state(self, y_true, y_pred, sample_weight=None):\n+    \"\"\"Accumulates the confusion matrix statistics.\n+\n+    Args:\n+      y_true: The ground truth values.\n+      y_pred: The predicted values.\n+      sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n+        `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n+        be broadcastable to `y_true`.\n+\n+    Returns:\n+      Update op.\n+    \"\"\"\n+    # Select max hot-encoding channels to convert into all-class format\n+    y_true = tf.argmax(y_true, axis=-1, output_type=tf.int32)\n+    y_pred = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n+\n+    return super().update_state(y_true, y_pred, sample_weight)\n+\n+\n @keras_export('keras.metrics.MeanTensor')\n class MeanTensor(Metric):\n   \"\"\"Computes the element-wise (weighted) mean of the given tensors.\n\n@@ -1600,6 +1600,58 @@ class OneHotIoUTest(tf.test.TestCase):\n     self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n \n \n+@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+class OneHotMeanIoUTest(tf.test.TestCase):\n+\n+  def test_unweighted(self):\n+    y_true = tf.constant([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])\n+    # y_true will be converted to [2, 0, 1, 0]\n+    y_pred = tf.constant([[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1],\n+                          [0.1, 0.4, 0.5]])\n+    # y_pred will be converted to [2, 2, 0, 2]\n+    # cm = [[0, 0, 2],\n+    #       [1, 0, 0],\n+    #       [0, 0, 1]\n+    # sum_row = [1, 0, 3], sum_col = [2, 1, 1], true_positives = [0, 0, 1]\n+    # iou = true_positives / (sum_row + sum_col - true_positives))\n+    expected_result = (0 + 0 + 1 / (3 + 1 - 1)) / 3\n+    obj = metrics.OneHotMeanIoU(num_classes=3)\n+    self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+    result = obj(y_true, y_pred)\n+    self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+  def test_weighted(self):\n+    y_true = tf.constant([\n+        [0, 0, 1],\n+        [1, 0, 0],\n+        [0, 1, 0],\n+        [1, 0, 0],\n+        [1, 0, 0],\n+    ])\n+    # y_true will be converted to [2, 0, 1, 0, 0]\n+    y_pred = tf.constant([\n+        [0.2, 0.3, 0.5],\n+        [0.1, 0.2, 0.7],\n+        [0.5, 0.3, 0.1],\n+        [0.1, 0.4, 0.5],\n+        [0.6, 0.2, 0.2],\n+    ])\n+    # y_pred will be converted to [2, 2, 0, 2, 0]\n+    sample_weight = [0.1, 0.2, 0.3, 0.3, 0.1]\n+    # cm = [[0.1, 0, 0.2+0.3],\n+    #       [0.3, 0, 0],\n+    #       [0, 0, 0.1]]\n+    # sum_row = [0.4, 0, 0.6], sum_col = [0.6, 0.3, 0.1]\n+    # true_positives = [0.1, 0, 0.1]\n+    # iou = true_positives / (sum_row + sum_col - true_positives))\n+    expected_result = (0.1 / (0.4 + 0.6 - 0.1) + 0 + 0.1 /\n+                       (0.6 + 0.1 - 0.1)) / 3\n+    obj = metrics.OneHotMeanIoU(num_classes=3)\n+    self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+    result = obj(y_true, y_pred, sample_weight=sample_weight)\n+    self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+\n class MeanTensorTest(tf.test.TestCase, parameterized.TestCase):\n \n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4aabedcb294be97b2321c47e428c16ba3e687d8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 27 | Churn Cumulative: 5307 | Contributors (this commit): 61 | Commits (past 90d): 10 | Contributors (cumulative): 65 | DMM Complexity: 0.5\n\nDIFF:\n@@ -31,6 +31,7 @@ import weakref\n \n import numpy as np\n \n+from keras.utils import io_utils\n from keras.utils import tf_contextlib\n from keras.utils import tf_inspect\n from tensorflow.python.util.tf_export import keras_export\n@@ -902,6 +903,7 @@ class Progbar:\n         self._values[k] = [v, 1]\n     self._seen_so_far = current\n \n+    message = ''\n     now = time.time()\n     info = ' - %.0fs' % (now - self._start)\n     if current == self.target:\n@@ -912,10 +914,10 @@ class Progbar:\n \n       prev_total_width = self._total_width\n       if self._dynamic_display:\n-        sys.stdout.write('\\b' * prev_total_width)\n-        sys.stdout.write('\\r')\n+        message += '\\b' * prev_total_width\n+        message += '\\r'\n       else:\n-        sys.stdout.write('\\n')\n+        message += '\\n'\n \n       if self.target is not None:\n         numdigits = int(np.log10(self.target)) + 1\n@@ -934,7 +936,7 @@ class Progbar:\n         bar = '%7d/Unknown' % current\n \n       self._total_width = len(bar)\n-      sys.stdout.write(bar)\n+      message += bar\n \n       time_per_unit = self._estimate_step_duration(current, now)\n \n@@ -970,8 +972,9 @@ class Progbar:\n       if finalize:\n         info += '\\n'\n \n-      sys.stdout.write(info)\n-      sys.stdout.flush()\n+      message += info\n+      io_utils.print_msg(message, line_break=False)\n+      message = ''\n \n     elif self.verbose == 2:\n       if finalize:\n@@ -993,8 +996,9 @@ class Progbar:\n           info += ' -' + self._format_time(time_per_epoch, 'epoch')\n           info += ' -' + self._format_time(avg_time_per_step, self.unit_name)\n           info += '\\n'\n-        sys.stdout.write(info)\n-        sys.stdout.flush()\n+        message += info\n+        io_utils.print_msg(message, line_break=False)\n+        message = ''\n \n     self._last_update = now\n \n\n@@ -25,15 +25,16 @@ ABSL_LOGGING = threading.local()\n ABSL_LOGGING.enable = False\n \n \n-def print_msg(message):\n+def print_msg(message, line_break=True):\n   \"\"\"Print the message to absl logging or stdout.\"\"\"\n   # Use `getattr` in case `ABSL_LOGGING` does not have the `enable` attribute.\n   if getattr(ABSL_LOGGING, 'enable', False):\n     absl.logging.info(message)\n   else:\n-    # Simulate the print function,\n-    # which has a new line at the end.\n+    if line_break:\n       sys.stdout.write(message + '\\n')\n+    else:\n+      sys.stdout.write(message)\n     sys.stdout.flush()\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8766c22d198be8484ba58bb8ffe4aca8770aa2c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 16219 | Contributors (this commit): 116 | Commits (past 90d): 22 | Contributors (cumulative): 116 | DMM Complexity: None\n\nDIFF:\n@@ -2713,8 +2713,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     if index is not None:\n       if len(self.layers) <= index:\n-        raise ValueError(f'Was asked to retrieve layer at index {str(index)}'\n-                         f' but model only has {str(len(self.layers))}'\n+        raise ValueError(f'Was asked to retrieve layer at index {index}'\n+                         f' but model only has {len(self.layers)}'\n                          ' layers.')\n       else:\n         return self.layers[index]\n@@ -2723,8 +2723,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       for layer in self.layers:\n         if layer.name == name:\n           return layer\n-      raise ValueError(f'No such layer: {name}. Existing layers are '\n-                       f'{self.layers}.')\n+      raise ValueError(f'No such layer: {name}. Existing layers are: '\n+                       f'{list(layer.name for layer in self.layers)}.')\n     raise ValueError('Provide either a layer name or layer index at '\n                      '`get_layer`.')\n \n@@ -2850,7 +2850,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       raise ValueError(\n           f'Models passed to `{method_name}` can only have `training` '\n           'and the first argument in `call()` as positional arguments, '\n-          f'found: {str(extra_args)}.')\n+          f'found: {extra_args}.')\n \n   def _validate_compile(self, optimizer, metrics, **kwargs):\n     \"\"\"Performs validation checks for the default `compile()`.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#05daaa0891ca60ed41b8101a99c0bc71dc9c5c50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 21 | Churn Cumulative: 909 | Contributors (this commit): 3 | Commits (past 90d): 18 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,7 +26,7 @@ from keras.optimizer_v2 import utils as optimizer_utils\n import tensorflow.compat.v2 as tf\n \n \n-class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n+class _BaseOptimizer(tf.Module):\n   \"\"\"Optimizer base class, which only supports non-distribute use case.\"\"\"\n \n   def __init__(self, name, clipnorm=None, clipvalue=None, global_clipnorm=None):\n@@ -164,9 +164,12 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n     if isinstance(learning_rate, learning_rate_schedule.LearningRateSchedule):\n       # Create a variable to hold the current learning rate.\n       self._current_learning_rate = tf.Variable(\n-          learning_rate(self.iterations), dtype=tf.float32)\n+          learning_rate(self.iterations),\n+          name=\"learning_rate\",\n+          dtype=tf.float32)\n       return learning_rate\n-    return tf.Variable(learning_rate, dtype=backend.floatx())\n+    return tf.Variable(\n+        learning_rate, name=\"learning_rate\", dtype=backend.floatx())\n \n   @abc.abstractmethod\n   def build(self, var_list):\n\n@@ -90,6 +90,18 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n       _ = adam_new.Adam(\n           learning_rate=1, epsilon=0, global_clipnorm=1, clipnorm=1)\n \n+  def testReturnAllOptimizerVariables(self):\n+    x = tf.Variable([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32)\n+    optimizer = adam_new.Adam()\n+    grads = tf.convert_to_tensor([[1.0, 2.0], [3.0, 4.0]])\n+    optimizer.apply_gradients(zip([grads], [x]))\n+    optimizer_variables = optimizer.variables\n+    all_names = [var._shared_name for var in optimizer_variables]\n+    self.assertLen(optimizer_variables, 4)\n+    self.assertCountEqual(\n+        all_names,\n+        [\"iteration\", \"learning_rate\", \"Adam/m/Variable\", \"Adam/v/Variable\"])\n+\n   def testSetLearningRate(self):\n     optimizer = adam_new.Adam(learning_rate=1.0)\n     self.assertIsInstance(optimizer._learning_rate, tf.Variable)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#47526f18249658c460e3a9d22efa00dffd940c9d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 867 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -43,7 +43,7 @@ class ReductionV2:\n      `Reduction.NONE` just means that no **additional** reduction is applied by\n      the class wrapper. For categorical losses with an example input shape of\n      `[batch, W, H, n_classes]` the `n_classes` dimension is reduced. For\n-     pointwise losses your must include a dummy axis so that `[batch, W, H, 1]`\n+     pointwise losses you must include a dummy axis so that `[batch, W, H, 1]`\n      is reduced to `[batch, W, H]`. Without the dummy axis `[batch, W, H]`\n      will be incorrectly reduced to `[batch, W]`.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#414acdd0a9bae368ce65ca942f624c39405e7a21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 759 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -77,14 +77,14 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n     https://keras.io/guides/transfer_learning/).\n \n   Note: each Keras Application expects a specific kind of input preprocessing.\n-  For ModelNetV3, by default input preprocessing is included as a part of the\n+  For MobileNetV3, by default input preprocessing is included as a part of the\n   model (as a `Rescaling` layer), and thus\n   `tf.keras.applications.mobilenet_v3.preprocess_input` is actually a\n-  pass-through function. In this use case, ModelNetV3 models expect their inputs\n+  pass-through function. In this use case, MobileNetV3 models expect their inputs\n   to be float tensors of pixels with values in the [0-255] range.\n   At the same time, preprocessing as a part of the model (i.e. `Rescaling`\n   layer) can be disabled by setting `include_preprocessing` argument to False.\n-  With preprocessing disabled ModelNetV3 models expect their inputs to be float\n+  With preprocessing disabled MobileNetV3 models expect their inputs to be float\n   tensors of pixels with values in the [-1, 1] range.\n \n   Args:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1771dae94e6a8c12e78219dc489e616539c2b00c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 321 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -61,7 +61,7 @@ def timeseries_dataset_from_array(\n       `data[i], data[i + r], ... data[i + sequence_length]`\n       are used for create a sample sequence.\n     batch_size: Number of timeseries samples in each batch\n-      (except maybe the last one).\n+      (except maybe the last one). If `None`, it doesn't make batches.\n     shuffle: Whether to shuffle output samples,\n       or instead draw them in chronological order.\n     seed: Optional int; random seed for shuffling.\n@@ -227,7 +227,9 @@ def timeseries_dataset_from_array(\n   if shuffle:\n     # Shuffle locally at each iteration\n     dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n-  dataset = dataset.prefetch(tf.data.AUTOTUNE).batch(batch_size)\n+  dataset = dataset.prefetch(tf.data.AUTOTUNE)\n+  if batch_size is not None:\n+    dataset = dataset.batch(batch_size)\n   return dataset\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#692b4036b88ec196c89c88c4e5100e5991ce81b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 128 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 30 | Methods Changed: 6 | Complexity Δ (Sum/Max): 13/11 | Churn Δ: 143 | Churn Cumulative: 5232 | Contributors (this commit): 6 | Commits (past 90d): 11 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,6 +20,7 @@\n from keras import backend\n from keras.engine import base_layer\n from keras.engine import base_preprocessing_layer\n+from keras.layers.preprocessing import preprocessing_utils as utils\n from keras.preprocessing.image import smart_resize\n from keras.utils import control_flow_util\n import numpy as np\n@@ -93,6 +94,14 @@ class Resizing(base_layer.Layer):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('Resizing').set(True)\n \n   def call(self, inputs):\n+    # tf.image.resize will always output float32 and operate more efficiently on\n+    # float32 unless interpolation is nearest, in which case ouput type matches\n+    # input type.\n+    if self.interpolation == 'nearest':\n+      input_dtype = self.compute_dtype\n+    else:\n+      input_dtype = tf.float32\n+    inputs = utils.ensure_tensor(inputs, dtype=input_dtype)\n     if self.crop_to_aspect_ratio:\n       outputs = smart_resize(\n           inputs,\n@@ -103,6 +112,7 @@ class Resizing(base_layer.Layer):\n           inputs,\n           size=[self.height, self.width],\n           method=self._interpolation_method)\n+    outputs = tf.cast(outputs, self.compute_dtype)\n     return outputs\n \n   def compute_output_shape(self, input_shape):\n@@ -158,7 +168,7 @@ class CenterCrop(base_layer.Layer):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('CenterCrop').set(True)\n \n   def call(self, inputs):\n-    inputs = tf.convert_to_tensor(inputs)\n+    inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n     input_shape = tf.shape(inputs)\n     h_diff = input_shape[H_AXIS] - self.height\n     w_diff = input_shape[W_AXIS] - self.width\n@@ -169,10 +179,13 @@ class CenterCrop(base_layer.Layer):\n       return tf.image.crop_to_bounding_box(inputs, h_start, w_start,\n                                            self.height, self.width)\n \n-    outputs = tf.cond(\n-        tf.reduce_all((h_diff >= 0, w_diff >= 0)), center_crop,\n-        lambda: smart_resize(inputs, [self.height, self.width]))\n-    return tf.cast(outputs, inputs.dtype)\n+    def upsize():\n+      outputs = smart_resize(inputs, [self.height, self.width])\n+      # smart_resize will always output float32, so we need to re-cast.\n+      return tf.cast(outputs, self.compute_dtype)\n+\n+    return tf.cond(\n+        tf.reduce_all((h_diff >= 0, w_diff >= 0)), center_crop, upsize)\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n@@ -232,7 +245,7 @@ class RandomCrop(base_layer.BaseRandomLayer):\n   def call(self, inputs, training=True):\n     if training is None:\n       training = backend.learning_phase()\n-    inputs = tf.convert_to_tensor(inputs)\n+    inputs = utils.ensure_tensor(inputs)\n     input_shape = tf.shape(inputs)\n     h_diff = input_shape[H_AXIS] - self.height\n     w_diff = input_shape[W_AXIS] - self.width\n@@ -248,7 +261,7 @@ class RandomCrop(base_layer.BaseRandomLayer):\n     outputs = tf.cond(\n         tf.reduce_all((training, h_diff >= 0, w_diff >= 0)), random_crop,\n         lambda: smart_resize(inputs, [self.height, self.width]))\n-    return tf.cast(outputs, inputs.dtype)\n+    return tf.cast(outputs, self.compute_dtype)\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n@@ -305,7 +318,7 @@ class Rescaling(base_layer.Layer):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('Rescaling').set(True)\n \n   def call(self, inputs):\n-    dtype = self._compute_dtype\n+    dtype = self.compute_dtype\n     scale = tf.cast(self.scale, dtype)\n     offset = tf.cast(self.offset, dtype)\n     return tf.cast(inputs, dtype) * scale + offset\n@@ -379,6 +392,7 @@ class RandomFlip(base_layer.BaseRandomLayer):\n   def call(self, inputs, training=True):\n     if training is None:\n       training = backend.learning_phase()\n+    inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n \n     def random_flipped_inputs():\n       flipped_outputs = inputs\n@@ -523,7 +537,7 @@ class RandomTranslation(base_layer.BaseRandomLayer):\n     if training is None:\n       training = backend.learning_phase()\n \n-    inputs = tf.convert_to_tensor(inputs)\n+    inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n     original_shape = inputs.shape\n     unbatched = inputs.shape.rank == 3\n     # The transform op only accepts rank 4 inputs, so if we have an unbatched\n@@ -825,7 +839,7 @@ class RandomRotation(base_layer.BaseRandomLayer):\n     if training is None:\n       training = backend.learning_phase()\n \n-    inputs = tf.convert_to_tensor(inputs)\n+    inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n     original_shape = inputs.shape\n     unbatched = inputs.shape.rank == 3\n     # The transform op only accepts rank 4 inputs, so if we have an unbatched\n@@ -978,7 +992,7 @@ class RandomZoom(base_layer.BaseRandomLayer):\n     if training is None:\n       training = backend.learning_phase()\n \n-    inputs = tf.convert_to_tensor(inputs)\n+    inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n     original_shape = inputs.shape\n     unbatched = inputs.shape.rank == 3\n     # The transform op only accepts rank 4 inputs, so if we have an unbatched\n@@ -1130,6 +1144,7 @@ class RandomContrast(base_layer.BaseRandomLayer):\n     if training is None:\n       training = backend.learning_phase()\n \n+    inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n     def random_contrasted_inputs():\n       seed = self._random_generator.make_seed_for_stateless_op()\n       if seed is not None:\n@@ -1224,6 +1239,8 @@ class RandomHeight(base_layer.BaseRandomLayer):\n     if training is None:\n       training = backend.learning_phase()\n \n+    inputs = utils.ensure_tensor(inputs)\n+\n     def random_height_inputs():\n       \"\"\"Inputs height-adjusted with random ops.\"\"\"\n       inputs_shape = tf.shape(inputs)\n@@ -1237,13 +1254,16 @@ class RandomHeight(base_layer.BaseRandomLayer):\n       adjusted_size = tf.stack([adjusted_height, img_wd])\n       output = tf.image.resize(\n           images=inputs, size=adjusted_size, method=self._interpolation_method)\n+      # tf.resize will output float32 in many cases regardless of input type.\n+      output = tf.cast(output, self.compute_dtype)\n       output_shape = inputs.shape.as_list()\n       output_shape[H_AXIS] = None\n       output.set_shape(output_shape)\n       return output\n \n-    return control_flow_util.smart_cond(training, random_height_inputs,\n-                                        lambda: inputs)\n+    return control_flow_util.smart_cond(\n+        training, random_height_inputs,\n+        lambda: tf.cast(inputs, self.compute_dtype))\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n@@ -1325,6 +1345,8 @@ class RandomWidth(base_layer.BaseRandomLayer):\n     if training is None:\n       training = backend.learning_phase()\n \n+    inputs = utils.ensure_tensor(inputs)\n+\n     def random_width_inputs():\n       \"\"\"Inputs width-adjusted with random ops.\"\"\"\n       inputs_shape = tf.shape(inputs)\n@@ -1338,13 +1360,16 @@ class RandomWidth(base_layer.BaseRandomLayer):\n       adjusted_size = tf.stack([img_hd, adjusted_width])\n       output = tf.image.resize(\n           images=inputs, size=adjusted_size, method=self._interpolation_method)\n+      # tf.resize will output float32 in many cases regardless of input type.\n+      output = tf.cast(output, self.compute_dtype)\n       output_shape = inputs.shape.as_list()\n       output_shape[W_AXIS] = None\n       output.set_shape(output_shape)\n       return output\n \n-    return control_flow_util.smart_cond(training, random_width_inputs,\n-                                        lambda: inputs)\n+    return control_flow_util.smart_cond(\n+        training, random_width_inputs,\n+        lambda: tf.cast(inputs, self.compute_dtype))\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n\n@@ -159,6 +159,14 @@ class ResizingTest(keras_parameterized.TestCase):\n       expected_output = np.reshape(expected_output, (2, 2, 1))\n       self.assertAllEqual(expected_output, output_image)\n \n+  @testing_utils.run_v2_only\n+  def test_output_dtypes(self):\n+    inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n+    layer = image_preprocessing.Resizing(2, 2)\n+    self.assertAllEqual(layer(inputs).dtype, 'float32')\n+    layer = image_preprocessing.Resizing(2, 2, dtype='uint8')\n+    self.assertAllEqual(layer(inputs).dtype, 'uint8')\n+\n   @parameterized.named_parameters(\n       ('batch_crop_to_aspect_ratio', True, True),\n       ('batch_dont_crop_to_aspect_ratio', False, True),\n@@ -273,6 +281,14 @@ class CenterCropTest(keras_parameterized.TestCase):\n       expected_output = np.reshape(expected_output, (2, 2, 1))\n       self.assertAllEqual(expected_output, output_image)\n \n+  @testing_utils.run_v2_only\n+  def test_output_dtypes(self):\n+    inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n+    layer = image_preprocessing.CenterCrop(2, 2)\n+    self.assertAllEqual(layer(inputs).dtype, 'float32')\n+    layer = image_preprocessing.CenterCrop(2, 2, dtype='uint8')\n+    self.assertAllEqual(layer(inputs).dtype, 'uint8')\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomCropTest(keras_parameterized.TestCase):\n@@ -382,6 +398,14 @@ class RandomCropTest(keras_parameterized.TestCase):\n         actual_output = layer(inp, training=True)\n         self.assertAllClose(inp[2:10, 2:10, :], actual_output)\n \n+  @testing_utils.run_v2_only\n+  def test_output_dtypes(self):\n+    inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n+    layer = image_preprocessing.RandomCrop(2, 2)\n+    self.assertAllEqual(layer(inputs).dtype, 'float32')\n+    layer = image_preprocessing.RandomCrop(2, 2, dtype='uint8')\n+    self.assertAllEqual(layer(inputs).dtype, 'uint8')\n+\n \n class RescalingTest(keras_parameterized.TestCase):\n \n@@ -422,6 +446,14 @@ class RescalingTest(keras_parameterized.TestCase):\n     outputs = layer(inputs)\n     self.assertAllClose(outputs.numpy(), inputs.numpy() * (1. / 127.5) - 1)\n \n+  @testing_utils.run_v2_only\n+  def test_output_dtypes(self):\n+    inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n+    layer = image_preprocessing.Rescaling(0.5)\n+    self.assertAllEqual(layer(inputs).dtype, 'float32')\n+    layer = image_preprocessing.Rescaling(0.5, dtype='uint8')\n+    self.assertAllEqual(layer(inputs).dtype, 'uint8')\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomFlipTest(keras_parameterized.TestCase):\n@@ -521,6 +553,14 @@ class RandomFlipTest(keras_parameterized.TestCase):\n         actual_output = layer(input_image, training=True)\n         self.assertAllClose(expected_output, actual_output)\n \n+  @testing_utils.run_v2_only\n+  def test_output_dtypes(self):\n+    inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n+    layer = image_preprocessing.RandomFlip()\n+    self.assertAllEqual(layer(inputs).dtype, 'float32')\n+    layer = image_preprocessing.RandomFlip(dtype='uint8')\n+    self.assertAllEqual(layer(inputs).dtype, 'uint8')\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomContrastTest(keras_parameterized.TestCase):\n@@ -613,6 +653,14 @@ class RandomContrastTest(keras_parameterized.TestCase):\n         actual_output = layer(inp, training=True)\n         self.assertAllClose(expected_output, actual_output)\n \n+  @testing_utils.run_v2_only\n+  def test_output_dtypes(self):\n+    inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n+    layer = image_preprocessing.RandomContrast((.5, .6))\n+    self.assertAllEqual(layer(inputs).dtype, 'float32')\n+    layer = image_preprocessing.RandomContrast((.5, .6), dtype='uint8')\n+    self.assertAllEqual(layer(inputs).dtype, 'uint8')\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomTranslationTest(keras_parameterized.TestCase):\n@@ -800,6 +848,14 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n       expected_output = np.reshape(expected_output, (5, 5, 1))\n       self.assertAllEqual(expected_output, output_image)\n \n+  @testing_utils.run_v2_only\n+  def test_output_dtypes(self):\n+    inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n+    layer = image_preprocessing.RandomTranslation(.5, .6)\n+    self.assertAllEqual(layer(inputs).dtype, 'float32')\n+    layer = image_preprocessing.RandomTranslation(.5, .6, dtype='uint8')\n+    self.assertAllEqual(layer(inputs).dtype, 'uint8')\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomTransformTest(keras_parameterized.TestCase):\n@@ -1230,6 +1286,14 @@ class RandomRotationTest(keras_parameterized.TestCase):\n       expected_output = np.reshape(expected_output, (5, 5, 1))\n       self.assertAllClose(expected_output, output_image)\n \n+  @testing_utils.run_v2_only\n+  def test_output_dtypes(self):\n+    inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n+    layer = image_preprocessing.RandomRotation(.5)\n+    self.assertAllEqual(layer(inputs).dtype, 'float32')\n+    layer = image_preprocessing.RandomRotation(.5, dtype='uint8')\n+    self.assertAllEqual(layer(inputs).dtype, 'uint8')\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomZoomTest(keras_parameterized.TestCase):\n@@ -1344,6 +1408,14 @@ class RandomZoomTest(keras_parameterized.TestCase):\n       expected_output = np.reshape(expected_output, (5, 5, 1))\n       self.assertAllEqual(expected_output, output_image)\n \n+  @testing_utils.run_v2_only\n+  def test_output_dtypes(self):\n+    inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n+    layer = image_preprocessing.RandomZoom(.5, .5)\n+    self.assertAllEqual(layer(inputs).dtype, 'float32')\n+    layer = image_preprocessing.RandomZoom(.5, .5, dtype='uint8')\n+    self.assertAllEqual(layer(inputs).dtype, 'uint8')\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomHeightTest(keras_parameterized.TestCase):\n@@ -1445,6 +1517,14 @@ class RandomHeightTest(keras_parameterized.TestCase):\n         img_out = layer(img, training=True)\n         self.assertEqual(img_out.shape[0], 3)\n \n+  @testing_utils.run_v2_only\n+  def test_output_dtypes(self):\n+    inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n+    layer = image_preprocessing.RandomHeight(.2)\n+    self.assertAllEqual(layer(inputs).dtype, 'float32')\n+    layer = image_preprocessing.RandomHeight(.2, dtype='uint8')\n+    self.assertAllEqual(layer(inputs).dtype, 'uint8')\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomWidthTest(keras_parameterized.TestCase):\n@@ -1545,6 +1625,14 @@ class RandomWidthTest(keras_parameterized.TestCase):\n         img_out = layer(img, training=True)\n         self.assertEqual(img_out.shape[1], 3)\n \n+  @testing_utils.run_v2_only\n+  def test_output_dtypes(self):\n+    inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n+    layer = image_preprocessing.RandomWidth(.2)\n+    self.assertAllEqual(layer(inputs).dtype, 'float32')\n+    layer = image_preprocessing.RandomWidth(.2, dtype='uint8')\n+    self.assertAllEqual(layer(inputs).dtype, 'uint8')\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class LearningPhaseTest(keras_parameterized.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
