{"custom_id": "scrapy#1e245046ed8ac3d9f89860501c2da95b69aaabf6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 31 | Files Changed: 5 | Hunks: 30 | Methods Changed: 21 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 63 | Churn Cumulative: 4596 | Contributors (this commit): 62 | Commits (past 90d): 43 | Contributors (cumulative): 115 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,6 +6,7 @@ See documentation in docs/topics/feed-exports.rst\n \n import logging\n import os\n+import re\n import sys\n import warnings\n from datetime import datetime\n@@ -25,6 +26,7 @@ from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.python import without_none_values\n \n+\n logger = logging.getLogger(__name__)\n \n \n@@ -245,7 +247,7 @@ class FeedExporter:\n         for uri, feed in self.feeds.items():\n             if not self._storage_supported(uri):\n                 raise NotConfigured\n-            if not self._settings_are_valid(uri):\n+            if not self._settings_are_valid():\n                 raise NotConfigured\n             if not self._exporter_supported(feed['format']):\n                 raise NotConfigured\n@@ -298,7 +300,7 @@ class FeedExporter:\n     def _start_new_batch(self, batch_id, uri, feed, spider, uri_template):\n         \"\"\"\n         Redirect the output data stream to a new file.\n-        Execute multiple times if FEED_STORAGE_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified\n+        Execute multiple times if FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified\n         :param batch_id: sequence number of current batch\n         :param uri: uri of the new batch to start\n         :param feed: dict with parameters of feed\n@@ -334,7 +336,7 @@ class FeedExporter:\n             slot.start_exporting()\n             slot.exporter.export_item(item)\n             slot.itemcount += 1\n-            # create new slot for each slot with itemcount == FEED_STORAGE_BATCH_ITEM_COUNT and close the old one\n+            # create new slot for each slot with itemcount == FEED_EXPORT_BATCH_ITEM_COUNT and close the old one\n             if (\n                 self.feeds[slot.uri_template]['batch_item_count']\n                 and slot.itemcount >= self.feeds[slot.uri_template]['batch_item_count']\n@@ -367,16 +369,18 @@ class FeedExporter:\n             return True\n         logger.error(\"Unknown feed format: %(format)s\", {'format': format})\n \n-    def _settings_are_valid(self, uri):\n+    def _settings_are_valid(self):\n         \"\"\"\n-        If FEED_STORAGE_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified uri has to contain\n+        If FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified uri has to contain\n         %(batch_time)s or %(batch_id)s to distinguish different files of partial output\n         \"\"\"\n         for uri_template, values in self.feeds.items():\n-            if values['batch_item_count'] and not any(s in uri_template for s in ['%(batch_time)s', '%(batch_id)s']):\n+            if values['batch_item_count'] and not re.findall(r'(%\\(batch_time\\)s|(%\\(batch_id\\)0\\d*d))', uri_template):\n                 logger.error(\n-                    '%(batch_time)s or %(batch_id)s must be in uri({}) if FEED_STORAGE_BATCH_ITEM_COUNT setting '\n-                    'or FEEDS.batch_item_count is specified and greater than 0.'.format(uri_template)\n+                    '%(batch_time)s or %(batch_id)0xd must be in uri({}) if FEED_EXPORT_BATCH_ITEM_COUNT setting '\n+                    'or FEEDS.batch_item_count is specified and greater than 0. For more info see:'\n+                    'https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-export-batch-item-count'\n+                    ''.format(uri_template)\n                 )\n                 return False\n         return True\n\n@@ -146,7 +146,7 @@ FEED_STORAGES_BASE = {\n     's3': 'scrapy.extensions.feedexport.S3FeedStorage',\n     'ftp': 'scrapy.extensions.feedexport.FTPFeedStorage',\n }\n-FEED_STORAGE_BATCH_ITEM_COUNT = 0\n+FEED_EXPORT_BATCH_ITEM_COUNT = 0\n FEED_EXPORTERS = {}\n FEED_EXPORTERS_BASE = {\n     'json': 'scrapy.exporters.JsonItemExporter',\n\n@@ -113,12 +113,9 @@ def get_sources(use_closest=True):\n \n def feed_complete_default_values_from_settings(feed, settings):\n     out = feed.copy()\n+    out.setdefault(\"batch_item_count\", settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT'))\n     out.setdefault(\"encoding\", settings[\"FEED_EXPORT_ENCODING\"])\n     out.setdefault(\"fields\", settings.getlist(\"FEED_EXPORT_FIELDS\") or None)\n-    out.setdefault(\n-        \"batch_item_count\",\n-        out.get('batch_item_count', settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT'))\n-    )\n     out.setdefault(\"store_empty\", settings.getbool(\"FEED_STORE_EMPTY\"))\n     out.setdefault(\"uri_params\", settings[\"FEED_URI_PARAMS\"])\n     if settings[\"FEED_EXPORT_INDENT\"] is None:\n\n@@ -1108,7 +1108,7 @@ class FeedExportTest(FeedExportTestBase):\n \n class BatchDeliveriesTest(FeedExportTestBase):\n     __test__ = True\n-    _file_mark = '_%(batch_time)s_#%(batch_id)s_'\n+    _file_mark = '_%(batch_time)s_#%(batch_id)02d_'\n \n     @defer.inlineCallbacks\n     def run_and_export(self, spider_cls, settings):\n@@ -1144,7 +1144,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'jl', self._file_mark): {'format': 'jl'},\n             },\n         })\n-        batch_size = settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n+        batch_size = settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         for batch in data['jl']:\n@@ -1160,7 +1160,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'csv', self._file_mark): {'format': 'csv'},\n             },\n         })\n-        batch_size = settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n+        batch_size = settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n         data = yield self.exported_data(items, settings)\n         for batch in data['csv']:\n             got_batch = csv.DictReader(to_unicode(batch).splitlines())\n@@ -1176,7 +1176,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'xml', self._file_mark): {'format': 'xml'},\n             },\n         })\n-        batch_size = settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n+        batch_size = settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         for batch in data['xml']:\n@@ -1194,7 +1194,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'json', self._file_mark): {'format': 'json'},\n             },\n         })\n-        batch_size = settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n+        batch_size = settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         # XML\n@@ -1219,7 +1219,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'pickle', self._file_mark): {'format': 'pickle'},\n             },\n         })\n-        batch_size = settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n+        batch_size = settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         import pickle\n@@ -1236,7 +1236,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'marshal', self._file_mark): {'format': 'marshal'},\n             },\n         })\n-        batch_size = settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n+        batch_size = settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         import marshal\n@@ -1259,18 +1259,18 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             {'foo': 'bar3', 'baz': 'quux3', 'egg': ''}\n         ]\n         settings = {\n-            'FEED_STORAGE_BATCH_ITEM_COUNT': 2\n+            'FEED_EXPORT_BATCH_ITEM_COUNT': 2\n         }\n         header = self.MyItem.fields.keys()\n         yield self.assertExported(items, header, rows, settings=Settings(settings))\n \n     def test_wrong_path(self):\n-        \"\"\" If path is without %(batch_time)s and %(batch_id)s an exception must be raised \"\"\"\n+        \"\"\" If path is without %(batch_time)s and %(batch_id)0xd an exception must be raised \"\"\"\n         settings = {\n             'FEEDS': {\n                 self._random_temp_filename(): {'format': 'xml'},\n             },\n-            'FEED_STORAGE_BATCH_ITEM_COUNT': 1\n+            'FEED_EXPORT_BATCH_ITEM_COUNT': 1\n         }\n         crawler = get_crawler(settings_dict=settings)\n         self.assertRaises(NotConfigured, FeedExporter, crawler)\n@@ -1282,7 +1282,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 'FEEDS': {\n                     os.path.join(self._random_temp_filename(), fmt, self._file_mark): {'format': fmt},\n                 },\n-                'FEED_STORAGE_BATCH_ITEM_COUNT': 1\n+                'FEED_EXPORT_BATCH_ITEM_COUNT': 1\n             }\n             data = yield self.exported_no_data(settings)\n             data = dict(data)\n@@ -1304,7 +1304,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 },\n                 'FEED_STORE_EMPTY': True,\n                 'FEED_EXPORT_INDENT': None,\n-                'FEED_STORAGE_BATCH_ITEM_COUNT': 1,\n+                'FEED_EXPORT_BATCH_ITEM_COUNT': 1,\n             }\n             data = yield self.exported_no_data(settings)\n             data = dict(data)\n@@ -1352,7 +1352,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                     'encoding': 'utf-8',\n                 },\n             },\n-            'FEED_STORAGE_BATCH_ITEM_COUNT': 1,\n+            'FEED_EXPORT_BATCH_ITEM_COUNT': 1,\n         }\n         data = yield self.exported_data(items, settings)\n         for fmt, expected in formats.items():\n@@ -1398,7 +1398,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                     'format': 'json',\n                 },\n             },\n-            'FEED_STORAGE_BATCH_ITEM_COUNT': 1,\n+            'FEED_EXPORT_BATCH_ITEM_COUNT': 1,\n         }\n         data = yield self.exported_data(items, settings)\n         self.assertEqual(len(items) + 1, len(data['json']))\n@@ -1440,7 +1440,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                     'format': 'json',\n                 },\n             },\n-            'FEED_STORAGE_BATCH_ITEM_COUNT': 1,\n+            'FEED_EXPORT_BATCH_ITEM_COUNT': 1,\n         })\n         items = [\n             self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n@@ -1458,7 +1458,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n \n         s3 = boto3.resource('s3')\n         my_bucket = s3.Bucket(s3_test_bucket_name)\n-        batch_size = settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n+        batch_size = settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n \n         with MockServer() as s:\n             runner = CrawlerRunner(Settings(settings))\n\n@@ -149,7 +149,7 @@ class FeedExportConfigTestCase(unittest.TestCase):\n             \"FEED_EXPORT_INDENT\": 42,\n             \"FEED_STORE_EMPTY\": True,\n             \"FEED_URI_PARAMS\": (1, 2, 3, 4),\n-            \"FEED_STORAGE_BATCH_ITEM_COUNT\": 2,\n+            \"FEED_EXPORT_BATCH_ITEM_COUNT\": 2,\n         })\n         new_feed = feed_complete_default_values_from_settings(feed, settings)\n         self.assertEqual(new_feed, {\n@@ -171,7 +171,7 @@ class FeedExportConfigTestCase(unittest.TestCase):\n             \"FEED_EXPORT_FIELDS\": [\"f1\", \"f2\", \"f3\"],\n             \"FEED_EXPORT_INDENT\": 42,\n             \"FEED_STORE_EMPTY\": True,\n-            \"FEED_STORAGE_BATCH_ITEM_COUNT\": 2,\n+            \"FEED_EXPORT_BATCH_ITEM_COUNT\": 2,\n         })\n         new_feed = feed_complete_default_values_from_settings(feed, settings)\n         self.assertEqual(new_feed, {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3199048520ebe3798c14cfa7362612b51d156b55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 65 | Lines Deleted: 14 | Files Changed: 8 | Hunks: 22 | Methods Changed: 9 | Complexity Δ (Sum/Max): -54/7 | Churn Δ: 79 | Churn Cumulative: 5531 | Contributors (this commit): 33 | Commits (past 90d): 38 | Contributors (cumulative): 82 | DMM Complexity: 0.8888888888888888\n\nDIFF:\n@@ -1,7 +1,9 @@\n from urllib.parse import urlparse\n \n from twisted.internet import reactor\n-from twisted.names.client import createResolver\n+from twisted.names import cache, hosts as hostsModule, resolve\n+from twisted.names.client import Resolver\n+from twisted.python.runtime import platform\n \n from scrapy import Spider, Request\n from scrapy.crawler import CrawlerRunner\n@@ -10,6 +12,17 @@ from scrapy.utils.log import configure_logging\n from tests.mockserver import MockServer, MockDNSServer\n \n \n+# https://stackoverflow.com/a/32784190\n+def createResolver(servers=None, resolvconf=None, hosts=None):\n+    if hosts is None:\n+        hosts = (b'/etc/hosts' if platform.getType() == 'posix'\n+                 else r'c:\\windows\\hosts')\n+    theResolver = Resolver(resolvconf, servers)\n+    hostResolver = hostsModule.Resolver(hosts)\n+    L = [hostResolver, cache.CacheResolver(), theResolver]\n+    return resolve.ResolverChain(L)\n+\n+\n class LocalhostSpider(Spider):\n     name = \"localhost_spider\"\n \n\n@@ -247,9 +247,8 @@ class MockDNSServer:\n     def __enter__(self):\n         self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver', '-t', 'dns'],\n                           stdout=PIPE, env=get_testenv())\n-        host, port = self.proc.stdout.readline().strip().decode('ascii').split(\":\")\n-        self.host = host\n-        self.port = int(port)\n+        self.host = '127.0.0.1'\n+        self.port = int(self.proc.stdout.readline().strip().decode('ascii').split(\":\")[1])\n         return self\n \n     def __exit__(self, exc_type, exc_value, traceback):\n\n@@ -2,6 +2,7 @@ import inspect\n import json\n import optparse\n import os\n+import platform\n import subprocess\n import sys\n import tempfile\n@@ -10,6 +11,7 @@ from os.path import exists, join, abspath\n from shutil import rmtree, copytree\n from tempfile import mkdtemp\n from threading import Timer\n+from unittest import skipIf\n \n from twisted.trial import unittest\n \n@@ -319,6 +321,9 @@ class BadSpider(scrapy.Spider):\n         self.assertIn(\"start_requests\", log)\n         self.assertIn(\"badspider.py\", log)\n \n+    # https://twistedmatrix.com/trac/ticket/9766\n+    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n+            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n     def test_asyncio_enabled_true(self):\n         log = self.get_log(self.debug_log_spider, args=[\n             '-s', 'TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor'\n\n@@ -4,6 +4,7 @@ import platform\n import subprocess\n import sys\n import warnings\n+from unittest import skipIf\n \n from pytest import raises, mark\n from testfixtures import LogCapture\n@@ -252,6 +253,9 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n                 })\n \n     @defer.inlineCallbacks\n+    # https://twistedmatrix.com/trac/ticket/9766\n+    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n+            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n     def test_crawler_process_asyncio_enabled_true(self):\n         with LogCapture(level=logging.DEBUG) as log:\n             if self.reactor_pytest == 'asyncio':\n@@ -293,11 +297,17 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn('Spider closed (finished)', log)\n         self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n+    # https://twistedmatrix.com/trac/ticket/9766\n+    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n+            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n     def test_asyncio_enabled_no_reactor(self):\n         log = self.run_script('asyncio_enabled_no_reactor.py')\n         self.assertIn('Spider closed (finished)', log)\n         self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n+    # https://twistedmatrix.com/trac/ticket/9766\n+    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n+            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n     def test_asyncio_enabled_reactor(self):\n         log = self.run_script('asyncio_enabled_reactor.py')\n         self.assertIn('Spider closed (finished)', log)\n@@ -327,6 +337,9 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn(\"Spider closed (finished)\", log)\n         self.assertIn(\"Using reactor: twisted.internet.pollreactor.PollReactor\", log)\n \n+    # https://twistedmatrix.com/trac/ticket/9766\n+    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n+            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n     def test_reactor_asyncio(self):\n         log = self.run_script(\"twisted_reactor_asyncio.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n\n@@ -455,9 +455,15 @@ class FeedExportTest(unittest.TestCase):\n     def run_and_export(self, spider_cls, settings):\n         \"\"\" Run spider with specified settings; return exported data. \"\"\"\n \n+        def path_to_url(path):\n+            return urljoin('file:', pathname2url(str(path)))\n+\n+        def printf_escape(string):\n+            return string.replace('%', '%%')\n+\n         FEEDS = settings.get('FEEDS') or {}\n         settings['FEEDS'] = {\n-            urljoin('file:', pathname2url(str(file_path))): feed\n+            printf_escape(path_to_url(file_path)): feed\n             for file_path, feed in FEEDS.items()\n         }\n \n\n@@ -1,5 +1,6 @@\n import json\n import os\n+import platform\n import re\n import sys\n from subprocess import Popen, PIPE\n@@ -59,6 +60,8 @@ def _wrong_credentials(proxy_url):\n \n @skipIf(sys.version_info < (3, 5, 4),\n         \"requires mitmproxy < 3.0.0, which these tests do not support\")\n+@skipIf(platform.system() == 'Windows' and sys.version_info < (3, 7),\n+        \"mitmproxy does not support Windows when running Python < 3.7\")\n class ProxyConnectTestCase(TestCase):\n \n     def setUp(self):\n\n@@ -20,13 +20,20 @@ from scrapy.crawler import CrawlerRunner\n module_dir = os.path.dirname(os.path.abspath(__file__))\n \n \n+def _copytree(source, target):\n+    try:\n+        shutil.copytree(source, target)\n+    except shutil.Error:\n+        pass\n+\n+\n class SpiderLoaderTest(unittest.TestCase):\n \n     def setUp(self):\n         orig_spiders_dir = os.path.join(module_dir, 'test_spiders')\n         self.tmpdir = tempfile.mkdtemp()\n         self.spiders_dir = os.path.join(self.tmpdir, 'test_spiders_xxx')\n-        shutil.copytree(orig_spiders_dir, self.spiders_dir)\n+        _copytree(orig_spiders_dir, self.spiders_dir)\n         sys.path.append(self.tmpdir)\n         settings = Settings({'SPIDER_MODULES': ['test_spiders_xxx']})\n         self.spider_loader = SpiderLoader.from_settings(settings)\n@@ -124,7 +131,7 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n         self.tmpdir = self.mktemp()\n         os.mkdir(self.tmpdir)\n         self.spiders_dir = os.path.join(self.tmpdir, 'test_spiders_xxx')\n-        shutil.copytree(orig_spiders_dir, self.spiders_dir)\n+        _copytree(orig_spiders_dir, self.spiders_dir)\n         sys.path.append(self.tmpdir)\n         self.settings = Settings({'SPIDER_MODULES': ['test_spiders_xxx']})\n \n@@ -134,8 +141,8 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n \n     def test_dupename_warning(self):\n         # copy 1 spider module so as to have duplicate spider name\n-        shutil.copyfile(os.path.join(self.tmpdir, 'test_spiders_xxx/spider3.py'),\n-                        os.path.join(self.tmpdir, 'test_spiders_xxx/spider3dupe.py'))\n+        shutil.copyfile(os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider3.py'),\n+                        os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider3dupe.py'))\n \n         with warnings.catch_warnings(record=True) as w:\n             spider_loader = SpiderLoader.from_settings(self.settings)\n@@ -156,10 +163,10 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n     def test_multiple_dupename_warning(self):\n         # copy 2 spider modules so as to have duplicate spider name\n         # This should issue 2 warning, 1 for each duplicate spider name\n-        shutil.copyfile(os.path.join(self.tmpdir, 'test_spiders_xxx/spider1.py'),\n-                        os.path.join(self.tmpdir, 'test_spiders_xxx/spider1dupe.py'))\n-        shutil.copyfile(os.path.join(self.tmpdir, 'test_spiders_xxx/spider2.py'),\n-                        os.path.join(self.tmpdir, 'test_spiders_xxx/spider2dupe.py'))\n+        shutil.copyfile(os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider1.py'),\n+                        os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider1dupe.py'))\n+        shutil.copyfile(os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider2.py'),\n+                        os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider2dupe.py'))\n \n         with warnings.catch_warnings(record=True) as w:\n             spider_loader = SpiderLoader.from_settings(self.settings)\n\n@@ -1,4 +1,6 @@\n-from unittest import TestCase\n+import platform\n+import sys\n+from unittest import skipIf, TestCase\n \n from pytest import mark\n \n@@ -12,6 +14,9 @@ class AsyncioTest(TestCase):\n         # the result should depend only on the pytest --reactor argument\n         self.assertEqual(is_asyncio_reactor_installed(), self.reactor_pytest == 'asyncio')\n \n+    # https://twistedmatrix.com/trac/ticket/9766\n+    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n+            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n     def test_install_asyncio_reactor(self):\n         # this should do nothing\n         install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb937742566105f3525a9f76e4ae68cc18e9fd8d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 14 | Churn Cumulative: 95 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,10 @@\n import unittest\n from io import StringIO\n+from time import sleep, time\n from unittest import mock\n \n+from twisted.trial.unittest import SkipTest\n+\n from scrapy.utils import trackref\n \n \n@@ -55,7 +58,18 @@ Foo                                 1   oldest: 0s ago\\n\\n''')\n \n     def test_get_oldest(self):\n         o1 = Foo()  # NOQA\n+\n+        o1_time = time()\n+\n         o2 = Bar()  # NOQA\n+\n+        o3_time = time()\n+        if o3_time <= o1_time:\n+            sleep(0.01)\n+            o3_time = time()\n+        if o3_time <= o1_time:\n+            raise SkipTest('time.time is not precise enough')\n+\n         o3 = Foo()  # NOQA\n         self.assertIs(trackref.get_oldest('Foo'), o1)\n         self.assertIs(trackref.get_oldest('Bar'), o2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6454d456d2bcab0828aba6d81d98f7393ab7e04d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 66/66 | Churn Δ: 6 | Churn Cumulative: 3381 | Contributors (this commit): 27 | Commits (past 90d): 37 | Contributors (cumulative): 45 | DMM Complexity: None\n\nDIFF:\n@@ -375,9 +375,9 @@ class FeedExporter:\n         %(batch_time)s or %(batch_id)s to distinguish different files of partial output\n         \"\"\"\n         for uri_template, values in self.feeds.items():\n-            if values['batch_item_count'] and not re.findall(r'(%\\(batch_time\\)s|(%\\(batch_id\\)0\\d*d))', uri_template):\n+            if values['batch_item_count'] and not re.search(r'%\\(batch_time\\)s|%\\(batch_id\\)', uri_template):\n                 logger.error(\n-                    '%(batch_time)s or %(batch_id)0xd must be in uri({}) if FEED_EXPORT_BATCH_ITEM_COUNT setting '\n+                    '%(batch_time)s or %(batch_id) must be in uri({}) if FEED_EXPORT_BATCH_ITEM_COUNT setting '\n                     'or FEEDS.batch_item_count is specified and greater than 0. For more info see:'\n                     'https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-export-batch-item-count'\n                     ''.format(uri_template)\n\n@@ -1265,7 +1265,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         yield self.assertExported(items, header, rows, settings=Settings(settings))\n \n     def test_wrong_path(self):\n-        \"\"\" If path is without %(batch_time)s and %(batch_id)0xd an exception must be raised \"\"\"\n+        \"\"\" If path is without %(batch_time)s and %(batch_id) an exception must be raised \"\"\"\n         settings = {\n             'FEEDS': {\n                 self._random_temp_filename(): {'format': 'xml'},\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a94b30342a451b94e7f358f68ce1b1adc20723f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1969 | Contributors (this commit): 1 | Commits (past 90d): 29 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -243,7 +243,6 @@ class Stream:\n         bytes_to_send_size = min(window_size, self.remaining_content_length)\n \n         # We now need to send a number of data frames.\n-        data_frames_sent = 0\n         while bytes_to_send_size > 0:\n             chunk_size = min(bytes_to_send_size, max_frame_size)\n \n@@ -252,7 +251,6 @@ class Stream:\n \n             self._conn.send_data(self.stream_id, data_chunk, end_stream=False)\n \n-            data_frames_sent += 1\n             bytes_to_send_size = bytes_to_send_size - chunk_size\n             self.remaining_content_length = self.remaining_content_length - chunk_size\n \n\n@@ -42,8 +42,8 @@ def make_html_body(val):\n \n \n class Data:\n-    SMALL_SIZE = 1024 * 10  # 10 KB\n-    LARGE_SIZE = (1024 ** 2) * 10  # 10 MB\n+    SMALL_SIZE = 1024  # 1 KB\n+    LARGE_SIZE = 1024 ** 2  # 1 MB\n \n     STR_SMALL = generate_random_string(SMALL_SIZE)\n     STR_LARGE = generate_random_string(LARGE_SIZE)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7f5bb6b34c6a5137aa12cd4ad4f3845a8c67653f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 985 | Contributors (this commit): 27 | Commits (past 90d): 12 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -52,6 +52,9 @@ DEFAULT_LOGGING = {\n         'twisted': {\n             'level': 'ERROR',\n         },\n+        'hpack': {\n+            'level': 'ERROR',\n+        },\n     }\n }\n \n\n@@ -81,7 +81,8 @@ setup(\n         'zope.interface>=4.1.3',\n         'protego>=0.1.15',\n         'itemadapter>=0.1.0',\n-        'typing_extensions>=3.7'\n+        'typing_extensions>=3.7',\n+        'h2>=3.2.0'\n     ],\n     extras_require=extras_require,\n )\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ec06cf79a6a7264ec3e32d7de8c4e305c2afa05e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 56 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,8 +15,7 @@ from tests.mockserver import MockServer, MockDNSServer\n # https://stackoverflow.com/a/32784190\n def createResolver(servers=None, resolvconf=None, hosts=None):\n     if hosts is None:\n-        hosts = (b'/etc/hosts' if platform.getType() == 'posix'\n-                 else r'c:\\windows\\hosts')\n+        hosts = b'/etc/hosts' if platform.getType() == 'posix' else r'c:\\windows\\hosts'\n     theResolver = Resolver(resolvconf, servers)\n     hostResolver = hostsModule.Resolver(hosts)\n     L = [hostResolver, cache.CacheResolver(), theResolver]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#17aec5944cab33b3cdcd497d2362cacbf7773e47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 60 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -18,8 +18,8 @@ def createResolver(servers=None, resolvconf=None, hosts=None):\n         hosts = b'/etc/hosts' if platform.getType() == 'posix' else r'c:\\windows\\hosts'\n     theResolver = Resolver(resolvconf, servers)\n     hostResolver = hostsModule.Resolver(hosts)\n-    L = [hostResolver, cache.CacheResolver(), theResolver]\n-    return resolve.ResolverChain(L)\n+    chain = [hostResolver, cache.CacheResolver(), theResolver]\n+    return resolve.ResolverChain(chain)\n \n \n class LocalhostSpider(Spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f1020e0e6af064ab31b812c25bda6b0f08827222", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 873 | Contributors (this commit): 23 | Commits (past 90d): 18 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -305,7 +305,7 @@ class FeedExporter:\n         :param uri: uri of the new batch to start\n         :param feed: dict with parameters of feed\n         :param spider: user spider\n-        :param uri_template: template of uri which contains %(batch_time)s or %(batch_id)s to create new uri\n+        :param uri_template: template of uri which contains %(batch_time)s or %(batch_id)d to create new uri\n         \"\"\"\n         storage = self._get_storage(uri)\n         file = storage.open(spider)\n@@ -372,13 +372,13 @@ class FeedExporter:\n     def _settings_are_valid(self):\n         \"\"\"\n         If FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified uri has to contain\n-        %(batch_time)s or %(batch_id)s to distinguish different files of partial output\n+        %(batch_time)s or %(batch_id)d to distinguish different files of partial output\n         \"\"\"\n         for uri_template, values in self.feeds.items():\n             if values['batch_item_count'] and not re.search(r'%\\(batch_time\\)s|%\\(batch_id\\)', uri_template):\n                 logger.error(\n-                    '%(batch_time)s or %(batch_id) must be in uri({}) if FEED_EXPORT_BATCH_ITEM_COUNT setting '\n-                    'or FEEDS.batch_item_count is specified and greater than 0. For more info see:'\n+                    '%(batch_time)s or %(batch_id)d must be in the feed URI ({}) if FEED_EXPORT_BATCH_ITEM_COUNT '\n+                    'setting or FEEDS.batch_item_count is specified and greater than 0. For more info see: '\n                     'https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-export-batch-item-count'\n                     ''.format(uri_template)\n                 )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#54e4228c3a22164b79db36a29a5e2d64391b592a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 93 | Lines Deleted: 102 | Files Changed: 2 | Hunks: 74 | Methods Changed: 41 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 195 | Churn Cumulative: 2192 | Contributors (this commit): 2 | Commits (past 90d): 39 | Contributors (cumulative): 4 | DMM Complexity: 0.8392857142857143\n\nDIFF:\n@@ -11,32 +11,34 @@ from h2.events import (\n     StreamEnded, StreamReset, WindowUpdated\n )\n from h2.exceptions import ProtocolError\n+from twisted.internet.defer import Deferred\n from twisted.internet.protocol import connectionDone, Protocol\n from twisted.internet.ssl import Certificate\n+from twisted.python.failure import Failure\n \n from scrapy.core.http2.stream import Stream, StreamCloseReason\n from scrapy.core.http2.types import H2ConnectionMetadataDict\n from scrapy.http import Request\n \n+\n logger = logging.getLogger(__name__)\n \n \n class H2ClientProtocol(Protocol):\n-    def __init__(self):\n+    def __init__(self) -> None:\n         config = H2Configuration(client_side=True, header_encoding='utf-8')\n         self.conn = H2Connection(config=config)\n \n         # ID of the next request stream\n-        # Following the convention made by hyper-h2 each client ID\n-        # will be odd.\n-        self.stream_id_count = itertools.count(start=1, step=2)\n+        # Following the convention made by hyper-h2 all IDs will be odd\n+        self._stream_id_generator = itertools.count(start=1, step=2)\n \n         # Streams are stored in a dictionary keyed off their stream IDs\n         self.streams: Dict[int, Stream] = {}\n \n         # If requests are received before connection is made we keep\n         # all requests in a pool and send them as the connection is made\n-        self._pending_request_stream_pool = deque()\n+        self._pending_request_stream_pool: deque = deque()\n \n         # Counter to keep track of opened stream. This counter\n         # is used to make sure that not more than MAX_CONCURRENT_STREAMS\n@@ -48,15 +50,15 @@ class H2ClientProtocol(Protocol):\n         # We pass this instance to the streams ResponseFailed() failure\n         self._protocol_error: Optional[ProtocolError] = None\n \n-        self._metadata: H2ConnectionMetadataDict = {\n+        self.metadata: H2ConnectionMetadataDict = {\n             'certificate': None,\n             'ip_address': None,\n             'hostname': None,\n-            'port': None\n+            'port': None,\n         }\n \n     @property\n-    def is_connected(self):\n+    def is_connected(self) -> bool:\n         \"\"\"Boolean to keep track of the connection status.\n         This is used while initiating pending streams to make sure\n         that we initiate stream only during active HTTP/2 Connection\n@@ -75,7 +77,7 @@ class H2ClientProtocol(Protocol):\n             self.conn.remote_settings.max_concurrent_streams\n         )\n \n-    def _send_pending_requests(self):\n+    def _send_pending_requests(self) -> None:\n         \"\"\"Initiate all pending requests from the deque following FIFO\n         We make sure that at any time {allowed_max_concurrent_streams}\n         streams are active.\n@@ -89,37 +91,33 @@ class H2ClientProtocol(Protocol):\n             stream = self._pending_request_stream_pool.popleft()\n             stream.initiate_request()\n \n-    def _stream_close_cb(self, stream_id: int):\n-        \"\"\"Called when stream is closed completely\n+    def pop_stream(self, stream_id: int) -> Stream:\n+        \"\"\"Perform cleanup when a stream is closed\n         \"\"\"\n-        self.streams.pop(stream_id)\n+        stream = self.streams.pop(stream_id)\n         self._active_streams -= 1\n         self._send_pending_requests()\n+        return stream\n \n-    def _new_stream(self, request: Request):\n+    def _new_stream(self, request: Request) -> Stream:\n         \"\"\"Instantiates a new Stream object\n         \"\"\"\n-        stream_id = next(self.stream_id_count)\n-\n         stream = Stream(\n-            stream_id=stream_id,\n+            stream_id=next(self._stream_id_generator),\n             request=request,\n-            connection=self.conn,\n-            conn_metadata=self._metadata,\n-            cb_close=self._stream_close_cb\n+            protocol=self,\n         )\n-\n         self.streams[stream.stream_id] = stream\n         return stream\n \n-    def _write_to_transport(self):\n+    def _write_to_transport(self) -> None:\n         \"\"\" Write data to the underlying transport connection\n         from the HTTP2 connection instance if any\n         \"\"\"\n         data = self.conn.data_to_send()\n         self.transport.write(data)\n \n-    def request(self, request: Request):\n+    def request(self, request: Request) -> Deferred:\n         if not isinstance(request, Request):\n             raise TypeError(f'Expected scrapy.http.Request, received {request.__class__.__qualname__}')\n \n@@ -130,20 +128,20 @@ class H2ClientProtocol(Protocol):\n         self._pending_request_stream_pool.append(stream)\n         return d\n \n-    def connectionMade(self):\n+    def connectionMade(self) -> None:\n         \"\"\"Called by Twisted when the connection is established. We can start\n         sending some data now: we should open with the connection preamble.\n         \"\"\"\n         destination = self.transport.getPeer()\n         logger.debug('Connection made to {}'.format(destination))\n-        self._metadata['ip_address'] = ipaddress.ip_address(destination.host)\n-        self._metadata['port'] = destination.port\n-        self._metadata['hostname'] = self.transport.transport.addr[0]\n+        self.metadata['ip_address'] = ipaddress.ip_address(destination.host)\n+        self.metadata['port'] = destination.port\n+        self.metadata['hostname'] = self.transport.transport.addr[0]\n \n         self.conn.initiate_connection()\n         self._write_to_transport()\n \n-    def dataReceived(self, data):\n+    def dataReceived(self, data: bytes) -> None:\n         try:\n             events = self.conn.receive_data(data)\n             self._handle_events(events)\n@@ -158,32 +156,30 @@ class H2ClientProtocol(Protocol):\n         finally:\n             self._write_to_transport()\n \n-    def connectionLost(self, reason=connectionDone):\n+    def connectionLost(self, reason: Failure = connectionDone) -> None:\n         \"\"\"Called by Twisted when the transport connection is lost.\n         No need to write anything to transport here.\n         \"\"\"\n-        # Pop all streams which were pending and were not yet started\n-        # NOTE: Stream.close() pops the element from the streams dictionary\n-        # which raises `RuntimeError: dictionary changed size during iteration`\n-        # Hence, we copy the streams into a list.\n-        for stream in list(self.streams.values()):\n+        for stream in self.streams.values():\n             if stream.request_sent:\n-                stream.close(StreamCloseReason.CONNECTION_LOST, self._protocol_error)\n+                stream.close(StreamCloseReason.CONNECTION_LOST, self._protocol_error, from_protocol=True)\n             else:\n-                stream.close(StreamCloseReason.INACTIVE)\n+                stream.close(StreamCloseReason.INACTIVE, from_protocol=True)\n \n+        self._active_streams -= len(self.streams)\n+        self.streams.clear()\n+        self._send_pending_requests()\n         self.conn.close_connection()\n \n         if not reason.check(connectionDone):\n             logger.warning(\"Connection lost with reason \" + str(reason))\n \n-    def _handle_events(self, events):\n+    def _handle_events(self, events: list) -> None:\n         \"\"\"Private method which acts as a bridge between the events\n         received from the HTTP/2 data and IH2EventsHandler\n \n         Arguments:\n-            events {list} -- A list of events that the remote peer\n-                triggered by sending data\n+            events -- A list of events that the remote peer triggered by sending data\n         \"\"\"\n         for event in events:\n             if isinstance(event, DataReceived):\n@@ -202,27 +198,29 @@ class H2ClientProtocol(Protocol):\n                 logger.debug('Received unhandled event {}'.format(event))\n \n     # Event handler functions starts here\n-    def data_received(self, event: DataReceived):\n+    def data_received(self, event: DataReceived) -> None:\n         self.streams[event.stream_id].receive_data(event.data, event.flow_controlled_length)\n \n-    def response_received(self, event: ResponseReceived):\n+    def response_received(self, event: ResponseReceived) -> None:\n         self.streams[event.stream_id].receive_headers(event.headers)\n \n-    def settings_acknowledged(self, event: SettingsAcknowledged):\n+    def settings_acknowledged(self, event: SettingsAcknowledged) -> None:\n         # Send off all the pending requests as now we have\n         # established a proper HTTP/2 connection\n         self._send_pending_requests()\n \n         # Update certificate when our HTTP/2 connection is established\n-        self._metadata['certificate'] = Certificate(self.transport.getPeerCertificate())\n+        self.metadata['certificate'] = Certificate(self.transport.getPeerCertificate())\n \n-    def stream_ended(self, event: StreamEnded):\n-        self.streams[event.stream_id].close(StreamCloseReason.ENDED)\n+    def stream_ended(self, event: StreamEnded) -> None:\n+        stream = self.pop_stream(event.stream_id)\n+        stream.close(StreamCloseReason.ENDED, from_protocol=True)\n \n-    def stream_reset(self, event: StreamReset):\n-        self.streams[event.stream_id].close(StreamCloseReason.RESET)\n+    def stream_reset(self, event: StreamReset) -> None:\n+        stream = self.pop_stream(event.stream_id)\n+        stream.close(StreamCloseReason.RESET, from_protocol=True)\n \n-    def window_updated(self, event: WindowUpdated):\n+    def window_updated(self, event: WindowUpdated) -> None:\n         if event.stream_id != 0:\n             self.streams[event.stream_id].receive_window_update()\n         else:\n\n@@ -1,22 +1,27 @@\n import logging\n from enum import Enum\n from io import BytesIO\n-from typing import Callable, List\n+from typing import List, Optional, Tuple, TYPE_CHECKING\n from urllib.parse import urlparse\n \n-from h2.connection import H2Connection\n from h2.errors import ErrorCodes\n from h2.exceptions import StreamClosedError\n+from hpack import HeaderTuple\n from twisted.internet.defer import Deferred, CancelledError\n from twisted.internet.error import ConnectionClosed\n from twisted.python.failure import Failure\n from twisted.web.client import ResponseFailed\n \n-from scrapy.core.http2.types import H2ConnectionMetadataDict, H2ResponseDict\n+from scrapy.core.http2.types import H2ResponseDict\n from scrapy.http import Request\n from scrapy.http.headers import Headers\n from scrapy.responsetypes import responsetypes\n \n+\n+if TYPE_CHECKING:\n+    from scrapy.core.http2.protocol import H2ClientProtocol\n+\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -31,12 +36,12 @@ class InactiveStreamClosed(ConnectionClosed):\n \n class InvalidHostname(Exception):\n \n-    def __init__(self, request: Request, expected_hostname, expected_netloc):\n+    def __init__(self, request: Request, expected_hostname: Optional[str], expected_netloc: Optional[str]) -> None:\n         self.request = request\n         self.expected_hostname = expected_hostname\n         self.expected_netloc = expected_netloc\n \n-    def __str__(self):\n+    def __str__(self) -> str:\n         return f'InvalidHostname: Expected {self.expected_hostname} or {self.expected_netloc} in {self.request}'\n \n \n@@ -80,28 +85,20 @@ class Stream:\n         self,\n         stream_id: int,\n         request: Request,\n-        connection: H2Connection,\n-        conn_metadata: H2ConnectionMetadataDict,\n-        cb_close: Callable[[int], None],\n+        protocol: \"H2ClientProtocol\",\n         download_maxsize: int = 0,\n         download_warnsize: int = 0,\n         fail_on_data_loss: bool = True\n-    ):\n+    ) -> None:\n         \"\"\"\n         Arguments:\n-            stream_id -- For one HTTP/2 connection each stream is\n-                uniquely identified by a single integer\n-            request -- HTTP request\n-            connection -- HTTP/2 connection this stream belongs to.\n-            conn_metadata -- Reference to dictionary having metadata of HTTP/2 connection\n-            cb_close -- Method called when this stream is closed\n-                to notify the TCP connection instance.\n+            stream_id -- Unique identifier for the stream within a single HTTP/2 connection\n+            request -- The HTTP request associated to the stream\n+            protocol -- Parent H2ClientProtocol instance\n         \"\"\"\n-        self.stream_id = stream_id\n-        self._request = request\n-        self._conn = connection\n-        self._conn_metadata = conn_metadata\n-        self._cb_close = cb_close\n+        self.stream_id: int = stream_id\n+        self._request: Request = request\n+        self._protocol: \"H2ClientProtocol\" = protocol\n \n         self._download_maxsize = self._request.meta.get('download_maxsize', download_maxsize)\n         self._download_warnsize = self._request.meta.get('download_warnsize', download_warnsize)\n@@ -132,7 +129,7 @@ class Stream:\n         self._response: H2ResponseDict = {\n             'body': BytesIO(),\n             'flow_controlled_size': 0,\n-            'headers': Headers({})\n+            'headers': Headers({}),\n         }\n \n         def _cancel(_):\n@@ -145,7 +142,7 @@ class Stream:\n \n         self._deferred_response = Deferred(_cancel)\n \n-    def __str__(self):\n+    def __str__(self) -> str:\n         return f'Stream(id={self.stream_id!r})'\n \n     __repr__ = __str__\n@@ -169,12 +166,9 @@ class Stream:\n             and not self._reached_warnsize\n         )\n \n-    def get_response(self):\n+    def get_response(self) -> Deferred:\n         \"\"\"Simply return a Deferred which fires when response\n         from the asynchronous request is available\n-\n-        Returns:\n-            Deferred -- Calls the callback passing the response\n         \"\"\"\n         return self._deferred_response\n \n@@ -182,12 +176,12 @@ class Stream:\n         # Make sure that we are sending the request to the correct URL\n         url = urlparse(self._request.url)\n         return (\n-            url.netloc == self._conn_metadata['hostname']\n-            or url.netloc == f'{self._conn_metadata[\"hostname\"]}:{self._conn_metadata[\"port\"]}'\n-            or url.netloc == f'{self._conn_metadata[\"ip_address\"]}:{self._conn_metadata[\"port\"]}'\n+            url.netloc == self._protocol.metadata['hostname']\n+            or url.netloc == f'{self._protocol.metadata[\"hostname\"]}:{self._protocol.metadata[\"port\"]}'\n+            or url.netloc == f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"port\"]}'\n         )\n \n-    def _get_request_headers(self):\n+    def _get_request_headers(self) -> List[Tuple[str, str]]:\n         url = urlparse(self._request.url)\n \n         path = url.path\n@@ -207,10 +201,10 @@ class Stream:\n \n         return headers\n \n-    def initiate_request(self):\n+    def initiate_request(self) -> None:\n         if self.check_request_url():\n             headers = self._get_request_headers()\n-            self._conn.send_headers(self.stream_id, headers, end_stream=False)\n+            self._protocol.conn.send_headers(self.stream_id, headers, end_stream=False)\n             self.request_sent = True\n             self.send_data()\n         else:\n@@ -218,7 +212,7 @@ class Stream:\n             # Note that we have not sent any headers\n             self.close(StreamCloseReason.INVALID_HOSTNAME)\n \n-    def send_data(self):\n+    def send_data(self) -> None:\n         \"\"\"Called immediately after the headers are sent. Here we send all the\n          data as part of the request.\n \n@@ -233,10 +227,10 @@ class Stream:\n             raise StreamClosedError(self.stream_id)\n \n         # Firstly, check what the flow control window is for current stream.\n-        window_size = self._conn.local_flow_control_window(stream_id=self.stream_id)\n+        window_size = self._protocol.conn.local_flow_control_window(stream_id=self.stream_id)\n \n         # Next, check what the maximum frame size is.\n-        max_frame_size = self._conn.max_outbound_frame_size\n+        max_frame_size = self._protocol.conn.max_outbound_frame_size\n \n         # We will send no more than the window size or the remaining file size\n         # of data in this call, whichever is smaller.\n@@ -249,7 +243,7 @@ class Stream:\n             data_chunk_start_id = self.content_length - self.remaining_content_length\n             data_chunk = self._request.body[data_chunk_start_id:data_chunk_start_id + chunk_size]\n \n-            self._conn.send_data(self.stream_id, data_chunk, end_stream=False)\n+            self._protocol.conn.send_data(self.stream_id, data_chunk, end_stream=False)\n \n             bytes_to_send_size = bytes_to_send_size - chunk_size\n             self.remaining_content_length = self.remaining_content_length - chunk_size\n@@ -258,12 +252,12 @@ class Stream:\n \n         # End the stream if no more data needs to be send\n         if self.remaining_content_length == 0:\n-            self._conn.end_stream(self.stream_id)\n+            self._protocol.conn.end_stream(self.stream_id)\n \n         # Q. What about the rest of the data?\n         # Ans: Remaining Data frames will be sent when we get a WindowUpdate frame\n \n-    def receive_window_update(self):\n+    def receive_window_update(self) -> None:\n         \"\"\"Flow control window size was changed.\n         Send data that earlier could not be sent as we were\n         blocked behind the flow control.\n@@ -271,7 +265,7 @@ class Stream:\n         if self.remaining_content_length and not self.stream_closed_server and self.request_sent:\n             self.send_data()\n \n-    def receive_data(self, data: bytes, flow_controlled_length: int):\n+    def receive_data(self, data: bytes, flow_controlled_length: int) -> None:\n         self._response['body'].write(data)\n         self._response['flow_controlled_size'] += flow_controlled_length\n \n@@ -289,12 +283,12 @@ class Stream:\n             logger.warning(warning_msg)\n \n         # Acknowledge the data received\n-        self._conn.acknowledge_received_data(\n+        self._protocol.conn.acknowledge_received_data(\n             self._response['flow_controlled_size'],\n             self.stream_id\n         )\n \n-    def receive_headers(self, headers):\n+    def receive_headers(self, headers: List[HeaderTuple]) -> None:\n         for name, value in headers:\n             self._response['headers'][name] = value\n \n@@ -312,7 +306,7 @@ class Stream:\n             )\n             logger.warning(warning_msg)\n \n-    def reset_stream(self, reason=StreamCloseReason.RESET):\n+    def reset_stream(self, reason: StreamCloseReason = StreamCloseReason.RESET) -> None:\n         \"\"\"Close this stream by sending a RST_FRAME to the remote peer\"\"\"\n         if self.stream_closed_local:\n             raise StreamClosedError(self.stream_id)\n@@ -321,7 +315,7 @@ class Stream:\n         self._response['body'].truncate(0)\n \n         self.stream_closed_local = True\n-        self._conn.reset_stream(self.stream_id, ErrorCodes.REFUSED_STREAM)\n+        self._protocol.conn.reset_stream(self.stream_id, ErrorCodes.REFUSED_STREAM)\n         self.close(reason)\n \n     def _is_data_lost(self) -> bool:\n@@ -332,11 +326,8 @@ class Stream:\n \n         return expected_size != received_body_size\n \n-    def close(self, reason: StreamCloseReason, error: Exception = None):\n+    def close(self, reason: StreamCloseReason, error: Optional[Exception] = None, from_protocol: bool = False) -> None:\n         \"\"\"Based on the reason sent we will handle each case.\n-\n-        Arguments:\n-            reason -- One if StreamCloseReason\n         \"\"\"\n         if self.stream_closed_server:\n             raise StreamClosedError(self.stream_id)\n@@ -344,7 +335,9 @@ class Stream:\n         if not isinstance(reason, StreamCloseReason):\n             raise TypeError(f'Expected StreamCloseReason, received {reason.__class__.__qualname__}')\n \n-        self._cb_close(self.stream_id)\n+        if not from_protocol:\n+            self._protocol.pop_stream(self.stream_id)\n+\n         self.stream_closed_server = True\n \n         flags = None\n@@ -392,11 +385,11 @@ class Stream:\n         elif reason is StreamCloseReason.INVALID_HOSTNAME:\n             self._deferred_response.errback(InvalidHostname(\n                 self._request,\n-                self._conn_metadata['hostname'],\n-                f'{self._conn_metadata[\"ip_address\"]}:{self._conn_metadata[\"port\"]}'\n+                self._protocol.metadata['hostname'],\n+                f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"port\"]}'\n             ))\n \n-    def _fire_response_deferred(self, flags: List[str] = None):\n+    def _fire_response_deferred(self, flags: Optional[List[str]] = None) -> None:\n         \"\"\"Builds response from the self._response dict\n         and fires the response deferred callback with the\n         generated response instance\"\"\"\n@@ -405,7 +398,7 @@ class Stream:\n         response_cls = responsetypes.from_args(\n             headers=self._response['headers'],\n             url=self._request.url,\n-            body=body\n+            body=body,\n         )\n \n         response = response_cls(\n@@ -415,8 +408,8 @@ class Stream:\n             body=body,\n             request=self._request,\n             flags=flags,\n-            certificate=self._conn_metadata['certificate'],\n-            ip_address=self._conn_metadata['ip_address']\n+            certificate=self._protocol.metadata['certificate'],\n+            ip_address=self._protocol.metadata['ip_address'],\n         )\n \n         self._deferred_response.callback(response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
