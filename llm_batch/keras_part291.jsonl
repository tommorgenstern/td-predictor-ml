{"custom_id": "keras#58cebe948addc7c4cb8762b2e67c8ce656a9d957", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 8 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 30 | Churn Cumulative: 296 | Contributors (this commit): 14 | Commits (past 90d): 3 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -88,6 +88,11 @@ def model_to_dot(model,\n         dot.set('dpi', dpi)\n         dot.set_node_defaults(shape='record')\n \n+    sub_n_first_node = {}\n+    sub_n_last_node = {}\n+    sub_w_first_node = {}\n+    sub_w_last_node = {}\n+\n     if isinstance(model, Sequential):\n         if not model.built:\n             model.build()\n@@ -109,8 +114,8 @@ def model_to_dot(model,\n                                                 subgraph=True)\n                 # sub_w : submodel_wrapper\n                 sub_w_nodes = submodel_wrapper.get_nodes()\n-                sub_w_first_node = sub_w_nodes[0]\n-                sub_w_last_node = sub_w_nodes[len(sub_w_nodes) - 1]\n+                sub_w_first_node[layer.layer.name] = sub_w_nodes[0]\n+                sub_w_last_node[layer.layer.name] = sub_w_nodes[-1]\n                 dot.add_subgraph(submodel_wrapper)\n             else:\n                 layer_name = '{}({})'.format(layer_name, layer.layer.name)\n@@ -124,8 +129,8 @@ def model_to_dot(model,\n                                                 subgraph=True)\n             # sub_n : submodel_not_wrapper\n             sub_n_nodes = submodel_not_wrapper.get_nodes()\n-            sub_n_first_node = sub_n_nodes[0]\n-            sub_n_last_node = sub_n_nodes[len(sub_n_nodes) - 1]\n+            sub_n_first_node[layer.name] = sub_n_nodes[0]\n+            sub_n_last_node[layer.name] = sub_n_nodes[-1]\n             dot.add_subgraph(submodel_not_wrapper)\n \n         # Create node's label.\n@@ -181,19 +186,28 @@ def model_to_dot(model,\n                             # if current layer is Model\n                             elif is_model(layer):\n                                 add_edge(dot, inbound_layer_id,\n-                                         sub_n_first_node.get_name())\n+                                         sub_n_first_node[layer.name].get_name())\n                             # if current layer is wrapped Model\n                             elif is_wrapped_model(layer):\n                                 dot.add_edge(pydot.Edge(inbound_layer_id,\n                                                         layer_id))\n+                                name = sub_w_first_node[layer.layer.name].get_name()\n                                 dot.add_edge(pydot.Edge(layer_id,\n-                                                        sub_w_first_node.get_name()))\n+                                                        name))\n                         # if inbound_layer is Model\n                         elif is_model(inbound_layer):\n-                            add_edge(dot, sub_n_last_node.get_name(), layer_id)\n+                            name = sub_n_last_node[inbound_layer.name].get_name()\n+                            if is_model(layer):\n+                                output_name = sub_n_first_node[layer.name].get_name()\n+                                add_edge(dot, name, output_name)\n+                            else:\n+                                add_edge(dot, name, layer_id)\n                         # if inbound_layer is wrapped Model\n                         elif is_wrapped_model(inbound_layer):\n-                            add_edge(dot, sub_w_last_node.get_name(), layer_id)\n+                            inbound_layer_name = inbound_layer.layer.name\n+                            add_edge(dot,\n+                                     sub_w_last_node[inbound_layer_name].get_name(),\n+                                     layer_id)\n     return dot\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0792332f77e6a34093cab9c2bc6f638d1676bccd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 439 | Lines Deleted: 28 | Files Changed: 8 | Hunks: 20 | Methods Changed: 45 | Complexity Δ (Sum/Max): 45/20 | Churn Δ: 467 | Churn Cumulative: 26507 | Contributors (this commit): 177 | Commits (past 90d): 58 | Contributors (cumulative): 300 | DMM Complexity: 0.875\n\nDIFF:\n@@ -149,6 +149,7 @@ from .load_backend import normalize_data_format\n from .load_backend import name_scope\r\n from .load_backend import symbolic\r\n from .load_backend import eager\r\n+from .load_backend import size\r\n \r\n if backend() == 'theano':\r\n     from .load_backend import pattern_broadcast\r\n\n@@ -559,6 +559,10 @@ def cast(x, dtype):\n     return x\n \n \n+def size(x, name=None):\n+    return sum(ones_like(x, name=name))\n+\n+\n def dot(x, y):\n     if len(x.shape) > 2 or len(y.shape) > 2:\n         y_shape = int_shape(y)\n\n@@ -831,6 +831,29 @@ def ndim(x):\n     return x.shape.rank\n \n \n+def size(x, name=None):\n+    \"\"\"Returns the size of a tensor.\n+\n+    # Arguments\n+        x: Tensor or variable.\n+        name: A name for the operation (optional).\n+\n+    # Returns\n+        Size of the tensor.\n+\n+    # Examples\n+    ```python\n+    >>> from keras import backend as K\n+    >>> val = np.array([[1, 2], [3, 4]])\n+    >>> kvar = K.variable(value=val)\n+    >>> K.size(inputs)\n+    <tf.Tensor: id=9, shape=(), dtype=int32, numpy=4>\n+    ```\n+\n+    \"\"\"\n+    return tf.size(x, name=name)\n+\n+\n def dtype(x):\n     \"\"\"Returns the dtype of a Keras tensor or variable, as a string.\n \n\n@@ -376,6 +376,18 @@ def cast(x, dtype):\n     return T.cast(x, dtype)\n     \n \n+def size(x, name=None):\n+    \"\"\"Returns the size of a tensor.\n+    # Arguments\n+        x: The input tensor.\n+        name: A name for the operation (optional).\n+    # Returns\n+        Size of the tensor.\n+    ```\n+    \"\"\"\n+    return sum(ones_like(x, name=name))\n+\n+\n # UPDATES OPS\n \n \n\n@@ -4,12 +4,174 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+import abc\n import six\n+\n from . import backend as K\n+from .utils import losses_utils\n from .utils.generic_utils import deserialize_keras_object\n from .utils.generic_utils import serialize_keras_object\n \n \n+@six.add_metaclass(abc.ABCMeta)\n+class Loss(object):\n+    \"\"\"Loss base class.\n+\n+    To be implemented by subclasses:\n+        * `call()`: Contains the logic for loss calculation using `y_true`, `y_pred`.\n+\n+    Example subclass implementation:\n+    ```python\n+    class MeanSquaredError(Loss):\n+        def call(self, y_true, y_pred):\n+            y_pred = ops.convert_to_tensor(y_pred)\n+            y_true = math_ops.cast(y_true, y_pred.dtype)\n+            return K.mean(math_ops.square(y_pred - y_true), axis=-1)\n+    ```\n+\n+    # Arguments\n+        reduction: (Optional) Type of loss Reduction to apply to loss.\n+          Default value is `SUM_OVER_BATCH_SIZE`.\n+        name: Optional name for the op.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE,\n+                 name=None):\n+        self.reduction = reduction\n+        self.name = name\n+\n+    def __call__(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Invokes the `Loss` instance.\n+\n+        # Arguments\n+            y_true: Ground truth values.\n+            y_pred: The predicted values.\n+            sample_weight: Optional `Tensor` whose rank is either 0, or the same rank\n+            as `y_true`, or is broadcastable to `y_true`. `sample_weight` acts as a\n+            coefficient for the loss. If a scalar is provided, then the loss is\n+            simply scaled by the given value. If `sample_weight` is a tensor of size\n+            `[batch_size]`, then the total loss for each sample of the batch is\n+            rescaled by the corresponding element in the `sample_weight` vector. If\n+            the shape of `sample_weight` matches the shape of `y_pred`, then the\n+            loss of each measurable element of `y_pred` is scaled by the\n+            corresponding value of `sample_weight`.\n+\n+        # Returns\n+            Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same\n+                shape as `y_true`; otherwise, it is scalar.\n+\n+        # Raises\n+            ValueError: If the shape of `sample_weight` is invalid.\n+        \"\"\"\n+        # If we are wrapping a lambda function strip '<>' from the name as it is not\n+        # accepted in scope name.\n+        scope_name = 'lambda' if self.name == '<lambda>' else self.name\n+        with K.name_scope(scope_name):\n+            losses = self.call(y_true, y_pred)\n+            return losses_utils.compute_weighted_loss(\n+                losses, sample_weight, reduction=self.reduction)\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        \"\"\"Instantiates a `Loss` from its config (output of `get_config()`).\n+\n+        # Arguments\n+            config: Output of `get_config()`.\n+\n+        # Returns\n+            A `Loss` instance.\n+        \"\"\"\n+        return cls(**config)\n+\n+    def get_config(self):\n+        return {'reduction': self.reduction, 'name': self.name}\n+\n+    @abc.abstractmethod\n+    def call(self, y_true, y_pred):\n+        \"\"\"Invokes the `Loss` instance.\n+\n+        # Arguments\n+            y_true: Ground truth values, with the same shape as 'y_pred'.\n+            y_pred: The predicted values.\n+        \"\"\"\n+        raise NotImplementedError('Must be implemented in subclasses.')\n+\n+\n+class LossFunctionWrapper(Loss):\n+    \"\"\"Wraps a loss function in the `Loss` class.\n+\n+    # Arguments\n+        fn: The loss function to wrap, with signature `fn(y_true, y_pred,\n+            **kwargs)`.\n+        reduction: (Optional) Type of loss reduction to apply to loss.\n+            Default value is `SUM_OVER_BATCH_SIZE`.\n+        name: (Optional) name for the loss.\n+        **kwargs: The keyword arguments that are passed on to `fn`.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 fn,\n+                 reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE,\n+                 name=None,\n+                 **kwargs):\n+        super(LossFunctionWrapper, self).__init__(reduction=reduction, name=name)\n+        self.fn = fn\n+        self._fn_kwargs = kwargs\n+\n+    def call(self, y_true, y_pred):\n+        \"\"\"Invokes the `LossFunctionWrapper` instance.\n+\n+        # Arguments\n+            y_true: Ground truth values.\n+            y_pred: The predicted values.\n+\n+        # Returns\n+            Loss values per sample.\n+        \"\"\"\n+        return self.fn(y_true, y_pred, **self._fn_kwargs)\n+\n+    def get_config(self):\n+        config = {}\n+        for k, v in six.iteritems(self._fn_kwargs):\n+            config[k] = K.eval(v) if is_tensor_or_variable(v) else v\n+        base_config = super(LossFunctionWrapper, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class MeanSquaredError(LossFunctionWrapper):\n+    \"\"\"Computes the mean of squares of errors between labels and predictions.\n+\n+    For example, if `y_true` is [0., 0., 1., 1.] and `y_pred` is [1., 1., 1., 0.]\n+    then the mean squared error value is 3/4 (0.75).\n+\n+    Standalone usage:\n+\n+    ```python\n+    mse = keras.losses.MeanSquaredError()\n+    loss = mse([0., 0., 1., 1.], [1., 1., 1., 0.])\n+    ```\n+\n+    Usage with the `compile` API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss=keras.losses.MeanSquaredError())\n+    ```\n+\n+    # Arguments\n+        reduction: (Optional) Type of loss reduction to apply to loss.\n+            Default value is `SUM_OVER_BATCH_SIZE`.\n+        name: (Optional) name for the loss.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE,\n+                 name='mean_squared_error'):\n+        super(MeanSquaredError, self).__init__(\n+            mean_squared_error, name=name, reduction=reduction)\n+\n+\n def mean_squared_error(y_true, y_pred):\n     return K.mean(K.square(y_pred - y_true), axis=-1)\n \n\n@@ -4,6 +4,7 @@ from . import generic_utils\n from . import data_utils\n from . import io_utils\n from . import conv_utils\n+from . import losses_utils\n \n # Globally-importable utils.\n from .io_utils import HDF5Matrix\n\n@@ -0,0 +1,136 @@\n+\"\"\"Utilities related to losses.\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import numpy as np\n+\n+from .. import backend as K\n+\n+\n+class Reduction(object):\n+    \"\"\"Types of loss reduction.\n+\n+    Contains the following values:\n+\n+    * `NONE`: Un-reduced weighted losses with the same shape as input. When this\n+        reduction type used with built-in Keras training loops like\n+        `fit`/`evaluate`, the unreduced vector loss is passed to the optimizer but\n+        the reported loss will be a scalar value.\n+    * `SUM`: Scalar sum of weighted losses.\n+    * `SUM_OVER_BATCH_SIZE`: Scalar `SUM` divided by number of elements in losses.\n+    \"\"\"\n+\n+    NONE = 'none'\n+    SUM = 'sum'\n+    SUM_OVER_BATCH_SIZE = 'sum_over_batch_size'\n+\n+    @classmethod\n+    def all(cls):\n+        return (cls.NONE, cls.SUM, cls.SUM_OVER_BATCH_SIZE)\n+\n+    @classmethod\n+    def validate(cls, key):\n+        if key not in cls.all():\n+            raise ValueError('Invalid Reduction Key %s.' % key)\n+\n+\n+def squeeze_or_expand_dimensions(y_pred, y_true, sample_weight):\n+    \"\"\"Squeeze or expand last dimension if needed.\n+\n+    1. Squeezes last dim of `y_pred` or `y_true` if their rank differs by 1.\n+    2. Squeezes or expands last dim of `sample_weight` if its rank differs by 1\n+    from the new rank of `y_pred`.\n+    If `sample_weight` is scalar, it is kept scalar.\n+\n+    # Arguments\n+        y_pred: Predicted values, a `Tensor` of arbitrary dimensions.\n+        y_true: Optional label `Tensor` whose dimensions match `y_pred`.\n+        sample_weight: Optional weight scalar or `Tensor` whose dimensions match\n+            `y_pred`.\n+\n+    # Returns\n+        Tuple of `y_pred`, `y_true` and `sample_weight`. Each of them possibly has\n+        the last dimension squeezed, `sample_weight` could be extended by one\n+        dimension.\n+    \"\"\"\n+    if y_true is not None:\n+        y_pred_rank = K.ndim(y_pred)\n+        y_pred_shape = K.int_shape(y_pred)\n+        y_true_rank = K.ndim(y_true)\n+        y_true_shape = K.int_shape(y_true)\n+\n+        if (y_pred_rank - y_true_rank == 1) and (y_pred_shape[-1] == 1):\n+            y_pred = K.squeeze(y_pred, -1)\n+        elif (y_true_rank - y_pred_rank == 1) and (y_true_shape[-1] == 1):\n+            y_true = K.squeeze(y_true, -1)\n+\n+    if sample_weight is None:\n+        return y_pred, y_true, None\n+\n+    y_pred_rank = K.ndim(y_pred)\n+    weights_rank = K.ndim(sample_weight)\n+    if weights_rank != 0:\n+        if weights_rank - y_pred_rank == 1:\n+            sample_weight = K.squeeze(sample_weight, -1)\n+        elif y_pred_rank - weights_rank == 1:\n+            sample_weight = K.expand_dims(sample_weight, -1)\n+    return y_pred, y_true, sample_weight\n+\n+\n+def _num_elements(losses):\n+    \"\"\"Computes the number of elements in `losses` tensor.\"\"\"\n+    with K.name_scope('num_elements') as scope:\n+        return K.cast(K.size(losses, name=scope), losses.dtype)\n+\n+\n+def reduce_weighted_loss(weighted_losses, reduction=Reduction.SUM_OVER_BATCH_SIZE):\n+    \"\"\"Reduces the individual weighted loss measurements.\"\"\"\n+    if reduction == Reduction.NONE:\n+        loss = weighted_losses\n+    else:\n+        loss = K.sum(weighted_losses)\n+        if reduction == Reduction.SUM_OVER_BATCH_SIZE:\n+            loss = loss / _num_elements(weighted_losses)\n+    return loss\n+\n+\n+def compute_weighted_loss(losses,\n+                          sample_weight=None,\n+                          reduction=Reduction.SUM_OVER_BATCH_SIZE,\n+                          name=None):\n+    \"\"\"Computes the weighted loss.\n+\n+    # Arguments\n+        losses: `Tensor` of shape `[batch_size, d1, ... dN]`.\n+        sample_weight: Optional `Tensor` whose rank is either 0, or the same rank as\n+        `   losses`, or be broadcastable to `losses`.\n+        reduction: (Optional) Type of Reduction to apply to loss.\n+            Default value is `SUM_OVER_BATCH_SIZE`.\n+        name: Optional name for the op.\n+\n+    # Raises\n+        ValueError: If the shape of `sample_weight` is not compatible with `losses`.\n+\n+    # Returns\n+        Weighted loss `Tensor` of the same type as `losses`. If `reduction` is\n+            `NONE`, this has the same shape as `losses`; otherwise, it is scalar.\n+    \"\"\"\n+    Reduction.validate(reduction)\n+    if sample_weight is None:\n+        sample_weight = 1.0\n+    with K.name_scope(name or 'weighted_loss'):\n+        input_dtype = K.dtype(losses)\n+        losses = K.cast(losses, K.floatx())\n+        sample_weight = K.cast(sample_weight, K.floatx())\n+\n+        # Update dimensions of `sample_weight` to match with `losses` if possible.\n+        losses, _, sample_weight = squeeze_or_expand_dimensions(\n+            losses, None, sample_weight)\n+\n+        weighted_losses = losses * sample_weight\n+        # Apply reduction function to the individual weighted losses.\n+        loss = reduce_weighted_loss(weighted_losses, reduction)\n+        # Convert the result back to the input type.\n+        loss = K.cast(loss, input_dtype)\n+        return loss\n\n@@ -4,6 +4,7 @@ import numpy as np\n import keras\n from keras import losses\n from keras import backend as K\n+from keras.utils import losses_utils\n from keras.utils.generic_utils import custom_object_scope\n \n \n@@ -22,23 +23,36 @@ allobj = [losses.mean_squared_error,\n           losses.categorical_hinge]\n \n \n-def test_objective_shapes_3d():\n+class MSE_MAE_loss:\n+    \"\"\"Loss function with internal state, for testing serialization code.\"\"\"\n+    def __init__(self, mse_fraction):\n+        self.mse_fraction = mse_fraction\n+\n+    def __call__(self, y_true, y_pred):\n+        return (self.mse_fraction * losses.mse(y_true, y_pred) +\n+                (1 - self.mse_fraction) * losses.mae(y_true, y_pred))\n+\n+    def get_config(self):\n+        return {'mse_fraction': self.mse_fraction}\n+\n+\n+class TestLossFunctions:\n+\n+    def test_objective_shapes_3d(self):\n         y_a = K.variable(np.random.random((5, 6, 7)))\n         y_b = K.variable(np.random.random((5, 6, 7)))\n         for obj in allobj:\n             objective_output = obj(y_a, y_b)\n             assert K.eval(objective_output).shape == (5, 6)\n \n-\n-def test_objective_shapes_2d():\n+    def test_objective_shapes_2d(self):\n         y_a = K.variable(np.random.random((6, 7)))\n         y_b = K.variable(np.random.random((6, 7)))\n         for obj in allobj:\n             objective_output = obj(y_a, y_b)\n             assert K.eval(objective_output).shape == (6,)\n \n-\n-def test_cce_one_hot():\n+    def test_cce_one_hot(self):\n         y_a = K.variable(np.random.randint(0, 7, (5, 6)))\n         y_b = K.variable(np.random.random((5, 6, 7)))\n         objective_output = losses.sparse_categorical_crossentropy(y_a, y_b)\n@@ -48,8 +62,7 @@ def test_cce_one_hot():\n         y_b = K.variable(np.random.random((6, 7)))\n         assert K.eval(losses.sparse_categorical_crossentropy(y_a, y_b)).shape == (6,)\n \n-\n-def test_categorical_hinge():\n+    def test_categorical_hinge(self):\n         y_pred = K.variable(np.array([[0.3, 0.2, 0.1],\n                                       [0.1, 0.2, 0.7]]))\n         y_true = K.variable(np.array([[0, 1, 0],\n@@ -58,8 +71,7 @@ def test_categorical_hinge():\n         loss = K.eval(losses.categorical_hinge(y_true, y_pred))\n         assert np.isclose(expected_loss, np.mean(loss))\n \n-\n-def test_sparse_categorical_crossentropy():\n+    def test_sparse_categorical_crossentropy(self):\n         y_pred = K.variable(np.array([[0.3, 0.6, 0.1],\n                                       [0.1, 0.2, 0.7]]))\n         y_true = K.variable(np.array([1, 2]))\n@@ -67,8 +79,7 @@ def test_sparse_categorical_crossentropy():\n         loss = K.eval(losses.sparse_categorical_crossentropy(y_true, y_pred))\n         assert np.isclose(expected_loss, np.mean(loss))\n \n-\n-def test_sparse_categorical_crossentropy_4d():\n+    def test_sparse_categorical_crossentropy_4d(self):\n         y_pred = K.variable(np.array([[[[0.7, 0.1, 0.2],\n                                         [0.0, 0.3, 0.7],\n                                         [0.1, 0.1, 0.8]],\n@@ -87,21 +98,7 @@ def test_sparse_categorical_crossentropy_4d():\n         loss = K.eval(losses.sparse_categorical_crossentropy(y_true, y_pred))\n         assert np.isclose(expected_loss, np.mean(loss))\n \n-\n-class MSE_MAE_loss:\n-    \"\"\"Loss function with internal state, for testing serialization code.\"\"\"\n-    def __init__(self, mse_fraction):\n-        self.mse_fraction = mse_fraction\n-\n-    def __call__(self, y_true, y_pred):\n-        return (self.mse_fraction * losses.mse(y_true, y_pred) +\n-                (1 - self.mse_fraction) * losses.mae(y_true, y_pred))\n-\n-    def get_config(self):\n-        return {'mse_fraction': self.mse_fraction}\n-\n-\n-def test_serializing_loss_class():\n+    def test_serializing_loss_class(self):\n         orig_loss_class = MSE_MAE_loss(0.3)\n         with custom_object_scope({'MSE_MAE_loss': MSE_MAE_loss}):\n             serialized = losses.serialize(orig_loss_class)\n@@ -111,8 +108,7 @@ def test_serializing_loss_class():\n         assert isinstance(deserialized, MSE_MAE_loss)\n         assert deserialized.mse_fraction == 0.3\n \n-\n-def test_serializing_model_with_loss_class(tmpdir):\n+    def test_serializing_model_with_loss_class(self, tmpdir):\n         model_filename = str(tmpdir / 'custom_loss.hdf')\n \n         with custom_object_scope({'MSE_MAE_loss': MSE_MAE_loss}):\n@@ -129,5 +125,81 @@ def test_serializing_model_with_loss_class(tmpdir):\n             loaded_model.predict(np.random.rand(128, 2))\n \n \n+class TestMeanSquaredError:\n+\n+    def test_config(self):\n+        mse_obj = losses.MeanSquaredError(\n+            reduction=losses_utils.Reduction.SUM, name='mse_1')\n+        assert mse_obj.name == 'mse_1'\n+        assert mse_obj.reduction == losses_utils.Reduction.SUM\n+\n+    def test_all_correct_unweighted(self):\n+        mse_obj = losses.MeanSquaredError()\n+        y_true = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mse_obj(y_true, y_true)\n+        assert np.isclose(K.eval(loss), 0.0)\n+\n+    def test_unweighted(self):\n+        mse_obj = losses.MeanSquaredError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mse_obj(y_true, y_pred)\n+        assert np.isclose(K.eval(loss), 49.5, rtol=1e-3)\n+\n+    def test_scalar_weighted(self):\n+        mse_obj = losses.MeanSquaredError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mse_obj(y_true, y_pred, sample_weight=2.3)\n+        assert np.isclose(K.eval(loss), 113.85, rtol=1e-3)\n+\n+    def test_sample_weighted(self):\n+        mse_obj = losses.MeanSquaredError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        sample_weight = K.constant([1.2, 3.4], shape=(2, 1))\n+        loss = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.isclose(K.eval(loss), 767.8 / 6, rtol=1e-3)\n+\n+    def test_timestep_weighted(self):\n+        mse_obj = losses.MeanSquaredError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n+        sample_weight = K.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n+        loss = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.isclose(K.eval(loss), 587 / 6, rtol=1e-3)\n+\n+    def test_zero_weighted(self):\n+        mse_obj = losses.MeanSquaredError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mse_obj(y_true, y_pred, sample_weight=0)\n+        assert np.isclose(K.eval(loss), 0.0)\n+\n+    def test_invalid_sample_weight(self):\n+        mse_obj = losses.MeanSquaredError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n+        sample_weight = K.constant([3, 6, 5, 0], shape=(2, 2))\n+        with pytest.raises(Exception):\n+            mse_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+    def test_no_reduction(self):\n+        mse_obj = losses.MeanSquaredError(\n+            reduction=losses_utils.Reduction.NONE)\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mse_obj(y_true, y_pred, sample_weight=2.3)\n+        assert np.allclose(K.eval(loss), [84.3333, 143.3666], rtol=1e-3)\n+\n+    def test_sum_reduction(self):\n+        mse_obj = losses.MeanSquaredError(\n+            reduction=losses_utils.Reduction.SUM)\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mse_obj(y_true, y_pred, sample_weight=2.3)\n+        assert np.isclose(K.eval(loss), 227.69998, rtol=1e-3)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#29f27f333efb427830e62bec62fd79b444df4011", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 10 | Files Changed: 4 | Hunks: 12 | Methods Changed: 12 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 49 | Churn Cumulative: 15678 | Contributors (this commit): 74 | Commits (past 90d): 11 | Contributors (cumulative): 97 | DMM Complexity: 1.0\n\nDIFF:\n@@ -57,6 +57,7 @@ class Layer(object):\n             or `K.in_test_phase()`.\n         weights: The concatenation of the lists trainable_weights and\n             non_trainable_weights (in this order).\n+        dtype:  Default dtype of the layers's weights.\n \n \n     # Methods\n@@ -238,8 +239,8 @@ class Layer(object):\n         \"\"\"\n         initializer = initializers.get(initializer)\n         if dtype is None:\n-            dtype = K.floatx()\n-        weight = K.variable(initializer(shape),\n+            dtype = self.dtype\n+        weight = K.variable(initializer(shape, dtype=dtype),\n                             dtype=dtype,\n                             name=name,\n                             constraint=constraint)\n\n@@ -48,6 +48,7 @@ class Network(Layer):\n                 - ndim\n                 - dtype\n         trainable (boolean)\n+        dtype\n         input_shape\n         output_shape\n         weights (list of variables)\n@@ -95,7 +96,7 @@ class Network(Layer):\n             # Subclassed network\n             self._init_subclassed_network(**kwargs)\n \n-    def _base_init(self, name=None):\n+    def _base_init(self, name=None, trainable=True, dtype=None):\n         # The following are implemented as property functions:\n         # self.trainable_weights\n         # self.non_trainable_weights\n@@ -112,7 +113,10 @@ class Network(Layer):\n         # This acts just like the `trainable` attribute of any layer instance.\n         # It does not affect users of the underlying layers, only users of the\n         # Network instance.\n-        self.trainable = True\n+        self.trainable = trainable\n+        if dtype is None:\n+            dtype = K.floatx()\n+        self.dtype = dtype\n         self._is_compiled = False\n         self._expects_training_arg = False\n         self._initial_weights = None\n@@ -123,6 +127,8 @@ class Network(Layer):\n             self.optimizer = None\n \n         # Private attributes to implement compatibility with Layer.\n+        self._trainable_weights = []\n+        self._non_trainable_weights = []\n         self._updates = []\n         self._losses = []\n         self._per_input_losses = {}\n@@ -136,7 +142,7 @@ class Network(Layer):\n         self._outbound_nodes = []\n         self._inbound_nodes = []\n \n-    def _init_graph_network(self, inputs, outputs, name=None):\n+    def _init_graph_network(self, inputs, outputs, name=None, **kwargs):\n         self._uses_inputs_arg = True\n         # Normalize and set self.inputs, self.outputs.\n         self.inputs = to_list(inputs, allow_tuple=True)\n@@ -186,7 +192,7 @@ class Network(Layer):\n                                  'the output of a Keras `Layer` '\n                                  '(thus holding past layer metadata). '\n                                  'Found: ' + str(x))\n-        self._base_init(name=name)\n+        self._base_init(name=name, **kwargs)\n         self._compute_previous_mask = (\n             has_arg(self.call, 'mask') or\n             hasattr(self, 'compute_mask'))\n@@ -290,8 +296,8 @@ class Network(Layer):\n         for layer in self._output_layers:\n             self.output_names.append(layer.name)\n \n-    def _init_subclassed_network(self, name=None):\n-        self._base_init(name=name)\n+    def _init_subclassed_network(self, name=None, **kwargs):\n+        self._base_init(name=name, **kwargs)\n         self._is_graph_network = False\n         self._expects_training_arg = has_arg(self.call, 'training')\n         self._uses_inputs_arg = has_arg(self.call, 'inputs')\n\n@@ -1269,7 +1269,8 @@ class GRUCell(Layer):\n         input_dim = input_shape[-1]\n \n         if isinstance(self.recurrent_initializer, initializers.Identity):\n-            def recurrent_identity(shape, gain=1.):\n+            def recurrent_identity(shape, gain=1., dtype=None):\n+                del dtype\n                 return gain * np.concatenate(\n                     [np.identity(shape[0])] * (shape[1] // shape[0]), axis=1)\n \n@@ -1872,7 +1873,8 @@ class LSTMCell(Layer):\n         input_dim = input_shape[-1]\n \n         if type(self.recurrent_initializer).__name__ == 'Identity':\n-            def recurrent_identity(shape, gain=1.):\n+            def recurrent_identity(shape, gain=1., dtype=None):\n+                del dtype\n                 return gain * np.concatenate(\n                     [np.identity(shape[0])] * (shape[1] // shape[0]), axis=1)\n \n\n@@ -840,5 +840,25 @@ def test_constant_initializer_with_numpy():\n     model_from_yaml(yaml_str).summary()\n \n \n+@pytest.mark.skipif(K.backend() == 'ctnk',\n+                    reason='Float64 not supported with CNTK.')\n+def test_initialization_dtype():\n+    class TestLayer(Layer):\n+        def __init__(self):\n+            super(TestLayer, self).__init__(dtype='int64')\n+            self.w = self.add_weight('w', [], initializer=Constant(1))\n+\n+    layer = TestLayer()\n+    assert K.dtype(layer.w) == 'int64'\n+\n+    class TestModel(Model):\n+        def __init__(self):\n+            super(TestModel, self).__init__(dtype='int64')\n+            self.w = self.add_weight('w', [], initializer=Constant(1))\n+\n+    model = TestModel()\n+    assert K.dtype(model.w) == 'int64'\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#08f6bdeb5652550f36210f64aefda3d0d41e2d79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 223 | Lines Deleted: 118 | Files Changed: 2 | Hunks: 88 | Methods Changed: 23 | Complexity Δ (Sum/Max): 16/15 | Churn Δ: 341 | Churn Cumulative: 3094 | Contributors (this commit): 41 | Commits (past 90d): 11 | Contributors (cumulative): 49 | DMM Complexity: 0.4606741573033708\n\nDIFF:\n@@ -6,6 +6,7 @@ from __future__ import print_function\n \n import six\n import copy\n+import numpy as np\n from six.moves import zip\n \n from . import backend as K\n@@ -161,23 +162,24 @@ class SGD(Optimizer):\n     learning rate decay, and Nesterov momentum.\n \n     # Arguments\n-        lr: float >= 0. Learning rate.\n+        learning_rate: float >= 0. Learning rate.\n         momentum: float >= 0. Parameter that accelerates SGD\n             in the relevant direction and dampens oscillations.\n-        decay: float >= 0. Learning rate decay over each update.\n         nesterov: boolean. Whether to apply Nesterov momentum.\n     \"\"\"\n \n-    def __init__(self, lr=0.01, momentum=0., decay=0.,\n+    def __init__(self, learning_rate=0.01, momentum=0.,\n                  nesterov=False, **kwargs):\n-        super(SGD, self).__init__(**kwargs)\n+        self.initial_decay = kwargs.pop('decay', 0.0)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n-            self.lr = K.variable(lr, name='lr')\n+            learning_rate = kwargs.pop('lr', learning_rate)\n+            self.lr = K.variable(learning_rate, name='learning_rate')\n             self.momentum = K.variable(momentum, name='momentum')\n-            self.decay = K.variable(decay, name='decay')\n-        self.initial_decay = decay\n+            self.decay = K.variable(self.initial_decay, name='decay')\n+        self._momentum = True if momentum > 0. else False\n         self.nesterov = nesterov\n+        super(SGD, self).__init__(**kwargs)\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n@@ -188,28 +190,35 @@ class SGD(Optimizer):\n         if self.initial_decay > 0:\n             lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                       K.dtype(self.decay))))\n-        # momentum\n+        self.weights = [self.iterations]\n+        if self._momentum:\n             shapes = [K.int_shape(p) for p in params]\n             moments = [K.zeros(shape) for shape in shapes]\n-        self.weights = [self.iterations] + moments\n+            self.weights.extend([K.zeros(shape) for shape in shapes])\n+        else:\n+            moments = [None] * len(params)\n         for p, g, m in zip(params, grads, moments):\n+            if self._momentum:\n+                # momentum optimizer\n                 v = self.momentum * m - lr * g  # velocity\n                 self.updates.append(K.update(m, v))\n-\n                 if self.nesterov:\n                     new_p = p + self.momentum * v - lr * g\n                 else:\n                     new_p = p + v\n-\n+            else:\n+                # sgd optimizer.\n+                new_p = p - lr * g\n             # Apply constraints.\n             if getattr(p, 'constraint', None) is not None:\n                 new_p = p.constraint(new_p)\n \n             self.updates.append(K.update(p, new_p))\n+\n         return self.updates\n \n     def get_config(self):\n-        config = {'lr': float(K.get_value(self.lr)),\n+        config = {'learning_rate': float(K.get_value(self.lr)),\n                   'momentum': float(K.get_value(self.momentum)),\n                   'decay': float(K.get_value(self.decay)),\n                   'nesterov': self.nesterov}\n@@ -225,34 +234,41 @@ class RMSprop(Optimizer):\n     (except the learning rate, which can be freely tuned).\n \n     # Arguments\n-        lr: float >= 0. Learning rate.\n+        learning_rate: float >= 0. Learning rate.\n         rho: float >= 0.\n-        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n-        decay: float >= 0. Learning rate decay over each update.\n+        momentum: float hyperparameter >= 0 that accelerates RMSprop in the relevant\n+            direction and dampens oscillations.\n+        centered: If True, gradients are normalized by the estimated variance of\n+            the gradient; if False, by the uncentered second moment. Setting this to\n+            True may help with training, but is slightly more expensive in terms of\n+            computation and memory. Defaults to False.\n \n     # References\n         - [rmsprop: Divide the gradient by a running average of its recent magnitude\n            ](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n     \"\"\"\n \n-    def __init__(self, lr=0.001, rho=0.9, epsilon=None, decay=0.,\n+    def __init__(self, learning_rate=0.001, rho=0.9, momentum=0., centered=False,\n                  **kwargs):\n-        super(RMSprop, self).__init__(**kwargs)\n+        self.initial_decay = kwargs.pop('decay', 0.0)\n+        self._momentum = True if momentum > 0. else False\n         with K.name_scope(self.__class__.__name__):\n-            self.lr = K.variable(lr, name='lr')\n+            learning_rate = kwargs.pop('lr', learning_rate)\n+            self.lr = K.variable(learning_rate, name='learning_rate')\n             self.rho = K.variable(rho, name='rho')\n-            self.decay = K.variable(decay, name='decay')\n+            self.decay = K.variable(self.initial_decay, name='decay')\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n-        if epsilon is None:\n-            epsilon = K.epsilon()\n-        self.epsilon = epsilon\n-        self.initial_decay = decay\n+            if self._momentum:\n+                self.momentum = K.variable(momentum, name='momentum')\n+        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n+        self.centered = centered\n+        super(RMSprop, self).__init__(**kwargs)\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n         accumulators = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n-        self.weights = accumulators\n+        self.weights = [self.iterations] + accumulators\n         self.updates = [K.update_add(self.iterations, 1)]\n \n         lr = self.lr\n@@ -260,21 +276,52 @@ class RMSprop(Optimizer):\n             lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                       K.dtype(self.decay))))\n \n-        for p, g, a in zip(params, grads, accumulators):\n-            # update accumulator\n+        if self._momentum:\n+            ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n+            self.weights.extend(ms)\n+            if self.centered:\n+                mgs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n+                self.weights.extend(mgs)\n+            else:\n+                mgs = [None] * len(params)\n+        else:\n+            ms = [None] * len(params)\n+            mgs = [None] * len(params)\n+\n+        for p, g, a, m, mg in zip(params, grads, accumulators, ms, mgs):\n             new_a = self.rho * a + (1. - self.rho) * K.square(g)\n             self.updates.append(K.update(a, new_a))\n+            if m:\n+                if mg:\n+                    new_mg = self.rho * mg + (1. - self.rho) * g\n+                    new_m = self.momentum * m + lr * g / K.sqrt(\n+                        new_a - K.square(new_mg) + self.epsilon)\n+                    self.updates.append(K.update(mg, new_mg))\n+                else:\n+                    new_m = self.momentum * m + lr * g / K.sqrt(new_a + self.epsilon)\n+                self.updates.append(K.update(m, new_m))\n+                new_p = p - new_m\n+            else:\n                 new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)\n-\n             # Apply constraints.\n             if getattr(p, 'constraint', None) is not None:\n                 new_p = p.constraint(new_p)\n \n             self.updates.append(K.update(p, new_p))\n+\n         return self.updates\n \n+    def set_weights(self, weights):\n+        params = self.weights\n+        # Override set_weights for backward compatibility of Keras 2.2.4 optimizer\n+        # since it does not include iteration at head of the weight list. Set\n+        # iteration to 0.\n+        if len(params) == len(weights) + 1:\n+            weights = [np.array(0)] + weights\n+        super(RMSprop, self).set_weights(weights)\n+\n     def get_config(self):\n-        config = {'lr': float(K.get_value(self.lr)),\n+        config = {'learning_rate': float(K.get_value(self.lr)),\n                   'rho': float(K.get_value(self.rho)),\n                   'decay': float(K.get_value(self.decay)),\n                   'epsilon': self.epsilon}\n@@ -294,32 +341,33 @@ class Adagrad(Optimizer):\n     at their default values.\n \n     # Arguments\n-        lr: float >= 0. Initial learning rate.\n-        epsilon: float >= 0. If `None`, defaults to `K.epsilon()`.\n-        decay: float >= 0. Learning rate decay over each update.\n+        learning_rate: float >= 0. Initial learning rate.\n+        initial_accumulator_value: A floating point value.\n+            Starting value for the accumulators, must be positive.\n \n     # References\n         - [Adaptive Subgradient Methods for Online Learning and Stochastic\n            Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n     \"\"\"\n \n-    def __init__(self, lr=0.01, epsilon=None, decay=0., **kwargs):\n-        super(Adagrad, self).__init__(**kwargs)\n+    def __init__(self, learning_rate=0.001, initial_accumulator_value=0.1, **kwargs):\n+        self.initial_decay = kwargs.pop('decay', 0.0)\n         with K.name_scope(self.__class__.__name__):\n-            self.lr = K.variable(lr, name='lr')\n-            self.decay = K.variable(decay, name='decay')\n+            learning_rate = kwargs.pop('lr', learning_rate)\n+            self.lr = K.variable(learning_rate, name='learning_rate')\n+            self.decay = K.variable(self.initial_decay, name='decay')\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n-        if epsilon is None:\n-            epsilon = K.epsilon()\n-        self.epsilon = epsilon\n-        self.initial_decay = decay\n+        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n+        self._initial_accumulator_value = initial_accumulator_value\n+        super(Adagrad, self).__init__(**kwargs)\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n         shapes = [K.int_shape(p) for p in params]\n-        accumulators = [K.zeros(shape) for shape in shapes]\n-        self.weights = accumulators\n+        accumulators = [K.variable(np.full(shape, self._initial_accumulator_value))\n+                        for shape in shapes]\n+        self.weights = [self.iterations] + accumulators\n         self.updates = [K.update_add(self.iterations, 1)]\n \n         lr = self.lr\n@@ -339,10 +387,19 @@ class Adagrad(Optimizer):\n             self.updates.append(K.update(p, new_p))\n         return self.updates\n \n+    def set_weights(self, weights):\n+        params = self.weights\n+        # Override set_weights for backward compatibility of Keras 2.2.4 optimizer\n+        # since it does not include iteration at head of the weight list. Set\n+        # iteration to 0.\n+        if len(params) == len(weights) + 1:\n+            weights = [np.array(0)] + weights\n+        super(Adagrad, self).set_weights(weights)\n+\n     def get_config(self):\n-        config = {'lr': float(K.get_value(self.lr)),\n+        config = {'learning_rate': float(K.get_value(self.lr)),\n                   'decay': float(K.get_value(self.decay)),\n-                  'epsilon': self.epsilon}\n+                  'initial_accumulator_value': self._initial_accumulator_value}\n         base_config = super(Adagrad, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -362,30 +419,26 @@ class Adadelta(Optimizer):\n     at their default values.\n \n     # Arguments\n-        lr: float >= 0. Initial learning rate, defaults to 1.\n+        learning_rate: float >= 0. Initial learning rate, defaults to 1.\n             It is recommended to leave it at the default value.\n         rho: float >= 0. Adadelta decay factor, corresponding to fraction of\n             gradient to keep at each time step.\n-        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n-        decay: float >= 0. Initial learning rate decay.\n \n     # References\n         - [Adadelta - an adaptive learning rate method](\n            https://arxiv.org/abs/1212.5701)\n     \"\"\"\n \n-    def __init__(self, lr=1.0, rho=0.95, epsilon=None, decay=0.,\n-                 **kwargs):\n-        super(Adadelta, self).__init__(**kwargs)\n+    def __init__(self, learning_rate=0.001, rho=0.95, **kwargs):\n+        self.initial_decay = kwargs.pop('decay', 0.0)\n         with K.name_scope(self.__class__.__name__):\n-            self.lr = K.variable(lr, name='lr')\n-            self.decay = K.variable(decay, name='decay')\n+            learning_rate = kwargs.pop('lr', learning_rate)\n+            self.lr = K.variable(learning_rate, name='learning_rate')\n+            self.decay = K.variable(self.initial_decay, name='decay')\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n-        if epsilon is None:\n-            epsilon = K.epsilon()\n         self.rho = rho\n-        self.epsilon = epsilon\n-        self.initial_decay = decay\n+        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n+        super(Adadelta, self).__init__(**kwargs)\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n@@ -393,7 +446,7 @@ class Adadelta(Optimizer):\n         shapes = [K.int_shape(p) for p in params]\n         accumulators = [K.zeros(shape) for shape in shapes]\n         delta_accumulators = [K.zeros(shape) for shape in shapes]\n-        self.weights = accumulators + delta_accumulators\n+        self.weights = [self.iterations] + accumulators + delta_accumulators\n         self.updates = [K.update_add(self.iterations, 1)]\n \n         lr = self.lr\n@@ -421,11 +474,19 @@ class Adadelta(Optimizer):\n             self.updates.append(K.update(d_a, new_d_a))\n         return self.updates\n \n+    def set_weights(self, weights):\n+        params = self.weights\n+        # Override set_weights for backward compatibility of Keras 2.2.4 optimizer\n+        # since it does not include iteration at head of the weight list. Set\n+        # iteration to 0.\n+        if len(params) == len(weights) + 1:\n+            weights = [np.array(0)] + weights\n+        super(Adadelta, self).set_weights(weights)\n+\n     def get_config(self):\n-        config = {'lr': float(K.get_value(self.lr)),\n+        config = {'learning_rate': float(K.get_value(self.lr)),\n                   'rho': self.rho,\n-                  'decay': float(K.get_value(self.decay)),\n-                  'epsilon': self.epsilon}\n+                  'decay': float(K.get_value(self.decay))}\n         base_config = super(Adadelta, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -436,11 +497,9 @@ class Adam(Optimizer):\n     Default parameters follow those provided in the original paper.\n \n     # Arguments\n-        lr: float >= 0. Learning rate.\n+        learning_rate: float >= 0. Learning rate.\n         beta_1: float, 0 < beta < 1. Generally close to 1.\n         beta_2: float, 0 < beta < 1. Generally close to 1.\n-        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n-        decay: float >= 0. Learning rate decay over each update.\n         amsgrad: boolean. Whether to apply the AMSGrad variant of this\n             algorithm from the paper \"On the Convergence of Adam and\n             Beyond\".\n@@ -452,20 +511,19 @@ class Adam(Optimizer):\n            https://openreview.net/forum?id=ryQu7f-RZ)\n     \"\"\"\n \n-    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n-                 epsilon=None, decay=0., amsgrad=False, **kwargs):\n-        super(Adam, self).__init__(**kwargs)\n+    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n+                 amsgrad=False, **kwargs):\n+        self.initial_decay = kwargs.pop('decay', 0.0)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n-            self.lr = K.variable(lr, name='lr')\n+            learning_rate = kwargs.pop('lr', learning_rate)\n+            self.lr = K.variable(learning_rate, name='learning_rate')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n-            self.decay = K.variable(decay, name='decay')\n-        if epsilon is None:\n-            epsilon = K.epsilon()\n-        self.epsilon = epsilon\n-        self.initial_decay = decay\n+            self.decay = K.variable(self.initial_decay, name='decay')\n+        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n         self.amsgrad = amsgrad\n+        super(Adam, self).__init__(**kwargs)\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n@@ -483,39 +541,46 @@ class Adam(Optimizer):\n \n         ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n         vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n+        self.weights = [self.iterations] + ms + vs\n         if self.amsgrad:\n             vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n+            self.weights.extend(vhats)\n         else:\n-            vhats = [K.zeros(1) for _ in params]\n-        self.weights = [self.iterations] + ms + vs + vhats\n-\n+            vhats = [None] * len(params)\n         for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n             m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n             v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n+            self.updates.append(K.update(m, m_t))\n+            self.updates.append(K.update(v, v_t))\n             if self.amsgrad:\n                 vhat_t = K.maximum(vhat, v_t)\n                 p_t = p - lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n                 self.updates.append(K.update(vhat, vhat_t))\n             else:\n                 p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n-\n-            self.updates.append(K.update(m, m_t))\n-            self.updates.append(K.update(v, v_t))\n             new_p = p_t\n-\n             # Apply constraints.\n             if getattr(p, 'constraint', None) is not None:\n                 new_p = p.constraint(new_p)\n-\n             self.updates.append(K.update(p, new_p))\n+\n         return self.updates\n \n+    def set_weights(self, weights):\n+        params = self.weights\n+        # If the weights are generated by Keras V1 optimizer, it includes vhats\n+        # even without amsgrad, i.e, V1 optimizer has 3x + 1 variables, while V2\n+        # optimizer has 2x + 1 variables. Filter vhats out for compatibility.\n+        num_vars = int((len(params) - 1) / 2)\n+        if len(weights) == 3 * num_vars + 1:\n+            weights = weights[:len(params)]\n+        super(Adam, self).set_weights(weights)\n+\n     def get_config(self):\n-        config = {'lr': float(K.get_value(self.lr)),\n+        config = {'learning_rate': float(K.get_value(self.lr)),\n                   'beta_1': float(K.get_value(self.beta_1)),\n                   'beta_2': float(K.get_value(self.beta_2)),\n                   'decay': float(K.get_value(self.decay)),\n-                  'epsilon': self.epsilon,\n                   'amsgrad': self.amsgrad}\n         base_config = super(Adam, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n@@ -528,30 +593,26 @@ class Adamax(Optimizer):\n     Default parameters follow those provided in the paper.\n \n     # Arguments\n-        lr: float >= 0. Learning rate.\n+        learning_rate: float >= 0. Learning rate.\n         beta_1: float, 0 < beta < 1. Generally close to 1.\n         beta_2: float, 0 < beta < 1. Generally close to 1.\n-        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n-        decay: float >= 0. Learning rate decay over each update.\n \n     # References\n         - [Adam - A Method for Stochastic Optimization](\n            https://arxiv.org/abs/1412.6980v8)\n     \"\"\"\n \n-    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n-                 epsilon=None, decay=0., **kwargs):\n-        super(Adamax, self).__init__(**kwargs)\n+    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, **kwargs):\n+        self.initial_decay = kwargs.pop('decay', 0.0)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n-            self.lr = K.variable(lr, name='lr')\n+            learning_rate = kwargs.pop('lr', learning_rate)\n+            self.lr = K.variable(learning_rate, name='learning_rate')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n-            self.decay = K.variable(decay, name='decay')\n-        if epsilon is None:\n-            epsilon = K.epsilon()\n-        self.epsilon = epsilon\n-        self.initial_decay = decay\n+            self.decay = K.variable(self.initial_decay, name='decay')\n+        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n+        super(Adamax, self).__init__(**kwargs)\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n@@ -591,11 +652,10 @@ class Adamax(Optimizer):\n         return self.updates\n \n     def get_config(self):\n-        config = {'lr': float(K.get_value(self.lr)),\n+        config = {'learning_rate': float(K.get_value(self.lr)),\n                   'beta_1': float(K.get_value(self.beta_1)),\n                   'beta_2': float(K.get_value(self.beta_2)),\n-                  'decay': float(K.get_value(self.decay)),\n-                  'epsilon': self.epsilon}\n+                  'decay': float(K.get_value(self.decay))}\n         base_config = super(Adamax, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -611,11 +671,9 @@ class Nadam(Optimizer):\n     at their default values.\n \n     # Arguments\n-        lr: float >= 0. Learning rate.\n+        learning_rate: float >= 0. Learning rate.\n         beta_1: float, 0 < beta < 1. Generally close to 1.\n         beta_2: float, 0 < beta < 1. Generally close to 1.\n-        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n-        schedule_decay: float, 0 < schedule_decay < 1.\n \n     # References\n         - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n@@ -623,19 +681,17 @@ class Nadam(Optimizer):\n            http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n     \"\"\"\n \n-    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n-                 epsilon=None, schedule_decay=0.004, **kwargs):\n-        super(Nadam, self).__init__(**kwargs)\n+    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, **kwargs):\n+        self.schedule_decay = kwargs.pop('schedule_decay', 0.004)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n             self.m_schedule = K.variable(1., name='m_schedule')\n-            self.lr = K.variable(lr, name='lr')\n+            learning_rate = kwargs.pop('lr', learning_rate)\n+            self.lr = K.variable(learning_rate, name='learning_rate')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n-        if epsilon is None:\n-            epsilon = K.epsilon()\n-        self.epsilon = epsilon\n-        self.schedule_decay = schedule_decay\n+        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n+        super(Nadam, self).__init__(**kwargs)\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n@@ -657,7 +713,7 @@ class Nadam(Optimizer):\n         ms = [K.zeros(shape) for shape in shapes]\n         vs = [K.zeros(shape) for shape in shapes]\n \n-        self.weights = [self.iterations] + ms + vs\n+        self.weights = [self.iterations, self.m_schedule] + ms + vs\n \n         for p, g, m, v in zip(params, grads, ms, vs):\n             # the following equations given in [1]\n@@ -682,11 +738,19 @@ class Nadam(Optimizer):\n             self.updates.append(K.update(p, new_p))\n         return self.updates\n \n+    def set_weights(self, weights):\n+        params = self.weights\n+        # Override set_weights for backward compatibility of Keras 2.2.4 optimizer\n+        # since it does not include iteration at head of the weight list. Set\n+        # iteration to 0.\n+        if len(params) == len(weights) + 1:\n+            weights = [weights[0]] + [np.array(1.)] + weights[1:]\n+        super(Nadam, self).set_weights(weights)\n+\n     def get_config(self):\n-        config = {'lr': float(K.get_value(self.lr)),\n+        config = {'learning_rate': float(K.get_value(self.lr)),\n                   'beta_1': float(K.get_value(self.beta_1)),\n                   'beta_2': float(K.get_value(self.beta_2)),\n-                  'epsilon': self.epsilon,\n                   'schedule_decay': self.schedule_decay}\n         base_config = super(Nadam, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n\n@@ -81,35 +81,76 @@ def test_no_grad():\n def test_sgd():\n     sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n     _test_optimizer(sgd)\n+    sgd = optimizers.SGD(learning_rate=0.01, momentum=0.9, decay=0.1, nesterov=True)\n+    _test_optimizer(sgd)\n \n \n def test_rmsprop():\n     _test_optimizer(optimizers.RMSprop())\n     _test_optimizer(optimizers.RMSprop(decay=1e-3))\n+    _test_optimizer(optimizers.RMSprop(learning_rate=0.1, epsilon=0.01, decay=0.1))\n+    _test_optimizer(optimizers.RMSprop(learning_rate=0.1, momentum=0.1))\n+    _test_optimizer(optimizers.RMSprop(learning_rate=0.1, momentum=0.1,\n+                                       centered=True))\n \n \n def test_adagrad():\n-    _test_optimizer(optimizers.Adagrad())\n-    _test_optimizer(optimizers.Adagrad(decay=1e-3))\n+    adagrad = optimizers.Adagrad(lr=0.01)\n+    _test_optimizer(adagrad)\n+    _test_optimizer(optimizers.Adagrad(lr=0.01, decay=1e-3))\n+    new_adagrad = optimizers.Adagrad(learning_rate=0.01,\n+                                     initial_accumulator_value=0.001,\n+                                     epsilon=0.001, decay=0.001)\n+    _test_optimizer(new_adagrad)\n+    # test backward compatibility for adding opt.iterations.\n+    assert len(adagrad.get_weights()) == len(new_adagrad.get_weights())\n+    new_adagrad.set_weights(adagrad.get_weights()[1:])\n+    _test_optimizer(new_adagrad)\n \n \n def test_adadelta():\n-    _test_optimizer(optimizers.Adadelta(), target=0.6)\n-    _test_optimizer(optimizers.Adadelta(decay=1e-3), target=0.6)\n+    adadelta = optimizers.Adadelta(lr=1.0)\n+    _test_optimizer(adadelta, target=0.6)\n+    _test_optimizer(optimizers.Adadelta(lr=1.0, decay=1e-3), target=0.6)\n+    new_adadelta = optimizers.Adadelta(learning_rate=1.0,\n+                                       epsilon=0.001, decay=0.001)\n+    _test_optimizer(new_adadelta, target=0.6)\n+    # test backward compatibility for adding opt.iterations.\n+    assert len(adadelta.get_weights()) == len(new_adadelta.get_weights())\n+    new_adadelta.set_weights(adadelta.get_weights()[1:])\n+    _test_optimizer(new_adadelta, target=0.6)\n \n \n def test_adam():\n-    _test_optimizer(optimizers.Adam())\n+    adam = optimizers.Adam()\n+    _test_optimizer(adam)\n     _test_optimizer(optimizers.Adam(decay=1e-3))\n+    new_adam = optimizers.Adam(learning_rate=0.1, epsilon=0.01, decay=0.1)\n+    _test_optimizer(new_adam)\n+    num_vars = int((len(adam.weights) - 1) / 2)\n+    ms = adam.weights[1:(num_vars + 1)]\n+    adam.weights.extend([K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in ms])\n+    new_adam.set_weights(adam.get_weights())\n+    _test_optimizer(new_adam)\n \n \n def test_adamax():\n-    _test_optimizer(optimizers.Adamax())\n-    _test_optimizer(optimizers.Adamax(decay=1e-3))\n+    _test_optimizer(optimizers.Adamax(lr=0.002))\n+    _test_optimizer(optimizers.Adamax(lr=0.002, decay=1e-3))\n+    _test_optimizer(optimizers.Adamax(learning_rate=0.002, epsilon=0.001,\n+                                      decay=0.001))\n \n \n def test_nadam():\n-    _test_optimizer(optimizers.Nadam())\n+    nadam = optimizers.Nadam()\n+    _test_optimizer(nadam)\n+    new_nadam = optimizers.Nadam(learning_rate=0.1, epsilon=0.01, schedule_decay=0.1)\n+    _test_optimizer(new_nadam)\n+    # test backward compatibility for adding opt.m_schedule.\n+    assert len(nadam.get_weights()) == len(new_nadam.get_weights())\n+    weights = nadam.get_weights()\n+    new_nadam.set_weights([weights[0]] + weights[1:])\n+    _test_optimizer(new_nadam)\n \n \n def test_adam_amsgrad():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
