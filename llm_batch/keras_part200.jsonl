{"custom_id": "keras#4e8fc3325919b6eedb70468e7df353725c54c854", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 101 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 15 | Methods Changed: 10 | Complexity Δ (Sum/Max): 13/8 | Churn Δ: 115 | Churn Cumulative: 4046 | Contributors (this commit): 59 | Commits (past 90d): 16 | Contributors (cumulative): 73 | DMM Complexity: 0.9\n\nDIFF:\n@@ -658,6 +658,12 @@ class TensorBoard(Callback):\n                     if self.write_grads:\n                         grads = model.optimizer.get_gradients(model.total_loss,\n                                                               weight)\n+\n+                        def is_indexed_slices(grad):\n+                            return type(grad).__name__ == 'IndexedSlices'\n+                        grads = [\n+                            grad.values if is_indexed_slices(grad) else grad\n+                            for grad in grads]\n                         tf.summary.histogram('{}_grad'.format(mapped_weight_name), grads)\n                     if self.write_images:\n                         w_img = tf.squeeze(weight)\n@@ -741,6 +747,9 @@ class TensorBoard(Callback):\n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}\n \n+        if not self.validation_data and self.histogram_freq:\n+            raise ValueError(\"If printing histograms, validation_data must be \"\n+                             \"provided, and cannot be a generator.\")\n         if self.validation_data and self.histogram_freq:\n             if epoch % self.histogram_freq == 0:\n \n\n@@ -414,36 +414,112 @@ def test_TensorBoard(tmpdir):\n                   optimizer='sgd',\n                   metrics=['accuracy'])\n \n-    tsb = callbacks.TensorBoard(log_dir=filepath, histogram_freq=1,\n+    # we must generate new callbacks for each test, as they aren't stateless\n+    def callbacks_factory(histogram_freq):\n+        return [callbacks.TensorBoard(log_dir=filepath,\n+                                      histogram_freq=histogram_freq,\n                                       write_images=True, write_grads=True,\n                                       embeddings_freq=1,\n                                       embeddings_layer_names=['dense_1'],\n-                                batch_size=5)\n-    cbks = [tsb]\n+                                      batch_size=5)]\n \n     # fit without validation data\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              callbacks=cbks, epochs=3)\n+              callbacks=callbacks_factory(histogram_freq=0), epochs=3)\n \n     # fit with validation data and accuracy\n     model.fit(X_train, y_train, batch_size=batch_size,\n               validation_data=(X_test, y_test),\n-              callbacks=cbks, epochs=2)\n+              callbacks=callbacks_factory(histogram_freq=0), epochs=2)\n \n     # fit generator without validation data\n     model.fit_generator(data_generator(True), len(X_train), epochs=2,\n-                        callbacks=cbks)\n+                        callbacks=callbacks_factory(histogram_freq=0))\n \n     # fit generator with validation data and accuracy\n     model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                         validation_data=(X_test, y_test),\n-                        callbacks=cbks)\n+                        callbacks=callbacks_factory(histogram_freq=1))\n \n     assert os.path.isdir(filepath)\n     shutil.rmtree(filepath)\n     assert not tmpdir.listdir()\n \n \n+@keras_test\n+@pytest.mark.skipif((K.backend() != 'tensorflow'),\n+                    reason='Requires tensorflow backend')\n+def test_TensorBoard_histogram_freq_must_have_validation_data(tmpdir):\n+    np.random.seed(np.random.randint(1, 1e7))\n+    filepath = str(tmpdir / 'logs')\n+\n+    (X_train, y_train), (X_test, y_test) = get_test_data(\n+        num_train=train_samples,\n+        num_test=test_samples,\n+        input_shape=(input_dim,),\n+        classification=True,\n+        num_classes=num_class)\n+    y_test = np_utils.to_categorical(y_test)\n+    y_train = np_utils.to_categorical(y_train)\n+\n+    def data_generator(train):\n+        if train:\n+            max_batch_index = len(X_train) // batch_size\n+        else:\n+            max_batch_index = len(X_test) // batch_size\n+        i = 0\n+        while 1:\n+            if train:\n+                # simulate multi-input/output models\n+                yield (X_train[i * batch_size: (i + 1) * batch_size],\n+                       y_train[i * batch_size: (i + 1) * batch_size])\n+            else:\n+                yield (X_test[i * batch_size: (i + 1) * batch_size],\n+                       y_test[i * batch_size: (i + 1) * batch_size])\n+            i += 1\n+            i = i % max_batch_index\n+\n+    inp = Input((input_dim,))\n+    hidden = Dense(num_hidden, activation='relu')(inp)\n+    hidden = Dropout(0.1)(hidden)\n+    output = Dense(num_class, activation='softmax')(hidden)\n+    model = Model(inputs=inp, outputs=output)\n+    model.compile(loss='categorical_crossentropy',\n+                  optimizer='sgd',\n+                  metrics=['accuracy'])\n+\n+    # we must generate new callbacks for each test, as they aren't stateless\n+    def callbacks_factory(histogram_freq):\n+        return [callbacks.TensorBoard(log_dir=filepath,\n+                                      histogram_freq=histogram_freq,\n+                                      write_images=True, write_grads=True,\n+                                      embeddings_freq=1,\n+                                      embeddings_layer_names=['dense_1'],\n+                                      batch_size=5)]\n+\n+    # fit without validation data should raise ValueError if histogram_freq > 0\n+    with pytest.raises(ValueError) as raised_exception:\n+        model.fit(X_train, y_train, batch_size=batch_size,\n+                  callbacks=callbacks_factory(histogram_freq=1), epochs=3)\n+    assert 'validation_data must be provided' in str(raised_exception.value)\n+\n+    # fit generator without validation data should raise ValueError if\n+    # histogram_freq > 0\n+    with pytest.raises(ValueError) as raised_exception:\n+        model.fit_generator(data_generator(True), len(X_train), epochs=2,\n+                            callbacks=callbacks_factory(histogram_freq=1))\n+    assert 'validation_data must be provided' in str(raised_exception.value)\n+\n+    # fit generator with validation data generator should raise ValueError if\n+    # histogram_freq > 0\n+    with pytest.raises(ValueError) as raised_exception:\n+        model.fit_generator(data_generator(True), len(X_train), epochs=2,\n+                            validation_data=data_generator(False),\n+                            validation_steps=1,\n+                            callbacks=callbacks_factory(histogram_freq=1))\n+    assert 'validation_data must be provided' in str(raised_exception.value)\n+\n+\n @keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n                     reason='Requires tensorflow backend')\n@@ -489,30 +565,32 @@ def test_TensorBoard_multi_input_output(tmpdir):\n                   optimizer='sgd',\n                   metrics=['accuracy'])\n \n-    tsb = callbacks.TensorBoard(log_dir=filepath, histogram_freq=1,\n+    # we must generate new callbacks for each test, as they aren't stateless\n+    def callbacks_factory(histogram_freq):\n+        return [callbacks.TensorBoard(log_dir=filepath,\n+                                      histogram_freq=histogram_freq,\n                                       write_images=True, write_grads=True,\n                                       embeddings_freq=1,\n                                       embeddings_layer_names=['dense_1'],\n-                                batch_size=5)\n-    cbks = [tsb]\n+                                      batch_size=5)]\n \n     # fit without validation data\n     model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n-              callbacks=cbks, epochs=3)\n+              callbacks=callbacks_factory(histogram_freq=0), epochs=3)\n \n     # fit with validation data and accuracy\n     model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n               validation_data=([X_test] * 2, [y_test] * 2),\n-              callbacks=cbks, epochs=2)\n+              callbacks=callbacks_factory(histogram_freq=1), epochs=2)\n \n     # fit generator without validation data\n     model.fit_generator(data_generator(True), len(X_train), epochs=2,\n-                        callbacks=cbks)\n+                        callbacks=callbacks_factory(histogram_freq=0))\n \n     # fit generator with validation data and accuracy\n     model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                         validation_data=([X_test] * 2, [y_test] * 2),\n-                        callbacks=cbks)\n+                        callbacks=callbacks_factory(histogram_freq=1))\n \n     assert os.path.isdir(filepath)\n     shutil.rmtree(filepath)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca732cd117e042a9f69bdd5738466f51f7d4719c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 139 | Lines Deleted: 35 | Files Changed: 8 | Hunks: 43 | Methods Changed: 14 | Complexity Δ (Sum/Max): 28/18 | Churn Δ: 174 | Churn Cumulative: 3853 | Contributors (this commit): 13 | Commits (past 90d): 31 | Contributors (cumulative): 41 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -1,4 +1,5 @@\n import json\n+import warnings\n \n from ..utils.data_utils import get_file\n from .. import backend as K\n@@ -78,7 +79,8 @@ def _obtain_input_shape(input_shape,\n                         default_size,\n                         min_size,\n                         data_format,\n-                        include_top):\n+                        include_top,\n+                        weights='imagenet'):\n     \"\"\"Internal utility to compute/validate an ImageNet model's input shape.\n \n     # Arguments\n@@ -89,6 +91,9 @@ def _obtain_input_shape(input_shape,\n         data_format: image data format to use.\n         include_top: whether the model is expected to\n             be linked to a classifier via a Flatten layer.\n+        weights: one of `None` (random initialization)\n+            or 'imagenet' (pre-training on ImageNet).\n+            If weights='imagenet' input channels must be equal to 3.\n \n     # Returns\n         An integer shape tuple (may include None entries).\n@@ -96,6 +101,20 @@ def _obtain_input_shape(input_shape,\n     # Raises\n         ValueError: in case of invalid argument values.\n     \"\"\"\n+    if weights != 'imagenet' and input_shape is not None and len(input_shape) == 3:\n+        if data_format == 'channels_first':\n+            if input_shape[0] != 3 or input_shape[0] != 1:\n+                warnings.warn(\n+                    'This model usually expects 1 or 3 input channels. '\n+                    'However, it was passed ' + str(input_shape[0]) + ' input channels.')\n+            default_shape = (input_shape[0], default_size, default_size)\n+        else:\n+            if input_shape[-1] != 3 or input_shape[-1] != 1:\n+                warnings.warn(\n+                    'This model usually expects 1 or 3 input channels. '\n+                    'However, it was passed ' + str(input_shape[-1]) + ' input channels.')\n+            default_shape = (default_size, default_size, input_shape[-1])\n+    else:\n         if data_format == 'channels_first':\n             default_shape = (3, default_size, default_size)\n         else:\n@@ -111,7 +130,7 @@ def _obtain_input_shape(input_shape,\n             if input_shape is not None:\n                 if len(input_shape) != 3:\n                     raise ValueError('`input_shape` must be a tuple of three integers.')\n-                if input_shape[0] != 3:\n+                if input_shape[0] != 3 and weights == 'imagenet':\n                     raise ValueError('The input must have 3 channels; got '\n                                      '`input_shape=' + str(input_shape) + '`')\n                 if ((input_shape[1] is not None and input_shape[1] < min_size) or\n@@ -125,7 +144,7 @@ def _obtain_input_shape(input_shape,\n             if input_shape is not None:\n                 if len(input_shape) != 3:\n                     raise ValueError('`input_shape` must be a tuple of three integers.')\n-                if input_shape[-1] != 3:\n+                if input_shape[-1] != 3 and weights == 'imagenet':\n                     raise ValueError('The input must have 3 channels; got '\n                                      '`input_shape=' + str(input_shape) + '`')\n                 if ((input_shape[0] is not None and input_shape[0] < min_size) or\n\n@@ -92,7 +92,7 @@ def InceptionV3(include_top=True,\n     Optionally loads weights pre-trained\n     on ImageNet. Note that when using TensorFlow,\n     for best performance you should set\n-    `image_data_format=\"channels_last\"` in your Keras config\n+    `image_data_format='channels_last'` in your Keras config\n     at ~/.keras/keras.json.\n     The model and the weights are compatible with both\n     TensorFlow and Theano. The data format\n@@ -104,7 +104,7 @@ def InceptionV3(include_top=True,\n         include_top: whether to include the fully-connected\n             layer at the top of the network.\n         weights: one of `None` (random initialization)\n-            or \"imagenet\" (pre-training on ImageNet).\n+            or 'imagenet' (pre-training on ImageNet).\n         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n             to use as image input for the model.\n         input_shape: optional shape tuple, only to be specified\n@@ -151,7 +151,8 @@ def InceptionV3(include_top=True,\n         default_size=299,\n         min_size=139,\n         data_format=K.image_data_format(),\n-        include_top=include_top)\n+        include_top=include_top,\n+        weights=weights)\n \n     if input_tensor is None:\n         img_input = Input(shape=input_shape)\n\n@@ -110,7 +110,7 @@ class DepthwiseConv2D(Conv2D):\n             all spatial dimensions.\n             Specifying any stride value != 1 is incompatible with specifying\n             any `dilation_rate` value != 1.\n-        padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n+        padding: one of `'valid'` or `'same'` (case-insensitive).\n         depth_multiplier: The number of depthwise convolution output channels\n             for each input channel.\n             The total number of depthwise convolution output\n@@ -124,11 +124,11 @@ class DepthwiseConv2D(Conv2D):\n             `(batch, channels, height, width)`.\n             It defaults to the `image_data_format` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"channels_last\".\n+            If you never set it, then it will be 'channels_last'.\n         activation: Activation function to use\n             (see [activations](../activations.md)).\n             If you don't specify anything, no activation is applied\n-            (ie. \"linear\" activation: `a(x) = x`).\n+            (ie. 'linear' activation: `a(x) = x`).\n         use_bias: Boolean, whether the layer uses a bias vector.\n         depthwise_initializer: Initializer for the depthwise kernel matrix\n             (see [initializers](../initializers.md)).\n@@ -140,7 +140,7 @@ class DepthwiseConv2D(Conv2D):\n         bias_regularizer: Regularizer function applied to the bias vector\n             (see [regularizer](../regularizers.md)).\n         activity_regularizer: Regularizer function applied to\n-            the output of the layer (its \"activation\").\n+            the output of the layer (its 'activation').\n             (see [regularizer](../regularizers.md)).\n         depthwise_constraint: Constraint function applied to\n             the depthwise kernel matrix\n@@ -397,7 +397,9 @@ def MobileNet(input_shape=None,\n                                       default_size=default_size,\n                                       min_size=32,\n                                       data_format=K.image_data_format(),\n-                                      include_top=include_top or weights)\n+                                      include_top=include_top or weights,\n+                                      weights=weights)\n+\n     if K.image_data_format() == 'channels_last':\n         row_axis, col_axis = (0, 1)\n     else:\n\n@@ -130,7 +130,7 @@ def ResNet50(include_top=True, weights='imagenet',\n     Optionally loads weights pre-trained\n     on ImageNet. Note that when using TensorFlow,\n     for best performance you should set\n-    `image_data_format=\"channels_last\"` in your Keras config\n+    `image_data_format='channels_last'` in your Keras config\n     at ~/.keras/keras.json.\n \n     The model and the weights are compatible with both\n@@ -142,7 +142,7 @@ def ResNet50(include_top=True, weights='imagenet',\n         include_top: whether to include the fully-connected\n             layer at the top of the network.\n         weights: one of `None` (random initialization)\n-            or \"imagenet\" (pre-training on ImageNet).\n+            or 'imagenet' (pre-training on ImageNet).\n         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n             to use as image input for the model.\n         input_shape: optional shape tuple, only to be specified\n@@ -188,7 +188,8 @@ def ResNet50(include_top=True, weights='imagenet',\n                                       default_size=224,\n                                       min_size=197,\n                                       data_format=K.image_data_format(),\n-                                      include_top=include_top)\n+                                      include_top=include_top,\n+                                      weights=weights)\n \n     if input_tensor is None:\n         img_input = Input(shape=input_shape)\n\n@@ -41,7 +41,7 @@ def VGG16(include_top=True, weights='imagenet',\n     Optionally loads weights pre-trained\n     on ImageNet. Note that when using TensorFlow,\n     for best performance you should set\n-    `image_data_format=\"channels_last\"` in your Keras config\n+    `image_data_format='channels_last'` in your Keras config\n     at ~/.keras/keras.json.\n \n     The model and the weights are compatible with both\n@@ -53,7 +53,7 @@ def VGG16(include_top=True, weights='imagenet',\n         include_top: whether to include the 3 fully-connected\n             layers at the top of the network.\n         weights: one of `None` (random initialization)\n-            or \"imagenet\" (pre-training on ImageNet).\n+            or 'imagenet' (pre-training on ImageNet).\n         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n             to use as image input for the model.\n         input_shape: optional shape tuple, only to be specified\n@@ -98,7 +98,8 @@ def VGG16(include_top=True, weights='imagenet',\n                                       default_size=224,\n                                       min_size=48,\n                                       data_format=K.image_data_format(),\n-                                      include_top=include_top)\n+                                      include_top=include_top,\n+                                      weights=weights)\n \n     if input_tensor is None:\n         img_input = Input(shape=input_shape)\n\n@@ -41,7 +41,7 @@ def VGG19(include_top=True, weights='imagenet',\n     Optionally loads weights pre-trained\n     on ImageNet. Note that when using TensorFlow,\n     for best performance you should set\n-    `image_data_format=\"channels_last\"` in your Keras config\n+    `image_data_format='channels_last'` in your Keras config\n     at ~/.keras/keras.json.\n \n     The model and the weights are compatible with both\n@@ -53,7 +53,7 @@ def VGG19(include_top=True, weights='imagenet',\n         include_top: whether to include the 3 fully-connected\n             layers at the top of the network.\n         weights: one of `None` (random initialization)\n-            or \"imagenet\" (pre-training on ImageNet).\n+            or 'imagenet' (pre-training on ImageNet).\n         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n             to use as image input for the model.\n         input_shape: optional shape tuple, only to be specified\n@@ -98,7 +98,8 @@ def VGG19(include_top=True, weights='imagenet',\n                                       default_size=224,\n                                       min_size=48,\n                                       data_format=K.image_data_format(),\n-                                      include_top=include_top)\n+                                      include_top=include_top,\n+                                      weights=weights)\n \n     if input_tensor is None:\n         img_input = Input(shape=input_shape)\n\n@@ -54,7 +54,7 @@ def Xception(include_top=True, weights='imagenet',\n     on ImageNet. This model is available for TensorFlow only,\n     and can only be used with inputs following the TensorFlow\n     data format `(width, height, channels)`.\n-    You should set `image_data_format=\"channels_last\"` in your Keras config\n+    You should set `image_data_format='channels_last'` in your Keras config\n     located at ~/.keras/keras.json.\n \n     Note that the default input image size for this model is 299x299.\n@@ -63,7 +63,7 @@ def Xception(include_top=True, weights='imagenet',\n         include_top: whether to include the fully-connected\n             layer at the top of the network.\n         weights: one of `None` (random initialization)\n-            or \"imagenet\" (pre-training on ImageNet).\n+            or 'imagenet' (pre-training on ImageNet).\n         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n             to use as image input for the model.\n         input_shape: optional shape tuple, only to be specified\n@@ -128,7 +128,8 @@ def Xception(include_top=True, weights='imagenet',\n                                       default_size=299,\n                                       min_size=71,\n                                       data_format=K.image_data_format(),\n-                                      include_top=include_top)\n+                                      include_top=include_top,\n+                                      weights=weights)\n \n     if input_tensor is None:\n         img_input = Input(shape=input_shape)\n\n@@ -15,12 +15,25 @@ def test_resnet50():\n \n @keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason=\"cntk does not support padding with non-concrete dimension\")\n+                    reason='cntk does not support padding with non-concrete dimension')\n def test_resnet50_notop():\n     model = applications.ResNet50(weights=None, include_top=False)\n     assert model.output_shape == (None, None, None, 2048)\n \n \n+@keras_test\n+@pytest.mark.skipif((K.backend() == 'cntk'),\n+                    reason='cntk does not support padding with non-concrete dimension')\n+def test_resnet50_variable_input_channels():\n+    input_shape = (1, None, None) if K.image_data_format() == 'channels_first' else (None, None, 1)\n+    model = applications.ResNet50(weights=None, include_top=False, input_shape=input_shape)\n+    assert model.output_shape == (None, None, None, 2048)\n+\n+    input_shape = (4, None, None) if K.image_data_format() == 'channels_first' else (None, None, 4)\n+    model = applications.ResNet50(weights=None, include_top=False, input_shape=input_shape)\n+    assert model.output_shape == (None, None, None, 2048)\n+\n+\n @keras_test\n def test_resnet50_notop_specified_input_shape():\n     input_shape = (3, 300, 300) if K.image_data_format() == 'channels_first' else (300, 300, 3)\n@@ -31,7 +44,7 @@ def test_resnet50_notop_specified_input_shape():\n \n @keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason=\"cntk does not support padding with non-concrete dimension\")\n+                    reason='cntk does not support padding with non-concrete dimension')\n def test_resnet50_pooling():\n     model = applications.ResNet50(weights=None,\n                                   include_top=False,\n@@ -57,12 +70,25 @@ def test_vgg16():\n \n @keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason=\"cntk does not support padding with non-concrete dimension\")\n+                    reason='cntk does not support padding with non-concrete dimension')\n def test_vgg16_notop():\n     model = applications.VGG16(weights=None, include_top=False)\n     assert model.output_shape == (None, None, None, 512)\n \n \n+@keras_test\n+@pytest.mark.skipif((K.backend() == 'cntk'),\n+                    reason='cntk does not support padding with non-concrete dimension')\n+def test_vgg16_variable_input_channels():\n+    input_shape = (1, None, None) if K.image_data_format() == 'channels_first' else (None, None, 1)\n+    model = applications.VGG16(weights=None, include_top=False, input_shape=input_shape)\n+    assert model.output_shape == (None, None, None, 512)\n+\n+    input_shape = (4, None, None) if K.image_data_format() == 'channels_first' else (None, None, 4)\n+    model = applications.VGG16(weights=None, include_top=False, input_shape=input_shape)\n+    assert model.output_shape == (None, None, None, 512)\n+\n+\n @keras_test\n def test_vgg16_notop_specified_input_shape():\n     input_shape = (3, 300, 300) if K.image_data_format() == 'channels_first' else (300, 300, 3)\n@@ -73,7 +99,7 @@ def test_vgg16_notop_specified_input_shape():\n \n @keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason=\"cntk does not support padding with non-concrete dimension\")\n+                    reason='cntk does not support padding with non-concrete dimension')\n def test_vgg16_pooling():\n     model = applications.VGG16(weights=None, include_top=False, pooling='avg')\n     assert model.output_shape == (None, 512)\n@@ -94,12 +120,25 @@ def test_vgg19():\n \n @keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason=\"cntk does not support padding with non-concrete dimension\")\n+                    reason='cntk does not support padding with non-concrete dimension')\n def test_vgg19_notop():\n     model = applications.VGG19(weights=None, include_top=False)\n     assert model.output_shape == (None, None, None, 512)\n \n \n+@keras_test\n+@pytest.mark.skipif((K.backend() == 'cntk'),\n+                    reason='cntk does not support padding with non-concrete dimension')\n+def test_vgg19_variable_input_channels():\n+    input_shape = (1, None, None) if K.image_data_format() == 'channels_first' else (None, None, 1)\n+    model = applications.VGG19(weights=None, include_top=False, input_shape=input_shape)\n+    assert model.output_shape == (None, None, None, 512)\n+\n+    input_shape = (4, None, None) if K.image_data_format() == 'channels_first' else (None, None, 4)\n+    model = applications.VGG19(weights=None, include_top=False, input_shape=input_shape)\n+    assert model.output_shape == (None, None, None, 512)\n+\n+\n @keras_test\n def test_vgg19_notop_specified_input_shape():\n     input_shape = (3, 300, 300) if K.image_data_format() == 'channels_first' else (300, 300, 3)\n@@ -110,7 +149,7 @@ def test_vgg19_notop_specified_input_shape():\n \n @keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason=\"cntk does not support padding with non-concrete dimension\")\n+                    reason='cntk does not support padding with non-concrete dimension')\n def test_vgg19_pooling():\n     model = applications.VGG16(weights=None, include_top=False, pooling='avg')\n     assert model.output_shape == (None, 512)\n@@ -147,6 +186,19 @@ def test_xception_pooling():\n     assert model.output_shape == (None, 2048)\n \n \n+@keras_test\n+@pytest.mark.skipif((K.backend() != 'tensorflow'),\n+                    reason='Requires tensorflow backend')\n+def test_xception_variable_input_channels():\n+    input_shape = (1, None, None) if K.image_data_format() == 'channels_first' else (None, None, 1)\n+    model = applications.Xception(weights=None, include_top=False, input_shape=input_shape)\n+    assert model.output_shape == (None, None, None, 2048)\n+\n+    input_shape = (4, None, None) if K.image_data_format() == 'channels_first' else (None, None, 4)\n+    model = applications.Xception(weights=None, include_top=False, input_shape=input_shape)\n+    assert model.output_shape == (None, None, None, 2048)\n+\n+\n @keras_test\n def test_inceptionv3():\n     model = applications.InceptionV3(weights=None)\n@@ -155,7 +207,7 @@ def test_inceptionv3():\n \n @keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason=\"cntk does not support padding with non-concrete dimension\")\n+                    reason='cntk does not support padding with non-concrete dimension')\n def test_inceptionv3_notop():\n     model = applications.InceptionV3(weights=None, include_top=False)\n     assert model.output_shape == (None, None, None, 2048)\n@@ -163,15 +215,28 @@ def test_inceptionv3_notop():\n \n @keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason=\"cntk does not support padding with non-concrete dimension\")\n+                    reason='cntk does not support padding with non-concrete dimension')\n def test_inceptionv3_pooling():\n     model = applications.InceptionV3(weights=None, include_top=False, pooling='avg')\n     assert model.output_shape == (None, 2048)\n \n \n+@keras_test\n+@pytest.mark.skipif((K.backend() == 'cntk'),\n+                    reason='cntk does not support padding with non-concrete dimension')\n+def test_inceptionv3_variable_input_channels():\n+    input_shape = (1, None, None) if K.image_data_format() == 'channels_first' else (None, None, 1)\n+    model = applications.InceptionV3(weights=None, include_top=False, input_shape=input_shape)\n+    assert model.output_shape == (None, None, None, 2048)\n+\n+    input_shape = (4, None, None) if K.image_data_format() == 'channels_first' else (None, None, 4)\n+    model = applications.InceptionV3(weights=None, include_top=False, input_shape=input_shape)\n+    assert model.output_shape == (None, None, None, 2048)\n+\n+\n @keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason=\"MobileNets are supported only on TensorFlow\")\n+                    reason='MobileNets are supported only on TensorFlow')\n def test_mobilenet():\n     model = applications.MobileNet(weights=None)\n     assert model.output_shape == (None, 1000)\n@@ -179,7 +244,7 @@ def test_mobilenet():\n \n @keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason=\"MobileNets are supported only on TensorFlow\")\n+                    reason='MobileNets are supported only on TensorFlow')\n def test_mobilenet_no_top():\n     model = applications.MobileNet(weights=None, include_top=False)\n     assert model.output_shape == (None, None, None, 1024)\n@@ -187,7 +252,7 @@ def test_mobilenet_no_top():\n \n @keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason=\"MobileNets are supported only on TensorFlow\")\n+                    reason='MobileNets are supported only on TensorFlow')\n def test_mobilenet_pooling():\n     model = applications.MobileNet(weights=None, include_top=False, pooling='avg')\n     assert model.output_shape == (None, 1024)\n@@ -195,7 +260,20 @@ def test_mobilenet_pooling():\n \n @keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason=\"MobileNets are supported only on TensorFlow\")\n+                    reason='MobileNets are supported only on TensorFlow')\n+def test_mobilenet_variable_input_channels():\n+    input_shape = (1, None, None) if K.image_data_format() == 'channels_first' else (None, None, 1)\n+    model = applications.MobileNet(weights=None, include_top=False, input_shape=input_shape)\n+    assert model.output_shape == (None, None, None, 1024)\n+\n+    input_shape = (4, None, None) if K.image_data_format() == 'channels_first' else (None, None, 4)\n+    model = applications.MobileNet(weights=None, include_top=False, input_shape=input_shape)\n+    assert model.output_shape == (None, None, None, 1024)\n+\n+\n+@keras_test\n+@pytest.mark.skipif((K.backend() != 'tensorflow'),\n+                    reason='MobileNets are supported only on TensorFlow')\n def test_mobilenet_image_size():\n     valid_image_sizes = [128, 160, 192, 224]\n     for size in valid_image_sizes:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1bbd52c7081a842fb4af7f582c44de0d2ba644e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 154 | Lines Deleted: 50 | Files Changed: 3 | Hunks: 22 | Methods Changed: 10 | Complexity Δ (Sum/Max): 3/4 | Churn Δ: 204 | Churn Cumulative: 15019 | Contributors (this commit): 117 | Commits (past 90d): 39 | Contributors (cumulative): 139 | DMM Complexity: 0.881578947368421\n\nDIFF:\n@@ -465,46 +465,6 @@ def _weighted_masked_objective(fn):\n     return weighted\n \n \n-def _masked_objective(fn):\n-    \"\"\"Adds support for masking to an objective function.\n-\n-    It transforms an objective function `fn(y_true, y_pred)`\n-    into a cost-masked objective function\n-    `fn(y_true, y_pred, mask)`.\n-\n-    # Arguments\n-        fn: The objective function to wrap,\n-            with signature `fn(y_true, y_pred)`.\n-\n-    # Returns\n-        A function with signature `fn(y_true, y_pred, mask)`.\n-    \"\"\"\n-    def masked(y_true, y_pred, mask=None):\n-        \"\"\"Wrapper function.\n-\n-        # Arguments\n-            y_true: `y_true` argument of `fn`.\n-            y_pred: `y_pred` argument of `fn`.\n-            mask: Mask tensor.\n-\n-        # Returns\n-            Scalar tensor.\n-        \"\"\"\n-        # score_array has ndim >= 2\n-        score_array = fn(y_true, y_pred)\n-        if mask is not None:\n-            # Cast the mask to floatX to avoid float64 upcasting in theano\n-            mask = K.cast(mask, K.floatx())\n-            # mask should have the same shape as score_array\n-            score_array *= mask\n-            #  the loss per batch should be proportional\n-            #  to the number of unmasked samples.\n-            score_array /= K.mean(mask)\n-\n-        return K.mean(score_array)\n-    return masked\n-\n-\n def _standardize_weights(y, sample_weight=None, class_weight=None,\n                          sample_weight_mode=None):\n     \"\"\"Performs sample weight validation and standardization.\n@@ -604,7 +564,7 @@ class Model(Container):\n     \"\"\"\n \n     def compile(self, optimizer, loss, metrics=None, loss_weights=None,\n-                sample_weight_mode=None, **kwargs):\n+                sample_weight_mode=None, weighted_metrics=None, **kwargs):\n         \"\"\"Configures the model for training.\n \n         # Arguments\n@@ -637,6 +597,8 @@ class Model(Container):\n                 If the model has multiple outputs, you can use a different\n                 `sample_weight_mode` on each output by passing a\n                 dictionary or a list of modes.\n+            weighted_metrics: list of metrics to be evaluated and weighted\n+                by sample_weight or class_weight during training and testing\n             **kwargs: when using the Theano/CNTK backends, these arguments\n                 are passed into K.function. When using the TensorFlow backend,\n                 these arguments are passed into `tf.Session.run`.\n@@ -822,6 +784,7 @@ class Model(Container):\n \n         # Prepare metrics.\n         self.metrics = metrics\n+        self.weighted_metrics = weighted_metrics\n         self.metrics_names = ['loss']\n         self.metrics_tensors = []\n \n@@ -860,6 +823,7 @@ class Model(Container):\n         # List of same size as output_names.\n         # contains tuples (metrics for output, names of metrics).\n         nested_metrics = _collect_metrics(metrics, self.output_names)\n+        nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n \n         def append_metric(layer_num, metric_name, metric_tensor):\n             \"\"\"Helper function used in loop below.\"\"\"\n@@ -871,15 +835,21 @@ class Model(Container):\n         for i in range(len(self.outputs)):\n             if i in skip_indices:\n                 continue\n+\n             y_true = self.targets[i]\n             y_pred = self.outputs[i]\n+            weights = sample_weights[i]\n             output_metrics = nested_metrics[i]\n-            for metric in output_metrics:\n+            output_weighted_metrics = nested_weighted_metrics[i]\n+\n+            def handle_metrics(metrics, weights=None):\n+                metric_name_prefix = 'weighted_' if weights is not None else ''\n+\n+                for metric in metrics:\n                     if metric == 'accuracy' or metric == 'acc':\n                         # custom handling of accuracy\n                         # (because of class mode duality)\n                         output_shape = self.internal_output_shapes[i]\n-                    acc_fn = None\n                         if (output_shape[-1] == 1 or\n                            self.loss_functions[i] == losses.binary_crossentropy):\n                             # case: binary accuracy\n@@ -890,17 +860,21 @@ class Model(Container):\n                         else:\n                             acc_fn = metrics_module.categorical_accuracy\n \n-                    masked_fn = _masked_objective(acc_fn)\n-                    append_metric(i, 'acc', masked_fn(y_true, y_pred, mask=masks[i]))\n+                        acc_fn = _weighted_masked_objective(acc_fn)\n+                        metric_name = metric_name_prefix + 'acc'\n+                        append_metric(i, metric_name, acc_fn(y_true, y_pred, weights=weights, mask=masks[i]))\n                     else:\n                         metric_fn = metrics_module.get(metric)\n-                    masked_metric_fn = _masked_objective(metric_fn)\n-                    metric_result = masked_metric_fn(y_true, y_pred, mask=masks[i])\n+                        weighted_metric_fn = _weighted_masked_objective(metric_fn)\n+                        metric_result = weighted_metric_fn(y_true, y_pred, weights=weights, mask=masks[i])\n                         metric_result = {\n                             metric_fn.__name__: metric_result\n                         }\n                         for name, tensor in six.iteritems(metric_result):\n-                        append_metric(i, name, tensor)\n+                            append_metric(i, metric_name_prefix + name, tensor)\n+\n+            handle_metrics(output_metrics)\n+            handle_metrics(output_weighted_metrics, weights=weights)\n \n         # Prepare gradient updates and state updates.\n         self.total_loss = total_loss\n\n@@ -745,6 +745,7 @@ class Sequential(Model):\n     def compile(self, optimizer, loss,\n                 metrics=None,\n                 sample_weight_mode=None,\n+                weighted_metrics=None,\n                 **kwargs):\n         \"\"\"Configures the learning process.\n \n@@ -760,6 +761,8 @@ class Sequential(Model):\n             sample_weight_mode: if you need to do timestep-wise\n                 sample weighting (2D weights), set this to \"temporal\".\n                 \"None\" defaults to sample-wise weights (1D).\n+            weighted_metrics: list of metrics to be evaluated and weighted\n+                by sample_weight or class_weight during training and testing\n             **kwargs: for Theano/CNTK backends, these are passed into\n                 K.function. When using the TensorFlow backend, these are\n                 passed into `tf.Session.run`.\n@@ -780,12 +783,14 @@ class Sequential(Model):\n         self.model.compile(optimizer, loss,\n                            metrics=metrics,\n                            sample_weight_mode=sample_weight_mode,\n+                           weighted_metrics=weighted_metrics,\n                            **kwargs)\n         self.optimizer = self.model.optimizer\n         self.loss = self.model.loss\n         self.total_loss = self.model.total_loss\n         self.loss_weights = self.model.loss_weights\n         self.metrics = self.model.metrics\n+        self.weighted_metrics = self.model.weighted_metrics\n         self.metrics_tensors = self.model.metrics_tensors\n         self.metrics_names = self.model.metrics_names\n         self.sample_weight_mode = self.model.sample_weight_mode\n\n@@ -3,11 +3,13 @@ from __future__ import print_function\n import pytest\n import numpy as np\n \n+from keras import backend as K\n from keras.utils.test_utils import get_test_data\n-from keras.models import Sequential\n-from keras.layers import Dense, Activation, GRU, TimeDistributed\n+from keras.models import Sequential, Model\n+from keras.layers import Dense, Activation, GRU, TimeDistributed, Input\n from keras.utils import np_utils\n from keras.utils.test_utils import keras_test\n+from numpy.testing import assert_almost_equal, assert_array_almost_equal\n \n num_classes = 10\n batch_size = 128\n@@ -19,9 +21,16 @@ test_samples = 1000\n timesteps = 3\n input_dim = 10\n loss = 'mse'\n+loss_full_name = 'mean_squared_error'\n standard_weight = 1\n standard_score_sequential = 0.5\n \n+decimal_precision = {\n+    'cntk': 2,\n+    'theano': 6,\n+    'tensorflow': 6\n+}\n+\n \n def _get_test_data():\n     np.random.seed(1337)\n@@ -148,6 +157,122 @@ def test_sequential_temporal_sample_weights():\n     assert(score < standard_score_sequential)\n \n \n+@keras_test\n+def test_weighted_metrics_with_sample_weight():\n+    decimal = decimal_precision[K.backend()]\n+\n+    model = create_sequential_model()\n+    model.compile(loss=loss, optimizer='rmsprop', metrics=[loss], weighted_metrics=[loss])\n+\n+    (x_train, y_train), (x_test, y_test), (sample_weight, class_weight, test_ids) = _get_test_data()\n+\n+    history = model.fit(x_train, y_train, batch_size=batch_size,\n+                        epochs=epochs // 3, verbose=0,\n+                        sample_weight=sample_weight)\n+\n+    h = history.history\n+    assert_array_almost_equal(h['loss'], h['weighted_' + loss_full_name], decimal=decimal)\n+\n+    history = model.fit(x_train, y_train, batch_size=batch_size,\n+                        epochs=epochs // 3, verbose=0,\n+                        sample_weight=sample_weight,\n+                        validation_split=0.1)\n+\n+    h = history.history\n+    assert_almost_equal(h['val_loss'], h['val_weighted_' + loss_full_name], decimal=decimal)\n+\n+    model.train_on_batch(x_train[:32], y_train[:32],\n+                         sample_weight=sample_weight[:32])\n+    model.test_on_batch(x_train[:32], y_train[:32],\n+                        sample_weight=sample_weight[:32])\n+\n+    test_sample_weight = np.ones((y_test.shape[0])) * standard_weight\n+    test_sample_weight[test_ids] = high_weight\n+\n+    scores = model.evaluate(x_test, y_test, verbose=0, sample_weight=test_sample_weight)\n+    loss_score, metric_score, weighted_metric_score = scores\n+\n+    assert loss_score < standard_score_sequential\n+    assert loss_score != metric_score\n+    assert_almost_equal(loss_score, weighted_metric_score, decimal=decimal)\n+\n+\n+@keras_test\n+def test_weighted_metrics_with_no_sample_weight():\n+    decimal = decimal_precision[K.backend()]\n+\n+    model = create_sequential_model()\n+    model.compile(loss=loss, optimizer='rmsprop', metrics=[loss], weighted_metrics=[loss])\n+\n+    (x_train, y_train), (x_test, y_test), _ = _get_test_data()\n+\n+    history = model.fit(x_train, y_train, batch_size=batch_size,\n+                        epochs=epochs // 3, verbose=0)\n+\n+    h = history.history\n+    assert_array_almost_equal(h['loss'], h[loss_full_name], decimal=decimal)\n+    assert_array_almost_equal(h['loss'], h['weighted_' + loss_full_name], decimal=decimal)\n+\n+    history = model.fit(x_train, y_train, batch_size=batch_size,\n+                        epochs=epochs // 3, verbose=0, validation_split=0.1)\n+\n+    h = history.history\n+    assert_array_almost_equal(h['val_loss'], h['val_' + loss_full_name], decimal=decimal)\n+    assert_array_almost_equal(h['val_loss'], h['val_weighted_' + loss_full_name], decimal=decimal)\n+\n+    model.train_on_batch(x_train[:32], y_train[:32])\n+    model.test_on_batch(x_train[:32], y_train[:32])\n+\n+    scores = model.evaluate(x_test, y_test, verbose=0)\n+    loss_score, metric_score, weighted_metric_score = scores\n+\n+    assert_almost_equal(loss_score, metric_score, decimal=decimal)\n+    assert_almost_equal(loss_score, weighted_metric_score, decimal=decimal)\n+\n+\n+@keras_test\n+def test_weighted_metrics_with_weighted_accuracy_metric():\n+    model = create_sequential_model()\n+    model.compile(loss=loss, optimizer='rmsprop', metrics=['acc'], weighted_metrics=['acc'])\n+\n+    (x_train, y_train), _, (sample_weight, _, _) = _get_test_data()\n+\n+    history = model.fit(x_train, y_train, batch_size=batch_size,\n+                        epochs=epochs // 3, verbose=0,\n+                        sample_weight=sample_weight)\n+\n+    assert history.history['acc'] != history.history['weighted_acc']\n+\n+\n+@keras_test\n+def test_weighted_metrics_with_multiple_outputs():\n+    decimal = decimal_precision[K.backend()]\n+\n+    inputs = Input(shape=(5,))\n+    x = Dense(5)(inputs)\n+    output1 = Dense(1, name='output1')(x)\n+    output2 = Dense(1, name='output2')(x)\n+\n+    model = Model(inputs=inputs, outputs=[output1, output2])\n+\n+    metrics = {'output1': [loss], 'output2': [loss]}\n+    weighted_metrics = {'output2': [loss]}\n+    loss_map = {'output1': loss, 'output2': loss}\n+\n+    model.compile(loss=loss_map, optimizer='sgd', metrics=metrics, weighted_metrics=weighted_metrics)\n+\n+    x = np.array([[1, 1, 1, 1, 1]])\n+    y = {'output1': np.array([0]), 'output2': np.array([1])}\n+    weight = 5\n+\n+    history = model.fit(x, y, sample_weight={'output2': np.array([weight])})\n+\n+    unweighted_metric = history.history['output2_' + loss_full_name][0]\n+    weighted_metric = history.history['output2_weighted_' + loss_full_name][0]\n+\n+    assert_almost_equal(unweighted_metric * weight, weighted_metric, decimal=decimal)\n+\n+\n @keras_test\n def test_class_weight_wrong_classes():\n     model = create_sequential_model()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2afe0b0054694045aff3dd7e263b5c44f04c323a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 20 | Files Changed: 3 | Hunks: 32 | Methods Changed: 14 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 56 | Churn Cumulative: 16310 | Contributors (this commit): 128 | Commits (past 90d): 44 | Contributors (cumulative): 157 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -790,6 +790,7 @@ class Model(Container):\n \n         # Compute total loss.\n         total_loss = None\n+        with K.name_scope('loss'):\n             for i in range(len(self.outputs)):\n                 if i in skip_indices:\n                     continue\n@@ -799,6 +800,7 @@ class Model(Container):\n                 sample_weight = sample_weights[i]\n                 mask = masks[i]\n                 loss_weight = loss_weights_list[i]\n+                with K.name_scope(self.output_names[i] + '_loss'):\n                     output_loss = weighted_loss(y_true, y_pred,\n                                                 sample_weight, mask)\n                 if len(self.outputs) > 1:\n@@ -825,13 +827,14 @@ class Model(Container):\n         nested_metrics = _collect_metrics(metrics, self.output_names)\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n \n-        def append_metric(layer_num, metric_name, metric_tensor):\n+        def append_metric(layer_index, metric_name, metric_tensor):\n             \"\"\"Helper function used in loop below.\"\"\"\n             if len(self.output_names) > 1:\n-                metric_name = self.output_layers[layer_num].name + '_' + metric_name\n+                metric_name = self.output_layers[layer_index].name + '_' + metric_name\n             self.metrics_names.append(metric_name)\n             self.metrics_tensors.append(metric_tensor)\n \n+        with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_indices:\n                     continue\n@@ -860,18 +863,18 @@ class Model(Container):\n                             else:\n                                 acc_fn = metrics_module.categorical_accuracy\n \n-                        acc_fn = _weighted_masked_objective(acc_fn)\n+                            weighted_metric_fn = _weighted_masked_objective(acc_fn)\n                             metric_name = metric_name_prefix + 'acc'\n-                        append_metric(i, metric_name, acc_fn(y_true, y_pred, weights=weights, mask=masks[i]))\n                         else:\n                             metric_fn = metrics_module.get(metric)\n                             weighted_metric_fn = _weighted_masked_objective(metric_fn)\n-                        metric_result = weighted_metric_fn(y_true, y_pred, weights=weights, mask=masks[i])\n-                        metric_result = {\n-                            metric_fn.__name__: metric_result\n-                        }\n-                        for name, tensor in six.iteritems(metric_result):\n-                            append_metric(i, metric_name_prefix + name, tensor)\n+                            metric_name = metric_name_prefix + metric_fn.__name__\n+\n+                        with K.name_scope(metric_name):\n+                            metric_result = weighted_metric_fn(y_true, y_pred,\n+                                                               weights=weights,\n+                                                               mask=masks[i])\n+                        append_metric(i, metric_name, metric_result)\n \n                 handle_metrics(output_metrics)\n                 handle_metrics(output_weighted_metrics, weights=weights)\n@@ -905,6 +908,8 @@ class Model(Container):\n             if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n                 inputs += [K.learning_phase()]\n \n+            with K.name_scope('training'):\n+                with K.name_scope(self.optimizer.__class__.__name__):\n                     training_updates = self.optimizer.get_updates(\n                         params=self._collected_trainable_weights,\n                         loss=self.total_loss)\n\n@@ -149,7 +149,10 @@ def save_model(model, filepath, overwrite=True, include_optimizer=True):\n                         # Default values of symbolic_weights is /variable\n                         # for theano and cntk\n                         if K.backend() == 'theano' or K.backend() == 'cntk':\n-                            if hasattr(w, 'name') and w.name != \"/variable\":\n+                            if hasattr(w, 'name'):\n+                                if w.name.split('/')[-1] == 'variable':\n+                                    name = str(w.name) + '_' + str(i)\n+                                else:\n                                     name = str(w.name)\n                             else:\n                                 name = 'param_' + str(i)\n\n@@ -143,6 +143,7 @@ class SGD(Optimizer):\n     def __init__(self, lr=0.01, momentum=0., decay=0.,\n                  nesterov=False, **kwargs):\n         super(SGD, self).__init__(**kwargs)\n+        with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0., name='iterations')\n             self.lr = K.variable(lr, name='lr')\n             self.momentum = K.variable(momentum, name='momentum')\n@@ -212,12 +213,13 @@ class RMSprop(Optimizer):\n     def __init__(self, lr=0.001, rho=0.9, epsilon=1e-8, decay=0.,\n                  **kwargs):\n         super(RMSprop, self).__init__(**kwargs)\n+        with K.name_scope(self.__class__.__name__):\n             self.lr = K.variable(lr, name='lr')\n             self.rho = K.variable(rho, name='rho')\n-        self.epsilon = epsilon\n             self.decay = K.variable(decay, name='decay')\n-        self.initial_decay = decay\n             self.iterations = K.variable(0., name='iterations')\n+        self.epsilon = epsilon\n+        self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n@@ -270,11 +272,12 @@ class Adagrad(Optimizer):\n \n     def __init__(self, lr=0.01, epsilon=1e-8, decay=0., **kwargs):\n         super(Adagrad, self).__init__(**kwargs)\n+        with K.name_scope(self.__class__.__name__):\n             self.lr = K.variable(lr, name='lr')\n-        self.epsilon = epsilon\n             self.decay = K.variable(decay, name='decay')\n-        self.initial_decay = decay\n             self.iterations = K.variable(0., name='iterations')\n+        self.epsilon = epsilon\n+        self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n@@ -329,12 +332,13 @@ class Adadelta(Optimizer):\n     def __init__(self, lr=1.0, rho=0.95, epsilon=1e-8, decay=0.,\n                  **kwargs):\n         super(Adadelta, self).__init__(**kwargs)\n+        with K.name_scope(self.__class__.__name__):\n             self.lr = K.variable(lr, name='lr')\n+            self.decay = K.variable(decay, name='decay')\n+            self.iterations = K.variable(0., name='iterations')\n         self.rho = rho\n         self.epsilon = epsilon\n-        self.decay = K.variable(decay, name='decay')\n         self.initial_decay = decay\n-        self.iterations = K.variable(0., name='iterations')\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n@@ -398,12 +402,13 @@ class Adam(Optimizer):\n     def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n                  epsilon=1e-8, decay=0., **kwargs):\n         super(Adam, self).__init__(**kwargs)\n+        with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, name='iterations')\n             self.lr = K.variable(lr, name='lr')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n-        self.epsilon = epsilon\n             self.decay = K.variable(decay, name='decay')\n+        self.epsilon = epsilon\n         self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n@@ -468,12 +473,13 @@ class Adamax(Optimizer):\n     def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n                  epsilon=1e-8, decay=0., **kwargs):\n         super(Adamax, self).__init__(**kwargs)\n+        with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0., name='iterations')\n             self.lr = K.variable(lr, name='lr')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n-        self.epsilon = epsilon\n             self.decay = K.variable(decay, name='decay')\n+        self.epsilon = epsilon\n         self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n@@ -545,6 +551,7 @@ class Nadam(Optimizer):\n     def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n                  epsilon=1e-8, schedule_decay=0.004, **kwargs):\n         super(Nadam, self).__init__(**kwargs)\n+        with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0., name='iterations')\n             self.m_schedule = K.variable(1., name='m_schedule')\n             self.lr = K.variable(lr, name='lr')\n@@ -611,8 +618,9 @@ class TFOptimizer(Optimizer):\n \n     def __init__(self, optimizer):\n         self.optimizer = optimizer\n-        self.iterations = K.variable(0., name='iterations')\n         self.updates = []\n+        with K.name_scope(self.__class__.__name__):\n+            self.iterations = K.variable(0., name='iterations')\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
