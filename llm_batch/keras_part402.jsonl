{"custom_id": "keras#0e1af5c13a8c40c83b967e81f4c6b6ced35c94ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 65 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 71 | Churn Cumulative: 1743 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -304,6 +304,7 @@ class MetricsContainer(Container):\n     \"\"\"\n     super(MetricsContainer, self).__init__(output_names=output_names)\n \n+    self._check_duplicated_metrics(metrics, weighted_metrics)\n     # Keep user-supplied values untouched for recompiling and serialization.\n     self._user_metrics = metrics\n     self._user_weighted_metrics = weighted_metrics\n@@ -314,6 +315,39 @@ class MetricsContainer(Container):\n \n     self._from_serialized = from_serialized\n \n+  def _check_duplicated_metrics(self, metrics, weighted_metrics):\n+    \"\"\"Check and raise error when user provided metrics has any duplications.\n+\n+    Note that metrics are stateful container, a shared metric instance between\n+    model.metric and model.weighted_metric will make the same intance to be\n+    udpated twice, and report wrong value.\n+\n+    Args:\n+      metrics: User provided metrics list.\n+      weighted_metrics: User provided weighted metrics list.\n+\n+    Raises:\n+      ValueError, when duplicated metrics instance discovered in user provided\n+        metrics and weighted metrics.\n+    \"\"\"\n+    seen = set()\n+    duplicated = []\n+    for x in tf.nest.flatten(metrics) + tf.nest.flatten(weighted_metrics):\n+      # We only check metrics object. The string and function objects\n+      # will be converted to unique Metric instance.\n+      if not isinstance(x, metrics_mod.Metric):\n+        continue\n+      if x in seen:\n+        duplicated.append(x)\n+      seen.add(x)\n+\n+    if duplicated:\n+      raise ValueError('Found duplicated metrics object in the user provided '\n+                       'metrics and weighted metrics. This will cause the same '\n+                       'metric object to be updated multiple times, and report '\n+                       'wrong results. \\n'\n+                       f'Duplicated items: {duplicated}')\n+\n   @property\n   def metrics(self):\n     \"\"\"All metrics in this container.\"\"\"\n@@ -351,18 +385,26 @@ class MetricsContainer(Container):\n     y_pred = tf.__internal__.nest.list_to_tuple(y_pred)\n     y_true = tf.__internal__.nest.list_to_tuple(y_true)\n     self._metrics = tf.__internal__.nest.list_to_tuple(self._metrics)\n-    self._weighted_metrics = tf.__internal__.nest.list_to_tuple(self._weighted_metrics)\n+    self._weighted_metrics = tf.__internal__.nest.list_to_tuple(\n+        self._weighted_metrics)\n \n     # Convert to `Metric` objects, potentially disambiguating based on output\n     # properties.\n-    self._metrics = tf.__internal__.nest.map_structure_up_to(y_pred, self._get_metric_objects,\n-                                             self._metrics, y_true, y_pred)\n-    self._weighted_metrics = tf.__internal__.nest.map_structure_up_to(y_pred,\n+    self._metrics = tf.__internal__.nest.map_structure_up_to(\n+        y_pred,\n+        self._get_metric_objects,\n+        self._metrics,\n+        y_true,\n+        y_pred)\n+    self._weighted_metrics = tf.__internal__.nest.map_structure_up_to(\n+        y_pred,\n         self._get_metric_objects,\n         self._weighted_metrics,\n-                                                      y_true, y_pred)\n+        y_true,\n+        y_pred)\n \n-    self._metrics = tf.__internal__.nest.flatten_up_to(y_pred, self._metrics, check_types=False)\n+    self._metrics = tf.__internal__.nest.flatten_up_to(\n+        y_pred, self._metrics, check_types=False)\n     self._weighted_metrics = tf.__internal__.nest.flatten_up_to(\n         y_pred, self._weighted_metrics, check_types=False)\n \n\n@@ -799,6 +799,23 @@ class MetricsContainerTest(keras_parameterized.TestCase):\n     metric_container.reset_state()\n     self.assertEqual(metric.result().numpy(), 0.0)\n \n+  def test_duplicated_metric_instance(self):\n+    mean_obj = metrics_mod.Mean()\n+    metric = mean_obj\n+    with self.assertRaisesRegex(ValueError, 'Found duplicated metrics'):\n+      compile_utils.MetricsContainer(metrics=metric, weighted_metrics=metric)\n+\n+    # duplicated string should be fine\n+    metric = 'acc'\n+    compile_utils.MetricsContainer(metrics=metric, weighted_metrics=metric)\n+\n+    # complicated structure\n+    metric = [mean_obj, 'acc']\n+    weighted_metric = {'output1': mean_obj, 'output2': 'acc'}\n+    with self.assertRaisesRegex(ValueError, 'Found duplicated metrics'):\n+      compile_utils.MetricsContainer(\n+          metrics=metric, weighted_metrics=weighted_metric)\n+\n \n if __name__ == '__main__':\n   tf.compat.v1.enable_eager_execution()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1900977512f7b5ab949602bbb38f648223829941", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 13 | Files Changed: 3 | Hunks: 15 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 33 | Churn Cumulative: 28408 | Contributors (this commit): 197 | Commits (past 90d): 45 | Contributors (cumulative): 233 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1131,7 +1131,7 @@ class History(Callback):\n   >>> model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n   >>> model.compile(tf.keras.optimizers.SGD(), loss='mse')\n   >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n-  ...                     epochs=10)\n+  ...                     epochs=10, verbose=1)\n   >>> print(history.params)\n   {'verbose': 1, 'epochs': 10, 'steps': 1}\n   >>> # check the keys of history object\n\n@@ -14,15 +14,13 @@\n # ==============================================================================\n \"\"\"Training-related part of the Keras engine.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n import copy\n import itertools\n import json\n import os\n import warnings\n import weakref\n-from tensorflow.python.eager import context\n+\n from keras import backend\n from keras import callbacks as callbacks_module\n from keras import optimizer_v1\n@@ -36,25 +34,26 @@ from keras.mixed_precision import loss_scale_optimizer as lso\n from keras.mixed_precision import policy\n from keras.optimizer_experimental import optimizer as optimizer_experimental\n from keras.saving import hdf5_format\n+from keras.saving import pickle_utils\n from keras.saving import save\n from keras.saving import saving_utils\n-from keras.saving import pickle_utils\n from keras.saving.saved_model import json_utils\n from keras.saving.saved_model import model_serialization\n from keras.utils import generic_utils\n+from keras.utils import io_utils\n from keras.utils import layer_utils\n from keras.utils import object_identity\n from keras.utils import tf_utils\n from keras.utils import traceback_utils\n from keras.utils import version_utils\n-from keras.utils.io_utils import ask_to_proceed_with_overwrite\n-from keras.utils.io_utils import path_to_string\n from keras.utils.mode_keys import ModeKeys\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.eager import context\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n \n-\n # pylint: disable=g-import-not-at-top\n try:\n   import h5py\n@@ -2460,7 +2459,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             format.\n     \"\"\"\n     self._assert_weights_created()\n-    filepath = path_to_string(filepath)\n+    filepath = io_utils.path_to_string(filepath)\n     filepath_is_h5 = saving_utils.is_hdf5_filepath(filepath)\n     if save_format is None:\n       if filepath_is_h5:\n@@ -2493,7 +2492,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       check_filepath = filepath\n     # If file exists and should not be overwritten:\n     if not overwrite and os.path.isfile(check_filepath):\n-      proceed = ask_to_proceed_with_overwrite(check_filepath)\n+      proceed = io_utils.ask_to_proceed_with_overwrite(check_filepath)\n       if not proceed:\n         return\n     if save_format == 'h5':\n@@ -3205,8 +3204,10 @@ def _get_verbosity(verbose, distribute_strategy):\n         '`verbose=1` is not allowed with `ParameterServerStrategy` for '\n         f'performance reasons. Received: verbose={verbose}')\n   if verbose == 'auto':\n-    if distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n-      return 2  # Default to epoch-level logging for PSStrategy.\n+    if (distribute_strategy._should_use_with_coordinator or  # pylint: disable=protected-access\n+        not io_utils.is_interactive_logging_enabled()):\n+      # Default to epoch-level logging for PSStrategy or using absl logging.\n+      return 2\n     else:\n       return 1  # Default to batch-level logging otherwise.\n   return verbose\n@@ -3297,7 +3298,7 @@ def _disallow_inside_tf_function(method_name):\n def _detect_save_format(filepath):\n   \"\"\"Returns path to weights file and save format.\"\"\"\n \n-  filepath = path_to_string(filepath)\n+  filepath = io_utils.path_to_string(filepath)\n   if saving_utils.is_hdf5_filepath(filepath):\n     return filepath, 'h5'\n \n\n@@ -1864,12 +1864,18 @@ class TrainingTest(keras_parameterized.TestCase):\n         self._should_use_with_coordinator = True\n     with self.assertRaisesRegex(ValueError, '`verbose=1` is not allowed'):\n       training_module._get_verbosity(1, MyStrategy())\n+\n+    io_utils.enable_interactive_logging()\n     self.assertEqual(training_module._get_verbosity('auto', MyStrategy()), 2)\n     self.assertEqual(training_module._get_verbosity(\n         'auto', tf.distribute.MirroredStrategy()), 1)\n     self.assertEqual(training_module._get_verbosity(\n         2, tf.distribute.MirroredStrategy()), 2)\n \n+    io_utils.disable_interactive_logging()\n+    self.assertEqual(training_module._get_verbosity(\n+        'auto', tf.distribute.MirroredStrategy()), 2)\n+\n \n class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#24914501ca4829a8ef1846b12314953be9e6230f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 27 | Churn Cumulative: 3935 | Contributors (this commit): 34 | Commits (past 90d): 3 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -258,23 +258,29 @@ def get_file(fname=None,\n   if download:\n     io_utils.print_msg(f'Downloading data from {origin}')\n \n-    class ProgressTracker:\n-      # Maintain progbar for the lifetime of download.\n-      # This design was chosen for Python 2.7 compatibility.\n-      progbar = None\n+    class DLProgbar:\n+      \"\"\"Manage progress bar state for use in urlretrieve.\"\"\"\n \n-    def dl_progress(count, block_size, total_size):\n-      if ProgressTracker.progbar is None:\n+      def __init__(self):\n+        self.progbar = None\n+        self.finished = False\n+\n+      def __call__(self, block_num, block_size, total_size):\n+        if not self.progbar:\n           if total_size == -1:\n             total_size = None\n-        ProgressTracker.progbar = Progbar(total_size)\n-      else:\n-        ProgressTracker.progbar.update(count * block_size)\n+          self.progbar = Progbar(total_size)\n+        current = block_num * block_size\n+        if current < total_size:\n+          self.progbar.update(current)\n+        elif not self.finished:\n+          self.progbar.update(self.progbar.target)\n+          self.finished = True\n \n     error_msg = 'URL fetch failure on {}: {} -- {}'\n     try:\n       try:\n-        urlretrieve(origin, fpath, dl_progress)\n+        urlretrieve(origin, fpath, DLProgbar())\n       except urllib.error.HTTPError as e:\n         raise Exception(error_msg.format(origin, e.code, e.msg))\n       except urllib.error.URLError as e:\n@@ -283,7 +289,6 @@ def get_file(fname=None,\n       if os.path.exists(fpath):\n         os.remove(fpath)\n       raise\n-    ProgressTracker.progbar = None\n \n   if untar:\n     if not os.path.exists(untar_fpath):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c215f14d661de3e1587c26229d8f1a8cf0257563", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 13 | Files Changed: 6 | Hunks: 10 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 2806 | Contributors (this commit): 4 | Commits (past 90d): 65 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -79,7 +79,7 @@ class Adadelta(optimizer.Optimizer):\n                global_clipnorm=None,\n                use_ema=False,\n                ema_momentum=0.99,\n-               ema_overwrite_frequency=100,\n+               ema_overwrite_frequency=None,\n                jit_compile=False,\n                name='Adadelta',\n                **kwargs):\n\n@@ -74,7 +74,7 @@ class Adagrad(optimizer.Optimizer):\n                global_clipnorm=None,\n                use_ema=False,\n                ema_momentum=0.99,\n-               ema_overwrite_frequency=100,\n+               ema_overwrite_frequency=None,\n                jit_compile=False,\n                name='Adagrad',\n                **kwargs):\n@@ -105,7 +105,7 @@ class Adagrad(optimizer.Optimizer):\n               var, 'accumulator', initializer(shape=var.shape,\n                                               dtype=var.dtype)))\n \n-  def update_step(self, grad, variable, params=None):\n+  def update_step(self, grad, variable):\n     \"\"\"Update step given gradient and the associated model variable.\"\"\"\n     if self._var_key(variable) not in self._index_dict:\n       raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n\n@@ -104,7 +104,7 @@ class Adam(optimizer.Optimizer):\n                global_clipnorm=None,\n                use_ema=False,\n                ema_momentum=0.99,\n-               ema_overwrite_frequency=100,\n+               ema_overwrite_frequency=None,\n                jit_compile=False,\n                name='Adam',\n                **kwargs):\n\n@@ -40,7 +40,7 @@ class _BaseOptimizer(tf.Module):\n                global_clipnorm=None,\n                use_ema=False,\n                ema_momentum=0.99,\n-               ema_overwrite_frequency=100,\n+               ema_overwrite_frequency=None,\n                jit_compile=False,\n                **kwargs):\n     self._name = name\n@@ -514,12 +514,15 @@ class Optimizer(_BaseOptimizer):\n       the momentum to use when computing the EMA of the model's weights:\n         `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n         current_variable_value`.\n-    ema_overwrite_frequency: int or None, default to 100. Only used if\n+    ema_overwrite_frequency: int or None, default to None. Only used if\n       `use_ema=True`. Every ema_overwrite_frequency steps of iterations, we\n-      overwrite the model variable by its stored moving average. If None, we\n-      do not overwrite model variables in the middle of training, and users\n-      need to explicitly overwrite the model variable by calling\n-      `finalize_variable_values()`.\n+      overwrite the model variable by its moving average. If None, the optimizer\n+       does not overwrite model variables in the middle of training, and you\n+      need to explicitly overwrite the variables at the end of training\n+      by calling `optimizer.finalize_variable_values()` (which updates the model\n+      variables in-place). When using the built-in `fit()` training loop, this\n+      happens automatically after the last epoch, and you don't need to do\n+      anything.\n     jit_compile: bool, default to False. If True, the optimizer will use XLA\n       acceleration. `jit_compile` can only be False when using Parameter\n       Server Strategy.\n@@ -681,7 +684,7 @@ class Optimizer(_BaseOptimizer):\n                global_clipnorm=None,\n                use_ema=False,\n                ema_momentum=0.99,\n-               ema_overwrite_frequency=100,\n+               ema_overwrite_frequency=None,\n                jit_compile=False,\n                **kwargs):\n     \"\"\"Create a new Optimizer.\"\"\"\n\n@@ -134,7 +134,7 @@ class RMSprop(optimizer.Optimizer):\n         self._average_gradients.append(\n             self.add_variable_from_reference(var, 'average_gradient'))\n \n-  def update_step(self, gradient, variable, params=None):\n+  def update_step(self, gradient, variable):\n     \"\"\"Update step given gradient and the associated model variable.\"\"\"\n     if self._var_key(variable) not in self._index_dict:\n       raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n\n@@ -113,7 +113,7 @@ class SGD(optimizer.Optimizer):\n                global_clipnorm=None,\n                use_ema=False,\n                ema_momentum=0.99,\n-               ema_overwrite_frequency=100,\n+               ema_overwrite_frequency=None,\n                jit_compile=False,\n                name='SGD',\n                **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#83bee297d7176beb6ed197963669b0dd55acea0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1631 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 11 | DMM Complexity: 0.6\n\nDIFF:\n@@ -724,6 +724,7 @@ class BatchNormalizationBase(Layer):\n     return training\n \n   def call(self, inputs, training=None):\n+    inputs = tf.cast(inputs, self.compute_dtype)\n     training = self._get_training_value(training)\n \n     if self.virtual_batch_size is not None:\n\n@@ -152,7 +152,6 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n   def test_batchnorm_mixed_precision(self):\n     norm = keras.layers.BatchNormalization(\n         axis=-1,\n-        input_shape=(4, 4, 3),\n         momentum=0.8,\n         dtype='mixed_float16')\n     x = np.random.normal(size=(10, 4, 4, 3))\n@@ -161,6 +160,10 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n     self.assertEqual(norm.beta.dtype.base_dtype, 'float32')\n     self.assertEqual(norm.gamma.dtype.base_dtype, 'float32')\n \n+    x = np.arange(10 * 4 * 4 * 3).reshape((10, 4, 4, 3))\n+    y = norm(x)\n+    self.assertEqual(y.dtype, 'float16')\n+\n   @combinations.generate(combinations.combine(mode=['graph', 'eager'],\n                                               fused=[True, False]))\n   @testing_utils.enable_v2_dtype_behavior\n\n@@ -241,6 +241,7 @@ class LayerNormalization(Layer):\n     self.built = True\n \n   def call(self, inputs):\n+    inputs = tf.cast(inputs, self.compute_dtype)\n     # Compute the axes along which to reduce the mean / variance\n     input_shape = inputs.shape\n     ndims = len(input_shape)\n\n@@ -65,6 +65,7 @@ class UnitNormalization(base_layer.Layer):\n     self.axis = tf_utils.validate_axis(self.axis, input_shape)\n \n   def call(self, inputs):\n+    inputs = tf.cast(inputs, self.compute_dtype)\n     return tf.linalg.l2_normalize(inputs, axis=self.axis)\n \n   def compute_output_shape(self, input_shape):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9aa96cbf31b15fb863cc93c19b4ae6789541257d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 12/6 | Churn Δ: 72 | Churn Cumulative: 21039 | Contributors (this commit): 123 | Commits (past 90d): 33 | Contributors (cumulative): 133 | DMM Complexity: 0.7592592592592593\n\nDIFF:\n@@ -1663,6 +1663,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     version_utils.disallow_legacy_graph('Model', 'evaluate')\n     self._assert_compile_was_called()\n     self._check_call_args('evaluate')\n+    self._check_sample_weight_warning(x, sample_weight)\n     _disallow_inside_tf_function('evaluate')\n     use_cached_eval_dataset = kwargs.pop('_use_cached_eval_dataset', False)\n     if kwargs:\n@@ -3069,6 +3070,23 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                          'training/testing. '\n                          'Use `model.compile(optimizer, loss)`.')\n \n+  def _check_sample_weight_warning(self, x, sample_weight):\n+    # Datasets can include sample weight, by returning a tuple with the\n+    # structure of `(x, y, sample_weight)`.\n+    sample_weight_present = sample_weight is not None or (\n+        isinstance(x, tf.data.Dataset) and isinstance(x.element_spec, tuple) and\n+        len(x.element_spec) == 3)\n+\n+    # pylint: disable=protected-access\n+    if (sample_weight_present and\n+        self.compiled_metrics._user_weighted_metrics is None):\n+      logging.warning(\n+          '`evaluate()` received a value for `sample_weight`, but '\n+          '`weighted_metrics` were not provided.  Did you mean to pass metrics '\n+          'to `weighted_metrics` in `compile()`?  If this is intentional '\n+          'you can pass `weighted_metrics=[]` to `compile()` in order to '\n+          'silence this warning.')\n+\n   def _set_inputs(self, inputs, outputs=None, training=None):\n     \"\"\"This method is for compat with Modelv1. Only inputs are needed here.\"\"\"\n     self._set_save_spec(inputs)\n\n@@ -202,6 +202,60 @@ class TrainingTest(keras_parameterized.TestCase):\n     # The validation loss should be 1.0.\n     self.assertAllClose(history.history['val_loss'][0], 1.0)\n \n+  @keras_parameterized.run_all_keras_modes(\n+      always_skip_v1=True)\n+  def test_warn_on_evaluate(self):\n+    i = layers_module.Input((1,))\n+    x = np.ones((100, 1))\n+    y = np.ones((100, 1))\n+    sample_weight = np.ones((100,))\n+    model = training_module.Model(i, i)\n+    model.compile(loss='mse', metrics=['mse'])\n+\n+    logging.set_verbosity(2)\n+    with self.assertLogs(level=2) as logs:\n+      model.evaluate(x, y, sample_weight=sample_weight)\n+    self.assertTrue(\n+        any('`evaluate()` received a value for `sample_weight`' in log\n+            for log in logs.output))\n+\n+  @keras_parameterized.run_all_keras_modes(\n+      always_skip_v1=True)\n+  def test_sample_weight_warning_disable(self):\n+    i = layers_module.Input((1,))\n+    x = np.ones((100, 1))\n+    y = np.ones((100, 1))\n+    sample_weight = np.ones((100,))\n+    model = training_module.Model(i, i)\n+    model.compile(loss='mse', metrics=['mse'], weighted_metrics=[])\n+\n+    logging.set_verbosity(2)\n+    with self.assertLogs(level=2) as logs:\n+      model.evaluate(x, y, sample_weight=sample_weight)\n+    self.assertFalse(\n+        any('`evaluate()` received a value for `sample_weight`' in log\n+            for log in logs.output))\n+\n+  @keras_parameterized.run_all_keras_modes(\n+      always_skip_v1=True)\n+  def test_warn_on_evaluate_with_tf_dataset(self):\n+    i = layers_module.Input((1,))\n+\n+    x = tf.ones((100, 1), tf.float32)\n+    y = tf.ones((100, 1), tf.float32)\n+    sample_weight = tf.ones((100,), dtype=tf.float32)\n+    val_dataset = tf.data.Dataset.from_tensor_slices(\n+        (x, y, sample_weight)).batch(10)\n+    model = training_module.Model(i, i)\n+    model.compile(loss='mse', metrics=['mse'])\n+\n+    logging.set_verbosity(2)\n+    with self.assertLogs(level=2) as logs:\n+      model.evaluate(val_dataset)\n+    self.assertTrue(\n+        any('`evaluate()` received a value for `sample_weight`' in log\n+            for log in logs.output))\n+\n   @keras_parameterized.run_all_keras_modes\n   def test_fit_and_validate_training_arg(self):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1dfab0027e681509794842e64466a5c0876917bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 29 | Files Changed: 1 | Hunks: 27 | Methods Changed: 48 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 56 | Churn Cumulative: 2139 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -290,11 +290,9 @@ MODEL_CONFIGS = {\n \n BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n \n-  References:\n+  Reference:\n     - [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678)\n     (CVPR 2020)\n-    - [Fast and Accurate Model Scaling](https://arxiv.org/abs/2103.06877)\n-    (CVPR 2021)\n \n   For image classification use cases, see\n   [this page for detailed examples](\n@@ -310,8 +308,8 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n   values in the [0-255] range.\n \n   The naming of models is as follows: `RegNet<block_type><flops>` where\n-  `block_type` is one of `(Y, Z)` and `flops` signifies hundred million\n-  floating point operations. For example RegNetY64 corresponds to RegNet with\n+  `block_type` is one of `(X, Y)` and `flops` signifies hundred million\n+  floating point operations. For example RegNetY064 corresponds to RegNet with\n   Y block and 6.4 giga flops (64 hundred million flops).\n \n   Args:\n@@ -913,7 +911,7 @@ def RegNet(depths,\n def RegNetX002(model_name=\"regnetx002\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -941,7 +939,7 @@ def RegNetX002(model_name=\"regnetx002\",\n def RegNetX004(model_name=\"regnetx004\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -969,7 +967,7 @@ def RegNetX004(model_name=\"regnetx004\",\n def RegNetX006(model_name=\"regnetx006\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -997,7 +995,7 @@ def RegNetX006(model_name=\"regnetx006\",\n def RegNetX008(model_name=\"regnetx008\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1025,7 +1023,7 @@ def RegNetX008(model_name=\"regnetx008\",\n def RegNetX016(model_name=\"regnetx016\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1053,7 +1051,7 @@ def RegNetX016(model_name=\"regnetx016\",\n def RegNetX032(model_name=\"regnetx032\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1081,7 +1079,7 @@ def RegNetX032(model_name=\"regnetx032\",\n def RegNetX040(model_name=\"regnetx040\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1109,7 +1107,7 @@ def RegNetX040(model_name=\"regnetx040\",\n def RegNetX064(model_name=\"regnetx064\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1137,7 +1135,7 @@ def RegNetX064(model_name=\"regnetx064\",\n def RegNetX080(model_name=\"regnetx080\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1165,7 +1163,7 @@ def RegNetX080(model_name=\"regnetx080\",\n def RegNetX120(model_name=\"regnetx120\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1193,7 +1191,7 @@ def RegNetX120(model_name=\"regnetx120\",\n def RegNetX160(model_name=\"regnetx160\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1221,7 +1219,7 @@ def RegNetX160(model_name=\"regnetx160\",\n def RegNetX320(model_name=\"regnetx320\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1249,7 +1247,7 @@ def RegNetX320(model_name=\"regnetx320\",\n def RegNetY002(model_name=\"regnety002\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1277,7 +1275,7 @@ def RegNetY002(model_name=\"regnety002\",\n def RegNetY004(model_name=\"regnety004\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1305,7 +1303,7 @@ def RegNetY004(model_name=\"regnety004\",\n def RegNetY006(model_name=\"regnety006\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1333,7 +1331,7 @@ def RegNetY006(model_name=\"regnety006\",\n def RegNetY008(model_name=\"regnety008\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1361,7 +1359,7 @@ def RegNetY008(model_name=\"regnety008\",\n def RegNetY016(model_name=\"regnety016\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1389,7 +1387,7 @@ def RegNetY016(model_name=\"regnety016\",\n def RegNetY032(model_name=\"regnety032\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1417,7 +1415,7 @@ def RegNetY032(model_name=\"regnety032\",\n def RegNetY040(model_name=\"regnety040\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1445,7 +1443,7 @@ def RegNetY040(model_name=\"regnety040\",\n def RegNetY064(model_name=\"regnety064\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1473,7 +1471,7 @@ def RegNetY064(model_name=\"regnety064\",\n def RegNetY080(model_name=\"regnety080\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1501,7 +1499,7 @@ def RegNetY080(model_name=\"regnety080\",\n def RegNetY120(model_name=\"regnety120\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1529,7 +1527,7 @@ def RegNetY120(model_name=\"regnety120\",\n def RegNetY160(model_name=\"regnety160\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n@@ -1557,7 +1555,7 @@ def RegNetY160(model_name=\"regnety160\",\n def RegNetY320(model_name=\"regnety320\",\n                include_top=True,\n                include_preprocessing=True,\n-               weights=None,\n+               weights=\"imagenet\",\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#18ab3ed072ab4fc952ee64ed98378c7684023b66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 1793 | Contributors (this commit): 8 | Commits (past 90d): 13 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -346,12 +346,14 @@ class Attention(BaseDenseAttention):\n     else:\n       self.scale = None\n     if self.score_mode == 'concat':\n-      self.attention_v = self.add_weight(\n-          name='attention_v',\n+      self.concat_score_weight = self.add_weight(\n+          name='concat_score_weight',\n           shape=(),\n           initializer='ones',\n           dtype=self.dtype,\n           trainable=True)\n+    else:\n+      self.concat_score_weight = None\n     super(Attention, self).build(input_shape)\n \n   def _calculate_scores(self, query, key):\n@@ -374,10 +376,10 @@ class Attention(BaseDenseAttention):\n       # Reshape into [batch_size, 1, Tv, dim].\n       k_reshaped = tf.expand_dims(key, axis=-3)\n       if self.scale is not None:\n-        scores = self.attention_v * tf.reduce_sum(\n+        scores = self.concat_score_weight * tf.reduce_sum(\n          tf.tanh(self.scale * (q_reshaped + k_reshaped)), axis=-1)\n       else:\n-        scores = self.attention_v * tf.reduce_sum(\n+        scores = self.concat_score_weight * tf.reduce_sum(\n          tf.tanh(q_reshaped + k_reshaped), axis=-1)\n \n     return scores\n\n@@ -212,9 +212,9 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n         [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],\n         dtype=np.float32)\n     attention_layer = dense_attention.Attention(score_mode='concat')\n+    attention_layer.concat_score_weight = 1\n     attention_layer.build(input_shape=([1, 2, 4], [1, 3, 4]))\n     actual = attention_layer._calculate_scores(query=q, key=k)\n-    attention_layer.attention_v = 1\n \n     # pylint:disable=line-too-long\n     # expected000 = tanh(1.+1.5) + tanh(1.1+1.6) + tanh(1.2+1.7) + tanh(1.3+1.8) = 3.96753427840\n@@ -266,10 +266,10 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n     # Key tensor of shape [1, 1, 1]\n     k = np.array([[[1.6]]], dtype=np.float32)\n     attention_layer = dense_attention.Attention(use_scale=True, score_mode='concat')\n+    attention_layer.concat_score_weight = 1\n     attention_layer.build(input_shape=([1, 1, 1], [1, 1, 1]))\n     attention_layer.scale = 2.\n     actual = attention_layer._calculate_scores(query=q, key=k)\n-    attention_layer.attention_v = 1\n \n     # Expected tensor of shape [1, 1, 1].\n     # expected000 = tanh(2*(1.1+1.6)) = 0.9999592018254402\n@@ -301,7 +301,7 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n     # Value mask tensor of shape [1, 3]\n     v_mask = np.array([[True, True, False]], dtype=np.bool_)\n     attention_layer = dense_attention.Attention(score_mode='concat')\n-    attention_layer.attention_v = 1\n+    attention_layer.concat_score_weight = 1\n     actual = attention_layer([q, v], mask=[None, v_mask])\n \n     expected_shape = [1, 2, 4]\n@@ -340,7 +340,7 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n     # Value mask tensor of shape [1, 3]\n     v_mask = np.array([[True, True, False]], dtype=np.bool_)\n     attention_layer = dense_attention.Attention(score='concat')\n-    attention_layer.attention_v = 1\n+    attention_layer.concat_score_weight = 1\n     actual = attention_layer([q, v, k], mask=[None, v_mask])\n \n     expected_shape = [1, 2, 4]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#085f20bd0dc034c772e4d00f75ebb3e5f934e6bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 152 | Lines Deleted: 14 | Files Changed: 3 | Hunks: 9 | Methods Changed: 11 | Complexity Δ (Sum/Max): 8/7 | Churn Δ: 166 | Churn Cumulative: 1706 | Contributors (this commit): 7 | Commits (past 90d): 7 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,11 +16,16 @@\n \n from absl.testing import parameterized\n import keras\n+from keras import optimizers\n+from keras.applications import resnet_v2\n+from keras.datasets import fashion_mnist\n from keras.distribute import optimizer_combinations\n from keras.distribute import strategy_combinations\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n+from tensorflow.python.ops.losses import losses_impl  # pylint: disable=g-direct-tensorflow-import\n+\n _NUM_SAMPLES = 66\n _BATCH_SIZE = 32\n _RANDOM_SEED = 1337\n@@ -299,6 +304,112 @@ class TestDistributionStrategyDnnCorrectness(tf.test.TestCase,\n     self.assertAllClose(loss, loss_with_ds, atol=1e-3, rtol=1e-3)\n     self.assertAllClose(acc, acc_with_ds, atol=1e-3, rtol=1e-3)\n \n+  @tf.__internal__.distribute.combinations.generate(\n+      tf.__internal__.test.combinations.combine(\n+          distribution=[\n+              tf.__internal__.distribute.combinations\n+              .mirrored_strategy_with_two_gpus,\n+          ],\n+          mode=['eager'],\n+      ))\n+  def test_fused_batch_norm_uneven_batch(self, distribution):\n+    \"\"\"Test that fused batch norm works when the last device may get empty data.\n+\n+    Adapted from https://www.tensorflow.org/tutorials/distribute/custom_training\n+    but using ResNet, which uses fused batchnorm, as the model.\n+\n+    Arguments:\n+      distribution: distribute test configuration\n+    \"\"\"\n+    (train_images, train_labels), _ = fashion_mnist.load_data()\n+    # add channel dimension to make 2D data into 3D, since some ops of the model\n+    # require it.\n+    train_images = train_images[..., None]\n+    train_images = train_images / np.float32(255)\n+\n+    # Padding images because ResNet requires a minimal shape of (32, 32)\n+    padded_train_images = np.concatenate([\n+        np.zeros((len(train_images), 2, 28, 1)),\n+        train_images,\n+        np.zeros((len(train_images), 2, 28, 1))\n+    ], axis=1)\n+    padded_train_images = np.concatenate([\n+        np.zeros((len(train_images), 32, 2, 1)),\n+        padded_train_images,\n+        np.zeros((len(train_images), 32, 2, 1))\n+    ], axis=2)\n+\n+    buffer_size = len(train_images)\n+    global_batch_size = distribution.num_replicas_in_sync\n+    num_samples = global_batch_size - 1\n+\n+    epochs = 2\n+\n+    # Keep only the first images, so that the last GPU receives an empty batch\n+    padded_train_images = padded_train_images[:num_samples]\n+    train_labels = train_labels[:num_samples]\n+\n+    train_dataset = tf.data.Dataset.from_tensor_slices(\n+        (padded_train_images,\n+         train_labels)).shuffle(buffer_size).batch(global_batch_size)\n+    train_dist_dataset = distribution.experimental_distribute_dataset(\n+        train_dataset)\n+\n+    def create_model():\n+      inputs = keras.Input((32, 32, 1))\n+      preprocessed = keras.layers.Conv2D(3, (1, 1))(\n+          inputs)  # ResNet requires 3 channels\n+      features = resnet_v2.ResNet50V2(\n+          include_top=False,\n+          input_tensor=preprocessed,\n+          pooling='avg',\n+          weights=None).output\n+      return keras.Model(inputs, features)\n+\n+    with distribution.scope():\n+      # Set reduction to `none` so we can do the reduction afterwards and divide\n+      # by global batch size.\n+      loss_object = keras.losses.SparseCategoricalCrossentropy(\n+          from_logits=True,\n+          reduction=losses_impl.Reduction.NONE)\n+\n+      def compute_resnet_loss(labels, predictions):\n+        per_example_loss = loss_object(labels, predictions)\n+        return tf.nn.compute_average_loss(\n+            per_example_loss, global_batch_size=global_batch_size)\n+\n+      model = create_model()\n+\n+      optimizer = optimizers.adam_v2.Adam()\n+\n+    def train_step(inputs):\n+      images, labels = inputs\n+\n+      with tf.GradientTape() as tape:\n+        predictions = model(images, training=True)\n+        loss = compute_resnet_loss(labels, predictions)\n+\n+      gradients = tape.gradient(loss, model.trainable_variables)\n+      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n+      return loss\n+\n+    @tf.function\n+    def distributed_train_step(dataset_inputs):\n+      per_replica_losses = distribution.run(train_step, args=(dataset_inputs,))\n+      return distribution.reduce(\n+          tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n+\n+    for epoch in range(epochs):\n+      # Train loop\n+      total_loss = 0.0\n+      num_batches = 0\n+      for x in train_dist_dataset:\n+        total_loss += distributed_train_step(x)\n+        num_batches += 1\n+      train_loss = total_loss / num_batches\n+\n+      print(f'Epoch {epoch+1}, Loss: {train_loss}')\n+\n \n if __name__ == '__main__':\n   tf.__internal__.distribute.multi_process_runner.test_main()\n\n@@ -579,9 +579,6 @@ class BatchNormalizationBase(Layer):\n           data_format=self._data_format,\n           exponential_avg_factor=exponential_avg_factor)\n \n-    def _fused_batch_norm_training_empty():\n-      return inputs, self.moving_mean, self.moving_variance\n-\n     def _fused_batch_norm_inference():\n       return tf.compat.v1.nn.fused_batch_norm(\n           inputs,\n@@ -593,16 +590,8 @@ class BatchNormalizationBase(Layer):\n           is_training=False,\n           data_format=self._data_format)\n \n-    train_op = _fused_batch_norm_training\n-    if use_fused_avg_updates and input_batch_size is not None:\n-      # pylint: disable=g-long-lambda\n-      train_op = lambda: control_flow_util.smart_cond(\n-          input_batch_size > 0, _fused_batch_norm_training,\n-          _fused_batch_norm_training_empty)\n-      # pylint: enable=g-long-lambda\n-\n     output, mean, variance = control_flow_util.smart_cond(\n-        training, train_op, _fused_batch_norm_inference)\n+        training, _fused_batch_norm_training, _fused_batch_norm_inference)\n     variance = _maybe_add_or_remove_bessels_correction(variance, remove=True)\n \n     training_value = control_flow_util.constant_value(training)\n@@ -618,7 +607,12 @@ class BatchNormalizationBase(Layer):\n       def mean_update():\n         \"\"\"Update self.moving_mean with the most recent data point.\"\"\"\n         if use_fused_avg_updates:\n-          return self._assign_new_value(self.moving_mean, mean)\n+          if input_batch_size is not None:\n+            new_mean = control_flow_util.smart_cond(\n+                input_batch_size > 0, lambda: mean, lambda: self.moving_mean)\n+          else:\n+            new_mean = mean\n+          return self._assign_new_value(self.moving_mean, new_mean)\n         else:\n           return self._assign_moving_average(self.moving_mean, mean, momentum,\n                                              input_batch_size)\n@@ -626,7 +620,13 @@ class BatchNormalizationBase(Layer):\n       def variance_update():\n         \"\"\"Update self.moving_variance with the most recent data point.\"\"\"\n         if use_fused_avg_updates:\n-          return self._assign_new_value(self.moving_variance, variance)\n+          if input_batch_size is not None:\n+            new_variance = control_flow_util.smart_cond(\n+                input_batch_size > 0, lambda: variance,\n+                lambda: self.moving_variance)\n+          else:\n+            new_variance = variance\n+          return self._assign_new_value(self.moving_variance, new_variance)\n         else:\n           return self._assign_moving_average(self.moving_variance, variance,\n                                              momentum, input_batch_size)\n\n@@ -373,6 +373,33 @@ class BatchNormalizationV2Test(keras_parameterized.TestCase):\n     inp = keras.layers.Input(shape=(None, None, 3))\n     _ = norm(inp)\n \n+  @combinations.generate(combinations.combine(mode=['eager']))\n+  def test_fused_batchnorm_empty_batch(self):\n+    # Test case for https://github.com/tensorflow/tensorflow/issues/52986\n+    # create a simple strategy with the enable_partial_batch_handling flag\n+    # turned on, to trigger the empty batch code path in fused batchnorm\n+    strategy = tf.distribute.OneDeviceStrategy('/cpu:0')\n+    strategy.extended.enable_partial_batch_handling = True\n+    with strategy.scope():\n+      layer = batch_normalization.BatchNormalization()\n+\n+    def fn():\n+      with tf.GradientTape() as tape:\n+        x = tf.ones((0, 2, 2, 2))\n+        layer(x, training=True)\n+      return tape\n+\n+    tape = strategy.run(fn)\n+\n+    self.assertTrue(layer.fused)\n+\n+    self.assertIsNotNone(layer.moving_mean)\n+    self.assertIsNotNone(layer.moving_variance)\n+\n+    tape_vars = tape.watched_variables()\n+    self.assertAllEqual(layer.gamma, tape_vars[0])\n+    self.assertAllEqual(layer.beta, tape_vars[1])\n+\n \n def _run_batchnorm_correctness_test(layer, dtype='float32', fused=False):\n   model = keras.models.Sequential()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#66a4e2e7e292ef9f0f6aaec040346c7d4a567b21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 249 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -40,10 +40,9 @@ class RNNCellWrapperTest(tf.test.TestCase, parameterized.TestCase):\n         bias_initializer=tf.compat.v1.constant_initializer(0.5))\n     g, m_new = base_cell(x, m)\n     wrapper_object = wrapper_type(base_cell)\n-    (name, dep), = wrapper_object._checkpoint_dependencies\n+    self.assertDictEqual({\"cell\": base_cell},\n+                         wrapper_object._trackable_children())\n     wrapper_object.get_config()  # Should not throw an error\n-    self.assertIs(dep, base_cell)\n-    self.assertEqual(\"cell\", name)\n \n     g_res, m_new_res = wrapper_object(x, m)\n     self.evaluate([tf.compat.v1.global_variables_initializer()])\n@@ -84,10 +83,9 @@ class RNNCellWrapperTest(tf.test.TestCase, parameterized.TestCase):\n     m = tf.zeros([1, 3])\n     cell = rnn_cell_impl.GRUCell(3)\n     wrapped_cell = wrapper_type(cell, \"/cpu:0\")\n-    (name, dep), = wrapped_cell._checkpoint_dependencies\n+    self.assertDictEqual({\"cell\": cell},\n+                         wrapped_cell._trackable_children())\n     wrapped_cell.get_config()  # Should not throw an error\n-    self.assertIs(dep, cell)\n-    self.assertEqual(\"cell\", name)\n \n     outputs, _ = wrapped_cell(x, m)\n     self.assertIn(\"cpu:0\", outputs.device.lower())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
