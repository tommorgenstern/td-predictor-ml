{"custom_id": "keras#69d7eff920fd2ad78bf4c38e2d3b480a9c5d1dda", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 100 | Lines Deleted: 59 | Files Changed: 3 | Hunks: 35 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 159 | Churn Cumulative: 1808 | Contributors (this commit): 18 | Commits (past 90d): 30 | Contributors (cumulative): 32 | DMM Complexity: 0.7777777777777778\n\nDIFF:\n@@ -2,7 +2,6 @@ import torch\n import torch.nn.functional as tnn\n \n from keras.src import backend\n-from keras.src import tree\n from keras.src.backend.common.backend_utils import (\n     compute_conv_transpose_padding_args_for_torch,\n )\n@@ -204,17 +203,27 @@ def sparsemax(logits, axis=-1):\n def _compute_padding_length(\n     input_length, kernel_length, stride, dilation_rate=1\n ):\n-    \"\"\"Compute padding length along one dimension.\"\"\"\n-    total_padding_length = (\n-        dilation_rate * (kernel_length - 1) - (input_length - 1) % stride\n+    \"\"\"Compute padding length along one dimension with support\n+    for asymmetric padding.\"\"\"\n+    effective_k_size = (kernel_length - 1) * dilation_rate + 1\n+    if stride == 1:\n+        # total padding is kernel_size - 1\n+        total_padding = effective_k_size - 1\n+    else:\n+        # calc. needed padding for case with stride involved\n+        output_size = (input_length + stride - 1) // stride\n+        total_padding = max(\n+            0, (output_size - 1) * stride + effective_k_size - input_length\n         )\n-    left_padding = total_padding_length // 2\n-    right_padding = (total_padding_length + 1) // 2\n+\n+    # divide padding evenly, with extra pixel going at the end if needed\n+    left_padding = total_padding // 2\n+    right_padding = total_padding - left_padding\n     return (left_padding, right_padding)\n \n \n def _apply_same_padding(\n-    inputs, kernel_size, strides, operation_type, dilation_rate=1\n+    inputs, kernel_size, strides, data_format, operation_type, dilation_rate=1\n ):\n     \"\"\"Apply same padding to the input tensor.\n \n@@ -231,50 +240,49 @@ def _apply_same_padding(\n     \"\"\"\n     spatial_shape = inputs.shape[2:]\n     num_spatial_dims = len(spatial_shape)\n-    padding = ()\n+    padding = []\n \n-    for i in range(num_spatial_dims):\n-        if operation_type == \"pooling\":\n-            padding_size = _compute_padding_length(\n-                spatial_shape[i], kernel_size[i], strides[i]\n-            )\n-            mode = \"replicate\"\n-        else:\n+    if operation_type != \"pooling\":\n         dilation_rate = standardize_tuple(\n             dilation_rate, num_spatial_dims, \"dilation_rate\"\n         )\n-            padding_size = _compute_padding_length(\n-                spatial_shape[i], kernel_size[i], strides[i], dilation_rate[i]\n-            )\n-            mode = \"constant\"\n-        padding = (padding_size,) + padding\n \n-    if all([left == right for left, right in padding]):\n+    for i in range(num_spatial_dims):\n+        dil = 1 if operation_type == \"pooling\" else dilation_rate[i]\n+        pad = _compute_padding_length(\n+            spatial_shape[i], kernel_size[i], strides[i], dil\n+        )\n+        padding.append(pad)\n+\n+    # convert padding to torch format\n+    if all(left == right for left, right in padding):\n         return inputs, [left for left, _ in padding]\n \n-    flattened_padding = tuple(\n-        value for left_and_right in padding for value in left_and_right\n-    )\n-    return tnn.pad(inputs, pad=flattened_padding, mode=mode), 0\n+    # else, need to pad manually\n+    flattened_padding = []\n+    for pad in reversed(padding):\n+        flattened_padding.extend(pad)\n+\n+    mode = \"replicate\" if operation_type == \"pooling\" else \"constant\"\n+    return tnn.pad(inputs, pad=tuple(flattened_padding), mode=mode), 0\n \n \n def _transpose_spatial_inputs(inputs):\n-    num_spatial_dims = inputs.ndim - 2\n+    \"\"\"Transpose inputs from channels_last to channels_first format.\"\"\"\n     # Torch pooling does not support `channels_last` format, so\n     # we need to transpose to `channels_first` format.\n-    if num_spatial_dims == 1:\n-        inputs = torch.permute(inputs, (0, 2, 1))\n-    elif num_spatial_dims == 2:\n-        inputs = torch.permute(inputs, (0, 3, 1, 2))\n-    elif num_spatial_dims == 3:\n-        inputs = torch.permute(inputs, (0, 4, 1, 2, 3))\n-    else:\n+    ndim = inputs.ndim - 2\n+    if ndim == 1:  # 1D case\n+        return torch.permute(inputs, (0, 2, 1))\n+    elif ndim == 2:  # 2D case\n+        return torch.permute(inputs, (0, 3, 1, 2))\n+    elif ndim == 3:  # 3D case\n+        return torch.permute(inputs, (0, 4, 1, 2, 3))\n     raise ValueError(\n         \"Inputs must have ndim=3, 4 or 5, \"\n         \"corresponding to 1D, 2D and 3D inputs. \"\n         f\"Received input shape: {inputs.shape}.\"\n     )\n-    return inputs\n \n \n def _transpose_spatial_outputs(outputs):\n@@ -309,6 +317,7 @@ def max_pool(\n     padding=\"valid\",\n     data_format=None,\n ):\n+    \"\"\"Fixed max pooling implementation.\"\"\"\n     inputs = convert_to_tensor(inputs)\n     num_spatial_dims = inputs.ndim - 2\n     pool_size = standardize_tuple(pool_size, num_spatial_dims, \"pool_size\")\n@@ -325,7 +334,7 @@ def max_pool(\n         # Torch does not natively support `\"same\"` padding, we need to manually\n         # apply the right amount of padding to `inputs`.\n         inputs, padding = _apply_same_padding(\n-            inputs, pool_size, strides, operation_type=\"pooling\"\n+            inputs, pool_size, strides, data_format, \"pooling\"\n         )\n     else:\n         padding = 0\n@@ -370,26 +379,36 @@ def average_pool(\n     padding=\"valid\",\n     data_format=None,\n ):\n+    \"\"\"Fixed average pooling with correct padding calculation.\"\"\"\n     inputs = convert_to_tensor(inputs)\n     num_spatial_dims = inputs.ndim - 2\n     pool_size = standardize_tuple(pool_size, num_spatial_dims, \"pool_size\")\n-    if strides is None:\n-        strides = pool_size\n-    else:\n-        strides = standardize_tuple(strides, num_spatial_dims, \"strides\")\n+    strides = (\n+        pool_size\n+        if strides is None\n+        else standardize_tuple(strides, num_spatial_dims, \"strides\")\n+    )\n \n     data_format = backend.standardize_data_format(data_format)\n+    orig_format = data_format\n+\n     if data_format == \"channels_last\":\n         inputs = _transpose_spatial_inputs(inputs)\n+\n     if padding == \"same\":\n         # Torch does not natively support `\"same\"` padding, we need to manually\n         # apply the right amount of padding to `inputs`.\n         inputs, padding = _apply_same_padding(\n-            inputs, pool_size, strides, operation_type=\"pooling\"\n+            inputs,\n+            pool_size,\n+            strides,\n+            \"channels_first\",  # we're in channels_first here\n+            \"pooling\",\n         )\n     else:\n         padding = 0\n \n+    # apply pooling\n     if num_spatial_dims == 1:\n         outputs = tnn.avg_pool1d(\n             inputs,\n@@ -420,8 +439,10 @@ def average_pool(\n             \"corresponding to 1D, 2D and 3D inputs. \"\n             f\"Received input shape: {inputs.shape}.\"\n         )\n-    if data_format == \"channels_last\":\n+\n+    if orig_format == \"channels_last\":\n         outputs = _transpose_spatial_outputs(outputs)\n+\n     return outputs\n \n \n@@ -433,6 +454,7 @@ def conv(\n     data_format=None,\n     dilation_rate=1,\n ):\n+    \"\"\"Convolution with fixed group handling.\"\"\"\n     inputs = convert_to_tensor(inputs)\n     kernel = convert_to_tensor(kernel)\n     num_spatial_dims = inputs.ndim - 2\n@@ -441,53 +463,59 @@ def conv(\n     data_format = backend.standardize_data_format(data_format)\n     if data_format == \"channels_last\":\n         inputs = _transpose_spatial_inputs(inputs)\n-    # Transpose kernel from keras format to torch format.\n+\n     kernel = _transpose_conv_kernel(kernel)\n-    if padding == \"same\" and any(d != 1 for d in tree.flatten(strides)):\n-        # Torch does not support this case in conv2d().\n-        # Manually pad the tensor.\n+\n+    # calc. groups snippet\n+    in_channels = inputs.shape[1]\n+    kernel_in_channels = kernel.shape[1]\n+    if in_channels % kernel_in_channels != 0:\n+        raise ValueError(\n+            f\"Input channels ({in_channels}) must be divisible by \"\n+            f\"kernel input channels ({kernel_in_channels})\"\n+        )\n+    groups = in_channels // kernel_in_channels\n+\n+    # handle padding\n+    if padding == \"same\":\n         inputs, padding = _apply_same_padding(\n             inputs,\n             kernel.shape[2:],\n             strides,\n-            operation_type=\"conv\",\n-            dilation_rate=dilation_rate,\n+            data_format,\n+            \"conv\",\n+            dilation_rate,\n         )\n-    channels = inputs.shape[1]\n-    kernel_in_channels = kernel.shape[1]\n-    if channels % kernel_in_channels > 0:\n-        raise ValueError(\n-            \"The number of input channels must be evenly divisible by \"\n-            f\"kernel.shape[1]. Received: inputs.shape={inputs.shape}, \"\n-            f\"kernel.shape={kernel.shape}\"\n-        )\n-    groups = channels // kernel_in_channels\n+    else:\n+        padding = 0\n+\n+    # apply convolution\n     if num_spatial_dims == 1:\n         outputs = tnn.conv1d(\n             inputs,\n             kernel,\n             stride=strides,\n+            padding=padding,\n             dilation=dilation_rate,\n             groups=groups,\n-            padding=padding,\n         )\n     elif num_spatial_dims == 2:\n         outputs = tnn.conv2d(\n             inputs,\n             kernel,\n             stride=strides,\n+            padding=padding,\n             dilation=dilation_rate,\n             groups=groups,\n-            padding=padding,\n         )\n     elif num_spatial_dims == 3:\n         outputs = tnn.conv3d(\n             inputs,\n             kernel,\n             stride=strides,\n+            padding=padding,\n             dilation=dilation_rate,\n             groups=groups,\n-            padding=padding,\n         )\n     else:\n         raise ValueError(\n\n@@ -174,6 +174,7 @@ class AveragePoolingBasicTest(testing.TestCase):\n         (2, 1, \"same\", \"channels_first\", (3, 5, 5, 4), (3, 5, 5, 4)),\n         ((2, 3), (2, 2), \"valid\", \"channels_last\", (3, 5, 5, 4), (3, 2, 2, 4)),\n         ((2, 3), (2, 2), \"same\", \"channels_last\", (3, 5, 5, 4), (3, 3, 3, 4)),\n+        ((2, 3), (3, 3), \"same\", \"channels_first\", (3, 5, 5, 4), (3, 5, 2, 2)),\n     )\n     def test_average_pooling2d(\n         self,\n\n@@ -1597,6 +1597,18 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             knn.average_pool(x, 2, (2, 1), padding=\"same\"),\n             np_avgpool2d(x, 2, (2, 1), padding=\"same\", data_format=data_format),\n         )\n+        # Test 2D average pooling with different pool size.\n+        if data_format == \"channels_last\":\n+            input_shape = (2, 10, 9, 3)\n+        else:\n+            input_shape = (2, 3, 10, 9)\n+        x = np.arange(540, dtype=float).reshape(input_shape)\n+        self.assertAllClose(\n+            knn.average_pool(x, (2, 3), (3, 3), padding=\"same\"),\n+            np_avgpool2d(\n+                x, (2, 3), (3, 3), padding=\"same\", data_format=data_format\n+            ),\n+        )\n \n     @parameterized.product(\n         strides=(1, 2, 3),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6c9dfcafae24711a07391b68e4abe89019662e90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 161 | Lines Deleted: 50 | Files Changed: 4 | Hunks: 40 | Methods Changed: 15 | Complexity Δ (Sum/Max): 10/6 | Churn Δ: 211 | Churn Cumulative: 839 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -12,7 +12,5 @@ from keras.src.quantizers.quantizers import Quantizer\n from keras.src.quantizers.quantizers import abs_max_quantize\n from keras.src.quantizers.quantizers import compute_float8_amax_history\n from keras.src.quantizers.quantizers import compute_float8_scale\n-from keras.src.quantizers.quantizers import (\n-    fake_quant_with_min_max_vars as fake_quant_with_min_max_vars_per_channel,\n-)\n+from keras.src.quantizers.quantizers import fake_quant_with_min_max_vars\n from keras.src.quantizers.quantizers import quantize_and_dequantize\n\n@@ -12,7 +12,5 @@ from keras.src.quantizers.quantizers import Quantizer\n from keras.src.quantizers.quantizers import abs_max_quantize\n from keras.src.quantizers.quantizers import compute_float8_amax_history\n from keras.src.quantizers.quantizers import compute_float8_scale\n-from keras.src.quantizers.quantizers import (\n-    fake_quant_with_min_max_vars as fake_quant_with_min_max_vars_per_channel,\n-)\n+from keras.src.quantizers.quantizers import fake_quant_with_min_max_vars\n from keras.src.quantizers.quantizers import quantize_and_dequantize\n\n@@ -4,8 +4,11 @@ import numpy as np\n from keras.src import backend\n from keras.src import ops\n from keras.src.api_export import keras_export\n+from keras.src.backend import KerasTensor\n+from keras.src.backend import any_symbolic_tensors\n from keras.src.backend.common.backend_utils import canonicalize_axis\n from keras.src.backend.common.backend_utils import standardize_axis_for_numpy\n+from keras.src.ops.operation import Operation\n \n \"\"\"Int8-related classes and methods\"\"\"\n \n@@ -130,17 +133,20 @@ class AbsMaxQuantizer(Quantizer):\n \n def adjust_and_nudge(min_range, max_range, num_bits, narrow_range):\n     \"\"\"Adjusts and nudges the quantization range for better accuracy.\"\"\"\n+    # Use higher precision for the computation.\n+    compute_dtype = backend.result_type(min_range.dtype, \"float32\")\n+    min_range = ops.cast(min_range, compute_dtype)\n+    max_range = ops.cast(max_range, compute_dtype)\n \n-    quant_max = ops.cast(ops.subtract(ops.power(2, num_bits), 1.0), \"float32\")\n-\n-    quant_min = ops.cast(0.0 if not narrow_range else 1.0, \"float32\")\n+    quant_max = (1 << num_bits) - 1\n+    quant_min = 0 if not narrow_range else 1\n+    diff_range = ops.subtract(max_range, min_range)\n \n     # Calculate the scale and ensure it's positive\n-    scale = ops.divide(\n-        ops.subtract(max_range, min_range), ops.subtract(quant_max, quant_min)\n-    )\n+    scale = ops.divide(diff_range, quant_max - quant_min)\n \n-    inv_scale = ops.reciprocal(scale)\n+    # Re-calculate the inverse to avoid loss of precision\n+    inv_scale = ops.divide(quant_max - quant_min, diff_range)\n \n     # Calculate the zero point from the min range\n     zero_point_from_min = quant_min - ops.divide(min_range, scale)\n@@ -158,17 +164,37 @@ def adjust_and_nudge(min_range, max_range, num_bits, narrow_range):\n     return nudged_min, nudged_max, scale, inv_scale\n \n \n-@keras_export(\"keras.quantizers.fake_quant_with_min_max_vars_per_channel\")\n+class FakeQuantWithMinMaxVars(Operation):\n+    def __init__(self, num_bits=8, narrow_range=False, axis=None):\n+        super().__init__()\n+        self.num_bits = num_bits\n+        self.narrow_range = narrow_range\n+        self.axis = axis\n+\n+    def call(self, inputs, min_vals, max_vals):\n+        return fake_quant_with_min_max_vars(\n+            inputs,\n+            min_vals,\n+            max_vals,\n+            num_bits=self.num_bits,\n+            narrow_range=self.narrow_range,\n+            axis=self.axis,\n+        )\n+\n+    def compute_output_spec(self, inputs, min_vals, max_vals):\n+        return KerasTensor(inputs.shape, dtype=inputs.dtype)\n+\n+\n+@keras_export(\"keras.quantizers.fake_quant_with_min_max_vars\")\n def fake_quant_with_min_max_vars(\n     inputs,\n     min_vals,\n     max_vals,\n-    num_bits,\n+    num_bits=8,\n     narrow_range=False,\n     axis=None,\n ):\n-    \"\"\"\n-    Perform per-tensor or per-channel fake quantization.\n+    \"\"\"Perform per-tensor or per-channel fake quantization.\n \n     `[min_vals, max_vals]` define the clamping range for the `inputs`.\n \n@@ -183,27 +209,68 @@ def fake_quant_with_min_max_vars(\n     `max_vals` to be trained.\n \n     Args:\n-        inputs: Input tensor of float dtype.\n+        inputs: Input Keras tensor of float dtype.\n         min_vals: A global minimum scalar or a per-channel minimum tensor.\n         max_vals: A global maximum scalar or a per-channel maximum tensor.\n-        num_bits: Quantization bit width (e.g., `8` for int8).\n-        narrow_range: Whether to use narrow quantization range.\n+        num_bits: Quantization bit width (e.g., `8` for int8). Defaults to `8`.\n+        narrow_range: Whether to use narrow quantization range. Defaults to\n+            `False`.\n         axis: Axis along which to perform per-channel quantization. If `None`,\n               per-tensor quantization is performed. Defaults to `None`.\n \n \n     Returns:\n-        Fake-quantized tensor\n+        Tensor: A Keras tensor with fake quantization applied.\n     \"\"\"\n+    if any_symbolic_tensors((inputs,)):\n+        return FakeQuantWithMinMaxVars().symbolic_call(\n+            inputs, min_vals, max_vals\n+        )\n+\n     inputs = ops.convert_to_tensor(inputs)\n     min_vals = ops.convert_to_tensor(min_vals)\n     max_vals = ops.convert_to_tensor(max_vals)\n+    num_bits = int(num_bits)\n \n     if axis is not None:\n         axis = canonicalize_axis(axis, inputs.ndim)\n \n+    # Shortcut for TensorFlow backend by using `tf.quantization.fake_quant_*`\n+    # apis. This is necessary to be recognizable for the TFLite converter.\n+    if backend.backend() == \"tensorflow\":\n+        import tensorflow as tf\n+\n+        # `tf.quantization.fake_quant_*` only supports float32.\n+        dtype = backend.standardize_dtype(inputs.dtype)\n+        if axis is None:\n+            outputs = tf.quantization.fake_quant_with_min_max_vars(\n+                ops.cast(inputs, \"float32\"),\n+                ops.cast(ops.reshape(min_vals, ()), \"float32\"),\n+                ops.cast(ops.reshape(max_vals, ()), \"float32\"),\n+                num_bits=num_bits,\n+                narrow_range=narrow_range,\n+            )\n+            return ops.cast(outputs, dtype=dtype)\n+        else:\n+            # `tf.quantization.fake_quant_with_min_max_vars_per_channel` only\n+            # supports the last channel for the per-channel quantization. We\n+            # use `ops.swapaxes` for the pre- and post-processing.\n+            last_axis = inputs.ndim - 1\n+            inputs = ops.swapaxes(inputs, axis, last_axis)\n+            outputs = tf.quantization.fake_quant_with_min_max_vars_per_channel(\n+                ops.cast(inputs, \"float32\"),\n+                ops.cast(min_vals, \"float32\"),\n+                ops.cast(max_vals, \"float32\"),\n+                num_bits=num_bits,\n+                narrow_range=narrow_range,\n+            )\n+            outputs = ops.cast(outputs, dtype=dtype)\n+            return ops.swapaxes(outputs, last_axis, axis)\n+\n     @ops.custom_gradient\n     def _fake_quant_with_min_max_vars_per_channel(x, min_val, max_val):\n+        dtype = backend.standardize_dtype(x.dtype)\n+\n         # Calculate quantization parameters for all channels at once\n         nudged_min, nudged_max, scale, inv_scale = adjust_and_nudge(\n             min_val, max_val, num_bits, narrow_range\n@@ -212,7 +279,9 @@ def fake_quant_with_min_max_vars(\n         quant_zero = ops.floor(\n             ops.add(ops.multiply(-nudged_min, inv_scale), 0.5)\n         )\n-        x_clamped = ops.clip(x, nudged_min, nudged_max)\n+        x_clamped = ops.clip(\n+            x, ops.cast(nudged_min, x.dtype), ops.cast(nudged_max, x.dtype)\n+        )\n         x_clamped_shifted = ops.subtract(x_clamped, nudged_min)\n         result = ops.multiply(\n             ops.floor(\n@@ -225,11 +294,11 @@ def fake_quant_with_min_max_vars(\n             ),\n             scale,\n         )\n+        result = ops.cast(result, dtype=dtype)\n \n         # Create gradient mask for all channels\n-        masks = ops.cast(\n-            (x >= nudged_min) & (x <= nudged_max),\n-            dtype=\"float32\",\n+        masks = ops.logical_and(\n+            ops.greater_equal(x, nudged_min), ops.less_equal(x, nudged_max)\n         )\n \n         def grad(*args, upstream=None):\n@@ -237,12 +306,13 @@ def fake_quant_with_min_max_vars(\n                 (upstream,) = args\n \n             # Gradient for x\n-            dx = ops.multiply(upstream, masks)\n+            dx = ops.where(masks, upstream, 0.0)\n             axes = [i for i in range(len(dx.shape)) if i != axis]\n+\n             # Gradient for min_val\n             # When x is clipped to min, the gradient flows to min_val\n-            min_mask = ops.cast(x <= nudged_min, dtype=\"float32\")\n-            grad_min = ops.multiply(upstream, min_mask)\n+            min_mask = ops.less_equal(x, nudged_min)\n+            grad_min = ops.where(min_mask, upstream, 0.0)\n             if axis is not None:\n                 grad_min = ops.sum(grad_min, axis=axes)\n             else:\n@@ -250,8 +320,8 @@ def fake_quant_with_min_max_vars(\n \n             # Gradient for max_val\n             # When x is clipped to max, the gradient flows to max_val\n-            max_mask = ops.cast(x >= nudged_max, dtype=\"float32\")\n-            grad_max = ops.multiply(upstream, max_mask)\n+            max_mask = ops.greater_equal(x, nudged_max)\n+            grad_max = ops.where(max_mask, upstream, 0.0)\n             if axis is not None:\n                 grad_max = ops.sum(grad_max, axis=axes)\n             else:\n\n@@ -1,3 +1,6 @@\n+import sys\n+\n+import pytest\n from absl.testing import parameterized\n \n from keras.src import backend\n@@ -104,6 +107,17 @@ class QuantizersTest(testing.TestCase):\n         # A loose assertion due to an expected quantization error\n         self.assertAllClose(qdq_values, values, atol=5e-1)\n \n+    @parameterized.named_parameters(\n+        (\"per_tensor\", None),\n+        (\"per_channel\", -1),\n+    )\n+    def test_fake_quant_with_min_max_vars_symbolic(self, axis):\n+        x = backend.KerasTensor((2, 3, 4))\n+        y = quantizers.fake_quant_with_min_max_vars(x, -3.0, 3.0, axis=axis)\n+\n+        self.assertIsInstance(y, backend.KerasTensor)\n+        self.assertEqual(y.shape, (2, 3, 4))\n+\n     @parameterized.named_parameters(\n         [\n             {\n@@ -334,7 +348,11 @@ class QuantizersTest(testing.TestCase):\n             },\n         ]\n     )\n-    def test_op(\n+    @pytest.mark.skipif(\n+        backend.backend() not in (\"tensorflow\", \"jax\", \"torch\"),\n+        reason=f\"{backend.backend()} doesn't support `custom_gradient`.\",\n+    )\n+    def test_fake_quant_with_min_max_vars(\n         self,\n         input_mins,\n         input_maxs,\n@@ -401,6 +419,8 @@ class QuantizersTest(testing.TestCase):\n         initial_gradients = ops.transpose(\n             ops.array(initial_gradients_list, dtype=\"float32\")\n         )\n+\n+        # Test gradients.\n         if backend.backend() == \"tensorflow\":\n             import tensorflow as tf\n \n@@ -420,16 +440,12 @@ class QuantizersTest(testing.TestCase):\n                     )\n                 return initial_gradients * tape.gradient(result, inputs)\n \n-            gradients = test_op(\n-                inputs, input_mins, input_maxs, num_bits, narrow_range, axis\n-            )\n-            # test gradients\n-            self.assertAllClose(gradients, expected_backprops_wrt_input)\n-\n         if backend.backend() == \"torch\":\n             import torch\n \n-            def test_op(inputs, input_mins, input_maxs, num_bits, narrow_range):\n+            def test_op(\n+                inputs, input_mins, input_maxs, num_bits, narrow_range, axis\n+            ):\n                 # Create tensor and enable gradient tracking\n                 inputs = torch.tensor(\n                     inputs, dtype=torch.float32, requires_grad=True\n@@ -437,7 +453,7 @@ class QuantizersTest(testing.TestCase):\n \n                 # Apply the quantization operation\n                 result = quantizers.fake_quant_with_min_max_vars(\n-                    inputs, input_mins, input_maxs, num_bits, narrow_range\n+                    inputs, input_mins, input_maxs, num_bits, narrow_range, axis\n                 )\n \n                 # Compute gradients\n@@ -445,33 +461,39 @@ class QuantizersTest(testing.TestCase):\n \n                 return initial_gradients * inputs.grad\n \n-            gradients = test_op(\n-                inputs, input_min, input_max, num_bits, narrow_range\n-            )\n-            # test gradients\n-            self.assertAllClose(gradients, expected_backprops_wrt_input)\n-\n         if backend.backend() == \"jax\":\n             import jax\n \n-            def test_op(inputs, input_mins, input_maxs, num_bits, narrow_range):\n+            def test_op(\n+                inputs, input_mins, input_maxs, num_bits, narrow_range, axis\n+            ):\n                 # Define the function to compute gradients for\n                 def quantize_fn(x):\n                     return quantizers.fake_quant_with_min_max_vars(\n-                        x, input_mins, input_maxs, num_bits, narrow_range\n+                        x, input_mins, input_maxs, num_bits, narrow_range, axis\n                     )\n \n                 _, f_vjp = jax.vjp(quantize_fn, inputs)\n-                # NOTE:python 3.10 input_gradients = f_vjp.args[0].args[0][0] !\n+\n+                # NOTE: When python version >= 3.10, the gradients are at\n+                # `f_vjp.args[0].args[0][0]`. Otherwise, they are at\n+                # `f_vjp.args[0].args[0][1]`.\n+                if sys.version_info >= (3, 10):\n+                    input_gradients = f_vjp.args[0].args[0][0]\n+                else:\n                     input_gradients = f_vjp.args[0].args[0][1]\n \n                 return ops.multiply(initial_gradients, input_gradients)\n \n         gradients = test_op(\n-                inputs, input_min, input_max, num_bits, narrow_range\n+            inputs, input_min, input_max, num_bits, narrow_range, axis\n         )\n-            # test gradients\n+        if backend.backend() != \"jax\" or not testing.jax_uses_gpu():\n+            # JAX GPU produces less precise numbers, causing the CI to fail.\n+            # For example, 127.5 / 255.0 results in 0.49999997 instead of 0.5.\n             self.assertAllClose(gradients, expected_backprops_wrt_input)\n+\n+        # Test outputs.\n         outputs = quantizers.fake_quant_with_min_max_vars(\n             inputs,\n             input_min,\n@@ -481,3 +503,26 @@ class QuantizersTest(testing.TestCase):\n             axis=axis,\n         )\n         self.assertAllClose(outputs, expected)\n+\n+        # Test bfloat16 & float16 dtype\n+        outputs = quantizers.fake_quant_with_min_max_vars(\n+            ops.cast(inputs, \"bfloat16\"),\n+            input_min,\n+            input_max,\n+            num_bits=num_bits,\n+            narrow_range=narrow_range,\n+            axis=axis,\n+        )\n+        self.assertDType(outputs, \"bfloat16\")\n+        self.assertAllClose(outputs, expected)\n+\n+        outputs = quantizers.fake_quant_with_min_max_vars(\n+            ops.cast(inputs, \"float16\"),\n+            input_min,\n+            input_max,\n+            num_bits=num_bits,\n+            narrow_range=narrow_range,\n+            axis=axis,\n+        )\n+        self.assertDType(outputs, \"float16\")\n+        self.assertAllClose(outputs, expected)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a25881cc1bbd266cdcfc53914d24c47ac3ac965c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 8 | Files Changed: 5 | Hunks: 9 | Methods Changed: 9 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 1131 | Contributors (this commit): 15 | Commits (past 90d): 18 | Contributors (cumulative): 39 | DMM Complexity: 1.0\n\nDIFF:\n@@ -547,7 +547,6 @@ class JAXTrainer(base_trainer.Trainer):\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n             callbacks = callbacks_module.CallbackList(\n                 callbacks,\n-                add_history=True,\n                 add_progbar=verbose != 0,\n                 verbose=verbose,\n                 epochs=1,\n@@ -642,7 +641,6 @@ class JAXTrainer(base_trainer.Trainer):\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n             callbacks = callbacks_module.CallbackList(\n                 callbacks,\n-                add_history=True,\n                 add_progbar=verbose != 0,\n                 verbose=verbose,\n                 epochs=1,\n\n@@ -185,7 +185,6 @@ class NumpyTrainer(base_trainer.Trainer):\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n             callbacks = callbacks_module.CallbackList(\n                 callbacks,\n-                add_history=True,\n                 add_progbar=verbose != 0,\n                 verbose=verbose,\n                 epochs=1,\n@@ -265,7 +264,6 @@ class NumpyTrainer(base_trainer.Trainer):\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n             callbacks = callbacks_module.CallbackList(\n                 callbacks,\n-                add_history=True,\n                 add_progbar=verbose != 0,\n                 verbose=verbose,\n                 epochs=1,\n\n@@ -465,7 +465,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n             callbacks = callbacks_module.CallbackList(\n                 callbacks,\n-                add_history=True,\n                 add_progbar=verbose != 0,\n                 verbose=verbose,\n                 epochs=1,\n@@ -510,7 +509,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n             callbacks = callbacks_module.CallbackList(\n                 callbacks,\n-                add_history=True,\n                 add_progbar=verbose != 0,\n                 verbose=verbose,\n                 epochs=1,\n\n@@ -353,7 +353,6 @@ class TorchTrainer(base_trainer.Trainer):\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n             callbacks = callbacks_module.CallbackList(\n                 callbacks,\n-                add_history=True,\n                 add_progbar=verbose != 0,\n                 verbose=verbose,\n                 epochs=1,\n@@ -399,7 +398,6 @@ class TorchTrainer(base_trainer.Trainer):\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n             callbacks = callbacks_module.CallbackList(\n                 callbacks,\n-                add_history=True,\n                 add_progbar=verbose != 0,\n                 verbose=verbose,\n                 epochs=1,\n\n@@ -241,3 +241,7 @@ class CallbackList(Callback):\n         logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_predict_end(logs)\n+\n+    def __del__(self):\n+        if self._executor is not None:\n+            self._executor.shutdown(cancel_futures=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#35f76b89b67eaa6fccf2ea6edb1e8945adb5ae9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 36 | Churn Cumulative: 181 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -522,6 +522,30 @@ def ConvNeXt(\n \n     model = Functional(inputs=inputs, outputs=x, name=name)\n \n+    # Validate weights before requesting them from the API\n+    if weights == \"imagenet\":\n+        expected_config = MODEL_CONFIGS[weights_name.split(\"convnext_\")[-1]]\n+        if (\n+            depths != expected_config[\"depths\"]\n+            or projection_dims != expected_config[\"projection_dims\"]\n+        ):\n+            raise ValueError(\n+                f\"Architecture configuration does not match {weights_name} \"\n+                f\"variant. When using pre-trained weights, the model \"\n+                f\"architecture must match the pre-trained configuration \"\n+                f\"exactly. Expected depths: {expected_config['depths']}, \"\n+                f\"got: {depths}. Expected projection_dims: \"\n+                f\"{expected_config['projection_dims']}, got: {projection_dims}.\"\n+            )\n+\n+        if weights_name not in name:\n+            raise ValueError(\n+                f'Model name \"{name}\" does not match weights variant '\n+                f'\"{weights_name}\". When using imagenet weights, model name '\n+                f'must contain the weights variant (e.g., \"convnext_'\n+                f'{weights_name.split(\"convnext_\")[-1]}\").'\n+            )\n+\n     # Load weights.\n     if weights == \"imagenet\":\n         if include_top:\n\n@@ -935,9 +935,17 @@ def EfficientNetV2(\n         num_channels = input_shape[bn_axis - 1]\n         if name.split(\"-\")[-1].startswith(\"b\") and num_channels == 3:\n             x = layers.Rescaling(scale=1.0 / 255)(x)\n+            if backend.image_data_format() == \"channels_first\":\n+                mean = [[[[0.485]], [[0.456]], [[0.406]]]]  # shape [1,3,1,1]\n+                variance = [\n+                    [[[0.229**2]], [[0.224**2]], [[0.225**2]]]\n+                ]  # shape [1,3,1,1]\n+            else:\n+                mean = [0.485, 0.456, 0.406]\n+                variance = [0.229**2, 0.224**2, 0.225**2]\n             x = layers.Normalization(\n-                mean=[0.485, 0.456, 0.406],\n-                variance=[0.229**2, 0.224**2, 0.225**2],\n+                mean=mean,\n+                variance=variance,\n                 axis=bn_axis,\n             )(x)\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#70125d421a4ec5e0dae6e04f6592f6c74c1a4df5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 285 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -267,9 +267,9 @@ def compute_ciou(boxes1, boxes2, bounding_box_format, image_shape=None):\n     )\n \n     v = ops.squeeze(\n-        ops.power(\n         (4 / math.pi**2)\n-            * (ops.arctan(width_2 / height_2) - ops.arctan(width_1 / height_1)),\n+        * ops.power(\n+            (ops.arctan(width_2 / height_2) - ops.arctan(width_1 / height_1)),\n             2,\n         ),\n         axis=-1,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e0464a917275b0b97111aca48b3d1225990dd043", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 314 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 5 | Methods Changed: 16 | Complexity Δ (Sum/Max): 29/22 | Churn Δ: 314 | Churn Cumulative: 1182 | Contributors (this commit): 7 | Commits (past 90d): 47 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -146,6 +146,7 @@ from keras.src.layers.preprocessing.image_preprocessing.auto_contrast import (\n from keras.src.layers.preprocessing.image_preprocessing.center_crop import (\n     CenterCrop,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.cut_mix import CutMix\n from keras.src.layers.preprocessing.image_preprocessing.equalization import (\n     Equalization,\n )\n\n@@ -146,6 +146,7 @@ from keras.src.layers.preprocessing.image_preprocessing.auto_contrast import (\n from keras.src.layers.preprocessing.image_preprocessing.center_crop import (\n     CenterCrop,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.cut_mix import CutMix\n from keras.src.layers.preprocessing.image_preprocessing.equalization import (\n     Equalization,\n )\n\n@@ -90,6 +90,7 @@ from keras.src.layers.preprocessing.image_preprocessing.auto_contrast import (\n from keras.src.layers.preprocessing.image_preprocessing.center_crop import (\n     CenterCrop,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.cut_mix import CutMix\n from keras.src.layers.preprocessing.image_preprocessing.equalization import (\n     Equalization,\n )\n\n@@ -0,0 +1,226 @@\n+from keras.src.api_export import keras_export\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n+from keras.src.random import SeedGenerator\n+\n+\n+@keras_export(\"keras.layers.CutMix\")\n+class CutMix(BaseImagePreprocessingLayer):\n+    \"\"\"CutMix data augmentation technique.\n+\n+    CutMix is a data augmentation method where patches are cut and pasted\n+    between two images in the dataset, while the labels are also mixed\n+    proportionally to the area of the patches.\n+\n+    Args:\n+        factor: A single float or a tuple of two floats between 0 and 1.\n+            If a tuple of numbers is passed, a `factor` is sampled\n+            between the two values.\n+            If a single float is passed, a value between 0 and the passed\n+            float is sampled. These values define the range from which the\n+            mixing weight is sampled. A higher factor increases the variability\n+            in patch sizes, leading to more diverse and larger mixed patches.\n+            Defaults to 1.\n+        seed: Integer. Used to create a random seed.\n+\n+    References:\n+       - [CutMix paper]( https://arxiv.org/abs/1905.04899).\n+    \"\"\"\n+\n+    _USE_BASE_FACTOR = False\n+    _FACTOR_BOUNDS = (0, 1)\n+\n+    def __init__(self, factor=1.0, seed=None, data_format=None, **kwargs):\n+        super().__init__(data_format=data_format, **kwargs)\n+        self._set_factor(factor)\n+        self.seed = seed\n+        self.generator = SeedGenerator(seed)\n+\n+        if self.data_format == \"channels_first\":\n+            self.height_axis = -2\n+            self.width_axis = -1\n+            self.channel_axis = -3\n+        else:\n+            self.height_axis = -3\n+            self.width_axis = -2\n+            self.channel_axis = -1\n+\n+    def get_random_transformation(self, data, training=True, seed=None):\n+        if not training:\n+            return None\n+\n+        if isinstance(data, dict):\n+            images = data[\"images\"]\n+        else:\n+            images = data\n+\n+        images_shape = self.backend.shape(images)\n+        if len(images_shape) == 3:\n+            return None\n+\n+        batch_size = images_shape[0]\n+        image_height = images_shape[self.height_axis]\n+        image_width = images_shape[self.width_axis]\n+\n+        seed = seed or self._get_seed_generator(self.backend._backend)\n+\n+        mix_weight = self._generate_mix_weight(batch_size, seed)\n+        ratio = self.backend.numpy.sqrt(1.0 - mix_weight)\n+\n+        x0, x1 = self._compute_crop_bounds(batch_size, image_width, ratio, seed)\n+        y0, y1 = self._compute_crop_bounds(\n+            batch_size, image_height, ratio, seed\n+        )\n+\n+        batch_masks, mix_weight = self._generate_batch_mask(\n+            images_shape,\n+            (x0, x1, y0, y1),\n+        )\n+\n+        permutation_order = self.backend.random.shuffle(\n+            self.backend.numpy.arange(0, batch_size, dtype=\"int32\"),\n+            seed=seed,\n+        )\n+\n+        return {\n+            \"permutation_order\": permutation_order,\n+            \"batch_masks\": batch_masks,\n+            \"mix_weight\": mix_weight,\n+        }\n+\n+    def _generate_batch_mask(self, images_shape, box_corners):\n+        def _generate_grid_xy(image_height, image_width):\n+            grid_y, grid_x = self.backend.numpy.meshgrid(\n+                self.backend.numpy.arange(\n+                    image_height, dtype=self.compute_dtype\n+                ),\n+                self.backend.numpy.arange(\n+                    image_width, dtype=self.compute_dtype\n+                ),\n+                indexing=\"ij\",\n+            )\n+            if self.data_format == \"channels_last\":\n+                grid_y = self.backend.cast(\n+                    grid_y[None, :, :, None], dtype=self.compute_dtype\n+                )\n+                grid_x = self.backend.cast(\n+                    grid_x[None, :, :, None], dtype=self.compute_dtype\n+                )\n+            else:\n+                grid_y = self.backend.cast(\n+                    grid_y[None, None, :, :], dtype=self.compute_dtype\n+                )\n+                grid_x = self.backend.cast(\n+                    grid_x[None, None, :, :], dtype=self.compute_dtype\n+                )\n+            return grid_x, grid_y\n+\n+        image_height, image_width = (\n+            images_shape[self.height_axis],\n+            images_shape[self.width_axis],\n+        )\n+        grid_x, grid_y = _generate_grid_xy(image_height, image_width)\n+\n+        x0, x1, y0, y1 = box_corners\n+\n+        x0 = x0[:, None, None, None]\n+        y0 = y0[:, None, None, None]\n+        x1 = x1[:, None, None, None]\n+        y1 = y1[:, None, None, None]\n+\n+        batch_masks = (\n+            (grid_x >= x0) & (grid_x < x1) & (grid_y >= y0) & (grid_y < y1)\n+        )\n+        batch_masks = self.backend.numpy.repeat(\n+            batch_masks, images_shape[self.channel_axis], axis=self.channel_axis\n+        )\n+        mix_weight = 1.0 - (x1 - x0) * (y1 - y0) / (image_width * image_height)\n+        return batch_masks, mix_weight\n+\n+    def _compute_crop_bounds(self, batch_size, image_length, crop_ratio, seed):\n+        crop_length = self.backend.cast(\n+            crop_ratio * image_length, dtype=self.compute_dtype\n+        )\n+\n+        start_pos = self.backend.random.uniform(\n+            shape=[batch_size],\n+            minval=0,\n+            maxval=1,\n+            dtype=self.compute_dtype,\n+            seed=seed,\n+        ) * (image_length - crop_length)\n+\n+        end_pos = start_pos + crop_length\n+\n+        return start_pos, end_pos\n+\n+    def _generate_mix_weight(self, batch_size, seed):\n+        alpha = (\n+            self.backend.random.uniform(\n+                shape=(),\n+                minval=self.factor[0],\n+                maxval=self.factor[1],\n+                dtype=self.compute_dtype,\n+                seed=seed,\n+            )\n+            + 1e-6\n+        )\n+        mix_weight = self.backend.random.beta(\n+            (batch_size,), alpha, alpha, seed=seed, dtype=self.compute_dtype\n+        )\n+        return mix_weight\n+\n+    def transform_images(self, images, transformation=None, training=True):\n+        if training and transformation is not None:\n+            images = self.backend.cast(images, self.compute_dtype)\n+\n+            permutation_order = transformation[\"permutation_order\"]\n+            batch_masks = transformation[\"batch_masks\"]\n+\n+            images = self.backend.numpy.where(\n+                batch_masks,\n+                self.backend.numpy.take(images, permutation_order, axis=0),\n+                images,\n+            )\n+        images = self.backend.cast(images, self.compute_dtype)\n+        return images\n+\n+    def transform_labels(self, labels, transformation, training=True):\n+        if training and transformation is not None:\n+            permutation_order = transformation[\"permutation_order\"]\n+            mix_weight = transformation[\"mix_weight\"]\n+\n+            cutout_labels = self.backend.numpy.take(\n+                labels, permutation_order, axis=0\n+            )\n+            mix_weight = self.backend.numpy.reshape(mix_weight, [-1, 1])\n+            labels = mix_weight * labels + (1.0 - mix_weight) * cutout_labels\n+\n+        return labels\n+\n+    def transform_bounding_boxes(\n+        self,\n+        bounding_boxes,\n+        transformation,\n+        training=True,\n+    ):\n+        raise NotImplementedError()\n+\n+    def transform_segmentation_masks(\n+        self, segmentation_masks, transformation, training=True\n+    ):\n+        return self.transform_images(\n+            segmentation_masks, transformation, training\n+        )\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def get_config(self):\n+        config = {\n+            \"factor\": self.factor,\n+            \"seed\": self.seed,\n+        }\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,85 @@\n+import numpy as np\n+import pytest\n+from tensorflow import data as tf_data\n+\n+from keras.src import backend\n+from keras.src import layers\n+from keras.src import testing\n+\n+\n+class CutMixTest(testing.TestCase):\n+    @pytest.mark.requires_trainable_backend\n+    def test_layer(self):\n+        self.run_layer_test(\n+            layers.CutMix,\n+            init_kwargs={\n+                \"factor\": 1.0,\n+                \"seed\": 1,\n+            },\n+            input_shape=(8, 3, 4, 3),\n+            supports_masking=False,\n+            expected_output_shape=(8, 3, 4, 3),\n+            # StatelessRandomGammaV3 is not supported on XLA_GPU_JIT\n+            run_training_check=not testing.tensorflow_uses_gpu(),\n+        )\n+\n+    def test_cut_mix_inference(self):\n+        seed = 3481\n+        layer = layers.CutMix()\n+\n+        np.random.seed(seed)\n+        inputs = np.random.randint(0, 255, size=(224, 224, 3))\n+        output = layer(inputs, training=False)\n+        self.assertAllClose(inputs, output)\n+\n+    def test_cut_mix_basic(self):\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            image1 = np.ones((2, 2, 1))\n+            image2 = np.zeros((2, 2, 1))\n+            inputs = np.asarray([image1, image2])\n+            expected_output = np.array(\n+                [\n+                    [[[1.0], [1.0]], [[1.0], [1.0]]],\n+                    [[[0.0], [0.0]], [[0.0], [0.0]]],\n+                ]\n+            )\n+        else:\n+            image1 = np.ones((1, 2, 2))\n+            image2 = np.zeros((1, 2, 2))\n+            inputs = np.asarray([image1, image2])\n+            expected_output = np.asarray(\n+                [\n+                    [[[1.0, 1.0], [1.0, 1.0]], [[1.0, 1.0], [1.0, 1.0]]],\n+                    [[[0.0, 0.0], [0.0, 0.0]], [[0.0, 0.0], [0.0, 0.0]]],\n+                ]\n+            )\n+\n+        layer = layers.CutMix(data_format=data_format)\n+\n+        transformation = {\n+            \"batch_masks\": np.asarray(\n+                [\n+                    [[[False], [True]], [[False], [False]]],\n+                    [[[False], [False]], [[True], [False]]],\n+                ]\n+            ),\n+            \"mix_weight\": np.asarray([[[[0.7826548]]], [[[0.8133545]]]]),\n+            \"permutation_order\": np.asarray([0, 1]),\n+        }\n+\n+        output = layer.transform_images(inputs, transformation)\n+\n+        self.assertAllClose(expected_output, output)\n+\n+    def test_tf_data_compatibility(self):\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            input_data = np.random.random((2, 8, 8, 3))\n+        else:\n+            input_data = np.random.random((2, 3, 8, 8))\n+        layer = layers.CutMix(data_format=data_format)\n+\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        for output in ds.take(1):\n+            output.numpy()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5df8fb93bb5f514d5ac8fc5968b9d52d08255e7d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 203 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 5 | Methods Changed: 13 | Complexity Δ (Sum/Max): 22/14 | Churn Δ: 203 | Churn Cumulative: 1074 | Contributors (this commit): 7 | Commits (past 90d): 50 | Contributors (cumulative): 23 | DMM Complexity: 0.7480916030534351\n\nDIFF:\n@@ -181,6 +181,9 @@ from keras.src.layers.preprocessing.image_preprocessing.random_grayscale import\n from keras.src.layers.preprocessing.image_preprocessing.random_hue import (\n     RandomHue,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.random_invert import (\n+    RandomInvert,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.random_posterization import (\n     RandomPosterization,\n )\n\n@@ -181,6 +181,9 @@ from keras.src.layers.preprocessing.image_preprocessing.random_grayscale import\n from keras.src.layers.preprocessing.image_preprocessing.random_hue import (\n     RandomHue,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.random_invert import (\n+    RandomInvert,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.random_posterization import (\n     RandomPosterization,\n )\n\n@@ -125,6 +125,9 @@ from keras.src.layers.preprocessing.image_preprocessing.random_grayscale import\n from keras.src.layers.preprocessing.image_preprocessing.random_hue import (\n     RandomHue,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.random_invert import (\n+    RandomInvert,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.random_posterization import (\n     RandomPosterization,\n )\n\n@@ -0,0 +1,126 @@\n+from keras.src.api_export import keras_export\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n+\n+\n+@keras_export(\"keras.layers.RandomInvert\")\n+class RandomInvert(BaseImagePreprocessingLayer):\n+    \"\"\"Preprocessing layer for random inversion of image colors.\n+\n+    This layer randomly inverts the colors of input images with a specified\n+    probability range. When applied, each image has a chance of having its\n+    colors inverted, where the pixel values are transformed to their\n+    complementary values. Images that are not selected for inversion\n+    remain unchanged.\n+\n+    Args:\n+        factor: A single float or a tuple of two floats.\n+            `factor` controls the probability of inverting the image colors.\n+            If a tuple is provided, the value is sampled between the two values\n+            for each image, where `factor[0]` is the minimum and `factor[1]` is\n+            the maximum probability. If a single float is provided, a value\n+            between `0.0` and the provided float is sampled.\n+            Defaults to `(0, 1)`.\n+        value_range: a tuple or a list of two elements. The first value\n+            represents the lower bound for values in passed images, the second\n+            represents the upper bound. Images passed to the layer should have\n+            values within `value_range`. Defaults to `(0, 255)`.\n+        seed: Integer. Used to create a random seed.\n+    \"\"\"\n+\n+    _USE_BASE_FACTOR = False\n+    _FACTOR_BOUNDS = (0, 1)\n+\n+    def __init__(\n+        self,\n+        factor=1.0,\n+        value_range=(0, 255),\n+        seed=None,\n+        data_format=None,\n+        **kwargs,\n+    ):\n+        super().__init__(data_format=data_format, **kwargs)\n+        self._set_factor(factor)\n+        self.value_range = value_range\n+        self.seed = seed\n+        self.generator = self.backend.random.SeedGenerator(seed)\n+\n+    def get_random_transformation(self, data, training=True, seed=None):\n+        if not training:\n+            return None\n+\n+        if isinstance(data, dict):\n+            images = data[\"images\"]\n+        else:\n+            images = data\n+\n+        seed = seed or self._get_seed_generator(self.backend._backend)\n+\n+        images_shape = self.backend.shape(images)\n+        rank = len(images_shape)\n+        if rank == 3:\n+            batch_size = 1\n+        elif rank == 4:\n+            batch_size = images_shape[0]\n+        else:\n+            raise ValueError(\n+                \"Expected the input image to be rank 3 or 4. Received \"\n+                f\"inputs.shape={images_shape}\"\n+            )\n+\n+        invert_probability = self.backend.random.uniform(\n+            shape=(batch_size,),\n+            minval=self.factor[0],\n+            maxval=self.factor[1],\n+            seed=seed,\n+        )\n+\n+        random_threshold = self.backend.random.uniform(\n+            shape=(batch_size,),\n+            minval=0,\n+            maxval=1,\n+            seed=seed,\n+        )\n+\n+        apply_inversion = random_threshold < invert_probability\n+        return {\"apply_inversion\": apply_inversion}\n+\n+    def transform_images(self, images, transformation, training=True):\n+        if training:\n+            images = self.backend.cast(images, self.compute_dtype)\n+            apply_inversion = transformation[\"apply_inversion\"]\n+            return self.backend.numpy.where(\n+                apply_inversion[:, None, None, None],\n+                self.value_range[1] - images,\n+                images,\n+            )\n+        return images\n+\n+    def transform_labels(self, labels, transformation, training=True):\n+        return labels\n+\n+    def transform_bounding_boxes(\n+        self,\n+        bounding_boxes,\n+        transformation,\n+        training=True,\n+    ):\n+        return bounding_boxes\n+\n+    def transform_segmentation_masks(\n+        self, segmentation_masks, transformation, training=True\n+    ):\n+        return segmentation_masks\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def get_config(self):\n+        config = {\n+            \"factor\": self.factor,\n+            \"value_range\": self.value_range,\n+            \"seed\": self.seed,\n+        }\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,68 @@\n+import numpy as np\n+import pytest\n+from tensorflow import data as tf_data\n+\n+from keras.src import backend\n+from keras.src import layers\n+from keras.src import testing\n+\n+\n+class RandomInvertTest(testing.TestCase):\n+    @pytest.mark.requires_trainable_backend\n+    def test_layer(self):\n+        self.run_layer_test(\n+            layers.RandomInvert,\n+            init_kwargs={\n+                \"factor\": 0.75,\n+                \"value_range\": (20, 200),\n+                \"seed\": 1,\n+            },\n+            input_shape=(8, 3, 4, 3),\n+            supports_masking=False,\n+            expected_output_shape=(8, 3, 4, 3),\n+        )\n+\n+    def test_random_invert_inference(self):\n+        seed = 3481\n+        layer = layers.RandomInvert()\n+        np.random.seed(seed)\n+        inputs = np.random.randint(0, 255, size=(224, 224, 3))\n+        output = layer(inputs, training=False)\n+        self.assertAllClose(inputs, output)\n+\n+    def test_random_invert_no_op(self):\n+        seed = 3481\n+        layer = layers.RandomInvert(factor=0)\n+        np.random.seed(seed)\n+        inputs = np.random.randint(0, 255, size=(224, 224, 3))\n+        output = layer(inputs)\n+        self.assertAllClose(inputs, output)\n+\n+    def test_random_invert_basic(self):\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            input_data = np.random.random((1, 8, 8, 3))\n+        else:\n+            input_data = np.random.random((1, 3, 8, 8))\n+        layer = layers.RandomInvert(\n+            factor=(1, 1),\n+            value_range=[0, 1],\n+            data_format=data_format,\n+            seed=1337,\n+        )\n+        output = layer(input_data)\n+        self.assertAllClose(1 - input_data, output)\n+\n+    def test_tf_data_compatibility(self):\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            input_data = np.random.random((2, 8, 8, 3))\n+        else:\n+            input_data = np.random.random((2, 3, 8, 8))\n+        layer = layers.RandomInvert(\n+            factor=0.5, value_range=[0, 1], data_format=data_format, seed=1337\n+        )\n+\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        for output in ds.take(1):\n+            output.numpy()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
