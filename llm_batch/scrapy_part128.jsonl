{"custom_id": "scrapy#541ed3913b63fc3505ef426cc22d5ce36a48014b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 11 | Churn Cumulative: 92 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,6 +25,7 @@ from twisted.internet.defer import TimeoutError as UserTimeoutError\n from twisted.web.client import PartialDownloadError\n \n from scrapy import log\n+from scrapy.exceptions import NotConfigured\n from scrapy.utils.response import response_status_message\n from scrapy.conf import settings\n \n@@ -37,8 +38,10 @@ class RetryMiddleware(object):\n                            ConnectionLost, PartialDownloadError, IOError)\n \n     def __init__(self):\n+        if not settings.getbool('RETRY_ENABLED'):\n+            raise NotConfigured\n         self.max_retry_times = settings.getint('RETRY_TIMES')\n-        self.retry_http_codes = map(int, settings.getlist('RETRY_HTTP_CODES'))\n+        self.retry_http_codes = set(int(x) for x in settings.getlist('RETRY_HTTP_CODES'))\n         self.priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')\n \n     def process_response(self, request, response, spider):\n@@ -66,6 +69,6 @@ class RetryMiddleware(object):\n             retryreq.priority = request.priority + self.priority_adjust\n             return retryreq\n         else:\n-            log.msg(\"Discarding %s (failed %d times): %s\" % (request, retries, reason),\n+            log.msg(\"Gave up retrying %s (failed %d times): %s\" % (request, retries, reason),\n                     spider=spider, level=log.DEBUG)\n \n\n@@ -211,9 +211,9 @@ REDIRECT_MAX_METAREFRESH_DELAY = 100\n REDIRECT_MAX_TIMES = 20 # uses Firefox default setting\n REDIRECT_PRIORITY_ADJUST = +2\n \n-# contrib.middleware.retry.RetryMiddleware default settings\n+RETRY_ENABLED = True\n RETRY_TIMES = 2 # initial response + 2 retries = 3 requests\n-RETRY_HTTP_CODES = ['500', '503', '504', '400', '408']\n+RETRY_HTTP_CODES = [500, 503, 504, 400, 408]\n RETRY_PRIORITY_ADJUST = -1\n \n ROBOTSTXT_OBEY = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#804c0279ec9735bba20774b3bb77cf648e6b95d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 163 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -120,10 +120,15 @@ setup_args = {\n \n try:\n     from setuptools import setup\n-    setup_args['install_requires'] = ['Twisted>=2.5', 'lxml', 'w3lib', 'pyOpenSSL']\n-    if sys.version_info < (2, 6):\n-        setup_args['install_requires'] += ['simplejson']\n except ImportError:\n     from distutils.core import setup\n+else:\n+    setup_args['install_requires'] = ['Twisted>=2.5', 'w3lib', 'pyOpenSSL']\n+    if sys.version_info < (2, 6):\n+        setup_args['install_requires'] += ['simplejson']\n+    try:\n+        import libxml2\n+    except ImportError:\n+        setup_args['install_requires'] += ['lxml']\n  \n setup(**setup_args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0b6c7ce9b88923c7a0f473c825946c24bd609c56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 38 | Files Changed: 3 | Hunks: 14 | Methods Changed: 9 | Complexity Δ (Sum/Max): -4/1 | Churn Δ: 56 | Churn Cumulative: 2432 | Contributors (this commit): 6 | Commits (past 90d): 16 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,7 +13,7 @@ from scrapy import log, signals\n from scrapy.stats import stats\n from scrapy.core.downloader import Downloader\n from scrapy.core.scraper import Scraper\n-from scrapy.exceptions import IgnoreRequest, DontCloseSpider\n+from scrapy.exceptions import DontCloseSpider\n from scrapy.http import Response, Request\n from scrapy.utils.misc import load_object\n from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n@@ -139,7 +139,7 @@ class ExecutionEngine(object):\n         # response is a Response or Failure\n         d = defer.Deferred()\n         d.addBoth(self.scraper.enqueue_scrape, request, spider)\n-        d.addErrback(log.err, \"Unhandled error on engine.crawl()\", spider=spider)\n+        d.addErrback(log.err, spider=spider)\n         if isinstance(response, Failure):\n             d.errback(response)\n         else:\n@@ -204,27 +204,12 @@ class ExecutionEngine(object):\n                     response=response, request=request, spider=spider)\n             return response\n \n-        def _on_error(_failure):\n-            \"\"\"handle an error processing a page\"\"\"\n-            exc = _failure.value\n-            if isinstance(exc, IgnoreRequest):\n-                errmsg = _failure.getErrorMessage()\n-            else:\n-                errmsg = str(_failure)\n-            if errmsg:\n-                log.msg(\"Error downloading <%s>: %s\" % (request.url, errmsg), \\\n-                    level=log.ERROR, spider=spider)\n-            return Failure(IgnoreRequest(str(exc)))\n-\n         def _on_complete(_):\n             self.next_request(spider)\n             return _\n \n-        if spider not in self.downloader.sites:\n-            return defer.fail(Failure(IgnoreRequest())).addBoth(_on_complete)\n-\n         dwld = mustbe_deferred(self.downloader.fetch, request, spider)\n-        dwld.addCallbacks(_on_success, _on_error)\n+        dwld.addCallback(_on_success)\n         dwld.addBoth(_on_complete)\n         return dwld\n \n\n@@ -10,7 +10,7 @@ from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errba\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.misc import load_object\n from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n-from scrapy.exceptions import CloseSpider, IgnoreRequest, DropItem\n+from scrapy.exceptions import CloseSpider, DropItem\n from scrapy import signals\n from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n@@ -134,7 +134,7 @@ class Scraper(object):\n         else:\n             # FIXME: don't ignore errors in spider middleware\n             dfd = self.call_spider(request_result, request, spider)\n-            return dfd.addErrback(self._check_propagated_failure, \\\n+            return dfd.addErrback(self._log_download_errors, \\\n                 request_result, request, spider)\n \n     def call_spider(self, result, request, spider):\n@@ -142,15 +142,12 @@ class Scraper(object):\n         dfd.addCallbacks(request.callback or spider.parse, request.errback)\n         return dfd.addCallback(iterate_spider_output)\n \n-    def handle_spider_error(self, _failure, request, response, spider, propagated_failure=None):\n+    def handle_spider_error(self, _failure, request, response, spider):\n         exc = _failure.value\n         if isinstance(exc, CloseSpider):\n             self.engine.close_spider(spider, exc.reason or 'cancelled')\n             return\n-        referer = request.headers.get('Referer', None)\n-        msg = \"Spider error processing <%s> (referer: <%s>)\" % \\\n-            (request.url, referer)\n-        log.err(_failure, msg, spider=spider)\n+        log.err(_failure, \"Spider error processing %s\" % request, spider=spider)\n         send_catch_log(signal=signals.spider_error, failure=_failure, response=response, \\\n             spider=spider)\n         stats.inc_value(\"spider_exceptions/%s\" % _failure.value.__class__.__name__, \\\n@@ -183,20 +180,15 @@ class Scraper(object):\n             log.msg(\"Spider must return Request, BaseItem or None, got %r in %s\" % \\\n                 (type(output).__name__, request), log.ERROR, spider=spider)\n \n-    def _check_propagated_failure(self, spider_failure, propagated_failure, request, spider):\n-        \"\"\"Log and silence the bugs raised outside of spiders, but still allow\n-        spiders to be notified about general failures while downloading spider\n-        generated requests\n+    def _log_download_errors(self, spider_failure, download_failure, request, spider):\n+        \"\"\"Log and silence errors that come from the engine (typically download\n+        errors that got propagated thru here)\n         \"\"\"\n-        # ignored requests are commonly propagated exceptions safes to be silenced\n-        if isinstance(spider_failure.value, IgnoreRequest):\n+        if spider_failure is download_failure:\n+            log.msg(\"Error downloading %s: %s\" % \\\n+                (request, spider_failure.getErrorMessage()), log.ERROR, spider=spider)\n             return\n-        elif spider_failure is propagated_failure:\n-            log.err(spider_failure, 'Unhandled error propagated to spider', \\\n-                spider=spider)\n-            return # stop propagating this error\n-        else:\n-            return spider_failure # exceptions raised in the spider code\n+        return spider_failure\n \n     def _itemproc_finished(self, output, item, response, spider):\n         \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n\n@@ -1,5 +1,7 @@\n import os\n \n+from twisted.python.failure import Failure\n+\n class LogFormatter(object):\n     \"\"\"Class for generating log messages for different actions. All methods\n     must return a plain string which doesn't include the log level or the\n@@ -13,7 +15,8 @@ class LogFormatter(object):\n             request, referer, flags)\n \n     def scraped(self, item, response, spider):\n-        return u\"Scraped from %s%s%s\" % (response, os.linesep, item)\n+        src = response.getErrorMessage() if isinstance(response, Failure) else response\n+        return u\"Scraped from %s%s%s\" % (src, os.linesep, item)\n \n     def dropped(self, item, exception, response, spider):\n         return u\"Dropped: %s%s%s\" % (exception, os.linesep, item)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#39a2ea97c86fc100e89a119381b3c321b6fbc04d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 134 | Contributors (this commit): 6 | Commits (past 90d): 11 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,6 +11,8 @@ class RedirectMiddleware(object):\n     \"\"\"Handle redirection of requests based on response status and meta-refresh html tag\"\"\"\n \n     def __init__(self):\n+        if not settings.getbool('REDIRECT_ENABLED'):\n+            raise NotConfigured\n         self.max_metarefresh_delay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY')\n         self.max_redirect_times = settings.getint('REDIRECT_MAX_TIMES')\n         self.priority_adjust = settings.getint('REDIRECT_PRIORITY_ADJUST')\n\n@@ -207,6 +207,7 @@ QUEUE_POLL_INTERVAL = 5\n \n RANDOMIZE_DOWNLOAD_DELAY = True\n \n+REDIRECT_ENABLED = True\n REDIRECT_MAX_METAREFRESH_DELAY = 100\n REDIRECT_MAX_TIMES = 20 # uses Firefox default setting\n REDIRECT_PRIORITY_ADJUST = +2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#359129adf9d661ed1a58f7dec0b7afb02817a31e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 12 | Churn Cumulative: 368 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,13 +3,12 @@ import os\n from subprocess import Popen, PIPE\n import unittest\n \n-import scrapy\n-\n class CmdlineTest(unittest.TestCase):\n \n     def setUp(self):\n         self.env = os.environ.copy()\n-        self.env['PYTHONPATH'] = os.path.dirname(scrapy.__path__[0])\n+        if 'PYTHONPATH' in os.environ:\n+            self.env['PYTHONPATH'] = os.environ['PYTHONPATH']\n         self.env['SCRAPY_SETTINGS_MODULE'] = 'scrapy.tests.test_cmdline.settings'\n \n     def _execute(self, *new_args, **kwargs):\n\n@@ -3,14 +3,12 @@ from __future__ import with_statement\n import sys\n import os\n import subprocess\n-from os.path import exists, join, dirname, abspath\n+from os.path import exists, join, abspath\n from shutil import rmtree\n from tempfile import mkdtemp\n \n from twisted.trial import unittest\n \n-import scrapy\n-\n \n class ProjectTest(unittest.TestCase):\n     project_name = 'testproject'\n@@ -21,7 +19,8 @@ class ProjectTest(unittest.TestCase):\n         self.proj_path = join(self.temp_path, self.project_name)\n         self.proj_mod_path = join(self.proj_path, self.project_name)\n         self.env = os.environ.copy()\n-        self.env['PYTHONPATH'] = dirname(scrapy.__path__[0])\n+        if 'PYTHONPATH' in os.environ:\n+            self.env['PYTHONPATH'] = os.environ['PYTHONPATH']\n \n     def tearDown(self):\n         rmtree(self.temp_path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#830255eea3625d4ecf800aebc7b7f71fd05c0511", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 84 | Files Changed: 2 | Hunks: 2 | Methods Changed: 9 | Complexity Δ (Sum/Max): -18/0 | Churn Δ: 84 | Churn Cumulative: 232 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.2692307692307692\n\nDIFF:\n@@ -1,68 +0,0 @@\n-from twisted.internet import reactor, threads\n-\n-from scrapy.command import ScrapyCommand\n-from scrapy.commands import runserver\n-from scrapy.exceptions import UsageError\n-from scrapy.utils.conf import arglist_to_dict\n-\n-class Command(runserver.Command):\n-\n-    requires_project = True\n-    default_settings = {'LOG_LEVEL': 'WARNING'}\n-\n-    def syntax(self):\n-        return \"[options] <list|clear|count|add spider1 ..>\"\n-\n-    def short_desc(self):\n-        return \"Deprecated command. See Scrapyd documentation.\"\n-\n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-a\", \"--arg\", dest=\"spargs\", action=\"append\", default=[], \\\n-            help=\"set spider argument (may be repeated)\")\n-\n-    def process_options(self, args, opts):\n-        ScrapyCommand.process_options(self, args, opts)\n-        try:\n-            opts.spargs = arglist_to_dict(opts.spargs)\n-        except ValueError:\n-            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n-\n-    def run(self, args, opts):\n-        if len(args) < 1:\n-            raise UsageError()\n-        cmd = args[0]\n-\n-        import warnings\n-        warnings.warn(\"Scrapy queue command is deprecated - use Scrapyd instead.\", \\\n-            DeprecationWarning)\n-\n-        q = self.crawler.queue._queue\n-\n-        if cmd == 'add' and len(args) < 2:\n-            raise UsageError()\n-\n-        d = threads.deferToThread(self._run_in_thread, args, opts, q, cmd)\n-        d.addBoth(lambda _: reactor.stop())\n-        from scrapy import log\n-        log.start()\n-        reactor.run()\n-\n-    def _run_in_thread(self, args, opts, q, cmd):\n-        if cmd == 'add':\n-            for x in args[1:]:\n-                self._call(q.add, x, **opts.spargs)\n-                print \"Added: name=%s args=%s\" % (x, opts.spargs)\n-        elif cmd == 'list':\n-            x = self._call(q.list)\n-            print \"\\n\".join(map(str, x))\n-        elif cmd == 'count':\n-            print self._call(q.count)\n-        elif cmd == 'clear':\n-            self._call(q.clear)\n-        else:\n-            raise UsageError()\n-\n-    def _call(self, f, *a, **kw):\n-        return threads.blockingCallFromThread(reactor, f, *a, **kw)\n-\n\n@@ -1,16 +0,0 @@\n-from scrapy.command import ScrapyCommand\n-from scrapy.conf import settings\n-\n-class Command(ScrapyCommand):\n-\n-    requires_project = True\n-    default_settings = {'KEEP_ALIVE': True}\n-\n-    def short_desc(self):\n-        return \"Deprecated command. Use 'server' command instead\"\n-\n-    def run(self, args, opts):\n-        import warnings\n-        warnings.warn(\"Scrapy queue command is deprecated - use 'server' command instead.\", \\\n-            DeprecationWarning)\n-        self.crawler.start()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d207c0afe4a82ac1b0299cc85a47914db6d409f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1719 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -177,7 +177,7 @@ class ExecutionEngine(object):\n         return self.scheduler.enqueue_request(spider, request)\n \n     def download(self, request, spider):\n-        slot = self.slots[request]\n+        slot = self.slots[spider]\n         slot.add_request(request)\n         if isinstance(request, Response):\n             return request\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4dadeb7ccb7386fa1859f63a60e47b2ee4537ea4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1729 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -179,16 +179,14 @@ class ExecutionEngine(object):\n     def download(self, request, spider):\n         slot = self.slots[spider]\n         slot.add_request(request)\n-        if isinstance(request, Response):\n-            return request\n         d = self._download(request, spider)\n-        d.addCallback(self.download, spider)\n-        d.addBoth(self._remove_request, slot, request)\n+        d.addBoth(self._downloaded, slot, request, spider)\n         return d\n \n-    def _remove_request(self, _, slot, request):\n+    def _downloaded(self, response, slot, request, spider):\n         slot.remove_request(request)\n-        return _\n+        return self.download(response, spider) \\\n+                if isinstance(response, Request) else response\n \n     def _download(self, request, spider):\n         slot = self.slots[spider]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#84f518fc5ebbf3052ac38b06cbd3f966e01c97d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 334 | Files Changed: 13 | Hunks: 38 | Methods Changed: 51 | Complexity Δ (Sum/Max): -60/3 | Churn Δ: 365 | Churn Cumulative: 4259 | Contributors (this commit): 10 | Commits (past 90d): 40 | Contributors (cumulative): 26 | DMM Complexity: 0.06153846153846154\n\nDIFF:\n@@ -1,32 +1,21 @@\n-from w3lib.url import is_url\n-\n-from scrapy import log\n from scrapy.command import ScrapyCommand\n-from scrapy.conf import settings\n-from scrapy.http import Request\n from scrapy.utils.conf import arglist_to_dict\n from scrapy.exceptions import UsageError\n \n-from collections import defaultdict\n-\n class Command(ScrapyCommand):\n \n     requires_project = True\n \n     def syntax(self):\n-        return \"[options] <spider|url> ...\"\n+        return \"[options] <spider>\"\n \n     def short_desc(self):\n         return \"Start crawling from a spider or URL\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--spider\", dest=\"spider\", default=None, \\\n-            help=\"always use this spider when arguments are urls\")\n         parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n             help=\"set spider argument (may be repeated)\")\n-        parser.add_option(\"-n\", \"--nofollow\", dest=\"nofollow\", action=\"store_true\", \\\n-            help=\"don't follow links (for use with URLs only)\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -34,49 +23,11 @@ class Command(ScrapyCommand):\n             opts.spargs = arglist_to_dict(opts.spargs)\n         except ValueError:\n             raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n-        if opts.nofollow:\n-            settings.overrides['CRAWLSPIDER_FOLLOW_LINKS'] = False\n \n     def run(self, args, opts):\n-        q = self.crawler.queue\n-        urls, names = self._split_urls_and_names(args)\n-        for name in names:\n-            q.append_spider_name(name, **opts.spargs)\n-\n-        if opts.spider:\n-            try:\n-                spider = self.crawler.spiders.create(opts.spider, **opts.spargs)\n-                for url in urls:\n-                    q.append_url(url, spider)\n-            except KeyError:\n-                log.msg('Unable to find spider: %s' % opts.spider, log.ERROR)\n-        else:\n-            for name, urls in self._group_urls_by_spider(urls):\n-                spider = self.crawler.spiders.create(name, **opts.spargs)\n-                for url in urls:\n-                    q.append_url(url, spider)\n+        if len(args) < 1:\n+            raise UsageError()\n+        for spname in args:\n+            spider = self.crawler.spiders.create(spname, **opts.spargs)\n+            self.crawler.crawl(spider)\n         self.crawler.start()\n-\n-    def _group_urls_by_spider(self, urls):\n-        spider_urls = defaultdict(list)\n-        for url in urls:\n-            spider_names = self.crawler.spiders.find_by_request(Request(url))\n-            if not spider_names:\n-                log.msg('Could not find spider that handles url: %s' % url,\n-                        log.ERROR)\n-            elif len(spider_names) > 1:\n-                log.msg('More than one spider can handle url: %s - %s' % \\\n-                    (url, \", \".join(spider_names)), log.ERROR)\n-            else:\n-                spider_urls[spider_names[0]].append(url)\n-        return spider_urls.items()\n-\n-    def _split_urls_and_names(self, args):\n-        urls = []\n-        names = []\n-        for arg in args:\n-            if is_url(arg):\n-                urls.append(arg)\n-            else:\n-                names.append(arg)\n-        return urls, names\n\n@@ -1,10 +1,10 @@\n from w3lib.url import is_url\n \n-from scrapy import log\n from scrapy.command import ScrapyCommand\n from scrapy.http import Request\n from scrapy.spider import BaseSpider\n from scrapy.exceptions import UsageError\n+from scrapy.utils.spider import create_spider_for_request\n \n class Command(ScrapyCommand):\n \n@@ -49,12 +49,10 @@ class Command(ScrapyCommand):\n \n         spider = None\n         if opts.spider:\n-            try:\n             spider = self.crawler.spiders.create(opts.spider)\n-            except KeyError:\n-                log.msg(\"Could not find spider: %s\" % opts.spider, log.ERROR)\n-\n-        self.crawler.queue.append_request(request, spider, \\\n+        else:\n+            spider = create_spider_for_request(self.crawler.spiders, request, \\\n                 default_spider=BaseSpider('default'))\n+        self.crawler.crawl(spider, [request])\n         self.crawler.start()\n \n\n@@ -84,7 +84,7 @@ class Command(ScrapyCommand):\n         spider = self.get_spider(request, opts)\n         if not spider:\n             return None, None\n-        self.crawler.queue.append_request(request, spider)\n+        self.crawler.crawl(spider, [request])\n         self.crawler.start()\n         if not responses:\n             log.msg('No response downloaded for: %s' % request, log.ERROR, \\\n\n@@ -60,6 +60,6 @@ class Command(ScrapyCommand):\n         if not spclasses:\n             raise UsageError(\"No spider found in file: %s\\n\" % filename)\n         spider = spclasses.pop()(**opts.spargs)\n-        # schedule spider and start engine\n-        self.crawler.queue.append_spider(spider)\n+\n+        self.crawler.crawl(spider)\n         self.crawler.start()\n\n@@ -11,7 +11,7 @@ from scrapy import log\n class Command(ScrapyCommand):\n \n     requires_project = False\n-    default_settings = {'KEEP_ALIVE': True}\n+    default_settings = {'KEEP_ALIVE': True, 'LOGSTATS_INTERVAL': 0}\n \n     def syntax(self):\n         return \"[url|file]\"\n\n@@ -21,9 +21,11 @@ from scrapy.utils.defer import mustbe_deferred\n \n class Slot(object):\n \n-    def __init__(self):\n+    def __init__(self, start_requests, close_if_idle):\n         self.closing = False\n         self.inprogress = set() # requests in progress\n+        self.requests = iter(start_requests)\n+        self.close_if_idle = close_if_idle\n \n     def add_request(self, request):\n         self.inprogress.add(request)\n@@ -107,6 +109,12 @@ class ExecutionEngine(object):\n                 break\n \n         if self.spider_is_idle(spider):\n+            slot = self.slots[spider]\n+            try:\n+                request = slot.requests.next()\n+                self.crawl(request, spider)\n+            except StopIteration:\n+                if slot.close_if_idle:\n                     self._spider_idle(spider)\n \n     def _needs_backout(self, spider):\n@@ -212,11 +220,11 @@ class ExecutionEngine(object):\n         return dwld\n \n     @defer.inlineCallbacks\n-    def open_spider(self, spider):\n+    def open_spider(self, spider, start_requests=None, close_if_idle=True):\n         assert self.has_capacity(), \"No free spider slots when opening %r\" % \\\n             spider.name\n         log.msg(\"Spider opened\", spider=spider)\n-        self.slots[spider] = Slot()\n+        self.slots[spider] = Slot(start_requests or (), close_if_idle)\n         yield self.scheduler.open_spider(spider)\n         self.downloader.open_spider(spider)\n         yield self.scraper.open_spider(spider)\n\n@@ -3,7 +3,6 @@ import signal\n from twisted.internet import reactor, defer\n \n from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.queue import ExecutionQueue\n from scrapy.core.engine import ExecutionEngine\n from scrapy.extension import ExtensionManager\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n@@ -34,51 +33,25 @@ class Crawler(object):\n         self.extensions = ExtensionManager.from_settings(self.settings)\n         spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])\n         self.spiders = spman_cls.from_settings(self.settings)\n-        spq_cls = load_object(self.settings['SPIDER_QUEUE_CLASS'])\n-        spq = spq_cls.from_settings(self.settings)\n-        keepalive = self.settings.getbool('KEEP_ALIVE')\n-        pollint = self.settings.getfloat('QUEUE_POLL_INTERVAL')\n-        self.queue = ExecutionQueue(self.spiders, spq, poll_interval=pollint,\n-            keep_alive=keepalive)\n         self.engine = ExecutionEngine(self.settings, self._spider_closed)\n \n-    @defer.inlineCallbacks\n-    def _start_next_spider(self):\n-        spider, requests = yield defer.maybeDeferred(self.queue.get_next)\n-        if spider:\n-            self._start_spider(spider, requests)\n-        if self.engine.has_capacity() and not self._nextcall.active():\n-            self._nextcall = reactor.callLater(self.queue.poll_interval, \\\n-                self._spider_closed)\n-\n-    @defer.inlineCallbacks\n-    def _start_spider(self, spider, requests):\n-        \"\"\"Don't call this method. Use self.queue to start new spiders\"\"\"\n+    def crawl(self, spider, requests=None):\n         spider.set_crawler(self)\n-        yield defer.maybeDeferred(self.engine.open_spider, spider)\n-        for request in requests:\n-            self.engine.crawl(request, spider)\n+        if requests is None:\n+            requests = spider.start_requests()\n+        return self.engine.open_spider(spider, requests)\n \n-    @defer.inlineCallbacks\n     def _spider_closed(self, spider=None):\n         if not self.engine.open_spiders:\n-            is_finished = yield defer.maybeDeferred(self.queue.is_finished)\n-            if is_finished:\n             self.stop()\n-                return\n-        if self.engine.has_capacity():\n-            self._start_next_spider()\n \n     @defer.inlineCallbacks\n     def start(self):\n         yield defer.maybeDeferred(self.configure)\n         yield defer.maybeDeferred(self.engine.start)\n-        self._nextcall = reactor.callLater(0, self._start_next_spider)\n \n     @defer.inlineCallbacks\n     def stop(self):\n-        if self._nextcall.active():\n-            self._nextcall.cancel()\n         if self.engine.running:\n             yield defer.maybeDeferred(self.engine.stop)\n \n\n@@ -1,96 +0,0 @@\n-from twisted.internet import defer\n-\n-from scrapy.http import Request\n-from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.spider import create_spider_for_request\n-from scrapy.utils.python import stringify_dict\n-from scrapy import log\n-\n-\n-class ExecutionQueue(object):\n-\n-    def __init__(self, spiders, queue, poll_interval=5, keep_alive=False):\n-        self.spider_requests = []\n-        self.poll_interval = poll_interval\n-        self._spiders = spiders\n-        self._queue = queue\n-        self._keepalive = keep_alive\n-\n-    @defer.inlineCallbacks\n-    def _append_next(self):\n-        \"\"\"Called when there are no more items left in self.spider_requests.\n-        This method is meant to be overriden in subclasses to add new (spider,\n-        requests) tuples to self.spider_requests. It can return a Deferred.\n-        \"\"\"\n-        msg = yield self._queue.pop()\n-        if msg:\n-            name = msg.pop('name')\n-            msg = stringify_dict(msg) # see #250\n-            self.append_spider_name(name, **msg)\n-\n-    def get_next(self):\n-        \"\"\"Return a tuple (spider, requests) containing a list of Requests and\n-        the Spider which will be used to crawl those Requests. If there aren't\n-        any more spiders to crawl it must return (None, []).\n-\n-        This method can return a deferred.\n-        \"\"\"\n-        if self.spider_requests:\n-            return self._get_next_now()\n-        d = defer.maybeDeferred(self._append_next)\n-        d.addCallback(lambda _: self._get_next_now())\n-        return d\n-\n-    def _get_next_now(self):\n-        try:\n-            return self.spider_requests.pop(0)\n-        except IndexError:\n-            return (None, [])\n-\n-    def is_finished(self):\n-        \"\"\"Return True if the queue is empty and there won't be any more\n-        spiders to crawl (this is for one-shot runs). If it returns ``False``\n-        Scrapy will keep polling this queue for new requests to scrape\n-        \"\"\"\n-        return not self._keepalive and not bool(self.spider_requests)\n-\n-    def append_spider(self, spider):\n-        \"\"\"Append a Spider to crawl\"\"\"\n-        requests = spider.start_requests()\n-        self.spider_requests.append((spider, requests))\n-\n-    def append_request(self, request, spider=None, **kwargs):\n-        if spider is None:\n-            spider = create_spider_for_request(self._spiders, request, **kwargs)\n-        if spider:\n-            self.spider_requests.append((spider, [request]))\n-\n-    def append_url(self, url=None, spider=None, **kwargs):\n-        \"\"\"Append a URL to crawl with the given spider. If the spider is not\n-        given, a spider will be looked up based on the URL\n-        \"\"\"\n-        if url is None:\n-            raise ValueError(\"A url is required\")\n-        if spider is None:\n-            spider = create_spider_for_request(self._spiders, Request(url), \\\n-                **kwargs)\n-        if spider:\n-            requests = arg_to_iter(spider.make_requests_from_url(url))\n-            self.spider_requests.append((spider, requests))\n-\n-    def append_spider_name(self, name=None, **spider_kwargs):\n-        \"\"\"Append a spider to crawl given its name and optional arguments,\n-        which are used to instantiate it. The SpiderManager is used to lookup\n-        the spider\n-        \"\"\"\n-        if name is None:\n-            raise ValueError(\"A spider name is required\")\n-        try:\n-            spider = self._spiders.create(name, **spider_kwargs)\n-        except KeyError:\n-            log.msg('Unable to find spider: %s' % name, log.ERROR)\n-        except:\n-            log.err(None, 'Error creating spider %r with arguments: %r' % \\\n-                (name, spider_kwargs))\n-        else:\n-            self.append_spider(spider)\n\n@@ -174,8 +174,6 @@ ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'\n # Item pipelines are typically set in specific commands settings\n ITEM_PIPELINES = []\n \n-KEEP_ALIVE = False\n-\n LOG_ENABLED = True\n LOG_ENCODING = 'utf-8'\n LOG_FORMATTER = 'scrapy.logformatter.LogFormatter'\n@@ -203,8 +201,6 @@ MEMUSAGE_WARNING_MB = 0\n \n NEWSPIDER_MODULE = ''\n \n-QUEUE_POLL_INTERVAL = 5\n-\n RANDOMIZE_DOWNLOAD_DELAY = True\n \n REDIRECT_ENABLED = True\n\n@@ -61,7 +61,7 @@ class Shell(object):\n             spider = create_spider_for_request(self.crawler.spiders, request, \\\n                 BaseSpider('default'), log_multiple=True)\n         spider.set_crawler(self.crawler)\n-        self.crawler.engine.open_spider(spider)\n+        self.crawler.engine.open_spider(spider, close_if_idle=False)\n         d = request_deferred(request)\n         d.addCallback(lambda x: (x, spider))\n         self.crawler.engine.crawl(request, spider)\n\n@@ -103,9 +103,6 @@ class GenspiderCommandTest(CommandTest):\n \n class MiscCommandsTest(CommandTest):\n \n-    def test_crawl(self):\n-        self.assertEqual(0, self.call('crawl'))\n-\n     def test_list(self):\n         self.assertEqual(0, self.call('list'))\n \n\n@@ -97,7 +97,7 @@ class CrawlerRun(object):\n         self.crawler = get_crawler()\n         self.crawler.install()\n         self.crawler.configure()\n-        self.crawler.queue.append_spider(self.spider)\n+        self.crawler.crawl(self.spider)\n         self.crawler.start()\n \n         self.deferred = defer.Deferred()\n\n@@ -1,130 +0,0 @@\n-import unittest\n-\n-from scrapy.queue import ExecutionQueue\n-from scrapy.spider import BaseSpider\n-from scrapy.http import Request\n-\n-class TestSpider(BaseSpider):\n-\n-    name = \"default\"\n-\n-    def start_requests(self):\n-        return [Request(\"http://www.example.com/1\"), \\\n-            Request(\"http://www.example.com/2\")]\n-\n-    def make_requests_from_url(self, url):\n-        return [Request(url + \"/make1\"), Request(url + \"/make2\")]\n-\n-\n-class TestSpiderManager(object):\n-\n-    def find_by_request(self, request):\n-        return ['create_for_request']\n-\n-    def create(self, spider_name, **spider_kwargs):\n-        return TestSpider(spider_name, **spider_kwargs)\n-\n-\n-class ExecutionQueueTest(unittest.TestCase):\n-\n-    keep_alive = False\n-\n-    def setUp(self):\n-        self.queue = ExecutionQueue(TestSpiderManager(), None, keep_alive=self.keep_alive)\n-        self.spider = TestSpider()\n-        self.request = Request('about:none')\n-\n-    def tearDown(self):\n-        del self.queue, self.spider, self.request\n-\n-    def test_is_finished(self):\n-        self.assert_(self.queue.is_finished())\n-        self.queue.append_request(self.request, self.spider)\n-        self.assert_(not self.queue.is_finished())\n-\n-    def test_append_spider(self):\n-        spider = TestSpider()\n-        self.queue.append_spider(spider)\n-        self.assert_(self.queue.spider_requests[0][0] is spider)\n-        self._assert_request_urls(self.queue.spider_requests[0][1],\n-            [\"http://www.example.com/1\", \"http://www.example.com/2\"])\n-\n-    def test_append_request1(self):\n-        spider = TestSpider()\n-        request = Request('about:blank')\n-        self.queue.append_request(request, spider=spider)\n-        self.assert_(self.queue.spider_requests[0][0] is spider)\n-        self.assert_(self.queue.spider_requests[0][1][0] is request)\n-\n-    def test_append_request2(self):\n-        request = Request('about:blank')\n-        self.queue.append_request(request, arg='123')\n-        spider = self.queue.spider_requests[0][0]\n-        self.assert_(spider.name == 'create_for_request')\n-        self.assert_(spider.arg == '123')\n-\n-    def test_append_url(self):\n-        spider = TestSpider()\n-        url = 'http://www.example.com/asd'\n-        self.queue.append_url(url, spider=spider)\n-        self.assert_(self.queue.spider_requests[0][0] is spider)\n-        self._assert_request_urls(self.queue.spider_requests[0][1], \\\n-            ['http://www.example.com/asd/make1', 'http://www.example.com/asd/make2'])\n-\n-    def test_append_url_kwarg(self):\n-        spider = TestSpider()\n-        url = 'http://www.example.com/asd'\n-        self.queue.append_url(url=url, spider=spider)\n-        self.assert_(self.queue.spider_requests[0][0] is spider)\n-        self._assert_request_urls(self.queue.spider_requests[0][1], \\\n-            ['http://www.example.com/asd/make1', 'http://www.example.com/asd/make2'])\n-\n-    def test_append_url2(self):\n-        url = 'http://www.example.com/asd'\n-        self.queue.append_url(url, arg='123')\n-        self._assert_request_urls(self.queue.spider_requests[0][1], \\\n-            ['http://www.example.com/asd/make1', 'http://www.example.com/asd/make2'])\n-        spider = self.queue.spider_requests[0][0]\n-        self.assert_(spider.name == 'create_for_request')\n-        self.assert_(spider.arg == '123')\n-\n-    def test_append_spider_name(self):\n-        self.queue.append_spider_name('test123', arg='123')\n-        spider = self.queue.spider_requests[0][0]\n-        self.assert_(spider.name == 'test123')\n-        self.assert_(spider.arg == '123')\n-\n-    def test_append_spider_name_kwarg(self):\n-        self.queue.append_spider_name(name='test123', arg='123')\n-        spider = self.queue.spider_requests[0][0]\n-        self.assert_(spider.name == 'test123')\n-        self.assert_(spider.arg == '123')\n-\n-    def test_append_next(self):\n-        # the reason for this test: http://dev.scrapy.org/ticket/250\n-        class MockQueue(object):\n-            def pop(self):\n-                return {u'name': u'test123', u'test': u'hello'}\n-        self.queue._queue = MockQueue()\n-        self.queue._append_next()\n-        spider = self.queue.spider_requests[0][0]\n-        self.assert_(spider.name == 'test123')\n-        self.assert_(spider.test == 'hello')\n-\n-    def _assert_request_urls(self, requests, urls):\n-        assert all(isinstance(x, Request) for x in requests)\n-        self.assertEqual([x.url for x in requests], urls)\n-\n-class KeepAliveExecutionQueueTest(ExecutionQueueTest):\n-\n-    keep_alive = True\n-\n-    def test_is_finished(self):\n-        self.assert_(not self.queue.is_finished())\n-        self.queue.append_request(self.request, self.spider)\n-        self.assert_(not self.queue.is_finished())\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bdd627fe1ddce6610e593cb66e16a611e6e0392e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 629 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -154,6 +154,8 @@ class ImagesPipeline(MediaPipeline):\n             }\n \n     def __init__(self, store_uri, download_func=None):\n+        if not store_uri:\n+            raise NotConfigured\n         self.store = self._get_store(store_uri)\n         super(ImagesPipeline, self).__init__(download_func=download_func)\n \n@@ -167,8 +169,6 @@ class ImagesPipeline(MediaPipeline):\n         s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n         s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n         store_uri = settings['IMAGES_STORE']\n-        if not store_uri:\n-            raise NotConfigured\n         return cls(store_uri)\n \n     def _get_store(self, uri):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#442c0bdc181cd51c6cdcb92880aec303cc3dde0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 98 | Files Changed: 2 | Hunks: 2 | Methods Changed: 15 | Complexity Δ (Sum/Max): -19/0 | Churn Δ: 98 | Churn Cumulative: 218 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,81 +0,0 @@\n-from zope.interface import implements\n-\n-from twisted.internet import threads\n-from boto.sqs.connection import SQSConnection\n-from boto.sqs.message import Message\n-from boto.sqs import regions\n-\n-from scrapy.interfaces import ISpiderQueue\n-from scrapy.utils.py26 import json\n-\n-class SQSSpiderQueue(object):\n-\n-    implements(ISpiderQueue)\n-\n-    def __init__(self, *a, **kw):\n-        self.queue_name = kw.pop('queue_name', 'scrapy')\n-        self.region_name = kw.pop('region_name', 'us-east-1')\n-        self.visibility_timeout = kw.pop('visibility_timeout', 7200)\n-        self.aws_access_key_id = kw.pop('aws_access_key_id', None)\n-        self.aws_secret_access_key = kw.pop('aws_secret_access_key', None)\n-        self.region = self._get_region(self.region_name)\n-        self.conn.create_queue(self.queue_name)\n-        super(SQSSpiderQueue, self).__init__(*a, **kw)\n-\n-    @classmethod\n-    def from_settings(cls, settings):\n-        return cls(\n-            queue_name=settings['SQS_QUEUE'],\n-            region_name=settings['SQS_REGION'],\n-            visibility_timeout=settings.getint('SQS_VISIBILITY_TIMEOUT'),\n-            aws_access_key_id=settings['AWS_ACCESS_KEY_ID'],\n-            aws_secret_access_key=settings['AWS_SECRET_ACCESS_KEY']\n-        )\n-        \n-    def _get_region(self, name):\n-        return [r for r in regions() if r.name == name][0]\n-\n-    @property\n-    def conn(self):\n-        return SQSConnection(self.aws_access_key_id, self.aws_secret_access_key, \\\n-            region=self.region)\n-\n-    @property\n-    def queue(self):\n-        return self.conn.get_queue(self.queue_name)\n-\n-    def _queue_method(self, method, *a, **kw):\n-        return getattr(self.queue, method)(*a, **kw)\n-\n-    def pop(self):\n-        return threads.deferToThread(self._pop)\n-\n-    def _pop(self):\n-        msgs = self.queue.get_messages(1, visibility_timeout=self.visibility_timeout)\n-        if msgs:\n-            msg = msgs[0]\n-            msg.delete()\n-            return json.loads(msg.get_body())\n-\n-    def add(self, name, **spider_args):\n-        d = spider_args.copy()\n-        d['name'] = name\n-        msg = Message(body=json.dumps(d))\n-        return threads.deferToThread(self._queue_method, 'write', msg)\n-\n-    def count(self):\n-        return threads.deferToThread(self._queue_method, 'count')\n-\n-    def list(self):\n-        return threads.deferToThread(self._list)\n-\n-    def _list(self):\n-        msgs = []\n-        m = self.queue.read(visibility_timeout=100)\n-        while m:\n-            msgs.append(json.loads(m.get_body()))\n-            m = self.queue.read(visibility_timeout=100)\n-        return msgs\n-\n-    def clear(self):\n-        return threads.deferToThread(self._queue_method, 'clear')\n\n@@ -1,17 +0,0 @@\n-from twisted.trial import unittest\n-from zope.interface.verify import verifyObject\n-\n-from scrapy.interfaces import ISpiderQueue\n-from scrapy.utils.test import assert_aws_environ\n-\n-class SQSSpiderQueueTest(unittest.TestCase):\n-\n-    def setUp(self):\n-        assert_aws_environ()\n-\n-    def test_interface(self):\n-        from scrapy.contrib.spiderqueue import SQSSpiderQueue\n-        verifyObject(ISpiderQueue, SQSSpiderQueue())\n-\n-    # XXX: testing SQS queue operations is hard because there are long delays\n-    # for the operations to complete\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d97d6d20c6d1f82e160125bb4abce09d72ca5216", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 49 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -245,10 +245,6 @@ SPIDER_CONTEXT_STORAGE_CLASS = 'scrapy.contrib.spidercontext.SqliteSpiderContext\n \n SQLITE_DB = 'scrapy.db'\n \n-SQS_QUEUE = 'scrapy'\n-SQS_VISIBILITY_TIMEOUT = 7200\n-SQS_REGION = 'us-east-1'\n-\n STATS_CLASS = 'scrapy.statscol.MemoryStatsCollector'\n STATS_ENABLED = True\n STATS_DUMP = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#75e2c3eb338ea03e487907fa8c99bb12317e9435", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 40 | Files Changed: 6 | Hunks: 6 | Methods Changed: 12 | Complexity Δ (Sum/Max): 14/8 | Churn Δ: 79 | Churn Cumulative: 359 | Contributors (this commit): 3 | Commits (past 90d): 17 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,39 +1,5 @@\n from zope.interface import Interface\n \n-class ISpiderQueue(Interface):\n-\n-    def from_settings(settings):\n-        \"\"\"Class method to instantiate from settings\"\"\"\n-\n-    def add(name, **spider_args):\n-        \"\"\"Add a spider to the queue given its name a some spider arguments.\n-        \n-        This method can return a deferred. \"\"\"\n-\n-    def pop():\n-        \"\"\"Pop the next mesasge from the queue. The messages is a dict\n-        conaining a key 'name' with the spider name and other keys as spider\n-        attributes.\n-\n-        This method can return a deferred. \"\"\"\n-\n-    def list():\n-        \"\"\"Return a list with the messages in the queue. Each message is a dict\n-        which must have a 'name' key (with the spider name), and other optional\n-        keys that will be used as spider arguments, to create the spider.\n-        \n-        This method can return a deferred. \"\"\"\n-\n-    def count():\n-        \"\"\"Return the number of spiders in the queue.\n-        \n-        This method can return a deferred. \"\"\"\n-\n-    def clear():\n-        \"\"\"Clear the queue.\n-\n-        This method can return a deferred. \"\"\"\n-\n class ISpiderManager(Interface):\n \n     def from_settings(settings):\n\n@@ -237,8 +237,6 @@ SPIDER_MIDDLEWARES_BASE = {\n \n SPIDER_MODULES = []\n \n-SPIDER_QUEUE_CLASS = 'scrapy.spiderqueue.SqliteSpiderQueue'\n-\n SPIDER_CONTEXT_ENABLED = True\n SPIDER_CONTEXT_STORAGE_CLASS = 'scrapy.contrib.spidercontext.SqliteSpiderContextStorage'\n \n\n@@ -47,6 +47,41 @@ class IPoller(Interface):\n         projects\"\"\"\n \n \n+class ISpiderQueue(Interface):\n+\n+    def from_settings(settings):\n+        \"\"\"Class method to instantiate from settings\"\"\"\n+\n+    def add(name, **spider_args):\n+        \"\"\"Add a spider to the queue given its name a some spider arguments.\n+\n+        This method can return a deferred. \"\"\"\n+\n+    def pop():\n+        \"\"\"Pop the next mesasge from the queue. The messages is a dict\n+        conaining a key 'name' with the spider name and other keys as spider\n+        attributes.\n+\n+        This method can return a deferred. \"\"\"\n+\n+    def list():\n+        \"\"\"Return a list with the messages in the queue. Each message is a dict\n+        which must have a 'name' key (with the spider name), and other optional\n+        keys that will be used as spider arguments, to create the spider.\n+\n+        This method can return a deferred. \"\"\"\n+\n+    def count():\n+        \"\"\"Return the number of spiders in the queue.\n+\n+        This method can return a deferred. \"\"\"\n+\n+    def clear():\n+        \"\"\"Clear the queue.\n+\n+        This method can return a deferred. \"\"\"\n+\n+\n class ISpiderScheduler(Interface):\n     \"\"\"A component to schedule spider runs\"\"\"\n \n\n@@ -1,6 +1,6 @@\n from zope.interface import implements\n \n-from scrapy.interfaces import ISpiderQueue\n+from scrapyd.interfaces import ISpiderQueue\n from scrapy.utils.sqlite import JsonSqlitePriorityQueue\n from scrapy.utils.project import sqlite_db\n \n\n@@ -3,8 +3,8 @@ from twisted.trial import unittest\n \n from zope.interface.verify import verifyObject\n \n-from scrapy.interfaces import ISpiderQueue\n-from scrapy.spiderqueue import SqliteSpiderQueue\n+from scrapyd.interfaces import ISpiderQueue\n+from scrapyd.spiderqueue import SqliteSpiderQueue\n \n class SpiderQueueTest(unittest.TestCase):\n     \"\"\"This test case can be used easily for testing other SpiderQueue's by\n\n@@ -3,7 +3,7 @@ import os\n from subprocess import Popen, PIPE\n from ConfigParser import NoSectionError\n \n-from scrapy.spiderqueue import SqliteSpiderQueue\n+from scrapyd.spiderqueue import SqliteSpiderQueue\n from scrapy.utils.python import stringify_dict, unicode_to_str\n from scrapyd.config import Config\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
