{"custom_id": "keras#b7b7c2ea9494c9c0b60c084fe20a5736654cd536", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 12 | Methods Changed: 20 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 16292 | Contributors (this commit): 73 | Commits (past 90d): 29 | Contributors (cumulative): 85 | DMM Complexity: 1.0\n\nDIFF:\n@@ -77,7 +77,7 @@ class Convolution1D(Layer):\n         `steps` value might have changed due to padding.\n     '''\n     def __init__(self, nb_filter, filter_length,\n-                 init='uniform', activation='linear', weights=None,\n+                 init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample_length=1,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None,\n@@ -249,7 +249,7 @@ class AtrousConvolution1D(Convolution1D):\n         `steps` value might have changed due to padding.\n     '''\n     def __init__(self, nb_filter, filter_length,\n-                 init='uniform', activation='linear', weights=None,\n+                 init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample_length=1, atrous_rate=1,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None,\n@@ -365,7 +365,7 @@ class Convolution2D(Layer):\n         `rows` and `cols` values might have changed due to padding.\n     '''\n     def __init__(self, nb_filter, nb_row, nb_col,\n-                 init='glorot_uniform', activation='linear', weights=None,\n+                 init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample=(1, 1), dim_ordering='default',\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None,\n@@ -607,7 +607,7 @@ class Deconvolution2D(Convolution2D):\n         [3] [Deconvolutional Networks](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n     '''\n     def __init__(self, nb_filter, nb_row, nb_col, output_shape,\n-                 init='glorot_uniform', activation='linear', weights=None,\n+                 init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample=(1, 1),\n                  dim_ordering='default',\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n@@ -740,7 +740,7 @@ class AtrousConvolution2D(Convolution2D):\n         - [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122)\n     '''\n     def __init__(self, nb_filter, nb_row, nb_col,\n-                 init='glorot_uniform', activation='linear', weights=None,\n+                 init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample=(1, 1),\n                  atrous_rate=(1, 1), dim_ordering='default',\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n@@ -887,7 +887,7 @@ class SeparableConvolution2D(Layer):\n         `rows` and `cols` values might have changed due to padding.\n     '''\n     def __init__(self, nb_filter, nb_row, nb_col,\n-                 init='glorot_uniform', activation='linear', weights=None,\n+                 init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample=(1, 1),\n                  depth_multiplier=1, dim_ordering='default',\n                  depthwise_regularizer=None, pointwise_regularizer=None,\n@@ -1102,7 +1102,7 @@ class Convolution3D(Layer):\n     '''\n \n     def __init__(self, nb_filter, kernel_dim1, kernel_dim2, kernel_dim3,\n-                 init='glorot_uniform', activation='linear', weights=None,\n+                 init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample=(1, 1, 1), dim_ordering='default',\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None,\n\n@@ -661,7 +661,8 @@ class Dense(Layer):\n     # Output shape\n         2D tensor with shape: `(nb_samples, output_dim)`.\n     '''\n-    def __init__(self, output_dim, init='glorot_uniform', activation='linear', weights=None,\n+    def __init__(self, output_dim, init='glorot_uniform',\n+                 activation=None, weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None,\n                  bias=True, input_dim=None, **kwargs):\n@@ -962,7 +963,7 @@ class Highway(Layer):\n         - [Highway Networks](http://arxiv.org/pdf/1505.00387v2.pdf)\n     '''\n     def __init__(self, init='glorot_uniform', transform_bias=-2,\n-                 activation='linear', weights=None,\n+                 activation=None, weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None,\n                  bias=True, input_dim=None, **kwargs):\n@@ -1105,7 +1106,7 @@ class TimeDistributedDense(Layer):\n     '''\n \n     def __init__(self, output_dim,\n-                 init='glorot_uniform', activation='linear', weights=None,\n+                 init='glorot_uniform', activation=None, weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None,\n                  bias=True, input_dim=None, input_length=None, **kwargs):\n\n@@ -75,7 +75,7 @@ class LocallyConnected1D(Layer):\n         `steps` value might have changed due to padding.\n     '''\n     def __init__(self, nb_filter, filter_length,\n-                 init='uniform', activation='linear', weights=None,\n+                 init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample_length=1,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None,\n@@ -257,7 +257,7 @@ class LocallyConnected2D(Layer):\n         `rows` and `cols` values might have changed due to padding.\n     '''\n     def __init__(self, nb_filter, nb_row, nb_col,\n-                 init='glorot_uniform', activation='linear', weights=None,\n+                 init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample=(1, 1),\n                  dim_ordering='default',\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c95c32e473f35917b2365558cb6d794c144860f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 26 | Files Changed: 1 | Hunks: 19 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 53 | Churn Cumulative: 3880 | Contributors (this commit): 24 | Commits (past 90d): 15 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -95,33 +95,34 @@ class Node(object):\n                  input_tensors, output_tensors,\n                  input_masks, output_masks,\n                  input_shapes, output_shapes):\n-        # layer instance (NOT a list).\n+        # Layer instance (NOT a list).\n         # this is the layer that takes a list of input tensors\n         # and turns them into a list of output tensors.\n-        # the current node will be added to the inbound_nodes of outbound_layer\n+        # the current node will be added to the inbound_nodes of outbound_layer.\n         self.outbound_layer = outbound_layer\n \n-        # the following 3 properties describe where\n+        # The following 3 properties describe where\n         # the input tensors come from: which layers,\n         # and for each layer, which node and which\n         # tensor output of each node.\n-        self.inbound_layers = inbound_layers  # list of layer instances\n-        self.node_indices = node_indices  # list of integers, 1:1 mapping with inbound_layers\n-        self.tensor_indices = tensor_indices  # list of integers, 1:1 mapping with inbound_layers\n \n-        # tensor inputs and outputs of outbound_layer\n-        self.input_tensors = input_tensors  # list of tensors. 1:1 mapping with inbound_layers\n-        self.output_tensors = output_tensors  # list of tensors, created by outbound_layer.call()\n+        self.inbound_layers = inbound_layers  # List of layer instances\n+        self.node_indices = node_indices  # List of integers, 1:1 mapping with inbound_layers.\n+        self.tensor_indices = tensor_indices  # List of integers, 1:1 mapping with inbound_layers.\n+\n+        # Tensor inputs and outputs of outbound_layer.\n+        self.input_tensors = input_tensors  # List of tensors. 1:1 mapping with inbound_layers.\n+        self.output_tensors = output_tensors  # List of tensors, created by outbound_layer.call().\n \n         # input and output masks\n-        self.input_masks = input_masks  # list of tensors, 1:1 mapping with input_tensor\n-        self.output_masks = output_masks  # list of tensors, created by outbound_layer.compute_mask()\n+        self.input_masks = input_masks  # List of tensors, 1:1 mapping with input_tensor.\n+        self.output_masks = output_masks  # List of tensors, created by outbound_layer.compute_mask().\n \n         # input and output shapes\n-        self.input_shapes = input_shapes  # list of shape tuples, shapes of input_tensors\n-        self.output_shapes = output_shapes  # list of shape tuples, shapes of output_tensors\n+        self.input_shapes = input_shapes  # List of shape tuples, shapes of input_tensors.\n+        self.output_shapes = output_shapes  # List of shape tuples, shapes of output_tensors.\n \n-        # add nodes to all layers involved.\n+        # Add nodes to all layers involved.\n         for layer in inbound_layers:\n             if layer is not None:\n                 layer.outbound_nodes.append(self)\n@@ -152,7 +153,7 @@ class Node(object):\n         if len(input_tensors) == 1:\n             output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n             output_masks = to_list(outbound_layer.compute_mask(input_tensors[0], input_masks[0]))\n-            # TODO: try to auto-infer shape if exception is raised by get_output_shape_for\n+            # TODO: try to auto-infer shape if exception is raised by get_output_shape_for.\n             output_shapes = to_list(outbound_layer.get_output_shape_for(input_shapes[0]))\n         else:\n             output_tensors = to_list(outbound_layer.call(input_tensors, mask=input_masks))\n@@ -270,7 +271,7 @@ class Layer(object):\n         assert_input_compatibility()\n     '''\n     def __init__(self, **kwargs):\n-        # these properties should have been set\n+        # These properties should have been set\n         # by the child class, as appropriate.\n         if not hasattr(self, 'input_spec'):\n             self.input_spec = None\n@@ -279,12 +280,12 @@ class Layer(object):\n         if not hasattr(self, 'uses_learning_phase'):\n             self.uses_learning_phase = False\n \n-        # these lists will be filled via successive calls\n-        # to self.add_inbound_node()\n+        # These lists will be filled via successive calls\n+        # to self.add_inbound_node().\n         self.inbound_nodes = []\n         self.outbound_nodes = []\n \n-        # these properties will be set upon call of self.build(),\n+        # These properties will be set upon call of self.build(),\n         # which itself will be called upon self.add_inbound_node if necessary.\n         if not hasattr(self, 'trainable_weights'):\n             self.trainable_weights = []\n@@ -296,7 +297,7 @@ class Layer(object):\n             self.constraints = {}  # dict {tensor: constraint instance}\n         self.built = False\n \n-        # these properties should be set by the user via keyword arguments.\n+        # These properties should be set by the user via keyword arguments.\n         # note that 'input_dtype', 'input_shape' and 'batch_input_shape'\n         # are only applicable to input layers: do not pass these keywords\n         # to non-input layers.\n@@ -317,7 +318,7 @@ class Layer(object):\n \n         self.trainable = kwargs.get('trainable', True)\n         if 'batch_input_shape' in kwargs or 'input_shape' in kwargs:\n-            # in this case we will create an input layer\n+            # In this case we will create an input layer\n             # to insert before the current layer\n             if 'batch_input_shape' in kwargs:\n                 batch_input_shape = tuple(kwargs['batch_input_shape'])\n@@ -364,10 +365,10 @@ class Layer(object):\n         self.batch_input_shape = batch_input_shape\n         self.input_dtype = input_dtype\n \n-        # instantiate the input layer\n+        # Instantiate the input layer.\n         x = Input(batch_shape=batch_input_shape,\n                   dtype=input_dtype, name=name)\n-        # this will build the current layer\n+        # This will build the current layer\n         # and create the node connecting the current layer\n         # to the input layer we just created.\n         self(x)\n@@ -394,7 +395,7 @@ class Layer(object):\n             if spec is None:\n                 continue\n \n-            # check ndim\n+            # Check ndim.\n             if spec.ndim is not None:\n                 if type(spec.ndim) is str:\n                     int_ndim = spec.ndim[:spec.ndim.find('+')]\n@@ -423,7 +424,7 @@ class Layer(object):\n                 if hasattr(x, '_keras_shape'):\n                     x_shape = x._keras_shape\n                 elif hasattr(K, 'int_shape'):\n-                    # tensorflow shape inference\n+                    # Tensorflow shape inference.\n                     x_shape = K.int_shape(x)\n                 else:\n                     continue\n@@ -502,7 +503,7 @@ class Layer(object):\n         tensor_indices = []\n         for input_tensor in input_tensors:\n             if hasattr(input_tensor, '_keras_history') and input_tensor._keras_history:\n-                # this is a Keras tensor\n+                # This is a Keras tensor.\n                 previous_layer, node_index, tensor_index = input_tensor._keras_history\n                 inbound_layers.append(previous_layer)\n                 node_indices.append(node_index)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d32b8fa4bd81c69433acb1a2052a49425f373318", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 3886 | Contributors (this commit): 24 | Commits (past 90d): 16 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -236,6 +236,8 @@ class Layer(object):\n         input_mask, output_mask: Same as above, for masks.\n         trainable_weights: List of variables.\n         non_trainable_weights: List of variables.\n+        weights: The concatenation of the lists trainable_weights and\n+            non_trainable_weights (in this order).\n         regularizers: List of regularizers.\n         constraints: Dict mapping weights to constraints.\n \n@@ -872,7 +874,7 @@ class Layer(object):\n         '''\n         params = self.weights\n         if len(params) != len(weights):\n-            raise Exception('You called `set_weights(weights)` on layer \"' + self.name +\n+            raise ValueError('You called `set_weights(weights)` on layer \"' + self.name +\n                              '\" with a  weight list of length ' + str(len(weights)) +\n                              ', but the layer was expecting ' + str(len(params)) +\n                              ' weights. Provided weights: ' + str(weights)[:50] + '...')\n@@ -882,7 +884,7 @@ class Layer(object):\n         param_values = K.batch_get_value(params)\n         for pv, p, w in zip(param_values, params, weights):\n             if pv.shape != w.shape:\n-                raise Exception('Layer weight shape ' +\n+                raise ValueError('Layer weight shape ' +\n                                  str(pv.shape) +\n                                  ' not compatible with '\n                                  'provided weight shape ' + str(w.shape))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7db6de848ac683b2e25bc7b6c92b2184da4d3cda", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 6 | Churn Cumulative: 3344 | Contributors (this commit): 41 | Commits (past 90d): 31 | Contributors (cumulative): 41 | DMM Complexity: 0.0\n\nDIFF:\n@@ -777,14 +777,16 @@ def resize_images(X, height_factor, width_factor, dim_ordering):\n         X = permute_dimensions(X, [0, 2, 3, 1])\n         X = tf.image.resize_nearest_neighbor(X, new_shape)\n         X = permute_dimensions(X, [0, 3, 1, 2])\n-        X.set_shape((None, None, original_shape[2] * height_factor, original_shape[3] * width_factor))\n+        X.set_shape((None, None, original_shape[2] * height_factor if original_shape[2] is not None else None,\n+                    original_shape[3] * width_factor if original_shape[3] is not None else None))\n         return X\n     elif dim_ordering == 'tf':\n         original_shape = int_shape(X)\n         new_shape = tf.shape(X)[1:3]\n         new_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))\n         X = tf.image.resize_nearest_neighbor(X, new_shape)\n-        X.set_shape((None, original_shape[1] * height_factor, original_shape[2] * width_factor, None))\n+        X.set_shape((None, original_shape[1] * height_factor if original_shape[1] is not None else None,\n+                    original_shape[2] * width_factor if original_shape[2] is not None else None, None))\n         return X\n     else:\n         raise Exception('Invalid dim_ordering: ' + dim_ordering)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#00e8d20eae00ca487e887d79b2da3cf3e8914d3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 4397 | Contributors (this commit): 29 | Commits (past 90d): 6 | Contributors (cumulative): 29 | DMM Complexity: None\n\nDIFF:\n@@ -383,7 +383,7 @@ class SimpleRNN(Recurrent):\n             input_shape = self.input_spec[0].shape\n             input_dim = input_shape[-1]\n             ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n-            ones = K.tile(ones, (1, input_dim))\n+            ones = K.tile(ones, (1, int(input_dim)))\n             B_W = K.in_train_phase(K.dropout(ones, self.dropout_W), ones)\n             constants.append(B_W)\n         else:\n@@ -597,7 +597,7 @@ class GRU(Recurrent):\n             input_shape = self.input_spec[0].shape\n             input_dim = input_shape[-1]\n             ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n-            ones = K.tile(ones, (1, input_dim))\n+            ones = K.tile(ones, (1, int(input_dim)))\n             B_W = [K.in_train_phase(K.dropout(ones, self.dropout_W), ones) for _ in range(3)]\n             constants.append(B_W)\n         else:\n@@ -837,7 +837,7 @@ class LSTM(Recurrent):\n             input_shape = self.input_spec[0].shape\n             input_dim = input_shape[-1]\n             ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n-            ones = K.tile(ones, (1, input_dim))\n+            ones = K.tile(ones, (1, int(input_dim)))\n             B_W = [K.in_train_phase(K.dropout(ones, self.dropout_W), ones) for _ in range(4)]\n             constants.append(B_W)\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b74c5953f007da42095af06ec31b27f4ff2743b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 8 | Churn Cumulative: 1926 | Contributors (this commit): 31 | Commits (past 90d): 10 | Contributors (cumulative): 31 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -337,6 +337,7 @@ class EarlyStopping(Callback):\n         self.verbose = verbose\n         self.min_delta = min_delta\n         self.wait = 0\n+        self.stopped_epoch = 0\n \n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('EarlyStopping mode %s is unknown, '\n@@ -374,11 +375,14 @@ class EarlyStopping(Callback):\n             self.wait = 0\n         else:\n             if self.wait >= self.patience:\n-                if self.verbose > 0:\n-                    print('Epoch %05d: early stopping' % (epoch))\n+                self.stopped_epoch = epoch\n                 self.model.stop_training = True\n             self.wait += 1\n \n+    def on_train_end(self, logs={}):\n+        if self.stopped_epoch > 0 and self.verbose > 0:\n+            print('Epoch %05d: early stopping' % (self.stopped_epoch))\n+\n \n class RemoteMonitor(Callback):\n     '''Callback used to stream events to a server.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5266fdacf1580dd3d8f9661f3a078fad7e933730", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 37 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,9 +11,10 @@ def load_batch(fpath, label_key='labels'):\n     else:\n         d = cPickle.load(f, encoding=\"bytes\")\n         # decode utf8\n+        d_decoded = {}\n         for k, v in d.items():\n-            del(d[k])\n-            d[k.decode(\"utf8\")] = v\n+            d_decoded[k.decode(\"utf8\")] = v\n+        d = d_decoded\n     f.close()\n     data = d[\"data\"]\n     labels = d[label_key]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fa3b17cd9622cad0f34da4ed6c3d69515033ee86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 8388 | Contributors (this commit): 57 | Commits (past 90d): 14 | Contributors (cumulative): 62 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,7 +9,7 @@ from . import backend as K\n from . import optimizers\n from .utils.io_utils import ask_to_proceed_with_overwrite\n from .engine.training import Model\n-from .engine.topology import get_source_inputs, Node, Layer\n+from .engine.topology import get_source_inputs, Node, Layer, Merge\n from .optimizers import optimizer_from_config\n from .legacy.models import Graph\n \n@@ -170,7 +170,7 @@ def load_model(filepath, custom_objects={}):\n     # set optimizer weights\n     if 'optimizer_weights' in f:\n         # build train function (to get weight updates)\n-        if model.__class__.__name__ == 'Sequential':\n+        if isinstance(model, Sequential):\n             model.model._make_train_function()\n         else:\n             model._make_train_function()\n@@ -418,7 +418,7 @@ class Sequential(Model):\n             return self._flattened_layers\n         layers = []\n         if self.layers:\n-            if self.layers[0].__class__.__name__ == 'Merge':\n+            if isinstance(self.layers[0], Merge):\n                 merge = self.layers[0]\n                 for layer in merge.layers:\n                     if hasattr(layer, 'flattened_layers'):\n@@ -973,7 +973,7 @@ class Sequential(Model):\n         as a Python list.\n         '''\n         config = []\n-        if self.layers[0].__class__.__name__ == 'Merge':\n+        if isinstance(self.layers[0], Merge):\n             assert hasattr(self.layers[0], 'layers')\n             layers = []\n             for layer in self.layers[0].layers:\n\n@@ -1,6 +1,7 @@\n import os\n \n from ..layers.wrappers import Wrapper\n+from ..models import Sequential\n \n try:\n     # pydot-ng is a fork of pydot that is better maintained\n@@ -19,7 +20,7 @@ def model_to_dot(model, show_shapes=False, show_layer_names=True):\n     dot.set('concentrate', True)\n     dot.set_node_defaults(shape='record')\n \n-    if model.__class__.__name__ == 'Sequential':\n+    if isinstance(model, Sequential):\n         if not model.built:\n             model.build()\n         model = model.model\n@@ -34,7 +35,8 @@ def model_to_dot(model, show_shapes=False, show_layer_names=True):\n         class_name = layer.__class__.__name__\n         if isinstance(layer, Wrapper):\n             layer_name = '{}({})'.format(layer_name, layer.layer.name)\n-            class_name = '{}({})'.format(class_name, layer.layer.__class__.__name__)\n+            child_class_name = layer.layer.__class__.__name__\n+            class_name = '{}({})'.format(class_name, child_class_name)\n \n         # Create node's label.\n         if show_layer_names:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#49a5cdf76dafce69834db2737b84c1e10011877d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 3891 | Contributors (this commit): 24 | Commits (past 90d): 17 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -771,7 +771,10 @@ class Layer(object):\n         the layer has exactly one inbound node, i.e. if it is connected\n         to one incoming layer).\n         '''\n-        if len(self.inbound_nodes) != 1:\n+        if len(self.inbound_nodes) == 0:\n+            raise Exception('Layer ' + self.name +\n+                            ' has no inbound nodes.')\n+        if len(self.inbound_nodes) > 1:\n             raise Exception('Layer ' + self.name +\n                             ' has multiple inbound nodes, ' +\n                             'hence the notion of \"layer output\" '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cb3de665d18c579fa36fe8b6b2e2c084ace0bd12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 79 | Files Changed: 3 | Hunks: 37 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 154 | Churn Cumulative: 2040 | Contributors (this commit): 18 | Commits (past 90d): 20 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -404,22 +404,20 @@ def test_zero_padding_1d():\n \n     # correctness test\n     layer = convolutional.ZeroPadding1D(padding=2)\n-    layer.set_input(K.variable(input), shape=input.shape)\n-\n-    out = K.eval(layer.output)\n+    output = layer(K.variable(input))\n+    np_output = K.eval(output)\n     for offset in [0, 1, -1, -2]:\n-        assert_allclose(out[:, offset, :], 0.)\n-    assert_allclose(out[:, 2:-2, :], 1.)\n+        assert_allclose(np_output[:, offset, :], 0.)\n+    assert_allclose(np_output[:, 2:-2, :], 1.)\n \n     layer = convolutional.ZeroPadding1D(padding=(1, 2))\n-    layer.set_input(K.variable(input), shape=input.shape)\n-\n-    out = K.eval(layer.output)\n+    output = layer(K.variable(input))\n+    np_output = K.eval(output)\n     for left_offset in [0]:\n-        assert_allclose(out[:, left_offset, :], 0.)\n+        assert_allclose(np_output[:, left_offset, :], 0.)\n     for right_offset in [-1, -2]:\n-        assert_allclose(out[:, right_offset, :], 0.)\n-    assert_allclose(out[:, 1:-2, :], 1.)\n+        assert_allclose(np_output[:, right_offset, :], 0.)\n+    assert_allclose(np_output[:, 1:-2, :], 1.)\n     layer.get_config()\n \n \n@@ -450,44 +448,42 @@ def test_zero_padding_2d():\n \n     # correctness test\n     layer = convolutional.ZeroPadding2D(padding=(2, 2))\n-    layer.set_input(K.variable(input), shape=input.shape)\n-\n-    out = K.eval(layer.output)\n+    output = layer(K.variable(input))\n+    np_output = K.eval(output)\n     if dim_ordering == 'tf':\n         for offset in [0, 1, -1, -2]:\n-            assert_allclose(out[:, offset, :, :], 0.)\n-            assert_allclose(out[:, :, offset, :], 0.)\n-        assert_allclose(out[:, 2:-2, 2:-2, :], 1.)\n+            assert_allclose(np_output[:, offset, :, :], 0.)\n+            assert_allclose(np_output[:, :, offset, :], 0.)\n+        assert_allclose(np_output[:, 2:-2, 2:-2, :], 1.)\n     elif dim_ordering == 'th':\n         for offset in [0, 1, -1, -2]:\n-            assert_allclose(out[:, :, offset, :], 0.)\n-            assert_allclose(out[:, :, :, offset], 0.)\n-        assert_allclose(out[:, 2:-2, 2:-2, :], 1.)\n+            assert_allclose(np_output[:, :, offset, :], 0.)\n+            assert_allclose(np_output[:, :, :, offset], 0.)\n+        assert_allclose(np_output[:, 2:-2, 2:-2, :], 1.)\n \n     layer = convolutional.ZeroPadding2D(padding=(1, 2, 3, 4))\n-    layer.set_input(K.variable(input), shape=input.shape)\n-\n-    out = K.eval(layer.output)\n+    output = layer(K.variable(input))\n+    np_output = K.eval(output)\n     if dim_ordering == 'tf':\n         for top_offset in [0]:\n-            assert_allclose(out[:, top_offset, :, :], 0.)\n+            assert_allclose(np_output[:, top_offset, :, :], 0.)\n         for bottom_offset in [-1, -2]:\n-            assert_allclose(out[:, bottom_offset, :, :], 0.)\n+            assert_allclose(np_output[:, bottom_offset, :, :], 0.)\n         for left_offset in [0, 1, 2]:\n-            assert_allclose(out[:, :, left_offset, :], 0.)\n+            assert_allclose(np_output[:, :, left_offset, :], 0.)\n         for right_offset in [-1, -2, -3, -4]:\n-            assert_allclose(out[:, :, right_offset, :], 0.)\n-        assert_allclose(out[:, 1:-2, 3:-4, :], 1.)\n+            assert_allclose(np_output[:, :, right_offset, :], 0.)\n+        assert_allclose(np_output[:, 1:-2, 3:-4, :], 1.)\n     elif dim_ordering == 'th':\n         for top_offset in [0]:\n-            assert_allclose(out[:, :, top_offset, :], 0.)\n+            assert_allclose(np_output[:, :, top_offset, :], 0.)\n         for bottom_offset in [-1, -2]:\n-            assert_allclose(out[:, :, bottom_offset, :], 0.)\n+            assert_allclose(np_output[:, :, bottom_offset, :], 0.)\n         for left_offset in [0, 1, 2]:\n-            assert_allclose(out[:, :, :, left_offset], 0.)\n+            assert_allclose(np_output[:, :, :, left_offset], 0.)\n         for right_offset in [-1, -2, -3, -4]:\n-            assert_allclose(out[:, :, :, right_offset], 0.)\n-        assert_allclose(out[:, :, 1:-2, 3:-4], 1.)\n+            assert_allclose(np_output[:, :, :, right_offset], 0.)\n+        assert_allclose(np_output[:, :, 1:-2, 3:-4], 1.)\n     layer.get_config()\n \n \n@@ -509,13 +505,13 @@ def test_zero_padding_3d():\n \n     # correctness test\n     layer = convolutional.ZeroPadding3D(padding=(2, 2, 2))\n-    layer.set_input(K.variable(input), shape=input.shape)\n-    out = K.eval(layer.output)\n+    output = layer(K.variable(input))\n+    np_output = K.eval(output)\n     for offset in [0, 1, -1, -2]:\n-        assert_allclose(out[:, offset, :, :, :], 0.)\n-        assert_allclose(out[:, :, offset, :, :], 0.)\n-        assert_allclose(out[:, :, :, offset, :], 0.)\n-    assert_allclose(out[:, 2:-2, 2:-2, 2:-2, :], 1.)\n+        assert_allclose(np_output[:, offset, :, :, :], 0.)\n+        assert_allclose(np_output[:, :, offset, :, :], 0.)\n+        assert_allclose(np_output[:, :, :, offset, :], 0.)\n+    assert_allclose(np_output[:, 2:-2, 2:-2, 2:-2, :], 1.)\n     layer.get_config()\n \n \n@@ -546,15 +542,14 @@ def test_upsampling_2d():\n                 layer = convolutional.UpSampling2D(\n                     size=(length_row, length_col),\n                     dim_ordering=dim_ordering)\n-                layer.set_input(K.variable(input), shape=input.shape)\n-\n-                out = K.eval(layer.output)\n+                output = layer(K.variable(input))\n+                np_output = K.eval(output)\n                 if dim_ordering == 'th':\n-                    assert out.shape[2] == length_row * input_nb_row\n-                    assert out.shape[3] == length_col * input_nb_col\n+                    assert np_output.shape[2] == length_row * input_nb_row\n+                    assert np_output.shape[3] == length_col * input_nb_col\n                 else:  # tf\n-                    assert out.shape[1] == length_row * input_nb_row\n-                    assert out.shape[2] == length_col * input_nb_col\n+                    assert np_output.shape[1] == length_row * input_nb_row\n+                    assert np_output.shape[2] == length_col * input_nb_col\n \n                 # compare with numpy\n                 if dim_ordering == 'th':\n@@ -564,7 +559,7 @@ def test_upsampling_2d():\n                     expected_out = np.repeat(input, length_row, axis=1)\n                     expected_out = np.repeat(expected_out, length_col, axis=2)\n \n-                assert_allclose(out, expected_out)\n+                assert_allclose(np_output, expected_out)\n \n \n def test_upsampling_3d():\n@@ -587,17 +582,16 @@ def test_upsampling_3d():\n                     layer = convolutional.UpSampling3D(\n                         size=(length_dim1, length_dim2, length_dim3),\n                         dim_ordering=dim_ordering)\n-                    layer.set_input(K.variable(input), shape=input.shape)\n-\n-                    out = K.eval(layer.output)\n+                    output = layer(K.variable(input))\n+                    np_output = K.eval(output)\n                     if dim_ordering == 'th':\n-                        assert out.shape[2] == length_dim1 * input_len_dim1\n-                        assert out.shape[3] == length_dim2 * input_len_dim2\n-                        assert out.shape[4] == length_dim3 * input_len_dim3\n+                        assert np_output.shape[2] == length_dim1 * input_len_dim1\n+                        assert np_output.shape[3] == length_dim2 * input_len_dim2\n+                        assert np_output.shape[4] == length_dim3 * input_len_dim3\n                     else:  # tf\n-                        assert out.shape[1] == length_dim1 * input_len_dim1\n-                        assert out.shape[2] == length_dim2 * input_len_dim2\n-                        assert out.shape[3] == length_dim3 * input_len_dim3\n+                        assert np_output.shape[1] == length_dim1 * input_len_dim1\n+                        assert np_output.shape[2] == length_dim2 * input_len_dim2\n+                        assert np_output.shape[3] == length_dim3 * input_len_dim3\n \n                     # compare with numpy\n                     if dim_ordering == 'th':\n@@ -609,7 +603,7 @@ def test_upsampling_3d():\n                         expected_out = np.repeat(expected_out, length_dim2, axis=2)\n                         expected_out = np.repeat(expected_out, length_dim3, axis=3)\n \n-                    assert_allclose(out, expected_out)\n+                    assert_allclose(np_output, expected_out)\n \n \n @keras_test\n@@ -633,19 +627,22 @@ def test_cropping_2d():\n     dim_ordering = K.image_dim_ordering()\n \n     if dim_ordering == 'th':\n-        input = np.random.rand(nb_samples, stack_size, input_len_dim1, input_len_dim2)\n+        input = np.random.rand(nb_samples, stack_size,\n+                               input_len_dim1, input_len_dim2)\n     else:\n-        input = np.random.rand(nb_samples, input_len_dim1, input_len_dim2, stack_size)\n+        input = np.random.rand(nb_samples,\n+                               input_len_dim1, input_len_dim2,\n+                               stack_size)\n     # basic test\n     layer_test(convolutional.Cropping2D,\n                kwargs={'cropping': cropping,\n                        'dim_ordering': dim_ordering},\n                input_shape=input.shape)\n     # correctness test\n-    layer = convolutional.Cropping2D(cropping=cropping, dim_ordering=dim_ordering)\n-    layer.set_input(K.variable(input), shape=input.shape)\n-\n-    out = K.eval(layer.output)\n+    layer = convolutional.Cropping2D(cropping=cropping,\n+                                     dim_ordering=dim_ordering)\n+    output = layer(K.variable(input))\n+    np_output = K.eval(output)\n     # compare with numpy\n     if dim_ordering == 'th':\n         expected_out = input[:,\n@@ -657,8 +654,7 @@ def test_cropping_2d():\n                              cropping[0][0]: -cropping[0][1],\n                              cropping[1][0]: -cropping[1][1],\n                              :]\n-\n-    assert_allclose(out, expected_out)\n+    assert_allclose(np_output, expected_out)\n \n \n def test_cropping_3d():\n@@ -671,19 +667,22 @@ def test_cropping_3d():\n     dim_ordering = K.image_dim_ordering()\n \n     if dim_ordering == 'th':\n-        input = np.random.rand(nb_samples, stack_size, input_len_dim1, input_len_dim2, input_len_dim3)\n+        input = np.random.rand(nb_samples, stack_size,\n+                               input_len_dim1, input_len_dim2, input_len_dim3)\n     else:\n-        input = np.random.rand(nb_samples, input_len_dim1, input_len_dim2, input_len_dim3, stack_size)\n+        input = np.random.rand(nb_samples,\n+                               input_len_dim1, input_len_dim2,\n+                               input_len_dim3, stack_size)\n     # basic test\n     layer_test(convolutional.Cropping3D,\n                kwargs={'cropping': cropping,\n                        'dim_ordering': dim_ordering},\n                input_shape=input.shape)\n     # correctness test\n-    layer = convolutional.Cropping3D(cropping=cropping, dim_ordering=dim_ordering)\n-    layer.set_input(K.variable(input), shape=input.shape)\n-\n-    out = K.eval(layer.output)\n+    layer = convolutional.Cropping3D(cropping=cropping,\n+                                     dim_ordering=dim_ordering)\n+    output = layer(K.variable(input))\n+    np_output = K.eval(output)\n     # compare with numpy\n     if dim_ordering == 'th':\n         expected_out = input[:,\n@@ -697,8 +696,7 @@ def test_cropping_3d():\n                              cropping[1][0]: -cropping[1][1],\n                              cropping[2][0]: -cropping[2][1],\n                              :]\n-\n-    assert_allclose(out, expected_out)\n+    assert_allclose(np_output, expected_out)\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -110,9 +110,8 @@ def test_recurrent_convolutional():\n                       'border_mode': \"same\"}\n \n             layer = convolutional_recurrent.ConvLSTM2D(**kwargs)\n-            layer.set_input(K.variable(np.ones(input.shape)),\n-                            shape=input.shape)\n-            K.eval(layer.output)\n+            output = layer(K.variable(np.ones(input.shape)))\n+            K.eval(output)\n \n             # check dropout\n             layer_test(convolutional_recurrent.ConvLSTM2D,\n\n@@ -129,9 +129,8 @@ def test_regularizer(layer_class):\n                         U_regularizer=regularizers.WeightRegularizer(l1=0.01),\n                         b_regularizer='l2')\n     shape = (nb_samples, timesteps, embedding_dim)\n-    layer.set_input(K.variable(np.ones(shape)),\n-                    shape=shape)\n-    K.eval(layer.output)\n+    output = layer(K.variable(np.ones(shape)))\n+    K.eval(output)\n \n \n @keras_test\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#92e8a20761bedbde8fd56a02a165884e8132f045", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 59 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 59 | Churn Cumulative: 3950 | Contributors (this commit): 24 | Commits (past 90d): 18 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -706,65 +706,6 @@ class Layer(object):\n         return self._get_node_attribute_at_index(0, 'input_tensors',\n                                                  'input')\n \n-    def set_input(self, input_tensor, shape=None):\n-        if len(self.inbound_nodes) > 1:\n-            raise Exception('Cannot `set_input` for layer ' + self.name +\n-                            ' because it has more than one inbound connection.')\n-        if len(self.inbound_nodes) == 1:\n-            # Check that the inbound node is an Input node.\n-            if self.inbound_nodes[0].inbound_layers:\n-                warnings.warn('You are manually setting the input for layer ' +\n-                              self.name + ' but it is not an Input layer. '\n-                              'This will cause part of your model '\n-                              'to be disconnected.')\n-        if self.outbound_nodes:\n-            warnings.warn('You are manually setting the input for layer ' +\n-                          self.name + ' but it has ' +\n-                          str(len(self.outbound_nodes)) +\n-                          ' outbound layers. '\n-                          'This will cause part of your model '\n-                          'to be disconnected.')\n-        if hasattr(K, 'int_shape'):\n-            # Auto-infered shape takes priority.\n-            shape = K.int_shape(input_tensor)\n-        elif not shape:\n-            raise Exception('`set_input` needs to know the shape '\n-                            'of the `input_tensor` it receives, but '\n-                            'Keras was not able to infer it automatically.'\n-                            ' Specify it via: '\n-                            '`model.set_input(input_tensor, shape)`')\n-        # Reset layer connections.\n-        self.inbound_nodes = []\n-        self.outbound_nodes = []\n-        input_shape = tuple(shape)\n-        self.build(input_shape=input_shape)\n-\n-        # Set Keras tensor metadata.\n-        input_tensor._uses_learning_phase = False\n-        input_tensor._keras_history = (None, 0, 0)\n-        input_tensor._keras_shape = input_shape\n-\n-        output_tensors = to_list(self.call(input_tensor))\n-        output_shapes = to_list(self.get_output_shape_for(input_shape))\n-        output_masks = to_list(self.compute_mask(input_tensor, None))\n-\n-        for i, output_tensor in enumerate(output_tensors):\n-            output_tensor._keras_history = (self, 0, i)\n-            output_tensor._keras_shape = output_shapes[i]\n-            output_tensor._uses_learning_phase = self.uses_learning_phase\n-\n-        # Create node.\n-        Node(self,\n-             inbound_layers=[],\n-             node_indices=[],\n-             tensor_indices=[],\n-             input_tensors=[input_tensor],\n-             output_tensors=output_tensors,\n-             input_masks=[None],\n-             output_masks=output_masks,\n-             input_shapes=[input_shape],\n-             output_shapes=output_shapes)\n-\n     @property\n     def output(self):\n         '''Retrieves the output tensor(s) of a layer (only applicable if\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e916f748db761b53ec2e6cb301e7b0fd006ccca3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 1 | Files Changed: 9 | Hunks: 27 | Methods Changed: 15 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 23381 | Contributors (this commit): 93 | Commits (past 90d): 67 | Contributors (cumulative): 154 | DMM Complexity: 0.2962962962962963\n\nDIFF:\n@@ -142,6 +142,7 @@ class Convolution1D(Layer):\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n+        self.built = True\n \n     def get_output_shape_for(self, input_shape):\n         length = conv_output_length(input_shape[1],\n@@ -434,6 +435,7 @@ class Convolution2D(Layer):\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n+        self.built = True\n \n     def get_output_shape_for(self, input_shape):\n         if self.dim_ordering == 'th':\n@@ -982,6 +984,7 @@ class SeparableConvolution2D(Layer):\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n+        self.built = True\n \n     def get_output_shape_for(self, input_shape):\n         if self.dim_ordering == 'th':\n@@ -1179,6 +1182,7 @@ class Convolution3D(Layer):\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n+        self.built = True\n \n     def get_output_shape_for(self, input_shape):\n         if self.dim_ordering == 'th':\n\n@@ -371,6 +371,7 @@ class ConvLSTM2D(ConvRecurrent2D):\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n+        self.built = True\n \n     def reset_states(self):\n         assert self.stateful, 'Layer must be stateful.'\n\n@@ -723,6 +723,7 @@ class Dense(Layer):\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n+        self.built = True\n \n     def call(self, x, mask=None):\n         output = K.dot(x, self.W)\n@@ -891,6 +892,7 @@ class MaxoutDense(Layer):\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n+        self.built = True\n \n     def get_output_shape_for(self, input_shape):\n         assert input_shape and len(input_shape) == 2\n@@ -1028,6 +1030,7 @@ class Highway(Layer):\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n+        self.built = True\n \n     def call(self, x, mask=None):\n         y = K.dot(x, self.W_carry)\n@@ -1168,6 +1171,7 @@ class TimeDistributedDense(Layer):\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n+        self.built = True\n \n     def get_output_shape_for(self, input_shape):\n         return (input_shape[0], input_shape[1], self.output_dim)\n\n@@ -110,6 +110,7 @@ class Embedding(Layer):\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n+        self.built = True\n \n     def compute_mask(self, x, mask=None):\n         if not self.mask_zero:\n\n@@ -139,6 +139,7 @@ class LocallyConnected1D(Layer):\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n+        self.built = True\n \n     def get_output_shape_for(self, input_shape):\n         length = conv_output_length(input_shape[1],\n@@ -333,6 +334,7 @@ class LocallyConnected2D(Layer):\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n+        self.built = True\n \n     def get_output_shape_for(self, input_shape):\n         if self.dim_ordering == 'th':\n\n@@ -325,6 +325,7 @@ class SimpleRNN(Recurrent):\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n+        self.built = True\n \n     def reset_states(self):\n         assert self.stateful, 'Layer must be stateful.'\n@@ -515,6 +516,7 @@ class GRU(Recurrent):\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n+        self.built = True\n \n     def reset_states(self):\n         assert self.stateful, 'Layer must be stateful.'\n@@ -745,6 +747,7 @@ class LSTM(Recurrent):\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n+        self.built = True\n \n     def reset_states(self):\n         assert self.stateful, 'Layer must be stateful.'\n\n@@ -389,7 +389,8 @@ def test_zero_padding_1d():\n     nb_samples = 2\n     input_dim = 2\n     nb_steps = 5\n-    input = np.ones((nb_samples, nb_steps, input_dim))\n+    shape = (nb_samples, nb_steps, input_dim)\n+    input = np.ones(shape)\n \n     # basic test\n     layer_test(convolutional.ZeroPadding1D,\n@@ -404,6 +405,7 @@ def test_zero_padding_1d():\n \n     # correctness test\n     layer = convolutional.ZeroPadding1D(padding=2)\n+    layer.build(shape)\n     output = layer(K.variable(input))\n     np_output = K.eval(output)\n     for offset in [0, 1, -1, -2]:\n@@ -411,6 +413,7 @@ def test_zero_padding_1d():\n     assert_allclose(np_output[:, 2:-2, :], 1.)\n \n     layer = convolutional.ZeroPadding1D(padding=(1, 2))\n+    layer.build(shape)\n     output = layer(K.variable(input))\n     np_output = K.eval(output)\n     for left_offset in [0]:\n@@ -449,6 +452,7 @@ def test_zero_padding_2d():\n     # correctness test\n     layer = convolutional.ZeroPadding2D(padding=(2, 2))\n     output = layer(K.variable(input))\n+    layer.build(input.shape)\n     np_output = K.eval(output)\n     if dim_ordering == 'tf':\n         for offset in [0, 1, -1, -2]:\n@@ -462,6 +466,7 @@ def test_zero_padding_2d():\n         assert_allclose(np_output[:, 2:-2, 2:-2, :], 1.)\n \n     layer = convolutional.ZeroPadding2D(padding=(1, 2, 3, 4))\n+    layer.build(input.shape)\n     output = layer(K.variable(input))\n     np_output = K.eval(output)\n     if dim_ordering == 'tf':\n@@ -505,6 +510,7 @@ def test_zero_padding_3d():\n \n     # correctness test\n     layer = convolutional.ZeroPadding3D(padding=(2, 2, 2))\n+    layer.build(input.shape)\n     output = layer(K.variable(input))\n     np_output = K.eval(output)\n     for offset in [0, 1, -1, -2]:\n@@ -542,6 +548,7 @@ def test_upsampling_2d():\n                 layer = convolutional.UpSampling2D(\n                     size=(length_row, length_col),\n                     dim_ordering=dim_ordering)\n+                layer.build(input.shape)\n                 output = layer(K.variable(input))\n                 np_output = K.eval(output)\n                 if dim_ordering == 'th':\n@@ -582,6 +589,7 @@ def test_upsampling_3d():\n                     layer = convolutional.UpSampling3D(\n                         size=(length_dim1, length_dim2, length_dim3),\n                         dim_ordering=dim_ordering)\n+                    layer.build(input.shape)\n                     output = layer(K.variable(input))\n                     np_output = K.eval(output)\n                     if dim_ordering == 'th':\n@@ -641,6 +649,7 @@ def test_cropping_2d():\n     # correctness test\n     layer = convolutional.Cropping2D(cropping=cropping,\n                                      dim_ordering=dim_ordering)\n+    layer.build(input.shape)\n     output = layer(K.variable(input))\n     np_output = K.eval(output)\n     # compare with numpy\n@@ -681,6 +690,7 @@ def test_cropping_3d():\n     # correctness test\n     layer = convolutional.Cropping3D(cropping=cropping,\n                                      dim_ordering=dim_ordering)\n+    layer.build(input.shape)\n     output = layer(K.variable(input))\n     np_output = K.eval(output)\n     # compare with numpy\n\n@@ -110,6 +110,7 @@ def test_recurrent_convolutional():\n                       'border_mode': \"same\"}\n \n             layer = convolutional_recurrent.ConvLSTM2D(**kwargs)\n+            layer.build(input.shape)\n             output = layer(K.variable(np.ones(input.shape)))\n             K.eval(output)\n \n\n@@ -129,6 +129,7 @@ def test_regularizer(layer_class):\n                         U_regularizer=regularizers.WeightRegularizer(l1=0.01),\n                         b_regularizer='l2')\n     shape = (nb_samples, timesteps, embedding_dim)\n+    layer.build(shape)\n     output = layer(K.variable(np.ones(shape)))\n     K.eval(output)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
