{"custom_id": "scrapy#de146ad7cef9e3478290be021129979f69fc6d03", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 957 | Contributors (this commit): 25 | Commits (past 90d): 4 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -370,12 +370,11 @@ class FilesystemCacheStorage:\n         with self._open(rpath / \"pickled_meta\", \"wb\") as f:\n             pickle.dump(metadata, f, protocol=4)\n         with self._open(rpath / \"response_headers\", \"wb\") as f:\n-            # headers_dict_to_raw() needs a better type hint\n-            f.write(cast(bytes, headers_dict_to_raw(response.headers)))\n+            f.write(headers_dict_to_raw(response.headers))\n         with self._open(rpath / \"response_body\", \"wb\") as f:\n             f.write(response.body)\n         with self._open(rpath / \"request_headers\", \"wb\") as f:\n-            f.write(cast(bytes, headers_dict_to_raw(request.headers)))\n+            f.write(headers_dict_to_raw(request.headers))\n         with self._open(rpath / \"request_body\", \"wb\") as f:\n             f.write(request.body)\n \n\n@@ -118,8 +118,7 @@ class Headers(CaselessDict):\n         ]\n \n     def to_string(self) -> bytes:\n-        # cast() can be removed if the headers_dict_to_raw() hint is improved\n-        return cast(bytes, headers_dict_to_raw(self))\n+        return headers_dict_to_raw(self)\n \n     def to_unicode_dict(self) -> CaseInsensitiveDict:\n         \"\"\"Return headers as a CaseInsensitiveDict with str keys\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#262c10d85bd34732b0c692bdc8d16375d83a178f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1202 | Contributors (this commit): 25 | Commits (past 90d): 2 | Contributors (cumulative): 25 | DMM Complexity: None\n\nDIFF:\n@@ -3,11 +3,11 @@ import functools\n import inspect\n import json\n import logging\n-from types import CoroutineType\n from typing import (\n     Any,\n     AsyncGenerator,\n     Callable,\n+    Coroutine,\n     Dict,\n     Iterable,\n     List,\n@@ -140,7 +140,7 @@ class Command(BaseRunSpiderCommand):\n \n     @overload\n     def iterate_spider_output(\n-        self, result: Union[AsyncGenerator[_T, None], CoroutineType[Any, Any, _T]]\n+        self, result: Union[AsyncGenerator[_T, None], Coroutine[Any, Any, _T]]\n     ) -> Deferred[_T]: ...\n \n     @overload\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#480a11b68bee19162cc0da59e9bed42b29bc9cfe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2452 | Contributors (this commit): 31 | Commits (past 90d): 6 | Contributors (cumulative): 39 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import argparse\n import functools\n import inspect\n\n@@ -4,6 +4,8 @@ Spider Middleware manager\n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n import logging\n from inspect import isasyncgenfunction, iscoroutine\n from itertools import islice\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#feb0b8f7dcb78c3df012085f00b992a7fac81f7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 86 | Files Changed: 18 | Hunks: 44 | Methods Changed: 32 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 154 | Churn Cumulative: 21239 | Contributors (this commit): 105 | Commits (past 90d): 36 | Contributors (cumulative): 318 | DMM Complexity: 0.2222222222222222\n\nDIFF:\n@@ -107,7 +107,7 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n         ctx.set_options(0x4)  # OP_LEGACY_SERVER_CONNECT\n         return ctx\n \n-    def creatorForNetloc(self, hostname: bytes, port: int) -> \"ClientTLSOptions\":\n+    def creatorForNetloc(self, hostname: bytes, port: int) -> ClientTLSOptions:\n         return ScrapyClientTLSOptions(\n             hostname.decode(\"ascii\"),\n             self.getContext(),\n@@ -134,7 +134,7 @@ class BrowserLikeContextFactory(ScrapyClientContextFactory):\n     ``SSLv23_METHOD``) which allows TLS protocol negotiation.\n     \"\"\"\n \n-    def creatorForNetloc(self, hostname: bytes, port: int) -> \"ClientTLSOptions\":\n+    def creatorForNetloc(self, hostname: bytes, port: int) -> ClientTLSOptions:\n         # trustRoot set to platformTrust() will use the platform's root CAs.\n         #\n         # This means that a website like https://www.cacert.org will be rejected\n@@ -158,8 +158,8 @@ class AcceptableProtocolsContextFactory:\n         self._wrapped_context_factory: Any = context_factory\n         self._acceptable_protocols: List[bytes] = acceptable_protocols\n \n-    def creatorForNetloc(self, hostname: bytes, port: int) -> \"ClientTLSOptions\":\n-        options: \"ClientTLSOptions\" = self._wrapped_context_factory.creatorForNetloc(\n+    def creatorForNetloc(self, hostname: bytes, port: int) -> ClientTLSOptions:\n+        options: ClientTLSOptions = self._wrapped_context_factory.creatorForNetloc(\n             hostname, port\n         )\n         _setAcceptableProtocols(options._ctx, self._acceptable_protocols)\n\n@@ -147,9 +147,7 @@ class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n         if not settings.getbool(\"RETRY_ENABLED\"):\n             raise NotConfigured\n         self.max_retry_times = settings.getint(\"RETRY_TIMES\")\n-        self.retry_http_codes = set(\n-            int(x) for x in settings.getlist(\"RETRY_HTTP_CODES\")\n-        )\n+        self.retry_http_codes = {int(x) for x in settings.getlist(\"RETRY_HTTP_CODES\")}\n         self.priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n \n         try:\n\n@@ -55,7 +55,7 @@ class StackTraceDump:\n         )\n \n     def _thread_stacks(self) -> str:\n-        id2name = dict((th.ident, th.name) for th in threading.enumerate())\n+        id2name = {th.ident: th.name for th in threading.enumerate()}\n         dumps = \"\"\n         for id_, frame in sys._current_frames().items():\n             name = id2name.get(id_, \"\")\n\n@@ -189,10 +189,10 @@ class Request(object_ref):\n     def __repr__(self) -> str:\n         return f\"<{self.method} {self.url}>\"\n \n-    def copy(self) -> \"Request\":\n+    def copy(self) -> Request:\n         return self.replace()\n \n-    def replace(self, *args: Any, **kwargs: Any) -> \"Request\":\n+    def replace(self, *args: Any, **kwargs: Any) -> Request:\n         \"\"\"Create a new Request with the same attributes except for those given new values\"\"\"\n         for x in self.attributes:\n             kwargs.setdefault(x, getattr(self, x))\n@@ -237,7 +237,7 @@ class Request(object_ref):\n         request_kwargs.update(kwargs)\n         return cls(**request_kwargs)\n \n-    def to_dict(self, *, spider: Optional[\"scrapy.Spider\"] = None) -> Dict[str, Any]:\n+    def to_dict(self, *, spider: Optional[scrapy.Spider] = None) -> Dict[str, Any]:\n         \"\"\"Return a dictionary containing the Request's data.\n \n         Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.\n\n@@ -21,7 +21,7 @@ DUMPS_ARGS = get_func_args(xmlrpclib.dumps)\n class XmlRpcRequest(Request):\n     def __init__(self, *args: Any, encoding: Optional[str] = None, **kwargs: Any):\n         if \"body\" not in kwargs and \"params\" in kwargs:\n-            kw = dict((k, kwargs.pop(k)) for k in DUMPS_ARGS if k in kwargs)\n+            kw = {k: kwargs.pop(k) for k in DUMPS_ARGS if k in kwargs}\n             kwargs[\"body\"] = xmlrpclib.dumps(**kw)\n \n         # spec defines that requests must use POST method\n\n@@ -275,7 +275,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n         assert isinstance(value, (dict, list))\n         return copy.deepcopy(value)\n \n-    def getwithbase(self, name: _SettingsKeyT) -> \"BaseSettings\":\n+    def getwithbase(self, name: _SettingsKeyT) -> BaseSettings:\n         \"\"\"Get a composition of a dictionary-like setting and its `_BASE`\n         counterpart.\n \n@@ -438,7 +438,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n         if self.frozen:\n             raise TypeError(\"Trying to modify an immutable Settings object\")\n \n-    def copy(self) -> \"Self\":\n+    def copy(self) -> Self:\n         \"\"\"\n         Make a deep copy of current settings.\n \n@@ -460,7 +460,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n         \"\"\"\n         self.frozen = True\n \n-    def frozencopy(self) -> \"Self\":\n+    def frozencopy(self) -> Self:\n         \"\"\"\n         Return an immutable copy of the current settings.\n \n\n@@ -22,9 +22,7 @@ class Root(Resource):\n         for nl in nlist:\n             args[\"n\"] = nl\n             argstr = urlencode(args, doseq=True)\n-            request.write(\n-                f\"<a href='/follow?{argstr}'>follow {nl}</a><br>\".encode(\"utf8\")\n-            )\n+            request.write(f\"<a href='/follow?{argstr}'>follow {nl}</a><br>\".encode())\n         request.write(b\"</body></html>\")\n         return b\"\"\n \n\n@@ -49,9 +49,9 @@ def _serialize_headers(\n             yield from request.headers.getlist(header)\n \n \n-_fingerprint_cache: (\n-    \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], bytes]]\"\n-)\n+_fingerprint_cache: WeakKeyDictionary[\n+    Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], bytes]\n+]\n _fingerprint_cache = WeakKeyDictionary()\n \n \n\n@@ -189,10 +189,10 @@ class Raw(LeafResource):\n class Echo(LeafResource):\n     def render_GET(self, request):\n         output = {\n-            \"headers\": dict(\n-                (to_unicode(k), [to_unicode(v) for v in vs])\n+            \"headers\": {\n+                to_unicode(k): [to_unicode(v) for v in vs]\n                 for k, vs in request.requestHeaders.getAllRawHeaders()\n-            ),\n+            },\n             \"body\": to_unicode(request.content.read()),\n         }\n         return to_bytes(json.dumps(output))\n\n@@ -362,7 +362,7 @@ class CookiesMiddlewareTest(TestCase):\n \n     def test_request_cookies_encoding(self):\n         # 1) UTF8-encoded bytes\n-        req1 = Request(\"http://example.org\", cookies={\"a\": \"á\".encode(\"utf8\")})\n+        req1 = Request(\"http://example.org\", cookies={\"a\": \"á\".encode()})\n         assert self.mw.process_request(req1, self.spider) is None\n         self.assertCookieValEqual(req1.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n \n@@ -379,7 +379,7 @@ class CookiesMiddlewareTest(TestCase):\n     @pytest.mark.xfail(reason=\"Cookie header is not currently being processed\")\n     def test_request_headers_cookie_encoding(self):\n         # 1) UTF8-encoded bytes\n-        req1 = Request(\"http://example.org\", headers={\"Cookie\": \"a=á\".encode(\"utf8\")})\n+        req1 = Request(\"http://example.org\", headers={\"Cookie\": \"a=á\".encode()})\n         assert self.mw.process_request(req1, self.spider) is None\n         self.assertCookieValEqual(req1.headers[\"Cookie\"], b\"a=\\xc3\\xa1\")\n \n\n@@ -1125,7 +1125,7 @@ class RedirectMiddlewareTest(Base.Test):\n \n     def test_utf8_location(self):\n         req = Request(\"http://scrapytest.org/first\")\n-        utf8_location = \"/ação\".encode(\"utf-8\")  # header using UTF-8 encoding\n+        utf8_location = \"/ação\".encode()  # header using UTF-8 encoding\n         resp = Response(\n             \"http://scrapytest.org/first\",\n             headers={\"Location\": utf8_location},\n\n@@ -40,9 +40,7 @@ Disallow: /wiki/K%C3%A4ytt%C3%A4j%C3%A4:\n Disallow: /wiki/Käyttäjä:\n User-Agent: UnicödeBöt\n Disallow: /some/randome/page.html\n-\"\"\".encode(\n-            \"utf-8\"\n-        )\n+\"\"\".encode()\n         response = TextResponse(\"http://site.local/robots.txt\", body=ROBOTS)\n \n         def return_response(request):\n\n@@ -1359,13 +1359,13 @@ class FeedExportTest(FeedExportTestBase):\n         items = [dict({\"foo\": \"Test\\xd6\"})]\n \n         formats = {\n-            \"json\": '[{\"foo\": \"Test\\\\u00d6\"}]'.encode(\"utf-8\"),\n-            \"jsonlines\": '{\"foo\": \"Test\\\\u00d6\"}\\n'.encode(\"utf-8\"),\n+            \"json\": b'[{\"foo\": \"Test\\\\u00d6\"}]',\n+            \"jsonlines\": b'{\"foo\": \"Test\\\\u00d6\"}\\n',\n             \"xml\": (\n                 '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n                 \"<items><item><foo>Test\\xd6</foo></item></items>\"\n-            ).encode(\"utf-8\"),\n-            \"csv\": \"foo\\r\\nTest\\xd6\\r\\n\".encode(\"utf-8\"),\n+            ).encode(),\n+            \"csv\": \"foo\\r\\nTest\\xd6\\r\\n\".encode(),\n         }\n \n         for fmt, expected in formats.items():\n@@ -1379,13 +1379,13 @@ class FeedExportTest(FeedExportTestBase):\n             self.assertEqual(expected, data[fmt])\n \n         formats = {\n-            \"json\": '[{\"foo\": \"Test\\xd6\"}]'.encode(\"latin-1\"),\n-            \"jsonlines\": '{\"foo\": \"Test\\xd6\"}\\n'.encode(\"latin-1\"),\n+            \"json\": b'[{\"foo\": \"Test\\xd6\"}]',\n+            \"jsonlines\": b'{\"foo\": \"Test\\xd6\"}\\n',\n             \"xml\": (\n-                '<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n-                \"<items><item><foo>Test\\xd6</foo></item></items>\"\n-            ).encode(\"latin-1\"),\n-            \"csv\": \"foo\\r\\nTest\\xd6\\r\\n\".encode(\"latin-1\"),\n+                b'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n+                b\"<items><item><foo>Test\\xd6</foo></item></items>\"\n+            ),\n+            \"csv\": b\"foo\\r\\nTest\\xd6\\r\\n\",\n         }\n \n         for fmt, expected in formats.items():\n@@ -1404,12 +1404,12 @@ class FeedExportTest(FeedExportTestBase):\n         items = [dict({\"foo\": \"FOO\", \"bar\": \"BAR\"})]\n \n         formats = {\n-            \"json\": '[\\n{\"bar\": \"BAR\"}\\n]'.encode(\"utf-8\"),\n+            \"json\": b'[\\n{\"bar\": \"BAR\"}\\n]',\n             \"xml\": (\n-                '<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n-                \"<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>\"\n-            ).encode(\"latin-1\"),\n-            \"csv\": \"bar,foo\\r\\nBAR,FOO\\r\\n\".encode(\"utf-8\"),\n+                b'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n+                b\"<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>\"\n+            ),\n+            \"csv\": b\"bar,foo\\r\\nBAR,FOO\\r\\n\",\n         }\n \n         settings = {\n@@ -1663,8 +1663,8 @@ class FeedExportTest(FeedExportTestBase):\n     def test_extend_kwargs(self):\n         items = [{\"foo\": \"FOO\", \"bar\": \"BAR\"}]\n \n-        expected_with_title_csv = \"foo,bar\\r\\nFOO,BAR\\r\\n\".encode(\"utf-8\")\n-        expected_without_title_csv = \"FOO,BAR\\r\\n\".encode(\"utf-8\")\n+        expected_with_title_csv = b\"foo,bar\\r\\nFOO,BAR\\r\\n\"\n+        expected_without_title_csv = b\"FOO,BAR\\r\\n\"\n         test_cases = [\n             # with title\n             {\n@@ -2519,22 +2519,22 @@ class BatchDeliveriesTest(FeedExportTestBase):\n \n         formats = {\n             \"json\": [\n-                '[\\n{\"bar\": \"BAR\"}\\n]'.encode(\"utf-8\"),\n-                '[\\n{\"bar\": \"BAR1\"}\\n]'.encode(\"utf-8\"),\n+                b'[\\n{\"bar\": \"BAR\"}\\n]',\n+                b'[\\n{\"bar\": \"BAR1\"}\\n]',\n             ],\n             \"xml\": [\n                 (\n-                    '<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n-                    \"<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>\"\n-                ).encode(\"latin-1\"),\n+                    b'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n+                    b\"<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>\"\n+                ),\n                 (\n-                    '<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n-                    \"<items>\\n  <item>\\n    <foo>FOO1</foo>\\n  </item>\\n</items>\"\n-                ).encode(\"latin-1\"),\n+                    b'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n'\n+                    b\"<items>\\n  <item>\\n    <foo>FOO1</foo>\\n  </item>\\n</items>\"\n+                ),\n             ],\n             \"csv\": [\n-                \"foo,bar\\r\\nFOO,BAR\\r\\n\".encode(\"utf-8\"),\n-                \"foo,bar\\r\\nFOO1,BAR1\\r\\n\".encode(\"utf-8\"),\n+                b\"foo,bar\\r\\nFOO,BAR\\r\\n\",\n+                b\"foo,bar\\r\\nFOO1,BAR1\\r\\n\",\n             ],\n         }\n \n@@ -2577,8 +2577,8 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         items = [dict({\"foo\": \"FOO\"}), dict({\"foo\": \"FOO1\"})]\n         formats = {\n             \"json\": [\n-                '[{\"foo\": \"FOO\"}]'.encode(\"utf-8\"),\n-                '[{\"foo\": \"FOO1\"}]'.encode(\"utf-8\"),\n+                b'[{\"foo\": \"FOO\"}]',\n+                b'[{\"foo\": \"FOO1\"}]',\n             ],\n         }\n         settings = {\n\n@@ -728,9 +728,7 @@ class TextResponseTest(BaseResponseTest):\n         resp1 = self.response_class(\n             \"http://example.com\",\n             encoding=\"utf8\",\n-            body='<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode(\n-                \"utf8\"\n-            ),\n+            body='<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode(),\n         )\n         req = self._assert_followed_url(\n             resp1.css(\"a\")[0],\n\n@@ -107,9 +107,7 @@ class FileDownloadCrawlTestCase(TestCase):\n \n         # check that the images/files checksums are what we know they should be\n         if self.expected_checksums is not None:\n-            checksums = set(\n-                i[\"checksum\"] for item in items for i in item[self.media_key]\n-            )\n+            checksums = {i[\"checksum\"] for item in items for i in item[self.media_key]}\n             self.assertEqual(checksums, self.expected_checksums)\n \n         # check that the image files where actually written to the media store\n\n@@ -628,7 +628,7 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n \n class NoimagesDropTestCase(unittest.TestCase):\n     def test_deprecation_warning(self):\n-        arg = str()\n+        arg = \"\"\n         with warnings.catch_warnings(record=True) as w:\n             NoimagesDrop(arg)\n             self.assertEqual(len(w), 1)\n\n@@ -29,7 +29,7 @@ class ResponseTypesTest(unittest.TestCase):\n         mappings = [\n             (b'attachment; filename=\"data.xml\"', XmlResponse),\n             (b\"attachment; filename=data.xml\", XmlResponse),\n-            (\"attachment;filename=data£.tar.gz\".encode(\"utf-8\"), Response),\n+            (\"attachment;filename=data£.tar.gz\".encode(), Response),\n             (\"attachment;filename=dataµ.tar.gz\".encode(\"latin-1\"), Response),\n             (\"attachment;filename=data高.doc\".encode(\"gbk\"), Response),\n             (\"attachment;filename=دورهdata.html\".encode(\"cp720\"), HtmlResponse),\n\n@@ -36,10 +36,10 @@ class BaseRobotParserTest:\n \n     def test_allowed(self):\n         robotstxt_robotstxt_body = (\n-            \"User-agent: * \\n\"\n-            \"Disallow: /disallowed \\n\"\n-            \"Allow: /allowed \\n\"\n-            \"Crawl-delay: 10\".encode(\"utf-8\")\n+            b\"User-agent: * \\n\"\n+            b\"Disallow: /disallowed \\n\"\n+            b\"Allow: /allowed \\n\"\n+            b\"Crawl-delay: 10\"\n         )\n         rp = self.parser_cls.from_crawler(\n             crawler=None, robotstxt_body=robotstxt_robotstxt_body\n@@ -48,15 +48,13 @@ class BaseRobotParserTest:\n         self.assertFalse(rp.allowed(\"https://www.site.local/disallowed\", \"*\"))\n \n     def test_allowed_wildcards(self):\n-        robotstxt_robotstxt_body = \"\"\"User-agent: first\n+        robotstxt_robotstxt_body = b\"\"\"User-agent: first\n                                 Disallow: /disallowed/*/end$\n \n                                 User-agent: second\n                                 Allow: /*allowed\n                                 Disallow: /\n-                                \"\"\".encode(\n-            \"utf-8\"\n-        )\n+                                \"\"\"\n         rp = self.parser_cls.from_crawler(\n             crawler=None, robotstxt_body=robotstxt_robotstxt_body\n         )\n@@ -77,18 +75,14 @@ class BaseRobotParserTest:\n         self.assertTrue(rp.allowed(\"https://www.site.local/is_allowed_too\", \"second\"))\n \n     def test_length_based_precedence(self):\n-        robotstxt_robotstxt_body = (\n-            \"User-agent: * \\n\" \"Disallow: / \\n\" \"Allow: /page\".encode(\"utf-8\")\n-        )\n+        robotstxt_robotstxt_body = b\"User-agent: * \\n\" b\"Disallow: / \\n\" b\"Allow: /page\"\n         rp = self.parser_cls.from_crawler(\n             crawler=None, robotstxt_body=robotstxt_robotstxt_body\n         )\n         self.assertTrue(rp.allowed(\"https://www.site.local/page\", \"*\"))\n \n     def test_order_based_precedence(self):\n-        robotstxt_robotstxt_body = (\n-            \"User-agent: * \\n\" \"Disallow: / \\n\" \"Allow: /page\".encode(\"utf-8\")\n-        )\n+        robotstxt_robotstxt_body = b\"User-agent: * \\n\" b\"Disallow: / \\n\" b\"Allow: /page\"\n         rp = self.parser_cls.from_crawler(\n             crawler=None, robotstxt_body=robotstxt_robotstxt_body\n         )\n@@ -123,9 +117,7 @@ class BaseRobotParserTest:\n         Disallow: /wiki/Käyttäjä:\n \n         User-Agent: UnicödeBöt\n-        Disallow: /some/randome/page.html\"\"\".encode(\n-            \"utf-8\"\n-        )\n+        Disallow: /some/randome/page.html\"\"\".encode()\n         rp = self.parser_cls.from_crawler(\n             crawler=None, robotstxt_body=robotstxt_robotstxt_body\n         )\n@@ -145,14 +137,14 @@ class BaseRobotParserTest:\n \n class DecodeRobotsTxtTest(unittest.TestCase):\n     def test_native_string_conversion(self):\n-        robotstxt_body = \"User-agent: *\\nDisallow: /\\n\".encode(\"utf-8\")\n+        robotstxt_body = b\"User-agent: *\\nDisallow: /\\n\"\n         decoded_content = decode_robotstxt(\n             robotstxt_body, spider=None, to_native_str_type=True\n         )\n         self.assertEqual(decoded_content, \"User-agent: *\\nDisallow: /\\n\")\n \n     def test_decode_utf8(self):\n-        robotstxt_body = \"User-agent: *\\nDisallow: /\\n\".encode(\"utf-8\")\n+        robotstxt_body = b\"User-agent: *\\nDisallow: /\\n\"\n         decoded_content = decode_robotstxt(robotstxt_body, spider=None)\n         self.assertEqual(decoded_content, \"User-agent: *\\nDisallow: /\\n\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#144ff6c756fa58da2bc1a85879aa6f89300030d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 740 | Contributors (this commit): 19 | Commits (past 90d): 2 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -59,6 +59,7 @@ class Selector(_ParselSelector, object_ref):\n \n     * ``\"html\"`` for :class:`~scrapy.http.HtmlResponse` type\n     * ``\"xml\"`` for :class:`~scrapy.http.XmlResponse` type\n+    * ``\"json\"`` for :class:`~scrapy.http.TextResponse` type\n     * ``\"html\"`` for anything else\n \n     Otherwise, if ``type`` is set, the selector type will be forced and no\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ed3a7acaf3169ed6b9f9ffbcffed35db63d840f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 439 | Contributors (this commit): 16 | Commits (past 90d): 1 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -33,12 +33,6 @@ version_info = tuple(int(v) if v.isdigit() else v for v in __version__.split(\".\"\n twisted_version = (_txv.major, _txv.minor, _txv.micro)\n \n \n-# Check minimum required Python version\n-if sys.version_info < (3, 8):\n-    print(f\"Scrapy {__version__} requires Python 3.8+\")\n-    sys.exit(1)\n-\n-\n # Ignore noisy twisted deprecation warnings\n warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"twisted\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ddc98fe91b454a0944a8558daa2000da08921b62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 23 | Churn Cumulative: 1310 | Contributors (this commit): 29 | Commits (past 90d): 3 | Contributors (cumulative): 38 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,6 +6,7 @@ from __future__ import annotations\n \n import asyncio\n import inspect\n+import warnings\n from asyncio import Future\n from functools import wraps\n from types import CoroutineType\n@@ -35,7 +36,7 @@ from twisted.internet.task import Cooperator\n from twisted.python import failure\n from twisted.python.failure import Failure\n \n-from scrapy.exceptions import IgnoreRequest\n+from scrapy.exceptions import IgnoreRequest, ScrapyDeprecationWarning\n from scrapy.utils.reactor import _get_asyncio_event_loop, is_asyncio_reactor_installed\n \n if TYPE_CHECKING:\n@@ -281,6 +282,12 @@ def process_chain_both(\n     **kw: _P.kwargs,\n ) -> Deferred:\n     \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n+    warnings.warn(\n+        \"process_chain_both() is deprecated and will be removed in a future\"\n+        \" Scrapy version.\",\n+        ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     d: Deferred = Deferred()\n     for cb, eb in zip(callbacks, errbacks):\n         d.addCallback(cb, *a, **kw)\n\n@@ -14,7 +14,6 @@ from scrapy.utils.defer import (\n     mustbe_deferred,\n     parallel_async,\n     process_chain,\n-    process_chain_both,\n     process_parallel,\n )\n \n@@ -80,19 +79,6 @@ class DeferUtilsTest(unittest.TestCase):\n             gotexc = True\n         self.assertTrue(gotexc)\n \n-    @defer.inlineCallbacks\n-    def test_process_chain_both(self):\n-        x = yield process_chain_both(\n-            [cb_fail, cb2, cb3], [None, eb1, None], \"res\", \"v1\", \"v2\"\n-        )\n-        self.assertEqual(x, \"(cb3 (eb1 TypeError v1 v2) v1 v2)\")\n-\n-        fail = Failure(ZeroDivisionError())\n-        x = yield process_chain_both(\n-            [eb1, cb2, cb3], [eb1, None, None], fail, \"v1\", \"v2\"\n-        )\n-        self.assertEqual(x, \"(cb3 (cb2 (eb1 ZeroDivisionError v1 v2) v1 v2) v1 v2)\")\n-\n     @defer.inlineCallbacks\n     def test_process_parallel(self):\n         x = yield process_parallel([cb1, cb2, cb3], \"res\", \"v1\", \"v2\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1282ddf8f77299edf613679c2ee0b606e96808ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 236 | Lines Deleted: 131 | Files Changed: 16 | Hunks: 134 | Methods Changed: 88 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 367 | Churn Cumulative: 15836 | Contributors (this commit): 93 | Commits (past 90d): 60 | Contributors (cumulative): 287 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,22 @@\n+from __future__ import annotations\n+\n import random\n import warnings\n from collections import deque\n from datetime import datetime\n from time import time\n-from typing import TYPE_CHECKING, Any, Deque, Dict, Optional, Set, Tuple, cast\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    Deque,\n+    Dict,\n+    Optional,\n+    Set,\n+    Tuple,\n+    TypeVar,\n+    Union,\n+    cast,\n+)\n \n from twisted.internet import task\n from twisted.internet.defer import Deferred\n@@ -22,6 +35,8 @@ from scrapy.utils.httpobj import urlparse_cached\n if TYPE_CHECKING:\n     from scrapy.crawler import Crawler\n \n+_T = TypeVar(\"_T\")\n+\n \n class Slot:\n     \"\"\"Downloader slot\"\"\"\n@@ -40,7 +55,7 @@ class Slot:\n         self.throttle = throttle\n \n         self.active: Set[Request] = set()\n-        self.queue: Deque[Tuple[Request, Deferred]] = deque()\n+        self.queue: Deque[Tuple[Request, Deferred[Response]]] = deque()\n         self.transferring: Set[Request] = set()\n         self.lastseen: float = 0\n         self.latercall = None\n@@ -93,7 +108,7 @@ def _get_concurrency_delay(\n class Downloader:\n     DOWNLOAD_SLOT = \"download_slot\"\n \n-    def __init__(self, crawler: \"Crawler\"):\n+    def __init__(self, crawler: Crawler):\n         self.settings: BaseSettings = crawler.settings\n         self.signals: SignalManager = crawler.signals\n         self.slots: Dict[str, Slot] = {}\n@@ -114,13 +129,17 @@ class Downloader:\n             \"DOWNLOAD_SLOTS\", {}\n         )\n \n-    def fetch(self, request: Request, spider: Spider) -> Deferred:\n-        def _deactivate(response: Response) -> Response:\n+    def fetch(\n+        self, request: Request, spider: Spider\n+    ) -> Deferred[Union[Response, Request]]:\n+        def _deactivate(response: _T) -> _T:\n             self.active.remove(request)\n             return response\n \n         self.active.add(request)\n-        dfd = self.middleware.download(self._enqueue_request, request, spider)\n+        dfd: Deferred[Union[Response, Request]] = self.middleware.download(\n+            self._enqueue_request, request, spider\n+        )\n         return dfd.addBoth(_deactivate)\n \n     def needs_backout(self) -> bool:\n@@ -163,7 +182,7 @@ class Downloader:\n         )\n         return self.get_slot_key(request)\n \n-    def _enqueue_request(self, request: Request, spider: Spider) -> Deferred:\n+    def _enqueue_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n         key, slot = self._get_slot(request, spider)\n         request.meta[self.DOWNLOAD_SLOT] = key\n \n@@ -175,7 +194,7 @@ class Downloader:\n         self.signals.send_catch_log(\n             signal=signals.request_reached_downloader, request=request, spider=spider\n         )\n-        deferred: Deferred = Deferred().addBoth(_deactivate)\n+        deferred: Deferred[Response] = Deferred().addBoth(_deactivate)\n         slot.queue.append((request, deferred))\n         self._process_queue(spider, slot)\n         return deferred\n@@ -208,11 +227,15 @@ class Downloader:\n                 self._process_queue(spider, slot)\n                 break\n \n-    def _download(self, slot: Slot, request: Request, spider: Spider) -> Deferred:\n+    def _download(\n+        self, slot: Slot, request: Request, spider: Spider\n+    ) -> Deferred[Response]:\n         # The order is very important for the following deferreds. Do not change!\n \n         # 1. Create the download deferred\n-        dfd = mustbe_deferred(self.handlers.download_request, request, spider)\n+        dfd: Deferred[Response] = mustbe_deferred(\n+            self.handlers.download_request, request, spider\n+        )\n \n         # 2. Notify response_downloaded listeners about the recent download\n         # before querying queue for next request\n@@ -233,7 +256,7 @@ class Downloader:\n         # middleware itself)\n         slot.transferring.add(request)\n \n-        def finish_transferring(_: Any) -> Any:\n+        def finish_transferring(_: _T) -> _T:\n             slot.transferring.remove(request)\n             self._process_queue(spider, slot)\n             self.signals.send_catch_log(\n\n@@ -3,13 +3,25 @@\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Any, Callable, Dict, Generator, Union, cast\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    Callable,\n+    Dict,\n+    Generator,\n+    Optional,\n+    Protocol,\n+    Type,\n+    Union,\n+    cast,\n+)\n \n from twisted.internet import defer\n from twisted.internet.defer import Deferred\n \n from scrapy import Request, Spider, signals\n from scrapy.exceptions import NotConfigured, NotSupported\n+from scrapy.http import Response\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import without_none_values\n@@ -20,13 +32,21 @@ if TYPE_CHECKING:\n logger = logging.getLogger(__name__)\n \n \n+class DownloadHandlerProtocol(Protocol):\n+    def download_request(\n+        self, request: Request, spider: Spider\n+    ) -> Deferred[Response]: ...\n+\n+\n class DownloadHandlers:\n     def __init__(self, crawler: Crawler):\n         self._crawler: Crawler = crawler\n         self._schemes: Dict[str, Union[str, Callable[..., Any]]] = (\n             {}\n         )  # stores acceptable schemes on instancing\n-        self._handlers: Dict[str, Any] = {}  # stores instanced handlers for schemes\n+        self._handlers: Dict[str, DownloadHandlerProtocol] = (\n+            {}\n+        )  # stores instanced handlers for schemes\n         self._notconfigured: Dict[str, str] = {}  # remembers failed handlers\n         handlers: Dict[str, Union[str, Callable[..., Any]]] = without_none_values(\n             cast(\n@@ -40,7 +60,7 @@ class DownloadHandlers:\n \n         crawler.signals.connect(self._close, signals.engine_stopped)\n \n-    def _get_handler(self, scheme: str) -> Any:\n+    def _get_handler(self, scheme: str) -> Optional[DownloadHandlerProtocol]:\n         \"\"\"Lazy-load the downloadhandler for a scheme\n         only on the first request for that scheme.\n         \"\"\"\n@@ -54,10 +74,12 @@ class DownloadHandlers:\n \n         return self._load_handler(scheme)\n \n-    def _load_handler(self, scheme: str, skip_lazy: bool = False) -> Any:\n+    def _load_handler(\n+        self, scheme: str, skip_lazy: bool = False\n+    ) -> Optional[DownloadHandlerProtocol]:\n         path = self._schemes[scheme]\n         try:\n-            dhcls = load_object(path)\n+            dhcls: Type[DownloadHandlerProtocol] = load_object(path)\n             if skip_lazy and getattr(dhcls, \"lazy\", True):\n                 return None\n             dh = build_from_crawler(\n@@ -80,17 +102,17 @@ class DownloadHandlers:\n             self._handlers[scheme] = dh\n             return dh\n \n-    def download_request(self, request: Request, spider: Spider) -> Deferred:\n+    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n         scheme = urlparse_cached(request).scheme\n         handler = self._get_handler(scheme)\n         if not handler:\n             raise NotSupported(\n                 f\"Unsupported URL scheme '{scheme}': {self._notconfigured[scheme]}\"\n             )\n-        return cast(Deferred, handler.download_request(request, spider))\n+        return handler.download_request(request, spider)\n \n     @defer.inlineCallbacks\n-    def _close(self, *_a: Any, **_kw: Any) -> Generator[Deferred, Any, None]:\n+    def _close(self, *_a: Any, **_kw: Any) -> Generator[Deferred[Any], Any, None]:\n         for dh in self._handlers.values():\n             if hasattr(dh, \"close\"):\n                 yield dh.close()\n\n@@ -91,7 +91,7 @@ class FTPDownloadHandler:\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler.settings)\n \n-    def download_request(self, request: Request, spider: Spider) -> Deferred:\n+    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n         from twisted.internet import reactor\n \n         parsed_url = urlparse_cached(request)\n@@ -103,10 +103,14 @@ class FTPDownloadHandler:\n         creator = ClientCreator(\n             reactor, FTPClient, user, password, passive=passive_mode\n         )\n-        dfd: Deferred = creator.connectTCP(parsed_url.hostname, parsed_url.port or 21)\n+        dfd: Deferred[FTPClient] = creator.connectTCP(\n+            parsed_url.hostname, parsed_url.port or 21\n+        )\n         return dfd.addCallback(self.gotClient, request, unquote(parsed_url.path))\n \n-    def gotClient(self, client: FTPClient, request: Request, filepath: str) -> Deferred:\n+    def gotClient(\n+        self, client: FTPClient, request: Request, filepath: str\n+    ) -> Deferred[Response]:\n         self.client = client\n         protocol = ReceivedDataProtocol(request.meta.get(\"ftp_local_filename\"))\n         d = client.retrieveFile(filepath, protocol)\n\n@@ -9,6 +9,7 @@ from twisted.internet.defer import Deferred\n \n from scrapy import Request, Spider\n from scrapy.crawler import Crawler\n+from scrapy.http import Response\n from scrapy.settings import BaseSettings\n from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import to_unicode\n@@ -38,7 +39,7 @@ class HTTP10DownloadHandler:\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler.settings, crawler)\n \n-    def download_request(self, request: Request, spider: Spider) -> Deferred:\n+    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n         factory = self.HTTPClientFactory(request)\n         self._connect(factory)\n\n@@ -8,7 +8,7 @@ import re\n from contextlib import suppress\n from io import BytesIO\n from time import time\n-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union, cast\n+from typing import TYPE_CHECKING, Any, List, Optional, Tuple, TypedDict, TypeVar, Union\n from urllib.parse import urldefrag, urlunparse\n \n from twisted.internet import ssl\n@@ -38,12 +38,22 @@ from scrapy.settings import BaseSettings\n from scrapy.utils.python import to_bytes, to_unicode\n \n if TYPE_CHECKING:\n-    # typing.Self requires Python 3.11\n-    from typing_extensions import Self\n-\n+    # typing.NotRequired and typing.Self require Python 3.11\n+    from typing_extensions import NotRequired, Self\n \n logger = logging.getLogger(__name__)\n \n+_T = TypeVar(\"_T\")\n+\n+\n+class _ResultT(TypedDict):\n+    txresponse: TxResponse\n+    body: bytes\n+    flags: Optional[List[str]]\n+    certificate: Optional[ssl.Certificate]\n+    ip_address: Union[ipaddress.IPv4Address, ipaddress.IPv6Address, None]\n+    failure: NotRequired[Optional[Failure]]\n+\n \n class HTTP11DownloadHandler:\n     lazy = False\n@@ -71,7 +81,7 @@ class HTTP11DownloadHandler:\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler.settings, crawler)\n \n-    def download_request(self, request: Request, spider: Spider) -> Deferred:\n+    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n         agent = ScrapyAgent(\n             contextFactory=self._contextFactory,\n@@ -83,10 +93,10 @@ class HTTP11DownloadHandler:\n         )\n         return agent.download_request(request)\n \n-    def close(self) -> Deferred:\n+    def close(self) -> Deferred[None]:\n         from twisted.internet import reactor\n \n-        d: Deferred = self._pool.closeCachedConnections()\n+        d: Deferred[None] = self._pool.closeCachedConnections()\n         # closeCachedConnections will hang on network or server issues, so\n         # we'll manually timeout the deferred.\n         #\n@@ -97,7 +107,7 @@ class HTTP11DownloadHandler:\n         # issue a callback after `_disconnect_timeout` seconds.\n         delayed_call = reactor.callLater(self._disconnect_timeout, d.callback, [])\n \n-        def cancel_delayed_call(result: Any) -> Any:\n+        def cancel_delayed_call(result: _T) -> _T:\n             if delayed_call.active():\n                 delayed_call.cancel()\n             return result\n@@ -137,7 +147,7 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     ):\n         proxyHost, proxyPort, self._proxyAuthHeader = proxyConf\n         super().__init__(reactor, proxyHost, proxyPort, timeout, bindAddress)\n-        self._tunnelReadyDeferred: Deferred = Deferred()\n+        self._tunnelReadyDeferred: Deferred[Protocol] = Deferred()\n         self._tunneledHost: str = host\n         self._tunneledPort: int = port\n         self._contextFactory: IPolicyForHTTPS = contextFactory\n@@ -198,7 +208,7 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n         \"\"\"Propagates the errback to the appropriate deferred.\"\"\"\n         self._tunnelReadyDeferred.errback(reason)\n \n-    def connect(self, protocolFactory: Factory) -> Deferred:\n+    def connect(self, protocolFactory: Factory) -> Deferred[Protocol]:\n         self._protocolFactory = protocolFactory\n         connectDeferred = super().connect(protocolFactory)\n         connectDeferred.addCallback(self.requestTunnel)\n@@ -271,7 +281,7 @@ class TunnelingAgent(Agent):\n         headers: Optional[TxHeaders],\n         bodyProducer: Optional[IBodyProducer],\n         requestPath: bytes,\n-    ) -> Deferred:\n+    ) -> Deferred[TxResponse]:\n         # proxy host and port are required for HTTP pool `key`\n         # otherwise, same remote host connection request could reuse\n         # a cached tunneled connection to a different proxy\n@@ -310,7 +320,7 @@ class ScrapyProxyAgent(Agent):\n         uri: bytes,\n         headers: Optional[TxHeaders] = None,\n         bodyProducer: Optional[IBodyProducer] = None,\n-    ) -> Deferred:\n+    ) -> Deferred[TxResponse]:\n         \"\"\"\n         Issue a new request via the configured proxy.\n         \"\"\"\n@@ -394,7 +404,7 @@ class ScrapyAgent:\n             pool=self._pool,\n         )\n \n-    def download_request(self, request: Request) -> Deferred:\n+    def download_request(self, request: Request) -> Deferred[Response]:\n         from twisted.internet import reactor\n \n         timeout = request.meta.get(\"download_timeout\") or self._connectTimeout\n@@ -411,22 +421,20 @@ class ScrapyAgent:\n         else:\n             bodyproducer = None\n         start_time = time()\n-        d: Deferred = agent.request(\n+        d: Deferred[TxResponse] = agent.request(\n             method, to_bytes(url, encoding=\"ascii\"), headers, bodyproducer\n         )\n         # set download latency\n         d.addCallback(self._cb_latency, request, start_time)\n         # response body is ready to be consumed\n-        d.addCallback(self._cb_bodyready, request)\n-        d.addCallback(self._cb_bodydone, request, url)\n+        d2: Deferred[_ResultT] = d.addCallback(self._cb_bodyready, request)\n+        d3: Deferred[Response] = d2.addCallback(self._cb_bodydone, request, url)\n         # check download timeout\n-        self._timeout_cl = reactor.callLater(timeout, d.cancel)\n-        d.addBoth(self._cb_timeout, request, url, timeout)\n-        return d\n+        self._timeout_cl = reactor.callLater(timeout, d3.cancel)\n+        d3.addBoth(self._cb_timeout, request, url, timeout)\n+        return d3\n \n-    def _cb_timeout(\n-        self, result: Any, request: Request, url: str, timeout: float\n-    ) -> Any:\n+    def _cb_timeout(self, result: _T, request: Request, url: str, timeout: float) -> _T:\n         if self._timeout_cl.active():\n             self._timeout_cl.cancel()\n             return result\n@@ -437,7 +445,7 @@ class ScrapyAgent:\n \n         raise TimeoutError(f\"Getting {url} took longer than {timeout} seconds.\")\n \n-    def _cb_latency(self, result: Any, request: Request, start_time: float) -> Any:\n+    def _cb_latency(self, result: _T, request: Request, start_time: float) -> _T:\n         request.meta[\"download_latency\"] = time() - start_time\n         return result\n \n@@ -451,7 +459,7 @@ class ScrapyAgent:\n \n     def _cb_bodyready(\n         self, txresponse: TxResponse, request: Request\n-    ) -> Union[Dict[str, Any], Deferred]:\n+    ) -> Union[_ResultT, Deferred[_ResultT]]:\n         headers_received_result = self._crawler.signals.send_catch_log(\n             signal=signals.headers_received,\n             headers=self._headers_from_twisted_response(txresponse),\n@@ -520,7 +528,7 @@ class ScrapyAgent:\n             # Abort connection immediately.\n             txresponse._transport._producer.abortConnection()\n \n-        d: Deferred = Deferred(_cancel)\n+        d: Deferred[_ResultT] = Deferred(_cancel)\n         txresponse.deliverBody(\n             _ResponseReader(\n                 finished=d,\n@@ -539,7 +547,7 @@ class ScrapyAgent:\n         return d\n \n     def _cb_bodydone(\n-        self, result: Dict[str, Any], request: Request, url: str\n+        self, result: _ResultT, request: Request, url: str\n     ) -> Union[Response, Failure]:\n         headers = self._headers_from_twisted_response(result[\"txresponse\"])\n         respcls = responsetypes.from_args(headers=headers, url=url, body=result[\"body\"])\n@@ -559,8 +567,9 @@ class ScrapyAgent:\n             protocol=protocol,\n         )\n         if result.get(\"failure\"):\n+            assert result[\"failure\"]\n             result[\"failure\"].value.response = response\n-            return cast(Failure, result[\"failure\"])\n+            return result[\"failure\"]\n         return response\n \n \n@@ -570,7 +579,7 @@ class _RequestBodyProducer:\n         self.body = body\n         self.length = len(body)\n \n-    def startProducing(self, consumer: IConsumer) -> Deferred:\n+    def startProducing(self, consumer: IConsumer) -> Deferred[None]:\n         consumer.write(self.body)\n         return succeed(None)\n \n@@ -584,7 +593,7 @@ class _RequestBodyProducer:\n class _ResponseReader(Protocol):\n     def __init__(\n         self,\n-        finished: Deferred,\n+        finished: Deferred[_ResultT],\n         txresponse: TxResponse,\n         request: Request,\n         maxsize: int,\n@@ -592,7 +601,7 @@ class _ResponseReader(Protocol):\n         fail_on_dataloss: bool,\n         crawler: Crawler,\n     ):\n-        self._finished: Deferred = finished\n+        self._finished: Deferred[_ResultT] = finished\n         self._txresponse: TxResponse = txresponse\n         self._request: Request = request\n         self._bodybuf: BytesIO = BytesIO()\n\n@@ -37,7 +37,7 @@ class H2DownloadHandler:\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler.settings, crawler)\n \n-    def download_request(self, request: Request, spider: Spider) -> Deferred:\n+    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n         agent = ScrapyH2Agent(\n             context_factory=self._context_factory,\n             pool=self._pool,\n@@ -98,7 +98,7 @@ class ScrapyH2Agent:\n             pool=self._pool,\n         )\n \n-    def download_request(self, request: Request, spider: Spider) -> Deferred:\n+    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n         from twisted.internet import reactor\n \n         timeout = request.meta.get(\"download_timeout\") or self._connect_timeout\n\n@@ -8,6 +8,7 @@ from scrapy import Request, Spider\n from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n+from scrapy.http import Response\n from scrapy.settings import BaseSettings\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.httpobj import urlparse_cached\n@@ -76,7 +77,7 @@ class S3DownloadHandler:\n     def from_crawler(cls, crawler: Crawler, **kwargs: Any) -> Self:\n         return cls(crawler.settings, crawler=crawler, **kwargs)\n \n-    def download_request(self, request: Request, spider: Spider) -> Deferred:\n+    def download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n         p = urlparse_cached(request)\n         scheme = \"https\" if request.meta.get(\"is_secure\") else \"http\"\n         bucket = p.hostname\n\n@@ -4,6 +4,8 @@ Downloader Middleware manager\n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n from typing import Any, Callable, Generator, List, Union, cast\n \n from twisted.internet.defer import Deferred, inlineCallbacks\n@@ -34,10 +36,15 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n             self.methods[\"process_exception\"].appendleft(mw.process_exception)\n \n     def download(\n-        self, download_func: Callable, request: Request, spider: Spider\n-    ) -> Deferred:\n+        self,\n+        download_func: Callable[[Request, Spider], Deferred[Response]],\n+        request: Request,\n+        spider: Spider,\n+    ) -> Deferred[Union[Response, Request]]:\n         @inlineCallbacks\n-        def process_request(request: Request) -> Generator[Deferred, Any, Any]:\n+        def process_request(\n+            request: Request,\n+        ) -> Generator[Deferred[Any], Any, Union[Response, Request]]:\n             for method in self.methods[\"process_request\"]:\n                 method = cast(Callable, method)\n                 response = yield deferred_from_coro(\n@@ -52,12 +59,12 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                     )\n                 if response:\n                     return response\n-            return (yield download_func(request=request, spider=spider))\n+            return (yield download_func(request, spider))\n \n         @inlineCallbacks\n         def process_response(\n             response: Union[Response, Request]\n-        ) -> Generator[Deferred, Any, Union[Response, Request]]:\n+        ) -> Generator[Deferred[Any], Any, Union[Response, Request]]:\n             if response is None:\n                 raise TypeError(\"Received None in process_response\")\n             elif isinstance(response, Request):\n@@ -80,7 +87,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n         @inlineCallbacks\n         def process_exception(\n             failure: Failure,\n-        ) -> Generator[Deferred, Any, Union[Failure, Response, Request]]:\n+        ) -> Generator[Deferred[Any], Any, Union[Failure, Response, Request]]:\n             exception = failure.value\n             for method in self.methods[\"process_exception\"]:\n                 method = cast(Callable, method)\n@@ -98,7 +105,9 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                     return response\n             return failure\n \n-        deferred = mustbe_deferred(process_request, request)\n+        deferred: Deferred[Union[Response, Request]] = mustbe_deferred(\n+            process_request, request\n+        )\n         deferred.addErrback(process_exception)\n         deferred.addCallback(process_response)\n         return deferred\n\n@@ -8,7 +8,7 @@ from twisted.internet.protocol import ClientFactory\n from twisted.web.http import HTTPClient\n \n from scrapy import Request\n-from scrapy.http import Headers\n+from scrapy.http import Headers, Response\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes, to_unicode\n@@ -145,7 +145,7 @@ class ScrapyHTTPClientFactory(ClientFactory):\n         self.response_headers: Optional[Headers] = None\n         self.timeout: float = request.meta.get(\"download_timeout\") or timeout\n         self.start_time: float = time()\n-        self.deferred: defer.Deferred = defer.Deferred().addCallback(\n+        self.deferred: defer.Deferred[Response] = defer.Deferred().addCallback(\n             self._build_response, request\n         )\n \n@@ -155,7 +155,7 @@ class ScrapyHTTPClientFactory(ClientFactory):\n         # needed to add the callback _waitForDisconnect.\n         # Specifically this avoids the AttributeError exception when\n         # clientConnectionFailed method is called.\n-        self._disconnectedDeferred: defer.Deferred = defer.Deferred()\n+        self._disconnectedDeferred: defer.Deferred[None] = defer.Deferred()\n \n         self._set_connection_attributes(request)\n \n\n@@ -19,6 +19,7 @@ from typing import (\n     Optional,\n     Set,\n     Type,\n+    TypeVar,\n     Union,\n     cast,\n )\n@@ -43,10 +44,13 @@ from scrapy.utils.reactor import CallLaterOnce\n \n if TYPE_CHECKING:\n     from scrapy.core.scheduler import BaseScheduler\n+    from scrapy.core.scraper import _HandleOutputDeferred\n     from scrapy.crawler import Crawler\n \n logger = logging.getLogger(__name__)\n \n+_T = TypeVar(\"_T\")\n+\n \n class Slot:\n     def __init__(\n@@ -56,7 +60,7 @@ class Slot:\n         nextcall: CallLaterOnce[None],\n         scheduler: BaseScheduler,\n     ) -> None:\n-        self.closing: Optional[Deferred] = None\n+        self.closing: Optional[Deferred[None]] = None\n         self.inprogress: Set[Request] = set()\n         self.start_requests: Optional[Iterator[Request]] = iter(start_requests)\n         self.close_if_idle: bool = close_if_idle\n@@ -71,7 +75,7 @@ class Slot:\n         self.inprogress.remove(request)\n         self._maybe_fire_closing()\n \n-    def close(self) -> Deferred:\n+    def close(self) -> Deferred[None]:\n         self.closing = Deferred()\n         self._maybe_fire_closing()\n         return self.closing\n@@ -123,20 +127,20 @@ class ExecutionEngine:\n         return scheduler_cls\n \n     @inlineCallbacks\n-    def start(self) -> Generator[Deferred, Any, None]:\n+    def start(self) -> Generator[Deferred[Any], Any, None]:\n         if self.running:\n             raise RuntimeError(\"Engine already running\")\n         self.start_time = time()\n         yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n         self.running = True\n-        self._closewait: Deferred = Deferred()\n+        self._closewait: Deferred[None] = Deferred()\n         yield self._closewait\n \n-    def stop(self) -> Deferred:\n+    def stop(self) -> Deferred[None]:\n         \"\"\"Gracefully stop the execution engine\"\"\"\n \n         @inlineCallbacks\n-        def _finish_stopping_engine(_: Any) -> Generator[Deferred, Any, None]:\n+        def _finish_stopping_engine(_: Any) -> Generator[Deferred[Any], Any, None]:\n             yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n             self._closewait.callback(None)\n \n@@ -151,7 +155,7 @@ class ExecutionEngine:\n         )\n         return dfd.addBoth(_finish_stopping_engine)\n \n-    def close(self) -> Deferred:\n+    def close(self) -> Deferred[None]:\n         \"\"\"\n         Gracefully close the execution engine.\n         If it has already been started, stop it. In all cases, close the spider and the downloader.\n@@ -214,7 +218,7 @@ class ExecutionEngine:\n             or self.scraper.slot.needs_backout()\n         )\n \n-    def _next_request_from_scheduler(self) -> Optional[Deferred]:\n+    def _next_request_from_scheduler(self) -> Optional[Deferred[None]]:\n         assert self.slot is not None  # typing\n         assert self.spider is not None  # typing\n \n@@ -222,7 +226,7 @@ class ExecutionEngine:\n         if request is None:\n             return None\n \n-        d = self._download(request)\n+        d: Deferred[Union[Response, Request]] = self._download(request)\n         d.addBoth(self._handle_downloader_output, request)\n         d.addErrback(\n             lambda f: logger.info(\n@@ -236,8 +240,8 @@ class ExecutionEngine:\n             assert self.slot\n             self.slot.remove_request(request)\n \n-        d.addBoth(_remove_request)\n-        d.addErrback(\n+        d2: Deferred[None] = d.addBoth(_remove_request)\n+        d2.addErrback(\n             lambda f: logger.info(\n                 \"Error while removing request from slot\",\n                 exc_info=failure_to_exc_info(f),\n@@ -245,19 +249,19 @@ class ExecutionEngine:\n             )\n         )\n         slot = self.slot\n-        d.addBoth(lambda _: slot.nextcall.schedule())\n-        d.addErrback(\n+        d2.addBoth(lambda _: slot.nextcall.schedule())\n+        d2.addErrback(\n             lambda f: logger.info(\n                 \"Error while scheduling new request\",\n                 exc_info=failure_to_exc_info(f),\n                 extra={\"spider\": self.spider},\n             )\n         )\n-        return d\n+        return d2\n \n     def _handle_downloader_output(\n         self, result: Union[Request, Response, Failure], request: Request\n-    ) -> Optional[Deferred]:\n+    ) -> Optional[_HandleOutputDeferred]:\n         assert self.spider is not None  # typing\n \n         if not isinstance(result, (Request, Response, Failure)):\n@@ -319,20 +323,23 @@ class ExecutionEngine:\n                 signals.request_dropped, request=request, spider=spider\n             )\n \n-    def download(self, request: Request) -> Deferred:\n+    def download(self, request: Request) -> Deferred[Response]:\n         \"\"\"Return a Deferred which fires with a Response as result, only downloader middlewares are applied\"\"\"\n         if self.spider is None:\n             raise RuntimeError(f\"No open spider to crawl: {request}\")\n-        return self._download(request).addBoth(self._downloaded, request)\n+        d: Deferred[Union[Response, Request]] = self._download(request)\n+        # Deferred.addBoth() overloads don't seem to support a Union[_T, Deferred[_T]] return type\n+        d2: Deferred[Response] = d.addBoth(self._downloaded, request)  # type: ignore[arg-type]\n+        return d2\n \n     def _downloaded(\n         self, result: Union[Response, Request, Failure], request: Request\n-    ) -> Union[Deferred, Response, Failure]:\n+    ) -> Union[Deferred[Response], Response, Failure]:\n         assert self.slot is not None  # typing\n         self.slot.remove_request(request)\n         return self.download(result) if isinstance(result, Request) else result\n \n-    def _download(self, request: Request) -> Deferred:\n+    def _download(self, request: Request) -> Deferred[Union[Response, Request]]:\n         assert self.slot is not None  # typing\n \n         self.slot.add_request(request)\n@@ -359,13 +366,15 @@ class ExecutionEngine:\n                 )\n             return result\n \n-        def _on_complete(_: Any) -> Any:\n+        def _on_complete(_: _T) -> _T:\n             assert self.slot is not None\n             self.slot.nextcall.schedule()\n             return _\n \n         assert self.spider is not None\n-        dwld = self.downloader.fetch(request, self.spider)\n+        dwld: Deferred[Union[Response, Request]] = self.downloader.fetch(\n+            request, self.spider\n+        )\n         dwld.addCallback(_on_success)\n         dwld.addBoth(_on_complete)\n         return dwld\n@@ -376,7 +385,7 @@ class ExecutionEngine:\n         spider: Spider,\n         start_requests: Iterable[Request] = (),\n         close_if_idle: bool = True,\n-    ) -> Generator[Deferred, Any, None]:\n+    ) -> Generator[Deferred[Any], Any, None]:\n         if self.slot is not None:\n             raise RuntimeError(f\"No free spider slot when opening {spider.name!r}\")\n         logger.info(\"Spider opened\", extra={\"spider\": spider})\n@@ -422,7 +431,7 @@ class ExecutionEngine:\n             assert isinstance(ex, CloseSpider)  # typing\n             self.close_spider(self.spider, reason=ex.reason)\n \n-    def close_spider(self, spider: Spider, reason: str = \"cancelled\") -> Deferred:\n+    def close_spider(self, spider: Spider, reason: str = \"cancelled\") -> Deferred[None]:\n         \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n         if self.slot is None:\n             raise RuntimeError(\"Engine slot not assigned\")\n\n@@ -71,7 +71,7 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):\n         \"\"\"\n         return cls()\n \n-    def open(self, spider: Spider) -> Optional[Deferred]:\n+    def open(self, spider: Spider) -> Optional[Deferred[None]]:\n         \"\"\"\n         Called when the spider is opened by the engine. It receives the spider\n         instance as argument and it's useful to execute initialization code.\n@@ -81,7 +81,7 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):\n         \"\"\"\n         pass\n \n-    def close(self, reason: str) -> Optional[Deferred]:\n+    def close(self, reason: str) -> Optional[Deferred[None]]:\n         \"\"\"\n         Called when the spider is closed by the engine. It receives the reason why the crawl\n         finished as argument and it's useful to execute cleaning code.\n@@ -216,7 +216,7 @@ class Scheduler(BaseScheduler):\n     def has_pending_requests(self) -> bool:\n         return len(self) > 0\n \n-    def open(self, spider: Spider) -> Optional[Deferred]:\n+    def open(self, spider: Spider) -> Optional[Deferred[None]]:\n         \"\"\"\n         (1) initialize the memory queue\n         (2) initialize the disk queue if the ``jobdir`` attribute is a valid directory\n@@ -227,7 +227,7 @@ class Scheduler(BaseScheduler):\n         self.dqs: Optional[ScrapyPriorityQueue] = self._dq() if self.dqdir else None\n         return self.df.open()\n \n-    def close(self, reason: str) -> Optional[Deferred]:\n+    def close(self, reason: str) -> Optional[Deferred[None]]:\n         \"\"\"\n         (1) dump pending requests to disk if there is a disk queue\n         (2) return the result of the dupefilter's ``close`` method\n\n@@ -12,6 +12,7 @@ from typing import (\n     Deque,\n     Generator,\n     Iterable,\n+    Iterator,\n     Optional,\n     Set,\n     Tuple,\n@@ -33,6 +34,7 @@ from scrapy.logformatter import LogFormatter\n from scrapy.pipelines import ItemPipelineManager\n from scrapy.signalmanager import SignalManager\n from scrapy.utils.defer import (\n+    DeferredListResultListT,\n     aiter_errback,\n     defer_fail,\n     defer_succeed,\n@@ -48,13 +50,18 @@ if TYPE_CHECKING:\n     from scrapy.crawler import Crawler\n \n \n-_T = TypeVar(\"_T\")\n-QueueTuple = Tuple[Union[Response, Failure], Request, Deferred]\n-\n-\n logger = logging.getLogger(__name__)\n \n \n+_T = TypeVar(\"_T\")\n+_ParallelResult = DeferredListResultListT[Iterator[Any]]\n+\n+if TYPE_CHECKING:\n+    # parameterized Deferreds require Twisted 21.7.0\n+    _HandleOutputDeferred = Deferred[Union[_ParallelResult, None]]\n+    QueueTuple = Tuple[Union[Response, Failure], Request, _HandleOutputDeferred]\n+\n+\n class Slot:\n     \"\"\"Scraper slot (one per running spider)\"\"\"\n \n@@ -66,12 +73,12 @@ class Slot:\n         self.active: Set[Request] = set()\n         self.active_size: int = 0\n         self.itemproc_size: int = 0\n-        self.closing: Optional[Deferred] = None\n+        self.closing: Optional[Deferred[Spider]] = None\n \n     def add_response_request(\n         self, result: Union[Response, Failure], request: Request\n-    ) -> Deferred:\n-        deferred: Deferred = Deferred()\n+    ) -> _HandleOutputDeferred:\n+        deferred: _HandleOutputDeferred = Deferred()\n         self.queue.append((result, request, deferred))\n         if isinstance(result, Response):\n             self.active_size += max(len(result.body), self.MIN_RESPONSE_SIZE)\n@@ -117,12 +124,12 @@ class Scraper:\n         self.logformatter: LogFormatter = crawler.logformatter\n \n     @inlineCallbacks\n-    def open_spider(self, spider: Spider) -> Generator[Deferred, Any, None]:\n+    def open_spider(self, spider: Spider) -> Generator[Deferred[Any], Any, None]:\n         \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n         self.slot = Slot(self.crawler.settings.getint(\"SCRAPER_SLOT_MAX_ACTIVE_SIZE\"))\n         yield self.itemproc.open_spider(spider)\n \n-    def close_spider(self, spider: Spider) -> Deferred:\n+    def close_spider(self, spider: Spider) -> Deferred[Spider]:\n         \"\"\"Close a spider being scraped and release its resources\"\"\"\n         if self.slot is None:\n             raise RuntimeError(\"Scraper slot not assigned\")\n@@ -142,12 +149,12 @@ class Scraper:\n \n     def enqueue_scrape(\n         self, result: Union[Response, Failure], request: Request, spider: Spider\n-    ) -> Deferred:\n+    ) -> _HandleOutputDeferred:\n         if self.slot is None:\n             raise RuntimeError(\"Scraper slot not assigned\")\n         dfd = self.slot.add_response_request(result, request)\n \n-        def finish_scraping(_: Any) -> Any:\n+        def finish_scraping(_: _T) -> _T:\n             assert self.slot is not None\n             self.slot.finish_response(result, request)\n             self._check_if_closing(spider)\n@@ -174,7 +181,7 @@ class Scraper:\n \n     def _scrape(\n         self, result: Union[Response, Failure], request: Request, spider: Spider\n-    ) -> Deferred:\n+    ) -> _HandleOutputDeferred:\n         \"\"\"\n         Handle the downloaded response or failure through the spider callback/errback\n         \"\"\"\n@@ -182,32 +189,35 @@ class Scraper:\n             raise TypeError(\n                 f\"Incorrect type: expected Response or Failure, got {type(result)}: {result!r}\"\n             )\n-        dfd = self._scrape2(\n+        dfd: Deferred[Union[Iterable[Any], AsyncIterable[Any]]] = self._scrape2(\n             result, request, spider\n         )  # returns spider's processed output\n         dfd.addErrback(self.handle_spider_error, request, result, spider)\n-        dfd.addCallback(\n+        dfd2: _HandleOutputDeferred = dfd.addCallback(\n             self.handle_spider_output, request, cast(Response, result), spider\n         )\n-        return dfd\n+        return dfd2\n \n     def _scrape2(\n         self, result: Union[Response, Failure], request: Request, spider: Spider\n-    ) -> Deferred:\n+    ) -> Deferred[Union[Iterable[Any], AsyncIterable[Any]]]:\n         \"\"\"\n         Handle the different cases of request's result been a Response or a Failure\n         \"\"\"\n         if isinstance(result, Response):\n-            return self.spidermw.scrape_response(\n+            # Deferreds are invariant so Mutable*Chain isn't matched to *Iterable\n+            return self.spidermw.scrape_response(  # type: ignore[return-value]\n                 self.call_spider, result, request, spider\n             )\n         # else result is a Failure\n         dfd = self.call_spider(result, request, spider)\n-        return dfd.addErrback(self._log_download_errors, result, request, spider)\n+        dfd.addErrback(self._log_download_errors, result, request, spider)\n+        return dfd\n \n     def call_spider(\n         self, result: Union[Response, Failure], request: Request, spider: Spider\n-    ) -> Deferred:\n+    ) -> Deferred[Union[Iterable[Any], AsyncIterable[Any]]]:\n+        dfd: Deferred[Any]\n         if isinstance(result, Response):\n             if getattr(result, \"request\", None) is None:\n                 result.request = request\n@@ -225,7 +235,10 @@ class Scraper:\n             if request.errback:\n                 warn_on_generator_with_return_value(spider, request.errback)\n                 dfd.addErrback(request.errback)\n-        return dfd.addCallback(iterate_spider_output)\n+        dfd2: Deferred[Union[Iterable[Any], AsyncIterable[Any]]] = dfd.addCallback(\n+            iterate_spider_output\n+        )\n+        return dfd2\n \n     def handle_spider_error(\n         self,\n@@ -262,10 +275,11 @@ class Scraper:\n         request: Request,\n         response: Response,\n         spider: Spider,\n-    ) -> Deferred:\n+    ) -> _HandleOutputDeferred:\n         if not result:\n             return defer_succeed(None)\n         it: Union[Iterable[_T], AsyncIterable[_T]]\n+        dfd: Deferred[_ParallelResult]\n         if isinstance(result, AsyncIterable):\n             it = aiter_errback(\n                 result, self.handle_spider_error, request, response, spider\n@@ -290,11 +304,12 @@ class Scraper:\n                 response,\n                 spider,\n             )\n-        return dfd\n+        # returning Deferred[_ParallelResult] instead of Deferred[Union[_ParallelResult, None]]\n+        return dfd  # type: ignore[return-value]\n \n     def _process_spidermw_output(\n         self, output: Any, request: Request, response: Response, spider: Spider\n-    ) -> Optional[Deferred]:\n+    ) -> Optional[Deferred[Any]]:\n         \"\"\"Process each Request/Item (given in the output parameter) returned\n         from the given spider\n         \"\"\"\n\n@@ -45,7 +45,9 @@ logger = logging.getLogger(__name__)\n \n \n _T = TypeVar(\"_T\")\n-ScrapeFunc = Callable[[Union[Response, Failure], Request, Spider], Any]\n+ScrapeFunc = Callable[\n+    [Union[Response, Failure], Request, Spider], Union[Iterable[_T], AsyncIterable[_T]]\n+]\n \n \n def _isiterable(o: Any) -> bool:\n@@ -80,7 +82,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         response: Response,\n         request: Request,\n         spider: Spider,\n-    ) -> Any:\n+    ) -> Union[Iterable[_T], AsyncIterable[_T]]:\n         for method in self.methods[\"process_spider_input\"]:\n             method = cast(Callable, method)\n             try:\n@@ -311,7 +313,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         response: Response,\n         request: Request,\n         spider: Spider,\n-    ) -> Deferred:\n+    ) -> Deferred[Union[MutableChain[_T], MutableAsyncChain[_T]]]:\n         async def process_callback_output(\n             result: Union[Iterable[_T], AsyncIterable[_T]]\n         ) -> Union[MutableChain[_T], MutableAsyncChain[_T]]:\n@@ -322,12 +324,14 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         ) -> Union[Failure, MutableChain[_T], MutableAsyncChain[_T]]:\n             return self._process_spider_exception(response, spider, _failure)\n \n-        dfd: Deferred = mustbe_deferred(\n+        dfd: Deferred[Union[Iterable[_T], AsyncIterable[_T]]] = mustbe_deferred(\n             self._process_spider_input, scrape_func, response, request, spider\n         )\n+        dfd2: Deferred[Union[MutableChain[_T], MutableAsyncChain[_T]]] = (\n             dfd.addCallback(deferred_f_from_coro_f(process_callback_output))\n-        dfd.addErrback(process_spider_exception)\n-        return dfd\n+        )\n+        dfd2.addErrback(process_spider_exception)\n+        return dfd2\n \n     def process_start_requests(\n         self, start_requests: Iterable[Request], spider: Spider\n\n@@ -4,6 +4,8 @@ Item pipeline\n See documentation in docs/item-pipeline.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n from typing import Any, List\n \n from twisted.internet.defer import Deferred\n@@ -29,5 +31,5 @@ class ItemPipelineManager(MiddlewareManager):\n                 deferred_f_from_coro_f(pipe.process_item)\n             )\n \n-    def process_item(self, item: Any, spider: Spider) -> Deferred:\n+    def process_item(self, item: Any, spider: Spider) -> Deferred[Any]:\n         return self._process_chain(\"process_item\", item, spider)\n\n@@ -46,6 +46,12 @@ if TYPE_CHECKING:\n     _P = ParamSpec(\"_P\")\n \n _T = TypeVar(\"_T\")\n+_T2 = TypeVar(\"_T2\")\n+\n+# copied from twisted.internet.defer\n+_SelfResultT = TypeVar(\"_SelfResultT\")\n+_DeferredListResultItemT = Tuple[bool, _SelfResultT]\n+DeferredListResultListT = List[_DeferredListResultItemT[_SelfResultT]]\n \n \n def defer_fail(_failure: Failure) -> Deferred:\n@@ -62,7 +68,7 @@ def defer_fail(_failure: Failure) -> Deferred:\n     return d\n \n \n-def defer_succeed(result: Any) -> Deferred:\n+def defer_succeed(result: _T) -> Deferred[_T]:\n     \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n     next reactor loop\n \n@@ -128,10 +134,10 @@ def mustbe_deferred(\n def parallel(\n     iterable: Iterable[_T],\n     count: int,\n-    callable: Callable[Concatenate[_T, _P], Any],\n+    callable: Callable[Concatenate[_T, _P], _T2],\n     *args: _P.args,\n     **named: _P.kwargs,\n-) -> Deferred:\n+) -> Deferred[DeferredListResultListT[Iterator[_T2]]]:\n     \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n     using no more than ``count`` concurrent calls.\n \n@@ -191,12 +197,12 @@ class _AsyncCooperatorAdapter(Iterator[Deferred]):\n     def __init__(\n         self,\n         aiterable: AsyncIterable[_T],\n-        callable: Callable[Concatenate[_T, _P], Any],\n+        callable: Callable[Concatenate[_T, _P], _T2],\n         *callable_args: _P.args,\n         **callable_kwargs: _P.kwargs,\n     ):\n         self.aiterator: AsyncIterator[_T] = aiterable.__aiter__()\n-        self.callable: Callable[Concatenate[_T, _P], Any] = callable\n+        self.callable: Callable[Concatenate[_T, _P], _T2] = callable\n         self.callable_args: Tuple[Any, ...] = callable_args\n         self.callable_kwargs: Dict[str, Any] = callable_kwargs\n         self.finished: bool = False\n@@ -249,10 +255,10 @@ class _AsyncCooperatorAdapter(Iterator[Deferred]):\n def parallel_async(\n     async_iterable: AsyncIterable[_T],\n     count: int,\n-    callable: Callable[Concatenate[_T, _P], Any],\n+    callable: Callable[Concatenate[_T, _P], _T2],\n     *args: _P.args,\n     **named: _P.kwargs,\n-) -> Deferred:\n+) -> Deferred[DeferredListResultListT[Iterator[_T2]]]:\n     \"\"\"Like parallel but for async iterators\"\"\"\n     coop = Cooperator()\n     work = _AsyncCooperatorAdapter(async_iterable, callable, *args, **named)\n\n@@ -36,7 +36,7 @@ class ManagerTestCase(TestCase):\n         if not response:\n             response = Response(request.url)\n \n-        def download_func(**kwargs):\n+        def download_func(request, spider):\n             return response\n \n         dfd = self.mwman.download(download_func, request, self.spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
