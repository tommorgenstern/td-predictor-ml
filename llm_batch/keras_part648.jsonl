{"custom_id": "keras#dc8b81091cbb088e3eeb08474f2f4e9a57592cc5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 28 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 69 | Churn Cumulative: 729 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -12,6 +12,7 @@ import zipfile\n import numpy as np\n from tensorflow.io import gfile\n \n+from keras_core.backend.common import global_state\n from keras_core.layers.layer import Layer\n from keras_core.losses.loss import Loss\n from keras_core.metrics.metric import Metric\n@@ -34,33 +35,6 @@ _VARS_FNAME = \"model.weights\"  # Will become e.g. \"model.weights.h5\"\n _ASSETS_DIRNAME = \"assets\"\n \n \n-ATTR_SKIPLIST = frozenset(\n-    {\n-        \"_operations\",\n-        \"_layers\",\n-        \"_functional\",\n-        \"_losses\",\n-        \"_inbound_nodes\",\n-        \"_outbound_nodes\",\n-        \"_variables\",\n-        \"weights\",\n-        \"non_trainable_weights\",\n-        \"trainable_weights\",\n-        \"variables\",\n-        \"non_trainable_variables\",\n-        \"trainable_variables\",\n-        # TF trackable attrs\n-        \"_unconditional_checkpoint_dependencies\",\n-        \"_unconditional_dependency_names\",\n-        \"_checkpoint_dependencies\",\n-        \"_deferred_dependencies\",\n-        \"_deserialization_dependencies\",\n-        \"_lookup_dependency\",\n-        \"_self_unconditional_dependency_names\",\n-    }\n-)\n-\n-\n def save_model(model, filepath, weights_format=\"h5\"):\n     \"\"\"Save a zip-archive representing a Keras model to the given filepath.\n \n@@ -290,8 +264,20 @@ def _write_to_zip_recursively(zipfile_to_save, system_path, zip_path):\n \n \n def _walk_trackable(trackable):\n+    if isinstance(trackable, Layer):\n+        obj_type = \"Layer\"\n+    elif isinstance(trackable, Optimizer):\n+        obj_type = \"Optimizer\"\n+    elif isinstance(trackable, Metric):\n+        obj_type = \"Metric\"\n+    elif isinstance(trackable, Loss):\n+        obj_type = \"Loss\"\n+    else:\n+        raise ValueError(f\"Invalid obj_type: {obj_type}\")\n+    attr_skiplist = get_attr_skiplist(obj_type)\n+\n     for child_attr in dir(trackable):\n-        if child_attr.startswith(\"__\") or child_attr in ATTR_SKIPLIST:\n+        if child_attr.startswith(\"__\") or child_attr in attr_skiplist:\n             continue\n         try:\n             child_obj = getattr(trackable, child_attr)\n@@ -612,6 +598,33 @@ def get_temp_dir():\n     return temp_dir\n \n \n+def get_attr_skiplist(obj_type):\n+    skiplist = global_state.get_global_attribute(\n+        f\"saving_attr_skiplist_{obj_type}\", None\n+    )\n+    if skiplist is not None:\n+        return skiplist\n+    if obj_type == \"Layer\":\n+        ref_obj = Layer()\n+        skiplist = dir(ref_obj)\n+    elif obj_type == \"Metric\":\n+        ref_obj = Metric()\n+        skiplist = dir(ref_obj)\n+    elif obj_type == \"Optimizer\":\n+        ref_obj = Optimizer(1.0)\n+        skiplist = dir(ref_obj)\n+        skiplist.remove(\"variables\")\n+    elif obj_type == \"Loss\":\n+        ref_obj = Loss()\n+        skiplist = dir(ref_obj)\n+    else:\n+        raise ValueError(f\"Invalid obj_type: {obj_type}\")\n+    global_state.set_global_attribute(\n+        f\"saving_attr_skiplist_{obj_type}\", skiplist\n+    )\n+    return skiplist\n+\n+\n def _is_keras_trackable(obj):\n     return isinstance(\n         obj,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#39bbd2a03d8830b4dffc889292be2bafe4d33c63", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 163 | Lines Deleted: 71 | Files Changed: 12 | Hunks: 79 | Methods Changed: 55 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 234 | Churn Cumulative: 6312 | Contributors (this commit): 3 | Commits (past 90d): 59 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.activation_benchmark \\\n     --benchmark_name=benchmark_elu \\\n-    --num_samples=8192 \\\n-    --batch_size=1024 \\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -38,6 +38,10 @@ def benchmark_elu(\n         num_samples=num_samples,\n         batch_size=batch_size,\n     )\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n \n \n def benchmark_prelu(\n@@ -58,6 +62,10 @@ def benchmark_prelu(\n         num_samples=num_samples,\n         batch_size=batch_size,\n     )\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n \n \n def benchmark_relu(\n@@ -78,6 +86,10 @@ def benchmark_relu(\n         num_samples=num_samples,\n         batch_size=batch_size,\n     )\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n \n \n def benchmark_leaky_relu(\n@@ -98,6 +110,10 @@ def benchmark_leaky_relu(\n         num_samples=num_samples,\n         batch_size=batch_size,\n     )\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n \n \n def benchmark_softmax(\n@@ -118,6 +134,10 @@ def benchmark_softmax(\n         num_samples=num_samples,\n         batch_size=batch_size,\n     )\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n \n \n BENCHMARK_NAMES = {\n@@ -136,7 +156,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.attention_benchmark \\\n     --benchmark_name=benchmark_attention \\\n-    --num_samples=8192 \\\n-    --batch_size=1024 \\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -115,7 +115,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -108,15 +108,28 @@ class LayerBenchmark:\n         input_shape,\n         flat_call_inputs=True,\n         jit_compile=True,\n+        keras_core_layer=None,\n+        tf_keras_layer=None,\n     ):\n         self.layer_name = layer_name\n-        self.input_shape = input_shape\n         _keras_core_layer_class = getattr(keras_core.layers, layer_name)\n         _tf_keras_layer_class = getattr(tf.keras.layers, layer_name)\n \n+        if keras_core_layer is None:\n+            # Sometimes you want to initialize the keras_core layer and tf_keras\n+            # layer in a different way. For example, `Bidirectional` layer,\n+            # which takes in `keras_core.layers.Layer` and\n+            # `tf.keras.layer.Layer` separately.\n             self._keras_core_layer = _keras_core_layer_class(**init_args)\n-        self._tf_keras_layer = _tf_keras_layer_class(**init_args)\n+        else:\n+            self._keras_core_layer = keras_core_layer\n \n+        if tf_keras_layer is None:\n+            self._tf_keras_layer = _tf_keras_layer_class(**init_args)\n+        else:\n+            self._tf_keras_layer = tf_keras_layer\n+\n+        self.input_shape = input_shape\n         self._keras_core_model = self._build_keras_core_model(\n             input_shape, flat_call_inputs\n         )\n\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.conv_benchmark \\\n     --benchmark_name=benchmark_conv2D \\\n-    --num_samples=2000 \\\n-    --batch_size=20 \\\n+    --num_samples=2046 \\\n+    --batch_size=256 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -28,13 +28,13 @@ def benchmark_conv1D(\n ):\n     layer_name = \"Conv1D\"\n     init_args = {\n-        \"filters\": 16,\n+        \"filters\": 64,\n         \"kernel_size\": 2,\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 16],\n+        input_shape=[1024, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -118,7 +118,7 @@ def benchmark_depthwise_conv1D(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 4],\n+        input_shape=[256, 64],\n         jit_compile=jit_compile,\n     )\n \n@@ -175,7 +175,7 @@ def benchmark_separable_conv1D(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 4],\n+        input_shape=[256, 64],\n         jit_compile=jit_compile,\n     )\n \n@@ -226,13 +226,13 @@ def benchmark_conv1D_transpose(\n ):\n     layer_name = \"Conv1DTranspose\"\n     init_args = {\n-        \"filters\": 16,\n-        \"kernel_size\": 2,\n+        \"filters\": 32,\n+        \"kernel_size\": 4,\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 4],\n+        input_shape=[256, 256],\n         jit_compile=jit_compile,\n     )\n \n\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.core_benchmark \\\n     --benchmark_name=benchmark_dense \\\n-    --num_samples=8192 \\\n-    --batch_size=1024 \\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -27,7 +27,7 @@ def benchmark_dense(\n     jit_compile=True,\n ):\n     layer_name = \"Dense\"\n-    init_args = {\"units\": 128}\n+    init_args = {\"units\": 256}\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n@@ -54,12 +54,12 @@ def benchmark_einsum_dense(\n     layer_name = \"EinsumDense\"\n     init_args = {\n         \"equation\": \"abc,cd->abd\",\n-        \"output_shape\": (None, 128),\n+        \"output_shape\": (None, 256),\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[64, 32],\n+        input_shape=[256, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -81,17 +81,19 @@ def benchmark_embedding(\n ):\n     layer_name = \"Embedding\"\n     init_args = {\n-        \"input_dim\": 30,\n-        \"output_shape\": 128,\n+        \"input_dim\": 128,\n+        \"output_dim\": 256,\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[64, 32],\n+        input_shape=[\n+            256,\n+        ],\n         jit_compile=jit_compile,\n     )\n \n-    data = np.random.randint(30, size=(num_samples, 32))\n+    data = [np.random.randint(30, size=(num_samples, 256))]\n     benchmark.benchmark_predict(\n         num_samples=num_samples,\n         batch_size=batch_size,\n@@ -108,6 +110,7 @@ def benchmark_embedding(\n BENCHMARK_NAMES = {\n     \"benchmark_dense\": benchmark_dense,\n     \"benchmark_einsum_dense\": benchmark_einsum_dense,\n+    \"benchmark_embedding\": benchmark_embedding,\n }\n \n \n@@ -118,7 +121,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.merge_benchmark \\\n     --benchmark_name=benchmark_add \\\n-    --num_samples=8192 \\\n-    --batch_size=1024 \\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -40,6 +40,11 @@ def benchmark_add(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n def benchmark_average(\n     num_samples,\n@@ -61,6 +66,11 @@ def benchmark_average(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n def benchmark_concatenate(\n     num_samples,\n@@ -82,6 +92,11 @@ def benchmark_concatenate(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n def benchmark_dot(\n     num_samples,\n@@ -89,7 +104,7 @@ def benchmark_dot(\n     jit_compile=True,\n ):\n     layer_name = \"Dot\"\n-    init_args = {}\n+    init_args = {\"axes\": [2, 1]}\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n@@ -103,6 +118,11 @@ def benchmark_dot(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n def benchmark_maximum(\n     num_samples,\n@@ -124,6 +144,11 @@ def benchmark_maximum(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n def benchmark_minimum(\n     num_samples,\n@@ -145,6 +170,11 @@ def benchmark_minimum(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n def benchmark_multiply(\n     num_samples,\n@@ -156,7 +186,7 @@ def benchmark_multiply(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[[256, 256], [256, 32]],\n+        input_shape=[[256, 64], [256, 64]],\n         flat_call_inputs=False,\n         jit_compile=jit_compile,\n     )\n@@ -166,6 +196,11 @@ def benchmark_multiply(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n def benchmark_subtract(\n     num_samples,\n@@ -187,6 +222,11 @@ def benchmark_subtract(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n BENCHMARK_NAMES = {\n     \"benchmark_add\": benchmark_add,\n@@ -207,7 +247,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.normalization_benchmark \\\n     --benchmark_name=benchmark_batch_normalization \\\n-    --num_samples=2400 \\\n-    --batch_size=300 \\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -82,7 +82,7 @@ def benchmark_layer_normalization(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 256, 4],\n+        input_shape=[256, 128, 4],\n         jit_compile=jit_compile,\n     )\n \n@@ -107,7 +107,7 @@ def benchmark_unit_normalization(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 256, 4],\n+        input_shape=[256, 128, 4],\n         jit_compile=jit_compile,\n     )\n \n@@ -116,6 +116,11 @@ def benchmark_unit_normalization(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n BENCHMARK_NAMES = {\n     \"benchmark_batch_normalization\": benchmark_batch_normalization,\n@@ -132,7 +137,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -33,7 +33,7 @@ def benchmark_average_pooling1d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 3],\n+        input_shape=[1024, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -87,7 +87,7 @@ def benchmark_average_pooling3d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[64, 64, 64, 3],\n+        input_shape=[64, 64, 32, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -114,7 +114,7 @@ def benchmark_max_pooling1d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 3],\n+        input_shape=[1024, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -168,7 +168,7 @@ def benchmark_max_pooling3d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[64, 64, 64, 3],\n+        input_shape=[64, 64, 32, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -193,7 +193,7 @@ def benchmark_global_average_pooling1d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 3],\n+        input_shape=[1024, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -243,7 +243,7 @@ def benchmark_global_average_pooling3d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[64, 64, 64, 3],\n+        input_shape=[64, 64, 32, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -268,7 +268,7 @@ def benchmark_global_max_pooling1d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 3],\n+        input_shape=[1024, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -318,7 +318,7 @@ def benchmark_global_max_pooling3d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[64, 64, 64, 3],\n+        input_shape=[64, 64, 32, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -356,7 +356,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -52,7 +52,7 @@ def benchmark_gaussian_dropout(\n     batch_size,\n     jit_compile=True,\n ):\n-    layer_name = \"GaussionDropout\"\n+    layer_name = \"GaussianDropout\"\n     init_args = {\n         \"rate\": 0.5,\n     }\n@@ -79,7 +79,7 @@ def benchmark_gaussian_noise(\n     batch_size,\n     jit_compile=True,\n ):\n-    layer_name = \"GaussionNoise\"\n+    layer_name = \"GaussianNoise\"\n     init_args = {\n         \"stddev\": 0.5,\n     }\n@@ -199,7 +199,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -30,7 +30,7 @@ def benchmark_cropping1d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 3],\n+        input_shape=[1024, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -127,7 +127,7 @@ def benchmark_permute(\n ):\n     layer_name = \"Permute\"\n     init_args = {\n-        \"dim\": (2, 1),\n+        \"dims\": (2, 1),\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n@@ -182,7 +182,7 @@ def benchmark_up_sampling2d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 256, 3],\n+        input_shape=[128, 128, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -207,7 +207,7 @@ def benchmark_up_sampling3d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 32, 32, 3],\n+        input_shape=[32, 16, 16, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -319,7 +319,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -12,6 +12,7 @@ python3 -m benchmarks.layer_benchmark.rnn_benchmark \\\n ```\n \"\"\"\n \n+import tensorflow as tf\n from absl import app\n from absl import flags\n \n@@ -62,7 +63,7 @@ def benchmark_conv_lstm2d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 64, 64, 3],\n+        input_shape=[32, 32, 32, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -84,13 +85,13 @@ def benchmark_conv_lstm3d(\n ):\n     layer_name = \"ConvLSTM3D\"\n     init_args = {\n-        \"filters\": 16,\n+        \"filters\": 8,\n         \"kernel_size\": 2,\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[16, 32, 32, 16, 3],\n+        input_shape=[8, 16, 16, 16, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -117,7 +118,7 @@ def benchmark_gru(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 256],\n+        input_shape=[256, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -144,7 +145,7 @@ def benchmark_lstm(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 256],\n+        input_shape=[256, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -171,7 +172,7 @@ def benchmark_simple_rnn(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 256],\n+        input_shape=[256, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -192,14 +193,18 @@ def benchmark_bidirectional(\n     jit_compile=True,\n ):\n     layer_name = \"Bidirectional\"\n-    init_args = {\n-        \"layer\": keras_core.layers.LSTM(32),\n-    }\n+    init_args = {}\n+    keras_core_layer = keras_core.layers.Bidirectional(\n+        keras_core.layers.LSTM(32)\n+    )\n+    tf_keras_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 256],\n+        input_shape=[256, 256],\n         jit_compile=jit_compile,\n+        keras_core_layer=keras_core_layer,\n+        tf_keras_layer=tf_keras_layer,\n     )\n \n     benchmark.benchmark_predict(\n@@ -219,14 +224,20 @@ def benchmark_time_distributed(\n     jit_compile=True,\n ):\n     layer_name = \"TimeDistributed\"\n-    init_args = {\n-        \"layer\": keras_core.layers.Conv2D(64, (3, 3)),\n-    }\n+    init_args = {}\n+    keras_core_layer = keras_core.layers.TimeDistributed(\n+        keras_core.layers.Conv2D(16, (3, 3))\n+    )\n+    tf_keras_layer = tf.keras.layers.TimeDistributed(\n+        tf.keras.layers.Conv2D(16, (3, 3))\n+    )\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[10, 128, 128, 3],\n+        input_shape=[10, 32, 32, 3],\n         jit_compile=jit_compile,\n+        keras_core_layer=keras_core_layer,\n+        tf_keras_layer=tf_keras_layer,\n     )\n \n     benchmark.benchmark_predict(\n@@ -259,7 +270,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -56,8 +56,8 @@ class ZeroPadding1D(Layer):\n \n     def compute_output_shape(self, input_shape):\n         output_shape = list(input_shape)\n-        if input_shape[1] is not None:\n-            input_shape[1] += self.padding[0] + self.padding[1]\n+        if output_shape[1] is not None:\n+            output_shape[1] += self.padding[0] + self.padding[1]\n         return tuple(output_shape)\n \n     def call(self, inputs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06225c04cde2c636b03c7fae61f084fe461ee0c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 89 | Lines Deleted: 169 | Files Changed: 14 | Hunks: 83 | Methods Changed: 57 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 258 | Churn Cumulative: 8705 | Contributors (this commit): 11 | Commits (past 90d): 162 | Contributors (cumulative): 38 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.activation_benchmark \\\n     --benchmark_name=benchmark_elu \\\n-    --num_samples=2048 \\\n-    --batch_size=256 \\\n+    --num_samples=8192 \\\n+    --batch_size=1024 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -38,10 +38,6 @@ def benchmark_elu(\n         num_samples=num_samples,\n         batch_size=batch_size,\n     )\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n \n \n def benchmark_prelu(\n@@ -62,10 +58,6 @@ def benchmark_prelu(\n         num_samples=num_samples,\n         batch_size=batch_size,\n     )\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n \n \n def benchmark_relu(\n@@ -86,10 +78,6 @@ def benchmark_relu(\n         num_samples=num_samples,\n         batch_size=batch_size,\n     )\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n \n \n def benchmark_leaky_relu(\n@@ -110,10 +98,6 @@ def benchmark_leaky_relu(\n         num_samples=num_samples,\n         batch_size=batch_size,\n     )\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n \n \n def benchmark_softmax(\n@@ -134,10 +118,6 @@ def benchmark_softmax(\n         num_samples=num_samples,\n         batch_size=batch_size,\n     )\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n \n \n BENCHMARK_NAMES = {\n@@ -156,7 +136,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES.items():\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.attention_benchmark \\\n     --benchmark_name=benchmark_attention \\\n-    --num_samples=2048 \\\n-    --batch_size=256 \\\n+    --num_samples=8192 \\\n+    --batch_size=1024 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -115,7 +115,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES.items():\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -108,28 +108,15 @@ class LayerBenchmark:\n         input_shape,\n         flat_call_inputs=True,\n         jit_compile=True,\n-        keras_core_layer=None,\n-        tf_keras_layer=None,\n     ):\n         self.layer_name = layer_name\n+        self.input_shape = input_shape\n         _keras_core_layer_class = getattr(keras_core.layers, layer_name)\n         _tf_keras_layer_class = getattr(tf.keras.layers, layer_name)\n \n-        if keras_core_layer is None:\n-            # Sometimes you want to initialize the keras_core layer and tf_keras\n-            # layer in a different way. For example, `Bidirectional` layer,\n-            # which takes in `keras_core.layers.Layer` and\n-            # `tf.keras.layer.Layer` separately.\n         self._keras_core_layer = _keras_core_layer_class(**init_args)\n-        else:\n-            self._keras_core_layer = keras_core_layer\n-\n-        if tf_keras_layer is None:\n         self._tf_keras_layer = _tf_keras_layer_class(**init_args)\n-        else:\n-            self._tf_keras_layer = tf_keras_layer\n \n-        self.input_shape = input_shape\n         self._keras_core_model = self._build_keras_core_model(\n             input_shape, flat_call_inputs\n         )\n\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.conv_benchmark \\\n     --benchmark_name=benchmark_conv2D \\\n-    --num_samples=2046 \\\n-    --batch_size=256 \\\n+    --num_samples=2000 \\\n+    --batch_size=20 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -28,13 +28,13 @@ def benchmark_conv1D(\n ):\n     layer_name = \"Conv1D\"\n     init_args = {\n-        \"filters\": 64,\n+        \"filters\": 16,\n         \"kernel_size\": 2,\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[1024, 256],\n+        input_shape=[256, 16],\n         jit_compile=jit_compile,\n     )\n \n@@ -118,7 +118,7 @@ def benchmark_depthwise_conv1D(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 64],\n+        input_shape=[32, 4],\n         jit_compile=jit_compile,\n     )\n \n@@ -175,7 +175,7 @@ def benchmark_separable_conv1D(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 64],\n+        input_shape=[32, 4],\n         jit_compile=jit_compile,\n     )\n \n@@ -226,13 +226,13 @@ def benchmark_conv1D_transpose(\n ):\n     layer_name = \"Conv1DTranspose\"\n     init_args = {\n-        \"filters\": 32,\n-        \"kernel_size\": 4,\n+        \"filters\": 16,\n+        \"kernel_size\": 2,\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 256],\n+        input_shape=[32, 4],\n         jit_compile=jit_compile,\n     )\n \n\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.core_benchmark \\\n     --benchmark_name=benchmark_dense \\\n-    --num_samples=2048 \\\n-    --batch_size=256 \\\n+    --num_samples=8192 \\\n+    --batch_size=1024 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -27,7 +27,7 @@ def benchmark_dense(\n     jit_compile=True,\n ):\n     layer_name = \"Dense\"\n-    init_args = {\"units\": 256}\n+    init_args = {\"units\": 128}\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n@@ -54,12 +54,12 @@ def benchmark_einsum_dense(\n     layer_name = \"EinsumDense\"\n     init_args = {\n         \"equation\": \"abc,cd->abd\",\n-        \"output_shape\": (None, 256),\n+        \"output_shape\": (None, 128),\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 256],\n+        input_shape=[64, 32],\n         jit_compile=jit_compile,\n     )\n \n@@ -81,19 +81,17 @@ def benchmark_embedding(\n ):\n     layer_name = \"Embedding\"\n     init_args = {\n-        \"input_dim\": 128,\n-        \"output_dim\": 256,\n+        \"input_dim\": 30,\n+        \"output_shape\": 128,\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[\n-            256,\n-        ],\n+        input_shape=[64, 32],\n         jit_compile=jit_compile,\n     )\n \n-    data = [np.random.randint(30, size=(num_samples, 256))]\n+    data = np.random.randint(30, size=(num_samples, 32))\n     benchmark.benchmark_predict(\n         num_samples=num_samples,\n         batch_size=batch_size,\n@@ -110,7 +108,6 @@ def benchmark_embedding(\n BENCHMARK_NAMES = {\n     \"benchmark_dense\": benchmark_dense,\n     \"benchmark_einsum_dense\": benchmark_einsum_dense,\n-    \"benchmark_embedding\": benchmark_embedding,\n }\n \n \n@@ -121,7 +118,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES.items():\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.merge_benchmark \\\n     --benchmark_name=benchmark_add \\\n-    --num_samples=2048 \\\n-    --batch_size=256 \\\n+    --num_samples=8192 \\\n+    --batch_size=1024 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -40,11 +40,6 @@ def benchmark_add(\n         batch_size=batch_size,\n     )\n \n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n \n def benchmark_average(\n     num_samples,\n@@ -66,11 +61,6 @@ def benchmark_average(\n         batch_size=batch_size,\n     )\n \n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n \n def benchmark_concatenate(\n     num_samples,\n@@ -92,11 +82,6 @@ def benchmark_concatenate(\n         batch_size=batch_size,\n     )\n \n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n \n def benchmark_dot(\n     num_samples,\n@@ -104,7 +89,7 @@ def benchmark_dot(\n     jit_compile=True,\n ):\n     layer_name = \"Dot\"\n-    init_args = {\"axes\": [2, 1]}\n+    init_args = {}\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n@@ -118,11 +103,6 @@ def benchmark_dot(\n         batch_size=batch_size,\n     )\n \n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n \n def benchmark_maximum(\n     num_samples,\n@@ -144,11 +124,6 @@ def benchmark_maximum(\n         batch_size=batch_size,\n     )\n \n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n \n def benchmark_minimum(\n     num_samples,\n@@ -170,11 +145,6 @@ def benchmark_minimum(\n         batch_size=batch_size,\n     )\n \n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n \n def benchmark_multiply(\n     num_samples,\n@@ -186,7 +156,7 @@ def benchmark_multiply(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[[256, 64], [256, 64]],\n+        input_shape=[[256, 256], [256, 32]],\n         flat_call_inputs=False,\n         jit_compile=jit_compile,\n     )\n@@ -196,11 +166,6 @@ def benchmark_multiply(\n         batch_size=batch_size,\n     )\n \n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n \n def benchmark_subtract(\n     num_samples,\n@@ -222,11 +187,6 @@ def benchmark_subtract(\n         batch_size=batch_size,\n     )\n \n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n \n BENCHMARK_NAMES = {\n     \"benchmark_add\": benchmark_add,\n@@ -247,7 +207,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES.items():\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.normalization_benchmark \\\n     --benchmark_name=benchmark_batch_normalization \\\n-    --num_samples=2048 \\\n-    --batch_size=256 \\\n+    --num_samples=2400 \\\n+    --batch_size=300 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -82,7 +82,7 @@ def benchmark_layer_normalization(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 128, 4],\n+        input_shape=[256, 256, 4],\n         jit_compile=jit_compile,\n     )\n \n@@ -107,7 +107,7 @@ def benchmark_unit_normalization(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 128, 4],\n+        input_shape=[256, 256, 4],\n         jit_compile=jit_compile,\n     )\n \n@@ -116,11 +116,6 @@ def benchmark_unit_normalization(\n         batch_size=batch_size,\n     )\n \n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n \n BENCHMARK_NAMES = {\n     \"benchmark_batch_normalization\": benchmark_batch_normalization,\n@@ -137,7 +132,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES.items():\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -33,7 +33,7 @@ def benchmark_average_pooling1d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[1024, 256],\n+        input_shape=[256, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -87,7 +87,7 @@ def benchmark_average_pooling3d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[64, 64, 32, 3],\n+        input_shape=[64, 64, 64, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -114,7 +114,7 @@ def benchmark_max_pooling1d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[1024, 256],\n+        input_shape=[256, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -168,7 +168,7 @@ def benchmark_max_pooling3d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[64, 64, 32, 3],\n+        input_shape=[64, 64, 64, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -193,7 +193,7 @@ def benchmark_global_average_pooling1d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[1024, 256],\n+        input_shape=[256, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -243,7 +243,7 @@ def benchmark_global_average_pooling3d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[64, 64, 32, 3],\n+        input_shape=[64, 64, 64, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -268,7 +268,7 @@ def benchmark_global_max_pooling1d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[1024, 256],\n+        input_shape=[256, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -318,7 +318,7 @@ def benchmark_global_max_pooling3d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[64, 64, 32, 3],\n+        input_shape=[64, 64, 64, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -356,7 +356,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES.items():\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -52,7 +52,7 @@ def benchmark_gaussian_dropout(\n     batch_size,\n     jit_compile=True,\n ):\n-    layer_name = \"GaussianDropout\"\n+    layer_name = \"GaussionDropout\"\n     init_args = {\n         \"rate\": 0.5,\n     }\n@@ -79,7 +79,7 @@ def benchmark_gaussian_noise(\n     batch_size,\n     jit_compile=True,\n ):\n-    layer_name = \"GaussianNoise\"\n+    layer_name = \"GaussionNoise\"\n     init_args = {\n         \"stddev\": 0.5,\n     }\n@@ -199,7 +199,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES.items():\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -30,7 +30,7 @@ def benchmark_cropping1d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[1024, 256],\n+        input_shape=[256, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -127,7 +127,7 @@ def benchmark_permute(\n ):\n     layer_name = \"Permute\"\n     init_args = {\n-        \"dims\": (2, 1),\n+        \"dim\": (2, 1),\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n@@ -182,7 +182,7 @@ def benchmark_up_sampling2d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[128, 128, 3],\n+        input_shape=[256, 256, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -207,7 +207,7 @@ def benchmark_up_sampling3d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 16, 16, 3],\n+        input_shape=[32, 32, 32, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -319,7 +319,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES.items():\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -12,7 +12,6 @@ python3 -m benchmarks.layer_benchmark.rnn_benchmark \\\n ```\n \"\"\"\n \n-import tensorflow as tf\n from absl import app\n from absl import flags\n \n@@ -63,7 +62,7 @@ def benchmark_conv_lstm2d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 32, 32, 3],\n+        input_shape=[32, 64, 64, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -85,13 +84,13 @@ def benchmark_conv_lstm3d(\n ):\n     layer_name = \"ConvLSTM3D\"\n     init_args = {\n-        \"filters\": 8,\n+        \"filters\": 16,\n         \"kernel_size\": 2,\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[8, 16, 16, 16, 3],\n+        input_shape=[16, 32, 32, 16, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -118,7 +117,7 @@ def benchmark_gru(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 256],\n+        input_shape=[32, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -145,7 +144,7 @@ def benchmark_lstm(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 256],\n+        input_shape=[32, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -172,7 +171,7 @@ def benchmark_simple_rnn(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 256],\n+        input_shape=[32, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -193,18 +192,14 @@ def benchmark_bidirectional(\n     jit_compile=True,\n ):\n     layer_name = \"Bidirectional\"\n-    init_args = {}\n-    keras_core_layer = keras_core.layers.Bidirectional(\n-        keras_core.layers.LSTM(32)\n-    )\n-    tf_keras_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))\n+    init_args = {\n+        \"layer\": keras_core.layers.LSTM(32),\n+    }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 256],\n+        input_shape=[32, 256],\n         jit_compile=jit_compile,\n-        keras_core_layer=keras_core_layer,\n-        tf_keras_layer=tf_keras_layer,\n     )\n \n     benchmark.benchmark_predict(\n@@ -224,20 +219,14 @@ def benchmark_time_distributed(\n     jit_compile=True,\n ):\n     layer_name = \"TimeDistributed\"\n-    init_args = {}\n-    keras_core_layer = keras_core.layers.TimeDistributed(\n-        keras_core.layers.Conv2D(16, (3, 3))\n-    )\n-    tf_keras_layer = tf.keras.layers.TimeDistributed(\n-        tf.keras.layers.Conv2D(16, (3, 3))\n-    )\n+    init_args = {\n+        \"layer\": keras_core.layers.Conv2D(64, (3, 3)),\n+    }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[10, 32, 32, 3],\n+        input_shape=[10, 128, 128, 3],\n         jit_compile=jit_compile,\n-        keras_core_layer=keras_core_layer,\n-        tf_keras_layer=tf_keras_layer,\n     )\n \n     benchmark.benchmark_predict(\n@@ -270,7 +259,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES.items():\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -869,12 +869,20 @@ class Layer(BackendLayer, Operation):\n                     f\"Layer '{self.name}' was never built \"\n                     \"and thus it doesn't have any variables. \"\n                     f\"However the weights file lists {len(store.keys())} \"\n-                    \"variables for this layer. In most cases, \"\n-                    \"this indicates that you need to implement the \"\n-                    \"`def build_from_config(self, config)` method \"\n-                    \"on the layer. \"\n-                    \"You might also want to implement the method \"\n-                    \"that generates the config at saving time, \"\n+                    \"variables for this layer.\\n\"\n+                    \"In most cases, this error indicates that either:\\n\\n\"\n+                    \"1. The layer is owned by a parent layer that \"\n+                    \"implements a `build()` method, but calling the \"\n+                    \"parent's `build()` method did NOT create the state of \"\n+                    f\"the child layer '{self.name}'. A `build()` method \"\n+                    \"must create ALL state for the layer, including \"\n+                    \"the state of any children layers.\\n\\n\"\n+                    \"2. You need to implement \"\n+                    \"the `def build_from_config(self, config)` method \"\n+                    f\"on layer '{self.name}', to specify how to rebuild \"\n+                    \"it during loading. \"\n+                    \"In this case, you might also want to implement the \"\n+                    \"method that generates the build config at saving time, \"\n                     \"`def get_build_config(self)`. \"\n                     \"The method `build_from_config()` is meant \"\n                     \"to create the state \"\n\n@@ -56,8 +56,8 @@ class ZeroPadding1D(Layer):\n \n     def compute_output_shape(self, input_shape):\n         output_shape = list(input_shape)\n-        if output_shape[1] is not None:\n-            output_shape[1] += self.padding[0] + self.padding[1]\n+        if input_shape[1] is not None:\n+            input_shape[1] += self.padding[0] + self.padding[1]\n         return tuple(output_shape)\n \n     def call(self, inputs):\n\n@@ -76,10 +76,14 @@ class Nadam(optimizer.Optimizer):\n         \"\"\"\n         if self.built:\n             return\n+        if var_list:\n+            dtype = var_list[0].dtype\n+        else:\n+            dtype = backend.floatx()\n         super().build(var_list)\n         self._momentums = []\n         self._velocities = []\n-        self._u_product = backend.Variable(1.0, dtype=var_list[0].dtype)\n+        self._u_product = backend.Variable(1.0, dtype=dtype)\n \n         for var in var_list:\n             self._momentums.append(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#79a64649780e7f0ad9c923edfd5508207db9cd19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 163 | Lines Deleted: 71 | Files Changed: 12 | Hunks: 79 | Methods Changed: 55 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 234 | Churn Cumulative: 6778 | Contributors (this commit): 3 | Commits (past 90d): 83 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.activation_benchmark \\\n     --benchmark_name=benchmark_elu \\\n-    --num_samples=8192 \\\n-    --batch_size=1024 \\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -38,6 +38,10 @@ def benchmark_elu(\n         num_samples=num_samples,\n         batch_size=batch_size,\n     )\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n \n \n def benchmark_prelu(\n@@ -58,6 +62,10 @@ def benchmark_prelu(\n         num_samples=num_samples,\n         batch_size=batch_size,\n     )\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n \n \n def benchmark_relu(\n@@ -78,6 +86,10 @@ def benchmark_relu(\n         num_samples=num_samples,\n         batch_size=batch_size,\n     )\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n \n \n def benchmark_leaky_relu(\n@@ -98,6 +110,10 @@ def benchmark_leaky_relu(\n         num_samples=num_samples,\n         batch_size=batch_size,\n     )\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n \n \n def benchmark_softmax(\n@@ -118,6 +134,10 @@ def benchmark_softmax(\n         num_samples=num_samples,\n         batch_size=batch_size,\n     )\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n \n \n BENCHMARK_NAMES = {\n@@ -136,7 +156,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.attention_benchmark \\\n     --benchmark_name=benchmark_attention \\\n-    --num_samples=8192 \\\n-    --batch_size=1024 \\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -115,7 +115,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -108,15 +108,28 @@ class LayerBenchmark:\n         input_shape,\n         flat_call_inputs=True,\n         jit_compile=True,\n+        keras_core_layer=None,\n+        tf_keras_layer=None,\n     ):\n         self.layer_name = layer_name\n-        self.input_shape = input_shape\n         _keras_core_layer_class = getattr(keras_core.layers, layer_name)\n         _tf_keras_layer_class = getattr(tf.keras.layers, layer_name)\n \n+        if keras_core_layer is None:\n+            # Sometimes you want to initialize the keras_core layer and tf_keras\n+            # layer in a different way. For example, `Bidirectional` layer,\n+            # which takes in `keras_core.layers.Layer` and\n+            # `tf.keras.layer.Layer` separately.\n             self._keras_core_layer = _keras_core_layer_class(**init_args)\n-        self._tf_keras_layer = _tf_keras_layer_class(**init_args)\n+        else:\n+            self._keras_core_layer = keras_core_layer\n \n+        if tf_keras_layer is None:\n+            self._tf_keras_layer = _tf_keras_layer_class(**init_args)\n+        else:\n+            self._tf_keras_layer = tf_keras_layer\n+\n+        self.input_shape = input_shape\n         self._keras_core_model = self._build_keras_core_model(\n             input_shape, flat_call_inputs\n         )\n\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.conv_benchmark \\\n     --benchmark_name=benchmark_conv2D \\\n-    --num_samples=2000 \\\n-    --batch_size=20 \\\n+    --num_samples=2046 \\\n+    --batch_size=256 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -28,13 +28,13 @@ def benchmark_conv1D(\n ):\n     layer_name = \"Conv1D\"\n     init_args = {\n-        \"filters\": 16,\n+        \"filters\": 64,\n         \"kernel_size\": 2,\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 16],\n+        input_shape=[1024, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -118,7 +118,7 @@ def benchmark_depthwise_conv1D(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 4],\n+        input_shape=[256, 64],\n         jit_compile=jit_compile,\n     )\n \n@@ -175,7 +175,7 @@ def benchmark_separable_conv1D(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 4],\n+        input_shape=[256, 64],\n         jit_compile=jit_compile,\n     )\n \n@@ -226,13 +226,13 @@ def benchmark_conv1D_transpose(\n ):\n     layer_name = \"Conv1DTranspose\"\n     init_args = {\n-        \"filters\": 16,\n-        \"kernel_size\": 2,\n+        \"filters\": 32,\n+        \"kernel_size\": 4,\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 4],\n+        input_shape=[256, 256],\n         jit_compile=jit_compile,\n     )\n \n\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.core_benchmark \\\n     --benchmark_name=benchmark_dense \\\n-    --num_samples=8192 \\\n-    --batch_size=1024 \\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -27,7 +27,7 @@ def benchmark_dense(\n     jit_compile=True,\n ):\n     layer_name = \"Dense\"\n-    init_args = {\"units\": 128}\n+    init_args = {\"units\": 256}\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n@@ -54,12 +54,12 @@ def benchmark_einsum_dense(\n     layer_name = \"EinsumDense\"\n     init_args = {\n         \"equation\": \"abc,cd->abd\",\n-        \"output_shape\": (None, 128),\n+        \"output_shape\": (None, 256),\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[64, 32],\n+        input_shape=[256, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -81,17 +81,19 @@ def benchmark_embedding(\n ):\n     layer_name = \"Embedding\"\n     init_args = {\n-        \"input_dim\": 30,\n-        \"output_shape\": 128,\n+        \"input_dim\": 128,\n+        \"output_dim\": 256,\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[64, 32],\n+        input_shape=[\n+            256,\n+        ],\n         jit_compile=jit_compile,\n     )\n \n-    data = np.random.randint(30, size=(num_samples, 32))\n+    data = [np.random.randint(30, size=(num_samples, 256))]\n     benchmark.benchmark_predict(\n         num_samples=num_samples,\n         batch_size=batch_size,\n@@ -108,6 +110,7 @@ def benchmark_embedding(\n BENCHMARK_NAMES = {\n     \"benchmark_dense\": benchmark_dense,\n     \"benchmark_einsum_dense\": benchmark_einsum_dense,\n+    \"benchmark_embedding\": benchmark_embedding,\n }\n \n \n@@ -118,7 +121,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.merge_benchmark \\\n     --benchmark_name=benchmark_add \\\n-    --num_samples=8192 \\\n-    --batch_size=1024 \\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -40,6 +40,11 @@ def benchmark_add(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n def benchmark_average(\n     num_samples,\n@@ -61,6 +66,11 @@ def benchmark_average(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n def benchmark_concatenate(\n     num_samples,\n@@ -82,6 +92,11 @@ def benchmark_concatenate(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n def benchmark_dot(\n     num_samples,\n@@ -89,7 +104,7 @@ def benchmark_dot(\n     jit_compile=True,\n ):\n     layer_name = \"Dot\"\n-    init_args = {}\n+    init_args = {\"axes\": [2, 1]}\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n@@ -103,6 +118,11 @@ def benchmark_dot(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n def benchmark_maximum(\n     num_samples,\n@@ -124,6 +144,11 @@ def benchmark_maximum(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n def benchmark_minimum(\n     num_samples,\n@@ -145,6 +170,11 @@ def benchmark_minimum(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n def benchmark_multiply(\n     num_samples,\n@@ -156,7 +186,7 @@ def benchmark_multiply(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[[256, 256], [256, 32]],\n+        input_shape=[[256, 64], [256, 64]],\n         flat_call_inputs=False,\n         jit_compile=jit_compile,\n     )\n@@ -166,6 +196,11 @@ def benchmark_multiply(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n def benchmark_subtract(\n     num_samples,\n@@ -187,6 +222,11 @@ def benchmark_subtract(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n BENCHMARK_NAMES = {\n     \"benchmark_add\": benchmark_add,\n@@ -207,7 +247,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -6,8 +6,8 @@ flag to your custom value:\n ```\n python3 -m benchmarks.layer_benchmark.normalization_benchmark \\\n     --benchmark_name=benchmark_batch_normalization \\\n-    --num_samples=2400 \\\n-    --batch_size=300 \\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n     --jit_compile=True\n ```\n \"\"\"\n@@ -82,7 +82,7 @@ def benchmark_layer_normalization(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 256, 4],\n+        input_shape=[256, 128, 4],\n         jit_compile=jit_compile,\n     )\n \n@@ -107,7 +107,7 @@ def benchmark_unit_normalization(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 256, 4],\n+        input_shape=[256, 128, 4],\n         jit_compile=jit_compile,\n     )\n \n@@ -116,6 +116,11 @@ def benchmark_unit_normalization(\n         batch_size=batch_size,\n     )\n \n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n \n BENCHMARK_NAMES = {\n     \"benchmark_batch_normalization\": benchmark_batch_normalization,\n@@ -132,7 +137,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -33,7 +33,7 @@ def benchmark_average_pooling1d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 3],\n+        input_shape=[1024, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -87,7 +87,7 @@ def benchmark_average_pooling3d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[64, 64, 64, 3],\n+        input_shape=[64, 64, 32, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -114,7 +114,7 @@ def benchmark_max_pooling1d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 3],\n+        input_shape=[1024, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -168,7 +168,7 @@ def benchmark_max_pooling3d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[64, 64, 64, 3],\n+        input_shape=[64, 64, 32, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -193,7 +193,7 @@ def benchmark_global_average_pooling1d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 3],\n+        input_shape=[1024, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -243,7 +243,7 @@ def benchmark_global_average_pooling3d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[64, 64, 64, 3],\n+        input_shape=[64, 64, 32, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -268,7 +268,7 @@ def benchmark_global_max_pooling1d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 3],\n+        input_shape=[1024, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -318,7 +318,7 @@ def benchmark_global_max_pooling3d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[64, 64, 64, 3],\n+        input_shape=[64, 64, 32, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -356,7 +356,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -52,7 +52,7 @@ def benchmark_gaussian_dropout(\n     batch_size,\n     jit_compile=True,\n ):\n-    layer_name = \"GaussionDropout\"\n+    layer_name = \"GaussianDropout\"\n     init_args = {\n         \"rate\": 0.5,\n     }\n@@ -79,7 +79,7 @@ def benchmark_gaussian_noise(\n     batch_size,\n     jit_compile=True,\n ):\n-    layer_name = \"GaussionNoise\"\n+    layer_name = \"GaussianNoise\"\n     init_args = {\n         \"stddev\": 0.5,\n     }\n@@ -199,7 +199,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -30,7 +30,7 @@ def benchmark_cropping1d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 3],\n+        input_shape=[1024, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -127,7 +127,7 @@ def benchmark_permute(\n ):\n     layer_name = \"Permute\"\n     init_args = {\n-        \"dim\": (2, 1),\n+        \"dims\": (2, 1),\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n@@ -182,7 +182,7 @@ def benchmark_up_sampling2d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[256, 256, 3],\n+        input_shape=[128, 128, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -207,7 +207,7 @@ def benchmark_up_sampling3d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 32, 32, 3],\n+        input_shape=[32, 16, 16, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -319,7 +319,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -12,6 +12,7 @@ python3 -m benchmarks.layer_benchmark.rnn_benchmark \\\n ```\n \"\"\"\n \n+import tensorflow as tf\n from absl import app\n from absl import flags\n \n@@ -62,7 +63,7 @@ def benchmark_conv_lstm2d(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 64, 64, 3],\n+        input_shape=[32, 32, 32, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -84,13 +85,13 @@ def benchmark_conv_lstm3d(\n ):\n     layer_name = \"ConvLSTM3D\"\n     init_args = {\n-        \"filters\": 16,\n+        \"filters\": 8,\n         \"kernel_size\": 2,\n     }\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[16, 32, 32, 16, 3],\n+        input_shape=[8, 16, 16, 16, 3],\n         jit_compile=jit_compile,\n     )\n \n@@ -117,7 +118,7 @@ def benchmark_gru(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 256],\n+        input_shape=[256, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -144,7 +145,7 @@ def benchmark_lstm(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 256],\n+        input_shape=[256, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -171,7 +172,7 @@ def benchmark_simple_rnn(\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 256],\n+        input_shape=[256, 256],\n         jit_compile=jit_compile,\n     )\n \n@@ -192,14 +193,18 @@ def benchmark_bidirectional(\n     jit_compile=True,\n ):\n     layer_name = \"Bidirectional\"\n-    init_args = {\n-        \"layer\": keras_core.layers.LSTM(32),\n-    }\n+    init_args = {}\n+    keras_core_layer = keras_core.layers.Bidirectional(\n+        keras_core.layers.LSTM(32)\n+    )\n+    tf_keras_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[32, 256],\n+        input_shape=[256, 256],\n         jit_compile=jit_compile,\n+        keras_core_layer=keras_core_layer,\n+        tf_keras_layer=tf_keras_layer,\n     )\n \n     benchmark.benchmark_predict(\n@@ -219,14 +224,20 @@ def benchmark_time_distributed(\n     jit_compile=True,\n ):\n     layer_name = \"TimeDistributed\"\n-    init_args = {\n-        \"layer\": keras_core.layers.Conv2D(64, (3, 3)),\n-    }\n+    init_args = {}\n+    keras_core_layer = keras_core.layers.TimeDistributed(\n+        keras_core.layers.Conv2D(16, (3, 3))\n+    )\n+    tf_keras_layer = tf.keras.layers.TimeDistributed(\n+        tf.keras.layers.Conv2D(16, (3, 3))\n+    )\n     benchmark = LayerBenchmark(\n         layer_name,\n         init_args,\n-        input_shape=[10, 128, 128, 3],\n+        input_shape=[10, 32, 32, 3],\n         jit_compile=jit_compile,\n+        keras_core_layer=keras_core_layer,\n+        tf_keras_layer=tf_keras_layer,\n     )\n \n     benchmark.benchmark_predict(\n@@ -259,7 +270,7 @@ def main(_):\n     jit_compile = FLAGS.jit_compile\n \n     if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n+        for name, benchmark_fn in BENCHMARK_NAMES.items():\n             benchmark_fn(num_samples, batch_size, jit_compile)\n         return\n \n\n@@ -56,8 +56,8 @@ class ZeroPadding1D(Layer):\n \n     def compute_output_shape(self, input_shape):\n         output_shape = list(input_shape)\n-        if input_shape[1] is not None:\n-            input_shape[1] += self.padding[0] + self.padding[1]\n+        if output_shape[1] is not None:\n+            output_shape[1] += self.padding[0] + self.padding[1]\n         return tuple(output_shape)\n \n     def call(self, inputs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
