{"custom_id": "scrapy#af842a3e6ddc77e68d67bd332a20d111802a1c2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 478 | Contributors (this commit): 2 | Commits (past 90d): 23 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -82,10 +82,13 @@ class UrlUtilsTest(unittest.TestCase):\n                          'product.html?id=200&foo=bar')\n \n     def test_canonicalize_url(self):\n-        # no query arguments\n+        # simplest case\n         self.assertEqual(canonicalize_url(\"http://www.example.com\"),\n                                           \"http://www.example.com\")\n \n+        # always return a str\n+        assert isinstance(canonicalize_url(u\"http://www.example.com\"), str)\n+\n         # typical usage\n         self.assertEqual(canonicalize_url(\"http://www.example.com/do?a=1&b=2&c=3\"),\n                                           \"http://www.example.com/do?a=1&b=2&c=3\")\n@@ -148,6 +151,14 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertEqual(canonicalize_url(u\"http://user:pass@www.example.com/do?a=1#frag\", keep_fragments=True),\n                                           u\"http://user:pass@www.example.com/do?a=1#frag\")\n \n+        # urllib.quote uses a mapping cache of encoded characters. when parsing\n+        # an already percent-encoded url, it will fail if that url was not\n+        # percent-encoded as utf-8, that's why canonicalize_url must always\n+        # convert the urls to string. the following test asserts that\n+        # functionality.\n+        self.assertEqual(canonicalize_url(u'http://www.example.com/caf%E9-con-leche.htm'),\n+                                           'http://www.example.com/caf%E9-con-leche.htm')\n+\n     def test_check_valid_urlencode(self):\n         self.assertFalse(check_valid_urlencode(r'http://www.example.com/pictures\\detail\\CAN43664.jpg'))\n         self.assertTrue(check_valid_urlencode('http://www.example.com/pictures%5Cdetail%5CCAN43664.jpg'))\n\n@@ -125,7 +125,7 @@ def add_or_replace_parameter(url, name, new_value, sep='&'):\n     return next_url\n \n def canonicalize_url(url, keep_blank_values=False, keep_fragments=False):\n-    \"\"\"Canonicalize url by applying the following procedures:\n+    \"\"\"Canonicalize the given url by applying the following procedures:\n \n     - sort query arguments, first by key, then by value\n     - percent encode paths and query arguments. non-ASCII characters are\n@@ -135,9 +135,13 @@ def canonicalize_url(url, keep_blank_values=False, keep_fragments=False):\n     - remove query arguments with blank values (unless keep_blank_values is True)\n     - remove fragments (unless keep_fragments is True)\n \n+    The url passed can be a str or unicode, while the url returned is always a\n+    str.\n+\n     For examples see the tests in scrapy.tests.test_utils_url\n     \"\"\"\n \n+    url = url.encode('utf-8')\n     parts = list(urlparse.urlparse(url))\n     keyvals = cgi.parse_qsl(parts[4], keep_blank_values)\n     keyvals.sort()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b714b5ab0978b39e32550bdf21ab235989223609", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 279 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -160,5 +160,6 @@ def check_valid_urlencode(url):\n     def check_param(p):\n         return all(check_str(s) for s in p.split('=', 1))\n \n+    url = url.encode('utf-8')\n     split_result = urlparse.urlsplit(url)\n     return check_str(split_result.path) and all(check_param(p) for p in split_result.query.split('&'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2f5cff0b2491d40b3b4cd5bcd4f62ddeac4d565d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 10 | Churn Cumulative: 56 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,12 +1,12 @@\n from django.conf import settings\n from django.conf.urls.defaults import *\n \n-from scrapyorg.docs.views import index, document\n+from scrapyorg.docs.views import document\n \n \n urlpatterns = patterns('',\n-    (r'^$', index),\n-    (r'^(?P<url>[\\w./-]*)/$', document),\n+    url(r'^$', document, {'url': ''}),\n+    url(r'^(?P<url>[\\w./-]*)/$', document),\n )\n \n if settings.DEBUG: # devel\n\n@@ -7,10 +7,6 @@ from django.shortcuts import render_to_response\n from django.template import RequestContext\n \n \n-def index(request):\n-    return document(request, '')\n-\n-\n def document(request, url):\n     docroot = settings.DOC_PICKLE_ROOT\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#142cd72faee721774f5983f388b9766d9159fe7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 28 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 29 | Churn Cumulative: 508 | Contributors (this commit): 2 | Commits (past 90d): 26 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,5 @@\n import unittest\n-from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, url_query_parameter, add_or_replace_parameter, url_query_cleaner, canonicalize_url, check_valid_urlencode\n+from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, url_query_parameter, add_or_replace_parameter, url_query_cleaner, canonicalize_url\n \n class UrlUtilsTest(unittest.TestCase):\n \n@@ -159,20 +159,6 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertEqual(canonicalize_url(u'http://www.example.com/caf%E9-con-leche.htm'),\n                                            'http://www.example.com/caf%E9-con-leche.htm')\n \n-    def test_check_valid_urlencode(self):\n-        self.assertFalse(check_valid_urlencode(r'http://www.example.com/pictures\\detail\\CAN43664.jpg'))\n-        self.assertTrue(check_valid_urlencode('http://www.example.com/pictures%5Cdetail%5CCAN43664.jpg'))\n-\n-        self.assertFalse(check_valid_urlencode('http://www.example.com/pictures detail CAN43664.jpg'))\n-        self.assertTrue(check_valid_urlencode('http://www.example.com/pictures+detail%20CAN43664.jpg'))\n-\n-        self.assertFalse(check_valid_urlencode('http://www.example.com/?q=foo bar&q2=foo2 bar2'))\n-        self.assertTrue(check_valid_urlencode('http://www.example.com/?q=foo+bar&q2=foo2%20bar2'))\n-\n-        self.assertFalse(check_valid_urlencode('http://www.example.com/.,:;!@$%^*()_-[]{}|'))\n-        self.assertTrue(check_valid_urlencode('http://www.example.com/.,:;!@%24%25%5E*()_-%5B%5D%7B%7D%7C'))\n-        self.assertTrue(check_valid_urlencode('http://www.example.com/.%2C%3A%3B%21%40%24%25%5E%2A%28%29_-%5B%5D%7B%7D%7C'))\n-\n if __name__ == \"__main__\":\n     unittest.main()\n \n\n@@ -150,16 +150,3 @@ def canonicalize_url(url, keep_blank_values=False, keep_fragments=False):\n     if not keep_fragments:\n         parts[5] = ''\n     return urlparse.urlunparse(parts)\n-\n-def check_valid_urlencode(url):\n-    \"\"\" check that the url-path and arguments are properly quoted \"\"\"\n-    def check_str(s):\n-        for ignore_char in ',:;!@*()':\n-            s = s.replace(ignore_char, '')\n-        return all(s_plus == urllib.quote(urllib.unquote(s_plus)) for s_plus in s.split('+'))\n-    def check_param(p):\n-        return all(check_str(s) for s in p.split('=', 1))\n-\n-    url = url.encode('utf-8')\n-    split_result = urlparse.urlsplit(url)\n-    return check_str(split_result.path) and all(check_param(p) for p in split_result.query.split('&'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6d5a130b96411f97a21e0810866f1e8f675510b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 51 | Files Changed: 1 | Hunks: 16 | Methods Changed: 14 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 90 | Churn Cumulative: 237 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import copy\n+\n from scrapy.http import Request\n from scrapy.spider import BaseSpider\n from scrapy.item import ScrapedItem\n@@ -19,18 +21,18 @@ class Rule(object):\n        If True, links will be followed from the pages crawled by this rule.\n        It defaults to True when no callback is specified or False when a\n        callback is specified\n-    link_filter (optional)\n+    process_links (optional)\n        Can be either a callable, or a string with the name of a method defined\n        in the spider's class.\n        This method will be called with the list of extracted links matching\n        this rule (if any) and must return another list of links.\n     \"\"\"\n \n-    def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, link_filter=None):\n+    def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None):\n         self.link_extractor = link_extractor\n         self.callback = callback\n         self.cb_kwargs = cb_kwargs or {}\n-        self.link_filter = link_filter\n+        self.process_links = process_links\n         if follow is None:\n             self.follow = False if callback else True\n         else:\n@@ -50,38 +52,29 @@ class CrawlSpider(BaseSpider):\n     rules = ()\n \n     def __init__(self):\n-        def _get_method(method):\n-            if isinstance(method, basestring):\n-                return getattr(self, method, None)\n-            elif method and callable(method):\n-                return method\n-\n+        \"\"\"Constructor takes care of compiling rules\"\"\"\n         super(CrawlSpider, self).__init__()\n-        for rule in self.rules:\n-            rule.callback = _get_method(rule.callback)\n-            rule.link_filter = _get_method(rule.link_filter)\n+        self._compile_rules()\n \n     def parse(self, response):\n-        \"\"\"This function is called by the core for all the start_urls. Do not\n-        override this function, override parse_start_url instead.\"\"\"\n-        if response.url in self.start_urls:\n-            return self._parse_wrapper(response, self.parse_start_url, cb_kwargs={}, follow=True)\n-        else:\n-            return self.parse_url(response)\n+        \"\"\"This function is called by the framework core for all the\n+        start_urls. Do not override this function, override parse_start_url\n+        instead.\"\"\"\n+        return self._response_downloaded(response, self.parse_start_url, cb_kwargs={}, follow=True)\n \n     def parse_start_url(self, response):\n-        \"\"\"Callback function for processing start_urls. It must return a list\n-        of ScrapedItems and/or Requests.\"\"\"\n+        \"\"\"Overrideable callback function for processing start_urls. It must\n+        return a list of ScrapedItems and/or Requests\"\"\"\n         return []\n \n-    def scraped_item(self, response, item):\n-        \"\"\"\n-        This method is called for each item returned by the spider, and it's intended\n-        to do anything that it's needed before returning the item to the core, specially\n-        setting its GUID.\n-        It receives and returns an item\n-        \"\"\"\n-        return item\n+    def process_results(self, results, response):\n+        \"\"\"This overridable method is called for each result (item or request)\n+        returned by the spider, and it's intended to perform any last time\n+        processing required before returning the results to the framework core,\n+        for example setting the item GUIDs. It receives a list of results and\n+        the response which originated that results. It must return a list\n+        of results (Items or Requests).\"\"\"\n+        return results\n \n     def _requests_to_follow(self, response):\n         \"\"\"\n@@ -91,18 +84,18 @@ class CrawlSpider(BaseSpider):\n         \"\"\"\n         requests = []\n         seen = set()\n-        for rule in self.rules:\n-            links = [link for link in rule.link_extractor.extract_urls(response) if link not in seen]\n-            if rule.link_filter:\n-                links = rule.link_filter(links)\n+        for rule in self._rules:\n+            links = [l for l in rule.link_extractor.extract_urls(response) if l not in seen]\n+            if rule.process_links:\n+                links = rule.process_links(links)\n             seen = seen.union(links)\n             for link in links:\n                 r = Request(url=link.url, link_text=link.text)\n-                r.append_callback(self._parse_wrapper, rule.callback, cb_kwargs=rule.cb_kwargs, follow=rule.follow)\n+                r.append_callback(self._response_downloaded, rule.callback, cb_kwargs=rule.cb_kwargs, follow=rule.follow)\n                 requests.append(r)\n         return requests\n \n-    def _parse_wrapper(self, response, callback, cb_kwargs, follow):\n+    def _response_downloaded(self, response, callback, cb_kwargs, follow):\n         \"\"\"\n         This is were any response arrives, and were it's decided whether\n         to extract links or not from it, and if it will be parsed or not.\n@@ -113,25 +106,20 @@ class CrawlSpider(BaseSpider):\n             res.extend(self._requests_to_follow(response))\n         if callback:\n             cb_res = callback(response, **cb_kwargs) or ()\n-            for entry in cb_res:\n-                if isinstance(entry, ScrapedItem):\n-                    entry = self.scraped_item(response, entry)\n+            cb_res = self.process_results(cb_res, response)\n             res.extend(cb_res)\n         return res\n \n-    def parse_url(self, response):\n-        \"\"\"\n-        This method is called whenever you run scrapy with the 'parse' command\n-        over an URL.\n-        \"\"\"\n-        ret = set()\n-        for rule in self.rules:\n-            links = [link for link in rule.link_extractor.extract_urls(response) if link not in ret]\n-            if rule.link_filter:\n-                links = rule.link_filter(links)\n-            ret = ret.union(links)\n+    def _compile_rules(self):\n+        \"\"\"Compile the crawling rules\"\"\"\n \n-            if rule.callback and rule.link_extractor.match(response.url):\n-                ret = ret.union(rule.callback(response))\n-        return list(ret)\n+        def get_method(method):\n+            if callable(method):\n+                return method\n+            elif isinstance(method, basestring):\n+                return getattr(self, method, None)\n \n+        self._rules = [copy.copy(r) for r in self.rules]\n+        for rule in self._rules:\n+            rule.callback = get_method(rule.callback)\n+            rule.process_links = get_method(rule.process_links)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#778b77f2bbceb5bb73e3b0096df94e419dfe4731", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 15/15 | Churn Δ: 52 | Churn Cumulative: 52 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.3142857142857143\n\nDIFF:\n@@ -0,0 +1,52 @@\n+from scrapy.command import ScrapyCommand\n+from scrapy.fetcher import fetch\n+from scrapy.http import Request\n+from scrapy.item import ScrapedItem\n+from scrapy.spider import spiders\n+from scrapy.utils import display\n+from scrapy import log\n+\n+class Command(ScrapyCommand):\n+    def syntax(self):\n+        return \"[options] <url> <method>\"\n+\n+    def short_desc(self):\n+        return \"Parse the URL with the given spider method and print the results\"\n+\n+    def add_options(self, parser):\n+        ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\", help=\"show extracted links\")\n+        parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", help=\"don't show scraped items\")\n+        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", help=\"avoid using pygments to colorize the output\")\n+\n+    def pipeline_process(self, item, opts):\n+        return item\n+\n+    def run(self, args, opts):\n+        if len(args) != 2:\n+            print \"A URL and method is required\"\n+            return\n+\n+        url, method = args\n+        responses = fetch([url])\n+        for response in responses:\n+            spider = spiders.fromurl(response.url)\n+            if spider:\n+                method = getattr(spider, method)\n+                result = method(response)\n+\n+                links = [i for i in result if isinstance(i, Request)]\n+                items = [self.pipeline_process(i, opts) for i in result if isinstance(i, ScrapedItem)]\n+                for item in items:\n+                    item.__dict__.pop('_adaptors_dict', None)\n+\n+                display.nocolour = opts.nocolour\n+                if not opts.noitems:\n+                    print \"# Scraped Items\", \"-\"*60\n+                    display.pprint(items)\n+\n+                if opts.nolinks:\n+                    print \"# Links\", \"-\"*68\n+                    display.pprint(links)\n+            else:\n+                log.msg('cannot find spider for url: %s' % response.url, level=log.ERROR)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7f05b923abdd127254961bdbb6a6287cd6460490", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 372 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -48,7 +48,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n         MediaPipeline.__init__(self)\n \n     def s3request(self, key, method, body=None, headers=None):\n-        url = 'http://%s.s3.amazonaws.com/%s' % (self.bucket_name, key)\n+        url = 'http://%s.s3.amazonaws.com/%s%s' % (self.bucket_name, self.prefix, key)\n         req = Request(url, method=method, body=body, headers=headers)\n         sign_request(req, self.access_key, self.secret_key)\n         return req\n@@ -71,14 +71,14 @@ class S3ImagesPipeline(BaseImagesPipeline):\n         downloaded to.\n         \"\"\"\n         image_guid = hashlib.sha1(url).hexdigest()\n-        return '%s/full/%s.jpg' % (self.prefix, image_guid)\n+        return 'full/%s.jpg' % (image_guid)\n \n     def s3_thumb_key(self, url, thumb_id):\n         \"\"\"Return the relative path on the target filesystem for an image to be\n         downloaded to.\n         \"\"\"\n         image_guid = hashlib.sha1(url).hexdigest()\n-        return '%s/thumbs/%s/%s.jpg' % (self.prefix, thumb_id, image_guid)\n+        return 'thumbs/%s/%s.jpg' % (thumb_id, image_guid)\n \n     def media_to_download(self, request, info):\n         \"\"\"Return if the image should be downloaded by checking if it's already in\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8b37282752a27104da23dfaa931046dc2b16d6e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 105 | Lines Deleted: 247 | Files Changed: 7 | Hunks: 13 | Methods Changed: 24 | Complexity Δ (Sum/Max): -34/18 | Churn Δ: 352 | Churn Cumulative: 956 | Contributors (this commit): 2 | Commits (past 90d): 31 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,7 +4,9 @@ Adaptors related with extraction of data\n \n import urlparse\n import re\n+from scrapy import log\n from scrapy.http import Response\n+from scrapy.utils.url import is_url\n from scrapy.utils.python import flatten\n from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n \n@@ -64,7 +66,7 @@ class ExtractImages(object):\n             self.base_url = base_url\n \n         if not self.base_url:\n-            raise AttributeError('You must specify either a response or a base_url to the ExtractImages adaptor.')\n+            log.msg('No base URL was found for ExtractImages adaptor, will only extract absolute URLs', log.WARNING)\n \n     def extract_from_xpath(self, selector):\n         ret = []\n@@ -80,10 +82,13 @@ class ExtractImages(object):\n                     ret.extend(selector.x('@href'))\n             elif selector.xmlNode.name == 'img':\n                 ret.extend(selector.x('@src'))\n+            elif selector.xmlNode.name == 'text':\n+                ret.extend(selector)\n             else:\n                 ret.extend(selector.x('.//@href'))\n                 ret.extend(selector.x('.//@src'))\n-        elif selector.xmlNode.type == 'attribute' and selector.xmlNode.name in ['href', 'src']:\n+                ret.extend(selector.x('.//text()'))\n+        else:\n             ret.append(selector)\n \n         return ret\n@@ -92,11 +97,15 @@ class ExtractImages(object):\n         if isinstance(locations, basestring):\n             locations = [locations]\n \n-        rel_links = []\n+        rel_urls = []\n         for location in flatten(locations):\n             if isinstance(location, (XPathSelector, XPathSelectorList)):\n-                rel_links.extend(self.extract_from_xpath(location))\n+                rel_urls.extend(self.extract_from_xpath(location))\n             else:\n-                rel_links.append(location)\n-        rel_links = extract(rel_links)\n-        return [urlparse.urljoin(self.base_url, link) for link in rel_links]\n+                rel_urls.append(location)\n+        rel_urls = extract(rel_urls)\n+\n+        if self.base_url:\n+            return [urlparse.urljoin(self.base_url, url) for url in rel_urls]\n+        else:\n+            return filter(rel_urls, is_url)\n\n@@ -1,159 +0,0 @@\n-\"\"\"\n-This module contains some basic spiders for scraping websites (CrawlSpider)\n-and XML feeds (XMLFeedSpider).\n-\"\"\"\n-\n-from scrapy.conf import settings\n-from scrapy.http import Request\n-from scrapy.spider import BaseSpider\n-from scrapy.item import ScrapedItem\n-from scrapy.xpath.selector import XmlXPathSelector\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.iterators import xmliter, csviter\n-\n-\n-def _set_guid(spider, item):\n-    \"\"\"\n-    This method is called whenever the spider returns items, for each item.\n-    It should set the 'guid' attribute to the given item with a string that\n-    identifies the item uniquely.\n-    \"\"\"\n-    raise NotConfigured('You must define a set_guid method in order to scrape items.')\n-\n-class CrawlSpider(BaseSpider):\n-    \"\"\"\n-    This class works as a base class for spiders that crawl over websites\n-    \"\"\"\n-    set_guid = _set_guid\n-\n-    def __init__(self):\n-        super(CrawlSpider, self).__init__()\n-        \n-        self._links_callback = []\n-        for attr in dir(self):\n-            if attr.startswith('links_'):\n-                suffix = attr.split('_', 1)[1]\n-                value = getattr(self, attr)\n-                callback = getattr(self, 'parse_%s' % suffix, None)\n-                self._links_callback.append((value, callback))\n-\n-    def parse(self, response):\n-        \"\"\"This function is called by the core for all the start_urls. Do not\n-        override this function, override parse_start_url instead.\"\"\"\n-        if response.url in self.start_urls:\n-            return self._parse_wrapper(response, self.parse_start_url)\n-        else:\n-            return self.parse_url(response)\n-\n-    def parse_start_url(self, response):\n-        \"\"\"Callback function for processing start_urls. It must return a list\n-        of ScrapedItems and/or Requests.\"\"\"\n-        return []\n-\n-    def _links_to_follow(self, response):\n-        res = []\n-        links_to_follow = {}\n-        for lx, callback in self._links_callback:\n-            links = lx.extract_urls(response)\n-            links = self.post_extract_links(links) if hasattr(self, 'post_extract_links') else links\n-            for link in links:\n-                links_to_follow[link.url] = (callback, link.text)\n-\n-        for url, (callback, link_text) in links_to_follow.iteritems():\n-            request = Request(url=url, link_text=link_text)\n-            request.append_callback(self._parse_wrapper, callback)\n-            res.append(request)\n-        return res\n-\n-    def _parse_wrapper(self, response, callback):\n-        res = []\n-        if settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True):\n-            res.extend(self._links_to_follow(response))\n-        res.extend(callback(response) if callback else ())\n-        for entry in res:\n-            if isinstance(entry, ScrapedItem):\n-                self.set_guid(entry)\n-        return res\n-\n-    def parse_url(self, response):\n-        \"\"\"\n-        This method is called whenever you run scrapy with the 'parse' command\n-        over an URL.\n-        \"\"\"\n-        extractor_names = [attrname for attrname in dir(self) if attrname.startswith('links_')]\n-        ret = []\n-        for name in extractor_names:\n-            extractor = getattr(self, name)\n-            if settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True):\n-                ret.extend(self._links_to_follow(response))\n-            callback_name = 'parse_%s' % name[6:]\n-            if hasattr(self, callback_name):\n-                if extractor.match(response.url):\n-                    ret.extend(getattr(self, callback_name)(response))\n-        for entry in ret:\n-            if isinstance(entry, ScrapedItem):\n-                self.set_guid(entry)\n-        return ret\n-\n-class XMLFeedSpider(BaseSpider):\n-    \"\"\"\n-    This class intends to be the base class for spiders that scrape\n-    from XML feeds.\n-\n-    You can choose whether to parse the file using the iternodes tool,\n-    or not using it (which just splits the tags using xpath)\n-    \"\"\"\n-    set_guid = _set_guid\n-    iternodes = True\n-    itertag = 'item'\n-\n-    def parse_item_wrapper(self, response, xSel):\n-        ret = self.parse_item(response, xSel)\n-        if isinstance(ret, ScrapedItem):\n-            self.set_guid(ret)\n-        return ret\n-\n-    def parse(self, response):\n-        if not hasattr(self, 'parse_item'):\n-            raise NotConfigured('You must define parse_item method in order to scrape this XML feed')\n-\n-        if self.iternodes:\n-            nodes = xmliter(response, self.itertag)\n-        else:\n-            nodes = XmlXPathSelector(response).x('//%s' % self.itertag)\n-\n-        return (self.parse_item_wrapper(response, xSel) for xSel in nodes)\n-\n-class CSVFeedSpider(BaseSpider):\n-    \"\"\"\n-    Spider for parsing CSV feeds.\n-    It receives a CSV file in a response; iterates through each of its rows,\n-    and calls parse_row with a dict containing each field's data.\n-\n-    You can set some options regarding the CSV file, such as the delimiter\n-    and the file's headers.\n-    \"\"\"\n-    set_guid = _set_guid\n-    delimiter = None # When this is None, python's csv module's default delimiter is used\n-    headers = None\n-\n-    def adapt_feed(self, response):\n-        \"\"\"You can override this function in order to make any changes you want\n-        to into the feed before parsing it. This function may return either a\n-        response or a string.  \"\"\"\n-\n-        return response\n-\n-    def parse_row_wrapper(self, response, row):\n-        ret = self.parse_row(response, row)\n-        if isinstance(ret, ScrapedItem):\n-            self.set_guid(ret)\n-        return ret\n-\n-    def parse(self, response):\n-        if not hasattr(self, 'parse_row'):\n-            raise NotConfigured('You must define parse_row method in order to scrape this CSV feed')\n-\n-        feed = self.adapt_feed(response)\n-        return (self.parse_row_wrapper(feed, row) for row in csviter(response, self.delimiter, self.headers))\n-\n\n@@ -0,0 +1,2 @@\n+from scrapy.contrib.spiders.crawl import CrawlSpider, Rule\n+from scrapy.contrib.spiders.feed import XMLFeedSpider, CSVFeedSpider\n\n\n@@ -0,0 +1,87 @@\n+# -*- coding: utf8 -*-\n+from scrapy.spider import BaseSpider\n+from scrapy.item import ScrapedItem\n+from scrapy.http import Request\n+from scrapy.utils.iterators import xmliter, csviter\n+from scrapy.xpath.selector import XmlXPathSelector\n+from scrapy.core.exceptions import UsageError, NotConfigured\n+\n+class XMLFeedSpider(BaseSpider):\n+    \"\"\"\n+    This class intends to be the base class for spiders that scrape\n+    from XML feeds.\n+\n+    You can choose whether to parse the file using the iternodes tool,\n+    or not using it (which just splits the tags using xpath)\n+    \"\"\"\n+    iternodes = True\n+    itertag = 'item'\n+\n+    def process_results(self, results, response):\n+        \"\"\"This overridable method is called for each result (item or request)\n+        returned by the spider, and it's intended to perform any last time\n+        processing required before returning the results to the framework core,\n+        for example setting the item GUIDs. It receives a list of results and\n+        the response which originated that results. It must return a list\n+        of results (Items or Requests).\"\"\"\n+        return results\n+\n+    def parse_nodes(self, response, nodes):\n+        for xSel in nodes:\n+            ret = self.parse_item(response, xSel)\n+            if isinstance(ret, (ScrapedItem, Request)):\n+                ret = [ret]\n+            if not isinstance(ret, (list, tuple)):\n+                raise UsageError('You cannot return an \"%s\" object from a spider' % type(ret).__name__)\n+            for result_item in self.process_results(ret, response):\n+                yield result_item\n+\n+    def parse(self, response):\n+        if not hasattr(self, 'parse_item'):\n+            raise NotConfigured('You must define parse_item method in order to scrape this XML feed')\n+\n+        if self.iternodes:\n+            nodes = xmliter(response, self.itertag)\n+        else:\n+            nodes = XmlXPathSelector(response).x('//%s' % self.itertag)\n+\n+        return self.parse_nodes(response, nodes)\n+\n+class CSVFeedSpider(BaseSpider):\n+    \"\"\"\n+    Spider for parsing CSV feeds.\n+    It receives a CSV file in a response; iterates through each of its rows,\n+    and calls parse_row with a dict containing each field's data.\n+\n+    You can set some options regarding the CSV file, such as the delimiter\n+    and the file's headers.\n+    \"\"\"\n+    delimiter = None # When this is None, python's csv module's default delimiter is used\n+    headers = None\n+\n+    def process_results(self, results, response):\n+        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n+        return results\n+\n+    def adapt_response(self, response):\n+        \"\"\"You can override this function in order to make any changes you want\n+        to into the feed before parsing it. This function must return a response.\"\"\"\n+        return response\n+\n+    def parse_rows(self, response):\n+        for row in csviter(response, self.delimiter, self.headers):\n+            ret = self.parse_row(response, row)\n+            if isinstance(ret, (ScrapedItem, Request)):\n+                ret = [ret]\n+            if not isinstance(ret, (list, tuple)):\n+                raise UsageError('You cannot return an \"%s\" object from a spider' % type(ret).__name__)\n+            for result_item in self.process_results(ret, response):\n+                yield result_item\n+\n+    def parse(self, response):\n+        if not hasattr(self, 'parse_row'):\n+            raise NotConfigured('You must define parse_row method in order to scrape this CSV feed')\n+\n+        response = self.adapt_response(response)\n+        return self.parse_rows(response)\n+\n\n@@ -1,2 +0,0 @@\n-from scrapy.contrib.spiders2.crawl import CrawlSpider, Rule\n-from scrapy.contrib.spiders2.feed import XMLFeedSpider, CSVFeedSpider\n\n@@ -1,79 +0,0 @@\n-from scrapy.spider import BaseSpider\n-from scrapy.item import ScrapedItem\n-from scrapy.utils.iterators import xmliter, csviter\n-from scrapy.xpath.selector import XmlXPathSelector\n-from scrapy.core.exceptions import NotConfigured\n-\n-class XMLFeedSpider(BaseSpider):\n-    \"\"\"\n-    This class intends to be the base class for spiders that scrape\n-    from XML feeds.\n-\n-    You can choose whether to parse the file using the iternodes tool,\n-    or not using it (which just splits the tags using xpath)\n-    \"\"\"\n-    iternodes = True\n-    itertag = 'item'\n-\n-    def item_scraped(self, response, item):\n-        \"\"\"\n-        This method is called for each item returned by the spider, and it's intended\n-        to do anything that it's needed before returning the item to the core, specially\n-        setting its GUID.\n-        It receives and returns an item\n-        \"\"\"\n-        return item\n-\n-    def parse_item_wrapper(self, response, xSel):\n-        ret = self.parse_item(response, xSel)\n-        if isinstance(ret, ScrapedItem):\n-            self.scraped_item(response, ret)\n-        return ret\n-\n-    def parse(self, response):\n-        if not hasattr(self, 'parse_item'):\n-            raise NotConfigured('You must define parse_item method in order to scrape this XML feed')\n-\n-        if self.iternodes:\n-            nodes = xmliter(response, self.itertag)\n-        else:\n-            nodes = XmlXPathSelector(response).x('//%s' % self.itertag)\n-\n-        return (self.parse_item_wrapper(response, xSel) for xSel in nodes)\n-\n-class CSVFeedSpider(BaseSpider):\n-    \"\"\"\n-    Spider for parsing CSV feeds.\n-    It receives a CSV file in a response; iterates through each of its rows,\n-    and calls parse_row with a dict containing each field's data.\n-\n-    You can set some options regarding the CSV file, such as the delimiter\n-    and the file's headers.\n-    \"\"\"\n-    delimiter = None # When this is None, python's csv module's default delimiter is used\n-    headers = None\n-\n-    def scraped_item(self, response, item):\n-        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n-        return item\n-\n-    def adapt_feed(self, response):\n-        \"\"\"You can override this function in order to make any changes you want\n-        to into the feed before parsing it. This function may return either a\n-        response or a string.  \"\"\"\n-\n-        return response\n-\n-    def parse_row_wrapper(self, response, row):\n-        ret = self.parse_row(response, row)\n-        if isinstance(ret, ScrapedItem):\n-            self.scraped_item(response, ret)\n-        return ret\n-\n-    def parse(self, response):\n-        if not hasattr(self, 'parse_row'):\n-            raise NotConfigured('You must define parse_row method in order to scrape this CSV feed')\n-\n-        feed = self.adapt_feed(response)\n-        return (self.parse_row_wrapper(feed, row) for row in csviter(response, self.delimiter, self.headers))\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0cff67b25e775b6e3344732384a52a46bdfa3ead", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 92 | Lines Deleted: 80 | Files Changed: 3 | Hunks: 12 | Methods Changed: 16 | Complexity Δ (Sum/Max): 2/22 | Churn Δ: 172 | Churn Cumulative: 306 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,6 @@\n-from scrapy.command import ScrapyCommand\n+from scrapy.command.commands.parse_method import Command as ScrapyCommand\n from scrapy.fetcher import fetch\n-from scrapy.http import Request\n-from scrapy.item import ScrapedItem\n from scrapy.spider import spiders\n-from scrapy.utils import display\n from scrapy import log\n \n class Command(ScrapyCommand):\n@@ -11,44 +8,38 @@ class Command(ScrapyCommand):\n         return \"[options] <url>\"\n \n     def short_desc(self):\n-        return \"Parse the URL and print their results\"\n+        return \"Parse the URL and print its results\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--links\", dest=\"links\", action=\"store_true\", help=\"show extracted links\")\n-        parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", help=\"don't show scraped items\")\n         parser.add_option(\"--identify\", dest=\"identify\", action=\"store_true\", help=\"try to use identify instead of parse\")\n-        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", help=\"avoid using pygments to colorize the output\")\n-\n-    def pipeline_process(self, item, opts):\n-        return item\n \n     def run(self, args, opts):\n         if not args:\n             print \"A URL is required\"\n             return\n \n+        items = set()\n+        links = set()\n         responses = fetch(args)\n         for response in responses:\n             spider = spiders.fromurl(response.url)\n             if spider:\n                 if opts.identify and hasattr(spider, 'identify_products'):\n-                    result = spider.identify_products(response)\n+                    ret_items, ret_links = ScrapyCommand.run_method(self, response, 'identify_products', args, opts)\n                 else:\n-                    result = spider.parse(response)\n-\n-                links = [i for i in result if isinstance(i, Request)]\n-                items = [self.pipeline_process(i, opts) for i in result if isinstance(i, ScrapedItem)]\n-                for item in items:\n-                    item.__dict__.pop('_adaptors_dict', None)\n-\n-                display.nocolour = opts.nocolour\n-                if not opts.noitems:\n-                    print \"# Scraped Items\", \"-\"*60\n-                    display.pprint(items)\n-\n-                if opts.links:\n-                    print \"# Links\", \"-\"*68\n-                    display.pprint(links)\n+                    if hasattr(spider, 'rules'):\n+                        for rule in spider.rules:\n+                            if rule.link_extractor.match(response.url):\n+                                ret_items, ret_links = ScrapyCommand.run_method(self, response, rule.callback, args, opts)\n+                                break\n                     else:\n-                log.msg('cannot find spider for url: %s' % response.url, level=log.ERROR)\n+                        ret_items, ret_links = ScrapyCommand.run_method(self, response, 'parse', args, opts)\n+                    items = items.union(ret_items)\n+                    links = links.union(ret_links)\n+            else:\n+                log.msg('Couldnt find method %s in spider %s' % (method, spider.__name__))\n+                continue\n+\n+        self.print_results(items, links, opts)\n+\n\n@@ -1,52 +0,0 @@\n-from scrapy.command import ScrapyCommand\n-from scrapy.fetcher import fetch\n-from scrapy.http import Request\n-from scrapy.item import ScrapedItem\n-from scrapy.spider import spiders\n-from scrapy.utils import display\n-from scrapy import log\n-\n-class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"[options] <url> <method>\"\n-\n-    def short_desc(self):\n-        return \"Parse the URL with the given spider method and print the results\"\n-\n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\", help=\"show extracted links\")\n-        parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", help=\"don't show scraped items\")\n-        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", help=\"avoid using pygments to colorize the output\")\n-\n-    def pipeline_process(self, item, opts):\n-        return item\n-\n-    def run(self, args, opts):\n-        if len(args) != 2:\n-            print \"A URL and method is required\"\n-            return\n-\n-        url, method = args\n-        responses = fetch([url])\n-        for response in responses:\n-            spider = spiders.fromurl(response.url)\n-            if spider:\n-                method = getattr(spider, method)\n-                result = method(response)\n-\n-                links = [i for i in result if isinstance(i, Request)]\n-                items = [self.pipeline_process(i, opts) for i in result if isinstance(i, ScrapedItem)]\n-                for item in items:\n-                    item.__dict__.pop('_adaptors_dict', None)\n-\n-                display.nocolour = opts.nocolour\n-                if not opts.noitems:\n-                    print \"# Scraped Items\", \"-\"*60\n-                    display.pprint(items)\n-\n-                if opts.nolinks:\n-                    print \"# Links\", \"-\"*68\n-                    display.pprint(links)\n-            else:\n-                log.msg('cannot find spider for url: %s' % response.url, level=log.ERROR)\n\n@@ -0,0 +1,73 @@\n+from scrapy.command import ScrapyCommand\n+from scrapy.fetcher import fetch\n+from scrapy.http import Request\n+from scrapy.item import ScrapedItem\n+from scrapy.spider import spiders\n+from scrapy.utils import display\n+from scrapy import log\n+\n+class Command(ScrapyCommand):\n+    def syntax(self):\n+        return \"[options] <url> <method>\"\n+\n+    def short_desc(self):\n+        return \"Parse the URL with the given spider method and print the results\"\n+\n+    def add_options(self, parser):\n+        ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"--links\", dest=\"links\", action=\"store_true\", help=\"show extracted links\")\n+        parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", help=\"don't show scraped items\")\n+        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", help=\"avoid using pygments to colorize the output\")\n+\n+    def pipeline_process(self, item, opts):\n+        return item\n+\n+    def run_method(self, response, method, args, opts):\n+        spider = spiders.fromurl(response.url)\n+        if not spider:\n+            log.msg('Couldnt find spider for url: %s' % response.url, level=log.ERROR)\n+            return (), ()\n+\n+        items = []\n+        links = []\n+        if method:\n+            method_fcn = method if callable(method) else getattr(spider, method, None)\n+            if not method_fcn:\n+                log.msg('Couldnt find method %s in spider %s' % (method, spider.__name__))\n+                return (), ()\n+\n+            result = method_fcn(response)\n+            links = [i for i in result if isinstance(i, Request)]\n+            items = [self.pipeline_process(i, opts) for i in result if isinstance(i, ScrapedItem)]\n+            for item in items:\n+                for key in item.__dict__.keys():\n+                    if key.startswith('_'):\n+                        item.__dict__.pop(key, None)\n+\n+        return items, links\n+\n+    def print_results(self, items, links, opts):\n+        display.nocolour = opts.nocolour\n+        if not opts.noitems:\n+            print \"# Scraped Items\", \"-\"*60\n+            display.pprint(list(items))\n+\n+        if opts.links:\n+            print \"# Links\", \"-\"*68\n+            display.pprint(list(links))\n+\n+    def run(self, args, opts):\n+        if len(args) < 2:\n+            print \"A URL and method is required\"\n+            return\n+\n+        items = set()\n+        links = set()\n+        url, method = args[:2]\n+        for response in fetch([url]):\n+            ret_items, ret_links = self.run_method(response, method, args, opts)\n+            items = items.union(ret_items)\n+            links = links.union(ret_links)\n+\n+        self.print_results(items, links, opts)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aa3ea811d190ae748fdd5566ee3cb97401cdf7eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 3 | Churn Cumulative: 132 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -38,8 +38,9 @@ class Command(ScrapyCommand):\n                     items = items.union(ret_items)\n                     links = links.union(ret_links)\n             else:\n-                log.msg('Couldnt find method %s in spider %s' % (method, spider.__name__))\n+                log.msg('Couldnt find spider for \"%s\"' % response.url)\n                 continue\n \n+        if items or links:\n             self.print_results(items, links, opts)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dd37bb5a4cac773176c0251316d9e075baa95d2b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 30 | Churn Cumulative: 75 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.23809523809523808\n\nDIFF:\n@@ -3,7 +3,8 @@ Monkey patches\n \n These are generally a bad idea.\n \"\"\"\n-from twisted.web.client import HTTPClientFactory\n+from twisted.web.client import HTTPClientFactory, HTTPPageGetter\n+import twisted\n \n \n # Extend limit for BeautifulSoup parsing loops\n@@ -13,6 +14,9 @@ from twisted.web.client import HTTPClientFactory\n def apply_patches():\n     patch_HTTPClientFactory_gotHeaders()\n \n+    if twisted.__version__ < '8.0.0':\n+        patch_HTTPPageGetter_handleResponse()\n+\n \n # XXX: Monkeypatch for twisted.web.client-HTTPClientFactory\n # HTTPClientFactory.gotHeaders dies when parsing malformed cookies,\n@@ -43,3 +47,27 @@ def _new_gotHeaders(self, headers):\n \n def patch_HTTPClientFactory_gotHeaders():\n     setattr(HTTPClientFactory, 'gotHeaders', _new_gotHeaders)\n+\n+def patch_HTTPPageGetter_handleResponse():\n+    def _handleResponse(self, response):\n+        if self.quietLoss:\n+            return\n+        if self.failed:\n+            self.factory.noPage(\n+                failure.Failure(\n+                    error.Error(\n+                        self.status, self.message, response)))\n+        if self.factory.method.upper() == 'HEAD':\n+            # Callback with empty string, since there is never a response\n+            # body for HEAD requests.\n+            self.factory.page('')\n+        elif self.length != None and self.length != 0:\n+            self.factory.noPage(failure.Failure(\n+                PartialDownloadError(self.status, self.message, response)))\n+        else:\n+            self.factory.page(response)\n+        # server might be stupid and not close connection. admittedly\n+        # the fact we do only one request per connection is also\n+        # stupid...\n+        self.transport.loseConnection()\n+    setattr(HTTPPageGetter, 'handleResponse', _handleResponse)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3b7f282bbd2264e9dcc362011aec4592459a664e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 81 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,8 +3,8 @@ Monkey patches\n \n These are generally a bad idea.\n \"\"\"\n-from twisted.web.client import HTTPClientFactory, HTTPPageGetter\n import twisted\n+from twisted.web.client import HTTPClientFactory\n \n \n # Extend limit for BeautifulSoup parsing loops\n@@ -49,6 +49,10 @@ def patch_HTTPClientFactory_gotHeaders():\n     setattr(HTTPClientFactory, 'gotHeaders', _new_gotHeaders)\n \n def patch_HTTPPageGetter_handleResponse():\n+    from twisted.web.client import PartialDownloadError, HTTPPageGetter\n+    from twisted.python import failure\n+    from twisted.web import error\n+\n     def _handleResponse(self, response):\n         if self.quietLoss:\n             return\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
