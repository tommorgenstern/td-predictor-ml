{"custom_id": "scrapy#fc4e0eaa2a9e85b78562a4c13b20611872d6f678", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 419 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -80,7 +80,7 @@ class Node:\n         def _run_callback(status):\n             if status['callresponse'][0] == 1:\n                 #slots are complete. Reschedule in master. This is a security issue because could happen that the slots were completed since last status update by another cluster (thinking at future with full-distributed worker-master clusters)\n-                self.master.schedule(pending['domain'], pending['settings'], pending['priority'])\n+                self.master.schedule([pending['domain']], pending['settings'], PRIORITY_NOW)\n                 log.msg(\"Domain %s rescheduled: no proc space in node.\" % pending['domain'], log.WARNING)\n             self._set_status(status)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#805eeec8447030dda32bccf87c1888451fde885c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 420 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -39,7 +39,6 @@ class Node:\n         self.master = master\n \n     def _set_status(self, status):\n-        log.msg(\"Response from worker: %s\" % status)\n         self.status_as_dict = status\n         if not status:\n             self.available = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c97e7d0daf2e8a51ccf790053fac0fa97a70cf1f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 547 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -344,7 +344,8 @@ class ExecutionEngine(object):\n             log.msg(\"Requested %s\" % request.traceinfo(), level=log.TRACE, domain=domain)\n             if isinstance(response, Response):\n                 response.request = request # tie request to obtained response\n-                log.msg(\"Crawled <%s> from <%s>\" % (response.url, response.parent), level=log.DEBUG, domain=domain)\n+                cached = 'cached' if response.cached else 'live'\n+                log.msg(\"Crawled %s <%s> from <%s>\" % (cached, response.url, response.parent), level=log.DEBUG, domain=domain)\n                 return response\n             elif isinstance(response, Request):\n                 redirected = response # proper alias\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#94993b769a5a2ad6c4414156d13a2d2e39060e3c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 76 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,4 @@\n-import os\n+import os, pickle\n \n SETTINGS_MODULE = os.environ.get('SCRAPYSETTINGS_MODULE', 'scrapy_settings')\n \n@@ -15,7 +15,8 @@ class Settings(object):\n     def __init__(self):\n         self.overrides = {}\n         self.settings = self._import(SETTINGS_MODULE)\n-        self.defaults = {}\n+        pickled_defaults = os.environ.get(\"SCRAPY_PICKLED_DEFAULT_SETTINGS\")\n+        self.defaults = pickle.loads(pickled_defaults) if pickled_defaults else {}\n         self.core = self._import('scrapy.conf.core_settings')\n \n     def _import(self, modulepath):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8ea2dc94d7bff46e77ed7ffa6e94cb8092cd1a84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 78 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -15,7 +15,7 @@ class Settings(object):\n     def __init__(self):\n         self.overrides = {}\n         self.settings = self._import(SETTINGS_MODULE)\n-        pickled_defaults = os.environ.get(\"SCRAPY_PICKLED_DEFAULT_SETTINGS\")\n+        pickled_defaults = os.environ.get(\"SCRAPY_PICKLED_SETTINGS\")\n         self.defaults = pickle.loads(pickled_defaults) if pickled_defaults else {}\n         self.core = self._import('scrapy.conf.core_settings')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0c35b7ed2ae3300ba88f29b194b80140ba584fd5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 9 | Churn Cumulative: 295 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,4 @@\n-import sys\n-import os\n-import time\n-import datetime\n+import sys, os, time, datetime, pickle\n \n from twisted.internet import protocol, reactor\n from twisted.spread import pb\n@@ -23,8 +20,8 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         #We conserve original setting format for info purposes (avoid lots of unnecesary \"SCRAPY_\")\n         self.scrapy_settings = spider_settings or {}\n         self.scrapy_settings.update({'LOGFILE': self.logfile, 'CLUSTER_WORKER_ENABLED': '0', 'WEBCONSOLE_ENABLED': '0'})\n-        for k in self.scrapy_settings:\n-            self.env[\"SCRAPY_%s\" % k] = str(self.scrapy_settings[k])\n+        pickled_settings = pickle.dumps(self.scrapy_settings)\n+        self.env[\"SCRAPY_PICKLED_SETTINGS\"] = pickled_settings\n         self.env[\"PYTHONPATH\"] = \":\".join(sys.path)#this is need so this crawl process knows where to locate local_scrapy_settings.\n \n     def __str__(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#585f35fb3ab9720f2886c183c16d9adb2a4e280f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 28 | Churn Cumulative: 448 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -78,8 +78,8 @@ class Node:\n \n         def _run_callback(status):\n             if status['callresponse'][0] == 1:\n-                #slots are complete. Reschedule in master. This is a security issue because could happen that the slots were completed since last status update by another cluster (thinking at future with full-distributed worker-master clusters)\n-                self.master.schedule([pending['domain']], pending['settings'], PRIORITY_NOW)\n+                #slots are complete. Reschedule in master with priority reduced by one. This is a security issue because offen happens that the slots were completed and not yet notified because of the asynchronous response from worker.\n+                self.master.schedule([pending['domain']], pending['settings'], pending['priority'] - 1)\n                 log.msg(\"Domain %s rescheduled: no proc space in node.\" % pending['domain'], log.WARNING)\n             self._set_status(status)\n \n@@ -91,7 +91,7 @@ class Node:\n         else:\n             deferred.addCallbacks(callback=_run_callback, errback=lambda reason:log.msg(reason, log.ERROR))\n \n-class ClusterMaster(object):\n+class ClusterMaster(pb.Root):\n \n     def __init__(self):\n \n@@ -110,11 +110,26 @@ class ClusterMaster(object):\n             self.pending = []\n \n         self.nodes = {}\n+        \n+        self.global_settings = {}\n+        #load cluster global settings\n+        for sname in settings.getlist('GLOBAL_CLUSTER_SETTINGS'):\n+            self.global_settings[sname] = settings[sname]\n+        \n         dispatcher.connect(self._engine_started, signal=signals.engine_started)\n         dispatcher.connect(self._engine_stopped, signal=signals.engine_stopped)\n+        port = settings.getint('CLUSTER_MASTER_PORT')\n+        scrapyengine.listenTCP(port, pb.PBServerFactory(self))\n         \n     def load_nodes(self):\n \n+        \"\"\"Loads nodes from the CLUSTER_MASTER_NODES setting\"\"\"\n+\n+        for name, url in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():\n+            self.load_node(name, url)\n+            \n+    def load_node(self, name, url):\n+\n         def _make_callback(_factory, _name, _url):\n \n             def _errback(_reason):\n@@ -123,9 +138,6 @@ class ClusterMaster(object):\n             d = _factory.getRootObject()\n             d.addCallbacks(callback=lambda obj: self.add_node(obj, _name), errback=_errback)\n \n-        \"\"\"Loads nodes from the CLUSTER_MASTER_NODES setting\"\"\"\n-\n-        for name, url in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():\n         if name not in self.nodes:\n             server, port = url.split(\":\")\n             port = eval(port)\n@@ -139,6 +151,9 @@ class ClusterMaster(object):\n             else:\n                 _make_callback(factory, name, url)\n \n+    def remote_connect(self, name, url):\n+        self.load_node(name, url)\n+\n     def update_nodes(self):\n         for node in self.nodes.itervalues():\n             node.get_status()\n@@ -162,6 +177,7 @@ class ClusterMaster(object):\n                 break\n         for domain in domains:\n             final_spider_settings = self.get_spider_groupsettings(domain)\n+            final_spider_settings.update(self.global_settings)\n             final_spider_settings.update(spider_settings or {})\n             self.pending.insert(i, {'domain': domain, 'settings': final_spider_settings, 'priority': priority})\n         self.update_nodes()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ba0f87ed2c951e7b7c41170ccd95e45e1dbbeb40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 297 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -47,7 +47,7 @@ class ClusterWorker(pb.Root):\n             raise NotConfigured\n \n         self.maxproc = settings.getint('CLUSTER_WORKER_MAXPROC')\n-        self.logdir = settings['CLUSTER_WORKER_LOGDIR']\n+        self.logdir = settings['CLUSTER_LOGDIR']\n         self.running = {}\n         self.starttime = time.time()\n         port = settings.getint('CLUSTER_WORKER_PORT')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1eb583a357e70d0beec8458566d5a145e2892566", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 298 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -52,6 +52,7 @@ class ClusterWorker(pb.Root):\n         self.starttime = time.time()\n         port = settings.getint('CLUSTER_WORKER_PORT')\n         scrapyengine.listenTCP(port, pb.PBServerFactory(self))\n+        log.msg(\"PYTHON_PATH: %s\" % repr(sys.path))\n \n     def remote_stop(self, domain):\n         \"\"\"Stop running domain.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a4cfea54e8fc90bf54dc9dac5982e335700524fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 300 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -52,7 +52,7 @@ class ClusterWorker(pb.Root):\n         self.starttime = time.time()\n         port = settings.getint('CLUSTER_WORKER_PORT')\n         scrapyengine.listenTCP(port, pb.PBServerFactory(self))\n-        log.msg(\"PYTHON_PATH: %s\" % repr(sys.path))\n+        log.msg(\"PYTHONPATH: %s\" % repr(sys.path))\n \n     def remote_stop(self, domain):\n         \"\"\"Stop running domain.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#377e17b7dbaa1b0bb2fa9d5b8b5833873434dcf1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 82 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ class SpiderStats(object):\n     \n     def __init__(self):\n         if not settings['SCRAPING_DB']:\n-            raise NotConfigured(\"Requires SCRAPING_DB setting\")\n+            raise NotConfigured(\"SpiderStats: Requires SCRAPING_DB setting\")\n \n         from scrapy.store.db import DomainDataHistory\n         self.ddh = DomainDataHistory(settings['SCRAPING_DB'], 'domain_data_history')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9b3c3c14e50533c362bff32d9b065f5e96f4aa81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 43 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,9 +26,11 @@ class ExtensionManager(object):\n             try:\n                 cls = load_class(extension_path)\n                 self.enabled[cls.__name__] = cls()\n-            except NotConfigured:\n+            except NotConfigured, e:\n                 self.disabled[cls.__name__] = extension_path\n-                pass\n+                if e.args:\n+                    from scrapy.core import log\n+                    log.msg(e)\n         self.loaded = True\n \n     def reload(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#90e93a76352be9dafacfd437ff5272b680e8c79c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 45 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,6 +5,7 @@ ExtensionManager (extensions) to be used as singleton.\n \"\"\"\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_class\n+from scrapy.core import log\n from scrapy.conf import settings\n \n class ExtensionManager(object):\n@@ -29,7 +30,6 @@ class ExtensionManager(object):\n             except NotConfigured, e:\n                 self.disabled[cls.__name__] = extension_path\n                 if e.args:\n-                    from scrapy.core import log\n                     log.msg(e)\n         self.loaded = True\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#28b5bb3240f1bfa11f5fd89d9626e93e1433e8d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 21 | Churn Cumulative: 81 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import os\n import unittest\n \n from scrapy.utils.xml import xpathselector_iternodes\n@@ -26,5 +27,13 @@ class UtilsXmlTestCase(unittest.TestCase):\n         self.assertEqual(attrs, \n                          [(['001'], ['Name 1'], ['Type 1']), (['002'], ['Name 2'], ['Type 2'])])\n \n+    def test_iterator_utf16(self):\n+        samplefile = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"sample_data\", \"feeds\", \"feed-sample3.xml\")\n+        body = open(samplefile).read()\n+        response = Response(domain=\"example.com\", url=\"http://example.com\", headers={\"Content-type\": \"text/xml; encoding=UTF-16\"}, body=body)\n+        self.assertEqual([x.x(\"@id\").extract() for x in xpathselector_iternodes(response, 'product')],\n+                         [['34017532'], ['34017557'], ['34017563'], ['34018057'], ['34018313'], ['34018599']])\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()   \n\n@@ -16,15 +16,11 @@ def xpathselector_iternodes(obj, nodename):\n     assert isinstance(obj, (Response, basestring)), \"obj must be Response or basestring, not %s\" % type(obj).__name__\n \n     if isinstance(obj, Response):\n-        text = obj.body.to_string()\n-        enc = obj.body.get_real_encoding()\n-    else:\n-        text = obj\n-        enc = 'utf-8'\n-\n+        text = obj.body.to_unicode()\n+    elif isinstance(obj, str):\n+        text = obj.decode('utf-8')\n \n     r = re.compile(r\"<%s[\\s>].*?</%s>\" % (nodename, nodename), re.DOTALL)\n-\n     for match in r.finditer(text):\n-        nodetext = match.group().decode(enc)\n+        nodetext = match.group()\n         yield XmlXPathSelector(text=nodetext).x('/' + nodename)[0]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#49175d1990240dd71a3a215c9bc798233c4ff70a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 7 | Churn Cumulative: 88 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,6 +34,11 @@ class UtilsXmlTestCase(unittest.TestCase):\n         self.assertEqual([x.x(\"@id\").extract() for x in xpathselector_iternodes(response, 'product')],\n                          [['34017532'], ['34017557'], ['34017563'], ['34018057'], ['34018313'], ['34018599']])\n \n+    def test_iterator_text(self):\n+        body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n+        \n+        self.assertEqual([x.x(\"text()\").extract() for x in xpathselector_iternodes(body, 'product')],\n+                         [[u'one'], [u'two']])\n \n if __name__ == \"__main__\":\n     unittest.main()   \n\n@@ -19,6 +19,8 @@ def xpathselector_iternodes(obj, nodename):\n         text = obj.body.to_unicode()\n     elif isinstance(obj, str):\n         text = obj.decode('utf-8')\n+    else:\n+        text = obj\n \n     r = re.compile(r\"<%s[\\s>].*?</%s>\" % (nodename, nodename), re.DOTALL)\n     for match in r.finditer(text):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#11bc122a54dacec840acf790e25e9c3b710203a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 84 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -13,10 +13,10 @@ class Settings(object):\n     core = None\n \n     def __init__(self):\n-        self.overrides = {}\n+        pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS\")\n+        self.overrides = pickle.loads(pickled_settings) if pickled_settings else {}\n         self.settings = self._import(SETTINGS_MODULE)\n-        pickled_defaults = os.environ.get(\"SCRAPY_PICKLED_SETTINGS\")\n-        self.defaults = pickle.loads(pickled_defaults) if pickled_defaults else {}\n+        self.defaults = {}\n         self.core = self._import('scrapy.conf.core_settings')\n \n     def _import(self, modulepath):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2b6189a3f87e9c48879cb710e7f72d7a91eae19b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 388 | Contributors (this commit): 2 | Commits (past 90d): 17 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,7 @@ class Settings(object):\n     core = None\n \n     def __init__(self):\n-        pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS\")\n+        pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\")\n         self.overrides = pickle.loads(pickled_settings) if pickled_settings else {}\n         self.settings = self._import(SETTINGS_MODULE)\n         self.defaults = {}\n\n@@ -21,7 +21,7 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.scrapy_settings = spider_settings or {}\n         self.scrapy_settings.update({'LOGFILE': self.logfile, 'CLUSTER_WORKER_ENABLED': '0', 'WEBCONSOLE_ENABLED': '0'})\n         pickled_settings = pickle.dumps(self.scrapy_settings)\n-        self.env[\"SCRAPY_PICKLED_SETTINGS\"] = pickled_settings\n+        self.env[\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\"] = pickled_settings\n         self.env[\"PYTHONPATH\"] = \":\".join(sys.path)#this is need so this crawl process knows where to locate local_scrapy_settings.\n \n     def __str__(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e71aa06f978da0a7eb37df799871eb162f311c2d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 306 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -82,12 +82,13 @@ class ClusterWorker(pb.Root):\n     def remote_run(self, domain, spider_settings=None):\n         \"\"\"Spawn process to run the given domain.\"\"\"\n         if len(self.running) < self.maxproc:\n+            if not domain in self.running:\n+                self.running[domain] = scrapy_proc\n                 logfile = os.path.join(self.logdir, domain, time.strftime(\"%FT%T.log\"))\n                 if not os.path.exists(os.path.dirname(logfile)):\n                     os.makedirs(os.path.dirname(logfile))\n                 scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings)\n                 args = [sys.executable, sys.argv[0], 'crawl', domain]\n-            self.running[domain] = scrapy_proc\n                 try:\n                     import pysvn\n                     c=pysvn.Client()\n@@ -97,4 +98,5 @@ class ClusterWorker(pb.Root):\n                     pass\n                 proc = reactor.spawnProcess(scrapy_proc, sys.executable, args=args, env=scrapy_proc.env)\n                 return self.status(0, \"Started process %s.\" % scrapy_proc)\n+            return self.status(2, \"Domain %s already running.\" % domain )\n         return self.status(1, \"No free slot to run another process.\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#69d1c57692acdce760e937ba57d60ca8aecd946f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 308 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -83,12 +83,12 @@ class ClusterWorker(pb.Root):\n         \"\"\"Spawn process to run the given domain.\"\"\"\n         if len(self.running) < self.maxproc:\n             if not domain in self.running:\n-                self.running[domain] = scrapy_proc\n                 logfile = os.path.join(self.logdir, domain, time.strftime(\"%FT%T.log\"))\n                 if not os.path.exists(os.path.dirname(logfile)):\n                     os.makedirs(os.path.dirname(logfile))\n                 scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings)\n                 args = [sys.executable, sys.argv[0], 'crawl', domain]\n+                self.running[domain] = scrapy_proc\n                 try:\n                     import pysvn\n                     c = pysvn.Client()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ace6e3c430d8a07bc4412dfa55da5ca2c5648c79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 33 | Churn Cumulative: 829 | Contributors (this commit): 2 | Commits (past 90d): 20 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -51,10 +51,19 @@ class Node:\n             self.loadavg = status['loadavg']\n             self.logdir = status['logdir']\n             free_slots = self.maxproc - len(self.running)\n+\n+            to_reschedule = []\n             while free_slots > 0 and self.master.pending:\n                 pending = self.master.pending.pop(0)\n+                #if domain already running in some node, reschedule with same priority (so will be moved to run later)\n+                if pending['domain'] in self.master.running or pending['domain'] in self.master.loading:\n+                    to_reschedule.append(pending)\n+                else:\n                     self.run(pending)\n+                    self.master.loading.append(pending['domain'])\n                     free_slots -= 1\n+            for pending in to_reschedule:\n+                self.master.schedule([pending['domain']], pending['settings'], pending['priority'])\n \n     def get_status(self):\n         try:\n@@ -77,10 +86,17 @@ class Node:\n     def run(self, pending):\n \n         def _run_callback(status):\n+            self.master.loading.remove(pending['domain'])\n             if status['callresponse'][0] == 1:\n                 #slots are complete. Reschedule in master with priority reduced by one. This is a security issue because offen happens that the slots were completed and not yet notified because of the asynchronous response from worker.\n                 self.master.schedule([pending['domain']], pending['settings'], pending['priority'] - 1)\n                 log.msg(\"Domain %s rescheduled: no proc space in node.\" % pending['domain'], log.WARNING)\n+            elif status['callresponse'][0] == 2:\n+                #domain already running in node. Reschedule with same priority. Dont know if this could really happen,\n+                #because the first check in self.master.loading should avoid to reach this point.\n+                self.master.schedule([pending['domain']], pending['settings'], pending['priority'])\n+                log.msg(\"Domain %s rescheduled: already running in node.\" % pending['domain'], log.WARNING)\n+            if not self.master.loading:\n                 self._set_status(status)\n \n         try:\n@@ -108,7 +124,7 @@ class ClusterMaster(pb.Root):\n             self.pending = pickle.load( open(settings[\"CLUSTER_MASTER_CACHEFILE\"], \"r\") )\n         except IOError:\n             self.pending = []\n-\n+        self.loading = []\n         self.nodes = {}\n         \n         self.global_settings = {}\n@@ -151,9 +167,6 @@ class ClusterMaster(pb.Root):\n             else:\n                 _make_callback(factory, name, url)\n \n-    def remote_connect(self, name, url):\n-        self.load_node(name, url)\n-\n     def update_nodes(self):\n         for node in self.nodes.itervalues():\n             node.get_status()\n@@ -176,6 +189,11 @@ class ClusterMaster(pb.Root):\n             else:\n                 break\n         for domain in domains:\n+            pd = self.find_ifpending(domain)\n+            if pd: #domain already pending, so just change priority if new is higher\n+                if priority < pd['priority']:\n+                    pd['priority'] = priority\n+            else:\n                 final_spider_settings = self.get_spider_groupsettings(domain)\n                 final_spider_settings.update(self.global_settings)\n                 final_spider_settings.update(spider_settings or {})\n@@ -227,8 +245,14 @@ class ClusterMaster(pb.Root):\n     def available_nodes(self):\n         return (node for node in self.nodes.itervalues() if node.available)\n \n+    def find_ifpending(self, domain):\n+        for p in self.pending:\n+            if domain == p['domain']:\n+                return p\n+\n     def _engine_started(self):\n         self.load_nodes()\n         scrapyengine.addtask(self.update_nodes, settings.getint('CLUSTER_MASTER_POLL_INTERVAL'))\n     def _engine_stopped(self):\n         pickle.dump( self.pending, open(settings[\"CLUSTER_MASTER_CACHEFILE\"], \"w\") )\n+        log.msg(\"Pending saved in %s\" % settings[\"CLUSTER_MASTER_CACHEFILE\"])\n\n@@ -220,5 +220,6 @@ class ClusterMasterWeb(ClusterMaster):\n             nodes_status[d] = n.status_as_dict\n         status[\"nodes\"] = nodes_status\n         status[\"pending\"] = self.pending\n+        status[\"loading\"] = self.loading\n         content = serialize(status, format)\n         return content\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#338a485bc152c13057c63bcc91dc46cd803365e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 15 | Churn Cumulative: 495 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 2 | DMM Complexity: 0.75\n\nDIFF:\n@@ -53,17 +53,14 @@ class Node:\n             free_slots = self.maxproc - len(self.running)\n \n             to_reschedule = []\n-            while free_slots > 0 and self.master.pending:\n+            if free_slots > 0 and self.master.pending:\n                 pending = self.master.pending.pop(0)\n                 #if domain already running in some node, reschedule with same priority (so will be moved to run later)\n                 if pending['domain'] in self.master.running or pending['domain'] in self.master.loading:\n-                    to_reschedule.append(pending)\n+                    self.master.schedule([pending['domain']], pending['settings'], pending['priority'])\n                 else:\n                     self.run(pending)\n                     self.master.loading.append(pending['domain'])\n-                    free_slots -= 1\n-            for pending in to_reschedule:\n-                self.master.schedule([pending['domain']], pending['settings'], pending['priority'])\n \n     def get_status(self):\n         try:\n@@ -88,15 +85,15 @@ class Node:\n         def _run_callback(status):\n             self.master.loading.remove(pending['domain'])\n             if status['callresponse'][0] == 1:\n-                #slots are complete. Reschedule in master with priority reduced by one. This is a security issue because offen happens that the slots were completed and not yet notified because of the asynchronous response from worker.\n+                #slots are complete. Reschedule in master with priority reduced by one.\n+                #self.master.loading check should avoid this to happen\n                 self.master.schedule([pending['domain']], pending['settings'], pending['priority'] - 1)\n                 log.msg(\"Domain %s rescheduled: no proc space in node.\" % pending['domain'], log.WARNING)\n             elif status['callresponse'][0] == 2:\n-                #domain already running in node. Reschedule with same priority. Dont know if this could really happen,\n-                #because the first check in self.master.loading should avoid to reach this point.\n+                #domain already running in node. Reschedule with same priority.\n+                #self.master.loading check should avoid this to happen\n                 self.master.schedule([pending['domain']], pending['settings'], pending['priority'])\n                 log.msg(\"Domain %s rescheduled: already running in node.\" % pending['domain'], log.WARNING)\n-            if not self.master.loading:\n             self._set_status(status)\n \n         try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8eab24dc0dc44fb0f4af9d855895898a2423660", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 499 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 2 | DMM Complexity: 0.5\n\nDIFF:\n@@ -52,7 +52,8 @@ class Node:\n             self.logdir = status['logdir']\n             free_slots = self.maxproc - len(self.running)\n \n-            to_reschedule = []\n+            #load domains by one, so to mix up better the domain loading between nodes. The next one in the same node will be loaded\n+            #when there is no loading domain or in the next status update. Th\n             if free_slots > 0 and self.master.pending:\n                 pending = self.master.pending.pop(0)\n                 #if domain already running in some node, reschedule with same priority (so will be moved to run later)\n@@ -94,7 +95,6 @@ class Node:\n                 #self.master.loading check should avoid this to happen\n                 self.master.schedule([pending['domain']], pending['settings'], pending['priority'])\n                 log.msg(\"Domain %s rescheduled: already running in node.\" % pending['domain'], log.WARNING)\n-            self._set_status(status)\n \n         try:\n             deferred = self.__remote.callRemote(\"run\", pending[\"domain\"], pending[\"settings\"])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5403fd3d9f4189745f00769eef1bf49c999fc183", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 14 | Methods Changed: 10 | Complexity Δ (Sum/Max): 12/8 | Churn Δ: 60 | Churn Cumulative: 908 | Contributors (this commit): 2 | Commits (past 90d): 24 | Contributors (cumulative): 4 | DMM Complexity: 0.78125\n\nDIFF:\n@@ -1,14 +1,10 @@\n-import urlparse\n-import urllib\n-import bisect\n import sys\n-import pickle, socket\n+import pickle\n \n from pydispatch import dispatcher\n \n from twisted.spread import pb\n from twisted.internet import reactor\n-from twisted.python import util\n \n from scrapy.core import log, signals\n from scrapy.core.engine import scrapyengine\n@@ -37,13 +33,25 @@ class Node:\n         self._set_status(status)\n         self.name = name\n         self.master = master\n+        self.available = True\n+\n+    @property\n+    def status_as_dict(self):\n+        status = {\"alive\": self.alive}\n+        if self.alive:\n+            status[\"running\"] = self.running\n+            status[\"maxproc\"] = self.maxproc\n+            status[\"available\"] = self.available\n+            status[\"starttime\"] = self.starttime\n+            status[\"timestamp\"] = self.timestamp\n+            status[\"loadavg\"] = self.loadavg\n+        return status\n         \n     def _set_status(self, status):\n-        self.status_as_dict = status\n         if not status:\n-            self.available = False\n+            self.alive = False\n         else:\n-            self.available = True\n+            self.alive = True\n             self.running = status['running']\n             self.maxproc = status['maxproc']\n             self.starttime = status['starttime']\n@@ -53,8 +61,8 @@ class Node:\n             free_slots = self.maxproc - len(self.running)\n \n         #load domains by one, so to mix up better the domain loading between nodes. The next one in the same node will be loaded\n-            #when there is no loading domain or in the next status update. Th\n-            if free_slots > 0 and self.master.pending:\n+        #when there is no loading domain or in the next status update. This way also we load the nodes softly\n+        if self.alive and self.available and free_slots > 0 and self.master.pending:\n             pending = self.master.pending.pop(0)\n             #if domain already running in some node, reschedule with same priority (so will be moved to run later)\n             if pending['domain'] in self.master.running or pending['domain'] in self.master.loading:\n@@ -83,6 +91,12 @@ class Node:\n \n     def run(self, pending):\n \n+        def _run_errback(reason):\n+            log.msg(reason, log.ERROR)\n+            self.master.loading.remove(pending['domain'])\n+            self.master.schedule([pending['domain']], pending['settings'], pending['priority'] - 1)\n+            log.msg(\"Domain %s rescheduled: lost connection to node.\" % pending['domain'], log.WARNING)\n+            \n         def _run_callback(status):\n             self.master.loading.remove(pending['domain'])\n             if status['callresponse'][0] == 1:\n@@ -102,7 +116,7 @@ class Node:\n             self._set_status(None)\n             log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n         else:\n-            deferred.addCallbacks(callback=_run_callback, errback=lambda reason:log.msg(reason, log.ERROR))\n+            deferred.addCallbacks(callback=_run_callback, errback=_run_errback)\n \n class ClusterMaster(pb.Root):\n \n@@ -151,7 +165,6 @@ class ClusterMaster(pb.Root):\n             d = _factory.getRootObject()\n             d.addCallbacks(callback=lambda obj: self.add_node(obj, _name), errback=_errback)\n \n-        if name not in self.nodes:\n         server, port = url.split(\":\")\n         port = eval(port)\n         log.msg(\"Connecting to cluster worker %s...\" % name)\n@@ -165,8 +178,13 @@ class ClusterMaster(pb.Root):\n             _make_callback(factory, name, url)\n \n     def update_nodes(self):\n-        for node in self.nodes.itervalues():\n-            node.get_status()\n+        for name, url in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():\n+            if name in self.nodes and self.nodes[name].alive:\n+                log.msg(\"Updating node. name: %s, url: %s\" % (name, url) )\n+                self.nodes[name].get_status()\n+            else:\n+                log.msg(\"Reloading node. name: %s, url: %s\" % (name, url) )\n+                self.load_node(name, url)\n \n     def add_node(self, cworker, name):\n         \"\"\"Add node given its node\"\"\"\n@@ -175,6 +193,12 @@ class ClusterMaster(pb.Root):\n         self.nodes[name] = node\n         log.msg(\"Added cluster worker %s\" % name)\n \n+    def disable_node(self, name):\n+        self.nodes[name].available = False\n+        \n+    def enable_node(self, name):\n+        self.nodes[name].available = True\n+\n     def remove_node(self, nodename):\n         raise NotImplemented\n \n\n@@ -95,6 +95,14 @@ class ClusterMasterWeb(ClusterMaster):\n             self.remove(domains)\n             if ws:\n                 return self.ws_status(wc_request)\n+        if \"disable_node\" in args:\n+            self.disable_node(args[\"disable_node\"][0])\n+            if ws:\n+                return self.ws_status(wc_request)\n+        if \"enable_node\" in args:\n+            self.enable_node(args[\"enable_node\"][0])\n+            if ws:\n+                return self.ws_status(wc_request)\n             \n         if ws:\n             return self.ws_status(wc_request)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f515942f546f73befcac86f794d9a7746ae3d2d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 30 | Churn Cumulative: 1246 | Contributors (this commit): 2 | Commits (past 90d): 41 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -35,10 +35,17 @@ class Node:\n         self.master = master\n         self.available = True\n \n-    @property\n-    def status_as_dict(self):\n+    def status_as_dict(self, verbosity=0):\n         status = {\"alive\": self.alive}\n         if self.alive:\n+            if verbosity == 0:\n+                #dont show spider settings\n+                status[\"running\"] = []\n+                for proc in self.running:\n+                    proccopy = proc.copy()\n+                    del proccopy[\"settings\"]\n+                    status[\"running\"].append(proccopy)\n+            else:\n                 status[\"running\"] = self.running\n             status[\"maxproc\"] = self.maxproc\n             status[\"available\"] = self.available\n@@ -271,6 +278,18 @@ class ClusterMaster(pb.Root):\n             if domain == p['domain']:\n                 return p\n \n+    def print_pending(self, verbosity=0):\n+        if verbosity == 0:\n+            pending = []\n+            for p in self.pending:\n+                pp = p.copy()\n+                del pp[\"settings\"]\n+                pending.append(pp)\n+            return pending\n+        else:\n+            return self.pending\n+            \n+        \n     def _engine_started(self):\n         self.load_nodes()\n         scrapyengine.addtask(self.update_nodes, settings.getint('CLUSTER_MASTER_POLL_INTERVAL'))\n\n@@ -221,13 +221,14 @@ class ClusterMasterWeb(ClusterMaster):\n \n     def ws_status(self, wc_request):\n         format = wc_request.args['format'][0] if 'format' in wc_request.args else 'json'\n+        verbosity = wc_request.args['verbosity'][0] if 'verbosity' in wc_request.args else '0'\n         wc_request.setHeader('content-type', 'text/plain')\n         status = {}\n         nodes_status = {}\n         for d, n in self.nodes.iteritems():\n-            nodes_status[d] = n.status_as_dict\n+            nodes_status[d] = n.status_as_dict(int(verbosity))\n         status[\"nodes\"] = nodes_status\n-        status[\"pending\"] = self.pending\n+        status[\"pending\"] = self.print_pending(int(verbosity))\n         status[\"loading\"] = self.loading\n         content = serialize(status, format)\n         return content\n\\ No newline at end of file\n\n@@ -83,7 +83,7 @@ class ClusterWorker(pb.Root):\n         \"\"\"Spawn process to run the given domain.\"\"\"\n         if len(self.running) < self.maxproc:\n             if not domain in self.running:\n-                logfile = os.path.join(self.logdir, domain, time.strftime(\"%FT%T.log\"))\n+                logfile = os.path.join(self.logdir, time.strftime(\"%F\"), domain, time.strftime(\"%FT%T.log\"))\n                 if not os.path.exists(os.path.dirname(logfile)):\n                     os.makedirs(os.path.dirname(logfile))\n                 scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f028cc38a1548a34a7a3a6af12ab2c7316f6b3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 17/17 | Churn Δ: 74 | Churn Cumulative: 74 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1,74 @@\n+#!/usr/bin/env python\n+\"\"\"\n+Cluster control script\n+\"\"\"\n+\n+from optparse import OptionParser\n+import urllib\n+\n+def main():\n+    parser = OptionParser(usage=\"Usage: scrapy-cluster-ctl.py [domain [domain [...]]] [options]\" )\n+    parser.add_option(\"--disablenode\", dest=\"disable_node\", help=\"Disable given node (by name) so it will no accept more run requests.\")\n+    parser.add_option(\"--enablenode\", dest=\"enable_node\", help=\"Enable given node (by name) so it will accept again run requests\")\n+    parser.add_option(\"--format\", dest=\"format\", help=\"Output format. Default: pprint.\", default=\"pprint\")\n+    parser.add_option(\"--list\", metavar=\"FILE\", dest=\"list\", help=\"Specify a file from where to read domains, one per line.\")\n+    parser.add_option(\"--now\", action=\"store_true\", dest=\"now\", help=\"Schedule domains to run with priority now.\")\n+    parser.add_option(\"--output\", metavar=\"FILE\", dest=\"output\", help=\"Output file. If not given, output to stdout.\")\n+    parser.add_option(\"--port\", dest=\"port\", type=\"int\", help=\"Cluster master port. Default: 8080.\", default=8080)\n+    parser.add_option(\"--remove\", dest=\"remove\", action=\"store_true\", help=\"Remove from schedule domains given as args.\")\n+    parser.add_option(\"--schedule\", dest=\"schedule\", action=\"store_true\", help=\"Schedule domains given as args.\")\n+    parser.add_option(\"--server\", dest=\"server\", help=\"Cluster master server name. Default: localhost.\", default=\"localhost\")\n+    parser.add_option(\"--status\", dest=\"status\", action=\"store_true\", help=\"Print cluster master status and quit.\")\n+    parser.add_option(\"--stop\", dest=\"stop\", action=\"store_true\", help=\"Stops a running domain.\")\n+    parser.add_option(\"--verbosity\", dest=\"verbosity\", type=\"int\", help=\"Sets the report status verbosity.\", default=0)\n+    (opts, args) = parser.parse_args()\n+    \n+    output = \"\"\n+    domains = []\n+    urlstring = \"http://%s:%s/cluster_master/ws/\" % (opts.server, opts.port)\n+    post = {\"format\":opts.format, \"verbosity\":opts.verbosity}\n+    \n+    if args:\n+        domains = \",\".join(args)\n+    elif opts.list:\n+        try:\n+            domainlist = []\n+            for d in open(opts.list, \"r\").readlines():\n+                domainlist.append(d.strip())\n+            domains = \",\".join(domainlist)\n+        except IOError:\n+            print \"Can't open file %s\" % opts.list\n+    \n+    if opts.status:\n+        pass\n+    elif opts.schedule and domains:\n+        post[\"schedule\"] = domains\n+        if opts.now:\n+            post[\"priority\"] = \"PRIORITY_NOW\"\n+            post[\"settings\"] = \"UNAVAILABLES_NOTIFY=2\"\n+    elif opts.remove and domains:\n+        post[\"remove\"] = domains\n+    elif opts.stop and domains:\n+        post[\"stop\"] = domains\n+    elif opts.disable_node:\n+        post[\"disable_node\"] = opts.disable_node\n+    elif opts.enable_node:\n+        post[\"enable_node\"] = opts.enable_node\n+    else:\n+        parser.print_help()\n+        return\n+    \n+    f = urllib.urlopen(urlstring, urllib.urlencode(post))\n+    output=f.read()\n+\n+    if not opts.output:\n+        print output\n+    else:\n+        try:\n+            open(opts.output, \"w\").write(output)\n+        except IOError:\n+            open(\"/tmp/decobot-schedule.tmp\", \"w\").write(output)\n+            print \"Could not open file %s for writing. Output dumped to /tmp/decobot-schedule.tmp instead.\" % opts.output\n+\n+if __name__ == '__main__':\n+    main()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
